Discriminative Hidden Markov Modeling with Long State Dependence  
using a kNN Ensemble 
ZHOU GuoDong 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
Email: zhougd@i2r.a-star.edu.sg 
 
Abstract 
This paper proposes a discriminative HMM 
(DHMM) with long state dependence (LSD-
DHMM) to segment and label sequential data. 
The LSD-DHMM overcomes the strong context 
independent assumption in traditional generative 
HMMs (GHMMs) and models the sequential data 
in a discriminative way, by assuming a novel 
mutual information independence. As a result, the 
LSD-DHMM separately models the long state 
dependence in its state transition model and the 
observation dependence in its output model. In 
this paper, a variable-length mutual information-
based modeling approach and an ensemble of 
kNN probability estimators are proposed to 
capture the long state dependence and the 
observation dependence respectively. The 
evaluation on shallow parsing shows that the 
LSD-DHMM not only significantly outperforms 
GHMMs but also much outperforms other 
DHMMs. This suggests that the LSD-DHMM can 
effectively capture the long context dependence to 
segment and label sequential data. 
1. Introduction 
A Hidden Markov Model (HMM) is a model 
where a sequence of observations is generated in 
addition to the Markov state sequence. It is a latent 
variable model in the sense that only the 
observation sequence is known while the state 
sequence remains ?hidden?. In recent years, 
HMMs have enjoyed great success in many 
tagging applications, most notably part-of-speech 
(POS) tagging (Church 1988; Weischedel et al
1993; Merialdo 1994) and named entity 
recognition (Bikel et al1999; Zhou et al2002). 
Moreover, there have been also efforts to extend 
the use of HMMs to word sense disambiguation 
(Segond et al1997) and shallow/full parsing 
(Brants et al1997; Skut et al1998; Zhou et al
2000). 
Traditionally, a HMM segments and labels 
sequential data in a generative way, assigning a 
joint probability to paired observation and state 
sequences. More formally, a generative (first-order) 
HMM (GHMM) is given by a finite set of states  
including an designated initial state and an 
designated final state, a set of possible observation 
, two conditional probability distributions: a 
state transition model from s  to ,  for 
and an output model,  for 
S
O
s ,'
o
' s )|( 'ssp
)|( sopSs?
sO S?? ,
)| 'ss
. A sequence of observations is 
generated by starting from the designated initial 
state, transmiting to a new state according to 
, emitting an observation selected by that 
new state according to p , transmiting to 
another new state and so on until the designated 
final state is generated.  
(p
)( s|o
There are several problems with this generative 
approach. First, many tasks would benefit from a 
richer representation of observations?in particular 
a representation that describes observations in 
terms of many overlapping features, such as 
capitalization, word endings, part-of-speech in 
addition to the traditional word identity. Note that 
these features always depends on each other. 
Furthermore, to define a joint probability over the 
observation and state sequences, the generative 
approach needs to enumerate all the possible 
observation sequences. However, in some tasks, 
the set of all the possible observation sequences is 
not reasonably enumerable. Second, the generative 
approach fails to effectively model the dependence 
in the observation sequence. Moreover, it is 
difficult for the generative approach to model the 
long state dependence since it is not reasonably 
practical for ngram modeling(e.g. bigram for the 
first-order GHMM and trigram for the secnod-
order GHMM) to be beyond trigram. Third, the 
generative approach normally estimates the 
parameters to maximize the likelihood of the 
observation sequence. However, in many NLP 
tasks, the goal is to predict the state sequence given 
the observation sequence. In other words, the 
generative approach inappropriately applies a 
generative joint probability model for a conditional 
probability problem. In summary, the main reasons 
behind these problems of the generative approach 
are the strong context independent assumption and 
the generative nature in modeling sequential data. 
While the dependence between successive states 
can be directly modeled by its state transition 
model, the generative approach fails to directly 
capture the observation dependence in the output 
model. From this viewpoint, a GHMM can be also 
called an observation independent HMM. 
To resolve above problems in GHMMs, some 
researches have been done to move from  the 
generative approach to the discriminative approach. 
Discriminative HMMs (DHMMs) do not expend 
modeling effort on the observation sequnce, which 
are fixed at test time. Instead, DHMMs model the 
state sequence depending on arbitrary, non-
independent features of the observation sequence, 
normally without forcing the model to account for 
the distribution of those dependencies. Punyakanok 
and Roth (2000) proposed a projection-based 
DHMM (PDHMM) which represents the 
probability of a state transition given not only the 
current observation but also past and future 
observations and used the SNoW classifier (Roth 
1998, Carlson et al1999) to estimate it (SNoW-
PDHMM thereafter). McCallum et al(2000) 
proposed the extact same model and used 
maximum  entropy to estimate it (ME-PDHMM 
thereafter). Lafferty et al(2001) extanded ME-
PDHMM using conditional random fields by 
incorporating the factored state representation of 
the same model (that is, representing the 
probability of a state given the observation 
sequence and the previous state)  to alleviate the 
label bias problem in projection-based DHMMs, 
which can be biased towards states with few 
successor states (CRF-DHMM thereafter). Similar 
work can also be found in Bouttou (1991). 
Punyakanok and Roth (2000) also proposed a non-
projection-based DMM which separates the 
dependence of a state on the previous state and the 
observation sequence, by rewriting the GHMM in 
a discriminative way and heuristically extending 
the notation of an observation to the observation 
sequence. Zhou et al(2000) systematically derived 
the exact same model as in Punyakanok and Roth 
(2000) and used back-off modeling to esimate the 
probability of a state given the observation 
sequence (Backoff-DHMM thereafter) while 
Punyakanok and Roth (2000) used the SNoW 
classifier to estimate it(SNoW-DHMM thereafter). 
This paper follows our previous work in Zhou 
et al(2000) and proposes an alternative non-
projection-based DHMM with long state 
dependence (LSD-DHMM), which separates the 
dependence of a state on the previous states and 
the observation sequence. Moreover, a variable-
length mutual information based modeling 
approach (VLMI) is proposed to capture the long 
state dependence of a state on the previous states. 
In addition, an ensemble of kNN probability 
estimators is proposed to capture the observation 
dependence of a state on the observation sequence. 
Experimentation shows that VLMI effectively 
captures the long state dependence. It also shows 
that the kNN ensemble captures the dependence 
between the features of the observation sequence 
more effectively than classifier-based approaches, 
by forcing the model to account for the distribution 
of those dependencies.  
The layout of this paper is as follows. Section 2 
first proposes the LSD-DHMM and then presents 
the VLMI to capture the long state dependence. 
Section 3 presents the kNN probability estimator to 
capture the observation dependence while Section 
4 presents the kNN ensemble. Section 5 introduces 
shallow parsing, while experimental results are 
given in Section 6. Finally, some conclusion will 
be drawn in Section 7. 
2. LSD-DHMM: Discriminative HMM with 
Long State Dependence 
In principle, given an observation sequence 
, the goal of a conditional 
probability model is to find a stochastic optimal 
state sequence s  that maximizes 
  
n
n oooo L211 =
)|(log 11
nn osp
n
n sss L211 =
)|(logmaxarg 11
*
1
nn
s
ospS
n
=                 (1) 
By applying the Bayes? rule, we can rewrite the 
equation (1) as: 
}),()({logmaxarg
)}|({logmaxarg
111
11
*
1
1
nnn
s
nn
s
osMIsp
osps
n
n
+=
=
         (2) 
Obviously, the second term MI  
captures the mutual information between the state 
sequence  and the observation sequence o . To 
compute  efficiently, we propose a 
novel mutual information independence 
assumption: 
),( 11
nn os
n
1
ns1
MI ),( 11
nn os
?
=
=
n
i
n
i
nn osMIosMI
1
111 ),(),( or  
?
=
=?
n
i
nn
nn
popsp
osp
111
11 log
)()(
),(log ? ni
n
i
ops
osp
1
1
)()(
),(
     (3) 
That is, we assume a state is only dependent on 
the observation sequence o  and independent on 
other states in the state sequence s . This 
assumption is reasonable because the dependence 
among the states in the state sequence  has been 
n
1
n
1
n
1s
directly captured by the first term log in 
equation (2). 
)( 1
nsp
|(log
)(log
})|(
)(
1
2
1
1
?
=
n
i
n
i
n
i
osp
sp
os
sp
?is 11 )
By applying the assumption (3) into the 
equation (2) and using the chain rule, we have: 
})),({maxarg
})|(log
)|(log{maxarg
log)(log
log)|(log{maxarg
12
1
1
1
1
2
1
1
11
2
1
1
*
1
1
1
??
?
?
??
?
==
?
=
=
?
==
=
?
+=
+
?=
+?
+=
n
i
n
i
i
i
s
n
i
n
i
i
n
i
i
i
s
n
i
n
i
i
n
i
i
i
s
ssMI
osp
ssp
psp
ssps
n
n
n
         (4) 
The above model consists of two models: the 
state transition model ?  which 
measures the state dependence of a state given the 
previous states, and the output model 
which measures the observation 
dependence of a state given the observation 
sequence in a discriminative way. Therefore, we 
call the above model as in equation (4) a 
discriminative HMM (DHMM) with long state 
dependence (LSD-DHMM). The LSD-DHMM 
separates the dependence of a state on the previous 
states and the observation sequence. The main 
difference between a GHMM and a LSD-DHMM 
lies in their output models in that the output model 
of a LSD-DHMM directly captures the context 
dependence between successive observations in 
determining the ?hidden? states while the output 
model of the GHMM fails to do so. That is, the 
output model of a LSD-DHMM overcomes the 
strong context independent assumption in the 
GHMM and becomes observation context 
dependent. Therefore, the LSD-DHMM can also 
be called an observation context dependent HMM. 
Compared with other DHMMs, the LSD-DHMM 
explicitly models the long state dependence and 
the non-projection nature of the LSD-DHMM 
alleviates the label bias problem inherent in 
projection-based DHMMs. 
=
n
i
isMI
2
,(
?
=
n
i
n
i osp
1
1 )|(log
Computation of a LSD-DHMM consists of two 
parts. The first is to compute the state transition 
model: . Traditionally, ngram 
modeling(e.g. bigram for the first-order GHMM 
and trigram for the second-order GHMM) is used 
to estimate the state transition model. However, 
such approach fails to capture the long state 
dependence since it is not reasonably practical for 
ngram modeling to be beyond trigram. In this 
paper, a variable-length mutual information-based 
modeling approach (VLMI) is proposed as follow: 
For each i
?
=
?n
i
i
i ssMI
2
1
1 ),(
)2( ni ?? , we first find a minimal 
)i0( kk p?  where the frequency of  s  is 
bigger than a threshold (e.g. 10) and then estimate 
using 
1?i
k
)1?,( 1
i
i ssMI ))(
(
),( 1? ?= i
i
ki
ki psp
sp
ss
)
)| 1
no
)| ii Es
Nio +
(
)
1?i
ks
MI
n
i osp 1|(log
( is
?)|( 1ni os (p
iNi oo ?
. 
In this way, the long state dependence can be 
captured maximally in a dynamical way. Here, the 
frequencies of variable-length state sequences are 
estimated using the simple Good-Turing approach 
(Gale et al1995). 
?
=
n
i 1
p
iE = LL
)|( iE?
The second is to estimate the output 
model: . Ideally, we would have 
sufficient training data for every event whose 
conditional probability we wish to calculate. 
Unfortunately, there is rarely enough training data 
to compute accurate probabilities when decoding 
on new data. Traditionally, there are two existing 
approaches to resolve this problem: linear 
interpolation (Jelinek 1989) and back-off (Katz 
1987). However, these two approaches only work 
well when the number of different information 
sources is limited. When a long context is 
considered, the number of different information 
sources is exponential and not reasonably 
enumerable. The current tendency is to recast it as 
a classification problem and use the output of a 
classifier, e.g. the maximum entropy classifier 
(Ratnaparkhi 1999) to estimate the state probability 
distribution given the observation sequence. In the 
next two sections, we will propose a more effective 
ensemble of kNN probability estimators to resolve 
this problem. 
3. kNN Probability Estimator 
The main challenge for the LSD-DHMM is how to 
reliably estimate p  in its output model. 
For efficiency, we can always 
assume , where the pattern 
entry . That is, we only 
consider the observation dependence in a window 
of 2N+1 observations (e.g. we only consider the 
current observation, the previous observation and 
the next observation when N=1). For convenience, 
we denote P  as the conditional state 
probability distribution of the states given E  and i
)|( ii Esp
is iE
)|( iEP ?
)(EkNN i
)|( iEP ?
FrequentEn
FrequentEn
|(? kNNEp ki
 as the conditional state probability of 
 given .  
=
=? iEP )|(
The kNN probability estimator estimates 
 by first finding the K nearest neighbors 
of frequently occurring pattern entries  
and then 
aggregating them to make a proper estimation of 
. Here, the conditional state probability 
distribution is estimated instead of the 
classification in a traditional kNN classifier. To do 
so, all the frequently occurring pattern entries are 
extracted from the training corpus in an exhaustive 
way and stored in a dictionary 
. In order to limit the 
dictionary size and keep efficiency, we constrain a 
valid set of pattern entry forms ValidEntry  
to consider only the most informative information 
sources. Generally, ValidEntry  can be 
determined manually or automatically according to 
the applications. In Section 5, we will give an 
example. 
},...,2,1|{ KkE ki =
arytryDiction
Form
Form
Given a pattern entry E  and a dictionary of 
frequently occurring pattern entries 
, a simple algorithm is 
applied to find the K nearest neighbors of the 
pattern entry  from the dictionary as follows: 
i
arytryDiction
iE
? compare  with each entry in the dictionary  
and find all the compatible entries 
iE
? compute the cosine similarity between E  and 
each of the compatible entries 
i
? sort out the K nearest neighbors according to 
their cosine similarities 
Finally, the conditional state probability 
distribution of the pattern entry is aggregated over 
those of its K nearest neighbors weighted by their 
frequencies and cosine similarities 
:  
)( kiEf
)
?
?
=
=
?
???
K
k
k
i
k
i
K
k
k
i
k
i
k
i
EfkNNEp
EPEfkNNEp
1
1
)()|(?
)|()()|(?
      (5) p
4. kNN Ensemble 
In the literature, an ensemble has been widely used 
in the classification problem to combine several 
classifiers (Breiman 1996; Hamamoto 1997; 
Dietterich 1998; Zhou Z.H. et al2002; Kim et al
2003). It is well known that an ensemble often 
outperforms the individual classifiers that make it 
up (Hansen et al1990). 
In this paper, an ensemble of kNN probability 
estimators is proposed to estimate the conditional 
state probability distribution P  instead of 
the classification. This is done through a bagging 
technique (Breiman 1996) to aggregate several 
kNN probability estimators. In bagging, the M 
kNN probability estimators in the ensemble 
)|( iE?
}M,...,2,1|{ mkNNENS m == are trained 
independently via a bootstrap technique and then 
they are aggregated via an appropriate aggregation 
method. Usually, we have a single training set and 
need M training sample sets to construct a kNN 
ensemble with M independent kNN probability 
estimators. From the statistical viewpoint, we need 
to make the training sample sets different as much 
as possible in order to obtain a higher aggregation 
performance. For doing this, we often use the 
bootstrap technique which builds M replicate data 
sets by randomly re-sampling with replacement 
from the given training set repeatedly. Each 
example in the given training set may appear 
repeatedly or not at all in any particular replicate 
training sample set. Each training sample set is 
used to train a certain kNN probability estimator. 
Finally, the conditional state probability 
distribution of the pattern entry E  is averaged 
over those of the M kNN probability estimators in 
the ensemble:  
i
M
kNNEP
EP
M
m
mi
i
?
=
?
=? 1
),|(
)|(               (6) 
5. Shallow Parsing 
In order to evaluate the LSD-DHMM and the 
proposed variable-length mutual information 
modeling approach for the long state dependence 
in the state transition model and the kNN ensemble 
for the observation dependence in the output model, 
we have applied it in the application of shallow 
parsing. 
For shallow parsing, we have o , where 
 is the word sequence and 
 is the part-of-speech (POS) 
sequence, while the ?hidden? states are represented 
as structural tags to bracket and differentiate 
various categories of phrases. The basic idea of 
using the structural tags to represent the ?hidden? 
states is similar to Skut et al(1998) and Zhou et al
(2000).  Here, a structural tag consists of three 
parts: 
ii wp=1
n
n wwww L211 =
n
n ppp L211 =
? Boundary Category (BOUNDARY): it is a set 
of four values: ?O?/?B?/?M?/?E?, where ?O? 
means that current word is a whOle phrase and 
?B?/?M?/?E? means that current word is at the 
Beginning/in the Middle/at the End of a phrase. 
? Phrase Category (PHRASE): it is used to 
denote the category of the phrase. 
? Part-of-Speech (POS): Because of the limited 
number of boundary and phrase categories, the 
POS is added into the structural tag to represent 
more accurate state transition and output 
models. 
For example, given the following POS tagged 
sentence as the observation sequence: 
He/PRP  reckons/VBZ  the/DT  current/JJ  
account/NN  deficit/NN  will/MD  narrow/VB  
to/TO  only/RB $/$  1.8/CD  billion/CD  in/IN  
September/NNP  ./. 
We can have a corresponding sequence of  
structural tags as the ?hidden? state sequence: 
O_NP_PRP(He/PRP)  O_VP _VBZ 
(reckons/VBZ)  B_NP _DT (the/DT)  M_NP _JJ 
(current/JJ)  M_NP _NN (account/NN)  E_NP 
_NN (deficit/NN)  B_VP _MD (will/MD)  E_VP 
_VB (narrow/VB)  O_PP _TO (to/TO)  B_QP _RB 
(only/RB)  M_QP _$ ($/$)  M_QP _CD (1.8/CD)  
E_QP _CD (billion/CD)  O_PP _IN (in/IN)  O_NP 
_NNP(September/NNP)  O_O _. (./.) 
and an equivalent phrase chunked sentence as the 
shallow parsing result: 
[NP He/PRP] [VP reckons/VBZ] [ NP the/DT 
current/JJ account/NN deficit/NN] [VP will/MD 
narrow/VB] [PP to/TO] [QP only/RB $/$ 1.8/CD 
billion/CD] [PP in/IN] [NP September/NNP] [O ./.] 
6. Experimentation 
The corpus used in shallow parsing is extracted 
from the PENN TreeBank (Marcus et al 1993) of 
1 million words (25 sections) by a program 
provided by Sabine Buchholz from Tilburg 
University. All the evaluations are 5-fold cross-
validated. For shallow parsing, we use the F-
measure to measure the performance. Here, the F-
measure is the weighted harmonic mean of the 
precision (P) and the recall (R): 
PR
RP
+
+= 2
2 )1(
?
?F  
with =1 (Rijsbergen 1979), where the precision 
(P) is the percentage of predicted phrase chunks 
that are actually correct and the recall (R) is the 
percentage of correct phrase chunks that are 
actually found.  
2?
Tables 1, 2 and 3 show the detailed 
performance of LSD-DHMMs. In this paper, the 
valid set of pattern entry forms ValidEntry  
is defined to include those pattern entry forms 
within a windows of 7  observations(including 
current, left 3 and right 3 observations) where for 
 to be included in a pattern entry, all or one of 
the overlapping features in each of 
Form
jw
,p j )(...,1 ijpp ij ?+  or )(...,, 1 jipp jipi ?+
)(..., 11 jipp ji p?+
 
should be included in the same pattern entry while 
for  to be included in a pattern entry, all or one 
of the overlapping features in each of  
 or  
should be included in the same pattern entry. 
jp
(...,, 21 ijpp ij p+ )p j+ ,pi
Table 1 shows the effect of different number of 
nearest neighbors in the kNN probability estimator 
and considered previous states in the variable-
length mutual information modeling approach of 
the LSD-DHMM, using only one kNN probability 
estimator in the ensemble to estimate in 
the output model. It shows that finding 3 nearest 
neighbors in the kNN probability estimator 
performs best. It also shows that further increasing 
the number of nearest neighbors does not increase 
or even decrease the performance. This may be due 
to introduction of noisy neighbors when the 
number of nearest neighbors increases. Moreover, 
Table 1 shows that the LSD-DHMM performs best 
when six previous states is considered in the 
variable-length mutual information-based 
modeling approach and further considering more 
previous states only slightly increase the 
performance. This suggests that the state 
dependence exists well beyond traditional ngram 
modeling (e.g. bigram and trigram) to six previous 
states and the variable-length mutual information-
based modeling approach can capture the long 
state dependence. In the following experimentation, 
we will only use the LSD-DHMM with 3 nearest 
neighbors used in the kNN probability estimator 
and 6 previous states considered in the variable-
length mutual information modeling approach.  
)|( 1
n
i osp
Table 2 shows the effect of different number of 
kNN probability estimators in the ensemble. It 
shows that 15 bootstrap replicates are enough for 
the k-NN ensemble on shallow parsing and 
increase the F-measure by 0.71 compared with the 
ensemble of only one kNN probability estimator. 
Table 3 compares the LSD-DHMM with 
GHMMs and other DHMMs. It shows that all the 
DHMMs significantly outperform GHMMs due to 
the modeling of the observation dependence and 
allowing for non-independent, difficult to 
enumerate observation features. It also shows that 
our LSD-DHMM much outperforms other 
DHMMs due to the modeling of the long state 
dependence using the variable-length mutual 
information-based modeling approach in the LSD-
DHMM. Moverover, Table 3 shows that no-
projection-based DHMMs (i.e. CRF-DHMM, 
SNoW-DHMM, Backoff-DHMM and LSD-
DHMM) outperform projection-based DHMMs. It 
may be  due to alleviation of the label bias problem 
inherent in the projection-based DHMMs. Finally, 
Table 2 also compares the kNN ensemble with 
popular classifier-based approaches, such as 
SNoW and Maximum Entropy, in estimating the 
output model of the LSD-DHMM. It shows that 
the kNN ensemble outperforms these classifier-
based approaches. This suggests that the kNN 
ensemble captures the dependence between the 
features of the observation sequence more 
effectively by forcing the model to account for the 
distribution of those dependencies. 
Table 1: Effect of different numbers of nearest neighbors in the kNN probability estimator and previous 
states considered in the variable-length mutual information modeling approach of the LSD-DHMMs, using 
only a probability estimator in the ensemble 
Number of nearest neighbors Shallow Parsing 
1 2 3 4 5 
1 93.12 93.50 93.76 93.70 93.66 
2 93.65 93.82 94.23 94.19 94.12 
4 93.90 94.15 94.42 94.38 94.35 
6 94.12 94.28 94.53 94.54 94.51 Nu
m
be
r o
f 
co
ns
id
er
ed
 
pr
ev
io
us
 
st
at
es
 
8 94.15 94.35 94.55  94.52 94.50 
 
Table 2: The Effect of different number of kNN 
probability estimators in the ensemble on shallow 
parsing 
Number of kNN probability 
estimators in the ensemble 
F-measure 
1 94.53 
2 94.77 
4 94.93 
8 95.06 
14 95.21 
15 95.24 
16 95.24 
20 95.25 
25 95.25 
28 95.36 
 
Table 3: Comparison of LSD-DHMMs with  
GHMMs and other DHMMs 
Models F 
First order 92.14 GHMMs 
Second order 92.41 
ME-PDMM 93.26  
CRF-DMM 94.04 
SNoW-PDMM 93.44 
SNoW-DMM 94.12 
Backoff-DMM 93.68 
LSD-DMM(Ensemble) 95.24 
LSD-DMM(ME) 94.25 
DHMMs 
LSD-DMM(SNoW) 94.41 
7. Conclusion 
Hidden Markov Models (HMMs) are a powerful 
probabilistic tool for modeling sequential data and 
have been applied with success to many text-
related tasks, such as shallow paring. In these cases, 
the observations are usually modified as 
multinomial distributions over a discrete dictionary 
and the HMM parameters are set to maximize the 
likelihood of the observations. This paper presents 
a discriminative HMM with long state dependence 
that allows observations to be represented as 
arbitrary overlapping features and defines the 
conditional probability of the state sequence given 
the observation sequence. It does so by assuming a 
novel mutual information independence to separate 
the dependence of a state given the observation 
sequence and the previous states. Finally, the long 
state dependence and the observation dependence 
can be effectively captured by a variable-length 
mutual information model and a kNN ensemble 
respectively. 
In future work, we will explore our model in 
other applications, such as full parsing. 
References 
Bikel D.M., Schwartz R. & Weischedel R.M. 
(1999). An Algorithm that Learns What's in a 
Name. Machine Learning (Special Issue on NLP).  
34(3): 211-231. 
Bottou L. (1991). Une approche theorique de 
l?apprentissage connexionniste: Applications a la 
reconnaissance de la parole. Doctoral 
dissertation, Universite de Paris XI. 
Brants T., Skut W., & Krenn B. (1997). Tagging 
Grammatical Functions. Proceedings of the 
Conference on Empirical Methods on Natural 
Language Processing (EMNLP?1997). Brown 
Univ. RI. 
Carlson A, Cumby C. Rosen J. and Roth D. 1999. 
The SNoW learning architecture. Techinical 
Report UIUCDCS-R-99-2101. UIUC Computer 
Science Department.  
Church K.W. (1998). A Stochastic Pars Program 
and Noun Phrase Parser for Unrestricted Text. 
Proceedings of the Second Conference on 
Applied Natural Language Processing 
(ANLP?1998). Austin, Texas. 
Fausett L. (1994). Fundamentals of neural 
networks. Prentice Hall Press. 
Gale W.A. and Sampson G. 1995. Good-Turing 
frequency estimation without tears. Journal of 
Quantitative Linguistics. 2:217-237. 
Jelinek F. (1989). Self-Organized Language 
Modeling for Speech Recognition. In Alex 
Waibel and Kai-Fu Lee(Editors). Readings in 
Speech Recognitiopn. Morgan Kaufmann. 450-
506.  
Katz S.M. (1987). Estimation of Probabilities from 
Sparse Data for the Language Model Component 
of a Speech Recognizer. IEEE Transactions on 
Acoustics. Speech and Signal Processing. 35: 
400-401. 
Lafferty J. McCallum A and Pereira F. (2001). 
Conditional random fields: probabilistic models 
for segmenting and labeling sequence data. 
ICML-20. 
Marcus M., Santorini B. & Marcinkiewicz M.A. 
(1993). Buliding a large annotated corpus of 
English: The Penn Treebank. Computational 
Linguistics. 19(2):313-330. 
McCallum A. Freitag D. and Pereira F. 2000. 
Maximum entropy Markov models for 
information extraction and segmentation. ICML-
19. 591-598. Stanford, California. 
Merialdo B. (1994). Tagging English Text with a 
Probabilistic Model. Computational Linguistics. 
20(2): 155-171. 
Punyakanok V. and Roth D. (2000).   
The Use of Classifiers in Sequential Inference 
NIPS-13. 
Rabiner L.R. (1989). A Tutorial on Hidden 
Markov Models and Selected Applications in 
Speech Recognition?. Proceedings of the IEEE, 
77(2): 257-286. 
Ratnaparkhi A. 1999. Learning to parsing natural 
language with maximum entropy models. 
Machine Learning. 34:151-175. 
Roth D. 1998. Learning to resolve natural language 
ambiguities: A unified approach. In Proceedings 
of the National Conference on Artificial 
Intelligence. 806-813. 
Segond F., Schiller A., Grefenstette & Chanod F.P. 
(1997). An Experiment in Semantic Tagging 
using Hidden Markov Model Tagging. 
Proceedings of the Joint ACL/EACL workshop on 
Automatic Information Extraction and Building 
of Lexical Semantic Resources. pp.78-81. Madrid, 
Spain. 
Skut W. & Brants T. (1998). Chunk Tagger ? 
Statistical Recognition of Noun Phrases. 
Proceedings of the ESSLLI?98 workshop on 
Automatic Acquisition of Syntax and Parsing. 
Univ. of Saarbrucken. Germany. 
van Rijsbergen C.J. (1979). Information Retrieval. 
Buttersworth, London. 
Viterbi A.J. (1967). Error Bounds for 
Convolutional Codes and an Asymptotically 
Optimum Decoding Algorithm. IEEE 
Transactions on Information Theory. 13: 260-269. 
Weischedel R., Meteer M., Schwartz R., Ramshaw 
L. & Palmucci J. (1993). Coping  with Ambiguity 
and Unknown Words through Probabilistic 
Methods. Computational Linguistics. 19(2): 359-
382. 
Zhou GuoDong & Su Jian, (2000). Error-driven 
HMM-based Chunk Tagger with Context-
Dependent Lexicon. Proceedings of the Joint 
Conference on Empirical Methods on Natural 
Language Processing and Very Large Corpus 
(EMNLP/ VLC'2000). Hong Kong. 
Zhou GuoDong & Su Jian. (2002). Named Entity 
Recognition Using a HMM-based Chunk Tagger, 
Proceedings of the Conference on Annual 
Meeting for Computational Linguistics 
(ACL?2002). 473-480, Philadelphia.  
Modeling of Long Distance Context Dependency  
 
ZHOU GuoDong  
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
Email: zhougd@i2r.a-star.edu.sg 
 
 
Abstract 
Ngram models are simple in language 
modeling and have been successfully used in 
speech recognition and other tasks. However, 
they can only capture the short distance 
context dependency within an n-words 
window where currently the largest practical n 
for a natural language is three while much of 
the context dependency in a natural language 
occurs beyond a three words window. In order 
to incorporate this kind of long distance 
context dependency in the ngram model of our 
Mandarin speech recognition system, this 
paper proposes a novel MI-Ngram modeling 
approach. This new MI-Ngram model consists 
of two components: a normal ngram model 
and a novel MI model. The ngram model 
captures the short distance context dependency 
within an n-words window while the MI 
model captures the context dependency 
between the word pairs over a long distance 
by using the concept of mutual information. 
That is, the MI-Ngram model incorporates the 
word occurrences beyond the scope of the 
normal ngram model. It is found that MI-
Ngram modeling has much better performance 
than the normal word ngram modeling. 
Experimentation shows that about 20% of 
errors can be corrected by using a MI-Trigram 
model compared with the pure word trigram 
model.   
1 Introduction 
Language modeling is the attempt to 
characterize, capture and exploit the 
regularities and constraints in a natural 
language and has been successfully applied to 
many domains. Among all the language 
modeling approaches, ngram models have 
been most widely used in speech recognition 
(Jelinek 1990; Gale and Church 1990; Brown 
et al 1992; Yang et al 1996) and other 
applications. While ngram models are simple 
in language modeling and have been 
successfully used in speech recognition and 
other tasks, they have obvious deficiencies. 
For instance, ngram models can only capture 
the short-distance dependency within an n-
words window where currently the largest 
practical N for a natural language is three.  
In the meantime, it is found that there 
always exist many preferred relationships 
between words. Two highly associated word 
pairs are ?not only/but also? and 
?doctor/nurse?. Psychological experiments in 
Meyer D. et al (1975) indicated that the 
human?s reaction to a highly associated word 
pair was stronger and faster than that to a 
poorly associated word pair. Such preference 
information is very useful for natural language 
processing (Church K.W. et al 1990; Hiddle 
D. et al 1993; Rosenfeld R. 1994 and Zhou 
G.D. et al1998). Obviously, the preference 
relationships between words can expand from 
a short to long distance. While we can use 
conventional ngram models to capture the 
short distance dependency, the long distance 
dependency should also be exploited properly. 
The purpose of this paper is to propose a 
new modeling approach to capture the context 
dependency over both a short distance and a 
long distance and apply it in Mandarin speech 
recognition. 
This paper is organized as follows. In 
Section 2, we present the normal ngram 
modeling while a new modeling approach, 
named MI-ngram modeling, is proposed in 
Section 3. In Section 4, we will describe its 
use in our Mandarin speech recognition 
system. Finally we give a summary of this 
paper. 
2 Ngram Modeling 
Let , where ?s are the 
words that make up the hypothesis, the 
m
m wwwwS ...211 == iw
probability of the word string, , can be 
computed by using the chain rule: 
)(SP
| 11
?i
i ww
| 11
?i
i w
1?n
)| 1?iw
niw ?1
)...|( 11 ?nn wwwP    for all Vwww ni ?,...,,1 . 
Given mwwwS ...21= , an ngram model 
estimates the log probability of the word 
string, log , by re-writing Equation (2.2): )(SP
?
=
?=
m
i
i
i wwPwPSP
2
1
11 )|()()(                      (2.1) 
By taking a log function to both sides of 
Equation (2.1), we have the log probability of 
the word string, log : )(SP
??
=
?+=
1
2
1
11 )|(log)(log)(log
n
i
i
ingram wwPwPSP
?
=
?
+?+
m
ni
i
nii wwP )|(log
1
1                      (2.6) )(log)(log)(log
2
1
=
?+= m
i
PwPSP   (2.2) 
where  is the string length,  is the i -th 
word in the string .  
m iw
SSo, the classical task of statistical language modeling becomes how to effectively and 
efficiently predict the next word, given the 
previous words, that is to say, to estimate 
expressions of the form  . For 
convenience,  is often written as 
, where h , is called history.  
)(wP
)|( 11
?i
i wwP
1
1
?= iw)|( hwP i
From the ngram model as in Equation 
(2.3),  we have: 
)(
)(
)(
)(
1
1
1
1
1
1
1
1
?
+?
?
+?
?
?
? i
ni
i
i
ni
i
i
i
wP
wwP
wP
wwP
             
)()(
)(
)()(
)(
1
1
1
1
1
1
1
1
i
i
ni
i
i
ni
i
i
i
i
wPwP
wwP
wPwP
wwP
?
+?
?
+?
?
?
?                   
Traditionally, simple statistical models, 
known as ngram models, have been widely 
used in speech recognition. Within an ngram 
model, the probability of a word occurring 
next is estimated based on the  previous 
words. That is to say, 
)()(
)(
log
)()(
)(
log 1
1
1
1
1
1
1
1
i
i
ni
i
i
ni
i
i
i
i
wPwP
wwP
wPwP
wwP
?
+?
?
+?
?
?
?     (2.7) 
Obviously, the normal ngram model has 
the assumption: 
)|()|( 1 1
1
1
?
+?
? ? i niiii wwPwwP                        (2.3) )1,,()1,,( 1 111 =?= ? +?? dwwMIdwwMI ii niii  (2.8) 
For example, in bigram model (n=2) the 
probability of a word is assumed to depend 
only on the previous word: 
where 
)()(
)(log)1,,( 1
1
1
11
1
i
i
i
i
i
i
wPwP
wwPdwwMI ?
?
? ==
),( 11 i
i ww ?
 
is the mutual information of the word string 
pair , and 
)()(
)(log)1,, 1
1
1
1
1
i
i
ni
i
i
ni
i wPwP
wwPdw ?
+?
?
+?
+ ==
)iw d
( 1i niwMI
?
?
,( 1 1
i
niw
?
+?
 is 
the mutual information of the word string pair 
.  is the distance of the two word 
strings in  the word string pair and is equal 
to 1 when the two word strings are adjacent. 
)|()|( 1
1
1 ?
? ? iiii wwPwwP                      (2.4) 
And the probability  can be 
estimated by using maximum likelihood 
estimation (MLE) principle: 
( iwP
)(
)(
)|(
1
1
1
?
?
? =
i
ii
ii wC
wwC
wwP                      (2.5) 
Where  represents the number of times the 
sequence occurs in the training text. In 
practice, due to the data sparseness problem, 
some smoothing technique (e.g. Good Turing 
in [Chen and Goodman 1999]) is applied to get 
more accurate estimation.  
)(?C
For a pair (  over a distance  where 
 and 
), BA d
A B  are word strings, the mutual 
information  reflects the degree of 
preference relationship between the two 
strings over a distance . Several properties 
of the mutual information are apparent: 
)d,,( BAMI
dObviously, an ngram model assumes that 
the probability of the next word  is 
independent of the word string  in the 
history. The difference between bigram, 
trigram and other ngram models is the value of 
n. The parameters of an ngram model are thus 
the probabilities: 
iw ? For the same distance , d
),,(),,( dABMIdBAMI ? . 
? For different distances  and , 1d 2d
),,(),,( 21 dBAMIdBAMI ? . 
? If  and A B  are independent over a 
distance d ,  then  . 0),,( =dBAMI
)1,,()1,,( === dwXMIdwHMI ii
,,1 idww i =
  
     (MI )+           (3.4) 
),,( dBAMI  reflects the change of  the 
information content when two word strings  
and 
A
B   are correlated.  That is to say, the 
higher the value of ,  the stronger 
affinity  and 
),,( dBAMI
A B  have. Therefore, we can use 
the mutual information to measure the 
preference relationship degree of a word string 
pair. 
where  ,  and i . That is 
to say, the mutual information of the next 
word with the history is assumed equal to the 
summation of that of the next word with the 
first word in the history and that of the next 
word with the rest word string in the history. 
Then we can re-write Equation (3.3) by using 
Equation (3.4), 
1
1
?= iwH 12?= iwX N>
Using an alternative view of equivalence, 
an ngram model is one that partitions the data 
into equivalence classes based on the last n-1 
words in the history. Viewed in this way, a 
bigram induces a partition based on the last 
word in the history. A trigram model further 
refines this partition by considering the next-
to-last word and so on.  
)|(log HwP i   
)1,,()(log ii wHMIwP +=  
),,()1,,()(log 1 iwwMIwXMIwP iii ++=  
),,(
)()(
)(log)(log 1 iwwMIXPwP
XwPwP i
i
i
i ++=  
),,(
)(
)(log 1 iwwMIXP
XwP
i
i +=  As the word trigram model is most widely 
used in current research, we will mainly 
consider the word trigram-based model. By re-
writing Equation (2.2), the word trigram model 
estimates the log probability of the string 
 as: )(log SP
),,()|(log 1 iwwMIXwP ii +=                    (3.5) 
That is, we have             
),,()|(log)|(log 1
1
2
1
1 iwwMIwwPwwP i
i
i
i
i += ??
                )|log()(log)(log 121 wwwPSPTrigram +=
?
=
?
?
m
i
i
ii wwP
3
1
2 )|(log
 
            +             (2.9) 
        (3.6) 
    By applying Equation (3.6) repeatedly, we 
have a modified estimation of the conditional 
probability: 3 MI-Ngram Modeling 
Given  and  
,  we have  
121
1
1 ... ?
? == ii wwwwH
132 ... ?iww
1
2
? == i wwX ),,()|(log
)|(log
1
1
2
1
1
iwwMIwwP
wwP
i
i
i
i
i
+= ?
?
)|(log 13
?= ii wwP
),,()1,,( 12 iwwMIiwwMI ii +?+  XwH 1=                        (3.1) 
and L L L 
)|()|( 1XwwPHwP ii = .          (3.2) 
)|(log 1 1
?
+?= i nii wwP ??=
=
+?+
nik
k
ik kiwwMI
1
)1,,(  By taking a log function to both sides of 
Equation (3.2), we have 
        (3.7)                  
)|(log HwP i  Obviously, the first item in equation (3.7) 
contributes to the log probability of the normal 
word ngram within an N-words window while 
the second item is the mutual information 
which contributes to the long distance context 
dependency of the next word  with the 
previous words  
outside the n-words window of the normal 
word ngram model. 
wi
i ? ),1( NiNjwj >??
)(
)(log
HP
HwP i=                
)()(
)(log)(log
i
i
i wPHP
HwPwP +=
)1,,()(log HwMIwP ii +=                           (3.3) 
Now we assume  
By using Equation (3.7) iteratively, 
Equation (2.2) can be re-written as: ? ?=
?=
=
+?+
m
i
ik
k
ik kiwwMI
4
3
1
)1,,(                     (3.10) 
)|(log)(log
)(log
1
1
2
1
?
=
?+= iim
i
wwPwP
SP
?=
=
?+=
ni
i
i
i wwwP
2
1
11 )|log()(log
)|(log 11
n
n wwP ++ (log
2+=
?+ im
ni
wP
?=
=
?+=
ni
i
i
i wwwP
2
1
11 )|log()(log
)|(log 21
n
n wwP ++
)| 11
?iw               
 
Compared with the normal word ngram 
model, the novel MI-Ngram model also 
incorporates the long distance context 
dependency by computing the mutual 
information of the distance dependent word 
pairs. That is, the MI-Ngram model 
incorporates the word occurrences beyond the 
scope of the normal ngram model. 
Since the number of possible distance-
dependent word pairs may be very huge, it is 
impossible for the MI-Ngram model to 
incorporate all the possible distance-dependent 
word pairs. Therefore, for the MI-Ngram 
model to be practically useful, how to select a 
reasonable number of word pairs becomes 
most important. Here two approaches are used 
(Zhou G.D., et al1998): 
)1,,( 11 ++ + nwwMI n )|(log 11
2
?
+=
?+ iim
ni
wwP  
L L L 
?=
=
?+=
ni
i
i
i wwwP
2
1
11 )|log()(log
?
+=
?
?+
m
ni
i
ii wwP
1
1
2 )|(log  
One approach is to restrict the window size 
of possible word pairs by computing and 
comparing the conditional perplexities 
(Shannon C.E. 1951) of the long distance word 
bigram models for different distances. 
Conditional perplexity is a measure of the 
average number of possible choices there are 
for a conditional distribution. The conditional 
perplexity of a conditional distribution with 
the conditional entropy  is defined to 
be 2 . Given two random variables 
H Y X( | )
H Y X( | ) X and 
Y , a conditional probability mass function 
, and a marginal probability mass 
function , the conditional entropy of 
P y xY X| ( | )
P yY ( ) Y  
given X , , is defined as: H Y( | X )
                           
            (3.8) ? ?
+=
?=
=
+?+
m
ni
nik
k
ik kiwwMI
1 1
)1,,(
From Equation (3.8), we can see that the 
first three items are the values computed by 
the normal word trigram model as shown in 
Equation (2.9)  and the forth item 
 contributes to 
summation of the mutual information of the 
next word with the words in the history . 
Therefore, we call Equation (3.8) as a MI-
Ngram model and rewrite it as: 
? ?
+=
?=
=
+?
m
ni
nik
k
ik kiwwMI
1 1
)1,,(
niw ?1 ??
? ?
?=
Xx Yy
XYYX xyPyxPXYH )|(log),()|( |2,      
      (3.11) 
)(log
)(log
SP
SP
Ngram
NgramMI
=
?  
For a large enough corpus, the conditional 
perplexity is usually an indication of the 
amount of information conveyed by the model: 
the lower the conditional perplexity, the more 
information it conveys and thus a better model.  
This is because the model captures as much as 
it can of that information, and whatever 
uncertainty remains shows up in the 
conditional perplexity. Here, the corpus is the 
XinHua corpus, which has about 57M(million) 
characters or 29M words. For all the 
experiments, 80% of the corpus is used for 
? ?
+=
?=
=
+?+
m
ni
nik
k
ik kiwwMI
1 1
)1,,(                     (3.9) 
As a special case of N=3, the MI-Trigram 
model estimate the log probability of the string 
as follows: 
)(log
)(log
SP
SP
Trigram
TrigramMI
=
?  
training while the remaining 20% is used for 
testing. 
Table 1 shows that the conditional 
perplexity is lowest for d = 1 and increases 
significantly as we move through d = 2, 3, 4, 5 
and 6. For d = 7, 8, 9, the conditional 
perplexity increases slightly while further 
increasing d almost does not increase the 
conditional perplexity. This suggests that 
significant information exists only in the last 6 
words of the history. In this paper, we restrict 
the maximum window size to 10. 
Table 1: Conditional perplexities of the 
long-distance word bigram models for 
different distances 
Distanc
e 
Perplexity Distanc
e 
Perplexity 
1 230 7 1479 
2 575 8 1531 
3 966 9 1580 
4 1157 10 1599 
5 1307 11 1611 
6 1410 20 1647 
Another approach is to adapt average 
mutual information to select a reasonable 
number of distance-dependent word pairs: 
)()(
)(log),();(
BPAP
ABPBAPBAAMI =     
                  
)()(
)(log),(
BPAP
BAPBAP+  
                  +
)()(
)(log),(
BPAP
BAPBAP    
                  +
)()(
)(log),(
BPAP
BAPBAP          (3.12) 
Obviously, Equation (3.12) takes the joint 
probability into consideration. That is, those 
frequently occurring word pairs are more 
important and have much more potential to be 
incorporated into the MI-Ngram model than 
less frequently occurring word pairs. 
4 Experimentation 
We have evaluated the new MI-Ngram model 
in  an experimental speaker-dependent 
continuous Mandarin speech recognition 
system (Zhou G.D. et al1999). For base 
syllable recognition, 14 cepstral and 14 delta-
cepstral coefficients, energy(normalized) and 
delta-energy are used as feature parameters to 
form a feature vector with dimension 30, while 
for tone recognition, the pitch period and the 
energy together with their first order and 
second order delta coefficients are used to 
form a feature vector with dimension 6. All the 
acoustic units are modeled by semi-continuous 
HMMs (Rabiner 1993). For base syllable 
recognition, 138 HMMs are used to model 100 
context-dependent INITIALs and 38 context-
independent FINALs while 5 HMMs are used 
to model five different tones in Mandarin 
Chinese. 5,000 short sentences are used for 
training and another 600 sentences (6102 
Chinese characters) are used for testing.  All 
the training and testing data are recorded by 
one same speaker in an office-like laboratory 
environment with a sampling frequency of 
16KHZ. 
As a reference, the base syllable recognition 
rate and the tone recognition rate are shown in 
Table 2 and Table 3, respectively. As the word 
trigram model is most widely used in current 
research, all the experiments have been done 
using a MI-Trigram model which is trained on 
the XINHUA news corpus of 29 million 
words(automatically segmented) while the 
lexicon contains about 28000 words. As a 
result, the perplexities and Chinese character 
recognition rates of different MI-Trigram 
models with the same window size of 10 and 
different numbers of distance-dependent word 
pairs are shown in Table 4. 
Table 2: The top-n recognition rates of  base syllables 
Top-N Base Syllables  1  5 10 15 20 
Recognition Rate of Base Syllables 88.2 97.6 99.2 99.5 99.8 
 
Table 3: The recognition rates of the tones 
 tone 1 tone 2 tone 3 tone 4 tone 5 
tone 1    90.4     0.8       0.6       0.8       7.4 
tone 2     8.3   81.1       5.4       0.2       4.9 
tone 3     5.0   20.9     43.0     29.1      2.0 
tone 4     4.3     0.2       1.8     93.5      0.2 
tone 5     24.1     8.6       0.9       8.2    58.2 
Table 4: The effect of different numbers of word pairs in the MI-Trigram models with the same 
window size 10 on the Chinese character recognition rates 
Number of  word pairs  Perplexity Recognition Rate 
0 204 90.5 
100,000 196 91.2 
200,000 189 91.7 
400,000 183 92.1 
600,000 179 92.3 
800,000 175 92.4 
1,000,000 172 92.5 
1,500,000 171 92.5 
2,000,000 170 92.6 
2,500,000 170 92.5 
3,000,000 168 92.6 
3,500,000 169 92.6 
4,000,000 168 92.7 
Table 4 shows that the perplexity and the 
recognition rate rise quickly as the number of 
the long distance-dependent word pairs in the 
MI-Trigram model increase from 0 to 800,000, 
and then rise slowly. This suggests that the 
best 800,000 word pairs carry most of the long 
distance context dependency and should be 
included in the MI-Ngram model. It also 
shows that the recognition rate of the MI-
Trigram model with 800,000 word pairs is 
1.9% higher than the pure word trigram model 
(the MI-Trigram model with 0 long distance-
dependent word pairs). That is to say, about 
20% of errors can be corrected by 
incorporating only 800,000 word pairs to the 
MI-Trigram model compared with the pure 
word trigram model. 
It is clear that MI-Ngram modeling has 
much better performance than normal word 
ngram modeling. One advantage of MI-Ngram 
modeling is that its number of parameters is 
just a little more than that of word ngram 
modeling. Another advantage of MI-Ngram 
modeling is that the number of the word pairs 
can be reasonable in size without losing too 
much of its modeling power. Compared to 
ngram modeling, MI-Ngram modeling also 
captures the long distance dependency of word 
pairs using the concept of mutual information. 
4. CONCLUSION 
This paper proposes a novel MI-Ngram 
modeling approach to capture the context 
dependency over both a short distance and a 
long distance. This is done by incorporating 
long distance-dependent word pairs into 
normal ngram modeling by using the concept 
of mutual information. It is found that MI-
Ngram modeling has much better performance 
than word ngram modeling.  
REFERENCE 
Brown P.F. et al (1992). Class-based Ngram 
models of natural language. Computational 
Linguistics  18(4), 467-479. 
Chen S.F. and Goodman J. (1999). An 
empirical study of smoothing technique for 
language modeling. Computer, Speech and 
Language. 13(5). pp.359-394. 
Church K.W. et al (1991). Enhanced good 
Turing and Cat-Cal: two new methods for 
estimating probabilities of English 
bigrams. Computer, Speech and Language  
5(1), 19-54. 
Gale W.A. & Church K.W. (1990).  Poor 
estimates of context are worse than none. 
Proceedings of DARPA Speech and 
Natural Language Workshop, Hidden 
Valley, Pennsylvania, pp. 293-295. 
Hindle D. et al (1993). Structural ambiguity 
and lexical relations. Computational 
Linguistics  19(1),  103-120. 
Jelinek F. (1990). Self-organized language 
modeling for speech recognition. In 
Readings in Speech Recognition. Edited by 
Waibel A. and Lee K.F. Morgan Kaufman. 
San Mateo. CA. pp.450-506. 
Meyer D.  et al (1975). Loci of contextual 
effects on visual word recognition. In 
Attention and Performance V, edited by 
P.Rabbitt and S.Dornie. pp. 98-116. 
Acdemic Press. 
Rabiner L.R. et al (1993). Foundamentals to 
Speech Recognition. Prentice Hall. 
Rosenfeld R. (1994). Adaptive statistical 
language modeling: A Maximum Entropy 
Approach. Ph.D. Thesis, Carneige Mellon 
University. 
Shannon C.E. (1951). Prediction and entropy 
of printed English. Bell Systems Technical 
Journal   30, 50-64. 
Yang Y.J. et al (1996). Adaptive linguistic 
decoding system for Mandarin speech 
recognition applications. Computer 
Processing of Chinese & Oriental 
Languages  10(2), 211-224. 
Zhou G.D. and Lua K.T. (1998). Word 
association and MI-Trigger-based 
language modeling, COLING-ACL?98. 
Montreal Canada, 8-14 August. 
Zhou G.D. and Lua KimTeng (1999). 
Interpolation of N-gram and MI-based 
Trigger Pair Language Modeling in 
Mandarin Speech Recognition, Computer, 
Speech and Language, Vol. 13, No. 2, 
pp.123-135. 
 
An NP-Cluster Based Approach to Coreference Resolution
Xiaofeng Yang?? Jian Su? Guodong Zhou? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian,zhougd}
@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
{yangxiao,tancl}@comp.nus.edu.sg
Abstract
Traditionally, coreference resolution is done
by mining the reference relationships be-
tween NP pairs. However, an individual NP
usually lacks adequate description informa-
tion of its referred entity. In this paper,
we propose a supervised learning-based ap-
proach which does coreference resolution by
exploring the relationships between NPs and
coreferential clusters. Compared with indi-
vidual NPs, coreferential clusters could pro-
vide richer information of the entities for bet-
ter rules learning and reference determina-
tion. The evaluation done on MEDLINE
data set shows that our approach outper-
forms the baseline NP-NP based approach
in both recall and precision.
1 Introduction
Coreference resolution is the process of linking
as a cluster1 multiple expressions which refer
to the same entities in a document. In recent
years, supervised machine learning approaches
have been applied to this problem and achieved
considerable success (e.g. Aone and Bennett
(1995); McCarthy and Lehnert (1995); Soon et
al. (2001); Ng and Cardie (2002b)). The main
idea of most supervised learning approaches is to
recast this task as a binary classification prob-
lem. Specifically, a classifier is learned and then
used to determine whether or not two NPs in a
document are co-referring. Clusters are formed
by linking coreferential NP pairs according to a
certain selection strategy. In this way, the identi-
fication of coreferential clusters in text is reduced
to the identification of coreferential NP pairs.
One problem of such reduction, however,
is that the individual NP usually lacks ade-
quate descriptive information of its referred en-
tity. Consequently, it is often difficult to judge
whether or not two NPs are talking about the
1In this paper the term ?cluster? can be interchange-
ably used as ?chain?, while the former better emphasizes
the equivalence property of coreference relationship.
same entity simply from the properties of the
pair alone. As an example, consider the pair of a
non-pronoun and its pronominal antecedent can-
didate. The pronoun itself gives few clues for the
reference determination. Using such NP pairs
would have a negative influence for rules learn-
ing and subsequent resolution. So far, several
efforts (Harabagiu et al, 2001; Ng and Cardie,
2002a; Ng and Cardie, 2002b) have attempted to
address this problem by discarding the ?hard?
pairs and select only those confident ones from
the NP-pair pool. Nevertheless, this eliminat-
ing strategy still can not guarantee that the NPs
in ?confident? pairs bear necessary description
information of their referents.
In this paper, we present a supervised
learning-based approach to coreference resolu-
tion. Rather than attempting to mine the ref-
erence relationships between NP pairs, our ap-
proach does resolution by determining the links
of NPs to the existing coreferential clusters. In
our approach, a classifier is trained on the in-
stances formed by an NP and one of its possi-
ble antecedent clusters, and then applied dur-
ing resolution to select the proper cluster for an
encountered NP to be linked. As a coreferen-
tial cluster offers richer information to describe
an entity than a single NP in the cluster, we
could expect that such an NP-Cluster framework
would enhance the resolution capability of the
system. Our experiments were done on the the
MEDLINE data set. Compared with the base-
line approach based on NP-NP framework, our
approach yields a recall improvement by 4.6%,
with still a precision gain by 1.3%. These results
indicate that the NP-Cluster based approach is
effective for the coreference resolution task.
The remainder of this paper is organized as
follows. Section 2 introduces as the baseline the
NP-NP based approach, while Section 3 presents
in details our NP-Cluster based approach. Sec-
tion 4 reports and discusses the experimental re-
sults. Section 5 describes related research work.
Finally, conclusion is given in Section 6.
2 Baseline: the NP-NP based
approach
2.1 Framework description
We built a baseline coreference resolution sys-
tem, which adopts the common NP-NP based
learning framework as employed in (Soon et al,
2001).
Each instance in this approach takes the form
of i{NPj , NPi}, which is associated with a fea-
ture vector consisting of 18 features (f1 ? f18) as
described in Table 2. Most of the features come
from Soon et al (2001)?s system. Inspired by the
work of (Strube et al, 2002) and (Yang et al,
2004), we use two features, StrSim1 (f17) and
StrSim2 (f18), to measure the string-matching
degree of NPj and NPi. Given the following sim-
ilarity function:
Str Simlarity(Str1, Str2) = 100? |Str1 ? Str2|Str1
StrSim1 and StrSim2 are computed
using Str Similarity(SNPj , SNPi) and
Str Similarity(SNPi , SNPj ), respectively. Here
SNP is the token list of NP, which is obtained
by applying word stemming, stopword removal
and acronym expansion to the original string as
described in Yang et al (2004)?s work.
During training, for each anaphor NPj in a
given text, a positive instance is generated by
pairing NPj with its closest antecedent. A set
of negative instances is also formed by NPj and
each NP occurring between NPj and NPi.
When the training instances are ready, a clas-
sifier is learned by C5.0 algorithm (Quinlan,
1993). During resolution, each encountered noun
phrase, NPj , is paired in turn with each preced-
ing noun phrase, NPi. For each pair, a test-
ing instance is created as during training, and
then presented to the decision tree, which re-
turns a confidence value (CF)2 indicating the
likelihood that NPi is coreferential to NPj . In
our study, two antecedent selection strategies,
Most Recent First (MRF) and Best First (BF),
are tried to link NPj to its a proper antecedent
with CF above a threshold (0.5). MRF (Soon
et al, 2001) selects the candidate closest to the
anaphor, while BF (Aone and Bennett, 1995; Ng
2The confidence value is obtained by using the
smoothed ratio p+1t+2 , where p is the number of positiveinstances and t is the total number of instances contained
in the corresponding leaf node.
and Cardie, 2002b) selects the candidate with
the maximal CF.
2.2 Limitation of the approach
Nevertheless, the problem of the NP-NP based
approach is that the individual NP usually lacks
adequate description information about its re-
ferred entity. Consequently, it is often difficult
to determine whether or not two NPs refer to
the same entity simply from the properties of
the pair. See the the text segment in Table 1,
for example,
[1 A mutant of [2 KBF1/p50] ], unable to
bind to DNA but able to form homo- or [3 het-
erodimers] , has been constructed.
[4 This protein] reduces or abolishes the DNA
binding activity of wild-type proteins of [5 the
same family ([6 KBF1/p50] , c- and v-rel)].
[7 This mutant] also functions in vivo as a
transacting dominant negative regulator:. . .
Table 1: An Example from the data set
In the above text, [1 A mutant of KBF1/p50],
[4 This protein] and [7 This mutant] are anno-
tated in the same coreferential cluster. Accord-
ing to the above framework, NP7 and its closest
antecedent, NP4, will form a positive instance.
Nevertheless, such an instance is not informa-
tive in that NP4 bears little information related
to the entity and thus provides few clues to ex-
plain its coreference relationship with NP7.
In fact, this relationship would be clear if [1 A
mutant of KBF1/p50], the antecedent of NP4,
is taken into consideration. NP1 gives a de-
tailed description of the entity. By comparing
the string of NP7 with this description, it is ap-
parent that NP7 belongs to the cluster of NP1,
and thus should be coreferential to NP4. This
suggests that we use the coreferential cluster,
instead of its single element, to resolve an NP
correctly. In our study, we propose an approach
which adopts an NP-Cluster based framework to
do resolution. The details of the approach are
given in the next section.
3 The NP-Cluster based approach
Similar to the baseline approach, our approach
also recasts coreference resolution as a binary
classification problem. The difference, however,
is that our approach aims to learn a classifier
which would select the most preferred cluster,
instead of the most preferred antecedent, for an
encountered NP in text. We will give the frame-
work of the approach, including the instance rep-
Features describing the relationships between NPj and NPi
1. DefNp 1 1 if NPj is a definite NP; else 0
2. DemoNP 1 1 if NPj starts with a demonstrative; else 0
3. IndefNP 1 1 if NPj is an indefinite NP; else 0
4. Pron 1 1 if NPj is a pronoun; else 0
5. ProperNP 1 1 if NPj is a proper NP; else 0
6. DefNP 2 1 if NPi is a definite NP; else 0
7. DemoNP 2 1 if NPi starts with a demonstrative; else 0
8. IndefNP 2 1 if NPi is an indefinite NP; else 0
9. Pron 2 1 if NPi is a pronoun; else 0
10. ProperNP 2 1 if NPi is a proper NP; else 0
11. Appositive 1 if NPi and NPj are in an appositive structure; else 0
12. NameAlias 1 if NPi and NPj are in an alias of the other; else 0
13. GenderAgree 1 if NPi and NPj agree in gender; else 0
14. NumAgree 1 if NPi and NPj agree in number; else 0
15. SemanticAgree 1 if NPi and NPj agree in semantic class; else 0
16. HeadStrMatch 1 if NPi and NPj contain the same head string; else 0
17. StrSim 1 The string similarity of NPj against NPi
18. StrSim 2 The string similarity of NPi against NPj
Features describing the relationships between NPj and cluster Ck
19. Cluster NumAgree 1 if Ck and NPj agree in number; else 0
20. Cluster GenAgree 1 if Ck and NPj agree in gender; else 0
21. Cluster SemAgree 1 if Ck and NPj agree in semantic class; else 0
22. Cluster Length The number of elements contained in Ck
23. Cluster StrSim The string similarity of NPj against Ck
24. Cluster StrLNPSim The string similarity of NPj against the longest NP in Ck
Table 2: The features in our coreference resolution system (Features 1 ? 18 are also used in the
baseline system using NP-NP based approach)
resentation, the training and the resolution pro-
cedures, in the following subsections.
3.1 Instance representation
An instance in our approach is composed of three
elements like below:
i{NPj , Ck, NPi}
where NPj , like the definition in the baseline,
is the noun phrase under consideration, while Ck
is an existing coreferential cluster. Each cluster
could be referred by a reference noun phrase NPi,
a certain element of the cluster. A cluster would
probably contain more than one reference NPs
and thus may have multiple associated instances.
For a training instance, the label is positive if
NPj is annotated as belonging to Ck, or negative
if otherwise.
In our system, each instance is represented as
a set of 24 features as shown in Table 2. The
features are supposed to capture the properties
of NPj and Ck as well as their relationships. In
the table we divide the features into two groups,
one describing NPj and NPi and the other de-
scribing NPj and Ck. For the former group, we
just use the same features set as in the baseline
system, while for the latter, we introduce 6 more
features:
Cluster NumAgree, Cluster GenAgree
and Cluster SemAgree: These three fea-
tures mark the compatibility of NPj and Ck
in number, gender and semantic agreement,
respectively. If NPj mismatches the agreement
with any element in Ck, the corresponding
feature is set to 0.
Cluster Length: The number of NPs in the
cluster Ck. This feature reflects the global
salience of an entity in the sense that the more
frequently an entity is mentioned, the more im-
portant it would probably be in text.
Cluster StrSim: This feature marks the string
similarity between NPj and Ck. Suppose
SNPj is the token set of NPj , we compute
the feature value using the similarity function
Str Similarity(SNPj , SCk), where
SCk =
?
NPi?Ck
SNPi
Cluster StrLNPSim: It marks the string
matching degree of NPj and the noun phrase
in Ck with the most number of tokens. The
intuition here is that the NP with the longest
string would probably bear richer description in-
formation of the referent than other elements in
the cluster. The feature is calculated using the
similarity function Str Similarity(SNPj , SNPk),
where
NPk = arg maxNPi?Ck |SNPi |
3.2 Training procedure
Given an annotated training document, we pro-
cess the noun phrases from beginning to end.
For each anaphoric noun phrase NPj , we consider
its preceding coreferential clusters from right to
left3. For each cluster, we create only one in-
stance by taking the last NP in the cluster as
the reference NP. The process will not terminate
until the cluster to which NPj belongs is found.
To make it clear, consider the example in Ta-
ble 1 again. For the noun phrase [7 This mu-
tant], the annotated preceding coreferential clus-
ters are:
C1: { . . . , NP2, NP6 }
C2: { . . . , NP5 }
C3: { NP1, NP4 }
C4: { . . . , NP3 }
Thus three training instances are generated:
i{ NP7, C1, NP6 }
i{ NP7, C2, NP5 }
i{ NP7, C3, NP4 }
Among them, the first two instances are la-
belled as negative while the last one is positive.
After the training instances are ready, we use
C5.0 learning algorithm to learn a decision tree
classifier as in the baseline approach.
3.3 Resolution procedure
The resolution procedure is the counterpart of
the training procedure. Given a testing docu-
ment, for each encountered noun phrase, NPj ,
we create a set of instances by pairing NPj with
each cluster found previously. The instances are
presented to the learned decision tree to judge
the likelihood that NPj is linked to a cluster.
The resolution algorithm is given in Figure 1.
As described in the algorithm, for each clus-
ter under consideration, we create multiple in-
stances by using every NP in the cluster as the
reference NP. The confidence value of the cluster
3We define the position of a cluster as the position of
the last NP in the cluster.
algorithm RESOLVE (a testing document d)
ClusterSet = ?;
//suppose d has N markable NPs;
for j = 1 to N
foreach cluster in ClusterSet
CFcluster = maxNPi?clusterCFi(NPj ,cluster,NPi)
select a proper cluster, BestCluster, according
to a ceterin cluster selection strategy;
if BestCluster != NULL
BestCluster = BestCluster ? {NPj};
else
//create a new cluster
NewCluster = { NPj };
ClusterSet = ClusterSet ? {NewCluster};
Figure 1: The clusters identification algorithm
is the maximal confidence value of its instances.
Similar to the baseline system, two cluster selec-
tion strategies, i.e. MRF and BF, could be ap-
plied to link NPj to a proper cluster. For MRF
strategy, NPj is linked to the closest cluster with
confidence value above 0.5, while for BF, it is
linked to the cluster with the maximal confidence
value (above 0.5).
3.4 Comparison of NP-NP and
NP-Cluster based approaches
As noted above, the idea of the NP-Cluster based
approach is different from the NP-NP based ap-
proach. However, due to the fact that in our
approach a cluster is processed based on its refer-
ence NPs, the framework of our approach could
be reduced to the NP-NP based framework if
the cluster-related features were removed. From
this point of view, this approach could be con-
sidered as an extension of the baseline approach
by applying additional cluster features as the
properties of NPi. These features provide richer
description information of the entity, and thus
make the coreference relationship between two
NPs more apparent. In this way, both rules
learning and coreference determination capabili-
ties of the original approach could be enhanced.
4 Evaluation
4.1 Data collection
Our coreference resolution system is a compo-
nent of our information extraction system in
biomedical domain. For this purpose, an anno-
tated coreference corpus have been built 4, which
4The annotation scheme and samples are avail-
able in http://nlp.i2r.a-star.edu.sg/resources/GENIA-
coreference
MRF BF
Experiments R P F R P F
Baseline 80.2 77.4 78.8 80.3 77.5 78.9
AllAnte 84.4 70.2 76.6 85.7 71.4 77.9
Our Approach 84.4 78.2 81.2 84.9 78.8 81.7
Table 3: The performance of different coreference resolution systems
consists of totally 228 MEDLINE abstracts se-
lected from the GENIA data set. The aver-
age length of the documents in collection is 244
words. One characteristic of the bio-literature
is that pronouns only occupy about 3% among
all the NPs. This ratio is quite low compared
to that in newswire domain (e.g. above 10% for
MUC data set).
A pipeline of NLP components is applied to
pre-process an input raw text. Among them,
NE recognition, part-of-speech tagging and text
chunking adopt the same HMM based engine
with error-driven learning capability (Zhou and
Su, 2002). The NE recognition component
trained on GENIA (Shen et al, 2003) can
recognize up to 23 common biomedical entity
types with an overall performance of 66.1 F-
measure (P=66.5% R=65.7%). In addition, to
remove the apparent non-anaphors (e.g., em-
bedded proper nouns) in advance, a heuristic-
based non-anaphoricity identification module is
applied, which successfully removes 50.0% non-
anaphors with a precision of 83.5% for our data
set.
4.2 Experiments and discussions
Our experiments were done on first 100 docu-
ments from the annotated corpus, among them
70 for training and the other 30 for testing.
Throughout these experiments, default learning
parameters were applied in the C5.0 algorithm.
The recall and precision were calculated auto-
matically according to the scoring scheme pro-
posed by Vilain et al (1995).
In Table 3 we compared the performance of
different coreference resolution systems. The
first line summarizes the results of the baseline
system using traditional NP-NP based approach
as described in Section 2. Using BF strategy,
Baseline obtains 80.3% recall and 77.5% preci-
sion. These results are better than the work by
Castano et al (2002) and Yang et al (2004),
which were also tested on the MEDLINE data
set and reported a F-measure of about 74% and
69%, respectively.
In the experiments, we evaluated another NP-
NP based system, AllAnte. It adopts a similar
learning framework as Baseline except that dur-
ing training it generates the positive instances by
paring an NP with all its antecedents instead of
only the closest one. The system attempts to use
such an instance selection strategy to incorpo-
rate the information from coreferential clusters.
But the results are nevertheless disappointing:
although this strategy boosts the recall by 5.4%,
the precision drops considerably by above 6% at
the same time. The overall F-measure is even
lower than the baseline systems.
The last line of Table 3 demonstrates the re-
sults of our NP-Cluster based approach. For BF
strategy, the system achieves 84.9% recall and
78.8% precision. As opposed to the baseline sys-
tem, the recall rises by 4.6% while the precision
still gains slightly by 1.3%. Overall, we observe
the increase of F-measure by 2.8%.
The results in Table 3 also indicate that the
BF strategy is superior to the MRF strategy.
A similar finding was also reported by Ng and
Cardie (2002b) in the MUC data set.
To gain insight into the difference in the per-
formance between our NP-Cluster based system
and the NP-NP based system, we compared the
decision trees generated in the two systems in
Figure 2. In both trees, the string-similarity
features occur on the top portion, which sup-
ports the arguments by (Strube et al, 2002)
and (Yang et al, 2004) that string-matching is a
crucial factor for NP coreference resolution. As
shown in the figure, the feature StrSim 1 in left
tree is completely replaced by the Cluster StrSim
and Cluster StrLNPSim in the right tree, which
means that matching the tokens with a cluster
is more reliable than with a single NP. More-
over, the cluster length will also be checked when
the NP under consideration has low similarity
against a cluster. These evidences prove that
the information from clusters is quite important
for the coreference resolution on the data set.
The decision tree visualizes the importance of
the features for a data set. However, the tree is
learned from the documents where coreferential
clusters are correctly annotated. During resolu-
HeadMatch = 0:
:...NameAlias = 1: 1 (22/1)
: NameAlias = 0:
: :...Appositive = 0: 0 (13095/265)
: Appositive = 1: 1 (15/4)
HeadMatch = 1:
:...StrSim_1 > 71:
:...DemoNP_1 = 0: 1 (615/29)
: DemoNP_1 = 1:
: :...NumAgree = 0: 0 (5)
: NumAgree = 1: 1 (26)
StrSim_1 <= 71:
:...DemoNP_2 = 1: 1 (12/2)
DemoNP_2 = 0:
:...StrSim_2 <= 77: 0 (144/17)
StrSim_2 > 77:
:...StrSim_1 <= 33: 0 (42/11)
StrSim_1 > 33: 1 (38/11)
HeadMatch = 1:
:...Cluster_StrSim > 66: 1 (663/36)
: Cluster_StrSim <= 66:
: :...StrSim_2 <= 85: 0 (140/14)
: StrSim_2 > 85:
: :...Cluster_StrLNPSim > 50: 1 (16/1)
: Cluster_StrLNPSim <= 50:
: :...Cluster_Length <= 5: 0 (59/17)
: Cluster_Length > 5: 1 (4)
HeadMatch = 0:
:...NameAlias = 1: 1 (22/1)
NameAlias = 0:
:...Appositive = 1: 1 (15/4)
Appositive = 0:
:...StrSim_2 <= 54:
:..
StrSim_2 > 54:
:..
Figure 2: The resulting decision trees for the NP-NP and NP-Cluster based approaches
Features R P F
f1?21 80.3 77.5 78.9
f1?21, f22 84.1 74.4 79.0
f1?21, f23 84.7 78.8 81.6
f1?21, f24 84.3 78.0 81.0
f1?21, f23, f22 84.9 78.6 81.6
f1?21, f23, f24 84.9 78.9 81.8
f1?21, f23, f24, f22 84.9 78.8 81.7
Table 4: Performance using combined features
(fi refers to the i(th) feature listed in Table 2)
tion, unfortunately, the found clusters are usu-
ally not completely correct, and as a result the
features important in training data may not be
also helpful for testing data. Therefore, in the
experiments we were concerned about which fea-
tures really matter for the real coreference res-
olution. For this purpose, we tested our system
using different features and evaluated their per-
formance in Table 4. Here we just considered fea-
ture Cluster Length (f22), Cluster StrSim (f23)
and Cluster StrLNPSim (f24), as Figure 2 has
indicated that among the cluster-related features
only these three are possibly effective for resolu-
tion. Throughout the experiment, the Best-First
strategy was applied.
As illustrated in the table, we could observe
that:
1. Without the three features, the system is
equivalent to the baseline system in terms
of the same recall and precision.
2. Cluster StrSim (f23) is the most effective
as it contributes most to the system per-
formance. Simply using this feature boosts
the F-measure by 2.7%.
3. Cluster StrLNPSim (f24) is also effective by
improving the F-measure by 2.1% alone.
When combined with f23, it leads to the
best F-measure.
4. Cluster Length (f22) only brings 0.1% F-
measure improvement. It could barely
increase, or even worse, reduces the F-
measure when used together with the the
other two features.
5 Related work
To our knowledge, our work is the first
supervised-learning based attempt to do coref-
erence resolution by exploring the relationship
between an NP and coreferential clusters. In the
heuristic salience-based algorithm for pronoun
resolution, Lappin and Leass (1994) introduce
a procedure for identifying anaphorically linked
NP as a cluster for which a global salience value
is computed as the sum of the salience values of
its elements. Cardie and Wagstaff (1999) have
proposed an unsupervised approach which also
incorporates cluster information into considera-
tion. Their approach uses hard constraints to
preclude the link of an NP to a cluster mismatch-
ing the number, gender or semantic agreements,
while our approach takes these agreements to-
gether with other features (e.g. cluster-length,
string-matching degree,etc) as preference factors
for cluster selection. Besides, the idea of cluster-
ing can be seen in the research of cross-document
coreference, where NPs with high context simi-
larity would be chained together based on certain
clustering methods (Bagga and Biermann, 1998;
Gooi and Allan, 2004).
6 Conclusion
In this paper we have proposed a supervised
learning-based approach to coreference resolu-
tion. Rather than mining the coreferential re-
lationship between NP pairs as in conventional
approaches, our approach does resolution by ex-
ploring the relationships between an NP and the
coreferential clusters. Compared to individual
NPs, coreferential clusters provide more infor-
mation for rules learning and reference determi-
nation. In the paper, we first introduced the con-
ventional NP-NP based approach and analyzed
its limitation. Then we described in details the
framework of our NP-Cluster based approach,
including the instance representation, training
and resolution procedures. We evaluated our ap-
proach in the biomedical domain, and the experi-
mental results showed that our approach outper-
forms the NP-NP based approach in both recall
(4.6%) and precision (1.3%).
While our approach achieves better perfor-
mance, there is still room for further improve-
ment. For example, the approach just resolves
an NP using the cluster information available so
far. Nevertheless, the text after the NP would
probably give important supplementary infor-
mation of the clusters. The ignorance of such
information may affect the correct resolution of
the NP. In the future work, we plan to work out
more robust clustering algorithm to link an NP
to a globally best cluster.
References
C. Aone and S. W. Bennett. 1995. Evaluating
automated and manual acquistion of anaphora
resolution strategies. In Proceedings of the
33rd Annual Meeting of the Association for
Compuational Linguistics, pages 122?129.
A. Bagga and A. Biermann. 1998. Entity-based
cross document coreferencing using the vector
space model. In Proceedings of the 36th An-
nual Meeting of the Association for Computa-
tional Linguisticsthe 17th International Con-
ference on Computational Linguistics, pages
79?85.
C. Cardie and K. Wagstaff. 1999. Noun phrase
coreference as clustering. In Proceedings of
the Joint Conference on Empirical Methods in
NLP and Very Large Corpora.
J. Castano, J. Zhang, and J. Pustejovsky. 2002.
Anaphora resolution in biomedical literature.
In International Symposium on Reference Res-
olution, Alicante, Spain.
C. Gooi and J. Allan. 2004. Cross-document
coreference on a large scale corpus. In Pro-
ceedings of 2004 Human Language Technology
conference / North American chapter of the
Association for Computational Linguistics an-
nual meeting.
S. Harabagiu, R. Bunescu, and S. Maiorano.
2001. Text knowledge mining for coreference
resolution. In Proceedings of the 2nd An-
nual Meeting of the North America Chapter of
the Association for Compuational Linguistics,
pages 55?62.
S. Lappin and H. Leass. 1994. An algorithm
for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):525?561.
J. McCarthy and Q. Lehnert. 1995. Using de-
cision trees for coreference resolution. In Pro-
ceedings of the 14th International Conference
on Artificial Intelligences, pages 1050?1055.
V. Ng and C. Cardie. 2002a. Combining sam-
ple selection and error-driven pruning for ma-
chine learning of coreference rules. In Proceed-
ings of the conference on Empirical Methods
in Natural Language Processing, pages 55?62,
Philadelphia.
V. Ng and C. Cardie. 2002b. Improving ma-
chine learning approaches to coreference res-
olution. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics, pages 104?111, Philadelphia.
J. R. Quinlan. 1993. C4.5: Programs for ma-
chine learning. Morgan Kaufmann Publishers,
San Francisco, CA.
D. Shen, J. Zhang, G. Zhou, J. Su, and
C. Tan. 2003. Effective adaptation of hid-
den markov model-based named-entity recog-
nizer for biomedical domain. In Proceedings of
ACL03 Workshop on Natural Language Pro-
cessing in Biomedicine, Japan.
W. Soon, H. Ng, and D. Lim. 2001. A ma-
chine learning approach to coreference resolu-
tion of noun phrases. Computational Linguis-
tics, 27(4):521?544.
M. Strube, S. Rapp, and C. Muller. 2002. The
influence of minimum edit distance on refer-
ence resolution. In Proceedings of the Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, pages 312?319, Philadel-
phia.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly,
and L. Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proceedings of
the Sixth Message understanding Conference
(MUC-6), pages 45?52, San Francisco, CA.
Morgan Kaufmann Publishers.
X. Yang, G. Zhou, J. Su, and C. Tan. 2004. Im-
proving noun phrase coreference resolution by
matching strings. In Proceedings of the 1st In-
ternational Joint Conference on Natural Lan-
guage Processing, Hainan.
G. Zhou and J. Su. 2002. Named Entity recog-
nition using a HMM-based chunk tagger. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics, Philadelphia.
A High-Performance Coreference Resolution System  
using a Constraint-based Multi-Agent Strategy 
 
ZHOU GuoDong            SU Jian  
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
Email: zhougd@i2r.a-star.edu.sg 
 
 
Abstract 
This paper presents a constraint-based multi-
agent strategy to coreference resolution of 
general noun phrases in unrestricted English 
text. For a given anaphor and all the preceding 
referring expressions as the antecedent 
candidates, a common constraint agent is first 
presented to filter out invalid antecedent 
candidates using various kinds of general 
knowledge.  Then, according to the type of the 
anaphor, a special constraint agent is proposed to 
filter out more invalid antecedent candidates 
using constraints which are derived from various 
kinds of special knowledge. Finally, a simple 
preference agent is used to choose an antecedent 
for the anaphor form the remaining antecedent 
candidates, based on the proximity principle. 
One interesting observation is that the most 
recent antecedent of an anaphor in the 
coreferential chain is sometimes indirectly 
linked to the anaphor via some other antecedents 
in the chain.  In this case, we find that the most 
recent antecedent always contains little 
information to directly determine the coreference 
relationship with the anaphor. Therefore, for a 
given anaphor, the corresponding special 
constraint agent can always safely filter out these 
less informative antecedent candidates. In this 
way, rather than finding the most recent 
antecedent for an anaphor, our system tries to 
find the most direct and informative antecedent. 
Evaluation shows that our system achieves 
Precision / Recall / F-measures of 84.7% / 
65.8% / 73.9 and 82.8% / 55.7% / 66.5 on MUC-
6 and MUC-7 English coreference tasks 
respectively. This means that our system 
achieves significantly better precision rates by 
about 8 percent over the best-reported systems 
while keeping recall rates.   
1 Introduction 
Coreference accounts for cohesion in texts. 
Especially, a coreference denotes an identity of 
reference and holds between two expressions, 
which can be named entities, definite noun 
phrases, pronouns and so on. Coreference 
resolution is the process of determining whether 
two referring expressions refer to the same entity 
in the world. The ability to link referring 
expressions both within and across the sentence is 
critical to discourse and language understanding in 
general. For example, coreference resolution is a 
key task in natural language interfaces, machine 
translation, text summarization, information 
extraction and question answering. In particular, 
information extraction systems like those built in 
the DARPA Message Understanding Conferences 
(MUC) have revealed that coreference resolution is 
such a crucial component of an information 
extraction system that a separate coreference task 
has been defined and evaluated in MUC-6 (1995) 
and MUC-7 (1998).  
There is a long tradition of work on 
coreference resolution within computational 
linguistics. Many of the earlier works in 
coreference resolution heavily exploited domain 
and linguistic knowledge (Carter 1987; Rich and 
LuperFoy 1988; Carbonell and Brown 1988). 
However, the pressing need for the development of 
robust and inexpensive solutions encouraged the 
drive toward knowledge-poor strategies (Dagan 
and Itai 1990; Lappin and Leass 1994; Mitkov 
1998; Soon, Ng and Lim 2001; Ng and Cardie 
2002), which was further motivated by the 
emergence of cheaper and more reliable corpus-
based NLP tools such as part-of-speech taggers 
and shallow parsers alongside the increasing 
availability of corpora and other resources (e.g. 
ontology).  
Approaches to coreference resolution usually 
rely on a set of factors which include gender and 
number agreements, c-command constraints, 
semantic consistency, syntactic parallelism, 
semantic parallelism, salience, proximity, etc. 
These factors can be either ?constraints? which 
discard invalid ones from the set of possible 
candidates (such as gender and number 
agreements, c-command constraints, semantic 
consistency), or ?preferences? which gives more 
preference to certain candidates and less to others 
(such as syntactic parallelism, semantic 
parallelism, salience, proximity). While a number 
of approaches use a similar set of factors, the 
computational strategies (the way antecedents are 
determined, i.e. the algorithm and formula for 
assigning antecedents) may differ, i.e. from simple 
co-occurrence rules (Dagan and Itai 1990) to 
decision trees (Soon, Ng and Lim 2001; Ng and 
Cardie 2002) to pattern induced rules (Ng and 
Cardie 2002) to centering algorithms (Grosz and 
Sidner 1986; Brennan, Friedman and Pollard 1987; 
Strube 1998; Tetreault 2001). 
This paper proposes a simple constraint-based 
multi-agent system to coreference resolution of 
general noun phrases in unrestricted English text. 
For a given anaphor and all the preceding referring 
expressions as the antecedent candidates, a 
common constraint agent is first presented to filter 
out invalid antecedent candidates using various 
kinds of general knowledge.  Then, according to 
the type of the anaphor, a special constraint agent 
is proposed to filter out more invalid antecedent 
candidates using constraints which are derived 
from various kinds of special knowledge. Finally, a 
simple preference agent is used to choose an 
antecedent for the anaphor form the remaining 
antecedent candidates, based on the proximity 
principle. One interesting observation is that the 
most recent antecedent of an anaphor in the 
coreferential chain is sometimes indirectly linked 
to the anaphor via some other antecedents in the 
chain.  In this case, we find that the most recent 
antecedent always contains little information to 
directly determine the coreference relationship 
with the anaphor. Therefore, for a given anaphor, 
the corresponding special constraint agent can 
always safely filter out these less informative 
antecedent candidates. In this way, rather than 
finding the most recent antecedent for an anaphor, 
our system tries to find the most direct and 
informative antecedent. 
In this paper, we focus on the task of 
determining coreference relations as defined in 
MUC-6 (1995) and MUC-7 (1998). In order to 
evaluate the performance of our approach on 
coreference resolution, we utilize the annotated 
corpus and the scoring programs from MUC-6 and 
MUC-7. For MUC-6, 30 dry-run documents 
annotated with coreference information are used as 
the training data. There are also 30 annotated 
training documents from MUC-7. The total size of 
30 training documents is close 12,400 words for 
MUC-6 and 19,000 for MUC-7. For testing, we 
utilize the 30 standard test documents from MUC-
6 and the 20 standard test documents from MUC-7. 
The layout of this paper is as follows: in 
Section 2, we briefly describe the preprocessing: 
determination of referring expressions. In Section 
3, we differentiate coreference types and discuss 
how to restrict possible types of direct and 
informative antecedent candidates according to 
anaphor types. In Section 4, we describe the 
constraint-based multi-agent system. In Section 5, 
we evaluate the multi-agent algorithm. Finally, we 
present our conclusions. 
2 Preprocessing: Determination of 
Referring Expressions 
The prerequisite for automatic coreference 
resolution is to obtain possible referring 
expressions in an input document. In our system, 
the possible referring expressions are determined 
by a pipeline of NLP components: 
? Tokenization and sentence segmentation 
? Named entity recognition 
? Part-of-speech tagging 
? Noun phrase chunking 
Among them, named entity recognition, part-
of-speech tagging and noun phrase chunking apply 
the same Hidden Markov Model (HMM) based 
engine with error-driven learning capability (Zhou 
and Su 2000). The named entity recognition 
component (Zhou and Su 2002) recognizes various 
types of MUC-style named entities, that is, 
organization, location, person, date, time, money 
and percentage. The HMM-based noun phrase 
chunking component (Zhou and Su 2000) 
determines various noun phrases based on the 
results of named entity recognition and part-of-
speech tagging. 
3 Coreference Types 
Since coreference is a symmetrical and transitive 
relation, it leads to a simple partitioning of a set of 
referring expressions and each partition forms a 
coreference chain. Although any two referring 
expressions in the coreference chain is 
coreferential, some of conference pairs may be 
direct while others may be indirect since they only 
become conferential via other referring expressions 
in the same coreference chain. This indicates that 
the most recent antecedent of an anaphor in the 
coreferential chain is sometimes indirectly linked 
to the anaphor via some other antecedents in the 
chain. In these indirect cases, we find that the most 
recent antecedent always contains little 
information to directly determine the coreference 
relationship with the anaphor. Generally, direct and 
informative coreference pairs are much easier to 
resolve than indirect and less informative ones. In 
the following example1,  
Microsoft Corp. (i) announced its (i) new CEO 
yesterday. Microsoft (i) said ? 
                                                     
1 The italic markables with the same identification 
symbol are coreferential. 
?Microsoft Corp.?, ?its? and ?Microsoft? form a 
coreference chain. Among the three coreference 
pairs in the chain,  
1) The coreference pair between ?Microsoft 
Corp.? and ?Microsoft? is direct.  
2) The coreference pair between ?Microsoft 
Corp.? and ?its? is direct. 
3) The coreference pair between ?its? and 
?Microsoft? is indirect. This coreference pair 
only becomes coreferential via another 
referring expression ?Microsoft Corp.? Direct 
resolution of this coreference pair is error-
prone and not necessary since it can be 
indirectly linked by the other two coreference 
pairs in the coreference chain.  
Therefore, for a given anaphor, we can always 
safely filter out these less informative antecedent 
candidates. In this way, rather than finding the 
most recent antecedent for an anaphor, our system 
tries to find the most direct and informative 
antecedent. This also suggests that we can classify 
coreference types according to the types of 
anaphors and restrict the possible types of 
antecedent candidates for a given anaphor type as 
follows: 
? Name alias coreference 
This is the most widespread type of coreference 
which is realised by the name alias phenomenon. 
The success of name alias coreference resolution is 
largely conditional on success at determining when 
one referring expression is a name alias of another 
referring expression. Here, the direct antecedent 
candidate of a named entity anaphor can only be 
the type of named entity. For example, 
Microsoft Corp. (i) announced its new CEO 
yesterday. Microsoft (i) said ? 
? Apposition coreference 
This is the easiest type of coreference. A typical 
use of an appositional noun phrase is to provide an 
alternative description for a named entity. For 
example 
Julius Caesar (i), the well-known emperor (i), 
was born in 100 BC. 
? Predicate nominal coreference 
Predicate nominal is typically coreferential with 
the subject. For example, 
George W. Bush (i) is the president of the 
United States (i). 
? Pronominal coreference 
This is the second widespread type of coreference 
which is realised by pronouns. Pronominal 
coreference has been widely studied in literature of 
traditional anaphora resolution. The direct 
antecedent candidate of a pronoun anaphor can be 
any type of referring expressions. For example, 
Computational linguistics (i) from different 
countries attended the tutorial. They (i) took 
extensive note. 
? Definite noun phrase coreference 
This is the third widespread type of coreference 
which is realised by definite noun phrases. It has 
also been widely studied in the literature of 
traditional anaphora resolution. A typical case of 
definite noun phrase coreference is when the 
antecedent is referred by a definite noun phrase 
anaphor representing either same concept 
(repetition) or semantically close concept (e.g. 
synonyms, super-ordinates). The direct antecedent 
candidate of a definite noun phrase anaphor can be 
any type of referring expressions except pronouns. 
For example, 
Computational linguistics (i) from different 
countries attended the tutorial. The 
participants (i) took extensive note. 
? Demonstrative noun phrase coreference 
This type of coreference is not widespread. Similar 
to that of definite noun phrase coreference, the 
direct antecedent candidate of a demonstrative 
noun phrase anaphor can be any type of referring 
expressions except pronouns. For example, 
Boorda wants to limit the total number of 
sailors on the arsenal ship (i) to between 50 
and 60. Currently, this ship (i) has about 90 
sailors. 
? Bare noun phrase coreference 
The direct antecedent candidate of a bare noun 
phrase anaphor can be any type of referring 
expressions except pronouns. For example, 
The price of aluminium (i) siding has steadily 
increased, as the market for aluminium (i) 
reacts to the strike in Chile. 
4 Constraint-based Multi-Agent System 
for Coreference Resolution 
In accordance with the above differentiation of 
coreference types according to the anaphor types, a 
constraint-based multi-agent system is developed.  
4.1 Common Constraint Agent 
For all coreference types described in Section 3, a 
common constraint agent is applied first using 
following constraints:  
Morphological agreements 
These constraints require that an anaphor and its 
antecedent candidate should agree in gender and 
number. These kinds of morphological agreements 
has been widely used in the literature of anaphora 
resolution 
Semantic consistency 
This constraint stipulates that the anaphor and its 
antecedent candidate must be consistent in 
semantics. For example, the anaphor and its 
antecedent candidate should contain the same 
sense or the anaphor contains a sense which is 
parental to the antecedent candidate. In this paper, 
WordNet (Miller 1990) is used for semantic 
consistency check. 
For example, 
IBM (i) announced its new CEO yesterday. 
The company (i) said ? 
4.2 Special Constraint Agents 
For each coreference type described in Section 3, a 
special constraint agent is applied next using some 
heuristic rules mainly based on the accessibility 
space, which is learnt from the training data as 
follows:  
For a given coreference type and a given valid 
antecedent type, all the anaphors of the given 
coreference type are identified first from left to 
right as they appear in the sentences. For each 
anaphor, its antecedent is then determined using 
the principle of proximity. If the most recent 
antecedent candidate has the given antecedent 
type, meet the morphological agreements and 
semantic consistency and is in the same 
coreference chain as the anaphor, this coreference 
pair is counted as a correct instance for the given 
conference type and the given antecedent type. 
Otherwise, it is counted as an error instance. In this 
way, the precision rates of the coreference type 
over different valid antecedent types and different 
accessibility spaces are computed as the percentage 
of the correct instances among all the correct and 
error instances. Finally, the accessibility space for 
a given coreference type and a given antecedent 
type is decided using a precision rate threshold 
(e.g. 95%).  
? Agent for name alias coreference 
A named entity is co-referred with another named 
entity when the formal is a name alias of the latter. 
This type of coreference has an accessibility space 
of the whole document. In this paper, it is tackled 
by a named entity recognition component, as in 
Zhou and Su (2002), using the following name 
alias algorithm in the ascending order of 
complexity: 
1) The simplest case is to recognize full identity 
of strings. This applies to all types of entity 
names. 
2) The next simplest case is to recognize the 
various forms of location names. Normally, 
various acronyms are applied, e.g. ?NY? vs. 
?New York? and ?N.Y.? vs. ?New York?. 
Sometime, partial mention is also applied, e.g. 
?Washington? vs. ?Washington D.C.?. 
3) The third case is to recognize the various 
forms of personal proper names. Thus an 
article on Microsoft may include ?Bill Gates?, 
?Bill? and ?Mr. Gates?. Normally, the full 
personal name is mentioned first in a document 
and later mention of the same person is 
replaced by various short forms such as 
acronym, the last name and, to a less extent, 
the first name, of the full person name. 
4) The most difficult case is to recognize the 
various forms of organizational names. For 
various forms of company names, consider a) 
?International Business Machines Corp.?, 
?International Business Machines? and ?IBM?; 
b) ?Atlantic Richfield Company? and 
?ARCO?. Normally, various abbreviation 
forms (e.g. contractions and acronym) and 
dropping of company suffix are applied. For 
various forms of other organizational names, 
consider a) ?National University of 
Singapore?, ?National Univ. of Singapore? and 
?NUS?; b) ?Ministry of Education? and 
?MOE?. Normally, acronyms and 
abbreviations are applied. 
? Agent for apposition coreference 
If the anaphor is in apposition to the antecedent 
candidate, they are coreferential. The MUC-6 and 
MUC-7 coreference task definitions are slightly 
different. In MUC-6, the appositive should be a 
definite noun phrase while both indefinite and 
definite noun phrases are acceptable in MUC-7.  
? Agent for predicate nominal coreference 
If the anaphor is the predicate nominal and the 
antecedent candidate is the subject, they are 
coreferential. This agent is still under construction.  
? Agent for pronominal coreference 
This agent is applied to the most widely studied 
coreference: pronominal coreference. 6 heuristic 
rules are learnt and applied depending on the 
accessibility space and the types of the antecedent 
candidates: 
1) If the anaphor is a person pronoun and the 
antecedent candidate is a person named entity, 
they are coreferential over the whole 
document. 
2) If the anaphor is a neuter pronoun and the 
antecedent candidate is an organization named 
entity, they are coreferential when they are in 
the same sentence. 
3) If the anaphor is a neuter plural pronoun and 
the antecedent candidate is a plural noun 
phrase, they are coreferential over the whole 
document. 
4) If both the anaphor and the antecedent 
candidate are third person pronouns, they are 
coreferential over the whole document. 
5) If both the anaphor and the antecedent 
candidate are first or second person pronouns, 
they are coreferential when they are in the 
same paragraph. 
6) If both the anaphor and the antecedent 
candidate are neuter pronouns, they are 
coreferential when they are in the same 
paragraph or the antecedent candidate is in the 
previous paragraph of the anaphor. 
? Agent for definite noun phrase coreference 
The agent for definite noun phrase coreference is 
mainly based on the accessibility space. This agent 
is based on the following 3 heuristic rules: 
1) The definite noun phrase will be coreferential 
with a named entity if they are in same 
paragraph or the entity name is in the previous 
paragraph of the definite noun phrase. 
2) The definite noun phrase will be coreferential 
with a named entity if the head word of the 
definite noun phrase is only modified by the 
determiner ?the?. That is, the definite noun 
phrase is of type ?the HEADWORD?, e.g. ?the 
company?. 
3) The definite noun phrase will be coreferential 
with a definite/demonstrative/indefinite noun 
phrase if they string-match2. 
? Agent for demonstrative noun phrase 
coreference 
The agent for demonstrative noun phrase 
coreference is similar to the agent for definite noun 
phrase coreference except that the anaphor is a 
demonstrative noun phrase. 
? Agent for base noun phrase coreference 
This is the most complicated and confusing 
coreference in MUC coreference task definitions. 
Although this type of coreference occupies a large 
portion, it is hard to find heuristic rules to deal 
with it. In our system, only one heuristic rule is 
applied: If the anaphor and the antecedent 
candidate string-match and include at least two 
words except the determiner, they are coreferential 
over the whole document.  
                                                     
2 The determiners, e.g. ?a?, ?an? and ?the?, are removed 
from the strings before comparison. Therefore, ?the 
company? string-matches ?a company?. 
4.3 Common Preference Agent 
For a given anaphor, invalid antecedents are first 
filtered out using the above common constraint 
agent and the special constraint agent. Then, the 
strategy has to choose which of the remaining 
candidates, if any, is the most likely antecedent 
candidate. In our strategy, this is done through a 
common preference agent based on the principle of 
proximity. That is, our common preference agent 
takes advantages of the relative locations of the 
remaining antecedent candidates in the text. 
Among the antecedent candidates: 
1) First it looks for those occurring earlier in the 
current sentence, preferring the one that occurs 
earliest in the natural left-to-right order. 
2) If there are no antecedent candidates occurring 
earlier in the current sentence, look to those 
occurring in the immediately preceding 
sentence of the same paragraph, again 
preferring the one that occurs earliest in that 
sentence in left-to-right order. 
3) If nothing comes up, look back at those 
occurring in the earlier sentences of the same 
paragraph, moving back a sentence at a time, 
but now, within a given sentence preferring the 
most rightward candidate that occurs later in 
the sentence. 
4) Finally, if the scope extends back beyond a 
paragraph boundary, it looks to those that 
occur in the sentences of the preceding 
paragraph, again preferring later to earlier 
occurrences. 
4.4 Multi-Agent Algorithm 
The coreference resolution algorithm is 
implemented based on the previous multi-agents. 
First, all the anaphors are identified from left to 
right as they appear in the sentences. Then, for a 
given anaphor,  
1) All the referring expressions occurred before 
the anaphor are identified as antecedent 
candidates. 
2) The common constraint agent is applied to 
filter out the invalid antecedent candidates 
using various general constraints, such as 
morphological agreements and semantic 
consistency constraints. 
3) The corresponding special constraint agent (if 
exists) is recalled to first filter out indirect and 
less informative antecedent candidates and 
then check the validity of the remaining 
antecedent candidates by using some heuristic 
rules. In this way, more invalid antecedent 
candidates are discarded using various special 
constraints, such as the accessibility space. 
4) The antecedent is chosen from the remaining 
antecedent candidates, if any, using the 
common preference agent based on the 
principle of proximity. 
5 Experimentation 
Table 1 shows the performance of our constraint-
based multi-agent system on MUC-6 and MUC-7 
standard test data using the standard MUC 
evaluation programs while Table 2 gives the 
comparisons of our system with others using the 
same MUC test data and the same MUC evaluation 
programs. Here, the precision (P) measures the 
number of correct coreference pairs in the answer 
file over the total number of coreference pairs in 
the answer file and the recall (R) measures the 
number of correct coreference pairs in the answer 
file over the total number of coreference pairs in 
the key file while F-measure is the weighted 
harmonic mean of precision and recall: 
PR
RPF +
+= 2
2 )1(
?
?
 with =1.  2?
Table 1: Results of our baseline multi-agent coreference resolution system on MUC-6 and MUC-7 
MUC-6 MUC-7 Performance 
R P F R P F 
Overall 65.8 84.7 73.9 55.7 82.8 66.5 
? Agent for name alias coreference 32.7 (35) 92.3 - 33.6 (36) 89.0 - 
? Agent for apposition coreference  4.3 (5) 95.5 -   2.6 (3) 84.6 - 
? Agent for predicate nominal coreference3 - (2) - - - (3) - - 
? Agent for pronominal coreference 18.6 (22) 77.5 - 10.8 (16) 72.3 - 
? Agent for definite noun phrase coreference  9.4 (15) 80.0 -   7.0 (20) 85.0 - 
? Agent for demonstrative noun phrase coreference  0.1 (2) 50.0 -   0.2 (2) 66.7 - 
? Agent for bare noun phrase coreference  1.9 (19) 63.0    1.7 (20) 61.1 - 
 
Table 2: Comparison of our system with the best-reported systems on MUC-6 and MUC-7 
MUC-6 MUC-7 Performance Comparison 
R P F R P F 
Ours 65.8 84.7 73.9 55.7 82.8 66.5 
Ng and Cardie 2002 (C4.5) 64.1 74.9 69.1 57.4 70.8 63.4 
Ng and Cardie 2002 (RIPPER) 64.2 78.0 70.4 55.7  72.8 63.1 
 
 
Table 1 shows that our system achieves F-
measures of 73.9 and 66.5 on MUC-6 and MUC-7 
standard test data, respectively. The figures outside 
the parentheses show the contributions of various 
agents to the overall recall while the figures inside 
the parentheses show the frequency distribution of 
various coreference types in the answer file. It 
shows that the performance difference between 
MUC-6 and MUC-7 mainly comes from the 
significant distribution variation of pronominal 
coreference. It also shows that there are much 
room for improvement, especially for the types of 
pronominal coreference and definite noun pronoun 
resolution. Table 2 shows that our system achieves 
significantly better F-measures by 3.1~4.8 percent 
over the best-reported systems (Ng and Cardie 
2002). Most of the contributions come form 
precision gains. Our system achieves significantly 
better precision rates by 6.7~10.0 percent over the 
best-reported systems (Ng and Cardie 2002) while 
keeping recall rates. One reason behind such high 
performance is the restriction of indirect and less 
informative antecedent candidates according to the 
type of the anaphor. Another reason is 
differentiation of various types of coreference and 
the use of multi-agents. In this way, various types 
of coreference are dealt with effectively by 
different agents according to their characteristics. 
The recall difference between our system and the 
RIPPER system in (Ng and Cardie 2002) maybe 
come from the predicate nominal coreference, 
which can be easily resolved using a machine 
learning algorithm, e.g. (Cohen 1995). Completion 
of the agent for predicate nominal coreference can 
easily fill the difference. 
6 Conclusions 
This paper presents a constraint-based multi-agent 
strategy to coreference resolution of general noun 
phrases in unrestricted English text. 
The first contribution of this paper comes from 
the high performance of our system and its easy 
                                                     
3 The agent for predicate nominal coreference is still under construction. 
implementation. The second contribution is to 
filter out indirect and less informative antecedent 
candidates according to the anaphor type. The third 
contribution is the differentiation of various 
coreference types according to the anaphor types 
and the use of multi-agents.  
Future work includes: 
? The exploration of new constraints to improve 
the precision and new coreference types to 
increase the recall. 
? The problem of type coercion or metonymy 
which is a general problem and accounts for 
much of the overall missing recall. 
? The problem of cataphora, which is not 
handled in the current mechanism. 
References 
Brennan S. E. Friedman M. W. and Pollard C. J. 
1987. A centering approach to pronouns. 
Proceedings of the 25th Annual Meeting of the 
Association for Computational Linguistics 
(ACL?1987), pages 155-162. 
Carbonell J. and Brown R. 1988. Anaphora 
resolution: a multi-strategy approach. 
Proceedings of the 12th International Conference 
on Computational Linguistics (COLING?1988), 
pages 96-101, Budapest, Hungary. 
Carter D. M. 1987. Interpreting Anaphors in 
Natural Language Texts. Ellis Horwood, 
Chichester, UK. 
Cohen W. 1995. Fast effective rule induction. 
Proceedings of the Twelfth International 
Conference on Machine Learning (ICML?1995). 
pages 115-123. Tahoe City, CA.   
Dagan I. and Itai A. 1990. Automatic processing of 
large corpora for the resolution of anaphora 
references. Proceedings of the 13th International 
Conference on Computational Linguistics 
(COLING?1990), pages 1-3, Helsinki, Finland. 
Grosz B. J. and Sidner C. L. 1986. Attention, 
intentions and the structure of discourse. 
Computational Linguistics, 12(3):175-204. 
Lappin S. and Leass H. 1994. An algorithm for 
pronominal anaphora resolution. Computational 
Linguistics. 20(4):535-561. 
Miller G.A. 1990. WordNet: An online lexical 
database. International Journal of Lexicography. 
3(4):235-312. 
Mitkov R. 1998. Robust pronoun resolution with 
limited knowledge. Proceedings of the 36th 
Annual Meeting for Computational Linguistics 
and the 17th International Conference on 
Computational Linguistics 
(COLING/ACL?1998), pages 869-875, 
Montreal, Canada. 
MUC-6. 1995. Proceedings of the 6th Message 
Understanding Conference (MUC-6). Morgan 
Kaufmann, San Francisco, CA. 
MUC-7. 1998. Proceedings of the 7th Message 
Understanding Conference (MUC-7). Morgan 
Kaufmann, San Mateo, CA. 
Ng V. and Cardie C. 2002. Improving machine 
learning approaches to coreference resolution. 
Proceedings of the 40th Annual Meeting of the 
Association for Computational Linguistics 
(ACL?2002), pages 104-111, Philadelphia, Penn. 
Rich E. and LuperFoy S. 1988. An architecture for 
anaphora resolution. Proceedings of the 2nd 
Conference on Applied Natural Language 
Processing (ANLP?1988), pages 18-24, Austin, 
TX. 
Soon W. M.., Ng H. T. and Lim C. Y. 2001. A 
machine learning approach to coreference 
resolution of noun phrases. Computational 
Linguistics, 27(4):521-544. 
Strube M. 1998. Never look back: An alternative to 
centering. Proceedings of the 36th Annual 
Meeting of the Association for Computational 
Linguistics and the 17th International Conference 
on Computational Linguistics, pages 1251-1257. 
Tetreault J. R. 2001. A corpus-based evaluation of 
centering and pronoun resolution. Computation 
Linguistics, 27(4):507-520.  
Zhou G. D. and Su Jian, 2000. Error-driven HMM-
based chunk tagger with context-dependent 
lexicon. Proceedings of the Joint Conference on 
Empirical Methods on Natural Language 
Processing and Very Large Corpus (EMNLP/ 
VLC'2000). Hong Kong.  
Zhou G. D. and Su Jian. 2002. Named Entity 
Recognition Using a HMM-based Chunk 
Tagger, Proceedings of the 40th Annual Meeting 
of the Association for Computational Linguistics 
(ACL?2002). Philadelphia.  
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 378 ? 389, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Discovering Relations Between Named Entities  
from a Large Raw Corpus Using Tree  
Similarity-Based Clustering 
Min Zhang1, Jian Su1, Danmei Wang1,2, Guodong Zhou1, and Chew Lim Tan2 
1 Institute for Infocomm Research, 
21 Heng Mui Keng Terrace, Singapore 119613 
{mzhang, sujian, stuwang, zhougd}@i2r.a-star.edu.sg 
2 Department of Computer Science,  
National University of Singapore, 
Singapore, 117543 
tancl@comp.nus.edu.sg 
Abstract. We propose a tree-similarity-based unsupervised learning method to 
extract relations between Named Entities from a large raw corpus. Our method 
regards relation extraction as a clustering problem on shallow parse trees. First, 
we modify previous tree kernels on relation extraction to estimate the similarity 
between parse trees more efficiently. Then, the similarity between parse trees is 
used in a hierarchical clustering algorithm to group entity pairs into different 
clusters. Finally, each cluster is labeled by an indicative word and unreliable 
clusters are pruned out. Evaluation on the New York Times (1995) corpus 
shows that our method outperforms the only previous work by 5 in F-measure. 
It also shows that our method performs well on both high-frequent and less-
frequent entity pairs. To the best of our knowledge, this is the first work to use a 
tree similarity metric in relation clustering. 
1   Introduction  
The relation extraction task identifies various semantic relations such as location, 
affiliation, revival and so on between entities from text. For example, the sentence 
?George Bush is the president of the United States.? conveys the semantic relation 
?President?, between the entities ?George Bush? (PERSON) and ?the United States? 
(GPE1). The task of relation extraction was first introduced as part of the Template 
Element task in MUC6 and formulated as the Template Relation task in MUC7 [1]. 
Most work at MUC [1] was rule-based, which tried to use syntactic and semantic 
patterns to capture the corresponding relations by means of manually written linguis-
tic rules. The major drawback of this method is the poor adaptability and the poor 
robustness in handling large-scale or new domain data due to two reasons. First, rules 
have to be rewritten for different tasks or when porting to different domains. Second, 
generating rules manually is quite labor- and time-consuming. 
                                                          
1
 GPE is an acronym introduced by the ACE (2004) program to represent a Geo-Political Entity 
--- an entity with land and a government. 
 Discovering Relations Between Named Entities from a Large Raw Corpus 379 
Since then, various supervised learning approaches [2,3,4,5] have been explored ex-
tensively in relation extraction. These approaches automatically learn relation patterns 
or models from a large annotated corpus. To decrease the corpus annotation require-
ment, some researchers turned to weakly supervised learning approaches [6,7], which 
rely on a small set of initial seeds instead of a large annotated corpus. However, there is 
no systematic way in selecting initial seeds and deciding an ?optimal? number of them. 
Alternatively, Hasegawa et al [8] proposed a cosine similarity-based unsupervised 
learning approach for extracting relations from a large raw corpus. The context words 
in between the same entity pairs in different sentences are used to form word vectors, 
which are then clustered according to the cosine similarity. This approach does not 
rely on any annotated corpus and works effectively on high-frequent entity pairs [8]. 
However, there are two problems in this approach: 
? The assumption that the same entity pairs in different sentences have the same 
relation. 
? The cosine similarity measure between the flat feature vectors, which only con-
sider the words between entities. 
In this paper, we propose a tree similarity-based unsupervised learning approach 
for relation extraction. In order to resolve the above two problems in Hasegawa et al 
[8], we assume that the same entity pairs in different sentences can have different 
relation types. Moreover, rather than the cosine similarity measure, a similarity func-
tion over parse trees is proposed to capture much larger feature spaces instead of the 
simple word features. 
The rest of the paper is organized as follows. In Section 2, we discuss the proposed 
tree-similarity-based clustering algorithm. Section 3 shows the experimental result. 
Section 4 compares our work with the previous work. We conclude our work with a 
summary and an outline of the future direction in Section 5. 
2   Tree Similarity-Based Unsupervised Learning 
We use the shallow parse tree as the representation of relation instances, and regard 
relation extraction as a clustering problem on shallow parse trees. Our method con-
sists of three steps: 
1) Calculating the similarity between two parse trees using a tree similarity func-
tion; 
2) Clustering relation instances based on the similarities using a hierarchical 
clustering algorithm; 
3) Labeling each cluster using indicative words as its relation type, and pruning 
out unreliable clusters.  
In this section, we introduce the parse tree representation for a relation instance, 
define the tree similarity function, and describe the clustering algorithm. 
2.1   Parse Tree Representation for Relation Instance 
A parse tree T is a set of node {p1?pn}, which are connected hierarchically. Here, a 
node pi includes a set of features { f1,?, f4} as follows: 
380 M. Zhang et al 
? Head Word ( 1f ): for a leaf (or terminal) node, it is the word itself of the leaf 
node; for a non-terminal node, it is a ?Head Word? propagated from a leaf 
node. This feature defines the main meaning of the phrase or the sub-tree rooted 
by the current node. 
? Node Tag ( 2f ): for a leaf node, it is the part-of-speech of this node; for a non-
terminal node, it is a phrase name, such as Noun Phrase (NP), Verb Phrase (VP). 
This feature defines the linguistic category of this node. 
? Entity Type ( 3f )2:it indicates the entity type which can be PER, COM or GPE 
if the current node refers to a Named Entity.  
? Relation Order ( 4f ): it is used to differentiate asymmetric relations, e.g., ?A 
belongs to B? or ?B belongs to A?. 
These features are widely-adopted in Relation Extraction task.  In the parse tree repre-
sentation, we denote by .
i jfp  the j
th
 feature of node ip , by [ ]ip j   the j
th
 child of 
node ip , and by [ ]ip C  the set of all children of node ip , i.e., [ ] [ ]i ip j p? C .   
2.2   Tree Similarity Function  
Inspired by the special property of kernel-based methods3, we extend the tree kernels 
in Zelenko et al [3] to a novel tree similarity measure function, and apply the above 
tree similarity function to unsupervised learning for relation extraction. Mostly, in 
previous work, kernels are used in supervised learning algorithms such as SVM, Per-
ceptron and PCA (Collins and Duffy, 2001). In our approach, the hierarchical cluster-
ing algorithm is adopted, this allows us to explore more robust and powerful similar-
ity functions, other than a proper kernel function4.  
A similarity function returns a normalized, symmetric similarity score in the range 
[0, 1]. Especially, our tree similarity function 1 2( , )K T T over two trees 1T  and 2T , with 
the root nodes 1r  and 2r , is defined as follows: 
1 2 1 2 1 2 1 2
( , ) ( , ) * ( , ) ( [ ], [ ]){ }
C
K T T m s Kr r r r r r= + c c                                   (1) 
where,  
                                                          
2
  For the features of ?Entity Type?, please refer to the literature ACE [22] for details. 
3
  As an alternative to the feature-based method [5], the advantage of kernels [9] is that they can 
replace any dot product between input points in a high dimensional feature space. Compared 
with the feature-based method, the kernel method displays several unique characteristics, 
such as implicitly mapping the feature space from low-dimension to high-dimension, and ef-
fectively modeling structure data. A few kernels over structured data have been proposed in 
NLP study [10-16]. Zelenko et al [3] and Culotta et al [4] explored tree kernels with SVM 
[9] for relation extraction. We study the tree kernels from similarity measure viewpoints. 
4
  A function is a kernel function if and only if the function is symmetric and positive semi-
definite [3, 9].  
 Discovering Relations Between Named Entities from a Large Raw Corpus 381 
? , )( i jm p p is a matching function over the features of two tree nodes ip  and jp . 
In this paper, only the node tag feature ( 2f ) is considered:  
2 2  
, )  1      if . .(
 0     otherwise
 ji
i j
f fp p
m p p
?? =???
=                                                (2) 
The binary function (1) means that two nodes are matched only if they share 
the same Node Tag. 
? 1 2( , )p ps  is a similarity function between two nodes ip  and jp : 
1 1
3 3
1 1
  
 &   
if 
, else if
other features match
no match
 
 
       
. .
1         
. .
( ) 0.5       . .
0.25    
0    
i j
i j
jii j
p f p f
p f p f
p p f fp ps
? =??
=??
????
= =?????
                                 (3) 
where the values of the weights are assigned empirically according to the discrimina-
tive ability of the feature types. Function (3) measures the similarity between two 
nodes according to the weights of matched features. 
? CK  is the similarity function over the two children node sequences 1[ ]p c  
and 2[ ]p c :  
 
1 2 1 2
, , ( ) ( )
( [ ], [ ]) ( [ ], [ ])argmax { }
C l l
K p p K p p
=
=
a b a b
c c a b                              (4)  
  
1 2 21
( )
1
( [ ], [ ]) ( [ ], [ ])
l
i
K p p K p p
=
= ? i i
a
a ba b                                             (5) 
where a and b are two index sequences, i.e., a is a sequence  1 10 ... [ ]ma a p< < ? | |C  and 
l(a) is the length of sequence a, and likewise for b. The node set 
1 1 1
[ ] { [ ], ..., [ ]}p p p= 1 ma a a  is the subset of 1[ ]p c , 1 1[ ] [ ]p p?a c , 1[ ]p ai is the i
th
 
node of
1
[ ]p a , and likewise for 2p . 
We define 1 2( , )K T T in terms of the similarity function 1 2( , )r rs  between the par-
ent nodes and the similarity function CK  over the two children node sequences 1[ ]r c  
and 2[ ]r c . Formula (5) defines the similarity between two node sequences by sum-
ming up the similarity of each corresponding node pair. CK  in Formula (4) searches 
out such two children node subsequences 1[ ]p a and 2[ ]p b , which has the maximum 
node sequence similarity among all the possible combining pairs of node subse-
quences. Given the similarity scores of all children node pairs, Formula (4) can be 
382 M. Zhang et al 
easily resolved by the dynamic programming (DP) algorithm5. By traversing the two 
trees from top to down and applying the DP algorithm layer by layer, the parse tree 
similarity 1 2( , )K T T defined by Formula (1) is obtained. Due to the DP algorithm, the 
defined tree similarity function is computable in O(mn), where m and n are the num-
ber of nodes in the two trees, respectively. The matching function , )( i jm p p in For-
mula (2) can narrow down the search space during similarity calculation, since the 
sub-trees with unmatched root nodes are unnecessary to be further explored.  
 
Fig. 1.  Sub-structure with maximum similarity 
From the above discussion, we can see that our defined tree similarity function is 
trying to detect the two trees? maximum isomorphic sub-structures. The similarity 
score between the maximum isomorphic sub-structures is returned as the value of the 
similarity function. Fig. 1 illustrates the sub-structures with the maximum similarity 
between two trees. Among the all matched sub-structures, only the sub-structures 
circled by the dashed lines are the isomorphic sub-structures with the maximum simi-
larity. The similarity score between the sub-structures is obtained by summing up the 
similarity score between the corresponding matched nodes.  
Finally, since the size of the input parse tree is not constant, the similarity score is 
normalized as follows: 
1 2
1 1 2 2
1 2
( , )
( , )* ( , )
? ( , ) K T T
K T T K T T
K T T =                (6) 
The value of 1 2? ( , )K T T ranges from 0 to 1. In particular, 1 2? ( , )K T T =1 if and only 
if 1 2T T= . For example, given two parse trees A and B, and A is a subtree of B, then 
under Formula (1), K(A, B) = K(A, A). However, after the normalization through 
                                                          
5
  A well-known application of Dynamic Programming is to compute the edit distance between 
two character strings. Let us regard a node as a character and a node sequence as a character 
string. Then given the similarity  score between nodes, Formula (4) can be resolved using DP 
algorithm in the same way as that of strings. Due to space limitation, the implementation 
deatils are not discussed here. 
 Discovering Relations Between Named Entities from a Large Raw Corpus 383 
Equation (6), we can get ? ?
  ( , ) ( , ) 1K A B K A A< = . In this way, we can differentiate such 
two cases.  
According to the Formula (1) to (5), the similarity function 1 2( , )K T T  over the two 
trees in Fig. 1 is computed as follows: 
1 2 ([NP, VP], [NP, VP])
([bought, NP], [sold, NP, yesterday])
1 bought sold (NP, NP)
= 1+0.25+0.25+ ([a, red, car]
( , ) (S,S) *{ (S,S) }
0.25 (NP, NP) (VP, VP)
0.25 0.25 (Paul, Smith) 0.25
  
( , )
c
c
c
K T T m s K
K K
K
K
K K
K
= +
= +
= + +
= + + +
+
+
, [the, flat])
1.5 a the car flat( , ) ( , )
2
K K= + +
=
 
The above similarity score is more than one. This is because we did not normalize 
the score using Formula (6). 
2.3   Tree Similarity Based Unsupervised Learning 
Our method consists of five steps: 
1) Named Entity (NE) tagging and sentence parsing: Detailed and accurate NE 
types provide more effective information for relation discovery. Here we use Sekine?s 
NE tagger [20], where 150 hierarchical types and subtypes of Named Entities are 
defined [21]. This NE tagger has also been adopted by Hasegawa et al [8]. Besides, 
Collin?s parser [18] is adopted to generate shallow parse trees.  
2) Similarity calculation: The similarity between two relation instances is defined 
between two parse trees. However, the state-of-the-art of parser is always error-prone. 
Therefore, we only use the minimum span parse tree including the NE pairs when 
calculating the similarity function [4]. Please note that the two entities may not be the 
leftmost or rightmost node in the sub-tree. 
3) NE pairs clustering: Clustering of NE pairs is based on the similarity score gener-
ated by the tree similarity function. Rather than k-means [17], we used a bottom-up 
hierarchical clustering method so that there is no need to determine the number of 
clusters in advance. This means that we are not restricted to the limited types of rela-
tions defined in MUC [1] or ACE [22]. Therefore, more substantial existing relations 
can be discovered. We adopt the group-average clustering algorithm [17] since it 
produces the best performance compared with the complete-link and single-link algo-
rithms in our study.  
 
4) Cluster labeling: In our study, we label each cluster by the most frequent ?Head 
Word? in this cluster. As indicated in subsection 2.1, the ?Head Word? of root node 
defines the main meaning of a parse tree. This way, the ?Head Word? of the root 
384 M. Zhang et al 
node of the minimum span tree naturally characterizes the relation between this NE 
pair in this tree. Thus, we simply count the frequency of the ?Head Word? of the root 
node in the cluster, and then chose the most frequent ?Head Word? as the relation 
type of the cluster.  
 
5) Cluster pruning: Unreliable clusters may be generated due to various reasons 
such as divergent relation type distributions and the fact that most of the entity pairs 
inside this cluster are totally unrelated. Therefore, pruning is necessary and done in 
our approach using two criteria. Firstly, if the most frequent ?Head Word? occurs 
less than a predefined percentage in this cluster, which means that the relation type 
defined by this ?Head Word? is not significant statistically, the cluster is pruned out. 
Secondly, we prune out the clusters whose NE pair number is below a predefined 
threshold because such clusters may not be representative enough for this relation.  
3   Experiments 
3.1   Experimental Setting 
To verify our proposed method and establish proper comparison with Hasegawa et al 
[8], we use the same corpus ?The New York Times (1995)?, and evaluate our work on 
the same two kinds of NE pairs: COMPANY-COMPANY (COM-COM) and 
PERSON-GPE (PER-GPE) as Hasegawa et al in [8]. First, we iterate over all pairs of 
Named Entities occurring in the same sentence to generate potential relation in-
stances. Then, according to the co-occurrence frequency of NE pairs, all the relation 
instances are grouped into three categories: 
1) High frequent instances with the co-occurrence frequency not less than 30. In 
this category, only the relation instances, which satisfy the all criteria of Ha-
segawa et al [8]6, are kept for final experiment. By doing so, this category 
data is the same as the entire experimental set used by Hasegawa et al [8]. 
2) Intermediate frequent instances with the co-occurrence frequency between 5 
and 30. In this category, only two distinct NE pairs are randomly picked at 
each frequency for final evaluation due to the large number of such NE pairs. 
3) Less frequent instances with the co-occurrence frequency not more than 5. In 
this category, twenty distinct NE pairs are randomly picked at each fre-
quency for final evaluation due to the similar reason as 2). 
Table 1 reports the statistics of the entire evaluation corpus7 which is manually 
tagged. Table 2 reports the percentage of the NE pairs which carry more than one 
relation types when occurring at different relation instances. The numbers inside pa-
rentheses in Table 1 and Table 2 correspond to the statistical values of the NE pair 
?PER-GPE?, while the numbers outside parentheses are related to the NE pair ?COM-
COM?. Table 2 shows that at least 9.88% of distinct NE pairs have more than one 
                                                          
6
 To discover reliable relations, Hasegawa et al [8] sets five conditions to generate relation 
instance set. NE pair co-occurrence more than 30 times is one of the five conditions. 
7
  Due to the parsing errors and NE tagging errors, the actual number of relation instances is 
less than the theory number that we should pick up. 
 Discovering Relations Between Named Entities from a Large Raw Corpus 385 
relation types in the test corpus. Thus it is reasonable and necessary to assume that 
each occurrence of NE pairs forms one individual relation instance. 
Table 1. Statistics on the manually annotated evaluation data 
Category by 
frequency 
# of instances # of distinct NE pairs    # of relation  
types 
High 8931 (13205) 65 (177) 10 (38) 
Intermediate 672  (783) 38 (41) 6 (7) 
Less 276  (215) 76 (81) 5 (8) 
Table 2. % of distinct NE pairs with more than one relation types on the evaluation data 
Category by frequency % of NE pairs have more than one relations 
    High  15.4   (12.99) 
    Intermediate 28.9   (24.4) 
    Less 11.8   (9.88) 
3.2   Evaluation Measures 
All the experiments are carried out against the manually annotated evaluation corpus. 
We adopt the same criteria as Hasegawa et al [8] to evaluate the performance of our 
method. Grouping and labeling are evaluated separately. For grouping evaluation, all 
the single NE pair clusters are labeled as non-relation while all the other clusters are 
labeled as the most frequent relation type counted in this cluster. For each individual 
relation instance, if the manually assigned relation type is the same as its cluster label, 
the grouping of this relation instance is counted as correct, otherwise, are counted as 
incorrect. Recall (R), Precision (P) and F-measure (F) are adopted as the main per-
formance measure for grouping [8]. For labeling evaluation, a cluster is labeled cor-
rectly only if the labeling relation type, represented by most frequent ?Head Word? 
of the root node of the minimal-span subtree, is the same as the cluster label gotten in 
the grouping process. 
3.3   Experimental Results 
Like other applications using clustering algorithms, the performance of the proposed 
method also depends on the threshold of the clustering similarity. Here this threshold 
is used to truncate the hierarchical tree, so that the different clusters are generated. 
When the threshold is set to 1, then each individual relation instance forms one unique 
group; when the threshold is set to 0, then the all relation instance form one big group. 
Table 3 reports the evaluation results of grouping, where the best F-measures and the 
corresponding similarity thresholds are listed. We can see that our method not only 
achieves good performance on the high-frequent data, but also performs well on the 
386 M. Zhang et al 
intermediate and less-frequent data. The higher frequency, the higher performance. 
Since the best thresholds of the two NE cases are the almost same, we just fix the 
universal threshold as the one used in ?PER-GPE? case in each category.  
Table 3.  Performance evaluation of Grouping phase, the numbers inside parentheses corre-
spond to the evaluation score of ?PER-GPE? while the numbers outside parentheses are related 
to ?COM-COM?. 
Performance Category by fre-
quency 
  F P (%) R (%) 
Threshold 
High 80 (87) 82 (90) 78 (84) 0.28 (0.29) 
Intermediate 74 (76) 87 (84) 64 (69)  0.32 (0.30) 
Less   62 (65) 75 (77) 53 (56)  0.36 (0.35) 
Table 4. Best performance comparison in the high-frequent data (F) 
 Our approach  Hasegawa et al [8] 
PER-GPE 87 82 
COM-COM 80 77 
Table 4 compares the performances of the proposed method and Hasegawa et al 
[8], where the best F-measures on the same high-frequent data are reported. Table 4 
shows that our method outperforms the previous approach by 5 and 3 F-measures in 
clustering NE pairs of ?PER-GPE? and ?COM-COM?, respectively.  
An interesting phenomenon is that the best threshold is set to be just above 0 for 
the cosine similarity in Hasegawa et al [8]. This means that each word feature vector 
of each combination of NE pairs in the same cluster shares at least one word in com-
mon --- and most of these common words were pertinent to the relations [8]. This also 
prevents them from working well on less-frequent data [8]. In contrast, for the simi-
larity function in our approach, the best threshold is much greater than 0. The differ-
ence between the two thresholds implies that the similarity function over the parse 
trees can capture more common structured features than the word feature vectors can. 
This is also the reason why our method is effective on both high and less- 
frequent data. 
It is not surprising that we do have that a few identical NE pairs, occurring in dif-
ferent relation instances, are grouped into different relation sets. For example, the NE 
pairs ?General Electric Co. and NBC?, in one sentence ?General Electric Co., which 
bought NBC in 1986, will announce a new marketing plan.?, is grouped into the rela-
tion set ?M&A?, but in another sentence ?Prime Star Partners and General Electric 
Co., parent of NBC, has signed up 430,000 subscribers.?, is grouped into another 
relation set ?parent?. Among all the NE pairs that carry more than one relation types, 
41.8% of them are grouped correctly using our tree similarity function.  
 Discovering Relations Between Named Entities from a Large Raw Corpus 387 
The performance of grouping is the upper bound of the performances of labeling 
and pruning. In the final, there are 146 PER-GPE clusters and 95 COM-COM clusters 
are generated after grouping. Out of which, only 57 PER-GPE clusters and 42 COM-
COM clusters are labeled correctly before pruning. This is because that a large por-
tion of the non-relation clusters are labeled as one kind of true relations. After prun-
ing, 117 PER-GPE clusters and 84 COM-COM clusters are labeled correctly. This is 
because lots of the non-relation clusters are labeled correctly by the pruning process, 
so we can say that pruning is a non-relation labeling process, which greatly improves 
the performance of labeling.  
The experimental results discussed above suggest that our proposed method is an 
effective solution for discovering relation from a large raw corpus. 
4   Discussions 
It would be interesting to review and summarize how the proposed method deals with 
the relation extraction issue differently from other related works. Table 5 in the next 
page summarizes the differences between our method and Hasegawa et al [8]. 
Table 5. The differences between our method and Hasegawa et al [8] 
 Our approach  Hasegawa et al [8] 
Similarity  
Measure 
tree similarity over parse 
tree structures 
cosine similarity between the 
context word feature vectors 
Assumption No Yes (The same entity pairs in 
different sentences have the 
same relation) 
Labeling the most frequent ?Head 
Word? of the root node of 
sub-tree 
the most frequent context 
word 
Pruning Yes (We present two prun-
ing criterion) 
No 
Data Frequency effective on both high and 
less-frequent data 
effective only on high-
frequent data 
In addition, since our tree similarity function has benefited from the relation tree 
kernels of Zelenko et al [3], let us compare our similarity measure function with their 
relation kernel function [3] from the viewpoint of computational efficiency. Zelenko 
et al [3] defined the first parse tree kernels for relation extraction, and then this  
relation tree kernels were extended to dependency tree kernels by Culotta et al [4].  
Their tree kernels sum up the similarity scores among all possible subsequences of 
children nodes with matching parents, and give a penalty to longer sequences. Their 
388 M. Zhang et al 
tree kernels are closely related to the convolution kernels [12]. But, by doing so, lots 
of sub-trees will be considered again and again. An extreme case occurs when two 
tree structures are identical. In that situation all the sub-trees will be considered 
exhaustedly, even if the sub-tree is a part of other bigger sub-trees. We use the maxi-
mum score in Formula (4) instead of the summation in our approach. With our ap-
proach, the entire tree is only considered once. The replacement of summation with 
maximization reduces the computational time greatly. 
5   Conclusions and Future Directions 
We modified the relation tree kernels [3] to be a tree similarity measure function by 
replacing the summation over all possible subsequences of children nodes with 
maximization, and used it in clustering for relation extraction. The experimental result 
showed much improvement over the previous best result [8] on the same test corpus. 
It also showed that our method is high effective on both high-frequent and less-
frequent data. Our work demonstrated the effectiveness of combining the tree similar-
ity measure with unsupervised learning for relation extraction. 
Although our method shows good performance, there are still other aspects of the 
proposed method worth discussing here. Without additional knowledge, relation de-
tecting and relation labeling are still not easy to be resolved, especially in less-
frequent data. We expect that using additional easily-acquired knowledge can im-
prove the performance of the proposed method. For example, we can introduce the 
WordNet [19] thesaurus information into Formula (3) to obtain more accurate node 
similarities and resolve data sparse problem. We can also use the same resource to 
improve the labeling scheme and find more abstract relation types like the definitions 
used in ACE program [22].  
References  
1. MUC. 1987-1998. The nist MUC website:  http://www.itl.nist.gov/iaui/894.02/related_ 
projects/muc/ 
2. Miller, S., Fox, H., Ramshaw, L. and Weischedel, R. 2000. A novel use of statistical pars-
ing to extract information from text. Proceedings of NAACL-00 
3. Zelenko, D., Aone, C. and Richardella, A. 2003. Kernel Methods for Relation Extraction. 
Journal of Machine Learning Research. 2003(2):1083-1106 
4. Culotta, A. and Sorensen, J. 2004. Dependency Tree Kernel for Relation Extraction. Pro-
ceeding of ACL-04 
5. Kambhatla, N. 2004. Combining Lexical, Syntactic, and Semantic Features with Maxi-
mum Entropy Models for Extracting Relations. Proceeding of ACL-04, Poster paper. 
6. Agichtein, E. and Gravano, L. 2000. Snow-ball: Extracting Relations from Large Plain-
text Collections. Proceedings of the Fifth ACM International Conference on Digital Li-
braries. 
7. Stevenson, M. 2004. An Unsupervised WordNet-based Algorithm for Relation Extraction. 
Proceedings of the 4th LREC workshop "Beyond Named Entity: Semantic Labeling for 
NLP tasks" 
 Discovering Relations Between Named Entities from a Large Raw Corpus 389 
8. Hasegawa, T., Sekine, S. and Grishman, R. 2004. Discovering Relations among Named 
Entities from Large Corpora. Proceeding of ACL-04 
9. Vapnik, V. 1998. Statistical Learning Theory. John Wiley 
10. Collins, M. and Duffy, N. 2001. Convolution Kernels for Natural Language. Proceeding of 
NIPS-01 
11. Collins, M. and Duffy, N. 2002. New Ranking Algorithm for Parsing and Tagging: Kernel 
over Discrete Structure, and the Voted Perceptron. Proceeding of ACL-02. 
12. Haussler, D. 1999. Convolution Kernels on Discrete Structures. Technical Report UCS-
CRL-99-10, University of California 
13. Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N. and Watkins, C. 2002. Text clas-
sification using string kernel. Journal of Machine Learning Research, 2002(2):419-444 
14. Suzuki, J., Hirao, T., Sasaki Y. and Maeda, E. 2003. Hierarchical Directed Acyclic Graph 
Kernel: Methods for Structured Natural Language Data. Proceedings of ACL-03 
15. Suzuki, J., Isozaki, H. and Maeda, E. 2003. Convolution Kernels with Feature Selection 
for Natural Language Processing Tasks. Proceedings of ACL-04 
16. Moschitti, A. 2004. A study on Convolution Kernels for Shallow Semantic Parsing. Pro-
ceedings of ACL-04 
17. Manning, C. and Schutze, H. 1999. Foundations of Statistical Natural Language Process-
ing. The MIT Press: 500-527 
18. Collins, M. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. 
Thesis. University of Pennsylvania 
19. Fellbaum, C. 1998. WordNet: An Electronic Lexical Database and some of its Applica-
tions. Cambridge, MA: MIT Press. 
20. Sekine, S. 2001. OAK System (English Sentence Analysis). Http://nlp.cs.nyu.edu/oak 
21. Sekine, S., Sudo, K. and Nobata, C. 2002. Extended named entity hierarchy. Proceedings 
of LREC-02 
22. ACE. 2004. The Automatic Content Extraction (ACE) Projects. http://www.ldc.upenn.edu/ 
Projects/ACE/ 
Coreference Resolution Using Competition Learning Approach 
Xiaofeng Yang*+ Guodong Zhou*  Jian Su* Chew Lim Tan + 
*Institute for Infocomm Research, 
21 Heng Mui Keng Terrace, 
Singapore 119613 
+Department of Computer Science, 
National University of Singapore,  
Singapore 117543  
*{xiaofengy,zhougd,sujian}@ 
i2r.a-star.edu.sg 
+(yangxiao,tancl)@comp.nus.edu.sg
  
Abstract 
In this paper we propose a competition 
learning approach to coreference resolu-
tion. Traditionally, supervised machine 
learning approaches adopt the single-
candidate model. Nevertheless the prefer-
ence relationship between the antecedent 
candidates cannot be determined accu-
rately in this model. By contrast, our ap-
proach adopts a twin-candidate learning 
model. Such a model can present the 
competition criterion for antecedent can-
didates reliably, and ensure that the most 
preferred candidate is selected. Further-
more, our approach applies a candidate 
filter to reduce the computational cost and 
data noises during training and resolution. 
The experimental results on MUC-6 and 
MUC-7 data set show that our approach 
can outperform those based on the single-
candidate model.  
1 Introduction 
Coreference resolution is the process of linking 
together multiple expressions of a given entity. The 
key to solve this problem is to determine the ante-
cedent for each referring expression in a document.  
In coreference resolution, it is common that two 
or more candidates compete to be the antecedent of 
an anaphor (Mitkov, 1999). Whether a candidate is 
coreferential to an anaphor is often determined by 
the competition among all the candidates. So far, 
various algorithms have been proposed to deter-
mine the preference relationship between two can-
didates. Mitkov?s knowledge-poor pronoun 
resolution method (Mitkov, 1998), for example, 
uses the scores from a set of antecedent indicators 
to rank the candidates. And centering algorithms 
(Brennan et al, 1987; Strube, 1998; Tetreault, 
2001), sort the antecedent candidates based on the 
ranking of the forward-looking or backward-
looking centers. 
In recent years, supervised machine learning 
approaches have been widely used in coreference 
resolution (Aone and Bennett, 1995; McCarthy, 
1996; Soon et al, 2001; Ng and Cardie, 2002a), 
and have achieved significant success. Normally, 
these approaches adopt a single-candidate model in 
which the classifier judges whether an antecedent 
candidate is coreferential to an anaphor with a con-
fidence value. The confidence values are generally 
used as the competition criterion for the antecedent 
candidates. For example, the ?Best-First? selection 
algorithms (Aone and Bennett, 1995; Ng and 
Cardie, 2002a) link the anaphor to the candidate 
with the maximal confidence value (above 0.5). 
One problem of the single-candidate model, 
however, is that it only takes into account the rela-
tionships between an anaphor and one individual 
candidate at a time, and overlooks the preference 
relationship between candidates. Consequently, the 
confidence values cannot accurately represent the 
true competition criterion for the candidates. 
In this paper, we present a competition learning 
approach to coreference resolution. Motivated by 
the research work by Connolly et al (1997), our 
approach adopts a twin-candidate model to directly 
learn the competition criterion for the antecedent 
candidates. In such a model, a classifier is trained 
based on the instances formed by an anaphor and a 
pair of its antecedent candidates. The classifier is 
then used to determine the preference between any 
two candidates of an anaphor encountered in a new 
document. The candidate that wins the most com-
parisons is selected as the antecedent. In order to 
reduce the computational cost and data noises, our 
approach also employs a candidate filter to elimi-
nate the invalid or irrelevant candidates.  
The layout of this paper is as follows. Section 2 
briefly describes the single-candidate model and 
analyzes its limitation. Section 3 proposes in de-
tails the twin-candidate model and Section 4 pre-
sents our coreference resolution approach based on 
this model. Section 5 reports and discusses the ex-
perimental results. Section 6 describes related re-
search work. Finally, conclusion is given in 
Section 7. 
2 The Single-Candidate Model 
The main idea of the single-candidate model for 
coreference resolution is to recast the resolution as 
a binary classification problem. 
During training, a set of training instances is 
generated for each anaphor in an annotated text. 
An instance is formed by the anaphor and one of 
its antecedent candidates. It is labeled as positive 
or negative based on whether or not the candidate 
is tagged in the same coreferential chain of the 
anaphor. 
After training, a classifier is ready to resolve the 
NPs1 encountered in a new document. For each NP 
under consideration, every one of its antecedent 
candidates is paired with it to form a test instance. 
The classifier returns a number between 0 and 1 
that indicates the likelihood that the candidate is 
coreferential to the NP. 
The returned confidence value is commonly 
used as the competition criterion to rank the candi-
date. Normally, the candidates with confidences 
less than a selection threshold (e.g. 0.5) are dis-
carded. Then some algorithms are applied to 
choose one of the remaining candidates, if any, as 
the antecedent. For example, ?Closest-First? (Soon 
et al, 2001) selects the candidate closest to the 
anaphor, while ?Best-First? (Aone and Bennett, 
1995; Ng and Cardie, 2002a) selects the candidate 
with the maximal confidence value.  
One limitation of this model, however, is that it 
only considers the relationships between a NP en-
countered and one of its candidates at a time dur-
ing its training and testing procedures. The 
confidence value reflects the probability that the 
candidate is coreferential to the NP in the overall 
                                                          
1 In this paper a NP corresponds to a Markable in MUC 
coreference resolution tasks. 
distribution 2 , but not the conditional probability 
when the candidate is concurrent with other com-
petitors. Consequently, the confidence values are 
unreliable to represent the true competition crite-
rion for the candidates.  
To illustrate this problem, just suppose a data 
set where an instance could be described with four 
exclusive features: F1, F2, F3 and F4. The ranking 
of candidates obeys the following rule: 
CSF1 >> CSF2 >> CSF3 >> CSF4 
Here CSFi ( 41 ?? i ) is the set of antecedent can-
didates with the feature Fi on. The mark of ?>>? 
denotes the preference relationship, that is, the 
candidates in CSF1 is preferred to those in CSF2, and 
to those in CSF3 and CSF4.  
Let CF2 and CF3 denote the class value of a leaf 
node ?F2 = 1? and ?F3 = 1?, respectively. It is pos-
sible that CF2 < CF3, if the anaphors whose candi-
dates all belong to CSF3 or CSF4 take the majority in 
the training data set.  In this case, a candidate in 
CSF3 would be assigned a larger confidence value 
than a candidate in CSF2. This nevertheless contra-
dicts the ranking rules. If during resolution, the 
candidates of an anaphor all come from CSF2 or 
CSF3, the anaphor may be wrongly linked to a can-
didate in CSF3 rather than in CSF2. 
3 The Twin-Candidate Model 
Different from the single-candidate model, the 
twin-candidate model aims to learn the competition 
criterion for candidates. In this section, we will 
introduce the structure of the model in details. 
3.1 Training Instances Creation 
Consider an anaphor ana and its candidate set can-
didate_set, {C1, C2, ?, Ck}, where Cj is closer to 
ana than Ci if j > i. Suppose positive_set is the set 
of candidates that occur in the coreferential chain 
of ana, and negative_set is the set of candidates not 
in the chain, that is, negative_set = candidate_set  
- positive_set. The set of training instances based 
on ana, inst_set, is defined as follows:  
                                                          
2 Suppose we use C4.5 algorithm and the class value takes the 
smoothed ration, 
2
1
+
+
t
p , where p is the number of positive 
instances and t is the total number of instances contained in 
the corresponding leaf node. 
} _  C  , _Cj,i |{
  } _  C  ,_ C j,i |{
_
ji),,(
ji),,(
setpositvesetnegativeinst
setnegativesetpositveinst
setinst
anaCjCi
anaCjCi
??>
??>
=
U
 
From the above definition, an instance is 
formed by an anaphor, one positive candidate and 
one negative candidate. For each instance, 
)ana,cj,ci(inst , the candidate at the first position, Ci, 
is closer to the anaphor than the candidate at the 
second position, Cj.  
A training instance )ana,cj,ci(inst is labeled as 
positive if Ci ?  positive-set and Cj ?  negative-set; 
or negative if Ci ?  negative-set and Cj ?  positive-
set.  
See the following example:  
 
Any design to link China's accession to the WTO 
with the missile tests1 was doomed to failure.  
 ?If some countries2 try to block China TO acces-
sion, that will not be popular and will fail to win the 
support of other countries3? she said.  
Although no governments4 have suggested formal 
sanctions5 on China over the missile tests6, the United 
States has called them7 ?provocative and reckless? and 
other countries said they could threaten Asian stability.  
 
In the above text segment, the antecedent can-
didate set of the pronoun ?them7? consists of six 
candidates highlighted in Italics. Among the can-
didates, Candidate 1 and 6 are in the coreferential 
chain of ?them7?, while Candidate 2, 3, 4, 5 are not. 
Thus, eight instances are formed for ?them7?:  
 
(2,1,7)  (3,1,7)  (4,1,7)  (5,1,7) 
(6,5,7)  (6,4,7)  (6,3,7)  (6,2,7) 
 
Here the instances in the first line are negative, 
while those in the second line are all positive.  
3.2 Features Definition 
A feature vector is specified for each training or 
testing instance. Similar to those in the single-
candidate model, the features may describe the 
lexical, syntactic, semantic and positional relation-
ships of an anaphor and any one of its candidates. 
Besides, the feature set may also contain inter-
candidate features characterizing the relationships 
between the pair of candidates, e.g. the distance 
between the candidates in the number distances or 
paragraphs. 
3.3 Classifier Generation 
Based on the feature vectors generated for each 
anaphor encountered in the training data set, a 
classifier can be trained using a certain machine 
learning algorithm, such as C4.5, RIPPER, etc. 
Given the feature vector of a test instance 
)ana,cj,ci(inst  (i > j), the classifier returns the posi-
tive class indicating that Ci is preferred to Cj as the 
antecedent of ana; or negative indicating that Cj is 
preferred.  
3.4 Antecedent Identification 
Let CR( )ana,cj,ci(inst ) denote the classification re-
sult for an instance )ana,cj,ci(inst . The antecedent of 
an anaphor is identified using the algorithm shown 
in Figure 1.  
 
Algorithm ANTE-SEL 
Input: ana: the anaphor under consideration  
candidate_set: the set of antecedent can-
didates of ana, {C1, C2,?,Ck} 
 
for i = 1 to K do 
   Score[ i ] = 0; 
for  i = K downto 2 do 
for j = i ? 1 downto 1 do 
  if  CR( )ana,cj,ci(inst ) = = positive then  
Score[ i ]++; 
else  
Score[ j ] ++; 
  endif 
SelectedIdx= ][maxarg
_
iScore
setcandidateCii ?
 
return CselectedIdx; 
Figure 1:The antecedent identification algorithm
 
Algorithm ANTE-SEL takes as input an ana-
phor and its candidate set candidate_set, and re-
turns one candidate as its antecedent. In the 
algorithm, each candidate is compared against any 
other candidate. The classifier acts as a judge dur-
ing each comparison. The score of each candidate 
increases by one every time when it wins. In this 
way, the final score of a candidate records the total 
times it wins. The candidate with the maximal 
score is singled out as the antecedent.  
If two or more candidates have the same maxi-
mal score, the one closest to the anaphor would be 
selected. 
3.5 Single-Candidate Model: A Special Case 
of Twin-Candidate Model? 
While the realization and the structure of the twin-
candidate model are significantly different from 
the single-candidate model, the single-candidate 
model in fact can be regarded as a special case of 
the twin-candidate model.  
To illustrate this, just consider a virtual ?blank? 
candidate C0 such that we could convert an in-
stance )ana,ci(inst in the single-candidate model to 
an instance )ana,c,ci( 0inst in the twin-candidate 
model. Let )ana,c,ci( 0inst have the same class label 
as )ana,ci(inst , that is, )ana,c,ci( 0inst is positive if Ci is 
the antecedent of ana; or negative if not.  
Apparently, the classifier trained on the in-
stance set { )ana,ci(inst }, T1, is equivalent to that 
trained on { )ana,c,ci( 0inst }, T2.  T1 and T2 would 
assign the same class label for the test instances 
)ana,ci(inst  and )ana,c,ci( 0inst , respectively. That is to 
say, determining whether Ci is coreferential to ana 
by T1 in the single-candidate model equals to 
determining whether Ci is better than C0 w.r.t ana 
by T2 in the twin-candidate model. Here we could 
take C0 as a ?standard candidate?. 
While the classification in the single-candidate 
model can find its interpretation in the twin-
candidate model, it is not true vice versa. Conse-
quently, we can safely draw the conclusion that the 
twin-candidate model is more powerful than the 
single-candidate model in characterizing the rela-
tionships among an anaphor and its candidates. 
4 The Competition Learning Approach 
Our competition learning approach adopts the 
twin-candidate model introduced in the Section 3. 
The main process of the approach is as follows: 
1. The raw input documents are preprocessed to 
obtain most, if not all, of the possible NPs.  
2. During training, for each anaphoric NP, we 
create a set of candidates, and then generate 
the training instances as described in Section 3.  
3. Based on the training instances, we make use 
of the C5.0 learning algorithm (Quinlan, 1993) 
to train a classifier. 
4. During resolution, for each NP encountered, 
we also construct a candidate set. If the set is 
empty, we left this NP unresolved; otherwise 
we apply the antecedent identification algo-
rithm to choose the antecedent and then link 
the NP to it.  
4.1 Preprocessing 
To determine the boundary of the noun phrases, a 
pipeline of Nature Language Processing compo-
nents are applied to an input raw text: 
z Tokenization and sentence segmentation 
z Named entity recognition 
z Part-of-speech tagging 
z Noun phrase chunking 
Among them, named entity recognition, part-of-
speech tagging and text chunking apply the same 
Hidden Markov Model (HMM) based engine with 
error-driven learning capability (Zhou and Su, 
2000 & 2002). The named entity recognition 
component recognizes various types of MUC-style 
named entities, i.e., organization, location, person, 
date, time, money and percentage.  
4.2 Features Selection 
For our study, in this paper we only select those 
features that can be obtained with low annotation 
cost and high reliability. All features are listed in 
Table 1 together with their respective possible val-
ues.  
4.3 Candidates Filtering 
For a NP under consideration, all of its preceding 
NPs could be the antecedent candidates. Neverthe-
less, since in the twin-candidate model the number 
of instances for a given anaphor is about the square 
of the number of its antecedent candidates, the 
computational cost would be prohibitively large if 
we include all the NPs in the candidate set. More-
over, many of the preceding NPs are irrelevant or 
even invalid with regard to the anaphor. These data 
noises may hamper the training of a good-
performanced classifier, and also damage the accu-
racy of the antecedent selection: too many com-
parisons are made between incorrect candidates. 
Therefore, in order to reduce the computational 
cost and data noises, an effective candidate filter-
ing strategy must be applied in our approach. 
During training, we create the candidate set for 
each anaphor with the following filtering algorithm: 
1. If the anaphor is a pronoun,  
(a) Add to the initial candidate set al the pre-
ceding NPs in the current and the previous 
two sentences. 
(b) Remove from the candidate set those that 
disagree in number, gender, and person. 
(c) If the candidate set is empty, add the NPs in 
an earlier sentence and go to 1(b). 
2. If the anaphor is a non-pronoun, 
(a) Add all the non-pronominal antecedents to 
the initial candidate set. 
(b) For each candidate added in 2(a), add the 
non-pronouns in the current, the previous 
and the next sentences into the candidate set. 
During resolution, we filter the candidates for 
each encountered pronoun in the same way as dur-
ing training. That is, we only consider the NPs in 
the current and the preceding 2 sentences. Such a 
context window is reasonable as the distance be-
tween a pronominal anaphor and its antecedent is 
generally short. In the MUC-6 data set, for exam-
ple, the immediate antecedents of 95% pronominal 
anaphors can be found within the above distance. 
Comparatively, candidate filtering for non-
pronouns during resolution is complicated. A po-
tential problem is that for each non-pronoun under 
consideration, the twin-candidate model always 
chooses a candidate as the antecedent, even though 
all of the candidates are ?low-qualified?, that is, 
unlikely to be coreferential to the non-pronoun un-
der consideration.  
In fact, the twin-candidate model in itself can 
identify the qualification of a candidate. We can 
compare every candidate with a virtual ?standard 
candidate?, C0. Only those better than C0 are 
deemed qualified and allowed to enter the ?round 
robin?, whereas the losers are eliminated. As we 
have discussed in Section 3.5, the classifier on the 
pairs of a candidate and C0 is just a single-
candidate classifier. Thus, we can safely adopt the 
single-candidate classifier as our candidate filter.  
The candidate filtering algorithm during resolu-
tion is as follows:  
Features describing the candidate: 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10 
ante_DefNp_1(2) 
ante_IndefNP_1(2) 
ante_Pron_1(2) 
ante_ProperNP_1(2) 
ante_M_ProperNP_1(2) 
ante_ProperNP_APPOS_1(2) 
ante_Appositive_1(2) 
ante_NearestNP_1(2) 
ante_Embeded_1(2) 
ante_Title_1(2) 
1 if Ci (Cj) is a definite NP; else 0 
1 if Ci (Cj) is an indefinite NP; else 0 
1 if Ci (Cj) is a pronoun; else 0 
1 if Ci (Cj) is a proper NP; else 0  
1 if Ci (Cj) is a mentioned proper NP; else 0 
1 if Ci (Cj) is a proper NP modified by an appositive; else 0 
1 if Ci (Cj) is in a apposition structure; else 0 
1 if Ci (Cj) is the nearest candidate to the anaphor; else 0 
1 if Ci (Cj) is in an embedded NP; else 0 
1 if Ci (Cj) is in a title; else 0 
Features describing the anaphor: 
11. 
12. 
13. 
14. 
15. 
 
16. 
ana_DefNP 
ana_IndefNP 
ana_Pron 
ana_ProperNP 
ana_PronType 
 
ana_FlexiblePron 
1 if ana is a definite NP; else 0 
1 if ana is an indefinite NP; else 0 
1 if ana is a pronoun; else 0 
1 if ana is a proper NP; else 0 
1 if ana is a third person pronoun; 2 if a single neuter pro-
noun; 3 if a plural neuter pronoun; 4 if other types 
1 if ana is a flexible pronoun; else 0 
Features describing the candidate and the anaphor: 
17. 
18. 
 
18. 
 
20. 
21. 
ante_ana_StringMatch_1(2) 
ante_ana_GenderAgree_1(2) 
 
ante_ana_NumAgree_1(2) 
 
ante_ana_Appositive_1(2) 
ante_ana_Alias_1(2) 
1 if Ci (Cj) and ana match in string; else 0 
1 if Ci (Cj) and ana agree in gender; else 0 if disagree; -1 if 
unknown 
1 if Ci (Cj) and ana agree in number; 0 if disagree; -1 if un-
known 
1 if Ci (Cj) and ana are in an appositive structure; else 0 
1 if Ci (Cj) and ana are in an alias of the other; else 0 
Features describing the two candidates 
22. 
23. 
inter_SDistance 
inter_Pdistance 
Distance between Ci and Cj in sentences 
Distance between Ci and Cj in paragraphs 
Table 1:  Feature set for coreference resolution (Feature 22, 23 and features involving Cj are not 
used in the single-candidate model) 
1. If the current NP is a pronoun, construct the 
candidate set in the same way as during training.  
2. If the current NP is a non-pronoun,  
(a) Add all the preceding non-pronouns to the ini-
tial candidate set. 
(b) Calculate the confidence value for each candi-
date using the single-candidate classifier. 
(c) Remove the candidates with confidence value 
less than 0.5. 
5 Evaluation and Discussion 
Our coreference resolution approach is evaluated 
on the standard MUC-6 (1995) and MUC-7 (1998) 
data set. For MUC-6, 30 ?dry-run? documents an-
notated with coreference information could be used 
as training data. There are also 30 annotated train-
ing documents from MUC-7. For testing, we util-
ize the 30 standard test documents from MUC-6 
and the 20 standard test documents from MUC-7. 
5.1 Baseline Systems 
In the experiment we compared our approach with 
the following research works:  
1. Strube?s S-list algorithm for pronoun resolu-
tion (Stube, 1998).  
2. Ng and Cardie?s machine learning approach to 
coreference resolution (Ng and Cardie, 2002a).  
3. Connolly et al?s machine learning approach to 
anaphora resolution (Connolly et al, 1997).  
Among them, S-List, a version of centering 
algorithm, uses well-defined heuristic rules to rank 
the antecedent candidates; Ng and Cardie?s ap-
proach employs the standard single-candidate 
model and ?Best-First? rule to select the antece-
dent; Connolly et al?s approach also adopts the 
twin-candidate model, but their approach lacks of 
candidate filtering strategy and uses greedy linear 
search to select the antecedent (See ?Related 
work? for details). 
We constructed three baseline systems based on 
the above three approaches, respectively. For com-
parison, in the baseline system 2 and 3, we used 
the similar feature set as in our system (see table 1).  
5.2 Results and Discussion 
Table 2 and 3 show the performance of different 
approaches in the pronoun and non-pronoun reso-
lution, respectively. In these tables we focus on the 
abilities of different approaches in resolving an 
anaphor to its antecedent correctly. The recall 
measures the number of correctly resolved ana-
phors over the total anaphors in the MUC test data 
set, and the precision measures the number of cor-
rect anaphors over the total resolved anaphors. The 
F-measure F=2*RP/(R+P) is the harmonic mean of 
precision and recall. 
The experimental result demonstrates that our 
competition learning approach achieves a better 
performance than the baseline approaches in re-
solving pronominal anaphors. As shown in Table 2, 
our approach outperforms Ng and Cardie?s single-
candidate based approach by 3.7 and 5.4 in F-
measure for MUC-6 and MUC-7, respectively. 
Besides, compared with Strube?s S-list algorithm, 
our approach also achieves gains in the F-measure 
by 3.2 (MUC-6), and 1.6 (MUC-7). In particular, 
our approach obtains significant improvement 
(21.1 for MUC-6, and 13.1 for MUC-7) over Con-
nolly et al?s twin-candidate based approach. 
 
MUC-6 MUC-7  
 R P F R P F 
Strube (1998)  76.1 74.3 75.1 62.9 60.3 61.6 
Ng and Cardie (2002a) 75.4 73.8 74.6 58.9 56.8 57.8 
Connolly et al (1997) 57.2 57.2 57.2 50.1 50.1 50.1 
Our approach 79.3 77.5 78.3 64.4 62.1 63.2 
Table 2:  Results for the pronoun resolution  
 
MUC-6 MUC-7  
R P F R P F 
Ng and Cardie (2002a) 51.0 89.9 65.0 39.1 86.4 53.8 
Connolly et al (1997) 52.2 52.2 52.2 43.7 43.7 43.7 
Our approach  51.3 90.4 65.4 39.7 87.6 54.6 
Table 3:  Results for the non-pronoun resolution  
MUC-6 MUC-7  
R P F R P F 
Ng and Cardie (2002a) 62.2 78.8 69.4 48.4 74.6 58.7 
Our approach 64.0 80.5 71.3 50.1 75.4 60.2 
Table 4: Results for the coreference resolution  
 
Compared with the gains in pronoun resolution, 
the improvement in non-pronoun resolution is 
slight. As shown in Table 3, our approach resolves 
non-pronominal anaphors with the recall of 51.3 
(39.7) and the precision of 90.4 (87.6) for MUC-6 
(MUC-7). In contrast to Ng and Cardie?s approach, 
the performance of our approach improves only 0.3 
(0.6) in recall and 0.5 (1.2) in precision. The rea-
son may be that in non-pronoun resolution, the 
coreference of an anaphor and its candidate is usu-
ally determined only by some strongly indicative 
features such as alias, apposition, string-matching, 
etc (this explains why we obtain a high precision 
but a low recall in non-pronoun resolution). There-
fore, most of the positive candidates are coreferen-
tial to the anaphors even though they are not the 
?best?. As a result, we can only see comparatively 
slight difference between the performances of the 
two approaches.  
Although Connolly et al?s approach also adopts 
the twin-candidate model, it achieves a poor per-
formance for both pronoun resolution and non-
pronoun resolution. The main reason is the absence 
of candidate filtering strategy in their approach 
(this is why the recall equals to the precision in the 
tables). Without candidate filtering, the recall may 
rise as the correct antecedents would not be elimi-
nated wrongly. Nevertheless, the precision drops 
largely due to the numerous invalid NPs in the 
candidate set. As a result, a significantly low F-
measure is obtained in their approach. 
Table 4 summarizes the overall performance of 
different approaches to coreference resolution. Dif-
ferent from Table 2 and 3, here we focus on 
whether a coreferential chain could be correctly 
identified. For this purpose, we obtain the recall, 
the precision and the F-measure using the standard 
MUC scoring program (Vilain et al 1995) for the 
coreference resolution task. Here the recall means 
the correct resolved chains over the whole 
coreferential chains in the data set, and precision 
means the correct resolved chains over the whole 
resolved chains.  
In line with the previous experiments, we see 
reasonable improvement in the performance of the 
coreference resolution: compared with the baseline 
approach based on the single-candidate model, the 
F-measure of approach increases from 69.4 to 71.3 
for MUC-6, and from 58.7 to 60.2 for MUC-7.  
6 Related Work 
A similar twin-candidate model was adopted in the 
anaphoric resolution system by Connolly et al 
(1997). The differences between our approach and 
theirs are: 
(1) In Connolly et al?s approach, all the preceding 
NPs of an anaphor are taken as the antecedent 
candidates, whereas in our approach we use 
candidate filters to eliminate invalid or irrele-
vant candidates.  
(2) The antecedent identification in Connolly et 
al.?s approach is to apply the classifier to 
successive pairs of candidates, each time 
retaining the better candidate. However, due to 
the lack of strong assumption of transitivity, 
the selection procedure is in fact a greedy 
search. By contrast, our approach evaluates a 
candidate according to the times it wins over 
the other competitors. Comparatively this 
algorithm could lead to a better solution. 
(3) Our approach makes use of more indicative 
features, such as Appositive, Name Alias, 
String-matching, etc. These features are effec-
tive especially for non-pronoun resolution. 
7 Conclusion 
In this paper we have proposed a competition 
learning approach to coreference resolution. We 
started with the introduction of the single-
candidate model adopted by most supervised ma-
chine learning approaches. We argued that the con-
fidence values returned by the single-candidate 
classifier are not reliable to be used as ranking cri-
terion for antecedent candidates. Alternatively, we 
presented a twin-candidate model that learns the 
competition criterion for antecedent candidates 
directly. We introduced how to adopt the twin-
candidate model in our competition learning ap-
proach to resolve the coreference problem. Particu-
larly, we proposed a candidate filtering algorithm 
that can effectively reduce the computational cost 
and data noises.  
The experimental results have proved the effec-
tiveness of our approach. Compared with the base-
line approach using the single-candidate model, the 
F-measure increases by 1.9 and 1.5 for MUC-6 and 
MUC-7 data set, respectively. The gains in the 
pronoun resolution contribute most to the overall 
improvement of coreference resolution. 
Currently, we employ the single-candidate clas-
sifier to filter the candidate set during resolution. 
While the filter guarantees the qualification of the 
candidates, it removes too many positive candi-
dates, and thus the recall suffers. In our future 
work, we intend to adopt a looser filter together 
with an anaphoricity determination module (Bean 
and Riloff, 1999; Ng and Cardie, 2002b). Only if 
an encountered NP is determined as an anaphor, 
we will select an antecedent from the candidate set 
generated by the looser filter. Furthermore, we 
would like to incorporate more syntactic features 
into our feature set, such as grammatical role or 
syntactic parallelism. These features may be help-
ful to improve the performance of pronoun resolu-
tion.  
References 
Chinatsu Aone and Scott W.Bennett. 1995. Evaluating 
automated and manual acquisition of anaphora reso-
lution strategies. In Proceedings of the 33rd Annual 
Meeting of the Association for Computational Lin-
guistics, Pages 122-129. 
D.Bean and E.Riloff. 1999. Corpus-Based identification 
of non-anaphoric noun phrases. In Proceedings of the 
37th Annual Meeting of the Association for Computa-
tional Linguistics, Pages 373-380. 
Brennan, S, E., M. W. Friedman and C. J. Pollard. 1987. 
A Centering approach to pronouns. In Proceedings of 
the 25th Annual Meeting of The Association for Com-
putational Linguistics, Page 155-162. 
Dennis Connolly, John D. Burger and David S. Day. 
1997. A machine learning approach to anaphoric ref-
erence. New Methods in Language Processing, Page 
133-144.  
Joseph F. McCarthy. 1996. A trainable approach to 
coreference resolution for Information Extraction. 
Ph.D. thesis. University of Massachusetts. 
Ruslan Mitkov. 1998. Robust pronoun resolution with 
limited knowledge. In Proceedings of the 17th Int. 
Conference on Computational Linguistics (COLING-
ACL'98), Page 869-875. 
Ruslan Mitkov. 1999. Anaphora resolution: The state of 
the art. Technical report. University of Wolverhamp-
ton, Wolverhampton. 
MUC-6. 1995. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). Morgan Kauf-
mann, San Francisco, CA. 
MUC-7. 1998. Proceedings of the Seventh Message 
Understanding Conference (MUC-7). Morgan Kauf-
mann, San Francisco, CA. 
Vincent Ng and Claire Cardie. 2002a. Improving ma-
chine learning approaches to coreference resolution. 
In Proceedings of the 40rd Annual Meeting of the As-
sociation for Computational Linguistics, Pages 104-
111. 
Vincent Ng and Claire Cardie. 2002b. Identifying ana-
phoric and non-anaphoric noun phrases to improve 
coreference resolution. In Proceedings of 19th Inter-
national Conference on Computational Linguistics 
(COLING-2002). 
J R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann, San Mateo, CA. 
Wee Meng Soon, Hwee Tou Ng and Daniel Chung 
Yong Lim. 2001. A machine learning approach to 
coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4), Page 521-544. 
Michael Strube. Never look back: An alternative to 
Centering. 1998. In Proceedings of the 17th Int. Con-
ference on Computational Linguistics and 36th An-
nual Meeting of ACL, Page 1251-1257 
Joel R. Tetreault. 2001. A Corpus-Based evaluation of 
Centering and pronoun resolution. Computational 
Linguistics, 27(4), Page 507-520. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and 
L.Hirschman. 1995. A model-theoretic coreference 
scoring scheme. In Proceedings of the Sixth Message 
understanding Conference (MUC-6), Pages 42-52. 
GD Zhou and J. Su, 2000. Error-driven HMM-based 
chunk tagger with context-dependent lexicon. In 
Proceedings of the Joint Conference on Empirical 
Methods on Natural Language Processing and Very 
Large Corpus (EMNLP/ VLC'2000).  
GD Zhou and J. Su. 2002. Named Entity recognition 
using a HMM-based chunk tagger. In Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics, P473-478. 
Improving Pronoun Resolution by Incorporating Coreferential
Information of Candidates
Xiaofeng Yang?? Jian Su? Guodong Zhou? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian,zhougd}
@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
{yangxiao,tancl}@comp.nus.edu.sg
Abstract
Coreferential information of a candidate, such
as the properties of its antecedents, is important
for pronoun resolution because it reflects the
salience of the candidate in the local discourse.
Such information, however, is usually ignored in
previous learning-based systems. In this paper
we present a trainable model which incorporates
coreferential information of candidates into pro-
noun resolution. Preliminary experiments show
that our model will boost the resolution perfor-
mance given the right antecedents of the can-
didates. We further discuss how to apply our
model in real resolution where the antecedents
of the candidate are found by a separate noun
phrase resolution module. The experimental re-
sults show that our model still achieves better
performance than the baseline.
1 Introduction
In recent years, supervised machine learning ap-
proaches have been widely explored in refer-
ence resolution and achieved considerable suc-
cess (Ge et al, 1998; Soon et al, 2001; Ng and
Cardie, 2002; Strube and Muller, 2003; Yang et
al., 2003). Most learning-based pronoun res-
olution systems determine the reference rela-
tionship between an anaphor and its antecedent
candidate only from the properties of the pair.
The knowledge about the context of anaphor
and antecedent is nevertheless ignored. How-
ever, research in centering theory (Sidner, 1981;
Grosz et al, 1983; Grosz et al, 1995; Tetreault,
2001) has revealed that the local focusing (or
centering) also has a great effect on the pro-
cessing of pronominal expressions. The choices
of the antecedents of pronouns usually depend
on the center of attention throughout the local
discourse segment (Mitkov, 1999).
To determine the salience of a candidate
in the local context, we may need to check
the coreferential information of the candidate,
such as the existence and properties of its an-
tecedents. In fact, such information has been
used for pronoun resolution in many heuristic-
based systems. The S-List model (Strube,
1998), for example, assumes that a co-referring
candidate is a hearer-old discourse entity and
is preferred to other hearer-new candidates.
In the algorithms based on the centering the-
ory (Brennan et al, 1987; Grosz et al, 1995), if
a candidate and its antecedent are the backward-
looking centers of two subsequent utterances re-
spectively, the candidate would be the most pre-
ferred since the CONTINUE transition is al-
ways ranked higher than SHIFT or RETAIN.
In this paper, we present a supervised
learning-based pronoun resolution system which
incorporates coreferential information of candi-
dates in a trainable model. For each candi-
date, we take into consideration the properties
of its antecedents in terms of features (hence-
forth backward features), and use the supervised
learning method to explore their influences on
pronoun resolution. In the study, we start our
exploration on the capability of the model by
applying it in an ideal environment where the
antecedents of the candidates are correctly iden-
tified and the backward features are optimally
set. The experiments on MUC-6 (1995) and
MUC-7 (1998) corpora show that incorporating
coreferential information of candidates boosts
the system performance significantly. Further,
we apply our model in the real resolution where
the antecedents of the candidates are provided
by separate noun phrase resolution modules.
The experimental results show that our model
still outperforms the baseline, even with the low
recall of the non-pronoun resolution module.
The remaining of this paper is organized as
follows. Section 2 discusses the importance of
the coreferential information for candidate eval-
uation. Section 3 introduces the baseline learn-
ing framework. Section 4 presents and evaluates
the learning model which uses backward fea-
tures to capture coreferential information, while
Section 5 proposes how to apply the model in
real resolution. Section 6 describes related re-
search work. Finally, conclusion is given in Sec-
tion 7.
2 The Impact of Coreferential
Information on Pronoun
Resolution
In pronoun resolution, the center of attention
throughout the discourse segment is a very im-
portant factor for antecedent selection (Mitkov,
1999). If a candidate is the focus (or center)
of the local discourse, it would be selected as
the antecedent with a high possibility. See the
following example,
<s> Gitano1 has pulled off a clever illusion2
with its3 advertising4. <s>
<s> The campaign5 gives its6 clothes a
youthful and trendy image to lure consumers
into the store. <s>
Table 1: A text segment from MUC-6 data set
In the above text, the pronoun ?its6? has
several antecedent candidates, i.e., ?Gitano1?,
?a clever illusion2?, ?its3?, ?its advertising4?
and ?The campaign5?. Without looking back,
?The campaign5? would be probably selected
because of its syntactic role (Subject) and its
distance to the anaphor. However, given the
knowledge that the company Gitano is the fo-
cus of the local context and ?its3? refers to
?Gitano1?, it would be clear that the pronoun
?its6? should be resolved to ?its3? and thus
?Gitano1?, rather than other competitors.
To determine whether a candidate is the ?fo-
cus? entity, we should check how the status (e.g.
grammatical functions) of the entity alternates
in the local context. Therefore, it is necessary
to track the NPs in the coreferential chain of
the candidate. For example, the syntactic roles
(i.e., subject) of the antecedents of ?its3? would
indicate that ?its3? refers to the most salient
entity in the discourse segment.
In our study, we keep the properties of the an-
tecedents as features of the candidates, and use
the supervised learning method to explore their
influence on pronoun resolution. Actually, to
determine the local focus, we only need to check
the entities in a short discourse segment. That
is, for a candidate, the number of its adjacent
antecedents to be checked is limited. Therefore,
we could evaluate the salience of a candidate
by looking back only its closest antecedent in-
stead of each element in its coreferential chain,
with the assumption that the closest antecedent
is able to provide sufficient information for the
evaluation.
3 The Baseline Learning Framework
Our baseline system adopts the common
learning-based framework employed in the sys-
tem by Soon et al (2001).
In the learning framework, each training or
testing instance takes the form of i{ana, candi},
where ana is the possible anaphor and candi is
its antecedent candidate1. An instance is associ-
ated with a feature vector to describe their rela-
tionships. As listed in Table 2, we only consider
those knowledge-poor and domain-independent
features which, although superficial, have been
proved efficient for pronoun resolution in many
previous systems.
During training, for each anaphor in a given
text, a positive instance is created by paring
the anaphor and its closest antecedent. Also a
set of negative instances is formed by paring the
anaphor and each of the intervening candidates.
Based on the training instances, a binary classi-
fier is generated using C5.0 learning algorithm
(Quinlan, 1993). During resolution, each possi-
ble anaphor ana, is paired in turn with each pre-
ceding antecedent candidate, candi, from right
to left to form a testing instance. This instance
is presented to the classifier, which will then
return a positive or negative result indicating
whether or not they are co-referent. The pro-
cess terminates once an instance i{ana, candi}
is labelled as positive, and ana will be resolved
to candi in that case.
4 The Learning Model Incorporating
Coreferential Information
The learning procedure in our model is similar
to the above baseline method, except that for
each candidate, we take into consideration its
closest antecedent, if possible.
4.1 Instance Structure
During both training and testing, we adopt the
same instance selection strategy as in the base-
line model. The only difference, however, is the
structure of the training or testing instances.
Specifically, each instance in our model is com-
posed of three elements like below:
1In our study candidates are filtered by checking the
gender, number and animacy agreements in advance.
Features describing the candidate (candi)
1. candi DefNp 1 if candi is a definite NP; else 0
2. candi DemoNP 1 if candi is an indefinite NP; else 0
3. candi Pron 1 if candi is a pronoun; else 0
4. candi ProperNP 1 if candi is a proper name; else 0
5. candi NE Type 1 if candi is an ?organization? named-entity; 2 if ?person?, 3 if
other types, 0 if not a NE
6. candi Human the likelihood (0-100) that candi is a human entity (obtained
from WordNet)
7. candi FirstNPInSent 1 if candi is the first NP in the sentence where it occurs
8. candi Nearest 1 if candi is the candidate nearest to the anaphor; else 0
9. candi SubjNP 1 if candi is the subject of the sentence it occurs; else 0
Features describing the anaphor (ana):
10. ana Reflexive 1 if ana is a reflexive pronoun; else 0
11. ana Type 1 if ana is a third-person pronoun (he, she,. . . ); 2 if a single
neuter pronoun (it,. . . ); 3 if a plural neuter pronoun (they,. . . );
4 if other types
Features describing the relationships between candi and ana:
12. SentDist Distance between candi and ana in sentences
13. ParaDist Distance between candi and ana in paragraphs
14. CollPattern 1 if candi has an identical collocation pattern with ana; else 0
Table 2: Feature set for the baseline pronoun resolution system
i{ana, candi, ante-of-candi}
where ana and candi, similar to the defini-
tion in the baseline model, are the anaphor and
one of its candidates, respectively. The new
added element in the instance definition, ante-
of-candi, is the possible closest antecedent of
candi in its coreferential chain. The ante-of-
candi is set to NIL in the case when candi has
no antecedent.
Consider the example in Table 1 again. For
the pronoun ?it6?, three training instances will
be generated, namely, i{its6, The compaign5,
NIL}, i{its6, its advertising4, NIL}, and
i{its6, its3, Gitano1}.
4.2 Backward Features
In addition to the features adopted in the base-
line system, we introduce a set of backward fea-
tures to describe the element ante-of-candi. The
ten features (15-24) are listed in Table 3 with
their respective possible values.
Like feature 1-9, features 15-22 describe the
lexical, grammatical and semantic properties of
ante-of-candi. The inclusion of the two features
Apposition (23) and candi NoAntecedent (24) is
inspired by the work of Strube (1998). The
feature Apposition marks whether or not candi
and ante-of-candi occur in the same appositive
structure. The underlying purpose of this fea-
ture is to capture the pattern that proper names
are accompanied by an appositive. The entity
with such a pattern may often be related to the
hearers? knowledge and has low preference. The
feature candi NoAntecedent marks whether or
not a candidate has a valid antecedent in the
preceding text. As stipulated in Strube?s work,
co-referring expressions belong to hearer-old en-
tities and therefore have higher preference than
other candidates. When the feature is assigned
value 1, all the other backward features (15-23)
are set to 0.
4.3 Results and Discussions
In our study we used the standard MUC-
6 and MUC-7 coreference corpora. In each
data set, 30 ?dry-run? documents were anno-
tated for training as well as 20-30 documents
for testing. The raw documents were prepro-
cessed by a pipeline of automatic NLP com-
ponents (e.g. NP chunker, part-of-speech tag-
ger, named-entity recognizer) to determine the
boundary of the NPs, and to provide necessary
information for feature calculation.
In an attempt to investigate the capability of
our model, we evaluated the model in an opti-
mal environment where the closest antecedent
of each candidate is correctly identified. MUC-
6 and MUC-7 can serve this purpose quite well;
the annotated coreference information in the
data sets enables us to obtain the correct closest
Features describing the antecedent of the candidate (ante-of-candi):
15. ante-candi DefNp 1 if ante-of-candi is a definite NP; else 0
16. ante-candi IndefNp 1 if ante-of-candi is an indefinite NP; else 0
17. ante-candi Pron 1 if ante-of-candi is a pronoun; else 0
18. ante-candi Proper 1 if ante-of-candi is a proper name; else 0
19. ante-candi NE Type 1 if ante-of-candi is an ?organization? named-entity; 2 if ?per-
son?, 3 if other types, 0 if not a NE
20. ante-candi Human the likelihood (0-100) that ante-of-candi is a human entity
21. ante-candi FirstNPInSent 1 if ante-of-candi is the first NP in the sentence where it occurs
22. ante-candi SubjNP 1 if ante-of-candi is the subject of the sentence where it occurs
Features describing the relationships between the candidate (candi) and ante-of-candi :
23. Apposition 1 if ante-of-candi and candi are in an appositive structure
Features describing the candidate (candi):
24. candi NoAntecedent 1 if candi has no antecedent available; else 0
Table 3: Backward features used to capture the coreferential information of a candidate
antecedent for each candidate and accordingly
generate the training and testing instances. In
the next section we will further discuss how to
apply our model into the real resolution.
Table 4 shows the performance of different
systems for resolving the pronominal anaphors 2
in MUC-6 and MUC-7. Default learning param-
eters for C5.0 were used throughout the exper-
iments. In this table we evaluated the perfor-
mance based on two kinds of measurements:
? ?Recall-and-Precision?:
Recall = #positive instances classified correctly#positive instances
Precision = #positive instances classified correctly#instances classified as positive
The above metrics evaluate the capability
of the learned classifier in identifying posi-
tive instances3. F-measure is the harmonic
mean of the two measurements.
? ?Success?:
Success = #anaphors resolved correctly#total anaphors
The metric4 directly reflects the pronoun
resolution capability.
The first and second lines of Table 4 compare
the performance of the baseline system (Base-
2The first and second person pronouns are discarded
in our study.
3The testing instances are collected in the same ways
as the training instances.
4In the experiments, an anaphor is considered cor-
rectly resolved only if the found antecedent is in the same
coreferential chain of the anaphor.
ante-candi_SubjNP = 1: 1 (49/5)
ante-candi_SubjNP = 0:
:..candi_SubjNP = 1:
:..SentDist = 2: 0 (3)
: SentDist = 0:
: :..candi_Human > 0: 1 (39/2)
: : candi_Human <= 0:
: : :..candi_NoAntecedent = 0: 1 (8/3)
: : candi_NoAntecedent = 1: 0 (3)
: SentDist = 1:
: :..ante-candi_Human <= 50 : 0 (4)
: ante-candi_Human > 50 : 1 (10/2)
:
candi_SubjNP = 0:
:..candi_Pron = 1: 1 (32/7)
candi_Pron = 0:
:..candi_NoAntecedent = 1:
:..candi_FirstNPInSent = 1: 1 (6/2)
: candi_FirstNPInSent = 0: ...
candi_NoAntecedent = 0: ...
Figure 1: Top portion of the decision tree
learned on MUC-6 with the backward features
line) and our system (Optimal), where DTpron
and DTpron?opt are the classifiers learned in
the two systems, respectively. The results in-
dicate that our system outperforms the base-
line system significantly. Compared with Base-
line, Optimal achieves gains in both recall (6.4%
for MUC-6 and 4.1% for MUC-7) and precision
(1.3% for MUC-6 and 9.0% for MUC-7). For
Success, we also observe an apparent improve-
ment by 4.7% (MUC-6) and 3.5% (MUC-7).
Figure 1 shows the portion of the pruned deci-
sion tree learned for MUC-6 data set. It visual-
izes the importance of the backward features for
the pronoun resolution on the data set. From
Testing Backward feature MUC-6 MUC-7
Experiments classifier assigner* R P F S R P F S
Baseline DTpron NIL 77.2 83.4 80.2 70.0 71.9 68.6 70.2 59.0
Optimal DTpron?opt (Annotated) 83.6 84.7 84.1 74.7 76.0 77.6 76.8 62.5
RealResolve-1 DTpron?opt DTpron?opt 75.8 83.8 79.5 73.1 62.3 77.7 69.1 53.8
RealResolve-2 DTpron?opt DTpron 75.8 83.8 79.5 73.1 63.0 77.9 69.7 54.9
RealResolve-3 DT?pron DTpron 79.3 86.3 82.7 74.7 74.7 67.3 70.8 60.8
RealResolve-4 DT?pron DT
?
pron 79.3 86.3 82.7 74.7 74.7 67.3 70.8 60.8
Table 4: Results of different systems for pronoun resolution on MUC-6 and MUC-7
(*Here we only list backward feature assigner for pronominal candidates. In RealResolve-1 to
RealResolve-4, the backward features for non-pronominal candidates are all found by DTnon?pron.)
the tree we could find that:
1.) Feature ante-candi SubjNP is of the most
importance as the root feature of the tree.
The decision tree would first examine the
syntactic role of a candidate?s antecedent,
followed by that of the candidate. This
nicely proves our assumption that the prop-
erties of the antecedents of the candidates
provide very important information for the
candidate evaluation.
2.) Both features ante-candi SubjNP and
candi SubjNP rank top in the decision tree.
That is, for the reference determination,
the subject roles of the candidate?s referent
within a discourse segment will be checked
in the first place. This finding supports well
the suggestion in centering theory that the
grammatical relations should be used as the
key criteria to rank forward-looking centers
in the process of focus tracking (Brennan
et al, 1987; Grosz et al, 1995).
3.) candi Pron and candi NoAntecedent are
to be examined in the cases when the
subject-role checking fails, which confirms
the hypothesis in the S-List model by
Strube (1998) that co-refereing candidates
would have higher preference than other
candidates in the pronoun resolution.
5 Applying the Model in Real
Resolution
In Section 4 we explored the effectiveness of
the backward feature for pronoun resolution. In
those experiments our model was tested in an
ideal environment where the closest antecedent
of a candidate can be identified correctly when
generating the feature vector. However, during
real resolution such coreferential information is
not available, and thus a separate module has
algorithm PRON-RESOLVE
input:
DTnon?pron: classifier for resolving non-pronouns
DTpron: classifier for resolving pronouns
begin:
M1..n:= the valid markables in the given docu-
ment
Ante[1..n] := 0
for i = 1 to N
for j = i - 1 downto 0
if (Mi is a non-pron and
DTnon?pron(i{Mi,Mj}) == + )
or
(Mi is a pron and
DTpron(i{Mi,Mj , Ante[j]}) == +)
then
Ante[i] := Mj
break
return Ante
Figure 2: The pronoun resolution algorithm by
incorporating coreferential information of can-
didates
to be employed to obtain the closest antecedent
for a candidate. We describe the algorithm in
Figure 2.
The algorithm takes as input two classifiers,
one for the non-pronoun resolution and the
other for pronoun resolution. Given a testing
document, the antecedent of each NP is identi-
fied using one of these two classifiers, depending
on the type of NP. Although a separate non-
pronoun resolution module is required for the
pronoun resolution task, this is usually not a
big problem as these two modules are often in-
tegrated in coreference resolution systems. We
just use the results of the one module to improve
the performance of the other.
5.1 New Training and Testing
Procedures
For a pronominal candidate, its antecedent can
be obtained by simply using DTpron?opt. For
Training Procedure:
T1. Train a non-pronoun resolution clas-
sifier DTnon?pron and a pronoun resolution
classifier DTpron, using the baseline learning
framework (without backward features).
T2. Apply DTnon?pron and DTpron to iden-
tify the antecedent of each non-pronominal
and pronominal markable, respectively, in a
given document.
T3. Go through the document again. Gen-
erate instances with backward features as-
signed using the antecedent information ob-
tained in T2.
T4. Train a new pronoun resolution classifier
DT?pron on the instances generated in T3.
Testing Procedure:
R1. For each given document, do T2?T3.
R2. Resolve pronouns by applying DT?pron.
Table 5: New training and testing procedures
a non-pronominal candidate, we built a non-
pronoun resolution module to identify its an-
tecedent. The module is a duplicate of the
NP coreference resolution system by Soon et
al. (2001)5 , which uses the similar learn-
ing framework as described in Section 3. In
this way, we could do pronoun resolution
just by running PRON-RESOLVE(DTnon?pron,
DTpron?opt), where DTnon?pron is the classifier
of the non-pronoun resolution module.
One problem, however, is that DTpron?opt is
trained on the instances whose backward fea-
tures are correctly assigned. During real resolu-
tion, the antecedent of a candidate is found by
DTnon?pron or DTpron?opt, and the backward
feature values are not always correct. Indeed,
for most noun phrase resolution systems, the
recall is not very high. The antecedent some-
times can not be found, or is not the closest
one in the preceding coreferential chain. Con-
sequently, the classifier trained on the ?perfect?
feature vectors would probably fail to output
anticipated results on the noisy data during real
resolution.
Thus we modify the training and testing pro-
cedures of the system. For both training and
testing instances, we assign the backward fea-
ture values based on the results from separate
NP resolution modules. The detailed proce-
dures are described in Table 5.
5Details of the features can be found in Soon et al
(2001)
algorithm REFINE-CLASSIFIER
begin:
DT1pron := DT
?
pron
for i = 1 to ?
Use DTipron to update the antecedents of
pronominal candidates and the correspond-
ing backward features;
Train DTi+1pron based on the updated training
instances;
if DTi+1pron is not better than DTipron then
break;
return DTipron
Figure 3: The classifier refining algorithm
The idea behind our approach is to train
and test the pronoun resolution classifier on
instances with feature values set in a consis-
tent way. Here the purpose of DTpron and
DTnon?pron is to provide backward feature val-
ues for training and testing instances. From this
point of view, the two modules could be thought
of as a preprocessing component of our pronoun
resolution system.
5.2 Classifier Refining
If the classifier DT?pron outperforms DTpron
as expected, we can employ DT?pron in place
of DTpron to generate backward features for
pronominal candidates, and then train a clas-
sifier DT??pron based on the updated training in-
stances. Since DT?pron produces more correct
feature values than DTpron, we could expect
that DT??pron will not be worse, if not better,
than DT?pron. Such a process could be repeated
to refine the pronoun resolution classifier. The
algorithm is described in Figure 3.
In algorithm REFINE-CLASSIFIER, the it-
eration terminates when the new trained clas-
sifier DTi+1pron provides no further improvement
than DTipron. In this case, we can replace
DTi+1pron by DTipron during the i+1(th) testing
procedure. That means, by simply running
PRON-RESOLVE(DTnon?pron,DTipron), we can
use for both backward feature computation and
instance classification tasks, rather than apply-
ing DTpron and DT?pron subsequently.
5.3 Results and Discussions
In the experiments we evaluated the perfor-
mance of our model in real pronoun resolution.
The performance of our model depends on the
performance of the non-pronoun resolution clas-
sifier, DTnon?pron. Hence we first examined the
coreference resolution capability of DTnon?pron
based on the standard scoring scheme by Vi-
lain et al (1995). For MUC-6, the module ob-
tains 62.2% recall and 78.8% precision, while for
MUC-7, it obtains 50.1% recall and 75.4% pre-
cision. The poor recall and comparatively high
precision reflect the capability of the state-of-
the-art learning-based NP resolution systems.
The third block of Table 4 summarizes the
performance of the classifier DTpron?opt in real
resolution. In the systems RealResolve-1 and
RealResolve-2, the antecedents of pronominal
candidates are found by DTpron?opt and DTpron
respectively, while in both systems the an-
tecedents of non-pronominal candidates are by
DTnon?pron. As shown in the table, compared
with the Optimal where the backward features
of testing instances are optimally assigned, the
recall rates of two systems drop largely by 7.8%
for MUC-6 and by about 14% for MUC-7. The
scores of recall are even lower than those of
Baseline. As a result, in comparison with Op-
timal, we see the degrade of the F-measure and
the success rate, which confirms our hypothesis
that the classifier learned on perfect training in-
stances would probably not perform well on the
noisy testing instances.
The system RealResolve-3 listed in the fifth
line of the table uses the classifier trained
and tested on instances whose backward fea-
tures are assigned according to the results from
DTnon?pron and DTpron. From the table we can
find that: (1) Compared with Baseline, the sys-
tem produces gains in recall (2.1% for MUC-6
and 2.8% for MUC-7) with no significant loss
in precision. Overall, we observe the increase in
F-measure for both data sets. If measured by
Success, the improvement is more apparent by
4.7% (MUC-6) and 1.8% (MUC-7). (2) Com-
pared with RealResolve-1(2), the performance
decrease of RealResolve-3 against Optimal is
not so large. Especially for MUC-6, the system
obtains a success rate as high as Optimal.
The above results show that our model can
be successfully applied in the real pronoun res-
olution task, even given the low recall of the
current non-pronoun resolution module. This
should be owed to the fact that for a candidate,
its adjacent antecedents, even not the closest
one, could give clues to reflect its salience in
the local discourse. That is, the model prefers a
high precision to a high recall, which copes well
with the capability of the existing non-pronoun
resolution module.
In our experiments we also tested the clas-
sifier refining algorithm described in Figure 3.
We found that for both MUC-6 and MUC-7
data set, the algorithm terminated in the second
round. The comparison of DT2pron and DT1pron
(i.e. DT?pron) showed that these two trees were
exactly the same. The algorithm converges fast
probably because in the data set, most of the
antecedent candidates are non-pronouns (89.1%
for MUC-6 and 83.7% for MUC-7). Conse-
quently, the ratio of the training instances with
backward features changed may be not substan-
tial enough to affect the classifier generation.
Although the algorithm provided no further
refinement for DT?pron, we can use DT
?
pron, as
suggested in Section 5.2, to calculate back-
ward features and classify instances by running
PRON-RESOLVE(DTnon?pron, DT?pron). The
results of such a system, RealResolve-4, are
listed in the last line of Table 4. For both MUC-
6 and MUC-7, RealResolve-4 obtains exactly
the same performance as RealResolve-3.
6 Related Work
To our knowledge, our work is the first ef-
fort that systematically explores the influence of
coreferential information of candidates on pro-
noun resolution in learning-based ways. Iida et
al. (2003) also take into consideration the con-
textual clues in their coreference resolution sys-
tem, by using two features to reflect the ranking
order of a candidate in Salience Reference List
(SRL). However, similar to common centering
models, in their system the ranking of entities
in SRL is also heuristic-based.
The coreferential chain length of a candidate,
or its variants such as occurrence frequency and
TFIDF, has been used as a salience factor in
some learning-based reference resolution sys-
tems (Iida et al, 2003; Mitkov, 1998; Paul et
al., 1999; Strube and Muller, 2003). However,
for an entity, the coreferential length only re-
flects its global salience in the whole text(s), in-
stead of the local salience in a discourse segment
which is nevertheless more informative for pro-
noun resolution. Moreover, during resolution,
the found coreferential length of an entity is of-
ten incomplete, and thus the obtained length
value is usually inaccurate for the salience eval-
uation.
7 Conclusion and Future Work
In this paper we have proposed a model which
incorporates coreferential information of candi-
dates to improve pronoun resolution. When
evaluating a candidate, the model considers its
adjacent antecedent by describing its properties
in terms of backward features. We first exam-
ined the effectiveness of the model by applying
it in an optimal environment where the clos-
est antecedent of a candidate is obtained cor-
rectly. The experiments show that it boosts
the success rate of the baseline system for both
MUC-6 (4.7%) and MUC-7 (3.5%). Then we
proposed how to apply our model in the real res-
olution where the antecedent of a non-pronoun
is found by an additional non-pronoun resolu-
tion module. Our model can still produce Suc-
cess improvement (4.7% for MUC-6 and 1.8%
for MUC-7) against the baseline system, de-
spite the low recall of the non-pronoun resolu-
tion module.
In the current work we restrict our study only
to pronoun resolution. In fact, the coreferential
information of candidates is expected to be also
helpful for non-pronoun resolution. We would
like to investigate the influence of the coreferen-
tial factors on general NP reference resolution in
our future work.
References
S. Brennan, M. Friedman, and C. Pollard.
1987. A centering approach to pronouns. In
Proceedings of the 25th Annual Meeting of
the Association for Compuational Linguis-
tics, pages 155?162.
N. Ge, J. Hale, and E. Charniak. 1998. A
statistical approach to anaphora resolution.
In Proceedings of the 6th Workshop on Very
Large Corpora.
B. Grosz, A. Joshi, and S. Weinstein. 1983.
Providing a unified account of definite noun
phrases in discourse. In Proceedings of the
21st Annual meeting of the Association for
Computational Linguistics, pages 44?50.
B. Grosz, A. Joshi, and S. Weinstein. 1995.
Centering: a framework for modeling the
local coherence of discourse. Computational
Linguistics, 21(2):203?225.
R. Iida, K. Inui, H. Takamura, and Y. Mat-
sumoto. 2003. Incorporating contextual cues
in trainable models for coreference resolu-
tion. In Proceedings of the 10th Confer-
ence of EACL, Workshop ?The Computa-
tional Treatment of Anaphora?.
R. Mitkov. 1998. Robust pronoun resolution
with limited knowledge. In Proceedings of the
17th Int. Conference on Computational Lin-
guistics, pages 869?875.
R. Mitkov. 1999. Anaphora resolution: The
state of the art. Technical report, University
of Wolverhampton.
MUC-6. 1995. Proceedings of the Sixth Message
Understanding Conference. Morgan Kauf-
mann Publishers, San Francisco, CA.
MUC-7. 1998. Proceedings of the Seventh
Message Understanding Conference. Morgan
Kaufmann Publishers, San Francisco, CA.
V. Ng and C. Cardie. 2002. Improving machine
learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics, pages 104?111, Philadelphia.
M. Paul, K. Yamamoto, and E. Sumita. 1999.
Corpus-based anaphora resolution towards
antecedent preference. In Proceedings of
the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, Work-
shop ?Coreference and It?s Applications?,
pages 47?52.
J. R. Quinlan. 1993. C4.5: Programs for ma-
chine learning. Morgan Kaufmann Publish-
ers, San Francisco, CA.
C. Sidner. 1981. Focusing for interpretation
of pronouns. American Journal of Computa-
tional Linguistics, 7(4):217?231.
W. Soon, H. Ng, and D. Lim. 2001. A ma-
chine learning approach to coreference reso-
lution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
M. Strube and C. Muller. 2003. A machine
learning approach to pronoun resolution in
spoken dialogue. In Proceedings of the 41st
Annual Meeting of the Association for Com-
putational Linguistics, pages 168?175, Japan.
M. Strube. 1998. Never look back: An alterna-
tive to centering. In Proceedings of the 17th
Int. Conference on Computational Linguis-
tics and 36th Annual Meeting of ACL, pages
1251?1257.
J. R. Tetreault. 2001. A corpus-based eval-
uation of centering and pronoun resolution.
Computational Linguistics, 27(4):507?520.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly,
and L. Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proceedings of
the Sixth Message understanding Conference
(MUC-6), pages 45?52, San Francisco, CA.
Morgan Kaufmann Publishers.
X. Yang, G. Zhou, J. Su, and C. Tan.
2003. Coreference resolution using competi-
tion learning approach. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, Japan.
Multi-Criteria-based Active Learning for Named Entity Recognition 
Dan Shen??1 Jie Zhang?? Jian Su? Guodong Zhou? Chew-Lim Tan? 
? Institute for Infocomm Technology 
21 Heng Mui Keng Terrace 
Singapore 119613 
? Department of Computer Science 
National University of Singapore 
3 Science Drive 2, Singapore 117543 
{shendan,zhangjie,sujian,zhougd}@i2r.a-star.edu.sg 
{shendan,zhangjie,tancl}@comp.nus.edu.sg 
                                                                 
1 Current address of the first author: Universit?t des Saarlandes, Computational Linguistics Dept., 66041 Saarbr?cken, Germany 
dshen@coli.uni-sb.de 
 
 
 
 
Abstract 
In this paper, we propose a multi-criteria -
based active learning approach and effec-
tively apply it to named entity recognition. 
Active learning targets to minimize the 
human annotation efforts by selecting ex-
amples for labeling.  To maximize the con-
tribution of the selected examples, we 
consider the multiple criteria: informative-
ness, representativeness and diversity  and 
propose measures to quantify them.  More 
comprehensively, we incorporate all the 
criteria using two selection strategies, both 
of which result in less labeling cost than 
single-criterion-based method.  The results 
of the named entity recognition in both 
MUC-6 and GENIA show that the labeling 
cost can be reduced by at least 80% with-
out degrading the performance. 
1 Introduction 
In the machine learning approaches of natural lan-
guage processing (NLP), models are generally 
trained on large annotated corpus.  However, anno-
tating such corpus is expensive and time-
consuming, which makes it difficult to adapt an 
existing model to a new domain.  In order to over-
come this difficulty, active learning (sample selec-
tion) has been studied in more and more NLP 
applications such as POS tagging (Engelson and 
Dagan 1999), information extraction (Thompson et 
al. 1999), text classif ication (Lewis and Catlett 
1994; McCallum and Nigam 1998; Schohn and 
Cohn 2000; Tong and Koller 2000; Brinker 2003), 
statistical parsing (Thompson et al 1999; Tang et 
al. 2002; Steedman et al 2003), noun phrase 
chunking (Ngai and Yarowsky 2000), etc. 
Active learning is based on the assumption that 
a small number of annotated examples and a large 
number of unannotated examples are available.  
This assumption is valid in most NLP tasks.  Dif-
ferent from supervised learning in which the entire 
corpus are labeled manually, active learning is to 
select the most useful example for labeling and add 
the labeled example  to training set to retrain model.  
This procedure is repeated until the model achieves 
a certain level of performance.  Practically, a batch 
of examples are selected at a time, called batched-
based sample selection (Lewis and Catlett 1994) 
since it is time consuming to retrain the model if 
only one new example is added to the training set.  
Many existing work in the area focus on two ap-
proaches: certainty-based methods (Thompson et 
al. 1999; Tang et al 2002; Schohn and Cohn 2000; 
Tong and Koller 2000; Brinker 2003) and commit-
tee-based methods (McCallum and Nigam 1998; 
Engelson and Dagan 1999; Ngai and Yarowsky 
2000) to select the most informative examples for 
which the current model are most uncertain. 
Being the first piece of work on active learning 
for name entity recognition (NER) task, we target 
to minimize the human annotation efforts yet still 
reaching the same level of performance as a super-
vised learning approach.  For this purpose, we 
make a more comprehensive consideration on the 
contribution of individual examples, and more im-
portantly maximizing the contribution of a batch 
based on three criteria : informativeness, represen-
tativeness and diversity. 
First, we propose three scoring functions to 
quantify the informativeness of an example , which 
can be used to select the most uncertain examples.  
Second, the representativeness measure is further 
proposed to choose the examples representing the 
majority.  Third, we propose two diversity consid-
erations (global and local) to avoid repetition 
among the examples of a batch.  Finally, two com-
bination strategies with the above three criteria are 
proposed to reach the maximum effectiveness on 
active learning for NER. 
We build our NER model using Support Vec-
tor Machines (SVM).  The experiment shows that 
our active learning methods achieve a promising 
result in this NER task.  The results in both MUC-
6 and GENIA show that the amount of the labeled 
training data can be reduced by at least 80% with-
out degrading the quality of the named entity rec-
ognizer.  The contributions not only come from the 
above measures, but also the two sample selection 
strategies which effectively incorporate informa-
tiveness, representativeness and diversity criteria.  
To our knowledge, it is the first work on consider-
ing the three criteria all together for active learning.  
Furthermore, such measures and strategies can be 
easily adapted to other active learning tasks as well.  
 
2 Multi-criteria for NER Active Learning 
Support Vector Machines (SVM) is a powerful 
machine learning method, which has been applied 
successfully in NER tasks, such as (Kazama et al 
2002; Lee et al 2003).  In this paper, we apply ac-
tive learning methods to a simple  and effective 
SVM model to recognize one class of names at a 
time, such as protein names, person names, etc.  In 
NER, SVM is to classify a word into positive class 
?1? indicating that the word is a part of an entity, 
or negative class ?-1? indicating that the word is 
not a part of an entity.  Each word in SVM is rep-
resented as a high-dimensional feature vector in-
cluding surface word information, orthographic 
features, POS feature and semantic trigger features 
(Shen et al 2003).  The semantic trigger features 
consist of some special head nouns for an entity 
class which is supplied by users.  Furthermore, a 
window (size = 7), which represents the local con-
text of the target word w, is also used to classify w.   
However, for active learning in NER, it is not 
reasonable to select a single word without context 
for human to label.  Even if we require human to 
label a single word, he has to make an addition 
effort to refer to the context of the word.  In our 
active learning process, we select a word sequence 
which consists of a machine-annotated named en-
tity and its context rather than a single word.  
Therefore, all of the measures we propose for ac-
tive learning should be applied to the machine-
annotated named entities and we have to further 
study how to extend the measures for words to 
named entities.  Thus, the active learning in SVM-
based NER will be more complex than that in sim-
ple classification tasks, such as text classif ication 
on which most SVM active learning works are 
conducted (Schohn and Cohn 2000; Tong and 
Koller 2000; Brinker 2003).  In the next part, we 
will introduce informativeness, representativeness 
and diversity measures for the SVM-based NER. 
2.1 Informativeness 
The basic idea of informativeness criterion is simi-
lar to certainty-based sample selection methods, 
which have been used in many previous works.  In 
our task, we use a distance-based measure to 
evaluate the informativeness of a word and extend 
it to the measure of an entity using three scoring 
functions.  We prefer the examples with high in-
formative degree for which the current model are 
most uncertain. 
2.1.1 Informativeness Measure for Word 
In the simplest linear form, training SVM is to find 
a hyperplane that can separate the posit ive and 
negative examples in training set with maximum 
margin.  The margin is defined by the distance of 
the hyperplane to the nearest of the positive and 
negative examples.  The training examples which 
are closest to the hyperplane are called support 
vectors.  In SVM, only the support vectors are use-
ful for the classification, which is different from 
statistical models.  SVM training is to get these 
support vectors and their weights from training set 
by solving quadratic programming problem.  The 
support vectors can later be used to classify the test 
data. 
Intuitively, we consider the informativeness of 
an example  as how it can make effect on the sup-
port vectors by adding it to training set.  An exam-
ple may be informative for the learner if the 
distance of its feature vector to the hyperplane is 
less than that of the support vectors to the hyper-
plane (equal to 1).  This intuition is also justified 
by (Schohn and Cohn 2000; Tong and Koller 2000) 
based on a version space analysis.  They state that 
labeling an example that lies on or close to the hy-
perplane is guaranteed to have an effect on the so-
lution.  In our task, we use the distance to measure 
the informativeness of an example. 
The distance of a word?s feature vector to the 
hyperplane is computed as follows: 
1
( ) ( , )
N
i i i
i
Dist y k ba
=
= +?w s w  
where w is the feature vector of the word, ai, yi, si 
corresponds to the weight, the class and the feature 
vector of the ith support vector respectively.  N is 
the number of the support vectors in current model. 
We select the example with minimal Dist, 
which indicates that it comes closest to the hyper-
plane in feature space.  This example is considered 
most informative for current model. 
2.1.2 Informativeness Measure for Named 
Entity 
Based on the above informativeness measure for a 
word, we compute the overall informativeness de-
gree of a named entity NE.  In this paper, we pro-
pose three scoring functions as follows. Let NE = 
w1?wN in which wi is the feature vector of the ith 
word of NE. 
? Info_Avg: The informativeness of NE is 
scored by the average distance of the words in 
NE to the hyperplane.  
 ( ) 1 ( )
i
i
N E
Info NE Dist
?
= - ?
w
w  
 where, wi is the feature vector of the ith word in 
NE. 
? Info_Min: The informativeness of NE is 
scored by the minimal distance of the words in 
NE. 
 ( ) 1 { ( )}
i
iNE
Info NE Min Dist
?
= -
w
w  
? Info_S/N: If the distance of a word to the hy-
perplane is less than a threshold a (= 1 in our 
task), the word is considered with short dis-
tance.  Then, we compute the proportion of the 
number of words with short distance to the to-
tal number of words in the named entity and 
use this proportion to quantify the informa-
tiveness of the named entity.  
 
( ( ) )
( ) i
i
N E
NUM Dist
Info NE
N
a
?
<
= w
w
 
In Section 4.3, we will evaluate the effective-
ness of these scoring functions. 
2.2 Representativeness 
In addition to the most informative example, we 
also prefer the most representative example.  The 
representativeness of an example can be evaluated 
based on how many examples there are similar or 
near to it.  So, the examples with high representa-
tive degree are less likely to be an outlier.  Adding 
them to the training set will have effect on a large 
number of unlabeled examples.  There are only a 
few works considering this selection criterion 
(McCallum and Nigam 1998; Tang et al 2002) and 
both of them are specific to their tasks, viz. text 
classification and statistical parsing.  In this section, 
we compute the simila rity between words using a 
general vector-based measure, extend this measure 
to named entity level using dynamic time warping 
algorithm and quantify the representativeness of a 
named entity by its density. 
2.2.1 Similarity Measure  between Words 
In general vector space model, the similarity be-
tween two vectors may be measured by computing 
the cosine value of the angle between them.  The 
smaller the angle is, the more similar between the 
vectors are.  This measure, called cosine-similarity 
measure, has been widely used in information re-
trieval tasks (Baeza-Yates and Ribeiro-Neto 1999).    
In our task, we also use it to quantify the similarity 
between two words.  Particularly, the calculation in 
SVM need be projected to a higher dimensional 
space by using a certain kernel function ( , )i jK w w .  
Therefore, we adapt the cosine-similarity measure 
to SVM as follows: 
( , )
( , )
( , ) ( , )
i j
i j
i i j j
k
Sim
k k
=
w w
w w
w w w w
 
where, wi and wj are the feature vectors of the 
words i and j.  This calculation is also supported by 
(Brinker 2003)?s work.  Furthermore, if we use the 
linear kernel ( , )i j i jk = ?w w w w , the measure is 
the same as the traditional cosine similarity meas-
ure cos i j
i j
q
?
=
?
w w
w w
 and may be regarded as a 
general vector-based similarity measure. 
2.2.2 Similarity Meas ure between Named En-
tities 
In this part, we compute the similarity between two 
machine-annotated named entities given the simi-
larities between words.  Regarding an entity as a 
word sequence, this work is analogous to the 
alignment of two sequences.  We employ the dy-
namic time warping (DTW) algorithm (Rabiner et 
al. 1978) to find an optimal alignment between the 
words in the sequences which maximize the accu-
mulated similarity degree between the sequences.  
Here, we adapt it to our task.  A sketch of the 
modified algorithm is as follows. 
Let NE1 = w11w12?w1n?w1N, (n = 1,?, N) and 
NE2 = w21w22?w2m?w2M, (m = 1,?, M) denote two 
word sequences to be matched.  NE1 and NE2 con-
sist of M and N words respectively.  NE1(n) = w1n 
and NE2(m) = w2m.  A similarity value Sim(w1n ,w2m) 
has been known for every pair of words (w1n,w2m) 
within NE1 and NE2.  The goal of DTW is to find a 
path, m = map(n), which map n onto the corre-
sponding m such that the accumulated similarity 
Sim* along the path is maximized. 
1 2
{ ( )} 1
* { ( ( ), ( ( ))}
N
m a p n n
Sim M a x Sim N E n N E m a p n
=
= ?  
A dynamic programming method is used to deter-
mine the optimum path map(n).  The accumulated 
similarity SimA to any grid point (n, m) can be re-
cursively calculated as 
1 2( , ) ( , ) ( 1, )A n m Aq mSim n m Sim w w M a x S i m n q?= + -
Finally, * ( , )ASim Sim N M=  
Certainly, the overall similarity measure Sim* 
has to be normalized as longer sequences normally 
give higher similarity value.  So, the similarity be-
tween two sequences NE1 and NE2 is calculated as 
1 2
*( , )
( , )
SimSim NE NE
Max N M
=  
2.2.3 Representativeness Measure for Named 
Entity 
Given a set of machine-annotated named entities 
NESet = {NE1, ? , NEN}, the representativeness of 
a named entity NEi in NESet is quantified by its 
density.  The density of NEi is defined as the aver-
age similarity between NEi and all the other enti-
ties NEj in NESet as follows. 
( , )
( )
1
i j
j i
i
Sim NE NE
Density N E
N
?=
-
?
 
If NEi has the largest density among all the entities 
in NESet, it can be regarded as the centroid of NE-
Set and also the most representative examples in 
NESet. 
2.3 Diversity 
Diversity criterion is to maximize the training util-
ity of a batch.  We prefer the batch in which the 
examples have high variance to each other.  For 
example, given the batch size 5, we try not to se-
lect five repetitious examples at a time.  To our 
knowledge, there is only one work (Brinker 2003) 
exploring this criterion.  In our task, we propose 
two methods: local and global, to make the exam-
ples diverse enough in a batch.   
2.3.1 Global Consideration 
For a global consideration, we cluster all named 
entities in NESet based on the similarity measure 
proposed in Section 2.2.2.  The named entities in 
the same cluster may be considered similar to each 
other, so we will select the named entities from 
different clusters at one time.  We employ a K-
means clustering algorithm (Jelinek 1997), which 
is shown in Figure 1. 
Given: 
NESet = {NE1, ? , NEN} 
Suppose: 
The number of clusters is K 
Initialization: 
Randomly equally partition {NE1, ? , NEN} into K 
initial clusters Cj (j = 1, ? , K). 
Loop until the number of changes for the centroids of 
all clusters is less than a threshold 
? Find the centroid of each cluster Cj (j = 1, ? , K). 
 arg ( ( , ))
j i j
j i
NE C NE C
NECent max Sim NE NE
? ?
= ?  
? Repartition {NE1, ? , NEN} into K clusters.  NEi 
will be assigned to Cluster Cj if 
 
( , ) ( , ),i j i wSim NE NECent Sim NE NECent w j? ?  
Figure 1: Global Consideration for Diversity: K-
Means Clustering algorithm 
In each round, we need to compute the pair-
wise similarities within each cluster to get the cen-
troid of the cluster.  And then, we need to compute 
the similarities between each example and all cen-
troids to repartition the examples.  So, the algo-
rithm is time-consuming.  Based on the assumption 
that N examples are uniformly distributed between 
the K clusters, the time complexity of the algo-
rithm is about O(N2/K+NK) (Tang et al 2002).  In 
one of our experiments, the size of the NESet (N) is 
around 17000 and K is equal to 50, so the time 
complexity is about O(106).  For efficiency, we 
may filter the entities in NESet before clustering 
them, which will be further discussed in Section 3.  
2.3.2 Local Consideration 
When selecting a machine-annotated named entity, 
we compare it with all previously selected named 
entities in the current batch.  If the similarity be-
tween them is above a threshold ?, this example 
cannot be allowed to add into the batch.  The order 
of selecting examples is based on some measure, 
such as informativeness measure, representative-
ness measure or their combination.  This local se-
lection method is shown in Figure 2.  In this way, 
we avoid selecting too similar examples (similarity 
value ?  ?) in a batch.  The threshold ? may be the 
average similarity between the examples in NESet. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
Initialization:  
BatchSet  = empty 
Loop until BatchSet is full 
? Select NEi based on some measure from NESet. 
? RepeatFlag = false; 
? Loop from j = 1 to CurrentSize(BatchSet)  
 If ( , )i jSim NE NE b? Then 
 RepeatFlag = true; 
 Stop the Loop; 
? If RepeatFlag == false Then 
add NEi into BatchSet 
? remove NEi from NESet 
Figure 2: Local Consideration for Diversity 
 
This consideration only requires O(NK+K2) 
computational time.  In one of our experiments (N 
 ?17000 and K = 50), the time complexity is about 
O(105).  It is more efficient than clustering algo-
rithm described in Section 2.3.1.  
 
3 Sample Selection strategies 
In this section, we will study how to combine and 
strike a proper balance between these criteria, viz. 
informativeness, representativeness and diversity, 
to reach the maximum effectiveness on NER active 
learning.  We build two strategies to combine the 
measures proposed above.  These strategies are 
based on the varying priorities of the criteria and 
the varying degrees to satisfy the criteria. 
? Strategy 1: We first consider the informative-
ness criterion.  We choose m examples with the 
most informativeness score from NESet to an in-
termediate set called INTERSet.  By this pre-
selecting, we make the selection process faster in 
the later steps since the size of INTERSet is much 
smaller than that of NESet.  Then we cluster the 
examples in INTERSet and choose the centroid of 
each cluster into a batch called BatchSet.  The cen-
troid of a cluster is the most representative exam-
ple in that cluster since it has the largest density.  
Furthermore, the examples in different clusters 
may be considered diverse to each other.  By this 
means, we consider representativeness and diver-
sity criteria at the same time.  This strategy is 
shown in Figure 3.  One limitation of this strategy 
is that clustering result may not reflect the distribu-
tion of whole sample space since we only cluster 
on INTERSet for efficiency.  The other is that since 
the representativeness of an example is only evalu-
ated on a cluster.  If the cluster size is too small, 
the most representative example in this cluster may 
not be representative in the whole sample space. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
INTERSet with the maximal size M 
Steps :  
? BatchSet  = ?  
? INTERSet = ?  
? Select M entities with most Info score from NESet 
to INTERSet. 
? Cluster the entities in INTERSet into K clusters 
? Add the centroid entity of each cluster to BatchSet 
Figure 3: Sample Selection Strategy 1 
 
? Strategy 2: (Figure 4) We combine the infor-
mativeness and representativeness criteria  using 
the functio ( ) (1 ) ( )i iInfo NE Density NEl l+ - , in 
which the Info and Density  value of NEi are nor-
malized first.  The individual importance of each 
criterion in this function is adjusted by the trade-
off parameter l ( 0 1l? ? ) (set to 0.6 in our 
experiment).  First, we select a candidate example 
NEi with the maximum value of this function from 
NESet.  Second, we consider diversity criterion 
using the local method in Section 3.3.2.  We add 
the candidate example NEi to a batch only if NEi is 
different enough from any previously selected ex-
ample in the batch.  The threshold ? is set to the 
average pair-wise similarity of the entities in NE-
Set. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
Initialization:  
BatchSet  = ?  
Loop until BatchSet is full 
? Select NEi which have the maximum value for the 
combination function between Info score and Den-
sity socre from NESet. 
arg ( ( ) (1 ) ( ))
i
i i i
N E NESet
N E Max Info NE Density NEl l
?
= + -
 
? RepeatFlag = false; 
? Loop from j = 1 to CurrentSize(BatchSet)  
 If ( , )i jSim NE NE b? Then 
 RepeatFlag = true; 
 Stop the Loop; 
? If RepeatFlag == false Then 
add NEi into BatchSet 
? remove NEi from NESet 
Figure 4: Sample Selection Strategy 2 
 
4 Experimental Results and Analysis 
4.1 Experiment Settings  
In order to evaluate the effectiveness of our selec-
tion strategies, we apply them to recognize protein 
(PRT) names in biomedical domain using GENIA 
corpus V1.1 (Ohta et al 2002) and person (PER), 
location (LOC), organization (ORG) names in 
newswire domain using MUC-6 corpus.  First, we 
randomly split the whole corpus into three parts: an 
initial training set to build an in itial model, a test 
set to evaluate the performance of the model and 
an unlabeled set to select examples.  The size of 
each data set is shown in Table 1.  Then, iteratively, 
we select a batch of examples following the selec-
tion strategies proposed, require human experts to 
label them and add them into the training set.  The 
batch size K = 50 in GENIA and 10 in MUC-6.  
Each example is defined as a machine-recognized 
named entity and its context words (previous 3 
words and next 3 words). 
Domain Class Corpus Initial Training Set Test Set Unlabeled Set 
Biomedical PRT GENIA1.1 10 sent. (277 words) 900 sent. (26K words) 8004 sent. (223K words) 
PER 5 sent. (131 words) 7809 sent. (157K words) 
LOC 5 sent. (130 words) 7809 sent. (157K words) 
 
Newswire 
ORG 
 
MUC-6 
 5 sent. (113 words) 
 
602 sent. (14K words) 
 7809 sent. (157K words) 
Table 1: Experiment settings for active learning using GENIA1.1(PRT) and MUC-6(PER,LOC,ORG) 
The goal of our work is to minimize the human 
annotation effort to learn a named entity recognizer 
with the same performance level as supervised 
learning.  The performance of our model is evalu-
ated using ?precision/recall/F-measure?. 
4.2 Overall Result in GENIA and MUC-6 
In this section, we evaluate our selection strategies 
by comparing them with a random selection 
method, in which a batch of examples is randomly 
selected iteratively, on GENIA and MUC-6 corpus.  
Table 2 shows the amount of training data needed 
to achieve the performance of supervised learning 
using various selection methods, viz. Random, 
Strategy1 and Strategy2.  In GENIA, we find: 
? The model achieves 63.3 F-measure using 223K  
words in the supervised learning. 
? The best performer is Strategy2 (31K words), 
requiring less than 40% of the training data that 
Random (83K words) does and 14% of the train-
ing data that the supervised learning does. 
? Strategy1 (40K words) performs slightly worse 
than Strategy2, requiring 9K more words.  It is 
probably because Strategy1 cannot avoid select-
ing outliers if a cluster is too small. 
? Random (83K words) requires about 37% of the 
training data that the supervised learning does.  It 
indicates that only the words in and around a 
named entity are useful for classification and the 
words far from the named entity may not be 
helpful. 
 
Class Supervised Random Strategy1 Strategy2 
PRT 223K (F=63.3) 83K 40K 31K 
PER 157K (F=90.4) 11.5K 4.2K 3.5K 
LOC 157K (F=73.5) 13.6K 3.5K 2.1K 
ORG 157K (F=86.0) 20.2K 9.5K 7.8K 
Table 2: Overall Result in GENIA and MUC-6 
Furthermore, when we apply our model to news-
wire domain (MUC-6) to recognize person, loca-
tion and organization names, Strategy1 and 
Strategy2 show a more promising result by com-
paring with the supervised learning and Random, 
as shown in Table 2.  On average, about 95% of 
the data can be reduced to achieve the same per-
formance with the supervised learning in MUC-6.  
It is probably because NER in the newswire do-
main is much simpler than that in the biomedical 
domain (Shen et al 2003) and named entities are 
less and distributed much sparser in the newswire 
texts than in the biomedical texts. 
 
4.3 Effectiveness of Informativeness-based 
Selection Method 
In this section, we investigate the effectiveness of 
informativeness criterion in NER task.  Figure 5 
shows a plot of training data size versus F-measure 
achieved by the informativeness-based measures in 
Section 3.1.2: Info_Avg, Info_Min  and Info_S/N as 
well as Random.  We make the comparisons in 
GENIA corpus.  In Figure 5, the horizontal line is 
the performance level (63.3 F-measure) achieved 
by supervised learning (223K words).  We find 
that the three informativeness-based measures per-
form similarly and each of them outperforms Ran-
dom.  Table 3 highlights the various data sizes to 
achieve the peak performance using these selection 
methods.  We find that Random (83K words) on 
average requires over 1.5 times as much as data to 
achieve the same performance as the informative-
ness-based selection methods (52K words). 
 
0.5
0.55
0.6
0.65
0 20 40 60 80K words
F
Supervised
Random
Info_Min
Info_S/N
Info_Avg
 
Figure 5: Active learning curves: effectiveness of the three in-
formativeness-criterion-based selections comparing with the 
Random selection. 
Supervised Random Info_Avg Info_Min Info_ S/N 
223K 83K 52.0K 51.9K 52.3K 
Table 3: Training data sizes for various selection methods to 
achieve the same performance level as the supervised learning 
 
4.4 Effectiveness of Two Sample Selection 
Strategies 
In addition to the informativeness criterion, we 
further incorporate representativeness and diversity 
criteria into active learning using two strategies 
described in Section 3.  Comparing the two strate-
gies with the best result of the single-criterion-
based selection methods Info_Min , we are to jus-
tify that representativeness and diversity are also 
important factors for active learning.  Figure 6 
shows the learning curves for the various methods: 
Strategy1, Strategy2 and Info_Min.  In the begin-
ning iterations (F-measure < 60), the three methods 
performed similarly.  But with the larger training 
set, the efficiencies of Stratety1 and Strategy2 be-
gin to be evident.  Table 4 highlights the final re-
sult of the three methods.  In order to reach the 
performance of supervised learning, Strategy1 
(40K words) and Strategyy2 (31K words) require 
about 80% and 60% of the data that Info_Min 
(51.9K) does.  So we believe the effective combi-
nations of informativeness, representativeness and 
diversity will help to learn the NER model more 
quickly and cost less in annotation. 
0.5
0.55
0.6
0.65
0 20 40 60 K words
F
Supervised
Info_Min
Strategy1
Strategy2
 
Figure 6: Active learning curves: effectiveness of the two 
multi-criteria-based selection strategies comparing with the 
informativeness-criterion-based selection (Info_Min). 
Info_Min Strategy1 Strategy2 
51.9K 40K 31K 
Table 4: Comparisons of training data sizes for the multi-
criteria-based selection strategies and the informativeness-
criterion-based selection (Info_Min) to achieve the same per-
formance level as the supervised learning. 
 
5 Related Work 
Since there is no study on active learning for NER 
task previously, we only introduce general active 
learning methods here.  Many existing active learn-
ing methods are to select the most uncertain exam-
ples using various measures (Thompson et al 1999; 
Schohn and Cohn 2000; Tong and Koller 2000; 
Engelson and Dagan 1999; Ngai and Yarowsky 
2000).  Our informativeness-based measure is 
similar to these works.  However these works just 
follow a single criterion.  (McCallum and Nigam 
1998; Tang et al 2002) are the only two works 
considering the representativeness criterion in ac-
tive learning.  (Tang et al 2002) use the density 
information to weight the selected examples while 
we use it to select examples.  Moreover, the repre-
sentativeness measure we use is relatively general 
and easy to adapt to other tasks, in which the ex-
ample selected is a sequence of words, such as text 
chunking, POS tagging, etc.  On the other hand, 
(Brinker 2003) first incorporate diversity in active 
learning for text classification.  Their work is simi-
lar to our local consideration in Section 2.3.2.  
However, he didn?t further explore how to avoid 
selecting outliers to a batch.  So far, we haven?t 
found any previous work integrating the informa-
tiveness, representativeness and diversity all to-
gether. 
 
6 Conclusion and Future Work 
In this paper, we study the active learning in a 
more complex NLP task, named entity recognition.  
We propose a multi-criteria -based approach to se-
lect examples based on their informativeness, rep-
resentativeness and diversity, which are 
incorporated all together by two strategies (local 
and global).  Experiments show that, in both MUC-
6 and GENIA, both of the two strategies combin-
ing the three criteria outperform the single criterion 
(informativeness).  The labeling cost can be sig-
nificantly reduced by at least 80% comparing with 
the supervised learning.  To our best knowledge, 
this is not only the first work to report the empiri-
cal results of active learning for NER, but also the 
first work to incorporate the three criteria all to-
gether for selecting examples. 
Although the current experiment results are 
very promising, some parameters in our experi-
ment, such as the batch size K and the l in the 
function of strategy 2, are decided by our experi-
ence in the domain.  In practical application, the 
optimal value of these parameters should be de-
cided automatically based on the training process.  
Furthermore, we will study how to overcome the 
limitation of the strategy 1 discussed in Section 3 
by using more effective clustering algorithm.  An-
other interesting work is to study when to stop ac-
tive learning.  
 
References 
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Mod-
ern Information Retrieval. ISBN 0-201-39829-X. 
K. Brinker. 2003. Incorporating Diversity in Ac-
tive Learning with Support Vector Machines. In 
Proceedings of ICML, 2003. 
S. A. Engelson and I. Dagan. 1999. Committee-
Based Sample Selection for Probabilistic Classi-
fiers. Journal of Artifical Intelligence Research. 
F. Jelinek. 1997. Statistical Methods for Speech 
Recognition. MIT Press. 
J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002. 
Tuning Support Vector Machines for Biomedi-
cal Named Entity Recognition. In Proceedings 
of the ACL2002 Workshop on NLP in Biomedi-
cine. 
K. J. Lee, Y. S. Hwang and H. C. Rim. 2003. Two-
Phase Biomedical NE Recognition based on 
SVMs.  In Proceedings of the ACL2003 Work-
shop on NLP in Biomedicine. 
D. D. Lewis and J. Catlett. 1994. Heterogeneous 
Uncertainty Sampling for Supervised Learning. 
In Proceedings of ICML, 1994. 
A. McCallum and K. Nigam. 1998. Employing EM 
in Pool-Based Active Learning for Text Classi-
fication. In Proceedings of ICML, 1998. 
G. Ngai and D. Yarowsky. 2000. Rule Writing or 
Annotation: Cost-efficient Resource Usage for 
Base Noun Phrase Chunking. In Proceedings of 
ACL, 2000. 
T. Ohta, Y. Tateisi, J. Kim, H. Mima and J. Tsujii. 
2002. The GENIA corpus: An annotated re-
search abstract corpus in molecular biology do-
main. In Proceedings of HLT 2002. 
L. R. Rabiner, A. E. Rosenberg and S. E. Levinson. 
1978. Considerations in Dynamic Time Warping 
Algorithms for Discrete Word Recognition.  In 
Proceedings of IEEE Transactions on acoustics, 
speech and signal processing. Vol. ASSP-26, 
NO.6. 
D. Schohn and D. Cohn. 2000. Less is More: Ac-
tive Learning with Support Vector Machines. In 
Proceedings of the 17th International Confer-
ence on Machine Learning. 
D. Shen, J. Zhang, G. D. Zhou, J. Su and C. L. Tan. 
2003. Effective Adaptation of a Hidden Markov 
Model-based Named Entity Recognizer for Bio-
medical Domain. In Proceedings of the 
ACL2003 Workshop on NLP in Biomedicine. 
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. 
Sarkar, J. Hockenmaier, P. Ruhlen, S. Baker and 
J. Crim. 2003. Example Selection for Bootstrap-
ping Statistical Parsers. In Proceedings of HLT-
NAACL, 2003. 
M. Tang, X. Luo and S. Roukos. 2002. Active 
Learning for Statistical Natural Language Pars-
ing. In Proceedings of the ACL 2002. 
C. A. Thompson, M. E. Califf and R. J. Mooney. 
1999. Active Learning for Natural Language 
Parsing and Information Extraction. In Proceed-
ings of ICML 1999. 
S. Tong and D. Koller. 2000. Support Vector Ma-
chine Active Learning with Applications to Text 
Classification. Journal of Machine Learning Re-
search. 
V. Vapnik. 1998. Statistical learning theory. 
N.Y.:John Wiley. 
 
Proceedings of the 43rd Annual Meeting of the ACL, pages 427?434,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Exploring Various Knowledge in Relation Extraction 
 
ZHOU GuoDong   SU Jian  ZHANG Jie  ZHANG Min  
Institute for Infocomm research  
21 Heng Mui Keng Terrace, Singapore 119613  
Email: {zhougd, sujian, zhangjie, mzhang}@i2r.a-star.edu.sg  
  
Abstract 
Extracting semantic relationships between en-
tities is challenging. This paper investigates 
the incorporation of diverse lexical, syntactic 
and semantic knowledge in feature-based rela-
tion extraction using SVM. Our study illus-
trates that the base phrase chunking 
information is very effective for relation ex-
traction and contributes to most of the per-
formance improvement from syntactic aspect 
while additional information from full parsing 
gives limited further enhancement. This sug-
gests that most of useful information in full 
parse trees for relation extraction is shallow 
and can be captured by chunking. We also 
demonstrate how semantic information such as 
WordNet and Name List, can be used in fea-
ture-based relation extraction to further im-
prove the performance. Evaluation on the 
ACE corpus shows that effective incorporation 
of diverse features enables our system outper-
form previously best-reported systems on the 
24 ACE relation subtypes and significantly 
outperforms tree kernel-based systems by over 
20 in F-measure on the 5 ACE relation types. 
1 Introduction 
With the dramatic increase in the amount of textual 
information available in digital archives and the 
WWW, there has been growing interest in tech-
niques for automatically extracting information 
from text. Information Extraction (IE) systems are 
expected to identify relevant information (usually 
of pre-defined types) from text documents in a cer-
tain domain and put them in a structured format.  
According to the scope of the NIST Automatic 
Content Extraction (ACE) program, current 
research in IE has three main objectives: Entity 
Detection and Tracking (EDT), Relation Detection 
and Characterization (RDC), and Event Detection 
and Characterization (EDC). The EDT task entails 
the detection of entity mentions and chaining them 
together by identifying their coreference. In ACE 
vocabulary, entities are objects, mentions are 
references to them, and relations are semantic 
relationships between entities. Entities can be of 
five types: persons, organizations, locations, 
facilities and geo-political entities (GPE: 
geographically defined regions that indicate a 
political boundary, e.g. countries, states, cities, 
etc.). Mentions have three levels: names, nomial 
expressions or pronouns. The RDC task detects 
and classifies implicit and explicit relations1 
between entities identified by the EDT task. For 
example, we want to determine whether a person is 
at a location, based on the evidence in the context. 
Extraction of semantic relationships between 
entities can be very useful for applications such as 
question answering, e.g. to answer the query ?Who 
is the president of the United States??.  
This paper focuses on the ACE RDC task and 
employs diverse lexical, syntactic and semantic 
knowledge in feature-based relation extraction 
using Support Vector Machines (SVMs). Our 
study illustrates that the base phrase chunking 
information contributes to most of the performance 
inprovement from syntactic aspect while additional 
full parsing information does not contribute much, 
largely due to the fact that most of relations 
defined in ACE corpus are within a very short 
distance. We also demonstrate how semantic in-
formation such as WordNet (Miller 1990) and 
Name List can be used in the feature-based frame-
work. Evaluation shows that the incorporation of 
diverse features enables our system achieve best 
reported performance. It also shows that our fea-
                                                          
1 In ACE (http://www.ldc.upenn.edu/Projects/ACE), 
explicit relations occur in text with explicit evidence 
suggesting the relationships. Implicit relations need not 
have explicit supporting evidence in text, though they 
should be evident from a reading of the document.  
427
ture-based approach outperforms tree kernel-based 
approaches by 11 F-measure in relation detection 
and more than 20 F-measure in relation detection 
and classification on the 5 ACE relation types.  
The rest of this paper is organized as follows. 
Section 2 presents related work. Section 3 and 
Section 4 describe our approach and various 
features employed respectively. Finally, we present 
experimental setting and  results in Section 5 and 
conclude with some general observations in 
relation extraction in Section 6. 
2 Related Work 
The relation extraction task was formulated at the 
7th Message Understanding Conference (MUC-7 
1998) and is starting to be addressed more and 
more within the natural language processing and 
machine learning communities.  
Miller et al(2000) augmented syntactic full 
parse trees with semantic information correspond-
ing to entities and relations, and built generative 
models for the augmented trees. Zelenko et al
(2003) proposed extracting relations by computing 
kernel functions between parse trees. Culotta et al
(2004) extended this work to estimate kernel func-
tions between augmented dependency trees and 
achieved 63.2 F-measure in relation detection and 
45.8 F-measure in relation detection and classifica-
tion on the 5 ACE relation types. Kambhatla 
(2004) employed Maximum Entropy models for 
relation extraction with features derived from 
word, entity type, mention level, overlap, depend-
ency tree and parse tree. It achieves 52.8 F-
measure on the 24 ACE relation subtypes. Zhang 
(2004) approached relation classification by com-
bining various lexical and syntactic features with 
bootstrapping on top of Support Vector Machines. 
Tree kernel-based approaches proposed by Ze-
lenko et al(2003) and Culotta et al(2004) are able 
to explore the implicit feature space without much 
feature engineering. Yet further research work is 
still expected to make it effective with complicated 
relation extraction tasks such as the one defined in 
ACE. Complicated relation extraction tasks may 
also impose a big challenge to the modeling ap-
proach used by Miller et al(2000) which integrates 
various tasks such as part-of-speech tagging, 
named entity recognition, template element extrac-
tion and relation extraction, in a single model.   
This paper will further explore the feature-based 
approach with a systematic study on the extensive 
incorporation of diverse lexical, syntactic and se-
mantic information. Compared with Kambhatla 
(2004), we separately incorporate the base phrase 
chunking information, which contributes to most 
of the performance improvement from syntactic 
aspect. We also show how semantic information 
like WordNet and Name List can be equipped to 
further improve the performance. Evaluation on 
the ACE corpus shows that our system outper-
forms Kambhatla (2004) by about 3 F-measure on 
extracting 24 ACE relation subtypes. It also shows 
that our system outperforms tree kernel-based sys-
tems (Culotta et al2004) by over 20 F-measure on 
extracting 5 ACE relation types. 
3 Support Vector Machines 
Support Vector Machines (SVMs) are a supervised 
machine learning technique motivated by the sta-
tistical learning theory (Vapnik 1998). Based on 
the structural risk minimization of the statistical 
learning theory, SVMs seek an optimal separating 
hyper-plane to divide the training examples into 
two classes and make decisions based on support 
vectors which are selected as the only effective 
instances in the training set. 
Basically, SVMs are binary classifiers. 
Therefore, we must extend SVMs to multi-class 
(e.g. K) such as the ACE RDC task. For efficiency, 
we apply the one vs. others strategy, which builds 
K classifiers so as to separate one class from all 
others, instead of the pairwise strategy, which 
builds K*(K-1)/2 classifiers considering all pairs of 
classes. The final decision of an instance in the 
multiple binary classification is determined by the 
class which has the maximal SVM output. 
Moreover, we only apply the simple linear kernel, 
although other kernels can peform better.  
The reason why we choose SVMs for this 
purpose is that SVMs represent the state-of?the-art 
in  the machine learning research community, and 
there are good implementations of the algorithm 
available. In this paper, we use the binary-class 
SVMLight2 deleveloped by Joachims (1998). 
                                                          
2 Joachims has just released a new version of SVMLight 
for multi-class classification. However, this paper only 
uses the binary-class version. For details about 
SVMLight, please see http://svmlight.joachims.org/ 
428
4 Features 
The semantic relation is determined between two 
mentions. In addition, we distinguish the argument 
order of the two mentions (M1 for the first mention 
and M2 for the second mention), e.g. M1-Parent-
Of-M2 vs. M2-Parent-Of-M1. For each pair of 
mentions3, we compute various lexical, syntactic 
and semantic features. 
4.1 Words 
According to their positions, four categories of 
words are considered: 1) the words of both the 
mentions, 2) the words between the two mentions, 
3) the words before M1, and 4) the words after M2. 
For the words of both the mentions, we also differ-
entiate the head word4 of a mention from other 
words since the head word is generally much more 
important. The words between the two mentions 
are classified into three bins: the first word in be-
tween, the last word in between and other words in 
between. Both the words before M1 and after M2 
are classified into two bins: the first word next to 
the mention and the second word next to the men-
tion. Since a pronominal mention (especially neu-
tral pronoun such as ?it? and ?its?) contains little 
information about the sense of the mention, the co-
reference chain is used to decide its sense. This is 
done by replacing the pronominal mention with the 
most recent non-pronominal antecedent when de-
termining the word features, which include: 
? WM1: bag-of-words in M1 
? HM1: head word of M1 
                                                          
3 In ACE, each mention has a head annotation and an 
extent annotation. In all our experimentation, we only 
consider the word string between the beginning point of 
the extent annotation and the end point of the head an-
notation. This has an effect of choosing the base phrase 
contained in the extent annotation. In addition, this also 
can reduce noises without losing much of information in 
the mention. For example, in the case where the noun 
phrase ?the former CEO of McDonald? has the head 
annotation of ?CEO? and the extent annotation of ?the 
former CEO of McDonald?, we only consider ?the for-
mer CEO? in this paper. 
4 In this paper, the head word of a mention is normally 
set as the last word of the mention. However, when a 
preposition exists in the mention, its head word is set as 
the last word before the preposition. For example, the 
head word of the name mention ?University of Michi-
gan? is ?University?. 
? WM2: bag-of-words in M2 
? HM2: head word of M2 
? HM12: combination of HM1 and HM2 
? WBNULL: when no word in between 
? WBFL: the only word in between when only 
one word in between 
? WBF: first word in between when at least two 
words in between 
? WBL: last word in between when at least two 
words in between 
? WBO: other words in between except first and 
last words when at least three words in between 
? BM1F: first word before M1 
? BM1L: second word before M1 
? AM2F: first word after M2 
? AM2L: second word after M2 
4.2 Entity Type 
This feature concerns about the entity type of both 
the mentions, which can be PERSON, 
ORGANIZATION, FACILITY, LOCATION and 
Geo-Political Entity or GPE: 
? ET12: combination of mention entity types 
4.3 Mention Level 
This feature considers the entity level of both the 
mentions, which can be NAME, NOMIAL and 
PRONOUN: 
? ML12: combination of mention levels 
4.4 Overlap 
This category of features includes: 
? #MB: number of other mentions in between 
? #WB: number of words in between 
? M1>M2 or M1<M2: flag indicating whether 
M2/M1is included in M1/M2.  
Normally, the above overlap features are too 
general to be effective alone. Therefore, they are 
also combined with other features: 1) 
ET12+M1>M2; 2) ET12+M1<M2; 3) 
HM12+M1>M2; 4) HM12+M1<M2. 
4.5 Base Phrase Chunking 
It is well known that chunking plays a critical role 
in the Template Relation task of the 7th Message 
Understanding Conference (MUC-7 1998). The 
related work mentioned in Section 2 extended to 
explore the information embedded in the full parse 
trees. In this paper, we separate the features of base 
429
phrase chunking from those of full parsing. In this 
way, we can separately evaluate the contributions 
of base phrase chunking and full parsing. Here, the 
base phrase chunks are derived from full parse 
trees using the Perl script5 written by Sabine 
Buchholz from Tilburg University and the Collins? 
parser (Collins 1999) is employed for full parsing. 
Most of the chunking features concern about the 
head words of the phrases between the two men-
tions. Similar to word features, three categories of 
phrase heads are considered: 1) the phrase heads in 
between are also classified into three bins: the first 
phrase head in between, the last phrase head in 
between and other phrase heads in between; 2) the 
phrase heads before M1 are classified into two 
bins: the first phrase head before and the second 
phrase head before; 3) the phrase heads after M2 
are classified into two bins: the first phrase head 
after and the second phrase head after. Moreover, 
we also consider the phrase path in between. 
? CPHBNULL when no phrase in between 
? CPHBFL: the only phrase head when only one 
phrase in between 
? CPHBF: first phrase head in between when at 
least two phrases in between 
? CPHBL: last phrase head in between when at 
least two phrase heads in between 
? CPHBO: other phrase heads in between except 
first and last phrase heads when at least three 
phrases in between 
? CPHBM1F: first phrase head before M1 
? CPHBM1L: second phrase head before M1 
? CPHAM2F: first phrase head after M2 
? CPHAM2F: second phrase head after M2 
? CPP: path of phrase labels connecting the two 
mentions in the chunking  
? CPPH: path of phrase labels connecting the two 
mentions in the chunking augmented with head 
words, if at most two phrases in between 
4.6 Dependency Tree 
This category of features includes information 
about the words, part-of-speeches and phrase la-
bels of the words on which the mentions are de-
pendent in the dependency tree derived from the 
syntactic full parse tree. The dependency tree is 
built by using the phrase head information returned 
by the Collins? parser and linking all the other 
                                                          
5 http://ilk.kub.nl/~sabine/chunklink/ 
fragments in a phrase to its head. It also includes 
flags indicating whether the two mentions are in 
the same NP/PP/VP. 
? ET1DW1: combination of the entity type and 
the dependent word for M1 
? H1DW1: combination of the head word and the 
dependent word for M1 
? ET2DW2: combination of the entity type and 
the dependent word for M2 
? H2DW2: combination of the head word and the 
dependent word for M2 
? ET12SameNP: combination of ET12 and 
whether M1 and M2 included in the same NP 
? ET12SamePP: combination of ET12 and 
whether M1 and M2 exist in the same PP 
? ET12SameVP: combination of ET12 and 
whether M1 and M2 included in the same VP 
4.7 Parse Tree 
This category of features concerns about the in-
formation inherent only in the full parse tree.  
? PTP: path of phrase labels (removing dupli-
cates) connecting M1 and M2 in the parse tree  
? PTPH: path of phrase labels (removing dupli-
cates) connecting M1 and M2 in the parse tree 
augmented with the head word of the top phrase 
in the path.  
4.8 Semantic Resources 
Semantic information from various resources, such 
as WordNet, is used to classify important words 
into different semantic lists according to their indi-
cating relationships. 
Country Name List 
This is to differentiate the relation subtype 
?ROLE.Citizen-Of?, which defines the relationship 
between a person and the country of the person?s 
citizenship, from other subtypes, especially 
?ROLE.Residence?, where defines the relationship 
between a person and the location in which the 
person lives. Two features are defined to include 
this information: 
? ET1Country: the entity type of M1 when M2 is 
a country name 
? CountryET2: the entity type of M2 when M1 is 
a country name 
 
 
430
Personal Relative Trigger Word List 
This is used to differentiate the six personal social 
relation subtypes in ACE: Parent, Grandparent, 
Spouse, Sibling, Other-Relative and Other-
Personal. This trigger word list is first gathered 
from WordNet by checking whether a word has the 
semantic class ?person|?|relative?. Then, all the 
trigger words are semi-automatically6 classified 
into different categories according to their related 
personal social relation subtypes. We also extend 
the list by collecting the trigger words from the 
head words of the mentions in the training data 
according to their indicating relationships. Two 
features are defined to include this information: 
? ET1SC2: combination of the entity type of M1 
and the semantic class of M2 when M2 triggers 
a personal social subtype. 
? SC1ET2: combination of the entity type of M2 
and the semantic class of M1 when the first 
mention triggers a personal social subtype. 
5 Experimentation 
This paper uses the ACE corpus provided by LDC 
to train and evaluate our feature-based relation ex-
traction system. The ACE corpus is gathered from 
various newspapers, newswire and broadcasts. In 
this paper, we only model explicit relations be-
cause of poor inter-annotator agreement in the an-
notation of implicit relations and their limited 
number. 
5.1 Experimental Setting 
We use the official ACE corpus from LDC. The 
training set consists of 674 annotated text docu-
ments (~300k words) and 9683 instances of rela-
tions. During development, 155 of 674 documents 
in the training set are set aside for fine-tuning the 
system. The testing set is held out only for final 
evaluation. It consists of 97 documents (~50k 
words) and 1386 instances of relations. Table 1 
lists the types and subtypes of relations for the 
ACE Relation Detection and Characterization 
(RDC) task, along with their frequency of occur-
rence in the ACE training set. It shows that the 
                                                          
6 Those words that have the semantic classes ?Parent?, 
?GrandParent?, ?Spouse? and ?Sibling? are automati-
cally set with the same classes without change. How-
ever, The remaining words that do not have above four 
classes are manually classified. 
ACE corpus suffers from a small amount of anno-
tated data for a few subtypes such as the subtype 
?Founder? under the type ?ROLE?. It also shows 
that the ACE RDC task defines some difficult sub-
types such as the subtypes ?Based-In?, ?Located? 
and ?Residence? under the type ?AT?, which are 
difficult even for human experts to differentiate.  
Type Subtype Freq 
AT(2781) Based-In 347 
 Located 2126 
 Residence 308 
NEAR(201) Relative-Location 201 
PART(1298) Part-Of 947 
 Subsidiary 355 
 Other 6 
ROLE(4756) Affiliate-Partner 204 
 Citizen-Of 328 
 Client 144 
 Founder 26 
 General-Staff 1331 
 Management 1242 
 Member 1091 
 Owner 232 
 Other 158 
SOCIAL(827) Associate 91 
 Grandparent 12 
 Other-Personal 85 
 Other-Professional 339 
 Other-Relative 78 
 Parent 127 
 Sibling 18 
 Spouse 77 
Table 1: Relation types and subtypes in the ACE 
training data 
In this paper, we explicitly model the argument 
order of the two mentions involved. For example, 
when comparing mentions m1 and m2, we distin-
guish between m1-ROLE.Citizen-Of-m2 and m2-
ROLE.Citizen-Of-m1. Note that only 6 of these 24 
relation subtypes are symmetric: ?Relative-
Location?, ?Associate?, ?Other-Relative?, ?Other-
Professional?, ?Sibling?, and ?Spouse?. In this 
way, we model relation extraction as a multi-class 
classification problem with 43 classes, two for 
each relation subtype (except the above 6 symmet-
ric subtypes) and a ?NONE? class for the case 
where the two mentions are not related. 
5.2 Experimental Results 
In this paper, we only measure the performance of 
relation extraction on ?true? mentions with ?true? 
chaining of coreference (i.e. as annotated by the 
corpus annotators) in the ACE corpus. Table 2 
measures the performance of our relation extrac-
431
tion system over the 43 ACE relation subtypes on 
the testing set. It shows that our system achieves 
best performance of 63.1%/49.5%/ 55.5 in preci-
sion/recall/F-measure when combining diverse 
lexical, syntactic and semantic features. Table 2 
also measures the contributions of different fea-
tures by gradually increasing the feature set. It 
shows that: 
Features P R F 
Words 69.2 23.7 35.3 
+Entity Type 67.1 32.1 43.4 
+Mention Level 67.1 33.0 44.2 
+Overlap 57.4 40.9 47.8 
+Chunking 61.5 46.5 53.0 
+Dependency Tree 62.1 47.2 53.6 
+Parse Tree 62.3 47.6 54.0 
+Semantic Resources 63.1 49.5 55.5 
Table 2: Contribution of different features over 43 
relation subtypes in the test data 
? Using word features only achieves the perform-
ance of 69.2%/23.7%/35.3 in precision/recall/F-
measure.  
? Entity type features are very useful and improve 
the F-measure by 8.1 largely due to the recall 
increase. 
? The usefulness of mention level features is quite 
limited. It only improves the F-measure by 0.8 
due to the recall increase. 
? Incorporating the overlap features gives some 
balance between precision and recall. It in-
creases the F-measure by 3.6 with a big preci-
sion decrease and a big recall increase. 
? Chunking features are very useful. It increases 
the precision/recall/F-measure by 4.1%/5.6%/ 
5.2 respectively. 
? To our surprise, incorporating the dependency 
tree and parse tree features only improve the F-
measure by 0.6 and 0.4 respectively. This may 
be due to the fact that most of relations in the 
ACE corpus are quite local. Table 3 shows that 
about 70% of relations exist where two men-
tions are embedded in each other or separated 
by at most one word. While short-distance rela-
tions dominate and can be resolved by above 
simple features, the dependency tree and parse 
tree features can only take effect in the remain-
ing much less long-distance relations. However, 
full parsing is always prone to long distance er-
rors although the Collins? parser used in our 
system represents the state-of-the-art in full 
parsing. 
? Incorporating semantic resources such as the 
country name list and the personal relative trig-
ger word list further increases the F-measure by 
1.5 largely due to the differentiation of the rela-
tion subtype ?ROLE.Citizen-Of? from ?ROLE. 
Residence? by distinguishing country GPEs 
from other GPEs. The effect of personal relative 
trigger words is very limited due to the limited 
number of testing instances over personal social 
relation subtypes. 
Table 4 separately measures the performance of 
different relation types and major subtypes. It also 
indicates the number of testing instances, the num-
ber of correctly classified instances and the number 
of wrongly classified instances for each type or 
subtype. It is not surprising that the performance 
on the relation type ?NEAR? is low because it oc-
curs rarely in both the training and testing data. 
Others like ?PART.Subsidary? and ?SOCIAL. 
Other-Professional? also suffer from their low oc-
currences. It also shows that our system performs 
best on the subtype ?SOCIAL.Parent? and ?ROLE. 
Citizen-Of?. This is largely due to incorporation of 
two semantic resources, i.e. the country name list 
and the personal relative trigger word list. Table 4 
also indicates the low performance on the relation 
type ?AT? although it frequently occurs in both the 
training and testing data. This suggests the diffi-
culty of detecting and classifying the relation type 
?AT? and its subtypes. 
Table 5 separates the performance of relation 
detection from overall performance on the testing 
set. It shows that our system achieves the perform-
ance of 84.8%/66.7%/74.7 in precision/recall/F-
measure on relation detection. It also shows that 
our system achieves overall performance of 
77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in preci-
sion/recall/F-measure on the 5 ACE relation types 
and the best-reported systems on the ACE corpus. 
It shows that our system achieves better perform-
ance by ~3 F-measure largely due to its gain in 
recall. It also shows that feature-based methods 
dramatically outperform kernel methods. This sug-
gests that feature-based methods can effectively 
combine different features from a variety of 
sources (e.g. WordNet and gazetteers) that can be 
brought to bear on relation extraction. The tree 
kernels developed in Culotta et al(2004) are yet to 
be effective on the ACE RDC task. 
Finally, Table 6 shows the distributions of er-
rors. It shows that 73% (627/864) of errors results 
432
from relation detection and 27% (237/864) of er-
rors results from relation characterization, among 
which 17.8% (154/864) of errors are from misclas-
sification across relation types and 9.6% (83/864) 
of errors are from misclassification of relation sub-
types inside the same relation types. This suggests 
that relation detection is critical for relation extrac-
tion. 
# of other mentions in between # of relations 
0 1 2 3 >=4 Overall 
0 3991 161 11 0 0 4163 
1 2350 315 26 2 0 2693 
2 465 95 7 2 0 569 
3 311 234 14 0 0 559 
4 204 225 29 2 3 463 
5 111 113 38 2 1 265 
>=6 262 297 277 148 134 1118 
#  
of  
the words 
 in  
between 
Overall 7694 1440 402 156 138 9830 
Table 3: Distribution of relations over #words and #other mentions in between in the training data 
Type Subtype #Testing Instances #Correct #Error P R F 
AT  392 224 105 68.1 57.1 62.1 
 Based-In 85 39 10 79.6 45.9 58.2 
 Located 241 132 120 52.4 54.8 53.5 
 Residence 66 19 9 67.9 28.8 40.4 
NEAR  35 8 1 88.9 22.9 36.4 
 Relative-Location 35 8 1 88.9 22.9 36.4 
PART  164 106 39 73.1 64.6 68.6 
 Part-Of 136 76 32 70.4 55.9 62.3 
 Subsidiary 27 14 23 37.8 51.9 43.8 
ROLE  699 443 82 84.4 63.4 72.4 
 Citizen-Of 36 25 8 75.8 69.4 72.6 
 General-Staff 201 108 46 71.1 53.7 62.3 
 Management 165 106 72 59.6 64.2 61.8 
 Member 224 104 36 74.3 46.4 57.1 
SOCIAL  95 60 21 74.1 63.2 68.5 
 Other-Professional 29 16 32 33.3 55.2 41.6 
 Parent 25 17 0 100 68.0 81.0 
Table 4: Performance of different relation types and major subtypes in the test data 
Relation Detection RDC on Types RDC on Subtypes System 
P R F P R F P R F 
Ours: feature-based 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5 
Kambhatla (2004):feature-based - - - - - - 63.5 45.2 52.8 
Culotta et al(2004):tree kernel 81.2 51.8 63.2 67.1 35.0 45.8 - - - 
Table 5: Comparison of our system with other best-reported systems on the ACE corpus 
Error Type #Errors 
False Negative 462 Detection Error 
False Positive 165 
Cross Type Error 154 Characterization  
Error Inside Type Error 83 
Table 6: Distribution of errors 
6 Discussion and Conclusion 
In this paper, we have presented a feature-based 
approach for relation extraction where diverse 
lexical, syntactic and semantic knowledge are em-
ployed. Instead of exploring the full parse tree in-
formation directly as previous related work, we 
incorporate the base phrase chunking information 
first. Evaluation on the ACE corpus shows that 
base phrase chunking contributes to most of the 
performance improvement from syntactic aspect 
while further incorporation of the parse tree and 
dependence tree information only slightly im-
proves the performance. This may be due to three 
reasons: First, most of relations defined in ACE 
have two mentions being close to each other. 
While short-distance relations dominate and can be 
resolved by simple features such as word and 
chunking features, the further dependency tree and 
parse tree features can only take effect in the re-
maining much less and more difficult long-distance 
relations. Second, it is well known that full parsing 
433
is always prone to long-distance parsing errors al-
though the Collins? parser used in our system 
achieves the state-of-the-art performance. There-
fore, the state-of-art full parsing still needs to be 
further enhanced to provide accurate enough in-
formation, especially PP (Preposition Phrase) at-
tachment. Last, effective ways need to be explored 
to incorporate information embedded in the full 
parse trees. Besides, we also demonstrate how se-
mantic information such as WordNet and Name 
List, can be used in feature-based relation extrac-
tion to further improve the performance. 
The effective incorporation of diverse features 
enables our system outperform previously best-
reported systems on the ACE corpus. Although 
tree kernel-based approaches facilitate the explora-
tion of the implicit feature space with the parse tree 
structure, yet the current technologies are expected 
to be further advanced to be effective for relatively 
complicated relation extraction tasks such as the 
one defined in ACE where 5 types and 24 subtypes 
need to be extracted. Evaluation on the ACE RDC 
task shows that our approach of combining various 
kinds of evidence can scale better to problems, 
where we have a lot of relation types with a rela-
tively small amount of annotated data. The ex-
periment result also shows that our feature-based 
approach outperforms the tree kernel-based ap-
proaches by more than 20 F-measure on the extrac-
tion of 5 ACE relation types.  
In the future work, we will focus on exploring 
more semantic knowledge in relation extraction, 
which has not been covered by current research. 
Moreover, our current work is done when the En-
tity Detection and Tracking (EDT) has been per-
fectly done. Therefore, it would be interesting to 
see how imperfect EDT affects the performance in 
relation extraction. 
References  
Agichtein E. and Gravano L. (2000). Snowball: Extract-
ing relations from large plain text collections. In Pro-
ceedings of 5th ACM International Conference on 
Digital Libraries. 4-7 June 2000. San Antonio, TX. 
Brin S. (1998). Extracting patterns and relations from 
the World Wide Web. In Proceedings of WebDB 
workshop at 6th International Conference on Extend-
ing DataBase Technology (EDBT?1998).23-27 
March 1998, Valencia, Spain 
Collins M. (1999).  Head-driven statistical models for 
natural language parsing. Ph.D. Dissertation, Univer-
sity of Pennsylvania. 
Collins M. and Duffy N. (2002). Covolution kernels for 
natural language. In Dietterich T.G., Becker S. and 
Ghahramani Z. editors. Advances in Neural Informa-
tion Processing Systems 14. Cambridge, MA.  
Culotta A. and Sorensen J. (2004). Dependency tree 
kernels for relation extraction. In Proceedings of 42th 
Annual Meeting of the Association for Computational 
Linguistics. 21-26 July 2004. Barcelona, Spain 
Cumby C.M. and Roth D. (2003). On kernel methods 
for relation learning. In Fawcett T. and Mishra N. 
editors. In Proceedings of 20th International Confer-
ence on Machine Learning (ICML?2003). 21-24 Aug 
2003. Washington D.C. USA. AAAI Press. 
Haussler D. (1999). Covention kernels on discrete struc-
tures. Technical Report UCS-CRL-99-10. University 
of California, Santa Cruz. 
Joachims T. (1998). Text categorization with Support 
Vector Machines: Learning with many relevant fea-
tures. In Proceedings of European Conference on 
Machine Learning(ECML?1998).  21-23 April 1998. 
Chemnitz, Germany 
Miller G.A. (1990). WordNet: An online lexical data-
base. International Journal of Lexicography. 
3(4):235-312. 
Miller S., Fox H., Ramshaw L. and Weischedel R. 
(2000). A novel use of statistical parsing to extract 
information from text. In Proceedings of 6th Applied 
Natural Language Processing Conference. 29 April  
- 4 May 2000, Seattle, USA 
MUC-7. (1998). Proceedings of the 7th Message Under-
standing Conference (MUC-7). Morgan Kaufmann, 
San Mateo, CA. 
Kambhatla N. (2004). Combining lexical, syntactic and 
semantic features with Maximum Entropy models for 
extracting relations. In Proceedings of 42th Annual 
Meeting of the Association for Computational Lin-
guistics. 21-26 July 2004. Barcelona, Spain. 
Roth D. and Yih W.T. (2002). Probabilistic reasoning 
for entities and relation recognition. In Proceedings 
of 19th International Conference on Computational 
Linguistics(CoLING?2002). Taiwan. 
Vapnik V. (1998). Statistical Learning Theory. Whiley, 
Chichester, GB. 
Zelenko D., Aone C. and Richardella. (2003). Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. pp1083-1106. 
Zhang Z. (2004). Weekly-supervised relation classifica-
tion for Information Extraction. In Proceedings of 
ACM 13th Conference on Information and Knowl-
edge Management (CIKM?2004). 8-13 Nov 2004. 
Washington D.C., USA. 
434
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 121?128,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Modeling Commonality among Related Classes in Relation Extraction 
 
Zhou GuoDong      Su Jian      Zhang Min 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, Singapore 119613 
Email: {zhougd, sujian, mzhang}@i2r.a-star.edu.sg 
 
Abstract 
This paper proposes a novel hierarchical learn-
ing strategy to deal with the data sparseness 
problem in relation extraction by modeling the 
commonality among related classes. For each 
class in the hierarchy either manually prede-
fined or automatically clustered, a linear dis-
criminative function is determined in a top-
down way using a perceptron algorithm with 
the lower-level weight vector derived from the 
upper-level weight vector. As the upper-level 
class normally has much more positive train-
ing examples than the lower-level class, the 
corresponding linear discriminative function 
can be determined more reliably. The upper-
level discriminative function then can effec-
tively guide the discriminative function learn-
ing in the lower-level, which otherwise might 
suffer from limited training data. Evaluation 
on the ACE RDC 2003 corpus shows that the 
hierarchical strategy much improves the per-
formance by 5.6 and 5.1 in F-measure on 
least- and medium- frequent relations respec-
tively. It also shows that our system outper-
forms the previous best-reported system by 2.7 
in F-measure on the 24 subtypes using the 
same feature set. 
1 Introduction 
With the dramatic increase in the amount of tex-
tual information available in digital archives and 
the WWW, there has been growing interest in 
techniques for automatically extracting informa-
tion from text. Information Extraction (IE) is 
such a technology that IE systems are expected 
to identify relevant information (usually of pre-
defined types) from text documents in a certain 
domain and put them in a structured format. 
According to the scope of the ACE program 
(ACE 2000-2005), current research in IE has 
three main objectives: Entity Detection and 
Tracking (EDT), Relation Detection and 
Characterization (RDC), and Event Detection 
and Characterization (EDC). This paper will 
focus on the ACE RDC task, which detects and 
classifies various semantic relations between two 
entities. For example, we want to determine 
whether a person is at a location, based on the 
evidence in the context. Extraction of semantic 
relationships between entities can be very useful 
for applications such as question answering, e.g. 
to answer the query ?Who is the president of the 
United States??.  
One major challenge in relation extraction is 
due to the data sparseness problem (Zhou et al
2005). As the largest annotated corpus in relation 
extraction, the ACE RDC 2003 corpus shows 
that different subtypes/types of relations are 
much unevenly distributed and a few relation 
subtypes, such as the subtype ?Founder? under 
the type ?ROLE?, suffers from a small amount of 
annotated data. Further experimentation in this 
paper (please see Figure 2) shows that most rela-
tion subtypes suffer from the lack of the training 
data and fail to achieve steady performance given 
the current corpus size. Given the relative large 
size of this corpus, it will be time-consuming and 
very expensive to further expand the corpus with 
a reasonable gain in performance. Even if we can 
somehow expend the corpus and achieve steady 
performance on major relation subtypes, it will 
be still far beyond practice for those minor sub-
types given the much unevenly distribution 
among different relation subtypes. While various 
machine learning approaches, such as generative 
modeling (Miller et al2000), maximum entropy 
(Kambhatla 2004) and support vector machines 
(Zhao and Grisman 2005; Zhou et al2005), have 
been applied in the relation extraction task, no 
explicit learning strategy is proposed to deal with 
the inherent data sparseness problem caused by 
the much uneven distribution among different 
relations.  
This paper proposes a novel hierarchical 
learning strategy to deal with the data sparseness 
problem by modeling the commonality among 
related classes. Through organizing various 
classes hierarchically, a linear discriminative 
function is determined for each class in a top-
down way using a perceptron algorithm with the 
lower-level weight vector derived from the up-
per-level weight vector. Evaluation on the ACE 
RDC 2003 corpus shows that the hierarchical 
121
strategy achieves much better performance than 
the flat strategy on least- and medium-frequent 
relations. It also shows that our system based on 
the hierarchical strategy outperforms the previ-
ous best-reported system. 
The rest of this paper is organized as follows. 
Section 2 presents related work. Section 3 
describes the hierarchical learning strategy using 
the perceptron algorithm. Finally, we present 
experimentation in Section 4 and conclude this 
paper in Section 5.  
2 Related Work 
The relation extraction task was formulated at 
MUC-7(1998). With the increasing popularity of 
ACE, this task is starting to attract more and 
more researchers within the natural language 
processing and machine learning communities. 
Typical works include Miller et al(2000), Ze-
lenko et al(2003), Culotta and Sorensen (2004), 
Bunescu and Mooney (2005a), Bunescu and 
Mooney  (2005b), Zhang et al(2005), Roth and 
Yih (2002), Kambhatla (2004), Zhao and Grisman  
(2005) and Zhou et al(2005). 
Miller et al(2000) augmented syntactic full 
parse trees with semantic information of entities 
and relations, and built generative models to in-
tegrate various tasks such as POS tagging, named 
entity recognition, template element extraction 
and relation extraction. The problem is that such 
integration may impose big challenges, e.g. the 
need of a large annotated corpus. To overcome 
the data sparseness problem, generative models 
typically applied some smoothing techniques to 
integrate different scales of contexts in parameter 
estimation, e.g. the back-off approach in Miller 
et al(2000).  
Zelenko et al(2003) proposed extracting re-
lations by computing kernel functions between 
parse trees. Culotta and Sorensen (2004) extended 
this work to estimate kernel functions between 
augmented dependency trees and achieved F-
measure of 45.8 on the 5 relation types in the 
ACE RDC 2003 corpus1. Bunescu and Mooney 
(2005a) proposed a shortest path dependency 
kernel. They argued that the information to 
model a relationship between two entities can be 
typically captured by the shortest path between 
them in the dependency graph. It achieved the F-
measure of 52.5 on the 5 relation types in the 
ACE RDC 2003 corpus. Bunescu and Mooney 
(2005b) proposed a subsequence kernel and ap-
                                                          
1 The ACE RDC 2003 corpus defines 5/24 relation 
types/subtypes between 4 entity types. 
plied it in protein interaction and ACE relation 
extraction tasks. Zhang et al(2005) adopted clus-
tering algorithms in unsupervised relation extrac-
tion using tree kernels. To overcome the data 
sparseness problem, various scales of sub-trees 
are applied in the tree kernel computation. Al-
though tree kernel-based approaches are able to 
explore the huge implicit feature space without 
much feature engineering, further research work 
is necessary to make them effective and efficient. 
Comparably, feature-based approaches 
achieved much success recently. Roth and Yih 
(2002) used the SNoW classifier to incorporate 
various features such as word, part-of-speech and 
semantic information from WordNet, and pro-
posed a probabilistic reasoning approach to inte-
grate named entity recognition and relation 
extraction. Kambhatla (2004) employed maxi-
mum entropy models with features derived from 
word, entity type, mention level, overlap, de-
pendency tree, parse tree and achieved F-
measure of 52.8 on the 24 relation subtypes in 
the ACE RDC 2003 corpus. Zhao and Grisman 
(2005) 2  combined various kinds of knowledge 
from tokenization, sentence parsing and deep 
dependency analysis through support vector ma-
chines and achieved F-measure of 70.1 on the 7 
relation types of the ACE RDC 2004 corpus3. 
Zhou et al(2005) further systematically explored 
diverse lexical, syntactic and semantic features 
through support vector machines and achieved F-
measure of 68.1 and 55.5 on the 5 relation types 
and the 24 relation subtypes in the ACE RDC 
2003 corpus respectively. To overcome the data 
sparseness problem, feature-based approaches 
normally incorporate various scales of contexts 
into the feature vector extensively. These ap-
proaches then depend on adopted learning algo-
rithms to weight and combine each feature 
effectively. For example, an exponential model 
and a linear model are applied in the maximum 
entropy models and support vector machines re-
spectively to combine each feature via the 
learned weight vector. 
In summary, although various approaches 
have been employed in relation extraction, they 
implicitly attack the data sparseness problem by 
using features of different contexts in feature-
based approaches or including different sub-
                                                          
2 Here, we classify this paper into feature-based ap-
proaches since the feature space in the kernels of 
Zhao and Grisman (2005) can be easily represented 
by an explicit feature vector. 
3 The ACE RDC 2004 corpus defines 7/27 relation 
types/subtypes between 7 entity types. 
122
structures in kernel-based approaches. Until now, 
there are no explicit ways to capture the hierar-
chical topology in relation extraction. Currently, 
all the current approaches apply the flat learning 
strategy which equally treats training examples 
in different relations independently and ignore 
the commonality among different relations. This 
paper proposes a novel hierarchical learning 
strategy to resolve this problem by considering 
the relatedness among different relations and 
capturing the commonality among related rela-
tions. By doing so, the data sparseness problem 
can be well dealt with and much better perform-
ance can be achieved, especially for those rela-
tions with small amounts of annotated examples.  
3 Hierarchical Learning Strategy 
Traditional classifier learning approaches apply 
the flat learning strategy. That is, they equally 
treat training examples in different classes 
independently and ignore the commonality 
among related classes. The flat strategy will not 
cause any problem when there are a large amount 
of training examples for each class, since, in this 
case, a classifier learning approach can always 
learn a nearly optimal discriminative function for 
each class against the remaining classes. How-
ever, such flat strategy may cause big problems 
when there is only a small amount of training 
examples for some of the classes. In this case, a 
classifier learning approach may fail to learn a 
reliable (or nearly optimal) discriminative func-
tion for a class with a small amount of training 
examples, and, as a result, may significantly af-
fect the performance of the class or even the 
overall performance. 
To overcome the inherent problems in the 
flat strategy, this paper proposes a hierarchical 
learning strategy which explores the inherent 
commonality among related classes through a 
class hierarchy. In this way, the training exam-
ples of related classes can help in learning a reli-
able discriminative function for a class with only 
a small amount of training examples. To reduce 
computation time and memory requirements, we 
will only consider linear classifiers and apply the 
simple and widely-used perceptron algorithm for 
this purpose with more options open for future 
research. In the following, we will first introduce 
the perceptron algorithm in linear classifier 
learning, followed by the hierarchical learning 
strategy using the perceptron algorithm. Finally, 
we will consider several ways in building the 
class hierarchy. 
3.1 Perceptron Algorithm 
_______________________________________ 
Input:  the initial weight vector w , the training 
example sequence 
TtYXyx tt ...,2,1,),( =?? and the number of 
the maximal iterations N (e.g. 10 in this 
paper) of the training sequence4  
Output: the weight vector w  for the linear 
discriminative function  xwf ?=  
BEGIN 
    ww =1  
    REPEAT for t=1,2,?,T*N 
1. Receive the instance nt Rx ?  
2. Compute the output ttt xwo ?=  
3. Give the prediction )( tt osigny =
?
 
4. Receive the desired label }1,1{ +??ty  
5. Update the hypothesis according to   
   ttttt xyww ?+=+1            (1) 
                where 0=t? if the margin of tw  at the 
given example ),( tt yx  0>? ttt xwy  
and 1=t?  otherwise 
    END REPEAT 
    Return 5/
4
1*?
?=
+=
N
Ni
iTww  
END BEGIN 
_______________________________________ 
Figure 1: the perceptron algorithm 
This section first deals with binary classification 
using linear classifiers. Assume an instance space 
nRX =  and a binary label space }1,1{ +?=Y . 
With any weight vector nRw?  and a given 
instance nRx? , we associate a linear classifier 
wh  with a linear discriminative function
5 
xwxf ?=)(  by )()( xwsignxhw ?=  , where 
1)( ?=? xwsign  if 0<? xw  and 1)( +=? xwsign  
otherwise. Here, the margin of w  at ),( tt yx  is 
defined as tt xwy ? . Then if the margin is positive, 
we have a correct prediction with tw yxh =)( , and 
if the margin is negative, we have an error with 
tw yxh ?)( . Therefore, given a sequence of 
training examples TtYXyx tt ...,2,1,),( =?? , 
linear classifier learning attemps to find a weight 
vector w  that achieves a positive margin on as 
many examples as possible. 
                                                          
4 The training example sequence is feed N times for 
better performance. Moreover, this number can con-
trol the maximal affect a training example can pose. 
This is similar to the regulation parameter C in 
SVM, which affects the trade-off between complex-
ity and proportion of non-separable examples. As a 
result, it can be used to control over-fitting and 
robustness. 
5 )( xw ?  denotes the dot product of the weight vector 
nRw?  and a given instance nRx? . 
123
The well-known perceptron algorithm, as 
shown in Figure 1, belongs to online learning of 
linear classifiers, where the learning algorithm 
represents its t -th hyposthesis by a weight vector 
n
t Rw ? . At trial t , an online algorithm receives 
an instance nt Rx ? , makes its prediction 
)( ttt xwsigny ?=
?
 and receives the desired label 
}1,1{ +??ty . What distinguishes different online 
algorithms is how they update tw  into 1+tw  based 
on the example ),( tt yx  received at trial t . In 
particular, the perceptron algorithm updates the 
hypothesis by adding a scalar multiple of the 
instance, as shown in Equation 1 of Figure 1, 
when there is an error. Normally, the tradictional 
perceptron algorithm initializes the hypothesis as 
the zero vector 01 =w . This is usually the most 
natural choice, lacking any other preference. 
Smoothing 
In order to further improve the performance, we 
iteratively feed the training examples for a possi-
ble better discriminative function. In this paper, 
we have set the maximal iteration number to 10 
for both efficiency and stable performance and 
the final weight vector in the discriminative func-
tion is averaged over those of the discriminative 
functions in the last few iterations (e.g. 5 in this 
paper).  
Bagging 
One more problem with any online classifier 
learning algorithm, including the perceptron al-
gorithm, is that the learned discriminative func-
tion somewhat depends on the feeding order of 
the training examples. In order to eliminate such 
dependence and further improve the perform-
ance, an ensemble technique, called bagging 
(Breiman 1996), is applied in this paper. In bag-
ging, the bootstrap technique is first used to build 
M (e.g. 10 in this paper) replicate sample sets by 
randomly re-sampling with replacement from the 
given training set repeatedly. Then, each training 
sample set is used to train a certain discrimina-
tive function. Finally, the final weight vector in 
the discriminative function is averaged over 
those of the M discriminative functions in the 
ensemble. 
Multi-Class Classification 
Basically, the perceptron algorithm is only for 
binary classification. Therefore, we must extend 
the perceptron algorithms to multi-class 
classification, such as the ACE RDC task. For 
efficiency, we apply the one vs. others strategy, 
which builds K classifiers so as to separate one 
class from all others. However, the outputs for 
the perceptron algorithms of different classes 
may be not directly comparable since any 
positive scalar multiple of the weight vector will 
not affect the actual prediction of a perceptron 
algorithm. For comparability, we map the 
perceptron algorithm output into the probability 
by using an additional sigmoid model: 
)exp(1
1)|1(
BAf
fyp ++==          (2) 
where xwf ?=  is the output of a perceptron 
algorithm and the coefficients A & B are to be 
trained using the model trust alorithm as 
described in Platt (1999). The final decision of an 
instance in multi-class classification is 
determined by the class which has the maximal 
probability from the corresponding perceptron 
algorithm.  
3.2 Hierarchical Learning Strategy using the 
Perceptron Algorithm 
Assume we have a class hierarchy for a task, e.g. 
the one in the ACE RDC 2003 corpus as shown 
in Table 1 of Section 4.1. The hierarchical learn-
ing strategy explores the inherent commonality 
among related classes in a top-down way. For 
each class in the hierarchy, a linear discrimina-
tive function is determined in a top-down way 
with the lower-level weight vector derived from 
the upper-level weight vector iteratively. This is 
done by initializing the weight vector in training 
the linear discriminative function for the lower-
level class as that of the upper-level class. That 
is, the lower-level discriminative function has the 
preference toward the discriminative function of 
its upper-level class. For an example, let?s look 
at the training of the ?Located? relation subtype 
in the class hierarchy as shown in Table 1: 
1) Train the weight vector of the linear 
discriminative function for the ?YES? 
relation vs. the ?NON? relation with the 
weight vector initialized as the zero vector. 
2) Train the weight vector of the linear 
discriminative function for the ?AT? relation 
type vs. all the remaining relation types 
(including the ?NON? relation) with the 
weight vector initialized as the weight vector 
of the linear discriminative function for the 
?YES? relation vs. the ?NON? relation. 
3) Train the weight vector of the linear 
discriminative function for the ?Located? 
relation subtype vs. all the remaining relation 
subtypes under all the relation types 
(including the ?NON? relation) with the 
124
weight vector initialized as the weight vector 
of the linear discriminative function for the 
?AT? relation type vs. all the remaining 
relation types. 
4) Return the above trained weight vector as the 
discriminatie function for the ?Located? 
relation subtype. 
In this way, the training examples in differ-
ent classes are not treated independently any 
more, and the commonality among related 
classes can be captured via the hierarchical learn-
ing strategy. The intuition behind this strategy is 
that the upper-level class normally has more 
positive training examples than the lower-level 
class so that the corresponding linear discrimina-
tive function can be determined more reliably. In 
this way, the training examples of related classes 
can help in learning a reliable discriminative 
function for a class with only a small amount of 
training examples in a top-down way and thus 
alleviate its data sparseness problem. 
3.3 Building the Class Hierarchy  
We have just described the hierarchical learning 
strategy using a given class hierarchy. Normally, 
a rough class hierarchy can be given manually 
according to human intuition, such as the one in 
the ACE RDC 2003 corpus. In order to explore 
more commonality among sibling classes, we 
make use of binary hierarchical clustering for 
sibling classes at both lowest and all levels. This 
can be done by first using the flat learning strat-
egy to learn the discriminative functions for indi-
vidual classes and then iteratively combining the 
two most related classes using the cosine similar-
ity function between their weight vectors in a 
bottom-up way. The intuition is that related 
classes should have similar hyper-planes to sepa-
rate from other classes and thus have similar 
weight vectors. 
? Lowest-level hybrid: Binary hierarchical 
clustering is only done at the lowest level 
while keeping the upper-level class hierar-
chy. That is, only sibling classes at the low-
est level are hierarchically clustered. 
? All-level hybrid: Binary hierarchical cluster-
ing is done at all levels in a bottom-up way. 
That is, sibling classes at the lowest level are 
hierarchically clustered first and then sibling 
classes at the upper-level. In this way, the bi-
nary class hierarchy can be built iteratively 
in a bottom-up way. 
 
 
4 Experimentation 
This paper uses the ACE RDC 2003 corpus pro-
vided by LDC to train and evaluate the hierarchi-
cal learning strategy. Same as Zhou et al(2005), 
we only model explicit relations and explicitly 
model the argument order of the two mentions 
involved.  
4.1 Experimental Setting 
Type Subtype Freq Bin Type 
AT Based-In 347 Medium 
 Located 2126 Large 
 Residence 308 Medium 
NEAR Relative-Location 201 Medium 
PART Part-Of 947 Large 
 Subsidiary 355 Medium 
 Other 6 Small 
ROLE Affiliate-Partner 204 Medium 
 Citizen-Of 328 Medium 
 Client 144 Small 
 Founder 26 Small 
 General-Staff 1331 Large 
 Management 1242 Large 
 Member 1091 Large 
 Owner 232 Medium 
 Other 158 Small 
SOCIAL Associate 91 Small 
 Grandparent 12 Small 
 Other-Personal 85 Small 
 Other-Professional 339 Medium 
 Other-Relative 78 Small 
 Parent 127 Small 
 Sibling 18 Small 
 Spouse 77 Small 
Table 1: Statistics of relation types and subtypes 
in the training data of the ACE RDC 2003 corpus 
(Note: According to frequency, all the subtypes 
are divided into three bins: large/ middle/ small, 
with 400 as the lower threshold for the large bin 
and 200 as the upper threshold for the small bin). 
The training data consists of 674 documents 
(~300k words) with 9683 relation examples 
while the held-out testing data consists of 97 
documents (~50k words) with 1386 relation ex-
amples. All the experiments are done five times 
on the 24 relation subtypes in the ACE corpus, 
except otherwise specified, with the final per-
formance averaged using the same re-sampling 
with replacement strategy as the one in the bag-
ging technique. Table 1 lists various types and 
subtypes of relations for the ACE RDC 2003 
corpus, along with their occurrence frequency in 
the training data. It shows that this corpus suffers 
from a small amount of annotated data for a few 
subtypes such as the subtype ?Founder? under 
the type ?ROLE?. 
For comparison, we also adopt the same fea-
ture set as Zhou et al(2005): word, entity type, 
125
mention level, overlap, base phrase chunking, 
dependency tree, parse tree and semantic infor-
mation. 
4.2 Experimental Results 
Table 2 shows the performance of the hierarchi-
cal learning strategy using the existing class hier-
archy in the given ACE corpus and its 
comparison with the flat learning strategy, using 
the perceptron algorithm. It shows that the pure 
hierarchical strategy outperforms the pure flat 
strategy by 1.5 (56.9 vs. 55.4) in F-measure. It 
also shows that further smoothing and bagging 
improve the performance of the hierarchical and 
flat strategies by 0.6 and 0.9 in F-measure re-
spectively. As a result, the final hierarchical 
strategy achieves F-measure of 57.8 and outper-
forms the final flat strategy by 1.8 in F-measure. 
Strategies  P R F 
Flat 58.2 52.8 55.4 
Flat+Smoothing 58.9 53.1 55.9 
Flat+Bagging 59.0 53.1 55.9 
Flat+Both 59.1 53.2 56.0 
Hierarchical 61.9 52.6 56.9 
Hierarchical+Smoothing 62.7 53.1 57.5 
Hierarchical+Bagging 62.9 53.1 57.6 
Hierarchical+Both 63.0 53.4 57.8 
Table 2: Performance of the hierarchical learning 
strategy using the existing class hierarchy and its 
comparison with the flat learning strategy 
Class Hierarchies P R F 
Existing 63.0 53.4 57.8 
Entirely Automatic 63.4 53.1 57.8 
Lowest-level Hybrid 63.6 53.5 58.1 
All-level Hybrid 63.6 53.6 58.2 
Table 3: Performance of the hierarchical learning 
strategy using different class hierarchies 
Table 3 compares the performance of the hi-
erarchical learning strategy using different class 
hierarchies. It shows that, the lowest-level hybrid 
approach, which only automatically updates the 
existing class hierarchy at the lowest level, im-
proves the performance by 0.3 in F-measure 
while further updating the class hierarchy at up-
per levels in the all-level hybrid approach only 
has very slight effect. This is largely due to the 
fact that the major data sparseness problem oc-
curs at the lowest level, i.e. the relation subtype 
level in the ACE corpus. As a result, the final 
hierarchical learning strategy using the class hi-
erarchy built with the all-level hybrid approach 
achieves F-measure of 58.2 in F-measure, which 
outperforms the final flat strategy by 2.2 in F-
measure. In order to justify the usefulness of our 
hierarchical learning strategy when a rough class 
hierarchy is not available and difficult to deter-
mine manually, we also experiment using en-
tirely automatically built class hierarchy (using 
the traditional binary hierarchical clustering algo-
rithm and the cosine similarity measurement) 
without considering the existing class hierarchy. 
Table 3 shows that using automatically built 
class hierarchy performs comparably with using 
only the existing one. 
With the major goal of resolving the data 
sparseness problem for the classes with a small 
amount of training examples, Table 4 compares 
the best-performed hierarchical and flat learning 
strategies on the relation subtypes of different   
training data sizes. Here, we divide various rela-
tion subtypes into three bins: large/middle/small, 
according to their available training data sizes. 
For the ACE RDC 2003 corpus, we use 400 as 
the lower threshold for the large bin6 and 200 as 
the upper threshold for the small bin7. As a re-
sult, the large/medium/small bin includes 5/8/11 
relation subtypes, respectively. Please see Table 
1 for details. Table 4 shows that the hierarchical 
strategy outperforms the flat strategy by 
1.0/5.1/5.6 in F-measure on the 
large/middle/small bin respectively. This indi-
cates that the hierarchical strategy performs 
much better than the flat strategy for those 
classes with a small or medium amount of anno-
tated examples although the hierarchical strategy 
only performs slightly better by 1.0 and 2.2 in F-
measure than the flat strategy on those classes 
with a large size of annotated corpus and on all 
classes as a whole respectively. This suggests 
that the proposed hierarchical strategy can well 
deal with the data sparseness problem in the 
ACE RDC 2003 corpus.  
An interesting question is about the similar-
ity between the linear discriminative functions 
learned using the hierarchical and flat learning 
strategies.  Table 4 compares the cosine similari-
ties between the weight vectors of the linear dis-
criminative functions using the two strategies for 
different bins, weighted by the training data sizes 
                                                          
6 The reason to choose this threshold is that no rela-
tion subtype in the ACE RC 2003 corpus has train-
ing examples in between 400 and 900. 
7 A few minor relation subtypes only have very few 
examples in the testing set. The reason to choose 
this threshold is to guarantee a reasonable number of 
testing examples in the small bin. For the ACE RC 
2003 corpus, using 200 as the upper threshold will 
fill the small bin with about 100 testing examples 
while using 100 will include too few testing exam-
ples for reasonable performance evaluation. 
126
of different relation subtypes. It shows that the 
linear discriminative functions learned using the 
two strategies are very similar (with the cosine 
similarity 0.98) for the relation subtypes belong-
ing to the large bin while the linear discrimina-
tive functions learned using the two strategies are 
not for the relation subtypes belonging to the 
medium/small bin with the cosine similarity 
0.92/0.81 respectively. This means that the use of 
the hierarchical strategy over the flat strategy 
only has very slight change on the linear dis-
criminative functions for those classes with a 
large amount of annotated examples while its 
effect on those with a small amount of annotated 
examples is obvious. This contributes to and ex-
plains (the degree of) the performance difference 
between the two strategies on the different train-
ing data sizes as shown in Table 4. 
Due to the difficulty of building a large an-
notated corpus, another interesting question is 
about the learning curve of the hierarchical learn-
ing strategy and its comparison with the flat 
learning strategy. Figure 2 shows the effect of 
different training data sizes for some major rela-
tion subtypes while keeping all the training ex-
amples of remaining relation subtypes. It shows 
that the hierarchical strategy performs much bet-
ter than the flat strategy when only a small 
amount of training examples is available. It also 
shows that the hierarchical strategy can achieve 
stable performance much faster than the flat 
strategy. Finally, it shows that the ACE RDC 
2003 task suffers from the lack of training exam-
ples. Among the three major relation subtypes, 
only the subtype ?Located? achieves steady per-
formance. 
Finally, we also compare our system with the 
previous best-reported systems, such as Kamb-
hatla  (2004) and Zhou et al(2005). Table 5 
shows that our system outperforms the previous 
best-reported system by 2.7 (58.2 vs. 55.5) in F-
measure, largely due to the gain in recall. It indi-
cates that, although support vector machines and 
maximum entropy models always perform better 
than the simple perceptron algorithm in most (if 
not all) applications, the hierarchical learning 
strategy using the perceptron algorithm can eas-
ily overcome the difference and outperforms the 
flat learning strategy using the overwhelming 
support vector machines and maximum entropy 
models in relation extraction, at least on the ACE 
RDC 2003 corpus. 
Large Bin (0.98) Middle Bin (0.92) Small Bin (0.81) Bin Type(cosine similarity) 
P R F P R F P R F 
Flat Strategy 62.3 61.9 62.1 60.8 38.7 47.3 33.0 21.7 26.2 
Hierarchical Strategy 66.4 60.2 63.1 67.6 42.7 52.4 40.2 26.3 31.8 
Table 4: Comparison of the hierarchical and flat learning strategies on the relation subtypes of differ-
ent training data sizes. Notes: the figures in the parentheses indicate the cosine similarities between 
the weight vectors of the linear discriminative functions learned using the two strategies. 
10
20
30
40
50
60
70
20
0
40
0
60
0
80
0
10
00
12
00
14
00
16
00
18
00
20
00
Training Data Size
F
-m
ea
su
re
HS: General-Staff
FS: General-Staff
HS: Part-Of
FS: Part-Of
HS: Located
FS: Located
Figure 2: Learning curve of the hierarchical strategy and its comparison with the flat strategy for some 
major relation subtypes (Note: FS for the flat strategy and HS for the hierarchical strategy) 
Performance System 
P R F 
Our: Perceptron Algorithm + Hierarchical Strategy 63.6 53.6 58.2 
Zhou et al(2005): SVM + Flat Strategy 63.1 49.5 55.5 
Kambhatla (2004): Maximum Entropy + Flat Strategy 63.5 45.2 52.8 
Table 5: Comparison of our system with other best-reported systems 
127
5 Conclusion 
This paper proposes a novel hierarchical learning 
strategy to deal with the data sparseness problem 
in relation extraction by modeling the common-
ality among related classes. For each class in a 
class hierarchy, a linear discriminative function 
is determined in a top-down way using the per-
ceptron algorithm with the lower-level weight 
vector derived from the upper-level weight vec-
tor. In this way, the upper-level discriminative 
function can effectively guide the lower-level 
discriminative function learning. Evaluation on 
the ACE RDC 2003 corpus shows that the hier-
archical strategy performs much better than the 
flat strategy in resolving the critical data sparse-
ness problem in relation extraction. 
In the future work, we will explore the hier-
archical learning strategy using other machine 
learning approaches besides online classifier 
learning approaches such as the simple percep-
tron algorithm applied in this paper. Moreover, 
just as indicated in Figure 2, most relation sub-
types in the ACE RDC 2003 corpus (arguably 
the largest annotated corpus in relation extrac-
tion) suffer from the lack of training examples. 
Therefore, a critical research in relation extrac-
tion is how to rely on semi-supervised learning 
approaches (e.g. bootstrap) to alleviate its de-
pendency on a large amount of annotated training 
examples and achieve better and steadier per-
formance. Finally, our current work is done when 
NER has been perfectly done. Therefore, it 
would be interesting to see how imperfect NER 
affects the performance in relation extraction. 
This will be done by integrating the relation ex-
traction system with our previously developed 
NER system as described in Zhou and Su (2002). 
References  
ACE. (2000-2005). Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/  
Bunescu R. & Mooney R.J. (2005a). A shortest 
path dependency kernel for relation extraction. 
HLT/EMNLP?2005: 724-731. 6-8 Oct 2005. 
Vancover, B.C. 
Bunescu R. & Mooney R.J. (2005b). Subsequence 
Kernels for Relation Extraction  NIPS?2005. 
Vancouver, BC, December 2005  
Breiman L. (1996) Bagging Predictors. Machine 
Learning, 24(2): 123-140. 
Collins M. (1999).  Head-driven statistical models 
for natural language parsing. Ph.D. Dissertation, 
University of Pennsylvania. 
Culotta A. and Sorensen J. (2004). Dependency 
tree kernels for relation extraction. ACL?2004. 
423-429. 21-26 July 2004. Barcelona, Spain. 
Kambhatla N. (2004). Combining lexical, syntactic 
and semantic features with Maximum Entropy 
models for extracting relations. 
ACL?2004(Poster). 178-181. 21-26 July 2004. 
Barcelona, Spain. 
Miller G.A. (1990). WordNet: An online lexical 
database. International Journal of Lexicography. 
3(4):235-312. 
Miller S., Fox H., Ramshaw L. and Weischedel R. 
(2000). A novel use of statistical parsing to ex-
tract information from text. ANLP?2000. 226-
233. 29 April  - 4 May 2000, Seattle, USA 
MUC-7. (1998). Proceedings of the 7th Message 
Understanding Conference (MUC-7). Morgan 
Kaufmann, San Mateo, CA. 
Platt J. 1999. Probabilistic Outputs for Support 
Vector Machines and Comparisions to regular-
ized Likelihood Methods. In Advances in Large 
Margin Classifiers. Edited by Smola .J., Bartlett 
P., Scholkopf B. and Schuurmans D. MIT Press. 
Roth D. and Yih W.T. (2002). Probabilistic reason-
ing for entities and relation recognition. CoL-
ING?2002. 835-841.26-30 Aug 2002. Taiwan. 
Zelenko D., Aone C. and Richardella. (2003). Ker-
nel methods for relation extraction. Journal of 
Machine Learning Research. 3(Feb):1083-1106. 
Zhang M., Su J., Wang D.M., Zhou G.D. and Tan 
C.L. (2005). Discovering Relations from a Large 
Raw Corpus Using Tree Similarity-based Clus-
tering, IJCNLP?2005, Lecture Notes in 
Computer Science (LNCS 3651). 378-389. 11-16 
Oct 2005. Jeju Island, South Korea. 
Zhao S.B. and Grisman R. 2005. Extracting rela-
tions with integrated information using kernel 
methods. ACL?2005: 419-426. Univ of Michi-
gan-Ann Arbor? USA? 25-30 June 2005. 
Zhou G.D. and Su Jian. Named Entity Recogni-
tion Using a HMM-based Chunk Tagger, 
ACL?2002. pp473-480. Philadelphia. July 
2002.  
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). 
Exploring various knowledge in relation extrac-
tion. ACL?2005. 427-434. 25-30 June, Ann Ar-
bor, Michgan, USA. 
128
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825?832,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Composite Kernel to Extract Relations between Entities with 
both Flat and Structured Features 
Min Zhang         Jie Zhang       Jian Su      Guodong Zhou 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, Singapore 119613 
{mzhang, zhangjie, sujian, zhougd}@i2r.a-star.edu.sg 
 
Abstract 
This paper proposes a novel composite ker-
nel for relation extraction. The composite 
kernel consists of two individual kernels: an 
entity kernel that allows for entity-related 
features and a convolution parse tree kernel 
that models syntactic information of relation 
examples. The motivation of our method is 
to fully utilize the nice properties of kernel 
methods to explore diverse knowledge for 
relation extraction. Our study illustrates that 
the composite kernel can effectively capture 
both flat and structured features without the 
need for extensive feature engineering, and 
can also easily scale to include more fea-
tures. Evaluation on the ACE corpus shows 
that our method outperforms the previous 
best-reported methods and significantly out-
performs previous two dependency tree ker-
nels for relation extraction. 
1 Introduction 
The goal of relation extraction is to find various 
predefined semantic relations between pairs of 
entities in text. The research on relation extrac-
tion has been promoted by the Message Under-
standing Conferences (MUCs) (MUC, 1987-
1998) and Automatic Content Extraction (ACE) 
program (ACE, 2002-2005). According to the 
ACE Program, an entity is an object or set of ob-
jects in the world and a relation is an explicitly 
or implicitly stated relationship among entities. 
For example, the sentence ?Bill Gates is chair-
man and chief software architect of Microsoft 
Corporation.? conveys the ACE-style relation 
?EMPLOYMENT.exec? between the entities 
?Bill Gates? (PERSON.Name) and ?Microsoft 
Corporation? (ORGANIZATION. Commercial).  
In this paper, we address the problem of rela-
tion extraction using kernel methods (Sch?lkopf 
and Smola, 2001). Many feature-based learning 
algorithms involve only the dot-product between 
feature vectors. Kernel methods can be regarded 
as a generalization of the feature-based methods 
by replacing the dot-product with a kernel func-
tion between two vectors, or even between two 
objects. A kernel function is a similarity function 
satisfying the properties of being symmetric and 
positive-definite. Recently, kernel methods are 
attracting more interests in the NLP study due to 
their ability of implicitly exploring huge amounts 
of structured features using the original represen-
tation of objects. For example, the kernels for 
structured natural language data, such as parse 
tree kernel (Collins and Duffy, 2001), string ker-
nel (Lodhi et al, 2002) and graph kernel (Suzuki 
et al, 2003) are example instances of the well-
known convolution kernels1 in NLP. In relation 
extraction, typical work on kernel methods in-
cludes: Zelenko et al (2003), Culotta and Soren-
sen (2004) and Bunescu and Mooney (2005). 
This paper presents a novel composite kernel 
to explore diverse knowledge for relation extrac-
tion. The composite kernel consists of an entity 
kernel and a convolution parse tree kernel. Our 
study demonstrates that the composite kernel is 
very effective for relation extraction. It also 
shows without the need for extensive feature en-
gineering the composite kernel can not only cap-
ture most of the flat features used in the previous 
work but also exploit the useful syntactic struc-
ture features effectively. An advantage of our 
method is that the composite kernel can easily 
cover more knowledge by introducing more ker-
nels. Evaluation on the ACE corpus shows that 
our method outperforms the previous best-
reported methods and significantly outperforms 
the previous kernel methods due to its effective 
exploration of various syntactic features. 
The rest of the paper is organized as follows. 
In Section 2, we review the previous work. Sec-
tion 3 discusses our composite kernel. Section 4 
reports the experimental results and our observa-
tions. Section 5 compares our method with the 
                                                 
1 Convolution kernels were proposed for a discrete structure 
by Haussler (1999) in the machine learning field. This 
framework defines a kernel between input objects by apply-
ing convolution ?sub-kernels? that are the kernels for the 
decompositions (parts) of the objects.  
825
previous work from the viewpoint of feature ex-
ploration. We conclude our work and indicate the 
future work in Section 6. 
2 Related Work 
Many techniques on relation extraction, such as 
rule-based (MUC, 1987-1998; Miller et al, 
2000), feature-based (Kambhatla 2004; Zhou et 
al., 2005) and kernel-based (Zelenko et al, 2003; 
Culotta and Sorensen, 2004; Bunescu and 
Mooney, 2005), have been proposed in the litera-
ture. 
Rule-based methods for this task employ a 
number of linguistic rules to capture various rela-
tion patterns. Miller et al (2000) addressed the 
task from the syntactic parsing viewpoint and 
integrated various tasks such as POS tagging, NE 
tagging, syntactic parsing, template extraction 
and relation extraction using a generative model. 
Feature-based methods (Kambhatla, 2004; 
Zhou et al, 2005; Zhao and Grishman, 20052) 
for this task employ a large amount of diverse 
linguistic features, such as lexical, syntactic and 
semantic features. These methods are very effec-
tive for relation extraction and show the best-
reported performance on the ACE corpus. How-
ever, the problems are that these diverse features 
have to be manually calibrated and the hierarchi-
cal structured information in a parse tree is not 
well preserved in their parse tree-related features, 
which only represent simple flat path informa-
tion connecting two entities in the parse tree 
through a path of non-terminals and a list of base 
phrase chunks. 
Prior kernel-based methods for this task focus 
on using individual tree kernels to exploit tree 
structure-related features. Zelenko et al (2003) 
developed a kernel over parse trees for relation 
extraction. The kernel matches nodes from roots 
to leaf nodes recursively layer by layer in a top-
down manner. Culotta and Sorensen (2004) gen-
eralized it to estimate similarity between depend-
ency trees. Their tree kernels require the match-
able nodes to be at the same layer counting from 
the root and to have an identical path of ascend-
ing nodes from the roots to the current nodes. 
The two constraints make their kernel high preci-
sion but very low recall on the ACE 2003 corpus. 
Bunescu and Mooney (2005) proposed another 
dependency tree kernel for relation extraction. 
                                                 
2 We classify the feature-based kernel defined in (Zhao and 
Grishman, 2005) into the feature-based methods since their 
kernels can be easily represented by the dot-products be-
tween explicit feature vectors. 
Their kernel simply counts the number of com-
mon word classes at each position in the shortest 
paths between two entities in dependency trees. 
The kernel requires the two paths to have the 
same length; otherwise the kernel value is zero. 
Therefore, although this kernel shows perform-
ance improvement over the previous one (Culotta 
and Sorensen, 2004), the constraint makes the 
two dependency kernels share the similar behav-
ior: good precision but much lower recall on the 
ACE corpus. 
The above discussion shows that, although 
kernel methods can explore the huge amounts of 
implicit (structured) features, until now the fea-
ture-based methods enjoy more success. One 
may ask: how can we make full use of the nice 
properties of kernel methods and define an effec-
tive kernel for relation extraction? 
In this paper, we study how relation extraction 
can benefit from the elegant properties of kernel 
methods: 1) implicitly exploring (structured) fea-
tures in a high dimensional space; and 2) the nice 
mathematical properties, for example, the sum, 
product, normalization and polynomial expan-
sion of existing kernels is a valid kernel 
(Sch?lkopf and Smola, 2001). We also demon-
strate how our composite kernel effectively cap-
tures the diverse knowledge for relation extrac-
tion.  
3 Composite Kernel for Relation Ex-
traction  
In this section, we define the composite kernel 
and study the effective representation of a rela-
tion instance. 
3.1 Composite Kernel 
Our composite kernel consists of an entity kernel 
and a convolution parse tree kernel. To our 
knowledge, convolution kernels have not been 
explored for relation extraction. 
 
(1) Entity Kernel: The ACE 2003 data defines 
four entity features: entity headword, entity type 
and subtype (only for GPE), and mention type 
while the ACE 2004 data makes some modifica-
tions and introduces a new feature ?LDC men-
tion type?. Our statistics on the ACE data reveals 
that the entity features impose a strong constraint 
on relation types. Therefore, we design a linear 
kernel to explicitly capture such features: 
1 2 1 21,2
( , ) ( . , . )L E i iiK R R K R E R E== ?  (1) 
where 1R and 2R stands for two relation instances, 
Ei means the ith entity of a relation instance, and 
826
( , )EK ? ?  is a simple kernel function over the fea-
tures of entities: 
1 2 1 2( , ) ( . , . )E i iiK E E C E f E f=? (2) 
where if represents the i
th entity feature, and the 
function ( , )C ? ?  returns 1 if the two feature val-
ues are identical and 0 otherwise. ( , )EK ? ?  re-
turns the number of feature values in common of 
two entities. 
 
(2) Convolution Parse Tree Kernel: A convo-
lution kernel aims to capture structured informa-
tion in terms of substructures. Here we use the 
same convolution parse tree kernel as described 
in Collins and Duffy (2001) for syntactic parsing 
and Moschitti (2004) for semantic role labeling. 
Generally, we can represent a parse tree T  by a 
vector of integer counts of each sub-tree type 
(regardless of its ancestors):  
 
( )T? = (# subtree1(T), ?, # subtreei(T), ?,  # 
subtreen(T) ) 
 
where # subtreei(T) is the occurrence number of 
the ith sub-tree type (subtreei) in T. Since the 
number of different sub-trees is exponential with 
the parse tree size, it is computationally infeasi-
ble to directly use the feature vector ( )T? . To 
solve this computational issue, Collins and Duffy 
(2001) proposed the following parse tree kernel 
to calculate the dot product between the above 
high dimensional vectors implicitly. 
 
 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
1 2
( , ) ( ), ( )
   # ( ) # ( )
   ( ) ( )
   ( , )
( ) ( )
i i
i ii
subtree subtreei n N n N
n N n N
K T T T T
subtree T subtree T
I n I n
n n
? ?
? ?
? ?
=< >
=
=
= ?
?
?
?
? ? ?
? ?
(3) 
 
where N1 and N2 are the sets of nodes in trees T1 
and T2, respectively, and ( )
isubtree
I n  is a function 
that is 1 iff the subtreei occurs with root at node n 
and zero otherwise, and 1 2( , )n n?  is the number of 
the common subtrees rooted at n1 and n2, i.e. 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? = ??  
 
1 2( , )n n? can be computed by the following recur-
sive rules:  
(1) if the productions (CFP rules) at 1n  and 2n  
are different, 1 2( , ) 0n n? = ; 
(2) else if both 1n  and 2n  are pre-terminals (POS 
tags), 1 2( , ) 1n n ?? = ? ; 
(3) else, 1( )1 2 1 21( , ) (1 ( ( , ), ( , )))
nc n
j
n n ch n j ch n j? =? = +?? ,  
where 1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and? (0<? <1) is the de-
cay factor in order to make the kernel value less 
variable with respect to the subtree sizes. In ad-
dition, the recursive rule (3) holds because given 
two nodes with the same children, one can con-
struct common sub-trees using these children and 
common sub-trees of further offspring.  
The parse tree kernel counts the number of 
common sub-trees as the syntactic similarity 
measure between two relation instances. The 
time complexity for computing this kernel 
is 1 2(| | | |)O N N? . 
In this paper, two composite kernels are de-
fined by combing the above two individual ker-
nels in the following ways: 
 
1) Linear combination: 
 
1 1 2 1 2 1 2
? ?( , ) ( , ) (1 ) ( , )LK R R K R R K T T? ?? ?= + ? (4) 
 
Here, ? ( , )K ? ?  is the normalized3 ( , )K ? ? and ?  
is the coefficient. Evaluation on the development 
set shows that this composite kernel yields the 
best performance when ? is set to 0.4. 
 
2) Polynomial expansion: 
 
2 1 2 1 2 1 2
? ?( , ) ( , ) (1 ) ( , )PLK R R K R R K T T? ?? ?= + ? (5) 
 
Here, ? ( , )K ? ?  is the normalized ( , )K ? ? , ( , )pK ? ?  
is the polynomial expansion of ( , )K ? ?  with de-
gree d=2, i.e. 2( , ) ( ( , ) 1)pK K? ? ? ?= + , and ?  is the 
coefficient. Evaluation on the development set 
shows that this composite kernel yields the best 
performance when ? is set to 0.23. 
The polynomial expansion aims to explore the 
entity bi-gram features, esp. the combined fea-
tures from the first and second entities, respec-
tively. In addition, due to the different scales of 
the values of the two individual kernels, they are 
normalized before combination. This can avoid 
one kernel value being overwhelmed by that of 
another one.  
The entity kernel formulated by eqn. (1) is a 
proper kernel since it simply calculates the dot 
product of the entity feature vectors. The tree 
kernel formulated by eqn. (3) is proven to be a 
proper kernel (Collins and Duffy, 2001). Since 
kernel function set is closed under normalization, 
polynomial expansion and linear combination 
(Sch?lkopf and Smola, 2001), the two composite 
kernels are also proper kernels. 
                                                 
3  A kernel ( , )K x y  can be normalized by dividing it by 
( , ) ( , )K x x K y y? .  
827
3.2 Relation Instance Spaces 
A relation instance is encapsulated by a parse 
tree. Thus, it is critical to understand which por-
tion of a parse tree is important in the kernel cal-
culation. We study five cases as shown in Fig.1. 
 
(1) Minimum Complete Tree (MCT): the com-
plete sub-tree rooted by the nearest common an-
cestor of the two entities under consideration. 
 
(2) Path-enclosed Tree (PT): the smallest com-
mon sub-tree including the two entities. In other 
words, the sub-tree is enclosed by the shortest 
path linking the two entities in the parse tree (this 
path is also commonly-used as the path tree fea-
ture in the feature-based methods). 
 
(3) Context-Sensitive Path Tree (CPT): the PT 
extended with the 1st left word of entity 1 and the 
1st right word of entity 2. 
 
(4) Flattened Path-enclosed Tree (FPT): the 
PT with the single in and out arcs of non-
terminal nodes (except POS nodes) removed. 
 
(5) Flattened CPT (FCPT): the CPT with the 
single in and out arcs of non-terminal nodes (ex-
cept POS nodes) removed.  
 
Fig. 1 illustrates different representations of an 
example relation instance. T1 is MCT for the 
relation instance, where the sub-tree circled by a 
dashed line is PT, which is also shown in T2 for 
clarity. The only difference between MCT and 
PT lies in that MCT does not allow partial pro-
duction rules (for example, NP?PP is a partial 
production rule while NP?NP+PP is an entire 
production rule in the top of T2). For instance, 
only the most-right child in the most-left sub-tree 
[NP [CD 200] [JJ domestic] [E1-PER ?]] of T1 
is kept in T2. By comparing the performance of 
T1 and T2, we can evaluate the effect of sub-trees 
with partial production rules as shown in T2 and 
the necessity of keeping the whole left and right 
context sub-trees as shown in T1 in relation ex-
traction. T3 is CPT, where the two sub-trees cir-
cled by dashed lines are included as the context 
to T2 and make T3 context-sensitive. This is to 
evaluate whether the limited context information 
in CPT can boost performance. FPT in T4 is 
formed by removing the two circled nodes in T2. 
This is to study whether and how the elimination 
of single non-terminal nodes affects the perform-
ance of relation extraction.  
T1): MCT T2): PT 
T3):CPT T4): FPT 
Figure 1. Different representations of a relation instance in the example sentence ??provide bene-
fits to 200 domestic partners of their own workers in New York?, where the phrase type 
?E1-PER? denotes that the current node is the 1st entity with type ?PERSON?, and like-
wise for the others. The relation instance is excerpted from the ACE 2003 corpus, where 
a relation ?SOCIAL.Other-Personal? exists between entities ?partners? (PER) and 
?workers? (PER). We use Charniak?s parser (Charniak, 2001) to parse the example sen-
tence. To save space, the FCPT is not shown here. 828
4 Experiments 
4.1 Experimental Setting 
Data: We use the English portion of both the 
ACE 2003 and 2004 corpora from LDC in our 
experiments. In the ACE 2003 data, the training 
set consists of 674 documents and 9683 relation 
instances while the test set consists of 97 docu-
ments and 1386 relation instances. The ACE 
2003 data defines 5 entity types, 5 major relation 
types and 24 relation subtypes. The ACE 2004 
data contains 451 documents and 5702 relation 
instances. It redefines 7 entity types, 7 major re-
lation types and 23 subtypes. Since Zhao and 
Grishman (2005) use a 5-fold cross-validation on 
a subset of the 2004 data (newswire and broad-
cast news domains, containing 348 documents 
and 4400 relation instances), for comparison, we 
use the same setting (5-fold cross-validation on 
the same subset of the 2004 data, but the 5 parti-
tions may not be the same) for the ACE 2004 
data. Both corpora are parsed using Charniak?s 
parser (Charniak, 2001). We iterate over all pairs 
of entity mentions occurring in the same sen-
tence to generate potential relation instances. In 
this paper, we only measure the performance of 
relation extraction models on ?true? mentions 
with ?true? chaining of coreference (i.e. as anno-
tated by LDC annotators). 
 
Implementation: We formalize relation extrac-
tion as a multi-class classification problem. SVM 
is selected as our classifier. We adopt the one vs. 
others strategy and select the one with the largest 
margin as the final answer. The training parame-
ters are chosen using cross-validation (C=2.4 
(SVM); ? =0.4(tree kernel)). In our implementa-
tion, we use the binary SVMLight (Joachims, 
1998) and Tree Kernel Tools (Moschitti, 2004). 
Precision (P), Recall (R) and F-measure (F) are 
adopted to measure the performance. 
4.2 Experimental Results 
In this subsection, we report the experiments of 
different kernel setups for different purposes. 
 
(1) Tree Kernel only over Different Relation 
Instance Spaces: In order to better study the im-
pact of the syntactic structure information in a 
parse tree on relation extraction, we remove the 
entity-related information from parse trees by 
replacing the entity-related phrase types (?E1-
PER? and so on as shown in Fig. 1) with ?NP?. 
Table 1 compares the performance of 5 tree ker-
nel setups on the ACE 2003 data using the tree 
structure information only. It shows that:  
 
? Overall the five different relation instance 
spaces are all somewhat effective for relation 
extraction. This suggests that structured syntactic 
information has good predication power for rela-
tion extraction and the structured syntactic in-
formation can be well captured by the tree kernel.  
? MCT performs much worse than the others. 
The reasons may be that MCT includes too 
much left and right context information, which 
may introduce many noisy features and cause 
over-fitting (high precision and very low recall 
as shown in Table 1). This suggests that only 
keeping the complete (not partial) production 
rules in MCT does harm performance. 
? PT achieves the best performance. This means 
that only keeping the portion of a parse tree en-
closed by the shortest path between entities can 
model relations better than all others. This may 
be due to that most significant information is 
with PT and including context information may 
introduce too much noise. Although context 
may include some useful information, it is still a 
problem to correctly utilize such useful informa-
tion in the tree kernel for relation extraction. 
? CPT performs a bit worse than PT. In some 
cases (e.g. in sentence ?the merge of company A 
and company B?.?, ?merge? is a critical con-
text word), the context information is helpful. 
However, the effective scope of context is hard 
to determine given the complexity and variabil-
ity of natural languages. 
? The two flattened trees perform worse than the 
original trees. This suggests that the single non-
terminal nodes are useful for relation extraction.  
Evaluation on the ACE 2004 data also shows 
that PT achieves the best performance (72.5/56.7 
/63.6 in P/R/F). More evaluations with the entity 
type and order information incorporated into tree 
nodes (?E1-PER?, ?E2-PER? and ?E-GPE? as 
shown in Fig. 1) also show that PT performs best 
with 76.1/62.6/68.7 in P/R/F on the 2003 data 
and 74.1/62.4/67.7 in P/R/F on the 2004 data. 
 
Instance Spaces P(%) R(%) F 
Minimum Complete Tree 
(MCT) 77.5 38.4 51.3 
Path-enclosed Tree (PT) 72.8 53.8 61.9 
Context-Sensitive PT(CPT) 75.9 48.6 59.2 
Flattened PT 72.7 51.7 60.4 
Flattened CPT 76.1 47.2 58.2 
 
Table 1. five different tree kernel setups on the 
ACE 2003 five major types using the parse 
tree structure information only (regardless of 
any entity-related information) 
829
PTs (with Tree Struc-
ture Information only) 
P(%) R(%) F 
Entity kernel only 75.1 
(79.5) 
42.7 
(34.6)
54.4 
(48.2) 
Tree kernel only 72.5 
(72.8) 
56.7 
(53.8)
63.6 
(61.9) 
Composite kernel 1 
(linear combination) 
73.5 
(76.3) 
67.0 
(63.0)
70.1 
(69.1) 
Composite kernel 2 
(polynomial expansion)
76.1 
(77.3) 
68.4 
(65.6)
72.1 
(70.9) 
 
Table 2. Performance comparison of different 
kernel setups over the ACE major types of 
both the 2003 data (the numbers in parenthe-
ses) and the 2004 data (the numbers outside 
parentheses) 
 
(2) Composite Kernels: Table 2 compares the 
performance of different kernel setups on the 
ACE major types. It clearly shows that:  
? The composite kernels achieve significant per-
formance improvement over the two individual 
kernels. This indicates that the flat and the struc-
tured features are complementary and the com-
posite kernels can well integrate them: 1) the 
flat entity information captured by the entity 
kernel; 2) the structured syntactic connection 
information between the two entities captured 
by the tree kernel. 
 
? The composite kernel via the polynomial ex-
pansion outperforms the one via the linear com-
bination by ~2 in F-measure. It suggests that the 
bi-gram entity features are very useful.  
 
? The entity features are quite useful, which can 
achieve F-measures of 54.4/48.2 alone and can 
boost the performance largely by ~7 (70.1-
63.2/69.1-61.9) in F-measure when combining 
with the tree kernel.  
 
? It is interesting that the ACE 2004 data shows 
consistent better performance on all setups than 
the 2003 data although the ACE 2003 data is 
two times larger than the ACE 2004 data. This 
may be due to two reasons: 1) The ACE 2004 
data defines two new entity types and re-defines 
the relation types and subtypes in order to re-
duce the inconsistency between LDC annota-
tors. 2) More importantly, the ACE 2004 data 
defines 43 entity subtypes while there are only 3 
subtypes in the 2003 data. The detailed classifi-
cation in the 2004 data leads to significant per-
formance improvement of 6.2 (54.4-48.2) in F-
measure over that on the 2003 data. 
Our composite kernel can achieve 
77.3/65.6/70.9 and 76.1/68.4/72.1 in P/R/F over 
the ACE 2003/2004 major types, respectively. 
Methods (2002/2003 data) P(%) R(%) F 
Ours: composite kernel 2 
(polynomial expansion) 
77.3 
(64.9) 
65.6 
(51.2) 
70.9 
(57.2) 
Zhou et al (2005):  
feature-based SVM 
77.2 
(63.1) 
60.7 
(49.5) 
68.0 
(55.5) 
Kambhatla (2004):  
feature-based ME 
 (-) 
(63.5) 
 (-) 
(45.2) 
 (-) 
(52.8) 
Ours: tree kernel with en-
tity information at node 
76.1 
(62.4) 
62.6 
(48.5) 
68.7 
(54.6) 
Bunescu and Mooney 
(2005): shortest path de-
pendency kernel 
65.5 
(-) 
43.8 
(-) 
52.5 
(-) 
Culotta and Sorensen 
(2004): dependency kernel 
67.1 
(-) 
35.0 
(-) 
45.8 
(-) 
 
Table 3. Performance comparison on the ACE 
2003/2003 data over both 5 major types (the 
numbers outside parentheses) and 24 subtypes 
(the numbers in parentheses)  
 
Methods (2004 data) P(%) R(%) F 
Ours: composite kernel 2 
(polynomial expansion) 
76.1 
 (68.6) 
68.4 
(59.3)
72.1 
 (63.6)
Zhao and Grishman (2005): 
feature-based kernel 
69.2 
(-) 
70.5 
(-) 
70.4 
(-) 
 
Table 4. Performance comparison on the ACE 
2004 data over both 7 major types (the numbers 
outside parentheses) and 23 subtypes (the num-
bers in parentheses) 
 
(3) Performance Comparison: Tables 3 and 4 
compare our method with previous work on the 
ACE 2002/2003/2004 data, respectively. They 
show that our method outperforms the previous 
methods and significantly outperforms the previ-
ous two dependency kernels4. This may be due to 
two reasons: 1) the dependency tree (Culotta and 
Sorensen, 2004) and the shortest path (Bunescu 
and Mooney, 2005) lack the internal hierarchical 
phrase structure information, so their correspond-
ing kernels can only carry out node-matching 
directly over the nodes with word tokens; 2) the 
parse tree kernel has less constraints. That is, it is 
                                                 
4 Bunescu and Mooney (2005) used the ACE 2002 corpus, 
including 422 documents, which is known to have many 
inconsistencies than the 2003 version. Culotta and Sorensen 
(2004) used a generic ACE corpus including about 800 
documents (no corpus version is specified). Since the testing 
corpora are in different sizes and versions, strictly speaking, 
it is not ready to compare these methods exactly and fairly. 
Therefore Table 3 is only for reference purpose. We just 
hope that we can get a few clues from this table. 
830
not restricted by the two constraints of the two 
dependency kernels (identical layer and ances-
tors for the matchable nodes and identical length 
of two shortest paths, as discussed in Section 2).  
 
The above experiments verify the effective-
ness of our composite kernels for relation extrac-
tion. They suggest that the parse tree kernel can 
effectively explore the syntactic features which 
are critical for relation extraction.  
 
# of error instances Error Type 
  2004 data 2003 data 
False Negative 198  416 
False Positive 115 171 
Cross Type 62 96 
 
Table 5. Error distribution of major types on 
both the 2003 and 2004 data for the compos-
ite kernel by polynomial expansion 
 
(4) Error Analysis: Table 5 reports the error 
distribution of the polynomial composite kernel 
over the major types on the ACE data. It shows 
that 83.5%(198+115/198+115+62) / 85.8%(416 
+171/416+171+96) of the errors result from rela-
tion detection and only 16.5%/14.2% of the er-
rors result from relation characterization. This 
may be due to data imbalance and sparseness 
issues since we find that the negative samples are 
8 times more than the positive samples in the 
training set. Nevertheless, it clearly directs our 
future work. 
5 Discussion 
In this section, we compare our method with the 
previous work from the feature engineering 
viewpoint and report some other observations 
and issues in our experiments. 
5.1 Comparison with Previous Work 
This is to explain more about why our method 
performs better and significantly outperforms the 
previous two dependency tree kernels from the 
theoretical viewpoint. 
(1) Compared with Feature-based Methods: 
The basic difference lies in the relation instance 
representation (parse tree vs. feature vector) and 
the similarity calculation mechanism (kernel 
function vs. dot-product). The main difference is 
the different feature spaces. Regarding the parse 
tree features, our method implicitly represents a 
parse tree by a vector of integer counts of each 
sub-tree type, i.e., we consider the entire sub-tree 
types and their occurring frequencies. In this way, 
the parse tree-related features (the path features 
and the chunking features) used in the feature-
based methods are embedded (as a subset) in our 
feature space. Moreover, the in-between word 
features and the entity-related features used in 
the feature-based methods are also captured by 
the tree kernel and the entity kernel, respectively. 
Therefore our method has the potential of effec-
tively capturing not only most of the previous 
flat features but also the useful syntactic struc-
ture features. 
 
(2) Compared with Previous Kernels: Since 
our method only counts the occurrence of each 
sub-tree without considering the layer and the 
ancestors of the root node of the sub-tree, our 
method is not limited by the constraints (identi-
cal layer and ancestors for the matchable nodes, 
as discussed in Section 2) in Culotta and Soren-
sen (2004). Moreover, the difference between 
our method and Bunescu and Mooney (2005) is 
that their kernel is defined on the shortest path 
between two entities instead of the entire sub-
trees. However, the path does not maintain the 
tree structure information. In addition, their ker-
nel requires the two paths to have the same 
length. Such constraint is too strict. 
5.2 Other Issues 
(1) Speed Issue: The recursively-defined convo-
lution kernel is much slower compared to fea-
ture-based classifiers. In this paper, the speed 
issue is solved in three ways. First, the inclusion 
of the entity kernel makes the composite kernel 
converge fast. Furthermore, we find that the 
small portion (PT) of a full parse tree can effec-
tively represent a relation instance. This signifi-
cantly improves the speed. Finally, the parse tree 
kernel requires exact match between two sub-
trees, which normally does not occur very fre-
quently. Collins and Duffy (2001) report that in 
practice, running time for the parse tree kernel is 
more close to linear (O(|N1|+|N2|), rather than 
O(|N1|*|N2| ). As a result, using the PC with Intel 
P4 3.0G CPU and 2G RAM, our system only 
takes about 110 minutes and 30 minutes to do 
training on the ACE 2003 (~77k training in-
stances) and 2004 (~33k training instances) data, 
respectively.  
(2) Further Improvement: One of the potential 
problems in the parse tree kernel is that it carries 
out exact matches between sub-trees, so that this 
kernel fails to handle sparse phrases (i.e. ?a car? 
vs. ?a red car?) and near-synonymic grammar 
tags (for example, the variations of a verb (i.e. 
go, went, gone)). To some degree, it could possi-
bly lead to over-fitting and compromise the per-
831
formance. However, the above issues can be 
handled by allowing grammar-driven partial rule 
matching and other approximate matching 
mechanisms in the parse tree kernel calculation. 
Finally, it is worth noting that by introducing 
more individual kernels our method can easily 
scale to cover more features from a multitude of 
sources (e.g. Wordnet, gazetteers, etc) that can 
be brought to bear on the task of relation extrac-
tion. In addition, we can also easily implement 
the feature weighting scheme by adjusting the 
eqn.(2) and the rule (2) in calculating 1 2( , )n n?  
(see subsection 3.1). 
6 Conclusion and Future Work 
Kernel functions have nice properties. In this 
paper, we have designed a composite kernel for 
relation extraction. Benefiting from the nice 
properties of the kernel methods, the composite 
kernel could well explore and combine the flat 
entity features and the structured syntactic fea-
tures, and therefore outperforms previous best-
reported feature-based methods on the ACE cor-
pus. To our knowledge, this is the first research 
to demonstrate that, without the need for exten-
sive feature engineering, an individual tree ker-
nel achieves comparable performance with the 
feature-based methods. This shows that the syn-
tactic features embedded in a parse tree are par-
ticularly useful for relation extraction and which 
can be well captured by the parse tree kernel. In 
addition, we find that the relation instance repre-
sentation (selecting effective portions of parse 
trees for kernel calculations) is very important 
for relation extraction. 
The most immediate extension of our work is 
to improve the accuracy of relation detection. 
This can be done by capturing more features by 
including more individual kernels, such as the 
WordNet-based semantic kernel (Basili et al, 
2005) and other feature-based kernels. We can 
also benefit from machine learning algorithms to 
study how to solve the data imbalance and 
sparseness issues from the learning algorithm 
viewpoint. In the future work, we will design a 
more flexible tree kernel for more accurate simi-
larity measure.  
 
Acknowledgements: We would like to thank 
Dr. Alessandro Moschitti for his great help in 
using his Tree Kernel Toolkits and fine-tuning 
the system. We also would like to thank the three 
anonymous reviewers for their invaluable sug-
gestions. 
References 
ACE. 2002-2005. The Automatic Content Extraction 
Projects. http://www.ldc.upenn.edu/Projects /ACE/ 
Basili R., Cammisa M. and Moschitti A. 2005. A Se-
mantic Kernel to classify text with very few train-
ing examples. ICML-2005 
Bunescu R. C. and Mooney R. J. 2005. A Shortest 
Path Dependency Kernel for Relation Extraction. 
EMNLP-2005 
Charniak E. 2001. Immediate-head Parsing for Lan-
guage Models. ACL-2001 
Collins M. and Duffy N. 2001. Convolution Kernels 
for Natural Language. NIPS-2001 
Culotta A. and Sorensen J. 2004. Dependency Tree 
Kernel for Relation Extraction. ACL-2004 
Haussler D. 1999. Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, 
University of California, Santa Cruz. 
Joachims T. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Kambhatla N. 2004. Combining lexical, syntactic and 
semantic features with Maximum Entropy models 
for extracting relations. ACL-2004 (poster) 
Lodhi H., Saunders C., Shawe-Taylor J., Cristianini 
N. and Watkins C. 2002. Text classification using 
string kernel. Journal of Machine Learning Re-
search, 2002(2):419-444 
Miller S., Fox H., Ramshaw L. and Weischedel R. 
2000. A novel use of statistical parsing to extract 
information from text. NAACL-2000 
Moschitti A. 2004. A Study on Convolution Kernels 
for Shallow Semantic Parsing. ACL-2004 
MUC. 1987-1998. http://www.itl.nist.gov/iaui/894.02/ 
related_projects/muc/ 
Sch?lkopf B. and Smola A. J. 2001. Learning with 
Kernels: SVM, Regularization, Optimization and 
Beyond. MIT Press, Cambridge, MA 407-423 
Suzuki J., Hirao T., Sasaki Y. and Maeda E. 2003. 
Hierarchical Directed Acyclic Graph Kernel: 
Methods for Structured Natural Language Data. 
ACL-2003 
Zelenko D., Aone C. and Richardella A. 2003. Kernel 
Methods for Relation Extraction. Journal of Ma-
chine Learning Research. 2003(2):1083-1106 
Zhao S.B. and Grishman R. 2005. Extracting Rela-
tions with Integrated Information Using Kernel 
Methods. ACL-2005 
Zhou G.D., Su J, Zhang J. and Zhang M. 2005. Ex-
ploring Various Knowledge in Relation Extraction. 
ACL-2005 
832
Effective Adaptation of a Hidden Markov Model-based Named Entity 
Recognizer for Biomedical Domain 
Dan Shen?? Jie Zhang?? Guodong Zhou? Jian Su? Chew-Lim Tan?
? Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
? Department of Computer Science 
National University of Singapore 
3 Science Drive 2, Singapore 117543 
{shendan,zhangjie,zhougd,sujian}@i2r.a-star.edu.sg 
{tancl}@comp.nus.edu.sg 
 
 
Abstract 
In this paper, we explore how to adapt a 
general Hidden Markov Model-based 
named entity recognizer effectively to 
biomedical domain.  We integrate various 
features, including simple deterministic 
features, morphological features, POS 
features and semantic trigger features, to 
capture various evidences especially for 
biomedical named entity and evaluate 
their contributions.  We also present a 
simple algorithm to solve the abbreviation 
problem and a rule-based method to deal 
with the cascaded phenomena in biomedi-
cal domain.  Our experiments on GENIA 
V3.0 and GENIA V1.1 achieve the 66.1 
and 62.5 F-measure respectively, which 
outperform the previous best published 
results by 8.1 F-measure when using the 
same training and testing data.  
1 Introduction 
As the research in biomedical domain has grown 
rapidly in recent years, a huge amount of nature 
language resources have been developed and be-
come a rich knowledge base.  The technique of 
named entity (NE) recognition (NER) is strongly 
demanded to be applied in biomedical domain.  
Since in previous work, many NER systems have 
been applied successfully in newswire domain 
(Zhou and Su 2002; Bikel et al 1999; Borthwich et 
al. 1999), more and more explorations have been 
done to port existing NER system into biomedical 
domain (Kazama et al 2002; Takeuchi et al 2002; 
Nobata et al 1999 and 2000; Collier et al 2000; 
Gaizauskas et al 2000; Fukuda et al 1998; Proux 
et al 1998).  However, compared with those in 
newswire domain, these systems haven?t got high 
performance.  It is probably because of the follow-
ing factors of biomedical NE (Zhang et al 2003): 
1. Some modifiers are often before basic NEs, 
e.g. activated B cell lines, and sometimes biomedi-
cal NEs are very long, e.g. 47 kDa sterol regula-
tory element binding factor.  This kind of factor 
highlights the difficulty for identifying the bound-
ary of NE. 
2. Two or more NEs share one head noun by 
using conjunction or disjunction construction, e.g. 
91 and 84 kDa proteins.  It is hard to identify these 
NEs respectively. 
3. An entity may be found with various spelling 
forms, e.g. N-acetylcysteine, N-acetyl-cysteine, 
NAcetylCysteine, etc.  Since the use of capitaliza-
tion is casual, the capitalization information may 
not be so evidential in this domain. 
4. NE may be cascaded.  One NE may be em-
bedded in another NE, e.g. <PROTEIN><DNA> 
kappa 3</DNA> binding factor </PROTEIN>.  
More effort must be made to identify this kind of 
NE. 
5. Abbreviations are frequently used in bio-
medical domain, e.g. TCEd, IFN, TPA, etc.  Since 
abbreviations don?t have many evidences for cer-
tain NE class, it is difficult to classify them cor-
rectly. 
These factors above make NER in biomedical 
domain difficult.  Therefore, it is necessary to ex-
plore more evidential features and more effective 
methods to cope with such difficulties. 
In this paper, we will study how to adapt a gen-
eral Hidden Markov Model (HMM)-based NE rec-
ognizer (Zhou and Su 2002) to biomedical domain.  
We specially explore various evidences for bio-
medical NE and propose methods to cope with ab-
breviations and cascaded phenomena.  As a result, 
features (simple deterministic features, morpho-
logical features, part-of-speech features and head 
noun trigger features) and methods (abbreviation 
recognition algorithm and rule-based cascaded 
phenomena resolution) are integrated in our system.  
The experiment shows that system outperforms the 
best published system by 8.1 F-measure. 
In Section 2, we will introduce the HMM-
based NE recognizer briefly.  In Section 3, we will 
focus on the features that we have used.  The 
methods and the adaptations of different features 
will be discussed in detail.  In Section 5 and 6, we 
will present the solutions of abbreviation and cas-
caded phenomena. Finally, our experiment results 
will be presented and the contributions of different 
features will be analyzed in Section 7. 
2 
3 
3.1 
HMM-based Named Entity Recognizer 
Our system is adapted from a HMM-based NE 
recognizer, which has been proved very effective 
in MUC (Zhou and Su 2002). 
The purpose of HMM is to find the most likely 
tag sequence T for a given sequence 
of tokens G  that maxi-
mizes . 
n
n ttt ???= 211
n gg ?= 211
)1
nG
ng??
|( 1
nTP
In token sequence G , the token g  is defined 
as , where w is the word and is 
the feature set related with the word . 
n
1 i
i
>=< iii wfg , i if
w
In tag sequenceT , each tag consists of three 
parts: 1. Boundary category, which denotes the 
position of the current word in NE.  2. Entity cate-
gory, which indicates the NE class.  3. Feature set, 
which will be discussed in Section 3. 
n
1 it
When we incorporate a plentiful feature set in 
HMM, we will encounter data sparseness problem.  
An alternative back-off modeling approach by 
means of constraint relaxation is applied in our 
model (Zhou and Su 2002).  It enables the decod-
ing process effectively find a near optimal fre-
quently occurred pattern entry in determining the 
NE tag probability distribution of current word. 
Finally, the Viterbi algorithm (Viterbi 1967) is 
implemented to find the most likely tag sequence 
in the state space of the possible tag distribution 
based on the state transition probabilities.  Fur-
thermore, some constraints on the boundary cate-
gory and entity category between two consecutive 
tags are applied to filter the invalid NE tags (Zhou 
and Su 2002). 
Feature Set 
Simple Deterministic Features (Fsd) 
The purpose of simple deterministic features is to 
capture the capitalization, digitalization and word 
formation information.  This kind of features have 
been widely used in both newswire NER system, 
such as (Zhou and Su 2002), and biomedical NER 
system, such as (Nobata et al 1999; Gaizauskas et 
al. 2000; Collier et al 2000; Takeuchi and Collier 
2002; Kazama et al 2002).  Based on the charac-
teristics of biomedical NEs, we designed simple 
deterministic features manually.  Table 1 shows the 
simple deterministic features with descending or-
der of priority. 
 
Fsd Name Example 
Comma , 
Dot . 
LRB ( 
RRB ) 
LSB [ 
RSB ] 
RomanDigit II 
GreekLetter Beta 
StopWord in, at 
ATCGsequence AACAAAG 
OneDigit 5 
AllDigits 60 
DigitCommaDigit 1,25 
DigitDotDigit 0.5 
OneCap T 
AllCaps CSF 
CapLowAlpha All 
CapMixAlpha IgM 
LowMixAlpha kDa 
AlphaDigitAlpha H2A 
AlphaDigit T4 
DigitAlphaDigit 6C2 
DigitAlpha 19D 
Table 1: Simple deterministic features 
From Table 1, we can find that: 
1. Features such as comma, dot, StopWord, etc. 
are designed intuitively to provide information to 
detect the boundary of NE. 
2. Features Parenthesis is often used to indicate 
the definition of abbreviation in biomedical docu-
ments. 
3. Features GreekLetter and RomanDigit are 
specially designed to capture the symbols 
frequently occurred in biomedical NE. 
4. Feature ATCG sequence identify the similar-
ity of words according to their word formations, 
e.g. AACAAAG, CTCAGGA, etc. 
5. Features dealing with mixed alphabets and 
digits such as AlphaDigitAlpha, CapMixAlpha, etc. 
are beneficial for biomedical abbreviations. 
Furthermore, we evaluate these features and 
compare with those used in MUC (Zhou and Su, 
2002).  The reported result of the simple determi-
nistic features used in MUC can achieve F-
measure of 74.1 (Zhou and Su 2002), but when 
they are used in biomedical domain, they only get 
F-measure of 24.3.  By contrast, using the simple 
deterministic features we designed for biomedical 
NER, the system achieves F-measure of 29.4.  Ac-
cording to the comparison, some findings may be 
concluded as follows: 
1) Simple deterministic features are domain de-
pendent, which suggests that it is necessary to de-
sign special features for biomedical NER. 
2) Simple deterministic features have weaker 
predictive power for NE classes in biomedical do-
main than in newswire domain. 
3.2 Morphological Feature (Fm) 
Morphological information, such as prefix/suffix, 
is considered as an important cue for terminology 
identification.  In our system, we get most frequent 
100 prefixes and suffixes from training data as 
candidates.  Then, each of these candidates is 
evaluated according to formula f1.  ( )
i
ii
i N
OUTIN
Wt
## ?=   (f1) 
in which, #INi is the number that prefix/suffix i 
occurs within NEs; #OUTi is the number that pre-
fix/suffix i occurs out of NEs; Ni is the total num-
ber of prefix/suffix i. 
The formula assumes that the particular pre-
fix/suffix, which is most likely inside NEs and 
least likely outside NEs, may be thought as a good 
evidence for distinguishing the NEs.  The candi-
dates with Wt above a certain threshold (0.7 in ex-
periment) are chosen.  Then, we calculated the 
frequency of each prefix/suffix in each NE class 
and group the prefixes/suffixes with the similar 
distribution among NE classes into one feature.  
This is because prefixes/suffixes with the similar 
distribution have the similar contribution, and it 
will avoid suffering from the data sparseness prob-
lem.  Some of morphological features were listed 
in Table 2. 
 
Fm Name Prefix/Suffix Example 
sOOC ~cin actinomycin 
 ~mide Cycloheximide 
 ~zole Sulphamethoxazole 
sLPD ~lipid Phospholipids 
 ~rogen Estrogen 
 ~vitamin dihydroxyvitamin 
sCTP ~blast erythroblast 
 ~cyte thymocyte 
 ~phil eosinophil 
sPEPT ~peptide neuropeptide 
sMA ~ma hybridoma 
sVIR ~virus cytomegalovirus 
Table 2: Examples of morphological features 
 
From Table 2, the suffixes ~cin, ~mide, ~zole 
have been grouped into one feature sOOC because 
they all have the high frequency in the NE class 
OtherOrganicCompound and relatively low fre-
quencies in the other NE classes.   In our system, 
totally 37 prefixes and suffixes were selected and 
grouped to 23 features. 
3.3 Part-of-Speech Features (Fpos) 
In the previous NER research in newswire domain, 
part-of-speech (POS) features were stated not use-
ful, as POS features may affect the use of some 
important capitalization information (Zhou and Su 
2002).  However, since more and more words with 
lower case are included in NEs, capitalization in-
formation in biomedical domain is not as eviden-
tial as it in newswire domain (Zhang et al 2003).  
Moreover, since many biomedical NEs are descrip-
tive and long, identifying NE boundary is not a 
trivial task.  POS tagging can provide the evidence 
of noun phrase region based on word syntactic in-
formation and the noun phrases are most likely to 
be NE.  Therefore, we reconsidered the POS tag-
ging.   
In previous research, (Kazama et al 2002) 
make use of POS information and conclude that it 
only slightly improves performance.  Moreover, 
(Collier et al 2000; Nobata et al 2000; Takeuchi 
and Collier. 2002) don?t incorporate POS informa-
tion in their systems.  The probable reason ex-
plained by them is that since POS tagger they used 
is trained on newswire articles, the assigned POS 
tags are often incorrect in biomedical documents.  
On the whole, it can be concluded that POS infor-
mation hasn?t been well used in previous work. 
In our experiment, a POS tagger was trained us-
ing 80% of GENIA V2.1 corpus (536 abstracts, 
123K words) and evaluated on the rest 20% (134 
abstracts, 29K words).  We use GENIA corpus to 
train the POS tagger in order to let it be adapted for 
biomedical domain.  As for comparison, we also 
trained the POS tagger on Wall Street Journal arti-
cles (2500 articles, 756K words) and tested on the 
20% of GENIA corpus.  The results are shown in 
Table 3. 
 
Training set Testing set Precision 
2500 WSJ articles 84.31 
536 GENIA abstracts 
134 GENIA 
abstracts 97.37 
Table 3: Comparison of POS tagger using dif-
ferent training data  
 
From Table 3, it can be found that POS tagger 
trained on the biomedical documents performs 
much better on the biomedical testing documents 
than that trained on WSJ articles.  This is consis-
tent with earlier explanation for why POS features 
are not so useful in biomedical NER (Nobata et al 
2000; Takeuchi and Collier 2002).   
3.4 Semantic Trigger Features 
Semantic trigger features are collected to capture 
the evidence of certain NE class based on the se-
mantic information of some key words.  Initially, 
we design two types of semantic triggers: head 
noun triggers and special verb triggers. 
3.4.1 Head Noun Triggers (Fhnt) 
Head noun means the main noun or noun phrase of 
some compound words and describes the function 
or the property, e.g. ?B cells? is the head noun for 
the NE ?activated human B cells?.  Compared with 
the other words in NE, head noun is a much more 
decisive factor for distinguishing NE classes.  For 
instance, 
<OtherName>IFN-gamma treatment</OtherName> 
<DNA>IFN-gamma activation sequence</DNA> 
In our work, we extract uni-gram and bi-grams 
of head nouns automatically from training data, 
and rank them by frequency.  According to the ex-
periment, we selected 60% top ranked head nouns 
as trigger features for each NE class.  Some exam-
ples are shown in Table 4. 
In the future application, we may also extract 
the head nouns from some public resources to en-
hance the triggers. 
 
1-gram 2-grams 
PROTEIN 
interleukin activator protein 
interferon binding protein 
kinase cell receptor 
ligand gene product 
CELL TYPE 
lymphocyte blast cell 
astrocyte blood lymphocyte 
eosinophil killer cell 
fibroblast peripheral monocyte 
DNA 
DNA X chromosome 
breakpoint alpha promoter 
cDNA binding motif 
chromosome promoter element 
Table 4: Examples of head noun triggers 
3.4.2 Special Verb Triggers (Fsvt) 
Besides collecting the triggers, such as head noun 
triggers, from the NEs themselves, we also extract 
the triggers from the local contexts of the NEs.  
Recently, some frequently occurred verbs in bio-
medical document have been proved useful for 
extracting the interaction between entities (Thomas 
et al 2000; Sekimizu et al 1998).  In biomedical 
NER, we have the intuition that particular verbs 
may also provide the evidence for boundary and 
NE class.  For instance, the verb bind is often used 
to indicate the interaction between proteins. 
In our system, we selected 20 most frequent 
verbs which occur adjacent to NE from training 
data automatically as the verb trigger features, 
which is shown in Table 5.   
 
 
Special Verb Triggers 
activate express 
bind induce 
inhibit interact 
regulate stimulate 
Table 5: Examples of special verb triggers 
4 Method for Abbreviation Recognition 
Abbreviations are widely used in biomedical do-
main.  Identifying the class of them constitutes an 
important and difficult problem (Zhang et al 2003). 
In our current system, we incorporate a method 
to classify abbreviation by mapping the abbrevia-
tion to its full form. This approach is based on the 
assumption that it is easier to classify the full form 
than abbreviation.  In most cases, this assumption 
is valid because the full form has more evidences 
than its abbreviation to capture the NE class.  
Moreover, if we can map the abbreviation to its 
full form in the current document, the recognized 
abbreviation is still helpful for classifying the same 
forthcoming abbreviations in the same document, 
as in (Zhou and Su 2002). 
In practice, abbreviation and its full form often 
occur simultaneously with parenthesis when first 
appear in biomedical documents.  There are two 
cases: 
1. full form (abbreviation) 
2. abbreviation (full form) 
Most patterns conform to the first case and if 
the content inside the parenthesis includes more 
than two words, the second case is assumed 
(Schwartz and Hearst 2003).   
In these two cases, the use of parenthesis is 
both evidential and confusing.  On one hand, it is 
evidential because it can provide the indication to 
map the abbreviation to its full form.  On the other 
hand, it is confusing because it makes the annota-
tion of NE more complicated.  Sometimes, the ab-
breviation and its full form are annotated 
separately, such as  
<CellType>human mononulear leuko-
cytes</CellType>(<CellType>hMNL</CellType>), 
and sometimes, they are all embedded in the whole 
entity, such as 
<OtherName>leukotriene B4 (LTB4) genera-
tion</OtherName>.   
Therefore, parenthesis needs to be treated specially.  
We develop an abbreviation recognition algorithm 
described in Figure 1. 
In preprocessing stage, we remove the abbre-
viations and parentheses from the sentence, when 
the abbreviation is first defined.  This measure will 
make the annotation simpler and the NE recognizer 
more effective.  The main work in this stage is to 
judge which case the current pattern belongs to and 
record the original positions of the abbreviation 
and parenthesis. 
After applying the HMM-based NE recognizer 
to the sentence, we restore the abbreviation and 
parenthesis to the original position in the sentence.  
Next, the abbreviation is classified.  There are two 
priorities of the class (from high to low): the class 
of its full form identified by the recognizer, and the 
class of the abbreviation itself identified by the 
recognizer.  At last, the same abbreviation occur-
ring in the rest sentences of the current document 
are assigned the same NE class.   
 
for each sentence Si in the document{ 
if exist parenthesis{ 
judge the case of { 
?full form (abbr.)?; 
?abbr. (full form)?; 
} 
store the abbr. A and position Pa  to a list; 
record the parenthesis position Pp; 
remove A and parenthesis from sentence; 
apply HMM-based NE recognizer to Si; 
restore A and parenthesis into Pa, Pp; 
if Pp within an identified NE E with the class CE 
parenthesis is included in E; 
else{ 
parenthesis is not included; 
   classify A to CE; 
   classify A in the rest part of document to CE; 
} 
} 
else apply HMM-based NE recognizer to Si; 
} 
Figure 1: Abbreviation recognition algorithm 
5 Solution of Cascaded Phenomena 
In (Zhang et al 2003), they state that 16.57% of 
NEs in GENIA V3.0 have cascaded annotations, 
such as  
<RNA><DNA>CIITA</DNA> mRNA</RNA>.   
Currently, we only consider the longest NE and 
ignore the embedded NEs.   
Based on the features described in section 3, 
our system counters some problems when dealing 
with cascaded NEs.  The probable reason is that 
the features we used are not so effective for this 
kind of NEs.   
For instance, POS is based on the assumption 
that NE is most likely to be a noun phrase.  For 
cascaded NE, this assumption may not always be 
valid because one NE may consist of two or more 
noun phrases connected by some special words, 
such as TSH receptor specific T cell lines. 
Moreover, in section 3.4.1, we have shown that 
head noun is the significant clue for distinguishing 
NE classes.  Even for cascaded NEs, head noun 
features are still effective to some extent, such as 
IL-2 mRNA.  However, cascaded NEs sometimes 
contain two or more head nouns, which belong to 
different NE classes.  For example, <DNA>IgG Fc 
receptor type IC gene</DNA>, in which receptor 
is the head noun of protein and gene is the head 
noun of DNA.  In general, the latter head noun will 
be more important.  Unfortunately, it seems that 
sometimes the shorter NE is more possible to be 
identified, such as <protein>IgG Fc recep-
tor</protein> type IC gene.   
On the whole, we have to explore an additional 
method to cope with the cascaded phenomena 
separately.  In our experiment, we attempt to solve 
this problem based on some rules. 
In GENIA corpus, we find that there are four 
basic types of cascaded NEs: 
1. < <NE> head noun >  
2. < modifier <NE> > 
3. < <NE1> <NE2> > 
4. < <NE1> word <NE2> > 
Moreover, these cascaded NEs may be generated 
iteratively.  For instance, 
5. < modifier <NE> head noun > 
6. < <NE1> <NE2> head noun > 
The rules are constructed automatically from 
the cascaded NEs in training data.  Corresponding 
to the four basic types of cascaded NEs mentioned 
before, we propose four patterns and apply them 
iteratively in each sentence: 
1. <entity1> head noun ? <entity2>  
e.g. <Protein> binding motif ? <DNA> 
2. <entity1> <entity2> ? <entity3> 
e.g. <Lipid> <Protein> ? <Protein> 
3. modifier <entity1> ? <entity2> 
e.g. anti <Protein> ? <Protein> 
4. <entity1> word <entity2> ? <entity3> 
e.g. <Virus> infected <Multicell> ? <Multicell> 
In our system, 102 rules are incorporated to 
classify the cascaded NEs. 
6 
6.1 
6.2 
Experiments 
GENIA Corpus 
GENIA corpus is the largest annotated corpus in 
molecular biology domain available to public 
(Ohta et al 2002).  In our experiment, three ver-
sions are used: 
? GENIA Version 1.1 (V1.1) -- It contains 670 
MEDLINE abstracts.  Since a lot of previous re-
lated work used this version, we use it to compare 
our result with others?. 
? GENIA Version 2.1 (V2.1) -- It contains the 
same 670 abstracts as V1.1 and POS tagging.  We 
use it to train and evaluate our POS tagger. 
? GENIA Version 3.0 (V3.0) -- It contains 2000 
abstracts, which is the superset of V1.1.  We use it 
to get the latest result and find out the effect of 
training data size. 
The annotation of NE is based on the GENIA 
ontology.  In our task, we use 23 distinct NE 
classes.  As for the conjunctive and disjunctive 
NEs, we ignore such cases and take the whole con-
struction as one entity.  In addition, for the cas-
caded annotations in V3.0, currently, we only 
consider the longest one level of the annotations. 
Experimental Results 
The system is evaluated using standard ?preci-
sion/recall/F-measure?, in which ?F-measure? is 
defined as F-measure = (2PR) / (P+R). 
We evaluate our NER system on both V3.0 and 
V1.1, each of which has been split into a training 
set and a testing set.  As for V1.1, we divide the 
corpus into 590 abstracts (136K words) as training 
set and the rest 80 abstracts (17K words) as testing 
set.  As for V3.0, we use the same testing set as 
V1.1 and the rest 1920 abstracts (447K words) as 
training set. 
 
Corpus P R F 
Our system on V3.0 66.5 65.7 66.1 
Our system on V1.1 63.8 61.3 62.5 
Kazama?s on V1.1 56.2 52.8 54.4 
Table 6: Comparison of overall performance 
 
Table 6 shows the overall performance of our 
system on V3.0 and V1.1, and the best reported 
system on V1.1 described in (Kazama et al 2002).  
On V1.1, we use the same training and testing data 
and capture the same NE classes as (Kazama et al 
2002).  Our system (62.5 F-measure) outperforms 
Kazama?s (54.4 F-measure) by 8.1 F-measure.  
This probably benefits from the various evidential 
features and the effective methods we proposed.  
Furthermore, as our expectation, the performance 
achieved on V3.0 (66.1 F-measure) is better than 
that on V1.1 (62.5 F-measure), which indicate that 
our system still has some room for improvement 
with the larger training data set. 
 
 
Figure 2: Performance of each NE class 
 
In addition, Figure 2 shows the detailed per-
formance chart of each NE class on V3.0.  In the 
figure, the numbers in the parenthesis are the num-
ber that NEs of that class occur in training/testing 
data.  It can be found that the performances vary a 
lot among the NE classes.  Some NE classes that 
have very few training data, such as Carbohydrate 
and Organism, get extremely low performance.  
In order to evaluate the contributions of differ-
ent features, we evaluate our system using different 
combinations of features (Table 7). 
From Table 7, several findings are concluded:  
1) With only Fsd, our system achieves a basic 
level F-measure of 29.4. 
2) Fm shows the positive effect with 2.4 F-
measure improvement based on the basic level.  
However, it only can slightly improve the perform-
ance (+1.2 F-measure) based on Fsd, Fpos and Fhnt.  
The probable reason is that the evidences included 
in Fm have already been captured by Fhnt.  More-
over, the evidences captured by Fhnt are more accu-
rate than that captured by Fm.  The contribution 
made by Fm may come from where there is no indi-
cation of Fhnt. 
 
Fsd Fm Fpos Fhnt Fsvt P R F 
?     42.4 22.5 29.4 
? ?    44.8 24.6 31.8 
? ? ?   58.3 50.9 54.3 
?  ? ?  62.0 61.6 61.8 
? ? ? ?  64.4 61.7 63.0 
? ? ? ? ? 60.6 59.3 60.0 
Table 7: Effects of different features on V3.0 
 
3) Fpos is proved very beneficial as it makes 
great increase on F-measure (+22.5) based on Fsd 
and Fm.   
4) Fhnt leads to an improvement of 8.7 F-
measure based on Fsd, Fm and Fpos. 
5) Out of our expectation, the use of Fsvt de-
creases both precision and recall, which may be 
explained as the present and past participles of 
some special verbs often play the adjective-like 
roles inside biomedical NEs, such as IL10-
inhibited lymphocytes.  
 
 P R F 
Fsd+Fm+Fpos+Fhnt 64.4 61.7 63.0 
+abbr. recog. algorithm 64.6 62.5 63.5 
+rule-based casc. method 66.2 65.8 66.0 
+both 66.5 65.7 66.1 
Table 8: Effects of solution for abbr. and casc. 
 
From Table 8, it can be found that the abbrevia-
tion recognition method slightly improves the per-
formance by 0.5 F-measure.  The probable reason 
is that the recognition of abbreviation relies too 
much on the recognition of its full form.  Once the 
full form is wrongly classified, the abbreviation 
and the forthcoming ones throughout the document 
are wrong altogether.  In the near future, the pre-
defined abbreviation dictionary may be incorpo-
rated to enhance the decision of NE class. 
Moreover, it can be found that the rule-based 
method effectively solves the problem of cascaded 
phenomena and shows prominent improvement 
(+3.0 F-measure) based on the performance of 
?Fsd+Fm+Fpos+Fhnt?. 
7 Conclusion 
In the paper, we describe our exploration on how 
to adapt a general HMM-based named entity rec-
ognizer to biomedical domain.  We integrate vari-
ous evidences for biomedical NER, including 
lexical, morphological, syntactic and semantic in-
formation.  Furthermore, we present a simple algo-
rithm to solve the abbreviation problem and a rule-
based method to deal with the cascaded phenom-
ena. Based on such evidences and methods, our 
system is successfully adapted to biomedical do-
main and achieves significantly better performance 
than the best published system.  In the near future, 
more effective abbreviation recognition algorithm 
and some pre-defined NE lists for some classes 
may be incorporated to enhance our system. 
Acknowledgements 
We would like to thank Mr. Tan Soon Heng for his 
support of biomedical knowledge.   
References 
M. Bikel Danie, R.Schwartz and M. Weischedel Ralph. 
1999.  An Algorithm that Learns What's in a Name. 
In Proc. of Machine Learning (Special Issue on NLP). 
A. Borthwick.  1999.  A Maximum Entropy Approach 
to Named Entity Recognition. Ph.D. Thesis. New 
York University. 
N. Collier, C. Nobata, and J. Tsujii.  2000.  Extracting 
the names of genes and gene products with a hidden 
Markov model.  In Proc. of COLING 2000, pages 
201-207. 
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi.  
1998.  Toward information extraction: identifying 
protein names from biological papers.  In Proc. of the 
Pacific Symposium on Biocomputing?98 (PSB?98), 
pages 707-718, January. 
R. Gaizauskas, G. Demetriou and K. Humphreys.  Term 
Recognition and Classification in Biological Science 
Journal Articles.  2000.  In Proc. of the Computional 
Terminology for Medical and Biological Applications 
Workshop of the 2nd International Conference on 
NLP, pages 37-44. 
J. Kazama, T. Makino, Y.Ohta, and J. Tsujii.  2002.  
Tuning Support Vector Machines for Biomedical 
Named Entity Recognition.  In Proc. of the Work-
shop on Natural Language Processing in the Bio-
medical Domain (at ACL?2002), pages 1-8. 
C. Nobata, N. Collier, and J. Tsujii.  1999.  Automatic 
term identification and classification in biology texts.  
In Proc. of the 5th NLPRS, pages 369-374. 
C. Nobata, N. Collier, and J. Tsujii.  2000.  Comparison 
between tagged corpora for the named entity task.  In 
Proc. of the Workshop on Comparing Corpora (at 
ACL?2000), pages 20-27. 
T. Ohta, Y. Tateisi, J. Kim, H. Mima, and J. Tsujii.  
2002.  The GENIA corpus: An annotated research 
abstract corpus in molecular biology domain.  In 
Proc. of HLT 2002. 
D. Proux, F. Rechenmann, L. Julliard, V. Pillet and B. 
Jacq.  1998.  Detecting Gene Symbols and Names in 
Biological Texts: A First Step toward Pertinent In-
formation Extraction.  In Proc. of Genome Inform 
Ser Workshop Genome Inform, pages 72-80. 
A.S. Schwartz and M.A. Hearst.  2003.  A Simple Algo-
rithm for Identifying Abbreviation Definitions in 
Biomedical Text.  In Proc. of the Pacific Symposium 
on Biocomputing (PSB 2003) Kauai. 
T. Sekimizu, H. Park, and J. Tsujii.  1998.  Identifying 
the interaction between genes and gene products 
based on frequently seen verbs in medline abstracts.  
In Proc. of Genome Informatics, Universal Academy 
Press, Inc.  
K. Takeuchi and N. Collier.  2002.  Use of Support Vec-
tor Machines in Extended Named Entity Recognition.  
In Proc. of the Sixth Conference on Natural Lan-
guage Learning (CONLL 2002), pages 119-125. 
J. Thomas, D. Milward, C. Ouzounis, S. Pulman, and M. 
Carroll.  2000.  Automatic extraction of protein inter-
actions from scientific abstracts.  In Proc. of the Pa-
cific Symposium on Biocomputing?2000 (PSB?2000), 
pages 541-551, Hawaii, January. 
A. J. Viterbi.  1967.  Error bounds for convolutional 
codes and an asymptotically optimum decoding algo-
rithm.  In Proc. of IEEE Transactions on Information 
Theory, pages 260-269. 
J. Zhang, D. Shen, G. Zhou, J. Su and C. Tan. 2003.  
Exploring Various Evidences for Recognition of 
Named Entities in Biomedical Domain.  Submitted to 
EMNLP 2003. 
G. Zhou and J. Su.  2002.  Named Entity Recognition 
using an HMM-based Chunk Tagger.  In Proc. of the 
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 473-480. 
Modeling of Long Distance Context Dependency in Chinese 
 
GuoDong ZHOU 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore, 119613 
zhougd@i2r.a-star.edu.sg 
 
Abstract 
Ngram modeling is simple in language 
modeling and has been widely used in many 
applications. However, it can only capture the 
short distance context dependency within an 
N-word window where the largest practical N 
for natural language is three. In the meantime, 
much of context dependency in natural 
language occurs beyond a three-word window. 
In order to incorporate this kind of long 
distance context dependency, this paper 
proposes a new MI-Ngram modeling approach. 
The MI-Ngram model consists of two 
components: an ngram model and an MI 
model. The ngram model captures the short 
distance context dependency within an N-word 
window while the MI model captures the long 
distance context dependency between the word 
pairs beyond the N-word window by using the 
concept of mutual information. It is found that 
MI-Ngram modeling has much better 
performance than ngram modeling. Evaluation 
on the XINHUA new corpus of 29 million 
words shows that inclusion of the best 
1,600,000 word pairs decreases the perplexity 
of the MI-Trigram model by 20 percent 
compared with the trigram model. In the 
meanwhile, evaluation on Chinese word 
segmentation shows that about 35 percent of 
errors can be corrected by using the 
MI-Trigram model compared with the trigram 
model.  
1 Introduction 
Language modeling is the attempt to characterize, 
capture and exploit the regularities and constraints 
in natural language. Among various language 
modeling approaches, ngram modeling has been 
widely used in many applications, such as speech 
recognition, machine translation (Katz 1987; 
Jelinek 1989; Gale and Church 1990; Brown et al 
1992; Yang et al 1996; Bai et al1998; Zhou et al
1999; Rosenfeld 2000; Gao et al2002). Although 
ngram modeling is simple in nature and easy to use, 
it has obvious deficiencies. For instance, ngram 
modeling can only capture the short distance 
context dependency within an N-word window 
where currently the largest practical N for natural 
language is three.  
In the meantime, it is found that there always 
exist many preferred relationships between words. 
Two highly associated word pairs are ??/?? 
(?not only/but also?) and ? ? / ? ? 
(?doctor/nurse?). Psychological experiments in 
Meyer et al (1975) indicated that the human?s 
reaction to a highly associated word pair was 
stronger and faster than that to a poorly associated 
word pair. Such preference information is very 
useful for natural language processing (Church et 
al. 1990; Hiddle et al 1993; Rosenfeld 1994; Zhou 
et al1998; Zhou et al1999). Obviously, the 
preference relationships between words can expand 
from a short to long distance. While we can use 
traditional ngram modeling to capture the short 
distance context dependency, the long distance 
context dependency should also be exploited 
properly. 
The purpose of this paper is to propose a new 
MI-Ngram modeling approach to capture the 
context dependency over both a short distance and a 
long distance. Experimentation shows that this new 
MI-Ngram modeling approach can significantly 
decrease the perplexity of the new MI-Ngram 
model compared with traditional ngram model.  In 
the meantime, evaluation on Chinese word 
segmentation shows that this new approach can 
significantly reduce the error rate. 
This paper is organized as follows. In section 2, 
we describe the traditional ngram modeling 
approach and discuss its main property. In section 3, 
we propose the new MI-Ngram modeling approach 
to capture context dependency over both a short 
distance and a long distance. In section 4, we 
measure the MI-Ngram modeling approach and 
evaluate its application in Chinese word 
segmentation. Finally we give a summary of this 
paper in section 5. 
And the probability P  can be estimated 
by using maximum likelihood estimation (MLE) 
principle: 
)|( 1?ii ww
)(
)(
)|(
1
1
1
?
?
? =
i
ii
ii wC
wwC
wwP             (2.5) 
Where )(?C  represents the number of times the 
sequence occurs in the training data. In practice, due 
to the data sparseness problem, some smoothing 
techniques, such as linear interpolation (Jelinek 
1989; Chen and Goodman 1999) and back-off 
modeling (Katz 1987), are applied.  
2 Ngram Modeling 
Let , where ?s are the words 
that make up the hypothesis, the probability of the 
word string P  can be computed by using the 
chain rule: 
m
m wwwwS ...211 ==
)(S
iw Obviously, an ngram model assumes that the 
probability of the next word w is independent of 
word string w  in the history. The difference 
between bigram, trigram and other ngram models is 
the value of N. The parameters of an ngram are thus 
the probabilities: 
i
ni?
1
?
=
?=
m
i
i
i wwPwPSP
2
1
11 )|()()(                  (2.1)  
)...|( 11 ?nn wwwP    For all Vwww nw ?,...,,1 . By taking log function to both sides of equation 
(2.1), we have the log probability of the word 
string log : )(SP Given mwwwS ...21= , an ngram model 
estimates the log probability of the word string 
 by re-writing equation (2.2): )(SP
)|(log
)(log)(log
1
1
2
1
?
=
?+
=
i
i
m
i
wwP
wPSP
                  (2.2) 
?
?
=
?
+?
?
=
?
+
+
=
m
ni
i
nii
n
i
i
i
ngram
wwP
wwP
wPSP
)|(log
)|(log
)(log)(log
1
1
1
2
1
1
1
               (2.6) So, the classical task of statistical language 
modeling becomes how to effectively and 
efficiently predict the next word, given the previous 
words, that is to say, to estimate expressions of the 
form  . For convenience, P  
is often written as , where , is 
called history. 
)|( 11
?i
i wwP )|(
1
1
?i
i ww
1
1
?= iwh)|( hwP i
Where  is the string length, w  is the -th word 
in string .  
m
S
i i
From equation (2.3) 
,  we have: )|()|( 1 1
1
1
?
+?
? ? i niiii wwPwwPNgram modeling has been widely used in estimating .  Within an ngram model, the 
probability of a word occurring next is estimated 
based on the  previous words. That is to say, 
)|( hwP i
1?n )(
)(
)(
)(
1
1
1
1
1
1
1
1
?
+?
?
+?
?
?
? i
ni
i
i
ni
i
i
i
wP
wwP
wP
wwP
                    
)|()|( 1 1
1
1
?
+?
? ? i niiii wwPwwP                 (2.3) 
)()(
)(
)()(
)(
1
1
1
1
1
1
1
1
i
i
ni
i
i
ni
i
i
i
i
wPwP
wwP
wPwP
wwP
?
+?
?
+?
?
?
?                   For example, in bigram model (n=2), the 
probability of a word is assumed to depend only on 
the previous word: 
)()(
)(
log
)()(
)(
log 1
1
1
1
1
1
1
1
i
i
ni
i
i
ni
i
i
i
i
wPwP
wwP
wPwP
wwP
?
+?
?
+?
?
?
?      (2.7) 
)|()|( 1
1
1 ?
? ? iiii wwPwwP            (2.4) 
Obviously, we can get 
)1,,()1,,( 1 1
1
1 =?= ? +?? dwwMIdwwMI ii niii   (2.8) 
where 
)()(
)(log)1,,( 1
1
1
11
1
i
i
i
i
i
i
wPwP
wwPdwwMI ?
?
? ==
)iw
 is 
the mutual information between the word string pair 
 and ,( 11
iw ?
)()(
)(log)1,, 1
1
1
11
1
i
i
ni
i
i
ni
i
i
ni wPwP
wwPdw ?
+?
?
+??
+? ==
),1 iw d
(wMI
( 1i niw
?
+?
 is the 
mutual information between the word string pair 
.  is the distance of two word strings 
in  the word string pair and is equal to 1 when the 
two word strings are adjacent. 
For a word string pair (  over a distance 
 where  and 
), BA
d A B  are word strings, mutual 
information  reflects the degree of 
preference relationship between the two strings 
over a distance . Several properties of mutual 
information are apparent: 
)( dAMI
d
,, B
? For the same distance , 
. 
d
),,(),,( dABMIdBAMI ?
? For different distances d  and , 
. 
1
)
2d
,,(),,( 21 dBAMIdBAMI ?
? If  and A B  are independent over a distance 
,  then  . d 0),,( =dBAMI
),,( dBAMI  reflects the change of  the 
information content when the word strings A  and 
B   are correlated.  That is to say, the higher the 
value of ,  the stronger affinity  and ),d,( BAMI A
B  have. Therefore, we can use mutual information 
to measure the preference relationship degree 
between a word string pair. 
From the view of mutual information, an ngram 
model assumes the mutual information 
independency between ( . Using an 
alternative view of equivalence, an ngram model is 
one that partitions the data into equivalence classes 
based on the last n-1 words in the history.  
),1 i
ni ww ?
As trigram model is most widely used in current 
research, we will mainly consider the trigram-based 
model. By re-writing equation (2.6), the trigram 
model estimates the log probability of the string 
 as: )(SP
?
=
?
?+
+
=
m
i
i
ii
Trigram
wwP
ww
wPSP
3
1
2
12
1
)|(log
)|log(
)(log)(log
           (2.9) 
3 MI-Ngram Modeling 
Given history H , we can 
assume . Then we have  
121
1
1 ... ?
? == ii wwww
132
1 ... ?
? = iwww2= iwX
XwH 1=                      (3.1) 
and  
)|()|( 1XwwPHwP ii = .                        (3.2) 
Since 
)1,,()(log
)()(
)(
log)(log
)(
)(
log)|(log
HwMIwP
wPHP
HwP
wP
HP
HwP
HwP
ii
i
i
i
i
i
+=
+=
=
(3.3) 
Here we assume  
),,(
)1,,()1,,(
1 idwwMI
dwXMIdwHMI
i
ii
=+
===
           (3.4) 
where  ,  and i . That is to 
say, the mutual information of the next word with 
the history is assumed equal to the summation of 
that of the next word with the first word in the 
history and that of the next word with the rest word 
string in the history. 
1
1
?= iwH 12?= iwX N>
We can re-writing equation (3.3) by using 
equation (3.4): 
)|(log HwP i   
)1,,()(log ii wHMIwP +=  
),,()1,,()(log 1 iwwMIwXMIwP iii ++=  
),,(
)()(
)(log)(log 1 iwwMIXPwP
XwPwP i
i
i
i ++=  ?=
=
?+=
ni
i
i
i wwwP
2
1
11 )|log()(log
)|(log 21
n
n wwP ++  
),,(
)(
)(log 1 iwwMIXP
XwP
i
i +=  
)1,,( 11 ++ + nwwMI n )|(log 11
2
?
+=
?+ iim
ni
wwP  
),,()|(log 1 iwwMIXwP ii +=                       (3.5) 
L L L Then we have 
?=
=
?+=
ni
i
i
i wwwP
2
1
11 )|log()(log
?
+=
?
?+
m
ni
i
ii wwP
1
1
2 )|(log  
                (3.6) 
),,(
)|(log)|(log
1
1
2
1
1
iwwMI
wwPwwP
i
i
i
i
i
+
= ??
By applying equation (3.6) repeatedly, we have: 
),,()|(log
)|(log
1
1
2
1
1
iwwMIwwP
wwP
i
i
i
i
i
+= ?
?
 ? ?
+=
?=
=
+?+
m
ni
nik
k
ik kiwwMI
1 1
)1,,(                        (3.8) 
)|(log 13
?= ii wwP
)1,,( 2 iwwMI i ?+ ),,( 1 iwwMI i+  
L L L 
)|(log 1 1
?
+?= i nii wwP  
??=
=
+?+
nik
k
ik kiwwMI
1
)1,,(                              (3.7) 
Obviously, the first item in equation (3.7) 
contributes to the log probability of ngram within an 
N-word window while the second item is the 
summation of mutual information which 
contributes to the long distance context dependency 
of the next word w  with the individual previous 
word  over the long 
distance outside the N-word window. 
i
i ? ),1( NiNjwj >??
log
In equation (3.8), the first three items are the 
values computed by the trigram model as shown in 
equation (2.9) and the forth item 
 contributes to 
summation of the mutual information of the next 
word with the words over the long distance outside 
the N-word window. That is, the new model as 
shown in equation (3.8) consists of two 
components: an ngram model and an MI model. 
Therefore, we call equation (3.8) as an MI-Ngram 
model and equation (3.8) can be re-written as: 
? ?
+=
?=
=
+?
m
ni
nik
k
ik kiwwMI
1 1
)1,,(
? ?
+=
?=
=
?
+?+
=
m
ni
nik
k
ik
Ngram
NgramMI
kiwwMI
SP
SP
1 1
)1,,(
)(log
)(
                       (3.9) 
By using equation (3.7), equation (2.2) can be 
re-written as: 
As a special case N=3, the MI-Trigram model 
estimate the log probability of the string as follows: 
)|(log)(log)(log 11
2
1
?
=
?+= iim
i
wwPwPSP  
? ?
=
?=
=
?
+?+
=
m
i
ik
k
ik
Trigram
TrigramMI
kiwwMI
SP
SP
4
3
1
)1,,(
)(log
)(log
                      (3.10) 
?=
=
?+=
ni
i
i
i wwwP
2
1
11 )|log()(log
)|(log 11
n
n wwP ++ (log
2+=
?+ m
ni
P )| 11
?i
i ww  Compared with traditional ngram modeling, 
MI-Ngram modeling incorporates the long distance 
context dependency by computing mutual 
information of the long distance dependent word 
)()(
),,(log),,(
)()(
),,(log),,(
)()(
),,(log),,(
)()(
),,(log),,(
),,(
BPAP
dBAPdBAP
BPAP
dBAPdBAP
BPAP
dBAPdBAP
BPAP
dBAPdBAP
dBAAMI
+
+
+
=
        (3.11) 
pairs. Since the number of possible long distance 
dependent word pairs may be very huge, it is 
impossible for MI-Ngram modeling to incorporate 
all of them. Therefore, for MI-Ngram modeling to 
be practically useful, how to select a reasonable 
number of word pairs becomes very important. 
Here two approaches are used (Zhou et al1998 and 
1999). One is to restrict the window size of possible 
word pairs by computing and comparing the 
perplexities1 (Shannon C.E. 1951) of various long 
distance bigram models for different distances. It is 
found that the bigram perplexities for different 
distances outside the 10-word window become 
stable. Therefore, we only consider MI-Ngram 
modeling with a window size of 10 words. Another 
is to adapt average mutual information to select a 
reasonable number of long distance dependent word 
pairs. Given distance d and two words A and B, its 
average mutual information is computed as: 
Compared with mutual information, average 
mutual information takes joint probabilities into 
consideration. In this way, average mutual 
information prefers frequently occurred word pairs. 
In our paper, different numbers of long distance 
dependent word pairs will be considered in 
MI-Ngram modeling within a window size of 10 
words to evaluate the effect of different MI model 
size. 
4 Experimentation 
As trigram modeling is most widely used in current 
research, only MI-Trigram modeling is studied 
here. Furthermore, in order to demonstrate the 
effect of different numbers of word pairs in 
MI-Trigram modeling, various MI-Trigram models 
with different numbers of word pairs and the same 
window size of 10 words are trained on the 
XINHUA news corpus of 29 million words while 
the lexicon contains about 56,000 words. Finally, 
various MI-Trigram models are tested on the same 
task of Chinese word segmentation using the 
Chinese tag bank PFR1.0 2  of 3.69M Chinese 
characters (1.12M Chinese Words). 
                                                     
1  Perplexity is a measure of the average number of 
possible choices there are for a random variable. The 
perplexity PP  of a random variable X  with entropy 
 is defined as: )(XH
)(2)( XHXPP =   
Entropy is a measure of uncertainty about a random 
variable. If a random variable X  occurs with a 
probability distribution P x( ) , then the entropy H  
of that event is defined as: 
)(X
?
?
?=
Xx
xPxPXH )(log)()( 2
x xlog2 0?
                                  
Since  as x ? 0
0 0 02log =
, it is conventional  to 
use the relation  when computing entropy. 
                                   
Table 1 shows the perplexities of various 
MI-Trigram models and their performances on 
Chinese word segmentation. Here, the precision (P) 
measures the number of correct words in the answer 
file over the total number of words in the answer file 
and the recall (R) measures the number of correct 
words in the answer file over the total number of The units of entropy are bits of information. This is 
because the entropy of a random variable corresponds to 
the average number of bits per event needed to encode a 
typical sequence of event samples from that random 
variable? s distribution. 
                                                     
2  PFR1.0 is developed by Institute of Computational 
Linguistics at Beijing Univ. Here, only the word 
segmentation annotation is used.  
words in the key file. F-measure is the weighted 
harmonic mean of precision and recall: 
PR
RPF +
+= 2
2 )1(
?
?
 with =1. 2?
Table 1 shows that 
? The perplexity and the F-measure rise quickly 
as the number of word pairs in MI-Trigram 
modeling increases from 0 to 1,600,000 and 
then rise slowly. Therefore, the best 1,600,000 
word pairs should at least be included. 
? Inclusion of the best 1,600,000 word pairs 
decreases the perplexity of MI-Trigram 
modeling by about 20 percent compared with 
the pure trigram model. 
? The performance of Chinese word 
segmentation using the MI-Trigram model with 
1,600,000 word pairs is 0.8 percent higher than 
using the pure trigram model (MI-Trigram with 
0 word pairs). That is to say, about 35 percent of 
errors can be corrected by incorporating only 
1,600,000 word pairs to the MI-Trigram model 
compared with the pure trigram model. 
? For Chinese word segmentation task, recalls are 
about 0.7 percent higher than precisions. The 
main reason may be the existence of unknown 
words. In our experimentation, unknown words 
are segmented into individual Chinese 
characters. This makes the number of 
segmented words in the answer file higher than 
that in the key file. 
It is clear that MI-Ngram modeling has much 
better performance than ngram modeling. One 
advantage of MI-Ngram modeling is that its number 
of parameters is just a little more than that of ngram 
modeling. Another advantage of MI-Ngram 
modeling is that the number of the word pairs can be 
reasonable in size without losing too much of its 
modeling power. Compared to ngram modeling, 
MI-Ngram modeling also captures the 
long-distance context dependency of word pairs 
using the concept of mutual information. 
Table 1: The effect of different numbers of word pairs in MI-Trigram modeling with the same window size 
of 10 words on Chinese word segmentation 
Number of  word pairs  Perplexity Precision Recall F-measure 
0 316 97.5 98.2 97.8 
100,000 295 97.9 98.4 98.1 
200,000 281 98.1 98.6 98.3 
400,000 269 98.2 98.7 98.4 
800,000 259 98.2 98.8 98.5 
1,600,000 250 98.4 98.8 98.6 
3,200,000 245 98.3 98.9 98.6 
6,400,000 242 98.4 98.9 98.6 
 
6  Conclusion 
This paper proposes a new MI-Ngram modeling 
approach to capture the context dependency over 
both a short distance and a long distance. This is 
done by incorporating long distance dependent 
word pairs into traditional ngram model by using 
the concept of mutual information. It is found that 
MI-Ngram modeling has much better performance 
than ngram modeling.  
Future works include the explorations of the 
new MI-Trigram modeling approach in other 
applications, such as Mandarin speech recognition 
and PINYIN to Chinese character conversion. 
References 
Bai S.H., Li H.Z., Lin Z.M. and Yuan B.S. 1989. 
Building class-based language models with 
contextual statistics. Proceedings of International 
Conference on Acoustics, Speech and Signal 
Processing (ICASSP?1998). pages173-176. 
Brown P.F. et al 1992. Class-based ngram models of 
natural language. Computational Linguistics  18(4), 
467-479. 
Chen S.F. and Goodman J. 1999. An empirical study 
of smoothing technique for language modeling. 
Computer, Speech and Language. 13(5). 359-394. 
Church K.W. et al 1991. Enhanced good Turing and 
Cat-Cal: two new methods for estimating 
probabilities of English bigrams. Computer, Speech 
and Language  5(1), 19-54. 
Gale W.A. and Church K.W. 1990.  Poor estimates of 
context are worse than none. Proceedings of 
DARPA Speech and Natural Language Workshop, 
Hidden Valley, Pennsylvania, pages293-295. 
Gao J.F., Goodman J.T., Cao G.H. and Li H. 2002. 
Exploring asymmetric clustering for statistical 
language modelling. Proceedings of the Fortieth 
Annual Meeting of the Association for 
Computational Linguistics (ACL?2002). 
Philadelphia. pages183-190. 
Hindle D. et al 1993. Structural ambiguity and lexical 
relations. Computational Linguistics  19(1),  
103-120. 
Jelinek F. 1989. Self-organized language modeling for 
speech recognition. In Readings in Speech 
Recognition. Edited by Waibel A. and Lee K.F. 
Morgan Kaufman. San Mateo. CA. pages450-506. 
Katz S.M. 1987. ? Estimation of Probabilities from 
Sparse Data for the Language Model Component 
of a Speech Recognizer?. IEEE Transactions on 
Acoustics. Speech and Signal Processing. 35. 
400-401. 
Meyer D.  et al 1975. Loci of contextual effects on 
visual word recognition. In Attention and 
Performance V, edited by P.Rabbitt and S.Dornie. 
pages98-116. Acdemic Press. 
Rosenfeld R. 1994. Adaptive statistical language 
modeling: A Maximum Entropy Approach. Ph.D. 
Thesis, Carneige Mellon University. 
Rosenfeld R. 2000. Two decades of language 
modelling: where do we go from here. Proceedings 
of IEEE. 88:1270-1278. August. 
Shannon C.E. 1951. Prediction and entropy of printed 
English. Bell Systems Technical Journal   30, 50-64. 
Yang Y.J. et al 1996. Adaptive linguistic decoding 
system for Mandarin speech recognition 
applications. Computer Processing of Chinese & 
Oriental Languages  10(2), 211-224. 
Zhou GuoDong and Lua Kim Teng, 1998. Word 
Association and MI-Trigger-based Language 
Modeling. Proceedings of the Thirtieth-sixth 
Annual Meeting of the Association for 
Computational Linguistics and the Seventeenth 
International Conference on Computational 
Linguistics (COLING-ACL?1998). Montreal, 
Canada. pages10-14. August. 
Zhou GuoDong and Lua KimTeng. 1999. 
Interpolation of N-gram and MI-based Trigger 
Pair Language Modeling in Mandarin Speech 
Recognition, Computer, Speech and Language, 
13(2), 123-135. 
A Chinese Efficient Analyser Integrating Word Segmentation, 
Part-Of-Speech Tagging, Partial Parsing and Full  Parsing 
 
GuoDong ZHOU 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore, 119613 
zhougd@i2r.a-star.edu.sg 
Jian SU 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore, 119613 
sujian@ i2r.a-star.edu.sg 
 
The objective of this paper is to develop an 
efficient analyser for the Chinese language, 
exploring different intermediate forms, achieving a 
target speed in the region of 1,000 wps for full 
parsing, 2,000 wps for partial parsing and 10,000 
wps for word segmentation and part-of-speech 
tagging, with state-of-art performances. 
Abstract  
This paper introduces an efficient analyser for 
the Chinese language, which efficiently and 
effectively integrates  word segmentation, 
part-of-speech tagging, partial parsing and full 
parsing. The Chinese efficient analyser is based 
on a Hidden Markov Model (HMM) and an 
HMM-based tagger. That is, all the 
components are based on the same 
HMM-based tagging engine. One advantage of 
using the same single engine is that it largely 
decreases the code size and makes the 
maintenance easy. Another advantage is that it 
is easy to optimise the code and thus improve 
the speed while speed plays a critical important 
role in many applications. Finally, the 
performances of all the components can benefit 
from the optimisation of existing algorithms 
and/or adoption of better algorithms to a single 
engine. Experiments show that all the 
components can achieve state-of-art 
performances with high efficiency for the 
Chinese language.  
The layout of this paper is as follows. Section 2 
describes the Chinese efficient analyser. Section 3 
presents the HMM and the HMM-based tagger. 
Sections 4 and 5 describe the applications of the 
HMM-based tagger in integrated word 
segmentation and part-of-speech tagging, partial 
parsing, and full parsing respectively. Section 6 
gives the experimental results. Finally, some 
conclusions are drawn with possible extensions of 
future work in section 7. 
2 Chinese Efficient Analyser 
The Chinese efficient analyser can be described by 
the example as shown in Figure 1. Here, "." in 
Figure 1 means that the current node has not been 
chunked till now. For convenience, it is regarded as 
a "special chunk" in this paper and others as 
"normal chunks". Therefore, every node in Figure 1 
can be represented as a 3tuple , where  
is the i -th chunk in the input chunk sequence and 
),( iii wpc ic
1  Introduction 
Traditionally, a text parser outputs a complete parse 
tree for each input sentence, achieving a speed in 
the order of 10 words per second (wps) (Abney 
1997). However, for many applications like text 
mining, a parse tree is not necessary and a speed of 
10 wps is unacceptable when we have to process 
millions of words in thousands of documents in a 
reasonable time (Feldman 1997). Therefore, there is 
a compromise between speed and performance in 
many applications. 
?  is the head word of c  and  is the POS 
tag of w  when 
iw i ip
i .?ic  ( c  is a normal chunk). 
In this case, we call node c  a normal 
chunk node. 
i
)(i p , ii w
Full Parsing = N levels of Partial Parsing  
(N = Parsing Depth and for Figure 1, N=3) 
 
 
 S(VB, ??) 
3rd-level 3tuple sequence  
 
 
 
 
   
   
   
   
   
   
   
   
   
  
  
   
   
   
   
   
   
   
 
?  is just the word linked with c  and  is 
the POS tag of  when  ( c  is a special 
chunk). In this case, we call node c  a 
special chunk or POS node. 
iw i
i
i
ip
)iw
iw .=ic
,( ip
Figure 1 shows that, sequentially from bottom 
to top, 
1) Given a Chinese sentence (e.g. ????????
?????), it is segmented and tagged into a 
sequence of special chunk, POS and word 
3tuples (.(ADJ, ? ? ) .(NN, ? ? ) .(ADV, 
?) .(VB, ??) .(ADJ, , ??) ) via the 
integrated word segme
In this paper, this res
called "0th-level 3tuple
2) The 0th-level 3tuple sequence is then chunked 
into 1st-level 3tuple sequence (NP(NN, ?
?) .(ADV, ?) .(VB, ??) NP(NN, ??)) via 
1st-level partial parsing, while POS nodes .(ADJ, 
??) and .(NN, ??) are chunked into a normal 
chunk node NP(NN, ? ? ), and POS 
nodes .(ADJ, ??) .(NN, ??) into NP(NN, ?
?). 
3) The 1st-level 3tuple sequence is further chunked 
into 2nd-level 3 N, ??) 
VP(VB, ?? )) parsing, 
while mixed al chunk 
nodes .(ADV, ? N, ??) are 
NP(NN, ??) VP(VB, ??) 
NP(NN, ??) .(ADV,?) .(VB,??) NP(NN, ??)) 
.(ADJ,??) .(NN,??)  .(ADV,?) .(VB,??) .(ADJ,??) .(NN,??) 
      ?           ?            ?      ?                      ?                          ?  ?                   ?    ?                    ?         ? 
   developed      country             also                 exist              man
   (English Translation: There also exists many problems in deve
Figure 1: An Example Sentence ????????????? of the Chinese Efficien
2nd-level Partial Parsing 
1st-level  PartialParsing  
egmentation  
agging
3rd-level Partial Parsing
2nd-level 3tuple sequence
1st-level 3tuple sequence
0th-level 3tuple  
sequence??) .(NN 
 
Integrated Word S
and POS Tntation and POS tagging. 
ulting 3tuple sequence is 
 sequence". 
chunked i
?). y              problem 
loped countries) nto atuple sequence (NP(N
 via 2nd-level partial  normal chunt Analyser POS and norm
) .(VB, ??) NP(Nk node VP(VB, ?
4) Finally, 2nd-level 3tuple sequence is chunked 
into 3rd-level 3tuple sequence (S(VB, ??)) via 
3rd-level partial parsing, while normal chunk 
nodes NP(NN, ?? ) and VP(VB, ?? ) are 
chunked into a normal chunk node S(VB, ??).  
5) In this way, full parsing is completed with a 
fully parsed tree after several levels (3 in the 
example of Figure 1) of cascaded partial 
parsing.  
3 HMM-based Tagger 
The Chinese efficient analyser is based on the 
HMM-based tagger described in Zhou et al2000a. 
Given a token sequence G , the goal 
of tagging is to find a stochastic optimal tag 
sequence  that maximizes       
n
n ggg L211 =
n
n tttT L211 =
)()(
),(log)(log)|(log
11
11
111 nn
nn
nnn
GPTP
GTPTPGTP ?+=  
By assuming mutual information 
independence:  
?
=
=
n
i
n
i
nn GtMIGTMI
1
111 ),(),(  or 
?
= ?=?
n
i
n
i
n
i
nn
nn
GPtP
GtP
GPTP
GTP
1 1
1
11
11
)()(
),(log
)()(
),(log   
we have: 
?
?
=
=
+
?=
n
i
n
i
n
i
i
nnn
GtP
tPTPGTP
1
1
1
111
)|(log
)(log)(log)|(log
  
Both the first and second items correspond to 
the language model component of the tagger. We 
will not discuss these two items further in this paper 
since they are well studied in ngram modeling. This 
paper will focus on the third item 
, which is the main difference 
between our tagger and other HMM-based taggers. 
Ideally, it can be estimated by using the 
forward-backward algorithm (Rabiner 1989) 
recursively for the first-order (Rabiner 1989) or 
second-order HMMs (Watson et al1992). To 
simplify the complexity, several context dependent 
approximations on it will be attempted in this paper 
instead, as detailed in sections 3 and 4. 
?
=
n
i
n
i GtP
1
1 )|(log
All of this modelling would be for naught were 
it not for the existence of an efficient algorithm for 
finding the optimal state sequence, thereby 
"decoding" the original sequence of tags. The 
stochastic optimal tag sequence can be found by 
maximizing the previous equation over all the 
possible tag sequences. This is implemented via the 
well-known Viterbi algorithm (Viterbi 1967) by 
using dynamic programming and an appropriate 
merging of multiple theories when they converge 
on a particular state. Since we are interested in 
recovering the tag state sequence, we pursue 16 
theories at every given step of the algorithm. 
4 Word Segmentation and POS Tagging 
Traditionally, in Chinese Language Processing, 
word segmentation and POS tagging are 
implemented sequentially. That is, the input 
Chinese sentence is segmented into words first and 
then the segmented result (in the form of word 
lattice or N-best word sequences) is passed to POS 
tagging component. However, this processing 
strategy has following disadvantages: 
? The word lexicons used in word segmentation 
and POS tagging may be different. This 
difference is difficult to overcome and largely 
drops the system accuracy although different 
optimal algorithms may be applied to word 
segmentation and POS tagging. 
? With speed in consideration, the two-stage 
processing strategy is not efficient. 
Therefore, we apply the strategy of integrating 
word segmentation and POS tagging in a single 
stage. This can be implemented as follows:  
1) Given an input sentence, a 3tuple (special 
chunk, POS and word) lattice is generated by 
skimming the sentence from left-to-right, and 
looking up the word and POS lexicon to 
determine all the possible words and get POS 
tag probability distribution for each possible 
word. 
2) Viterbi algorithm is applied to decode the 
3tuple lattice to find the most possible POS tag 
sequence. 
3) In this way, the given sentence is segmented 
into words with POS tags. 
The rationale behind the above algorithm is the 
ability of HMM in parallel segmentation and 
classification (Rabiner 1989). 
In order to overcome the coarse n-gram models 
raised by the limited number of orignial POS tags 
used in current Chinese POS tag bank (corpus), a 
word clustering algorithm (Bai et al1998) is applied 
to classify words into classes first and then the N 
(e.g. N=500) most frequently occurred word class 
and POS pairs are added to the original POS tag set 
to achieve more accurate models. For example, 
ADJ(<?? >) represents a special POS tag ADJ 
which pairs with the word class <??>. Here, <??> 
is a word class label. For convenience and clarity, 
we use the most frequently occurred word in a word 
class as the label to represent the word class. 
5 Partial Parsing and Full Parsing 
As discussed in section 2, obviously partial parsing 
can have different levels and full parsing can be 
achieved by cascading several levels of partial 
parsing (e.g. 3 levels of cascaded partial parsing can 
achieve full parsing for the example as shown in 
Figure 1).  
In this paper, a certain level (e.g. l -th level) of 
partial parsing is implemented via a chunking 
model, built on the HMM-based tagger as described 
in section 2, with ( -th level 3tuple sequence 
as input. That is, for the l -th level partial parsing, 
the chunking model has the ( -th level 3tuple 
sequence  (Here, 3tuple 
) as input. In the meantime, chunk 
tag t  used in the chunking model is structural and 
consists of following three parts: 
)1?l
gg 21=
)1?l
ng
nG1
)iw
L
,( iii pcg =
i
? Boundary Category B: It is a set of four values 
0, 1, 2, 3, where "0" means that the current 
3tuple is a whole chunk, "1" means that the 
current 3tuple is at the beginning of a chunk,  
"2" means that the current 3tuple is in the 
middle of a chunk and "3" means that the 
current stuple is at the end of a chunk. 
? Chunk Category C:  It is used to denote the 
output chunk category of the chunking model, 
which includes normal chunks and the special 
chunk ("."). The reason to include the special 
chunk is that some of POS 3tuple in the input 
sequence may not be chunked in the current 
chunking stage. 
? POS Category POS: Because of the limited 
number in boundary category and output chunk 
category, the POS category is added into the 
structural tag to represent more accurate 
models.  
Therefore,  can be represented by 
, where b  is the boundary type of 
,  is the output chunk type of t  and  is 
the POS type of t . Obviously, there exist some 
constraints between t  and  on the boundary 
categories and output chunk categories, as briefed 
in table 1, where "valid"/"invalid" means the chunk 
tag sequence t  is valid/invalid while "validon" 
means  is valid on the condition 
it
i
it
iii poscb __
it ic
ii tt 1?
i
1?i
i
c
ipos
ic
it
i 1?
i =?1 . 
 0 1 2 3 
0 Valid Valid Invalid Invalid 
1 Invalid Invalid Valid on Validon 
2 Invalid Invalid Valid Valid 
3 Valid Valid Invalid Invalid 
Table 1: Constraints between t  and t  1?i i
(Column: b  in t ; Row:  in t ) 1?i 1?i ib i
For the example as shown in Figure 1, we can 
see that: 
? In the 1st-level partial parsing, the input 3tuple 
sequence is the 0th-level 3tuple sequence .(ADJ, 
??) .(NN, ??) .(ADV, ?) .(VB, ??) .(ADJ, ?
? ) .(NN, ?? ) and the output tag sequence 
1_NP_ADJ 3_NP_NN 0_._ADV 0_._VB 
1_NP_ADJ 3_NP_NN, from where derived is 
the 1st-level 3tuple sequence NP(NN, ?
?) .(ADV, ?) .(VB, ??) NP(NN, ??). 
? In the 2nd-level partial parsing, the input 3tuple 
sequence is the 1st-level 3tuple sequence 
NP(NN, ??) .(ADV, ?) .(VB, ??) NP(NN, ?
? ) and the output tag sequence 0_NP_NN 
1_VP_ADV 2_VP_VB 3_VP_NN, from where 
derived is the 2nd-level 3tuple sequence NP(NN, 
??) VP(VB, ??). 
? In the 3rd-level partial parsing, the input 3tuple 
sequence is the 2nd-level 3tuple sequence 
NP(NN, ??) VP(VB, ??) and the output tag 
sequence 1_S_NN 3_S_VB, from where 
derived is the 3rd-level 3tuple sequence S(VB, ?
?). In this way, a fully parsed tree is reached. 
? In the cascaded chunking procedure, necessary 
information is stored for back-tracing. 
Partially/fully parsed trees can be constructed 
by tracing from the final 3tuple sequence back 
to 0th-level 3tuple sequence. Different levels of 
partial parsing can be achieved according to the 
need of the application. 
6 Experimental Results 
The Chinese efficient analyser is implemented in 
C++, providing a rapid and easy 
code-compile-train-test development cycle. In fact, 
many NLP systems suffer from a lack of software 
and computer-science engineering effort: running 
efficiency is key to performing numerous 
experiments, which, in turn, is key to improving 
performance. A system may have excellent 
performance on a given task, but if it takes long to 
compile and/or run on test data, the rate of 
improvement of that system will be contrained 
compared to that which can run very efficiently. 
Moreover, speed plays a critical role in many 
applications such as text mining.  
All the experiments are implemented on a 
Pentium II/450MHZ PC. All the performances are 
measured in precisions, recalls and F-measures. 
Here, the precision (P) measures the number of 
correct units in the answer file over the total number 
of units in the answer file and the recall (R) 
measures the number of correct units in the answer 
file over the total number of units in the key file 
while F-measure is the weighted harmonic mean of 
precision and recall: 
PR
RP
+
+= 2
2 )1(
?
?F  with 
=1. 2?
6.1 Word Segmentation and POS Tagging 
Table 2 shows the integrated word segmentation 
and POS tagging results on the Chinese tag bank 
PFR1.0 of 3.69M Chinese characters (1.12 Chinese 
Words) developed by Institute of Computational 
Linguistics at Beijing Univ. Here, 80% of the 
corpus is used as formal training data, another 10% 
as development data and remaining 10% as formal 
test data. 
Function P R F Speed  
Word Segment. 97.5 98.2 97.8 
POS Tagging 93.5 94.1 93.8 
11,000 
wps 
Table 2: Performances of Word Segmentation  
and POS Tagging (wps: words per second) 
The word segmentation corresponds to 
bracketing of the chunking model while POS 
tagging corresponds to bracketing and labelling. 
Table 2 shows that recall (P) is higher than 
precision (P). The main reason may be the existence 
of unknown words. In the Chinese efficient 
analyser, unknown words are segmented into 
individual Chinese characters. This makes the 
number of segmented words/POS tagged words in 
the system output higher than that in the correct 
answer. 
6.2 Partial Parsing and Full Parsing 
Table 3 shows the results of 1st-level partial parsing 
and full parsing, using the PARSEVAL evaluation 
methodology (Black et al1991) on the UPENN 
Chinese Tree Bank of 100k words developed by 
Univ. of Penn. Here, 80% of the corpus is used as 
formal training data, another 10% as development 
data and remaining 10% as formal test data. 
Function P R F  Speed 
Partial Parsing 85.1 82.5 83.8 4500 wps 
Full Parsing 77.1 70.3 73.7 2100 wps 
Table 3: Performances of 1st-level Partial Parsing 
and Full Parsing (wps: words per second) 
Table 3 shows that the performances of partial 
parsing and full parsing are quite low, compared to 
those of state-of-art partial parsing and full parsing 
for the English language (Zhou et al2000a; Collins 
1997). The main reason behind is the small size of 
the training corpus used in our experiments. 
However, the Chinese PENN Tree Bank is the 
largest corpus we can find for partial parsing and 
full parsing. Therefore, developing a much larger 
Chinese tree bank (comparable to UPENN English 
Tree Bank) becomes an urgent task for the Chinese 
language processing community. Actually, the best 
individual system (Zhou et al2000b) in 
CoNLL?2000 chunking shared task for the English 
language (Tjong et al2000) used the same 
HMM-based tagging engine.  
7  Conclusion 
This paper presents an efficient analyser for the 
Chinese language, based on a HMM and a single 
engine -- HMM-based tagger. Experiments show 
that the analyser achieves state-of-art performance 
at very high speed, which can meet the requirement 
of speed-critical applications such as text mining. 
Our future work includes: 
? Syntactic analysis of the partial/full parsing 
results into a meaningful intermediate form. 
? Research and development of Chinese named 
entity recognition using the same HMM-based 
tagger and its integration to the Chinese 
efficient analyser. 
Acknowledgements 
Thanks go to Institute of Computational Linguistics 
at Beijing Univ. and LDC at Univ. of Penn. for free 
research use of their Chinese Tag Bank and Chinese 
Tree Bank. 
References  
Abney S. 1997. Part-of-Speech Tagging and Partial 
Parsing. Corpus-based Methods in Natural 
Language Processing. Edited by Steve Young 
and Gerrit Bloothooft. Kluwer Academic 
Publishers, Dordrecht.  
Bai ShuanHu, Li HaiZhou, Lin ZhiWei and Yuan 
BaoSheng. 1998. Building class-based language 
models with contextual statistics. Proceedings of 
International Conference on Acoustics, Speech 
and Signal Processing (ICASSP'1998). 
pages173-176. Seattle, Washington, USA.  
Black E. and Abney S. 1991. A Procedure for 
Quantitatively Comparing the Syntactic Coverage 
of English Grammars. Proceedings of DRAPA 
workshop on Speech and Natural Language. 
pages306-311. Pacific Grove, CA. DRAPA.  
Collins M.J. 1997. Three Generative, Lexicalised 
Models for Statistical Parsing. Proceedings of the 
Thirtieth-Five Annual Meeting of the Association 
for Computational Linguistics (ACL?97). 
pages184-191. 
Feldman R. 1997. Text Mining - Theory and 
Practice. Proceedings of the Third International 
Conference on Knowledge Discovery & Data 
Mining (KDD?1997).  
Rabiner L. 1989. A Tutorial on Hidden Markov 
Models and Selected Applications in Speech 
Recognition. IEEE 77(2), pages257-285.  
Tjong K.S. Erik and Buchholz S. 2000. Introduction 
to the CoNLL-2000 Shared Task: Chunking. 
Proceedings of the Conference on Computational 
Language Learning (CoNLL'2000). 
Pages127-132. Lisbon, Portugal. 11-14 Sept. 
Viterbi A.J. 1967. Error Bounds for Convolutional 
Codes and an Asymptotically Optimum Decoding 
Algorithm. IEEE Transactions on Information 
Theory, IT 13(2), 260-269. 
Watson B. and Tsoi A Chunk. 1992. Second order 
Hidden Markov Models for speech recognition. 
Proceeding of the Fourth Australian 
International Conference on Speech Science and 
Technology. pages146-151.  
Zhou GuoDong and Su Jian. 2000a. Error-driven 
HMM-based Chunk Tagger with 
Context-dependent Lexicon. Proceedings of the 
Joint Conference on Empirical Methods on 
Natural Language Processing and Very Large 
Corpus (EMNLP/ VLC'2000). Hong Kong, 7-8 
Oct. 
Zhou GuoDong, Su Jian and Tey TongGuan. 
2000b. Hybrid Text Chunking. Proceedings of 
the Conference on Computational Language 
Learning (CoNLL'2000). Pages163-166. Lisbon, 
Portugal, 11-14 Sept. 
 
Chunking-based Chinese Word Tokenization 
 
GuoDong ZHOU 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore, 119613 
zhougd@i2r.a-star.edu.sg 
 
)()(
),(
log)(log)|(log
11
11
111 nn
nn
nnn
GPTP
GTP
TPGTP ?+=  Abstract  
This paper introduces a Chinese word 
tokenization system through HMM-based 
chunking. Experiments show that such a 
system can well deal with the unknown word 
problem in Chinese word tokenization. 
The second term in (2-1) is the mutual 
information between T  and . In order to 
simplify the computation of this term, we assume 
mutual information independence (2-2): 
n
1
nG1
?
=
= n
i
n
i
nn GtMIGTMI
1
111 ),(),(  or 
?
=
=?
n
i
nn
nn
GPTP
GTP
111
11 log
)()(
),(
log ? ni
n
i
GPtP
GtP
1
1
)()(
),(
    
1  Introduction 
Word Tokenization is regarded as one of major 
bottlenecks in Chinese Language Processing. 
Normally, word tokenization is implemented 
through word segmentation in Chinese Language 
Processing literature.  This is also affected in the 
title of this competition. 
That is, an individual tag is only dependent on the 
token sequence G  and independent on other tags 
in the tag sequence T . This assumption is 
reasonable because the dependence among the tags 
in the tag sequence T  has already been captured 
by the first term in equation (2-1).  Applying it to 
equation (2-1), we have (2-3): 
n
1
n
1
n
1
There exists two major problems in Chinese 
word segmentation: ambiguity and unknown word 
detection. While ngarm modeling and/or word 
co-ocurrence has been successfully applied to deal 
with ambiguity problem, unknown word detection 
has become major bottleneck in word tokenization. 
This paper proposes a HMM-based chunking 
scheme to cope with unkown words in Chinese 
word tokenization. The unknown word detection is 
re-casted as chunking several words 
(single-character word or multi-character word) 
together to form a new word. 
?
?
=
=
+
?=
n
i
n
i
n
i
i
nnn
GtP
tPTPGTP
1
1
1
111
)|(log
)(log)(log)|(log
    
From equation (2-3), we can see that: 
? The first term can be computed by applying 
chain rules. In ngram modeling, each tag is 
assumed to be probabilistically dependent on 
the N-1 previous tags.  
2 HMM-based Chunking 
2.1  HMM 
? The second term is the summation of log 
probabilities of all the individual tags. 
Given an input sequence , the goal 
of Chunking is to find a stochastic optimal tag 
sequence  that maximizes (Zhou and 
Su 2000) (2-1) 
n
n gggG L211 =
n
n tttT L211 = ? The third term corresponds to the ?lexical? component (dictionary) of the tagger.  
We will not discuss either the first or the second 
term further in this paper because ngram modeling 
has been well studied in the literature. We will focus 
on the third term . ?
=
n
i
n
i GtP
1
1 )|(log
2.2 Chinese Word Tokenization 
Given the previous HMM, for Chinese word 
tokenization, we have (Zhou and Su 2002): 
? ;  is the word 
sequence;  is the word 
formation pattern sequence and  is the word 
formation pattern of . Here  consists of: 
>=< iii wpg ,
nP1
n
n wwwW L211 =
nppp L21
iw p
=
ip
i
o The percentage of w  occurring as a whole 
word (round to 10%) 
i
o The percentage of w  occurring at the 
beginning of other words (round to 10%) 
i
o The percentage of  occurring at the end of 
other words (round to 10%) 
iw
o The length of  iw
o The occurring frequence feature, which is set 
to max(log(Frequence), 9 ). 
? tag : Here, a word is regarded as a chunk 
(called "Word-Chunk") and the tags are used to 
bracket and differentiate various types of 
Word-chunks. Chinese word tokenization can be 
regarded as a bracketing process while 
differentiation of different word types can help 
the bracketing process. For convenience, here 
the tag used in Chinese word tokenization is 
called ?Word-chunk tag?.  The Word-chunk tag 
 is structural and consists of three parts: 
it
it
o Boundary category (B): it is a set of four 
values: 0,1,2,3, where 0 means that current 
word is a whole entity and 1/2/3 means that 
current word is at the beginning/in the 
middle/at the end of a word. 
o Word category (W): used to denote the class 
of the word. In our system, word is classified 
into two types: pure Chinese word  type and 
mixed word type (for example, including 
English characters/Chinese 
digits/Chinesenumbers).  
o Word Formation Pattern(P): Because of 
the limited number of boundary and word 
categories, the word formation pattern is 
added into the structural chunk tag to 
represent more accurate models. 
3 Context-dependent Lexicons 
The major problem with Chunking-based Chinese 
word tokenization is how to effectively 
approximate . This can be done by 
adding lexical entries with more contextual 
information into the lexicon ? . In the following, 
we will discuss five context-dependent lexicons 
which consider different contextual information. 
)/( 1
n
i GtP
3.1 Context of current word formation pattern 
and current word 
Here, we assume: 
??
?
??
??=
iiii
iiiiin
i wpptP
wpwptP
GtP
)/(
)/(
)/( 1  
where 
? = },{ Cwpwp iiii ? +  and  is a 
word formation pattern and word pair existing in the 
training data C .  
},{ Cpp ii ? ii wp
3.2 Context of previous word formation pattern 
and current word formation pattern 
Here, we assume : 
??
?
??
??=
?
??
iiii
iiiii
n
i
ppptP
pppptP
GtP
1
11
1
)/(
)/(
)/(
 
where 
? = },{ 11 Cpppp iiii ??? + },{ Cpp ii ?  and 
 is a pair of previous word formation pattern 
and current word formation pattern existing in the 
training data C . 
ip1ip ?
3.3 Context of previous word formation pattern, 
previous word and current word formation 
pattern  
Here, we assume :  
??
?
??
??=
??
????
iiiii
iiiiiii
n
i
pwpptP
pwppwptP
GtP
11
1111
1
)/(
)/(
)/(
  
where  
? = +},{ 1111 Cpwppwp iiiiii ????? },{ Cpp ii ? , 
where  is a triple pattern existing in the 
training corpus. 
iii pwp 11 ??
3.4 Context of previous word formation pattern, 
current word formation pattern and current 
word 
Here, we assume :  
??
?
??
??=
?
??
iiiii
iiiiiii
n
i
wppptP
wppwpptP
GtP
1
11
1
)/(
)/(
)/(
  
where  
? = +},{ 11 Cwppwpp iiiiii ??? },{ Cpp ii ? , 
where  is a triple pattern.  ii wp1?ip
3.5 Context of previous word formation pattern, 
previous word, current word formation pattern 
and current word 
Here, the context of previous word formation 
pattern, previous word, current word formation 
pattern and current word is used as a lexical entry to 
determine the current structural chunk tag and ? = 
+},{ 1111 Cwpwpwpwp iiiiiiii ????? },{ Cpp ii ? , 
where  is a pattern existing in the 
training corpus. Due to memory limitation, only 
lexical entries which occurs at least 3 times are kept.  
iiii wpwp 11 ??
4  Error-Driven Learning 
In order to reduce the size of lexicon effectively, an 
error-driven learning approach is adopted to 
examine the effectiveness of lexical entries and 
make it possible to further improve the chunking 
accuracy by merging all the above 
context-dependent lexicons in a single lexicon.  
For a new lexical entry e , the effectiveness 
 is measured by the reduction in error which 
results from adding the lexical entry to the lexicon : 
. Here, 
 is the chunking error number of the 
lexical entry e  for the old lexicon 
i
( ie
)( ieF?
)( ieF?
(Error eF?
))( Errori
Error FeF ??+?? ?=
)i
i ?  and 
 is the chunking error number of the 
lexical entry e  for the new lexicon 
)i
i
(Error eF ??+?
??+?  
where ???ie ( ??
i
 is the list of new lexical 
entries added to the old lexicon ). If , 
we define the lexical entry e  as positive for 
lexicon 
? 0( >)ie?F
? . Otherwise, the lexical entry e  is 
negative for lexicon 
i
? .  
5 Implementation 
In training process, only the words occurs at least 5 
times are kept in the training corpus and in the word 
table while those less-freqently occurred words are 
separated into short words (most of such short 
words are single-character words) to simulate the 
chunking. That is, those less-frequently words are 
regarded as chunked from several short words. 
In word tokenization process, the 
Chunking-based Chinese word tokenization can be 
implemented as follows:  
1) Given an input sentence, a lattice of word and 
word formation pattern pair is generated by 
skimming the sentence from left-to-right, 
looking up the word table to determine all the 
possible words, and determining the word 
formation pattern for each possible word. 
2) Viterbi algorithm is applied to decode the  
lattice to find the most possible tag sequence. 
3) In this way, the given sentence is chunked into  
words with word category information 
discarded. 
6 Experimental Results 
Table 1 shows the performance of our 
chunking-based Chinese word tokenization in the 
competition.  
 
 PK (closed, 
official) 
CTB (closed, 
unofficial) 
Precision 94.5 90.7 
Recall 93.6 89.6 
F 94.0 90.1 
OOV 6.9 18.1 
Recall on OOV 76.3 75.2 
Recall on In-Voc 94.9 92.7 
Speed on P1.8G 420 KB/min 390 KB/min 
The most important advantage of 
chunking-based Chinese word segmentation is the 
ability to cope with the unknown words. Table 1 
shows that about 75% of the unknown words can be 
detected correctly using the chunking approach on 
the PK and CTB corpus. 
 
7 Conclusion 
This paper proposes a HMM-based chunking 
scheme to cope with the unkown words in Chinese 
word tokenization. In the meantime, error-driven 
learning is applied to effectively incorporate 
various context-dependent information. 
Experiments show that such a system can well deal 
with the unknown word problem in Chinese word 
tokenization. 
References  
Rabiner L. 1989. A Tutorial on Hidden Markov 
Models and Selected Applications in Speech 
Recognition. IEEE 77(2), pages257-285.  
Viterbi A.J. 1967. Error Bounds for Convolutional 
Codes and an Asymptotically Optimum Decoding 
Algorithm. IEEE Transactions on Information 
Theory, IT 13(2), 260-269. 
Zhou GuoDong and Su Jian. 2000. Error-driven 
HMM-based Chunk Tagger with 
Context-dependent Lexicon. Proceedings of the 
Joint Conference on Empirical Methods on 
Natural Language Processing and Very Large 
Corpus (EMNLP/ VLC'2000). Hong Kong, 7-8 
Oct. 
Recognizing Names in Biomedical Texts  
using Hidden Markov Model and SVM plus Sigmoid 
ZHOU GuoDong 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
Email: zhougd@i2r.a-star.edu.sg  
 
 
ABSTRACT 
In this paper, we present a named entity 
recognition system in the biomedical domain, 
called PowerBioNE. In order to deal with the 
special phenomena in the biomedical domain, 
various evidential features are proposed and 
integrated through a Hidden Markov Model 
(HMM). In addition, a Support Vector Machine 
(SVM) plus sigmoid is proposed to resolve the 
data sparseness problem in our system. Finally, 
we present two post-processing modules to deal 
with the cascaded entity name and abbreviation 
phenomena. Evaluation shows that our system 
achieves the F-measure of 69.1 and 71.2 on the 23 
classes of GENIA V1.1 and V3.0 respectively. In 
particular, our system achieves the F-measure of 
77.8 on the ?protein? class of GENIA V3.0. It 
shows that our system outperforms the best 
published system on GENIA V1.1 and V3.0. 
1. INTRODUCTION 
With an overwhelming amount of textual 
information in molecular biology and biomedicine, 
there is a need for effective and efficient literature 
mining and knowledge discovery that can help 
biologists to gather and make use of the knowledge 
encoded in text documents. In order to make 
organized and structured information available, 
automatically recognizing biomedical entity names 
becomes critical and is important for protein-
protein interaction extraction, pathway 
construction, automatic database curation, etc. 
Such a task, called named entity recognition, 
has been well developed in the Information 
Extraction literature (MUC-6; MUC-7). In MUC, 
the task of named entity recognition is to recognize 
the names of persons, locations, organizations, etc. 
in the newswire domain. In the biomedical domain, 
we care about entities like gene, protein, virus, etc. 
In recent years, many explorations have been done 
to port existing named entity recognition systems 
into the biomedical domain (Kazama et al2002; 
Lee et al2003; Shen et al2003; Zhou et al2004). 
However, few of them have achieved satisfactory 
performance due to the special characteristics in 
the biomedical domain, such as long and 
descriptive naming conventions, conjunctive and 
disjunctive structure, causal naming convention 
and rapidly emerging new biomedical names, 
abbreviation, and cascaded construction. On all 
accounts, we can say that the entity names in the 
biomedical domain are much more complex than 
those in the newswire domain.  
In this paper, we present a named entity 
recognition system in the biomedical domain, 
called PowerBioNE. In order to deal with the 
special phenomena in the biomedical domain, 
various evidential features are proposed and 
integrated effectively and efficiently through a 
Hidden Markov Model (HMM). In addition, a 
Support Vector Machine (SVM) plus sigmoid is 
proposed to resolve the data sparseness problem in 
our system. Finally, we present two post-
processing modules to deal with the cascaded 
entity name and abbreviation phenomena to further 
improve the performance.  
All of our experiments are done on the GENIA 
corpus, which is the largest annotated corpus in the 
molecular biology domain available to public 
(Ohta et al 2002).  In our experiments, two 
versions are used: 1) Genia V1.1 which contains     
670 MEDLINE abstracts of 123K words; 2) Genia 
V3.0 which is a superset of GENIA V1.1 and 
contains 2000 MEDLINE abstracts of 360K words. 
The annotation of biomedical entities is based on 
the GENIA ontology (Ohta et al 2002), which 
includes 23 distinct classes: multi-cell, mono-cell, 
virus, body part, tissue, cell type, cell component, 
organism, cell line, other artificial source, protein, 
peptide, amino acid monomer, DNA, RNA, poly 
nucleotide, nucleotide, lipid, carbohydrate, other 
organic compound, inorganic, atom and other. 
2. FEATURES 
In order to deal with the special phenomena in the 
biomedical domain, various evidential features are 
explored. 
? Word Formation Pattern (FWFP): The purpose 
of this feature is to capture capitalization, 
digitalization and other word formation 
1
information. This feature has been widely used in 
the biomedical domain (Kazama et al2002; Shen 
et al2003; Zhou et al2004). In this paper, the 
same feature as in Shen et al2003 is used. 
? Morphological Pattern (FMP): Morphological 
information, such as prefix and suffix, is 
considered as an important cue for terminology 
identification and has been widely applied in the 
biomedical domain (Kazama et al2002; Lee et al
2003; Shen et al2003; Zhou et al2004).  Same as 
Shen et al2003, we use a statistical method to get 
the most useful prefixes/suffixes from the training 
data.  
? Part-of-Speech (FPOS): Since many of the 
words in biomedical entity names are in lowercase, 
capitalization information in the biomedical 
domain is not as evidential as that in the newswire 
domain. Moreover, many biomedical entity names 
are descriptive and very long. Therefore, POS may 
provide useful evidence about the boundaries of 
biomedical entity names. 
? Head Noun Trigger (FHEAD): The head noun, 
which is the major noun of a noun phrase, often 
describes the function or the property of the noun 
phrase. In this paper, we automatically extract 
unigram and bigram head nouns from the training 
data, and rank them by frequency. For each entity 
class, we select 50% of top ranked head nouns as 
head noun triggers.  Table 1 shows some of the 
examples.  
Table 1: Examples of auto-generated head nouns 
Class Unigram  bigram  
interleukin activator protein 
interferon binding protein 
PROTEIN 
kinase cell receptor 
DNA X chromosome 
cDNA binding motif 
DNA 
chromosome promoter element 
? Name Alias Feature (FALIAS): Besides the 
above widely used features, we also propose a 
novel name alias feature. The intuition behind this 
feature is the name alias phenomenon that relevant 
entities will be referred to in many ways 
throughout a given text and thus success of named 
entity recognition is conditional on success at 
determining when one noun phrase refers to the 
very same entity as another noun phrase.  
During decoding, the entity names already 
recognized from the previous sentences of the 
document are stored in a list. When the system 
encounters an entity name candidate (e.g. a word 
with a special word formation pattern), a name 
alias algorithm (similar to Schwartz et al2003) is 
invoked to first dynamically determine whether the 
entity name candidate might be alias for a 
previously recognized name in the recognized list. 
This is done by checking whether all the characters 
in the entity name candidate exist in a recognized 
entity name in the same order and whether the first 
character in the entity name candidate is same as 
the first character in the recognized name. For a 
relevant work, please see Jacquemin (2001). The 
name alias feature FALIAS is represented as 
ENTITYnLm (L indicates the locality of the name 
alias phenomenon). Here ENTITY indicates the 
class of the recognized entity name and n indicates 
the number of the words in the recognized entity 
name while m indicates the number of the words in 
the recognized entity name from which the name 
alias candidate is formed.  For example, when the 
decoding process encounters the word ?TCF?, the 
word ?TCF? is proposed as an entity name 
candidate and the name alias algorithm is invoked 
to check if the word ?TCF? is an alias of a 
recognized named entity. If ?T cell Factor? is a 
?Protein? name recognized earlier in the 
document, the word ?TCF? is determined as an 
alias of ?T cell Factor? with the name alias feature 
Protein3L3 by taking the three initial letters of the 
three-word ?protein? name ?T cell Factor?. 
3. METHODS 
3.1 Hidden Markov Model 
Given above various features, the key problem is 
how to effectively and efficiently integrate them 
together and find the optimal resolution to 
biomedical named entity recognition. Here, we use 
the Hidden Markov Model (HMM) as described in 
Zhou et al2002.  A HMM is a model where a 
sequence of outputs is generated in addition to the 
Markov state sequence. It is a latent variable model 
in the sense that only the output sequence is 
observed while the state sequence remains 
?hidden?. 
Given an observation sequence O , 
the purpose of a HMM is to find the most likely 
state sequence S  that maximizes 
. Here, the observation o , 
where  is the word and 
 is the 
feature set of the word w , and the state  is 
structural and s
n
n ooo ...211 =
>=< iii wf ,
>
is
ii FEATURE_
n
n sss ...211 =
i
HEAD
i
POS FF ,,
i
iBOUNDARY _
)|( 11
nn OSP
=< ii Ff
iw
F, iALIAS
i
MPWFP F,
i ENTITY= , 
where  denotes the position of the 
current word in the entity; ENTITY  indicates the 
class of the entity; and FEATURE  is the feature set 
used to model the ngram more precisely.  
i
i
i
BOUNDARY
By rewriting , we have: )|(log 11 nn OSP
2
)()(
),(log)(log)|(log
11
11
111 nn
nn
nnn
OPSP
OSPSPOSP ?+=          (1) 
The second term in Equation (1) is the mutual 
information between S  and . In order to 
simplify the computation of this term, we assume 
mutual information independence:  
n
1
nO1
?
=
=
n
i
n
i
nn OsMIOSMI
1
111 ),(),(  or  
?
= ?
=?
n
i
n
i
n
i
nn
nn
OPsP
OsP
OPSP
OSP
1 1
1
11
11
)()(
),(log
)()(
),(log                (2) 
That is, an individual tag is only dependent on the 
output sequence O  and independent on other tags 
in the tag sequence S . This assumption is 
reasonable because the dependence among the tags 
in the tag sequence S  has already been captured 
by the first term in Equation (1).  Applying the 
assumption (2) to Equation (1), we have: 
n
1
n
1
n
1
?
?
=
=
+
?=
n
i
n
i
n
i
i
nnn
OsP
sPSPOSP
1
1
1
111
)|(log
)(log)(log)|(log
                 (3) 
From Equation (3), we can see that: 
? The first term can be computed by applying 
chain rules. In ngram modeling (Chen et al1996), 
each tag is assumed to be dependent on the N-1 
previous tags.  
? The second term is the summation of log 
probabilities of all the individual tags. 
? The third term corresponds to the ?lexical? 
component (dictionary) of the tagger.   
The idea behind the model is that it tries to 
assign each output an appropriate tag (state), which 
contains boundary and class information.  For 
example, ?TCF 1 binds stronger than NF kB to 
TCEd DNA?. The tag assigned to token ?TCF? 
should indicate that it is at the beginning of an 
entity name and it belongs to the ?Protein? class; 
and the tag assigned to token ?binds? should 
indicate that it does not belong to an entity name.  
Here, the Viterbi algorithm (Viterbi 1967) is 
implemented to find the most likely tag sequence.  
The problem with the above HMM lies in the 
data sparseness problem raised by P  in the 
third term of Equation (3). Ideally, we would have 
sufficient training data for every event whose 
conditional probability we wish to calculate. 
Unfortunately, there is rarely enough training data 
to compute accurate probabilities when decoding 
on new data. Generally, two smoothing approaches 
(Chen et al1996) are applied to resolve this 
problem: linear interpolation and back-off. 
However, these two approaches only work well 
when the number of different information sources 
is limited. When a few features and/or a long 
context are considered, the number of different 
information sources is exponential. In this paper, a 
Support Vector Machine (SVM) plus sigmoid is 
proposed to resolve this problem in our system. 
)|( 1
n
i Os
3.2 Support Vector Machine plus Sigmoid 
Support Vector Machines (SVMs) are a popular 
machine learning approach first presented by 
Vapnik (1995). Based on the structural risk 
minimization of statistical learning theory, SVMs 
seek an optimal separating hyper-plane to divide 
the training examples into two classes and make 
decisions based on support vectors which are 
selected as the only effective examples in the 
training set. However, SVMs produce an un-
calibrated value that is not probability. That is, the 
unthresholded output of an SVM can be 
represented as 
?
?
+??=
SVi
iii bxxkyaxf ),()(                 (4) 
To map the SVM output into the probability, we 
train an additional sigmoid model(Platt 1999): 
)exp(1
1)|(
BAf
fsp
i
ii ++=                (5) 
Basically, SVMs are binary classifiers. 
Therefore, we must extend SVMs to multi-class 
(e.g. K) classifiers. For efficiency, we apply the 
one vs. others strategy, which builds K classifiers 
so as to separate one class from all others, instead 
of the pairwise strategy, which builds K*(K-1)/2 
classifiers considering all pairs of classes. 
Moreover, we only apply the simple linear kernel, 
although other kernels (e.g. polynomial kernel) and 
pairwise strategy can have better performance. 
Finally, for each state s , there is one sigmoid 
. Therefore, the sigmoid outputs are 
normalized to get a probability distribution using 
i
)|( ii fsp
?=
i
ii
ii
fsp
fsp
)|(
)|(n
i Osp )|( 1 . 
3.3 Post-Processing 
Two post-processing modules, namely cascaded 
entity name resolution and abbreviation resolution, 
are applied in our system to further improve the 
performance. 
Cascaded Entity Name Resolution 
It is found (Shen et al2003) that 16.57% of entity 
names in GENIA V3.0 have cascaded 
constructions, e.g.  
<RNA><DNA>CIITA</DNA> mRNA</RNA>.   
Therefore, it is important to resolve such 
phenomenon.  
Here, a pattern-based module is proposed to 
resolve the cascaded entity names while the above 
HMM is applied to recognize embedded entity 
3
names and non-cascaded entity names. In the 
GENIA corpus, we find that there are six useful 
patterns of cascaded entity name constructions: 
? <ENTITY> := <ENTITY> + head noun, e.g. 
<PROTEIN> binding motif?<DNA> 
? <ENTITY> := <ENTITY>  + <ENTITY>, e.g. 
<LIPID> <PROTEIN>?<PROTEIN> 
? <ENTITY> := modifier + <ENTITY>, e.g.          
anti <Protein>?<Protein> 
? <ENTITY> := <ENTITY>  + word + 
<ENTITY>, e.g. 
<VIRUS> infected 
<MULTICELL>?<MULTICELL > 
? <ENTITY> :=  modifier + <ENTITY> + head 
noun 
? <ENTITY> := <ENTITY> +  <ENTITY>  + 
head noun 
In our experiments, all the rules of above six 
patterns are extracted from the cascaded entity 
names in the training data to deal with the 
cascaded entity name phenomenon. 
Abbreviation Resolution 
While the name alias feature is useful to detect the 
inter-sentential name alias phenomenon, it is 
unable to identify the inner-sentential name alias 
phenomenon: the inner-sentential abbreviation.  
Such abbreviations widely occur in the biomedical 
domain.   
In our system, we present an effective and 
efficient algorithm to recognize the inner-sentential 
abbreviations more accurately by mapping them to 
their full expanded forms. In the GENIA corpus, 
we observe that the expanded form and its 
abbreviation often occur together via parentheses. 
Generally, there are two patterns: ?expanded form 
(abbreviation)? and ?abbreviation (expanded 
form)?.  
Our algorithm is based on the fact that it is 
much harder to classify an abbreviation than its 
expanded form. Generally, the expanded form is 
more evidential than its abbreviation to determine 
its class.  The algorithm works as follows: Given a 
sentence with parentheses, we use a similar 
algorithm as in Schwartz et al2003 to determine 
whether it is an abbreviation with parentheses. This 
is done by starting from the end of both the 
abbreviation and the expanded form, moving from 
right to left and trying to find the shortest 
expanded form that matches the abbreviation. Any 
character in the expanded form can match a 
character in the abbreviation with one exception: 
the match of the character at the beginning of the 
abbreviation must match the first alphabetic 
character of the first word in the expanded form. If 
yes, we remove the abbreviation and the 
parentheses from the sentence. After the sentence 
is processed, we restore the abbreviation with 
parentheses to its original position in the sentence.  
Then, the abbreviation is classified as the same 
class of the expanded form, if the expanded form is 
recognized as an entity name. In the meanwhile, 
we also adjust the boundaries of the expanded form 
according to the abbreviation, if necessary. Finally, 
the expanded form and its abbreviation are stored 
in the recognized list of biomedical entity names 
from the document to help the resolution of 
forthcoming occurrences of the same abbreviation 
in the document. 
4. EXPERIMENTS AND EVALUATION 
We evaluate our PowerBioNE system on GENIA 
V1.1 and GENIA V3.0 using precision/recall/F-
measure. For each evaluation, we select 20% of the 
corpus as the held-out test data and the remaining 
80% as the training data. All the experimentations 
are done 5 times and the evaluations are averaged 
over the held-out test data. For cascaded entity 
name resolution, an average of 59 and 97 rules are 
extracted from the cascaded entity names in the 
training data of GENIA V1.1 and V3.0 
respectively. For POS, all the POS taggers are 
trained on the training data with POS imported 
from the corresponding GENIA V3.02p with POS 
annotated. 
Table 2 shows the performance of our system 
on GENIA V1.1 and GENIA V3.0, and the 
comparison with that of the best reported system 
(Shen et al2003). It shows that our system 
achieves the F-measure of 69.1 on GENIA V1.1 
and the F-measure of 71.2 on GENIA V3.0 
respectively, without help of any dictionaries. It 
also shows that our system outperforms Shen et al
(2003) by 6.9 in F-measure on GENIA V1.1 and 
4.6 in F-measure on GENIA V3.0. This is largely 
due to the superiority of the SVM plus sigmoid in 
our system (improvement of 3.7 in F-measure on 
GENIA V3.0) over the back-off approach in Shen 
et al(2003) and the novel name alias feature 
(improvement of 1.2 in F-measure on GENIA 
V3.0). Finally, evaluation also shows that the 
cascaded entity name resolution and the 
abbreviation resolution contribute 3.4 and 2.1 
respectively in F-measure on GENIA V3.0. 
Table 2: Performance of our PowerBioNE system  
Performance P R F 
Shen et alon GENIA V3.0 66.5 66.6 66.6
Shen et al on GENIA V1.1 63.1 61.2 62.2
Our system on GENIA V3.0 72.7 69.8 71.2
Our system on GENIA V1.1 70.4 67.9 69.1
4
Table 3: Performance of different entity classes on 
GENIA V3.0 
Entity 
Class 
Number of instances in 
the training data 
F 
Cell Type 6034 81.8 
Lipid 1602 68.6 
Multi-Cell 1463 78.1 
Protein 21380 77.8 
DNA 7538 70.8 
Cell Line 3216 68.5 
RNA 695 56.2 
Virus 873 67.2 
One important question is about the 
performance of different entity classes. Table 3 
shows the performance of some of the biomedical 
entity classes on GENIA V3.0. Of particular 
interest, our system achieves the F-measure of 77.8 
on the class ?Protein?. It shows that the 
performance varies a lot among different entity 
classes.  One reason may be due to different 
difficulties in recognizing different entity classes. 
Another reason may be due to the different 
numbers of instances in different entity classes. 
Though GENIA V3.0 provides a good basis for 
named entity recognition in the biomedical domain 
and probably the best available, it has clear bias. 
Table 3 shows that, while GENIA V3.0 is of 
enough size for recognizing the major classes, such 
as ?Protein?, ?Cell Type?, ?Cell Line?, ?Lipid? 
etc, it is of limited size in recognizing other classes, 
such as  ?Virus?.  
5. ERROR ANALYSIS 
In order to further evaluate our system and explore 
possible improvement, we have implemented an 
error analysis. This is done by randomly choosing 
100 errors from our recognition results. During the 
error analysis, we find many errors are due to the 
strict annotation scheme and the annotation 
inconsistence in the GENIA corpus, and can be 
considered acceptable. Therefore, we will also 
examine the acceptable F-measure of our system, 
in particular, the acceptable F-measure on the 
?protein? class.  
All the 100 errors are classified as follows: 
? Left boundary errors (14): It includes the 
errors with correct class identification, correct right 
boundary detection and only wrong left boundary 
detection. We find that most of such errors come 
from the long and descriptive naming convention. 
We also find that 11 of 14 errors are acceptable 
and ignorance of the descriptive words often does 
not make a much difference for the entity names. 
In fact, it is even hard for biologists to decide 
whether the descriptive words should be a part of 
the entity names, such as ?normal?, ?activated?, 
etc.  In particular, 4 of 14 errors belong to the 
?protein? class. Among them, two errors are 
acceptable, e.g. ?classical <PROTEIN>1,25 (OH) 
2D3 receptor</PROTEIN>? => 
?<PROTEIN>classical 1,25 (OH) 2D3 
receptor</PROTEIN>? (with format of 
?annotation in the corpus => identification made 
by our system?), while the other two are 
unacceptable, e.g. ?<PROTEIN>viral 
transcription factor</PROTEIN> => viral 
<PROTEIN>transcription factor</PROTEIN>?. 
? Cascaded entity name errors (15): It includes 
the errors caused by the cascaded entity name 
phenomenon. We find that most of such errors 
come from the annotation inconsistence in the 
GENIA corpus: In some cases, only the embedded 
entity names are annotated while in other cases, the 
embedded entity names are not annotated. Our 
system tends to annotate both the embedded entity 
names and the whole entity names. Among them, 
we find that 13 of 16 errors are acceptable.  In 
particular, 2 of 16 errors belong to the ?protein? 
class and both are acceptable, e.g. ?<DNA>NF 
kappa B binding site</DNA>? => 
?<DNA><PROTEIN>NF kappa B</PROTEIN> 
binding site</DNA>?. 
? Misclassification errors (18): It includes the 
errors with wrong class identification, correct right 
boundary detection and correct left boundary 
detection. We find that this kind of errors mainly 
comes from the sense ambiguity of biomedical 
entity names and is very difficult to disambiguate. 
Among them, 8 errors are related with the ?DNA? 
class and 6 errors are related with the ?Cell Line? 
and ?Cell Type? classes. We also find that only 3 
of 18 errors are acceptable. In particular, there are 
6 errors related to the ?protein? class. Finally, we 
find that all the 6 errors are caused by 
misclassification of the ?DNA? class to the 
?protein? class and all of them are unacceptable, 
e.g. ?<DNA>type I IFN<DNA>? => 
?<PROTEIN>type I IFN</PROTEIN>?.  
? True negative (23): It includes the errors by 
missing the identification of biomedical entity 
names. We find that 16 errors come from the 
?other? class and 10 errors from the ?protein? class. 
We also find that the GENIA corpus annotates 
some general noun phrases as biomedical entity 
names, e.g. ?protein? in ?the protein? and 
?cofactor? in ?a cofactor?. Finally, we find that 11 
of 23 errors are acceptable. In particular, 9 of 23 
errors related to the ?protein? class. Among them, 
3 errors are acceptable, e.g. ?the 
<PROTEIN>protein</PROTEIN> => ?the 
5
protein?, while the other 6 are unacceptable, e.g. 
? <PROTEIN>80 kDa</PROTEIN> => ?80 kDa?. 
? False positive (15):  It includes the errors by 
wrongly identifying biomedical entity names 
which are not annotated in the GENIA corpus. We 
find that 9 of 15 errors come from the ?other? class. 
This suggests that the annotation of the ?other? 
class is much lack of consistency and most 
problematic in the GENIA corpus. We also find 
that 7 of 15 errors are acceptable. In particular, 2 of 
15 errors are related to the ?protein? class and both 
are acceptable, e.g. ?affinity sites?=> 
?<PROTEIN>affinity sites</PROTEIN>?. 
? Miscellaneous (14): It includes all the other 
errors, e.g. combination of the above errors and the 
errors caused by parentheses. We find that only 1 
of 14 errors is acceptable. We also find that, 
among them, 2 errors are related with the ?protein? 
class and both are unacceptable, e.g. 
?<PROTEIN>17 amino acid 
epitope</PROTEIN>? => ?17 <RNA>amino acid 
epitope</RNA>?. 
From above error analysis, we find that about 
half (46/100) of errors are acceptable and can be 
avoided by flexible annotation scheme (e.g. 
regarding the modifiers in the left boundaries) and 
consistent annotation (e.g. in the annotation of the 
?other? class and the cascaded entity name 
phenomenon). In particular, about one third (9/25) 
of errors are acceptable on the ?protein? class. This 
means that the acceptable F-measure can reach 
about 84.4 on the 23 classes of GENIA V3.0. In 
particular, the acceptable F-measure on the 
?protein? class is about 85.8. In addition, this 
performance is achieved without using any extra 
resources (e.g. dictionaries). With help of extra 
resources, we think an acceptable F-measure of 
near 90 can be achieved in the near future. 
6. RELATED WORK 
Previous approaches in biomedical named entity 
recognition typically use some domain specific 
heuristic rules and heavily rely on existing 
dictionaries (Fukuda et al1998, Proux et al1998 
and Gaizauskas et al2000). 
The current trend is to apply machine learning 
approaches in biomedical named entity recognition, 
largely due to the development of the GENIA 
corpus. The typical explorations include Kazama et 
al 2002, Lee et al2003, Tsuruoka et al2003, Shen 
et al2003. Kazama et al2002 applies SVM and 
incorporates a rich feature set, including word 
feature, POS, prefix feature, suffix feature, 
previous class feature, word cache feature and 
HMM state feature. The experiment on GENIA 
V1.1 shows the F-measure of 54.4. Tsuruoka et al
2003 applies a dictionary-based approach and a 
na?ve Bayes classifier to filter out false positives. It 
only evaluates against the ?protein? class in 
GENIA V3.0, and receives the F-measure of 70.2 
with help of a large dictionary. Lee et al2003 uses 
a two phase SVM-based recognition approach and 
incorporates word formation pattern and part-of-
speech. The evaluation on GENIA V3.0 shows the 
F-measure of 66.5 with help of an entity name 
dictionary. Shen et al2003 proposes a HMM-based 
approach and two post-processing modules 
(cascaded entity name resolution and abbreviation 
resolution). Evaluation shows the F-measure of 
62.2 and 66.6 on GENIA V1.1 and V3.0 
respectively. 
7. CONCLUSION 
In the paper, we describe our HMM-based named 
entity recognition system in the biomedical domain, 
named PowerBioNE. Various lexical, 
morphological, syntactic, semantic and discourse 
features are incorporated to cope with the special 
phenomena in biomedical named entity recognition. 
In addition, a SVM plus sigmoid is proposed to 
effectively resolve the data sparseness problem. 
Finally, we present two post-processing modules to 
deal with cascaded entity name and abbreviation 
phenomena.  
The main contributions of our work are the 
novel name alias feature in the biomedical domain, 
the SVM plus sigmoid approach in the effective 
resolution of the data sparseness problem in our 
system and its integration with the Hidden Markov 
Model. 
In the near future, we will further improve the 
performance by investigating more on conjunction 
and disjunction construction, the synonym 
phenomenon, and exploration of extra resources 
(e.g. dictionary).  
REFERENCES 
Chen and Goodman. 1996. An Empirical Study of 
Smoothing Technniques for Language Modeling. 
In Proceedings of the 34th Annual Meeting of the 
Association of Computational Linguistics 
(ACL?1996). pp310-318. Santa Cruz, California, 
USA. 
Fukuda K., Tsunoda T., Tamura A., and Takagi T.  
1998.  Toward information extraction: identifying 
protein names from biological papers.  In Proc. of 
the Pacific Symposium on Biocomputing?98 
(PSB?98), 707-718. 
Gaizauskas R., Demetriou G. and Humphreys K.  
2000. Term Recognition and Classification in 
Biological Science Journal Articles. In Proc. of the 
Computational Terminology for Medical and 
Biological Applications Workshop of the 2nd 
International Conference on NLP, 37-44. 
6
Jacquemin C. 2001. Spotting and Discovering Terms 
through Natural Language Processing, Cambridge: 
MIT Press 
Kazama J., Makino T., Ohta Y., and Tsujii J.  2002.  
Tuning Support Vector Machines for Biomedical 
Named Entity Recognition.  In Proc. of the 
Workshop on Natural Language Processing in the 
Biomedical Domain (at ACL?2002), 1-8. 
Lee K.J. Hwang Y.S. and Rim H.C. Two-phase 
biomedical NE Recognition based on SVMs. In 
Proceedings of the ACL?2003 Workshop on 
Natural Language Processing in Biomedicine. 
pp.33-40.  Sapporo, Japan. 
MUC6. 1995. Morgan Kaufmann Publishers, Inc. In 
Proceedings of the Sixth Message Understanding 
Conference (MUC-6). Columbia, Maryland. 
MUC7. 1998. Morgan Kaufmann Publishers, Inc. In 
Proceedings of the Seventh Message 
Understanding Conference (MUC-7). Fairfax, 
Virginia. 
Ohta T., Tateisi Y., Kim J., Mima H., and Tsujii J.  
2002.  The GENIA corpus: An annotated research 
abstract corpus in molecular biology domain.  In 
Proc. of HLT 2002. 
Platt J. 1999. Probabilistic Outputs for Support 
Vector Machines and comparisions to regularized 
Likelihood Methods. MIT Press. 
Proux D., Rechenmann F., Julliard L., Pillet V. and 
Jacq B.  1998.  Detecting Gene Symbols and 
Names in Biological Texts: A First Step toward 
Pertinent Information Extraction.  In Proc. of 
Genome Inform Ser Workshop Genome Inform, 
72-80. 
Schwartz A.S. and Hearst M.A.  2003.  A Simple 
Algorithm for Identifying Abbreviation 
Definitions in Biomedical Text.  In Proc. of the 
Pacific Symposium on Biocomputing (PSB 2003) 
Kauai. 
Shen Dan, Zhang Jie, Zhou GuoDong, Su Jian and 
Tan Chew Lim, Effective Adaptation of a Hidden 
Markov Model-based Named Entity Recognizer 
for Biomedical Domain, Proceedings of ACL?2003 
Workshop on Natural Language Processing in 
Biomedicine, Sapporo, Japan, 11 July 2003. pp49-
56. 
Tsuruoka Y. and Tsujii J. 2003. Boosting precision 
and recall of dictionary-based protein name 
recognition. In Proceedings of the ACL?2003 
Workshop on Natural Language Processing in 
Biomedicine. pp.41-48.  Sapporo, Japan.  
Vapnik V. 1995. The Nature of Statistical Learning 
Theory. NY, USA: Springer-Verlag. 
Viterbi A.J.  1967.  Error bounds for convolutional 
codes and an asymptotically optimum decoding 
algorithm. IEEE Transactions on Information 
Theory, 260-269. 
Zhou G.D. and Su J.  2002.  Named Entity 
Recognition using an HMM-based Chunk Tagger.  
In Proc. of the 40th Annual Meeting of the 
Association for Computational Linguistics (ACL), 
473-480. 
7
Exploring Deep Knowledge Resources in Biomedical Name Recognition 
 
ZHOU GuoDong    SU Jian 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
Email: {zhougd, sujian}@i2r.a-star.edu.sg  
 
Abstract 
In this paper, we present a named entity 
recognition system in the biomedical domain. In 
order to deal with the special phenomena in the 
biomedical domain, various evidential features are 
proposed and integrated through a Hidden 
Markov Model (HMM). In addition, a Support 
Vector Machine (SVM) plus sigmoid is proposed 
to resolve the data sparseness problem in our 
system. Besides the widely used lexical-level 
features, such as word formation pattern, 
morphological pattern, out-domain POS and 
semantic trigger, we also explore the name alias 
phenomenon, the cascaded entity name 
phenomenon, the use of both a closed dictionary 
from the training corpus and an open dictionary 
from the database term list SwissProt and the alias 
list LocusLink, the abbreviation resolution and in-
domain POS using the GENIA corpus.  
1. The Baseline System 
1.1 Hidden Markov Model 
In this paper, we use the Hidden Markov Model 
(HMM) as described in Zhou et al(2002). Given 
an output sequence O , the system finds 
the most likely state sequence S  that 
maximizes  as follows:  
n
n ooo ...211 =
)
n
n sss ...211 =
|( 11
nn OSP
?
?
=
=
+
?=
n
i
n
i
n
i
i
nnn
OsP
sPSPOSP
1
1
1
111
)|(log
)(log)(log)|(log
            (1) 
From Equation (1), we can see that: 
? The first term can be computed by applying 
chain rules. In ngram modeling (Chen et al1996), 
each tag is assumed to be dependent on the N-1 
previous tags.  
? The second term is the summation of log 
probabilities of all the individual tags. 
? The third term corresponds to the ?lexical? 
component (dictionary) of the tagger.   
The idea behind the model is that it tries to 
assign each output an appropriate tag (state), which 
contains boundary and class information.  For 
example, ?TCF 1 binds stronger than NF kB to 
TCEd DNA?. The tag assigned to token ?TCF? 
should indicate that it is at the beginning of an 
entity name and it belongs to the ?Protein? class; 
and the tag assigned to token ?binds? should 
indicate that it does not belong to an entity name.  
Here, the Viterbi algorithm (Viterbi 1967) is 
implemented to find the most likely tag sequence.  
The problem with the above HMM lies in the 
data sparseness problem raised by P  in the 
third term of Equation (1). In this paper, a Support 
Vector Machine (SVM) plus sigmoid is proposed 
to resolve this problem in our system. 
)|( 1
n
i Os
1.2 Support Vector Machine plus Sigmoid 
Support Vector Machines (SVMs) are a popular 
machine learning approach first presented by 
Vapnik (1995). Based on the structural risk 
minimization of statistical learning theory, SVMs 
seek an optimal separating hyper-plane to divide 
the training examples into two classes and make 
decisions based on support vectors which are 
selected as the only effective examples in the 
training set. However, SVMs produce an un-
calibrated value that is not probability. That is, the 
unthresholded output of an SVM can be 
represented as 
?
?
+??=
SVi
iii bxxkyaxf ),()(                 (2) 
To map the SVM output into the probability, we 
train an additional sigmoid model(Platt 1999): 
)exp(1
1)|1(
BAf
fyp ++==                (3) 
Basically, SVMs are binary classifiers. 
Therefore, we must extend SVMs to multi-class 
(e.g. K) classifiers. For efficiency, we apply the 
one vs. others strategy, which builds K classifiers 
so as to separate one class from all others, instead 
of the pairwise strategy, which builds K*(K-1)/2 
classifiers considering all pairs of classes. 
Moreover, we only apply the simple linear kernel, 
although other kernels (e.g. polynomial kernel) and 
pairwise strategy can have better performance. 
96
1.3 Features 
Various widely used lexical-level features are 
explored in the baseline system. 
? Word Formation Pattern (FWFP): The purpose 
of this feature is to capture capitalization, 
digitalization and other word formation 
information. In this paper, the same feature as in 
Shen et al2003 is used. 
? Morphological Pattern (FMP): Morphological 
information, such as prefix and suffix, is 
considered as an important cue for terminology 
identification.  Same as Shen et al2003, we use a 
statistical method to get the most useful 
prefixes/suffixes from the training data.  
? Part-of-Speech (FPOS): Since many of the 
words in biomedical entity names are in lowercase, 
capitalization information in the biomedical 
domain is not as evidential as that in the newswire 
domain. Moreover, many biomedical entity names 
are descriptive and very long. Therefore, POS may 
provide useful evidence about the boundaries of 
biomedical entity names. In the baseline system, an 
out-domain POS using the PENN TreeBank is 
applied. 
? Head Noun Trigger (FHEAD): The head noun, 
which is the major noun of a noun phrase, often 
describes the function or the property of the noun 
phrase. In this paper, we automatically extract 
unigram and bigram head nouns from the training 
data, and rank them by frequency. For each entity 
class, we select 50% of top ranked head nouns as 
head noun triggers.   
2. Deep Knowledge Resources 
Besides the widely used lexical-level features as 
described above, we also explore the name alias 
phenomenon, the cascaded entity name 
phenomenon, the use of both a closed dictionary 
from the training corpus and an open dictionary 
from the database term list SwissProt and the alias 
list LocusLink, the abbreviation resolution and in-
domain POS using the GENIA corpus. 
2.1 Name Alias Resolution 
A novel name alias feature is proposed to resolve 
the name alias phenomenon. The intuition behind 
this feature is the name alias phenomenon that 
relevant entities will be referred to in many ways 
throughout a given text and thus success of named 
entity recognition is conditional on success at 
determining when one noun phrase refers to the 
very same entity as another noun phrase.  
During decoding, the entity names already 
recognized from the previous sentences of the 
document are stored in a list. When the system 
encounters an entity name candidate (e.g. a word 
with a special word formation pattern), a name 
alias algorithm (similar to Schwartz et al2003) is 
invoked to first dynamically determine whether the 
entity name candidate might be alias for a 
previously recognized name in the recognized list. 
The name alias feature FALIAS is represented as 
ENTITYnLm (L indicates the locality of the name 
alias phenomenon). Here ENTITY indicates the 
class of the recognized entity name and n indicates 
the number of the words in the recognized entity 
name while m indicates the number of the words in 
the recognized entity name from which the name 
alias candidate is formed.  For example, when the 
decoding process encounters the word ?TCF?, the 
word ?TCF? is proposed as an entity name 
candidate and the name alias algorithm is invoked 
to check if the word ?TCF? is an alias of a 
recognized named entity. If ?T cell Factor? is a 
?Protein? name recognized earlier in the 
document, the word ?TCF? is determined as an 
alias of ?T cell Factor? with the name alias feature 
Protein3L3 by taking the three initial letters of the 
three-word ?protein? name ?T cell Factor?. 
2.2 Cascaded Entity Name Resolution 
It is found (Shen et al2003) that 16.57% of entity 
names in GENIA V3.0 have cascaded 
constructions, e.g.  
<RNA><DNA>CIITA</DNA> mRNA</RNA>.   
Therefore, it is important to resolve such 
phenomenon.  
Here, a pattern-based module is proposed to 
resolve the cascaded entity names while the above 
HMM is applied to recognize embedded entity 
names and non-cascaded entity names. In the 
GENIA corpus, we find that there are six useful 
patterns of cascaded entity name constructions: 
? <ENTITY> := <ENTITY> + head noun, e.g. 
<PROTEIN> binding motif?<DNA> 
? <ENTITY> := <ENTITY>  + <ENTITY> 
? <ENTITY> := modifier + <ENTITY>, e.g.          
anti <Protein>?<Protein> 
? <ENTITY> := <ENTITY>  + word + 
<ENTITY> 
? <ENTITY> :=  modifier + <ENTITY> + head 
noun 
? <ENTITY> := <ENTITY> +  <ENTITY>  + 
head noun 
In our experiments, all the rules of above six 
patterns are extracted from the cascaded entity 
names in the GENIA V3.0 to deal with the 
97
cascaded entity name phenomenon where the 
<ENTITY> above is restricted to the five 
categories in the shared task: Protein, DNA, RNA, 
CellLine, CellType.  
2.3 Abbreviation Resolution 
While the name alias feature is useful to detect the 
inter-sentential name alias phenomenon, it is 
unable to identify the inner-sentential name alias 
phenomenon: the inner-sentential abbreviation.  
Such abbreviations widely occur in the biomedical 
domain.   
In our system, we present an effective and 
efficient algorithm to recognize the inner-sentential 
abbreviations more accurately by mapping them to 
their full expanded forms. In the GENIA corpus, 
we observe that the expanded form and its 
abbreviation often occur together via parentheses. 
Generally, there are two patterns: ?expanded form 
(abbreviation)? and ?abbreviation (expanded 
form)?.  
Our algorithm is based on the fact that it is 
much harder to classify an abbreviation than its 
expanded form. Generally, the expanded form is 
more evidential than its abbreviation to determine 
its class.  The algorithm works as follows: Given a 
sentence with parentheses, we use a similar 
algorithm as in Schwartz et al(2003) to determine 
whether it is an abbreviation with parentheses. If 
yes, we remove the abbreviation and the 
parentheses from the sentence. After the sentence 
is processed, we restore the abbreviation with 
parentheses to its original position in the sentence.  
Then, the abbreviation is classified as the same 
class of the expanded form, if the expanded form is 
recognized as an entity name. In the meanwhile, 
we also adjust the boundaries of the expanded form 
according to the abbreviation, if necessary. Finally, 
the expanded form and its abbreviation are stored 
in the recognized list of biomedical entity names 
from the document to help the resolution of 
forthcoming occurrences of the same abbreviation 
in the document. 
2.4 Dictionary 
In our system, two different features are explored 
to capture the existence of an entity name in a 
closed dictionary and an open dictionary. Here, the 
closed dictionary is constructed by extracting all 
entity names from the training data while the open 
dictionary (~700,000 entries) is combined from the 
database term list Swissport and the alias list 
LocusLink. The closed dictionary feature is 
represented as ClosedENTITYn (Here ENTITY 
indicates the class of the entity name and n 
indicates the number of the words in the entity 
name) while the open dictionary feature is 
represented as Openn (Here n indicates the number 
of the words in the entity name. We don?t 
differentiate the class of the entity name since the 
open dictionary only contains protein/gene names 
and their aliases). 
2.5 In-domain POS 
We also examine the impact of an in-domain POS 
feature instead of an out-domain POS feature 
which is trained on PENN TreeBank. Here, the in-
domain POS is trained on the GENIA corpus 
V3.02p. 
3. Evaluation 
Table 1 shows the performance of the baseline 
system and the impact of deep knowledge 
resources while Table 2-4 show the detailed 
performance using the provided scoring algorithm. 
Table 1 shows that: 
? The baseline system achieves F-measure of 
60.3 while incorporation of deep knowledge 
resources can improve the performance by 12.2 to 
72.5 in F-measure. 
? The replacement of the out-domain POS with 
in-domain POS improves the performance by 3.8 
in F-measure. This suggests in-domain POS can 
much improve the performance. 
? The name alias feature in name alias resolution 
slightly improves the performance by 0.9 in F-
measure. 
? The cascaded entity name resolution improves 
the performance by 3.1 in F-measure. This 
suggests that the cascaded entity name resolution is 
very useful due to the fact that about 16% of entity 
names have cascaded constructions. 
? The abbreviation resolution improves the 
performance by 2.1 in F-measure. 
? The small closed dictionary improves the 
performance by 1.5 in F-measure. In the 
meanwhile, the large open dictionary improves the 
performance by 1.2 in F-measure largely due to the 
performance improvement for the protein class. It 
is interesting that the small closed dictionary 
contributes more than the large open dictionary 
does. This may be due to the high ambiguity in the 
open dictionary and that the open dictionary only 
contains protein and gene names. 
 
Table 1: Impact of Deep Knowledge Resources 
Performance F 
Baseline 60.3 
98
+In-domain POS +3.8 
+Name Alias Feature +0.9 
+Cascaded Entity Name Res. +3.1 
+Abbreviation Resolution +2.1 
+Small Closed Dictionary +1.5 
+Large Open Dictionary +1.2 
+All Deep Knowledge Resources +12.2 
Table 2: Final Detailed Performance: full correct 
answer  
(# of correct 
answers) 
P R F 
Protein (4015) 69.01 79.24 73.77 
DNA (772) 66.84 73.11 69.83 
RNA (75) 64.66 63.56 64.10 
Cell Line (329) 53.85 65.80 59.23 
Cell Type (1391) 78.06 72.41 75.13 
Overall (6582) 69.42 75.99 72.55 
Table 3: Final Detailed Performance: correct left 
boundary with correct class information  
(# of correct 
answers) 
P R F 
Protein (4239) 72.86 83.66 77.89 
DNA (798) 69.09 75.57 72.18 
RNA (76) 65.52 64.41 64.96 
Cell Line (346) 56.63 69.20 62.29 
Cell Type (1418) 79.57 73.82 76.59 
Overall (6877) 72.53 79.39 75.80 
Table 4: Final Detailed Performance: correct right 
boundary with correct class information  
(# of correct 
answers) 
P R F 
Protein (4285) 73.65 84.57 78.73 
DNA (854) 73.94 80.87 77.25 
RNA (83) 71.55 70.34 70.94 
Cell Line (383) 62.68 76.60 68.95 
Cell Type (1532) 85.97 79.75 82.74 
Overall (7137) 75.27 82.39 78.67 
4. Conclusion 
In the paper, we have explored various deep 
knowledge resources such as the name alias 
phenomenon, the cascaded entity name 
phenomenon, the use of both a closed dictionary 
from the training corpus and an open dictionary 
from the database term list SwissProt and the alias 
list LocusLink, the abbreviation resolution and in-
domain POS using the GENIA corpus.  
In the near future, we will further improve the 
performance by investigating more on conjunction 
and disjunction construction and the combination 
of coreference resolution.  
Acknowledgement 
We thank ZHANG Zhuo for providing the 
database entity name list SwissProt and the alias 
list LocusLink. 
References 
Chen and Goodman. 1996. An Empirical Study of 
Smoothing Technniques for Language 
Modeling. In Proceedings of the 34th Annual 
Meeting of the Association of Computational 
Linguistics (ACL?1996). pp310-318. Santa Cruz, 
California, USA. 
Ohta T., Tateisi Y., Kim J., Mima H., and Tsujii J.  
2002.  The GENIA corpus: An annotated 
research abstract corpus in molecular biology 
domain.  In Proc. of HLT 2002. 
Platt J. 1999. Probabilistic Outputs for Support 
Vector Machines and comparisions to 
regularized Likelihood Methods. MIT Press. 
Schwartz A.S. and Hearst M.A.  2003.  A Simple 
Algorithm for Identifying Abbreviation 
Definitions in Biomedical Text.  In Proc. of the 
Pacific Symposium on Biocomputing (PSB 
2003) Kauai. 
Shen Dan, Zhang Jie, Zhou GuoDong, Su Jian and 
Tan Chew Lim, Effective Adaptation of a 
Hidden Markov Model-based Named Entity 
Recognizer for Biomedical Domain, 
Proceedings of ACL?2003 Workshop on Natural 
Language Processing in Biomedicine, Sapporo, 
Japan, 11 July 2003. pp49-56. 
Vapnik V. 1995. The Nature of Statistical 
Learning Theory. NY, USA: Springer-Verlag. 
Viterbi A.J.  1967.  Error bounds for convolutional 
codes and an asymptotically optimum decoding 
algorithm. IEEE Transactions on Information 
Theory, 260-269. 
Zhou G.D. and Su J.  2002.  Named Entity 
Recognition using an HMM-based Chunk 
Tagger.  In Proc. of the 40th Annual Meeting of 
the Association for Computational Linguistics 
(ACL), 473-480. 
99
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 154?157,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese word segmentation and named entity recognition based 
on a context-dependent Mutual Information Independence Model 
 
 
Zhang Min    Zhou GuoDong    Yang LingPeng    Ji DongHong 
 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore, 119613 
Email: (mzhang, zhougd, lpyang, dhji)@i2r.a-star.edu.sg 
 
Abstract  
This paper briefly describes our system in the 
third SIGHAN bakeoff on Chinese word 
segmentation and named entity recognition. 
This is done via a word chunking strategy 
using a context-dependent Mutual 
Information Independence Model. 
Evaluation shows that our system performs 
well on all the word segmentation closed 
tracks and achieves very good scalability 
across different corpora. It also shows that 
the use of the same strategy in named entity 
recognition shows promising performance 
given the fact that we only spend less than 
three days in total on extending the system in 
word segmentation to incorporate named 
entity recognition, including training and 
formal testing. 
1  Introduction 
Word segmentation and named entity recognition 
aim at recognizing the implicit word boundaries 
and proper nouns, such as names of persons, 
locations and organizations, respectively in plain 
Chinese text, and are critical in Chinese 
information processing.  However, there exist two 
problems when developing a practical word 
segmentation or named entity recognition system 
for large open applications, i.e. the resolution of 
ambiguous segmentations and the identification 
of OOV words or OOV entity names.  
In order to resolve above problems, we 
developed a purely statistical Chinese word 
segmentation system and a named entity 
recognition system using a three-stage strategy 
under an unified framework.  
The first stage is called known word 
segmentation, which aims to segment an input 
sequence of Chinese characters into a sequence of 
known words (called word atoms in this paper). In 
this paper, all Chinese characters are regarded as 
known words and a word unigram model is 
applied to perform this task for efficiency.  Also, 
for convenience, all the English characters are 
transformed into the Chinese counterparts in 
preprocessing, which will be recovered just 
before outputting results. 
The second stage is the word and/or named 
entity identification and classification on the 
sequence of atomic words in the first step. Here, a 
word chunking strategy is applied to detect words 
and/or entity names by chunking one or more 
atomic words together according to the word 
formation patterns of the word atoms and optional 
entity name formation patterns for named entity 
recognition. The problem of word segmentation 
and/or entity name recognition are re-cast as 
chunking one or more word atoms together to 
form a new word and/or entity name, and a 
discriminative Markov model, named Mutual 
Information Independence Model (MIIM), is 
adopted in chunking. Besides, a SVM plus 
sigmoid model is applied to integrate various 
types of contexts and implement the 
discriminative modeling in MIIM. 
The third step is post processing, which tries 
to further resolve ambiguous segmentations and 
unknown word segmentation. Due to time limit, 
this is only done in Chinese word segmentation. 
No post processing is done on Chinese named 
entity recognition. 
The rest of this paper is as follows: Section 2 
describes the context-dependent Mutual 
Information Independence Model in details while 
purely statistical post-processing in Chinese word 
segmentation is presented in Section 3. Finally, 
we report the results of our system in Chinese 
word segmentation and named entity recognition 
in Section 4 and conclude our work in Section 5. 
154
2 Mutual Information Independence 
Model  
In this paper, we use a discriminative Markov 
model, called Mutual Information Independence 
Model (MIIM) as proposed by Zhou et al(2002), 
for Chinese word segmentation and named entity 
recognition. MIIM is derived from a conditional 
probability model. Given an observation sequence 
n
n oooO L211 = , MIIM finds a stochastic optimal 
state(tag) sequence nn sssS L211 =  that 
maximizes: 
??
==
? +=
n
i
n
i
n
i
i
i
nn OsPSsPMIOSP
1
1
2
1
111 )|(log),()|(log  
We call the above model the Mutual 
Information Independence Model due to its 
Pair-wise Mutual Information (PMI) assumption 
(Zhou et al2002). The above model consists of 
two sub-models: the state transition model 
?
=
?n
i
i
i SsPMI
2
1
1 ),( , which can be computed by 
applying ngram modeling, and the output model 
?
=
n
i
n
i OsP
1
1 )|(log , which can be estimated by any 
probability-based classifier, such as a maximum 
entropy classifier or a SVM plus sigmoid 
classifier (Zhou et al2006).  In this competition, 
the SVM plus sigmoid classifier is used in 
Chinese word segmentation while a simple 
backoff  approach as described in Zhou et al
(2002) is used in named entity recognition. 
Here, a variant of the Viterbi algorithm 
(Viterbi 1967) in decoding the standard Hidden 
Markov Model (HMM) (Rabiner 1989) is 
implemented to find the most likely state 
sequence by replacing the state transition model 
and the output model of the standard HMM with 
the state transition model and the output model of 
the MIIM, respectively. The above MIIM has 
been successfully applied in many applications, 
such as text chunking (Zhou 2004), Chinese word 
segmentation ( Zhou 2005), English named entity 
recognition in the newswire domain (Zhou et al
2002) and the biomedical domain (Zhou et al
2004; Zhou et al2006). 
For Chinese word segmentation and named 
entity recognition by chunking, a word or a entity 
name is regarded as a chunk of one or more word 
atoms and we have: 
? >=< iii wpo , ; iw is the thi ?  word atom in 
the sequence of word atoms nn wwwW L211 = ; 
ip  is the word formation pattern of the word 
atom iw . Here ip  measures the word 
formation power of the word atom iw  and 
consists of: 
o The percentage of iw  occurring as a whole 
word (round to 10%) 
o The percentage of iw  occurring at the 
beginning of other words (round to 10%) 
o The percentage of iw  occurring at the end 
of other words (round to 10%) 
o The length of iw  
o Especially for named entity recognition, 
the percentages of a word occurring in 
different entity types (round to 10%). 
? is : the states are used to bracket and 
differentiate various types of words and 
optional entity types for named entity 
recognition. In this way, Chinese word 
segmentation and named entity recognition 
can be regarded as a bracketing and 
classification process. is  is structural and 
consists of two parts: 
o Boundary category (B): it includes four 
values: {O, B, M, E}, where O means that 
current word atom is a whOle word or 
entity name and B/M/E means that current 
word atom is at the Beginning/in the 
Middle/at the End of a word or entity name. 
o Unit category (W): It is used to denote the 
type of the word or entity name.  
Because of the limited number of boundary 
and unit categories, the current word atom 
formation pattern ip  described above is added 
into the state transition model in MIIM. This 
makes the above MIIM context dependent as 
follows: 
??
==
?
? +=
n
i
n
i
n
i
ii
i
i
nn
OsPppSsPMI
OSP
1
1
2
1
1
1
11
)|(log)|,(
)|(log
 
3 Post Processing in Word 
Segmentation 
The third step is post processing, which tries to 
resolve ambiguous segmentations and false 
unknown word generation raised in the second 
step. Due to time limit, this is only done in 
Chinese word segmentation, i.e. no post 
processing is done on Chinese named entity 
recognition. 
155
A simple pattern-based method is employed to 
capture context information to correct the 
segmentation errors generated in the second steps. 
The pattern is designed as follows: 
<Ambiguous Entry (AE)> | <Left Context, 
Right Context> => <Proper Segmentation> 
The ambiguity entry (AE) means ambiguous 
segmentations or forced-generated unknown 
words. We use the 1st and 2nd words before AE as 
the left context and the 1st and 2nd words after AE 
as the right context. To reduce sparseness, we also 
only use the 1st left and right words as context. 
This means that there are two patterns generated 
for the same context. All the patterns are 
automatically learned from training corpus using 
the following algorithm. 
 
LearningPatterns() 
// Input: training corpus 
// Output: patterns 
BEGIN 
(1) Training a MIIM model using training 
corpus 
(2) Using the MIIM model to segment training 
corpus 
(3) Aligning the training corpus with the 
segmented training corpus 
(4) Extracting error segmentations 
(5) Generating disambiguation patterns using 
the left and right context 
(6) Removing the conflicting entries if two 
patterns have the same left hand side but 
different right hand side. 
END 
 
4 Evaluation 
We first develop our system using the PKU data 
released in the Second SIGHAN Bakeoff last 
year. Then, we train and evaluate it on the Third 
SIGHAN Bakeoff corpora without any 
fine-tuning. We only carry out our evaluation on 
the closed tracks. It means that we do not use any 
additional knowledge beyond the training corpus. 
Precision (P), Recall (R), F-measure (F), OOV 
Recall and IV Recall are adopted to measure the 
performance of word segmentation. Accuracy 
(A), Precision (P), Recall (R) and F-measure (F) 
are adopted to measure the performance of NER. 
Tables 1, 2 and 3 in the next page report the 
performance of our algorithm on different corpus 
in the SIGHAN Bakeoff 02 and Bakeoff 03, 
respectively. For the performance of other 
systems, please refer to 
http://sighan.cs.uchicago.edu/bakeoff2005/data/r
esults.php.htm for the Chinese bakeoff 2005 and 
http://sighan.cs.uchicago.edu/bakeoff2006/longst
ats.html for the Chinese bakeoff 2006.  
Comparison against other systems shows that 
our system achieves the state-of-the-art 
performance on all Chinese word segmentation 
closed tracks and shows good scalability across 
different corpora. The small performance gap 
should be able to overcome by replacing the word 
unigram model with the more powerful word 
bigram model. Due to very limited time of less 
than three days, although our NER system under 
the unified framework as Chinese word 
segmentation does not achieve the 
state-of-the-art, its performance in NER is quite 
promising and provides a good platform for 
further improvement. Error analysis reveals that 
OOV is still an open problem that is far from to 
resolve. In addition, different corpus defines 
different segmentation principles. This will stress 
OOV handling in the extreme. Therefore a system 
trained on one genre usually performances worse 
when faced with text from a different register. 
5 Conclusion 
This paper proposes a purely unified statistical 
three-stage strategy in Chinese word 
segmentation and named entity recognition, 
which are based on a context-dependent Mutual 
Information Independence Model. Evaluation 
shows that our system achieves the 
states-of-the-art segmentation performance and 
provides a good platform for further performance 
improvement of Chinese NER. 
References  
Rabiner L. 1989. A Tutorial on Hidden Markov 
Models and Selected Applications in Speech 
Recognition. IEEE 77(2), pages257-285.  
Viterbi A.J. 1967. Error Bounds for 
Convolutional Codes and an Asymptotically 
Optimum Decoding Algorithm. IEEE 
Transactions on Information Theory, IT 13(2), 
260-269. 
Zhou GuoDong and Su Jain. 2002. Named Entity 
Recognition Using a HMM-based Chunk 
Tagger, Proceedings of the 40th Annual Meeting 
of the Association for Computational 
Linguistics  (ACL?2002). Philadelphia. July 
2002. pp473-480.  
156
Zhou GuoDong, Zhang Jie, Su Jian, Shen Dan and 
Tan ChewLim. 2004. Recognizing Names in 
Biomedical Texts: a Machine Learning 
Approach. Bioinformatics. 20(7): 1178-1190. 
DOI: 10.1093/bioinformatics/bth060. 2004. 
ISSN: 1460-2059 
Zhou GuoDong. 2004. Discriminative hidden 
Markov modeling with long state dependence 
using a kNN ensemble. Proceedings of 20th 
International Conference on Computational 
Linguistics (COLING?2004).  23-27 Aug, 2004, 
Geneva, Switzerland.  
Zhou GuoDong. 2005. A chunking strategy 
towards unknown word detection in Chinese 
word segmentation. Proceedings of 2nd 
International Joint Conference on Natural 
Language Processing (IJCNLP?2005), Lecture 
Notes in Computer Science (LNCS 3651) 
Zhou GuoDong. 2006. Recognizing names in 
biomedical texts using Mutual Information 
Independence Model and SVM plus Sigmod. 
International Journal of Medical Informatics 
(Article in Press). ISSN 1386-5056 
Tables  
Task P R F OOV Recall IV Recall 
CityU 0.9 38 0.952 94.5 0.578 0.967 
MSRA 0.952 0.962 95.7 0.51 0.98 
CKIP 0.94 0.957 94.8 0.502 0.976 
PKU 0.952 0.952 95.2 0.71 0.967 
Table 1: Performance of Word Segmentation on Closed Tracks in the SIGHAN Bakeoff 02  
 
Task P R F OOV Recall IV Recall 
CityU 0.968 0.961 96.5 0.633 0.983 
MSRA 0.961 0.953 95.7 0.499 0.977 
CKIP 0.958 0.941 94.9 0.554 0.976 
UPUC 0.936 0.917 92.6 0.617 0.966 
Table 2: Performance of Word Segmentation on Closed Tracks in the SIGHAN Bakeoff 03 
 
Task A P R F 
MSRA 0.9743 0.8150 0.7882 79.92 
CityU 0.9725 0.8466 0.8061 82.59 
     Table 3: Performance of NER on Closed Tracks in the SIGHAN Bakeoff 03 
 
 
 
157
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 697?704
Manchester, August 2008
Exploiting Constituent Dependencies for Tree Kernel-based Semantic 
Relation Extraction 
Longhua Qian   Guodong Zhou   Fang Kong   Qiaoming Zhu   Peide Qian 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology, Soochow University 
1 Shizi Street, Suzhou, China 215006 
{qianlonghua,gdzhou,kongfang,qmzhu,pdqian}@suda.edu.cn
Abstract
This paper proposes a new approach to 
dynamically determine the tree span for 
tree kernel-based semantic relation ex-
traction. It exploits constituent dependen-
cies to keep the nodes and their head 
children along the path connecting the 
two entities, while removing the noisy in-
formation from the syntactic parse tree, 
eventually leading to a dynamic syntactic 
parse tree. This paper also explores entity 
features and their combined features in a 
unified parse and semantic tree, which in-
tegrates both structured syntactic parse 
information and entity-related semantic 
information. Evaluation on the ACE 
RDC 2004 corpus shows that our dy-
namic syntactic parse tree outperforms all 
previous tree spans, and the composite 
kernel combining this tree kernel with a 
linear state-of-the-art feature-based ker-
nel, achieves the so far best performance. 
1 Introduction 
Information extraction is one of the key tasks in 
natural language processing. It attempts to iden-
tify relevant information from a large amount of 
natural language text documents. Of three sub-
tasks defined by the ACE program1, this paper 
focuses exclusively on Relation Detection and 
Characterization (RDC) task, which detects and 
classifies semantic relationships between prede-
fined types of entities in the ACE corpus. For 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
1 http://www.ldc.upenn.edu/Projects/ACE/ 
example, the sentence ?Microsoft Corp. is based 
in Redmond, WA? conveys the relation ?GPE-
AFF.Based? between ?Microsoft Corp.? [ORG] 
and ?Redmond? [GPE]. Due to limited accuracy 
in state-of-the-art syntactic and semantic parsing, 
reliably extracting semantic relationships be-
tween named entities in natural language docu-
ments is still a difficult, unresolved problem. 
In the literature, feature-based methods have 
dominated the research in semantic relation ex-
traction. Featured-based methods achieve prom-
ising performance and competitive efficiency by 
transforming a relation example into a set of syn-
tactic and semantic features, such as lexical 
knowledge, entity-related information, syntactic 
parse trees and deep semantic information. How-
ever, detailed research (Zhou et al, 2005) shows 
that it?s difficult to extract new effective features 
to further improve the extraction accuracy. 
Therefore, researchers turn to kernel-based 
methods, which avoids the burden of feature en-
gineering through computing the similarity of 
two discrete objects (e.g. parse trees) directly. 
From prior work (Zelenko et al, 2003; Culotta 
and Sorensen, 2004; Bunescu and Mooney, 2005) 
to current research (Zhang et al, 2006; Zhou et 
al., 2007), kernel methods have been showing 
more and more potential in relation extraction. 
The key problem for kernel methods on rela-
tion extraction is how to represent and capture 
the structured syntactic information inherent in 
relation instances. While kernel methods using 
the dependency tree (Culotta and Sorensen, 2004) 
and the shortest dependency path (Bunescu and 
Mooney, 2005) suffer from low recall perform-
ance, convolution tree kernels (Zhang et al, 2006; 
Zhou et al, 2007) over syntactic parse trees 
achieve comparable or even better performance 
than feature-based methods. 
However, there still exist two problems re-
garding currently widely used tree spans. Zhang 
et al (2006) discover that the Shortest Path-
697
enclosed Tree (SPT) achieves the best perform-
ance. Zhou et al (2007) further extend it to Con-
text-Sensitive Shortest Path-enclosed Tree (CS-
SPT), which dynamically includes necessary 
predicate-linked path information. One problem 
with both SPT and CS-SPT is that they may still 
contain unnecessary information. The other prob-
lem is that a considerable number of useful con-
text-sensitive information is also missing from 
SPT/CS-SPT, although CS-SPT includes some 
contextual information relating to predicate-
linked path. 
This paper proposes a new approach to dy-
namically determine the tree span for relation 
extraction by exploiting constituent dependencies 
to remove the noisy information, as well as keep 
the necessary information in the parse tree. Our 
motivation is to integrate dependency informa-
tion, which has been proven very useful to rela-
tion extraction, with the structured syntactic in-
formation to construct a concise and effective 
tree span specifically targeted for relation extrac-
tion. Moreover, we also explore interesting com-
bined entity features for relation extraction via a 
unified parse and semantic tree. 
The other sections in this paper are organized 
as follows. Previous work is first reviewed in 
Section 2. Then, Section 3 proposes a dynamic 
syntactic parse tree while the entity-related se-
mantic tree is described in Section 4. Evaluation 
on the ACE RDC corpus is given in Section 5. 
Finally, we conclude our work in Section 6. 
2 Related Work 
Due to space limitation, here we only review 
kernel-based methods used in relation extraction. 
For those interested in feature-based methods, 
please refer to Zhou et al (2005) for more details. 
Zelenko et al (2003) described a kernel be-
tween shallow parse trees to extract semantic 
relations, where a relation instance is trans-
formed into the least common sub-tree connect-
ing the two entity nodes. The kernel matches the 
nodes of two corresponding sub-trees from roots 
to leaf nodes recursively layer by layer in a top-
down manner. Their method shows successful 
results on two simple extraction tasks. Culotta 
and Sorensen (2004) proposed a slightly general-
ized version of this kernel between dependency 
trees, in which a successful match of two relation 
instances requires the nodes to be at the same 
layer and in the identical path starting from the 
roots to the current nodes. These strong con-
straints make their kernel yield high precision but 
very low recall on the ACE RDC 2003 corpus. 
Bunescu and Mooney (2005) develop a shortest 
path dependency tree kernel, which simply 
counts the number of common word classes at 
each node in the shortest paths between two enti-
ties in dependency trees. Similar to Culotta and 
Sorensen (2004), this method also suffers from 
high precision but low recall.
Zhang et al (2006) describe a convolution tree 
kernel (CTK, Collins and Duffy, 2001) to inves-
tigate various structured information for relation 
extraction and find that the Shortest Path-
enclosed Tree (SPT) achieves the F-measure of 
67.7 on the 7 relation types of the ACE RDC 
2004 corpus. One problem with SPT is that it 
loses the contextual information outside SPT, 
which is usually critical for relation extraction. 
Zhou et al (2007) point out that both SPT and 
the convolution tree kernel are context-free. They 
expand SPT to CS-SPT by dynamically includ-
ing necessary predicate-linked path information 
and extending the standard CTK to context-
sensitive CTK, obtaining the F-measure of 73.2 
on the 7 relation types of the ACE RDC 2004 
corpus. However, the CS-SPT only recovers part 
of contextual information and may contain noisy 
information as much as SPT. 
In order to fully utilize the advantages of fea-
ture-based methods and kernel-based methods, 
researchers turn to composite kernel methods. 
Zhao and Grishman (2005) define several fea-
ture-based composite kernels to capture diverse 
linguistic knowledge and achieve the F-measure 
of 70.4 on the 7 relation types in the ACE RDC 
2004 corpus. Zhang et al (2006) design a com-
posite kernel consisting of an entity linear kernel 
and a standard CTK, obtaining the F-measure of 
72.1 on the 7 relation types in the ACE RDC 
2004 corpus. Zhou et al (2007) describe a com-
posite kernel to integrate a context-sensitive 
CTK and a state-of-the-art linear kernel. It 
achieves the so far best F-measure of 75.8 on the 
7 relation types in the ACE RDC 2004 corpus. 
In this paper, we will further study how to dy-
namically determine a concise and effective tree 
span for a relation instance by exploiting con-
stituent dependencies inherent in the parse tree 
derivation. We also attempt to fully capture both 
the structured syntactic parse information and 
entity-related semantic information, especially 
combined entity features, via a unified parse and 
semantic tree. Finally, we validate the effective-
ness of a composite kernel for relation extraction, 
which combines a tree kernel and a linear kernel. 
698
3 Dynamic Syntactic Parse Tree 
This section discusses how to generate dynamic 
syntactic parse tree by employing constituent 
dependencies to overcome the problems existing 
in currently used tree spans. 
3.1 Constituent Dependencies in Parse Tree 
Zhang et al (2006) explore five kinds of tree 
spans and find that the Shortest Path-enclosed 
Tree (SPT) achieves the best performance. Zhou 
et al (2007) further propose Context-Sensitive 
SPT (CS-SPT), which can dynamically deter-
mine the tree span by extending the necessary 
predicate-linked path information outside SPT. 
However, the key problem of how to represent 
the structured syntactic parse tree is still partially 
resolved. As we indicate as follows, current tree 
spans suffer from two problems: 
(1) Both SPT and CS-SPT still contain unnec-
essary information. For example, in the sentence 
??bought one of town?s two meat-packing
plants?, the condensed information ?one of 
plants? is sufficient to determine ?DISC? rela-
tionship between the entities ?one? [FAC] and 
?plants? [FAC], while SPT/CS-SPT include the 
redundant underlined part. Therefore more un-
necessary information can be safely removed 
from SPT/CS-SPT. 
(2) CS-SPT only captures part of context-
sensitive information relating to predicate-linked 
structure (Zhou et al, 2007) and still loses much 
context-sensitive information. Let?s take the 
same example sentence ??bought one of town?s
two meat-packing plants?, where indeed there is 
no relationship between the entities ?one? [FAC] 
and ?town? [GPE]. Nevertheless, the information 
contained in SPT/CS-SPT (?one of town?) may 
easily lead to their relationship being misclassi-
fied as ?DISC?, which is beyond our expectation. 
Therefore the underlined part outside SPT/CS-
SPT should be recovered so as to differentiate it 
from positive instances. 
Since dependency plays a key role in many 
NLP problems such as syntactic parsing, seman-
tic role labeling as well as semantic relation ex-
traction, our motivation is to exploit dependency 
knowledge to distinguish the necessary evidence 
from the unnecessary information in the struc-
tured syntactic parse tree.  
On one hand, lexical or word-word depend-
ency indicates the relationship among words 
occurring in the same sentence, e.g. predicate-
argument dependency means that arguments are 
dependent on their target predicates, modifier-
head dependency means that modifiers are de-
pendent on their head words. This dependency 
relationship offers a very condensed representa-
tion of the information needed to assess the rela-
tionship in the forms of the dependency tree (Cu-
lotta and Sorensen, 2004) or the shortest depend-
ency path (Bunescu and Mooney, 2005) that in-
cludes both entities.
On the other hand, when the parse tree corre-
sponding to the sentence is derived using deriva-
tion rules from the bottom to the top, the word-
word dependencies extend upward, making a 
unique head child containing the head word for 
every non-terminal constituent. As indicated as 
follows, each CFG rule has the form: 
P? Ln?L1H R1?Rm
Here, P is the parent node, H is the head child of 
the rule, Ln?L1 and R1?Rm are left and right 
modifiers of H respectively, and both n and m
may be zero. In other words, the parent node P
depends on the head child H, this is what we call 
constituent dependency. Vice versa, we can also 
determine the head child of a constituent in terms 
of constituent dependency. Our hypothesis stipu-
lates that the contribution of the parse tree to es-
tablishing a relationship is almost exclusively 
concentrated in the path connecting the two enti-
ties, as well as the head children of constituent 
nodes along this path. 
3.2 Generation of Dynamic Syntactic Parse 
Tree
Starting from the Minimum Complete Tree 
(MCT, the complete sub-tree rooted by the near-
est common ancestor of the two entities under 
consideration) as the representation of each rela-
tion instance, along the path connecting two enti-
ties, the head child of every node is found ac-
cording to various constituent dependencies. 
Then the path nodes and their head children are 
kept while any other nodes are removed from the 
tree. Eventually we arrive at a tree called Dy-
namic Syntactic Parse Tree (DSPT), which is 
dynamically determined by constituent depend-
encies and only contains necessary information 
as expected. 
There exist a considerable number of constitu-
ent dependencies in CFG as described by Collins 
(2003). However, since our task is to extract the 
relationship between two named entities, our fo-
cus is on how to condense Noun-Phrases (NPs) 
and other useful constituents for relation extrac-
tion. Therefore constituent dependencies can be 
classified according to constituent types of the 
CFG rules: 
699
(1) Modification within base-NPs: base-NPs 
mean that they do not directly dominate an NP
themselves, unless the dominated NP is a posses-
sive NP. The noun phrase right above the entity
headword, whose mention type is nominal or 
name, can be categorized into this type. In this 
case, the entity headword is also the headword of 
the noun phrase, thus all the constituents before 
the headword are dependent on the headword,
and may be removed from the parse tree, while 
the headword and the constituents right after the 
headword remain unchanged. For example, in the 
sentence ??bought one of town?s two meat-
packing plants? as illustrated in Figure 1(a), the
constituents before the headword  ?plants? can 
be removed from the parse tree. In this way the
parse tree ?one of plants? could capture the
?DISC? relationship more concisely and pre-
cisely. Another interesting example is shown in 
Figure 1(b), where the base-NP of the second
entity ?town? is a possessive NP and there is no 
relationship between the entities ?one? and
?town? defined in the ACE corpus. For both SPT
and CS-SPT, this example would be condensed 
to ?one of town? and therefore easily misclassi-
fied as the ?DISC? relationship between the two 
entities. In the contrast, our DSPT can avoid this 
problem by keeping the constituent ??s? and the 
headword ?plants?.
(2) Modification to NPs: except base-NPs,
other modification to NPs can be classified into 
this type. Usually these NPs are recursive, mean-
ing that they contain another NP as their child. 
The CFG rules corresponding to these modifica-
tions may have the following forms:
NP? NP SBAR [relative clause]
NP? NP VP [reduced relative]
NP? NP PP [PP attachment]
Here, the NPs in bold mean that the path con-
necting the two entities passes through them. For
every right hand side, the NP in bold is modified
by the constituent following them. That is, the 
latter is dependent on the former, and may be 
reduced to a single NP. In Figure 1(c) we show a
sentence ?one of about 500 people nominated
for ??, where there exists a ?DISC? relationship
between the entities ?one? and ?people?. Since 
the reduced relative ?nominated for ?? modifies
and is therefore dependent on the ?people?, they 
can be removed from the parse tree, that is, the 
right side (?NP VP?) can be reduced to the left 
hand side, which is exactly a single NP. 
(a) Removal of constituents before the headword in base-NP
(b) Keeping of constituents after the headword in base-NP
NN
one
IN
of
DT
the
NN
town
POS
's
E-FAC
NN
plantstwo
CD NN
one
IN
of
NN
town
POS
's
E-FAC
NN
plantsmeat-packing
JJ
NN
one
PP
IN
of
NP
DT
the
NN
town
POS
's
NN
plantstwo
CD NN
one
IN
of
NN
plantsmeat-packing
JJ NN
one
IN
of
RB
about
QP
CD
500
NNS
people
...
nominated
VBN
for
IN
VP
PP
...
E2-PER
NN
one
IN
of
NNS
people
NN
property
PRP
he
VP
VBZ IN
in
NP
PP
state
NNS
the
NP
JJ
rental
S
owns
DT NN
property
PRP
he
VP
VBZ
owns
governors from connecticut
NNS IN
NP
E-GPE
NNP
,
,
south
NP
E-GPE
NNP
dakota
NNP
,
,
and
CC
montana
NNP
governors from
NNS IN
montana
NNP
(c) Reduction of modification to NP
(d) Removal of arguments to verb
(e) Reduction of conjuncts for NP coordination
E-GPE
NPPP
E1-FAC
NP
E2-FAC
NP
E1-FAC
NP
NP
NP
NP
E2-FAC E1-PER
NP
NP
PP
NP
NP
NP
E1-PER
PP
NP
E2-PER
NP
SBAR
E2-PER
S
NPNP
E1-FAC
PP
NP
NP
E1-PER
NP
E2-GPE
NP
E1-PER
PP
NP
NP
NP
E2-GPE
NP
E1-FAC E2-PER
NP
NP
SBAR
NP
NP
E1-FAC
PP
NP
NP
E2-GPE
NP
NP
E1-PER
PP
NP
NP
E2-GPE
Figure 1. Removal and reduction of constituents using dependencies 
700
(3) Arguments/adjuncts to verbs: this type 
includes the CFG rules in which the left side in-
cludes S, SBAR or VP. An argument represents
the subject or object of a verb, while an adjunct
indicates the location, date/time or way of the
action corresponding to the verb. They depend
on the verb and can be removed if they are not
included in the path connecting the two entities.
However, when the parent tag is S or SBAR, and
its child VP is not included in the path, this VP
should be recovered to indicate the predicate
verb. Figure 1(d) shows a sentence ?? maintain
rental property he owns in the state?, where the
?ART.User-or-Owner? relation holds between 
the entities ?property? and ?he?. While PP can be
removed from the rule  (?VP? VBZ PP?), the 
VP should be kept in the rule (?S? NP VP?).
Consequently, the tree span looks more concise 
and precise for relation extraction. 
(4) Coordination conjunctions: In coordina-
tion constructions, several peer conjuncts may be 
reduced into a single constituent. Although the
first conjunct is always considered as the head-
word (Collins, 2003), actually all the conjuncts
play an equal role in relation extraction. As illus-
trated in Figure 1(e), the NP coordination in the 
sentence (?governors from connecticut, south
dakota, and montana?) can be reduced to a single 
NP (?governors from montana?) by keeping the
conjunct in the path while removing the other 
conjuncts.
(5) Modification to other constituents: ex-
cept for the above four types, other CFG rules 
fall into this type, such as modification to PP,
ADVP and PRN etc. These cases are similar to 
arguments/adjuncts to verbs, but less frequent 
than them, so we will not detail this scenario. 
In fact, SPT (Zhang et al, 2006) can be ar-
rived at by carrying out part of the above re-
moval operations using a single rule (i.e. all the 
constituents outside the linking path should be
removed) and CS-CSPT (Zhou et al, 2007) fur-
ther recovers part of necessary context-sensitive 
information outside SPT, this justifies that SPT
performs well, while CS-SPT outperforms SPT. 
4 Entity-related Semantic Tree 
Entity semantic features, such as entity headword, 
entity type and subtype etc., impose a strong
constraint on relation types in terms of relation
definition by the ACE RDC task. Experiments by
Zhang et al (2006) show that linear kernel using 
only entity features contributes much when com-
bined with the convolution parse tree kernel. 
Qian et al (2007) further indicates that among
these entity features, entity type, subtype, and 
mention type, as well as the base form of predi-
cate verb, contribute most while the contribution
of other features, such as entity class, headword 
and GPE role, can be ignored. 
In order to effectively capture entity-related
semantic features, and their combined features as
well, especially bi-gram or tri-gram features, we 
build an Entity-related Semantic Tree (EST) in 
three ways as illustrated in Figure 2. In the ex-
ample sentence ?they ?re here?, which is ex-
cerpted from the ACE RDC 2004 corpus, there 
exists a relationship ?Physical.Located? between
the entities ?they? [PER] and ?here?
[GPE.Population-Center]. The features are en-
coded as ?TP?, ?ST?, ?MT? and ?PVB?, which
denote type, subtype, mention-type of the two 
entities, and the base form of predicate verb if 
existing (nearest to the 2nd entity along the path 
connecting the two entities) respectively. For 
example, the tag ?TP1? represents the type of the 
1st entity, and the tag ?ST2? represents the sub-
type of the 2nd entity. The three entity-related
semantic tree setups are depicted as follows: 
TP2TP1
(a) Bag Of Features(BOF)
ENT
ST2ST1 MT2MT1 PVB
(c) Entity-Paired Tree(EPT)
ENT
E1 E2
(b) Feature Paired Tree(FPT)
ENT
TP ST MT
ST1TP1 MT1 TP2 ST2 MT2
PVB
TP1 TP2 ST1 ST2 MT1 MT2
PVB
PER null PRO GPE Pop. PRO be
PER null PRO GPE Pop. PRO
be
PER GPE null Pop. PRO PRO
be
Figure 2. Different setups for entity-related se-
mantic tree (EST) 
(a) Bag of Features (BOF, e.g. Fig. 2(a)): all 
feature nodes uniformly hang under the root node,
so the tree kernel simply counts the number of 
common features between two relation instances.
This tree setup is similar to linear entity kernel
explored by Zhang et al (2006). 
(b) Feature-Paired Tree (FPT, e.g. Fig. 2(b)): 
the features of two entities are grouped into dif-
ferent types according to their feature names, e.g.
?TP1? and ?TP2? are grouped to ?TP?. This tree 
setup is aimed to capture the additional similarity
701
of the single feature combined from different 
entities, i.e., the first and the second entities. 
(c) Entity-Paired Tree (EPT, e.g. Fig. 2(c)): all 
the features relating to an entity are grouped to 
nodes ?E1? or ?E2?, thus this tree kernel can fur-
ther explore the equivalence of combined entity 
features only relating to one of the entities be-
tween two relation instances. 
In fact, the BOF only captures the individual 
entity features, while the FPT/EPT can addition-
ally capture the bi-gram/tri-gram features respec-
tively. 
Rather than constructing a composite kernel, 
we incorporate the EST into the DSPT to pro-
duce a Unified Parse and Semantic Tree (UPST) 
to investigate the contribution of the EST to rela-
tion extraction. The entity features can be at-
tached under the top node, the entity nodes, or 
directly combined with the entity nodes as in 
Figure 1. However, detailed evaluation (Qian et 
al., 2007) indicates that the UPST achieves the 
best performance when the feature nodes are at-
tached under the top node. Hence, we also attach 
three kinds of entity-related semantic trees (i.e. 
BOF, FPT and EPT) under the top node of the 
DSPT right after its original children. Thereafter, 
we employ the standard CTK (Collins and Duffy, 
2001) to compute the similarity between two 
UPSTs, since this CTK and its variations are 
successfully applied in syntactic parsing, seman-
tic role labeling (Moschitti, 2004) and relation 
extraction (Zhang et al, 2006; Zhou et al, 2007) 
as well. 
5 Experimentation 
This section will evaluate the effectiveness of the 
DSPT and the contribution of entity-related se-
mantic information through experiments. 
5.1 Experimental Setting  
For evaluation, we use the ACE RDC 2004 cor-
pus as the benchmark data. This data set contains 
451 documents and 5702 relation instances. It 
defines 7 entity types, 7 major relation types and 
23 subtypes. For comparison with previous work, 
evaluation is done on 347 (nwire/bnews) docu-
ments and 4307 relation instances using 5-fold 
cross-validation. Here, the corpus is parsed using 
Charniak?s parser (Charniak, 2001) and relation 
instances are generated by iterating over all pairs 
of entity mentions occurring in the same sentence 
with given ?true? mentions and coreferential in-
formation. In our experimentations, SVMlight
(Joachims, 1998) with the tree kernel function
(Moschitti, 2004) 2  is selected as our classifier. 
For efficiency, we apply the one vs. others
strategy, which builds K classifiers so as to 
separate one class from all others. For 
comparison purposes, the training parameters C 
(SVM) and ? (tree kernel) are also set to 2.4 and 
0.4 respectively. 
5.2 Experimental Results 
Table 1 evaluates the contributions of different 
kinds of constituent dependencies to extraction 
performance on the 7 relation types of the ACE 
RDC 2004 corpus using the convolution parse 
tree kernel as depicted in Figure 1. The MCT 
with only entity-type information is first used as 
the baseline, and various constituent dependen-
cies are then applied sequentially to dynamically 
reshaping the tree in two different modes: 
--[M1] Respective:  every constituent depend-
ency is individually applied on MCT. 
--[M2] Accumulative: every constituent de-
pendency is incrementally applied on the previ-
ously derived tree span, which begins with the 
MCT and eventually gives rise to a Dynamic 
Syntactic Parse Tree (DSPT).  
Dependency types P(%) R(%) F
MCT (baseline) 75.1 53.8 62.7
Modification within 
base-NPs
76.5
(59.8)
59.8
(59.8)
67.1
(67.1)
Modification to NPs 
77.0
(76.2)
63.2
(56.9)
69.4
(65.1)
Arguments/adjuncts to verb 
77.1
(76.1)
63.9
(57.5)
69.9
(65.5)
Coordination conjunctions 
77.3
(77.3)
65.2
(55.1)
70.8
(63.8)
Other modifications 
77.4
(75.0)
65.4
(53.7)
70.9
(62.6)
Table 1. Contribution of constituent dependen-
cies in respective mode (inside parentheses) and 
accumulative mode (outside parentheses) 
The table shows that the final DSPT achieves 
the best performance of 77.4%/65.4%/70.9 in
precision/recall/F-measure respectively after ap-
plying all the dependencies, with the increase of 
F-measure by 8.2 units compared to the baseline 
MCT. This indicates that reshaping the tree by 
exploiting constituent dependencies may signifi-
cantly improve extraction accuracy largely due to 
the increase in recall. It further suggests that con-
stituent dependencies knowledge is very effec-
2 http://ai-nlp.info.uniroma2.it/moschitti/ 
702
tive and can be fully utilized in tree kernel-based 
relation extraction. This table also shows that: 
(1) Both modification within base-NPs and 
modification to NPs contribute much to perform-
ance improvement, acquiring the increase of F-
measure by 4.4/2.4 units in mode M1 and 4.4/2.3 
units in mode M2 respectively. This indicates the 
local characteristic of semantic relations, which 
can be effectively captured by NPs near the two 
involved entities in the DSPT. 
(2) All the other three dependencies show mi-
nor contribution to performance enhancement, 
they improve the F-measure only by 2.8/0.9/-0.1 
units in mode M1 and 0.5/0.9/0.1 units in mode 
M2. This may be due to the reason that these de-
pendencies only remove the nodes far from the 
two entities. 
We compare in Table 2 the performance of 
Unified Parse and Semantic Trees with different 
kinds of Entity Semantic Tree setups using stan-
dard convolution tree kernel, while the SPT and 
DSPT with only entity-type information are 
listed for reference. It shows that: 
(1) All the three unified parse and semantic 
tree kernels significantly outperform the DSPT 
kernel, obtaining an average increase of ~4 units 
in F-measure. This means that they can effec-
tively capture both the structured syntactic in-
formation and the entity-related semantic fea-
tures.
(2) The Unified Parse and Semantic Tree with 
Feature-Paired Tree achieves the best perform-
ance of 80.1/70.7/75.1 in P/R/F respectively, 
with an increase of F-measure by 0.4/0.3 units 
over BOF and EPT respectively. This suggests 
that additional bi-gram entity features captured 
by FPT are more useful than tri-gram entity fea-
tures captured by EPT. 
Tree setups P(%) R(%) F
SPT 76.3 59.8 67.1
DSPT 77.4 65.4 70.9
UPST (BOF) 80.4 69.7 74.7
UPST (FPT) 80.1 70.7 75.1
UPST (EPT) 79.9 70.2 74.8
Table 2. Performance of Unified Parse and 
Semantic Trees (UPSTs) on the 7 relation types 
of the ACE RDC 2004 corpus 
In Table 3 we summarize the improvements of 
different tree setups over SPT. It shows that in a 
similar setting, our DSPT outperforms SPT by 
3.8 units in F-measure, while CS-SPT outper-
forms SPT by 1.3 units in F-measure. This sug-
gests that the DSPT performs best among these 
tree spans. It also shows that the Unified Parse 
and Semantic Tree with Feature-Paired Tree per-
form significantly better than the other two tree 
setups (i.e., CS-SPT and DSPT) by 6.7/4.2 units 
in F-measure respectively. This implies that the 
entity-related semantic information is very useful 
and contributes much when they are incorporated 
into the parse tree for relation extraction. 
Tree setups P(%) R(%) F
CS-SPT over SPT3 1.5   1.1 1.3
DSPT over SPT 1.1   5.6 3.8
UPST (FPT) over SPT 3.8 10.9 8.0
Table 3. Improvements of different tree setups 
over SPT on the ACE RDC 2004 corpus 
Finally, Table 4 compares our system with 
other state-of-the-art kernel-based systems on the 
7 relation types of the ACE RDC 2004 corpus. It 
shows that our UPST outperforms all previous 
tree setups using one single kernel, and even bet-
ter than two previous composite kernels (Zhang 
et al, 2006; Zhao and Grishman, 2005). Fur-
thermore, when the UPST (FPT) kernel is com-
bined with a linear state-of-the-state feature-
based kernel (Zhou et al, 2005) into a composite 
one via polynomial interpolation in a setting 
similar to Zhou et al (2007) (i.e. polynomial de-
gree d=2 and coefficient ?=0.3), we get the so far 
best performance of 77.1 in F-measure for 7 rela-
tion types on the ACE RDC 2004 data set. 
Systems P(%) R(%) F
Ours:
composite kernel 
83.0 72.0 77.1
Zhou et al, (2007):
composite kernel 
82.2 70.2 75.8
Zhang et al, (2006):
composite kernel 
76.1 68.4 72.1
Zhao and Grishman, (2005):4
composite kernel 
69.2 70.5 70.4
Ours:
CTK with UPST 
80.1 70.7 75.1
Zhou et al, (2007): context-
sensitive CTK with CS-SPT 
81.1 66.7 73.2
Zhang et al, (2006):
CTK with SPT 
74.1 62.4 67.7
Table 4. Comparison of different systems on 
the ACE RDC 2004 corpus 
3  We arrive at these values by subtracting P/R/F 
(79.6/5.6/71.9) of Shortest-enclosed Path Tree from P/R/F  
(81.1/6.7/73.2) of Dynamic Context-Sensitive Shortest-
enclosed Path Tree according to Table 2 (Zhou et al, 2007) 
4 There might be some typing errors for the performance 
reported in Zhao and Grishman (2005) since P, R and F do 
not match. 
703
6 Conclusion
This paper further explores the potential of struc-
tured syntactic information for tree kernel-based 
relation extraction, and proposes a new approach 
to dynamically determine the tree span (DSPT) 
for relation instances by exploiting constituent 
dependencies. We also investigate different ways 
of how entity-related semantic features and their 
combined features can be effectively captured in 
a Unified Parse and Semantic Tree (UPST). 
Evaluation on the ACE RDC 2004 corpus shows 
that our DSPT is appropriate for structured repre-
sentation of relation instances. We also find that, 
in addition to individual entity features, com-
bined entity features (especially bi-gram) con-
tribute much when they are combined with a 
DPST into a UPST. And the composite kernel, 
combining the UPST kernel and a linear state-of-
the-art kernel, yields the so far best performance. 
For the future work, we will focus on improv-
ing performance of complex structured parse 
trees, where the path connecting the two entities 
involved in a relationship is too long for current 
kernel methods to take effect. Our preliminary 
experiment of applying certain discourse theory 
exhibits certain positive results.
Acknowledgements 
This research is supported by Project 60673041 
under the National Natural Science Foundation 
of China, Project 2006AA01Z147 under the 
?863? National High-Tech Research and Devel-
opment of China, and the National Research 
Foundation for the Doctoral Program of Higher 
Education of China under Grant No. 
20060285008. We would also like to thank the 
excellent and insightful comments from the three 
anonymous reviewers. 
References
Bunescu, Razvan C. and Raymond J. Mooney. 2005. 
A Shortest Path Dependency Kernel for Relation 
Extraction. In Proceedings of the Human Language 
Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP-2005), pages 724-731. Vancover, B.C. 
Charniak, Eugene. 2001. Intermediate-head Parsing 
for Language Models. In Proceedings of the 39th 
Annual Meeting of the Association of Computa-
tional Linguistics (ACL-2001), pages 116-123. 
Collins, Michael. 2003. Head-Driven Statistics Mod-
els for Natural Language Parsing. Computational 
linguistics, 29(4): 589-617. 
Collins, Michael and Nigel Duffy. 2001. Convolution 
Kernels for Natural Language. In Proceedings of 
Neural Information Processing Systems (NIPS-
2001), pages 625-632. Cambridge, MA. 
Culotta, Aron and Jeffrey Sorensen. 2004. Depend-
ency tree kernels for relation extraction. In Pro-
ceedings of the 42nd Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL-2004),
pages 423-439. Barcelona, Spain. 
Joachims, Thorsten. 1998. Text Categorization with 
Support Vector Machine: learning with many rele-
vant features. In Proceedings of the 10th European 
Conference on Machine Learning (ECML-1998),
pages 137-142. Chemnitz, Germany. 
Moschitti, Alessandro. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. In Proceed-
ings of the 42nd Annual Meeting of the Association 
of Computational Linguistics (ACL-2004). Barce-
lona, Spain. 
Qian, Longhua, Guodong Zhou, Qiaoming Zhu and 
Peide Qian. 2007. Relation Extraction using Con-
volution Tree Kernel Expanded with Entity Fea-
tures. In Proceedings of the 21st Pacific Asian 
Conference on Language, Information and Compu-
tation (PACLIC-21), pages 415-421. Seoul, Korea. 
Zelenko, Dmitry, Chinatsu Aone and Anthony Rich-
ardella. 2003. Kernel Methods for Relation Extrac-
tion. Journal of Machine Learning Research, 
3(2003): 1083-1106. 
Zhang, Min, Jie Zhang, Jian Su and Guodong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of the 21st International 
Conference on Computational Linguistics and the 
44th Annual Meeting of the Association of Compu-
tational Linguistics (COLING/ACL-2006), pages 
825-832. Sydney, Australia. 
Zhao, Shubin and Ralph Grishman. 2005. Extracting 
relations with integrated information using kernel 
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association of Computational Linguistics 
(ACL-2005), pages 419-426. Ann Arbor, USA. 
Zhou, Guodong, Jian Su, Jie Zhang and Min Zhang. 
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meet-
ing of the Association of Computational Linguistics 
(ACL-2005), pages 427-434. Ann Arbor, USA. 
Zhou, Guodong, Min Zhang, Donghong Ji and 
Qiaoming Zhu. 2007. Tree Kernel-based Relation 
Extraction with Context-Sensitive Structured Parse 
Tree Information. In Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural 
Language Learning (EMNLP/CoNLL-2007), pages 
728-736. Prague, Czech. 
704
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 978?986,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
Global Learning of Noun Phrase Anaphoricity in Coreference Resolu-
tion via Label Propagation 
ZHOU GuoDong      KONG Fang 
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow University. Suzhou, China 215006 
Email:{gdzhou,kongfang}@suda.edu.cn  
 
 
Abstract 
Knowledge of noun phrase anaphoricity might 
be profitably exploited in coreference resolu-
tion to bypass the resolution of non-anaphoric 
noun phrases. However, it is surprising to no-
tice that recent attempts to incorporate auto-
matically acquired anaphoricity information 
into coreference resolution have been some-
what disappointing. This paper employs a 
global learning method in determining the 
anaphoricity of noun phrases via a label 
propagation algorithm to improve learning-
based coreference resolution. In particular, 
two kinds of kernels, i.e. the feature-based 
RBF kernel and the convolution tree kernel, 
are employed to compute the anaphoricity 
similarity between two noun phrases. Experi-
ments on the ACE 2003 corpus demonstrate 
the effectiveness of our method in anaphoric-
ity determination of noun phrases and its ap-
plication in learning-based coreference resolu-
tion. 
1 Introduction 
Coreference resolution, the task of determining 
which noun phrases (NPs) in a text refer to the 
same real-world entity, has long been considered 
an important and difficult problem in natural 
language processing. Identifying the linguistic 
constraints on when two NPs can co-refer re-
mains an active area of research in the commu-
nity. One significant constraint on coreference, 
the anaphoricity constraint, specifies that a non-
anaphoric NP cannot be coreferent with any of 
its preceding NPs in a given text. Therefore, it is 
useful to skip over these non-anaphoric NPs 
rather than attempt an unnecessary search for an 
antecedent for them, only to end up with inaccu-
rate outcomes. Although many existing machine 
learning approaches to coreference resolution 
have performed reasonably well without explicit 
anaphoricity determination (e.g., Soon et al2001; 
Ng and Cardie 2002b; Strube and Muller 2003; 
Yang et al2003, 2008), anaphoricity determina-
tion has been studied fairly extensively in the 
literature, given the potential usefulness of NP 
anaphoricity in coreference resolution. One 
common approach involves the design of heuris-
tic rules to identify specific types of non-
anaphoric NPs, such as pleonastic pronouns (e.g. 
Paice and Husk 1987; Lappin and Leass 1994; 
Kennedy and Boguraev 1996; Denber 1998) and 
existential definite descriptions (e.g., Vieira and 
Poesio 2000). More recently, the problem has 
been tackled using statistics-based (e.g., Bean 
and Riloff 1999; Bergsma et al2008) and learn-
ing-based (e.g. Evans 2001; Ng and Cardie 
2002a; Ng 2004; Yang et al2005; Denis and 
Balbridge 2007) methods. Although there is em-
pirical evidence (e.g. Ng and Cardie 2002a, 
2004) that coreference resolution might be fur-
ther improved with proper anaphoricity informa-
tion, its contribution is still somewhat disap-
pointing and lacks systematic evaluation. 
This paper employs a label propagation (LP) 
algorithm for global learning of NP anaphoricity. 
Given the labeled data and the unlabeled data, 
the LP algorithm first represents labeled and 
unlabeled instances as vertices in a connected 
graph, then propagates the label information 
from any vertex to nearby vertices through 
weighted edges and finally infers the labels of 
unlabeled instances until a global stable stage is 
achieved. Here, the labeled data in this paper 
include all the NPs in the training texts with the 
anaphoricity labeled and the unlabeled data in-
clude all the NPs in a test text with the ana-
phoricity unlabeled. One major advantage of LP-
based anaphoricity determination is that the ana-
phoricity of all the NPs in a text can be deter-
mined together in a global way. Compared with 
previous methods, the LP algorithm can effec-
tively capture the natural clustering structure in 
both the labeled and unlabeled data to smooth 
the labeling function. In particular, two kinds of 
978
 kernels, i.e. the feature-based RBF kernel and 
the convolution tree kernel, are employed to 
compute the anaphoricity similarity between two 
NPs and weigh the edge between them. Experi-
ments on the ACE 2003 corpus show that our 
LP-based anaphoricity determination signifi-
cantly outperforms locally-optimized one, which 
adopts a classifier (e.g. SVM) to determine the 
anaphoricity of NPs in a text individually and 
significantly improves the performance of learn-
ing-based coreference resolution. It also shows 
that, while feature-based anaphoricity determi-
nation contributes much to pronoun resolution, 
its contribution on definite NP resolution can be 
ignored. In comparison, it shows that tree ker-
nel-based anaphoricity resolution contributes 
significantly to the resolution of both pronouns 
and definite NPs due to the inclusion of various 
kinds of syntactic structured information. 
The rest of this paper is organized as follows. 
In Section 2, we review related work in ana-
phoricity determination. Then, the LP algorithm 
is introduced in Section 3 while Section 4 de-
scribes different similarity measurements ex-
plored in the LP algorithm. Section 5 shows the 
experimental results. Finally, we conclude our 
work in Section 6.  
2 Related Work 
Given its potential usefulness in coreference 
resolution, anaphoricity determination has been 
studied fairly extensively in the literature and 
can be classified into three categories: heuristic 
rule-based (e.g. Paice and Husk 1987; Lappin 
and Leass 1994; Kennedy and Boguraev 1996; 
Denber 1998; Vieira and Poesio 2000), statis-
tics-based (e.g., Bean and Riloff 1999; Cherry 
and Bergsma 2005; Bergsma et al2008) and 
learning-based (e.g. Evans 2001; Ng and Cardie 
2002a; Ng 2004; Yang et al2005; Denis and 
Balbridge 2007). 
For the heuristic rule-based approaches, 
Paice and Husk (1987), Lappin and Leass (1994), 
Kennedy and Boguraev (1996), Denber (1998), 
and Cherry and Bergsma (2005) looked for par-
ticular constructions using certain trigger words 
to identify pleonastic pronouns while Vieira and 
Poesio (2000) recognized non-anaphoric definite 
NPs through the use of syntactic cues and case-
sensitive rules and found that nearly 50% of 
definite NPs are non-anaphoric. As a representa-
tive, Lappin and Leass (1994), and Kennedy and 
Boguraev (1996) looked for modal adjectives 
(e.g. ?necessary?) or cognitive verbs (e.g. ?It is 
thought that ? ?) in a set of patterned construc-
tions.  
For the statistics-based approaches, Bean 
and Riloff (1999) developed a statistics-based 
method for automatically identifying existential 
definite NPs which are non-anaphoric. The intui-
tion behind is that many definite NPs are not 
anaphoric since their meanings can be under-
stood from general world knowledge. They 
found that existential NPs account for 63% of all 
definite NPs and 76% of them could be identi-
fied by syntactic or lexical means. Using 1600 
MUC-4 terrorism news documents as the train-
ing data, they achieved 87% in precision and 
78% in recall at identifying non-anaphoric defi-
nite NPs. Cherry and Bergsma (2005) extended 
the work of Lappin and Leass (1994) for large-
scale anaphoricity determination by additionally 
detecting non-anaphoric instances of it using 
Minipar?s pleonastic category Subj. This is done 
by both employing Minipar?s named entity rec-
ognition to identify time expressions, such as ?it 
was midnight? ?, and providing a number of 
other linguistic patterns to match common non-
anaphoric it cases, such as in expressions ?darn 
it? and don?t overdo it?. Bergsma et al(2008) 
proposed a distributional method in detecting 
non-anaphoric pronouns by first extracting the 
surrounding textual context of the pronoun, then 
gathering the distribution of words that occurred 
within that context from a large corpus and fi-
nally learning to classify these distributions as 
representing either anaphoric and non-anaphoric 
pronoun instances. Experiments on  the Science 
News corpus of It-Bank 1  in identifying non-
anaphoric pronoun it show that their distribu-
tional method achieved the performance of 
81.4%, 71.0% and 75.8 in precision, recall and 
F1-measure, respectively, compared with the 
performance of 93.4%, 21.0% and 34.3 in preci-
sion, recall and F1-measure, respectively using 
the rule-based approach as described in Lappin 
and Leass (1994), and  the performance of 
66.4%, 49.7% and 56.9 in precision, recall and 
F1-measure, respectively using the rule-based 
approach as described in Cherry and Bergsma 
(2005).  
Among the learning-based methods, Evans 
(2001) applied a machine learning approach on 
identifying the non-anaphoricity of pronoun it. 
Ng and Cardie (2002a) employed various do-
main-independent features in identifying ana-
phoric NPs and showed how such information 
                                                 
1 www.cs.ualberta.ca/~bergsma/ItBank/ 
979
 can be incorporated into a coreference resolution 
system. Experiments show that their method im-
proves the performance of coreference resolu-
tion by 2.0 and 2.6 to 65.8 and 64.2 in F1-
measure on the MUC-6 and MUC-7 corpora, 
respectively, due to much more gain in precision 
compared with the loss in recall. Ng (2004) ex-
amined the representation and optimization is-
sues in computing and using anaphoricity infor-
mation to improve learning-based coreference 
resolution systems. He used an anaphoricity 
classifier as a filter for coreference resolution. 
Evaluation on the ACE 2003 corpus shows that, 
compared with a baseline coreference resolution 
system of no explicit anaphoricity determination, 
their method improves the performance by 2.8, 
2.2 and 4.5 to 54.5, 64.0 and 60.8 in F1-measure 
(due to the gain in precision) on the NWIRE, 
NPAPER and BNEWS domains, respectively, 
via careful determination of an anaphoricity 
threshold with proper constraint-based represen-
tation and global optimization. However, he did 
not look into the contribution of anaphoricity 
determination on coreference resolution of dif-
ferent NP types, such as pronoun and definite 
NPs. Yang et al(2005) made use of non-
anaphors to create a special class of training in-
stances in the twin-candidate model (Yang et al
2003) and thus equipped it with the non-
anaphoricity determination capability. Experi-
ments show that the proposed method improves 
the performance by 2.9 and 1.6 to 67.3 and 67.2 
in F1-measure on the MUC-6 and MUC-7 cor-
pora, respectively, due to much more gain in 
precision compared with the loss in recall. How-
ever, surprisingly, their experiments also show 
that eliminating non-anaphors using an ana-
phoricity determination module in advance 
harms the performance.  Denis and Balbridge 
(2007) employed an integer linear programming 
(ILP) formulation for coreference resolution 
which models anaphoricity and coreference as a 
joint task, such that each local model informs the 
other for final assignments. Experiments on the 
NWIRE, NPAPER and BNEWS domains of the 
ACE 2003 corpus shows that this joint ana-
phoricity-coreference ILP formulation improves 
the F1-measure by 0.7-1.0 over the coreference-
only ILP formulation. However, their experi-
ments assume true ACE mentions(i.e. all the 
ACE mentions are already known from the an-
notated corpus). Therefore, the actual effect of 
this joint anaphoricity-coreference ILP formula-
tion on fully-automatic coreference resolution is 
still unclear. 
3 Label Propagation  
In the LP algorithm (Zhu and Ghahramani 2002), 
the natural clustering structure in data is repre-
sented as a connected graph. Given the labeled 
data and unlabeled data, the LP algorithm first 
represents labeled and unlabeled instances as 
vertices in a connected graph, then propagates 
the label information from any vertex to nearby 
vertices through weighted edges and finally in-
fers the labels of unlabeled instances until a 
global stable stage is achieved. Figure 1 presents 
the label propagation algorithm. 
___________________________________________
Assume:  
Y : the rn * labeling matrix, where ijy  represents 
the probability of vertex )1( nixi K=  with 
label )1( rjr j K= ; 
LY : the top l  rows of 
0Y . LY corresponds to the 
l  labeled instances; 
UY : the bottom u  rows of 
0Y . UY corresponds to 
the u  unlabeled instances; 
T : a nn *  matrix, with ijt  is the probability 
jumping from vertex ix to vertex jx ; 
 
BEGIN (the algorithm) 
Initialization:  
1)  Set the iteration index 0=t ;  
2)  Let 0Y  be the initial soft labels attached to 
each vertex;  
3)  Let 0LY  be consistent with the labeling in the 
labeled data, where 0ijy = the weight of the 
labeled instance if ix  has the label jr  ;  
4)  Initialize 0UY ; 
REPEAT 
Propagate the labels of any vertex to nearby ver-
tices by tt YTY =+1 ; 
Clamp the labeled data, that is, replace 1+tLY  
with 0LY ; 
UNTIL Y converges(e.g. 1+tLY  converges to 
0
LY ); 
Assign each unlabeled instance with a label: for 
)( nilxi ?p , find its label with 
j
ijymaxarg ; 
END (the algorithm) 
___________________________________________ 
Figure 1: The LP algorithm 
Here, each vertex corresponds to an instance, 
and the edge between any two instances ix  and 
jx  is weighted by ijw  to measure their similar-
ity. In principle, larger edge weights allow labels 
to travel through easier. Thus the closer the in-
stances are, the more likely they have similar 
980
 labels. The algorithm first calculates the weight 
ijw  using a kernel, then transforms it 
to ?
=
=?=
n
k
kjijij wwijpt
1
/)( , which meas-
ures the probability of propagating a label from 
instance jx to instance ix , and finally normal-
izes ijt row by row using ?
=
=
n
k
ikijij ttt
1
/  to maintain 
the class probability interpretation of the label-
ing matrix Y .  
During the label propagation process, the la-
bel distribution of the labeled data is clamped in 
each loop using their initial weights and acts like 
forces to push out labels through the unlabeled 
data. With this push originating from the labeled 
data, the label boundaries will be pushed faster 
along edges with larger weights and settle in 
gaps along those with lower weights. Ideally, we 
can expect that ijw  across different classes 
should be as small as possible and ijw  within the 
same class as big as possible. In this way, label 
propagation tends to happen within the same 
class. This algorithm has been shown to con-
verge to a unique solution (Zhu and Ghahramani 
2002), which can be obtained without iteration 
in theory, and the initialization of YU0 (the unla-
beled data) is not important since YU0 does not 
affect its estimation. However, proper initializa-
tion of YU0 actually helps the algorithm converge 
more rapidly in practice. In this paper, each row 
in YU0 , i.e. the label distribution for each test 
instance, is initialized to the weighted similarity 
of the test instance with the labeled instances. 
4 Kernel-based Similarity  
The key issue in label propagation is how to 
compute the similarity ijw between two in-
stances ix  and jx . This paper examines two 
similarity measures: the feature-based RBF ker-
nel and the convolution tree kernel. 
Feature Type Feature Description 
IsPronoun 1 if current NP is a pronoun, else 0 
IsDefiniteNP 1 if current NP is a define NP, else 0 
IsDemonstrativeNP 1 if current NP is a demonstrative NP,  else 0 
IsArg0 1 if the semantic role of current NP is Arg0/agent, else 0 
IsArg0MainVerb 1 if current NP has the semantic role of Arg0/agent for the 
main predicate of the sentence, else 0 
IsArgs 0 if current NP has no semantic role, else 1 
IsSingularNP 1 if current NP is a singular noun, else 0 
Features  
related with  
current NP itself 
IsMaleFemalePronoun 1 if current NP is a male/female personal pronoun, else 0 
StringMatch 1 if there is a full string match between current NP and one 
of other phrases in the context, else 0 
NameAlias 1 if current NP and one of other phrases in the context is a 
name alias or abbreviation of the other, else 0 
Appositive 1 if current NP and one of other phrases in the context are 
in an appositive structure, else 0 
NPNested 1 if current NP is nested in another NP, else 0 
NPNesting 1 if current NP nests another NP, else 0 
WordSenseAgreement 1 if current NP and one of other phrases in the context agree 
in the WordNet sense, else 0 
IsFirstNPinSentence 1 if current NP is the first NP of this sentence, else 0 
BackwardDistance The distance between current NP and  the nearest backward 
clause, indicated by coordinating words (e.g. that,which). 
Features  
related with  
the local context 
surrounding 
current NP 
ForwardDistance The distance between the nearest forward clause, indicated 
by coordinating words (e.g. that, which), and current NP. 
Table 1: Features in anaphoricity determination of NPs. Note: the semantic role-related features are derived from 
an in-house state-of-the-art semantic role labeling system.
4.1 Feature-based Kernel 
In our feature-based RBF kernel to anaphoricity 
determination, an instance is represented by 17 
lexical, syntactic and semantic features, as 
shown in Table 1, which are specifically de-
signed for distinguishing anaphoric and non-
anaphoric NPs, according to common-sense 
knowledge and linguistic intuitions. Since the 
local context surrounding an NP plays a critical 
role in discriminating whether an NP is ana-
phoric or not, the features in Table 1 can be clas-
sified into two categories: (a) current NP (i.e. the 
NP in anaphoricity consideration) itself, e.g. 
981
 types and semantic roles of  current NP; (b) con-
textual information, e.g.  whether current NP is 
nested in another NP, the distance between cur-
rent NP and a clause structure, indicated by co-
ordinating words (e.g. that, this, which). 
4.2 Tree Kernel 
Given a NP in anaphoricity determination, a 
parse tree represents the local context surround-
ing current NP in a structural way and thus con-
tains much information in determining whether 
current NP is anaphoric or not. For example, the 
commonly used knowledge for anaphoricity de-
termination, such as the grammatical role of cur-
rent NP or whether current NP is nested in other 
NPs, can be directly captured by a parse tree 
structure.  
Given a parse tree and a NP in consideration, 
the problem is how to choose a proper parse tree 
structure to cover syntactic structured informa-
tion well in the tree kernel computation. Gener-
ally, the more a parse tree structure includes, the 
more syntactic structured information would be 
provided, at the expense of more 
noisy/unnecessary information. In this paper, we 
limit the window size to 5  chunks (either NPs or 
non-NPs), including previous two chunks, cur-
rent chunk (i.e. current NP) and following two 
chunks, and prune out the substructures outside 
the window.  Figure 2 shows the full parse tree 
for the sentence ?Mary said the woman in the 
room hit her too?, using the Charniak parser 
(Charniak 2001), and the chunk sequence de-
rived from the parse tree using the Perl script2 
written by Sabine Buchholz from Tilburg Uni-
versity. 
Here, we explore four parse tree structures 
in NP anaphoricity determination: the common 
tree (CT), the shortest path-enclosed tree (SPT), 
the minimum tree (MT) and the dynamically 
extended tree (DET), motivated by Yang et al
(2006) and Zhou et al(2008). Following are the 
examples of the four parse tree structures, corre-
sponding to the full parse tree and the chunk se-
quence, as shown in Figure 2, with the NP chunk 
?(NP (DT the) (NN woman))? in anaphoricity 
determination. 
Common Tree (CT) 
As shown in Figure 3(a), CT is the complete 
sub-tree rooted by the nearest common ancestor 
of the first chunk ?(NP (NNP Mary))? and the 
                                                 
2 http://ilk.kub.nl/~sabine/chunklink/  
last chunk ?(NP (DT the) (NN room))? of the 
five-chunk window.  
Shortest Path-enclosed Tree (SPT) 
As shown in Figure 3(b), SPT is  the smallest 
common sub-tree enclosed by the shortest path 
between the first chunk ?(NP (NNP Mary))? and 
the last chunk ?(NP (DT the) (NN room))? of the 
five-chunk window.  
 
(a) the full parse tree 
(NP (NNP Mary)) (VP (VBD said)) (NP-E (DT the) 
(NN woman)) (PP (IN in)) (NP (DT the) (NN room)) 
(VP (VBD hit)) (NP (PRP her)) (ADVP (RB too)) 
(b) the chunk sequence 
Figure 2: The full parse tree for the sentence ?Mary 
said the woman in the room hit her too?, using the 
Charniak parser, and the corresponding chunk se-
quence derived from it. Here, the label ?E? indicates 
the NP in consideration. 
 
(a) CT: Common Tree 
 
(b) SPT: Shortest Path-enclosed Tree 
982
  
(c) MT: Minimum Tree 
 
(d) DET: Dynamically Extended Tree 
Figure 3: Examples of parse tree structures. 
Minimum Tree (MT) 
As shown in Figure 3(c), MT only keeps the root 
path from the NP in anaphoricity determination 
to the root node of SPT. 
Dynamically Extended Tree (DET),  
The intuitions behind DET are that the informa-
tion related with antecedent candidates (all the  
antecedent candidates compatible3 with current 
NP in anaphoricity consideration), predicates 4 
and right siblings plays a critical role in corefer-
ence resolution. Given a MT, this is done by:  
1)  Attaching all the compatible antecedent can-
didates and their corresponding paths. As 
shown in Figure 3(d), ?Mary? is attached 
while ?the room? is not since the former is 
compatible with the NP ?the woman? and 
the latter is not compatible with the NP ?the 
woman?. In this way, possible coreference 
between current NP and the compatible an-
tecedent candidates can be included in the 
parse tree structure. In some sense, this is a 
natural extension of the twin-candidate 
                                                 
3 With matched number, person and gender agreements. 
4 For simplicity, only verbal predicates are considered in 
this paper. However, this can be extended to nominal predi-
cates with automatic identification of nominal predicates. 
learning method proposed in Yang et al
(2003), which explicitly models the compe-
tition between two antecedent candidates.  
2)  For each node in MT, attaching the path from 
the node to the leaf node of the correspond-
ing predicate, if it is predicate-headed, in the 
sense that such predicate-related information 
is useful in identifying certain kinds of ex-
pressions with non-anaphoric NPs, e.g. the 
non-anaphoric it in ?darn it?. As shown in 
Figure 3(d), ?said? and ?hit? are attached.  
3)  Attaching the path to the head word of the 
first right sibling if the parent of current NP 
is a NP and current NP has one or more right 
siblings. Normally, the NP in anaphoricity 
consideration, NP-E, in the production of 
?NP->NP-E+PP? introduces a new entity 
and thus non-anaphoric. 
4)  Pruning those nodes (except POS nodes) 
with the single in-arc and the single out-arc 
and with its syntactic phrase type same as its 
child node.  
In this paper, the similarity between two 
parse trees is measured using a convolution tree 
kernel, which counts the number of common 
sub-trees as the syntactic structure similarity 
between two parse trees. For details, please refer 
to Collins and Duffy (2001). 
5 Experimentation  
We have systematically evaluated the label 
propagation algorithm on global learning of NP 
anaphoricity determination on the ACE 2003 
corpus, and its application in coreference resolu-
tion. 
5.1 Experimental Setting 
The ACE 2003 corpus contains three domains: 
newswire (NWIRE), newspaper (NPAPER), and 
broadcast news (BNEWS). For each domain, 
there exist two data sets, training and devtest, 
which are used for training and testing respec-
tively.  
As a baseline coreference resolution system, 
a  raw test text is first preprocessed automati-
cally by a pipeline of NLP components, includ-
ing sentence boundary detection, POS tagging, 
named entity recognition and phrase chunking, 
and then a training or test instance is formed by 
a anaphor and one of its antecedent candidates, 
similar to Soon et al(2001). Among them, 
named entity recognition, part-of-speech tagging 
and noun phrase chunking apply the same Hid-
den Markov Model (HMM)-based engine with 
983
 error-driven learning capability (Zhou and Su, 
2000 & 2002). During training, for each anaphor 
encountered, a positive instance is created by 
pairing the anaphor and its closest antecedent 
while a set of negative instances is formed by 
pairing the anaphor with each of the non-
coreferential candidates. Based on the training 
instances, a binary classifier is generated using a 
particular learning algorithm. In this paper, we 
use SVMLight developed by Joachims (1998). 
During resolution, an anaphor is first paired in 
turn with each preceding antecedent candidate to 
form a test instance, which is presented to a 
classifier. The classifier then returns a confi-
dence value indicating the likelihood that the 
candidate is the antecedent. Finally, the candi-
date with the highest confidence value is se-
lected as the antecedent. As a baseline, the NPs 
with mismatched number, person and gender 
agreements are filtered out. On average, an ana-
phor has ~7 antecedent candidates. In particular, 
the test corpus is resolved in document-level, i.e. 
one document by one document. 
For anaphoricity determination, we report 
the performance in Acc+ and Acc-, which meas-
ure the accuracies of identifying anaphoric NPs 
and non-anaphoric NPs, respectively. Obviously, 
higher Acc+ means that more anaphoric NPs 
would be identified correctly, while higher Acc- 
means that more non-anaphoric NPs would be 
filtered out. For coreference resolution, we re-
port the performance in terms of recall, precision, 
and F1-measure using the commonly-used 
model theoretic MUC scoring program (Vilain 
et al, 1995). For separate scoring of different 
NP types, a recognized reference is considered 
correct if the reconized antecedent is in the 
coreferential chain of the anaphor. To see 
whether an improvement is significant, we con-
duct significance testing using paired t-test. In 
this paper, ?>>>?, ?>>? and ?>? denote p-values 
of an improvement smaller than 0.01, in-
between (0.01, 0,05] and bigger than 0.05, 
which mean significantly better, moderately 
better and slightly better, respectively.  
5.2 Experimental Results 
Table 2 shows the performance of LP-based ana-
phoricity determination using the feature-based 
RBF kernel. It shows that our method achieves 
the accuracies of 74.8/84.4, 76.2/81.3 and 
71.8/81.7 on identifying anaphoric/non-
anaphoric NPs in the NWIRE, NPAPER and 
BNEWS domains, respectively. This suggests 
that our approach can effectively filter out about 
82% of non-anaphoric NPs. However, it can 
only keep about 74% of anaphoric NPs. Table 2 
also shows the performance on different NP 
types. Considering the effectiveness of ana-
phoricity determination on indefinite NPs (due 
to that most of anaphoric indefinite NPs are in 
an appositive structure and thus can be easily 
captured by the IsAppositive feature) and that 
most of errors in anaphoricity determination on 
proper nouns are caused by the named entity 
recognition module in the preprocessing), it in-
dicates the difficulty of anaphoricity determina-
tion in filtering out non-anaphoric pronouns and 
identifying anaphoric definite NPs. As a com-
parison, Table 2 also shows the performance of 
locally-optimized anaphoricity determination 
using a classifier (SVM with the feature-based 
RBF kernel, as adopted in this paper) to deter-
mine the NPs in a text individually. It shows that 
the LP-based method systematically outperforms 
(>>>) the SVM-based method. This suggests the 
effectiveness of the LP algorithm in global mod-
eling of the natural clustering structure in ana-
phoricity determination. 
Table 3 shows the performance of LP-based 
anaphoricity determination using the convolu-
tion tree kernel on different parse tree structures. 
It shows that while MT performed worst due to 
its simple structure, DET outperforms MT(>>>), 
SPT(>>>) and CT(>>>) on all the three domains 
due to fine inclusion of necessary structural in-
formation, although inclusion of more informa-
tion in both CT and SPT also improves the per-
formance. It again verifies that LP-based ana-
phoricity determination outperforms (>>>) 
SVM-based one, using the tree kernel. Table 4 
further indicates that all the three kinds of struc-
tural information related with antecedent candi-
dates, predicates and right siblings in DET con-
tribute significantly (>>>). In addition, Table 5 
shows the detailed performance of LP-based 
anaphoricity determination on different anaphor 
types using DET. Compared with the feature-
based RBF kernel as shown in Table 2, it shows 
that the convolution tree kernel significantly 
outperforms (>>>) the feature-based RBF kernel 
in all the three domains, with much contribution 
due to performance improvement on both pro-
nouns and definite NPs, although the tree kernel 
performs moderately worse than the feature-
based RBF kernel due to the effectiveness of 
anaphoricity determination on proper nouns and 
indefinite NPs using the IsNameAlias and IsAp-
positive features respectively. 
 
984
 NWIRE NPAPER BNEWS Anaphor 
Type Acc
+ 
(%) 
Acc-
(%) 
Acc+ 
(%) 
Acc-
(%) 
Acc+ 
(%) 
Acc-
(%) 
Pronoun 88.7 56.2 90.2 58.6 87.4 57.8 
ProperNoun 72.5 85.2 74.6 80.5 70.6 78.8 
DefiniteNP 66.6 83.1 72.1 77.5 65.3 81.5 
InDefiniteNP 95.4 93.7 90.5 95.8 87.2 97.3 
Overall 74.8 84.4 76.2 81.3 71.8 81.7 
Overall(SVM) 71.3 80.2 73.5 79.1 68.4 78.6 
Table 2: The performance of LP-based anaphoric-
ity determination using the feature-based RBF kernel  
Parse Tree structure  
Scheme 
NWIRE
( %)  
NPAPER
( %)  
BNEWS
( %)  
Acc+ 72.6 74.3 74.2 CT Acc- 82.1 80.2 72.3 
Acc+ 72.4 74.1 73.8 SPT Acc- 80.8 79.5 72.5 
Acc+ 71.4 70.5 66.9 MT Acc- 77.2 75.3 78.2 
Acc+ 79.2 81.2 76.5 DET Acc- 87.8 84.5 85.3 
Acc+ 76.5 78.9 74.3 DET(SVM) 
Acc- 82.3 81.6 83.2 
Table 3: The performance of LP-based anaphoric-
ity determination using the convolution tree kernel on 
different parse tree structures 
Performance Change NWIRE( %)  
NPAPER
( %)  
BNEWS
( %)  
Acc+ -4.0 -3.8 -4.3 - antecedent 
candidates Acc- -5.2 -5.3 -4.5 
Acc+ -5.2 -4.8 -5.6 -predicate Acc- -4.3 -3.5 -4.9 
Acc+ -3.6 -4.1 -3.1 -first right 
sibling Acc- -4.8 -5.2 -4.4 
Table 4: The contribution of structural information 
in DET 
 
NWIRE NPAPER BNEWS Anaphor 
Type Acc
+ 
(%) 
Acc-
(%) 
Acc+ 
(%) 
Acc-
(%) 
Acc+ 
(%) 
Acc-
(%) 
Pronoun 90.1 75.6 90.7 79.2 89.2 77.5 
ProperNoun 71.4 83.5 72.8 78.1 68.3 77.2 
DefiniteNP 74.6 89.1 77.3 85.5 75.3 88.7 
InDefiniteNP 93.2 92.1 90.2 94.2 89.4 95.5 
Overall 79.2 87.8 81.2 84.5 76.5 85.3 
Table 5: The performance of LP-based anaphoric-
ity determination using the tree kernel on DET 
Finally, we evaluate the effect of LP-based 
anaphoricity determination on coreference reso-
lution by including it as a preprocessing step to a 
baseline coreference resolution system without 
explicit anaphoricity determination, which em-
ploys the same set of features, as adopted in the 
single-candidate model of Yang et al(2003), 
using a SVM-based classifier and the feature-
based RBF kernel. It shows that anaphoricity 
determination with the feature-based RBF Ker-
nel much improves (>>>) the performance of 
coreference resolution with most of the contribu-
tion due to pronoun resolution while its contri-
bution on definite NPs can be ignored. It indi-
cates the usefulness of anaphoricity determina-
tion in filtering out non-anaphoric pronouns and 
the difficulty in identifying anaphoric definite 
NPs, using the feature-based RBF kernel. It also 
shows that tree kernel-based anaphoricity deter-
mination can not only improve (>>>) the per-
formance on pronoun resolution but also im-
prove (>>>) the performance on definite NP 
resolution due to the much better performance of 
tree kernel-based anaphoricity determination on 
definite NPs. This suggests the necessity of ex-
ploring structural information in identifying 
anaphoric definite NPs. 
NWIRE NPAPER BNEWS System 
R% P% F R% P% F R% P% F 
Pronoun 66.5 61.6 64.0 70.1 64.2 67.0 61.7 63.2 62.4 
DefiniteNP 26.9 80.3 40.2 34.5 62.4 44.4 30.5 71.4 42.9 BaseLine (No Anaphoricity) 
Overall 53.1 67.4 59.4 57.7 67.0 62.1 48.0 65.9 55.5 
Pronoun 64.1 67.9 66.0 67.3 72.4 69.8 59.5 75.7 66.6 
DefiniteNP 26.7 80.6 40.3 34.2 62.5 44.3 30.4 71.9 43.1 +Anaphoricity determination  with the feature-based RBF kernel Overall 50.6 75.4 60.7 54.4 77.1 63.8 45.9 76.9 57.4 
Pronoun 63.5 70.9 67.0 68 74.9 71.3 61.1 77.6 68.3 
DefiniteNP 28.5 82.4 42.1 36.2 65.3 46.1 32.3 73.1 44.2 +Anaphoricity determination with the convolution tree kernel Overall 51.6 77.2 61.8 55.2 78.6 65.2 47.5 80.3 59.6 
Table 6: Employment of anaphoricity determination in coreference resolution 
6 Conclusion  
This paper systematically studies a global learn-
ing method in identifying the anaphoricity of 
noun phrases via a label propagation algorithm 
and the application of an explicit anaphoricity 
determination module in improving learning-
based coreference resolution. In particular, two 
kinds of kernels, i.e. the feature-based RBF ker-
nel and the convolution tree kernel, are em-
ployed to compute the anaphoricity similarity 
985
 between two NPs. Evaluation on the ACE 2003 
corpus indicates that LP-based anaphoricity de-
termination using both the kernels much im-
proves the performance of coreference resolu-
tion. It also shows the usefulness of various 
structural information, related with antecedent 
candidates, predicates and right siblings, in  tree 
kernel-based anaphoricity determination and in 
coreference resolution of both pronouns and 
definite NPs. 
To our knowledge, this is the first system-
atic exploration of both feature-based and tree 
kernel methods in anaphoricity determination 
and the application of an explicit anaphoricity 
determination module in learning coreference 
resolution.  
Acknowledgement  
This research is supported by Project 60873150 
under the National Natural Science Foundation of  
China, project 2006AA01Z147 under the  ?863? 
National High-Tech Research and Development of 
China, project 200802850006 under the National 
Research Foundation for the Doctoral Program of 
Higher Education of China. 
References  
Bean D. and Riloff E. (1999). Corpus-based Identifi-
cation of Non-Anaphoric Noun Phrases. 
ACL?1999:373-380. 
Bergsma S., Lin D.K. and Goebel R. (2008). Distri-
butional Identification of Non-Referential Pro-
nouns. ACL?2008: 10-18. 
Charniak E. (2001). Immediate-head Parsing for Lan-
guage Models. ACL?2001: 129-137.  
Cherry C. and Bergsma S. (2005). An expectation 
maximization approach to pronoun resolution. 
CoNLL?2005:88-95. 
Collins M. and Duffy N. (2001). Convolution kernels 
for natural language. NIPS?2001: 625-632. 
Denber M. (1998). Automatic Resolution of Anaph-
ora in English. Technical Report, Eastman Kodak 
Co. 
Denis P. and Baldridge J. (2007). Joint determination 
of anaphoricity and coreference using integer pro-
gramming. NAACL-HLT?2007:236-243. 
Evans R. (2001). Applying machine learning toward 
an automatic classification of it. Literary and Lin-
guistic Computing, 16(1):45.57. 
Joachims T. (1998). Text Categorization with Sup-
port Vector Machine: learning with many relevant 
features. ECML-1998: 137-142. 
Kennedy C. and Boguraev B. (1996). Anaphora for 
everyone: pronominal anaphora resolution without 
a parser. COLING?1996: 113-118. 
Lappin S. and Leass H.J. (1994). An algorithm for 
pronominal anaphora resolution. Computational 
Linguistics, 20(4):535.561. 
Ng V. and Cardie C. (2002a). Identifying anaphoric 
and non-anaphoric noun phrases to improve 
coreference resolution. COLING?2002:730-736. 
Ng V. and Cardie C. (2002b). Improving machine 
learning approaches to coreference resolution. 
ACL?2002: 104-111 
Ng V. (2004). Learning Noun Phrase Anaphoricity to 
Improve Conference Resolution: Issues in Repre-
sentation and Optimization. ACL?2004: 151-158 
Paice C.D. and Husk G.D. (1987). Towards the auto-
matic recognition of anaphoric features in English 
text: the impersonal pronoun it. Computer Speech 
and Language, 2:109-132. 
Soon W.M., Ng H.T. and Lim D. (2001). A machine 
learning approach to coreference resolution of 
noun phrase. Computational Linguistics, 2001, 
27(4):521-544. 
Strube M. and Muller C. (2003). A machine learning 
approach to pronoun resolution in spoken dialogue. 
ACL?2003: 168-175 
Vieira R. and Poesio M. (2000). An empirically 
based system for processing definite descriptions. 
Computational Linguistics, 27(4): 539?592. 
Vilain M., Burger J., Aberdeen J., Connolly D. and 
Hirschman L. (1995). A model theoretic corefer-
ence scoring scheme. MUC-6: 45?52. 
Yang X.F., Zhou G.D., Su J. and Chew C.L. (2003). 
Coreference Resolution Using Competition Learn-
ing Approach. ACL?2003:177-184 
Yang X.F., Su J. and Tan C.L. (2005). A Twin-
Candidate Model of Coreference Resolution with 
Non-Anaphor Identification Capability. 
IJCNLP?2005:719-730. 
Yang X.F., Su J. and Tan C.L. (2006). Kernel-based 
pronoun resolution with structured syntactic 
knowledge. COLING-ACL?2006: 41-48. 
Yang X.F., Su J., Lang J., Tan C.L., Liu T. and Li S. 
(2008). An Entity-Mention Model for Coreference 
Resolution with Inductive Logic Programming. 
ACL?2008: 843-851. 
Zhou G.D. and Su. J. (2000). Error-driven HMM-
based chunk tagger with context-dependent lexi-
con.  EMNLP-VLC?2000: 71?79 
Zhou G.D. and Su J. (2002). Named Entity recogni-
tion using a HMM-based chunk tagger. In 
ACL?2002:473?480. 
Zhou G.D., Kong F. and Zhu Q.M. (2008). Context-
sensitive convolution tree kernel for pronoun 
resolution . IJCNLP?2008:25-31. 
Zhu X. and Ghahramani Z. (2002). Learning from 
Labeled and Unlabeled Data with Label Propaga-
tion. CMU CALD Technical Report.CMU-CALD-
02-107. 
986
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 987?996,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
Employing the Centering Theory in Pronoun Resolution from the Se-
mantic Perspective 
 
KONG Fang     ZHOU GuoDong*    ZHU Qiaoming 
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow University. Suzhou, China 215006 
Email: {kongfang, gdzhou, qmzhu}@suda.edu.cn 
 
  
 
                                                          
* Corresponding author 
Abstract 
In this paper, we employ the centering the-
ory in pronoun resolution from the seman-
tic perspective. First, diverse semantic role 
features with regard to different predicates 
in a sentence are explored. Moreover, given 
a pronominal anaphor, its relative ranking 
among all the pronouns in a sentence, ac-
cording to relevant semantic role informa-
tion and its surface position, is incorporated. 
In particular, the use of both the semantic 
role features and the relative pronominal 
ranking feature in pronoun resolution is 
guided by extending the centering theory 
from the grammatical level to the semantic 
level in tracking the local discourse focus. 
Finally, detailed pronominal subcategory 
features are incorporated to enhance the 
discriminative power of both the semantic 
role features and the relative pronominal 
ranking feature. Experimental results on the 
ACE 2003 corpus show that the centering-
motivated features contribute much to pro-
noun resolution.  
1 Introduction 
Coreference accounts for cohesion in a text and 
is, in a sense, the hyperlink for a natural lan-
guage. Especially, a coreference instance de-
notes an identity of reference and holds between 
two referring expressions, which can be named 
entities, definite noun phrases, pronouns and so 
on. Coreference resolution is the process of link-
ing together multiple referring expressions of a 
given entity in the world. The key in coreference 
resolution is to determine the antecedent for 
each referring expression in a text. The ability of 
linking referring expressions both within a sen-
tence and across the sentences in a text is critical 
to discourse and language understanding in gen-
eral. For example, coreference resolution is a 
key task in information extraction, machine 
translation, text summarization, and question 
answering. 
There is a long tradition of research on 
coreference resolution within computational lin-
guistics. While earlier knowledge-lean ap-
proaches heavily depend on domain and 
linguistic knowledge (Carter 1987; Carbonell 
and Brown 1988) and have significantly influ-
enced the research, the later approaches usually 
rely on diverse lexical, syntactic and semantic 
properties of referring expressions (Soon et al, 
2001;Ng and Cardie, 2002; Zhou et al, 2004). 
Current research has been focusing on exploiting 
semantic information in coreference resolution. 
For example, Yang et al(2005) proposed a tem-
plate-based statistical approach to compute the 
semantic compatibility between a pronominal 
anaphor and an antecedent candidate, and Yang 
and Su (2007) explored semantic relatedness 
information from automatically discovered pat-
terns, while Ng (2007) automatically induced 
semantic class knowledge from a treebank and 
explored its application in coreference resolution. 
Particularly, this paper focuses on the center-
ing theory (Sidner,1981;Grosz et al,1995; 
Tetreault,2001), which reveals the significant 
impact of the local focus on referring expres-
sions in that the antecedent of a referring expres-
sion usually depends on the center of attention 
throughout the local discourse segment (Mit-
kov,1998). Although the centering theory has 
been considered as a critical theory and the driv-
ing force behind the coreferential phenomena 
since its proposal, its application in coreference 
resolution (in particular pronoun resolution) has 
been somewhat disappointing: it fails to improve 
or even harms the performance of the state-of-
987
 the-art coreference resolution systems in previ-
ous research (e.g. Yang et al 2004). This may be 
due to that centering was originally proposed as 
a model of discourse coherence instead of 
coreference. 
The purpose of this paper is to employ the 
centering theory in pronoun resolution by ex-
tending it from the grammatical level to the se-
mantic level. The intuition behind our approach 
is that, via determining the semantic roles of 
referring expressions in a sentence, such as 
agent and patient, we can derive various center-
ing theory-motivated features in tracking the 
continuity or shift of the local discourse focus, 
thus allowing us to include document-level 
event descriptive information in resolving the 
coreferential relations between referring expres-
sions.  
To the best of our knowledge, this is the first 
research, which successfully applies the center-
ing theory in pronoun resolution from the se-
mantic perspective.  
The rest of this paper is organized as follows. 
Section 2 briefly describes related work in em-
ploying the centering theory and semantic in-
formation in coreference resolution. Then, the 
centering theory is introduced in Section 3 while 
Section 4 details how to employ the centering 
theory from the semantic perspective. Section 5 
reports and discusses the experimental results. 
Finally, we conclude our work in Section 6. 
2 Related Work 
This section briefly overviews the related work 
in coreference resolution from both the centering 
theory and semantic perspectives. 
2.1 Centering Theory 
In the literature, there has been much research in 
the centering theory and its application to 
coreference resolution. 
In the centering theory itself, since the origi-
nal work of Sidner (1979) on immediate focus-
ing of pronouns and the subsequent work of 
Joshi and Weinstein (1981) on centering and 
inferences, much research has been done, in-
cluding centering and linguistic realizations 
(Cote 1993; Prince and Walker 1995), empirical 
and psycholinguistic evaluation of centering 
predictions (Gordon et al 1993,1995; Brennan 
1995; Walker et al1998; Kibble 2001), and the 
cross-linguistic work on centering (Ziv and 
Crosz1994). 
In applications of the centering theory to 
coreference resolution, representative work in-
cludes Brennan et al (1987), Strube (1998), 
Tetreault (1999) and Yang et al (2004). Brennan 
et al (1987) presented a centering theory-based 
formalism in modeling the local focus structure 
in discourse and used it to track the discourse 
context in binding occurring pronouns to corre-
sponding entities. In particular, a BFP (Brennan, 
Friedman and Pollard) algorithm is proposed to 
extend the original centering model to include 
two additional transitions called smooth shift 
and rough shift. Strube (1998) proposed an S-list 
model, assuming that a referring expression pre-
fers a hearer-old discourse entity to other hearer-
new candidates. Tetreault (1999) further ad-
vanced the BFP algorithm by adopting a left-to-
right breadth first walk of the syntactic parse 
trees to rank the antecedent candidates. However, 
the above methods have not been systematically 
evaluated on large annotated corpora, such as 
MUC and ACE. Thus their effects are still un-
clear in real coreference resolution tasks. Yang 
et al(2004) presented a learning-based approach 
by incorporating several S-list model-based fea-
tures to improve the performance in pronoun 
resolution. It shows that, although including S-
list model-based features can slightly boost the 
performance in the ideal case (i.e. given the cor-
rect antecedents of anaphor?s candidates), it de-
teriorates the overall performance in F-measure 
with slightly higher precision but much lower 
recall, in real cases, where the antecedents of 
anaphor?s candidates are determined automati-
cally by a separate coreference resolution mod-
ule.  
2.2 Semantic Information 
It is well known that semantic information plays 
a critical role in coreference resolution. Besides 
the common practice of employing a thesaurus 
(e.g. WordNet) in semantic consistency check-
ing, much research has been done to explore 
various kinds of semantic information, such as 
semantic similarity (Harabagiu et al2000), se-
mantic compatibility (Yang et al2005, 2007), 
and semantic class information (Soon et al2001; 
Ng 2007). Although these methods have been 
proven useful in coreference resolution, their 
contributions are much limited. For example, Ng 
(2007) showed that semantic similarity informa-
tion and semantic agreement information could 
only improve the performance of coreference 
resolution by 0.6 and 0.5 in F-measure respec-
tively, on the ACE 2003 NWIRE corpus.  
988
 3 Centering Theory 
The centering theory is a theory about the local 
discourse structure that models the interaction of 
referential continuity and the salience of dis-
course entities in the internal organization of a 
text. In natural languages, a given entity may be 
referred by different expressions and act as dif-
ferent grammatical roles throughout a text. For 
example, people often use pronouns to refer to 
the main subject of the discourse in focus, which 
can change over different portions of the dis-
course. One main goal of the centering theory is 
to track the focus entities throughout a text.  
The main claims of the centering theory can 
be formalized in terms of Cb (the backward-
looking center), Cf (a list of forward-looking 
centers for each utterance Un) and Cp (the pre-
ferred center, i.e. the most salient candidate for 
subsequent utterances). Given following two 
sentences: 1) Susani gave Betsyj a pet hamsterk; 
2) Shei reminded herj that such hamstersk were 
quite shy. We can have Ub, Uf and Up as follows: 
Ub= ?Susan?; Uf={?Susan?, ?Betsy?, ?a pet 
hamster?}; Up= ?Susan?. 
 Cb(Un)=Cb(Un-1)  or Cb(Un-1) undefined
Cb(Un)?Cb(Un-1)
Cb(Un)=Cp(Un) Continue Smooth Shift
Cb(Un)?Cp(Un) Retain Rough Shift
Table 1: Transitions in the centering theory 
Constraints 
C1. There is precisely one Cb. 
C2. Every element of Cf(Un) must be realized in Un. 
C3. Cb(Un) is the highest-ranked element of Cf(Un-1) 
that is realized in Un. 
Rules 
R1. If some element of Cf(Un-1) is realized as a pro-
noun in Un, then so is Cb(Un). 
R2.Transitions have the descending preference order 
of ?Continue > Retain > Smooth Shift > Rough 
Shift?. 
Table 2: Constraints and rules in the centering theory 
Furthermore, several kinds of focus transi-
tions are defined in terms of two tests: whether 
Cb stays the same (i.e. Cb(Un+1)=Cb(Un)), and 
whether Cb is realized as the most prominent 
referring expression (i.e. Cb(Un=Cp(Un)). We 
refer to the first test as cohesion, and the second 
test as salience. Therefore, there are four possi-
ble combinations, which are displayed in Table 
1 and can result in four kinds of transitions, 
namely Continue, Retain, Smooth Shift, and 
Rough Shift. Obviously, salience, which chooses 
a proper verb form to make Cb prominent within 
a clause or sentence, is an important matter for 
sentence planning, while cohesion, which orders 
propositions in a text to maintain referential con-
tinuity, is an important matter for text planning.  
Finally, the centering theory imposes several 
constraints and rules over Cb/Cf and above tran-
sitions, as shown in Table 2. 
Given the centering theory as described above, 
we can draw the following conclusions: 
1) The centering theory is discourse-related and 
centers are discourse constructs.   
2) The backward-looking center Cb of Un de-
pends only on the expressions that constitute 
the utterance. That is, it is independent of its 
surface position and grammatical roles. 
Moreover, it is not constrained by any previ-
ous utterance in the segment. While the ele-
ments of Cf(Un) are partially ordered to 
reflect relative prominence in Un, grammati-
cal role information is often a major determi-
nant in ranking Cf, e.g. in the descending 
priority order of ?Subject > Object > Others? 
in English (Grosz and Joshi, 2001).  
3) Psychological research (Gordon et al 1993) 
and cross-linguistic research (Kameyama 
1986, 1988; Walker et al 1990,1994) have 
validated that Cb is preferentially realized by 
a pronoun in English.  
4) Frequent rough shifts would lead to a lack of 
local cohesion. To keep local cohesion, peo-
ple tend to plan ahead and minimize the 
number of focus shifts. 
In this paper, we extend the centering theory 
from the grammatical level to the semantic level 
in attempt to better model the continuity or shift 
in the local discourse focus and improve the per-
formance of pronoun resolution via centering-
motivated semantic role features. 
4 Employing Centering Theory from  
Semantic Perspective 
In this section, we discuss how to employ the 
centering theory in pronoun resolution from the 
semantic perspective. In Subsection 4.1, we in-
troduce the semantic roles. In Subsection 4.2, we 
introduce how to employ the centering theory in 
pronoun resolution via semantic role features. 
Finally we compare our method with the previ-
ous work in Subsection 4.3. 
4.1 Semantic Role 
A semantic role is the underlying relationship 
that a participant has with a given predicate in a 
clause, i.e. the actual role a participant plays in 
989
 an event, apart from linguistic encoding of the 
situation. If, in some situation, someone named 
?John? purposely hits someone named ?Bill?, 
then ?John? is the agent and ?Bill? is the patient 
of the hitting event. Therefore, given the predi-
cate ?hit? in both of the following sentences, 
?John? has the same semantic role of agent and 
?Bill? has the same semantic role of patient: 1) 
John hit Bill. 2) Bill was hit by John.  
In the literature, labeling of such semantic 
roles has been well defined by the SRL (Seman-
tic Role Labeling) task, which first identifies the 
arguments of a given predicate and then assigns 
them appropriate semantic roles. During the last 
few years, there has been growing interest in 
SRL. For example, CoNLL 2004 and 2005 have 
made this problem a well-known shared task. 
However, there is still little consensus in the lin-
guistic and NLP communities about what set of 
semantic role labels are most appropriate. Typi-
cal semantic roles include core roles, such as 
agent, patient, instrument, and adjunct roles 
(such as locative, temporal, manner, and cause). 
For core roles, only agent and patient are consis-
tently defined across different predicates, e.g. in 
the popular PropBank (Palmer et al 2005) and 
the derived version evaluated in the CoNLL 
2004 and 2005 shared tasks, as ARG0 and 
ARG1.  
In this paper, we extend the centering theory 
from the grammatical level to the semantic level 
for its better application in pronoun resolution 
via proper semantic role features due to three 
reasons:  
Sentence Grammatical Role Semantic Role
Bob opened the 
door with a key. 
Bob:  
SUBJECT 
Bob:  
AGENT 
The key opened 
the door. 
The key: 
SUBJECT 
The key : 
INSTRUMENT
The door opened. The door: 
SUBJECT 
The door: 
PATIENT 
Table 3: Relationship between grammatical roles and 
semantic roles: an example 
1) Semantic roles are conceptual notions, 
whereas grammatical roles are morph-
syntactic. While the original centering theory 
mainly builds from the grammatical perspec-
tive and grammatical roles do not always cor-
respond directly to semantic roles (Table 3 
shows an example of various semantic roles 
which a subject can play), there is a close re-
lationship between semantic roles and gram-
matical roles. The statistics in the CoNLL 
2004 and 2005 shared tasks (Shen and Lapata, 
2007) shows that the semantic roles of 
ARG0/agent and ARG1/patient account for 
85% of all arguments and most likely act as 
the centers of the local focus structure in dis-
course due to the close relationship between 
subject/object and agent/patient. Therefore, it 
is appropriate to model the centers of an ut-
terance from the semantic perspective via 
semantic roles. 
2) In a sense, semantic roles imply the informa-
tion of grammatical roles, especially for sub-
ject/object. For example, the position of an 
argument and the voice of the predicate verb 
play a central role in SRL. In intuition, an ar-
gument, which occurs before an active verb 
and has the semantic role of Arg0/agent, 
tends to be a subject. That is to say, semantic 
roles (e.g. Arg0/agent and Arg1/patient) can 
be mapped into their corresponding gram-
matical roles (e.g. subject and object), using 
some heuristic rules. Therefore, it would be 
interesting to represent the centers of the ut-
terances and employ the centering theory 
from the semantic perspective. 
3) Semantic role labeling has been well studied 
in the literature and there are good ready-to-
use toolkits available. For example, Pradhan 
(2005) achieved 82.2 in F-measure on the 
CoNLL 2005 version of the Propbank. In 
contrast, the research on grammatical role la-
beling is much less with the much lower 
state-of-the-art performance of 71.2 in F-
measure (Buchholz, 1999). Therefore, it may 
be better to explore the centering theory from 
the semantic perspective. 
4.2 Designing Centering-motivated Fea-
tures from  Semantic Perspective 
In this paper, the centering theory is employed in 
pronoun resolution via three kinds of centering-
motivated features: 
1) Semantic role features. They are achieved by 
checking possible semantic roles of referring 
expressions with regard to various predicates 
in a sentence. Due to the close relationship 
between subject/object and agent/patient, se-
mantic role information should be also a ma-
jor determinant in deciding the center of an 
utterance, which is likely to be the antecedent 
of a referring expression in the descending 
priority order of ?Agent > Patient > Others? 
with regard to their semantic roles, corre-
sponding to the descending priority order of 
?Subject > Object > Others? with regard to 
their grammatical roles. 
990
 2) Relative pronominal ranking feature. Due to 
the predominance of pronouns in tracking the 
local discourse structure1, the relative rank-
ing of a pronoun among all the pronouns in a 
sentence should be useful in pronoun resolu-
tion. This is realized in this paper according 
to its semantic roles (with regard to various 
predicates in a sentence) and surface position 
(in a left-to-right order) by mapping each 
pronoun into 5 levels: a) rank 1 for pronouns 
with semantic role ARG0/agent of the main 
predicate; b) rank 2 for pronouns with seman-
tic role ARG1/patient of the main predicate; c) 
rank 3 for pronouns with semantic role 
ARG0/agent of other predicates; d) rank 4 for 
pronouns with semantic role ARG1/patient of 
other predicates; e) rank 5 for remaining pro-
nouns. Furthermore, for those pronouns with 
the same ranking level, they are ordered ac-
cording to their surface positions in a left-to-
right order, motivated by previous research 
on the centering theory (Grosz et al 1995). 
3) Detailed pronominal subcategory features. 
Given a pronominal expression, its detailed 
pronominal subcategory features, such as 
whether it is a first person pronoun, second 
person pronoun, third person pronoun, neuter 
pronoun or others, are explored to enhance 
the discriminative power of both the semantic 
role features and the relative pronominal 
ranking feature, considering the predominant 
importance of pronouns in tracking the local 
focus structure in discourse.  
4.3 Comparison with Previous Work 
As a representative in explicitly employing se-
mantic role labeling in coreference resolution, 
Ponzetto and Strube (2006) explored two seman-
tic role features to capture the predicate-
argument structure information to benefit 
coreference resolution: I_SEMROLE, the predi-
cate-argument pairs of one referring expression, 
and J_SEMROLE, the predicate-argument pairs 
of another referring expression. Their experi-
ments on the ACE 2003 corpus shows that, 
while the two semantic role features much im-
prove the performance of common noun resolu-
tion by 3.8 and 2.7 in F-measure on the BNEWS 
and NWIRE domains respectively, they only 
                                                          
1 According to the centering theory, the backward-looking 
center Cb is preferentially realized by a pronoun in the sub-
ject position in natural languages, such as English, and 
people tend to plan ahead and minimize the number of 
focus shifts. 
slightly improve the performance of pronoun 
resolution by 0.4 and 0.3 in F-measure on the 
BNEWS and NWIRE domains respectively.  
In comparison, this paper proposes various 
kinds of centering-motivated semantic role fea-
tures in attempt to better model the continuity or 
shift in the local discourse focus by extending 
the centering theory from the grammatical level 
to the semantic level. For example, the 
CAARG0MainVerb feature (as shown in Table 
5) is designed to capture the semantic role of the 
antecedent candidate in the main predicate in 
modeling the discourse center, while, the AN-
PronounRanking feature (as shown in Table 5) is 
designed to determinate the relative priority of 
the pronominal anaphor in retaining the dis-
course center.  
Although both this paper and Ponzetto and 
Strube (2006) employs semantic role features, 
their ways of deriving such features are much 
different due to different driving 
forces/motivations behind. As a result, their con-
tributions on coreference resolution are different: 
while the semantic role features in Ponzette and 
Strube (2006) captures the predicate-argument 
structure information and contributes much to 
common noun resolution and their contribution 
on pronoun resolution can be ignored, the cen-
tering-motivated semantic role features in this 
paper contribute much in pronoun resolution. 
This justifies our attempt to better model the 
continuity or shift of the discourse focus in pro-
noun resolution by extending the centering the-
ory from the grammatical level to the semantic 
level and employing the centering-motivated 
features in pronoun resolution.. 
5 Experimentation and Discussion 
We have evaluated our approach of employing 
the centering theory in pronoun resolution from 
the semantic perspective on the ACE 2003 cor-
pus. 
5.1 Experimental Setting 
The ACE 2003 corpus contains three domains: 
newswire (NWIRE), newspaper (NPAPER), and 
broadcast news (BNEWS). For each domain, 
there exist two data sets, training and devtest, 
which are used for training and testing respec-
tively. Table 4 lists the pronoun distributions 
with coreferential relationships in the training 
data and the test data over pronominal subcate-
gories and sentence distances. Table 4(a) shows 
that third person pronouns occupy most and neu-
991
 tral pronouns occupy second while Table 4(b) 
shows that the antecedents of most pronouns 
occur within the current sentence and the previ-
ous sentence, with a little exception in the test 
data set of BNEWS.  
NWIRE NPAPER BNEWSPronoun  
Subcategory Train Test Train Test Train Test
First Person 263 103 283 120 455 258
Second Person 61 16 29 36 203 68
Third Person 618 179 919 263 736 158
Neuter 395 151 577 190 482 137
Reflexive 23 6 42 12 26 6
Other 0 0 2 0 2 3
(a) Distribution over pronominal subcategories 
NWIRE NPAPER BNEWSDistance 
Train Test Train Test Train Test
0 890 254 1281 347 1149 295
1 447 149 529 197 729 188
2 0 27 0 24 0 41
>2 0 19 0 41 0 100
Total 1337 449 1810 609 1878 624
(b) Distribution over sentence distances 
Table 4: Pronoun statistics on the ACE 2003 corpus 
For preparation, all the documents in the cor-
pus are preprocessed automatically using a pipe-
line of NLP components, including tokenization 
and sentence segmentation, named entity recog-
nition, part-of-speech tagging and noun phrase 
chunking. Among them, named entity recogni-
tion, part-of-speech tagging and noun phrase 
chunking apply the same Hidden Markov Model 
(HMM)-based engine with error-driven learning 
capability (Zhou and Su, 2000 & 2002). In par-
ticular for SRL, we use a state-of-the-art in-
house toolkit, which achieved the precision of 
87.07% for ARG0 identification and the preci-
sion of 78.97% for ARG1 identification, for easy 
integration. In addition, we use the SVM-light2 
toolkit with the radial basis kernel and default 
learning parameters. Finally, we report the per-
formance in terms of recall, precision, and F-
measure, where precision measures the percent-
age of correctly-resolved pronouns (i.e. correctly 
linked with any referring expression in the 
coreferential chain), recall measures the cover-
age of correctly-resolved pronouns, and F-
measure gives an overall figure on equal har-
mony between precision and recall. To see 
whether an improvement is significant, we also 
conduct significance testing using paired t-test. 
In this paper, ?>>>?, ?>>? and ?>? denote p-
values of an improvement smaller than 0.01, in-
between (0.01, 0,05] and bigger than 0.05, 
                                                          
2 http://svmlight.joachims.org/ 
which mean significantly better, moderately 
better and slightly better, respectively. 
5.2 Experimental Results 
Table 5 details various centering-motivated fea-
tures from the semantic perspective, which are 
incorporated in our final system. For example, 
the CAARG0MainVerb feature is designed to 
capture the semantic role of the antecedent can-
didate in the main predicate in modeling the dis-
course center, while the ANPronounRanking 
feature is designed to determinate the relative 
priority of the pronominal anaphor in retaining 
the discourse center. As the baseline, we dupli-
cated the representative system with the same set 
of 12 basic features, as described in Soon et al
(2001). Table 6 shows that our baseline system 
achieves the state-of-the-art performance of 62.3, 
65.3 and 59.0 in F-measure on the NWIRE, 
NPAPER and BNEWS domains, respectively. It 
also shows that the centering-motivated features 
(from the semantic perspective) significantly 
improve the F-measure by 3.6(>>>), 4.5(>>>) 
and 7.7(>>>) on the NWIRE, NPAPER and 
BNEWS domains, respectively. This justifies 
our attempt to model the continuity or shift of 
the discourse focus in pronoun resolution via 
centering-motivated features from the semantic 
perspective. For comparison, we also evaluate 
the performance of our final system from the 
grammatical perspective. This is done by replac-
ing semantic roles with grammatical roles in 
deriving centering-motivated features. Here, la-
beling of grammatical roles is achieved using a 
state-of-the-art toolkit, as described in Buchholz 
(1999). Table 6 shows that properly employing 
the centering theory in pronoun resolution from 
the grammatical perspective can also improve 
the performance. However, the performance im-
provement of employing the centering theory 
from the grammatical perspective is much lower, 
compared with that from the semantic perspec-
tive. This validates our attempt of employing the 
centering theory in pronoun resolution from the 
semantic perspective instead of from the gram-
matical perspective. This also suggests the great 
potential of applying the centering theory in 
pronoun resolution since the centering theory is 
a local coherence theory, which tells how subse-
quent utterances in a text link together.  
Table 7 shows the contribution of the seman-
tic role features and the relative pronominal 
ranking feature in pronoun resolution when the 
detailed pronominal subcategory features are 
included: 
992
  
Feature category Feature Remarks 
CAARG0 1 if the semantic role of the antecedent candidate is ARG0/agent; else 0 
CAARG0MainVerb 1 if the antecedent candidate has the semantic role of ARG0/agent for the main predicate of the sentence; else 0 
Semantic Role-based  Fea-
tures 
ANCASameTarget 1 if the anaphor and the antecedent candidate share the same predicate with regard to their semantic roles; else 0 
Relative Pronominal Rank-
ing Feature ANPronounRanking
Whether the pronominal anaphor is ranked highest among all 
the pronouns in the sentence 
ANPronounType Whether the anaphor is a first person, second person, third person, neuter pronoun or others Detailed Pronominal Sub-
category Features 
CAPronounType Whether the antecedent candidate is a first person, second person, third person, neuter pronoun or others 
Table 5: Centering-motivated features incorporated in our final system  
(with AN indicating the anaphor and CA indicating the antecedent candidate) 
NWIRE NPAPER BNEWS System Variation 
R% P% F R% P% F R% P% F 
Baseline System 57.0 68.6 62.3 61.1 70.1 65.3 49.0 73.9 59.0
Final System 
(from the semantic perspective) 
64.1 67.8 65.9 67.5 72.4 69.8 59.9 75.3 66.7
Final System  
(from the grammatical perspective, for comparison)
63.3 64 63.6 64.7 68.8 66.7 57.1 70.1 63.1
Table 6: Contributions of centering-motivated features in pronoun resolution 
NWIRE NPAPER BNEWS System Variation 
R% P% F R% P% F R% P% F 
Baseline System 57.0 68.6 62.3 61.1 70.1 65.3 49.0 73.9 59.0 
+SR and DC 64.8 67.8 66.3 67.2 72.9 69.9 59.1 75.3 66.3 
+PR and DC 61.5 65.4 63.4 64.9 72.1 68.3 57.4 73.5 64.5 
+SR, PR and DC (Final System) 64.1 67.8 65.9 67.5 72.4 69.8 59.9 75.3 66.7 
Table 7: Contribution of the semantic role features (SR) and the relative pronominal ranking feature (PR) in pro-
noun resolution when the detailed pronominal subcategory features are included 
1) The inclusion of the semantic role features 
improve the performance by 4.0(>>>), 
4.6(>>>) and 7.3(>>>) in F-measure on the 
NWIRE, NPAPER and BNEWS domains, re-
spectively. This suggests the impact of se-
mantic role information in determining the 
local discourse focus.  Since pronouns prefer-
entially occur in the subject position and tend 
to refer to the main subject (Ehrlich 1980; 
Brennan 1995; Walker et al 1998; Cahn 
1995; Gordon and Searce 1995; Kibble et al 
2001), this paper only applies semantic fea-
tures related with the semantic role of 
ARG0/agent, which is closely related with 
the grammatical role of subject, with regard 
to various predicates in a sentence. We have 
also explored features related with other se-
mantic roles. However, our preliminary ex-
perimentation shows that they do not improve 
the performance, even for ARG1/patient, and 
thus are not included in the final system. This 
may be due to that other semantic roles are 
not discriminative enough to make a differ-
ence in deciding the local discourse structure. 
2) It is surprising to notice that further inclusion 
of the relative pronominal ranking feature has 
only slight impact (slight positive impact on 
the BNEWS domain and slight negative im-
pact on the NWIRE and NPAPER domains) 
on the ACE 2003 corpus. This suggests that 
most of information in the relative pronomi-
nal ranking feature has been covered by the 
semantic role features. This is not surprising 
since the semantic role of ARG0/agent, 
which is explored to derive the semantic role 
features, is also applied to decide the relative 
pronominal ranking feature.  
The inclusion of the relative pronominal 
ranking feature improve the performance by 
1.1(>>>), 3.0(>>>) and 5.5(>>>) in F-measure. 
Our further evaluation reveals that the perform-
ance improvement difference among different 
domains of the ACE 2003 corpus is due to the 
distribution of pronouns? antecedents occurring 
over different sentence distances, as shown in 
993
 Table 4. This suggests the usefulness of the rela-
tive pronominal ranking feature in resolving 
pronominal anaphors over longer distance. This 
is consistent with our observation that, as the 
percentage of pronominal anaphors referring to 
more distant antecedents increase, its impact 
turns gradually from negative to positive, when 
further including the relative pronominal ranking 
feature after the semantic role features. The rea-
son that we include the detailed pronominal sub-
category information is due to predominant 
importance of pronouns in tracking the local 
focus structure in discourse and that such de-
tailed pronominal subcategory information is 
discriminative in tracking different subcatego-
ries of pronouns. This suggests the usefulness of 
considering the distribution of the local dis-
course focus over detailed pronominal subcate-
gories. One interesting finding in our 
preliminary experimentation is that the inclusion 
of the detailed pronominal subcategory features 
alone even harms the performance. This may be 
due to the reason that the detailed pronominal 
subcategory features do not have the discrimina-
tive power themselves and that the semantic role 
features and the relative pronominal ranking fea-
ture provide an effective mechanism to explore 
the role of such detailed pronominal subcategory 
features in helping determine the local discourse 
focus. 
 Pronoun  
Subcategory 
NWIRE NPAPER BNEWS
First Person 55.7 55.9 56.6 
Second Person 54.6 60.4 44.0 
Third Person 72.6 80.9 75.7 
Neuter 41.5 50.4 50.2 
Reflexive 85.7 70.0 60.0 
B
as
el
in
e 
Sy
st
em
 
Total 62.3 65.3 59.0 
First Person 64.7 67.0 65.6 
Second Person 78.6 70.0 51.9 
Third Person 80.9 81.8 80.4 
Neuter 48.3 53.0 58.3 
Reflexive 71.4 66.7 80.0 Fi
na
l S
ys
te
m
 
Total 65.9 69.8 66.7 
Table 8: Performance comparison of pronoun resolu-
tion in F-measure over pronoun subcategories 
Table 8 shows the contribution of the center-
ing-motivated features over different pronoun 
subcategories. It shows that the centering-
motivated features contribute much to the reso-
lution of the four major pronoun subcategories 
(i.e. first person, second person, third person and 
neuter) while its negative impact on the minor 
pronoun subcategories (e.g. reflexive) can be 
ignored due to their much less frequent occur-
rence in the corpus.  In particular, the centering-
motivated features improve the performance on 
the major three pronoun subcategories of third 
person / neuter / first person, by 
8.3(>>>)/6.8(>>>)/9.0(>>>), 0.9(>>)/ 2.6 
(>>>)/11.1(>>>) and 4.7(>>>)/8.1(>>>)/9.0 
(>>>), on the NWIRE, NPAPER and BNEWS 
domains of the ACE 2003 corpus, respectively. 
 Distance NWIRE NPAPER BNEWS
<=0 61.6 64.5 68.7 
<=1 60.4 67.5 60.0 
<=2 62.9 67.4 63.7 
B
as
el
in
e 
Sy
st
em
 
Total 62.3 65.3 59.0 
<=0 64.3 70.3 78.7 
<=1 66.8 72.3 72.5 
<=2 66.6 71.8 71.8 
Fi
na
l S
ys
-
te
m
 
Total 65.9 69.8 66.7 
Table 9: Performance comparison of pronoun resolu-
tion in F-measure over sentence distances 
Table 9 shows the contribution of the center-
ing-motivated features over different sentence 
distances. It shows that the centering-motivated 
features improve the performance of pronoun 
resolution on different sentence distances of 
0/1/2, by 2.7(>>>) / 5.8(>>>) / 10.0 (>>>), 
6.4(>>>) / 4.8(>>>) / 12.5(>>>) and 3.7 
(>>>)/4.4(>>>)/8.1(>>>), on the NWIRE, 
NPAPER and BNEWS domains of the ACE 
2003 corpus, respectively. This suggests that the 
centering-motivated features are helpful for both 
intra-sentential and inter-sentential pronoun 
resolution. 
6 Conclusion and Further Work 
This paper extends the centering theory from the 
grammatical level to the semantic level and 
much improves the performance of pronoun 
resolution via centering-motivated features from 
the semantic perspective. This is mainly realized 
by employing various semantic role features 
with regard to various predicates in a sentence, 
in attempt to model the continuity or shift of the 
local discourse focus. Moreover, the relative 
ranking feature of a pronoun among all the pro-
nouns is explored to help determine the relative 
priority of the pronominal anaphor in retaining 
the local discourse focus. Evaluation on the 
ACE 2003 corpus shows that both the centering-
motivated semantic role features and pronominal 
ranking feature much improve the performance 
of pronoun resolution, especially when the de-
tailed pronominal subcategory features of both 
the anaphor and the antecedent candidate are 
included. It is not surprising due to the predomi-
994
 nance of pronouns in tracking the local discourse 
structure in a text.   
To our best knowledge, this paper is the first 
research which successfully applies the center-
ing-motivated features in pronoun resolution 
from the semantic perspective. 
For future work, we will explore more kinds 
of semantic information and structured syntactic 
information in pronoun resolution. In particular, 
we will further employ the centering theory in 
pronoun resolution from both grammatical and 
semantic perspectives on more corpora. 
Acknowledgement  
This research is supported by Project 60673041 
under the National Natural Science Foundation 
of  China, project 2006AA01Z147 under the  
?863? National High-Tech Research and Devel-
opment of China, project 200802850006 under 
the National Research Foundation for the Doc-
toral Program of Higher Education of China, 
project 08KJD520010 under the Natural Science 
Foundation of the Jiangsu Higher Education In-
stitutions of China. 
References 
C. Aone and W.W. Bennett. 1995. Evaluating auto-
mated and manual acquisition of anaphora resolu-
tion strategies. ACL?1995:122-129. 
S.E. Brennan, M.W. Friedman, and C.J. Pollard. 1987. 
A centering approach to pronoun. ACL?1987: 290-
297. 
S.E. Brennan 1995. Centering attention in discourse. 
Language and Cognitive Process, 10/2: 137-67. 
S. Buchholz, J. Veenstra and W. Daelemans. 1999. 
Cascaded Grammatical Relation Assignment. 
EMNLP-VLC?1999: 239-246 
S. Cote. 1983. Ranking and forward-looking centers. 
In Proceedings of the Workshop on the Centering 
Theory in Naturally-Occurring Discourse. 1993. 
D.M. Carter. 1987. Interpreting Anaphors in Natural 
Language Texts. Ellis Horwood, Chichester, UK. 
J. Carbonell and R. Brown. 1988. Anaphora resolu-
tion: a multi-strategy approach. COLING?1988: 
96-101. 
B.J. Grosz, A.K. JoShi and S. Weinstein. 1995. Cen-
tering: a framework for modeling the local coher-
ence of discourse. Computational Linguistics, 
21(2):203-225. 
P.C. Gordon, B.J. Grosz and L.A. Gilliom. 1993. 
Pronouns, names and the centering of attention in 
discourse. Cognitive Science.1993.17(3):311-348 
P.C. Gordon and K. A. Searce. 1995. Pronominaliza-
tion and discourse coherence, discourse structure 
and pronoun interpretation. Memory and Cogni-
tion. 1995. 
S. Harabagiu and S. Maiorano. 2000. Multiligual 
coreference resolution. ANLP-NAACL 2000:142-
149. 
A.K. Joshi and S. Weinstein. 1981. Control of infer-
ence: Role of some aspects of discourse structure-
centering. IJCAI?1981:385-387 
R. Kibble. 2001. A Reformulation of Rule 2 of Cen-
tering. Computational Linguistics, 2001,27(4): 
579-587 
M. Kameyama. 1986. Aproperty-sharing constraint in 
centering. ACL 1986:200-206 
M. Kameyama. 1988. Japanese zero pronominal 
binding, where syntax and discourse meet. In Pro-
ceeding of the Second International Workshop on 
Japanese Syntax. 1988.  
R. Mitkov. 1998. Robust pronoun resolution with 
limited knowledge. COLING-ACL?1998:869-875.  
A. Moschitti and S. Quarteroni. 2008. Kernels on 
linguistics structures for answer entraction. 
ACL?08:113-116 
V. Ng and C. Cardie. 2002. Improving machine 
learning approaches to coreference resolution. 
ACL?2002: 104-111 
V. Ng. 2007. Semantic Class Induction and Corefer-
ence Resolution.  ACL?2007 536-543. 
M. Palmer, D. Gildea and P. Kingsbury. 2005. The 
proposition bank: A corpus annotated with seman-
tic roles. Computational Linguistics, 31(1):71-106. 
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J.H. 
Martin, and D. Jurafsky. 2005. Support vector 
learning for semantic argument classification. Ma-
chine Learning, 2005.60:11-39.  
S. P. Ponzetto and M. Strube. 2006. Semantic Role 
Labeling for Coreference Resolution. 
EMNLP?2006 143-146. 
E. F. Prince and M. A. Walker. 1995. A bilateral ap-
porach to givenness: a hearer-status algorithm and 
a centering algorithm. In Proceedings of 4th Inter-
national Pragmatics Conference. 
E. Rich and S. LuperFoy. 1988. An architecture for 
anaphora resolution. In Proceedings of the 2nd 
Conference on Applied Natural Language Proc-
essing. ANLP?1988: 18-24. 
W.M. Soon, H.T. Ng and D. Lim. 2001. A machine 
learning approach to coreference resolution of 
noun phrase. Computational Linguistics, 2001, 
27(4):521-544. 
D. Shen and M. Lapata. 2007. Using semantic roles 
to improve question answering. EMNLP-CoNIL 
2007:12-21 
C. Sidner. 1979. Toward a computation of intrasen-
tential coreference. Technical Report TR-537,MIT. 
Artificial Intelligence Laboratory. 
C. Sidner. 1981. Focusing for interpretation of pro-
nouns. Computational Linguistics,1981.7:217-231 
J. Tetreault. 1999. Analysis of syntax-based pronoun 
resolution methods. ACL 1999:602-605 
J. Tetreault. 2001. A corpus-based evaluation of cen-
tering and pronoun resolution. Computational Lin-
guistics. 2001. 27(4):507-520. 
995
 M. Walker, A. K. Joshi and E. Prince. 1998. Center-
ing in naturally occurring discourse: An overview. 
Clarendon Press:1-28 
X.F. Yang?J. Su?G.D. Zhou and C.L. Tan. 2004. 
Improving pronoun resolution by incorporating 
coreferential information of candidates. 
ACL?2004:127-134. 
X.F. Yang? J. Su and C.L. Tan. 2005. Improving 
Pronoun Resolution Using Statistics - Based Se-
mantic Compatibility Information. ACL?2005:165 
-172. 
X.F. Yang and J. Su. 2007. Coreference Resolution 
Using Semantic Relatedness Information from 
Automatically Discovered Patterns. ACL?2007: 
528-535. 
G.D. Zhou and J. Su. 2004. A high- performance 
coreference resolution system using a multi- agent 
strategy. COLING? 2004:522- 528. 
Y. Ziv and B.J. Grosz. 1994. Right dislocation and 
attentional state. Israel Association of Theoretical 
Linguistics Meetings?1994. 184-199. 
996
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1280?1288,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
Improving Nominal SRL in Chinese Language with Verbal SRL In-
formation and Automatic Predicate Recognition 
 
Junhui Li?   Guodong Zhou??   Hai Zhao??  Qiaoming Zhu?  Peide Qian? 
? Jiangsu Provincial Key Lab for Computer Information Processing Technologies
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
? Department of Chinese, Translation and Linguistics 
City University of HongKong, China 
Email: {lijunhui,gdzhou,hzhao,qmzhu,pdqian}@suda.edu.cn
 
                                                          
? Corresponding author 
Abstract 
This paper explores Chinese semantic role la-
beling (SRL) for nominal predicates. Besides 
those widely used features in verbal SRL, 
various nominal SRL-specific features are 
first included. Then, we improve the perform-
ance of nominal SRL by integrating useful 
features derived from a state-of-the-art verbal 
SRL system. Finally, we address the issue of 
automatic predicate recognition, which is es-
sential for a nominal SRL system. Evaluation 
on Chinese NomBank shows that our research 
in integrating various features derived from 
verbal SRL significantly improves the per-
formance. It also shows that our nominal SRL 
system much outperforms the state-of-the-art 
ones. 
1. Introduction 
Semantic parsing maps a natural language sen-
tence into a formal representation of its meaning. 
Due to the difficulty in deep semantic parsing, 
most of previous work focuses on shallow se-
mantic parsing, which assigns a simple structure 
(such as WHO did WHAT to WHOM, WHEN, 
WHERE, WHY, HOW) to each predicate in a 
sentence. In particular, the well-defined seman-
tic role labeling (SRL) task has been drawing 
more and more attention in recent years due to 
its importance in deep NLP applications, such as 
question answering (Narayanan and Harabagiu, 
2004), information extraction (Surdeanu et al, 
2003), and co-reference resolution (Ponzetto and 
Strube, 2006). Given a sentence and a predicate 
(either a verb or a noun) in it, SRL recognizes 
and maps all the constituents in the sentence into 
their corresponding semantic arguments (roles) 
of the predicate. According to the predicate 
types, SRL could be divided into SRL for verbal 
predicates (verbal SRL, in short) and SRL for 
nominal predicates (nominal SRL, in short). 
During the past few years, verbal SRL has 
dominated the research on SRL with the avail-
ability of FrameNet (Baker et al, 1998), Prop-
Bank (Palmer et al, 2005), and the consecutive 
CoNLL shared tasks (Carreras and M?rquez, 
2004 & 2005) in English language. As a com-
plement to PropBank on verbal predicates, 
NomBank (Meyers et al, 2004) annotates nomi-
nal predicates and their corresponding semantic 
roles using similar semantic framework as 
PropBank. As a representative, Jiang and Ng 
(2006) pioneered the exploration of various 
nominal SRL-specific features besides the tradi-
tional verbal SRL-related features on NomBank. 
They achieved the performance of 72.73 and 
69.14 in F1-measure on golden and automatic 
syntactic parse trees, respectively, given golden 
nominal predicates. 
For SRL in Chinese, Sun and Jurafsky (2004) 
and Pradhan et al (2004) pioneered the research 
on Chinese verbal and nominal SRLs, respec-
tively, on small private datasets. Taking the ad-
vantage of recent release of Chinese PropBank 
(Xue and Palmer, 2003) and Chinese NomBank 
(Xue, 2006a), Xue and his colleagues (Xue and 
Palmer 2005; Xue 2006b; Xue, 2008) pioneered 
the exploration of Chinese verbal and nominal 
SRLs, given golden predicates. Among them, 
Xue and Palmer (2005) studied Chinese verbal 
SRL using Chinese PropBank and achieved the 
performance of 91.3 and 61.3 in F1-measure on 
golden and automatic syntactic parse trees, re-
spectively. Xue (2006b) extended their study on 
Chinese nominal SRL and attempted to improve 
the performance of nominal SRL by simply in-
1280
 cluding the Chinese PropBank training instances 
into the training data for nominal SRL on Chi-
nese NomBank. However, such integration was 
empirically proven unsuccessful due to the dif-
ferent nature of certain features for verbal and 
nominal SRLs. Xue (2008) further improved the 
performance on both verbal and nominal SRLs 
with a better syntactic parser and new features. 
Ding and Chang (2008) focused on argument 
classification for Chinese verbal predicates with 
hierarchical feature selection strategy. They 
achieved the classification precision of 94.68% 
on golden parse trees on Chinese PropBank. 
This paper focuses on Chinese nominal SRL. 
This is done by adopting a traditional verbal 
SRL architecture to handle Chinese nominal 
predicates with additional nominal SRL-specific 
features. Moreover, we significantly enhance the 
performance of nominal SRL by properly inte-
grating various features derived from verbal 
SRL. Finally, this paper investigates the effect of 
automatic nominal predicate recognition on the 
performance of Chinese nominal SRL. Although 
previous research (e.g. CoNLL?2008) in English 
nominal SRL reveals the importance of auto-
matic predicate recognition, there has no re-
ported research on automatic predicate 
recognition in Chinese nominal SRL. 
The rest of this paper is organized as follows: 
Section 2 introduces Chinese NomBank while 
the baseline nominal SRL system is described in 
Section 3 with traditional and nominal SRL-
specific features. Then, the baseline nominal 
SRL system is improved by integrating useful 
features derived from verbal SRL (Section 4) 
and extended with automatic recognition of 
nominal predicates (Section 5). Section 6 gives 
experimental results and discussion. Finally, 
Section 7 concludes the paper.    
2. Chinese NomBank 
Chinese NomBank (Xue, 2006a) adopts similar 
semantic framework as NomBank, and focuses 
on Chinese nominal predicates with their argu-
ments in Chinese TreeBank. The semantic ar-
guments include:  
1) Core arguments: Arg0 to Arg5. Generally, 
Arg0 and Arg1 denotes the agent and the 
patient, respectively, while arguments from 
Arg2 to Arg5 are predicate-specific.  
2) Adjunct arguments, which are universal to 
all predicates, e.g. ArgM-LOC for locative, 
and ArgM-TMP for temporal. 
 
All the arguments are annotated on parse tree 
nodes with their boundaries aligning with the 
spans of tree nodes. Figure 1 gives an example 
with two nominal predicates and their respective 
arguments, while the nominal predicate ???
/investment? has two core arguments, ?NN(??
/foreign businessman)? as Arg0 and ?NN(??
/bank)? as Arg1, and the other nominal predicate 
??? /loan? also has two core arguments, 
?NP(???? /Bank of China)? as Arg1 and 
Figure 1: Two nominal predicates and their arguments in the style of NomBank. 
? 
?? ?? ??
??
???
P 
NN NN NN
VV
NN NN 
Arg0/Rel1 Rel1 Arg1/Rel1
NP 
PP 
Arg0/Rel2 
ArgM-MNR/Rel2 Rel2 
NP 
CD
QP
NP
VP
VP
??? ?? 
? 
NN NN 
PU 
NP 
Arg1/Rel2 
IP
?? ?? 
Sup/Rel2
Bank of China 
to 
Foreign  Investment  Bank 
provide
4 billion
RMB loan 
. 
Bank of China provides 4 billion RMB loan to Foreign Investment Bank. 
1281
 ?PP(??????? /to Foreign Investment 
Bank)? as Arg0,  and 1 adjunct argument, 
?NN(???/RMB)? as ArgM-MNR, denoting 
the manner of loan. It is worth noticing that 
there is a (Chinese) NomBank-specific label in 
Figure 1, Sup (support verb) (Xue, 2006a), in 
helping introduce the arguments, which occur 
outside the nominal predicate-headed noun 
phrase. This is illustrated by the nominal predi-
cate ???/loan?, whose Arg0 and Arg1 are both 
realized outside the nominal predicate-headed 
noun phrase, NP(????????/4 billion 
RMB loan). Normally, a verb is marked as a 
support verb only when it shares some argu-
ments with the nominal predicate. 
3. Baseline: Chinese Nominal SRL 
Popular SRL systems usually formulate SRL as 
a classification problem, which annotates each 
constituent in a parse tree with a semantic role 
label or with the non-argument label NULL. Be-
sides, we divide the system into three consecu-
tive phases so as to overcome the imbalance 
between the training instances of the NULL 
class and those of any other argument classes.  
Argument pruning. Here, several heuristic 
rules are adopted to filter out constituents, which 
are most likely non-arguments. According to the 
argument structures of nominal predicates, we 
categorize arguments into two types: arguments 
inside NP (called inside arguments) and argu-
ments introduced via a support verb (called out-
side arguments), and handle them separately. 
For the inside arguments, the following three 
heuristic rules are applied to find inside argu-
ment candidates: 
z All the sisters of the predicate are candi-
dates. 
z If a CP or DNP node is a candidate, its chil-
dren are candidates too. 
z For any node X, if its parent is an ancestral 
node of the predicate, and the internal 
nodes along the path between X and the 
predicate are all NPs, then X is a candidate. 
For outside arguments, we look for the sup-
port verb of the focus nominal predicate, and 
then adopt the rules as proposed in Xue and 
Palmer (2005) to find the candidates for the sup-
port verb, since outside argument candidates are 
introduced via this support verb. That to say, the 
argument candidates of the support verb are re-
garded as outside argument candidates of the 
nominal predicate. However, as support verbs 
are not annotated explicitly in the testing phase, 
we identify intervening verbs as alternatives to 
support verbs in both training and testing phases 
with the path between the nominal predicate and 
intervening verb in the form of 
?VV<VP>[NP>]+NN?, where ?[NP>]+? denotes 
one or more NPs.  Our statistics on Chinese 
NomBank shows that 51.96% of nominal predi-
cates have no intervening verb while 48.04% of 
nominal predicates have only one intervening 
verb. 
Taken the nominal predicate ???/loan? in 
Figure 1 as an example, NN(???/RMB) and 
QP(??? /4 billion) are identified as inside 
argument candidates, while PP(??????
?/to Foreign Investment Bank) and NP(???
?/Bank of China) are identified as outside ar-
gument candidates via the support verb VV(?
?/provide). 
Argument identification. A binary classifier 
is applied to determine the candidates as either 
valid arguments or non-arguments. It is worth 
pointing out that we only mark those candidates 
that are most likely to be NULL (with probabil-
ity > 0.90) as non-arguments. Our empirical 
study shows that this little trick much benefits 
nominal SRL, since argument identification for 
nominal predicates is much more difficult than 
that for verbal predicates and thus many argu-
ments would have been falsely marked as non-
arguments if the threshold is set as 0.5. 
Argument classification. A multi-class classi-
fier is employed to label identified arguments 
with specific argument labels (including the 
NULL class for non-argument). 
In the following, we first adapt some tradi-
tional features, which have been proven effec-
tive in verbal SRL, to nominal SRL, and then 
introduce several nominal SRL-specific features. 
3.1. Traditional Features 
Using the feature naming convention as adopted 
in Jiang and Ng (2006), Table 1 lists the tradi-
tional features, where ?I? and ?C? indicate the 
features for argument identification and classifi-
cation, respectively. Among them, the predicate 
class (b2) feature was first introduced in Xue 
and Palmer (2005) to overcome the imbalance of 
the predicate distribution in that some predicates 
can be only found in the training data while 
some predicates in the testing data are absent 
from the training data. In particular, the verb 
class is classified along three dimensions: the 
number of arguments, the number of framesets 
and selected syntactic alternations. For example, 
1282
 the verb class of ?C1C2a? means that it has two 
framesets, with the first frameset having one 
argument and the second having two arguments. 
The symbol ?a? in the second frameset repre-
sents a type of syntactic alternation. 
 
Feature Remarks: b1-b5(C, I), b6-b7(C) 
b1 Predicate: the nominal predicate itself. (??
/loan) 
b2 Predicate class: the verb class that the predi-
cate belongs to. (C4a) 
b3 Head word (b3H) and its POS (b3P).  (??
/bank, NN) 
b4 Phrase type: the syntactic category of the 
constituent. (NP) 
b5 Path: the path from the constituent to the 
nominal predicate. 
 (NP<IP>VP>VP>NP>NP>NN) 
b6 Position: the positional relationship of the 
constituent with the predicate. ?left? or 
?right?. (left) 
b7 First word (b7F) and last word (b7L) of the 
focus constituent. (??/China, ??/bank) 
Combined features: b11-b14(C, I), b15(C) 
b11: b1&b4;       b12: b1&b3H;       b13: b2&b4;  
b14: b2&b3H;    b15: b5&b6 
Table 1: Traditional features and their instantiations 
for argument identification and classification, with 
NP(????/Bank of China)  as the focus constitu-
ent and NN(??/loan) as the nominal predicate, re-
garding Figure 1. 
3.2. Nominal SRL-specific Features 
To capture more useful information in the predi-
cate-argument structure, we also study addi-
tional features which provide extra information. 
Statistics on Chinese NomBank show that about 
40% of pruned inside candidates are arguments. 
Since inside arguments usually locate near to the 
nominal predicate, its surroundings are expected 
to be helpful in SRL. Table 2 shows the features 
in better capturing the details between inside 
arguments and nominal predicates. Specially, 
features ai6 and ai7 are sister-related features, 
inspired by the features related with the 
neighboring arguments in Jiang and Ng (2006). 
Statistics on NomBank and Chinese Nom-
Bank show that about 20% and 22% of argu-
ments are introduced via a support verb, 
respectively. Since a support verb pivots outside 
arguments and the nominal predicate on its two 
sides, support verbs play an important role in 
labeling these arguments. Here, we also identify 
intervening verbs as alternatives to support verbs 
since support verbs are not explicitly in the test-
ing phase. Table 3 lists the intervening verb-
related features (ao1-ao4, ao11-ao14) employed 
in this paper. 
 
Feature Remarks 
ai1 Whether the focus constituent is adjacent to 
the predicate. Yes or No. (Yes) 
ai2 The headword (ai2H) and pos (ai2P) of the 
predicate?s nearest right sister. (??/bank, 
NN) 
ai3 Whether the predicate has right sisters. Yes 
or No. (Yes) 
ai4 Compressed path of b5: compressing se-
quences of identical labels into one. 
(NN<NP>NN) 
ai5 Whether the predicate has sisters. Yes or 
No. (Yes) 
ai6 For each sister of the focus constituent, 
combine b3H&b4&b5&b6. ( ? ?
/bank&NN & NN<NP>NN&right) 
ai7 Coarse version of ai6, b4&b6. (NN&right) 
Table 2: Additional features and their instantiations  
for inside argument candidates, with ?NN(??
/foreign businessman)? as the focus constituent and 
?NN(?? /investment)? as the nominal predicate, 
regarding Figure1. 
 
Feature Remarks 
ao1 Intervening verb itself. (??/provide) 
ao2 The verb class that the intervening verb 
belongs to. (C3b) 
ao3 The path from the focus constituent to the 
intervening verb. (NP<IP>VP>VP>VV) 
ao4 The compressed path of ao3: compressing 
sequences of identical labels into one. 
(NP<IP>VP>VV) 
Combined features: ao11-ao14 
ao11: ao1&ao3;      ao12: ao1&ao4;    
ao13: ao2&ao3;      ao14: ao2&ao4. 
Table 3: Additional features and their instantiations 
for outside argument candidates, with ?NP(????
/Bank of China)? as the focus constituent and ???
/loan? as the nominal predicate, regarding Figure1. 
Feature selection. Some Features proposed 
above may not be effective in tasks of identifica-
tion and classification. We adopt the greedy fea-
ture selection algorithm as described in Jiang 
and Ng (2006) to pick up positive features em-
pirically and incrementally according to their 
contributions on the development data. The al-
gorithm repeatedly selects one feature each time 
which contributes most, and stops when adding 
any of the remaining features fails to improve 
the performance. As far as the SRL task con-
cerned, the whole feature selection process could 
be done as follows: 1). Feature selection for ar-
gument identification: run the selection algo-
1283
 rithm with the basic set of features (b1-b5, b11-
b14) to pick up effective features from (ai1-ai7, 
ao1-ao4, ao11-ao14); 2). Feature selection for 
argument classification: fix the output returned 
in step1 as the feature set of argument identifica-
tion, and run the selection algorithm with the 
basic set of features (b1-b7, b11-b15) to select 
positive features from (ai1-ai7, ao1-ao4, ao11-
ao14) for argument classification. 
4. Integrating Features derived from 
Verbal SRL 
Since Chinese PropBank and NomBank are an-
notated on the same data set with the same lexi-
cal guidelines (e.g. frame files), it may be 
interesting to investigate the contribution of 
Chinese verbal SRL on the performance of Chi-
nese nominal SRL. In the frame files, argument 
labels are defined with regard to their semantic 
roles to the predicate, either a verbal or nominal 
predicate. For example, in the frame file of 
predicate ???/loan?, the borrower is always 
labeled with Arg0 and the lender labeled with 
Arg1. This can be demonstrated by the follow-
ing two sentences: ???/loan? is annotated as a 
nominal and a verbal predicate in S1 and S2, 
respectively. 
S1 [Arg1 ????/Bank of China] [Arg0 ???
????/to Foreign Investment Bank] ??
/provide [Rel??/loan] 
S2  [Arg0 ????/Bank of China] [Arg1 ???
????/from Foreign Investment Bank] [Rel 
??/loan] 
Therefore, it is straightforward to augment 
nominal training instances with verbal ones. 
However, Xue (2006b) found that simply adding 
the training instances for verbal SRL to the 
training data for nominal SRL and indiscrimi-
nately extracting the same features in both ver-
bal and nominal SRLs hurt the performance. 
This may be due to that certain features (e.g. the 
path feature) are much different for verbal and 
nominal SRLs. This can be illustrated in sen-
tences S1 and S2: the verbal instances in S2 are 
negative for semantic role labeling of the nomi-
nal predicate ???/loan? in S1, since ????
?/Bank of China? takes opposite roles in S1 
and S2. So does ????????/(from/to) 
Foreign Investment Bank?. 
Although several support verb-related features 
(ao1-ao4, ao11-ao14) have been proposed, one 
may still ask how large the role support verbs 
can play in nominal SRL. It is interesting to note 
that outside arguments and the highest NP 
phrase headed by the nominal predicate are also 
annotated as arguments of the support verb in 
Chinese PropBank. For example, Chinese Prop-
Bank marks ?????/Bank of China? as Arg0 
and ?????????/4 billion RMB loan? 
as Arg1 for verb ???/provide? in Figure1. Let 
OA be the outside argument, VV be the support 
verb, and NP be the highest NP phrase headed 
by the nominal predicate NN, then there exists a 
pattern ?OA VV NN? in the sentence, where the 
support verb VV plays a certain role in trans-
ferring roles between OA and NN. For example, 
if OA is the agent of VV, then OA is also the 
agent of phrase VP(VV NN). Like the example 
in Figure1, supposing a NP is the agent of sup-
port verb ???/provide? as well as VP phrase 
(??????????? /provide 4 billion 
RMB loan?), we can infer that the NP is the 
lender of the nominal predicate ???/loan? in-
dependently on any other information, such as 
the NP content and the path from the NP to the 
nominal predicate ???/loan?.  
Let C be the focus constituent, V be the inter-
vening verb, and NP be the highest NP headed 
by the nominal predicate. Table 4 shows the fea-
tures (ao5-ao8, p1-p7) derived from verbal SRL. 
In this paper, we develop a state-of-the-art Chi-
nese verbal SRL system, similar to the one as 
shown in Xue (2008), to achieve the goal. Based 
on golden parse trees on Chinese PropBank, our 
Chinese verbal SRL system achieves the per-
formance of 92.38 in F1-measure, comparable to 
Xue (2008) which achieved the performance of 
92.0 in F1-measure. 
 
Feature Remarks 
ao5 Whether C is an argument for V. Yes or No
ao6 The semantic role of C for V. 
ao7 Whether NP is an argument for V. Yes or No
ao8 The semantic role of NP for V. 
Combined features: p1-p7 
p1: ao1&ao5;         p2: ao1&ao6;    p3: ao1&ao5&b1; 
p4: ao1&ao6&b1;  p5: ao1&apo7;  p6: ao1&ao8;  
p7: ao5&ao7. 
Table 4: Features derived from verbal SRL. 
5. Automatic Predicate Recognition 
Unlike Chinese PropBank where almost all the 
verbs are annotated as predicates, Chinese Nom-
Bank only marks those nouns having arguments 
as predicates. Statistics on Chinese NomBank 
show that only 17.5% of nouns are marked as 
predicates. It is possible that a noun is a predi-
1284
 cate in some cases but not in others. Previous 
Chinese nominal SRL systems (Xue, 2006b; 
Xue, 2008) assume that nominal predicates have 
already been manually annotated and thus are 
available. To our best knowledge, there is no 
report on addressing automatic recognition of 
nominal predicates on Chinese nominal SRL. 
Automatic recognition of nominal predicates 
can be cast as a binary classification (e.g., Predi-
cate vs. Non-Predicate) problem. This paper 
employs the convolution tree kernel, as proposed 
in Collins and Duffy (2001), on automatic rec-
ognition of nominal predicates. 
Given the convolution tree kernel, the key 
problem is how to extract a parse tree structure 
from the parse tree for a nominal predicate can-
didate. In this paper, the parse tree structure is 
constructed as follows: 1) starting from the 
predicate candidate?s POS node, collect all of its 
sister nodes (with their headwords); 2). recur-
sively move one level up and collect all of its 
sister nodes (with their headwords) till reaching 
a non-NP node. Specially, in order to explicitly 
mark the positional relation between a node and 
the predicate candidate, all nodes on the left side 
of the candidate are augmented with tags 1 and 2 
for nodes on the right side. Figure 2 shows an 
example of the parse tree structure with regard 
to the predicate candidate ???/loan? as shown 
in Figure 1. 
In our extra experiments we found global sta-
tistic features (e.g. g1-g5) about the predicate 
candidate are helpful in a feature vector-based 
method for predicate recognition. Figure 2 
makes an attempt to utilize those features in ker-
nel-based method. We have explored other ways 
to include those global features. However, the 
way in Figure 2 works best.  
 
 
Let the predicate candidate be w0, and its left 
and right neighbor words be w-1 and w1, respec-
tively. The five global features are defined as 
follows. 
g1 Whether w0 is ever tagged as a verb in the 
training data? Yes or No. 
g2 Whether w0 is ever annotated as a nominal 
predicate in the training data? Yes or No. 
g3 The most likely label for w0 when it occurs 
together with w-1 and w1. 
g4 The most likely label for w0 when it occurs 
together with w-1. 
g5 The most likely label for w0 when it occurs 
together with w1. 
6. Experiment Results and Discussion 
We have evaluated our Chinese nominal SRL 
system on Chinese NomBank with Chinese 
PropBank 2.0 as its counterpart. 
6.1. Experimental Settings 
This version of Chinese NomBank consists of 
standoff annotations on the files (chtb_001 to 
1151.fid) of Chinese Penn TreeBank 5.1. Fol-
lowing the experimental setting in Xue (2008), 
648 files (chtb_081 to 899.fid) are selected as 
the training data, 72 files (chtb_001 to 040.fid 
and chtb_900 to 931.fid) are held out as the test 
data, and 40 files (chtb_041 to 080.fid) as the 
development data, with 8642, 1124, and 731 
propositions, respectively. 
As Chinese words are not naturally segmented 
in raw sentences, two Chinese automatic parsers 
are constructed: word-based parser (assuming 
golden word segmentation) and character-based 
parser (with automatic word segmentation). 
Here, Berkeley parser (Petrov and Klein, 2007)1 
is chosen as the Chinese automatic parser. With 
regard to character-based parsing, we employ a 
Chinese word segmenter, similar to Ng and Low 
(2004), to obtain the best automatic segmenta-
tion result for a given sentence, which is then 
fed into Berkeley parser for further syntactic 
parsing. Both the word segmenter and Berkeley 
parser are developed with the same training and 
development datasets as our SRL experiments. 
The word segmenter achieves the performance 
of 96.1 in F1-measure while the Berkeley parser 
gives a performance of 82.5 and 85.5 in F1-
measure on golden and automatic word segmen-
tation, respectively2.  
??? 1 In addition, SVMLight with the tree kernel 
function (Moschitti, 2004) 3  is selected as our 
classifier. In order to handle multi-classification 
                                                          
1 Berkeley Parser. http://code.google.com/p/berkeleyparser/ 
2 POSs are not counted in evaluating the performance of 
word-based syntactic parser, but they are counted in evalu-
ating the performance of character-based parser. Therefore 
the F1-measure for the later is higher than that for the for-
mer. 
3 SVM-LIGHT-TK. http://dit.unitn.it/~moschitt/ 
Figure 2: Semantic sub-tree for nominal predicate
RMB 
?? 
loan 
?? 1 
provide 
??? 1 
4 billion 
VV1 
NN1 NN 
NPQP1 
NP
VP 
g1 ?. g5
1285
 problem in argument classification, we apply the 
one vs. others strategy, which builds K classifi-
ers so as to separate one class from all others. 
For argument identification and classification, 
we adopt the linear kernel and the training pa-
rameter C is fine-tuned to 0.220. For automatic 
recognition of nominal predicates, the training 
parameter C and the decay factor ?  in the con-
volution tree kernel are fine-tuned to 2.0 and 0.2, 
respectively. 
6.2. Results with Golden Parse Trees and 
Golden Nominal Predicates 
Effect of nominal SRL-specific features 
 
 Rec.(%) Pre.(%) F1 
traditional features 62.83 73.58 67.78 
+nominal SRL-specific  
features 
69.90 75.11 72.55 
Table 5: The performance of nominal SRL on the 
development data with golden parse trees and golden 
nominal predicates 
After performing the greedy feature selection 
algorithm on the development data, features 
{ao1, ai6, ai2P, ai5, ao2, ao12, ao14}, as pro-
posed in Section 3.2, are selected consecutively 
for argument identification, while features {ai7, 
ao1, ai1, ao2, ai5, ao4} are selected for argument 
classification. Table 5 presents the SRL results 
on the development data. It shows that nominal 
SRL-specific features significantly improve the 
performance from 67.78 to 72.55 ( ) 
in F1-measure. 
05.0;2 <p?
Effect of features derived from verbal SRL 
 
Features Rec.(%) Pre.(%) F1 
baseline 67.86 73.63 70.63  
+ao5 68.15 73.60 70.77 (+0.14)
+ao6 67.66 72.80 70.14 (-0.49)
+ao7 68.20 75.41 71.62 (+0.99)
+ao8 68.30 75.39 71.67 (+1.04)
+p1 67.91 74.40 71.00 (+0.37)
+p2 67.76 74.20 70.83 (+0.20)
+p3 67.96 74.69 71.16 (+0.53)
+p4 68.01 74.18 70.96 (+0.33)
+p5 68.01 75.01 71.39 (+0.76)
+p6 68.20 75.12 71.49 (+0.86)
+p7 68.40 75.70 71.87 (+1.24)
Table 6: Effect of features derived from verbal SRL 
on the performance of nominal SRL on the test data 
with golden parse trees and golden nominal predi-
cates. The first row presents the performance using 
traditional and nominal SRL-specific features. 
 
 
 Rec.(%) Pre.(%) F1 
baseline  67.86 73.63 70.63 
+features derived 
from verbal SRL
68.40 77.51 72.67 
Xue (2008) 66.1 73.4 69.6 
Table 7: The performance of nominal SRL on the test 
data with golden parse trees and golden nominal 
predicates 
 
Table 6 shows the effect of features derived 
from verbal SRL in an incremental way. It 
shows that only the feature ao6 has negative ef-
fect due to its strong relevance with intervening 
verbs and thus not included thereafter. Table 7 
shows the performance on the test data with or 
without using the features derived from the ver-
bal SRL system. It shows these features signifi-
cantly improve the performance ( ) 
on nominal SRL. Table 7 also shows our system 
outperforms Xue (2008) by 3.1 in F1-measure. 
05.0;2 <p?
6.3. Results with Automatic Parse Trees 
and Golden Nominal Predicates 
In previous section we have assumed the avail-
ability of golden parse trees during the testing 
process. Here we conduct experiments on auto-
matic parse trees, using the Berkeley parser. 
Since arguments come from constituents in 
parse trees, those arguments, which do not align 
with any syntactic constituents, are simply dis-
carded. Moreover, for any nominal predicate 
segmented incorrectly by the word segmenter, 
all its arguments are unable to be labeled neither. 
Table 8 presents the SRL performance on the 
test data by using automatic parse trees. It shows 
that the performance drops from 72.67 to 60.87 
in F1-measure when replacing golden parse trees 
with word-based automatic ones, partly due to 
the absence of 6.9% arguments in automatic 
trees, and wrong POS tagging of nominal predi-
cates. Table 8 also compares our system with 
Xue (2008). It shows that our system also out-
performs Xue (2008) on Chinese NomBank. 
 Rec. (%) Pre. (%) F1 
This paper 56.95(53.55) 66.74(66.69) 60.87(59.40)
Xue (2008) 53.1 (52.9) 62.9 (62.3) 57.6 (57.3) 
Table 8: The performance of nominal SRL on the test 
data with automatic parse trees and golden predicates. 
Here, the numbers outside the parentheses indicate 
the performance using a word-based parser, while the 
numbers inside indicate the performance using a 
character-based parser4. 
                                                          
4 About 1.6% nominal predicates are mistakenly segmented 
by the character-based parser, thus their arguments are 
missed directly. 
1286
 6.4. Results with Automatic Nominal Predi-
cates 
So far nominal predicates are assumed to be 
manually annotated and available. Here we turn 
to a more realistic scenario in which both the 
parse tree and nominal predicates are automati-
cally obtained. In the following, we first report 
the results of automatic nominal predicate rec-
ognition and then the results of nominal SRL on 
automatic recognition of nominal predicates. 
Results of nominal predicate recognition 
Parses g1-g5 Rec.(%) Pre.(%) F1 
no 91.46 88.93 90.18 golden 
yes 92.62 89.36 90.96 
word-based yes 86.39 81.80 84.03 
character-based yes 84.79 81.94 83.34 
Table 9: The performance of automatic nominal 
predicate recognition on the test data 
 
Table 9 lists the predicate recognition results, 
using the parse tree structure, as shown in Sec-
tion 5, and the convolution tree kernel, as pro-
posed in Collins and Duffy (2001). The second 
column (g1-g5) indicates whether the global fea-
tures (g1-g5) are included in the parse tree struc-
ture. We have also defined a simple rule that 
treats a noun which is ever a verb or a nominal 
predicate in the training data as a nominal predi-
cate. Based on golden parse trees, the rule re-
ceives the performance of 81.40 in F1-measure. 
This suggests that our method significantly out-
performs the simple rule-based one. Table 9 also 
shows that: 
z As a complement to local structural informa-
tion, global features improve the performance 
of automatic nominal predicate recognition 
by 0.78 in F1-measure. 
z The word-based syntactic parser decreases 
the F1-measure from 90.96 to 84.03, mostly 
due to the POSTagging errors between NN 
and VV, while the character-based syntactic 
parser further drops the F1-measure by 0.69, 
due to automatic word segmentation. 
Results with automatic predicates 
 
Parses Predicates Rec.(%) Pre.(%) F1 
golden 68.40 77.51 72.67 golden 
automatic 65.07 74.65 69.53 
golden 55.95 66.74 60.87 word-
based automatic 52.67 59.56 55.90 
golden 53.55 66.69 59.40 character-
based automatic 50.66 59.60 54.77 
Table 10: The performance of nominal SRL on the 
test data with the choices of golden/automatic parse 
trees and golden/automatic predicates 
In order to have a clear performance comparison 
among nominal SRL on golden/automatic parse 
trees and golden/automatic predicates, Table 10 
lists all the results in those scenarios. 
6.5. Comparison 
Chinese nominal SRL vs. Chinese verbal SRL 
Comparison with Xue (2008) shows that the per-
formance of Chinese nominal SRL is about 20 
lower (e.g. 72.67 vs. 92.38 in F1-measure) than 
that of Chinese verbal SRL, partly due to the 
smaller amount of annotated data (about 1/5) in 
Chinese NomBank than that in Chinese Prop-
Bank. Moreover, according to Chinese Nom-
Bank annotation criteria (Xue 2006a), even 
when a noun is a true deverbal noun, not all of 
its modifiers are legitimate arguments or ad-
juncts of this predicate. Only arguments that can 
co-occur with both the nominal and verbal forms 
of the predicate are considered in the NomBank 
annotation. This means that the judgment of ar-
guments is semantic rather than syntactic. These 
facts may also partly explain the lower nominal 
SRL performance, especially the performance of 
argument identification. This can be illustrated 
by the statistics on the development data that 
96% (40%) of verbal (nominal) predicates? sis-
ters are annotated as arguments. Finally, the 
predicate-argument structure of nominal predi-
cates is more flexible and complicated than that 
of verbal predicates as illustrated in Xue (2006a). 
Chinese nominal SRL vs. English nominal 
SRL 
Liu and Ng (2007) reported the performance of 
77.04 and 72.83 in F1-measure on English Nom-
Bank when golden and automatic parse trees are 
used, respectively. Taking into account that Chi-
nese verbal SRL achieves comparable perform-
ance with English verbal SRL on golden parse 
trees, the performance gap between Chinese and 
English nominal SRL (e.g. 72.67 vs. 77.04 in 
F1-measure) presents great challenge for Chi-
nese nominal SRL. Moreover, while automatic 
parse trees only decrease the performance of 
English nominal SRL by about 4.2 in F1-
measure, automatic parse trees significantly de-
crease the performance of Chinese nominal SRL 
by more than 12 in F1-measure due to the much 
lower performance of Chinese syntactic parsing. 
7. Conclusion 
In this paper we investigate nominal SRL in 
Chinese language. In particular, some nominal 
SRL-specific features are included to improve 
1287
 the performance. Moreover, various features 
derived from verbal SRL are properly integrated 
into nominal SRL. Finally, a convolution tree 
kernel is adopted to address the issue of auto-
matic nominal predicates recognition, which is 
essential in a nominal SRL system.  
To our best knowledge, this is the first re-
search on 
1) Exploring Chinese nominal SRL on auto-
matic parse trees with automatic predicate 
recognition; 
2) Successfully integrating features derived 
from Chinese verbal SRL into Chinese nomi-
nal SRL with much performance improve-
ment. 
Acknowledgement  
This research was supported by Project 
60673041 and 60873150 under the National 
Natural Science Foundation of  China, Project 
2006AA01Z147 under the  ?863? National 
High-Tech Research and Development of China, 
and Project BK2008160 under the Natural Sci-
ence Foundation of the Jiangsu province of 
China. We also want to thank Dr. Nianwen Xue 
for share of the verb class file. We also want to 
thank the reviewers for insightful comments. 
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of COLING-ACL 1998. 
Xavier Carreras and Lluis M?rquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2004.  
Xavier Carreras and Lluis M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2005.  
Michael Collins and Nigel Duffy. 2001. Convolution 
Kernels for Natural Language. In Proceedings of 
NIPS 2001.  
Weiwei Ding and Baobao Chang. 2008. Improving 
Chinese Semantic Role Classification with Hierar-
chical Feature Selection Strategy. In Proceedings 
of EMNLP 2008. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic 
role Labeling of NomBank: a Maximum Entropy 
Approach. In Proceedings of EMNLP 2006.  
Chang Liu and Hwee Tou Ng. 2007. Learning Predic-
tive Structures for Semantic Role Labeling of 
NomBank. In Proceedings of ACL 2007. 
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Yong, and R. Grishman. 2004. Anno-
tating Noun Argument Structure for NomBank. In 
Proceedings of LREC 2004.  
Alessandro Moschitti. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. In Pro-
ceedings of ACL 2004. 
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion Answering based on Semantic Structures. In 
Proceedings of COLING 2004.  
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-speech Tagging: One-at-a-time or All-at-once? 
Word-based or Character-based? In Proceedings 
of EMNLP 2004. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics. 
Slav Petrov. and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceesings of 
NAACL 2007.  
Simone Paolo Ponzetto and Michael Strube. 2006. 
Semantic Role Labeling for Coreference Resolu-
tion. In Proceedings of EACL 2006. 
Sameer Pradhan, Honglin Sun, Wayne Ward, James 
H. Martin, and Dan Jurafsky. 2004. Parsing Ar-
guments of Nominalizations in English and Chi-
nese. In Proceedings of NAACL-HLT 2004.  
Honglin Sun and Daniel Jurafsky. 2004. Shallow 
Semantic Parsing of Chinese. In Proceedings of 
NAACL 2004.  
Mihai Surdeanu, Sanda Harabagiu, John Williams 
and Paul Aarseth. 2003. Using Predicate-argument 
Structures for Information Extraction. In Proceed-
ings of ACL 2003. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of 
Syntactic and Semantic Dependencies. In Pro-
ceedings of CoNLL 2008. 
Nianwen Xue and Martha Palmer. 2003. Annotating 
the Propositions in the Penn Chinese TreeBank. In 
Proceedings of 2nd SIGHAN Workshop on Chinese 
Language Processing.  
Nianwen Xue and Martha Palmer. 2005. Automatic 
Semantic Role Labeling for Chinese verbs. In 
Proceedings of IJCAI 2005.  
Nianwen Xue. 2006a. Annotating the Predicate-
Argument Structure of Chinese Nominalizations. 
In Proceedings of the LREC 2006. 
Nianwen Xue. 2006b. Semantic Role Labeling of 
Nominalized Predicates in Chinese. In Proceed-
ings of HLT-NAACL 2006. 
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
1288
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1437?1445,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Semi-Supervised Learning for Semantic Relation Classification using 
Stratified Sampling Strategy 
 
Longhua Qian   Guodong Zhou   Fang Kong   Qiaoming Zhu 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology, Soochow University 
1 Shizi Street, Suzhou, China 215006 
{qianlonghua,gdzhou,kongfang,qmzhu}@suda.edu.cn 
 
 
 
 
Abstract 
 
This paper presents a new approach to 
selecting the initial seed set using stratified 
sampling strategy in bootstrapping-based 
semi-supervised learning for semantic relation 
classification.  First, the training data is 
partitioned into several strata according to 
relation types/subtypes, then relation instances 
are randomly sampled from each stratum to 
form the initial seed set. We also investigate 
different augmentation strategies in iteratively 
adding reliable instances to the labeled set, and 
find that the bootstrapping procedure may stop 
at a reasonable point to significantly decrease 
the training time without degrading too much 
in performance. Experiments on the ACE 
RDC 2003 and 2004 corpora show the 
stratified sampling strategy contributes more 
than the bootstrapping procedure itself. This 
suggests that a proper sampling strategy is 
critical in semi-supervised learning. 
1 Introduction 
With the dramatic increase in the amount of 
textual information available in digital archives 
and the WWW, there has been growing interest 
in techniques for automatically extracting 
information from text documents. Information 
Extraction (IE) is such a technology that IE 
systems are expected to identify relevant 
information (usually of pre-defined types) from 
text documents in a certain domain and put them 
in a structured format. 
According to the scope of the NIST Automatic 
Content Extraction (ACE) program (ACE, 2000-
2007), current research in IE has three main 
objectives: Entity Detection and Tracking (EDT), 
Relation Detection and Characterization (RDC), 
and Event Detection and Characterization (EDC). 
This paper focuses on the ACE RDC subtask, 
where many machine learning methods have 
been proposed, including supervised methods 
(Miller et al, 2000; Zelenko et al, 2002; Culotta 
and Soresen, 2004; Kambhatla, 2004; Zhou et al, 
2005; Zhang et al, 2006; Qian et al, 2008), 
semi-supervised methods (Brin, 1998; Agichtein 
and Gravano, 2000; Zhang, 2004; Chen et al, 
2006; Zhou et al, 2008), and unsupervised 
methods (Hasegawa et al, 2004; Zhang et al, 
2005).  
Current work on semantic relation extraction 
task mainly uses supervised learning methods, 
since it achieves relatively better performance. 
However this method requires a large amount of 
manually labeled relation instances, which is 
both time-consuming and laborious. In the 
contrast, unsupervised methods do not need 
definitions of relation types and hand-tagged data, 
but it is difficult to evaluate their performance 
since there are no criteria for evaluation. 
Therefore, semi-supervised learning has received 
more and more attention, as it can balance the 
advantages and disadvantages between 
supervised and unsupervised methods. With the 
plenitude of unlabeled natural language data at 
hand, semi-supervised learning can significantly 
reduce the need for labeled data with only 
limited sacrifice in performance. Specifically, a 
bootstrapping algorithm chooses the unlabeled 
instances with the highest probability of being 
correctly labeled and use them to augment 
labeled training data iteratively.  
Although previous work (Yarowsky, 1995; 
Blum and Mitchell, 1998; Abney, 2000; Zhang, 
2004) has tackled the bootstrapping approach 
from both the theoretical and practical point of 
view, many key problems still remain unresolved, 
such as the selection of initial seed set. Since the 
size of the initial seed set is usually small (e.g. 
1437
100 instances), the imbalance of relation types or 
manifold structure (cluster structure) in it will 
severely weaken the strength of bootstrapping. 
Therefore, it is critical for a bootstrapping 
approach to select the most appropriate initial 
seed set. However, current systems (Zhang, 2004; 
Chen et al, 2006) use a randomly sampling 
strategy, which fails to explore the affinity nature 
among the training instances. Alternatively, 
Zhou et al (2008) bootstrap a set of weighted 
support vectors from both labeled and unlabeled 
data using SVM. Nevertheless, the initial labeled 
data is still randomly generated only to ensure 
that there are at least 5 instances for every 
relation subtype. 
This paper presents a new approach to 
selecting the initial seed set based on stratified 
sampling strategy in the bootstrapping procedure 
for semi-supervised semantic relation 
classification. The motivation behind the 
stratified sampling is that every relation type 
should be as much as possible represented in the 
initial seed set, thus leading to more instances 
with diverse structures being added to the labeled 
set. In addition, we also explore different 
strategies to augment reliably classified instances 
to the labeled data iteratively, and attempt to find 
a stoppage criterion for the iteration procedure to 
greatly decrease the training time, other than 
using up all the unlabeled set. 
The rest of this paper is organized as follows. 
First, Section 2 reviews related work on semi-
supervised relation extraction. Then we present 
an underlying supervised learner in Section 3. 
Section 4 details various key aspects of the 
bootstrapping procedure, including the stratified 
sampling strategy. Experimental results are 
reported in Section 5. Finally we conclude our 
work in Section 6. 
2 Related Work 
Within the realm of information extraction, 
currently there are several representative semi-
supervised learning systems for extracting 
relations between named entities. 
DIPRE (Dual Iterative Pattern Relation 
Expansion) (Brin, 1998) is a system based on 
bootstrapping that exploits the duality between 
patterns and relations to augment the target 
relation starting from a small sample. However, 
it only extracts simple relations such as (author, 
title) pairs from the WWW. Snowball (Agichtein 
and Gravano, 2000) is another bootstrapping-
based system that extracts relations from 
unstructured text. Snowball shares much in 
common with DIPRE, including the use of both 
the bootstrapping framework and the pattern 
matching approach to extract new unlabeled 
instances. Due to pattern matching techniques, 
their systems are hard to be adapted to the 
general problem of relation extraction. 
Zhang (2004) approaches the relation 
classification problem with bootstrapping on top 
of SVM. He uses various lexical and syntactic 
features in the BootProject algorithm based on 
random feature projection to extract top-level 
relation types in the ACE corpus. Evaluation 
shows that bootstrapping can alleviate the burden 
of hand annotations for supervised learning 
methods to a certain extent.  
Chen et al (2006) investigate a semi-
supervised learning algorithm based on label 
propagation for relation extraction, where labeled 
and unlabeled examples and their distances are 
represented as the nodes and the weights of 
edges respectively in a connected graph, then the 
label information is propagated from any vertex 
to nearby vertices through weighted edges 
iteratively, finally the labels of unlabeled 
examples are inferred after the propagation 
process converges.  
Zhou et al (2008) integrate the advantages of 
SVM bootstrapping in learning critical instances 
and label propagation in capturing the manifold 
structure in both the labeled and unlabeled data, 
by first bootstrapping a moderate number of 
weighted support vectors through a co-training 
procedure from all the available data, and then 
applying label propagation algorithm via the 
bootstrapped support vectors. 
However, in most current systems, the initial 
seed set is selected randomly such that they may 
not adequately represent the inherent structure of 
unseen examples, hence the power of 
bootstrapping may be severely weakened. 
This paper presents a simple yet effective 
approach to generate the initial seed set by 
applying the stratified sampling strategy, 
originated from statistics theory. Furthermore, 
we try to employ the same stratified strategy to 
augment the labeled set. Finally, we attempt to 
find a reasonable criterion to terminate the 
iteration process. 
3 Underlying Supervised Learning 
A semi-supervised learning system usually 
consists of two relevant components: an 
underlying supervised learner and a 
1438
bootstrapping algorithm on top of it. In this 
section we discuss the former, while the latter 
will be described in the following section.  
In this paper, we select Support Vector 
Machines (SVMs) as the underlying supervised 
classifier since it represents the state-of-the-art in 
the machine learning research community, and 
there are good implementations of the algorithm 
available. Specifically, we use LIBSVM (Chang 
et al, 2001), an effective tool for support vector 
classification, since it supports multi-class 
classification and provides probability estimation 
as well. 
For each pair of entity mentions, we extract 
and compute various lexical and syntactic 
features, as employed in a state-of-the-art 
relation extraction system (Zhou et al, 2005). 
(1) Words: According to their positions, four 
categories of words are considered: a) the words 
of both the mentions; b) the words between the 
two mentions; c) the words before M1; and d) 
the words after M2.  
(2) Entity type: This category of features 
concerns about the entity types of both the 
mentions. 
(3) Mention Level: This category of features 
considers the entity level of both the mentions. 
(4) Overlap: This category of features includes 
the number of other mentions and words between 
two mentions. Typically, the overlap features are 
usually combined with other features such as 
entity type and mention level. 
(5) Base phrase chunking: The base phrase 
chunking is proved to play an important role in 
semantic relation extraction. Most of the 
chunking features concern about the headwords 
of the phrases between the two mentions.  
In this paper, we do not employ any deep 
syntactic or semantic features (such as 
dependency tree, full parse tree etc.), since they 
contribute quite limited in relation extraction. 
4 Bootstrapping & Stratified Sampling 
We first present the self-bootstrapping algorithm, 
and then discuss several key problems on 
bootstrapping in the order of initial seed 
selection, augmentation of labeled data and 
stoppage criterion for iteration. 
4.1 Bootstrapping Algorithm 
Following Zhang (2004), we define a basic self-
bootstrapping strategy, which keeps augmenting 
the labeled data set with the models 
straightforwardly trained from previously 
available labeled data as follows: 
Require: labeled seed set L
Require: unlabeled data set U
Require: batch size S
Repeat
    Train a single classifier on L
    Run the classifier on U
    Find at most S instances in U that the classifier has
the highest prediction confidence
    Add them into L
Until: no data points available or the stoppage
condition is reached
Algorithm self-bootstrapping
Figure 1. Self-bootstrapping algorithm 
In order to measure the confidence of the 
classifier?s prediction, we compute the entropy 
of the label probability distribution that the 
classifier assigns to the class label on an example 
(the lower the entropy, the higher the confidence): 
log
n
i i
i
H p p= ??      (1) 
Where n denotes the total number of relation 
classes, and pi denotes the probability of current 
example being classified as the ith class.  
4.2 Stratified Sampling for Initial Seeds  
Normally, the number of available labeled 
instances is quite limited (usually less than 100 
instances) when the iterative bootstrapping 
procedure begins. If the distribution of the initial 
seed set fails to approximate the distribution of 
the test data, the augmented data generated from 
bootstrapping would not capture the essence of 
relation types, and the performance on the test 
set will significantly decrease even only after one 
or two rounds of iterations. Therefore, the 
selection of initial seed set plays an important 
role in bootstrapping-based semantic relation 
extraction. 
Sampling is a part of statistical practice 
concerned with the selection of individual 
observations, which is intended to yield some 
knowledge about a population of interest. When 
dealing with the task of semi-supervised 
semantic relation classification, the population is 
the training set of relation instances from the 
ACE RDC corpora. We compare two practical 
sampling strategies as follows: 
(1) Randomly sampling, which picks the initial 
seeds from the training data using a random 
scheme. Each element thus has an equal 
probability of selection, and the population is not 
1439
subdivided or partitioned. Currently, most work 
on semi-supervised relation extraction employs 
this method. However, since the size of the initial 
seed set is very small, they are not guaranteed to 
capture the statistical properties of the whole 
training data, let alne of the test data. 
(2) Stratified sampling. When the population 
embraces a number of distinct categories, 
stratified sampling (Neyman, 1934) can be 
applied to this case. First, the population can be 
organized by these categories into separate 
"strata", then a sample is selected within each 
"stratum" separately, and randomly. Generally, 
the sample size is normally proportional to the 
relative size of the strata. The main motivation 
for using a stratified sampling design is to ensure 
that particular groups within a population are 
adequately represented in the sample. 
It is well known that the number of the 
instances for each relation type in the ACE RDC 
corpora is greatly unbalanced  (Zhou et al, 2005) 
as shown in Table 1 for the ACE RDC 2004 
corpus. When the relation instances for a specific 
relation type occurs frequently in the initial seed 
set, the classifier will achieve good performance 
on this type, otherwise the classifier can hardly 
recognize them from the test set. In order for 
every type of relations to be properly represented, 
the stratified sampling strategy is applied to the 
seed selection procedure. 
Types Subtypes Train Test
Located 593 145
Near 70 17
PHYS 
Part-Whole 299 79
Business 134 39
Family 101 20
PER-SOC 
Other 44 11
Employ-Executive 388 101
Employ-Staff 427 112
Employ-Undetermined 66 12
Member-of-Group 152 39
Subsidiary 169 37
Partner 10 2
EMP-ORG 
Other 64 16
User-or-Owner 160 40
Inventor-or-Man. 8 1
ART 
Other 1 1
Ethnic 31 8
Ideology 39 9
OTHER-
AFF 
Other 43 11
Citizen-or-Resid. 226 47
Based-In 165 50
GPE-AFF 
Other 31 8
DISC  224 55
Total  3445 860
Table 1. Numbers of relations on the ACE RDC 
2004: break down by relation types and subtypes 
Figure 2 illustrates the stratified sampling 
strategy we use in bootstrapping, where RSET 
denotes the training set, V is the stratification 
variable, and SeedSET denotes the initial seed set. 
First, we divide the relation instances into 
different strata according to available properties, 
such as major relation type (considering reverse 
relations or not) and relation subtype 
(considering reverse relations or not). Then 
within every stratum, a certain number of 
instances are sampled randomly, and this number 
is normally proportional to the size of that 
stratum in the whole population. However, when 
this number is 0 due to the rounding of real 
numbers, it is set to 1. Also it must be ensured 
that the total number of instances being sampled 
is NS. Finally, these instances form the initial 
seed set and can be used as the input to the 
underlying supervised learning for the 
bootstrapping procedure. 
 
Require: RSET ={R1,R2,?,RN} 
Require: V = {v1, v2,?,vK} 
Require: SeedSET with the size of NS (100) 
Initialization: 
SeedSET = NULL 
Steps: 
z Group RSET into K strata according to the 
stratified variable V, i.e.:  
RSET={RSET1,RSET2,?,RSETK} 
z Calculate the class prior probability for each 
stratum i={1,2,?,K} 
)(/)( RSETNUMRSETNUMP ii =  
z Caculate the number of intances being sampled 
for each stratum 
NPN ii ?=  
If Ni =0 then Ni=1 
z Calculate the difference of numbers as follows: 
?
=
? ?=
K
i
iS NNN
1
 
z If N?>0 then add Ni (i=1,2,?,|N?|) by 1 
If N?<0 then subtract 1 from Ni (i=1,2,...,|N?|) 
z For each i from 1 to K 
Select Ni instances from RESTi randomly 
Add them into SeedSET 
 
Figure 2. Stratefied Sampling for initial seeds 
4.3 Augmentation of labeled data 
After each round of iteration, some newly 
classified instances with the highest confidence 
can be augmented to the labeled training data. 
Nevertheless, just like the selection of initial seed 
set, we still wish that every stratum would be 
represented as appropriately as possible in the 
1440
instances added to the labeled set. In this paper, 
we compare two kinds of augmentation strategies 
available: 
(1) Top n method: the classified instances are 
first sorted in the ascending order by their 
entropies (i.e. decreasing confidence), and then 
the top n (usually 100) instances are chosen to be 
added.  
(2) Stratified method: in order to make the 
added instances representative for their stratum, 
we first select m (usually greater than n) 
instances with the highest confidence, then we 
choose n instances from them using the stratified 
strategy. 
4.4 Stoppage of Iterations 
In a self-bootstrapping procedure, as the 
iterations go on, both the reliable and unreliable 
instances are added to the labeled data 
continuously, hence the performance will 
fluctuate in a relatively small range. The key 
question here is how we can know when the 
bootstrapping procedure reaches its best 
performance on the test data. The bootstrapping 
algorithm by Zhang (2004) stops after it runs out 
of all the training instances, which may take a 
relatively long time. In this paper, we present a 
method to determine the stoppage criterion based 
on the mean entropy as follows: 
Hi <= p    (2) 
Where Hi denotes the mean entropy of the 
confidently classified instances being augmented 
to the labeled data in each iteration, and p 
denotes a threshold for the mean entropy, which 
will be fixed through empirical experiments. 
This criterion is based on the assumption that 
when the mean entropy becomes less than or 
equal to a certain threshold, the classifier would 
achieve the most reliable confidence on the 
instances being added to the labeled set, and it 
may be impossible to yield better performance 
since then. Therefore, the iteration may stop at 
that reasonable point.  
5 Experimentation 
This section aims to empirically investigate the 
effectiveness of the bootstrapping-based semi-
supervised learning we discussed above for 
semantic relation classification. In particular, 
different methods for selecting the initial seed set 
and augmenting the labeled data are evaluated. 
5.1 Experimental Setting 
We use the ACE corpora as the benchmark data, 
which are gathered from various newspapers, 
newswire and broadcasts. The ACE 2004 corpus 
contains 451 documents and 5702 positive 
relation instances. It defines 7 relation types and 
23 subtypes between 7 entity types. For easy 
reference with related work in the literature, 
evaluation is also done on 347 documents 
(including nwire and bnews domains) and 4305 
relation instances using 5-fold cross-validation. 
That is, these relation instances are first divided 
into 5 sets, then, one of them (about 860 
instances) is used as the test data set, while the 
others are regarded as the training data set, from 
which the initial seed set is sampled. In the ACE 
2003 corpus, the training set consists of 674 
documents and 9683 positive relation instances 
while the test data consists of 97 documents and 
1386 positive relation instances. The ACE RDC 
2003 task defines 5 relation types and 24 
subtypes between 5 entity types. 
The corpora are first parsed using Collins?s 
parser (Collins, 2003) with the boundaries of all 
the entity mentions kept. Then, the parse trees 
are converted into chunklink format using 
chunklink.pl 1. Finally, various useful lexical and 
syntactic features, as described in Subsection 3.1, 
are extracted and computed accordingly. For the 
purpose of comparison, we define our task as the 
classification of the 5 or 7 major relation types in 
the ACE RDC 2003 and 2004 corpora. 
For LIBSVM parameters, we adopted the 
polynomial kernel, and c is set to 10, g is set to 
0.15. Under this setting, we achieved the best 
classification performance. 
5.2 Experimental Results 
In this subsection, we compare and discuss the 
experimental results using various sampling 
strategies, different augmentation methods, and 
iteration stoppage criterion. 
 
Comparison of sampling strategies in selecting 
the initial seed set 
Table 2 and Table 3 show the initial and the 
highest classification performance of 
Precision/Recall/F-measure for various sampling 
strategies of the initial seed set on 7 major 
relation types of the ACE RDC 2004 corpus 
respectively when the size of initial seed set L is 
100, the batch size S is 100, and the top 100 
                                                 
1 http://ilk.kub.nl/~sabine/chunklink/ 
1441
instances with the highest confidence are added 
at each iteration. Table 2 also lists the number of 
strata for stratified sampling methods from which 
the initial seeds are randomly chosen 
respectively. Table 3 additionally lists the time 
needed to complete the bootstrapping process (on 
a PC with a Pentium IV 3.0G CPU and 1G 
memory). In this paper, we consider the 
following five experimental settings when 
sampling the initial seeds: 
z Randomly Sampling: as described in 
Subsection 4.2. 
z Stratified-M Sampling: the strata are 
grouped in terms of major relation types 
without considering reverse relations. 
z Stratified-MR Sampling: the strata are 
grouped in terms of major relation types, 
including reverse relations. 
z Stratified-S Sampling: the strata are 
grouped in terms of relation subtypes 
without considering reverse subtypes. 
z Stratified-SR Sampling: the strata are 
grouped in terms of relation subtypes, 
including reverse subtypes. 
For each sampling strategies, we performed 20 
trials and computed average scores and the total 
time on the test set over these 20 trials. 
Sampling strategies 
for initial seeds 
# of 
strat. P(%) R(%) F 
Randomly 1 66.1 65.9 65.9
Stratified-M 7 69.1 66.5 67.7
Stratified-MR 13 69.3 67.3 68.2
Stratified-S 30 69.8 67.7 68.7
Stratified-SR 39 69.9 68.5 69.2
Table 2. The initial performance of applying 
various sampling strategies to selecting the initial 
seed set on the ACE RDC 2004 corpus 
Sampling strategies 
for initial seeds 
Time 
(min) P(%) R(%) F 
Randomly 52 68.6 66.2 67.3
Stratified-M 65 71.0 66.9 68.8
Stratified-MR 65 71.6 67.0 69.2
Stratified-S 71 72.7 67.8 70.1
Stratified-SR 77 72.9 68.4 70.6
Table 3. The highest performance of applying 
various sampling strategies in selecting the initial 
seed set on the ACE RDC 2004 corpus 
 
These two tables jointly indicate that the self-
bootstrapping procedure for all sampling 
strategies can moderately improve the 
classification performance by ~1.2 units in F-
score, which is also verified by Zhang (2004). 
Furthermore, they show that: 
z The most improvements in performance 
come from improvements in precision. Actually, 
for some settings the recalls even decrease 
slightly. The reason may be that due to the nature 
of self-bootstrapping, the instances augmented at 
each iteration are always those which are the 
most similar to the initial seed instances, 
therefore the models trained from them would 
exhibit higher precision on the test set, while it 
virtually does no help for recall. 
z All of the four stratified sampling methods 
outperform the randomly sampling method to 
various degrees, both in the initial performance 
and the highest performance. This means that 
sampling of the initial seed set based on 
stratification by major/sub relation types can be 
helpful to relation classification, largely due to 
the performance improvement of the initial seed 
set, which is caused by adequate representation 
of instances for every relation type. 
z Of all the four stratified sampling methods, 
the Stratified-SR sampling achieves the best 
performance of 72.9/68.4/70.6 in P/R/F. 
Moreover, the more the number of strata 
generated by the sampling strategy, the more 
appropriately they would be represented in the 
initial seed set, and the better performance it will 
yield. This also implies that the hierarchy of 
relation types/subtypes in the ACE RDC 2004 
corpus is fairly reasonably defined. 
z An important conclusion, which can be 
draw accordingly, is that the F-score 
improvement of Stratified-SR sampling over 
Randomly sampling in initial performance (3.3 
units) is significantly greater than the F-score 
improvement gained by bootstrapping itself 
using Randomly sampling (1.4 units). This means 
that the sampling strategy of the initial seed set is 
even more important than the bootstrapping 
algorithm itself for relation classification. 
z It is interesting to note that the time needed 
to bootstrap increases with the number of strata. 
The reason may be that due to more diverse 
structures in the labeled data for stratified 
sampling, the SVM needs more time to 
differentiate between instances, i.e. more time to 
learn the models. 
 
Comparison of different augmentation 
strategies of training data 
Figure 3 compares the performance of F-score 
for two augmentation strategies: the Top n 
method and the stratified method, over various 
initial seed sampling strategies on the ACE RDC 
2004 corpus. For each iteration, a variable 
1442
number (m is ranged from 100 to 500) of 
classified instances in the decreasing order of 
confidence are first chosen as the base examples, 
then at most 100 examples are selected from the 
base examples to be augmented to the labeled set. 
Specifically, when m is equal to 100, the whole 
set of the base example is added to the labeled 
data, i.e. degenerated to the Top n augmentation 
strategy. On the other hand, when m is greater 
than 100, we wish we would select examples of 
different major relation types from the base 
examples according to their distribution in the 
training set, in order to achieve the performance 
improvement as much as the stratified sampling 
does in the selection of the initial seed set. 
64
65
66
67
68
69
70
71
72
100 200 300 400 500
# Base examples
F-
sc
or
e
Randomly
Stratified-M 
Stratified-MR
Stratified-S
Stratified-SR
Figure 3. Comparison of two augmentation 
strategies over different sampling strategies in 
selecting the initial seed set. 
This figure shows that, except for randomly 
sampling strategy, the stratified augmentation 
strategies improve the performance. Nevertheless, 
this result is far from our expectation in two 
ways: 
z The performance improvement in F-score is 
trivial, at most 0.4 units on average. The reason 
may be that, although we try to add as many as 
100 classified instances to the labeled data 
according to the distribution of every major 
relation type in the training set, the top m 
instances with the highest confidence are usually 
focused on certain relation types (e.g. PHSY and 
PER-SOC), this leads to the stratified 
augmentation failing to function effectively. 
Hence, all the following experiments will only 
adopt Top n method for augmenting the labeled 
data. 
z With the increase of the number of the base 
examples, the performance fluctuates slightly, 
thus it is relatively difficult to recognize where 
the optima is. We think there are two 
contradictory factors that affect the performance. 
While the reliability of the instances extracted 
from the base examples decreases with the 
increase of the number of base examples, the 
probability of extracting instances of more 
relation types increases with the increase of the 
number of the base examples. These two factors 
inversely interact with each other, leading to the 
fluctuation in performance. 
 
Comparison of different threshold values for 
stoppage criterion 
We compare the performance and 
bootstrapping time (20 trials with the same initial 
seed set) when applying stoppage criterion in 
Formula (2) with different threshold p over 
various sampling strategies on the ACE RDC 
2004 corpus in Figure 4 and Figure 5 
respectively. These two figures jointly show that: 
64
65
66
67
68
69
70
71
0 0.2 0.22 0.24 0.26 0.28 0.3
p
F-
sc
or
e
Randomly
Stratified-M
Stratified-MR
Stratified-S
Stratified-SR
Figure 4. Performance for different p values 
0
10
20
30
40
50
60
70
80
90
0 0.2 0.22 0.24 0.26 0.28 0.3
p
Ti
m
e(
mi
n)
Randomly
Stratified-M
Stratified-MR
Stratified-S
Stratified-SR
Figure 5. Bootstrapping time for different p 
values 
z The performance decreases slowly while the 
bootstrapping time decreases dramatically with 
the increase of p from 0 to 0.3. Specifically, 
when the p equals to 0.3, the bootstrapping time 
tends to be neglected, while the performance is 
almost similar to the initial performance. It 
implies that we can find a reasonable point for 
each sampling strategy, at which the time falls 
greatly while the performance nearly does not 
degrade.  
1443
Bootproject LP-js Stratified Bootstrapping Relation types 
P R F P R F P R F 
ROLE 78.5 69.7 73.8 81.0 74.7 77.7 74.7 86.3 80.1
PART 65.6 34.1 44.9 70.1 41.6 52.2 66.4 47.0 55.0
AT 61.0 84.8 70.9 74.2 79.1 76.6 74.9 66.1 70.2
NEAR - - - 13.7 12.5 13.0 100.0 2.9 5.6
SOC 47.0 57.4 51.7 45.0 59.1 51.0 65.2 79.0 71.4
Average 67.9 67.4 67.6 73.6 69.4 70.9 73.8 73.3 73.5
Table 4. Comparison of semi-supervised relation classification systems on the ACE RDC 2003 corpus 
 
z Clearly, if the performance is the primary 
concern, then p=0.2 may be the best choice in 
that we can get ~30% saving on the time at the 
cost of only ~0.08 loss in F-score on average. If 
the time is a primary concern, then p=0.22 is a 
reasonable threshold in that we get ~50% saving 
on the time at the cost of ~0.25 units loss in F-
score on average. This suggests that our 
proposed stoppage criterion is effective to 
terminate the bootstrapping procedure with 
minor performance loss. 
 
Comparison of Stratified Bootstrapping with 
Bootproject and Label propagation  
Table 4 compares Bootproject (Zhang, 2004), 
Label propagation (Chen et al, 2006) with our 
Stratified Bootstrapping on the 5 major types of 
the ACE RDC 2003 corpus. 
Both Bootproject and Label propagation 
select 100 initial instances randomly, and at each 
iteration, the top 100 instances with the highest 
confidence are added to the labeled data. 
Differently, we choose 100 initial seeds using 
stratified sampling strategy; similarly, the top 
100 instances with the highest confidence are 
augmented to the labeled data at each iteration. 
Due to the lack of comparability followed from 
the different size of the labeled data used in 
(Zhou et al, 2008), we omit their results here. 
This table shows that our stratified 
bootstrapping procedure significantly 
outperforms both Bootproject and Label 
Propagation methods on the ACE RDC corpus, 
with the increase of 5.9/4.1 units in F-score on 
average respectively. Stratified bootstrapping 
consistently outperforms Bootproject in every 
major relation type, while it outperforms Label 
Propagation in three of the major relation types, 
especially SOC type, with the exception of AT 
and NEAR types. The reasons may be follows. 
Although there are many AT relation instances in 
the corpus, they are scattered divergently in 
multi-dimension space so that they tend to be 
relatively difficult to be recognized via SVM. 
For the NEAR relation instances, they occur least 
frequently in the whole corpus, so it is very hard 
for them to be identified via SVM. By contrast, 
even small size of labeled instances can be fully 
utilized to correctly induce the unlabeled 
instances via LP algorithm due to its ability to 
exploit manifold structures of both labeled and 
unlabeled instances (Chen et al, 2006). 
In general, these results again suggest that the 
sampling strategy in selecting the initial seed set 
plays a critical role for relation classification, and 
stratified sampling can significantly improve the 
performance due to proper selection of the initial 
seed set. 
6 Conclusion 
This paper explores several key issues in semi-
supervised learning based on bootstrapping for 
semantic relation classification. The application 
of stratified sampling originated from statistics 
theory to the selection of the initial seed set 
contributes most to the performance 
improvement in the bootstrapping procedure. In 
addition, the more strata the training data is 
divided into, the better performance will be 
achieved. However, the augmentation of the 
labeled data using the stratified strategy fails to 
function effectively largely due to the 
unbalanced distribution of the confidently 
classified instances, rather than the stratified 
sampling strategy itself. Furthermore, we also 
propose a mean entropy-based stoppage criterion 
in the bootstrapping procedure, which can 
significantly decrease the training time with little 
loss in performance. Finally, it also shows that 
our method outperforms other state-of-the-art 
semi-supervised ones. 
 
Acknowledgments 
This research is supported by Project 60673041 
and 60873150 under the National Natural 
Science Foundation of China, Project 
2006AA01Z147 under the ?863? National High-
Tech Research and Development of China, 
1444
Project BK2008160 under the Jiangsu Natural 
Science Foundation of China, and the National 
Research Foundation for the Doctoral Program 
of Higher Education of China under Grant No. 
20060285008. We would also like to thank the 
excellent and insightful comments from the three 
anonymous reviewers. 
References  
S. Abney. Bootstrapping. 2002. In Proceedings of the 
40th Annual Meeting of the Association for 
Computational  Linguistics (ACL 2002). 
ACE 2002-2007. The Automatic Content Extraction 
(ACE) Projects. 2007. http//www.ldc.upenn.edu/ 
Projects/ACE/. 
E. Agichtein and L. Gravano. 2000. Snowball: 
Extracting relations from large plain-text 
collections. In Proceedings of the 5th ACM 
international Conference on Digital Libraries 
(ACMDL 2000). 
A. Blum and T. Mitchell. 1996. Combining labeled 
and unlabeled data with co-training. In COLT: 
Proceedings of the workshop on Computational 
Learning Theory. Morgan Kaufmann Publishers. 
S. Brin. 1998. Extracting patterns and relations from 
the world wide web. In WebDB Workshop at 6th 
International Conference on Extending Database 
Technology (EDBT 98). 
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library 
for support vector machines. http:// 
www.csie.ntu.edu.tw/~cjlin/libsvm. 
M. Collins. 2003. Head-Driven Statistics Models for 
Natural Language Parsing. Computational 
linguistics, 29(4): 589-617. 
J.X. Chen, D.H. Ji, and L.T. Chew. 2006. Relation 
Extraction using Label Propagation Based Semi 
supervised Learning. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th Annual Meeting of the 
Association of Computational Linguistics 
(COLING/ACL 2006), pages 129-136. July 2006, 
Sydney, Australia.  
A. Culotta and J. Sorensen. 2004. Dependency tree 
kernels for relation extraction. In Proceedings of 
the 42nd Annual Meeting of the Association of 
Computational Linguistics (ACL 2004), pages 423-
439. 21-26 July 2004, Barcelona, Spain. 
T. Hasegawa, S. Sekine, and R. Grishman. 2004. 
Discovering Relations among Named Entities from 
Large Corpora. In Proceedings of the 42nd Annual 
Meeting of the Association of Computational 
Linguistics (ACL 2004). 21-26 July 2004, 
Barcelona, Spain. 
N. Kambhatla. Combining lexical, syntactic and 
semantic features with Maximum Entropy models 
for extracting relations. In Proceedings of the 42nd 
Annual Meeting of the Association of 
Computational Linguistics (ACL 2004)(posters), 
pages 178-181. 21-26 July 2004, Barcelona, Spain. 
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 
2000. A novel use of statistical parsing to extract 
information from text. In Proceedings of the 6th 
Applied Natural Language Processing Conference. 
29 April-4 May 2000, Seattle, USA. 
J. Neyman. 1934. On the Two Different Aspects of 
the Representative Method: The Method of 
Stratified Sampling and the Method of Purposive 
Selection. Journal of the Royal Statistical Society, 
97(4): 558-625. 
L.H. Qian, G.D. Zhou, Q.M. Zhu, and P.D Qian. 2008. 
Exploiting constituent dependencies for tree 
kernel-based semantic relation extraction. In 
Proceedings of The 22nd International Conference 
on Computational Linguistics (COLING 2008), 
pages 697-704. 18-22 August 2008, Manchester, 
UK. 
D. Yarowsky. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In the 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics (ACL 
95), pages 189-196. 26-30 June 1995, MIT, 
Cambridge, Massachusetts, USA. 
D. Zelenko, C. Aone, and A. Richardella. 2003. 
Kernel Methods for Relation Extraction. Journal of 
Machine Learning Research, (2): 1083-1106. 
M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A 
Composite Kernel to Extract Relations between 
Entities with both Flat and Structured Features. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and the 44th Annual 
Meeting of the Association of Computational 
Linguistics (COLING/ACL 2006), pages 825-832. 
Sydney, Australia. 
M. Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L. 
Tan. 2005. Discovering Relations between Named 
Entities from a Large Raw Corpus Using Tree 
Similarity-Based Clustering. In Proceedings of the 
2nd international Joint Conference on Natural 
Language Processing (IJCNLP-2005), pages 378-
389. Jeju Island, Korea.  
Z. Zhang. 2004. Weakly-supervised relation 
classification for Information Extraction. In 
Proceedings of ACM 13th conference on 
Information and Knowledge Management (CIKM 
2004). 8-13 Nov 2004, Washington D.C., USA. 
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. 
Exploring various knowledge in relation extraction. 
In Proceedings of the 43rd Annual Meeting of the 
Association of Computational Linguistics (ACL 
2005), pages 427-434. Ann Arbor, USA. 
G.D. Zhou, J.H. Li, L.H. Qian, and Q.M. Zhu. 2008. 
Semi-Supervised Learning for Relation Extraction. 
In Proceedings of the 3rd International Joint 
Conference on Natural Language Processing 
(IJCNLP-2008), page 32-38. 7-12 January 2008, 
Hyderabad, India. 
 
1445
Context-Sensitive Convolution Tree Kernel 
for Pronoun Resolution 
 
 
ZHOU GuoDong    KONG Fang    ZHU Qiaoming 
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow Univ.  Suzhou, China 215006 
Email: {gdzhou, kongfang, qmzhu}@suda.edu.cn 
 
 
Abstract 
This paper proposes a context-sensitive convo-
lution tree kernel for pronoun resolution. It re-
solves two critical problems in previous 
researches in two ways. First, given a parse 
tree and a pair of an anaphor and an antecedent 
candidate, it implements a dynamic-expansion 
scheme to automatically determine a proper 
tree span for pronoun resolution by taking 
predicate- and antecedent competitor-related 
information into consideration. Second, it ap-
plies a context-sensitive convolution tree ker-
nel, which enumerates both context-free and 
context-sensitive sub-trees by considering their 
ancestor node paths as their contexts. Evalua-
tion on the ACE 2003 corpus shows that our 
dynamic-expansion tree span scheme can well 
cover necessary structured information in the 
parse tree for pronoun resolution and the con-
text-sensitive tree kernel much outperforms 
previous tree kernels.  
1 Introduction 
It is well known that syntactic structured informa-
tion plays a critical role in many critical NLP ap-
plications, such as parsing, semantic role labeling, 
semantic relation extraction and co-reference reso-
lution. However, it is still an open question on 
what kinds of syntactic structured information are 
effective and how to well incorporate such struc-
tured information in these applications. 
Much research work has been done in this direc-
tion. Prior researches apply feature-based methods 
to select and define a set of flat features, which can 
be mined from the parse trees, to represent particu-
lar structured information in the parse tree, such as 
the grammatical role (e.g. subject or object), ac-
cording to the particular application. Indeed, such 
feature-based methods have been widely applied in 
parsing (Collins 1999; Charniak 2001), semantic 
role labeling (Pradhan et al2005), semantic rela-
tion extraction (Zhou et al2005) and co-reference 
resolution  (Lapin and Leass 1994; Aone and Ben-
nett 1995; Mitkov 1998; Yang et al2004; Luo and 
Zitouni 2005; Bergsma and Lin 2006). The major 
problem with feature-based methods on exploring 
structured information is that they may fail to well 
capture complex structured information, which is 
critical for further performance improvement.  
The current trend is to explore kernel-based 
methods (Haussler, 1999) which can implicitly 
explore features in a high dimensional space by 
employing a kernel to calculate the similarity be-
tween two objects directly. In particular, the ker-
nel-based methods could be very effective at 
reducing the burden of feature engineering for 
structured objects in NLP, e.g. the parse tree struc-
ture in coreference resolution. During recent years, 
various tree kernels, such as the convolution tree 
kernel (Collins and Duffy 2001), the shallow parse 
tree kernel (Zelenko et al2003) and the depend-
ency tree kernel (Culota and Sorensen 2004), have 
been proposed in the literature. Among previous 
tree kernels, the convolution tree kernel represents 
the state-of-the-art and have been successfully ap-
plied by Collins and Duffy (2002) on parsing, Mo-
schitti (2004) on semantic role labeling, Zhang et 
al (2006) on semantic relation extraction  and Yang 
et al(2006) on pronoun resolution.  
However, there exist two problems in Collins 
and Duffy?s kernel. The first is that the sub-trees 
enumerated in the tree kernel are context-free. That 
is, each sub-tree enumerated in the tree kernel does 
not consider the context information outside the 
sub-tree. The second is how to decide a proper tree 
span in the tree kernel computation according to 
the particular application. To resolve above two 
problems, this paper proposes a new tree span 
scheme and applies a new tree kernel and to better 
capture syntactic structured information in pronoun 
25
resolution, whose task is to find the corresponding 
antecedent for a given pronominal anaphor in text. 
The rest of this paper is organized as follows. In 
Section 2, we review related work on exploring 
syntactic structured information in pronoun resolu-
tion and their comparison with our method. Section 
3 first presents a dynamic-expansion tree span 
scheme by automatically expanding the shortest 
path to include necessary structured information, 
such as predicate- and antecedent competitor-
related information. Then it presents a context-
sensitive convolution tree kernel, which not only 
enumerates context-free sub-trees but also context-
sensitive sub-trees by considering their ancestor 
node paths as their contexts. Section 4 shows the 
experimental results. Finally, we conclude our 
work in Section 5.  
2 Related Work 
Related work on exploring syntactic structured 
information in pronoun resolution can be typically 
classified into three categories: parse tree-based 
search algorithms (Hobbs 1978), feature-based  
(Lappin and Leass 1994; Bergsma and Lin 2006) 
and tree kernel-based methods (Yang et al2006).  
As a representative for parse tree-based search 
algorithms, Hobbs (1978) found the antecedent for 
a given pronoun by searching the parse trees of 
current text. It processes one sentence at a time 
from current sentence to the first sentence in text 
until an antecedent is found. For each sentence, it 
searches the corresponding parse tree in a left-to-
right breadth-first way. The first antecedent candi-
date, which satisfies hard constraints (such as gen-
der and number agreement), would be returned as 
the antecedent. Since the search is completely done 
on the parse trees, one problem with the parse tree-
based search algorithms is that the performance 
would heavily rely on the accuracy of the parse 
trees. Another problem is that such algorithms are 
not good enough to capture necessary structured 
information for pronoun resolution. There is still a 
big performance gap even on correct parse trees. 
Similar to other NLP applications, feature-
based methods have been widely applied in pro-
noun resolution to explore syntactic structured in-
formation from the parse trees. Lappin and Leass 
(1994) derived a set of salience measures (e.g. sub-
ject, object or accusative emphasis) with manually 
assigned weights from the syntactic structure out-
put by McCord?s Slot Grammar parser. The candi-
date with the highest salience score would be 
selected as the antecedent. Bergsma and Lin (2006) 
presented an approach to pronoun resolution based 
on syntactic paths. Through a simple bootstrapping 
procedure, highly co-reference paths can be 
learned reliably to handle previously challenging 
instances and robustly address traditional syntactic 
co-reference constraints. Although feature-based 
methods dominate on exploring syntactic struc-
tured information in the literature of pronoun reso-
lution, there still exist two problems with them. 
One problem is that the structured features have to 
be selected and defined manually, usually by lin-
guistic intuition. Another problem is that they may 
fail to effectively capture complex structured parse 
tree information. 
As for tree kernel-based methods, Yang et al
(2006) captured syntactic structured information 
for pronoun resolution by using the convolution 
tree kernel (Collins and Duffy 2001) to measure 
the common sub-trees enumerated from the parse 
trees and achieved quite success on the ACE 2003 
corpus. They also explored different tree span 
schemes and found that the simple-expansion 
scheme performed best. One problem with their 
method is that the sub-trees enumerated in Collins 
and Duffy?s kernel computation are context-free, 
that is, they do not consider the information out-
side the sub-trees. As a result, their ability of ex-
ploring syntactic structured information is much 
limited. Another problem is that, among the three 
explored schemes, there exists no obvious over-
whelming one, which can well cover syntactic 
structured information.  
The above discussion suggests that structured 
information in the parse trees may not be well util-
ized in the previous researches, regardless of fea-
ture-based or tree kernel-based methods. This 
paper follows tree kernel-based methods. Com-
pared with Collins and Duffy?s kernel and its ap-
plication in pronoun resolution (Yang et al2006), 
the context-sensitive convolution tree kernel enu-
merates not only context-free sub-trees but also 
context-sensitive sub-trees by taking their ancestor 
node paths into consideration. Moreover, this paper 
also implements a dynamic-expansion tree span 
scheme by taking predicate- and antecedent com-
petitor-related information into consideration. 
26
3 Context Sensitive Convolution Tree 
Kernel for Pronoun Resolution 
In this section, we first propose an algorithm to 
dynamically determine a proper tree span for pro-
noun resolution and then present a context-
sensitive convolution tree kernel to compute simi-
larity between two tree spans. In this paper, all the 
texts are parsed using the Charniak parser 
(Charniak 2001) based on which the tree span is 
determined. 
3.1 Dynamic-Expansion Tree Span Scheme 
Normally, parsing is done on the sentence level. To 
deal with the cases that an anaphor and an antece-
dent candidate do not occur in the same sentence, 
we construct a pseudo parse tree for an entire text 
by attaching the parse trees of all its sentences to 
an upper ?S? node, similar to Yang et al(2006). 
Given the parse tree of a text, the problem is 
how to choose a proper tree span to well cover syn-
tactic structured information in the tree kernel 
computation. Generally, the more a tree span in-
cludes, the more syntactic structured information 
would be provided, at the expense of more noisy 
information. Figure 2 shows the three tree span 
schemes explored in Yang et al(2006): Min-
Expansion (only including the shortest path con-
necting the anaphor and the antecedent candidate), 
Simple-Expansion (containing not only all the 
nodes in Min-Expansion but also the first level 
children of these nodes) and Full-Expansion (cov-
ering the sub-tree between the anaphor and the 
candidate), such as the sub-trees inside the dash 
circles of Figures 2(a), 2(b) and 2(c) respectively. 
It is found (Yang et al2006) that the simple-
expansion tree span scheme performed best on the 
ACE 2003 corpus in pronoun resolution. This sug-
gests that inclusion of more structured information 
in the tree span may not help in pronoun resolution. 
To better capture structured information in the 
parse tree, this paper presents a dynamic-expansion 
scheme by trying to include necessary structured 
information in a parse tree. The intuition behind 
our scheme is that predicate- and antecedent com-
petitor- (all the other compatible1 antecedent can-
didates between the anaphor and the considered 
antecedent candidate) related information plays a 
critical role in pronoun resolution. Given an ana-
                                                           
1 With matched number, person and gender agreements. 
phor and an antecedent candidate, e.g. ?Mary? and 
?her? as shown in Figure 1, this is done by: 
1) Determining the min-expansion tree span via 
the shortest path, as shown in Figure 1(a). 
2) Attaching all the antecedent competitors along 
the corresponding paths to the shortest path. As 
shown in Figure 1(b), ?the woman? is attached 
while ?the room? is not attached since the for-
mer is compatible with the anaphor and the lat-
ter is not compatible with the anaphor. In this 
way, the competition between the considered 
candidate and other compatible candidates can 
be included in the tree span. In some sense, this 
is a natural extension of the twin-candidate 
learning approach proposed in Yang et al
(2003), which explicitly models the competition 
between two antecedent candidates. 
3) For each node in the tree span, attaching the 
path from the node to the predicate terminal 
node if it is a predicate-headed node. As shown 
in Figure 1(c), ?said? and ?bit? are attached. 
4) Pruning those nodes (except POS nodes) with 
the single in-arc and the single out-arc and with 
its syntactic phrase type same as its child node. 
As shown in Figure 1(d), the left child of the 
?SBAR? node, the ?NP? node, is removed and 
the sub-tree (NP the/DT woman/NN) is at-
tached to the ?SBAR? node directly.  
To show the difference among min-, simple-, 
full- and dynamic-expansion schemes, Figure 2 
compares them for three different sentences, given 
the anaphor ?her/herself? and the antecedent can-
didate ?Mary?. It shows that:  
? Min-, simple- and full-expansion schemes have 
the same tree spans (except the word nodes) for 
the three sentences regardless of the difference 
among the sentences while the dynamic-
expansion scheme can adapt to difference ones. 
? Normally, the min-expansion scheme is too 
simple to cover necessary information (e.g. ?the 
woman? in the 1st sentence is missing).  
? The full-expansion scheme can cover all the 
information at the expense of much noise (e.g. 
?the man in that room? in the 2nd sentence).  
? The simple-expansion scheme can cover some 
necessary predicate-related information (e.g. 
?said? and ?bit? in the sentences). However, it 
may introduce some noise (e.g. the left child of 
27
the ?SBAR? node, the ?NP? node, may not be 
necessary in the 2nd sentence) and ignore neces-
sary antecedent competitor-related information 
(e.g. ?the woman? in the 1st sentence). 
? The dynamic-expansion scheme normally 
works well. It can not only cover predicate-
related information but also structured informa-
tion related with the competitors of the consid-
ered antecedent candidate. In this way, the 
competition between the considered antecedent 
candidate and other compatible candidates can 
be included in the dynamic-expansion scheme. 
 
 
 
Figure 1: Dynamic-Expansion Tree Span Scheme 
 
  
 
Figure 2: Comparison of Min-, Simple-, Full-and Dynamic-Expansions: More Examples 
28
3.2 Context-Sensitive Convolution Tree Kernel 
Given any tree span scheme, e.g. the dynamic-
expansion scheme in the last subsection, we now 
study how to measure the similarity between two 
tree spans using a convolution tree kernel. 
A convolution kernel (Haussler D., 1999) aims 
to capture structured information in terms of sub-
structures. As a specialized convolution kernel, the 
convolution tree kernel, proposed in Collins and 
Duffy (2001), counts the number of common sub-
trees (sub-structures) as the syntactic structure 
similarity between two parse trees. This convolu-
tion tree kernel has been successfully applied by 
Yang et al(2006) in pronoun resolution. However, 
there is one problem with this tree kernel: the sub-
trees involved in the tree kernel computation are 
context-free (That is, they do not consider the in-
formation outside the sub-trees.). This is contrast 
to the tree kernel proposed in Culota and Sorensen 
(2004) which is context-sensitive, that is, it consid-
ers the path from the tree root node to the sub-tree 
root node. In order to integrate the advantages of 
both tree kernels and resolve the problem in 
Collins and Duffy?s kernel, this paper applies the 
same context-sensitive convolution tree kernel, 
proposed by Zhou et al(2007) on relation extrac-
tion. It works by taking ancestral information (i.e. 
the root node path) of sub-trees into consideration: 
? ?
=
?
?
D=
m
i
Nn
Nn
ii
C
ii
ii
nnTTK
1
]2[]2[
]1[]1[
11
11
11
])2[],1[(])2[],1[(  (1) 
where ][1 jN
i is the set of root node paths with 
length i in tree T[j] while the maximal length of a 
root node path is defined by m; and 
])2[],1[( 11
ii nnD  counts the common context-
sensitive sub-trees rooted at root node paths ]1[1
in  
and ]2[1
in . In the tree kernel, a sub-tree becomes 
context-sensitive via the ?root node path? moving 
along the sub-tree root. For more details, please 
refer to Zhou et al(2007). 
4 Experimentation 
This paper focuses on the third-person pronoun 
resolution and, in all our experiments, uses the 
ACE 2003 corpus for evaluation. This ACE corpus 
contains ~3.9k pronouns in the training data and 
~1.0k pronouns in the test data.  
Similar to Soon et al(2001), an input raw text is 
first preprocessed automatically by a pipeline of 
NLP components, including sentence boundary 
detection, POS tagging, named entity recognition 
and phrase chunking, and then a training or test 
instance is formed by a pronoun and one of its an-
tecedent candidates. During training, for each ana-
phor encountered, a positive instance is created by 
pairing the anaphor and its closest antecedent 
while a set of negative instances is formed by pair-
ing the anaphor with each of the non-coreferential 
candidates. Based on the training instances, a bi-
nary classifier is generated using a particular learn-
ing algorithm. In this paper, we use SVMLight 
deleveloped by Joachims (1998). During resolution, 
an anaphor is first paired in turn with each preced-
ing antecedent candidate to form a test instance, 
which is presented to a classifier. The classifier 
then returns a confidence value indicating the like-
lihood that the candidate is the antecedent. Finally, 
the candidate with the highest confidence value is 
selected as the antecedent. In this paper, the NPs 
occurring within the current and previous two sen-
tences are taken as the initial antecedent candidates, 
and those with mismatched number, person and 
gender agreements are filtered out. On average, an 
anaphor has ~7 antecedent candidates. The per-
formance is evaluated using F-measure instead of 
accuracy since evaluation is done on all the pro-
nouns occurring in the data.  
Scheme/m 1 2 3 4 
Min 78.5 79.8 80.8 80.8 
Simple 79.8 81.0 81.7 81.6 
Full 78.3 80.1 81.0 81.1 
Dynamic 80.8 82.3 83.0 82.9 
Table 1: Comparison of different context-sensitive  
convolution tree kernels and tree span schemes 
(with entity type info attached at both the anaphor 
and the antecedent candidate nodes by default) 
In this paper, the m parameter in our context-
sensitive convolution tree kernel as shown in 
Equation (1) indicates the maximal length of root 
node paths and is optimized to 3 using 5-fold cross 
validation on the training data. Table 1 systemati-
cally evaluates the impact of different m in our 
context-sensitive convolution tree kernel and com-
pares our dynamic-expansion tree span scheme 
with the existing three tree span schemes, min-, 
29
simple- and full-expansions as described in Yang 
et al(2006). It also shows that that our tree kernel 
achieves best performance with m = 3 on the test 
data, which outperforms the one with m = 1 by 
~2.2 in F-measure. This suggests that the parent 
and grandparent nodes of a sub-tree  contain much 
information for pronoun resolution while 
considering more ancestral nodes doesnot further 
improve the performance. This may be due to that, 
although our experimentation on the training data 
indicates that  more than 90% (on average) of 
subtrees has a root node path longer than 3 (since 
most of the subtrees are deep from the root node 
and more than 90% of the parsed trees are deeper 
than 6 levels in the ACE 2003 corpus), including a 
root node path longer than 3 may be vulnerable to 
the full parsing errors and have negative impact. It 
also shows that our dynamic-expansion tree span 
scheme outperforms min-expansion, simple-
expansion and full-expansion schemes by ~2.4, 
~1.2 and ~2.1 in F-measure respectively. This 
suggests the usefulness of dynamically expanding 
tree spans to cover necessary structured 
information in pronoun resolution. In all the 
following experiments, we will apply our tree 
kernel with m=3 and the dynamic-expansion tree 
span scheme by default, unless specified. 
We also evaluate the contributions of antecedent 
competitor-related information, predicate-related 
information and pruning in our dynamic-expansion 
tree span scheme by excluding one of them from 
the dynamic-expansion scheme. Table 2 shows that 
1) antecedent competitor-related information con-
tributes much to our scheme; 2) predicate-related 
information contributes moderately; 3) pruning 
only has slight contribution. This suggests the im-
portance of including the competition in the tree 
span and the effect of predicate-argument struc-
tures in pronoun resolution. This also suggests that 
our scheme can well make use of such predicate- 
and antecedent competitor-related information.  
Dynamic Expansion Effect 
- Competitors-related Info 81.1(-1.9) 
- Predicates-related Info 82.2 (-0.8) 
- Pruning 82.8(-0.2)  
All 83.0 
Table 2: Contributions of different factors in our 
dynamic-expansion tree span scheme 
Table 3 compares the performance of different 
tree span schemes for pronouns with antecedents in 
different sentences apart. It shows that our dy-
namic-expansion scheme is much more robust than 
other schemes with the increase of sentences apart. 
Scheme /  
#Sentences Apart 
0 1 2 
Min 86.3 76.7 39.6 
Simple 86.8 77.9 43.8 
Full 86.6 77.4 35.4 
Dynamic 87.6 78.8 54.2 
Table 3: Comparison of tree span schemes with 
antecedents in different sentences apart 
5 Conclusion 
Syntactic structured information holds great poten-
tial in many NLP applications. The purpose of this 
paper is to well capture syntactic structured infor-
mation in pronoun resolution. In this paper, we 
proposes a context-sensitive convolution tree ker-
nel to resolve two critical problems in previous 
researches in pronoun resolution by first automati-
cally determining a dynamic-expansion tree span, 
which effectively covers structured information in 
the parse trees by taking predicate- and antecedent 
competitor-related information into consideration, 
and then applying a context-sensitive convolution 
tree kernel, which enumerates both context-free 
sub-trees and context-sensitive sub-trees. Evalua-
tion on the ACE 2003 corpus shows that our dy-
namic-expansion tree span scheme can better 
capture necessary structured information than the 
existing tree span schemes and our tree kernel can 
better model structured information than the state-
of-the-art Collins and Duffy?s kernel.  
For the future work, we will focus on improving 
the context-sensitive convolution tree kernel by 
better modeling context-sensitive information and 
exploring new tree span schemes by better incor-
porating useful structured information. In the 
meanwhile, a more detailed quantitative evaluation 
and thorough qualitative error analysis will be per-
formed to gain more insights. 
Acknowledgement  
This research is supported by Project 60673041 
under the National Natural Science Foundation of 
China and Project 2006AA01Z147 under the ?863? 
National High-Tech Research and Development of 
China. 
30
References  
Aone C and Bennett W.W. (1995). Evaluating auto-
mated and manual acquisition of anaphora resolu-
tion strategies. ACL?1995:122-129. 
Bergsma S. and Lin D.K.(2006). Bootstrapping path-
based pronoun resolution. COLING-ACL?2006: 33-
40. 
Charniak E. (2001). Immediate-head Parsing for Lan-
guage Models. ACL?2001: 129-137. Toulouse, 
France 
Collins M. (1999) Head-driven statistical models for 
natural language parsing. Ph.D. Thesis. University 
of Pennsylvania. 
Collins M. and Duffy N. (2001). Convolution Ker-
nels for Natural Language. NIPS?2001: 625-632. 
Cambridge, MA 
Culotta A. and Sorensen J. (2004). Dependency tree 
kernels for relation extraction. ACL?2004. 423-429. 
21-26 July 2004. Barcelona, Spain. 
Haussler D. (1999). Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, Uni-
versity of California, Santa Cruz. 
Hobbs J. (1978). Resolving pronoun references. Lin-
gua. 44:339-352. 
Joachims T. (1998). Text Categorization with Sup-
port Vector Machine: learning with many relevant 
features. ECML-1998: 137-142.  Chemnitz, Ger-
many 
Lappin S. and Leass H. (1994). An algorithm for pro-
nominal anaphora resolution. Computational Lin-
guistics. 20(4):526-561. 
Mitkov R. (1998). Robust pronoun resolution with 
limited knowledge. COLING-ACL?1998:869-875. 
Montreal, Canada.  
Moschitti A. (2004). A study on convolution kernels 
for shallow semantic parsing. ACL?2004:335-342. 
Pradhan S., Hacioglu K., Krugler V., Ward W., Mar-
tin J.H. and Jurafsky D. (2005). Support Vector 
Learning for Semantic Argument Classification. 
Machine Learning. 60(1):11-39. 
Soon W. Ng H.T.and Lim D. (2001). A machine 
learning approach to creference resolution of noun 
phrases. Computational Linguistics. 27(4): 521-544. 
Yang X.F., Zhou G.D., Su J. and Tan C.L., Corefer-
ence Resolution Using Competition Learning Ap-
proach, ACL?2003):176-183. Sapporo, Japan, 7-12 
July 2003. 
Yang X.F., Su J. and Tan C.L. (2006). Kernel-based 
pronoun resolution with structured syntactic knowl-
edge. COLING-ACL?2006: 41-48. 
Zelenko D., Aone C. and Richardella. (2003). Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 3(Feb):1083-1106. 
Zhang M., Zhang J., Su J. and Zhou G.D. (2006). A 
Composite Kernel to Extract Relations between En-
tities with both Flat and Structured Features. 
COLING-ACL-2006: 825-832. Sydney, Australia 
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). Ex-
ploring various knowledge in relation extraction. 
ACL?2005. 427-434. 25-30 June, Ann Arbor, Mich-
gan, USA. 
Zhou G.D., Zhang M., Ji D.H. and Zhu Q.M. (2007). 
Tree Kernel-based Relation Extraction with Con-
text-Sensitive Structured Parse Tree Information. 
EMNLP-CoNLL?2007 
31
Semi-Supervised Learning for Relation Extraction  
 
ZHOU GuoDong    LI JunHui    QIAN LongHua    ZHU Qiaoming 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow Univ., Suzhou, China 215006 
Email : {gdzhou, lijunhui, qianlonghua, qmzhu}@suda.edu.cn 
 
Abstract 
This paper proposes a semi-supervised learn-
ing method for relation extraction. Given a 
small amount of labeled data and a large 
amount of unlabeled data, it first bootstraps a 
moderate number of weighted support vectors 
via SVM through a co-training procedure with 
random feature projection and then applies a 
label propagation (LP) algorithm via the boot-
strapped support vectors. Evaluation on the 
ACE RDC 2003 corpus shows that our method 
outperforms the normal LP algorithm via all 
the available labeled data without SVM boot-
strapping. Moreover, our method can largely 
reduce the computational burden. This sug-
gests that our proposed method can integrate 
the advantages of both SVM bootstrapping 
and label propagation.  
1 Introduction 
Relation extraction is to detect and classify various 
predefined semantic relations between two entities 
from text and can be very useful in many NLP ap-
plications such as question answering, e.g. to an-
swer the query ?Who is the president of the United 
States??, and information retrieval, e.g. to expand 
the query ?George W. Bush? with ?the president of 
the United States? via his relationship with ?the 
United States?. 
During the last decade, many methods have 
been proposed in relation extraction, such as su-
pervised learning (Miller et al2000; Zelenko et al
2003; Culota and Sorensen 2004; Zhao and Grish-
man 2005; Zhang et al2006; Zhou et al2005, 
2006), semi-supervised learning (Brin 1998; 
Agichtein and Gravano 2000; Zhang 2004; Chen et 
al 2006), and unsupervised learning (Hasegawa et 
al 2004; Zhang et al2005). Among these methods, 
supervised learning-based methods perform much 
better than the other two alternatives. However, 
their performance much depends on the availability 
of a large amount of manually labeled data and it is 
normally difficult to adapt an existing system to 
other applications and domains. On the other hand, 
unsupervised learning-based methods do not need 
the definition of relation types and the availability 
of manually labeled data. However, they fail to 
classify exact relation types between two entities 
and their performance is normally very low. To 
achieve better portability and balance between hu-
man efforts and performance, semi-supervised 
learning has drawn more and more attention re-
cently in relation extraction and other NLP appli-
cations. 
This paper proposes a semi-supervised learning 
method for relation extraction. Given a small 
amount of labeled data and a large amount of unla-
beled data, our proposed method first bootstraps a 
moderate number of weighted support vectors from 
all the available data via SVM using a co-training 
procedure with random feature projection and then 
applies a label propagation (LP) algorithm to cap-
ture the manifold structure in both the labeled and 
unlabeled data via the bootstrapped support vectors. 
Compared with previous methods, our method can 
integrate the advantages of both SVM bootstrap-
ping in learning critical instances for the labeling 
function and label propagation in capturing the 
manifold structure in both the labeled and unla-
beled data to smooth the labeling function. 
The rest of this paper is as follows. In Section 2, 
we review related semi-supervised learning work 
in relation extraction. Then, the LP algorithm via 
bootstrapped support vectors is proposed in Sec-
tion 3 while Section 4 shows the experimental re-
sults. Finally, we conclude our work in Section 5.  
2 Related Work 
Generally, supervised learning is preferable to un-
supervised learning due to prior knowledge in the 
32
annotated training data and better performance. 
However, the annotated data is usually expensive 
to obtain. Hence, there has been growing interest in 
semi-supervised learning, aiming at inducing clas-
sifiers by leveraging a small amount of labeled 
data and a large amount of unlabeled data. Related 
work in relation extraction using semi-supervised 
learning can be classified into two categories: 
bootstrapping-based (Brin 1998; Agichtein and 
Gravano 2000; Zhang 2004) and label propaga-
tion(LP)-based (Chen et al2006).  
Currently, bootstrapping-based methods domi-
nate semi-supervised learning in relation extraction. 
Bootstrapping works by iteratively classifying 
unlabeled instances and adding confidently classi-
fied ones into labeled data using a model learned 
from augmented labeled data in previous iteration. 
Brin (1998) proposed a bootstrapping-based 
method on the top of a self-developed pattern 
matching-based classifier to exploit the duality 
between patterns and relations. Agichtein and Gra-
vano (2000) shared much in common with Brin 
(1998). They employed an existing pattern match-
ing-based classifier (i.e. SNoW) instead. Zhang 
(2004) approached the much simpler relation clas-
sification sub-task by bootstrapping on the top of 
SVM. Although bootstrapping-based methods have 
achieved certain success, one problem is that they 
may not be able to well capture the manifold struc-
ture among unlabeled data. 
As an alternative to the bootstrapping-based 
methods, Chen et al(2006) employed a LP-based 
method in relation extraction. Compared with 
bootstrapping, the LP algorithm can effectively 
combine labeled data with unlabeled data in the 
learning process by exploiting the manifold struc-
ture (e.g. the natural clustering structure) in both 
the labeled and unlabeled data. The rationale be-
hind this algorithm is that the instances in high-
density areas tend to carry the same labels. The LP 
algorithm has also been successfully applied in 
other NLP applications, such as word sense disam-
biguation (Niu et al2005), text classification 
(Szummer and Jaakkola 2001; Blum and Chawla 
2001; Belkin and Niyogi 2002; Zhu and Ghahra-
mani 2002; Zhu et al2003; Blum et al2004), and 
information retrieval (Yang et al2006). However, 
one problem is its computational burden, espe-
cially when a large amount of labeled and unla-
beled data is taken into consideration. 
In order to take the advantages of both boot-
strapping and label propagation, our proposed 
method propagates labels via bootstrapped support 
vectors. On the one hand, our method can well 
capture the manifold structure in both the labeled 
and unlabeled data. On the other hand, our method 
can largely reduce the computational burden in the 
normal LP algorithm via all the available data. 
3 Label Propagation via Bootstrapped 
Support Vectors 
The idea behind our LP algorithm via bootstrapped 
support vectors is that, instead of propagating la-
bels through all the available labeled data, our 
method propagates labels through critical instances 
in both the labeled and unlabeled data. In this pa-
per, we use SVM as the underlying classifier to 
bootstrap a moderate number of weighted support 
vectors for this purpose. This is based on an as-
sumption that the manifold structure in both the 
labeled and unlabeled data can be well preserved 
through the critical instances (i.e. the weighted 
support vectors bootstrapped from all the available 
labeled and unlabeled data). The reason why we 
choose SVM is that it represents the state-of-the-
art in machine learning research and there are good 
implementations of the algorithm available. In par-
ticular, SVMLight (Joachims 1998) is selected as 
our classifier. For efficiency, we apply the one vs. 
others strategy, which builds K classifiers so as to 
separate one class from all others. Another reason 
is that we can adopt the weighted support vectors 
returned by the bootstrapped SVMs as the critical 
instances, via which label propagation is done.  
3.1 Bootstrapping Support Vectors 
This paper modifies the SVM bootstrapping algo-
rithm BootProject(Zhang 2004) to bootstrap sup-
port vectors. Given a small amount of labeled data 
and a large amount of unlabeled data, the modified 
BootProject algorithm bootstraps on the top of  
SVM by iteratively classifying  unlabeled  in-
stances  and moving   confidently  classified  ones  
into  labeled data using a model learned from the 
augmented labeled data in previous  iteration,  until 
not enough unlabeled instances can be classified 
confidently. Figure 1 shows the modified BootPro-
ject algorithm for bootstrapping support vectors.  
 
33
_________________________________________ 
Assume: 
L :  the labeled data; 
U :  the unlabeled data; 
S :  the batch size (100 in our experiments); 
P :  the number of views(feature projections); 
r :   the number of classes (including all the rela-
tion (sub)types and the non-relation)  
 
BEGIN 
REPEAT 
FOR i = 1 to P DO 
Generate projected feature space iF  from 
the original feature space F ; 
Project both L  and U  onto iF , thus gener-
ate iL  and iU ; 
Train SVM classifier ijSVM  on iL  for each 
class )1( rjr j K= ; 
Run ijSVM  on iU  for each class 
)1( rjr j K=  
END FOR 
Find (at most) S instances in U  with the 
highest agreement (with threshold 70% in 
our experiments) and the highest average 
SVM-returned confidence value (with 
threshold 1.0 in our experiments); 
Move them from U to L; 
UNTIL not enough unlabeled instances (less 
than 10 in our experiments) can be confidently 
classified; 
Return all the (positive and negative) support 
vectors  included in all the latest SVM classifi-
ers ijSVM  with their collective weight (abso-
lute alpha*y) information as the set of 
bootstrapped support vectors to act as the la-
beled data in the LP algorithm; 
Return U (those hard cases which can not be 
confidently classified) to act as the unlabeled 
data in the LP algorithm; 
END 
_________________________________________ 
Figure 1: The algorithm  
for bootstrapping support vectors 
 
In particular, this algorithm generates multiple 
overlapping ?views? by projecting from the origi-
nal feature space. In this paper, feature views with 
random feature projection, as proposed in Zhang 
(2004), are explored. Section 4 will discuss this 
issue in more details. During the iterative training 
process, classifiers trained on the augmented la-
beled data using the projected views are then asked 
to vote on the remaining unlabeled instances and 
those with the highest probability of being cor-
rectly labeled are chosen to augment the labeled 
data.  
During the bootstrapping process, the support 
vectors included in all the trained SVM classifiers 
(for all the relation (sub)types and the non-relation) 
are bootstrapped (i.e. updated) at each iteration. 
When the bootstrapping process stops, all the 
(positive and negative) support vectors included in 
the SVM classifiers are returned as bootstrapped 
support vectors with their collective weights (abso-
lute a*y) to act as the labeled data in the LP algo-
rithm and all the remaining unlabeled instances (i.e. 
those hard cases which can not be confidently clas-
sified in the bootstrapping process) in the unla-
beled data are returned to act as the unlabeled data 
in the LP algorithm. Through SVM bootstrapping, 
our LP algorithm will only depend on the critical 
instances (i.e. support vectors with their weight 
information bootstrapped from all the available 
labeled and unlabeled data) and those hard in-
stances, instead of all the available labeled and 
unlabeled data.  
3.2 Label Propagation 
In the LP algorithm (Zhu and Ghahramani 2002), 
the manifold structure in data is represented as a 
connected graph. Given the labeled data (the above 
bootstrapped support vectors with their weights) 
and unlabeled data (the remaining hard instances in 
the unlabeled data after bootstrapping, including 
all the test instances for evaluation), the LP algo-
rithm first represents labeled and unlabeled in-
stances as vertices in a connected graph, then 
propagates the label information from any vertex 
to nearby vertex through weighted edges and fi-
nally infers the labels of unlabeled instances until a 
global stable stage is achieved. Figure 2 presents 
the label propagation algorithm on bootstrapped 
support vectors in details. 
 
34
_________________________________________
Assume:  
Y : the rn * labeling matrix, where ijy  repre-
sents the probability of vertex )1( nixi K=  
with label )1( rjr j K=  (including the non-
relation label); 
LY : the top l  rows of 
0Y . LY corresponds to the 
l  labeled instances; 
UY : the bottom u  rows of 
0Y . UY corresponds 
to the u  unlabeled instances; 
T : a nn *  matrix, with ijt  is the probability 
jumping from vertex ix to vertex jx ; 
 
BEGIN (the algorithm) 
Initialization:  
1) Set the iteration index 0=t ;  
2) Let 0Y  be the initial soft labels attached to 
each vertex;  
3) Let 0LY  be consistent with the labeling in 
the labeled (including all the relation 
(sub)types and the non-relation) data, where 
0
ijy = the weight of the bootstrapped support 
vector if ix  has label jr  (Please note that 
jr  can be the non-relation label) and 0 oth-
erwise;  
4) Initialize 0UY ; 
REPEAT 
Propagate the labels of any vertex to nearby 
vertices by tt YTY =+1 ; 
Clamp the labeled data, that is, replace 1+tLY  
with 0LY ; 
UNTIL Y converges(e.g. 1+tLY  converges to 
0
LY ); 
Assign each unlabeled instance with a label: for 
)( nilxi ?p , find its label with 
j
ijymaxarg ; 
END (the algorithm) 
_________________________________________ 
Figure 2: The LP algorithm 
 
 
Here, each vertex corresponds to an instance, 
and the edge between any two instances ix  and jx  
is weighted by ijw  to measure their similarity. In 
principle, larger edge weights allow labels to travel 
through easier. Thus the closer the instances are, 
the more likely they have similar labels. The algo-
rithm first calculates the weight ijw  using a kernel, 
then transforms it to ?
=
=?=
n
k
kjijij wwijpt
1
/)( , 
which measures the probability of propagating a 
label from instance jx to instance ix , and finally 
normalizes ijt row by row using ?
=
=
n
k
ikijij ttt
1
/  to 
maintain the class probability interpretation of the 
labeling matrix Y .  
During the label propagation process, the label 
distribution of the labeled data is clamped in each 
loop using the weights of the bootstrapped support 
vectors and acts like forces to push out labels 
through the unlabeled data. With this push origi-
nates from the labeled data, the label boundaries 
will be pushed much faster along edges with larger 
weights and settle in gaps along those with lower 
weights. Ideally, we can expect that ijw  across 
different classes should be as small as possible and 
ijw  within the same class as big as possible. In this 
way, label propagation happens within the same 
class most likely. 
This algorithm has been shown to converge to 
a unique solution (Zhu and Ghahramani 2002), 
which can be obtained without iteration in theory, 
and the initialization of YU0 (the unlabeled data) is 
not important since YU0 does not affect its estima-
tion. However, proper initialization of YU0 actually 
helps the algorithm converge more rapidly in prac-
tice. In this paper, each row in YU0 is initialized to 
the average similarity with the labeled instances. 
4 Experimentation 
This paper uses the ACE RDC 2003 corpus pro-
vided by LDC for evaluation. This corpus is gath-
ered from various newspapers, newswires and 
broadcasts.  
 
35
Method 
LP via bootstrapped 
(weighted) SVs 
LP via bootstrapped  
(un-weighted) SVs 
LP w/o SVM  
bootstrapping 
SVM 
(BootProject) SVM  
Bootstrapping 
5% 46.5 (+1.4) 44.5 (+1.7) 43.1 (+1.0) 35.4 (-) 40.6 (+0.9) 
10% 48.6 (+1.7) 46.5 (+2.1) 45.2 (+1.5) 38.6 (-) 43.1 (+1.4) 
25% 51.7 (+1.9) 50.4 (+2.3) 49.6 (+1.8) 43.9 (-) 47.8 (+1.7) 
50% 53.6 (+1.8) 52.6 (+2.2) 52.1 (+1.7) 47.2 (-) 50.5 (+1.6) 
75% 55.2 (+1.3) 54.5 (+1.8) 54.2 (+1.2) 53.1 (-) 53.9 (+1.2) 
100% 56.2 (+1.0) 55.8 (+1.3) 55.6 (+0.8) 55.5 (-) 55.8 (+0.7) 
Table 1: Comparison of different methods using a state-of-the-art linear kernel on the ACE RDC 2003 
corpus (The numbers inside the parentheses indicate the increases in F-measure if we add the ACE RDC 
2004 corpus as the unlabeled data) 
4.1 Experimental Setting 
In the ACE RDC 2003 corpus, the training data 
consists of 674 annotated text documents (~300k 
words) and 9683 instances of relations. During 
development, 155 of 674 documents in the training 
set are set aside for fine-tuning. The test set is held 
out only for final evaluation. It consists of 97 
documents (~50k words) and 1386 instances of 
relations. The ACE RDC 2003 task defines 5 rela-
tion types and 24 subtypes between 5 entity types, 
i.e. person, organization, location, facility and GPE. 
All the evaluations are measured on the 24 sub-
types including relation identification and classifi-
cation. 
In all our experiments, we iterate over all pairs 
of entity mentions occurring in the same sentence 
to generate potential relation instances1. For better 
evaluation, we have adopted a state-of-the-art lin-
ear kernel as similarity measurements. In our linear 
kernel, we apply the same feature set as described 
in a state-of-the-art feature-based system (Zhou et 
al 2005): word, entity type, mention level, overlap, 
base phrase chunking, dependency tree, parse tree 
and semantic information. Given above various 
lexical, syntactic and semantic features, multiple 
overlapping feature views are generated in the 
bootstrapping process using random feature projec-
tion (Zhang 2004). For each feature projection in 
bootstrapping support vectors, a feature is ran-
domly selected with probability p and therefore the 
eventually projected feature space has p*F features 
                                                           
1  In this paper, we only measure the performance of 
relation extraction on ?true? mentions with ?true? 
chaining of co-reference (i.e. as annotated by the cor-
pus annotators) in the ACE corpora. We also explic-
itly model the argument order of the two mentions 
involved and only model explicit relations because of 
poor inter-annotator agreement in the annotation of 
implicit relations and their limited number. 
on average, where F is the size of the original fea-
ture space. In this paper, p and the number of dif-
ferent views are fine-tuned to 0.5 and 10 2 
respectively using 5-fold cross validation on the 
training data of the ACE RDC 2003 corpus. 
4.2 Experimental Results 
Table 1 presents the F-measures 3  (the numbers 
outside the parentheses) of our algorithm using the 
state-of-the-art linear kernel on different sizes of 
the ACE RDC training data with all the remaining 
training data and the test data4  as the unlabeled 
data on the ACE RDC 2003 corpus. In this paper, 
we only report the performance (averaged over 5 
trials) with the percentages of 5%, 10%, 25%, 50%, 
75% and 100%5. For example, our LP algorithm 
via bootstrapped (weighted) support vectors 
achieves the F-measure of 46.5 if using only 5% of 
the ACE RDC 2003 training data as the labeled 
data and the remaining training data and the test 
data in this corpus as the unlabeled data. Table 1 
                                                           
2 This suggests that the modified BootProject algorithm 
in the bootstrapping phase outperforms the SelfBoot 
algorithm (with p=1.0 and m=1) which uses all the 
features as the only view. In the related NLP literature, 
co-training has also shown to typically outperform 
self-bootstrapping. 
3 Our experimentation also shows that most of perform-
ance improvement with either bootstrapping or label 
propagation comes from gain in recall. Due to space 
limitation, this paper only reports the overall F-
measure. 
4  In our label propagation algorithm via bootstrapped 
support vectors, the test data is only included in the 
second phase (i.e. the label propagation phase) and not 
used in the first phase (i.e. bootstrapping support vec-
tors). This is to fairly compare different semi-
supervised learning methods. 
5 We have tried less percentage than 5%. However, our 
experiments show that using much less data will suffer 
from performance un-stability. Therefore, we only re-
port the performance with percentage not less than 5%. 
36
also compares our method with SVM and the 
original SVM bootstrapping algorithm BootPro-
ject(i.e. bootstrapping on the top of SVM with fea-
ture projection, as proposed in Zhang (2004)). 
Finally, Table 1 compares our LP algorithm via 
bootstrapped (weighted by default) support vectors 
with other possibilities, such as the scheme via 
bootstrapped (un-weighted, i.e. the importance of 
support vectors is not differentiated) support vec-
tors and the scheme via all the available labeled 
data (i.e. without SVM bootstrapping). Table 1 
shows that: 
1) Inclusion of unlabeled data using semi-
supervised learning, including the SVM boot-
strapping algorithm BootProject, the normal 
LP algorithm via all the available labeled and 
unlabeled data without SVM bootstrapping, 
and our LP algorithms via bootstrapped (either 
weighted or un-weighted) support vectors, 
consistently improves the performance, al-
though semi-supervised learning has shown to 
typically decrease the performance when a lot 
of (enough) labeled data is available (Nigam 
2001).  This may be due to the insufficiency of 
labeled data in the ACE RDC 2003 corpus. 
Actually, most of relation subtypes in the two 
corpora much suffer from the data sparseness 
problem (Zhou et al2006).  
2) All the three LP algorithms outperform the 
state-of-the-art SVM classifier and the SVM 
bootstrapping algorithm BootProject. Espe-
cially, when a small amount of labeled data is 
available, the performance improvements by 
the LP algorithms are significant. This indi-
cates the usefulness of the manifold structure 
in both labeled and unlabeled data and the 
powerfulness of the LP algorithm in modeling 
such information.  
3) Our LP algorithms via bootstrapped (either 
weighted or un-weighted) support vectors out-
performs the normal LP algorithm via all the 
available labeled data w/o SVM bootstrapping. 
For example, our LP algorithm via boot-
strapped (weighted) support vectors outper-
forms the normal LP algorithm from 0.6 to 3.4 
in F-measure on the ACE RDC 2003 corpus 
respectively when the labeled data ranges from 
100% to 5%. This suggests that the manifold 
structure in both the labeled and unlabeled data 
can be well preserved via bootstrapped support 
vectors, especially when only a small amount 
of labeled data is available. This implies that 
weighted support vectors may represent the 
manifold structure (e.g. the decision boundary 
from where label propagation is done) better 
than the full set of data ? an interesting result 
worthy more quantitative and qualitative justi-
fication in the future work.   
4) Our LP algorithms via bootstrapped (weighted) 
support vectors perform better than LP algo-
rithms via bootstrapped (un-weighted) support 
vectors by ~1.0 in F-measure on average. This 
suggests that bootstrapped support vectors with 
their weights can better represent the manifold 
structure in all the available labeled and unla-
beled data than bootstrapped support vectors 
without their weights. 
5) Comparison of SVM, SVM bootstrapping and 
label propagation with bootstrapped (weighted) 
support vectors shows that both bootstrapping 
and label propagation contribute much to the 
performance improvement. 
Table 1 also shows the increases in F-measure 
(the numbers inside the parentheses) if we add all 
the instances in the ACE RDC 20046 corpus into 
the ACE RDC 2003 corpus in consideration as 
unlabeled data in all the four semi-supervised 
learning methods. It shows that adding more unla-
beled data can consistently improve the perform-
ance. For example, compared with using only 5% 
of the ACE RDC 2003 training data as the labeled 
data and the remaining training data and the test 
data in this corpus as the unlabeled data, including 
the ACE RDC 2004 corpus as the unlabeled data 
increases the F-measures of 1.4 and 1.0 in our LP 
algorithm and the normal LP algorithm respec-
tively. Table 1 shows that the contribution grows 
first when the labeled data begins to increase and 
reaches a maximum of ~2.0 in F-measure at a cer-
tain point. 
Finally, it is found in our experiments that 
critical and hard instances normally occupy only 
15~20% (~18% on average) of all the available 
labeled and unlabeled data. This suggests that, 
through bootstrapped support vectors, our LP algo-
                                                           
6  Compared with the ACE RDC 2003 task, the ACE 
RDC 2004 task defines two more entity types, i.e. 
weapon and vehicle, much more entity subtypes, and 
different 7 relation types and 23 subtypes between 7 
entity types. The ACE RDC 2004 corpus from LDC 
contains 451 documents and 5702 relation instances. 
37
rithm can largely reduce the computational burden 
since it only depends on the critical instances (i.e. 
bootstrapped support vectors with their weights) 
and those hard instances.   
5 Conclusion 
This paper proposes a new effective and efficient 
semi-supervised learning method in relation ex-
traction. First, a moderate number of weighted 
support vectors are bootstrapped from all the avail-
able labeled and unlabeled data via SVM through a 
co-training procedure with feature projection. Here, 
a random feature projection technique is used to 
generate multiple overlapping feature views in 
bootstrapping using a state-of-the-art linear kernel. 
Then, a LP algorithm is applied to propagate labels 
via the bootstrapped support vectors, which, to-
gether with those hard unlabeled instances and the 
test instances, are represented as vertices in a con-
nected graph. During the classification process, the 
label information is propagated from any vertex to 
nearby vertex through weighted edges and finally 
the labels of unlabeled instances are inferred until a 
global stable stage is achieved.  In this way, the 
manifold structure in both the labeled and unla-
beled data can be well captured by label propaga-
tion via bootstrapped support vectors. Evaluation 
on the ACE RDC 2004 corpus suggests that our LP 
algorithm via bootstrapped support vectors can 
take the advantages of both SVM bootstrapping 
and label propagation.  
For the future work, we will systematically 
evaluate our proposed method on more corpora 
and explore better metrics of measuring the simi-
larity between two instances. 
Acknowledgement  
This research is supported by Project 60673041 
under the National Natural Science Foundation of 
China and Project 2006AA01Z147 under the ?863? 
National High-Tech Research and Development of 
China. 
References  
ACE. (2000-2005). Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/  
Agichtein E. and Gravano L. (2000). Snowball: 
Extracting relations from large plain-text collec-
tions. Proceedings of the 5th ACM International 
Conference on Digital Libraries 
(ACMDL?2000). 
Belkin, M. and Niyogi, P. (2002). Using Manifold 
Structure for Partially Labeled Classification. 
NIPS 15. 
Blum A. and Chawla S. (2001). Learning from la-
beled and unlabeled data using graph mincuts. 
ICML?2001. 
Blum A., Lafferty J., Rwebangira R and Reddy R. 
(2004). Semi-supervised learning using random-
ized mincuts. ICML?2004. 
Brin S. (1998). Extracting patterns and relations 
from world wide web. Proceedings of WebDB 
Workshop at 6th International Conference on 
Extending Database Technology:172-183. 
Charniak E. (2001). Immediate-head Parsing for 
Language Models. ACL?2001: 129-137. Tou-
louse, France 
Chen J.X., Ji D.H., Tan C.L. and Niu Z.Y. (2006). 
Relation extraction using label propagation 
based semi-supervised learning. COLING-
ACL?2006: 129-136. July 2006. Sydney, Austra-
lia. 
Culotta A. and Sorensen J. (2004). Dependency 
tree kernels for relation extraction. ACL?2004. 
423-429. 21-26 July 2004. Barcelona, Spain. 
Hasegawa T., Sekine S. and Grishman R. (2004). 
Discovering relations among named entities 
form large corpora. ACL?2004. Barcelona, Spain. 
Miller S., Fox H., Ramshaw L. and Weischedel R. 
(2000). A novel use of statistical parsing to ex-
tract information from text. ANLP?2000. 226-
233. 29 April  - 4 May 2000, Seattle, USA 
Moschitti A. (2004). A study on convolution ker-
nels for shallow semantic parsing. 
ACL?2004:335-342. 
Nigam K.P. (2001). Using unlabeled data to im-
prove text classification. Technical Report 
CMU-CS-01-126. 
Niu Z.Y., Ji D.H., and Tan C.L. (2005). Word 
Sense Disambiguation Using Label Propagation 
Based Semi-supervised Learning. 
ACL?2005:395-402., Ann Arbor, Michigan, 
USA. 
Szummer, M., & Jaakkola, T. (2001). Partially La-
beled Classification with Markov Random 
Walks. NIPS 14. 
38
Yang L.P., Ji D.H., Zhou G.D. and Nie Y. (2006). 
Document Re-ranking using cluster validation 
and label propagation. CIKM?2006. 5-11 Nov 
2006. Arlington, Virginia, USA. 
Zelenko D., Aone C. and Richardella. (2003). Ker-
nel methods for relation extraction. Journal of 
Machine Learning Research. 3(Feb):1083-1106. 
Zhang M., Su J., Wang D.M., Zhou G.D. and Tan 
C.L. (2005). Discovering Relations from a 
Large Raw Corpus Using Tree Similarity-based 
Clustering, IJCNLP?2005, Lecture Notes in Arti-
ficial Intelligence (LNAI 3651). 378-389. 
Zhang M., Zhang J., Su J. and Zhou G.D. (2006). 
A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured 
Features. COLING-ACL-2006: 825-832. Sydney, 
Australia 
Zhang Z. (2004). Weakly supervised relation clas-
sification for information extraction. 
CIKM?2004. 8-13 Nov 2004. Washington D.C. 
USA. 
Zhao S.B. and Grishman R. (2005). Extracting re-
lations with integrated information using kernel 
methods. ACL?2005: 419-426. Univ of Michi-
gan-Ann Arbor,  USA,  25-30 June 2005. 
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). 
Exploring various knowledge in relation extrac-
tion. ACL?2005. 427-434. 25-30 June, Ann Ar-
bor, Michgan, USA.  
Zhou G.D., Su J. and Zhang M. (2006). Modeling 
commonality among related classes in relation 
extraction, COLING-ACL?2006: 121-128. Syd-
ney, Australia. 
Zhu, X. and Ghahramani, Z. (2002). Learning from 
Labeled and Unlabeled Data with Label 
Propagation. CMU CALD Technical Report. 
CMU-CALD-02-107. 
Zhu, X., Ghahramani, Z. and Lafferty, J. (2003). 
Semi-Supervised Learning Using Gaussian 
Fields and Harmonic Functions. ICML?2003. 
39
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55?63,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Cross Language Dependency Parsing using a Bilingual Lexicon?
Hai Zhao(??)??, Yan Song(??)?, Chunyu Kit?, Guodong Zhou?
?Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
?School of Computer Science and Technology
Soochow University, Suzhou, China 215006
{haizhao,yansong,ctckit}@cityu.edu.hk, gdzhou@suda.edu.cn
Abstract
This paper proposes an approach to en-
hance dependency parsing in a language
by using a translated treebank from an-
other language. A simple statistical ma-
chine translation method, word-by-word
decoding, where not a parallel corpus but
a bilingual lexicon is necessary, is adopted
for the treebank translation. Using an en-
semble method, the key information ex-
tracted from word pairs with dependency
relations in the translated text is effectively
integrated into the parser for the target lan-
guage. The proposed method is evaluated
in English and Chinese treebanks. It is
shown that a translated English treebank
helps a Chinese parser obtain a state-of-
the-art result.
1 Introduction
Although supervised learning methods bring state-
of-the-art outcome for dependency parser infer-
ring (McDonald et al, 2005; Hall et al, 2007), a
large enough data set is often required for specific
parsing accuracy according to this type of meth-
ods. However, to annotate syntactic structure, ei-
ther phrase- or dependency-based, is a costly job.
Until now, the largest treebanks1 in various lan-
guages for syntax learning are with around one
million words (or some other similar units). Lim-
ited data stand in the way of further performance
enhancement. This is the case for each individual
language at least. But, this is not the case as we
observe all treebanks in different languages as a
whole. For example, of ten treebanks for CoNLL-
2007 shared task, none includes more than 500K
?The study is partially supported by City University of
Hong Kong through the Strategic Research Grant 7002037
and 7002388. The first author is sponsored by a research fel-
lowship from CTL, City University of Hong Kong.
1It is a tradition to call an annotated syntactic corpus as
treebank in parsing community.
tokens, while the sum of tokens from all treebanks
is about two million (Nivre et al, 2007).
As different human languages or treebanks
should share something common, this makes it
possible to let dependency parsing in multiple lan-
guages be beneficial with each other. In this pa-
per, we study how to improve dependency parsing
by using (automatically) translated texts attached
with transformed dependency information. As a
case study, we consider how to enhance a Chinese
dependency parser by using a translated English
treebank. What our method relies on is not the
close relation of the chosen language pair but the
similarity of two treebanks, this is the most differ-
ent from the previous work.
Two main obstacles are supposed to confront in
a cross-language dependency parsing task. The
first is the cost of translation. Machine translation
has been shown one of the most expensive lan-
guage processing tasks, as a great deal of time and
space is required to perform this task. In addition,
a standard statistical machine translation method
based on a parallel corpus will not work effec-
tively if it is not able to find a parallel corpus that
right covers source and target treebanks. How-
ever, dependency parsing focuses on the relations
of word pairs, this allows us to use a dictionary-
based translation without assuming a parallel cor-
pus available, and the training stage of translation
may be ignored and the decoding will be quite fast
in this case. The second difficulty is that the out-
puts of translation are hardly qualified for the pars-
ing purpose. The most challenge in this aspect is
morphological preprocessing. We regard that the
morphological issue should be handled aiming at
the specific language, our solution here is to use
character-level features for a target language like
Chinese.
The rest of the paper is organized as follows.
The next section presents some related existing
work. Section 3 describes the procedure on tree-
55
bank translation and dependency transformation.
Section 4 describes a dependency parser for Chi-
nese as a baseline. Section 5 describes how a
parser can be strengthened from the translated
treebank. The experimental results are reported in
Section 6. Section 7 looks into a few issues con-
cerning the conditions that the proposed approach
is suitable for. Section 8 concludes the paper.
2 The Related Work
As this work is about exploiting extra resources to
enhance an existing parser, it is related to domain
adaption for parsing that has been draw some in-
terests in recent years. Typical domain adaptation
tasks often assume annotated data in new domain
absent or insufficient and a large scale unlabeled
data available. As unlabeled data are concerned,
semi-supervised or unsupervised methods will be
naturally adopted. In previous works, two basic
types of methods can be identified to enhance an
existing parser from additional resources. The first
is usually focus on exploiting automatic generated
labeled data from the unlabeled data (Steedman
et al, 2003; McClosky et al, 2006; Reichart and
Rappoport, 2007; Sagae and Tsujii, 2007; Chen
et al, 2008), the second is on combining super-
vised and unsupervised methods, and only unla-
beled data are considered (Smith and Eisner, 2006;
Wang and Schuurmans, 2008; Koo et al, 2008).
Our purpose in this study is to obtain a further
performance enhancement by exploiting treebanks
in other languages. This is similar to the above
first type of methods, some assistant data should
be automatically generated for the subsequent pro-
cessing. The differences are what type of data are
concerned with and how they are produced. In our
method, a machine translation method is applied
to tackle golden-standard treebank, while all the
previous works focus on the unlabeled data.
Although cross-language technique has been
used in other natural language processing tasks,
it is basically new for syntactic parsing as few
works were concerned with this issue. The rea-
son is straightforward, syntactic structure is too
complicated to be properly translated and the cost
of translation cannot be afforded in many cases.
However, we empirically find this difficulty may
be dramatically alleviated as dependencies rather
than phrases are used for syntactic structure repre-
sentation. Even the translation outputs are not so
good as the expected, a dependency parser for the
target language can effectively make use of them
by only considering the most related information
extracted from the translated text.
The basic idea to support this work is to make
use of the semantic connection between different
languages. In this sense, it is related to the work of
(Merlo et al, 2002) and (Burkett and Klein, 2008).
The former showed that complementary informa-
tion about English verbs can be extracted from
their translations in a second language (Chinese)
and the use of multilingual features improves clas-
sification performance of the English verbs. The
latter iteratively trained a model to maximize the
marginal likelihood of tree pairs, with alignments
treated as latent variables, and then jointly parsing
bilingual sentences in a translation pair. The pro-
posed parser using features from monolingual and
mutual constraints helped its log-linear model to
achieve better performance for both monolingual
parsers and machine translation system. In this
work, cross-language features will be also adopted
as the latter work. However, although it is not es-
sentially different, we only focus on dependency
parsing itself, while the parsing scheme in (Bur-
kett and Klein, 2008) based on a constituent rep-
resentation.
Among of existing works that we are aware of,
we regard that the most similar one to ours is (Ze-
man and Resnik, 2008), who adapted a parser to a
new language that is much poorer in linguistic re-
sources than the source language. However, there
are two main differences between their work and
ours. The first is that they considered a pair of suf-
ficiently related languages, Danish and Swedish,
and made full use of the similar characteristics of
two languages. Here we consider two quite dif-
ferent languages, English and Chinese. As fewer
language properties are concerned, our approach
holds the more possibility to be extended to other
language pairs than theirs. The second is that a
parallel corpus is required for their work and a
strict statistical machine translation procedure was
performed, while our approach holds a merit of
simplicity as only a bilingual lexicon is required.
3 Treebank Translation and Dependency
Transformation
3.1 Data
As a case study, this work will be conducted be-
tween the source language, English, and the tar-
get language, Chinese, namely, we will investigate
56
how a translated English treebank enhances a Chi-
nese dependency parser.
For English data, the Penn Treebank (PTB) 3
is used. The constituency structures is converted
to dependency trees by using the same rules as
(Yamada and Matsumoto, 2003) and the standard
training/development/test split is used. However,
only training corpus (sections 2-21) is used for
this study. For Chinese data, the Chinese Treebank
(CTB) version 4.0 is used in our experiments. The
same rules for conversion and the same data split
is adopted as (Wang et al, 2007): files 1-270 and
400-931 as training, 271-300 as testing and files
301-325 as development. We use the gold stan-
dard segmentation and part-of-speech (POS) tags
in both treebanks.
As a bilingual lexicon is required for our task
and none of existing lexicons are suitable for trans-
lating PTB, two lexicons, LDC Chinese-English
Translation Lexicon Version 2.0 (LDC2002L27),
and an English to Chinese lexicon in StarDict2,
are conflated, with some necessary manual exten-
sions, to cover 99% words appearing in the PTB
(the most part of the untranslated words are named
entities.). This lexicon includes 123K entries.
3.2 Translation
A word-by-word statistical machine translation
strategy is adopted to translate words attached
with the respective dependency information from
the source language to the target one. In detail, a
word-based decoding is used, which adopts a log-
linear framework as in (Och and Ney, 2002) with
only two features, translation model and language
model,
P (c|e) = exp[
?2
i=1 ?ihi(c, e)]?
c exp[
?2
i=1 ?ihi(c, e)]
Where
h1(c, e) = log(p?(c|e))
is the translation model, which is converted from
the bilingual lexicon, and
h2(c, e) = log(p?(c))
is the language model, a word trigram model
trained from the CTB. In our experiment, we set
two weights ?1 = ?2 = 1.
2StarDict is an open source dictionary software, available
at http://stardict.sourceforge.net/.
The conversion process of the source treebank
is completed by three steps as the following:
1. Bind POS tag and dependency relation of a
word with itself;
2. Translate the PTB text into Chinese word by
word. Since we use a lexicon rather than a parallel
corpus to estimate the translation probabilities, we
simply assign uniform probabilities to all transla-
tion options. Thus the decoding process is actu-
ally only determined by the language model. Sim-
ilar to the ?bag translation? experiment in (Brown
et al, 1990), the candidate target sentences made
up by a sequence of the optional target words are
ranked by the trigram language model. The output
sentence will be generated only if it is with maxi-
mum probability as follows,
c = argmax{p?(c)p?(c|e)}
= argmax p?(c)
= argmax
?
p?(wc)
A beam search algorithm is used for this process
to find the best path from all the translation op-
tions; As the training stage, especially, the most
time-consuming alignment sub-stage, is skipped,
the translation only includes a decoding procedure
that takes about 4.5 hours for about one million
words of the PTB in a 2.8GHz PC.
3. After the target sentence is generated, the at-
tached POS tags and dependency information of
each English word will also be transferred to each
corresponding Chinese word. As word order is of-
ten changed after translation, the pointer of each
dependency relationship, represented by a serial
number, should be re-calculated.
Although we try to perform an exact word-by-
word translation, this aim cannot be fully reached
in fact, as the following case is frequently encoun-
tered, multiple English words have to be translated
into one Chinese word. To solve this problem,
we use a policy that lets the output Chinese word
only inherits the attached information of the high-
est syntactic head in the original multiple English
words.
4 Dependency Parsing: Baseline
4.1 Learning Model and Features
According to (McDonald and Nivre, 2007), all
data-driven models for dependency parsing that
have been proposed in recent years can be de-
scribed as either graph-based or transition-based.
57
Table 1: Feature Notations
Notation Meaning
s The word in the top of stack
s? The first word below the top of stack.
s?1,s1... The first word before(after) the word
in the top of stack.
i, i+1,... The first (second) word in the
unprocessed sequence, etc.
dir Dependent direction
h Head
lm Leftmost child
rm Rightmost child
rn Right nearest child
form word form
pos POS tag of word
cpos1 coarse POS: the first letter of POS tag of word
cpos2 coarse POS: the first two POS tags of word
lnverb the left nearest verb
char1 The first character of a word
char2 The first two characters of a word
char?1 The last character of a word
char?2 The last two characters of a word
. ?s, i.e., ?s.dprel? means dependent label
of character in the top of stack
+ Feature combination, i.e., ?s.char+i.char?
means both s.char and i.char work as a
feature function.
Although the former will be also used as compari-
son, the latter is chosen as the main parsing frame-
work by this study for the sake of efficiency. In de-
tail, a shift-reduce method is adopted as in (Nivre,
2003), where a classifier is used to make a parsing
decision step by step. In each step, the classifier
checks a word pair, namely, s, the top of a stack
that consists of the processed words, and, i, the
first word in the (input) unprocessed sequence, to
determine if a dependent relation should be estab-
lished between them. Besides two dependency arc
building actions, a shift action and a reduce ac-
tion are also defined to maintain the stack and the
unprocessed sequence. In this work, we adopt a
left-to-right arc-eager parsing model, that means
that the parser scans the input sequence from left
to right and right dependents are attached to their
heads as soon as possible (Hall et al, 2007).
While memory-based and margin-based learn-
ing approaches such as support vector machines
are popularly applied to shift-reduce parsing, we
apply maximum entropy model as the learning
model for efficient training and adopting over-
lapped features as our work in (Zhao and Kit,
2008), especially, those character-level ones for
Chinese parsing. Our implementation of maxi-
mum entropy adopts L-BFGS algorithm for pa-
rameter optimization as usual.
With notations defined in Table 1, a feature set
as shown in Table 2 is adopted. Here, we explain
some terms in Tables 1 and 2. We used a large
scale feature selection approach as in (Zhao et al,
2009) to obtain the feature set in Table 2. Some
feature notations in this paper are also borrowed
from that work.
The feature curroot returns the root of a par-
tial parsing tree that includes a specified node.
The feature charseq returns a character sequence
whose members are collected from all identified
children for a specified word.
In Table 2, as for concatenating multiple sub-
strings into a feature string, there are two ways,
seq and bag. The former is to concatenate all sub-
strings without do something special. The latter
will remove all duplicated substrings, sort the rest
and concatenate all at last.
Note that we systemically use a group of
character-level features. Surprisingly, as to our
best knowledge, this is the first report on using this
type of features in Chinese dependency parsing.
Although (McDonald et al, 2005) used the pre-
fix of each word form instead of word form itself
as features, character-level features here for Chi-
nese is essentially different from that. As Chinese
is basically a character-based written language.
Character plays an important role in many means,
most characters can be formed as single-character
words, and Chinese itself is character-order free
rather than word-order free to some extent. In ad-
dition, there is often a close connection between
the meaning of a Chinese word and its first or last
character.
4.2 Parsing using a Beam Search Algorithm
In Table 2, the feature preactn returns the previous
parsing action type, and the subscript n stands for
the action order before the current action. These
are a group of Markovian features. Without this
type of features, a shift-reduce parser may directly
scan through an input sequence in linear time.
Otherwise, following the work of (Duan et al,
2007) and (Zhao, 2009), the parsing algorithm is
to search a parsing action sequence with the max-
imal probability.
Sdi = argmax
?
i
p(di|di?1di?2...),
where Sdi is the object parsing action sequence,
p(di|di?1...) is the conditional probability, and di
58
Figure 1: A comparison before and after translation
Table 2: Features for Parsing
in.form, n = 0, 1
i.form + i1.form
in.char2 + in+1.char2, n = ?1, 0
i.char?1 + i1.char?1
in.char?2 n = 0, 3
i1.char?2 + i2.char?2 +i3.char?2
i.lnverb.char?2
i3.pos
in.pos + in+1.pos, n = 0, 1
i?2.cpos1 + i?1.cpos1
i1.cpos1 + i2.cpos1 + i3.cpos1
s?2.char1
s?.char?2 + s?1.char?2
s??2.cpos2
s??1.cpos2 + s?1.cpos2
s?.cpos2 + s?1.cpos2
s?.children.cpos2.seq
s?.children.dprel.seq
s?.subtree.depth
s?.h.form + s?.rm.cpos1
s?.lm.char2 + s?.char2
s.h.children.dprel.seq
s.lm.dprel
s.char?2 + i1.char?2
s.charn + i.charn, n = ?1, 1
s?1.pos + i1.pos
s.pos + in.pos, n = ?1, 0, 1
s : i|linePath.form.bag
s?.form + i.form
s?.char2 + in.char2, n = ?1, 0, 1
s.curroot.pos + i.pos
s.curroot.char2 + i.char2
s.children.cpos2.seq + i.children.cpos2.seq
s.children.cpos2.seq + i.children.cpos2.seq
+ s.cpos2 + i.cpos2
s?.children.dprel.seq + i.children.dprel.seq
preact?1
preact?2
preact?2+preact?1
is i-th parsing action. We use a beam search algo-
rithm to find the object parsing action sequence.
5 Exploiting the Translated Treebank
As we cannot expect too much for a word-by-word
translation, only word pairs with dependency rela-
tion in translated text are extracted as useful and
reliable information. Then some features based
on a query in these word pairs according to the
current parsing state (namely, words in the cur-
rent stack and input) will be derived to enhance
the Chinese parser.
A translation sample can be seen in Figure 1.
Although most words are satisfactorily translated,
to generate effective features, what we still have to
consider at first is the inconsistence between the
translated text and the target text.
In Chinese, word lemma is always its word form
itself, this is a convenient characteristic in com-
putational linguistics and makes lemma features
unnecessary for Chinese parsing at all. However,
Chinese has a special primary processing task, i.e.,
word segmentation. Unfortunately, word defini-
tions for Chinese are not consistent in various lin-
guistical views, for example, seven segmentation
conventions for computational purpose are for-
mally proposed since the first Bakeoff3.
Note that CTB or any other Chinese treebank
has its own word segmentation guideline. Chi-
nese word should be strictly segmented according
to the guideline before POS tags and dependency
relations are annotated. However, as we say the
3Bakeoff is a Chinese processing share task held by
SIGHAN.
59
English treebank is translated into Chinese word
by word, Chinese words in the translated text are
exactly some entries from the bilingual lexicon,
they are actually irregular phrases, short sentences
or something else rather than words that follows
any existing word segmentation convention. If the
bilingual lexicon is not carefully selected or re-
fined according to the treebank where the Chinese
parser is trained from, then there will be a serious
inconsistence on word segmentation conventions
between the translated and the target treebanks.
As all concerned feature values here are calcu-
lated from the searching result in the translated
word pair list according to the current parsing
state, and a complete and exact match cannot be
always expected, our solution to the above seg-
mentation issue is using a partial matching strat-
egy based on characters that the words include.
Above all, a translated word pair list, L, is ex-
tracted from the translated treebank. Each item in
the list consists of three elements, dependant word
(dp), head word (hd) and the frequency of this pair
in the translated treebank, f .
There are two basic strategies to organize the
features derived from the translated word pair list.
The first is to find the most matching word pair
in the list and extract some properties from it,
such as the matched length, part-of-speech tags
and so on, to generate features. Note that a
matching priority serial should be defined afore-
hand in this case. The second is to check every
matching models between the current parsing state
and the partially matched word pair. In an early
version of our approach, the former was imple-
mented. However, It is proven to be quite inef-
ficient in computation. Thus we adopt the sec-
ond strategy at last. Two matching model fea-
ture functions, ?(?) and ?(?), are correspondingly
defined as follows. The return value of ?(?) or
?(?) is the logarithmic frequency of the matched
item. There are four input parameters required
by the function ?(?). Two parameters of them
are about which part of the stack(input) words is
chosen, and other two are about which part of
each item in the translated word pair is chosen.
These parameters could be set to full or charn as
shown in Table 1, where n = ...,?2,?1, 1, 2, ....
For example, a possible feature could be
?(s.full, i.char1, dp.full, hd.char1), it tries to
find a match in L by comparing stack word and
dp word, and the first character of input word
Table 3: Features based on the translated treebank
?(i.char3, s?.full, dp.char3, hd.full)+i.char3
+s?.form
?(i.char3, s.char2, dp.char3, hd.char2)+s.char2
?(i.char3, s.full, dp.char3, hd.char2)+s.form
?(s?.char?2, hd.char?2, head)+i.pos+s?.pos
?(i.char3, s.full, dp.char3, hd.char2)+s.full
?(s?.full, i.char4, dp.full, hd.char4)+s?.pos+i.pos
?(i.full, hd.char2, root)+i.pos+s.pos
?(i.full, hd.char2, root)+i.pos+s?.pos
?(s.full, dp.full, dependant)+i.pos
pairscore(s?.pos, i.pos)+s?.form+i.form
rootscore(s?.pos)+s?.form+i.form
rootscore(s?.pos)+i.pos
and the first character of hd word. If such
a match item in L is found, then ?(?) returns
log(f). There are three input parameters required
by the function ?(?). One parameter is about
which part of the stack(input) words is chosen,
and the other is about which part of each item
in the translated word pair is chosen. The third
is about the matching type that may be set to
dependant, head, or root. For example, the
function ?(i.char1, hd.full, root) tries to find a
match in L by comparing the first character of in-
put word and the whole dp word. If such a match
item in L is found, then ?(?) returns log(f) as hd
occurs as ROOT f times.
As having observed that CTB and PTB share a
similar POS guideline. A POS pair list from PTB
is also extract. Two types of features, rootscore
and pairscore are used to make use of such infor-
mation. Both of them returns the logarithmic value
of the frequency for a given dependent event. The
difference is, rootscore counts for the given POS
tag occurring as ROOT, and pairscore counts for
two POS tag combination occurring for a depen-
dent relationship.
A full adapted feature list that is derived from
the translated word pairs is in Table 3.
6 Evaluation Results
The quality of the parser is measured by the pars-
ing accuracy or the unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
head. Two types of scores are reported for compar-
ison: ?UAS without p? is the UAS score without
all punctuation tokens and ?UAS with p? is the one
with all punctuation tokens.
The results with different feature sets are in Ta-
ble 4. As the features preactn are involved, a
60
beam search algorithm with width 5 is used for
parsing, otherwise, a simple shift-reduce decoding
is used. It is observed that the features derived
from the translated text bring a significant perfor-
mance improvement as high as 1.3%.
Table 4: The results with different feature sets
features with p without p
baseline -d 0.846 0.858
+da 0.848 0.860
+Tb -d 0.859 0.869
+d 0.861 0.870
a+d: using three Markovian features preact and
beam search decoding.
b+T: using features derived from the translated text
as in Table 3.
To compare our parser to the state-of-the-art
counterparts, we use the same testing data as
(Wang et al, 2005) did, selecting the sentences
length up to 40. Table 5 shows the results achieved
by other researchers and ours (UAS with p), which
indicates that our parser outperforms any other
ones 4. However, our results is only slightly better
than that of (Chen et al, 2008) as only sentences
whose lengths are less than 40 are considered. As
our full result is much better than the latter, this
comparison indicates that our approach improves
the performance for those longer sentences.
Table 5: Comparison against the state-of-the-art
full up to 40
(McDonald and Pereira, 2006)a - 0.825
(Wang et al, 2007) - 0.866
(Chen et al, 2008) 0.852 0.884
Ours 0.861 0.889
aThis results was reported in (Wang et al, 2007).
The experimental results in (McDonald and
Nivre, 2007) show a negative impact on the pars-
ing accuracy from too long dependency relation.
For the proposed method, the improvement rela-
tive to dependency length is shown in Figure 2.
From the figure, it is seen that our method gives
observable better performance when dependency
lengths are larger than 4. Although word order is
changed, the results here show that the useful in-
formation from the translated treebank still help
those long distance dependencies.
4There is a slight exception: using the same data splitting,
(Yu et al, 2008) reported UAS without p as 0.873 versus ours,
0.870.
1 4 7 10 13 16 19
0.4
0.5
0.6
0.7
0.8
0.9
1
 Dependency Length
 
F1
basline: +d
+T: +d
Figure 2: Performance vs. dependency length
7 Discussion
If a treebank in the source language can help im-
prove parsing in the target language, then there
must be something common between these two
languages, or more precisely, these two corre-
sponding treebanks. (Zeman and Resnik, 2008)
assumed that the morphology and syntax in the
language pair should be very similar, and that is
so for the language pair that they considered, Dan-
ish and Swedish, two very close north European
languages. Thus it is somewhat surprising that
we show a translated English treebank may help
Chinese parsing, as English and Chinese even be-
long to two different language systems. However,
it will not be so strange if we recognize that PTB
and CTB share very similar guidelines on POS and
syntactics annotation. Since it will be too abstract
in discussing the details of the annotation guide-
lines, we look into the similarities of two treebanks
from the matching degree of two word pair lists.
The reason is that the effectiveness of the proposed
method actually relies on how many word pairs at
every parsing states can find their full or partial
matched partners in the translated word pair list.
Table 6 shows such a statistics on the matching
degree distribution from all training samples for
Chinese parsing. The statistics in the table suggest
that most to-be-check word pairs during parsing
have a full or partial hitting in the translated word
pair list. The latter then obtains an opportunity to
provide a great deal of useful guideline informa-
tion to help determine how the former should be
tackled. Therefore we have cause for attributing
the effectiveness of the proposed method to the
similarity of these two treebanks. From Table 6,
61
we also find that the partial matching strategy de-
fined in Section 5 plays a very important role in
improving the whole matching degree. Note that
our approach is not too related to the characteris-
tics of two languages. Our discussion here brings
an interesting issue, which difference is more im-
portant in cross language processing, between two
languages themselves or the corresponding anno-
tated corpora? This may be extensively discussed
in the future work.
Table 6: Matching degree distribution
dependant-match head-match Percent (%)
None None 9.6
None Partial 16.2
None Full 9.9
Partial None 12.4
Partial Partial 42.6
Partial Full 7.3
Full None 3.7
Full Partial 7.0
Full Full 0.2
Note that only a bilingual lexicon is adopted in
our approach. We regard it one of the most mer-
its for our approach. A lexicon is much easier to
be obtained than an annotated corpus. One of the
remained question about this work is if the bilin-
gual lexicon should be very specific for this kind
of tasks. According to our experiences, actually, it
is not so sensitive to choose a highly refined lexi-
con or not. We once found many words, mostly
named entities, were outside the lexicon. Thus
we managed to collect a named entity translation
dictionary to enhance the original one. However,
this extra effort did not receive an observable per-
formance improvement in return. Finally we re-
alize that a lexicon that can guarantee two word
pair lists highly matched is sufficient for this work,
and this requirement may be conveniently satis-
fied only if the lexicon consists of adequate high-
frequent words from the source treebank.
8 Conclusion and Future Work
We propose a method to enhance dependency
parsing in one language by using a translated tree-
bank from another language. A simple statisti-
cal machine translation technique, word-by-word
decoding, where only a bilingual lexicon is nec-
essary, is used to translate the source treebank.
As dependency parsing is concerned with the re-
lations of word pairs, only those word pairs with
dependency relations in the translated treebank are
chosen to generate some additional features to en-
hance the parser for the target language. The ex-
perimental results in English and Chinese tree-
banks show the proposed method is effective and
helps the Chinese parser in this work achieve a
state-of-the-art result.
Note that our method is evaluated in two tree-
banks with a similar annotation style and it avoids
using too many linguistic properties. Thus the
method is in the hope of being used in other simi-
larly annotated treebanks 5. For an immediate ex-
ample, we may adopt a translated Chinese tree-
bank to improve English parsing. Although there
are still something to do, the remained key work
has been as simple as considering how to deter-
mine the matching strategy for searching the trans-
lated word pair list in English according to the
framework of our method. .
Acknowledgements
We?d like to give our thanks to three anonymous
reviewers for their insightful comments, Dr. Chen
Wenliang for for helpful discussions and Mr. Liu
Jun for helping us fix a bug in our scoring pro-
gram.
References
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16(2):79?85.
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP-2008, pages 877?886, Honolulu, Hawaii,
USA.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchi-
moto, Yujie Zhang, and Hitoshi Isahara. 2008. De-
pendency parsing with short dependency relations
in unlabeled data. In Proceedings of IJCNLP-2008,
Hyderabad, India, January 8-10.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic parsing action models for multi-lingual de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
940?946, Prague, Czech, June 28-30.
Johan Hall, Jens Nilsson, Joakim Nivre,
Gu?lsen Eryig?it, Bea?ta Megyesi, Mattias Nils-
son, and Markus Saers. 2007. Single malt or
5For example, Catalan and Spanish treebanks from the
AnCora(-Es/Ca) Multilevel Annotated Corpus that are an-
notated by the Universitat de Barcelona (CLiC-UB) and the
Universitat Polit?cnica de Catalunya (UPC).
62
blended? a study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 933?939,
Prague, Czech, June.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, USA, June.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of ACL-COLING 2006,
pages 337?344, Sydney, Australia, July.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2007), pages 122?131,
Prague, Czech, June 28-30.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL-2006, pages 81?88,
Trento, Italy, April.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL-2005,
pages 91?98, Ann Arbor, Michigan, USA, June 25-
30.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multilingual paradigm
for automatic verb classification. In ACL-2002,
pages 207?214, Philadelphia, Pennsylvania, USA.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, page
915?932, Prague, Czech, June.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of IWPT-
2003), pages 149?160, Nancy, France, April 23-25.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL-
2002, pages 295?302, Philadelphia, USA, July.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In Proceedings of
ACL-2007, pages 616?623, Prague, Czech Republic,
June.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with lr models and
parser ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, page
1044?1050, Prague, Czech, June 28-30.
Noah A. Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proceedings of ACL-COLING 2006,
page 569?576, Sydney, Australia, July.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of EACL-2003, page
331?338, Budapest, Hungary, April.
Qin Iris Wang and Dale Schuurmans. 2008. Semi-
supervised convex training for dependency parsing.
In Proceedings of ACL-08: HLT, pages 532?540,
Columbus, Ohio, USA, June.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2005. Strictly lexical dependency parsing. In Pro-
ceedings of IWPT-2005, pages 152?159, Vancouver,
BC, Canada, October.
Qin Iris Wang, Dekang Lin, and Dale Schuurmans.
2007. Simple training of dependency parsers via
structured boosting. In Proceedings of IJCAI 2007,
pages 1756?1762, Hyderabad, India, January.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Sta-
tistical dependency analysis with support vector
machines. In Proceedings of IWPT-2003), page
195?206, Nancy, France, April.
Kun Yu, Daisuke Kawahara, and Sadao Kurohashi.
2008. Chinese dependency parsing with large
scale automatically constructed case structures. In
Proceedings of COLING-2008, pages 1049?1056,
Manchester, UK, August.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of IJCNLP 2008 Workshop
on NLP for Less Privileged Languages, pages 35?
42, Hyderabad, India, January.
Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage max-
imum entropy models. In Proceeding of CoNLL-
2008, pages 203?207, Manchester, UK.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning:
A huge feature engineering method to semantic de-
pendency parsing. In Proceedings of CoNLL-2009,
Boulder, Colorado, USA.
Hai Zhao. 2009. Character-level dependencies in
chinese: Usefulness and learning. In EACL-2009,
pages 879?887, Athens, Greece.
63
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 55?60,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency Learning:
A Huge Feature Engineering Method to Semantic Dependency Parsing ?
Hai Zhao(??)??, Wenliang Chen(???)?, Chunyu Kit?, Guodong Zhou?
?Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
?Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
?School of Computer Science and Technology
Soochow University, Suzhou, China 215006
haizhao@cityu.edu.hk, chenwl@nict.go.jp
Abstract
This paper describes our system about mul-
tilingual semantic dependency parsing (SR-
Lonly) for our participation in the shared task
of CoNLL-2009. We illustrate that semantic
dependency parsing can be transformed into
a word-pair classification problem and im-
plemented as a single-stage machine learning
system. For each input corpus, a large scale
feature engineering is conducted to select the
best fit feature template set incorporated with a
proper argument pruning strategy. The system
achieved the top average score in the closed
challenge: 80.47% semantic labeled F1 for the
average score.
1 Introduction
The syntactic and semantic dependency parsing in
multiple languages introduced by the shared task
of CoNLL-2009 is an extension of the CoNLL-
2008 shared task (Hajic? et al, 2009). Seven lan-
guages, English plus Catalan, Chinese, Czech, Ger-
man, Japanese and Spanish, are involved (Taule? et
al., 2008; Palmer and Xue, 2009; Hajic? et al, 2006;
Surdeanu et al, 2008; Burchardt et al, 2006; Kawa-
hara et al, 2002). This paper presents our research
for participation in the semantic-only (SRLonly)
challenge of the CoNLL-2009 shared task, with a
?This study is partially supported by CERG grant 9040861
(CityU 1318/03H), CityU Strategic Research Grant 7002037,
Projects 60673041 and 60873041 under the National Natural
Science Foundation of China and Project 2006AA01Z147 un-
der the ?863? National High-Tech Research and Development
of China.
highlight on our strategy to select features from a
large candidate set for maximum entropy learning.
2 System Survey
We opt for the maximum entropy model with Gaus-
sian prior as our learning model for all classification
subtasks in the shared task. Our implementation of
the model adopts L-BFGS algorithm for parameter
optimization as usual. No additional feature selec-
tion techniques are applied.
Our system is basically improved from its early
version for CoNLL-2008 (Zhao and Kit, 2008). By
introducing a virtual root for every predicates, The
job to determine both argument labels and predicate
senses is formulated as a word-pair classification
task in four languages, namely, Catalan, Spanish,
Czech and Japanese. In other three languages, Chi-
nese, English and German, a predicate sense clas-
sifier is individually trained before argument label
classification. Note that traditionally (or you may
say that most semantic parsing systems did so) ar-
gument identification and classification are handled
in a two-stage pipeline, while ours always tackles
them in one step, in addition, predicate sense classi-
fication are also included in this unique learning/test
step for four of all languages.
3 Pruning Argument Candidates
We keep using a word-pair classification procedure
to formulate semantic dependency parsing. Specif-
ically, we specify the first word in a word pair as a
predicate candidate (i.e., a semantic head, and noted
as p in our feature representation) and the next as an
argument candidate (i.e., a semantic dependent, and
55
noted as a). We do not differentiate between verbal
and non-verbal predicates and our system handles
them in the exactly same way.
When no constraint available, however, all word
pairs in the an input sequence must be considered,
leading to very poor efficiency in computation for
no gain in effectiveness. Thus, the training sample
needs to be pruned properly. As predicates overtly
known in the share task, we only consider how to
effectively prune argument candidates.
We adopt five types of argument pruning strate-
gies for seven languages. All of them assume that a
syntactic dependency parsing tree is available.
As for Chinese and English, we continue to use
a dependency version of the pruning algorithm of
(Xue and Palmer, 2004) as described in (Zhao and
Kit, 2008). The pruning algorithm is readdressed as
the following.
Initialization: Set the given predicate candidate
as the current node;
(1) The current node and all of its syntactic chil-
dren are selected as argument candidates.
(2) Reset the current node to its syntactic head and
repeat step (1) until the root is reached.
Note that the given predicate candidate itself is
excluded from the argument candidate list for Chi-
nese, that is slightly different from English.
The above pruning algorithm has been shown ef-
fective. However, it is still inefficient for a single-
stage argument identification/classification classifi-
cation task. Thus we introduce an assistant argument
label ? NoMoreArgument? to alleviate this difficulty.
If an argument candidate in the above algorithm is
labeled as such a label, then the pruning algorithm
will end immediately. In training, this assistant label
means no more samples will be generated for the
current predicate, while in test, the decoder will not
search more argument candidates any more. This
adaptive technique more effectively prunes the ar-
gument candidates. In fact, our experiments show
1/3 training memory and time may be saved from it.
As for Catalan and Spanish, only syntactic chil-
dren of the predicate are considered as the argument
candidates.
As for Czech, only syntactic children, grandchil-
dren, great-grandchildren, parent and siblings of the
predicate are taken as the argument candidates.
As for German, only syntactic children, grand-
children, parent, siblings, siblings of parent and sib-
lings of grandparent of the predicate are taken as the
argument candidates.
The case is somewhat sophisticated for Japanese.
As we cannot identify a group of simple predicate-
argument relations from the syntactic tree. Thus
we consider top frequent 28 syntactic relations be-
tween the predicate and the argument. The parser
will search all words before and after the predicate,
and only those words that hold one of the 28 syn-
tactic relations to the predicate are considered as
the argument candidate. Similar to the pruning al-
gorithm for Chinese/English/German, we also in-
troduce two assistant labels ? leftNoMoreArgument?
and ? rightNoMoreArgument? to adaptively prune
words too far away from the predicate.
4 Feature Templates
As we don?t think that we can benefit from know-
ing seven languages, an automatic feature template
selection is conducted for each language.
About 1000 feature templates (hereafter this tem-
plate set is referred to FT ) are initially considered.
These feature templates are from various combina-
tions or integrations of the following basic elements.
Word Property. This type of elements include
word form, lemma, part-of-speech tag (PoS), FEAT
(additional morphological features), syntactic de-
pendency label (dprel), semantic dependency label
(semdprel) and characters (char) in the word form
(only suitable for Chinese and Japanese)1.
Syntactic Connection. This includes syntactic
head (h), left(right) farthest(nearest) child (lm, ln,
rm, and rn), and high(low) support verb or noun.
We explain the last item, support verb(noun). From
the predicate or the argument to the syntactic root
along the syntactic tree, the first verb(noun) that is
met is called as the low support verb(noun), and the
nearest one to the root is called as the high support
verb(noun).
Semantic Connection. This includes semantic
1All lemmas, PoS, and FEAT for either training or test are
from automatically pre-analyzed columns of every input files.
56
FEATn 1 2 3 4 5 6 7 8 9 10 11
Catalan/Spanish postype gen num person mood tense punct
Czech SubPOS Gen Num Cas Neg Gra Voi Var Sem Per Ten
Table 1: Notations of FEATs
head (semhead), left(right) farthest(nearest) seman-
tic child (semlm, semln, semrm, semrn). We say
a predicate is its argument?s semantic head, and the
latter is the former?s child. Features related to this
type may track the current semantic parsing status.
Path. There are two basic types of path between
the predicate and the argument candidates. One is
the linear path (linePath) in the sequence, the other
is the path in the syntactic parsing tree (dpPath). For
the latter, we further divide it into four sub-types
by considering the syntactic root, dpPath is the full
path in the syntactic tree. Leading two paths to the
root from the predicate and the argument, respec-
tively, the common part of these two paths will be
dpPathShare. Assume that dpPathShare starts from
a node r?, then dpPathPred is from the predicate to
r?, and dpPathArgu is from the argument to r?.
Family. Two types of children sets for the predi-
cate or argument candidate are considered, the first
includes all syntactic children (children), the second
also includes all but excludes the left most and the
right most children (noFarChildren).
Concatenation of Elements. For all collected el-
ements according to linePath, children and so on, we
use three strategies to concatenate all those strings
to produce the feature value. The first is seq, which
concatenates all collected strings without doing any-
thing. The second is bag, which removes all dupli-
cated strings and sort the rest. The third is noDup,
which removes all duplicated neighbored strings.
In the following, we show some feature template
examples derived from the above mentioned items.
a.lm.lemma The lemma of the left most child of
the argument candidate.
p.h.dprel The dependant label of the syntactic
head of the predicate candidate.
a.pos+p.pos The concatenation of PoS of the ar-
gument and the predicate candidates.
p?1.pos+p.pos PoS of the previous word of the
predicate and PoS of the predicate itself.
a:p|dpPath.lemma.bag Collect all lemmas along
the syntactic tree path from the argument to the pred-
icate, then removed all duplicated ones and sort the
rest, finally concatenate all as a feature string.
a:p.highSupportNoun|linePath.dprel.seq Collect
all dependant labels along the line path from the ar-
gument to the high support noun of the predicate,
then concatenate all as a feature string.
(a:p|dpPath.dprel.seq)+p.FEAT1 Collect all de-
pendant labels along the line path from the argument
to the predicate and concatenate them plus the first
FEAT of the predicate.
An important feature for the task is dpTreeRela-
tion, which returns the relationship of a and p in a
syntactic parse tree and cannot be derived from com-
bining the above basic elements. The possible values
for this feature include parent, sibling etc.
5 Automatically Discovered Feature
Template Sets
For each language, starting from a basic feature tem-
plate set (a small subset of FT ) according to our
previous result in English dependency parsing, each
feature template outside the basic set is added and
each feature template inside the basic set is removed
one by one to check the effectiveness of each fea-
ture template following the performance change in
the development set. This procedure will be contin-
uously repeated until no feature template is added or
removed or the performance is not improved.
There are some obvious heuristic rules that help
us avoid trivial feature template checking, for ex-
ample, FEAT features are only suitable for Cata-
lan, Czech and Spanish. Though FEAT features are
also available for Japanese, we don?t adopt them for
this language due to the hight training cost. To sim-
plify feature representation, we use FEAT1, FEAT2,
and so on to represent different FEAT for every lan-
guages. A lookup list can be found in Table 1. Ac-
cording to the list, FEAT4 represents person for
Catalan or Spanish, but Cas for Czech.
As we don?t manually interfere the selection pro-
cedure for feature templates, ten quite different fea-
57
Ca Ch Cz En Gr Jp Sp
Ca 53
Ch 5 75
Cz 11 10 76
En 11 11 12 73
Gr 7 7 7 14 45
Jp 6 22 13 15 10 96
Sp 22 9 18 15 9 12 66
Table 2: Feature template set: argument classifier
Ch En Gr
Ch 46
En 5 9
Gr 17 2 40
Table 3: Feature template set: sense classifier
ture template sets are obtained at last. Statistical in-
formation of seven sets for argument classifiers is in
Table 2, and those for sense classifiers are in Table 3.
Numbers in the diagonals of these two tables mean
the numbers of feature templates, and others mean
how many feature templates are identical for every
language pairs. The most matched feature template
sets are for Catalan/Spanish and Chinese/Japanese.
As for the former, it is not so surprised because these
two corpora are from the same provider.
Besides the above statistics, these seven feature
template sets actually share little in common. For
example, the intersection set from six languages, as
Chinese is excluded, only includes one feature tem-
plate, p.lemma (the lemma of the predicate candi-
date). If all seven sets are involved, then such an in-
tersection set will be empty. Does this mean human
languages share little in semantic representation? :)
It is unlikely to completely demonstrate full fea-
ture template sets for all languages in this short re-
port, we thus only demonstrate two sets, one for En-
glish sense classification in Table 4 and the other for
Catalan argument classification in Table 52.
6 Word Sense Determination
The shared task of CoNLL-2009 still asks for the
predicate sense. In our work for CoNLL-2008 (Zhao
and Kit, 2008), this was done by searching for a right
2Full feature lists and their explanation for all languages will
be available at the website, http://bcmi.sjtu.edu.cn/?zhaohai.
p.lm.pos
p.rm.pos
p.lemma
p.lemma + p.lemma1
p.lemma + p.children.dprel.noDup
p.lemma + p.currentSense
p.form
p.form?1 + p.form
p.form + p.form1
Table 4: Feature set for English sense classification
example in the given dictionary. Unfortunately, we
late found this caused a poor performance in sense
determination. This time, an individual classifier is
used to determine the sense for Chinese, English or
German, and this is done by the argument classifier
by introducing a virtual root for every predicates for
the rest four languages3. Features used for sense
determination are also selected following the same
procedure in Section 5. The difference is only pred-
icate related features are used for selection.
7 Decoding
The decoding for four languages, Catalan, Czech,
Japanese and Spanish is trivial, each word pairs will
be checked one by one. The first word of the pair
is the virtual root or the predicate, the second is the
predicate or every argument candidates. Argument
candidates are checked in the order of different syn-
tactic relations to their predicate, which are enumer-
ated by the pruning algorithms in Section 3, or from
left to right for the same syntactic relation. After
the sense of the predicate is determined, the label of
each argument candidate will be directly classified,
or, it is proved non-argument.
As for the rest languages, Chinese, English or
German, after the sense classifier outputs its result,
an optimal argument structure for each predicate is
determined by the following maximal probability.
Sp = argmax
?
i
P (ai|ai?1, ai?2, ...), (1)
where Sp is the argument structure, P (ai|ai?1...)
is the conditional probability to determine the la-
bel of the i-th argument candidate label. Note that
3For Japanese, no senses for predicates are defined. Thus it
is actually a trivial classification task in this case.
58
p.currentSense + p.lemma
p.currentSense + p.pos
p.currentSense + a.pos
p?1.FEAT1
p.FEAT2
p1.FEAT3
p.semrm.semdprel
p.lm.dprel
p.form + p.children.dprel.bag
p.lemman (n = ?1, 0)
p.lemma + p.lemma1
p.pos?1 + p.pos
p.pos1
p.pos + p.children.dprel.bag
a.FEAT1 + a.FEAT3 + a.FEAT4
+ a.FEAT5 + a.FEAT6
a?1.FEAT2 + a.FEAT2
a.FEAT3 + a1.FEAT3
a.FEAT3 + a.h.FEAT3
a.children.FEAT1.noDup
a.children.FEAT3.bag
a.h.lemma
a.lm.dprel + a.form
a.lm.form
a.lm?1.lemma
a.lmn.pos (n=0,1)
a.noFarChildren.pos.bag + a.rm.form
a.pphead.lemma
a.rm.dprel + a.form
a.rm?1.form
a.rm.lemma
a.rn.dprel + a.form
a.lowSupportVerb.lemma
a?1.form
a.form + a1.form
a.form + a.children.pos
a.lemma + a.h.form
a.lemma + a.pphead.form
a1.lemma
a1.pos + a.pos.seq
a.pos + a.children.dprel.bag
a.lemma + p.lemma
(a:p|dpPath.dprel) + p.FEAT1
a:p|linePath.distance
a:p|linePath.FEAT1.bag
a:p|linePath.form.seq
a:p|linePath.lemma.seq
a:p|linePath.dprel.seq
a:p|dpPath.lemma.seq
a:p|dpPath.lemma.bag
a:p|dpPathArgu.lemma.seq
a:p|dpPathArgu.lemma.bag
Table 5: Feature set for Catalan argument classification
P (ai|ai?1, ...) in equation (1) may be simplified as
P (ai) if the input feature template set does not con-
cerned with the previous argument label output. A
beam search algorithm is used to find the parsing de-
cision sequence.
8 Evaluation Results
Our evaluation is carried out on two computational
servers, (1) LEGA, a 64-bit ubuntu Linux installed
server with double dual-core AMD Opteron proces-
sors of 2.8GHz and 24GB memory. This server was
also used for our previous participation in CoNLL-
2008 shared task. (2) MEGA, a 64-bit ubuntu Linux
installed server with six quad-core Intel Xeon pro-
cessors of 2.33GHz and 128GB memory.
Altogether nearly 60,000 machine learning rou-
tines were run to select the best fit feature template
sets for all seven languages within two months. Both
LEGA and MEGA were used for this task. How-
ever, training and test for the final submission of
Chinese, Czech and English run in MEGA, and the
rest in LEGA. As we used multiple thread training
and multiple routines run at the same time, the exact
time cost for either training or test is hard to esti-
mate. Here we just report the actual time and mem-
ory cost in Table 7 for reference.
The official evaluation results of our system are in
Table 6. Numbers in bold in the table stand for the
best performances for the specific languages. The
results in development sets are also given. The first
row of the table reports the results using golden in-
put features.
Two facts as the following suggest that our system
does output robust and stable results. The first is that
two results for development and test sets in the same
language are quite close. The second is about out-of-
domain (OOD) task. Though for each OOD task, we
just used the same model trained from the respective
language and did nothing to strengthen it, this does
not hinder our system to obtain top results in Czech
and English OOD tasks.
In addition, the feature template sets from auto-
matical selection procedure in this task were used
for the joint task of this shared task, and also output
top results according to the average score of seman-
tic labeled F1 (Zhao et al, 2009).
59
average Catalan Chinese Czech English German Japanese Spanish
Development with Gold 81.24 81.52 78.32 86.96 84.19 77.75 78.67 81.32
Development 80.46 80.66 77.90 85.35 84.01 76.55 78.41 80.39
Test (official scores) 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
Out-of-domain 74.34 85.44 73.31 64.26
Table 6: Semantic labeled F1
Catalan Chinese Czech English German Japanese Spanish
Sense Training memory (MB) 418.0 136.0 63.0
Training time (Min.) 11.0 2.5 1.7
Test time (Min.) 0.7 0.2 0.03
Argument Training memory (GB) 0.4 3.7 3.2 3.8 0.2 1.4 0.4
Training time (Hours) 3.0 13.8 24.9 12.4 0.2 6.1 4.4
Test time (Min.) 3.0 144.0 27.1 88.0 1.0 4.2 7.0
Table 7: Computational cost
9 Conclusion
As presented in the above sections, we have tackled
semantic parsing for the CoNLL-2009 shared task
as a word-pair classification problem. Incorporated
with a proper argument candidate pruning strategy
and a large scale feature engineering for each lan-
guage, our system produced top results.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Nianwen Xue and Martha Palmer. 2004. Calibrating fea-
tures for semantic role labeling. In 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2004), pages 88?94, Barcelona, Spain,
July 25-26.
Hai Zhao and Chunyu Kit. 2008. Parsing syntac-
tic and semantic dependencies with two single-stage
maximum entropy models. In Twelfth Conference on
Computational Natural Language Learning (CoNLL-
2008), pages 203?207, Manchester, UK, August 16-
17.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009. Multilin-
gual dependency learning: Exploiting rich features for
tagging syntactic and semantic dependencies. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), June 4-5,
Boulder, Colorado, USA.
60
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200?207,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Grammar-driven Convolution Tree Kernel for Se-
mantic Role Classification 
 
Min ZHANG1     Wanxiang CHE2     Ai Ti AW1     Chew Lim TAN3     
Guodong ZHOU1,4     Ting LIU2     Sheng LI2     
 
1Institute for Infocomm Research   
{mzhang, aaiti}@i2r.a-star.edu.sg 
2Harbin Institute of Technology 
{car, tliu}@ir.hit.edu.cn   
lisheng@hit.edu.cn 
3National University of Singapore 
tancl@comp.nus.edu.sg 
4 Soochow Univ., China 215006 
gdzhou@suda.edu.cn 
 
 
 
 
 
Abstract 
Convolution tree kernel has shown promis-
ing results in semantic role classification. 
However, it only carries out hard matching, 
which may lead to over-fitting and less ac-
curate similarity measure. To remove the 
constraint, this paper proposes a grammar-
driven convolution tree kernel for semantic 
role classification by introducing more lin-
guistic knowledge into the standard tree 
kernel. The proposed grammar-driven tree 
kernel displays two advantages over the pre-
vious one: 1) grammar-driven approximate 
substructure matching and 2) grammar-
driven approximate tree node matching. The 
two improvements enable the grammar-
driven tree kernel explore more linguistically 
motivated structure features than the previ-
ous one. Experiments on the CoNLL-2005 
SRL shared task show that the grammar-
driven tree kernel significantly outperforms 
the previous non-grammar-driven one in 
SRL. Moreover, we present a composite 
kernel to integrate feature-based and tree 
kernel-based methods. Experimental results 
show that the composite kernel outperforms 
the previously best-reported methods. 
1 Introduction 
Given a sentence, the task of Semantic Role Label-
ing (SRL) consists of analyzing the logical forms 
expressed by some target verbs or nouns and some 
constituents of the sentence. In particular, for each 
predicate (target verb or noun) all the constituents in 
the sentence which fill semantic arguments (roles) 
of the predicate have to be recognized. Typical se-
mantic roles include Agent, Patient, Instrument, etc. 
and also adjuncts such as Locative, Temporal, 
Manner, and Cause, etc. Generally, semantic role 
identification and classification are regarded as two 
key steps in semantic role labeling. Semantic role 
identification involves classifying each syntactic 
element in a sentence into either a semantic argu-
ment or a non-argument while semantic role classi-
fication involves classifying each semantic argument 
identified into a specific semantic role. This paper 
focuses on semantic role classification task with the 
assumption that the semantic arguments have been 
identified correctly. 
Both feature-based and kernel-based learning 
methods have been studied for semantic role classi-
fication (Carreras and M?rquez, 2004; Carreras and 
M?rquez, 2005). In feature-based methods, a flat 
feature vector is used to represent a predicate-
argument structure while, in kernel-based methods, 
a kernel function is used to measure directly the 
similarity between two predicate-argument struc-
tures. As we know, kernel methods are more effec-
tive in capturing structured features. Moschitti 
(2004) and Che et al (2006) used a convolution 
tree kernel (Collins and Duffy, 2001) for semantic 
role classification. The convolution tree kernel 
takes sub-tree as its feature and counts the number 
of common sub-trees as the similarity between two 
predicate-arguments. This kernel has shown very 
200
promising results in SRL. However, as a general 
learning algorithm, the tree kernel only carries out 
hard matching between any two sub-trees without 
considering any linguistic knowledge in kernel de-
sign. This makes the kernel fail to handle similar 
phrase structures (e.g., ?buy a car? vs. ?buy a red 
car?) and near-synonymic grammar tags (e.g., the 
POS variations between ?high/JJ degree/NN? 1 and 
?higher/JJR degree/NN?) 2. To some degree, it may 
lead to over-fitting and compromise performance. 
This paper reports our preliminary study in ad-
dressing the above issue by introducing more lin-
guistic knowledge into the convolution tree kernel. 
To our knowledge, this is the first attempt in this 
research direction. In detail, we propose a gram-
mar-driven convolution tree kernel for semantic 
role classification that can carry out more linguisti-
cally motivated substructure matching. Experimental 
results show that the proposed method significantly 
outperforms the standard convolution tree kernel on 
the data set of the CoNLL-2005 SRL shared task. 
The remainder of the paper is organized as fol-
lows: Section 2 reviews the previous work and Sec-
tion 3 discusses our grammar-driven convolution 
tree kernel. Section 4 shows the experimental re-
sults. We conclude our work in Section 5. 
2 Previous Work 
Feature-based Methods for SRL: most features 
used in prior SRL research are generally extended 
from Gildea and Jurafsky (2002), who used a linear 
interpolation method and extracted basic flat fea-
tures from a parse tree to identify and classify the 
constituents in the FrameNet (Baker et al, 1998). 
Here, the basic features include Phrase Type, Parse 
Tree Path, and Position. Most of the following work 
focused on feature engineering (Xue and Palmer, 
2004; Jiang et al, 2005) and machine learning 
models (Nielsen and Pradhan, 2004; Pradhan et al, 
2005a). Some other work paid much attention to the 
robust SRL (Pradhan et al, 2005b) and post infer-
ence (Punyakanok et al, 2004). These feature-
based methods are considered as the state of the art 
methods for SRL. However, as we know, the stan-
dard flat features are less effective in modeling the 
                                                          
1 Please refer to http://www.cis.upenn.edu/~treebank/ for the 
detailed definitions of the grammar tags used in the paper. 
2 Some rewrite rules in English grammar are generalizations of 
others: for example, ?NP? DET JJ NN? is a specialized ver-
sion of ?NP? DET NN?. The same applies to POS. The stan-
dard convolution tree kernel is unable to capture the two cases. 
syntactic structured information. For example, in 
SRL, the Parse Tree Path feature is sensitive to 
small changes of the syntactic structures. Thus, a 
predicate argument pair will have two different 
Path features even if their paths differ only for one 
node. This may result in data sparseness and model 
generalization problems. 
Kernel-based Methods for SRL: as an alternative, 
kernel methods are more effective in modeling 
structured objects. This is because a kernel can 
measure the similarity between two structured ob-
jects using the original representation of the objects 
instead of explicitly enumerating their features. 
Many kernels have been proposed and applied to 
the NLP study. In particular, Haussler (1999) pro-
posed the well-known convolution kernels for a 
discrete structure. In the context of it, more and 
more kernels for restricted syntaxes or specific do-
mains (Collins and Duffy, 2001; Lodhi et al, 2002; 
Zelenko et al, 2003; Zhang et al, 2006) are pro-
posed and explored in the NLP domain. 
Of special interest here, Moschitti (2004) proposed 
Predicate Argument Feature (PAF) kernel for SRL 
under the framework of convolution tree kernel. He 
selected portions of syntactic parse trees as predicate-
argument feature spaces, which include salient sub-
structures of predicate-arguments, to define convo-
lution kernels for the task of semantic role classifi-
cation. Under the same framework, Che et al (2006) 
proposed a hybrid convolution tree kernel, which 
consists of two individual convolution kernels: a Path 
kernel and a Constituent Structure kernel. Che et al 
(2006) showed that their method outperformed PAF 
on the CoNLL-2005 SRL dataset.  
The above two kernels are special instances of 
convolution tree kernel for SRL. As discussed in 
Section 1, convolution tree kernel only carries out 
hard matching, so it fails to handle similar phrase 
structures and near-synonymic grammar tags. This 
paper presents a grammar-driven convolution tree 
kernel to solve the two problems 
3 Grammar-driven Convolution Tree 
Kernel 
3.1 Convolution Tree Kernel 
In convolution tree kernel (Collins and Duffy, 
2001), a parse tree T  is represented by a vector of 
integer counts of each sub-tree type (regardless of 
its ancestors): ( )T? = ( ?, # subtreei(T), ?), where 
201
# subtreei(T) is the occurrence number of the ith 
sub-tree type (subtreei) in T. Since the number of 
different sub-trees is exponential with the parse tree 
size, it is computationally infeasible to directly use 
the feature vector ( )T? . To solve this computa-
tional issue, Collins and Duffy (2001) proposed the 
following parse tree kernel to calculate the dot 
product between the above high dimensional vec-
tors implicitly. 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
 ( ) ( )
 ( , )
(( ) ( ))
i isubtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
=< >
=
= ?
?? ? ?
? ?
 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively, and ( )
isubtree
I n  is a function that is 
1 iff the subtreei occurs with root at node n and zero 
otherwise, and 1 2( , )n n?  is the number of the com-
mon subtrees rooted at n1 and n2, i.e., 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? = ??  
1 2( , )n n? can be further computed efficiently by the 
following recursive rules: 
Rule 1: if the productions (CFG rules) at 1n  and 
2n  are different, 1 2( , ) 0n n? = ; 
Rule 2: else if both 1n  and 2n  are pre-terminals 
(POS tags), 1 2( , ) 1n n ?? = ? ; 
Rule 3: else,  
1( )
1 2 1 21
( , ) (1 ( ( , ), ( , )))nc n
j
n n ch n j ch n j?
=
? = + ?? ,  
where 1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and ? (0< ? <1) is the decay 
factor in order to make the kernel value less vari-
able with respect to the subtree sizes. In addition, 
the recursive Rule 3 holds because given two 
nodes with the same children, one can construct 
common sub-trees using these children and com-
mon sub-trees of further offspring. The time com-
plexity for computing this kernel is 1 2(| | | |)O N N? . 
3.2 Grammar-driven Convolution Tree 
Kernel 
This Subsection introduces the two improvements 
and defines our grammar-driven tree kernel. 
 
Improvement 1: Grammar-driven approximate 
matching between substructures. The conven-
tional tree kernel requires exact matching between 
two contiguous phrase structures. This constraint 
may be too strict. For example, the two phrase 
structures ?NP?DT JJ NN? (NP?a red car) and 
?NP?DT NN? (NP->a car) are not identical, thus 
they contribute nothing to the conventional kernel 
although they should share the same semantic role 
given a predicate. In this paper, we propose a 
grammar-driven approximate matching mechanism 
to capture the similarity between such kinds of 
quasi-structures for SRL. 
First, we construct reduced rule set by defining 
optional nodes, for example, ?NP->DT [JJ] NP? or 
?VP-> VB [ADVP]  PP?, where [*] denotes op-
tional nodes. For convenience, we call ?NP-> DT 
JJ NP? the original rule and ?NP->DT [JJ] NP? the 
reduced rule. Here, we define two grammar-driven 
criteria to select optional nodes: 
1) The reduced rules must be grammatical. It 
means that the reduced rule should be a valid rule 
in the original rule set. For example, ?NP->DT [JJ] 
NP? is valid only when ?NP->DT NP? is a valid 
rule in the original rule set while ?NP->DT [JJ 
NP]? may not be valid since ?NP->DT? is not a 
valid rule in the original rule set. 
2) A valid reduced rule must keep the head 
child of its corresponding original rule and has at 
least two children. This can make the reduced rules 
retain the underlying semantic meaning of their 
corresponding original rules. 
Given the reduced rule set, we can then formu-
late the approximate substructure matching mecha-
nism as follows: 
11 2 1 2,
( , ) ( ( , ) )
a bi ji j
T r ri j
M r r I T T ?
+
= ??              (1)  
where 1r is a production rule, representing a sub-tree 
of depth one3, and 1
i
rT is the i
th variation of the sub-
tree 1r by removing one ore more optional nodes
4, 
and likewise for 2r and 2
j
rT . ( , )TI ? ? is a function 
that is 1 iff the two sub-trees are identical and zero 
otherwise. 1? (0? 1? ?1) is a small penalty to penal-
                                                          
3 Eq.(1) is defined over sub-structure of depth one. The ap-
proximate matching between structures of depth more than one 
can be achieved easily through the matching of sub-structures 
of depth one in the recursively-defined convolution kernel. We 
will discuss this issue when defining our kernel. 
4 To make sure that the new kernel is a proper kernel, we have 
to consider all the possible variations of the original sub-trees. 
Training program converges only when using a proper kernel. 
202
ize optional nodes and the two parameters ia  and 
jb stand for the numbers of occurrence of removed 
optional nodes in subtrees 1
i
rT and 2
j
rT , respectively. 
1 2( , )M r r returns the similarity (ie., the kernel 
value) between the two sub-trees 1r and 2r  by sum-
ming up the similarities between all possible varia-
tions of the sub-trees 1r and 2r . 
Under the new approximate matching mecha-
nism, two structures are matchable (but with a small 
penalty 1? ) if the two structures are identical after 
removing one or more optional nodes. In this case, 
the above example phrase structures ?NP->a red 
car? and ?NP->a car? are matchable with a pen-
alty 1?  in our new kernel. It means that one co-
occurrence of the two structures contributes 1?  to 
our proposed kernel while it contributes zero to the 
traditional one. Therefore, by this improvement, our 
method would be able to explore more linguistically 
appropriate features than the previous one (which is 
formulated as 1 2( , )TI r r ). 
Improvement 2: Grammar-driven tree nodes ap-
proximate matching. The conventional tree kernel 
needs an exact matching between two (termi-
nal/non-terminal) nodes. But, some similar POSs 
may represent similar roles, such as NN (dog) and 
NNS (dogs). In order to capture this phenomenon, 
we allow approximate matching between node fea-
tures. The following illustrates some equivalent 
node feature sets:  
? JJ, JJR, JJS 
? VB, VBD, VBG, VBN, VBP, VBZ 
? ?? 
where POSs in the same line can match each other 
with a small penalty 0? 2? ?1. We call this case 
node feature mutation. This improvement further 
generalizes the conventional tree kernel to get bet-
ter coverage. The approximate node matching can 
be formulated as: 
21 2 1 2,
( , ) ( ( , ) )
a bi ji j
fi j
M f f I f f ?
+
= ??           (2) 
where 1f is a node feature, 1
if is the ith mutation 
of 1f and ia is 0 iff 1
if and 1f are identical and 1 oth-
erwise, and likewise for 2f . ( , )fI ? ? is a function 
that is 1 iff the two features are identical and zero 
otherwise. Eq. (2) sums over all combinations of 
feature mutations as the node feature similarity. 
The same as Eq. (1), the reason for taking all the 
possibilities into account in Eq. (2) is to make sure 
that the new kernel is a proper kernel.  
The above two improvements are grammar-
driven, i.e., the two improvements retain the under-
lying linguistic grammar constraints and keep se-
mantic meanings of original rules. 
 
The Grammar-driven Kernel Definition: Given 
the two improvements discussed above, we can de-
fine the new kernel by beginning with the feature 
vector representation of a parse tree T as follows: 
( )T? =? (# subtree1(T), ?, # subtreen(T))       
where # subtreei(T) is the occurrence number of the 
ith sub-tree type (subtreei) in T. Please note that, 
different from the previous tree kernel, here we 
loosen the condition for the occurrence of a subtree 
by allowing both original and reduced rules (Im-
provement 1) and node feature mutations (Im-
provement 2). In other words, we modify the crite-
ria by which a subtree is said to occur. For example, 
one occurrence of the rule ?NP->DT JJ NP? shall 
contribute 1 times to the feature ?NP->DT JJ NP? 
and 1?  times to the feature ?NP->DT NP? in the 
new kernel while it only contributes 1 times to the 
feature ?NP->DT JJ NP? in the previous one. Now 
we can define the new grammar-driven kernel 
1 2( , )GK T T as follows: 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
( , ) ( ), ( )
( ) ( )
 ( , )
(( ) ( ))
i i
G
subtree subtreei n N n N
n N n N
K T T T T
I n I n
n n
? ?
? ?
? ?
? ?=< >
? ?=
?= ?
?? ? ?
? ?
 (3) 
where N1 and N2 are the sets of nodes in trees T1 and 
T2, respectively. ( )
isubtree
I n?  is a function that is 
1 2
a b? ?? iff the subtreei occurs with root at node n 
and zero otherwise, where a and b are the numbers 
of removed optional nodes and mutated node fea-
tures, respectively. 1 2( , )n n??  is the number of the 
common subtrees rooted at n1 and n2, i.e. , 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? ? ?? = ??         (4) 
Please note that the value of 1 2( , )n n?? is no longer 
an integer as that in the conventional one since op-
tional nodes and node feature mutations are consid-
ered in the new kernel. 1 2( , )n n??  can be further 
computed by the following recursive rules:  
 
203
============================================================================ 
Rule A: if 1n and 2n are pre-terminals, then: 
1 2 1 2( , ) ( , )n n M f f??? = ?                          (5) 
where 1f and 2f are features of nodes 1n and 2n re-
spectively, and 1 2( , )M f f  is defined at Eq. (2).  
Rule B: else if both 1n and 2n are the same non-
terminals, then generate all variations of the subtrees 
of depth one rooted by 1n and 2n (denoted by 1nT  
and 2nT  respectively) by removing different optional 
nodes, then: 
 
1
1
1 2 1 2,
( , )
1 21
( , ) ( ( , )
   (1 ( ( , , ), ( , , )))
a bi ji j
T n ni j
nc n i
k
n n I T T
ch n i k ch n j k
? ?
+
=
?? = ? ?
?? + ?
?
?
(6) 
 
where  
? 1inT and 2jnT stand for the ith and jth variations in 
sub-tree set 1nT and 2nT , respectively. 
? ( , )TI ? ? is a function that is 1 iff the two sub-
trees are identical and zero otherwise.  
? ia and jb stand for the number of removed op-
tional nodes in subtrees 1
i
nT and 2
j
nT , respectively. 
? 1( , )nc n i returns the child number of 1n in its ith 
subtree variation 1
i
nT . 
? 1( , , )ch n i k  is the kth child of node 1n  in its ith 
variation subtree 1
i
nT , and likewise for 2( , , )ch n j k . 
? Finally, the same as the previous tree kernel, 
? (0< ? <1) is the decay factor (see the discussion 
in Subsection 3.1). 
 
Rule C: else 1 2( , ) 0n n?? =  
  
============================================================================ 
 
Rule A accounts for Improvement 2 while Rule 
B accounts for Improvement 1. In Rule B, Eq. (6) 
is able to carry out multi-layer sub-tree approxi-
mate matching due to the introduction of the recur-
sive part while Eq. (1) is only effective for sub-
trees of depth one. Moreover, we note that Eq. (4) 
is a convolution kernel according to the definition 
and the proof given in Haussler (1999), and Eqs (5) 
and (6) reformulate Eq. (4) so that it can be com-
puted efficiently, in this way, our kernel defined by 
Eq (3) is also a valid convolution kernel. Finally, 
let us study the computational issue of the new 
convolution tree kernel. Clearly, computing Eq. (6) 
requires exponential time in its worst case. How-
ever, in practice, it may only need  1 2(| | | |)O N N? . 
This is because there are only 9.9% rules (647 out 
of the total 6,534 rules in the parse trees) have op-
tional nodes and most of them have only one op-
tional node. In fact, the actual running time is even 
much less and is close to linear in the size of the 
trees since 1 2( , ) 0n n?? =  holds for many node 
pairs (Collins and Duffy, 2001). In theory, we can 
also design an efficient algorithm to compute Eq. 
(6) using a dynamic programming algorithm (Mo-
schitti, 2006). We just leave it for our future work. 
3.3 Comparison with previous work 
In above discussion, we show that the conventional 
convolution tree kernel is a special case of the 
grammar-driven tree kernel. From kernel function 
viewpoint, our kernel can carry out not only exact 
matching (as previous one described by Rules 2 
and 3 in Subsection 3.1) but also approximate 
matching (Eqs. (5) and (6) in Subsection 3.2). From 
feature exploration viewpoint, although they ex-
plore the same sub-structure feature space (defined 
recursively by the phrase parse rules), their feature 
values are different since our kernel captures the 
structure features in a more linguistically appropri-
ate way by considering more linguistic knowledge 
in our kernel design. 
Moschitti (2006) proposes a partial tree (PT) 
kernel which can carry out partial matching be-
tween sub-trees. The PT kernel generates a much 
larger feature space than both the conventional and 
the grammar-driven kernels. In this point, one can 
say that the grammar-driven tree kernel is a spe-
cialization of the PT kernel. However, the impor-
tant difference between them is that the PT kernel 
is not grammar-driven, thus many non-
linguistically motivated structures are matched in 
the PT kernel. This may potentially compromise 
the performance since some of the over-generated 
features may possibly be noisy due to the lack of 
linguistic interpretation and constraint. 
Kashima and Koyanagi (2003) proposed a con-
volution kernel over labeled order trees by general-
izing the standard convolution tree kernel. The la-
beled order tree kernel is much more flexible than 
the PT kernel and can explore much larger sub-tree 
features than the PT kernel. However, the same as 
the PT kernel, the labeled order tree kernel is not 
grammar-driven. Thus, it may face the same issues 
204
(such as over-generated features) as the PT kernel 
when used in NLP applications. 
 Shen el al. (2003) proposed a lexicalized tree 
kernel to utilize LTAG-based features in parse 
reranking. Their methods need to obtain a LTAG 
derivation tree for each parse tree before kernel 
calculation. In contrast, we use the notion of op-
tional arguments to define our grammar-driven tree 
kernel and use the empirical set of CFG rules to de-
termine which arguments are optional. 
4 Experiments 
4.1 Experimental Setting 
Data: We use the CoNLL-2005 SRL shared task 
data (Carreras and M?rquez, 2005) as our experi-
mental corpus. The data consists of sections of the 
Wall Street Journal part of the Penn TreeBank 
(Marcus et al, 1993), with information on predi-
cate-argument structures extracted from the Prop-
Bank corpus (Palmer et al, 2005). As defined by 
the shared task, we use sections 02-21 for training, 
section 24 for development and section 23 for test. 
There are 35 roles in the data including 7 Core 
(A0?A5, AA), 14 Adjunct (AM-) and 14 Reference 
(R-) arguments. Table 1 lists counts of sentences 
and arguments in the three data sets. 
  
 Training Development Test
Sentences 39,832 1,346 2,416
Arguments 239,858 8,346 14,077
Table 1: Counts on the data set 
 
We assume that the semantic role identification 
has been done correctly. In this way, we can focus 
on the classification task and evaluate it more accu-
rately. We evaluate the performance with Accu-
racy. SVM (Vapnik, 1998) is selected as our classi-
fier and the one vs. others strategy is adopted and 
the one with the largest margin is selected as the 
final answer. In our implementation, we use the bi-
nary SVMLight (Joachims, 1998) and modify the 
Tree Kernel Tools (Moschitti, 2004) to a grammar-
driven one. 
 
Kernel Setup: We use the Constituent, Predicate, 
and Predicate-Constituent related features, which 
are reported to get the best-reported performance 
(Pradhan et al, 2005a), as the baseline features. We 
use Che et al (2006)?s hybrid convolution tree ker-
nel (the best-reported method for kernel-based 
SRL) as our baseline kernel. It is defined as 
(1 )  (0 1)hybrid path csK K K? ? ?= + ? ? ? (for the de-
tailed definitions of pathK and csK , please refer to 
Che et al (2006)). Here, we use our grammar-
driven tree kernel to compute pathK and csK , and we 
call it grammar-driven hybrid tree kernel while Che 
et al (2006)?s is non-grammar-driven hybrid convo-
lution tree kernel.  
We use a greedy strategy to fine-tune parameters. 
Evaluation on the development set shows that our 
kernel yields the best performance when ? (decay 
factor of tree kernel), 1? and 2? (two penalty factors 
for the grammar-driven kernel), ? (hybrid kernel 
parameter) and c (a SVM training parameter to 
balance training error and margin) are set to 0.4, 
0.6, 0.3, 0.6 and 2.4, respectively. For other parame-
ters, we use default setting. In the CoNLL 2005 
benchmark data, we get 647 rules with optional 
nodes out of the total 6,534 grammar rules and de-
fine three equivalent node feature sets as below: 
? JJ, JJR, JJS 
? RB, RBR, RBS 
? NN, NNS, NNP, NNPS, NAC, NX 
 
Here, the verb feature set ?VB, VBD, VBG, VBN, 
VBP, VBZ? is removed since the voice information 
is very indicative to the arguments of ARG0 
(Agent, operator) and ARG1 (Thing operated). 
 
Methods Accuracy (%) 
 Baseline: Non-grammar-driven 85.21 
 +Approximate Node Matching 86.27 
 +Approximate Substructure 
Matching 
87.12 
 Ours: Grammar-driven Substruc-
ture and Node Matching 
87.96 
Feature-based method with poly-
nomial kernel (d = 2) 
89.92 
 
Table 2: Performance comparison 
4.2 Experimental Results 
Table 2 compares the performances of different 
methods on the test set. First, we can see that the 
new grammar-driven hybrid convolution tree kernel 
significantly outperforms ( 2? test with p=0.05) the 
205
non-grammar one with an absolute improvement of 
2.75 (87.96-85.21) percentage, representing a rela-
tive error rate reduction of 18.6% (2.75/(100-85.21)) 
. It suggests that 1) the linguistically motivated 
structure features are very useful for semantic role 
classification and 2) the grammar-driven kernel is 
much more effective in capturing such kinds of fea-
tures due to the consideration of linguistic knowl-
edge. Moreover, Table 2 shows that 1) both the 
grammar-driven approximate node matching and the 
grammar-driven approximate substructure matching 
are very useful in modeling syntactic tree structures 
for SRL since they contribute relative error rate re-
duction of 7.2% ((86.27-85.21)/(100-85.21)) and 
12.9% ((87.12-85.21)/(100-85.21)), respectively; 2) 
the grammar-driven approximate substructure 
matching is more effective than the grammar-driven 
approximate node matching. However, we find that 
the performance of the grammar-driven kernel is 
still a bit lower than the feature-based method. This 
is not surprising since tree kernel methods only fo-
cus on modeling tree structure information. In this 
paper, it captures the syntactic parse tree structure 
features only while the features used in the feature-
based methods cover more knowledge sources.  
In order to make full use of the syntactic structure 
information and the other useful diverse flat fea-
tures, we present a composite kernel to combine the 
grammar-driven hybrid kernel and feature-based 
method with polynomial kernel: 
(1 )      (0 1)comp hybrid polyK K K? ? ?= + ? ? ?  
Evaluation on the development set shows that the 
composite kernel yields the best performance when 
? is set to 0.3. Using the same setting, the system 
achieves the performance of 91.02% in Accuracy 
in the same test set. It shows statistically significant 
improvement (?2 test with p= 0.10) over using the 
standard features with the polynomial kernel (? = 0, 
Accuracy = 89.92%) and using the grammar-driven 
hybrid convolution tree kernel (? = 1, Accuracy = 
87.96%). The main reason is that the tree kernel 
can capture effectively more structure features 
while the standard flat features can cover some 
other useful features, such as Voice, SubCat, which 
are hard to be covered by the tree kernel. The ex-
perimental results suggest that these two kinds of 
methods are complementary to each other. 
In order to further compare with other methods, 
we also do experiments on the dataset of English 
PropBank I (LDC2004T14). The training, develop-
ment and test sets follow the conventional split of 
Sections 02-21, 00 and 23. Table 3 compares our 
method with other previously best-reported methods 
with the same setting as discussed previously. It 
shows that our method outperforms the previous 
best-reported one with a relative error rate reduction 
of 10.8% (0.97/(100-91)). This further verifies the 
effectiveness of the grammar-driven kernel method 
for semantic role classification. 
  
Method Accuracy (%)
Ours (Composite Kernel)      91.97 
Moschitti (2006): PAF kernel only    87.7 
Jiang et al (2005): feature based    90.50 
Pradhan et al (2005a): feature based    91.0 
 
Table 3: Performance comparison between our 
method and previous work 
 
Training Time Method 
  4 Sections  19 Sections
Ours: grammar-
driven tree kernel 
~8.1 hours ~7.9 days 
Moschitti (2006): 
non-grammar-driven 
tree kernel 
~7.9 hours ~7.1 days 
 
Table 4: Training time comparison 
 
Table 4 reports the training times of the two ker-
nels. We can see that 1) the two kinds of convolu-
tion tree kernels have similar computing time. Al-
though computing the grammar-driven one requires 
exponential time in its worst case, however, in 
practice, it may only need 1 2(| | | |)O N N?  or lin-
ear and 2) it is very time-consuming to train a SVM 
classifier in a large dataset.  
5 Conclusion and Future Work 
In this paper, we propose a novel grammar-driven 
convolution tree kernel for semantic role classifica-
tion. More linguistic knowledge is considered in 
the new kernel design. The experimental results 
verify that the grammar-driven kernel is more ef-
fective in capturing syntactic structure features than 
the previous convolution tree kernel because it al-
lows grammar-driven approximate matching of 
substructures and node features. We also discuss 
the criteria to determine the optional nodes in a 
206
CFG rule in defining our grammar-driven convolu-
tion tree kernel. 
The extension of our work is to improve the per-
formance of the entire semantic role labeling system 
using the grammar-driven tree kernel, including all 
four stages: pruning, semantic role identification, 
classification and post inference. In addition, a 
more interesting research topic is to study how to 
integrate linguistic knowledge and tree kernel 
methods to do feature selection for tree kernel-
based NLP applications (Suzuki et al, 2004). In 
detail, a linguistics and statistics-based theory that 
can suggest the effectiveness of different substruc-
ture features and whether they should be generated 
or not by the tree kernels would be worked out. 
References  
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The 
Berkeley FrameNet Project. COLING-ACL-1998  
Xavier Carreras and Llu?s M?rquez. 2004. Introduction to 
the CoNLL-2004 shared task: Semantic role labeling. 
CoNLL-2004  
Xavier Carreras and Llu?s M?rquez. 2005. Introduction to 
the CoNLL-2005 shared task: Semantic role labeling. 
CoNLL-2005  
Eugene Charniak. 2000. A maximum-entropy-inspired 
parser. In Proceedings ofNAACL-2000 
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 
2006. A hybrid convolution tree kernel for semantic 
role labeling. COLING-ACL-2006(poster) 
Michael Collins and Nigel Duffy. 2001. Convolution 
kernels for natural language. NIPS-2001 
 Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics, 
28(3):245?288 
David Haussler. 1999. Convolution kernels on discrete 
structures. Technical Report UCSC-CRL-99-10 
Zheng Ping Jiang, Jia Li and Hwee Tou Ng. 2005. Se-
mantic argument classification exploiting argument 
interdependence. IJCAI-2005 
T. Joachims. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Kashima H. and Koyanagi T. 2003. Kernels for Semi-
Structured Data. ICML-2003 
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello 
Cristianini and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learn-
ing Research, 2:419?444 
Mitchell P. Marcus, Mary Ann Marcinkiewicz  and Bea-
trice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational 
Linguistics, 19(2):313?330 
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. ACL-2004 
Alessandro Moschitti. 2006. Syntactic kernels for natu-
ral language learning: the semantic role labeling 
case. HLT-NAACL-2006 (short paper)  
Rodney D. Nielsen and Sameer Pradhan. 2004. Mixing 
weak learners in semantic parsing. EMNLP-2004 
Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. 
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1) 
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne 
Ward, James H. Martin and Daniel Jurafsky. 2005a. 
Support vector learning for semantic argument classi-
fication. Journal of Machine Learning 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James 
Martin and Daniel Jurafsky. 2005b. Semantic role la-
beling using different syntactic views. ACL-2005 
Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zi-
mak. 2004. Semantic role labeling via integer linear 
programming inference. COLING-2004 
Vasin Punyakanok, Dan Roth and Wen Tau Yih. 2005. 
The necessity of syntactic parsing for semantic role 
labeling. IJCAI-2005 
Libin Shen, Anoop Sarkar and A. K. Joshi. 2003. Using 
LTAG based features in parse reranking. EMNLP-03 
Jun Suzuki, Hideki Isozaki and Eisaku Maede. 2004. 
Convolution kernels with feature selection for Natu-
ral Language processing tasks. ACL-2004 
Vladimir N. Vapnik. 1998. Statistical Learning Theory. 
Wiley 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. EMNLP-2004 
Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-
ardella. 2003. Kernel methods for relation extraction. 
Machine Learning Research, 3:1083?1106 
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. COLING-ACL-2006 
207
In: Proceedings of CoNLL-2000 and LLL-2000, pages 163-165, Lisbon, Portugal, 2000. 
Hybrid Text Chunking 
GuoDong Zhou and J ian  Su  and TongGuan Tey  
Kent Ridge Digital Labs 
21 Heng Mui Keng Terrace 
Singapore 119613 
{zhougd, sujian, tongguan}@krdl, org.sg 
Abst rac t  
This paper proposes an error-driven HMM- 
based text chunk tagger with context-dependent 
lexicon. Compared with standard HMM-based 
tagger, this tagger incorporates more contextual 
information into a lexical entry. Moreover, an 
error-driven learning approach is adopted to de- 
crease the memory requirement by keeping only 
positive lexical entries and makes it possible 
to further incorporate more context-dependent 
lexical entries. Finally, memory-based learning 
is adopted to further improve the performance 
of the chunk tagger. 
1 In t roduct ion  
The idea of using statistics for chunking goes 
back to Church(1988), who used corpus frequen- 
cies to determine the boundaries of simple non- 
recursive noun phrases. Skut and Brants(1998) 
modified Church's approach in a way permitting 
efficient and reliable recognition of structures of 
limited depth and encoded the structure in such 
a way that it can be recognised by a Viterbi 
tagger. Our approach follows Skut and Brants' 
way by employing HMM-based tagging method 
to model the chunking process. 
2 HMM-based  Chunk  Tagger  w i th  
Context -dependent  Lex icon  
Given a token sequence G~ = glg2""gn , 
the goal is to find an optimal tag sequence 
T~ = tit2.. "tn which maximizes log P(T~IG~): 
. P (T~,G?)  
log P(T~IG?) = log P(T~) +log p(T~)P(G?) 
The second item in the above equation is the 
mutual information between the tag sequence 
T~ and the given token sequence G~. By as- 
suming that the mutual information between 
G~ and T~ is equal to the summation off mutual 
information between G~ and the individual tag 
ti (l<i_<n): 
l P(T~, G~) n _ P(ti, G?) 
o g p - ~ )  = E log P'(t~-P-~) 
i=1  
n 
MI(T~, G?) = ~ MI(ti, G~), 
i=1  
we have: 
n P(ti, G~) 
log P(T~IG~)  = log P(T~)+~ log P(~i)P-~) 
i=1  
n n 
= log P (T~) -  ~ log P(ti) + ~ log P(tilG?) 
i----1 i=1  
The first item of above equation can be solved 
by chain rules. Normally, each tag is assumed 
to be probabilistic dependent on the N-1 previ- 
ous tags. Here, backoff bigram(N=2) model is 
used. The second item is the summation of log 
probabilities of all the tags. Both the first item 
and second item constitute the language model 
component while the third item constitutes the 
lexicon component. Ideally the third item can 
be estimated by the forward-backward algo- 
rithm(Rabiner 1989) recursively for the first- 
order(Rabiner 1989) or second-order HMMs. 
However, several approximations on it will be 
attempted later in this paper instead. The 
stochastic optimal tag sequence can be found 
by maximizing the above equation over all the 
possible tag sequences using the Viterbi algo- 
rithm. 
The main difference between our tagger and 
the standard taggers lies in our tagger has a 
context-dependent lexicon while others use a 
context-independent lexicon. 
163 
For chunk tagger, we have gl = piwi where 
W~ = wlw2""Wn is the word sequence and 
P~ = PlP2""Pn is the part-of-speech(POS) 
sequence. Here, we use structural tags to 
representing chunking(bracketing and labeling) 
structure. The basic idea of representing 
the structural tags is similar to Skut and 
Brants(1998) and the structural tag consists of 
three parts: 
1) Structural relation. The basic idea is sim- 
ple: structures of limited depth are encoded 
using a finite number of flags. Given a se- 
quence of input tokens(here, the word and POS 
pairs), we consider the structural relation be- 
tween the previous input token and the current 
one. For the recognition of chunks, it is suffi- 
cient to distinguish the following four different 
structural relations which uniquely identify the 
sub-structures of depth l(Skut and Brants used 
seven different structural relations to identify 
the sub-structures of depth 2). 
? 00: the current input token and the previ- 
ous one have the same parent 
? 90: one ancestor of the current input token 
and the previous input token have the same 
parent 
? 09: the current input token and one an- 
cestor of the previous input token have the 
same parent 
? 99 one ancestor of the current input token 
and one ancestor of the previous input to- 
ken have the same parent 
Compared with the B-Chunk and I-Chunk 
used in Ramshaw and Marcus(1995)~, structural 
relations 99 and 90 correspond to B-Chunk 
which represents the first word of the chunk, 
and structural relations 00 and 09 correspond 
to I-Chunk which represents each other in the 
chunk while 90 also means the beginning of the 
sentence and 09 means the end of the sentence. 
2)Phrase category. This is used to identify 
the phrase categories of input tokens. 
3)Part-of-speech. Because of the limited 
number of structural relations and phrase cate- 
gories, the POS is added into the structural tag 
to represent more accurate models. 
Principally, the current chunk is dependent 
on all the context words and their POSs. How- 
ever, in order to decrease memory require- 
ment and computational complexity, our base- 
line HMM-based chunk tagger only considers 
previous POS, current POS and their word to- 
kens whose POSs are of certain kinds, such as 
preposition and determiner etc. The overall 
precision, recall and F~=i rates of our baseline 
tagger on the test data of the shared task are 
89.58%, 89.56% and 89.57%. 
3 Error-driven Learning 
After analysing the chunking results, we find 
many errors are caused by a limited number of 
words. In order to overcome such errors, we 
include such words in the chunk dependence 
context by using error-driven learning. First, 
the above HMM-based chunk tagger is used to 
chunk the training data. Secondly, the chunk 
tags determined by the chunk tagger are com- 
pared with the given chunk tags in the training 
data. For each word, its chunking error number 
is summed. Finally, those words whose chunk- 
ing error numbers are equal to or above a given 
threshold(i.e. 3) are kept. The HMM-based 
chunk tagger is re-trained with those words con- 
sidered in the chunk dependence ontext. 
The overall precision, recall and FZ=i rates 
of our error-driven HMM-based chunk tagger 
on the test data of the shared task are 91.53%, 
92.02% and 91.77 
4 Memory based Learning 
Memory-based learning has been widely used 
in NLP tasks in the last decade. Principally, it 
falls into two paradigms. First paradigm rep- 
resents examples as sets of features and car- 
ries out induction by finding the most simi- 
lar cases. Such works include Daelemans et 
a1.(1996) for POS tagging and Cardie(1993) 
for syntactic and semantic tagging. Second 
paradigm makes use of raw sequential data 
and generalises by reconstructing test examples 
from different pieces of the training data. Such 
works include Bod(1992) for parsing, Argamon 
et a1.(1998) for shallow natural anguage pat- 
terns and Daelemans et a1.(1999) for shallow 
parsing. 
The memory-based method presented here 
follows the second paradigm and makes use of 
raw sequential data. Here, generalization is per- 
formed online at recognition time by comparing 
164 
the new pattern to the ones in the training cor- 
pus. 
Given one of the N most probable chunk se- 
quences extracted by the error-driven HMM- 
based chunk tagger, we can extract a set of 
chunk patterns, each of them with the format: 
XP 1 n n+l r~+l = poroPlrn Pn+l, where is the 
structural relation between Pi and Pi+l. 
As an example, from the bracketed and la- 
beled sentence: 
\[NP He/PRP \] \[VP reckons/VSZ \] 
\[NP the/DT current/ J J  account/NN 
deficit/NN \] \[VP will/MD narrow/VB 
\] \[ PP to /TO\ ]  \[NP only/RB #/# 
1.8/CD billion/CD \] \[PP in/IN \ ] \ [NP 
September/NNP \] \[O ./. \] 
we can extract following chunk patterns: 
NP=NULL 90 PRP 99 VBZ 
VP=PRP 99 VBZ 99 DT 
NP=VBZ 99 DT JJ NN NN 99 MD 
PP=VB 99 TO 99 RB 
NP=TO 99 RB # CD CD 99 IN 
PP=CD 99 IN 99 NNP 
NP=IN 99 NNP 99 . 
O=NNP 99 . 09 NULL 
For every chunk pattern, we estimate its proba- 
bility by using memory-based learning. If the 
chunk pattern exists in the training corpus, 
its probability is computed by the probability 
of such pattern among all the chunk patterns. 
Otherwise, its probability is estimated by the 
multiply of its overlapped sub-patterns. Then 
the probability of each of the N most probable 
chunk sequences i adjusted by multiplying the 
probabilities of its extracted chunk patterns. 
Table 1 shows the performance oferror-driven 
HMM-based chunk tagger with memory-based 
learning. 
5 Conc lus ion  
It is found that the performance with the help of 
error-driven learning is improved by 2.20% and 
integration of memory-based learning further 
improves the performance by 0.35% to 92.12%. 
For future work, the experimentation  large 
scale task will be speculated in the near future. 
Finally, a closer integration of memory-based 
method with HMM-based chunk tagger will also 
be conducted. 
test data 
ADJP 
ADVP 
CONJP 
INTJ 
LST 
NP 
PP 
PRT 
SBAR 
VP 
precision 
76.17% 
78.25% 
46.67% 
20.00% 
00.00% 
92.19% 
96.09% 
72.36% 
83.56% 
92.77% 
recall 
70.78% 
78.52% 
77.78% 
5O.OO% 
OO.O0% 
92.59% 
96.94% 
83.96% 
79.81% 
92.85% 
all 91.99% 92.25% 
F~=i 
73.37 
78.39 
58.33 
28.57 
00.00 
92.39 
96.51 
77.73 
81.64 
92.81 
92.12 
Table 1: performance of chunking 
References  
S. Argamon, I. Dagan, and Y. Krymolowski. 1998. 
A memory-based approach to learning shallow 
natural language patterns. In COLING/ACL- 
1998, pages 67-73. Montreal, Canada. 
R. Bod. 1992. A computational model of lan- 
guage performance: Data-oriented parsing. In 
COLING-1992, pages 855-859. Nantes, France. 
C. Cardie. 1993. A case-based approach to knowl- 
edge acquisition for domain-specific sentence anal- 
ysis. In Proceeding of the I1th National Con- 
ference on Artificial Intelligence, pages 798-803. 
Menlo Park, CA, USA. AAAI Press. 
K.W. Church. 1988. A stochastic parts program and 
noun phrase parser for unrestricted text. In Pro- 
ceedings of Second Conference on Applied Natu- 
ral Language Processing, pages 136-143. Austin, 
Texas, USA. 
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 
1996. Mbt: A memory-based part-of-speech tag- 
ger generator. In Proceeding of the Fourth Work- 
shop on Large Scale Corpora, pages 14-27. ACL 
SIGDAT. 
W. Daelemans, S. Buchholz, and J. Veenstra. 1999. 
Memory-based shallow parsing. In CoNLL-1999, 
pages 53-60. Bergen, Norway. 
L.R. Rabiner. 1989. A tutorial on hidden markov 
models and selected applications in speech recog- 
nition. In Proceedings of the IEEE, volume 77, 
pages 257-286. 
Lance A. Ramshaw and Mitchell P. Marcus. 1995. 
Text chunking using transformation-based learn- 
ing. In Proceedings of the Third ACL Work- 
shop on Very Large Corpora. Cambridge, Mas- 
sachusetts, USA. 
W. Skut and T. Brants. 1998. Chunk tagger: sta- 
tistical recognition of noun phrases. In ESSLLI- 
1998 Workshop on Automated Acquisition of Syn- 
tax and Parsing. Saarbruucken, Germany. 
165 
Error-driven HMM-based Chunk Tagger 
with Context-dependent Lexicon 
GuoDong ZHOU 
Kent Ridge Digital Labs 
21 Heng Hui Keng Terrace 
Singapore 119613 
zhougd@krdl.org.sg 
Jian SU 
Kent Ridge Digital Labs 
21 Heng Hui Keng Terrace 
Singapore 119613 
sujian@krdl.org.sg 
Abstract 
This paper proposes a new error-driven HMM- 
based text chunk tagger with context-dependent 
lexicon. Compared with standard HMM-based 
tagger, this tagger uses a new Hidden Markov 
Modelling approach which incorporates more 
contextual information into a lexical entry. 
Moreover, an error-driven learning approach is 
adopted to decrease the memory requirement by 
keeping only positive lexical entries and makes 
it possible to further incorporate more context- 
dependent lexical entries. Experiments how 
that this technique achieves overall precision 
and recall rates of 93.40% and 93.95% for all 
chunk types, 93.60% and 94.64% for noun 
phrases, and 94.64% and 94.75% for verb 
phrases when trained on PENN WSJ TreeBank 
section 00-19 and tested on section 20-24, while 
25-fold validation experiments of PENN WSJ 
TreeBank show overall precision and recall 
rates of 96.40% and 96.47% for all chunk types, 
96.49% and 96.99% for noun phrases, and 
97.13% and 97.36% for verb phrases. 
Introduction 
Text chunking is to divide sentences into non- 
overlapping segments on the basis of fairly 
superficial analysis. Abney(1991) proposed this 
as a useful and relatively tractable precursor to 
full parsing, since it provides a foundation for 
further levels of analysis, while still allowing 
more complex attachment decisions to be 
postponed toa later phase. 
Text chunking typically relies on fairly 
simple and efficient processing algorithms. 
Recently, many researchers have looked at text 
chunking in two different ways: Some 
researchers have applied rule-based methods, 
combining lexical data with finite state or other 
rule constraints, while others have worked on 
inducing statistical models either directly from 
the words and/or from automatically assigned 
part-of-speech classes. On the statistics-based 
approaches, Skut and Brants(1998) proposed a
HMM-based approach to recognise the syntactic 
structures of limited length. Buchholz, Veenstra 
and Daelemans(1999), and Veenstra(1999) 
explored memory-based learning method to fred 
labelled chunks. Ratnaparkhi(1998) used 
maximum entropy to recognise arbitrary chunk 
as part of a tagging task. On the rule-based 
approaches, Bourigaut(1992) used some 
heuristics and a grammar to extract 
"terminology noun phrases" from French text. 
Voutilainen(1993) used similar method to detect 
English noun phrases. Kupiec(1993) applied. 
finite state transducer in his noun phrases 
recogniser for both English and French. 
Ramshaw and Marcus(1995) used 
transformation-based l arning, an error-driven 
learning technique introduced by Eric 
Bn11(1993), to locate chunks in the tagged 
corpus. Grefenstette(1996) applied finite state 
transducers to fred noun phrases and verb 
phrases. 
In this paper, we will focus on statistics- 
based methods. The structure of this paper is as 
follows: In section 1, we will briefly describe 
the new error-driven HMM-based chunk tagger 
with context-dependent lexicon in principle. In 
section 2, a baseline system which only includes 
the current part-of-speech in the lexicon is 
given. In section 3, several extended systems 
with different context-dependent lexicons are 
described. In section 4, an error=driven learning 
method is used to decrease memory requirement 
of the lexicon by keeping only positive lexical 
71 
entries and make it possible to further improve 
the accuracy by merging different context- 
dependent lexicons into one after automatic 
analysis of the chunking errors. Finally, the 
conclusion is given. 
The data used for all our experiments i
extracted from the PENN" WSJ Treebank 
(Marcus et al 1993) by the program provided 
by Sabine Buchholz from Tilbug University. 
We use sections 00-19 as the training data and 
20-24 as test data. Therefore, the performance is 
on large scale task instead of small scale task on 
CoNLL-2000 with the same evaluation 
program. 
For evaluation of our results, we use the 
precision and recall measures. Precision is the 
percentage of predicted chunks that are actually 
correct while the recall is the percentage of 
correct chunks that are actually found. For 
convenient comparisons of only one value, we 
also list the F~= I value(Rijsbergen 1979): 
(/32 + 1). precision, recall 
, with/3 = 1. 
/3 2. precision + recall 
1 HMM-based Chunk Tagger 
The idea of using statistics for chunking goes 
back to Church(1988), who used corpus 
frequencies to determine the boundaries of 
simple non-recursive noun phrases. Skut and 
Brants(1998) modified Church's approach in a 
way permitting efficient and reliable recognition 
of structures of limited depth and encoded the 
structure in such a way that it can be recognised 
by a Viterbi tagger. This makes the process run 
in time linear to the length of the input string. 
Our approach follows Skut and Brants' way 
by employing HMM-based tagging method to 
model the chunking process. 
Given a token sequence G~ = g~g2 ""g, ,  
the goal is to fred a stochastic optimal tag 
sequence Tin = tlt2...t n which maximizes 
log P(T~" I Of ) : 
e(:q",G?) 
log P(Ti n \[ G? ) = log P(Ti n ) + log P(Ti n )" P(G? ) 
The second item in the above equation is the 
mutual information between the tag sequence 
Tin and the given token sequence G~. By 
assuming that the mutual information between 
G~ and T1 ~ is equal to the summation of mutual 
information between G~ and the individual tag 
ti(l_<i_<n ) : 
n log P(TI"' G?) = ~ log P(t,, G~) 
e(Tln ). P(G~) i=1 P(t,). P(G? ) 
or  
n 
n MI(T~ ~ , G~ ) = ~ MI(t, ,  G? ) , 
i= l  
we have: 
log P(T~ n I G~) 
= log P(T1 n ) + ~, log P(ti' G? )_ 
P(t i). P(G?) 
rl n 
= log P(T1 ~ ) - Z log P(t, ) + ~ log P(t, \[ G? ) 
i=1  i=1 
The first item of above equation can be 
solved by using chain rules. Normally, each tag 
is assumed to be probabilistic dependent on the 
N-1 previous tags. Here, backoff bigram(N=2) 
model is used. The second item is the 
summation of log probabilities of all the tags. 
Both the first item and second item correspond 
to the language model component of the tagger 
while the third item corresponds to the lexicon 
component of the tagger. Ideally the third item 
can be estimated by using the forward-backward 
algorithm(Rabiner 1989) recursively for the 
first-order(Rabiner 1989) or second-order 
HMMs(Watson and Chunk 1992). However, 
several approximations on it will be attempted 
later in this paper instead. The stochastic 
optimal tag sequence can be found by 
maxmizing the above equation over all the 
possible tag sequences. This is implemented by
the Viterbi algorithm. 
The main difference between our tagger and 
other standard taggers lies in our tagger has a 
context-dependent lexicon while others use a 
context-independent l xicon. 
For chunk tagger, we haveg 1= piwi where 
W~ n = w~w2---w n is the word-sequence and 
P~ = PiP2 "" P~ is the part-of-speech 
72 
sequence. Here, we use structural tags to 
representing chunking(bracketing and labelling) 
structure. The basic idea of representing the 
structural tags is similar to Skut and 
Brants(1998) and the structural tag consists of 
three parts: 
1) Structural relation. The basic idea is simple: 
structures of limited depth are encoded using a 
finite number of flags. Given a sequence of 
input tokens(here, the word and part-of-speech 
pairs), we consider the structural relation 
between the previous input token and the 
current one. For the recognition of chunks, it is 
sufficient to distinguish the following four 
different structural relations which uniquely 
identify the sub-structures of depth l(Skut and 
Brants used seven different structural relations 
to identify the sub-structures of depth 2). 
00 the current input token and the previous one 
have the same parent 
90 one ancestor of the current input token and 
the previous input oken have the same parent 
09 the current input token and one ancestor of 
the previous input oken have the same parent 
99 one ancestor of the current input token and 
one ancestor of the previous input token have 
the same parent 
For example, in the following chunk tagged 
sentence(NULL represents the beginning and 
end of the sentence): 
NULL \[NP He/PRP\] \[VP reckons/VBZ\] [NP 
the/DT current/JJ account/NN deficit/NN\] \[VP 
will/MD narrow/VB\] \[PP to/TO\] \[NP only/RB 
#/# 1.8/CD billion/CD\] \[PP in/IN\] \[NP 
September/NNP\] \[O./.\] NULL 
the corresponding structural relations between 
two adjacent input okens are: 
90(NULL He/PRP) 
99(He/PRP reckons/VBZ) 
99(reckons/VBZ the/DT) 
00(the/DT current/JJ) 
00(current/JJ account/NN) 
00(account/NN deficit/NN) 
99(deficit/NN will/MD) 
00(will/MD narrow/VB) 
99(narrow/VB to/TO) 
99(to/TO only/RB) 
O0(only/RB #/#) 
00(#/# 1.8/CD) 
00(1.8/CD billion/CD) 
99(billion/CD in/IN) 
99(in/IN september/NNP) 
99(september/NNP ./.)
09(./. NULL) 
Compared with the B-Chunk and I-Chunk 
used in Ramshaw and Marcus(1995), structural 
relations 99 and 90 correspond to B-Chunk 
which represents the first word of the chunk, 
and structural relations 00 and 09 correspond to 
I-Chunk which represnts each other in the chunk 
while 90 also means the beginning of the 
sentence and 09 means the end of the sentence. 
2)Phrase category. This is used to identify the 
phrase categories of input tokens. 
3)Part-of-speech. Because of the limited 
number of structural relations and phrase 
categories, the part-of-speech is added into the 
structural tag to represent more accurate models. 
For the above chunk tagged sentence, the 
structural tags for all the corresponding input 
tokens are: 
90 PRt~NP(He/PRP) 
99_VB Z_VP(reckons/VBZ) 
99 DT NP(the/DT) 
O0 JJ NP(currentJJJ) 
00_N/'~NP(account/NN) 
00 N1NNP(deficiffNN) 
99_MDSVP(will/MD) 
00 VB_VP(narrow/VB) 
99_TO PP(to/TO) 
99_RB~,IP(only/RB) 
oo_# NP(#/#) 
00 CD_NP(1.8/CD) 
0(~CD~qP(billion/CD) 
99_IN PP(in/IN) 
99~lNP~,lP(september/NNP) 
99_._0(./.) 
2 The Baseline System 
As the baseline system, we assume 
P(t i I G?)= P(t i I pi ). That is to say, only the 
current part-of-speech is used as a lexical entry 
to determine the current structural chunk tag. 
Here, we define: 
? ? is the list of lexical entries in the 
chunking lexicon, 
73 
? \[ @ \[ is the number of lexical entries(the size 
of the chunking lexicon) 
? C is the training data. 
For the baseline system, we have : 
? @={pi,p~3C}, where Pi is a part-of- 
speech existing in the tra\]Lning data C 
? \]@ \[=48 (the number of part-of-speech tags 
in the training data). 
Table 1 gives an overview of the results of 
the chunking experiments. For convenience, 
precision, recall and F#_ 1 values are given 
seperately for the chunk types NP, VP, ADJP, 
ADVP and PP. 
Type Precision Recall Fa__~ 
Overall 87.01 89.68 88.32 
NP 90.02 90.50 90.26 
VP 89.86 93.14 91.47 
ADJP 70.94 63.84 67.20 
ADVP 57.98 80.33 I 67.35 
PP 85.95 96.62 90.97 
Table 1 : Results of chunking experiments with 
the lexical entry list : ~ = { pi, p~3C} 
3 Context-dependent Lexicons 
In the last section, we only use current part-of- 
speech as a lexical entry. In this section, we will 
attempt o add more contextual information to 
approximate P(t i/G~). This can be done by 
adding lexical entries with more contextual 
information into the lexicon ~.  In the 
following, we will discuss five context- 
dependent lexicons which consider different 
contextual information. 
3.1 Context of current part-of-speech and 
current word 
Here, we assume: 
e(t i I G~) = I P(ti I p~wi) 
\[ P(tl I Pi) 
where 
piwi ~ dp 
PiWi ~ dp 
~={piwi,piwi3C}+{pi,pi3C } and piwi is a 
part-of-speech and word pair existing in the 
training data C.  
In this case, the current part-of-speech and 
word pair is also used as a lexical entry to 
determine the current structural chunk tag and 
we have a total of about 49563 lexical 
entries(\[ ? \]=49563). Actually, the lexicon used 
here can be regarded as context-independent. 
The reason we discuss it in this section is to 
distinguish it from the context-independent 
lexicon used in the baseline system. Table 2 
give an overview of the results of the chunking 
experiments on the test data. 
Type \[Precision 
Overall 90.32 
NP 90.75 
VP 90.88 
ADJP 76.01 
ADVP 72.67 
PP 94.96 
Table 2 : Results of chunking experiments 
the lexical entry 
= {piwi ,Piwi3C} "1" {Pi" Pi 3C} 
Recall Fa~.l 
92.18 9i.24 
92.14 91.44 
92.78 91.82 
70.00 72.88 
88.33 79.74 
96.48 95.71 
with 
list : 
Table 2 shows that incorporation of current 
word information improves the overall F~=~ 
value by 2.9%(especially for the ADJP, ADVP 
and PP chunks), compared with Table 1 of the 
baseline system which only uses current part-of- 
speech information. This result suggests that 
current word information plays a very important 
role in determining the current chunk tag. 
3.2 Context of previous part-of-speech and 
current part-of-speech 
Here, we assume : 
P(t i / G~) 
I P(ti / pi-lPi ) Pi-lPi E 
= \[ P(ti I Pi) Pi-! Pi ~ ~ 
where 
= {Pi-l Pi, P~-1Pi 3C} + { Pi, pi3C} and Pi-lPi 
is a pair of previous part-of-speech and current 
part-of-speech existing in the training data C.  
In this case, the previous part-of-speech and 
current part-of-speech pair is also used as a 
lexical entry to determine the current structural 
chunk tag and we have a total of about 1411 
lexical entries(l~\]=1411). Table 3 give an 
overview of the results of the chunking 
experiments. 
74 
Type 
Overall 
Precision 
88.63 
NP 90.77 
VP 92.46 
ADJP 74.93 60.13 66.72 
ADVP 71.65 73.21 72.42 
PP 87.28 91.80 89.49 
Table 3: Results of chunking experiments with 
the lexical entry list : ? = 
{Pi-lPi, Pi-lPi 3C} + {Pi, Pi 3C} 
Recall F#= I 
89.00 88.82 
91.18 90.97 
92.98 92.72 
Compared with Table 1 of the baseline 
system, Table 3 shows that additional contextual 
information of previous part-of-speech improves 
the overall F/~_~ value by 0.5%. Especially, 
F/3_ ~ value for VP improves by 1.25%, which 
indicates that previous part-of-speech 
information has a important role in determining 
the chunk type VP. Table 3 also shows that the 
recall rate for chunk type ADJP decrease by 
3.7%. It indicates that additional previous part- 
of-speech information makes ADJP chunks 
easier to merge with neibghbouring chunks. 
3.3 Context of previous part-of-speech, 
previous word and current part-of-speech 
Here, we assume : 
P(t, / G~) 
IP(ti / pi_lwi_lpi) pi_lwi_lpl ~ dp 
I 
\[ P(ti \[ Pi ) Pi-lWi-I Pi ~ ~ 
where 
= { Pi-i wi-l Pi, Pi-l wi-I Pi3 C} + { Pi, Pi 3 C }, 
where pi_lwi_lp~ is a triple pattern existing in 
the training corpus. 
In this case, the previous part-of-speech, 
previous word and current part-of-speech triple 
is also used as a lexical entry to determine the 
current structural chunk tag and } ? 1=136164. 
Table 4 gives the results of the chunking 
experiments. Compared with Table 1 of the 
baseline system, Table 4 shows that additional 
136116 new lexical entries of format 
Pi-lw~-lPi improves the overall F#= l value by 
3.3%. Compared with Table 3 of the extended 
system 2.2 which uses previous part-of-speech 
and current part-of-speech as a lexical entry, 
Table 4 shows that additional contextual 
information of previous word improves the 
overall Fa= 1 value by 2.8%. 
Type Precision Recall F~=l 
Overall 91.23 92.03 91.63 
NP 92.89 93.85 93.37 
VP 94.10 94.23 94.16 
ADJP 79.83 69.01 74.03 
ADVP 76.91 80.53 78.68 
PP 90.41 94.77 92.53 
Table 4 : Results of chunking experiments with 
the lexical entry list : 
={p,_lw~_~ p,,p,_~ w,_ip,3C } + {Pi , p~3C} 
3.4 Context of previous part-of-speech, current 
part-of-speech and current word 
Here, we assume : 
P(t i I G~ ) 
IP(tt I Pi-i PiWi) Pi-I piwi E dp 
\[ P(ti / Pi ) Pi-I Pi Wi ~ 1I) 
where 
= {Pi-lPiWi, Pi-lP~W~ 3C} + {Pi, Pi3C}, 
where pi_lpiw~ is a triple pattern existing in 
the training and \] ? \[=131416. 
Table 5 gives the results of the chunking 
experiments. 
Type Precision Recall F/3= 1
Overall 92.67 93.43 93.05 
NP 93.35 94.10 93.73 
VP 93.05 94.30 93.67 
ADJP 80.65 72.27 76.23 
ADVP 78.92 84.48 81.60 
PP 95.30 96.67 95.98 
Table 5: Results of chunking experiments with 
the lexical entry list : 
={Pi-lPiWi, P,-iP, w,3C} + {pi , Pi 3C} 
Compared with Table 2 of the extended 
system which uses current part-of-speech and 
current word as a lexical entry, Table 5 shows 
that additional contextual information of 
previous part-of-speech improves the overall 
Fa= 1 value by 1.8%. 
3.5 Context of previous part-of-speech, 
previous word, current part-of-speech and 
current word 
Here, the context of previous part-of-speech, 
current part-of-speech and current word is used 
as a lexical entry to determine the current 
75 
structural chunk tag and qb = 
{Pi-l wi-lPiWi, Pi-lwi-~piwi 36'} + {Pi, P i3C} , 
where p~_lWi_~P~W~ is a pattern existing in the 
training corpus. Due to memory limitation, only 
lexical entries which occurs :more than 1 times 
are kept. Out of 364365 possible lexical entries 
existing in the training data, 98489 are kept( 
1~ 1=98489). 
= I P(ti/Pi-\]wi-,PiWli) 
\[ P(t, lp,) pi_lwi_lpiwi ~ 
Table 6 gives the results of the chunking 
experiments. 
Type 
Overall 
NP 
VP 
ADJP 
ADVP 
PP 
Precision 
92.28 
93.50 
92.62 
81.39 
75.09 
94.12 
Recall 
93.04 
93.53 
94.07 
72.17 
86.23 
97.12 
F~=l 
92.66 
93.52 
93.35 
76.50 
80.27 
95.59 
Table 6: Results of chunking experiments with 
the lexical entry list : ? = 
{Pi-l wi-\]PiWi, Pi-lwi-lpiwi3C} + {Pi, p~3C} 
Compared with Table 2 of the extended 
system which uses current part-of-speech and 
current word as a lexical entry, Table 6 shows 
that additional contextual information of 
previous part-of-speech improves the overall 
Ft3=l value by 1.8%. 
3.6 Conclusion 
Above experiments shows that adding more 
contextual information i to lexicon significantly 
improves the chunking accuracy. However, this 
improvement is gained at the expense of a very 
large lexicon and we fred it difficult o merge all 
the above context-dependent l xicons in a single 
lexicon to further improve the chunking 
accurracy because of memory limitation. In 
order to reduce the size of lexicon effectively, 
an error-driven learning approach is adopted to 
examine the effectiveness of lexical entries and 
make it possible to further improve the 
chunking accuracy by merging all the above 
context-dependent l xicons in a single lexicon. 
This will be discussed in the next section. 
4 Error-driven Learning 
In section 2, we implement a basefine system 
which only considers current part-of-speech as a 
lexical entry to dete, ufine the current chunk tag 
while in section 3, we implement several 
extended systems which take more contextual 
information i to consideration. 
Here, we will examine the effectiveness of
lexical entries to reduce the size of lexicon and 
make it possible to further improve the 
chunking accuracy by merging several context- 
dependent lexicons in a single lexicon. 
For a new lexical entry e i, the effectiveness 
F~ (e i) is measured by the reduction in error 
which results from adding the lexical entry to 
- -  ~ Er ro r  (e,). the lexicon : F~ (e i ) = F :  rr?r (e i ) - o+Ao 
Here, F,~ r~?r (el) is the chunking error number 
of the lexical entry e i for the old lexicon 
r~ Er ror  / x and r~,+~ te i) is the chunking error number of 
the lexical entry e i for the new lexicon 
+ AO where e~ e A~ (A~ is the list of 
new lexical entries added to the old lexicon ~ ). 
If F o (e i ) > 0, we define the lexical entry ei as 
positive for lexicon ~.  Otherwise, the lexical 
entry e i is negative for lexicon ~.  
Tables 7 and 8 give an overview of the 
effectiveness distributions for different lexicons 
applied in the extended systems, compared with 
the lexicon appfied in the baseline system, on 
the test data and the training data, respectively. 
Tables 7 and 8 show that only a minority of 
lexical entries are positive. This indicates that 
discarding non-positive lexical entries will 
largely decrease the lexicon memory 
requirement while keeping the chunking 
accurracy. 
Context Positive 
1800 
209 
Negative 
314 
136 
Total 
4083 I 155 
Table 7 : The effectiveness of lexical 
the test data ..... 
49515 
1363 
2876 229 136116 
2895 193 131368 
98441 
entries on 
76 
Context Positive i Negative 
6724l 
Type i Precision Recall Fa=l 
Overall 91.02 92.21 91.61 
NP 92.36 93.69 93.02 
VP 93.68 94.94 94.30 
ADJP 78.28 71.46 74.71 
ADVP 76.77 81.79 79.20 
PP 90.67 95.37 92.96 
Total 
vos,w, 719 49515 
eos,_,Pos, 357 196 1363 
POS,.~w,.,eos,, 13205 582 136116 
POS,_,eos,w, 14186 325 131368 
POS,.,w,_leos,,w, 15516 144 98441 
Table 8 : The effectiveness of lexical entries on 
the training data 
Tables 9-13 give the performances of the 
five error-driven systems which discard all the 
non-positive l xical enrties on the training data. 
Here, ~'  is the lexicon used in the baseline 
system, dP'={pi,pi3C } and A~=~-~' .  It 
is found that Ffl_~ values of error driven 
systems for context of current part-of-speech 
and word pak and for context of previous part- 
of-speech and current part-of-speech increase by 
1.2% and 0.6%. Although F~= 1values for other 
three cases slightly decrease by 0.02%, 0.02% 
and 0.19%, the sizes of lexicons have been 
greatly reduced by 85% to 97%. 
Type Precision Recall F#=l 
Overall 91.69 93.28 92.48 
NP 92.64 93.48 93.06 
VP 92.16 93.66 92.90 
ADJP 78.39 71.69 74.89 
ADVP 73.66 87.80 80.11 
PP 95.18 97.38 96.27 
Table 9 : Results of chunking experiments with 
error-driven lexicon : dp= 
{ p~w~, p,w,3C & F~,. (p~w i ) > O} + { p~, p~3C} 
Type Precision Recall F~=l 
Overall 88.68 90.28 89.47 
NP 90.61 91.57 91.08 
VP 91.80 94.08 92.90 
ADJP 72.20 62.72 67.13 
ADVP 70.53 78.90 74.48 
PP 86.55 96.34 91.19 
Table 10: Results of chunking experiments 
with error-driven lexicon : ? = 
{ P,-~ Pi, Pi-1 Pi ~C & F~. (p,_~ p, ) > 0} 
+ { Pi, Pi 3C} 
Table 11: Results of chunking experiments 
with error-driven lexicon : ? = 
{ pi_l Wi_lPi , pi_l wi_lpi3C & V~,(Pi_l Wi_iPi ) > O} 
+{pi ,P i~C} 
Type 
Overall 92.84 
NP 
VP 
Precision 
93.35 
93.97 
79.49 
95.19 
Recall 
93.21 
93.65 
94.67 
72.94 
Ffl=l 
93.03 
93.50 
94.32 
76.07 ADJP 
ADVP 79.47 85.91 82.57 
PP 
77 
96.29 95.74 
Table 12: Results of chunking experiments 
with error-driven lexicon : ? = 
{ Pi-I P~W~, p~_~ Piw,3C & F.. (pi_~ p,w i) > 0} 
+{pi ,P i3C} 
Type Precision Recall F~_ 1 
Overall 91.99 92.95 92.47 
NP 93.35 93.39 93.37 
VP 92.89 94.36 93.62 
ADJP 80.01 71.70 75.63 
ADVP 73.40 87.32 79.76 
PP 93.42 97.33 95.33 
Table 13: Results of chunking experiments 
with error-driven lexicon : ? = 
{Pi-l Wi-lPiWi' Pi-lWi-lpiWi3C+{pi ' Pi3C} 
& F?.(p~_~w~_~p~w~) > O} 
After discussing the five context-dependent 
lexicons separately, now we explore the 
merging of context-dependent lexicons by 
assuming : 
CI~ .~ { Pi-lWi-I PiWi, Pi-lWi-I PiwigC 
& Fa,. (pi-lwi-t piwi ) > 0} 
+ { Pi-I PiW~, Pi-l piwi ~C & Fa" (Pi-l piwi ) > O} 
+ { Pi-lWi-I Pi" Pi-lWi-1Pi 3C & F~. (pi_lWi_l Pi ) > 0} 
+ { Pi-1 Pi, Pi-I Pii ~C & F~, (Pi-l Pi )> O} 
+ { piw~, Piw~3C & F~,. (PiWi) > 0} + { Pi, p~3C} 
and P(t i /G~) is approximatl~ by the following 
order :
1. if Pi_lWi_iPiWi E fI~, 
P(ti /G~)=P( t  i / p i _ lw i _ lP iWi )  
2. if p~_lp~wi E cb, 
P(ti /G~)=P( t  i /p i _ lw i _ lP iWi )  
3. if Pi-twi-lPi E ~,  
P(t i/G~) = P(t i / pi_l wi_l: pi ) 
4. if PiWi E ~,  P(t i / G~ ) = P(t i / piwi ) 
5. if Pi-I Pi E ~,  P(t i / G~ ) = P(t i / Pi-1Pi) 
6. P(t i lG : )=P( t  i lpi_lpi) 
Table 14 gives an overview of the chunking 
experiments using the above assumption. It 
shows that the F:=i value for the merged 
context-dependent lexicon inreases to 93.68%. 
For a comparison, the F/~=i value is 93.30% 
when all the possible lexical entries are included 
in ~ (Due to memory limitation, only the top 
150000 mostly occurred lexical entries are 
included). 
Type Precision Recall F#=i 
Overall 93.40 93.95 93.68 
NP 93.60 94.64 94.12 
VP 94.64 94.75 94.70 
ADJP 77.12 74.55 75.81 
ADVP 82.39 83.80 83.09 
PP 96.61 96.63 96.62 
Table 14: Results of chunking experiments 
with the merged context-dependent l xicon 
For the relationship between the training 
corpus size and error driven learning 
performance, Table 15 shows that the 
performance of error-driven learning improves 
stably when the training corpus ize increases. 
Training Sections I ~ I Accuracy i FB 1 
0-1 
0-3 
0-5 
0-7 
0-9 
0-11 
0-13 
0-i5 
0-17 
0-19 
14384 94.78% 91.95 
24507 95.19% i 92.51 
32316 95.28%1 92.77 
38286 95.41% 93.00 
39876 95.53% i 93.12 
43372 95.65% 93.31 
46029 95.62% 93.29 
47901 95.66% 93.34 
48813 95.74% i 93.41 
49988 95.92% 93.68 
Table 15: The performance oferror-driven 
learning with different training corpus ize 
For comparison with other chunk taggers, 
we also evaluate our chunk tagger with the 
merged context-dependent lexicon by cross- 
validation on all 25 partitions of the PENN WSJ 
TreeBank. Table 16 gives an overview of such 
chunking experiments. 
Type Precision Recall Fa=l 
Overall 96.40 96.47 96.44 
NP 96.49 96.99 96.74 
VP 97.13 97.36 97.25 
ADJP 89.92 88.15 89.03 
ADVP 91.52 87.57 89.50 
97.13 97.36 PP 97.25 
Table 16: Results of 25-fold cross-validation 
chunking experiments with the merged 
context-dependent l xicon 
Tables 14 and 16 shows that our new chunk 
tagger greatly outperforms other eported chunk 
taggers on the same training data and test data 
by 2%-3%.(Buchholz S., Veenstra J. and 
Daelmans W.(1999), Ramshaw L.A. and 
Marcus M.P.(1995), Daelemans W., Buchholz 
S. and Veenstra J.(1999), and Veenstra 
J.(1999)). 
Conclusion 
This paper proposes a new error-driven HMM- 
based chunk tagger with context-dependent 
lexicon. Compared with standard HMM-based 
tagger, this new tagger uses a new Hidden 
Markov Modelling approach which incorporates 
more contextual information into a lexical entry 
n 
by assuming MI(Tqn,G~)= 2Ml ( t , ,G f ) .  
i=1 
Moreover, an error-driven learning approach is 
adopted to drease the memeory requirement and 
further improve the accuracy by including more 
context-dependent information i to lexicon. 
It is found that our new chunk tagger 
singnificantly outperforms other eported chunk 
taggers on the same training data and test data. 
For future work, we will explore the 
effectivessness of considering even more 
contextual information on approximation of 
P(T~"IG ~) by using the forward-backward 
algodthm(Rabiner 1989) while currently we 
only consider the contextual information of 
current location and previous location. 
78 
Acknowledgement 
We wish to thank Sabine Buchholz 
from Tilbug University for kindly providing us 
her program which is also used to 
extact data for Conll-2000 share task. 
References 
Abney S. "Parsing by chunks ". Principle-Based 
Parsing edited by Berwick, Abney and Tenny. 
Kluwer Academic Publishers. 
Argamon S., Dagan I. and Krymolowski Y. "A 
memory-based approach to learning shallow 
natural language patterns." COL1NG/ACL- 
1998. Pp.67-73. Montreal, Canada. 1998. 
Bod R. "A computational model of language 
performance: Data-oriented parsing." 
COLING-1992. Pp.855-859. Nantes, France. 
1992. 
Boungault D. "Surface grammatical nalysis for 
the extraction of terminological noun 
phrases". COLING-92. Pp.977-981. 1992. 
Bdll Eric. "A corpus-based approach to 
language learning". Ph.D thesis. Univ. of 
Penn. 1993 
Buchholz S., Veenstra J. and Daelmans W. 
"Cascaded grammatical relation assignment." 
Proceeding of EMNLP/VLC-99, at ACL'99. 
1999 
Cardie C. "A case-based approach to knowledge 
acquisition for domain-specific sentence 
analysis." Proceeding of the 11 'h National 
Conference on Artificial Intelligence. Pp.798- 
803. Menlo Park, CA, USA. AAAI Press. 
1993. 
Church K.W. "A stochastic parts program and 
noun phrase parser for unrestricted Text." 
Proceeding of Second Conference on Applied 
Natural Language Processing. Pp.136-143. 
Austin, Texas, USA. 1988. 
Daelemans W., Buchholz S. and Veenstra J. 
"Memory-based shallow parsing." CoNLL- 
1999. Pp.53-60. Bergen, Norway. 1999. 
Daelemans W., Zavrel J., Berck P. and Gillis S. 
"MBT: A memory-based part-of-speech 
tagger generator." Proceeding of the Fourth 
Workshop on Large Scale Corpora. Pp. 14-27. 
ACL SIGDAT. 1996. 
Grefenstette G. "Light parsing as finite-state 
filtering". Workshop on Extended Finite State 
Models of Language at ECAI'96. Budapest, 
Hungary. 1996. 
Kupiec J. " An algorithm for finding noun 
phrase correspondences in bilingual corpora". 
ACL'93. Pp17-22. 1993. 
Marcus M., Santodni B. and Marcinkiewicz 
? M.A. "Buliding a large annotated corpus of 
English: The Penn Treebank". Computational 
Linguistics. 19(2):313-330. 1993. 
Rabiner L. "A tutorial on Hidden Markov 
Models and selected applications in speech 
recognition". IEEE 77(2), pp.257-285. 1989. 
Ramshaw L.A. and Marcus M.P. 
"Transformation-based Learning". 
Proceeding of 3th ACL Workshop on Very 
Large Corpora at ACL'95. 1995. 
Rijsbergen C.J.van. Information Retrieval. 
Buttersworth, London. 1979. 
Skut W. and Brants T. "Chunk tagger: statistical 
recognition of noun phrases." ESSLLI-1998 
Workshop on Automated Acquisition of Syntax 
and Parsing. Saarbruucken, Germany. 1998. 
Veenstra J. "Memory-based text chunking". 
Workshop on machine learning in human 
language technology at A CAI'99. 1999. 
Voutilainen A. "Nptool: a detector of English 
phrases". Proceeding of the Workshop on 
Very Large Corpora. Pp48-57. ACL' 93. 
1993 
Watson B. and Chunk Tsoi A. "Second order 
Hidden Markov Models for speech 
recognition". Proceeding of 4 ~ Australian 
International Conference on Speech Science 
and Technology. Pp. 146-151.1992. 
79 
Named Entity Recognition using an HMM-based Chunk Tagger 
 
GuoDong Zhou               Jian Su  
Laboratories for Information Technology 
21 Heng Mui Keng Terrace 
Singapore 119613 
           zhougd@lit.org.sg          sujian@lit.org.sg
 
Abstract 
This paper proposes a Hidden Markov 
Model (HMM) and an HMM-based chunk 
tagger, from which a named entity (NE) 
recognition (NER) system is built to 
recognize and classify names, times and 
numerical quantities. Through the HMM, 
our system is able to apply and integrate 
four types of internal and external 
evidences: 1) simple deterministic internal 
feature of the words, such as capitalization 
and digitalization; 2) internal semantic 
feature of important triggers; 3) internal 
gazetteer feature; 4) external macro context 
feature. In this way, the NER problem can 
be resolved effectively. Evaluation of our 
system on MUC-6 and MUC-7 English NE 
tasks achieves F-measures of 96.6% and 
94.1% respectively. It shows that the 
performance is significantly better than 
reported by any other machine-learning 
system. Moreover, the performance is even 
consistently better than those based on 
handcrafted rules.  
1 Introduction 
Named Entity (NE) Recognition (NER) is to 
classify every word in a document into some 
predefined categories and "none-of-the-above". In 
the taxonomy of computational linguistics tasks, it 
falls under the domain of "information extraction", 
which extracts specific kinds of information from 
documents as opposed to the more general task of 
"document management" which seeks to extract all 
of the information found in a document. 
Since entity names form the main content of a 
document, NER is a very important step toward 
more intelligent information extraction and 
management. The atomic elements of information 
extraction -- indeed, of language as a whole -- could 
be considered as the "who", "where" and "how 
much" in a sentence. NER performs what is known 
as surface parsing, delimiting sequences of tokens 
that answer these important questions. NER can 
also be used as the first step in a chain of processors: 
a next level of processing could relate two or more 
NEs, or perhaps even give semantics to that 
relationship using a verb. In this way, further 
processing could discover the "what" and "how" of 
a sentence or body of text.  
While NER is relatively simple and it is fairly 
easy to build a system with reasonable performance, 
there are still a large number of ambiguous cases 
that make it difficult to attain human performance. 
There has been a considerable amount of work on 
NER problem, which aims to address many of these 
ambiguity, robustness and portability issues. During 
last decade, NER has drawn more and more 
attention from the NE tasks [Chinchor95a] 
[Chinchor98a] in MUCs [MUC6] [MUC7], where 
person names, location names, organization names, 
dates, times, percentages and money amounts are to 
be delimited in text using SGML mark-ups.  
Previous approaches have typically used 
manually constructed finite state patterns, which 
attempt to match against a sequence of words in 
much the same way as a general regular expression 
matcher. Typical systems are Univ. of Sheffield's 
LaSIE-II [Humphreys+98], ISOQuest's NetOwl 
[Aone+98] [Krupha+98] and Univ. of Edinburgh's 
LTG [Mikheev+98] [Mikheev+99] for English 
NER. These systems are mainly rule-based. 
However, rule-based approaches lack the ability of 
coping with the problems of robustness and 
portability. Each new source of text requires 
significant tweaking of rules to maintain optimal 
performance and the maintenance costs could be 
quite steep. 
The current trend in NER is to use the 
machine-learning approach, which is more 
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 473-480.
                         Proceedings of the 40th Annual Meeting of the Association for
attractive in that it is trainable and adaptable and the 
maintenance of a machine-learning system is much 
cheaper than that of a rule-based one. The 
representative machine-learning approaches used in 
NER are HMM (BBN's IdentiFinder in [Miller+98] 
[Bikel+99] and KRDL's system [Yu+98] for 
Chinese NER.), Maximum Entropy (New York 
Univ.'s MEME in [Borthwick+98] [Borthwich99]) 
and Decision Tree (New York Univ.'s system in 
[Sekine98] and SRA's system in [Bennett+97]). 
Besides, a variant of Eric Brill's 
transformation-based rules [Brill95] has been 
applied to the problem [Aberdeen+95]. Among 
these approaches, the evaluation performance of 
HMM is higher than those of others. The main 
reason may be due to its better ability of capturing 
the locality of phenomena, which indicates names 
in text.  Moreover, HMM seems more and more 
used in NE recognition because of the efficiency of 
the Viterbi algorithm [Viterbi67] used in decoding 
the NE-class state sequence. However, the 
performance of a machine-learning system is 
always poorer than that of a rule-based one by about 
2% [Chinchor95b] [Chinchor98b]. This may be 
because current machine-learning approaches 
capture important evidence behind NER problem 
much less effectively than human experts who 
handcraft the rules, although machine-learning 
approaches always provide important statistical 
information that is not available to human experts. 
As defined in [McDonald96], there are two kinds 
of evidences that can be used in NER to solve the 
ambiguity, robustness and portability problems 
described above. The first is the internal evidence 
found within the word and/or word string itself 
while the second is the external evidence gathered 
from its context. In order to effectively apply and 
integrate internal and external evidences, we 
present a NER system using a HMM. The approach 
behind our NER system is based on the 
HMM-based chunk tagger in text chunking, which 
was ranked the best individual system [Zhou+00a] 
[Zhou+00b] in CoNLL'2000 [Tjong+00]. Here, a 
NE is regarded as a chunk, named "NE-Chunk". To 
date, our system has been successfully trained and 
applied in English NER. To our knowledge, our 
system outperforms any published 
machine-learning systems. Moreover, our system 
even outperforms any published rule-based 
systems.  
The layout of this paper is as follows. Section 2 
gives a description of the HMM and its application 
in NER: HMM-based chunk tagger. Section 3 
explains the word feature used to capture both the 
internal and external evidences. Section 4 describes 
the back-off schemes used to tackle the sparseness 
problem. Section 5 gives the experimental results of 
our system. Section 6 contains our remarks and 
possible extensions of the proposed work. 
2 HMM-based Chunk Tagger 
2.1  HMM Modeling 
Given a token sequence n
n gggG L211 = , the goal 
of NER is to find a stochastic optimal tag sequence 
n
n tttT L211 =  that maximizes                     (2-1) 
)()(
),(
log)(log)|(log
11
11
111 nn
nn
nnn
GPTP
GTP
TPGTP
?
+=  
The second item in (2-1) is the mutual 
information between nT1  and
nG1 . In order to 
simplify the computation of this item, we assume 
mutual information independence:  
?
=
=
n
i
n
i
nn GtMIGTMI
1
111 ),(),(   or              (2-2) 
?
= ?
=
?
n
i
n
i
n
i
nn
nn
GPtP
GtP
GPTP
GTP
1 1
1
11
11
)()(
),(
log
)()(
),(
log    (2-3) 
Applying it to equation (2.1), we have: 
?
?
=
=
+
?=
n
i
n
i
n
i
i
nnn
GtP
tPTPGTP
1
1
1
111
)|(log
)(log)(log)|(log
   (2-4) 
The basic premise of this model is to consider 
the raw text, encountered when decoding, as though 
it had passed through a noisy channel, where it had 
been originally marked with NE tags. The job of our 
generative model is to directly generate the original 
NE tags from the output words of the noisy channel. 
It is obvious that our generative model is reverse to 
the generative model of traditional HMM1, as used 
                                                     
1 In traditional HMM to maximise )|(log 11 nn GTP , first we 
apply Bayes' rule: 
)(
),(
)|(
1
11
11 n
nn
nn
GP
GTP
GTP =   
and have: 
in BBN's IdentiFinder, which models the original 
process that generates the NE-class annotated 
words from the original NE tags. Another 
difference is that our model assumes mutual 
information independence (2-2) while traditional 
HMM assumes conditional probability 
independence (I-1). Assumption (2-2) is much 
looser than assumption (I-1) because assumption 
(I-1) has the same effect with the sum of 
assumptions (2-2) and (I-3)2. In this way, our model 
can apply more context information to determine 
the tag of current token. 
From equation (2-4), we can see that: 
1) The first item can be computed by applying 
chain rules. In ngram modeling, each tag is 
assumed to be probabilistically dependent on the 
N-1 previous tags.  
2) The second item is the summation of log 
probabilities of all the individual tags. 
3) The third item corresponds to the "lexical" 
component of the tagger.  
We will not discuss both the first and second 
items further in this paper. This paper will focus on 
the third item?
=
n
i
n
i GtP
1
1 )|(log , which is the main 
difference between our tagger and other traditional 
HMM-based taggers, as used in BBN's IdentiFinder. 
Ideally, it can be estimated by using the 
forward-backward algorithm [Rabiner89] 
recursively for the 1st-order [Rabiner89] or 2nd 
-order HMMs [Watson+92]. However, an 
alternative back-off modeling approach is applied 
instead in this paper (more details in section 4). 
2.2 HMM-based Chunk Tagger 
                                                                                   
))(log)|((logmaxarg
)|(logmaxarg
111
11
nnn
T
nn
T
TPTGP
GTP
+=
 
Then we assume conditional probability 
independence: ?
=
=
n
i
ii
nn tgPTGP
1
11 )|()|(                 (I-1) 
and have: 
))(log)|(log(maxarg
)|(logmaxarg
1
1
11
nn
i
ii
T
nn
T
TPtgP
GTP
+= ?
=
                    (I-2) 
2 We can obtain equation (I-2) from (2.4) by assuming 
)|(log)|(log
1 ii
n
i tgPGtP =                                    (I-3) 
For NE-chunk tagging, we have 
token >=< iii wfg , , where nn wwwW L211 =  is the 
word sequence and nn fffF L211 =  is the 
word-feature sequence. In the meantime, NE-chunk 
tag it  is structural and consists of three parts: 
1) Boundary Category: BC = {0, 1, 2, 3}. Here 0 
means that current word is a whole entity and 
1/2/3 means that current word is at the 
beginning/in the middle/at the end of an entity. 
2) Entity Category: EC. This is used to denote the 
class of the entity name. 
3) Word Feature: WF. Because of the limited 
number of boundary and entity categories, the 
word feature is added into the structural tag to 
represent more accurate models. 
Obviously, there exist some constraints between 
1?it  and it  on the boundary and entity categories, as 
shown in Table 1, where "valid" / "invalid" means 
the tag sequence ii tt 1?  is valid / invalid while "valid 
on" means ii tt 1?  is valid with an additional 
condition ii ECEC =?1 . Such constraints have been 
used in Viterbi decoding algorithm to ensure valid 
NE chunking. 
 0 1 2 3 
0 Valid Valid Invalid Invalid
1 Invalid Invalid Valid on Valid on 
2 Invalid Invalid Valid Valid 
3 Valid Valid Invalid Invalid
Table 1: Constraints between 1?it  and it  (Column: 
1?iBC  in 1?it ; Row: iBC  in it ) 
3 Determining Word Feature 
As stated above, token is denoted as ordered pairs of 
word-feature and word itself: >=< iii wfg , . 
Here, the word-feature is a simple deterministic 
computation performed on the word and/or word 
string with appropriate consideration of context as 
looked up in the lexicon or added to the context. 
In our model, each word-feature consists of 
several sub-features, which can be classified into 
internal sub-features and external sub-features. The 
internal sub-features are found within the word 
and/or word string itself to capture internal 
evidence while external sub-features are derived 
within the context to capture external evidence. 
3.1 Internal Sub-Features 
Our model captures three types of internal 
sub-features: 1) 1f : simple deterministic internal 
feature of the words, such as capitalization and 
digitalization; 2) 2f : internal semantic feature of 
important triggers; 3) 3f : internal gazetteer feature. 
1) 1f  is the basic sub-feature exploited in this 
model, as shown in Table 2 with the descending 
order of priority. For example, in the case of 
non-disjoint feature classes such as 
ContainsDigitAndAlpha and 
ContainsDigitAndDash, the former will take 
precedence. The first eleven features arise from 
the need to distinguish and annotate monetary 
amounts, percentages, times and dates. The rest 
of the features distinguish types of capitalization 
and all other words such as punctuation marks. 
In particular, the FirstWord feature arises from 
the fact that if a word is capitalized and is the 
first word of the sentence, we have no good 
information as to why it is capitalized (but note 
that AllCaps and CapPeriod are computed before 
FirstWord, and take precedence.)  This 
sub-feature is language dependent. Fortunately, 
the feature computation is an extremely small 
part of the implementation. This kind of internal 
sub-feature has been widely used in 
machine-learning systems, such as BBN's 
IdendiFinder and New York Univ.'s MENE. The 
rationale behind this sub-feature is clear: a) 
capitalization gives good evidence of NEs in 
Roman languages; b) Numeric symbols can 
automatically be grouped into categories. 
2) 2f  is the semantic classification of important 
triggers, as seen in Table 3, and is unique to our 
system. It is based on the intuitions that 
important triggers are useful for NER and can be 
classified according to their semantics. This 
sub-feature applies to both single word and 
multiple words. This set of triggers is collected 
semi-automatically from the NEs and their local 
context of the training data. 
3) Sub-feature 3f , as shown in Table 4, is the 
internal gazetteer feature, gathered from the 
look-up gazetteers: lists of names of persons, 
organizations, locations and other kinds of 
named entities. This sub-feature can be 
determined by finding a match in the 
gazetteer of the corresponding NE type 
where n (in Table 4) represents the word 
number in the matched word string. In stead 
of collecting gazetteer lists from training 
data, we collect a list of 20 public holidays in 
several countries, a list of 5,000 locations 
from websites such as GeoHive3, a list of 
10,000 organization names from websites 
such as Yahoo4 and a list of 10,000 famous 
people from websites such as Scope 
Systems5. Gazetters have been widely used 
in NER systems to improve performance.  
3.2 External Sub-Features 
For external evidence, only one external macro 
context feature 4f , as shown in Table 5, is captured 
in our model. 4f  is about whether and how the 
encountered NE candidate is occurred in the list of 
NEs already recognized from the document, as 
shown in Table 5 (n is the word number in the 
matched NE from the recognized NE list and m is 
the matched word number between the word string 
and the matched NE with the corresponding NE 
type.). This sub-feature is unique to our system. The 
intuition behind this is the phenomena of name 
alias.  
During decoding, the NEs already recognized 
from the document are stored in a list. When the 
system encounters a NE candidate, a name alias 
algorithm is invoked to dynamically determine its 
relationship with the NEs in the recognized list. 
Initially, we also consider part-of-speech (POS) 
sub-feature. However, the experimental result is 
disappointing that incorporation of POS even 
decreases the performance by 2%. This may be 
because capitalization information of a word is 
submerged in the muddy of several POS tags and 
the performance of POS tagging is not satisfactory, 
especially for unknown capitalized words (since 
many of NEs include unknown capitalized words.). 
Therefore, POS is discarded. 
                                                     
3 http://www.geohive.com/ 
4 http://www.yahoo.com/ 
5 http://www.scopesys.com/ 
Sub-Feature 1f  Example  Explanation/Intuition 
OneDigitNum 9 Digital Number 
TwoDigitNum 90 Two-Digit year 
FourDigitNum 1990 Four-Digit year 
YearDecade 1990s Year Decade 
ContainsDigitAndAlpha A8956-67 Product Code 
ContainsDigitAndDash 09-99 Date 
ContainsDigitAndOneSlash 3/4 Fraction or Date 
ContainsDigitAndTwoSlashs 19/9/1999 DATE 
ContainsDigitAndComma 19,000 Money 
ContainsDigitAndPeriod 1.00 Money, Percentage 
OtherContainsDigit 123124 Other Number 
AllCaps IBM Organization 
CapPeriod M. Person Name Initial 
CapOtherPeriod St. Abbreviation 
CapPeriods N.Y. Abbreviation 
FirstWord First word of sentence No useful capitalization information 
InitialCap Microsoft Capitalized Word 
LowerCase Will Un-capitalized Word 
Other $ All other words 
Table 2: Sub-Feature 1f : the Simple Deterministic Internal Feature of the Words 
NE Type (No of Triggers) Sub-Feature 2f  Example Explanation/Intuition 
PERCENT (5) SuffixPERCENT % Percentage Suffix 
PrefixMONEY $ Money Prefix MONEY (298) 
SuffixMONEY Dollars Money Suffix 
SuffixDATE Day Date Suffix 
WeekDATE Monday Week Date 
MonthDATE July Month Date 
SeasonDATE Summer Season Date 
PeriodDATE1 Month Period Date 
PeriodDATE2 Quarter Quarter/Half of Year 
EndDATE Weekend Date End  
DATE (52) 
ModifierDATE Fiscal Modifier of Date 
SuffixTIME a.m. Time Suffix TIME (15) 
PeriodTime Morning Time Period 
PrefixPERSON1 Mr. Person Title 
PrefixPERSON2 President Person Designation  
PERSON (179) 
FirstNamePERSON Micheal Person First Name 
LOC (36) SuffixLOC River Location Suffix 
ORG (177) SuffixORG Ltd Organization Suffix 
Others (148) Cardinal, Ordinal, etc. Six,, Sixth Cardinal and Ordinal Numbers 
Table 3: Sub-Feature 2f : the Semantic Classification of Important Triggers 
NE Type (Size of Gazetteer) Sub-Feature 3f  Example 
DATE (20) DATEnGn Christmas Day: DATE2G2 
PERSON (10,000) PERSONnGn Bill Gates: PERSON2G2 
LOC (5,000) LOCnGn Beijing: LOC1G1 
ORG (10,000) ORGnGn United Nation: ORG2G2 
Table 4: Sub-Feature 3f : the Internal Gazetteer Feature (G means Global gazetteer) 
NE Type Sub-Feature Example 
PERSON PERSONnLm Gates: PERSON2L1 ("Bill Gates" already recognized as a person name) 
LOC LOCnLm N.J.: LOC2L2 ("New Jersey" already recognized as a location name) 
ORG ORGnLm UN: ORG2L2 ("United Nation" already recognized as a org name) 
Table 5: Sub-feature 4f : the External Macro Context Feature (L means Local document) 
4  Back-off Modeling 
Given the model in section 2 and word feature in 
section 3, the main problem is how to 
compute ?
=
n
i
n
i GtP
1
1 )/( . Ideally, we would have 
sufficient training data for every event whose 
conditional probability we wish to calculate. 
Unfortunately, there is rarely enough training data 
to compute accurate probabilities when decoding on 
new data, especially considering the complex word 
feature described above. In order to resolve the 
sparseness problem, two levels of back-off 
modeling are applied to approximate )/( 1
n
i GtP : 
1) First level back-off scheme is based on different 
contexts of word features and words themselves, 
and nG1  in )/( 1
n
i GtP  is approximated in the 
descending order of iiii wfff 12 ?? , 21 ++ iiii ffwf , 
iii wff 1? , 1+iii fwf , iii fwf 11 ?? , 11 ++ iii wff , 
iii fff 12 ?? , 21 ++ iii fff , ii wf , iii fff 12 ?? , 1+ii ff  
and if . 
2) The second level back-off scheme is based on 
different combinations of the four sub-features 
described in section 3, and kf  is approximated 
in the descending order of 4321 kkkk ffff , 
31
kk ff , 
41
kk ff , 
21
kk ff  and 
1
kf . 
5 Experimental Results 
In this section, we will report the experimental 
results of our system for English NER on MUC-6 
and MUC-7 NE shared tasks, as shown in Table 6, 
and then for the impact of training data size on 
performance using MUC-7 training data. For each 
experiment, we have the MUC dry-run data as the 
held-out development data and the MUC formal test 
data as the held-out test data.  
For both MUC-6 and MUC-7 NE tasks, Table 7 
shows the performance of our system using MUC 
evaluation while Figure 1 gives the comparisons of 
our system with others. Here, the precision (P) 
measures the number of correct NEs in the answer 
file over the total number of NEs in the answer file 
and the recall (R) measures the number of correct 
NEs in the answer file over the total number of NEs 
in the key file while F-measure is the weighted 
harmonic mean of precision and recall: 
PR
RPF
+
+
= 2
2 )1(
?
?
 with 2? =1. It shows that the 
performance is significantly better than reported by 
any other machine-learning system. Moreover, the 
performance is consistently better than those based 
on handcrafted rules. 
Statistics 
(KB) 
Training 
Data 
Dry Run 
Data 
Formal Test 
Data 
MUC-6 1330 121 124 
MUC-7 708 156 561 
Table 6: Statistics of Data from MUC-6  
and MUC-7 NE Tasks 
 F P R 
MUC-6 96.6 96.3 96.9 
MUC-7 94.1 93.7 94.5 
Table 7: Performance of our System on MUC-6 
and MUC-7 NE Tasks 
Composition F P R 
1ff =  77.6 81.0 74.1 
21 fff =  87.4 88.6 86.1 
321 ffff =  89.3 90.5 88.2 
421 ffff =  92.9 92.6 93.1 
4321 fffff =  94.1 93.7 94.5 
Table 8: Impact of Different Sub-Features 
With any learning technique, one important 
question is how much training data is required to 
achieve acceptable performance. More generally 
how does the performance vary as the training data 
size changes? The result is shown in Figure 2 for 
MUC-7 NE task. It shows that 200KB of training 
data would have given the performance of 90% 
while reducing to 100KB would have had a 
significant decrease in the performance. It also 
shows that our system still has some room for 
performance improvement. This may be because of 
the complex word feature and the corresponding sparseness problem existing in our system.  
Figure 1: Comparison of our system with others 
on MUC-6 and MUC-7 NE tasks
80
85
90
95
100
80 85 90 95 100Recall
Pr
ec
is
io
n Our MUC-6 System
Our MUC-7 System
Other MUC-6 Systems
Other MUC-7 Syetems
Figure 2: Impact of Various Training Data on Performance
80
85
90
95
100
100 200 300 400 500 600 700 800
Training Data Size(KB)
F-
m
ea
su
re
MUC-7
Another important question is about the effect of 
different sub-features. Table 8 answers the question 
on MUC-7 NE task: 
1) Applying only 1f  gives our system the 
performance of 77.6%. 
2) 2f  is very useful for NER and increases the 
performance further by 10% to 87.4%.   
3) 4f  is impressive too with another 5.5% 
performance improvement.  
4)  However, 3f  contributes only further 1.2% to 
the performance. This may be because 
information included in 3f  has already been 
captured by 2f  and 4f . Actually, the 
experiments show that the contribution of 3f  
comes from where there is no explicit indicator 
information in/around the NE and there is no 
reference to other NEs in the macro context of 
the document. The NEs contributed by 3f  are 
always well-known ones, e.g. Microsoft, IBM 
and Bach (a composer), which are introduced in 
texts without much helpful context. 
6  Conclusion 
This paper proposes a HMM in that a new 
generative model, based on the mutual information 
independence assumption (2-3) instead of the 
conditional probability independence assumption 
(I-1) after Bayes' rule, is applied. Moreover, it 
shows that the HMM-based chunk tagger can 
effectively apply and integrate four different kinds 
of sub-features, ranging from internal word 
information to semantic information to NE 
gazetteers to macro context of the document, to 
capture internal and external evidences for NER 
problem. It also shows that our NER system can 
reach "near human performance". To our 
knowledge, our NER system outperforms any 
published machine-learning system and any 
published rule-based system.  
While the experimental results have been 
impressive, there is still much that can be done 
potentially to improve the performance. In the near 
feature, we would like to incorporate the following 
into our system: 
? List of domain and application dependent person, 
organization and location names. 
? More effective name alias algorithm.  
? More effective strategy to the back-off modeling 
and smoothing. 
References 
[Aberdeen+95] J. Aberdeen, D. Day, L. 
Hirschman, P. Robinson and M. Vilain. MITRE: 
Description of the Alembic System Used for 
MUC-6. MUC-6. Pages141-155. Columbia, 
Maryland. 1995. 
[Aone+98] C. Aone, L. Halverson, T. Hampton, 
M. Ramos-Santacruz. SRA: Description of the IE2 
System Used for MUC-7. MUC-7. Fairfax, Virginia. 
1998. 
[Bennett+96] S.W. Bennett, C. Aone and C. 
Lovell. Learning to Tag Multilingual Texts 
Through Observation. EMNLP'1996. Pages109-116. 
Providence, Rhode Island. 1996. 
[Bikel+99] Daniel M. Bikel, Richard Schwartz 
and Ralph M. Weischedel. An Algorithm that 
Learns What's in a Name. Machine Learning 
(Special Issue on NLP). 1999. 
[Borthwick+98] A. Borthwick, J. Sterling, E. 
Agichtein, R. Grishman. NYU: Description of the 
MENE Named Entity System as Used in MUC-7. 
MUC-7. Fairfax, Virginia. 1998. 
[Borthwick99] Andrew Borthwick. A Maximum 
Entropy Approach to Named Entity Recognition. 
Ph.D. Thesis. New York University. September, 
1999. 
[Brill95] Eric Brill. Transform-based 
Error-Driven Learning and Natural Language 
Processing: A Case Study in Part-of-speech 
Tagging. Computational Linguistics 21(4). 
Pages543-565. 1995. 
[Chinchor95a] Nancy Chinchor. MUC-6 Named 
Entity Task Definition (Version 2.1). MUC-6. 
Columbia, Maryland. 1995. 
[Chinchor95b] Nancy Chinchor. Statistical 
Significance of MUC-6 Results. MUC-6. Columbia, 
Maryland. 1995. 
[Chinchor98a] Nancy Chinchor. MUC-7 Named 
Entity Task Definition (Version 3.5). MUC-7. 
Fairfax, Virginia. 1998. 
[Chinchor98b] Nancy Chinchor. Statistical 
Significance of MUC-7 Results. MUC-7. Fairfax, 
Virginia. 1998. 
[Humphreys+98] K. Humphreys, R. Gaizauskas, 
S. Azzam, C. Huyck, B. Mitchell, H. Cunningham, 
Y. Wilks. Univ. of Sheffield: Description of the 
LaSIE-II System as Used for MUC-7. MUC-7. 
Fairfax, Virginia. 1998. 
[Krupka+98]  G. R. Krupka, K. Hausman. 
IsoQuest Inc.: Description of the NetOwlTM 
Extractor System as Used for MUC-7. MUC-7. 
Fairfax, Virginia. 1998. 
[McDonald96] D. McDonald. Internal and 
External Evidence in the Identification and 
Semantic Categorization of Proper Names. In B. 
Boguraev and J. Pustejovsky editors: Corpus 
Processing for Lexical Acquisition. Pages21-39. 
MIT Press. Cambridge, MA. 1996. 
[Miller+98] S. Miller, M. Crystal, H. Fox, L. 
Ramshaw, R. Schwartz, R. Stone, R. Weischedel, 
and the Annotation Group. BBN: Description of the 
SIFT System as Used for MUC-7. MUC-7. Fairfax, 
Virginia. 1998. 
[Mikheev+98] A. Mikheev, C. Grover, M. 
Moens. Description of the LTG System Used for 
MUC-7. MUC-7. Fairfax, Virginia. 1998. 
[Mikheev+99] A. Mikheev, M. Moens, and C. 
Grover. Named entity recognition without gazeteers. 
EACL'1999. Pages1-8. Bergen, Norway. 1999.  
[MUC6] Morgan Kaufmann Publishers, Inc. 
Proceedings of the Sixth Message Understanding 
Conference (MUC-6). Columbia, Maryland. 1995. 
[MUC7] Morgan Kaufmann Publishers, Inc. 
Proceedings of the Seventh Message Understanding 
Conference (MUC-7). Fairfax, Virginia. 1998. 
[Rabiner89] L. Rabiner. A Tutorial on Hidden 
Markov Models and Selected Applications in 
Speech Recognition?. IEEE 77(2). Pages257-285. 
1989. 
[Sekine98] Satoshi Sekine. Description of the 
Japanese NE System Used for MET-2. MUC-7. 
Fairfax, Virginia. 1998. 
[Tjong+00] Erik F. Tjong Kim Sang and Sabine 
Buchholz. Introduction to the CoNLL-2000 Shared 
Task: Chunking. CoNLL'2000. Pages127-132. 
Lisbon, Portugal. 11-14 Sept 2000. 
[Viterbi67] A. J. Viterbi. Error Bounds for 
Convolutional Codes and an Asymptotically 
Optimum Decoding Algorithm. IEEE Transactions 
on Information Theory. IT(13). Pages260-269, 
April 1967. 
[Watson+92] B. Watson and Tsoi A Chunk. 
Second Order Hidden Markov Models for Speech 
Recognition?. Proceeding of 4th Australian 
International Conference on Speech Science and 
Technology. Pages146-151. 1992. 
 [Yu+98] Yu Shihong, Bai Shuanhu and Wu 
Paul. Description of the Kent Ridge Digital Labs 
System Used for MUC-7. MUC-7. Fairfax, Virginia. 
1998. 
 [Zhou+00] Zhou GuoDong, Su Jian and Tey 
TongGuan. Hybrid Text Chunking. CoNLL'2000. 
Pages163-166. Lisbon, Portugal, 11-14 Sept 2000. 
[Zhou+00b] Zhou GuoDong and Su Jian, 
Error-driven HMM-based Chunk Tagger with 
Context-dependent Lexicon. EMNLP/ VLC'2000. 
Hong Kong, 7-8 Oct 2000. 
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 728?736, Prague, June 2007. c?2007 Association for Computational Linguistics
Tree Kernel-based Relation Extraction  
with Context-Sensitive Structured Parse Tree Information 
  
    GuoDong ZHOU12     Min ZHANG 2     Dong Hong JI 2     QiaoMing ZHU 1 
         1School of Computer Science & Technology         2  Institute for Infocomm Research 
                      Soochow Univ.                                              Heng Mui Keng Terrace 
         Suzhou, China 215006                                           Singapore 119613 
                 Email: {gdzhou,qmzhu}@suda.edu.cn      Email: {zhougd, mzhang, dhji}@i2r.a-star.edu.sg 
  
Abstract 
This paper proposes a tree kernel with context-
sensitive structured parse tree information for re-
lation extraction. It resolves two critical problems 
in previous tree kernels for relation extraction in 
two ways. First, it automatically determines a dy-
namic context-sensitive tree span for relation ex-
traction by extending the widely-used Shortest 
Path-enclosed Tree (SPT) to include necessary 
context information outside SPT. Second, it pro-
poses a context-sensitive convolution tree kernel, 
which enumerates both context-free and context-
sensitive sub-trees by considering their  ancestor 
node paths as their contexts. Moreover, this paper 
evaluates the complementary nature between our 
tree kernel and a state-of-the-art linear kernel. 
Evaluation on the ACE RDC corpora shows that 
our dynamic context-sensitive tree span is much 
more suitable for relation extraction than SPT and 
our tree kernel outperforms the state-of-the-art 
Collins and Duffy?s convolution tree kernel. It 
also shows that our tree kernel achieves much bet-
ter performance than the state-of-the-art linear 
kernels . Finally, it shows that feature-based and 
tree kernel-based methods much complement each 
other and the composite kernel can well integrate 
both flat and structured features.  
1 Introduction 
Relation extraction is to find various predefined se-
mantic relations between pairs of entities in text. The 
research in relation extraction has been promoted by 
the Message Understanding Conferences (MUCs) 
(MUC, 1987-1998) and the NIST Automatic Content 
Extraction (ACE) program (ACE, 2002-2005). Ac-
cording to the ACE Program, an entity is an object or 
a set of objects in the world and a relation is an ex-
plicitly or implicitly stated relationship among enti-
ties. For example, the sentence ?Bill Gates is the 
chairman and chief software architect of Microsoft 
Corporation.? conveys the ACE-style relation 
?EMPLOYMENT.exec? between the entities ?Bill 
Gates? (person name) and ?Microsoft Corporation? 
(organization name). Extraction of semantic relations 
between entities can be very useful in many applica-
tions such as question answering, e.g. to answer the 
query ?Who is the president of the United States??, 
and information  retrieval, e.g. to expand the query 
?George W. Bush? with ?the president of the United 
States? via his relationship with ?the United States?. 
Many researches have been done in relation extrac-
tion. Among them, feature-based methods (Kamb-
hatla 2004; Zhou et al, 2005) achieve certain success 
by employing a large amount of diverse linguistic 
features, varying from lexical knowledge, entity-
related information to syntactic parse trees, depend-
ency trees and semantic information. However, it is 
difficult for them to effectively capture structured 
parse tree information (Zhou et al2005), which is 
critical for further performance improvement in rela-
tion extraction.  
As an alternative to feature-based methods, tree 
kernel-based methods provide an elegant solution to 
explore implicitly structured features by directly 
computing the similarity between two trees. Although 
earlier researches (Zelenko et al2003; Culotta and 
Sorensen 2004; Bunescu and Mooney 2005a) only 
achieve success on simple tasks and fail on complex 
tasks, such as the ACE RDC task, tree kernel-based 
methods achieve much progress recently. As the 
state-of-the-art, Zhang et al(2006) applied the convo-
lution tree kernel (Collins and Duffy 2001) and 
achieved comparable performance with a state-of-the-
art linear kernel (Zhou et al2005) on the 5 relation  
types in the ACE RDC 2003 corpus.  
However, there are two problems in Collins and 
Duffy?s convolution tree kernel for relation extraction.  
The first is that the sub-trees enumerated in the tree 
kernel computation are context-free. That is, each 
sub-tree enumerated in the tree kernel computation 
728
does not consider the context information outside the 
sub-tree. The second is to decide a proper tree span in 
relation extraction. Zhang et al(2006) explored five 
tree spans in relation extraction and it was  a bit sur-
prising to find that the Shortest Path-enclosed Tree 
(SPT, i.e. the sub-tree enclosed by the shortest path 
linking two involved entities in the parse tree) per-
formed best. This is contrast to our intuition. For ex-
ample, ?got married? is critical to determine the 
relationship between ?John? and ?Mary? in the sen-
tence ?John and Mary got married? ? as shown in 
Figure 1(e). It is obvious that the information con-
tained in SPT (?John and Marry?) is not enough to 
determine their relationship. 
This paper proposes a context-sensitive convolu-
tion tree kernel for relation extraction to resolve the 
above two problems. It first automatically determines 
a dynamic context-sensitive tree span for relation ex-
traction by extending the Shortest Path-enclosed Tree 
(SPT) to include necessary context information out-
side SPT. Then it proposes a context-sensitive convo-
lution tree kernel, whic h not only enumerates context-
free sub-trees but also context-sensitive sub-trees by 
considering their ancestor node paths as their contexts. 
Moreover, this paper evaluates the complementary 
nature of different linear kernels and tree kernels via a 
composite kernel.  
The layout of this paper is as follows. In Section 2, 
we review related work in more details. Then, the 
dynamic context-sensitive tree span and the context-
sensitive convolution tree kernel are proposed in Sec-
tion 3 while Section 4 shows the experimental results. 
Finally, we conclude our work in Sec tion 5.  
2 Related Work 
The relation extraction task was first introduced as 
part of the Template Element task in MUC6 and then 
formulated as the Template Relation task in MUC7. 
Since then, many methods, such as feature-based 
(Kambhatla 2004; Zhou et al2005, 2006), tree ker-
nel-based (Zelenko et al2003; Culotta and Sorensen 
2004; Bunescu and Mooney 2005a; Zhang et al2006) 
and composite kernel-based (Zhao and Gris hman 
2005; Zhang et al2006), have been proposed in lit-
erature. 
For the feature-based methods, Kambhatla (2004) 
employed Maximum Entropy models to combine di-
verse lexical, syntactic and semantic features in rela-
tion extraction, and achieved the F-measure of 52.8 
on the 24 relation subtypes in the ACE RDC 2003 
corpus. Zhou et al(2005) further systematically ex-
plored diverse features through a linear kernel and 
Support Vector Machines, and achieved the F-
measures of 68.0 and 55.5 on the 5 relation types and 
the 24 relation subtypes in the ACE RDC 2003 cor-
pus respectively. One problem with the feature-based 
methods is that they need extensive feature engineer-
ing. Another problem is that, although they can ex-
plore some structured information in the parse tree 
(e.g. Kambhatla (2004) used the non-terminal path 
connecting the given two entities in a parse tree while 
Zhou et al (2005) introduced additional chunking 
features to enhance the performance), it is found dif-
ficult to well preserve structured information in the 
parse trees using the feature-based methods. Zhou et 
al (2006) further improved the performance by ex-
ploring the commonality among related classes in a 
class hierarchy using hierarchical learning strategy. 
As an alternative to the feature-based methods, the 
kernel-based methods (Haussler, 1999) have been 
proposed to implicitly explore various features in a 
high dimensional space by employing a kernel to cal-
culate the similarity between two objects directly. In 
particular, the kernel-based methods could be very 
effective at reducing the burden of feature engineer-
ing for structured objects in NLP researches, e.g. the 
tree structure in relation extraction.   
Zelenko et al (2003) proposed a kernel between 
two parse trees, which recursively matches nodes 
from roots to leaves in a top-down manner. For each 
pair of matched nodes, a subsequence kernel on their 
child nodes is invoked. They achieved quite success 
on two simple relation extraction tasks. Culotta and 
Sorensen (2004) extended this work to estimate simi-
larity between augmented dependency trees and 
achieved the F-measure of 45.8 on the 5 relation 
types in the ACE RDC 2003 corpus. One problem 
with the above two tree kernels is that matched nodes 
must be at the same height and have the same path to 
the root node. Bunescu and Mooney (2005a) pro-
posed a shortest path dependency tree kernel, which 
just sums up the number of common word classes 
at each position in the two paths, and achieved the 
F-measure of 52.5 on the 5 relation types in the ACE 
RDC 2003 corpus. They argued that the information 
to model a relationship between two entities can be 
typically captured by the shortest path between them 
in the dependency graph. While the shortest path 
may not be able to well preserve structured de-
pendency tree information, another problem with 
their kernel is that the two paths should have same 
length. This makes it suffer from the similar behavior 
with that of Culotta and Sorensen (2004): high preci-
sion but very low recall.  
As the state-of-the-art tree kernel-based method, 
Zhang et al(2006) explored various structured feature 
729
spaces and used the convolution tree kernel over 
parse trees (Collins and Duffy 2001) to model syntac-
tic structured information for relation extraction. 
They achieved the F-measures of 61.9 and 63.6 on the 
5 relation types of the ACE RDC 2003 corpus and the 
7 relation types of the ACE RDC 2004 corpus respec-
tively without entity-related information while the F-
measure on the 5 relation types in the ACE RDC 
2003 corpus reached 68.7 when entity-related infor-
mation was included in the parse tree. One problem 
with Collins and Duffy?s convolution tree kernel is 
that the sub-trees involved in the tree kernel computa-
tion are context-free, that is, they do not consider the 
information outside the sub-trees. This is different 
from the tree kernel in Culota and Sorensen (2004), 
where the sub-trees involved in the tree kernel com-
putation are context-sensitive (that is, with the path 
from the tree root node to the sub-tree root node in 
consideration). Zhang et al(2006) also showed that 
the widely-used Shortest Path-enclosed Tree (SPT) 
performed best. One problem with SPT is that it fails 
to capture the contextual information outside the 
shortest path, which is important for relation extrac-
tion in many cases. Our random selection of 100 pos i-
tive training instances from the ACE RDC 2003 
training corpus shows that ~25% of the cases need 
contextual information outside the shortest path. 
Among other kernels, Bunescu and Mooney (2005b) 
proposed a subsequence kernel and applied it in pro-
tein interaction and ACE relation extraction tasks. 
In order to integrate the advantages of feature-
based and tree kernel-based methods, some research-
ers have turned to composite kernel-based methods. 
Zhao and Grishman (2005) defined several feature-
based composite kernels to integrate diverse features 
for relation extraction and achieved the F-measure of 
70.4 on the 7 relation types of the ACE RDC 2004 
corpus. Zhang et al(2006) proposed two composite 
kernels to integrate a linear kernel and Collins and 
Duffy?s convolution tree kernel. It achieved the F-
measure of 70.9/57.2 on the 5 relation types/24 rela-
tion subtypes in the ACE RDC 2003 corpus and the 
F-measure of 72.1/63.6 on the 7 relation types/23 
relation subtypes in the ACE RDC 2004 corpus. 
The above discussion suggests that structured in-
formation in the parse tree may not be fully utilized in 
the previous works, regardless of feature-based, tree 
kernel-based or composite kernel-based methods. 
Compared with the previous works, this paper pro-
poses a dynamic context-sensitive tree span trying to 
cover necessary structured information and a context-
sensitive convolution tree kernel considering both 
context-free and context-sensitive sub-trees. Further-
more, a composite kernel is applied to combine our 
tree kernel and a state-of-the-art linear kernel for in-
tegrating both flat and structured features in relation 
extraction as well as validating their complementary 
nature. 
3 Context Sensitive Convolution Tree 
Kernel for Relation Extraction 
In this section, we first propose an algorithm to dy-
namically determine a proper context-sensitive tree 
span and then a context-sensitive convolution tree 
kernel for relation extraction.  
3.1 Dynamic Context-Sensitive Tree Span in 
Relation Extraction 
A relation instance between two entities is encaps u-
lated by a parse tree. Thus, it is critical to understand 
which portion of a parse tree is important in the tree 
kernel calculation. Zhang et al(2006) systematically 
explored seven different tree spans, including the 
Shortest Path-enclosed Tree (SPT) and a Context-
Sensitive Path-enclosed Tree1 (CSPT), and found that 
SPT per formed best. That is, SPT even outperforms 
CSPT. This is contrary to our intuition. For example, 
?got married? is critical to determine the relationship 
between ?John? and ?Mary? in the sentence ?John 
and Mary got married? ? as shown in Figure 1(e), 
and the information contained in SPT (?John and 
Mary?) is not enough to determine their relationship. 
Obviously, context-sensitive tree spans should have 
the potential for better performance. One problem 
with the context-sensitive tree span explored in Zhang 
et al(2006) is that it only considers the availability of 
entities? siblings and fails to consider following two 
factors: 
1) Whether is the information contained in SPT 
enough to determine the relationship between 
two entities? It depends. In the embedded cases, 
SPT is enough. For example, ?John?s wife? is 
enough to determine the relationship between 
?John? and ?John?s wife? in the sentence ?John?s 
wife got a good job? ? as shown in Figure 1(a) . 
However, SPT is not enough in the coordinated 
cases, e.g. to determine the relationship between 
?John? and ?Mary? in the sentence ?John and 
Mary got married? ? as shown in Figure 1(e). 
                                                               
1 CSPT means SPT extending with the 1st left sibling of 
the node of entity 1 and the 1st right sibling of the node 
of entity 2.  In the case of no available  sibling, it moves 
to the parent of current node and repeat the same proc-
ess until a sibling is available or the root is reached. 
730
2) How can we extend SPT to include necessary 
context information if there is no enough infor-
mation in SPT for relation extraction?  
To answer the above two questions, we randomly 
chose 100 positive instances from the ACE RDC 
2003 training data and studied their necessary tree 
spans. It was observed that we can classify them into 
5 categories: 1) embedded (37 instances), where one 
entity is embedded in another entity, e.g. ?John? and 
?John?s wife? as shown in Figure 1(a); 2) PP-linked 
(21 instances), where one entity is linked to another 
entity via PP attachment, e.g. ?CEO? and ?Microsoft? 
in the sentence ?CEO of Microsoft announced ? ? as 
shown in Figure 1(b); 3) semi-structured (15 in-
stances), where the sentence consists of a sequence of 
noun phrases (including the two given entities), e.g. 
?Jane? and ?ABC news? in the sentence ?Jane, ABC 
news, California.? as shown in Figure 1(c); 4) de-
scriptive (7 instances), e.g. the citizenship between 
?his mother? and ?Lebanese? in the sentence ?his 
mother Lebanese landed at ?? as shown in Figure 
1(d); 5) predicate-linked and others (19 instances, 
including coordinated cases), where the predicate 
information is necessary to determine the relationship 
between two entities, e.g.  ?John? and ?Mary? in the 
sentence ?John and Mary got married?? as shown in 
Figure 1(e); 
Based on the above observations, we implement an 
algorithm to determine the necessary tree span for the 
relation extract task. The idea behind the algorithm is 
that the necessary tree span for a relation should be 
determined dynamically according to its tree span 
category and context. Given a parsed tree and two 
entities in consideration, it first determin es the tree 
span category and then extends the tree span accord-
ingly. By default, we adopt the Shortest Path-
enclosed Tree (SPT) as our tree span. We only ex-
pand the tree span when the tree span belongs to the 
?predicate-linked? category. This is based on our ob-
servation that the tree spans belonging to the ?predi-
cate-linked? category vary much syntactically and 
majority (~70%) of them need information outside 
SPT while it is quite safe (>90%) to use SPT as the 
tree span for the remaining categories. In our algo-
rithm, the expansion is done by first moving up until 
a predicate-headed phrase is found and then moving 
down along the predicated-headed path to the predi-
cate terminal node. Figure 1(e) shows an example for 
the ?predicate-linked? category where the lines with 
arrows indicate the expansion path.  
 
   
 
e) predicate-linked: SPT and the dynamic context-sensitive tree span  
Figure 1: Different tree span categories with SPT (dotted circle) and an ex-
ample of the dynamic context-sensitive tree span (solid circle) 
  
 
Figure 2: Examples of context-
free and context-sensitive sub-
trees related with Figure 1(b). 
Note: the bold node is the root 
for a sub-tree. 
A problem with our algorithm is how to deter-
mine whether an entity pair belongs to the ?predi-
cate-linked? category. In this paper, a simple 
method is applied by regarding the ?predicate-
linked? category as the default category. That is, 
those entity pairs, which do not belong to the four 
well defined and easily detected categories (i.e. 
embedded, PP-liked, semi-structured and descrip-
tive), are classified into the ?predicate-linked? cate-
gory. 
His mother Lebanese  landed 
PRP$ NNP VBD IN 
NP-E1-PER NP-E2-GPE PP 
S 
d)  descriptive 
NP 
NN 
at 
?  
VP 
Jane ABC news ,  
NNP , NNP NNS , NNP . 
NP NP-E1-PER NP-E2-ORG
NP 
c) semi-structured  
California . . 
, 
, 
, 
NP(NN) 
of Microsoft 
IN NNP 
NP-E2-ORG 
PP(IN)-subroot 
b) context -sensitive 
NP(NN) 
of Microsoft 
IN NNP 
NP-E2-ORG 
S(VBD) 
PP(IN)-subroot 
c) context -sensitive 
PP(IN)-subtoot 
NP-E2-ORG 
of Microsoft 
IN NNP 
a) context -free 
?  
NP 
John and Mary  got 
NNP CC NNP VBD 
married  
NP-E1-PER NP-E2-PER VP 
S 
VP 
VBN ?  
John and Mary  got 
NNP CC NNP VBD  
married 
NP-E1-PER NP-E2-PER VP 
 
NP VP 
 
?  
NP 
CEO of Microsoft announced 
NN IN NNP VBD ?  
NP-E1-PER NP-E2-ORG 
VP 
S 
b)  PP -linked  
PP 
?  
John ?s wife found a  job 
NNP POS NN VBD DT JJ NN 
NP NP-E1-PER 
NP-E2-PER VP 
S 
a) embedded  
good 
731
Since ?predicate -linked? instances only occupy 
~20% of cases, this explains why SPT performs 
better than the Context-Sensitive Path-enclosed 
Tree (CSPT) as described in Zhang et al(2006): 
consistently adopting CSPT may introduce too 
much noise/unnecessary information in the tree 
kernel. 
3.2 Context-Sensitive Convolution Tree Kernel 
Given any tree span, e.g. the dynamic context-
sensitive tree span in the last subsection, we now 
study how to measure the similarity between two 
trees, using a convolution tree kernel.A convolution 
kernel (Haussler D., 1999) aims to capture structured 
information in terms of substructures . As a special-
ized convolution kernel, Collins and Duffy?s convolu-
tion tree kernel ),( 21 TTKC  (?C? for convolution) 
counts the number of common sub-trees (sub-
structures) as the syntactic structure similarity be-
tween two parse trees T1 and T2 (Collins and Duffy 
2001): 
?
??
D=
2211 ,
2121 ),(),(
NnNn
C nnTTK    (1) 
where Nj is the set of nodes in tree Tj , and 1 2( , )n nD  
evaluates the common sub-trees rooted at n1 and n2 2 
and is computed recursively as follows:  
1) If the context-free productions (Context-Free 
Grammar(CFG) rules) at 1n  and 2n  are different, 
1 2( , ) 0n nD = ; Otherwise go to 2. 
2) If both 1n  and 2n  are POS tags, 1 2( , ) 1n n lD = ? ; 
Otherwise go to 3. 
3)  Calculate 1 2( , )n nD recursively as: 
?
=
D+=D
)(#
1
2121
1
)),(),,((1(),(
nch
k
knchknchnn l  (2) 
where )(# nch is the number of children of node n , 
),( knch  is the k th child of node n  andl (0< l <1) is 
the decay factor in order to make the kernel value less 
variable with respect to different sub-tree sizes.  
This convolution tree kernel has been successfully 
applied by Zhang et al(2006) in relation extraction. 
However, there is one problem with this tree kernel: 
the sub-trees involved in the tree kernel computation 
are context-free (That is, they do not consider the 
information outside the sub-trees). This is contrast to 
                                                               
2 That is, each node n encodes the identity of a sub-
tree rooted at n and, if there are two nodes in the 
tree with the same label, the summation will go over 
both of them. 
the tree kernel proposed in Culota and Sorensen 
(2004) which is context-sensitive, that is, it considers 
the path from the tree root node to the sub-tree root 
node. In order to integrate the advantages of both tree 
kernels and resolve the problem in Collins and 
Duffy?s convolution tree kernel, this paper proposes a 
context-sensitive convolution tree kernel. It works by 
taking ancestral information (i.e. the root node path) 
of sub-trees into consideration: 
? ?
= ??
D=
m
i NnNn
ii
C
iiii
nnTTK
1 ]2[]2[],1[]1[
11
1111
])2[],1[(])2[],1[(  (3) 
Where 
? ][1 jN i is the set of root node paths with length i 
in tree T[j] while the maximal length of a root 
node path is defined by m.  
? ])[...(][ 211 jnnnjn ii = is a root node path with 
length i in tree T[j] , which takes into account the 
i-1 ancestral nodes in2 [j] of 1n [j] in T[j]. Here, 
][1 jn k+  is the parent of ][ jn k and ][1 jn  is the 
root node of a context-free sub-tree in T[j]. For 
better differentiation, the label of each ancestral 
node in in1 [j] is augmented with the POS tag of 
its head word.  
? ])2[],1[( 11 ii nnD  measures the common context-
sensitive sub-trees rooted at root node paths 
]1[1in  and ]2[1in
3. In our tree kernel, a sub-tree 
becomes context-sensitive with its dependence on 
the root node path instead of the root node itself. 
Figure 2 shows a few examples of context-
sensitive sub-trees with comparison to context-
free sub-trees. 
Similar to Collins and Duffy (2001),   our tree ker-
nel computes ])2[],1[( 11 ii nnD recursively as follows:  
1) If the context-sensitive productions (Context-
Sensitive Grammar (CSG) rules with root node 
paths as their left hand sides) rooted at ]1[1in  and 
]2[1
in  are different, return ])2[],1[( 11
ii nnD =0; 
Otherwise go to Step 2. 
2) If both ]1[1n  and ]2[1n  are POS tags, 
l=D ])2[],1[( 11 ii nn ; Otherwise go to Step 3. 
                                                               
3 That is, each root node path in1  encodes the identity 
of a context-sensitive sub-tree rooted at in1  and, if 
there are two root node paths in the tree with the 
same label sequence, the summation will go over 
both of them.  
732
3) Calculate ])2[],1[( 11 ii nnD  recursively as: 
?
=
D+=
D
])1[(#
1
11
11
1
))],2[(),],1[((1(
])2[],1[(
inch
k
ii
ii
knchknch
nn
l
 (4) 
where ])],[( 1 kjnch i  is the k
th context-sensitive 
child of the context-sensitive sub-tree rooted at 
][1 jn i  with ])[(# 1 jnch i the number of the con-
text-sensitive children. Here, l (0< l <1) is the 
decay factor in order to make the kernel value 
less variable with respect to different sizes of the 
context-sensitive sub-trees. 
It is worth comparing our tree kernel with previous 
tree kernels. Obviously, our tree kernel is an exten-
sion of Collins and Duffy?s convolution tree kernel, 
which is a special case of our tree kernel (if m=1 in 
Equation (3)). Our tree kernel not only counts the 
occurrence of each context-free sub-tree, which does 
not consider its ancestors, but also counts the occur-
rence of each context-sensitive sub-tree, which con-
siders its ancestors. As a result, our tree kernel is not 
limited by the constraints in previous tree kernels (as 
discussed in Section 2), such as Collins and Duffy 
(2001), Zhang et al(2006), Culotta and Sorensen 
(2004) and Bunescu and Mooney (2005a). Finally, 
let?s study the computational issue with our tree ker-
nel. Although our tree kernel takes the context-
sensitive sub-trees into consideration, it only slightly 
increases the computational burden, compared with 
Collins and Duffy?s convolution tree kernel. This is 
due to that 0])2[],1[( 11 =D nn  holds for the major-
ity of context-free sub-tree pairs (Collins and Duffy 
2001) and that computation for context-sensitive sub-
tree pairs is necessary only when 
0])2[],1[( 11 ?D nn  and the context-sensitive sub-
tree pairs have the same root node path(i.e. 
]2[]1[ 11 ii nn =  in Equation (3)). 
4 Experimentation 
This paper uses the ACE RDC 2003 and 2004 cor-
pora provided by LDC in all our experiments. 
4.1 Experimental Setting  
The ACE RDC corpora are gathered from various 
newspapers, newswire and broadcasts. In the 2003 
corpus , the training set consists of 674 documents and 
9683 positive relation instances w hile the test set con-
sists of 97 documents and 1386 positive relation in-
stances. The 2003 corpus defines 5 entity types, 5 
major relation types and 24 relation subtypes. All the 
reported performances in this paper on the ACE RDC 
2003 corpus are evaluated on the test data. The 2004 
corpus  contains 451 documents and 5702 positive 
relation instances. It redefines 7 entity types, 7 major 
relation types and 23 relation subtypes. For compari-
son, we use the same setting as Zhang et al(2006) by 
applying a 5-fold cross-validation on a subset of the 
2004 data, containing 348 documents and 4400 rela-
tion instances. That is, all the reported performances 
in this paper on the ACE RDC 2004 corpus are evalu-
ated using 5-fold cross validation on the entire corpus . 
Both corpora are parsed using Charniak?s parser 
(Charniak, 2001) with the boundaries of all the entity 
mentions kept 4 . We iterate over all pairs of entity 
mentions occurring in the same sentence to generate 
potential relation instances5. In our experimentation, 
SVM (SVMLight, Joachims(1998)) is selected as our 
classifier. For efficiency, we apply the one vs. others 
strategy, which builds K classifiers so as to separate 
one class from all others. The training parameters are 
chosen using cross-validation on the ACE RDC 2003 
training data.  In particular, l  in our tree kernel is 
fine-tuned to 0.5. This suggests that about 50% dis-
count is done as our tree kernel moves down one 
level in computing ])2[],1[( 11 ii nnD .  
4.2 Experimental Results  
First, we systematically evaluate the context-sensitive 
convolution tree kernel and the dynamic context-
sensitive tree span proposed in this paper. 
Then, we evaluate the complementary nature be-
tween our tree kernel and a state-of-the-art linear ker-
nel via a composite kernel. Generally different 
feature-based methods and tree kernel-based methods 
have their own merits. It is usually easy to build a 
system using a feature-based method and achieve the 
state-of-the-art performance, while tree kernel-based 
methods  hold the potential for further performance 
improvement. Therefore, it is always a good idea to 
integrate them via a composite kernel.  
                                                               
4 This can be done by first representing all entity men-
tions with their head words and then restoring all the 
entity mentions after parsing. Moreover, please note 
that the final performance of relation extraction may 
change much with different range of parsing errors. 
We will study this issue in the near future. 
5 In this paper, we only measure the performance of rela-
tion extraction on ?true? mentions with ?true? chain-
ing of co-reference (i.e. as annotated by LDC 
annotators ). Moreover, we only model explicit relations and 
explicitly model the argument order of the two mentions in-
volved. 
733
Finally, we compare our system with the state-of-
the-art systems in the literature.  
Context-Sensitive Convolution Tree Kernel 
In this paper, the m parameter of our context-sensitive 
convolution tree kernel as shown in Equation (3) 
indicates the maximal length of root node paths and is 
optimized to 3 using 5-fold cross validation on the 
ACE RDC 2003 training data. Table 1 compares the 
impact of different m in context-sensitive convolution 
tree kernels using the Shortest Path-enclosed Tree 
(SPT) (as described in Zhang et al(2006)) on the 
major relation types of the ACE RDC 2003 and 2004 
corpora, in details. It also shows that our tree kernel 
achieves best performance on the test data using SPT 
with m = 3, which outperforms the one with m = 1 by 
~2.3 in F-measure. This suggests the parent and 
grandparent nodes of a sub-tree  contains much 
information for relation extraction while considering 
more ancestral nodes may not help. This may be due 
to that, although our experimentation on the 
training data indicates that  more than 80% (on 
average) of subtrees has a root node path longer 
than 3 (since most of the subtrees are deep from the 
root node and more than 90% of the parsed trees in 
the training data are deeper than 6 levels), 
including a root node path longer than 3 may be 
vulnerable to the full parsing errors and have 
negative impact. Table 1 also evaluates the impact of 
entity-related information in our tree kernel by 
attaching entity type information (e.g. ?PER? in the 
entity node 1 of Figure 1(b)) into both entity nodes. 
It shows that such information can significantly 
improve the performance by ~6.0 in F-measure. In all 
the following experiments, we will apply our tree 
kernel with m=3 and entity-related information by 
default. 
Table 2 compares the dynamic context-sensitive 
tree span with SPT using our tree kernel. It shows that 
the dynamic tree span can futher improve the 
performance by ~1.2 in F-measure6. This suggests the 
usefulness of extending the tree span beyond SPT for 
the ?predicate-linked? tree span category. In the 
future work, we will further explore expanding the 
dynamic tree span beyond SPT for the remaining tree 
span categories. 
  
  
  
                                                               
6 Significance test shows that the dynamic tree span per-
forms s tatistically significantly better than SPT with p-
values smaller than 0.05. 
m P(%) R(%) F 
1 72.3(72.7)  56.6(53.8) 63.5(61.8)  
2 74.9(75.2)  57.9(54.7) 65.3(63.5)  
3 75.7(76.1)  58.3(55.1) 65.9(64.0)  
4 76.0(75.9)  58.3(55.3) 66.0(63.9)  
a) without entity-related information 
m P(%) R(%) F 
1 77.2(76.9)  63.5(60.8) 69.7(67.9)  
2 79.1(78.6)  65.0(62.2) 71.3(69.4)  
3 79.6(79.4)  65.6(62.5) 71.9(69.9)  
4 79.4(79.1)  65.6(62.3) 71.8(69.7)  
b) with entity-related information 
Table 1: Evaluation of context-sensitive convolution 
tree kernels using SPT on the major relation types of 
the ACE RDC 2003 (inside the parentheses) and 2004 
(outside the parentheses) corpora. 
Tree Span P(%) R(%) F 
Shortest Path-  
enclosed Tree 
79.6 
(79.4) 
65.6 
(62.5) 
71.9 
(69.9) 
Dynamic Context- 
Sensitive Tee 
81.1 
(80.1) 
66.7 
(63.8) 
73.2 
(71.0) 
Table 2: Comparison of dynamic context-sensitive 
tree span with SPT using our context-sensitive 
convolution tree kernel on the major relation types of 
the ACE RDC 2003 (inside the parentheses) and 2004 
(outside the parentheses) corpora. 18% of positive 
instances in the ACE RDC 2003 test data belong to 
the predicate-linked category. 
  
Composite Kernel 
In this paper, a composite kernel via polynomial in-
terpolation, as described Zhang et al(2006), is ap-
plied to integrate the proposed context-sensitive 
convolution tree kernel with a state-of-the-art linear 
kernel (Zhou et al2005) 7: 
),()1(),(),(1 ???-+???=?? CPL KKK aa  (5) 
Here, ),( ??LK  and ),( ??CK  indicates the normal-
ized linear kernel and context-sensitive convolution 
tree kernel respectively while  ( , )pK ? ?  is the poly-
nomial expansion of ( , )K ? ?  with degree d=2, i.e. 
2( , ) ( ( , ) 1)pK K? ? ? ?= +  and a  is the coefficient (a  is 
set to 0.3 using cross-validation). 
                                                               
7 Here, we use the same set of flat features (i.e. word, 
entity type, mention level, overlap, base phrase chunk-
ing, dependency tree, parse tree and semantic informa-
tion) as Zhou et al(2005). 
734
Table 3 evaluates the performance of the 
composite kernel. It shows that the composite kernel 
much further improves the performance beyond that 
of either the state-of-the-art linear kernel or our tree 
kernel and achieves the F-measures of 74.1 and 75.8 
on the major relation types of the ACE RDC 2003 
and 2004 corpora respectively. This suggests that our 
tree kernel and the state-of-the-art linear kernel are 
quite complementary, and that our composite kernel 
can effectively integrate both flat and structured 
features. 
System P(%) R(%) F 
Linear Kernel 78.2 (77.2) 
63.4 
(60.7) 
70.1 
(68.0) 
Context-Sensitive Con-
volution Tree Kernel 
81.1 
(80.1) 
66.7 
(63.8) 
73.2 
(71.0) 
Composite Kernel 82.2 (80.8) 
70.2 
(68.4) 
75.8 
(74.1) 
Table 3: Performance of the compos ite kernel via 
polynomial interpolation on the major relation types 
of the ACE RDC 2003 (inside the parentheses) and 
2004 (outside the parentheses) corpora 
  
Comparison with Other Systems  
ACE RDC 2003 P(%) R(%) F 
Ours:  
composite kernel 
80.8 
(65.2) 
68.4 
(54.9) 
74.1 
(59.6) 
Zhang et al(2006):  
composite kernel 
77.3 
(64.9) 
65.6 
(51.2) 
70.9 
(57.2) 
Ours: context-sensitive  
convolution tree kernel 
80.1 
(63.4) 
63.8 
(51.9) 
71.0 
(57.1) 
Zhang et al(2006):  
convolution tree kernel 
76.1 
(62.4) 
62.6 
(48.5) 
68.7 
(54.6) 
Bunescu et al(2005):  
shortest path  
dependency kernel 
65.5 
(-) 
43.8 
(-) 
52.5 
(-) 
Culotta et al(2004):  
dependency kernel 
67.1 
(-) 
35.0 
(-) 
45.8 
(-) 
Zhou et al (2005):  
feature-based 
77.2 
(63.1) 
60.7 
(49.5) 
68.0 
(55.5) 
Kambhatla (2004):  
feature-based 
- 
(63.5) 
- 
(45.2) 
- 
(52.8) 
Table 4: Comparison of difference systems on the 
ACE RDC 2003 corpus over both 5 types (outside the 
parentheses) and 24 subtypes (inside the parentheses) 
  
  
  
ACE RDC 2004 P(%) R(%) F 
Ours:  
composite kernel 
82.2 
(70.3) 
70.2 
(62.2) 
75.8 
(66.0) 
Zhang et al(2006):  
composite kernel 
76.1 
(68.6) 
68.4 
(59.3) 
72.1 
(63.6) 
Zhao et al(2005):8  
composite kernel 
69.2 
(-) 
70.5 
(-) 
70.4 
(-) 
Ours: context-sensitive  
convolution tree kernel 
81.1 
(68.8) 
66.7 
(60.3) 
73.2 
(64.3) 
Zhang et al(2006):  
convolution tree kernel 
72.5 
(-) 
56.7 
(-) 
63.6 
(-) 
Table 5: Comparison of difference systems on the 
ACE RDC 2004 corpus over both 7 types (outside the 
parentheses) and 23 subtypes (inside the parentheses) 
  
Finally, Tables 4 and 5 compare our system with 
other state-of-the-art systems9 on the ACE RDC 2003 
and 2004 corpora, respectively. They show that our 
tree kernel-based system outperforms previous tree 
kernel-based systems. This is largely due to the con-
text-sensitive nature of our tree kernel which resolves 
the limitations of the previous tree kernels. They also 
show that our tree kernel-based system outperforms 
the state-of-the-art feature-based system. This proves 
the great potential inherent in the parse tree structure 
for relation extraction and our tree kernel takes a big 
stride towards the right direction. Finally, they also 
show that our composite kernel-based system outper-
forms other composite kernel-based systems. 
5 Conclusion 
Structured parse tree information holds great potential 
for relation extraction. This paper proposes a context-
sensitive convolution tree kernel to resolve two criti-
cal problems in previous tree kernels for relation ex-
traction by first automatically determining a dynamic 
context-sensitive tree span and then applying a con-
text-sensitive convolution tree kernel. Moreover, this 
paper evaluates the complementary nature between 
our tree kernel and a state-of-the-art linear kernel. 
Evaluation on the ACE RDC corpora shows that our 
dynamic context-sensitive tree span is much more 
suitable for relation extraction than the widely -used 
Shortest Path-enclosed Tree and our tree kernel out-
performs the state-of-the-art Collins and Duffy?s con-
volution tree kernel. It also shows that feature-based 
                                                               
8 There might be some typing errors for the performance 
reported in Zhao and Grishman(2005) since P, R and F 
do not match. 
9 All the state-of-the-art systems apply the entity-related 
information. It is not supervising: our experiments 
show that using the entity-related information gives a 
large performance improvement.  
735
and tree kernel-based methods well complement each 
other and the composite kernel can effectively inte-
grate both flat and structured features.  
To our knowledge, this is the first research to dem-
onstrate that, without extensive feature engineer ing, 
an individual tree kernel can achieve much better per-
formance than the state-of-the-art linear kernel in re-
lation extraction. This shows the great potential of 
structured parse tree information for relation extrac-
tion and our tree kernel takes a big stride towards the 
right direction.  
For the future work, we will focus on improving 
the context-sensitive convolution tree kernel by ex-
ploring more useful context information. Moreover, 
we will explore more entity-related information in the 
parse tree. Our preliminary work of including the en-
tity type information significantly improves the per-
formance. Finally, we will study how to resolve the 
data imbalance and sparseness issues from the learn-
ing algorithm viewpoint.  
Acknowledgement 
This research is supported by Project 60673041 under 
the National Natural Science Foundation of China 
and Project 2006AA01Z147 under the ?863? National 
High-Tech Research and Development of China. We 
would also like to thank the critical and insightful 
comments from the four anonymous reviewers. 
References  
ACE. (2000-2005). Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/  
Bunescu R. & Mooney R.J. (2005a). A shortest path 
dependency kernel for relation extraction. 
HLT/EMNLP?2005 : 724-731. 6-8 Oct 2005. Van-
cover, B.C. 
Bunescu R. & Mooney R.J. (2005b). Subsequence Ker-
nels for Relation Extraction  NIPS?2005. Vancouver, 
BC, December 2005  
Charniak E. (2001). Immediate-head Parsing for Lan-
guage Models. ACL?2001: 129-137. Toulouse, France 
Collins M. and Duffy N. (2001). Convolution Ke rnels 
for Natural Language. NIPS?2001: 625-632. Ca m-
bridge, MA 
Culotta A. and Sorensen J. (2004). Dependency tree 
kernels for relation extraction. ACL?2004 . 423-429. 
21-26 July 2004. Ba rcelona, Spain. 
Haussler D. (1999). Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, Uni-
versity of California, Santa Cruz 
Joachims T. (1998). Text Categorization with Su pport 
Vector Machine: learning with many relevant fea-
tures. ECML-1998 : 137-142.  Chemnitz, Germany 
Kambhatla N. (2004). Combining lexical, syntactic and 
semantic features with Maximum Entropy models for 
extracting relations. ACL?2004(Poster). 178-181. 21-
26 July 2004. Barcelona, Spain. 
MUC. (1987-1998). The NIST MUC website: http: 
//www.itl.nist.gov/iaui/894.02/related_projects/muc/ 
Zelenko D., Aone C. and Richardella. (2003). Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 3(Feb):1083-1106. 
Zhang M., Zhang J., Su J. and Zhou G.D. (2006). A 
Composite Kernel to Extract Relations between Enti-
ties with both Flat and Structured Features . COLING-
ACL-2006: 825-832. Sydney, Australia 
Zhao S.B. and Grishman R. (2005). Extracting relations 
with integrated information using kernel methods. 
ACL?2005: 419-426. Univ of Michigan-Ann Arbor,  
USA,  25-30 June 2005. 
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). Ex-
ploring various knowledge in relation extraction. 
ACL?2005. 427-434. 25-30 June, Ann Arbor, Mich-
gan, USA.  
Zhou G.D., Su J. and Zhang M. (2006). Modeling com-
monality among related classes in relation extraction, 
COLING-ACL?2006: 121-128. Sydney, Australia. 
736
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 599?607,
Beijing, August 2010
Dependency-driven Anaphoricity Determination for Coreference 
Resolution
Fang Kong  Guodong Zhou  Longhua Qian  Qiaoming Zhu*
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology Soochow University 
{kongfang, gdzhou, qianlonghua, qmzhu}@suda.edu.cn 
                                                          
* Corresponding author 
Abstract
This paper proposes a dependency-driven 
scheme to dynamically determine the syn-
tactic parse tree structure for tree ker-
nel-based anaphoricity determination in 
coreference resolution. Given a full syntactic 
parse tree, it keeps the nodes and the paths 
related with current mention based on con-
stituent dependencies from both syntactic 
and semantic perspectives, while removing 
the noisy information, eventually leading to 
a dependency-driven dynamic syntactic 
parse tree (D-DSPT). Evaluation on the ACE 
2003 corpus shows that the D-DSPT out-
performs all previous parse tree structures on 
anaphoricity determination, and that apply-
ing our anaphoricity determination module 
in coreference resolution achieves the so far 
best performance. 
1 Introduction 
Coreference resolution aims to identify which 
noun phrases (NPs, or mentions) refer to the 
same real-world entity in a text. According to 
Webber (1979), coreference resolution can be 
decomposed into two complementary sub-tasks: 
(1) anaphoricity determination, determining 
whether a given NP is anaphoric or not; and (2) 
anaphor resolution, linking together multiple 
mentions of a given entity in the world. Al-
though machine learning approaches have per-
formed reasonably well in coreference resolu-
tion without explicit anaphoricity determina-
tion (e.g. Soon et al 2001; Ng and Cardie 
2002b; Yang et al 2003, 2008; Kong et al 
2009), knowledge of NP anaphoricity is ex-
pected to much improve the performance of a 
coreference resolution system, since a 
non-anaphoric NP does not have an antecedent 
and therefore does not need to be resolved. 
Recently, anaphoricity determination has 
been drawing more and more attention. One 
common approach involves the design of some 
heuristic rules to identify specific types of 
non-anaphoric NPs, such as pleonastic it (e.g. 
Paice and Husk 1987; Lappin and Leass 1994, 
Kennedy and Boguraev 1996; Denber 1998) 
and definite descriptions (e.g. Vieira and Poe-
sio 2000). Alternatively, some studies focus on 
using statistics to tackle this problem (e.g., 
Bean and Riloff 1999; Bergsma et al 2008) 
and others apply machine learning approaches 
(e.g. Evans 2001;Ng and Cardie 2002a, 
2004,2009; Yang et al 2005; Denis and Bal-
bridge 2007; Luo 2007; Finkel and Manning 
2008; Zhou and Kong 2009).  
As a representative, Zhou and Kong (2009) 
directly employ a tree kernel-based method to 
automatically mine the non-anaphoric informa-
tion embedded in the syntactic parse tree. One 
main advantage of the kernel-based methods is 
that they are very effective at reducing the 
burden of feature engineering for structured 
objects. Indeed, the kernel-based methods have 
been successfully applied to mine structured 
information in various NLP applications like 
syntactic parsing (Collins and Duffy, 2001; 
Moschitti, 2004), semantic relation extraction 
(Zelenko et al, 2003; Zhao and Grishman, 
2005; Zhou et al 2007; Qian et al, 2008), se-
mantic role labeling (Moschitti, 2004); corefer-
ence resolution (Yang et al, 2006; Zhou et al, 
2008). One of the key problems for the ker-
nel-based methods is how to effectively capture 
the structured information according to the na-
ture of the structured object in the specific task. 
This paper advances the state-of-the-art per-
formance in anaphoricity determination by ef-
599
fectively capturing the structured syntactic in-
formation via a tree kernel-based method. In 
particular, a dependency-driven scheme is 
proposed to dynamically determine the syntac-
tic parse tree structure for tree kernel-based 
anaphoricity determination by exploiting con-
stituent dependencies from both the syntactic 
and semantic perspectives to keep the neces-
sary information in the parse tree as well as 
remove the noisy information. Our motivation 
is to employ critical dependency information in 
constructing a concise and effective syntactic 
parse tree structure, specifically targeted for 
tree kernel-based anaphoricity determination.  
The rest of this paper is organized as follows. 
Section 2 briefly describes the related work on 
both anaphoricity determination and exploring 
syntactic parse tree structures in related tasks. 
Section 3 presents our dependency-driven 
scheme to determine the syntactic parse tree 
structure. Section 4 reports the experimental 
results. Finally, we conclude our work in Sec-
tion 5. 
2 Related Work 
This section briefly overviews the related work 
on both anaphoricity determination and ex-
ploring syntactic parse tree structures. 
2.1 Anaphoricity Determination 
Previous work on anaphoricity determination 
can be broadly divided into three categories: 
heuristic rule-based (e.g. Paice and Husk 
1987;Lappin and Leass 1994; Kennedy and 
Boguraev 1996; Denber 1998; Vieira and Poe-
sio 2000; Cherry and Bergsma 2005), statis-
tics-based (e.g. Bean and Riloff 1999; Cherry 
and Bergsma 2005; Bergsma et al 2008) and 
learning-based methods (e.g. Evans 2001; Ng 
and Cardie 2002a; Ng 2004; Yang et al 2005; 
Denis and Balbridge 2007; Luo 2007; Finkel 
and Manning 2008; Zhou and Kong 2009; Ng 
2009).  
The heuristic rule-based methods focus on 
designing some heuristic rules to identify spe-
cific types of non-anaphoric NPs. Representa-
tive work includes: Paice and Husk (1987), 
Lappin and Leass (1994) and Kennedy and 
Boguraev (1996). For example, Kennedy and 
Boguraev (1996) looked for modal adjectives 
(e.g. ?necessary?) or cognitive verbs (e.g. ?It is 
thought that?? in a set of patterned construc-
tions) in identifying pleonastic it.
Among the statistics-based methods, Bean 
and Riloff (1999) automatically identified ex-
istential definite NPs which are non-anaphoric.  
The intuition behind is that many definite NPs 
are not anaphoric since their meanings can be 
understood from general world knowledge, e.g. 
?the FBI?. They found that existential NPs ac-
count for 63% of all definite NPs and 76% of 
them could be identified by syntactic or lexical 
means. Cherry and Bergsma (2005) extended 
the work of Lappin and Leass (1994) for 
large-scale anaphoricity determination by addi-
tionally detecting pleonastic it. Bergsma et al 
(2008) proposed a distributional method in de-
tecting non-anaphoric pronouns. They first ex-
tracted the surrounding context of the pronoun 
and gathered the distribution of words that oc-
curred within the context from a large corpus, 
and then identified the pronoun either ana-
phoric or non-anaphoric based on the word dis-
tribution.
Among the learning-based methods, Evans 
(2001) automatically identified the 
non-anaphoricity of pronoun it using various 
kinds of lexical and syntactic features. Ng and 
Cardie (2002a) employed various do-
main-independent features in identifying ana-
phoric NPs. They trained an anaphoricity clas-
sifier to determine whether a NP was anaphoric 
or not, and employed an independently-trained 
coreference resolution system to only resolve 
those mentions which were classified as ana-
phoric. Experiments showed that their method 
improved the performance of coreference 
resolution by 2.0 and 2.6 to 65.8 and 64.2 in 
F1-measure on the MUC-6 and MUC-7 cor-
pora, respectively. Ng (2004) examined the 
representation and optimization issues in com-
puting and using anaphoricity information to 
improve learning-based coreference resolution. 
On the basis, he presented a corpus-based ap-
proach (Ng, 2009) for achieving global opti-
mization by representing anaphoricity as a fea-
ture in coreference resolution. Experiments on 
the ACE 2003 corpus showed that their method 
improved the overall performance by 2.8, 2.2 
and 4.5 to 54.5, 64.0 and 60.8 in F1-measure 
on the NWIRE, NPAPER and BNEWS do-
mains, respectively. However, he did not look 
into the contribution of anaphoricity determi-
600
nation on coreference resolution of different 
NP types. Yang et al (2005) made use of 
non-anaphors to create a special class of train-
ing instances in the twin-candidate model 
(Yang et al 2003) and improved the perform-
ance by 2.9 and 1.6 to 67.3 and 67.2 in 
F1-measure on the MUC-6 and MUC-7 cor-
pora, respectively. However, their experiments 
show that eliminating non-anaphors using an 
anaphoricity determination module in advance 
harms the performance. Denis and Balbridge 
(2007) employed an integer linear program-
ming (ILP) formulation for coreference resolu-
tion which modeled anaphoricity and corefer-
ence as a joint task, such that each local model 
informed the other for the final assignments. 
Experiments on the ACE 2003 corpus showed 
that this joint anaphoricity-coreference ILP 
formulation improved the F1-measure by 
3.7-5.3 on various domains. However, their 
experiments assume true ACE mentions (i.e. all 
the ACE mentions are already known from the 
annotated corpus). Therefore, the actual effect 
of this joint anaphoricity-coreference ILP for-
mulation on fully automatic coreference reso-
lution is still unclear. Luo (2007) proposed a 
twin-model for coreference resolution: a link 
component, which models the coreferential 
relationship between an anaphor and a candi-
date antecedent, and a creation component, 
which models the possibility that a NP was not 
coreferential with any candidate antecedent. 
This method combined the probabilities re-
turned by the creation component (an ana-
phoricity model) with the link component (a 
coreference model) to score a coreference par-
tition, such that a partition was penalized 
whenever an anaphoric mention was resolved. 
Finkel and Manning (2008) showed that transi-
tivity constraints could be incorporated into an 
ILP-based coreference resolution system and 
much improved the performance. Zhou and 
Kong (2009) employed a global learning 
method in determining the anaphoricity of NPs 
via a label propagation algorithm to improve 
learning-based coreference resolution. Experi-
ments on the ACE 2003 corpus demonstrated 
that this method was very effective. It could 
improve the F1-measure by 2.4, 3.1 and 4.1 on 
the NWIRE, NPAPER and BNEWS domains, 
respectively. Ng (2009) presented a novel ap-
proach to the task of anaphoricity determina-
tion based on graph minimum cuts and demon-
strated the effectiveness in improving a learn-
ing-based coreference resolution system. 
In summary, although anaphoricity determi-
nation plays an important role in coreference 
resolution and achieves certain success in im-
proving the overall performance of coreference 
resolution, its contribution is still far from ex-
pectation.
2.2 Syntactic Parse Tree Structures 
For a tree kernel-based method, one key prob-
lem is how to represent and capture the struc-
tured syntactic information. During recent 
years, various tree kernels, such as the convo-
lution tree kernel (Collins and Duffy, 2001), 
the shallow parse tree kernel (Zelenko et al
2003) and the dependency tree kernel (Culota 
and Sorensen, 2004), have been proposed in the 
literature. Among these tree kernels, the con-
volution tree kernel represents the state-of-the 
art and has been successfully applied by 
Collins and Duffy (2002) on syntactic parsing, 
Zhang et al (2006) on semantic relation extrac-
tion and Yang et al (2006) on pronoun resolu-
tion.
Given a tree kernel, the key issue is how to 
generate a syntactic parse tree structure for ef-
fectively capturing the structured syntactic in-
formation. In the literature, various parse tree 
structures have been proposed and successfully 
applied in some NLP applications. As a repre-
sentative, Zhang et al (2006) investigated five 
parse tree structures for semantic relation ex-
traction and found that the Shortest 
Path-enclosed Tree (SPT) achieves the best 
performance on the 7 relation types of the ACE 
RDC 2004 corpus. Yang et al (2006) con-
structed a document-level syntactic parse tree 
for an entire text by attaching the parse trees of 
all its sentences to a new-added upper node and 
examined three possible parse tree structures 
(Min-Expansion, Simple-Expansion and 
Full-Expansion) that contain different sub-
structures of the parse tree for pronoun resolu-
tion. Experiments showed that their method 
achieved certain success on the ACE 2003 
corpus and the simple-expansion scheme per-
forms best. However, among the three explored 
schemes, there exists no obvious overwhelming 
one, which can well cover structured syntactic 
information. One problem of Zhang et al (2006) 
601
and Yang et al (2006) is that their parse tree 
structures are context-free and do not consider 
the information outside the sub-trees. Hence, 
their ability of exploring structured syntactic 
information is much limited. Motivated by 
Zhang et al (2006) and Yang et al (2006), 
Zhou et al (2007) extended the SPT to become 
context-sensitive (CS-SPT) by dynamically 
including necessary predicate-linked path in-
formation. Zhou et al (2008) further proposed 
a dynamic-expansion scheme to automatically 
determine a proper parse tree structure for 
pronoun resolution by taking predicate- and 
antecedent competitor-related information in 
consideration. Evaluation on the ACE 2003 
corpus showed that the dynamic-expansion 
scheme can well cover necessary structured 
information in the parse tree for pronoun reso-
lution. One problem with the above parse tree 
structures is that they may still contain unnec-
essary information and also miss some useful 
context-sensitive information. Qian et al (2008) 
dynamically determined the parse tree structure 
for semantic relation extraction by exploiting 
constituent dependencies to keep the necessary 
information in the parse tree as well as remove 
the noisy information. Evaluation on the ACE 
RDC 2004 corpus showed that their dynamic 
syntactic parse tree structure outperforms all 
previous parse tree structures. However, their 
solution has the limitation in that the depend-
encies were found according to some manu-
ally-written ad-hoc rules and thus may not be 
easily applicable to new domains and applica-
tions.
This paper proposes a new scheme to dy-
namically determine the syntactic parse tree 
structure for anaphoricity determination and 
systematically studies the application of an ex-
plicit anaphoricity determination module in 
improving coreference resolution. 
3 Dependency-driven Dynamic Syn-
tactic Parse Tree 
Given a full syntactic parse tree and a NP in 
consideration, one key issue is how to choose a 
proper syntactic parse tree structure to well 
cover structured syntactic information in the 
tree kernel computation. Generally, the more a 
syntactic parse tree structure includes, the more 
structured syntactic information would be 
available, at the expense of more noisy (or un-
necessary) information.  
It is well known that dependency informa-
tion plays a key role in many NLP problems, 
such as syntactic parsing, semantic role label-
ing as well as semantic relation extraction. Mo-
tivated by Qian et al (2008) and Zhou et al 
(2008), we propose a new scheme to dynami-
cally determine the syntactic parse tree struc-
ture for anaphoricity determination by exploit-
ing constituent dependencies from both the 
syntactic and semantic perspectives to distin-
guish the necessary evidence from the unnec-
essary information in the syntactic parse tree. 
That is, constituent dependencies are explored 
from two aspects: syntactic dependencies and 
semantic dependencies.  
1) Syntactic Dependencies: The Stanford de-
pendency parser1 is employed as our syn-
tactic dependency parser to automatically 
extract various syntactic (i.e. grammatical) 
dependencies between individual words. In 
this paper, only immediate syntactic de-
pendencies with current mention are con-
sidered. The intuition behind is that the im-
mediate syntactic dependencies carry the 
major contextual information of current 
mention.
2) Semantic Dependencies: A state-of-the-art 
semantic role labeling (SRL) toolkit (Li et 
al. 2009) is employed for extracting various 
semantic dependencies related with current 
mention. In this paper, semantic dependen-
cies include all the predicates heading any 
node in the root path from current mention 
to the root node and their compatible argu-
ments (except those overlapping with cur-
rent mention). 
We name our parse tree structure as a depend-
ency-driven dynamic syntactic parse tree 
(D-DSPT). The intuition behind is that the de-
pendency information related with current 
mention in the same sentence plays a critical 
role in anaphoricity determination. Given the 
sentence enclosing the mention under consid-
eration, we can get the D-DSPT as follows: 
(Figure 1 illustrates an example of the D-DSPT 
generation given the sentence ?Mary said the 
woman in the room bit her? with ?woman? as 
current mention.) 
                                                          
1 http://nlp.stanford.edu/software/lex-parser.shtml
602
           
Figure 1:  An example of generating the dependency-driven dynamic syntactic parse tree  
1) Generating the full syntactic parse tree of 
the given sentence using a full syntactic parser. 
In this paper, the Charniak parser (Charniak 
2001) is employed and Figure 1 (a) shows the 
resulting full parse tree. 
2) Keeping only the root path from current 
mention to the root node of the full parse tree. 
Figure 1(b) shows the root path corresponding 
to the current mention ?woman?. In the fol-
lowing steps, we attach the above two types of 
dependency information to the root path.  
3) Extracting all the syntactic dependencies 
in the sentence using a syntactic dependency 
parser, and attaching all the nodes, which have 
immediate dependency relationship with cur-
rent mention, and their corresponding paths to 
the root path. Figure 1(c) illustrates the syntac-
tic dependences extracted from the sentence, 
where the ones in italic mean immediate de-
pendencies with current mention. Figure 1(d) 
shows the parse tree structure after considering 
syntactic dependencies. 
4) Attaching all the predicates heading any 
node in the root path from current mention to 
the root node and their corresponding paths to 
the root path. For the example sentence, there 
are two predicates ?said? and ?bit?, which head 
the ?VP? and ?S? nodes in the root path re-
spectively. Therefore, these two predicates and 
their corresponding paths should be attached to 
the root path as shown in Figure 1(e). Note that 
the predicate ?bit? and its corresponding path 
has already been attached in Stop (3). As a re-
sult, the predicate-related information can be 
attached. According to Zhou and Kong (2009), 
such information is important to definite NP 
resolution.
5) Extracting the semantic dependencies re-
lated with those attached predicates using a 
(shallow) semantic parser, and attaching all the 
compatible arguments (except those overlap-
ping with current mention) and their corre-
sponding paths to the root path. For example, 
as shown in Figure 1(e), since the arguments 
?Mary? and ?her? are compatible with current 
mention ?woman?, these two nodes and their 
corresponding paths are attached while the ar-
gument ?room? is not since its gender does not 
agree with current mention. 
In this paper, the similarity between two 
parse trees is measured using a convolution tree 
kernel, which counts the number of common 
sub-tree as the syntactic structure similarity 
between two parse trees. For details, please 
refer to Collins and Duffy (2001). 
603
4 Experimentation and Discussion 
This section evaluates the performance of de-
pendency-driven anaphoricity determination 
and its application in coreference resolution on 
the ACE 2003 corpus. 
4.1 Experimental Setting 
The ACE 2003 corpus contains three domains: 
newswire (NWIRE), newspaper (NPAPER), 
and broadcast news (BNEWS). For each do-
main, there exist two data sets, training and 
devtest, which are used for training and testing.  
For preparation, all the documents in the 
corpus are preprocessed automatically using a 
pipeline of NLP components, including to-
kenization and sentence segmentation, named 
entity recognition, part-of-speech tagging and 
noun phrase chunking. Among them, named 
entity recognition, part-of-speech tagging and 
noun phrase chunking apply the same 
state-of-the-art HMM-based engine with er-
ror-driven learning capability (Zhou and Su, 
2000 & 2002). Our statistics finds that 62.0%, 
58.5% and 61.4% of entity mentions are pre-
served after preprocessing on the NWIRE, 
NPAPER and BNEWS domains of the ACE 
2003 training data respectively while only 
89.5%, 89.2% and 94% of entity mentions are 
preserved after preprocessing on  the NWIRE, 
NPAPER and BNEWS domains of the ACE 
2003 devtest data. This indicates the difficulty 
of coreference resolution. In addition, the cor-
pus is parsed using the Charniak parser for 
syntactic parsing and the Stanford dependency 
parser for syntactic dependencies while corre-
sponding semantic dependencies are extracted 
using a state-of-the-art semantic role labeling 
toolkit (Li et al 2009). Finally, we use the 
SVM-light2 toolkit with the tree kernel func-
tion as the classifier. For comparison purpose, 
the training parameters C (SVM) and ?(tree
kernel) are set to 2.4 and 0.4 respectively, as 
done in Zhou and Kong (2009).  
For anaphoricity determination, we report 
the performance in Acc+ and Acc-, which 
measure the accuracies of identifying anaphoric 
NPs and non-anaphoric NPs, respectively. Ob-
viously, higher Acc+ means that more ana-
phoric NPs would be identified correctly, while 
                                                          
2 http://svmlight.joachims.org/ 
higher Acc- means that more non-anaphoric 
NPs would be filtered out. For coreference 
resolution, we report the performance in terms 
of recall, precision, and F1-measure using the 
commonly-used model theoretic MUC scoring 
program (Vilain et al 1995). To see whether an 
improvement is significant, we also conduct 
significance testing using paired t-test. In this 
paper, ?***?, ?**? and ?*? denote p-values of an 
improvement smaller than 0.01, in-between 
(0.01, 0,05] and bigger than 0.05, which mean 
significantly better, moderately better and 
slightly better, respectively. 
4.2 Experimental Results 
Performance of anaphoricity determination 
Table 1 presents the performance of anaphoric-
ity determination using the convolution tree 
kernel on D-DSPT. It shows that our method 
achieves the accuracies of 83.27/77.13, 
86.77/80.25 and 90.02/64.24 on identifying 
anaphoric/non-anaphoric NPs in the NWIRE, 
NPAPER and BNEWS domains, respectively.  
This suggests that our approach can effectively 
filter out about 75% of non-anaphoric NPs and 
keep about 85% of anaphoric NPs. In com-
parison, in the three domains Zhou and Kong 
(2009) achieve the accuracies of 76.5/82.3, 
78.9/81.6 and 74.3/83.2, respectively, using the 
tree kernel on a dynamically-extended tree 
(DET). This suggests that their method can fil-
ter out about 82% of non-anaphoric NPs and 
only keep about 76% of anaphoric NPs. In 
comparison, their method outperforms our 
method on filtering out more non-anaphoric 
NPs while our method outperforms their 
method on keeping more anaphoric NPs in 
coreference resolution. While a coreference 
resolution system can detect some 
non-anaphoric NPs (when failing to find the 
antecedent candidate), filtering out anaphoric 
NPs in anaphoricity determination would defi-
nitely cause errors and it is almost impossible 
to recover. Therefore, it is normally more im-
portant to keeping more anaphoric NPs than 
filtering out more non-anaphoric NPs. Table 1 
further presents the performance of anaphoric-
ity determination on different NP types. It 
shows that our method performs best at keep-
ing pronominal NPs and filtering out proper 
NPs.
604
NWIRE NPAPER BNEWS NP Type 
Acc+ Acc- Acc+ Acc- Acc+ Acc-
Pronoun 95.07 50.36 96.40 56.44 98.26 54.03 
Proper NP 84.61 83.17 83.78 79.62 87.61 71.77 
Definite NP 87.17 46.74 82.24 49.18 86.87 53.65 
Indefinite NP 86.01 47.52 80.63 48.45 89.71 47.32 
Over all 83.27 77.13 86.77 80.25 90.02 64.24 
Table 1: Performance of anaphoricity determination using the D-DSPT  
NWIRE NPAPER BNEWS Performance Change 
Acc+ Acc- Acc+ Acc- Acc+ Acc-
D-DSPT 83.27 77.13 86.77 80.25 90.02 64.24 
-Syntactic Dependencies 78.67 72.56 80.14 73.74 87.05 60.20 
-Semantic Dependencies 81.67 76.74 83.47 77.93 89.58 60.67 
Table 2: Contribution of including syntactic and semantic dependencies  
in D-DSPT on anaphoricity determination  
NWIRE NPAPER BNEWS System 
R% P% F R% P% F R% P% F 
Pronoun 70.8 57.9 63.7 76.5 63.5 69.4 70.0 60.3 64.8
Proper NP 80.3 80.1 80.2 81.8 83.6 82.7 76.3 76.8 76.6
Definite NP 35.9 43.4 39.2 43.1 48.5 45.6 47.9 51.9 49.8
Indefinite NP 40.3 26.3 31.8 39.7 22.9 29.0 23.6 10.7 14.7
Without ana-
phoricity de-
termination 
(Baseline)
Over all 55.0 63.8 59.1 62.1 65.0 63.5 53.2 60.5 56.6
Pronoun 65.9 70.2 68.0 72.6 78.7 75.5 67.7 75.8 71.5
Proper NP 80.3 81.0 80.6 81.2 85.1 83.1 76.3 84.4 80.1
Definite NP 32.3  63.1 42.7 38.4 61.7 47.3 42.5 66.4 51.8
Indefinite NP 36.4 55.3 43.9 34.7 50.7 41.2 20.3 45.4 28.1
With D-DSPT 
-based ana-
phoricity de-
termination 
Over all 52.4 79.6 63.2 58.1 80.3 67.4 50.1 79.8 61.6
Pronoun 68.6 71.5 70.1 75.2 80.4 77.7 69.1 77.8 73.5
Proper NP 81.7 89.3 85.3 82.6 90.1 86.2 78.6 88.7 83.3
Definite NP 41.8 85.9 56.2 44.9 85.2 58.8 45.2 87.9 59.7
Indefinite NP 40.3 67.6 50.5 41.2 65.1 50.5 40.9 50.1 45.1
With golden 
anaphoricity
determination 
Over all 54.6 81.7 65.5 60.4 82.1 69.6 51.9 82.1 63.6
Table 3: Performance of anaphoricity determination on coreference resolution 
NWIRE NPAPER BNEWS System 
R% P% F R% P% F R% P% F 
Without anaphoricity determina-
tion (Baseline) 53.1 67.4 59.4 57.7 67.0 62.1 48.0 65.9 55.5Zhou and 
Kong (2009) With Dynamically Extended 
Tree-based anaphoricity determi-
nation
51.6 77.2 61.8 55.2 78.6 65.2 47.5 80.3 59.6
Without anaphoricity determina-
tion (Baseline)
59.1 58. 58.6 60.8 62.6 61.7 57.7 52.6 55.0
Ng (2009) 
With Graph Minimum Cut-based 
anaphoricity determination
54.1 69.0 60.6 57.9 71.2 63.9 53.1 67.5 59.4
Table 4: Performance comparison with other systems 
Table 2 further presents the contribution of 
including syntactic and semantic dependencies 
in the D-DSPT on anaphoricity determination 
by excluding one or both of them. It shows that 
both syntactic dependencies and semantic de-
pendencies contribute significantly (***). 
Performance of coreference resolution 
We have evaluated the effect of our 
D-DSPT-based anaphoricity determination 
module on coreference resolution by including 
it as a preprocessing step to a baseline corefer-
ence resolution system without explicit ana-
phoricity determination, by filtering our those 
non-anaphoric NPs according to the anaphoric-
ity determination module. Here, the baseline 
system employs the same set of features, as 
adopted in the single-candidate model of Yang 
et al (2003) and uses a SVM-based classifier 
with the feature-based RBF kernel. Table 3 
presents the detailed performance of the 
coreference resolution system without ana-
605
phoricity determination, with D-DSPT-based 
anaphoricity determination and. with golden 
anaphoricity determination. Table 3 shows that: 
1) There is a performance gap of 6.4, 6.1 and 
7.0 in F1-measure on the NWIRE, NPAPER 
and BNEWS domain, respectively, between the 
coreference resolution system with golden 
anaphoricity determination and the baseline 
system without anaphoricity determination. 
This suggests the usefulness of proper ana-
phoricity determination in coreference resolu-
tion. This also agrees with Stoyanov et al 
(2009) which measured the impact of golden 
anaphoricity determination on coreference 
resolution using only the annotated anaphors in 
both training and testing.  
2) Compared to the baseline system without 
anaphoricity determination, the D-DSPT-based 
anaphoricity determination module improves 
the performance by 4.1(***), 3.9(***) and 
5.0(***) to 63.2, 67.4 and 61.6 in F1-measure 
on the NWIRE, NPAPER and BNEWS do-
mains, respectively, due to a large gain in pre-
cision and a much smaller drop in recall. In 
addition, D-DSPT-based anaphoricity determi-
nation can not only much improve the per-
formance of coreference resolution on pro-
nominal NPs (***) but also on definite 
NPs(***) and indefinite NPs(***) while the 
improvement on proper NPs can be ignored 
due to the fact that proper NPs can be well ad-
dressed by the simple abbreviation feature in 
the baseline system. 
3) D-DSPT-based anaphoricity determination 
still lags (2.3, 2.2 and 2.0 on the NWIRE, 
NPAPER and BNEWS domains, respectively) 
behind golden anaphoricity determination in 
improving the overall performance of corefer-
ence resolution. This suggests that there exists 
some room in the performance improvement 
for anaphoricity determination. 
Performance comparison with other systems 
Table 4 compares the performance of our sys-
tem with other systems. Here, Zhou and Kong 
(2009) use the same set of features with ours in 
the baseline system and a dynami-
cally-extended tree structure in anaphoricity 
determination. Ng (2009) uses 33 features as 
described in Ng (2007) and a graph minimum 
cut algorithm in anaphoricity determination. It 
shows that the overall performance of our 
baseline system is almost as good as that of 
Zhou and Kong (2009) and a bit better than 
Ng?s (2009).  
For overall performance, our coreference 
resolution system with D-DSPT-based ana-
phoricity determination much outperforms 
Zhou and Kong (2009) in F1-measure by 1.4, 
2.2 and 2.0 on the NWIRE, NPAPER and 
BNEWS domains, respectively, due to the bet-
ter inclusion of dependency information. De-
tailed evaluation shows that such improvement 
comes from coreference resolution on both 
pronominal and definite NPs (Please refer to 
Table 6 in Zhou and Kong, 2009). Compared 
with Zhou and Kong (2009) and Ng (2009), our 
approach achieves the best F1-measure so far 
for each dataset. 
5 Conclusion and Further Work 
This paper systematically studies a depend-
ency-driven dynamic syntactic parse tree 
(DDST) for anaphoricity determination and the 
application of an explicit anaphoricity deter-
mination module in improving learning-based 
coreference resolution. Evaluation on the ACE 
2003 corpus indicates that D-DSPT-based 
anaphoricity determination much improves the 
performance of coreference resolution. 
To our best knowledge, this paper is the first 
research which directly explores constituent 
dependencies in tree kernel-based anaphoricty 
determination from both syntactic and semantic 
perspectives. 
For further work, we will explore more 
structured syntactic information in coreference 
resolution. In addition, we will study the inter-
action between anaphoricity determination and 
coreference resolution and better integrate 
anaphoricity determination with coreference 
resolution.
Acknowledgments 
This research was supported by Projects 
60873150, 60970056, and 90920004 under the 
National Natural Science Foundation of China, 
Project 200802850006 and 20093201110006 
under the Specialized Research Fund for the 
Doctoral Program of Higher Education of 
China.
606
References 
D. Bean and E. Riloff 1999. Corpus-based Identifi-
cation of Non-Anaphoric Noun Phrases. ACL? 
1999 
S. Bergsma, D. Lin and R. Goebel 2008. Distribu-
tional Identification of Non-referential Pronouns. 
ACL?2008 
C. Cherry and S. Bergsma. 2005. An expectation 
maximization approach to pronoun resolution. 
CoNLL?2005 
M. Collins and N. Duffy. 2001. Covolution kernels 
for natural language. NIPS?2001  
M. Denber 1998. Automatic Resolution of Anapho-
ria in English. Technical Report, Eastman Ko-
dakCo. 
P. Denis and J. Baldridge. 2007. Global, joint de-
termination of anaphoricity and coreference 
resolution using integer programming. 
NAACL/HLT?2007
R. Evans 2001. Applying machine learning toward 
an automatic classification of it. Literary and 
Linguistic Computing, 16(1):45-57 
F. Kong, G.D. Zhou and Q.M. Zhu. 2009 Employ-
ing the Centering Theory in Pronoun Resolution 
from the Semantic Perspective. EMNLP?2009 
F. Kong, Y.C. Li, G.D. Zhou and Q.M. Zhu. 2009. 
Exploring Syntactic Features for Pronoun Reso-
lution Using Context-Sensitive Convolution Tree 
Kernel. IALP?2009 
S. Lappin and J. L. Herbert. 1994. An algorithm for 
pronominal anaphora resolution. Computational 
Linguistics, 20(4) 
J.H. Li. G.D. Zhou, H. Zhao, Q.M. Zhu and P.D. 
Qian. Improving nominal SRL in Chinese lan-
guage with verbal SRL information and auto-
matic predicate recognition. EMNLP '2009 
X. Luo. 2007. Coreference or not: A twin model for 
coreference resolution.  NAACL-HLT?2007 
V. Ng and C. Cardie 2002. Identify Anaphoric and 
Non-Anaphoric Noun Phrases to Improve 
Coreference Resolution. COLING?2002 
V. Ng and C. Cardie 2002. Improving machine 
learning approaches to coreference resolution. 
ACL?2002 
V. Ng 2004. Learning Noun Phrase Anaphoricity to 
Improve Coreference Resolution: Issues in Rep-
resentation and Optimization. ACL? 2004 
V. Ng 2009. Graph-cut based anaphoricity determi-
nation for coreference resolution. NAACL?2009 
L.H. Qian, G.D. Zhou, F. Kong, Q.M. Zhu and P.D. 
Qian. 2008. Exploiting constituent dependencies 
for  tree kernel-based semantic relation extrac-
tion. COLING?2008 
W. M. Soon, H. T. Ng and D. Lim  2001. A ma-
chine learning approach to coreference resolution 
of noun phrase. Computational Linguistics, 
27(4):521-544. 
V. Stoyanov, N. Gilbert, C. Cardie and E. Riloff. 
2009. Conundrums in Noun Phrase Coreference 
Resolution: Making Sense of the State-of-the Art. 
ACL?2009 
B. L. Webber. 1979. A Formal Approach to Dis-
course Anaphora. Garland Publishing, Inc. 
X.F. Yang, G.D. Zhou, J. Su and C.L. Chew. 2003. 
Coreference Resolution Using Competition 
Learning Approach. ACL?2003 
X.F. Yang, J. Su and C.L. Chew. 2005. A Twin 
Candidate Model of Coreference Resolution with 
Non-Anaphor Identification Capability. 
IJCNLP?2005 
X.F. Yang, J. Su and C.L. Chew. 2006. Ker-
nel-based pronoun resolution with structured 
syntactic knowledge. COLING-ACL?2006 
X.F. Yang, J. Su and C.L. Tan 2008. A 
Twin-Candidate Model for Learning-Based 
Anaphora Resolution. Computational Linguistics 
34(3):327-356 
M. Zhang, J. Zhang, J. Su and G.D. Zhou. 2006. A 
composite kernel to extract relations between en-
tities with both flat and structured features. 
COLING/ACL?2006 
S. Zhao and R. Grishman. 2005. Extracting relations 
with integered information using kernel methods. 
ACL?2005 
D. Zelenko, A. Chinatsu and R. Anthony. 2003. 
Kernel methods for relation extraction. Machine 
Learning Researching 3(2003):1083-1106 
G.D. Zhou, F. Kong and Q.M. Zhu. 2008. Con-
text-sensitive convolution tree kernel for pronoun 
resolution. IJCNLP?2008 
G.D. Zhou and F. Kong. 2009. Global Learning of 
Noun Phrase Anaphoricity in Coreference Reso-
lution via Label Propagetion. EMNLP?2009 
G.D. Zhou and J. Su. 2002. Named Entity recogni-
tion using a HMM-based chunk tagger. 
ACL?2002 
G.D. Zhou, M. Zhang, D.H. Ji and Q.M. Zhu. 2007. 
Tree kernel-based relation extraction with con-
text-sensitive structured parse tree information. 
EMNLP/CoNLL?2007
607
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 635?643,
Beijing, August 2010
Sentiment Classification and Polarity Shifting 
 
Shoushan Li??  Sophia Yat Mei Lee?  Ying Chen?  Chu-Ren Huang?  Guodong Zhou?  
 
?Department of CBS 
The Hong Kong Polytechnic University 
{shoushan.li, sophiaym, 
chenying3176, churenhuang} 
@gmail.com 
 
?
 Natural Language Processing Lab 
School of Computer Science and 
Technology 
      Soochow University
gdzhou@suda.edu.cn 
 
  
Abstract 
Polarity shifting marked by various 
linguistic structures has been a challenge 
to automatic sentiment classification. In 
this paper, we propose a machine learning 
approach to incorporate polarity shifting 
information into a document-level 
sentiment classification system. First, a 
feature selection method is adopted to 
automatically generate the training data 
for a binary classifier on polarity shifting 
detection of sentences. Then, by using the 
obtained binary classifier, each document 
in the original polarity classification 
training data is split into two partitions, 
polarity-shifted and polarity-unshifted, 
which are used to train two base 
classifiers respectively for further 
classifier combination. The experimental 
results across four different domains 
demonstrate the effectiveness of our 
approach. 
1 Introduction 
Sentiment classification is a special task of text 
classification whose objective is to classify a text 
according to the sentimental polarities of 
opinions it contains (Pang et al, 2002), e.g., 
favorable or unfavorable, positive or negative. 
This task has received considerable interests in 
the computational linguistic community due to its 
potential applications.  
In the literature, machine learning approaches 
have dominated the research in sentiment 
classification and achieved the state-of-the-art 
performance (e.g., Kennedy and Inkpen, 2006; 
Pang et al, 2002). In a typical machine learning 
approach, a document (text) is modeled as a 
bag-of-words, i.e. a set of content words without 
any word order or syntactic relation information. 
In other words, the underlying assumption is that 
the sentimental orientation of the whole text 
depends on the sum of the sentimental polarities 
of content words. Although this assumption is 
reasonable and has led to initial success, it is 
linguistically unsound since many function 
words and constructions can shift the 
sentimental polarities of a text. For example, in 
the sentence ?The chair is not comfortable?, the 
polarity of the word ?comfortable? is positive 
while the polarity of the whole sentence is 
reversed because of the negation word ?not?. 
Therefore, the overall sentiment of a document is 
not necessarily the sum of the content parts 
(Turney, 2002). This phenomenon is one main 
reason why machine learning approaches fail 
under some circumstances. 
As a typical case of polarity shifting, negation 
has been paid close attention and widely studied 
in the literature (Na et al, 2004; Wilson et al, 
2009; Kennedy and Inkpen, 2006). Generally, 
there are two steps to incorporate negation 
information into a system: negation detection 
and negation classification. For negation 
detection, some negation trigger words, such as 
?no?, ?not?, and ?never?, are usually applied to 
recognize negation phrases or sentences. As for 
negation classification, one way to import 
negation information is to directly reverse the 
polarity of the words which contain negation 
trigger words as far as term-counting approaches 
are considered (Kennedy and Inkpen, 2006). An 
alternative way is to add some negation features 
(e.g., negation bigrams or negation phrases) into 
635
machine learning approaches (Na et al, 2004). 
Such approaches have achieved certain success.  
There are, however, some shortcomings with 
current approaches in incorporating negation 
information. In terms of negation detection, 
firstly, the negation trigger word dictionary is 
either manually constructed or relies on existing 
resources. This leads to certain limitations 
concerning the quality and coverage of the 
dictionary. Secondly, it is difficult to adapt 
negation detection to other languages due to its 
language dependence nature of negation 
constructions and words. Thirdly, apart from 
negation, many other phenomena, e.g., contrast 
transition with trigger words like ?but?, 
?however?, and ?nevertheless?, can shift the 
sentimental polarity of a phrase or sentence. 
Therefore, considering negation alone is 
inadequate to deal with the polarity shifting 
problem, especially for document-level 
sentiment classification. 
In terms of negation classification, although it 
is easy for term-counting approaches to integrate 
negation information, they rarely outperform a 
machine learning baseline (Kennedy and Inkpen, 
2006). Even for machine learning approaches, 
although negation information is sometimes 
effective for local cases (e.g., not good), it fails 
on long-distance cases (e.g., I don?t think it is 
good). 
In this paper, we first propose a feature 
selection method to automatically generate a 
large scale polarity shifting training data for 
polarity shifting detection of sentences. Then, a 
classifier combination method is presented for 
incorporating polarity shifting information. 
Compared with previous ones, our approach 
highlights the following advantages?First of all, 
we apply a binary classifier to detect polarity 
shifting rather than merely relying on trigger 
words or phrases. This enables our approach to 
handle different kinds of polarity shifting 
phenomena. More importantly, a feature 
selection method is presented to automatically 
generate the labeled training data for polarity 
shifting detection of sentences. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work of 
sentiment classification. Section 3 presents our 
approach in details. Experimental results are 
presented and analyzed in Section 4. Finally, 
Section 5 draws the conclusion and outlines the 
future work. 
2 Related Work 
Generally, sentiment classification can be 
performed at four different levels: word level 
(Wiebe, 2000), phrase level (Wilson et al, 2009), 
sentence level (Kim and Hovy, 2004; Liu et al, 
2005), and document level (Turney, 2002; Pang 
et al, 2002; Pang and Lee, 2004; Riloff et al, 
2006). This paper focuses on document-level 
sentiment classification. 
In the literature, there are mainly two kinds of 
approaches on document-level sentiment 
classification: term-counting approaches 
(lexicon-based) and machine learning 
approaches (corpus-based). Term-counting 
approaches usually involve deriving a sentiment 
measure by calculating the total number of 
negative and positive terms (Turney, 2002; Kim 
and Hovy, 2004; Kennedy and Inkpen, 2006). 
Machine learning approaches recast the 
sentiment classification problem as a statistical 
classification task (Pang and Lee, 2004). 
Compared to term-counting approaches, 
machine learning approaches usually achieve 
much better performance (Pang et al, 2002; 
Kennedy and Inkpen, 2006), and have been 
adopted to more complicated scenarios, such as 
domain adaptation (Blitzer et al, 2007), 
multi-domain learning (Li and Zong, 2008) and 
semi-supervised learning (Wan, 2009; Dasgupta 
and Ng, 2009) for sentiment classification. 
Polarity shifting plays a crucial role in 
phrase-level, sentence-level, and document-level 
sentiment classification. However, most of 
previous studies merely focus on negation 
shifting (polarity shifting caused by the negation 
structure). As one pioneer research on sentiment 
classification, Pang et al (2002) propose a 
machine learning approach to tackle negation 
shifting by adding the tag ?not? to every word 
between a negation trigger word/phrase (e.g., not, 
isn't, didn't, etc.) and the first punctuation mark 
following the negation trigger word/phrase. To 
their disappointment, considering negation 
shifting has a negligible effect and even slightly 
harms the overall performance. Kennedy and 
Inkpen (2006) explore negation shifting by 
incorporating negation bigrams as additional 
features into machine learning approaches. The 
636
experimental results show that considering 
sentiment shifting greatly improves the 
performance of term-counting approaches but 
only slightly improves the performance of 
machine learning approaches. Other studies such 
as Na et al (2004), Ding et al (2008), and Wilson 
et al (2009) also explore negation shifting and 
achieve some improvements1. Nonetheless, as far 
as machine learning approaches are concerned, 
the improvement is rather insignificant (normally 
less than 1%). More recently, Ikeda et al (2008) 
first propose a machine learning approach to 
detect polarity shifting for sentence-level 
sentiment classification, based on a 
manually-constructed dictionary containing 
thousands of positive and negative sentimental 
words, and then adopt a term-counting approach 
to incorporate polarity shifting information. 
3 Sentiment Classification with Polarity 
Shifting Detection 
 
 
Figure 1: General framework of our approach 
 
The motivation of our approach is to improve the 
performance of sentiment classification by robust 
treatment of sentiment polarity shifting between 
sentences. With the help of a binary classifier, the 
sentences in a document are divided into two 
parts: sentences which contain polarity shifting 
structures and sentences without any polarity 
shifting structure. Figure 1 illustrates the general 
framework of our approach. Note that this 
framework is a general one, that is, different 
polarity shifting detection methods can be applied 
to differentiate polarity-shifted sentences from 
those polarity-unshifted sentences and different 
                                                      
1
 Note that Ding et al (2006) also consider but-clause, another 
important structure for sentiment shifting. Wilson et al (2009) use 
conjunctive and dependency relations among polarity words. 
polarity classification methods can be adopted to 
incorporate sentiment shifting information. For 
clarification, the training data used for polarity 
shifting detection and polarity classification are 
referred to as the polarity shifting training data 
and the polarity classification training data, 
respectively. 
3.1 Polarity Shifting Detection 
In this paper, polarity shifting means that the 
polarity of a sentence is different from the 
polarity expressed by the sum of the content 
words in the sentence. For example, in the 
sentence ?I am not disappointed?, the negation 
structure makes the polarity of the word 
'disappointed' different from that of the whole 
sentence (negative vs. positive). Apart from the 
negation structure, many other linguistic 
structures allow polarity shifting, such as 
contrast transition, modals, and 
pre-suppositional items (Polanyi and Zaenen, 
2006). We refer these structures as polarity 
shifting structures. 
One of the great challenges in building a 
polarity shifting detector lies on the lack of 
relevant training data since manually creating a 
large scale corpus of polarity shifting sentences 
is time-consuming and labor-intensive. Ikeda et 
al. (2008) propose an automatic way for 
collecting the polarity shifting training data 
based on a manually-constructed large-scale 
dictionary. Instead, we adopt a feature selection 
method to build a large scale training corpus of 
polarity shifting sentences, given only the 
already available document-level polarity 
classification training data. With the help of the 
feature selection method, the top-ranked word 
features with strong sentimental polarity 
orientation, e.g., ?great?, ?love?, ?worst? are first 
chosen as the polarity trigger words. Then, those 
sentences with the top-ranked polarity trigger 
words in both categories of positive and negative 
documents are selected. Finally, those candidate 
sentences taking opposite-polarity compared to 
the containing trigger word are deemed as 
polarity-shifted. 
The basic idea of automatically generating the 
polarity shifting training data is based on the 
assumption that the real polarity of a word or 
phrase is decided by the major polarity category 
where the word or phrase appears more often. As 
a result, the sentences in the 
Polarity Shifting 
Detector 
Documents 
 
Polarity-shifted 
Sentences 
Polarity-unshifted 
Sentences 
Polarity Classifier Positive/Negative 
637
frequently-occurring category would be seen as 
polarity-unshifted while the sentences in the 
infrequently-occurring category would be seen 
as polarity-shifted. 
In the literature, various feature selection 
methods, such as Mutual Information (MI), 
Information Gain (IG) and Bi-Normal Separation 
(BNS) (Yang and Pedersen, 1997; Forman 2003), 
have been employed to cope with the problem of 
the high-dimensional feature space which is 
normal in sentiment classification.  
In this paper, we employ the theoretical 
framework, proposed by Li et al (2009), 
including two basic measurements, i.e. frequency 
measurement and ratio measurement, where the 
first measures, the document frequency of a term 
in one category, and the second measures, the 
ratio between the document frequency in one 
category and other categories. In particular, a 
novel method called Weighed Frequency and 
Odds (WFO) is proposed to incorporate both 
basic measurements: 
1( | )( , ) ( | ) {max(0, log )}( | )
i
i i
i
P t cWFO t c P t c
P t c
? ??
=  
where ( | )iP t c  denotes the probability that a 
document x contains the term t with the 
condition that x belongs to category ic ; 
( | )iP t c  denotes the probability that a document 
x contains the term t with the condition that x 
does not belong to category ic . The left part of 
the formula ( | )iP t c  implies the first basic 
measurement and the right part 
log( ( | ) / ( | ))i iP t c P t c  implies the second one. 
The parameter ?  0 1?? ?? ?is thus to tune the 
weight between the two basic measurements. 
Especially, when ?  equals 0, the WFO method 
fades to the MI method which fully prefers the 
second basic measurement. 
Figure 2 illustrates our algorithm for 
automatically generating the polarity shifting 
training data where 1c and 2c denote the two 
sentimental orientation categories, i.e. negative 
and positive. Step A segments a document into 
sentences with punctuations. Besides, two 
special words, ?but? and ?and?, are used to 
further segment some contrast transition 
structures and compound sentences. Step B 
employs the WFO method to rank all features 
including the words. Step D extracts those 
polarity-shifted and polarity-unshifted sentences 
containing top it ?  where maxN denotes the 
upper-limit number of sentences in each 
category of the polarity shifting training data and 
#(x) denotes the total number of the elements in 
x. Apart from that, the first word in the following 
sentence is also included to capture a common 
kind of long-distance polarity shifting structure: 
contrast transition. Thus, important trigger words 
like ?however? and ?but? may be considered. 
Finally, Step E guarantees the balance between 
the two categories of the polarity shifting 
training data. 
Given the polarity shifting training data, we 
apply SVM classification algorithm to train a 
polarity-shifting detector with word unigram 
features. 
Input: 
The polarity classification training data: the negative 
sentimental document set 
1c
D and the positive sentimental 
document set
 2c
D . 
Output: 
    The polarity shifting training data: the 
polarity-unshifted sentence set unshiftS  and the polarity- 
shifted sentence set
 
shiftS . 
Procedure: 
A. Segment documents 
1c
D  and  
2c
D  to single 
sentences  
1c
S  and  
2c
S . 
B. Apply feature selection on the polarity classification  
training data and get the ranked features, 
1( ,..., ,..., )top top i top Nt t t? ? ?  
C. shiftS  = {}, unshiftS  = {} 
D. For  top it ?  in  1( ,..., ,..., )top top i top Nt t t? ? ? : 
D1) if #( shiftS )> maxN : break 
D2) Collect all sentences  
1,top i c
S
?
 and  
2,top i c
S
?
 
which contain  top it ?  from  1cS  and  2cS  
respectively 
D3)  if #(
1,top i c
S
?
)>#(
2,top i c
S
?
): 
put  
2,top i c
S
?
 into  shiftS  
put  
1,top i c
S
?
 into  unshiftS  
else: 
put  
1,top i c
S
?
 into  shiftS  
put  
2,top i c
S
?
 into  unshiftS  
E. Randomly select 
maxN sentences from unshiftS as the 
output of 
unshiftS  
 
Figure 2: The algorithm for automatically 
generating the polarity shifting training data 
 
638
3.2 Polarity Classification with Classifier 
Combination  
After polarity shifting detection, each document 
in the polarity classification training data is 
divided into two parts, one containing 
polarity-shifted sentences and the other 
containing polarity-unshifted sentences, which 
are used to form the polarity-shifted training data 
and the polarity-unshifted training data. In this 
way, two different polarity classifiers, If  and 
2f , can be trained on the polarity-shifted 
training data and the polarity-unshifted training 
data respectively. Along with classifier 3f , 
trained on all original polarity classification 
training data, we now have three base classifiers 
in hand for possible classifier combination via a 
multiple classifier system. 
The key issue in constructing a multiple 
classifier system (MCS) is to find a suitable way 
to combine the outputs of the base classifiers. In 
MCS literature, various methods are available 
for combining the outputs, such as fixed rules 
including the voting rule, the product rule and 
the sum rule (Kittler et al, 1998) and trained 
rules including the weighted sum rule (Fumera 
and Roli, 2005) and the meta-learning 
approaches (Vilalta and Drissi, 2002). In this 
study, we employ the product rule, a popular 
fixed rule, and stacking (D?eroski and ?enko, 
2004), a well-known trained rule, to combine the 
outputs. 
Formally, each base classifier provides some 
kind of confidence measurements, e.g., posterior 
probabilities of the test sample belonging to each 
class. Formally, each base classifier 
 ( 1,2,3)lf l =  assigns a test sample (denoted as 
lx ) a posterior probability vector ( )lP x

:  
1 2( ) ( | ), ( | ))tl l lP x p c x p c x= (

 
where 1( | )lp c x  denotes the probability that the 
-thl base classifier considers the sample 
belonging 1c . 
The product rule combines the base classifiers 
by multiplying the posterior possibilities and 
using the multiplied possibility for decision, i.e. 
3
1
      arg max ( | )j i l
i l
assign y c when j p c x
=
? = ?  
Stacking belongs to well-known 
meta-learning (Vilalta and Drissi, 2002). The 
key idea behind meta-learning is to train a 
meta-classifier with input attributes that are the 
outputs of the base classifiers. Hence, 
meta-learning usually needs some development 
data for generating the meta-training data. Let 
'x  denote a feature vector of a sample from the 
development data. The output of the -thl base 
classifier lf on this sample is the probability 
distribution over the category set 1 2{ , }c c , i.e. 
1 2( ' ) ( ( | ' ), ( | ' ))l l l lP x p c x p c x=

 
A meta-classifier can be trained using the 
development data with the meta-level feature 
vector 2 3metax R ??  
1 2 3( ( ' ), ( ' ), ( ' ))meta l l lx P x P x P x= = ==
  
 
Stacking is a specific meta-learning rule, in 
which a leave-one-out or a cross-validation 
procedure on the training data is applied to 
generate the meta-training data instead of using 
extra development data. In our experiments, we 
perform stacking with 10-fold cross-validation to 
generate the meta-training data. 
4 Experimentation 
4.1 Experimental Setting 
The experiments are carried out on product 
reviews from four domains: books, DVDs, 
electronics, and kitchen appliances (Blitzer et al, 
2007)2. Each domain contains 1000 positive and 
1000 negative reviews. 
For sentiment classification, all classifiers 
including the polarity shifting detector, three 
base classifiers and the meta-classifier in 
stacking are trained by SVM using the 
SVM-light tool 3  with Logistic Regression 
method for probability measuring (Platt, 1999). 
In all the experiments, each dataset is 
randomly and evenly split into two subsets: 50% 
documents as the training data and the remaining 
50% as the test data. The features include word 
unigrams and bigrams with Boolean weights. 
4.2 Experimental Results on Polarity 
Shifting Data 
To better understand the polarity shifting 
phenomena in document-level sentiment 
classification, we randomly investigate 200 
                                                      
2
 This data set is collected by Blitzer et al (2007): 
http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
3
 It is available at: http://svmlight.joachims.org/ 
639
polarity-shifted sentences, together with their 
contexts (i.e. the sentences before and after it), 
automatically generated by the WFO ( 0? = ) 
feature selection method. We find that nearly 
half of the automatically generated polarity- 
shifted sentences are actually polarity-unshifted 
sentences or difficult to decide. That is to say, 
the polarity shifting training data is noisy to 
some extent. One main reason is that some 
automatically selected trigger words do not 
really contain sentiment information, e.g., ?hear?, 
?information? etc. Another reason is that some 
reversed opinion is given in a review without 
any explicit polarity shifting structures.  
To gain more insights, we manually checked 
100 sentences which are explicitly 
polarity-shifted and can also be judged by 
human according to their contexts. Table 1 
presents some typical structures causing polarity 
shifting. It shows that the most common polarity 
shifting type is Explicit Negation (37%), usually 
expressed by trigger words such as ?not?, ?no?, or 
?without?, e.g., in the sentence ?I am not happy 
with this flashcard at all?. Another common type 
of polarity shifting is Contrast Transition (20%), 
expressed by trigger words such as ?however?, 
e.g., in the sentence ?It is large and stylish, 
however, I cannot recommend it because of the 
lid?. Other less common yet productive polarity 
shifting types include Exception and Until. 
Exception structure is usually expressed by the 
trigger phrase ?the only? to indicate the one and 
only advantage of the product, e.g., in the 
sentence ?The only thing that I like about it is 
that bamboo is a renewable resource?. Until 
structure is often expressed by the trigger word 
?until? to show the reversed polarity, e.g. in the 
sentence ?This unit was a great addition until the 
probe went bad after only a few months?. 
 
Polarity Shifting 
Structures 
Trigger 
Words/Phrases 
Distribution 
(%) 
Explicit Negation not, no, without 37 
Contrast Transition but, however, 
unfortunately 
20 
Implicit Negation avoid, hardly,  7 
False Impression look, seem 6 
Likelihood probably, perhaps 5 
Counter-factual should, would 5 
Exception the only 5 
Until until 3 
Table 1: Statistics on various polarity shifting 
structures 
4.3 Experimental Results on Polarity 
Classification 
For comparison, several classifiers with different 
classification methods are developed.  
1) Baseline classifier, which applies SVM with 
all unigrams and bigrams. Note that it also 
serves as a base classifier in the following 
combined classifiers. 
2) Base classifier 1, a base classifier for the 
classifier combination method. It works on the 
polarity-unshifted data.  
3) Base classifier 2, another base classifier for 
the classifier combination method. It works on 
the polarity-shifted data. 
4) Negation classifier, which applies SVM with 
all unigrams and bigrams plus negation bigrams. 
It is a natural extension of the baseline classifier 
with the consideration of negation bigrams. In 
this study, the negation bigrams are collected 
using some negation trigger words, such as ?not? 
and ?never?. If a negation trigger word is found 
in a sentence, each word in the sentence is 
attached with the word ?_not? to form a negation 
bigram. 
5) Product classifier, which combines the 
baseline classifier, the base classifier 1 and the 
base classifier 2 using the product rule. 
6) Stacking classifier, a combined classifier 
similar to the Product classifier. It uses the 
stacking classifier combination method instead 
of the product rule.  
Please note that we do not compare our approach 
with the one as proposed in Ikeda et al (2008) 
due to the absence of a manually-collected 
sentiment dictionary. Besides, it is well known 
that a combination strategy itself is capable of 
improving the classification performance. To 
justify whether the improvement is due to the 
combination strategy or our polarity shifting 
detection or both, we first randomly split the 
training data into two portions and train two base 
classifiers on each portion, then apply the 
stacking method to combine them along with the 
baseline classifier. The corresponding results are 
shown as ?Random+Stacking? in Table 2. Finally, 
in our experiments, t-test is performed to 
evaluate the significance of the performance 
improvement between two systems employing 
different methods (Yang and Liu, 1999). 
 
640
Domain Baseline Base  
Classifier 
1 
Base  
Classifier 
2 
Negation 
Classifier 
Random 
+ 
Stacking 
Shifting 
+ 
Product 
Shifting 
+ 
Stacking 
Book 0.755 0.756 0.670 0.759 0.764 0.772 0.785 
DVD 0.750 0.743 0.667 0.748 0.759 0.768 0.770 
Electronic 0.779 0.786 0.711 0.785 0.789 0.820 0.830 
Kitchen 0.818 0.814 0.683 0.826 0.835 0.840 0.849 
Table 2: Performance comparison of different classifiers with equally-splitting between training and test data 
 
Performance comparison of different 
classifiers 
Table 2 shows the accuracy results of different 
methods using 2000 polarity shifted sentences 
and 2000 polarity-unshifted sentences to train the 
polarity shifting detector (Nmax=2000). Compared 
to the baseline classifier, it shows that: 1) The 
base classifier 1, which only uses the 
polarity-unshifted sentences as the training data, 
achieves similar performance. 2)  The base 
classifier 2 achieves much lower performance 
due to much fewer sentences involved. 3) 
Including negation bigrams usually allows 
insignificant improvements (p-value>0.1), which 
is consistent with most of previous works (Pang 
et al, 2002; Kennedy and Inkpen, 2006). 4) Both 
the product and stacking classifiers with polarity 
shifting detection significantly improve the 
performance (p-value<0.05). Compared to the 
product rule, the stacking classifier is preferable, 
probably due to the performance unbalance 
among the individual classifiers, e.g., the 
performance of the base classifier 2 is much 
lower than the other two. Although stacking with 
two randomly generated base classifiers, i.e. 
?Random + Stacking?, also consistently 
outperforms the baseline classifier, the 
improvements are much lower than what has 
been achieved by our approach. This suggests 
that both the classifier combination strategy and 
polarity shifting detection contribute to the 
overall performance improvement. 
Effect of WFO feature selection method 
Figure 3 presents the accuracy curve of the 
stacking classifier when using different Lambda 
( ? ) values in the WFO feature selection method. 
It shows that those feature selection methods 
which prefer frequency information, e.g., MI and 
BNS, are better in automatically generating the 
polarity shifting training data. This is reasonable 
since high frequency terms, e.g., ?is?, ?it?, ?a?, 
etc., tend to obey our assumption that the real 
polarity of one top term should belong to the 
polarity category where the term appears 
frequently. 
Performance of the Stacking Classifier
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Lambda=0 0.25 0.5 0.75 1
Ac
cu
ra
cy
Book DVD Electronic Kitchen
Figure 3: Performance of the stacking classifier using 
WFO with different Lambda ( ? ) values 
 Performance of the Stacking Classifier
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
200 500 1000 1500 2000 3000 4000 6000 8000
Ac
cu
ra
cy
Book DVD Electronic Kitchen
 Figure 4: Performance of the stacking classifier over 
different sizes of the polarity shifting training data 
(with Nmax sentences in each category) 
Effect of a classifier over different sizes of the 
polarity shifting training data 
Another factor which might influence the 
overall performance is the size of the polarity 
shifting training data. Figure 4 presents the 
overall performance on different numbers of the 
polarity shifting sentences when using the 
stacking classifier. It shows that 1000 to 4000 
sentences are enough for the performance 
improvement. When the number is too large, the 
noisy training data may harm polarity shifting 
detection. When the number is too small, it is not 
enough for the automatically generated polarity 
shifting training data to capture various polarity 
shifting structures. 
641
30% 40% 50% 60% 70% 80% 90% 100%
0.6
0.65
0.7
0.75
0.8
Domain: Book
The traning data sizes
Ac
c
ur
ac
y
 
 
Baseline BaseClassifier 1 BaseClassifier 2 Stacking
30% 40% 50% 60% 70% 80% 90% 100%
0.6
0.65
0.7
0.75
0.8
Domain: DVD
The traning data sizes
Ac
cu
ra
cy
30% 40% 50% 60% 70% 80% 90% 100%
0.65
0.7
0.75
0.8
0.85
0.9
Domain: Electronic
The traning data sizes
Ac
cu
ra
cy
30% 40% 50% 60% 70% 80% 90% 100%
0.65
0.7
0.75
0.8
0.85
0.9
Domain: Kitchen
The traning data sizes
Ac
c
ur
ac
y
 
 
Figure 5: Performance of different classifiers over different sizes of the polarity classification training data 
 
Effect of different classifiers over different 
sizes of the polarity classification training data 
Figure 5 shows the classification results of 
different classifiers with varying sizes of the 
polarity classification training data. It shows that 
our approach is able to improve the overall 
performance robustly. We also notice the big 
difference between the performance of the 
baseline classifier and that of the base classifier 
1 when using 30% training data in Book domain 
and 90% training data in DVD domain. Detailed 
exploration of the polarity shifting sentences in 
the training data shows that this difference is 
mainly attributed to the poor performance of the 
polarity shifting detector. Even so, the stacking 
classifier guarantees no worse performance than 
the baseline classifier. 
5 Conclusion and Future Work 
In this paper, we propose a novel approach to 
incorporate polarity shifting information into 
document-level sentiment classification. In our 
approach, we first propose a 
machine-learning-based classifier to detect 
polarity shifting and then apply two classifier 
combination methods to perform polarity 
classification. Particularly, the polarity shifting 
training data is automatically generated through 
a feature selection method. As shown in our 
experimental results, our approach is able to 
consistently improve the overall performance 
across different domains and training data sizes, 
although the automatically generated polarity 
shifting training data is prone to noise. 
Furthermore, we conclude that those feature 
selection methods, which prefer frequency 
information, e.g., MI and BNS, are good choices 
for generating the polarity shifting training data. 
In our future work, we will explore better 
ways in generating less-noisy polarity shifting 
training data. In addition, since our approach is 
language-independent, it is readily applicable to 
sentiment classification tasks in other languages. 
For availability of the automatically generated 
polarity shifting training data, please contact the 
first author (for research purpose only). 
Acknowledgments 
This research work has been partially supported 
by Start-up Grant for Newly Appointed 
Professors, No. 1-BBZM in the Hong Kong 
Polytechnic University and two NSFC grants, 
No. 60873150 and No. 90920004. We also thank 
the three anonymous reviewers for their helpful 
comments. 
642
References 
Blitzer J., M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain Adaptation for Sentiment 
Classification. In Proceedings of ACL-07. 
Dasgupta S. and V. Ng. 2009. Mine the Easy and 
Classify the Hard: Experiments with Automatic 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Ding X., B. Liu, and P. Yu. 2008. A Holistic 
Lexicon-based Approach to Opinion Mining. In 
Proceedings of the International Conference on 
Web Search and Web Data Mining, WSDM-08. 
D?eroski S. and B. ?enko. 2004. Is Combining 
Classifiers with Stacking Better than Selecting the 
Best One? Machine Learning, vol.54(3), 
pp.255-273, 2004. 
Forman G. 2003. An Extensive Empirical Study of 
Feature Selection Metrics for Text Classification. 
The Journal of Machine Learning Research, 3(1), 
pp.1289-1305. 
Fumera G. and F. Roli. 2005. A Theoretical and 
Experimental Analysis of Linear Combiners for 
Multiple Classifier Systems. IEEE Trans. PAMI, 
vol.27, pp.942?956, 2005 
Ikeda D., H. Takamura, L. Ratinov, and M. Okumura. 
2008. Learning to Shift the Polarity of Words for 
Sentiment Classification. In Proceedings of 
IJCNLP-08. 
Kennedy, A. and D. Inkpen. 2006. Sentiment 
Classification of Movie Reviews using Contextual 
Valence Shifters. Computational Intelligence, 
vol.22(2), pp.110-125, 2006. 
Kim S. and E. Hovy. 2004. Determining the 
Sentiment of Opinions. In Proceedings of 
COLING-04. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Li S., R. Xia, C. Zong, and C. Huang. 2009. A 
Framework of Feature Selection Methods for Text 
Categorization. In Proceedings of 
ACL-IJCNLP-09. 
Li S. and C. Zong. 2008. Multi-domain Sentiment 
Classification. In Proceedings of ACL-08: HLT, 
short paper. 
Liu B., M. Hu, and J. Cheng. 2005. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
In Proceedings of WWW-05. 
Na J., H. Sui, C. Khoo, S. Chan, and Y. Zhou. 2004. 
Effectiveness of Simple Linguistic Processing in 
Automatic Sentiment Classification of Product 
Reviews. In Conference of the International 
Society for Knowledge Organization (ISKO-04). 
Pang B. and L. Lee. 2004. A Sentimental Education: 
Sentiment Analysis using Subjectivity 
Summarization based on Minimum Cuts. In 
Proceedings of ACL-04. 
Pang B., L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of 
EMNLP-02. 
Platt J. 1999. Probabilistic Outputs for Support 
Vector Machines and Comparisons to Regularized 
Likelihood Methods. In: A. Smola, P. Bartlett, B. 
Schoelkopf and D. Schuurmans (Eds.): Advances 
in Large Margin Classiers. MIT Press, Cambridge, 
61?74. 
Polanyi L. and A. Zaenen. 2006. Contextual Valence 
Shifters. Computing attitude and affect in text: 
Theory and application. Springer Verlag. 
Riloff E., S. Patwardhan, and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In 
Proceedings of EMNLP-06. 
Turney P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. In Proceedings of 
ACL-02. 
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial 
Intelligence Review, 18(2), pp. 77?95. 
Wan X. 2009. Co-Training for Cross-Lingual 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Wiebe J. 2000. Learning Subjective Adjectives from 
Corpora. In Proceedings of AAAI-2000. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433, 
2009. 
Yang Y. and X. Liu, X. 1999. A Re-Examination of 
Text Categorization methods. In Proceedings of 
SIGIR-99. 
Yang Y. and J. Pedersen. 1997. A Comparative Study 
on Feature Selection in Text Categorization. In 
Proceedings of ICML-97. 
643
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 671?679,
Beijing, August 2010
Learning the Scope of Negation via Shallow Semantic Parsing 
Junhui Li  Guodong Zhou?  Hongling Wang  Qiaoming Zhu 
School of Computer Science and Technology 
        Soochow University at Suzhou 
{lijunhui, gdzhou, redleaf, qmzhu}@suda.edu.cn 
 
                                                          
? Corresponding author 
Abstract 
In this paper we present a simplified shallow 
semantic parsing approach to learning the 
scope of negation (SoN). This is done by 
formulating it as a shallow semantic parsing 
problem with the negation signal as the 
predicate and the negation scope as its ar-
guments. Our parsing approach to SoN 
learning differs from the state-of-the-art 
chunking ones in two aspects. First, we ex-
tend SoN learning from the chunking level 
to the parse tree level, where structured syn-
tactic information is available. Second, we 
focus on determining whether a constituent, 
rather than a word, is negated or not, via a 
simplified shallow semantic parsing frame-
work. Evaluation on the BioScope corpus 
shows that structured syntactic information 
is effective in capturing the domination rela-
tionship between a negation signal and its 
dominated arguments. It also shows that our 
parsing approach much outperforms the 
state-of-the-art chunking ones. 
1 Introduction 
Whereas negation in predicate logic is 
well-defined and syntactically simple, negation 
in natural language is much complex. Gener-
ally, learning the scope of negation involves 
two subtasks: negation signal finding and nega-
tion scope finding. The former decides whether 
the words in a sentence are negation signals 
(i.e., words indicating negation, e.g., no, not, 
fail, rather than), where the semantic informa-
tion of the words, rather than the syntactic in-
formation, plays a critical role. The latter de-
termines the sequences of words in the sen-
tence which are negated by the given negation 
signal. Compared with negation scope finding, 
negation signal finding is much simpler and has 
been well resolved in the literature, e.g. with 
the accuracy of 95.8%-98.7% on the three 
subcorpora of the Bioscope corpus (Morante 
and Daelemans, 2009). In this paper, we focus 
on negation scope finding instead. That is, we 
assume golden negation signal finding. 
Finding negative assertions is essential in 
information extraction (IE), where in general, 
the aim is to derive factual knowledge from 
free text. For example, Vincze et al (2008) 
pointed out that the extracted information 
within the scopes of negation signals should 
either be discarded or presented separately 
from factual information. This is especially 
important in the biomedical domain, where 
various linguistic forms are used extensively to 
express impressions, hypothesized explanations 
of experimental results or negative findings. 
Szarvas et al (2008) reported that 13.45% of 
the sentences in the abstracts subcorpus of the 
BioScope corpus and 12.70% of the sentences 
in the full papers subcorpus of the Bioscope 
corpus contain negative assertions. In addition 
to the IE tasks in the biomedical domain, SoN 
learning has attracted more and more attention 
in some natural language processing (NLP) 
tasks, such as sentiment classification (Turney, 
2002). For example, in the sentence ?The chair 
is not comfortable but cheap?, although both 
the polarities of the words ?comfortable? and 
?cheap? are positive, the polarity of ?the chair? 
regarding the attribute ?cheap? keeps positive 
while the polarity of ?the chair? regarding the 
attribute ?comfortable? is reversed due to the 
negation signal ?not?.  
Most of the initial research on SoN learning 
focused on negated terms finding, using either 
some heuristic rules (e.g., regular expression), 
or machine learning methods (Chapman et al, 
2001; Huang and Lowe, 2007; Goldin and 
Chapman, 2003). Negation scope finding has 
been largely ignored until the recent release of 
671
the BioScope corpus (Szarvas et al, 2008; 
Vincze et al, 2008). Morante et al (2008) and 
Morante and Daelemans (2009) pioneered the 
research on negation scope finding by formu-
lating it as a chunking problem, which classi-
fies the words of a sentence as being inside or 
outside the scope of a negation signal. How-
ever, this chunking approach suffers from low 
performance, in particular on long sentences, 
due to ignoring structured syntactic information. 
For example, given golden negation signals on 
the Bioscope corpus, Morante and Daelemans 
(2009) only got the performance of 50.26% in 
PCS (percentage of correct scope) measure on 
the full papers subcorpus (22.8 words per sen-
tence on average), compared to 87.27% in PCS 
measure on the clinical reports subcorpus (6.6 
words per sentence on average). 
This paper explores negation scope finding 
from a parse tree perspective and formulates it 
as a shallow semantic parsing problem, which 
has been extensively studied in the past few 
years (Carreras and M?rquez, 2005). In par-
ticular, the negation signal is recast as the pre-
dicate and the negation scope is recast as its 
arguments. The motivation behind is that 
structured syntactic information plays a critical 
role in negation scope finding and should be 
paid much more attention, as indicated by pre-
vious studies in shallow semantic parsing 
(Gildea and Palmer, 2002; Punyakanok et al, 
2005). Our parsing approach to negation scope 
finding differs from the state-of-the-art chunk-
ing ones in two aspects. First, we extend nega-
tion scope finding from the chunking level into 
the parse tree level, where structured syntactic 
information is available. Second, we focus on 
determining whether a constituent, rather than a 
word, is negated or not. Evaluation on the 
BioScope corpus shows that our parsing ap-
proach much outperforms the state-of-the-art 
chunking ones. 
The rest of this paper is organized as follows. 
Section 2 reviews related work. Section 3 in-
troduces the Bioscope corpus on which our 
approach is evaluated. Section 4 describes our 
parsing approach by formulating negation 
scope finding as a simplified shallow semantic 
parsing problem. Section 5 presents the ex-
perimental results. Finally, Section 6 concludes 
the work. 
2 Related Work 
While there is a certain amount of literature 
within the NLP community on negated terms 
finding (Chapman et al, 2001; Huang and 
Lowe, 2007; Goldin and Chapman, 2003), 
there are only a few studies on negation scope 
finding (Morante et al, 2008; Morante and 
Daelemans, 2009).  
Negated terms finding  
Rule-based methods dominated the initial re-
search on negated terms finding. As a repre-
sentative, Chapman et al (2001) developed a 
simple regular expression-based algorithm to 
detect negation signals and identify medical 
terms which fall within the negation scope. 
They found that their simple regular expres-
sion-based algorithm can effectively identify a 
large portion of the pertinent negative state-
ments from discharge summaries on determin-
ing whether a finding or disease is absent. Be-
sides, Huang and Lowe (2007) first proposed 
some heuristic rules from a parse tree perspec-
tive to identify negation signals, taking advan-
tage of syntactic parsing, and then located ne-
gated terms in the parse tree using a corre-
sponding negation grammar. 
As an alternative to the rule-based methods, 
various machine learning methods have been 
proposed for finding negated terms. As a rep-
resentative, Goldin and Chapman (2003) a-
dopted both Na?ve Bayes and decision trees to 
distinguish whether an observation is negated 
by the negation signal ?not? in hospital reports.  
Negation scope finding  
Morante et al (2008) pioneered the research on 
negation scope finding, largely due to the 
availability of a large-scale annotated corpus, 
the Bioscope corpus. They approached the ne-
gation scope finding task as a chunking prob-
lem which predicts whether a word in the sen-
tence is inside or outside of the negation scope, 
with proper post-processing to ensure consecu-
tiveness of the negation scope. Morante and 
Daelemans (2009) further improved the per-
formance by combing several classifiers.  
Similar to SoN learning, there are some ef-
forts in the NLP community on learning the 
scope of speculation. As a representative, 
?zg?r and Radev (2009) divided speculation 
672
learning into two subtasks: speculation signal 
finding and speculation scope finding. In par-
ticular, they formulated speculation signal 
finding as a classification problem while em-
ploying some heuristic rules from the parse tree 
perspective on speculation scope finding. 
3 Negation in the BioScope Corpus 
This paper employs the BioScope corpus 
(Szarvas et al, 2008; Vincze et al, 2008)1, a 
freely downloadable negation resource from 
the biomedical domain, as the benchmark cor-
pus. In this corpus, every sentence is annotated 
with negation signals and speculation signals 
(if it has), as well as their linguistic scopes. 
Figure 1 shows a self-explainable example. In 
this paper, we only consider negation signals, 
rather than speculation ones. Our statistics 
shows that 96.57%, 3.23% and 0.20% of nega-
tion signals are represented by one word, two 
words and three or more words, respectively. 
Additional, adverbs (e.g., not, never) and de-
terminers (e.g., no, neither) occupy 45.66% and 
30.99% of negation signals, respectively. 
 
The Bioscope corpus consists of three sub-
corpora: the full papers and the abstracts from 
the GENIA corpus (Collier et al, 1999), and 
clinical (radiology) reports. Among them, the 
full papers subcorpus and the abstracts subcor-
pus come from the same genre, and thus share 
some common characteristics in statistics, such 
as the number of words in the negation scope to 
the right (or left) of the negation signal and the 
average scope length. In comparison, the clini-
cal reports subcorpus consists of clinical radi-
ology reports with short sentences. For detailed 
statistics about the three subcorpora, please see 
Morante and Daelemans (2009). 
                                                          
                                                          
1 http://www.inf.u-szeged.hu/rgai/bioscope 
For preprocessing, all the sentences in the 
Bioscope corpus are tokenized and then parsed 
using the Berkeley parser2 (Petrov and Klein, 
2007) trained on the GENIA TreeBank (GTB) 
1.0 (Tateisi et al, 2005)3, which is a bracketed 
corpus in (almost) PTB style. 10-fold 
cross-validation on GTB1.0 shows that the 
parser achieves the performance of 86.57 in 
F1-measure. It is worth noting that the GTB1.0 
corpus includes all the sentences in the ab-
stracts subcorpus of the Bioscope corpus. 
4 Negation Scope Finding via Shallow 
Semantic Parsing 
In this section, we first formulate the negation 
scope finding task as a shallow semantic pars-
ing problem. Then, we deal with it using a sim-
plified shallow semantic parsing framework.  
4.1 Formulating Negation Scope Finding  
as a Shallow Semantic Parsing Prob-
lem 
Given a parse tree and a predicate in it, shallow 
semantic parsing recognizes and maps all the 
constituents in the sentence into their corre-
sponding semantic arguments (roles) of the 
predicate. As far as negation scope finding 
considered, the negation signal can be regarded 
as the predicate4, while the scope of the nega-
tion signal can be mapped into several con-
stituents which are negated and thus can be 
regarded as the arguments of the negation sig-
nal. In particular, given a negation signal and 
its negation scope which covers wordm, ?, 
wordn, we adopt the following two heuristic 
rules to map the negation scope of the negation 
signal into several constituents which can be 
deemed as its arguments in the given parse tree. 
<sentence id="S26.8">These findings <xcope 
id="X26.8.2"><cue type="speculation" 
ref="X26.8.2">indicate that</cue> <xcope 
id="X26.8.1">corticosteroid resistance in bron-
chial asthma <cue type="negation" 
ref="X26.8.1">can not</cue> be explained by 
abnormalities in corticosteroid receptor charac-
teristics</xcope></xcope>.</sentence> 
Figure 1: An annotated sentence in the BioScope 
corpus. 
1) The negation signal itself and all of its an-
cestral constituents are non-arguments. 
2) If constituent X is an argument of the given 
negation signal, then X should be the high-
est constituent dominated by the scope of 
wordm, ?, wordn. That is to say, X?s parent 
constituent must cross-bracket or include 
the scope of wordm, ?, wordn. 
2 http://code.google.com/p/berkeleyparser/ 
3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA  
4 If a negation signal consists of multiply words 
(e.g., rather than), the last word (e.g., than) is cho-
sen to represent the negation signal. 
673
 Figure 2: An illustration of a negation signal and its arguments in a parse tree. 
These findings 
indicates 
that 
corticosteroid resistance
NP0,1
VBP2,2 SBAR3,11
can not
IN3,3
be
explained by abnormalities 
NP4,5
MD6,6 RB7,7
VB8,8 VP9,11
VP8,11
VP6,11
S4,11
VP2,11
S0,11
predicate
arguments
 
The first rule ensures that no argument cov-
ers the negation signal while the second rule 
ensures no overlap between any two arguments. 
For example, in the sentence ?These findings 
indicate that corticosteroid resistance can not 
be explained by abnormalities?, the negation 
signal ?can not? has the negation scope ?corti-
costeroid resistance can not be explained by 
abnormalities?. As shown in Figure 2, the node 
?RB7,7? (i.e., not) represents the negation signal 
?can not? while its arguments include three 
constituents {NP4,5, MD6,6, and VP8,11}. It is 
worth noting that according to the above rules, 
negation scope finding via shallow semantic 
parsing, i.e. determining the arguments of a 
given negation signal, is robust to some varia-
tions in parse trees. This is also empirically 
justified by our later experiments. For example, 
if the VP6,11 in Figure 2 is incorrectly expanded 
by the rule VP6,11?MD6,6+RB7,7+VB8,8+VP9,11, 
the negation scope of the negation signal ?can 
not? can still be correctly detected as long as 
{NP4,5, MD6,6, VB8,8, and VP9,11} are predicted 
as the arguments of the negation signal ?can 
not?. 
Compared with common shallow semantic 
parsing which needs to assign an argument 
with a semantic label, negation scope finding 
does not involve semantic label classification 
and thus could be divided into three consequent 
phases: argument pruning, argument identifica-
tion and post-processing. 
4.2 Argument Pruning 
Similar to the predicate-argument structures in 
common shallow semantic parsing, the nega-
tion signal-scope structures in negation scope 
finding can be also classified into several cer-
tain types and argument pruning can be done 
by employing several heuristic rules to filter 
out constituents, which are most likely 
non-arguments of a negation signal. Similar to 
the heuristic algorithm as proposed in Xue and 
Palmer (2004) for argument pruning in com-
mon shallow semantic parsing, the argument 
pruning algorithm adopted here starts from 
designating the negation signal as the current 
node and collects its siblings. It then iteratively 
moves one level up to the parent of the current 
node and collects its siblings. The algorithm 
ends when it reaches the root of the parse tree. 
To sum up, except the negation signal and its 
ancestral constituents, any constituent in the 
parse tree whose parent covers the given nega-
tion signal will be collected as argument can-
didates. Taking the negation signal node 
?RB7,7? in Figure 2 as an example, constituents 
{MD6,6, VP8,11, NP4,5, IN3,3, VBP2,2, and NP0,1} 
are collected as its argument candidates conse-
quently. 
4.3 Argument Identification 
Here, a binary classifier is applied to determine 
the argument candidates as either valid argu-
ments or non-arguments. Similar to argument 
674
identification in common shallow semantic 
parsing, the structured syntactic information 
plays a critical role in negation scope finding.  
Basic Features 
Table 1 lists the basic features for argument 
identification. These features are also widely 
used in common shallow semantic parsing for 
both verbal and nominal predicates (Xue, 2008; 
Li et al, 2009). 
Feature Remarks 
b1 Negation: the stem of the negation signal, 
e.g., not, rather_than. (can_not) 
b2 Phrase Type: the syntactic category of the
argument candidate. (NP) 
b3 Path: the syntactic path from the argument 
candidate to the negation signal. 
(NP<S>VP>RB) 
b4 Position: the positional relationship of the
argument candidate with the negation sig-
nal. ?left? or ?right?. (left) 
Table 1: Basic features and their instantiations for 
argument identification in negation scope finding, 
with NP4,5 as the focus constituent (i.e., the argu-
ment candidate) and ?can not? as the given negation 
signal, regarding Figure 2. 
Additional Features 
To capture more useful information in the ne-
gation signal-scope structures, we also explore 
various kinds of additional features. Table 2 
shows the features in better capturing the de-
tails regarding the argument candidate and the 
negation signal. In particular, we categorize the 
additional features into three groups according 
to their relationship with the argument candi-
date (AC, in short) and the given negation sig-
nal (NS, in short). 
Some features proposed above may not be 
effective in argument identification. Therefore, 
we adopt the greedy feature selection algorithm 
as described in Jiang and Ng (2006) to pick up 
positive features incrementally according to 
their contributions on the development data. 
The algorithm repeatedly selects one feature 
each time which contributes most, and stops 
when adding any of the remaining features fails 
to improve the performance. As far as the ne-
gation scope finding task concerned, the whole 
feature selection process could be done by first 
running the selection algorithm with the basic 
features (b1-b4) and then incrementally picking 
up effective features from (ac1-ac6, AC1-AC2, 
ns1-ns4, NS1-NS2, nsac1-nsac2, and NSAC1 
-NSAC7). 
Feature Remarks 
argument candidate (AC) related 
ac1 the headword (ac1H) and its POS (ac1P). 
(resistance, NN) 
ac2 the left word (ac2W) and its POS (ac2P). 
(that, IN) 
ac3 the right word (ac3W) and its POS (ac3P). 
(can, MD) 
ac4 the phrase type of its left sibling (ac4L) 
and its right sibling (ac4R). (NULL, VP) 
ac5 the phrase type of its parent node. (S) 
ac6 the subcategory. (S:NP+VP) 
combined features (AC1-AC2) 
b2&fc1H, b2&fc1P 
negation signal (NS) related 
ns1 its POS. (RB) 
ns2 its left word (ns2L) and right word (ns2R). 
(can, be) 
ns3 the subcategory. (VP:MD+RB+VP) 
ns4 the phrase type of its parent node. (VP) 
combined features (NS1-NS2) 
b1&ns2L, b1&ns2R 
NS-AC-related 
nsac1 the compressed path of b3: compressing 
sequences of identical labels into one.  
(NP<S>VP>RB) 
nsac2 whether AC and NS are adjacent in posi-
tion. ?yes? or ?no?. (no) 
combined features (NSAC1-NSAC7) 
b1&b2, b1&b3, b1&nsac1, b3&NS1, b3&NS2, 
b4&NS1, b4&NS2 
Table 2: Additional features and their instantiations 
for argument identification in negation scope find-
ing, with NP4,5 as the focus constituent (i.e., the 
argument candidate) and ?can not? as the given 
negation signal, regarding Figure 2. 
4.4 Post-Processing 
Although a negation signal in the BioScope 
corpus always has only one continuous block 
as its negation scope (including the negation 
signal itself), the negation scope finder may 
result in discontinuous negation scope due to 
independent prediction in the argument identi-
fication phase. Given the golden negation sig-
nals, we observed that 6.2% of the negation 
scopes predicted by our negation scope finder 
are discontinuous.  
Figure 3 demonstrates the projection of all 
the argument candidates into the word level. 
According to our argument pruning algorithm 
in Section 4.2, except the words presented by 
675
the negation signal, the projection covers the 
whole sentence and each constituent (LACi or 
RACj in Figure 3) receives a probability distri-
bution of being an argument of the given nega-
tion signal in the argument identification phase. 
 Since a negation signal is deemed inside of its 
negation scope in the BioScope corpus, our 
post-processing algorithm first includes the 
negation signal in its scope and then starts to 
identify the left and the right scope boundaries, 
respectively. 
As shown in Figure 3, the left boundary has 
m+1 possibilities, namely the negation signal 
itself, the leftmost word of constituent LACi 
(1<=i<=m). Supposing LACi receives prob-
ability of Pi being an argument, we use the fol-
lowing formula to determine LACk* whose 
leftmost word represents the boundary of the 
left scope. If k*=0, then the negation signal 
itself represents its left boundary. 
( )*
1 1
arg max 1
k m
i i
k i i k
k P
= = +
= ?? ? P?
                                                          
 
Similarly, the right boundary of the given 
negation signal can be decided. 
5 Experimentation 
We have evaluated our shallow semantic pars-
ing approach to negation scope finding on the 
BioScope corpus. 
5.1 Experimental Settings 
Following the experimental setting in Morante 
and Daelemans (2009), the abstracts subcorpus 
is randomly divided into 10 folds so as to per-
form 10-fold cross validation, while the per-
formance on both the papers and clinical re-
ports subcorpora is evaluated using the system 
trained on the whole abstracts subcorpus. In 
addition, SVMLight5 is selected as our classi-
fier. In particular, we adopt the linear kernel 
and the training parameter C is fine-tuned to 
0.2. 
1
5 http://svmlight.joachims.org/ 
The evaluation is made using the accuracy. 
We report the accuracy using three measures: 
PCLB and PCRB, which indicate the percent-
ages of correct left boundary and right bound-
ary respectively, PCS, which indicates the per-
centage of correct scope as a whole.  
LACm   ?.   LAC1 RAC1   ?.   RACn
m n 
Figure 3: Projecting the left and the right argument 
candidates into the word level. 
5.2 Experimental Results on Golden Parse 
Trees 
In order to select beneficial features from the 
additional features proposed in Section 4.3, we 
randomly split the abstracts subcorpus into 
training and development datasets with propor-
tion of 4:1. After performing the greedy feature 
selection algorithm on the development data, 
features {NSAC5, ns2R, NS1, ac1P, ns3, 
NSAC7, ac4R} are selected consecutively for 
argument identification. Table 3 presents the 
effect of selected features in an incremental 
way on the development data. It shows that the 
additional features significantly improve the 
performance by 11.66% in PCS measure from 
74.93% to 86.59% ( ). 2; 0.0p? <
 
Feature PCLB PCRB PCS 
Baseline 84.26 88.92 74.93 
+NSAC5 90.96 88.92 81.34 
+ns2R 91.55 88.92 81.92 
+NS1 92.42 89.50 83.09 
+ac1P 93.59 89.50 84.26 
+ns3 93.88 90.09 84.84 
+NSAC7 94.75 89.80 85.42 
+ac4R 95.04 90.67 86.59 
Table 3: Performance improvement (%) of includ-
ing the additional features in an incremental way on 
the development data (of the abstracts subcorpus). 
However, Table 3 shows that the additional 
features behave quite differently in terms of 
PCLB and PCRB measures. For example, 
PCLB measure benefits more from features 
NSAC5, ns2R, NS1, ac1P, and NSAC7 while 
PCRB measure benefits more from features 
NS1 and ac4R. It also shows that the features 
(e.g., NSAC5, ns2R, NS1, NSAC7) related to 
neighboring words of the negation signal play a 
critical role in recognizing both left and right 
boundaries. This may be due to the fact that 
neighboring words usually imply sentential 
information. For example, ?can not be? indi-
cates a passive clause while ?did not? indicates 
an active clause. Table 3 also shows that the 
recognition of left boundaries is much easier 
than that of right boundaries. This may be due 
676
to the fact that 83.6% of negation signals have 
themselves as the left boundaries in the ab-
stracts subcorpus.  
gument candidate is outside or cross-brackets 
with the golden negation scope, then it is a 
non-argument. The oracle performance is pre-
sented in the rows of oracle in Table 5 and Ta-
ble 6. 
Table 4 presents the performance on the ab-
stracts subcorpus by performing 10-fold 
cross-validation. It shows that the additional 
features significantly improve the performance 
over the three measures ( ). 2; 0.0p? <
Table 5 and Table 6 show that: 
1) Automatic syntactic parsing lowers the per-
formance of negation scope finding on the 
abstracts subcorpus in all three measures (e.g. 
from 83.10 to 81.84 in PCS). As expected, 
the parser trained on the whole GTB1.0 
corpus works better than that trained on 
6,691 sentences (e.g. 64.02 Vs. 62.70, and 
89.79 Vs. 85.21 in PCS measure on the full 
papers and the clinical reports subcorpora, 
respectively). However, the performance de-
crease shows that negation scope finding is 
not as sensitive to automatic syntactic pars-
ing as common shallow semantic parsing, 
whose performance might decrease by about 
~10 in F1-measure (Toutanova et al, 2005). 
This indicates that negation scope finding 
via shallow semantic parsing is robust to 
some variations in the parse trees. 
1
Feature PCLB PCRB PCS 
Baseline 84.29 87.82 74.05 
+selected features 93.06 88.96 83.10 
Table 4: Performance (%) of negation scope finding 
on the abstracts subcorpus using 10-fold 
cross-validation.  
5.3 Experimental Results on Automatic 
Parse Trees 
The GTB1.0 corpus contains 18,541 sentences 
in which 11,850 of them (63.91%) overlap with 
the sentences in the abstracts subcorpus6. In 
order to get automatic parse trees for the sen-
tences in the abstracts subcorpus, we train the 
Berkeley parser with the remaining 6,691 sen-
tences in GTB1.0. The Berkeley parser trained 
on 6,691 sentences achieves the performance of 
85.22 in F1-measure on the other sentences in 
GTB1.0. For both the full papers and clinical 
reports subcorpora, we get their automatic 
parse trees by using two Berkeley parsers: one 
trained on 6,691 sentences in GBT1.0, and the 
other trained on all the sentences in GTB1.0.  
2) autoparse(test) consistently outperforms 
autoparse(t&t) on both the abstracts and the 
full papers subcorpora. However, it is sur-
prising to find that autoparse(t&t) achieves 
better performance on the clinical reports 
subcorpus than autoparse(test). This may be 
due to the special characteristics of the 
clinical reports subcorpus, which mainly 
consists of much shorter sentences with 6.6 
words per sentence on average, and better 
adaptation of the argument identification 
classifier to the variations in the automatic 
parse trees. 
To test the performance on automatic parse 
trees, we employ two different configurations. 
First, we train the argument identification clas-
sifier on the abstracts subcorpus using auto-
matic parse trees produced by Berkeley parser 
trained on 6,691 sentences. The experimental 
results are presented in the rows of auto-
parse(t&t) in Table 5 and Table 6. Then, we 
train the argument identification classifier on 
the abstracts subcorpus using golden parse 
trees. The experimental results are presented in 
the rows of autoparse(test) in Table 5 and Ta-
ble 6.  
3) The performance on all three subcorpora 
indicates that the recognition of right 
boundary is much harder than that of left 
boundary. This may be due to the longer 
right boundary on an average. Our statistics 
shows that the average left/right boundaries 
are 1.1/6.9, 0.1/3.7, and 1.2/6.5 words on the 
abstracts, the full papers and the clinical re-
ports subcorpora, respectively. 
We also report an oracle performance to ex-
plore the best possible performance of our sys-
tem by assuming that our negation scope finder 
can always correctly determine whether a can-
didate is an argument or not. That is, if an ar-
4) The oracle performance is less sensitive to 
automatic syntactic parsing. In addition, 
given the performance gap between the per-
formance of our negation scope finder and 
the oracle performance, there is still much 
room for further performance improvement. 
                                                          
6 There are a few cases where two sentences in the 
abstracts subcorpus map into one sentence in GTB. 
677
 Abstracts Papers Clinical 
 PCLB PCRB PCS PCLB PCRB PCS PCLB PCRB PCS 
autoparse(t&t) 91.97 87.82 80.88 85.45 67.20 59.26 97.48 88.30 85.89
autoparse(test) 92.71 88.33 81.84 87.57 68.78 62.70 97.48 87.73 85.21
oracle 99.72 94.59 94.37 98.94 84.13 83.33 99.89 98.39 98.39
Table 5: Performance (%) of negation scope finding on the three subcorpora by using automatic parser trained 
with 6,691 sentences in GTB1.0.  
 Papers Clinical 
 PCLB PCRB PCS PCLB PCRB PCS 
autoparse(t&t) 85.98 67.99 60.32 97.48 92.66 90.48 
autoparse(test) 87.83 70.11 64.02 97.36 92.20 89.79 
oracle 98.94 83.86 83.07 99.77 97.94 97.82 
Table 6: Performance (%) of negation scope finding on the two subcorpora by using automatic parser trained 
with all the sentences in GTB1.0.  
 
Method Abstracts Papers Clinical 
M et al (2008) 57.33 n/a n/a 
M & D (2009) 73.36 50.26 87.27 
Our baseline 73.42 53.70 88.42 
Our final system 81.84 64.02 89.79 
Table 7: Performance comparison over the PCS 
measure (%) of our system with other 
state-of-the-art ones.  
Table 7 compares our performance in PCS 
measure with related work. It shows that even 
our baseline system with four basic features as 
presented in Table 1 performs better than 
Morante et al (2008) and Morante and Daele-
mans(2009). This indicates the appropriateness 
of our simplified shallow semantic parsing ap-
proach and the effectiveness of structured syn-
tactic information on negation scope finding. It 
also shows that our final system significantly 
outperforms the state-of-the-art ones using a 
chunking approach, especially on the abstracts 
and full papers subcorpora. However, the im-
provement on the clinical reports subcorpus is 
less apparent, partly due to the fact that the 
sentences in this subcorpus are much simpler 
(with average length of 6.6 words per sentence) 
and thus a chunking approach can achieve high 
performance. Following are two typical sen-
tences from the clinical reports subcorpus, 
where the negation scope covers the whole sen-
tence (except the period punctuation). Such 
sentences account for 57% of negation sen-
tences in the clinical reports subcorpus. 
 
6 Conclusion 
In this paper we have presented a simplified 
shallow semantic parsing approach to negation 
scope finding by formulating it as a shallow 
semantic parsing problem, which has been ex-
tensively studied in the past few years. In par-
ticular, we regard the negation signal as the 
predicate while mapping the negation scope 
into several constituents which are deemed as 
arguments of the negation signal. Evaluation on 
the Bioscope corpus shows the appropriateness 
of our shallow semantic parsing approach and 
that structured syntactic information plays a 
critical role in capturing the domination rela-
tionship between a negation signal and its ne-
gation scope. It also shows that our parsing 
approach much outperforms the state-of-the-art 
chunking ones. To our best knowledge, this is 
the first research on exploring negation scope 
finding via shallow semantic parsing. 
Future research will focus on joint learning 
of negation signal and its negation scope find-
ings. Although Morante and Daelemans (2009) 
reported the performance of 95.8%-98.7% on 
negation signal finding, it lowers the perform-
ance of negation scope finding by about 
7.29%-16.52% in PCS measure.  
Acknowledgments 
This research was supported by Projects 
60683150, 60970056, and 90920004 under the 
National Natural Science Foundation of China, 
Project 20093201110006 under the Specialized 
Research Fund for the Doctoral Program of 
Higher Education of China. 
(1) No evidence of focal pneumonia . 
 
(2) No findings to account for symptoms . 
678
References 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2005.  
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 
2001. A Simple Algorithm for Identifying Ne-
gated Findings and Diseases in Discharge Sum-
maries. Journal of Biomedical Informatics, 34: 
301-310. 
Nigel Collier, Hyun Seok Park, Norihiro Ogata, et 
al. 1999. The GENIA project: corpus-based 
knowledge acquisition and information extrac-
tion from genome research papers. In Proceed-
ings of EACL 1999.  
Daniel Gildea and Martha Palmer. 2002. The Ne-
cessity of Parsing for Predicate Argument Rec-
ognition. In Proceedings of ACL 2002. 
Ilya M. Goldin and Wendy W. Chapman. 2003. 
Learning to Detect Negation with ?Not? in Medi-
cal Texts. In Proceedings of SIGIR 2003. 
Yang Huang and Henry Lowe. 2007. A Novel Hy-
brid Approach to Automated Negation Detection 
in Clinical Radiology Reports. Journal of the 
American Medical Informatics Association, 14(3): 
304-311. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Seman-
tic Role Labeling of NomBank: A Maximum En-
tropy Approach. In Proceedings of EMNLP 
2006. 
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu, 
and Peide Qian. Improving Nominal SRL in 
Chinese Language with Verbal SRL Information 
and Automatic Predicate Recognition. In Pro-
ceedings of EMNLP 2009. 
Roser Morante, Anthony Liekens, and Walter 
Daelemans. 2008. Learning the Scope of Nega-
tion in Biomedical Texts. In Proceedings of 
EMNLP 2008. 
Roser Morante and Walter Daelemans. 2009. A 
Metalearning Approach to Processing the Scope 
of Negation. In Proceedings of CoNLL 2009. 
Arzucan ?zg?r; Dragomir R. Radev. 2009. Detect-
ing Speculations and their Scopes in Scientific 
Text. In Proceedings of EMNLP 2009. 
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceedings of 
NAACL 2007. 
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 
2005. The Necessity of Syntactic Parsing for 
Semantic Role Labeling. In Proceedings of IJCAI 
2005. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, 
and J?nos Csirik. 2008. The BioScope corpus: 
annotation for negation, uncertainty and their 
scope in biomedical texts. In Proceedings of 
BioNLP 2008. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun?ichi Tsujii. 2005. Syntax Annotation for the 
GENIA Corpus. In Proceedings of IJCNLP 2005, 
Companion volume. 
Kristina Toutanova, Aria Haghighi, and Christopher 
D. Manning. 2005. Joint Learning Improves Se-
mantic Role Labeling. In Proceedings of ACL 
2005. 
Peter D. Turney. 2002. Thumbs Up or Thumbs 
Down? Semantic Orientation Applied to Unsu-
pervised Classification of Reviews. In Proceed-
ings of ACL 2002. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for 
uncertainty, negation and their scopes. BMC 
Bioinformatics, 9(Suppl 11):S9. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
Features for Semantic Role Labeling. In Pro-
ceedings of EMNLP 2004. 
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
 
 
679
Coling 2010: Poster Volume, pages 757?765,
Beijing, August 2010
Dependency-Driven Feature-based Learning for Extracting 
Protein-Protein Interactions from Biomedical Text 
Bing Liu   Longhua Qian   Hongling Wang   Guodong Zhou 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow University 
Email: liubingnlp@gmail.com 
{qianlonghua,redleaf,gdzhou}@suda.edu.cn
                                                          
 Corresponding author 
Abstract
Recent kernel-based PPI extraction 
systems achieve promising perform-
ance because of their capability to 
capture structural syntactic informa-
tion, but at the expense of computa-
tional complexity. This paper incorpo-
rates dependency information as well 
as other lexical and syntactic knowl-
edge in a feature-based framework. 
Our motivation is that, considering the 
large amount of biomedical literature 
being archived daily, feature-based 
methods with comparable performance 
are more suitable for practical applica-
tions. Additionally, we explore the 
difference of lexical characteristics be-
tween biomedical and newswire do-
mains. Experimental evaluation on the 
AIMed corpus shows that our system 
achieves comparable performance of 
54.7 in F1-Score with other 
state-of-the-art PPI extraction systems, 
yet the best performance among all the 
feature-based ones.  
1 Introduction 
In recent years, automatically extracting 
biomedical information has been the subject of 
significant research efforts due to the rapid 
growth in biomedical development and 
discovery. A wide concern is how to 
characterize protein interaction partners since 
it is crucial to understand not only the 
functional role of individual proteins but also 
the organization of the entire biological 
process. However, manual collection of 
relevant Protein-Protein Interaction (PPI) 
information from thousands of research papers 
published every day is so time-consuming that 
automatic extraction approaches with the help 
of Natural Language Processing (NLP) 
techniques become necessary.  
Various machine learning approaches for 
relation extraction have been applied to the 
biomedical domain, which can be classified 
into two categories: feature-based methods 
(Mitsumori et al, 2006; Giuliano et al, 2006; 
S?tre et al, 2007) and kernel-based methods 
(Bunescu et al, 2005; Erkan et al, 2007; 
Airola et al, 2008; Kim et al, 2010). 
Provided a large-scale manually annotated 
corpus, the task of PPI extraction can be 
formulated as a classification problem. 
Typically, for featured-based learning each 
protein pair is represented as a vector whose 
features are extracted from the sentence 
involving two protein names. Early studies 
identify the existence of protein interactions 
by using ?bag-of-words? features (usually 
uni-gram or bi-gram) around the protein 
names as well as various kinds of shallow 
linguistic information, such as POS tag, 
lemma and orthographical features. However, 
these systems do not achieve promising results 
since they disregard any syntactic or semantic 
information altogether, which are very useful 
for the task of relation extraction in the 
newswire domain (Zhao and Grishman, 2005; 
Zhou et al, 2005). Furthermore, feature-based 
methods fail to effectively capture the 
structural information, which is essential to 
757
identify the relationship between two proteins 
in a syntactic representation. 
With the wide application of kernel-based 
methods to many NLP tasks, various kernels 
such as subsequence kernels (Bunescu and 
Mooney, 2005) and tree kernels (Li et al, 
2008), are also applied to PPI detection.. 
Particularly, dependency-based kernels such 
as edit distance kernels (Erkan et al, 2007) 
and graph kernels (Airola et al, 2008; Kim et 
al., 2010) show some promising results for PPI 
extraction. This suggests that dependency 
information play a critical role in PPI 
extraction as well as in relation extraction 
from newswire stories (Culotta and Sorensen, 
2004). In order to appreciate the advantages of 
both feature-based methods and kernel-based 
methods, composite kernels (Miyao et al, 
2008; Miwa et al, 2009a; Miwa et al, 2009b) 
are further employed to combine structural 
syntactic information with flat word features 
and significantly improve the performance of 
PPI extraction. However, one critical 
challenge for kernel-based methods is their 
computation complexity, which prevents them 
from being widely deployed in real-world 
applications regarding the large amount of 
biomedical literature being archived everyday.  
Considering the potential of dependency in-
formation for PPI extraction and the challenge 
of computation complexity of kernel-based 
methods, one may naturally ask the question: 
?Can the essential dependency information be 
maximally exploited in featured-based PPI 
extraction so as to enhance the performance 
without loss of efficiency?? ?If the answer is 
Yes, then How?? 
This paper addresses these problems, focus-
ing on the application of dependency informa-
tion to feature-based PPI extraction. Starting 
from a baseline system in which common 
lexical and syntactic features are incorporated 
using Support Vector Machines (SVM), we 
further augment the baseline with various fea-
tures related to dependency information, 
including predicates in the dependency tree. 
Moreover, in order to reveal the linguistic 
difference between distinct domains we also 
compare the effects of various features on PPI 
extraction from biomedical texts with those on 
relation extraction from newswire narratives. 
Evaluation on the AIMed and other PPI cor-
pora shows that our method achieves the best 
performance among all feature-based systems. 
The rest of the paper is organized as follows. 
A feature-based PPI extraction baseline system 
is given in Section 2 while Section 3 describes 
our dependency-driven method. We report our 
experiments in Section 4, and compare our 
work with the related ones in Section 5.  
Section 6 concludes this paper and gives some 
future directions. 
2 Feature-based PPI extraction: 
Baseline
For feature-based methods, PPI extraction task 
is re-cast as a classification problem by first 
transforming PPI instances into 
multi-dimensional vectors with various fea-
tures, and then applying machine learning ap-
proaches to detect whether the potential 
relationship exists for a particular protein pair. 
In training, a feature-based classifier learning 
algorithm, such as SVM or MaxEnt, uses the 
annotated PPI instances to learn a classifier 
while, in testing, the learnt classifier is in turn 
applied to new instances to determine their PPI 
binary classes and thus candidate PPI instances 
are extracted. 
As a baseline, various linguistic features, 
such as words, overlap, chunks, parse tree fea-
tures as well as their combined ones are ex-
tracted from a sentence and formed as a vector 
into the feature-based learner. 
1) Words 
Four sets of word features are used in our sys-
tem: 1) the words of both the proteins; 2) the 
words between the two proteins; 3) the words 
before M1 (the 1st protein); and 4) the words 
after M2 (the 2nd protein). Both the words be-
fore M1 and after M2 are classified into two 
bins: the first word next to the proteins and the 
second word next to the proteins. This means 
that we only consider the two words before M1 
and after M2. Words features include: 
x MW1: bag-of-words in M1 
x MW2: bag-of-words in M2 
x BWNULL: when no word in between 
x BWO: other words in between except 
first and last words when at least three 
words in between 
x BWM1FL: the only word before M1 
758
x BWM1F: first word before M1 
x BWM1L: second word before M1 
x BWM1: first and second word before 
M1
x BWM2FL: the only word after M2 
x BWM2F: first word after M2 
x BWM2L: second word after M2 
x BWM2: first and second word after M2 
2) Overlap 
The numbers of other protein names as well as 
the words that appear between two protein 
names are included in the overlap features. 
This category of features includes: 
x #MB: number of other proteins in be-
tween
x #WB: number of words in between 
x E-Flag: flag indicating whether the two 
proteins are embedded or not 
3) Chunks
It is well known that chunking plays an 
important role in the task of relation extraction 
in the ACE program (Zhou et al, 2005). How-
ever, its significance in PPI extraction has not 
fully investigated. Here, the Stanford Parser1
is first employed for full parsing, and then 
base phrase chunks are derived from full parse 
trees using the Perl script2. The chunking fea-
tures usually concern about the head words of 
the phrases between the two proteins, which 
are further classified into three bins: the first 
phrase head in between, the last phrase head in 
between and other phrase heads in between. In 
addition, the path of phrasal labels connecting 
two proteins is also a common syntactic 
indicator of the polarity of the PPI instance, 
just as the path NP_VP_PP_NP in the sen-
tence ?The ability of PROT1 to interact with 
the PROT2 was investigated.? is likely to sug-
gest the positive interaction between two pro-
teins. These base phrase chunking features 
contain:
x CPHBNULL: when no phrase in be-
tween.
x CPHBFL: the only phrase head when 
only one phrase in between 
x CPHBF: the first phrase head in between 
when at least two phrases in between. 
                                                          
1 http://nlp.stanford.edu/software/lex-parser.shtml
2 http://ilk.kub.nl/~sabine/chunklink/ 
x CPHBL: the last phrase head in between 
when at least two phrase heads in be-
tween.
x CPHBO: other phrase heads in between 
except first and last phrase heads when 
at least three phrases in between. 
x CPP: path of phrase labels connecting 
the two entities in the chunking 
Furthermore, we also generate a set of 
bi-gram features which combine the above 
chunk features except CPP with their corre-
sponding chunk types.  
4) Parse Tree 
It is obvious that full pares trees encompass 
rich structural information of a sentence. 
Nevertheless, it is much harder to explore 
such information in featured-based methods 
than in kernel-based ones. Thus so far only 
the path connecting two protein names in the 
full-parse tree is considered as a parse tree 
feature.
x PTP: the path connecting two protein 
names in the full-parse tree. 
 Again, take the sentence ?The ability of 
PROT1 to interact with the PROT2 was 
investigated.? as an example, the parse path 
between PROT1 and PROT2 is 
NP_S_VP_PP_NP, which is slightly different 
from the CPP feature in the chunking feature 
set.
3 Dependency-Driven PPI Extraction 
The potential of dependency information for 
PPI extraction lies in the fact that the depend-
ency tree may well reveal non-local or 
long-range dependencies between the words 
within a sentence. In order to capture the 
necessary information inherent in the 
depedency tree for identifying their 
relationship, various kernels, such as edit 
distance kernel based on dependency path 
(Erkan et al, 2007), all-dependency-path 
graph kernel (Airola et al, 2008), and 
walk-weighted subsequence kernels (Kim et 
al., 2010) as well as other composite kernels 
(Miyao et al, 2008; Miwa et al, 2009a; Miwa 
et al, 2009b), have been proposed to address 
this problem. It?s true that these methods 
achieve encouraging results, neverthless, they 
suffer from prohibitive computation burden. 
759
Thus, our solution is to fold the structural 
dependency information back into flat 
features in a feature-based framework so as to 
speed up the learning process while retaining 
comparable performance. This is what we 
refer to as dependency-driven PPI extraction. 
 First, we construct dependency trees from 
grammatical relations generated by the Stan-
ford Parser. Every grammatical relation has the 
form of dependent-type (word1, word2),
Where word1 is the head word, word2 is de-
pendent on word1, and dependent-type denotes 
the pre-defined type of dependency. Then, 
from these grammatical relations the following 
features called DependenecySet1 are taken 
into consideration as illustrated in Figure 1: 
x DP1TR: a list of words connecting 
PROT1 and the dependency tree root. 
x DP2TR: a list of words connecting 
PROT2 and the dependency tree root. 
x DP12DT: a list of dependency types 
connecting the two proteins in the 
dependency tree. 
x DP12: a list of dependent words com-
bined with their dependency types con-
necting the two proteins in the depend-
ency tree. 
x DP12S: the tuple of every word com-
bined with its dependent type in DP12. 
x DPFLAG: a boolean value indicating 
whether the two proteins are directly 
dependent on each other. 
The typed dependencies produced by the 
Stanford Parser for the sentence ?PROT1 
contains a sequence motif binds to PROT2.? 
are listed as follows: 
nsubj(contains-2,PROT1-1)
det(motif-5, a-3) 
nn(motif-5, sequence-4) 
nsubj(binds-6, motif-5) 
ccomp(contains-2, binds-6) 
prep_to(binds-6, PROT2-8) 
Each word in a dependency tuple is fol-
lowed by its index in the original sentence, 
ensuring accurate positioning of the head 
word and dependent word. Figure 1 shows the 
dependency tree we construct from the above 
grammatical relations.  
contains 
PROT1 
motif 
binds 
PROT2
a sequence 
nsubj ccomp 
prep_to
nsubj 
det nn 
Figure 1: Dependency tree for the sentence 
?PROT1 contains a sequence motif binds to 
PROT2.? 
Erkan et al (2007) extract the path 
information between PROT1 and PROT2 in 
the dependency tree for kernel-based PPI 
extraction and report promising results, 
neverthless, such path is so specific for 
feature-based methods that it may incure 
higher precision but lower recall. Thus we 
alleviate this problem by collapsing the feature 
into multiple ones with finer granularity, 
leading to the features such as DP12S. 
It is widely acknowledged that predicates 
play an important role in PPI extraction. For 
example, the change of a pivot predicate 
between two proteins may easily lead to the 
polarity reversal of a PPI instance. Therefore, 
we extract the predicates and their positions in 
the dependency tree as predicate features 
called DependencySet2:  
x FVW: the predicates in the DP12 feature 
occurring prior to the first protein. 
x LVW: the predicates in the DP12 feature 
occurring next to the second entity. 
x MVW: other predicates in the DP12 
features. 
x #FVW: the number of FVW 
x #LVW: the number of LVW 
x #MVW: the number of MVW 
4 Experimentation
This section systematically evaluates our fea-
ture-based method on the AIMed corpus as 
well as other commonly used corpus and re-
ports our experimental results. 
760
4.1 Data Sets 
We use five corpora3 with the AIMed corpus 
as the main experimental data, which contains 
177 Medline abstracts with interactions be-
tween two interactions, and 48 abstracts with-
out any PPI within single sentences. There are 
4,084 protein references and around 1,000 
annotated interactions in this data set.  
For corpus pre-procession, we first rename 
two proteins of a pair as PROT1 and PROT2 
respectively in order to blind the learner for 
fair comparison with other work.  Then, all 
the instances are generated from the sentences 
which contain at least two proteins,  that is, if 
a sentence contains n different proteins, there 
are n2 different pairs of proteins and these 
pairs are considered untyped and undirected. 
For the purpose of comparison with previous 
work, all the self-interactions (59 instances) 
are removed, while all the PPI instances with 
nested protein names are retained (154 in-
stances). Finally, 1002 positive instances and 
4794 negative instances are generated and 
their corresponding features are extracted.  
We select Support Vector Machines (SVM) 
as the classifier since SVM represents the 
state-of-the-art in the machine learning re-
search community. In particular, we use the 
binary-class SVMLigh 4 developed by 
Joachims (1998) since it satisfies our require-
ment of detecting potential PPI instances. 
Evaluation is done using 10-fold docu-
ment-level cross-validation. Particularly, we 
apply the extract same 10-fold split that was 
used by Bunescu et al (2005) and Giuliano et 
al. (2006). Furthermore, OAOD (One Answer 
per Occurrence in the Document) strategy is 
adopted, which means that the correct interac-
tion must be extracted for each occurrence. 
This guarantees the maximal use of the avail-
able data, and more important, allows fair 
comparison with earlier relevant work.  
The evaluation metrics are commonly used 
Precision (P), Recall (R) and harmonic 
F1-score (F1). As an alternative to F1-score, 
the AUC (area under the receiver operating 
characteristics curve) measure is proved to be 
invariant to the class distribution of the train-
ing dataset. Thus we also provide AUC scores 
                                                          
3 http://mars.cs.utu.fi/PPICorpora/GraphKernel.html 
4 http://svmlight.joachims.org/
for our system as Airola et al (2008) and 
Miwa et al (2009a). 
4.2 Results and Discussion 
Features P(%) R(%) F1 
Baseline features 
Words 59.4 40.6 47.6
+Overlap 60.4 39.9 47.4
+Chunk 59.2 44.5 50.6
+Parse 60.9 44.8 51.4
Dependency-driven features 
+Dependency Set1 62.9 48.0 53.9
+Dependency Set2 63.4 48.8 54.7
Table 1: Performance of PPI extraction with vari-
ous features in the AIMed corpus 
We present in Table 1 the performance of our 
system using document-wise evaluation 
strategies and 10-fold cross-validation with 
different features in the AIMed corpus, where 
the plus sign before a feature means it is 
incrementally added to the feature set. Table 1 
reports that our system achieves the best per-
formance of 63.4/48.8/54.7 in P/R/F scores. It 
also shows that: 
x Words features alone achieve a relatively 
low performance of 59.4/40.9/47.6 in 
P/R/F, particularly with fairly low recall 
score. This suggests the difficulty of PPI 
extraction and words features alone can?t 
effectively capture the nature of protein 
interactions.
x Overlap features slightly decrease the per-
formance. Statistics show that both the 
distributions of #MB and #WB between 
positives and negatives are so similar that 
they are by no means the discriminators for 
PPI extraction. Hence, we exclude the 
overlap features in the succeeding experi-
ments.
x Chunk features significantly improves the 
F-measure by 3 units largely due to the in-
crease of recall by 3.9%, though at the 
slight expense of precision. This suggests 
the effectiveness of shallow parsing infor-
mation in the form of headwords captured 
by chunking on PPI extraction.  
x The usefulness of the parse tree features is 
quite limited. It only improves the 
F-measure by 0.8 units. The main reason 
may be that these paths are usually long 
761
and specific, thus they suffer from the 
problem of data sparsity. Furthermore, 
some of the parse tree features are already 
involved in the chunk features.  
x The DependencySet1 features are very 
effective in that it can increase the preci-
sion and recall by 2.0 and 3.2 units 
respectively, leading to the increase of F1 
score by 2.5 units. This means that the de-
pendency-related features can effectively 
retrieve more PPI instances without intro-
ducing noise that will severely harm the 
precision. According to our statistics, there 
are over 60% sentences with more than 5 
words between their protein entities in the 
AIMed corpus. Therefore, dependency in-
formation exhibit great potential to PPI 
extraction since they can capture 
long-range dependencies within sentences. 
Take the aforementioned sentence 
?PROT1 contains a sequence motif binds 
to PROT2.? as an example, although the 
two proteins step over a relatively long 
distance, the dependency path between 
them is concise and accurate, reflecting the 
essence of the interaction. 
x The predicate features also contribute to 
the F1-score gain of 0.8 units. It is not 
surprising since some predicates, such as 
?interact?, ?activate? and ?inhibit? etc, are 
strongly suggestive of the interaction 
polarity between two proteins. 
We compare in Table 2 the performance of 
our system with other systems in the AIMed 
corpus using the same 10-fold cross validation 
strategy. These systems are grouped into three 
distinct classes: feature-based, kernel-based 
and composite kernels. Except for Airola et al 
(2008) Miwa et al (2009a) and Kim et al 
(2010), which adopt graph kernels, our system 
performs comparably with other systems. In 
particular, our dependency-driven system 
achieves the best F1-score of 54.7 among all 
feature-based systems. 
In order to measure the generalization abil-
ity of our dependency-driven PPI extraction 
system across different corpora, we further 
apply our method to other four publicly avail-
able PPI corpora: BioInfer, HPRD50, IEPA 
and LLL.  
Table 2: Comparison with other PPI extraction 
systems in the AIMed corpus 
The corresponding performance of 
F1-score and AUC metrics as well as their 
standard deviations is present in Table 3.  
Comparative available results from Airola et 
al. (2008) and Miwa et al (2009a) are also 
included in Table 3 for comparison. This table 
shows that our system performs almost 
consistently with the other two systems, that is, 
the LLL corpus gets the best performance yet 
with the greatest variation, while the AIMed 
corpus achieves the lowest performance with 
reasonable variation. 
It is well known that biomedical texts ex-
hibit distinct linguistic characteristics from 
newswire narratives, leading to dramatic per-
formance gap between PPI extraction and 
relation detection in the ACE corpora. How-
ever, no previous work has ever addressed this 
problem and empirically characterized this 
difference. In this paper, we devise a series of 
experiments over the ACE RDC corpora using 
our dependency-driven feature-based method 
as a touchstone task. In order to do that, a sub-
                                                          
5 Airola et al (2008) repeat the method published by 
Giuliano et al (2006) with a correctly preprocessed 
AIMed and reported an F1-score of 52.4%. 
6 The results from Table 1 (Miyao et al, 2009) with the 
most similar settings to ours (Stanford Parser with SD 
representation) are reported. 
Systems P(%) R(%) F1
Feature-based methods 
Our system 63.4 48.8 54.7
Giuliano et al, 20065 60.9 57.2 59.0
S?tre et al, 2007 64.3 44.1 52.0
Mitsumori et al, 2006 54.2 42.6 47.7
Yakushiji et al, 2005 33.7 33.1 33.4
Kernel-based methods 
Kim et al, 2010 61.4 53.3 56.7
Airola et al, 2008 52.9 61.8 56.4
Bunescu et al, 2006  65.0 46.4 54.2
Composite kernels 
Miwa et al, 2009a - - 62.0
Miyao et al, 20086 51.8 58.1 54.5
762
set of 5796 relation instances is randomly 
sampled from the ACE 2003 and 2004 cor-
pora respectively.  The same cross-validation 
and evaluation metrics are applied to these 
two sets as PPI extraction in the AIMed cor-
pus.
Our system Airola et al (2008) 7 Miwa et al (2009a) 
Corpus F1 ?F1 AUC ?AUC F1 ?F1 AUC ?AUC F1 ?F1 AUC ?AUC
AIMed 54.7 4.5 82.4 3.5 56.4 5.0 84.8 2.3 60.8 6.6 86.8 3.3
BioInfer 59.8 3.5 80.9 3.3 61.3 5.3 81.9 6.5 68.1 3.2 85.9 4.4
HPRD50 64.9 13.4 79.8 8.5 63.4 11.4 79.7 6.3 70.9 10.3 82.2 6.3
IEPA 62.1 6.2 74.8 6.6 75.1 7.0 85.1 5.1 71.7 7.8 84.4 4.2
LLL 78.1 15.8 85.1 8.3 76.8 17.8 83.4 12.2 80.1 14.1 86.3 10.8
Table 3: Comparison of performance across the five PPI corpora 
AIMed ACE2003 ACE2004 
Features
P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 
Words 59.4 40.6 47.6 66.5 51.6 57.9 68.1 59.6 63.4
+Overlap +1.0 -0.7 -0.2 +5.4 +1.8 +3.2 +4.6 +1.2 +2.7
+Chunk -1.7 +4.6 +3.2 +2.3 +5.1 +4.0 +1.5 +1.9 +1.7
+Parse +1.7 +0.3 +0.8 +0.3 +0.6 +0.5 +0.6 +0.4 +0.5
+Dependency Set1 +2.0 +3.2 +2.5 +0.8 +0.7 +0.7 +0.5 +0.9 +0.7
+Dependency Set2 +0.5 +0.8 +0.8 +0.3 +0.2 +0.3 +0.2 +0.4 +0.3
Table 4: Comparison of contributions of different features to relation detection across multiple domains 
Table 4 compares the performance of our 
method over different domains. The table re-
ports that the words features alone achieve the 
best F1-score of 63.4 in ACE2004 but the low-
est F1-score of 47.6 in AIMed. This suggests 
the wide difference of lexical distribution be-
tween these domains. We extract the words 
appearing before the 1st mention, between the 
two mentions and after the 2nd mention from 
the training sets of these corpora respectively, 
and summarize the statistics (the number of 
tokens, the number of occurrences) in Table 5, 
where the KL divergence between positives 
and negatives is summed over the distribution 
of the 500 most frequently occurring words. 
                                                          
7 The performance results of F1 and AUC on the BioInfer corpus are slightly adjusted according to Table 3 in Miwa et 
al. (2009b) 
Table 5: Lexical statistics on three corpora 
The table shows that AIMed uses the most 
kinds of words and the most words around the 
two mentions than the other two. More impor-
tant, AIMed has the least distribution differ-
ence between the words appearing in positives 
and negatives, as indicated by its least KL 
divergence. Therefore, the lexical words in 
AIMed are less discriminative for relation 
detection than they do in the other two. This 
naturally explains the reason why the perform-
ance by words feature alone is 
AIMed<ACE2003<ACE2004. In addition, 
Table 4 also shows that: 
x The overlap features significantly improve 
the performance in ACE while slightly 
deteriorating that in AIMed. The reason is 
that, as indicated in Zhou et al (2005), most 
of the positive relation instances in ACE 
exist in local contexts, while the positive 
interactions in AIMed occur in relative 
long-range just as the negatives, therefore 
these features are not discriminative for 
AIMed.
Statistics AIMed ACE2003 ACE2004
# of tokens 2,340 2,064 2,099
# of occurrences 69,976 53,744 49,570
KL divergence  0.22 0.28 0.33 
x The chunk features consistently greatly 
boost the performance across multiple cor-
pora. This implies that the headwords in 
chunk phrases can well capture the partial 
nature of relation instances regardless of 
their genre. 
x It?s not surprising that the parse feature 
attain moderate performance gain in all do-
mains since these parse paths are usually 
763
long and specificity, leading to data 
sparseness problem. 
x It is interesting to note that the depend-
ency-related features exhibit more signifi-
cant improvement in AIMed than that in 
ACE. The reason may be that, these 
dependency features can effectively cap-
ture long-range relationships prevailing in 
AIMed, while in ACE a large number of 
local relationships dominate the corpora. 
5 Related Work 
Among feature-based methods, the PreBIND 
system (Donaldson et al, 2003) uses words and 
word bi-grams features to identify the existence 
of protein interactions in abstracts and such 
information is used to enhance manual expert 
reviewing for the BIND database. Mitsumori et 
al. (2006) use SVM to extract protein-protein 
interactions, where bag-of-words features, spe-
cifically the words around the protein names, 
are employed. Sugiyama et al (2003) extract 
various features from the sentences based on 
the verbs and nouns in the sentences such as the 
verbal forms, and the part-of-speech tags of the 
20 words surrounding the verb. In addition to 
word features, Giuliano et al (2006) extract 
shallow linguistic information such as POS tag, 
lemma, and orthographic features of tokens for 
PPI extraction. Unlike our dependency-driven 
method, these systems do not consider any 
syntactic information.  
For kernel-based methods, there are several 
systems which utilize dependency information. 
Erkan et al (2007) defines similarity functions 
based on cosine similarity and edit distance 
between dependency paths, and then incorpo-
rate them in SVM and KNN learning for PPI 
extraction. Airola et al (2008) introduce 
all-dependency-paths graph kernel to capture 
the complex dependency relationships between 
lexical words and attain significant perform-
ance boost at the expense of computational 
complexity. Kim et al (2010) adopt 
walk-weighted subsequence kernel based on 
dependency paths to explore various substruc-
tures such as e-walks, partial match, and 
non-contiguous paths. Essentially, their kernel 
is also a graph-based one. 
For composite kernel methods, S?tre et al 
(2007) combine a ?bag-of-words? kernel with 
dependency and PAS (Predicate Argument 
Structure) tree kernels to exploit both the words 
features and the structural syntactic information. 
Hereafter, Miyao et al (2008) investigate the 
contribution of various syntactic features using 
different representations from dependency 
parsing, phrase structure parsing and deep 
parsing by different parsers. Miwa et al 
(2009a) integrate ?bag-of-words? kernel, PAS 
tree kernel and all-dependency-paths graph 
kernel to achieve the higher performance. They 
(Miwa et al, 2009b) also use similar compos-
ite kernels for corpus weighting learning 
across multiple PPI corpora.  
6 Conclusion and Future Work 
In this paper, we have combined various lexical 
and syntactic features, particularly dependency 
information, into a feature-based PPI extraction 
system. We find that the dependency informa-
tion as well as the chunk features contributes 
most to the performance improvement.  The 
predicate features involved in the dependency 
tree can also moderately enhance the perform-
ance. Furthermore, comparative study between 
biomedical domain and the ACE newswire 
domain shows that these domains exhibit 
different lexical characteristics, rendering the 
task of PPI extraction much more difficult than 
that of relation detection from the ACE cor-
pora.
In future work, we will explore more syntac-
tic features such as PAS information for fea-
ture-based PPI extraction to further boost the 
performance. 
Acknowledgment 
This research is supported by Projects 
60873150 and 60970056 under the National 
Natural Science Foundation of China and Pro-
ject BK2008160 under the Natural Science 
Foundation of Jiangsu, China. We are also very 
grateful to Dr. Antti Airola from Truku 
University for providing partial experimental 
materials. 
References 
A. Airola, S. Pyysalo, J. Bj?rne, T. Pahikkala, F. 
Ginter, and T. Salakoski. 2008. All-paths graph 
kernel for protein-protein interaction extraction 
764
with evaluation of cross corpus learning. BMC
Bioinformatics.
R. Bunescu, R. Ge, R. Kate, E. Marcotte, R. Mooney, 
A. Ramani, and Y. Wong. 2005. Comparative 
Experiments on learning information extractors 
for Proteins and their interactions. Journal of 
Artificial Intelligence In Medicine, 33(2).  
R. Bunescu and R. Mooney. 2005. Subsequence 
kernels for relation extraction. In Proceedings of  
NIPS?05, pages 171?178. 
A. Culotta and J. Sorensen. 2004.  Dependency 
Tree Kernels for Relation Extraction. In 
Proceedings of ACL?04.
I. Donaldson, J. Martin, B. de Bruijn, C. Wolting, V. 
Lay, B. Tuekam, S. Zhang, B. Baskin, G. D. 
Bader, K. Michalockova, T. Pawson, and C. W. V. 
Hogue. 2003. Prebind and textomy - mining the 
biomedical literature for protein-protein 
interactions using a support vector machine. 
Journal of BMC Bioinformatics, 4(11). 
G. Erkan, A. ?zg?r, and D.R. Radev. 2007. 
Semi-Supervised Classification for Extracting 
Protein Interaction Sentences using Dependency 
Parsing, In Proceedings of EMNLP-CoNLL?07,
pages 228?237. 
C. Giuliano, A. Lavelli, and L. Romano. 2006. 
Exploiting Shallow Linguistic Information for 
Relation Extraction from Biomedical Literature. 
In Proceedings of EACL?06, pages 401?408. 
 S. Kim, J. Yoon, J. Yang, and S. Park. 2010. 
Walk-weighted subsequence kernels for 
protein-protein interaction extraction. Journal of 
BMC Bioinformatics, 11(107). 
J. Li, Z. Zhang, X. Li, and H. Chen. 2008. 
Kernel-Based Learning for Biomedical Relation 
extraction. Journal of the American Society for 
Information Science and Technology, 59(5). 
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. 
Doi. 2006. Extracting protein-protein interaction 
information from biomedical text with SVM. 
IEICE Transactions on Information and System, 
E89-D (8).
M. Miwa, R. S?tre, Y. Miyao, and J. Tsujii. 2009a. 
Protein-Protein Interaction Extraction by 
Leveraging Multiple Kernels and Parsers. 
Journal of Medical Informatics, 78(2009). 
M. Miwa, R. S?tre, Y. Miyao, and J. Tsujii. 2009b.  
A Rich Feature Vector for Protein-Protein 
Interaction Extraction from Multiple Corpora. In 
Proceedings of EMNLP?09, pages 121?130.  
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and 
J.Tsujii. 2008. Task-oriented evaluation of 
syntactic parsers and their representations. In 
Proceedings of ACL?08, pages 46?54. 
T. Ono, H. Hishigaki, A. Tanigami, and T. Takagi. 
2001. Automated extraction of information on 
protein-protein interactions from the biological 
literature. Journal of Bioinformatics, 17(2). 
 K. Sugiyama, K. Hatano, M. Yoshikawa, and S. 
Uemura. 2003. Extracting information on 
protein-protein interactions from biological 
literature based on machine learning approaches. 
Journal of Genome Informatics, (14): 699?700. 
R. S?tre, K. Sagae, and J. Tsujii. 2007. Syntactic 
features for protein-protein interaction extraction. 
In Proceedings of LBM?07, pages 6.1?6.14. 
A. Yakushiji, M. Yusuke, T. Ohta, Y. Tateishi, J. 
Tsujii. 2006. Automatic construction of 
predicate-argument structure patterns for 
biomedical information extraction. In 
Proceedings of EMNLP?06, pages 284?292. 
S.B. Zhao and R. Grishman. 2005. Extracting 
Relations with Integrated Information Using 
Kernel Methods. In Proceedings of ACL?05,
pages 419-426.  
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. 
Exploring various knowledge in relation 
extraction.  In Proceedings of ACL?05, pages 
427-434.  
765
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 520?529, Dublin, Ireland, August 23-29 2014.
Skill Inference with Personal and Skill Connections
Zhongqing Wang
?
, Shoushan Li
??
, Hanxiao Shi
?
, and Guodong Zhou
?
?
Natural Language Processing Lab, School of Computer Science and Technology,
Soochow University, China
?
School of Computer Science and Information Engineering,
Zhejiang Gongshang University, China
{wangzq.antony, shoushan.li}@gmail.com
hxshi@mail.zjgsu.edu.cn, gdzhou@suda.edu.cn
Abstract
Personal skill information on social media is at the core of many interesting applications. In
this paper, we propose a factor graph based approach to automatically infer skills from per-
sonal profile incorporated with both personal and skill connections. We first extract personal
connections with similar academic and business background (e.g. co-major, co-university, and
co-corporation). We then extract skill connections between skills from the same person. To well
integrate various kinds of connections, we propose a joint prediction factor graph (JPFG) model
to collectively infer personal skills with help of personal connection factor, skill connection fac-
tor, besides the normal textual attributes. Evaluation on a large-scale dataset from LinkedIn.com
validates the effectiveness of our approach.
1 Introduction
With the large amount of user-generated content (UGC) published online every day in the context of
social networks (Tan et al., 2011; Luo et al., 2013), such online social networks (e.g., Twitter, Facebook,
and LinkedIn) have significantly enlarged our social circles and much affected our everyday life. One
popular and important type of UGC is the personal profile, where people post their detailed information,
such as education, experience and other personal information, on online portals. Social websites like
Facebook.com and LinkedIn.com have created a viable business as profile portals, with the popularity
and success largely attributed to their comprehensive personal profiles.
Obviously, online personal profiles can help people connect with others of similar backgrounds and
provide valuable resources for businesses, especially for personnel resource managers to find talents
(Yang et al., 2011a; Guy et al., 2010). In the profiles, the personal skill information is the most impor-
tant aspect to reflect the expertise of a person. However, few social platforms allow users to manually
attach such personal skill information into their personal profiles. For example, in our collected dataset,
91.8% skills appear less than 10 times. Even the distribution of the top 10 frequently occurring skills is
asymmetric, and only 43.1% people attach skills on their profiles. For this regard, it is highly desirable
to develop reliable methods to automatically infer personal skills for personal profiles.
Although it is straightforward to recast skill inference as a standard text classification problem, i.e.,
predicting the skills with the profile text alone, personal profiles usually are poorly organized, even with
critical information missing. Thus, it is challenging to infer skills given the limited information from
the profile texts. We propose two assumptions to address above challenges by incorporating additional
connection information between persons and skills:
? People are always connected to others with similar academic and business backgrounds (e.g. co-
major, co-corporation). For example if there is co-major, co-university, or co-corporation rela-
tionship between two persons, it is very likely that they may share similar skills. Therefore, it is
reasonable to resort to personal connection information to improve the performance of skill infer-
ence.
*corresponding author
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
520
? One person tends to have some related skills. For example, it is very likely that C++, C, and Python
programming languages may co-occur in the one?s profile, i.e., if a person has skill C++, it is highly
possible that he would have the skills such as C or Python. Thus, it is useful to integrate skill
connection information when inferring personal skills.
Based on these assumptions, we propose a Joint Prediction Factor Graph (JPFG) model, which collec-
tively predicts personal skills with help of both personal and skill connections. In particular, the JPFG
model provides a general framework to integrate three kinds of knowledge, i.e. local textual attribute
functions of an individual person, personal connection factors between persons, and skill connection fac-
tors between skills, in collectively inferring personal skills. Specially, we extract personal connections
with similar academic and business background (e.g. co-major, co-corporation). We then extract skill
connections between skills from same person. Evaluation on a large-scale data set from LinkedIn.com
indicates that our JPFG model can significantly improve the performance of personal skill inference.
The remainder of this paper is structured as follows. We review the related work in Section 2. In Sec-
tion 3, we introduce the data collection. In Section 4, we give the problem definition and some analysis
on the task of personal skill reference. In Section 5, we propose the JPFG model and corresponding
algorithms for parameter estimation and prediction. In Section 6, we present our experimental results. In
Section 7, we summarize our work and discuss future directions.
2 Related Works
In this section, we briefly review related studies in expert finding, social tag suggestion and factor graph
model.
2.1 Expert Finding
Expert finding aims to find right persons with appropriate skills or knowledge, i.e. ?Who are the experts
on topic X?? TREC-2005 and TREC-2006 have provided a common platform for researchers to empiri-
cally evaluate methods and techniques on expert finding (Soboroff et al, 2006; Zhang et al., 2007a).
In the literature, expert finding tends to consider each skill individually and seeks the most authority
experts for each skill. Thus, expert finding is always considered as a ranking process, i.e., ranking the
experts from the candidates who are most suitable for the skill (Balog and Rijke, 2007). For example,
Campbell et al. (2003) investigated the issue of expert finding in an email network. They utilized the
link between email authors and receivers to improve the expert finding performance.
Besides that link structure-based algorithms, such as PageRank and HITS, are employed to analyze
the relationship of the link-relationship graph, social networks are utilized to improve the performance
of expert finding. Zhang et al. (2007a) proposed a unified propagation-based approach to address the
issue of expert finding in a social network, considering both personal local and network information (e.g.
the relationship between persons).
Expert finding is in nature different from skill inference. Our study predicts various skills attachable to
a person collectively with both personal and skill connections among people. One distinguishing charac-
teristics of our study is that several skills from a person are simultaneously modeled and the relationship
among these skills is fully leveraged in the inference.
2.2 Social Tag Suggestion
Social tag suggestion aims to extract proper tags from social media and can thus help people organize
their information in an unconstrained manner (Ohkura et al., 2006; Si et al., 2010). Ohkura et al. (2006)
created a multi-tagger to determine whether a particular tag from a candidate tag list should be attached
to a weblog. Lappas et al. (2011) proposed a social endorsement-based approach to generate social tags
from Twitter.com and Flickr.com where various kinds of information in recommendations and comments
are used. Liu et al. (2012) propose a probabilistic model to connect the semantic relations between words
and tags of microblog, and takes the social network structure as regularization. Li et al., (2012) propose
to model context-aware relations of tags for suggestion by regarding resource content as context of tags.
521
Different from above researches, our study is forced on skill inference instead of traditional tag sugges-
tion. Basically, the social connections in skill inference are much different from those in social tagging.
In our study, we use co-major, co-title and other academic and business relationships to build the social
connections. Meanwhile, there are also few researches concern to propose a joint model to leverage both
personal and skill connections.
2.3 Factor Graph Model
Among various approaches investigated in social networks in the last several years (Leskovec et al.,
2010; Lu et al., 2010; Lampos et al., 2013; Guo et al., 2013), Factor Graph Model (FGM) becomes an
effective way to represent and optimize the relationship in social networks (Dong et al., 2012; Yang et
al., 2012b) via a graph structure. Tang et al. (2011a) and Zhuang et al. (2012) formalized the problem
of social relationship learning as a semi-supervised framework, and proposed Partially-labeled Pairwise
Factor Graph Model (PLP-FGM) for inferring the types of social ties. Tang et al. (2013) further proposed
a factor graph based distributed learning method to construct a conformity influence model and formalize
the effects of social conformity in a probabilistic way.
Different from previous studies, this paper proposes a pairwise factor graph model to collectively infer
personal skills with both social connection factor and skill connection factor.
3 Data Construction
We collect our data set from LinkedIn.com. It contains a large number of personal profiles generated by
users, containing various kinds of information, such as personal Summary, Experience, Education, and
Skills & Expertise. We do not collect personal names in public profiles to protect people?s privacy.
The dataset contains 7,381 personal profiles, among which only 3,182 profiles (43.1% of all the pro-
files) show the Skills & Expertise field. In this study, we adopt only these profiles in all our experiments.
As a result, we get 6,863 skills in total, among which 6,299 skills (91.8% of them) appear less than 10
times. Among the remaining 564 skills, we select top 10 frequently occurring skills as the candidate
personal skills in this study (Since the remaining 554 skills only appear less than 250 times in total, it is
difficult to build an effective classifier for them). Table 1 illustrates the statistics.
Skill Number Ratio
Semiconductors 948 0.298
IC 369 0.116
Thin Films 328 0.103
Characterization 326 0.102
CMOS 311 0.098
Matlab 287 0.090
Microsoft Office 283 0.089
Manufacturing 278 0.087
Design of Experiments 262 0.082
Semiconductor Industry 250 0.079
Table 1: The distribution of the candidate personal skills
From Table 1, we can see that the skill distribution in the personal profiles is asymmetric. For example,
the Semiconductor skill occurs about 1,000 times, taking 29.8%, while the Semiconductor Industry skill
occurs 250 times only, taking 7.9%.
4 Problem Definition and Analysis
Before presenting our approach for skill inference, we first give the definition of the problem, and convey
a series of discoveries we observed from the data.
522
4.1 Problem Definition
We first introduce some necessary definitions and then formulate of the problem.
Definition 1: Skill inference. In principle, we cast skill inference as a skill prediction problem. Since
one person might have several skills, we build several vectors for a person and each vector is designed to
determine whether the corresponding skill is appropriate for the person or not (?Positive? means that the
person has the target skill, whereas ?Negative? stands for the opposite). Note that the number of vectors
for a person is equal to the number of candidate skills. For example, suppose we have m persons and
n candidate skills in the dataset, we totally build vectors to represent if these skills are attached in these
persons? profiles.
Definition 2: Textual information. We use texts of Summary and Experience as the textual information
for our research. Texts of Summary and Experience are unstructured information, while texts of Skills
& Expertise are structured information. However, some skills in the Skill & Expertise fields may not be
mentioned in the Summary and Experience fields.
Definition 3: Personal connections. We can explicitly extract four kinds of personal relationships
between two persons from the Education and Experience fields, as follows:
? co major, which denotes that two persons have the same major at school
? co univ, which denotes that two persons graduated from the same university
? co title, which denotes that two persons have the same title in a corporation.
? co corp, which denotes that two persons work in the same corporation.
Definition 4: Skill connections. We extract skill connections from same person. That is, if two vectors
are from the same person with different skills, we consider these two vectors share skill connections (e.g.
John has IC and Thin Films skills).
Learn task: Given the textual information of each profile, the personal connections between pro-
files, and skill connections of skill from same persons, the goal is to infer the skill through the above
information.
To learn the skill inference model, there are several requirements. First, the skills of persons are related
to multiple factors, e.g., network structure, personal connections, and skill connections, it is important to
find a unified model which is able to incorporate all the information together. Second, the algorithm to
learn the inference model should be efficient. In practice, the scale of the social network might be very
large.
4.2 Statistics and Observations
In the following, we give some statistics and observations on personal and skill connections.
Figure 1: The statistic of personal connection edges in our dataset
Statistics of personal connections: Figure 1 gives the statistics of personal connection edges. It
shows that with 3,182 profiles, there exist 332,390 personal connection edges. Besides, among all the
523
four relations, co major, co unvi, co title, and co corp occupy 11.7%, 40.0%, 17.7% and 30.6% respec-
tively.
Observations of skills connections: To validate the tendency of a person sharing similar skills, we
use PMI (Point-wise Mutual Information) to measure the co-occurrence between two skills. As a popular
way to measure the co-occurrence between a pair (Turney, 2002), PMI is calculated as follows:
PMI(i, j) = log
(
N
P (i&j)
P (i)P (j)
)
(1)
N is the number of profiles, P (i&j) denotes the probability of the skills (i.e., i and j) co-occurrence in
a person?s profile, while P (i) denotes the probability of the skill i appearing in a person?s profile.
Skill i Skill j PMI
C COMS 1.711
Thin Films Characterization 1.624
Thin Films Design of Experiments 1.543
Semiconductor Industry IC 1.345
Semiconductor Industry Design of Experiments 1.345
IC Microsoft Office -2.390
CMOS Microsoft Office -2.627
Semiconductor Industry Matlab -3.112
Average PMI score 0.190
Table 2: The top-5 and bottom-3 co-occurred skill pairs with their PMI scores
Table 2 lists the top-5 and bottom-3 co-occurred skill pairs with their PMI scores, together with the
average PMI score. From this table, we can see that if two skills are related, e.g., ?IC? and ?CMOS?,
these two skills tend to co-occur in one person?s profile, vice versa.
5 Joint Prediction Factor Graph Model
In this section, we propose a Joint Prediction Factor Graph (JPFG) model for learning and predicting the
skills with personal and skill connection information besides local textual information.
5.1 Model
We formalize the problem of skill prediction using a pairwise factor graph model, and our basic idea of
defining the correlations is to use different types of factor functions (i.e., personal connection factor, and
skill connection factor). Here, the objective function P
?
(Y |X,G) is defined based on the joint probability
of the factor functions, and the problem of collective skill inference model learning is cast as learning
model parameters ? that maximizes the joint probability of skills based on the input continuous dynamic
network.
Since directly maximizing the conditional probability P
?
(Y |X,G) is often intractable, we factorize
the ?global? probability as a product of ?local? factor functions, each of which depends on a subset of
the variables in the graph (Tang et al., 2013). In particular, we use three kinds of functions to represent
the local textual information of the vector (local textual attribute function), personal connection informa-
tion between vectors (personal connection factor) and skill connection information between skills (skill
connection factor), respectively. We now briefly introduce the ways to define the above three functions.
Local textual attribute functions f(x
ij
, y
i
)
j
: It denotes the attribute value associated with each
person i. Here, we define the local textual attribute as a feature (Lafferty et al., 2001) and accumulate all
the attribute functions to obtain local entropy for a person:
1
Z
1
exp
(
?
i
?
k
?
k
f
k
(x
ik
, y
i
)
)
(2)
524
Where ?
k
is the function weight, representing the influence degree of the attribute k. For simplicity, we
use word unigrams of a text as the basic textual attributes.
Personal connection factor function g(y
i
, y
j
) : For the personal correlation factor function, we
define it through the pairwise network structure. That is, if a person i and another person j have a
personal relationship, we define a personal connection factor function as follows:
g(y
i
, y
j
) = exp
{
?
ij
(y
i
? y
j
)
2
}
(3)
The personal connections are defined Section 4, i.e., co major, co univ, co title, and co corp. We define
that if two persons have at least one personal connection edge, they have a personal relationship. In
addition, ?
ij
is the weight of the function, representing the influence degree of i on j.
Skill connection factor function h(y
i
, y
j
): For the skill connection factor function, we define it
through the pairwise network structure. That is, if vector i and vector j are from the same person with
different skills, we define their skill connection influence factor function as follows:
h(y
i
, y
j
) = exp
{
?
ij
(y
i
? y
j
)
2
}
(4)
Where ?
ij
is the function weight, representing the influence degree of i on j.
By the above defined correlations, we can construct the graphical structure in the factor model. Ac-
cording to the Hammersley-Clifford theorem (Hammersley and Clifford, 1971), we integrate all the factor
functions and obtain the following log-likelihood objective function:
L(?) = log
?
P (Y |X,G)
=
1
Z
1
?
i
?
k
?
k
f
k
(x
ik
, y
i
)
+
1
Z
2
?
i
?
j?NB(i)
exp
{
?
ij
(y
i
? y
j
)
2
}
+
1
Z
3
?
i
?
k?SAME(i)
exp
{
?
ik
(y
i
? y
k
)
2
}
? logZ
(5)
Where (i, j) is a pair derived from the input network, Z = Z
1
Z
2
Z
3
is a normalization factor and
? = ({?}, {?}, {?}) indicates a parameter configuration, NB(i) denotes the set of social relationship
neighbors nodes of i (personal connection), and SAME(i) denotes the set of the node with the same
person of i (skill connection).
5.2 Learning and Prediction
Model Learning: Learning of the factor model is to find the best configuration for free parameters
? = ({?}, {?}, {?}) that maximizes the log likelihood objective function L(?).
?
?
= argmaxL(?) (6)
As the network structure in a social network can be arbitrary (e.g. possible of containing cycles), we
use the Loopy Belief Propagation (LBP) algorithm (Tang et al., 2011a) to approximate the marginal
distribution. To explain how we learn the parameters, we can get the gradient of each ?
k
with regard to
the objective function (Eq. 5), taking ? (the weight of the personal connection factor function g(y
i
, y
j
))
as an example:
L(?)
?
k
= E[g(i, j)] + E
?k
P (Y |X,G)
[g(i, j)] (7)
Where E[g(i, j)] is the expectation of factor function g(i, j) given the data distribution in the input
network and E
?k
P (Y |X,G)
[g(i, j)] represents the expectation under the distribution learned by the model,
i.e., P (y
i
|X,G) .
With the marginal probabilities, the gradient is obtained by summing up all triads (similar gradients
can be derived for parameter ?
k
and ?
ij
). It is worth noting that we need to perform the LBP process
525
twice in each iteration. The first run to estimate the marginal distribution of unknown variables y
i
=? and
the second one is to estimate the marginal distribution over all pairs. Finally, with the obtained gradient,
we update each parameter with a learning rate ?.
Skill Prediction: We can see that in the learning process, additional loopy belief propagation is used
to infer the label of unknown relationships. After learning, all unknown skills are assigned with labels
that maximize the marginal probabilities (Tang et al., 2011b), i.e.,
Y
?
= argmaxL(Y |X,G, ?) (8)
6 Experimentation
In this section, we first introduce the experimental setting, and then evaluate the performance of our
proposed JPFG model with both personal and skill connection information.
6.1 Experimental Setting
As described in Section 3, the experimental data are collected from LinkedIn.com. With top 10 frequently
used skills as candidate skills in all our experiments, we randomly select 2,000 profiles as training data
and 1,000 profiles as testing data.
Though positive and negative samples of each skill are imbalanced (In this paper, the number of the
negative samples is much larger than that of the positive samples), we select balanced testing and training
samples for each skill. Following models are implemented and compared.
? Keyword, for each profile, we consider the profile attached with the skill, only if the text of the skill
appears on the profile article with textual information.
? MaxEnt, which first uses local textual information as features to train a maximum entropy (ME)
classification model, and then employs the classification model to predict the skills in the testing
data set. The ME algorithm is implemented with the mallet toolkit
1
.
? JPFG, exactly our proposed model, which jointly predicts personal skills with local textual infor-
mation, personal connection and skill connection.
For performance evaluation, we adopt Precision (P.), Recall (R.) and F1-Measure (F1.).
6.2 Comparison with Baselines
Our first group of experiments is to investigate whether the JPFG model is able to improve skill inference
and whether the personal and skill connections are useful. The experimental results are shown in Table
3. From the table we can find that as some skills may not be mentioned on the Summary and Experience
fields directly, the performance of the Keyword approach is far from satisfaction. As incorporating
personal and skill connections, the JPFG model yields a much higher F1-measure, which improves the
performance with about 6.8% gain than the MaxEnt model.
6.3 Performance of JPFG with Different Training Data Sizes
After we evaluate the effective of the JPFG model with the large-scale training data, we carry out ex-
periments to test the effect of the JPFG model with different training data sizes. Experiment results are
shown in Figure 3. It shows that the JPFG model with both personal and skill connections always out-
perform the two baseline models. Impressively, our JPFG model using 20% training data outperforms
MaxEnt using 100% training data.
1
http://mallet.cs.umass.edu/
526
Figure 2: The performance of different methods for skill inference
Figure 3: The performance of JPFG with different training data sizes
6.4 Connections Contribution Analysis
Personal connections and skill connections can be also used to build the factor graph models to infer the
skills. We therefore want to compare our JPFG model with the factor graph model with only consider
the personal connections or skill connections, and analysis the contribution of each kinds of connection.
Specifically, MaxEnt-Personal employs the personal connections as additional features incorporated with
textual features to build the maximum entropy classification. FGM-Personal is a simplified version of
the JPFG model, which only employs textual attribute functions and personal connection factor functions
to build the factor graph model. Likewise, FGM-Skill only employs textual attribute functions and skill
connection factor functions to build the factor graph model. Table 3 shows the experiment results.
System P. R. F1.
MaxEnt 0.744 0.797 0.769
MaxEnt-Personal 0.758 0.812 0.783
FGM-Personal 0.765 0.817 0.790
FGM-Skill 0.704 0.967 0.815
JPFG 0.780 0.905 0.837
Table 3: The contribution of connections
From Table 3, we can observe that, 1) Both FGM-Personal and FGM-Skill outperform the baseline
527
MaxEnt approach. It shows that both personal connections and skill connections are helpful for skill
inference; 2) MaxEnt-Personal and FGM-Personal outperform the baseline MaxEnt approach, it show
that personal connections are helpful for inferring skills, and as considering the global optimization,
FGM-Personal is more effective; 3) FGM-Skill built on the skill connections is more effective than
MaxEnt-Personal and FGM-Personal, it show that skill connections are more useful than personal con-
nections; 4) JPFG model outperforms both FGM-Personal and FGM-Skill, it suggests that we should
incorporate both personal and skill connections to the factor graph model when we infer the skills from
profile.
7 Conclusion
In this study, we propose a novel task named personal skill inference, which aims to determine whether a
person takes a specific skill or not. To address this task, we propose a joint prediction factor graph model
with help of both personal and skill connections besides local textual information. Evaluation on a large-
scale dataset shows that our joint model performs much better than several baselines. In particular, it
shows that the performance on personal skill inference can be greatly improved by incorporating skill
connection information.
The general idea of exploring personal and skill connections to help predict people?s skills represents
an interesting research direction in social networking, which has many potential applications. Besides,
as skill information of a person is normally incomplete and fuzzy, how to better infer personal skills with
weakly labeled information is challenging.
Acknowledgements
This research work is supported by the National Natural Science Foundation of China (No. 61273320,
No. 61331011, and No. 61375073), National High-tech Research and Development Program of China
(No. 2012AA011102), Zhejiang Provincial Natural Science Foundation of China (No. LY13F020007),
the Humanity and Social Science on Young Fund of the Ministry of Education (No. 12YJC630170).
We thank Dr. Jie Tang and Honglei Zhuang for providing their software and useful suggestions about
PGM. We thank Prof. Deyi Xiong for helpful discussions, and we acknowledge Dr. Xinfang Liu, and
Yunxia Xue for corpus construction and insightful comments. We also thank anonymous reviewers for
their valuable suggestions and comments.
References
Balog K and M. Rijke. 2007. Determining Expert Profiles (With an Application to Expert Finding). In Proceedings
of IJCAI-07.
Campbell C, P. Maglio, A. Cozzi, and B. Dom. 2003. Expertise Identification Using Email Communications. In
Proceedings of CIKM-03.
Dong Y., J. Tang, S. Wu, J. Tian, N. Chawla, J. Rao, and H. Cao. 2012. Link Prediction and Recommendation
across Heterogeneous Social Networks. In Proceedings of ICDM-12.
Guo W., H. Li, H. Ji, and M. Diab. 2013. Linking Tweets to News: A Framework to Enrich Short Text Data in
Social Media. In Proceedings of ACL-13 .
Guy I., N. Zwerdling, I. Ronen, D. Carmel, and E. Uziel. 2010. Social Media Recommendation based on People
and Tags. In Proceedings of SIGIR-10 .
Hammersley J. and P. Clifford. 1971. Markov Field on Finite Graphs and Lattices, Unpublished manuscript.
Helic D. and M. Strohmaier. 2011. Building Directories for Social Tagging Systems. In Proceedings of CIKM-
2011.
Lafferty J, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting
and Labeling Sequence Data. In Proceedings of ICML-01.
528
Lampos V., D. Preo?iuc-Pietro, and T. Cohn. 2013. A User-centric Model of Voting Intention from Social Media.
In Proceedings of ACL-13.
Lappas T., K. Punera, and T. Sarlos. 2011. Mining Tags Using Social Endorsement Networks. In Proceedings of
SIGIR-11.
Li H., Z. Liu, and M. Sun. 2012. Random Walks on Context-Aware Relation Graphs for Ranking Social Tags. In
Proceedings of COLING-12.
Liu Z., X. Chen, and M. Sun. 2011. A Simple Word Trigger Method for Social Tag Suggestion. In Proceedings of
EMNLP-2011.
Liu Z., C. Tu, and M. Sun. 2012. Tag Dispatch Model with Social Network Regularization for Microblog User Tag
Suggestion. In Proceedings of COLING-12.
Lu Y., and P. Tsaparas, A. 2010. Ntoulas and L. Polanyi. 2010. Exploiting Social Context for Review Quality
Prediction. In Proceedings of WWW-10.
Luo T., J. Tang, J. Hopcroft, Z. Fang, and X. Ding. 2013. Learning to Predict Reciprocity and Triadic Closure in
Social Networks. ACM Transactions on Knowledge Discovery from Data. vol.7(2), Article No. 5.
Murphy K., Y. Weiss, and M. Jordan. 1999. Loopy Belief Propagation for Approximate Inference: An Empirical
Study. In Proceedings of UAI-99 .
Ohkura T., Y. Kiyota and H. Nakagawa. 2006. Browsing System for Weblog Articles based on Automated Folk-
sonomy. In Proceedings of WWW-06.
Si X., Z. Liu, and M. Sun. 2010. Explore the Structure of Social Tags by Subsumption Relations. In Proceedings
of COLING-10.
Soboroff I., A. Vries and N. Craswell. 2006. Overview of the TREC 2006 Enterprise Track In Proceedings of
TREC-06.
Turney P. 2002. Thumbs up or Thumbs down? Semantic Orientation Applied to Unsupervised Classification of
reviews. In Proceedings of ACL-02.
Tan C., L. Lee, J. Tang, L. Jiang, M. Zhou, and P. Li. 2011. User-Level Sentiment Analysis Incorporating Social
Networks. In Proceedings of KDD-11.
Tang W., H. Zhuang, and J. Tang. 2011a. Learning to Infer Social Ties in Large Networks. In Proceedings of
ECML/PKDD-11.
Tang J., Y. Zhang, J. Sun, J. Rao, W. Yu, Y. Chen, and A. Fong. 2011b. Quantitative Study of Individual Emotional
States in Social Networks. IEEE Transactions on Affective Computing. vol.3(2), Pages 132-144.
Tang J., S. Wu, J. Sun, and H. Su. 2012. Cross-domain Collaboration Recommendation. In Proceedings of KDD-
12.
Tang J., S. Wu, and J. Sun. 2013. Confluence: Conformity Influence in Large Social Networks. In Proceedings of
KDD-13.
Xing E, M. Jordan, and S. Russell. 2003. A Generalized Mean Field Algorithm for Variational Inference in Expo-
nential Families. In Proceedings of UAI-03.
Yang S., B. Long, A. Smola, N. Sadagopan, Z. Zheng, and H. Zha. 2011a. Like like alike - Joint Friendship and
Interest Propagation in Social Networks. In Proceedings of WWW-11.
Yang Z., K. Cai, J. Tang, L. Zhang, Z. Su, and J. Li. 2011b. Social Context Summarization. In Proceedings of
SIGIR-11.
Zhang J., J. Tang, and J. Li. 2007a. Expert Finding in A Social Network. In Proceedings of the Twelfth Database
Systems for Advanced Applications (DASFAA-2007).
Zhang J., M. Ackerman, and L. Adamic. 2007b. Expertise Networks in Online Communities: Structure and Algo-
rithms. In Proceedings of TREC-07.
Zhuang H, J. Tang, W. Tang, T. Lou, A. Chin, and X. Wang. 2012. Actively Learning to Infer Social Ties. In
Proceedings of Data Mining and Knowledge Discovery (DMKD-12), vol.25 (2), pages 270-297.
529
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2161?2171, Dublin, Ireland, August 23-29 2014.
Employing Event Inference to Improve Semi-Supervised Chinese 
Event Extraction 
 
 
Peifeng Li, Qiaoming Zhu, Guodong Zhou 
School of Computer Science & Technology 
Soochow University, Suzhou, 215006, China 
{pfli, qmzhu, gdzhou}@suda.edu.cn 
 
 
 
Abstract 
Although semi-supervised model can extract the event mentions matching frequent event patterns, it suf-
fers much from those event mentions, which match infrequent patterns or have no matching pattern. To 
solve this issue, this paper introduces various kinds of linguistic knowledge-driven event inference 
mechanisms to semi-supervised Chinese event extraction. These event inference mechanisms can capture 
linguistic knowledge from four aspects, i.e. semantics of argument role, compositional semantics of trig-
ger, consistency on coreference events and relevant events, to further recover missing event mentions 
from unlabeled texts. Evaluation on the ACE 2005 Chinese corpus shows that our event inference mech-
anisms significantly outperform the refined state-of-the-art semi-supervised Chinese event extraction 
system in F1-score by 8.5%. 
1 Introduction 
An event is a specific occurrence involving arguments (participants and attributes) of the specific roles. 
In an event, trigger is the main word which most clearly expresses its occurrence, so recognizing an 
event can be recast as identifying a corresponding trigger. An event may have several arguments, 
which are entity mentions (e.g., person name, time, location, etc.) and must fulfill the corresponding 
roles. Take the following sentence as an example: 
S1: On the 25th Dec. (A1: Artifact), peacekeepers (A2: Artifact) returned (E1: Transport) to Am-
man (A3: Place) by flight (A4: Vehicle). 
For this example, an event extraction system should identify one event mention E1, which is trig-
gered by verb ?returned? whose event type is Transport, with four arguments, ?peacekeepers?, ?25th 
Dec.?, ?flight?, and ?Amman?, fulfilling the roles of Artifact, Time, Vehicle, and Place, respectively. 
Automatically extracting events from free texts is a higher-level Information Extraction (IE) task, 
which is still a challenge due to the complexity of natural language and the domain-specific nature, 
especially in Chinese for its specific characteristics. In particular, most of previous studies have fo-
cused on English event extraction, while only a few concern Chinese. 
Currently, supervised learning models have dominated event extraction. To reduce the labeled data 
required, a few semi-supervised models have been applied to English event extraction (e.g., Riloff 
1996; Yangarber et al., 2000; Stevenson and Greenwood, 2005; Huang and Riloff, 2012). Since classi-
fier-based model needs dozens of annotated documents to train model, most of previous semi-
supervised models focused on pattern-based approach, which only needed a few seed (event) patterns. 
In those pattern-based approaches, frequent event patterns, which occur in many documents, were 
chosen as relevant patterns to match event mentions in unlabeled texts. However, the order of words in 
a Chinese sentence is rather agile for its open and flexible structure, and different orders might express 
the same meaning due to the semantics-driven nature of the Chinese language. This results in the di-
versity of Chinese event patterns and numerous infrequent patterns, even some event mentions having 
no matching patterns. Hence, it is an issue to extract the event mentions with infrequent patterns. 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 
2161
In this paper, we first implement a pattern-based semi-supervised model for Chinese event extrac-
tion as a baseline, following the state-of-the-art system as described in (Liao and Grishman, 2010a) 
and then refine this model to suit Chinese event extraction. Moreover, we propose various kinds of 
novel linguistic knowledge-driven event inference mechanisms to address the above issue and recover 
missing event mentions. These event inference mechanisms can capture the linguistic knowledge from 
semantics of argument role, compositional semantics of trigger, consistency on coreference events and 
relevant events. Evaluation on the ACE 2005 Chinese corpus shows that our event inference mecha-
nisms dramatically outperform the baseline. 
The rest of this paper is organized as follows. Section 2 overviews related work. Section 3 presents 
the refined semi-supervised model for Chinese event extraction. Section 4 proposes several linguistic 
knowledge-driven event inference mechanisms. Section 5 reports and analyzes the experimental re-
sults. Finally, we conclude our work in Section 6. 
2 Related Work 
Almost all previous semi-supervised models focus on English event extraction, which can be subdi-
vided into pattern-based models (e.g., Riloff, 1996; Yangrber et al., 2000; Liao and Grishman, 2010a; 
Chambers and Jurafsky, 2011; Balasubramanian et al., 2013) and classifier-based models (e.g., Chieu 
et al., 2003; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009; Liu and Strzalkowski, 2012; 
Wang et al., 2013). Classifier-based models normally require a small set of annotated data (e.g., 100 
annotated documents), while pattern-based models need dozens of high quality seed patterns. 
Riloff (1996) first divided unlabeled documents into irrelevant and relevant documents, and the lat-
ter was much likely to contain further relevant patterns. Then event patterns from relevant documents 
were generated by using an annotated data and a set of heuristic rules. Yangarber et al. (2000) pro-
posed a document-centric view to boost a semi-supervised event extraction system, which assumes 
relevant documents always contain some shared patterns. Yangarber (2003) further introduced multi-
ple learners into the bootstrapping procedure to make the final decision on the combination of multiple 
learners on distinct event types. Huang and Riloff (2012) employed role-identifying nouns, which pro-
posed by Phillips and Riloff (2007), as seed terms to extract patterns from relevant documents and 
then generated the labeled instances to train three classifiers in their event extraction system. 
As an alternative, Stevenson and Greenwood (2005) proposed a pattern similarity-centric view and 
selected relevant patterns on similarity scores. Normally, bootstrapping on the document-centric view 
tends to accept the irrelevant patterns with a high occurrence frequency in relevant documents. To ad-
dress this problem, Liao and Grishman (2010a) introduced a pattern similarity metric into the docu-
ment-centric view as a filter to eliminate those irrelevant patterns. Liao and Grishman (2011) further 
applied an information retrieval mechanism to detect relevant documents and proposed a self-training 
strategy for bootstrapping. 
In addition, several studies focused on the event pattern representation, such as pairwise (e.g., Sub-
ject-Verb, Verb-Object) (Chambers and Jurafsky, 2008, 2009), SVO (Subject-Verb-Object) 
(Yangarber, 2000; Balasubramanian et al., 2013), chain (Sudo et al., 2001), subtree (Sudo et al., 2003) 
and complex pattern (Liu and Strzalkowski, 2012). 
In the literature, only one paper concerns semi-supervised Chinese event extraction. Chen and Ji 
(2009a) applied various kinds of cross-lingual features in the bootstrapping procedure to extract Chi-
nese event. With the help of over 500 annotated seed event mentions in 100 documents, they only 
achieved 35% in F1-score. This indicates the critical challenge in semi-supervised Chinese event ex-
traction. 
Only a few studies concern event inference mechanisms. Ji and Grishman (2008) employed a rule-
based approach to propagate consistent triggers and arguments across topic-related documents. Liao 
and Grishman (2010b) employed cross-event consistent information to improve sentence-level event 
extraction. Hong et al. (2011) regarded entity type consistency as a key feature to predict event men-
tions and adopted an information retrieval mechanism to promote event extraction. Li et al. (2013) 
proposed a global argument inference model on Chinese argument extraction to explore specific rela-
tionships among relevant event mentions to recover those inter-sentence arguments in the sentence, 
discourse and document layers. Li et al. (2014) also introduced Markov Logic Network (MLN) to cap-
ture the discourse-level consistency between Chinese trigger mentions to further recover those poor-
2162
context event mentions. In a word, all of above mechanisms focus on supervised event extraction and 
no literature involves in the event inference of semi-supervised event extraction. 
3 Semi-supervised Model for Chinese Event Extraction 
In this section, we refine a semi-supervised model for Chinese event extraction as a baseline, which 
includes two views, the document-centric view and pattern similarity-centric view. 
3.1 Semi-supervised Model 
Liao and Grishman (2010a) proposed a state-of-the-art semi-supervised event extraction system, 
which was a pattern-based approach and adopted bootstrapping mechanism to extract relevant patterns. 
Besides, two distinct views, the document-centric view and the pattern similarity-centric view as de-
scribed in Subsection 3.2 and 3.3, are incorporated in the bootstrapping procedure to rank event pat-
terns on different metrics. In each iteration, the candidate patterns, which extracted from unlabeled 
texts as the candidates of relevant patterns, are ranked following the document-centric view, then the 
candidate patterns with pattern similarity scores below a similarity threshold (0.9 in (Liao and Grish-
man, 2010a)) will be removed; only top 3 candidate patterns in the ranking scores of the document-
centric view will be accepted as relevant patterns. In addition, if no pattern is found in the current iter-
ation, the threshold will be reduced by 0.1 until new relevant patterns are extracted. 
As we mentioned earlier, the open and flexible structure of Chinese sentences results in the diversi-
ty of Chinese event patterns. Moreover, the syntax or semantic path is often used to represent event 
patterns, but the performance in Chinese syntactic parsers and Semantic Role Labeling (SRL) tools is 
lower than that in English. Therefore, we refine this semi-supervised model to suit Chinese event ex-
traction in three aspects as follows, due to the above characteristics of Chinese language. 
Firstly, we construct a refined event pattern representation of Chinese events. Liao and Grishman 
(2010a) used semantic roles to represent the relationship between the trigger and its arguments. Due to 
the wide spread of ellipsis (especially entities) and the relatively low performance of Chinese SRL, 
pairwise (trigger-entity) representation and dependency path are introduced to represent Chinese event 
pattern in our refined model. Hence, the event pattern in this paper is a triple-style template as follows. 
<trigger, entity type, their dependency path > 
A pattern is formed by a trigger, the entity type of its argument1 and the dependency path from the 
trigger to the argument. For example, trigger ?returned? and its argument ?peacekeepers? (entity type: 
PER) in sentence S1 can be described as a pattern <returned, PER, nsubj>. 
Secondly, we introduce a novel mechanism to extract candidate patterns. Since verb and noun dom-
inate in triggering an event in Chinese and they are chosen as candidate triggers to create candidate 
patterns. Besides, since different event types may have different roles and different roles are fulfilled 
by entities with different types, the entities whose types can fulfil the core roles of a specific event are 
chosen as candidate entities. For example, Attacker and Target are the core roles of event Attack and 
entity types PER/ORG/GPE2 can fulfil above two roles, so we only accept those entities, whose types 
belong to PER/ORG/GPE, to form candidate patterns. For each sentence in the unlabeled data, all can-
didate trigger-entity pairs and their dependency path are enumerated as candidate patterns. 
Finally, we present a new mechanism to generate seed patterns based on seed triggers. Considering 
the relatively large number of Chinese triggers and the flexibility of Chinese sentences, an instance-
based approach is adopted by enumerating a few high-quality seed triggers with explicit meaning and 
high probability to trigger a specific event. Instead of dozens of predefined patterns required in previ-
ous studies, only one seed trigger is given to each event type or subtype without any predefined pat-
terns. Hence, all patterns consisting of a seed trigger in the candidate patterns are accepted as seed pat-
terns for their high probability to trigger a specific event. 
3.2 Document-centric View 
The document-centric view regards those documents containing the patterns always identified as rele-
vant to a specific event as relevant documents and concludes that they are likely to contain additional 
                                                 
1 All event arguments must be entity mentions following the ACE 2005 annotation guidelines of events. 
2 PER/ORG/GPE refers to person, organization and geo-political entity respectively, which are annotated in the ACE 2005 
corpus. These helpful information can be seen as ontological classes. 
2163
relevant patterns. Hence, those candidate patterns occurring in the relevant documents frequently will 
be extracted as relevant ones. Following Yangarber et al. (2000) and Liao and Grishman (2010a), we 
also employ the disjunctive voting scheme to calculate the ranking scores Rscore(p) of pattern p as fol-
lows. 
 
?
?
?
?
)p(Ld
)p(Ld
Score )d(lRelog*)p(L
)d(lRe
)p(R =
                                                   (1) 
 
where L(p) is the set of documents, which contain candidate pattern p, and Rel(d) is the relevance 
score of document d as follows. 
 
?
?
?
?--
Pp
)p(Ld
'
)
)p(L
)d(lRe
1(1)d(lRe =                                                          (2) 
 
where Rel?(d) is the relevance score of document d in the previous iteration. Initially the relevance 
score of document d is set to n if document d has n relevant patterns in the set of extracted patterns P. 
3.3 Pattern Similarity-centric View 
The similarity-centric view tries to find the candidate patterns who are similar to those seed patterns. 
The similarity scores derive from two aspects, lexical similarity and syntactic similarity, while the 
former is based on the trigger and entity type in a pattern and the latter is based on the relation be-
tween the trigger and the entity. Especially, we realize the pattern similarity view following the lexical 
and syntactic similarity, and refine the similarity ranking score Iscore(p) of candidate pattern p as fol-
lows: 
 
)d,d(DSim)e,e(ESim)t,t(WSim(Max)p(I spspspPsscore ??= ?
                      (3) 
 
where t, e and d represent the trigger, entity type and dependency path in candidate pattern p(tp, ep, dp) 
or seed pattern s(ts, es, ds) in the set of extracted patterns P, respectively; ESim identifies whether two 
entities have the same type, and assigned 1 if two entities have the same entity type and otherwise a 
small number 0.1; DSim calculates the similarity between two dependency paths in edit distance. Fi-
nally, WSim is to obtain the trigger similarity in lexical semantics, using Hownet (Dong and Dong, 
2006) following Liu and Li (2002): 
 
?
?
?? ),(),( spsp ttDisttWSim
                                                                (4) 
 
where Dis(tp,ts) is the distance between the sememes of triggers tp and ts, in HowNet?s sememe hierar-
chical architecture, with parameter ? assigned 0.75 following Liu and Li (2002). 
4 Event Inference 
The pattern-based semi-supervised model cannot extract those event mentions matching infrequent 
patterns or without matching patterns. The knowledge from linguistic aspect (e.g., definition of events, 
compositional semantics of Chinese words, coreference events and relevant events, etc.) is helpful to 
further recover missing event mentions or filter pseudo event mentions. In this section, various kinds 
of event inference mechanisms based on linguistic knowledge are proposed to improve the perfor-
mance of semi-supervised Chinese event extraction. 
We unify the semi-supervised model and the event inference mechanisms into one model as follows: 
In each iteration, after the top 3 patterns have been chosen following the document-centric view and 
event mentions in the unlabeled data have been extracted by pattern matching, all event inference 
2164
mechanisms are applied to recover missing event mentions,. Due to our inference mechanisms are 
trigger-based and each inferred event mention may have more than one pattern while most of them are 
noisy, we do not add those patterns in the set of relevant patterns for bootstrapping. 
4.1 Event Inference on Role Semantics 
The core of an event can be expressed as ?Who do What to Whom? in which ?Who? and ?Whom? are 
the core roles3 to participate in an event, while ?What? often refers to event trigger. The relationship 
between the verbal trigger and its core roles are the key clues to express event semantics. Since the 
subject or object always play the core roles in an event mention, SVO (Sbject-Verb-Object) is a better 
representation of event pattern. However, ellipsis is a widespread phenomenon in Chinese language 
and many sentences do not have an overt subject or object, so lots of event mentions cannot be repre-
sented as SVO pattern. In this paper, we only use the trigger-entity pair to represent event pattern and 
one of the disadvantages of this representation is its loose constraint on events, which will extract lots 
of pseudo event mentions. 
In most cases in Chinese, the object is often the most important core role to identify a specific event 
and it is more helpful than the subject to distinguish true event mentions from pseudo ones. Take fol-
lowing two sentences as examples: 
S2: ??(PER) ?(hit)? ????(PER)?(The teacher hit this student.) 
S3: ??(PER) ?(call)? ?? ? ????(PER)?(The teacher made a phone call to this stu-
dent.) 
The relation between verb ? (hit) and object???? (this student) is clear to indicate sentence 
S2 is an Attack event mention since the object is a person, while object ?? (phone) in sentence S3 is 
not a person and it indicates this sentence is not an Attack event mention following the sense of verb 
? (call). Therefore, the object is an effective evidence to indicate event mentions and it is incorpo-
rated in our model to remove pseudo event mentions as follows. 
Role Semantics: If the object of a candidate verbal trigger mention is not an entity or its entity type 
cannot fulfil the object roles (e.g., Victim in events Injure and Die) in a specific event, this candidate 
trigger mention4 will be inferred as pseudo one. 
For example, core role Target of event Attack often acts as the object of a verbal trigger and entity 
types PER, ORG and GPE can fulfill this role according to be definition of event Attack in the ACE 
2005 corpus. Hence, a candidate trigger mention of event Attack will be regarded as pseudo one when 
this mention has an object which is not an entity or whose entity type is not PER, ORG or GPE. 
4.2 Event Inference on Compositional Semantics 
In Chinese language, a word is composed of one or more characters. Almost all Chinese characters 
have their own meanings and are morpheme (or single-morpheme word), the minimal meaningful unit. 
If a Chinese word contains more than one character, its meaning can often be derived from its compo-
site morphemes. This more fine-grained semantics is compositional semantics of Chinese words. Ac-
tually, it is also a normal way for a native Chinese speaker to understand a new Chinese word. 
Two-morpheme words are used widely in Chinese language and almost all Chinese triggers contain 
one or two morphemes. The compositional semantics of a two-morpheme word comes from both its 
morphemes and morphological structure. Besides morphological structure Coordination, all other 
morphological structures (e.g., Modifier-Head, Predicate-Object, Predicate-Complement (Li and zhou, 
2012)) always have one head morpheme, the morpheme as the governing semantic element, to express 
the meaning of a word. Commonly, there are two head morphemes in a two-morpheme word of Coor-
dination structure. In particular, a two-morpheme word triggers an event if its two head morphemes 
are homogeneous (e.g., ?(attack)?(attack), ?(die)?(die)). Otherwise, it may refer to more than one 
event and this means that two triggers are within a word whose morphological structure is Coordina-
tion. Take the following sentence as an example: 
                                                 
3 We select core roles following the ACE Chinese annotation guidelines of events. Agent/Victim are the core roles of events 
Die/Injure while Attacker/Target are the core roles of event Attack. 
4 Recognizing a trigger mention can be recast as identifying a corresponding event mention, since trigger is the main word 
which most clearly expresses the occurrence of an event. 
2165
S4: ?????(E2: Attack)?(E3: Die)?????(A younger stabbed (E2: Attack) a woman to 
death (E3: Die).) 
In S4, two-morpheme word ?? (stab a person to death) is a trigger with the Coordination struc-
ture. There are two event mentions in sentence S4, one Attack (E2) and one Die (E3), while morpheme
? (stab) triggers an Attack event and ? (die) refers to a Die one.  
Almost all event extraction systems assigned only one event type to a trigger and this will lead to 
that the other event type does not have any patterns to match and then cannot be identified. To address 
this issue, we first identify those triggers who refers to two distinct events as follows: for each two-
morpheme candidate trigger in the candidate patterns whose morphemes are m1 and m2, it will be iden-
tified as candidate trigger with two event types and split into two single-morpheme word to generate 
two candidate trigger mentions when the following three conditions are satisfied: 
1) )m(POSverb)m(POSverb 21 ???  
2) 
)s(Etype)s(Etype))s,m(Wsim())s,m(Wsim( MaxMax seedssseedss 212211 11 21 ??? ?? ??
 
3) Morph(m1 m2)= Coordination 
where POS(m) returns all possible parts of speech of morpheme m in Hownet and Etype(s) is to obtain 
the event type of seed trigger s; WSim(m,s) is defined in Subsection 3.3 and returns 1 when one word 
m is the synonym of the other word s; Morph(w) is to obtain the morphological structure of word w 
following Li and Zhou (2012). 
Since there is a strong trigger consistency in those two-morpheme words of Coordination structure 
which refers to two distinct events, we propose an event inference mechanism as follows. 
Compositional semantics: For each two-morpheme word identified by the above three conditions, 
if one of its morphemes has been extracted as an trigger mention of a specific event type, the other 
morpheme in the same word will refer to an a relevant event type. 
4.3 Event Inference on Coreference Events 
To mine more event mentions, we use the simple trigger-entity pair to represent event pattern in this 
paper. However, lots of event mentions still cannot be extracted due to the ellipsis of arguments. Take 
following sentences as examples: 
S5: ?????????????(E4: Meeting)?(The US and DPRK finished talking (E4: 
Meeting) in Kuala Lumpur.) 
S6: ??(E5: Meeting)??????(The talks (E5: Meeting) are serious.) 
Obviously, more than one pattern of event mention E4 can be generated from sentence S5, since it 
contains more than one entity. On the contrary, no pattern can be extracted from S6 and this leads to 
event mention E5 cannot be extracted in our pattern-based semi-supervised model. 
Within a document, almost all event mentions are around a topic and there is a strong trigger con-
sistency: if one mention of a word triggers a specific event, its other mentions in the same document 
will refer to the same event type. Besides, similar words (e.g., ? (bomb), ?? (bomb), ?? (bomb)), 
which contains the same head morpheme, always express the same or similar meaning following the 
principle of compositional semantics. Similarly, there is a strong trigger consistency on those similar 
words: If one mention of a word refers to a specific event, the mentions of its similar words in the 
same document will trigger events of the same type. 
Since the mentions of the same word or similar words are often coreference ones and always refer 
to the same event type, we propose an event inference mechanism on coreference events to recover 
missing event mentions based on head morpheme as follows. In particular, head morphemes are also 
identified following Li and Zhou (2012). 
Coreference events: 1) if a mention of a candidate trigger refers to a specific event, all its other 
mentions in the same document will trigger the same type event; 2) if one mention of a candidate trig-
ger refers to a specific event, all the mentions of its similar words in the same document will trigger 
the same type event too. 
4.4 Event Inference on Relevant Events 
The bootstrapping procedure of the document-centric view selects frequent patterns in relevant docu-
2166
ments and ignores those infrequent patterns both in relevant or irrelevant documents. However, the 
number of infrequent patterns in Chinese is larger than that in English, due to its open and flexible 
sentence structure, as mentioned in Subsection 3.1. 
Besides the pattern-based semi-supervised model, we propose a trigger-based mechanism as a sup-
plement to recover those missing event mentions concerning infrequent patterns following this as-
sumption: if a trigger mention refers a specific event in a document, there is a high probability that its 
relevant events occur in the same document. Take the following sentence as an example: 
S7: ???(E6: Attack)??? 1???????(E7: Die)?(An Arabian was dying (E7: Die) in 
this conflict (E6: Attack).) 
In sentence S7, there is an extracted Die event mention E7 triggered by ?? (die) and ?? (con-
flict) is a candidate trigger mention. If there is an evidence that ?? (conflict) triggers an Attack event 
in the other documents, it is possible to identify ?? (conflict) as a trigger mention of Attack event in 
S7 for the high probability that events Die and Attack occur in the same document. We propose an in-
ference mechanism on relevant events as follows. 
Relevant Events: If a trigger mention is identified in a document, each candidate trigger mention in 
the same document will be recognized as true ones when it satisfies the following condition: this can-
didate trigger occurs in the other documents as an event trigger and refers to the relevant events of this 
identified trigger mentions.  
Since the seed triggers have a high probability to trigger a specific event, to further explore those 
missing event mentions, we expand this inference mechanism following compositional semantics in 
Chinese and expand the condition as follows: This candidate trigger occurs in the other documents as 
an event trigger or contains one of the seed triggers, which refers to the relevant events of this identi-
fied trigger mentions. 
5 Experimentation 
In this section, we systematically evaluate our event inference mechanisms on the ACE 2005 Chinese 
corpus and provide the analysis. 
5.1 Experimental Setting 
The ACE 2005 Chinese corpus is the only available corpus in Chinese event extraction and it is used 
in all our experiments. This corpus contains 633 documents annotated with 33 predefined types. Due 
to evaluation on all 33 types is a hard work for the time-consuming bootstrapping procedure and the 
diversity of distinct event types, most of previous works selected part of event types for evaluation. In 
this paper, 3 event types (i.e. Die, Injure and Attack) are selected for evaluation, because they reflect 
the relevance of different event types and occur at different frequencies in the corpus. While events 
Die and Injure are easy to define, event Attack is rather complicated and can be divided into several 
subtypes. In the ACE 2005 Chinese corpus, almost one third of the annotated event mentions belong to 
the above three event types. Moreover, we report the experimental results on all 33 event types to fur-
ther verify the effectiveness of our inference mechanisms in Subsection 5.2. 
Unlike MUC shared task, which only distinguishes whether a sentence contains a specific event 
mention or not, we follow previous studies on the ACE 2005 corpus and report the performance of 
trigger-based event extraction: a trigger is correctly identified if its position and event type match a 
reference trigger. As for evaluation, we use the ground truth entities, time and values annotated in the 
ACE 2005 Chinese corpus, and report the micro-average Precision (P), Recall (R) and F1-score (F1). 
Table 1 shows the seed triggers for the three event types. For example, only one seed trigger is pro-
vided for either the Die or Injure event, while three seed triggers are given for event Attack. Since the 
Attack event contains several distinct event subtypes, we assign one seed trigger to each of its major 
subtypes. Thus, all patterns whose triggers belong to the set of seed triggers are accepted as seed pat-
terns automatically. 
 
Type Die Injure Attack 
Seed triggers ?(die) ?(injure) ??(attack), ??(conflict), ?(hit) 
Table 1. Seed triggers of Die, Injure and Attack event types 
2167
Besides, all the sentences in the corpus are divided into words using a Chinese word segmentation 
tool (ICTCLAS) with all entities annotated in the corpus kept. We use Berkeley Parser and Stanford 
Parser to create the constituent and dependency parse trees. 
5.2 Experimental Results 
To verify the performance of our event inference mechanisms, it is compared with the refined baseline, 
a supervised model for Chinese event extraction. Table 2 shows the results of our event inference 
mechanisms with peak recall, precision and F1-score, following Liao and Grishman (2010a). Com-
pared with the baseline, Table 2 shows that our event inference mechanisms improve the F1-score of 
Chinese event extraction by 8.5%, largely due to the improvement of 11.8% in recall. These results 
confirm the effectiveness of our event inference mechanisms in recovering missing event mentions. 
The disadvantage of our event inference mechanisms is the fact that it will also introduce some pseudo 
event mentions into our model and harm the precision. Additionally, there is still a big performance 
gap between our model and the supervised model and this leaves much room for future research. 
 
Approach Attack Injure Die All (micro-average) 
P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 
Baseline 71.4 36.6 48.4 93.2 41.7 57.6 90.1 44.0 59.3 79.7 39.4 52.7 
+Event inference 70.9 47.5 56.9 83.2 54.6 65.9 80.8 57.2 67.0 75.5 51.2 61.2 
Supervised model 70.4 72.5 71.4 85.3 78.4 81.7 83.9 92.9 88.1 77.2 78.4 77.8 
Table 2. Performance of event inference mechanisms in Chinese event extraction (Attack/Injure/Die). 
 
Table 2 also indicates the performance difference of our inference mechanisms for distinct event 
types. Among all event types, event Attack achieves the highest improvement (8.5%) in F1-score, with 
a dramatic improvement of 10.9% in recall and a less loss of 0.5% in precision. Event Die and Injure 
also gain a significant improvement of 7.7% and 8.3% in F1-score respectively, largely due to the in-
crease in recall, while their precisions reduce rapidly due to those pseudo event mentions inferred by 
our inference mechanisms. However, the loss of precision of event Attack is much less than these of 
events Die and Injure. The reason is that the inference on role semantics mainly impacts on Attack 
events to remove pseudo event mentions. 
To well evaluate different approaches, it is better to compare them on different corpora. Since the 
ACE 2005 Chinese corpus is the only available corpus in Chinese event extraction, we divide it into 
three sub-corpora according to data sources, i.e. Broadcast News, Newswire and WebLog, which are 
much different in various aspects, such as quality, length and style. Figure 1 compares the perfor-
mance of different models on different sub-corpora. It indicates that our event inference mechanisms 
perfect better than the baseline in all three sub-corpora and that results confirm the huge influence of 
the event inference mechanisms. It also shows that the WebLog sub-corpus reports the worst F1-score 
due to the low document quality and the low percentage of relevant documents, and that the Newswire 
sub-corpus reports significantly better performance than the Broadcast News sub-corpus due to its 
spoken nature. 
 
Figure 1. Performance comparison (F1-score) on different data sources. 
To further verify the effectiveness of our event inference mechanisms, we evaluate them on all 33 
event types. Due to event extraction is a domain-specific task, distinct event types have the different 
seed triggers and different pro-process procedures. In this paper, we just report the final results for the 
50.9 57.8 
36.9 
59.6 65.7 
44.8 
0
50
100
Broadcast news Newwise WebLog
Baseline Baseline+Event Inference
2168
sake of brevity. Table 3 shows the experimental results on all 33 event types and it ensures that our 
mechanisms are effective on extracting all event types. Compared with the baseline, our approach im-
proves the F1-score by 7.6%, which is less than that reported in Table 2. Among all 33 event types, the 
performances of almost all event types associated with justice are higher than other event types for 
their unambiguous definitions and high coverage of seed triggers while event Transport achieves the 
lowest performance for its complexity and low coverage of seed triggers. Besides, the performance on 
all event types is lower than that on 3 event types and this result comes from the low performance of 
the Transport event which occupies almost 20% of all annotated event mentions in the ACE 2005 
Chinese corpus. 
Approach P(%) R(%) F1 
Baseline 70.7 34.2 46.1 
+Event inference 65.2 45.7 53.7 
Table 3. Performance of event inference mechanisms in Chinese event extraction (All 33 event types). 
5.3 Analysis on Event Inference Mechanisms 
Table 4 shows the contributions of the different event inference mechanisms. It is worthy to mention 
that an event mentions may be identified by both the semi-supervised model and the event inference 
mechanisms. In this paper, we attribute those extracted event mentions to the former and the contribu-
tion of our inference mechanisms is greater than those in Table 4. 
 
Inference P(%) R(%) F1 
Baseline 79.7 39.4 52.7 
+Inference on role semantics (RS) 87.5(+7.8) 39.1(-0.3) 54.1(+1.4) 
+Inference on compositional semantic (CS) 85.7(+6.0) 43.7(+4.3) 57.8(+3.7) 
+Inference on coreference events (CE) 83.0(+3.3) 45.8(+6.4) 59.0(+1.2) 
+Inference on relevant events (RE) 75.7(-4.0) 51.3(+11.9) 61.2(+2.2) 
Table 4. The contribution of event inference on Chinese event extraction. 
 
Actually, inference mechanism RS is a filter to remove those pseudo event mentions and it can im-
prove the precision (+7.8%), with a less lost (-0.3%) in recall. Moreover, it can also help the seed pat-
tern generation to generate high quality seed patterns. Table 5 shows the contribution of RS on seed 
pattern generation and we report the result of Chinese event extraction which only uses the seed pat-
terns5. It improves the accuracy from 75.8% to 82.5%, largely due to the decline (-30) in the set of 
pseudo event mentions. These results indicate that the object is a key clue to identify event mentions. 
 
Method #True event mentions #Pseudo event mentions 
w/o RS 273 87 
w/ RS 269 57 
Table 5. The contribution of RS on seed pattern generation. 
 
Chen and Ji (2009b) have reported that almost 13% of Chinese triggers are in-word or cross-words 
and this figure ensures it is an important issue. Inference mechanism CS gains the highest improve-
ment (+3.7%) in F1-score and this result indicates that compositional semantics is an effective way to 
solve such issue. The accuracy of this inference mechanism is very high (~92%) and most of the ex-
ceptions need the help of deep semantics since these instances are also hard to be distinguished by 
humans without the context. 
Inference mechanisms CE and RE improve the F1-scores by 1.2% and 2.2% respectively. CE as-
sumes all mentions of a word in a document only have one sense and it will introduce lots of pseudo 
event mentions to reduce precision. The experimental results also show that RE is an effective sup-
plement of the document-centric view to mine event mentions. Although they derive from the similar 
                                                 
5 Since sometimes a pattern can infer both true event mentions and pseudo event mentions, it is hard to identify whether a 
pattern is relevant or irrelevant without the test data. Hence, we compare their extracted event mentions in this paper. 
2169
principle of occurrence of relevant events, they focus on different perspectives where RE is trigger-
based and the document-centric view is pattern-based. RE ignores the difference on patterns and iden-
tifies event mentions on the occurrence of their relevant event mentions. In addition, sense shifting of 
Chinese words in different contexts is the main factor to extract lots of pseudo event mentions and 
then reduce the precision rapidly. 
It?s obvious that these inference mechanisms interact with others. In particular, almost 20% event 
mentions can be inferred by both CE and RE for the transitivity of event inference on coreference and 
relevant events. Besides, RS is not only beneficial to the semi-supervised model, but also helpful to 
the other inference mechanisms to further remove pseudo event mentions. 
6 Conclusion 
This paper proposes various kinds of novel linguistic knowledge-driven event inference mechanisms 
as a supplement of the semi-supervised Chinese event extraction to recover missing event mentions. 
The experimental results verify their effectiveness to extract the event mentions with infrequent pat-
terns or without matching pattern. Although this paper focuses on Chinese language, most of the event 
inference mechanisms are language-independent and can be applied to other languages. Our future 
work will focus on how to apply our event inference mechanisms to other languages and introduce 
more effective inference mechanisms to further improve the performance of semi-supervised event 
extraction. 
Acknowledgments 
The authors would like to thank three anonymous reviewers for their comments on this paper. This 
research was supported by the National Natural Science Foundation of China under Grant No. 
61331011 and No. 61272260, the National 863 Project of China under Grant No. 2012AA011102. 
Reference 
Niranjan Balasubramanian, Stephen Soderland, Mausam and Oren Etzioni. 2013. Generating Coherent Event 
Schemas at Scale. In Proc. EMNLP 2013, pages 1721-1731, Seatle, WA. 
Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised Learning of Narrative Event Chains. In Proc. ACL-
HLT 2008, pages 787-797, Hawaii. 
Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised Learning of Narrative Schemas and Their Partici-
pants. In Proc. ACL 2009, pages 602-610, Columbus, OH. 
Nathanael Chambers and Dan Jurafsky. 2011. Template-Based Information Extraction without the Templates. In 
Proc. ACL 2011, pages 976-986, Portland, OR. 
Hai Leong Chieu, Hwee Tou Ng and Yoong Keok Lee. 2003. Closing the Gap: Learning-based Information 
Extraction Rivaling Knowledge-Engineering Methods. In Proc. ACL 2003, pages 216-230, Sapporo, Japan. 
Zheng Chen and Heng Ji. 2009a. Can One Language Bootstrap the Other: A Case Study on Event Extraction. In 
Proc. NAACL-HLT 2009 Workshop on Semi-supervised Learning for Natural Language Processing, pages 
66-74, Boulder, CO. 
Zheng Chen and Heng Ji. 2009b. Language Specific Issue and Feature Exploration in Chinese Event Extraction. 
In Proc. NAACL-HLT 2009, pages 209-212, Boulder, CO. 
Zhengdong Dong and Qiang Dong. 2006. HowNet and the Computation of Meaning. World Scientific Pub Co. 
Inc. 
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, Guodong Zhou and Qiaoming Zhu. 2011. Using Cross-Entity 
Inference to Improve Event Extraction. In Proc. ACL 2011, pages 1127-1136, Portland, OR. 
Ruihong Huang and Ellen Riloff. 2012. Bootstrpped Training of Event Extraction Classifiers. In Proc. EACL 
2012, pages 286-295, Avignon, France. 
Heng Ji and Ralph Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In Proc. 
ACL-HLT 2008, pages 254-262, Columbus, OH. 
2170
Peifeng Li and Guodong Zhou. 2012. Employing Morphological Structures and Sememes for Chinese Event Ex-
traction. In Proc. COLING 2012, pages 1619-1634, Mumbai, India. 
Peifeng Li, Qiaoming Zhu, and Guodong Zhou. 2013. Argument Inference from Relevant Event Mentions in 
Chinese Argument Extraction. In Proc. ACL 2013, pages 1477-1487, Sofia, Bugaria. 
Peifeng Li, Qiaoming Zhu, Guodong Zhou. 2014. Using Compositional Semantics and Discourse Consistency to 
Improve Chinese Trigger Identification. Information Processing and Management, 50: 399?415.  
Shasha Liao and Ralph Grishman. 2010a. Filtered Ranking for Bootstrapping in Event Extraction. In Proc. COL-
ING 2010, pages 680-688, Beijing, China. 
Shasha Liao and Ralph Grishman. 2010b. Using Document Level Cross-Event Inference to Improve Event Ex-
traction. In Proc. ACL 2010, pages 789-797, Uppsala, Sweden. 
Shasha Liao and Ralph Grishman. 2011. Can Document Selection Help Semi-supervised Learning? A Case 
Study On Event Extraction. In Proc. ACL 2011, pages 260-265, Portland, OR. 
Qun Liu and Sujian Li. 2002. Word Similarity Computing Based on How-net. In Proc. 3th Chinese Lexical Se-
mantic Workshop, Taibei, Taiwan. 
Ting Liu and Tomek Strzalkowski. 2012. Bootstrapping Events and Relations from Text. In Proc. EACL 2012, 
pages 296-305, Avignon, France. 
Mstislav Maslennikov and Tat-Seng Chua. 2007. A Multi-resolution Framework for Information Extraction from 
Free Text. In Proc. ACL 2007, pages 592-599, Prague, Czech Republic. 
Siddharth Patwardhan and Ellen Riloff. 2009. A Unified Model of Phrasal and Sentential Evidence for Infor-
mation Extraction. In Proc. EMNLP 2009, pages 151-160, Singapore. 
William Phillips and Ellen Riloff. 2007. Exploiting Role-Identifying Nouns and Expressions for Information Ex-
traction. In Proc. RANLP 2007, pages 468-473, Borovets, Bulgaria. 
Ellen Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proc. AAAI 1996, 
pages 1044-1049, Portland, OR. 
Mark Stevenson and Mark Greenwood. 2005. A Semantic Approach to IE Pattern Induction. In Proc. ACL 2005, 
pages 379-386, Ann Arbor, MI. 
Kiyoshi Sudo, Satoshi Sekine, Ralph Grishman. 2001. Automatic Pattern Acquisition for Japanese Information 
Extraction. In Proc. HLT 2001, pages 1-7, San Diego, CA.  
Kiyoshi Sudo, Satoshi Sekine, Ralph Grishman. 2003. An Improved Extraction Pattern Representation Model 
for Automatic IE Pattern Acquisition. In Proc. ACL 2003, pages 224-231, Tokyo, Japan. 
Roman Yangarber, Ralph Grishman, Pasi Tapanainen and Silja Huttunen. 2000. Automatic Acquisition of Do-
main Knowledge for Information Extraction. In Proc. COLING 2000, pages 940-946, Hong Kong. 
Roman Yangarber. 2003. Counter-Training in Discovery of Semantic Patterns. In Proc. ACL 2003, pages 343-
350, Sapporo, Japan. 
Jian Wang, Qian Xu, Hongfei Lin, Zhihao Yang, Yanpeng Li. 2013. Semi-supervised Method for Biomedical 
Event Extraction. Proteome Science, 11(Suppl 1): S17. 
2171
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 346?355,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Clustering-based Stratified Seed Sampling for Semi-Supervised Relation 
Classification 
 
 
Longhua Qian Guodong Zhou 
Natural Language Processing Lab Natural Language Processing Lab 
School of Computer Science and Technology School of Computer Science and Technology 
Soochow University Soochow University 
1 Shizi Street, Suzhou, China 215006 1 Shizi Street, Suzhou, China 215006 
qianlonghua@suda.edu.cn gdzhou@suda.edu.cn 
 
 
 
Abstract 
Seed sampling is critical in semi-supervised 
learning. This paper proposes a clustering-
based stratified seed sampling approach to 
semi-supervised learning.  First, various clus-
tering algorithms are explored to partition the 
unlabeled instances into different strata with 
each stratum represented by a center. Then, 
diversity-motivated intra-stratum sampling is 
adopted to choose the center and additional 
instances from each stratum to form the unla-
beled seed set for an oracle to annotate. Fi-
nally, the labeled seed set is fed into a 
bootstrapping procedure as the initial labeled 
data. We systematically evaluate our stratified 
bootstrapping approach in the semantic rela-
tion classification subtask of the ACE RDC 
(Relation Detection and Classification) task. 
In particular, we compare various clustering 
algorithms on the stratified bootstrapping per-
formance. Experimental results on the ACE 
RDC 2004 corpus show that our clustering-
based stratified bootstrapping approach 
achieves the best F1-score of 75.9 on the sub-
task of semantic relation classification, ap-
proaching the one with golden clustering. 
1 Introduction 
Semantic relation extraction aims to detect and 
classify semantic relationships between a pair of 
named entities occurring in a natural language text. 
Many machine learning approaches have been pro-
posed to attack this problem, including supervised 
(Miller et al, 2000; Zelenko et al, 2003; Culotta 
and Soresen, 2004; Kambhatla, 2004; Zhao and 
Grishman, 2005; Zhou et al, 2005; Zhang et al, 
2006; Zhou and Zhang, 2007; Zhou et al, 2007; 
Qian et al, 2008; Zhou et al, 2010), semi-
supervised (Brin, 1998; Agichtein and Gravano, 
2000; Zhang, 2004; Chen et al, 2006; Qian et al, 
2009; Zhou et al, 2009), and unsupervised meth-
ods (Hasegawa et al, 2004; Zhang et al, 2005; 
Chen et al, 2005). 
Current work on relation extraction mainly 
adopts supervised learning methods, since they 
achieve much better performance. However, they 
normally require a large number of manually la-
beled relation instances, whose acquisition is both 
time consuming and labor intensive. In contrast, 
unsupervised methods do not need any manually 
labeled instances. Nevertheless, it is difficult to 
assess their performance due to the lack of evalua-
tion criteria. As something between them, semi-
supervised learning has received more and more 
attention recently. With the plenitude of unlabeled 
natural language text at hand, semi-supervised 
learning can significantly reduce the need for la-
beled data with only limited sacrifice in perform-
ance. For example, Abney (2002) proposes a 
bootstrapping algorithm which chooses the unla-
beled instances with the highest probability of be-
ing correctly labeled and add them in turn into the 
labeled training data iteratively. 
This paper focuses on bootstrapping-based semi-
supervised learning in relation extraction. Since the 
performance of bootstrapping depends much on the 
quality and quantity of the seed set and researchers 
tend to employ as few seeds as possible (e.g. 100 
instances) to save time and labor, the quality of the 
seed set plays a critical role in bootstrapping. Fur-
thermore, the imbalance of different classes and 
346
the inherent structural complexity of instances will 
severely weaken the strength of bootstrapping and 
semi-supervised learning as well. Therefore, it is 
critical for a bootstrapping procedure to select an 
appropriate seed set, which should be representa-
tive and diverse. However, most of current semi-
supervised relation extraction systems (Zhang, 
2004; Chen et al, 2006) use a random seed sam-
pling strategy, which fails to fully exploit the affin-
ity nature in the training data to derive the seed set. 
Alternatively, Zhou et al (2009) bootstrap a set of 
weighted support vectors from both labeled and 
unlabeled data using SVM and feed these instances 
into semi-supervised relation extraction. However, 
their seed set is sequentially generated only to en-
sure that there are at least 5 instances for each rela-
tion class. Our previous work (Qian et al, 2009) 
attempts to solve this problem via a simple strati-
fied sampling strategy for selecting the seed set. 
Experimentation on the ACE RDC 2004 corpus 
shows that the stratified sampling strategy achieves 
promising results for semi-supervised learning. 
Nevertheless, the success of the strategy relies on 
the assumption that the true distribution of all rela-
tion types is already known, which is impractical 
for real NLP applications. 
This paper presents a clustering-based stratified 
seed sampling approach for semi-supervised rela-
tion extraction, without the assumption on the true 
distribution of different relation types. The motiva-
tions behind our approach are that the unlabeled 
data can be partitioned into a number of strata us-
ing a clustering algorithm and that representative 
and diverse seeds can be derived from such strata 
in the framework of stratified sampling (Neyman, 
1934) for an oracle to annotate. Particularly, we 
employ a diversity-motivated intra-stratum sam-
pling scheme to pick a center and additional in-
stances as seeds from each stratum. Experimental 
results show the effectiveness of the clustering-
based stratified seed sampling for semi-supervised 
relation classification. 
The rest of this paper is organized as follows. 
First an overview of the related work is given in 
Section 2. Then, Section 3 introduces the stratified 
bootstrapping framework including an intra-
stratum sampling scheme while Section 4 describes 
various clustering algorithms. The experimental 
results on the ACE RDC 2004 corpus are reported 
in Section 5. Finally we conclude our work and 
indicate some future directions in Section 6. 
2 Related Work 
In semi-supervised learning for relation extraction, 
most of previous work construct the seed set either 
randomly (Zhang, 2004; Chen et al, 2006) or se-
quentially (Zhou et al, 2009). Qian et al (2009) 
adopt a stratified sampling strategy to select the 
seed set. However, their method needs a stratifica-
tion variable such as the known distribution of the 
relation types, while our method uses clustering to 
divide relation instances into different strata. 
In the literature, clustering techniques have been 
employed in active learning to sample representa-
tive seeds in a certain extent (Nguyen and 
Smeulders, 2004; Tang et al, 2002; Shen et al, 
2004). Our work is similar to the formal frame-
work, as proposed in Nguyen and Smeulders 
(2004), in which K-medoids clustering is incorpo-
rated into active learning. The cluster centers are 
used to construct a classifier and which in turn 
propagates classification decision to other exam-
ples via a local noise model. Unlike their probabil-
istic models, we apply various clustering 
algorithms together with intra-stratum sampling to 
select a seed set in discriminative models like 
SVMs. In active learning for syntactic parsing, 
Tang et al (2002) employ a sampling strategy of 
?most uncertain per cluster? to select representa-
tive examples and weight them using their cluster 
density, while we pick a few seeds (the number of 
the sampled seeds is proportional to the cluster 
density) from a cluster in addition to its center. 
Shen et al (2004) combine multiple criteria to 
measure the informativeness, representativeness, 
and diversity of examples in active learning for 
named entity recognition. Unlike our sampling 
strategy of clustering for representativeness and 
stratified sampling for diversity, they either select 
cluster centroids or diverse examples from a pre-
chosen set in terms of some combined metrics. To 
the best of our knowledge, this is the first work to 
address the issue of seed selection using clustering 
techniques for semi-supervised learning with dis-
criminative models. 
3 Stratified Bootstrapping Framework 
The stratified bootstrapping framework consists of 
three major components: an underlying supervised 
learner and a bootstrapping algorithm on top of it 
347
as usual, plus a clustering-based stratified seed 
sampler. 
3.1 Underlying Supervised Learner 
Due to recent success in tree kernel-based relation 
extraction, this paper adopts a tree kernel-based 
method in the underlying supervised learner. Fol-
lowing the previous work in relation extraction 
(Zhang et al, 2006; Zhou et al, 2007; Qian et al, 
2008), we use the standard convolution tree kernel 
(Collins and Duffy, 2001) to count the number of 
common sub-trees as the structural similarity be-
tween two parse trees. Besides, to properly repre-
sent a relation instance, this paper adopts the 
Unified Parse and Semantic Tree (UPST), as pro-
posed in Qian et al (2008). To our knowledge, the 
USPT has achieved the best performance in rela-
tion extraction so far on the ACE RDC 2004 cor-
pus. 
In particular, we use the SVMlight-TK1 package 
as our classifier. Since the package is a binary clas-
sifier, we adapt it to the multi-class tasks of rela-
tion extraction by applying the one vs. others 
strategy, which builds K binary classifiers so as to 
separate one class from all others. The final classi-
fication decision of an instance is determined by 
the class that has the maximal SVM output margin. 
3.2 Bootstrapping Algorithm 
Following Zhang (2004), we have developed a 
baseline self-bootstrapping procedure, which keeps 
augmenting the labeled data by employing the 
models trained from previously available labeled 
data, as shown in Figure 1. 
Since the SVMlight-TK package doesn?t output 
any probability that it assigns to the class label on 
an instance, we devise a metric to measure the con-
fidence with regard to the classifier?s prediction. 
Given a sequence of output margins of all K binary 
classifiers at some iteration, denoted as 
{m1,m2,?mK} with mi the margin for the i-th clas-
sifier, we compute the margin gap between the 
largest and the mean of the others, i.e. 
)1/()max(max
11
1
???=
===
? KmmmH K
i
i
K
i
ii
K
i
  (1) 
Where K denotes the total number of relation 
classes, and mi denotes the output margin of the i-
                                                          
1 http://ai-nlp.info.uniroma2.it/moschitti/ 
Require: labeled seed set L
Require: unlabeled data set U
Require: batch size S
Repeat
    Train a single classifier on L
    Run the classifier on U
    Find at most S instances in U that the classifier has
the highest prediction confidence
    Add them into L
Until: no data points available or the stoppage
condition is reached
Algorithm self-bootstrapping
 
Figure 1: Self-bootstrapping algorithm 
th classifier. Intuitively, the bigger the H, the 
greater the difference between the maximal margin 
and all others, and thus the more reliably the classi-
fier makes the prediction on the instance.  
3.3 Clustering-based Stratified Seed Sampler 
Stratified sampling is a method of sampling in 
statistics, in which the members of a population are 
grouped into relatively homogeneous subgroups 
(i.e. strata) according to one certain property, and 
then a sample is selected from each stratum. This 
process of grouping is called stratification, and the 
property on which the stratification is performed is 
called the stratification variable. Previous work 
justifies theoretically and practically that stratified 
sampling is more appropriate than random sam-
pling for general use (Neyman, 1934) as well as for 
relation extraction (Qian et al, 2009). However, 
the difficulty lies in how to find the appropriate 
stratification variable for complicated tasks, such 
as relation extraction. 
The idea of clustering-based stratification cir-
cumvents this problem by clustering the unlabeled 
data into a number of strata without the need to 
explicitly specify a stratification variable. Figure 2 
illustrates the clustering-based stratified seed sam-
pling strategy employed in the bootstrapping pro-
cedure, where RSET denotes the whole unlabeled 
data, SeedSET the seed set to be labeled and 
|RSETi| the number of instances in the i-th cluster2 
RSETi. Here, a relation instance is represented us-
ing USPT and the similarity between two instances 
is computed using the standard convolution tree 
                                                          
2 Hereafter, when we refer to clusters from the viewpoint of 
stratified sampling, they are often called ?strata?. 
348
kernel, as described in Section 3.1 (i.e., both the 
clustering and the classification adopt the same 
structural representation, since we want the repre-
sentative seeds in the clustering space to be also 
representative in the classification space). After 
clustering, a certain number of instances from 
every stratum are sampled using an intra-stratum 
scheme (c.f. Subsection 3.4). Normally, this num-
ber is proportional to the size of that stratum in the 
whole data set. However, in case this number is 0 
due to the rounding of real numbers, it is set to 1 to 
ensure the existence of at least one seed from that 
stratum. Furthermore, to ensure that the total num-
ber of instances being sampled equals the pre-
scribed NS, the number of seeds from dominant 
strata may be slightly adjusted accordingly. Finally, 
these instances form the unlabeled seed set for an 
oracle to annotate as the input to the underlying 
supervised learner in the bootstrapping procedure. 
3.4 Intra-stratum sampling 
Given the distribution of clusters, a simple way to 
select the most representative instances is to 
choose the center of each cluster with the cluster 
prior as the weight of the center (Tang et al, 2002; 
Nguyen and Smeulders, 2004). Nevertheless, for 
the complicated task of relation extraction on the 
ACE RDC corpora, which is highly skewed across 
different relation classes, only considering the cen-
ter of each cluster would severely under-represent 
the high-density data. To overcome this problem, 
we adopt a sampling approach, in particular strati-
fied sampling, which takes the size of each stratum 
into consideration. 
Given the size of the seed set NS and the number 
of strata K, a natural question will arise as how to 
select the remaining (NS-K) seeds after we have 
extracted the K centers from the K strata. We view 
this problem as intra-stratum sampling, which is 
required to choose the remaining number of seeds 
from inside individual stratum (excluding the cen-
ters themselves).  
At the first glance, sampling a certain number of 
seeds from one particular stratum (e.g., RSETi), 
seems to be the same sampling problem as we have 
encountered before, which aims to select the most 
representative and diverse seeds. This will natu-
rally lead to another application of a clustering al-
gorithm to the stratification of the stratum RSETi.  
Require: RSET ={R1,R2,?,RN}, the set of unlabeled 
relation instances and K, the number of strata being 
clustered 
Output: SeedSET with the size of NS (100) 
Procedure 
Initialize SeedSET = NULL 
Cluster RSET into K strata using a clustering 
algorithm and perform stratum pruning if 
necessary. 
Calculate the number of instances being sampled 
for each stratum i={1,2,?,K} 
S
i
i NN
RSET
N ?= ||    (2) 
and adjust this number if necessary. 
Perform intra-strata sampling to form SeedSETi 
from each stratum RSETi, by selecting the center 
Ci and (Ni-1) additional instances 
Generate SeedSET by summating RSETi from each 
stratum 
 
Figure 2: Clustering-based stratified seed sampling  
Nevertheless, remember the fact that, this time for 
the stratum RSETi, the center Ci has been chosen, 
so it may not be reasonable to extract additional 
centers in this way. Therefore, in order to avoid 
recursion and over-complexity, we employ a diver-
sity-motivated intra-stratum sampling scheme 
(Shen et al, 2004), called KDN (K-diverse 
neighbors), which aims to maximize the training 
utility of all seeds from a stratum. The motivation 
is that we prefer the seeds with high variance to 
each other, thus avoiding repetitious seeds from a 
single stratum. The basic idea is to add a candidate 
instance to the seed set only if it is sufficiently dif-
ferent from any previously selected seeds, i.e., the 
similarity between the candidate instance and any 
of the current seeds is less than a threshold ?. In 
this paper, the threshold ? is set to the average 
pair-wise similarity between any two instances in a 
stratum.  
4 Clustering Algorithms 
This section describes several typical clustering 
algorithms in the literature, such as K-means, HAC, 
spectral clustering and affinity propagation, as well 
as their application in this paper. 
4.1 K-medoids (KM) 
As a simple yet effective clustering method, the K-
means algorithm assigns each instance to the clus-
ter whose center (also called centroid) is nearest. In 
349
particular, the center is the average of all the in-
stances in the cluster, i.e., with its coordinates the 
arithmetic means for each dimension separately 
over all the instances in the cluster. 
One problem with K-means is that it does not 
yield the same result with each run while the other 
problem is the requirement for the concept of a 
mean to be definable, which is unfortunately not 
available in our setting (we employ a parse tree 
representation for a relation instance). Hence, we 
adopt a variant of K-means, namely, K-medoids, 
where a medoid, rather than a centroid, is defined 
as a representative of a cluster. Besides, K-
medoids has proved to be more robust to noise and 
outliers in comparison with K-means. 
4.2 Hierarchical Agglomerative Clustering 
(HAC) 
Different from K-medoids, hierarchical clustering 
creates a hierarchy of clusters which can be 
represented in a tree structure called a dendrogram. 
The root of the tree consists of a single cluster 
containing all objects, and the leaves correspond to 
individual object.  
Typically, hierarchical agglomerative clustering 
(HAC) starts at the leaves and successively merges 
two clusters together as long as they have the 
shortest distance among all the pair-wise distances 
between any two clusters.  
Given a specified number of clusters, the key 
problem is to determine where to cut the hierarchi-
cal tree into clusters. In this paper, we generate the 
final flat cluster structures greedily by maximizing 
the equal distribution of instances among different 
clusters. 
4.3 Spectral Clustering (SC) 
Spectral clustering has become more and more 
popular recently. Taking as input a similarity 
matrix between any two instances, spectral 
clustering makes use of the spectrum of the 
similarity matrix of the data to perform 
dimensionality reduction for clustering in fewer 
dimensions.  
Compared to the ?traditional algorithms? such 
as K-means or HAC, spectral clustering has many 
fundamental advantages. Results obtained by 
spectral clustering often outperform the traditional 
approaches. Furthermore, spectral clustering is 
very simple to implement and can be solved 
efficiently using standard linear algebra methods 
(von Luxburg, 2006). 
4.4 Affinity Propagation (AP) 
As a new emerging clustering algorithm, affinity 
propagation (AP) (Frey and Dueck, 2007) is basi-
cally an iterative message-passing procedure in 
which the instances being clustered compete to 
serve as cluster exemplars by exchanging two 
types of messages, namely, ?responsibility? and 
?availability?.  After the procedure converges or 
has repeated a finite number of iterations, each 
cluster is represented by an exemplar. AP was re-
ported to find clusters with much lower error than 
those found by other methods. 
For our application, affinity propagation takes as 
input a similarity matrix, whose elements represent 
either the similarity between two different in-
stances or the preference (a real number p) for an 
instance when two instances are the same. One 
problem with AP is that the number of clusters 
cannot be pre-defined, which is indirectly deter-
mined by the preference as well as the convergence 
procedure itself. 
5 Experimentation 
This section systematically evaluates the boot-
strapping approach using clustering-based strati-
fied seed sampling, in the relation classification 
(i.e., given the relationship already detected) sub-
task of relation extraction on the ACE RDC 2004 
corpus. 
5.1 Experimental Setting 
The ACE RDC 2004 corpus 3  is gathered from 
various newspapers, newswire and broadcasts. It 
contains 451 documents and 5702 positive relation 
instances of 7 relation types and 23 subtypes be-
tween 7 entity types. For easy reference with re-
lated work in the literature, evaluation is done on 
347 documents (from nwire and bnews domains), 
which include 4305 relation instances. Table 1 lists 
the major relation types and subtypes, including 
their corresponding instance numbers and ratios in 
our evaluation set. One obvious observation from 
the table is that the numbers of different relation 
types is highly imbalanced. These 347 documents 
are then divided into 3 disjoint sets randomly, with 
                                                          
3 http//www.ldc.upenn.edu/ Projects/ACE/ 
350
Types Subtypes # % 
Located 738 17.1 
Near 87 2.0 PHYS 
Part-Whole 378 8.8 
Business 173 4.0 
Family 121 2.8 PER-SOC 
Other 55 1.3 
Employ-Executive 489 11.4 
Employ-Staff 539 12.5 
Employ-Undeter. 78 1.8 
Member-of-Group 191 4.4 
Subsidiary 206 4.8 
Partner 12 0.3 
EMP-
ORG 
Other 80 1.9 
User-or-Owner 200 4.6 
Inventor-or-Man. 9 0.2 ART 
Other 2 0.0 
Ethnic 39 0.9 
Ideology 48 1.1 OTHER-AFF 
Other 54 1.3 
Citizen-or-Resid. 273 6.3 
Based-In 215 5.0 GPE-AFF 
Other 39 0.9 
DISC   279 6.5 
Total   4305 100.0 
Table 1: Relation types and their corresponding instance 
numbers and ratios in the ACE RDC 2004 corpus 
 
10% of them (35 files, around 400 instances) held 
out as the test data set, 10% of them (35 files, 
around 400 instances) used as the development 
data set to fine-tune various settings and parame-
ters, while the remaining 277 files (over 3400 in-
stances) used as the training data set, from which 
the seed set will be sampled. 
The corpus is parsed using Charniak?s parser 
(Charniak, 2001) and relation instances are gener-
ated by extracting all pairs of entity mentions oc-
curring in the same sentence with positive 
relationships. For easy comparison with related 
work, we only evaluate the relation classification 
task on the 7 major relation types of the ACE RDC 
2004 corpus. For the SVMlight-TK classifier, the 
training parameters C (SVM) and ? (tree kernel) 
are fine-tuned to 2.4 and 0.4 respectively.  
The performance is measured using the standard 
P/R/F1 (Precision/Recall/F1-measure). For each 
relation type, P is the ratio of the true relation in-
stances in all the relation instances being identified, 
R is the ratio of the true relation instances being 
identified in all the true relation instances in the 
corpus, and F1 is the harmonic mean of P and R. 
The overall performance P/R/F1 is then calculated 
using the micro-average measure over all major 
class types. 
5.2 Experimental Results 
Comparison of various seed sampling strategies 
without intra-stratum sampling on the devel-
opment data 
Table 2 compares the performance of bootstrap-
ping-based relation classification using various 
seed sampling strategies without intra-stratum 
sampling on the development data. Here, the size 
of the seed set L is set to 100, and the top 100 in-
stances with the highest confidence (c.f. Formula 1) 
are augmented at each iteration. For sampling 
strategies marked with an asterisk, we performed 
10 trials and calculated their averages. Since for 
these strategies the seed sets sampled from differ-
ent trials may be quite different, their performance 
scores vary in a great degree accordingly. This ex-
perimental setting and notation are also used in all 
the subsequent experiments unless specified. Be-
sides, two additional baseline sampling strategies 
are included for comparison: sequential sampling 
(SEQ), which selects a sequentially-occurring L 
instances as the seed set, and random sampling 
(RAND), which randomly selects L instances as 
the seed set. 
Table 2 shows that 
1) RAND outperforms SEQ by 1.2 units in F1-
score. This is due to the fact that the seed set 
via RAND may better reflect the distribution of 
the whole training data than that via SEQ, nev-
ertheless at the expense of collecting the whole 
training data in advance. 
2) While HAC performs moderately better than 
RAND, it is surprising that both KM and AP 
perform even worse than SEQ, and that SC per-
forms worse than RAND. Furthermore, all the 
four clustering-based seed sampling strategies 
achieve much smaller performance improve-
ment in F1-score than RAND, among which 
KM performs worst with performance im-
provement of only 0.1 in F1-score. 
 
351
Sampling 
strategies P(?P) R(?R) F1(?F1) 
RAND* 69.1(3.1) 66.4(0.2) 67.8(2.0) 
SEQ* 65.8(2.6) 68.0(0.1) 66.6(1.3) 
KM* 62.0(0.9) 61.0(-0.5) 61.3(0.1) 
HAC 69.9(1.3) 70.4(0.4) 70.1(0.8) 
SC* 67.1(1.5) 68.1(0.0) 67.5(0.8) 
AP 66.6(2.0) 66.2(0.1) 66.4(1.1) 
Table 2: Comparison of various seed sampling strate-
gies without intra-stratum sampling on the development 
data 
3) All the performance improvements from boot-
strapping largely come from the improvements 
in precision. While the bootstrapping procedure 
makes the SVM classifier more accurate, it 
lacks enough generalization ability.  
To explain above special phenomena, we have a 
look at the clustering results. Our inspection re-
veals that most of them are severely imbalanced, 
i.e., some clusters are highly dense while others are 
extremely sparse. This indicates that merely select-
ing the centers from each cluster cannot properly 
represent the overall distribution. Moreover, the 
centers with high density lack the generalization 
ability due to its solitude in the cluster, leading to 
less performance enhancement than expected. 
The only exception is HAC, which much outper-
forms RAND by 2.3 in F1-score, although HAC is 
usually not considered as an effective clustering 
algorithm. The reason may be that HAC creates a 
hierarchy of clusters in the top-down manner by 
cutting a cluster into two. Therefore, the centers in 
the two sibling clusters will be closer to each other 
than they are to the centers in other clusters. Be-
sides, the final flat cluster structures given a spe-
cial number of clusters are generated greedily from 
the cluster hierarchy by maximizing the equal dis-
tribution of instances among different clusters. In 
other words, when the cluster number reaches a 
certain threshold, the dense area will get more 
seeds represented in the seed set. As a consequence, 
the distribution of all the seeds sampled by HAC 
will approximate the distribution of the whole 
training data in some degree, while the seeds sam-
pled by other clustering algorithm are kept as far as 
possible due to the objective of clustering and the 
lack of intra-stratum sampling. 
These observations also justify the application 
of the stratified seed sampling to the bootstrapping 
procedure, which enforces the number of seeds 
sampled from a cluster to be proportional to its 
density, presumably approximated by its size in 
this paper. 
 
Comparison of different cluster numbers with 
intra-stratum sampling on the development 
data 
In order to fine-tune the optimal cluster numbers 
for seed sampling, we compare the performance of 
different numbers of clusters for each clustering 
algorithm on the development data set and report 
their F-scores in Table 3. For reference, we also 
list the F-score for golden clustering (GOLD), in 
which all instances are grouped in terms of their 
annotated ground relation major types (7), major 
types considering relation direction (13), subtypes 
(23), and subtypes considering direction (38). Be-
sides, the performance of clustering-based semi-
supervised relation classification is also measured 
over other typical cluster numbers (i.e., 1, 50, 60, 
80, 100). Particularly, when the cluster number 
equals 1, it means that only diversity other than 
representativeness is considered in the seed sam-
pling. Among these clustering algorithms, one of 
the distinct characteristics with the AP algorithm is 
that the number of clusters cannot be specified in 
advance, rather, it is determined by the pre-defined 
preference parameter (c.f. Subsection 4.4). There-
fore, we should tune the preference parameter so as 
to get the pre-defined cluster number. However, 
sometimes we still couldn?t get the exact number 
of clusters as we expected. In these cases, we use 
the approximate cluster numbers for AP instead.  
Table 3 shows that 
1) The performance for all the clustering algo-
rithms varies in some degree with the number 
of clusters being grouped. Interestingly, the 
performance with only one cluster is better 
than those of clustering-based strategies with 
100 clusters, at most cases. This implies that 
the diversity of the seeds is at least as impor-
tant as their representativeness. And this could 
be further explained by our observation that, 
with the increase of cluster numbers, the clus-
ters get smaller and denser while their centers 
also come closer to each other. Therefore, the 
representativeness and diversity as well as the 
distribution of the seeds sampled from them 
may vary accordingly, leading to different per-
formance. 
352
# of  
 Clusters GOLD KM* HAC SC* AP 
1 -  68.7  68.7  - - 
7 73.9 70.3  73.3 72.1 - 
13 70.2 68.9  70.3 67.3 - 
23 64.9 72.3  72.9 68.9 71.1 
38 60.8 69.9  71.6 68.0 71.6 
50 - 68.5  69.9 68.5 70.4 
60 - 66.3  68.5 68.6 69.7 
80 - 64.2  65.9 68.0 68.1 
100 - 61.3  70.1 67.5 66.4 
Table 3: Performance in F1-score over different cluster 
numbers with intra-stratum sampling on the develop-
ment data 
2) Golden clustering achieves the best performance 
of 73.9 in F1-score when the cluster number is 
set to 7, significantly higher than the perform-
ance using other cluster numbers. Interestingly, 
this number coincides with the number of major 
relation types needed to be classified in our task. 
This is reasonable since the instances with the 
same relation type should be much more similar 
than those with different relation types and it is 
easy to discriminate the seed set of one relation 
type from that of other relation types. 
3) Among the four clustering algorithms, HAC 
achieves best performance over most of cluster 
numbers. This further verifies the aforemen-
tioned analysis. That is, as a hierarchical clus-
tering algorithm, HAC can sample seeds that 
better capture the distribution of the training 
data. 
4) For KM, the best performance is achieved 
around the number of 23 while for both HAC 
and SC, the optimal cluster number is consis-
tent with GOLD clustering, namely, 7. For AP, 
the optimal cluster number for AP is 38. This is 
largely due to that we fail to cluster the training 
data into about 7 and 13 groups no matter how 
we vary the preference parameter.  
 
Final comparison of different clustering algo-
rithms on the held-out test data  
After the optimal cluster numbers are determined 
for each clustering algorithm, we apply these num-
bers on the held-out test data and report the per-
formance results (P/R/F1 and their respective 
improvements) in Table 4. For easy reference, we 
also include the performance for GOLD, RAND, 
and SEQ sampling strategies.  
 
Sampling 
strategies P(?P) R(?R) F1(?F1) 
GOLD 79.5(7.8) 72.7(2.1) 76.0(4.8) 
RAND* 71.9(3.7) 69.7(0.1) 70.8(1.8) 
SEQ* 71.9(2.6) 65.2(0.1) 69.3(1.3) 
KM* 73.6(2.1) 72.3(0.3) 72.9(1.2) 
HAC 79.0(10.2) 73.0(1.1) 75.9(5.6) 
SC* 72.3(2.1) 72.1(0.4) 72.2(1.2) 
AP 75.7(2.5) 72.0(0.4) 73.7(1.4)
Table 4: Performance of various clustering-based seed 
sampling strategies on the held-out test data with the 
optimal cluster number for each clustering algorithm 
 
Table 4 shows that 
1) Among all the clustering algorithms, HAC 
achieves the best F1-score of 75.9, significantly 
higher than RAND and SEQ by 5.1 and 6.6 re-
spectively. The improvement comes not only 
from significant precision boost, but also from 
moderate recall increase. This further justifies 
the merits of HAC as a clustering algorithm for 
stratified seed sampling in semi-supervised re-
lation classification.  
2) HAC approaches the best F1-score of 76.0 for 
golden clustering. Obviously, this doesn?t mean 
HAC performs as well as golden clustering in 
terms of clustering quality measures, rather it 
does imply that HAC achieves the performance 
improvement by making the seed set better rep-
resent the overall distribution over inherent 
structure of relation instances, while golden 
clustering accomplishes this using the distribu-
tion over relation types. Since the distribution 
over relation types doesn?t always conform to 
that over instance structures, and for a statistical 
discriminative classifier, often the latter is more 
important than the former, it will be no surprise 
if HAC outperforms golden clustering in some 
real applications, e.g. clustering-based stratified 
sampling. 
6 Conclusion and Future Work 
This paper presents a stratified seed sampling 
strategy based on clustering algorithms for semi-
supervised learning. Our strategy does not rely on 
any stratification variable to divide the training 
instances into a number of strata. Instead, the strata 
are formed via clustering, given a metric measur-
ing the similarity between any two instances. Fur-
ther, diversity-motivated intra-strata sampling is 
353
employed to sample additional instances from 
within each stratum besides its center. We compare 
the effect of various clustering algorithms on the 
performance of semi-supervised learning and find 
that HAC achieves the best performance since the 
distribution of its seed set better approximates that 
of the whole training data. Extensive evaluation on 
the ACE RDC 2004 benchmark corpus shows that 
our clustering-based stratified seed sampling strat-
egy significantly improves the performance of 
semi-supervised relation classification. 
We believe that our clustering-based stratified 
seed sampling strategy can not only be applied to 
other semi-supervised learning tasks, but also can 
be incorporated into active learning, where the in-
stances to be labeled at each iteration as well as the 
seed set could be selected using clustering tech-
niques, thus further reducing the amount of in-
stances needed to be annotated.  
For the future work, it is possible to adapt our 
one-level clustering-based sampling to the multi-
level one, where for every stratum it is still possi-
ble to divide it into lower sub-strata for further 
stratified sampling in order to make the seeds bet-
ter represent the true distribution of the data. 
Acknowledgments 
This research is supported by Projects 60873150, 
60970056, and 90920004 under the National Natu-
ral Science Foundation of China. 
References  
S. Abney. 2002. Bootstrapping. ACL-2002. 
E. Agichtein and L. Gravano. 2000. Snowball: Extract-
ing relations from large plain-text collections. In 
Proceedings of the 5th ACM international Conference 
on Digital Libraries (ACMDL 2000).  
S. Brin. 1998. Extracting patterns and relations from the 
world wide web. In WebDB Workshop at 6th Interna-
tional Conference on Extending Database Technol-
ogy (EDBT 98). 
E. Charniak. 2001. Intermediate-head Parsing for Lan-
guage Models. ACL-2001: 116-123. 
M. Collins and N. Duffy. 2001. Convolution Kernels for 
Natural Language. NIPS 2001: 625-632. 
J.X. Chen, D.H. Ji, C.L. Tan, and Z.Y. Niu. 2005. Un-
supervised Feature Selection for Relation Extraction. 
CIKM-2005: 411-418. 
J.X. Chen, D.H. Ji, and C. L. Tan. 2006. Relation Ex-
traction using Label Propagation Based Semi super-
vised Learning. ACL/COLING-2006: 129-136. 
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. ACL-2004: 423-439.  
B.J. Frey and D. Dueck. 2007. Clustering by Passing 
Messages between Data Points. Science, 315: 972-
976. 
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Dis-
covering Relations among Named Entities from 
Large Corpora. ACL-2004. 
N. Kambhatla. 2004. Combining lexical, syntactic and 
semantic features with Maximum Entropy models for 
extracting relations. ACL-2004(posters): 178-181.  
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 
2000. A novel use of statistical parsing to extract in-
formation from text. In Proceedings of the 6th Ap-
plied Natural Language Processing Conference. 
J. Neyman. 1934. On the Two Different Aspects of the 
Representative Method: The Method of Stratified 
Sampling and the Method of Purposive Selection. 
Journal of the Royal Statistical Society, 97(4): 558-
625. 
H.T. Nguyen and A. Smeulders. 2004. Active Learning 
Using Pre-clustering, ICML-2004. 
L.H. Qian, G.D. Zhou, Q.M. Zhu, and P.D. Qian. 2008. 
Exploiting constituent dependencies for tree kernel-
based semantic relation extraction. COLING-2008: 
697-704. 
L.H. Qian, G.D. Zhou, F. Kong, and Q.M. Zhu. 2009. 
Semi-Supervised Learning for Semantic Relation 
Classification using Stratified Sampling Strategy. 
EMNLP-2009: 1437-1445.  
D. Shen, J. Zhang, J. Su, G. Zhou and C. Tan. 2004. 
Multi-criteria-based active learning for named entity 
recognition. ACL-2004. 
M. Tang, X. Luo and S. Roukos. 2002. Active Learning 
for Statistical Natural Language Parsing. ACL-2002. 
U. von Luxburg. 2006. A tutorial on spectral clustering. 
Technical report, Max Planck Institute for Biological 
Cybernetics. 
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel 
Methods for Relation Extraction. Journal of Machine 
Learning Research, (2): 1083-1106. 
M. Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L. 
Tan. 2005. Discovering Relations between Named 
Entities from a Large Raw Corpus Using Tree Simi-
larity-Based Clustering. IJCNLP-2005: 378-389.  
M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A 
Composite Kernel to Extract Relations between Enti-
ties with both Flat and Structured Features. 
ACL/COLING-2006: 825-832. 
Z. Zhang. 2004. Weakly-supervised relation classifica-
tion for Information Extraction. CIKM-2004. 
S.B. Zhao and R. Grishman. 2005. Extracting relations 
with integrated information using kernel methods. 
ACL-2005: 419-426. 
354
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. Ex-
ploring various knowledge in relation extraction. 
ACL-2005: 427-434. 
G.D. Zhou, L.H. Qian, and J.X. Fan. 2010. Tree kernel-
based semantic relation extraction with rich syntactic 
and semantic information. Information Sciences, 
(179): 1785-1791. 
G.D. Zhou, L.H. Qian, and Q.M. Zhu. 2009. Label 
propagation via bootstrapped support vectors for se-
mantic relation extraction between named entities. 
Computer Speech and Language, 23(4): 464-478. 
G.D. Zhou and M. Zhang. 2007. Extraction relation 
information from text documents by exploring vari-
ous types of knowledge. Information Processing and 
Management, (42):969-982. 
G.D. Zhou, M. Zhang, D.H. Ji, and Q.M. Zhu. 2007. 
Tree Kernel-based Relation Extraction with Context-
Sensitive Structured Parse Tree Information. 
EMNLP/CoNLL-2007: 728-736.  
 
355
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 714?724,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Unified Framework for Scope Learning via Simplified Shallow Seman-
tic Parsing 
 
Qiaoming Zhu    Junhui Li    Hongling Wang    Guodong Zhou?  
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
{qmzhu, lijunhui, hlwang, gdzhou}@suda.edu.cn 
 
 
                                                          
? Corresponding author 
Abstract 
This paper approaches the scope learning 
problem via simplified shallow semantic pars-
ing. This is done by regarding the cue as the 
predicate and mapping its scope into several 
constituents as the arguments of the cue. 
Evaluation on the BioScope corpus shows that 
the structural information plays a critical role 
in capturing the relationship between a cue 
and its dominated arguments. It also shows 
that our parsing approach significantly outper-
forms the state-of-the-art chunking ones. Al-
though our parsing approach is only evaluated 
on negation and speculation scope learning 
here, it is portable to other kinds of scope 
learning.  
1 Introduction 
Recent years have witnessed an increasing interest 
in the analysis of linguistic scope in natural lan-
guage. The task of scope learning deals with the 
syntactic analysis of what part of a given sentence 
is under user?s special interest. For example, of 
negation assertion concerned, a negation cue (e.g., 
not, no) usually dominates a fragment of the given 
sentence, rather than the whole sentence, especially 
when the sentence is long. Generally, scope learn-
ing involves two subtasks: cue recognition and its 
scope identification. The former decides whether a 
word or phrase in a sentence is a cue of a special 
interest, where the semantic information of the 
word or phrase, rather than the syntactic informa-
tion, plays a critical role. The latter determines the 
sequences of words in the sentence which are 
dominated by the given cue.  
Recognizing the scope of a special interest (e.g., 
negative assertion and speculative assertion) is es-
sential in information extraction (IE), whose aim is 
to derive factual knowledge from free text. For 
example, Vincze et al (2008) pointed out that the 
extracted information within the scope of a nega-
tion or speculation cue should either be discarded 
or presented separately from factual information. 
This is especially important in the biomedical and 
scientific domains, where various linguistic forms 
are used extensively to express impressions, hy-
pothesized explanations of experimental results or 
negative findings. Besides, Vincez et al (2008) 
reported that 13.45% and 17.70% of the sentences 
in the abstracts subcorpus of the BioScope corpus 
contain negative and speculative assertions, respec-
tively, while 12.70% and 19.44% of the sentences 
in the full papers subcorpus contain negative and 
speculative assertions, respectively. In addition to 
the IE tasks in the biomedical domain, negation 
scope learning has attracted increasing attention in 
some natural language processing (NLP) tasks, 
such as sentiment classification (Turney, 2002). 
For example, in the sentence ?The chair is not 
comfortable but cheap?, although both the polari-
ties of the words ?comfortable? and ?cheap? are 
positive, the polarity of ?the chair? regarding the 
attribute ?cheap? keeps positive while the polarity 
of ?the chair? regarding the attribute ?comfortable? 
is reversed due to the negation cue ?not?. Similarly, 
seeing the increasing interest in speculation scope 
learning, the CoNLL?2010 shared task (Farkas et 
al., 2010) aims to detect uncertain information in 
resolving the scopes of speculation cues. 
Most of the initial research in this literature fo-
cused on either recognizing negated terms or iden-
tifying speculative sentences, using some heuristic 
714
rules (Chapman et al, 2001; Light et al, 2004), 
and machine learning methods (Goldin and Chap-
man, 2003; Medlock and Briscoe, 2007). However, 
scope learning has been largely ignored until the 
recent release of the BioScope corpus (Szarvas et 
al., 2008), where negation/speculation cues and 
their scopes are annotated explicitly. Morante et al 
(2008) and Morante and Daelemans (2009a & 
2009b) pioneered the research on scope learning 
by formulating it as a chunking problem, which 
classifies the words of a sentence as being inside or 
outside the scope of a cue. Alternatively, ?zg?r 
and Radev (2009) and ?vrelid et al (2010) defined 
heuristic rules for speculation scope learning from 
constituency and dependency parse tree perspec-
tives, respectively. 
Although the chunking approach has been 
evaluated on negation and speculation scope learn-
ing and can be easily ported to other scope learning 
tasks, it ignores syntactic information and suffers 
from low performance. Alternatively, even if the 
rule-based methods may be effective for a special 
scope learning task (e.g., speculation scope learn-
ing), it is not readily adoptable to other scope 
learning tasks (e.g., negation scope learning). In-
stead, this paper explores scope learning from 
parse tree perspective and formulates it as a simpli-
fied shallow semantic parsing problem, which has 
been extensively studied in the past few years 
(Carreras and M?rquez, 2005). In particular, the 
cue is recast as the predicate and the scope is recast 
as the arguments of the cue. The motivation behind 
is that the structured syntactic information plays a 
critical role in scope learning and should be paid 
much more attention, as indicated by previous 
studies in shallow semantic parsing (Gildea and 
Palmer, 2002; Punyakanok et al, 2005). Although 
our approach is evaluated only on negation and 
speculation scope learning here, it is portable to 
other kinds of scope learning. 
The rest of this paper is organized as follows. 
Section 2 reviews related work. Section 3 intro-
duces the Bioscope corpus on which our approach 
is evaluated. Section 4 describes our parsing ap-
proach by formulating scope learning as a simpli-
fied shallow semantic parsing problem. Section 5 
presents the experimental results. Finally, Section 
6 concludes the work. 
 
 
2 Related Work  
Most of the previous research on scope learning 
falls into negation scope learning and speculation 
scope learning.  
Negation Scope Learning 
Morante et al (2008) pioneered the research on 
negation scope learning, largely due to the avail-
ability of a large-scale annotated corpus, the Bio-
scope corpus. They approached negation cue 
recognition as a classification problem and formu-
lated negation scope identification as a chunking 
problem which predicts whether a word in the sen-
tence is inside or outside of the negation scope, 
with proper post-processing to ensure consecutive-
ness of the negation scope. Morante and Daele-
mans (2009a) further improved the performance by 
combing several classifiers and achieved the accu-
racy of ~98% for negation cue recognition and the 
PCS (Percentage of Correct Scope) of ~74% for 
negation scope identification on the abstracts sub-
corpus. However, this chunking approach suffers 
from low performance, in particular on long sen-
tences. For example, given golden negation cues 
on the Bioscope corpus, Morante and Daelemans 
(2009a) only got the performance of 50.26% in 
PCS on the full papers subcorpus (22.8 words per 
sentence on average), compared to 87.27% in PCS 
on the clinical reports subcorpus (6.6 words per 
sentence on average). 
Speculation Scope Learning 
Similar to Morante and Daelemans (2009a), 
Morante and Daelemans (2009b) formulated 
speculation scope identification as a chunking 
problem which predicts whether a word in the sen-
tence is inside or outside of the speculation scope, 
with proper post-processing to ensure consecutive-
ness of the speculation scope. They concluded that 
their method for negation scope identification is 
portable to speculation scope identification. How-
ever, of speculation scope identification concerned, 
it also suffers from low performance, with only 
60.59% in PCS for the clinical reports subcorpus 
of short sentences. 
Alternatively, ?zg?r and Radev (2009) em-
ployed some heuristic rules from constituency 
parse tree perspective on speculation scope identi-
fication. Given golden speculation cues, their rule-
based method achieves the accuracies of 79.89% 
715
and 61.13% on the abstracts and the full papers 
subcorpora, respectively. The more recent 
CoNLL?2010 shared task was dedicated to the de-
tection of speculation cues and their linguistic 
scope in natural language processing (Farkas et al, 
2010). As a representative, ?vrelid et al (2010) 
adopted some heuristic rules from dependency 
parse tree perspective to identify their speculation 
scopes. 
3 Cues and Scopes in the BioScope Cor-
pus 
This paper employs the BioScope corpus (Szarvas 
et al, 2008; Vincze et al, 2008) 1 , a freely 
downloadable resource from the biomedical do-
main, as the benchmark corpus. In this corpus, 
every sentence is annotated with negation cues and 
speculation cues (if it has), as well as their linguis-
tic scopes. Figure 1 shows a self-explainable ex-
ample. It is possible that a negation/speculation cue 
consists of multiple words, i.e., ?can not?/?indicate 
that? in Figure 1. 
 
The Bioscope corpus consists of three sub-
corpora: biological full papers from FlyBase and 
from BMC Bioinformatics, biological paper ab-
stracts from the GENIA corpus (Collier et al, 
1999), and clinical (radiology) reports. Among 
them, the full papers subcorpus and the abstracts 
subcorpus come from the same genre, and thus 
share some common characteristics in statistics, 
such as the number of words in the nega-
tion/speculation scope to the right (or left) of the 
negation/speculation cue and the average scope 
length. In comparison, the clinical reports subcor-
pus consists of clinical radiology reports with short 
sentences. For detailed statistics and annotation 
                                                          
                                                          
1 http://www.inf.u-szeged.hu/rgai/bioscope 
guidelines about the three subcorpora, please see 
Morante and Daelemans (2009a & 2009b). 
For preprocessing, all the sentences in the Bio-
scope corpus are tokenized and then parsed using 
the Berkeley parser (Petrov and Klein, 2007) 2  
trained on the GENIA TreeBank (GTB) 1.0 
(Tateisi et al, 2005)3, which is a bracketed corpus 
in (almost) PTB style. 10-fold cross-validation on 
GTB1.0 shows that the parser achieves the per-
formance of 86.57 in F1-measure. It is worth not-
ing that the GTB1.0 corpus includes all the 
sentences in the abstracts subcorpus of the Bio-
scope corpus. 
4 Scope Learning via Simplified Shallow 
Semantic Parsing 
In this section, we first formulate the scope learn-
ing task as a simplified shallow semantic parsing 
problem. Then, we deal with it using a simplified 
shallow semantic parsing framework. 
4.1 Formulating Scope Learning as a Simpli-
fied Shallow Semantic Parsing Problem 
<sentence id="S26.8">These findings <xcope 
id="X26.8.2"><cue type="speculation" 
ref="X26.8.2">indicate that</cue> <xcope 
id="X26.8.1">corticosteroid resistance in bron-
chial asthma <cue type="negation" 
ref="X26.8.1">can not</cue> be explained by 
abnormalities in corticosteroid receptor charac-
teristics</xcope></xcope>.</sentence> 
Figure 1: An annotated sentence in the BioScope 
corpus 
Given a parse tree and a predicate in it, shallow 
semantic parsing recognizes and maps all the con-
stituents in the sentence into their corresponding 
semantic arguments (roles) of the predicate or not. 
As far as scope learning considered, the cue can be 
regarded as the predicate4, while its scope can be 
mapped into several constituents dominated by the 
cue and thus can be regarded as the arguments of 
the cue. In particular, given a cue and its scope 
which covers wordm, ?, wordn, we adopt the fol-
lowing two heuristic rules to map the scope of the 
cue into several constituents which can be deemed 
as its arguments in the given parse tree. 
1) The cue itself and all of its ancestral constituents 
are non-arguments. 
2) If constituent X is an argument of the given cue, 
then X should be the highest constituent domi-
nated by the scope of wordm, ?, wordn. That is 
to say, X?s parent constituent must cross-bracket 
or include the scope of wordm, ?, wordn. 
2 http://code.google.com/p/berkeleyparser/ 
3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA 
4 If a speculation cue consists of multiply words (e.g., whether 
or not), the first word (e.g., whether) is chosen to represent the 
speculation signal. However, the last word (e.g., not) is chosen 
to represent the negation cue if it consists of multiple words 
(e.g., can not). 
716
 Figure 2: Examples of a negation/speculation cue and its arguments in a parse tree 
These findings 
indicate 
that 
corticosteroid resistance
NP0,1
VBP2,2 SBAR3,11
can not
IN3,3
be
explained by abnormalities
NP4,5
MD6,6 RB7,7
VB8,8 VP9,11
VP8,11
VP6,11
S4,11
VP2,11
S0,11
neg-predicate
neg-arguments
spec-predicate
spec-argument
 
The first rule ensures that no argument covers 
the cue while the second rule ensures no overlap 
between any two arguments. These two constraints 
between a cue and its arguments are consistent 
with shallow semantic parsing (Carreras and 
M?rquez, 2005). For example, in the sentence 
?These findings indicate that corticosteroid resis-
tance can not be explained by abnormalities?, the 
negation cue ?can not? has the negation scope 
?corticosteroid resistance can not be explained by 
abnormalities? while the speculation cue ?indicate 
that? has the speculation scope ?indicate that cor-
ticosteroid resistance can not be explained by ab-
normalities?. As shown in Figure 2, the node 
?RB7,7? (i.e., not) represents the negation cue ?can 
not? while its arguments include three constituents 
{NP4,5, MD6,6, and VP8,11}. Similarly, the node 
?VBP2,2? (i.e., indicate) represents the  speculation 
cue ?indicate that? while its arguments include one 
constituent SBAR3,11. It is worth noting that ac-
cording to the above rules, scope learning via shal-
low semantic parsing, i.e. determining the 
arguments of a given cue, is robust to some varia-
tions in the parse trees. This is also empirically 
justified by our later experiments. For example, if 
the VP6,11 in Figure 2 is incorrectly expanded by 
the rule VP6,11?MD6,6+RB7,7+VB8,8+VP9,11, the 
negation scope of the negation cue ?can not? can 
still be correctly detected as long as {NP4,5, MD6,6, 
VB8,8, and VP9,11} are predicated as the arguments 
of the negation cue ?can not?. 
Compared with common shallow semantic pars-
ing which needs to assign an argument with a se-
mantic label, scope identification does not involve 
semantic label classification and thus could be di-
vided into three consequent phases: argument 
pruning, argument identification and post-
processing. 
 
4.2 Argument Pruning 
Similar to the predicate-argument structures in 
common shallow semantic parsing, the cue-scope 
structures in scope learning can be also classified 
into several certain types and argument pruning 
can be done by employing several heuristic rules 
accordingly to filter out constituents, which are 
most likely non-arguments of a given cue. Similar 
to the heuristic algorithm proposed in Xue and 
Palmer (2004) for argument pruning in common 
shallow semantic parsing, the argument pruning 
algorithm adopted here starts from designating the 
cue node as the current node and collects its sib-
lings. It then iteratively moves one level up to the 
parent of the current node and collects its siblings. 
The algorithm ends when it reaches the root of the 
parse tree. To sum up, except the cue node itself 
and its ancestral constituents, any constituent in the 
parse tree whose parent covers the given cue will 
be collected as argument candidates. Taking the 
negation cue node ?RB7,7? in Figure 2 as an exam-
ple, constituents {MD6,6, VP8,11, NP4,5, IN3,3,  
717
 
 
Feature Remarks 
B1 Cue itself: the word of the cue, e.g., not,
rather_than. (can_not) 
B2 Phrase Type: the syntactic category of the
argument candidate. (NP) 
B3 Path: the syntactic path from the argument 
candidate to the cue. (NP<S>VP>RB) 
B4 Position: the positional relationship of the
argument candidate with the cue. ?left? or 
?right?. (left) 
Table 1: Basic features and their instantiations for ar-
gument identification in scope learning, with NP4,5 as 
the focus constituent (i.e., the argument candidate) and 
?can not? as the given cue, regarding Figure 2. 
 
 
Feature Remarks 
Argument Candidate (AC) related 
AC1 The headword (AC1H) and its POS
(AC1P). (resistance, NN) 
AC2 The left word (AC2W) and its POS
(AC2P). (that, IN) 
AC3 The right word (AC3W) and its POS
(AC3P). (can, MD) 
AC4 The phrase type of its left sibling (AC4L)
and its right sibling (AC4R). (NULL, VP)
AC5 The phrase type of its parent node. (S) 
AC6 The subcategory. (S:NP+VP) 
Cue/Predicate (CP) related 
CP1 Its POS. (RB) 
CP2 Its left word (CP2L) and right word
(CP2R). (can, be) 
CP3 The subcategory. (VP:MD+RB+VP) 
CP4 The phrase type of its parent node. (VP) 
Combined Features related with the Argument Candi-
date  (CFAC1-CFAC2) 
b2&AC1H, b2&AC1P 
Combined Features related with the given
Cue/Predicate  (CFCP1-CFCP2) 
B1&CP2L, B1&CP2R 
Combined Features related with both the Argument 
Candidate and the given Cue/Predicate (CFACCP1-
CFACCP7) 
B1&B2, B1&B3, B1&CP1, B3&CFCP1, B3&CFCP2, 
B4&CFCP1, B4&CFCP2 
Table 2: Additional features and their instantiations for 
argument identification in scope identification, with 
NP4,5 as the focus constituent (i.e., the argument candi-
date) and ?can not? as the given cue, regarding Figure 2. 
 
VBP2,2, and NP0,1} are collected as its argument 
candidates consequently. 
4.3 Argument Identification 
Here, a binary classifier is applied to determine the 
argument candidates as either valid arguments or 
non-arguments. Similar to argument identification 
in common shallow semantic parsing, the struc-
tured syntactic information plays a critical role in 
scope learning. 
Basic Features 
Table 1 lists the basic features for argument identi-
fication. These features are also widely used in 
common shallow semantic parsing for both verbal 
and nominal predicates (Xue, 2008; Li et al, 2009). 
Additional Features 
To capture more useful information in the cue-
scope structures, we also explore various kinds of 
additional features. Table 2 shows the features in 
better capturing the details regarding the argument 
candidate and the cue. In particular, we categorize 
the additional features into three groups according 
to their relationship with the argument candidate 
(AC, in short) and the given cue/predicate (CP, in 
short). 
Some features proposed above may not be effec-
tive in argument identification. Therefore, we 
adopt the greedy feature selection algorithm as de-
scribed in Jiang and Ng (2006) to pick up positive 
features incrementally according to their contribu-
tions on the development data. The algorithm re-
peatedly selects one feature each time, which con-
tributes most, and stops when adding any of the 
remaining features fails to improve the perform-
ance. 
4.4 Post-Processing 
Although a cue in the BioScope corpus always has 
only one continuous block as its scope (including 
the cue itself), the scope identifier may result in 
discontinuous scope due to independent predica-
tion in the argument identification phase. Given the 
golden negation/speculation cues, we observe that 
6.2%/9.1% of the negation/speculation scopes pre-
dicted by our scope identifier are discontinuous. 
718
 
Figure 3 demonstrates the projection of all the 
argument candidates into the word level. Accord-
ing to our argument pruning algorithm in Section 
4.2, except the words presented by the cue, the pro-
jection covers the whole sentence and each con-
stituent (LACi or RACj in Figure 3) receives a 
probability distribution of being an argument of the 
given cue in the argument identification phase. 
Since a cue is deemed inside its scope in the 
BioScope corpus, our post-processing algorithm 
first includes the cue in its scope and then starts to 
identify the left and the right scope boundaries, 
respectively. 
As shown in Figure 3, the left boundary has 
m+1 possibilities, namely the cue itself, the left-
most word of constituent LACi (1<=i<=m). Sup-
posing LACi receives probability of Pi being an 
argument, we use the following formula to deter-
mine LACk* whose leftmost word represents the 
boundary of the left scope. If k*=0, then the cue 
itself represents its left boundary. 
( )*
1 1
arg max 1
k m
i i
k i i k
k P
= = +
= ?? ?  P?
Similarly, the right boundary of the given cue 
can be decided. 
4.5 Cue Recognition 
Automatic recognition of cues of a special interest 
is the prerequisite for a scope learning system. The 
approaches to recognizing cues of a special interest 
usually fall into two categories: 1) substring 
matching approaches, which require a set of cue 
words or phrases in advance (e.g., Light et al, 
2004); 2) machine learning approaches, which 
train a classifier with either supervised or semi-
supervised learning methods (e.g., ?zg?r and 
Radev, 2009; Szarvas, 2008). Without loss of gen-
erality, we adopt a machine learning approach and 
train a classifier with supervised learning. In par-
ticular, we make an independent classification for 
each word with a BIO label to indicate whether it 
is the first word of a cue, inside a cue, or outside of 
it, respectively. 
LACm    ?.      LAC1 RAC1      ?.    RACn
m n 
Figure 3: Projecting the left and the right argument 
candidates into the word level. 
Inspired by previous studies on similar tasks 
such as WSD and nominal predicate recognition in 
shallow semantic parsing (Lee and Ng, 2002; Li et 
al., 2009), where various features on the word it-
self, surrounding words and syntactic information 
have been successfully used, we believe that such 
information is also valuable to automatic recogni-
tion of cues. Table 3 shows the features employed 
for cue recognition. In particular, we categorize 
these features into three groups: 1) features about 
the cue candidate itself (CC in short); 2) features 
about surrounding words (SW in short); and 3) 
structural features derived from the syntactic parse 
tree (SF in short).
 
Feature Remarks 
Cue Candidate (CC) related 
CC1 The cue candidate itself. (indicate) 
CC2 The stem of the cue candidate. (indicate)
CC3 The POS tag of the cue candidate. (VBP)
Surrounding Words (SW) related 
SW1 The left surrounding words with the win-
dow size of 3. (these, findings) 
SW2 The right surrounding words with the 
window size of 3. (that, corticosteroid,
resistance) 
Structural Features (SF) 
SF1 The subcategory of the candidate node.  
(VP-->VBP+SBAR) 
SF2 The subcategory of the candidate node?s 
parent. (S-->NP+VP) 
SF3 POS tag of the candidate node + Phrase 
type of its parent node + Phrase type of its 
grandpa node. (VBP + VP + S) 
Table 3: Features and their instantiations for cue recog-
nition, with VBP2,2 as the cue candidate, regarding Fig-
ure 2. 
5 Experimentation 
We have evaluated our simplified shallow seman-
tic parsing approach to negation and speculation 
scope learning on the BioScope corpus. 
5.1 Experimental Settings 
Following the experimental setting in Morante et al 
(2008) and Morante and Daelemans (2009a & 
2009b), the abstracts subcorpus is randomly di-
vided into 10 folds so as to perform 10-fold cross-
validation, while the performance on both the pa-
719
pers and clinical reports subcorpora is evaluated 
using the system trained on the whole abstracts 
subcorpus. In addition, SVMLight  is selected as 
our classifier. 
5
For cue recognition, we report its performance 
using precision/recall/F1-measure. For scope iden-
tification, we report the accuracy in PCS (Percent-
age of Correct Scopes) when the golden cues are 
given, and report precision/recall/F1-measure 
when the cues are automatically recognized. 
5.2 Experimental Results on Golden Parse 
Trees and Golden Cues 
In order to select beneficial features from the addi-
tional features proposed in Section 4.3, we ran-
domly split the abstracts subcorpus into the 
training data and the development data with pro-
portion of 4:1. After performing the greedy feature 
selection algorithm on the development data, 7 
features {CFACCP5, CP2R, CFCP1, AC1P, CP3, 
CFACCP7, AC4R} are selected consecutively for 
negation scope identification while 11 features 
{CFACCP5, AC2W, CFACCP2, CFACCP4, AC5, 
CFCP1, CFACCP7, CFACCP1, CP4, AC3P, 
CFAC2} are selected for speculation scope identi-
fication. Table 4 gives the contribution of addi-
tional features on the development data. It shows 
that the additional features significantly improve 
the performance by 11.66% in accuracy from 
74.93% to 86.59% ( ) for negation scope 
identification and improve the performance by 
11.07% in accuracy from 77.29% to 88.36% 
( ) for speculation scope identification. 
The feature selection experiments suggest that the 
features (e.g., CFACCP5, AC2W, CFCP1) related 
to neighboring words of the cue play a critical role 
for both negation and speculation scope identifica-
tion. This may be due to the fact that neighboring 
words usually imply important sentential informa-
tion. For example, ?can not be? indicates a passive 
clause while ?did not? indicates an active clause. 
2; 0.0p? < 1
1
                                                          
2; 0.0p? <
Since the additional selected features signifi-
cantly improve the performance for both negation 
and speculation scope identification, we will in-
clude those additional selected features in all the 
remaining experiments. 
 
 
5 http://svmlight.joachims.org/ 
Task Features Acc (%) 
Baseline 74.93 Negation scope 
identification +selected features 86.59 
Baseline 77.29 Speculation scope 
identification +selected features 88.36 
Table 4: Contribution of additional selected features on 
the development dataset of the abstracts subcorpus 
 
Since all the sentences in the abstracts subcorpus 
are included in the GTB1.0 corpus while we do not 
have golden parse trees for the sentences in the full 
papers and the clinical reports subcorpora, we only 
evaluate the performance of scope identification on 
the abstracts subcorpus with golden parse trees. 
Table 5 presents the performance on the abstracts 
subcorpus by performing 10-fold cross-validation. 
It shows that given golden parse trees and golden 
cues, speculation scope identification achieves 
higher performance (e.g., ~3.3% higher in accu-
racy) than negation scope identification. This is 
mainly due to the observation on the BioScope 
corpus that the scope of a speculation cue can be 
usually characterized by its POS and the syntactic 
structures of the sentence where it occurs. For ex-
ample, the scope of a verb in active voice usually 
starts at the cue itself and ends at its object (e.g., 
the speculation cue ?indicate that? in Figure 2 
scopes the fragment of ?indicate that corticoster-
oid resistance can not be explained by abnormali-
ties?). Moreover, the statistics on the abstracts 
subcorpus shows that the number of arguments per 
speculation cue is smaller than that of arguments 
per negation cue (e.g., 1.5 vs. 1.8). 
 
Task Acc (%) 
Negation scope identification 83.10 
Speculation scope identification 86.41 
Table 5: Accuracy (%) of scope identification with 
golden parse trees and golden cues on the abstracts sub-
corpus using 10-fold cross-validation 
 
It is worth nothing that we adopted the post-
processing algorithm proposed in Section 4.4 to 
ensure the continuousness of identified scope. As 
to examine the effectiveness of the algorithm, we 
abandon the proposed algorithm by simply taking 
the left and right-most boundaries of any nodes in 
the tree which are classified as in scope. Experi-
ments on the abstracts subcorpus using 10-fold 
cross-validation shows that the simple post-
processing rule gets the performance of 80.59 and 
86.08 in accuracy for negation and speculation 
720
scope identification, respectively, which is lower 
than the performance in Table 5 achieved by our 
post-processing algorithm.  
5.3 Experimental Results on Automatic 
Parse Trees and Golden Cues 
The GTB1.0 corpus contains 18,541 sentences in 
which 11,850 of them (63.91%) overlap with the 
sentences in the abstracts subcorpus6. In order to 
get automatic parse trees, we train the Berkeley 
parser with the remaining 6,691 sentences in 
GTB1.0, which achieves the performance of 85.22 
in F1-measure on the remaining 11,850 sentences 
in GTB1.0. Table 6 shows the performance of 
scope identification on automatic parse trees and 
golden cues. In addition, we also report an oracle 
performance to explore the best possible perform-
ance of our system by assuming that our scope 
finder can always correctly determine whether a 
candidate is an argument or not. That is, if an ar-
gument candidate falls within the golden scope, 
then it is a argument. This is to measure the impact 
of automatic syntactic parsing itself. Table 6 shows 
that: 
1) For both negation and speculaiton scope 
identification, automatic syntactic parsing 
lowers the performance on the abstracts 
subcorpus (e.g., from 83.10% to 81.84% in 
accuracy for negation scope identification and 
from 86.41% to 83.74% in accuracy for 
speculaiton scope identification). However, the 
performance drop shows that both negation and 
speculation scope identification are not as 
senstive to automatic syntactic parsing as 
common shallow semantic parsing, whose 
performance might decrease by about ~10 in F1-
measure (Toutanova et al, 2005). This indicates 
that scope identification via simplified shallow 
semantic parsing is robust to some variations in 
the parse trees.  
2) Although speculation scope identification 
consistently achieves higher performance than 
negaiton scope identification when golden parse 
trees are availabe, speculation scope 
identification achieves comparable performance 
with negation scope identification on the 
abstracts subcorpus and the full papers 
                                                          
6 There are a few cases where two sentences in the abstracts 
subcorpus map into one sentence in GTB1.0. 
subcorpus while speculation scope identification 
even performs ~20% lower in accuracy than 
negation scope identification on the clinical 
report subcorpus. This is largely due to that 
specuaiton scope identification is more sensitive 
to syntactic parsing errors than negation scope 
identification due to the wider scope of a 
speculation cue while the sentences of the 
clinical reports come from a different genre, 
which indicates low performance in syntactic 
parsing.  
3) Given the performance gap between the 
performance of our scope finder and the oracle 
performance, there is still much room for further 
performance improvement. 
 
Task Method Abstracts Papers Clinical
auto 81.84 62.70 85.21 Negation scope 
identification oracle 94.37 83.33 98.39 
auto 83.74 61.29 67.90 Speculation scope
identification oracle 95.69 83.72 83.29 
Table 6: Accuracy (%) of scope identification on the 
three subcorpora using automatic parser trained on 
6,691 sentences in GTB1.0 
 
Task Method Abstracts Papers Clinical
M et al (2008) 57.33 n/a n/a 
M & D (2009a) 73.36 50.26 87.27 
Our baseline 73.42 53.70 88.42 
Negation 
scope 
identification 
Our final  81.84 64.02 89.79 
M & D (2009b) 77.13 47.94 60.59 
? & R (2009) 79.89 61.13 n/a 
Our baseline 77.39 54.55 61.92 
Speculation 
scope 
identification 
Our final  83.74 63.49 68.78 
Table 7: Performance comparison of our system with 
the state-of-the-art ones in accuracy (%). Note that all 
the performances achieved on the full papers subcorpus 
and the clinical subcorpus are achieved using the whole 
GTB1.0 corpus of 18,541 sentences while all the per-
formances achieved on the abstract subcorpus are 
achieved using 6,691 sentences from GTB1.0 due to 
overlap of the abstract subcorpus with GTB1.0. 
 
Table 7 compares our performance with related 
ones. It shows that even our baseline system with 
the four basic features presented in Table 1 
achieves comparable performance with Morante et 
al. (2008) and Morante and Daelemans (2009a & 
2009b). This further indicates the appropriateness 
of our simplified shallow semantic parsing ap-
proach and the effectiveness of structured syntactic 
information on scope identification. It also shows 
that our final system significantly outperforms the 
721
state-of-the-art ones using a chunking approach, 
especially on the abstracts and full papers subcor-
pora. However, the improvement on the clinical 
reports subcorpora for negation scope identifica-
tion is much less apparent, partly due to the fact 
that the sentences in this subcorpus are much sim-
pler (with average length of 6.6 words per sentence) 
and thus a chunking approach can achieve high 
performance. Table 7 also shows that our parsing 
approach to speculation scope identification out-
performs the rule-based method in ?zg?r and 
Radev (2009), where 10-fold cross-validation is 
performed on both the abstracts and the full papers 
subcorpora. 
5.4 Experimental Results with Automatic 
Parse Trees and Automatic Cues 
So far negation/speculation cues are assumed to be 
manually annotated and available. Here we turn to 
a more realistic scenario in which cues are auto-
matically recognized. In the following, we first 
report the results of cue recognition and then the 
results of scope identification with automatic cues. 
Cue Recognition 
Task Features R (%) P (%) F1 
CC + SW 93.80 94.39 94.09Negation cue  
recognition CC+SW+SF 95.50 95.72 95.61
CC + SW 83.77 92.04 87.71Speculation cue  
recognition CC+SW+SF 84.33 93.07 88.49
Table 8: Performance of automatic cue recognition with 
gold parse trees on the abstracts subcorpus using 10-fold 
cross-validation 
 
Table 8 lists the performance of cue recognition on 
the abstracts subcorpus, assuming all words in the 
sentences as candidates. It shows that as a com-
plement to features derived from word/pos infor-
mation (CC+SW features), structural features (SF 
features) derived from the syntactic parse tree sig-
nificantly improve the performance of cue recogni-
tion by about 1.52 and 0.78 in F1-measure for 
negation and speculation cue recognition, respec-
tively, and thus included thereafter. In addition, we 
have also experimented on only these words, 
which happen to be a cue or inside a cue in the 
training data as cue candidates. However, this ex-
perimental setting achieves a lower performance 
than that when all words are considered. 
 
Task Corpus R (%) P (%) F1 
Abstracts 94.99 94.35 94.67 
Papers 90.48 87.47 88.95 
Negation cue 
recognition 
Clinical 86.81 88.54 87.67 
Abstracts 83.74 93.14 88.19 
Papers 73.02 82.31 77.39 
Speculation cue 
recognition 
Clinical 33.33 91.77 48.90 
Table 9: Performance of automatic cue recognition with 
automatic parse trees on the three subcorpora 
 
Table 9 presents the performance of cue recog-
nition achieved with automatic parse trees on the 
three subcorpora. It shows that: 
1) The performance gap of cue recognition 
between golden parse trees and automatic parse 
trees on the abstracts subcorpus is not salient 
(e.g., 95.61 vs. 94.67 in F1-measure for negation 
cues and 88.49 vs. 88.19 for speculation cues), 
largely due to the features defined for cue 
recognition are local and insenstive to syntactic 
variations. 
2) The performance of negation cue recognition is 
higher than that of speculation cue recognition 
on all the three subcorpora. This is prabably due 
to the fact that the collection of negation cue 
words or phrases is limitted while speculation 
cue words or phrases are more open. This is 
illustrated by our statistics that about only 1% 
and 1% of negation cues in the full papers and 
the clinical reports subcorpora are absent from 
the abstracts subcorpus, compared to about 6% 
and 20% for speculation cues. 
3) Unexpected, the recall of speculation cue 
recognition on the clinical reports subcorpus is 
very low (i.e., 33.33% in recall measure). This is 
probably due to the absence of about 20% 
speculation cues from the training data of the 
abstracts subcorpus. Moreover, the speculation 
cue ?or?, which accounts for about 24% of 
specuaiton cues in the clinical reports subcorpus, 
only acheives about 2% in recall largely due to 
the errors caused by the classifier trained on the 
abstracts subcorpus, where only about 11% of 
words ?or? are annotated as speculation cues. 
Scope Identification with Automatic Cue Rec-
ognition 
Table 10 lists the performance of both negation 
and speculation scope identification with automatic 
cues and automatic parse trees. It shows that auto-
matic cue recognition lowers the performance by 
722
3.34, 6.80, and 8.38 in F1-measure for negation 
scope identification on the abstracts, the full papers 
and the clinical reports subcorpora, respectively, 
while it lowers the performance by 6.50, 13.14 and 
31.23 in F1-measures for speculation scope identi-
fication on the three subcorpora, respectively, sug-
gesting the big challenge of cue recognition in the 
two scope learning tasks. 
 
Task Corpus R (%) P (%) F1 
Abstracts 78.77 78.24 78.50
Papers 58.20 56.27 57.22
Negation scope 
identification 
Clinical 80.62 82.22 81.41
Abstracts 73.34 81.58 77.24
Papers 47.51 53.55 50.35
Speculation scope 
identification 
Clinical 25.59 70.46 37.55
Table 10: Performance of both negation and speculation 
scope identification with automatic cues and automatic 
parse trees 
6 Conclusion  
In this paper we have presented a new approach to 
scope learning by formulating it as a simplified 
shallow semantic parsing problem, which has been 
extensively studied in the past few years. In par-
ticular, we regard the cue as the predicate and map 
its scope into several constituents which are 
deemed as arguments of the cue. Evaluation on the 
Bioscope corpus shows the appropriateness of our 
parsing approach and that structured syntactic in-
formation plays a critical role in capturing the 
domination relationship between a cue and its 
dominated arguments. It also shows that our pars-
ing approach outperforms the state-of-the-art 
chunking ones. Although our approach is only 
evaluated on negation and speculation scope learn-
ing here, it is portable to other kinds of scope 
learning. 
For the future work, we will explore tree kernel-
based methods to further improve the performance 
of scope learning in better capturing the structural 
information, and apply our parsing approach to 
other kinds of scope learning. 
Acknowledgments 
This research was supported by Projects 60873150, 
60970056, and 90920004 under the National Natu-
ral Science Foundation of China, Project 
20093201110006 under the Specialized Research 
Fund for the Doctoral Program of Higher Educa-
tion of China. 
References  
Xavier Carreras and Llu?s M?rquez. 2005. Introduction 
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. CoNLL? 2005. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. A 
Simple Algorithm for Identifying Negated Findings 
and Diseases in Discharge Summaries. Journal of 
Biomedical Informatics, 34: 301-310. 
Nigel Collier, Hyun Seok Park, Norihiro Ogata, et al 
1999. The GENIA Project: Corpus-Based Knowl-
edge Acquisition and Information Extraction from 
Genome Research Papers. EACL?1999. 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nos 
Csirik, and Gy?rgy Szarvas. 2010. The CoNLL-2010 
Shared Task: Learning to Detect Hedges and their 
Scope in Natural Language Text. CoNLL?2010: 
Shared Task. 
Daniel Gildea and Martha Palmer. 2002. The Necessity 
of Parsing for Predicate Argument Recognition. 
ACL?2002. 
Ilya M. Goldin and Wendy W. Chapman. 2003. Learn-
ing to Detect Negation with ?Not? in Medical Texts. 
SIGIR?2003. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic 
Role Labeling of NomBank: A Maximum Entropy 
Approach. EMNLP? 2006. 
Yoong Keok Lee and Hwee Tou Ng. 2002. An Empiri-
cal Evaluation of Knowledge Sources and Learning 
Algorithms for Word Sense Disambiguation. 
EMNLP?2002. 
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu, 
and Peide Qian. 2009. Improving Nominal SRL in 
Chinese Language with Verbal SRL Information and 
Automatic Predicate Recognition. EMNLP? 2009. 
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The Language of Bioscience: Facts, Specula-
tions, and Statements in Between. BioLink?2004. 
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific 
Literature. ACL?2007. 
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the Scope of Negation in Bio-
medical Texts. EMNLP?2008. 
723
Roser Morante and Walter Daelemans. 2009a. A 
Metalearning Approach to Processing the Scope of 
Negation. CoNLL?2009. 
Roser Morante and Walter Daelemans. 2009b. Learning 
the Scope of Hedge Cues in Biomedical Texts. 
BioNLP?2009. 
Lilja ?vrelid, Erik Velldal, and Stephan Oepen. 2010. 
Syntactic Scope Resolution in Uncertainty Analysis. 
COLING?2010. 
Arzucan ?zg?r, Dragomir R. Radev. 2009. Detecting 
Speculations and their Scopes in Scientific Text. 
EMNLP?2009. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. NAACL?2007. 
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2005. 
The Necessity of Syntactic Parsing for Semantic Role 
Labeling. IJCAI? 2005. 
Gy?rgy Szarvas. 2008. Hedge Classification in Bio-
medical Texts with a Weakly Supervised Selection of 
Keywords. ACL?2008. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, and 
J?nos Csirik. 2008. The BioScope corpus: Annota-
tion for Negation, Uncertainty and their Scope in 
Biomedical Texts. BioNLP?2008. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun?ichi Tsujii. 2005. Syntax Annotation for the 
GENIA Corpus. IJCNLP?2005 (Companion volume). 
Peter D. Turney. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised Clas-
sification of Reviews. ACL?2002. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The BioScope 
corpus: biomedical texts annotated for uncertainty, 
negation and their scopes. BMC Bioinformatics, 
9(Suppl 11):S9. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
Features for Semantic Role Labeling. EMNLP?2004. 
Nianwen Xue. 2008. Labeling Chinese Predicates with 
Semantic Roles. Computational Linguistics, 
34(2):225-255.  
724
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 882?891,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Tree Kernel-based Unified Framework                                               
for Chinese Zero Anaphora Resolution 
 
Fang Kong  Guodong Zhou*  
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology Soochow University 
{kongfang, gdzhou}@suda.edu.cn 
 
 
 
                                                          
* Corresponding author 
Abstract 
This paper proposes a unified framework for 
zero anaphora resolution, which can be di-
vided into three sub-tasks: zero anaphor detec-
tion, anaphoricity determination and 
antecedent identification. In particular, all the 
three sub-tasks are addressed using tree ker-
nel-based methods with appropriate syntactic 
parse tree structures. Experimental results on a 
Chinese zero anaphora corpus show that the 
proposed tree kernel-based methods signifi-
cantly outperform the feature-based ones. This 
indicates the critical role of the structural in-
formation in zero anaphora resolution and the 
necessity of tree kernel-based methods in 
modeling such structural information. To our 
best knowledge, this is the first systematic 
work dealing with all the three sub-tasks in 
Chinese zero anaphora resolution via a unified 
framework. Moreover, we release a Chinese 
zero anaphora corpus of 100 documents, 
which adds a layer of annotation to the manu-
ally-parsed sentences in the Chinese Treebank 
(CTB) 6.0.  
1 Introduction 
As one of the most important techniques in dis-
course analysis, anaphora resolution has been a 
focus of research in Natural Language Processing 
(NLP) for decades and achieved much success in 
English recently (e.g. Soon et al 2001; Ng and 
Cardie 2002; Yang et al 2003, 2008; Kong et al 
2009).  
However, there is little work on anaphora reso-
lution in Chinese. A major reason for this phe-
nomenon is that Chinese, unlike English, is a pro-
drop language, whereas in English, definite noun 
phrases (e.g. the company) and overt pronouns (e.g. 
he) are frequently employed as referring expres-
sions, which refer to preceding entities. Kim (2000) 
compared the use of overt subjects in English and 
Chinese. He found that overt subjects occupy over 
96% in English, while this percentage drops to 
only 64% in Chinese. This indicates the prevalence 
of zero anaphors in Chinese and the necessity of 
zero anaphora resolution in Chinese anaphora reso-
lution. Since zero anaphors give little hints (e.g. 
number or gender) about their possible antecedents, 
zero anaphora resolution is much more challenging 
than traditional anaphora resolution. 
Although Chinese zero anaphora has been 
widely studied in the linguistics research (Li and 
Thompson 1979; Li 2004), only a small body of 
prior work in computational linguistics deals with 
Chinese zero anaphora resolution (Converse 2006; 
Zhao and Ng 2007). Moreover, zero anaphor de-
tection, as a critical component for real applica-
tions of zero anaphora resolution, has been largely 
ignored.  
This paper proposes a unified framework for 
Chinese zero anaphora resolution, which can be 
divided into three sub-tasks: zero anaphor detec-
tion, which detects zero anaphors from a text, ana-
phoricity determination, which determines whether 
a zero anaphor is anaphoric or not, and antecedent 
identification, which finds the antecedent for an 
anaphoric zero anaphor. To our best knowledge, 
this is the first systematic work dealing with all the 
three sub-tasks via a unified framework. Moreover, 
we release a Chinese zero anaphora corpus of 100 
documents, which adds a layer of annotation to the 
882
manually-parsed sentences in the Chinese Tree-
bank (CTB) 6.0. This is done by assigning ana-
phoric/non-anaphoric zero anaphora labels to the 
null constituents in a parse tree. Finally, this paper 
illustrates the critical role of the structural informa-
tion in zero anaphora resolution and the necessity 
of tree kernel-based methods in modeling such 
structural information. 
The rest of this paper is organized as follows. 
Section 2 briefly describes the related work on 
both zero anaphora resolution and tree kernel-
based anaphora resolution. Section 3 introduces the 
overwhelming problem of zero anaphora in Chi-
nese and our developed Chinese zero anaphora 
corpus, which is available for research purpose. 
Section 4 presents our tree kernel-based unified 
framework in zero anaphora resolution. Section 5 
reports the experimental results. Finally, we con-
clude our work in Section 6. 
2 Related Work 
This section briefly overviews the related work on 
both zero anaphora resolution and tree kernel-
based anaphora resolution. 
2.1 Zero anaphora resolution 
Although zero anaphors are prevalent in many lan-
guages, such as Chinese, Japanese and Spanish, 
there only have a few works on zero anaphora 
resolution. 
Zero anaphora resolution in Chinese 
Converse (2006) developed a Chinese zero anaph-
ora corpus which only deals with zero anaphora 
category ?-NONE- *pro*? for dropped sub-
jects/objects and ignores other categories, such as 
?-NONE- *PRO*? for non-overt subjects in non-
finite clauses. Besides, Converse (2006) proposed 
a rule-based method to resolve the anaphoric zero 
anaphors only. The method did not consider zero 
anaphor detection and anaphoric identification, and 
performed zero anaphora resolution using the 
Hobbs algorithm (Hobbs, 1978), assuming the 
availability of golden anaphoric zero anaphors and 
golden parse trees.  
Instead, Zhao and Ng (2007) proposed feature-
based methods to zero anaphora resolution on the 
same corpus from Convese (2006). However, they 
only considered zero anaphors with explicit noun 
phrase referents and discarded those with split an-
tecedents or referring to events. Moreover, they 
focused on the sub-tasks of anaphoricity determi-
nation and antecedent identification. For zero ana-
phor detection, a simple heuristic rule was 
employed. Although this rule can recover almost 
all the zero anaphors, it suffers from very low pre-
cision by introducing too many false zero anaphors 
and thus leads to low performance in anaphoricity 
determination, much due to the imbalance between 
positive and negative training examples. 
Zero anaphora resolution in Japanese 
Seki et al (2002) proposed a probabilistic model 
for the sub-tasks of anaphoric identification and 
antecedent identification with the help of a verb 
dictionary. They did not perform zero anaphor de-
tection, assuming the availability of golden zero 
anaphors. Besides, their model needed a large-
scale corpus to estimate the probabilities to prevent 
them from the data sparseness problem.  
Isozaki and Hirao (2003) explored some ranking 
rules and a machine learning method on zero 
anaphora resolution. However, they assumed that 
zero anaphors were already detected and each zero 
anaphor?s grammatical case was already deter-
mined by a zero anaphor detector.  
Iida et al (2006) explored a machine learning 
method for the sub-task of antecedent identifica-
tion using rich syntactic pattern features, assuming 
the availability of golden anaphoric zero anaphors. 
Sasano et al (2008) proposed a fully-lexicalized 
probabilistic model for zero anaphora resolution, 
which estimated case assignments for the overt 
case components and the antecedents of zero ana-
phors simultaneously. However, this model needed 
case frames to detect zero anaphors and a large-
scale corpus to construct these case frames auto-
matically.  
For Japanese zero anaphora, we do not see any 
reports about zero anaphora categories. Moreover, 
all the above related works we can find on Japa-
nese zero anaphora resolution ignore zero anaphor 
detection, focusing on either anaphoricity determi-
nation or antecedent identification. Maybe, it is 
easy to detect zero anaphors in Japanese. However, 
it is out of the scope of our knowledge and this 
paper.  
Zero anaphora resolution in Spanish 
As the only work we can find, Ferrandez and Peral 
(2000) proposed a hand-engineered rule-based 
method for both anaphoricity determination and 
883
antecedent identification. That is, they ignored zero 
anaphor detection. Besides, they only dealt with 
zero anaphors that were in the subject position. 
2.2 Tree kernel-based anaphora resolution 
Although there is no research on tree kernel-based 
zero anaphora resolution in the literature, tree ker-
nel-based methods have been explored in tradi-
tional anaphora resolution to certain extent and 
achieved comparable performance with the domi-
nated feature-based ones. One main advantage of 
kernel-based methods is that they are very effec-
tive at reducing the burden of feature engineering 
for structured objects. Indeed, the kernel-based 
methods have been successfully applied to mine 
structural information in various NLP techniques 
and applications, such as syntactic parsing (Collins 
and Duffy 2001; Moschitti 2004), semantic rela-
tion extraction (Zelenko et al 2003; Zhao and 
Grishman 2005; Zhou et al 2007; Qian et al 2008), 
and semantic role labeling (Moschitti 2004).  
Representative works in tree kernel-based 
anaphora resolution include Yang et al (2006) and 
Zhou et al(2008). Yang et al (2006) employed a 
convolution tree kernel on anaphora resolution of 
pronouns. In particular, a document-level syntactic 
parse tree for an entire text was constructed by at-
taching the parse trees of all its sentences to a new-
added upper node. Examination of three parse tree 
structures using different construction schemes 
(Min-Expansion, Simple-Expansion and Full-
Expansion) on the ACE 2003 corpus showed 
promising results. However, among the three con-
structed parse tree structures, there exists no obvi-
ous overwhelming one, which can well cover 
structured syntactic information. One problem with 
this tree kernel-based method is that all the con-
structed parse tree structures are context-free and 
do not consider the information outside the sub-
trees. To overcome this problem, Zhou et al (2008) 
proposed a dynamic-expansion scheme to auto-
matically construct a proper parse tree structure for 
anaphora resolution of pronouns by taking predi-
cate- and antecedent competitor-related informa-
tion into consideration. Besides, they proposed a 
context-sensitive convolution tree kernel to com-
pute the similarity between the parse tree structures. 
Evaluation on the ACE 2003 corpus showed that 
the dynamic-expansion scheme can well cover 
necessary structural information in the parse tree 
for anaphora resolution of pronouns and the con-
text-sensitive convolution tree kernel much outper-
formed other tree kernels. 
3 Task Definition 
This section introduces the phenomenon of zero 
anaphora in Chinese and our developed Chinese 
zero anaphora corpus. 
3.1 Zero anaphora in Chinese 
A zero anaphor is a gap in a sentence, which refers 
to an entity that supplies the necessary information 
for interpreting the gap. Figure 1 illustrates an ex-
ample sentence from Chinese TreeBank (CTB) 6.0 
(File ID=001, Sentence ID=8). In this example, 
there are four zero anaphors denoted as ?i (i=1, 
2, ?4). Generally, zero anaphors can be under-
stood from the context and do not need to be speci-
fied. 
A zero anaphor can be classified into either ana-
phoric or non-anaphoric, depending on whether it 
has an antecedent in the discourse. Typically, a 
zero anaphor is non-anaphoric when it refers to an 
extra linguistic entity (e.g. the first or second per-
son in a conversion) or its referent is unspecified in 
the context. Among the four anaphors in Figure 1, 
zero anaphors ? 1 and ? 4 are non-anaphoric 
while zero anaphors ?2 and ?3 are anaphoric, 
referring to noun phrase ?????/building ac-
tion? and noun phrase ??????/new district 
managing committee? respectively. 
Chinese zero anaphora resolution is very diffi-
cult due to following reasons: 1) Zero anaphors 
give little hints (e.g. number or gender) about their 
possible antecedents. This makes antecedent iden-
tification much more difficult than traditional 
anaphora resolution. 2) A zero anaphor can be ei-
ther anaphoric or non-anaphoric. In our corpus de-
scribed in Section 3.2, about 60% of zero anaphors 
are non-anaphoric. This indicates the importance 
of anaphoricity determination. 3) Zero anaphors 
are not explicitly marked in a text. This indicates 
the necessity of zero anaphor detection, which has 
been largely ignored in previous research and has 
proved to be difficult in our later experiments. 
 
884
 Figure 1: An example sentence from CTB 6.0, which contains four zero anaphors 
(the example is : ???????????????????????????????????????
???????????????????????/ In order to standardize the building action and prevent the 
inorder phenomenon, the standing committee of new zone annouced a series of files to standardize building market 
based on the related provisions of China and Shanghai in time, and the realities of the development of Pudong are 
considered. ) 
3.2 Zero anaphora corpus in Chinese 
Due to lack of an available zero anaphora corpus 
for research purpose, we develop a Chinese zero 
anaphora corpus of 100 documents from CTB 6.0, 
which adds a layer of annotation to the manually-
parsed sentences. Hoping the public availability of 
this corpus can push the research of zero anaphora 
resolution in Chinese and other languages.  
  Figure 2: An example sentence annotated in CTB 6.0 
ID Cate-gory Description 
AZ
As ZAs
1 -NONE-  *T* 
Used in topicalization and 
object preposing con-
structions 
6 742
2 -NONE-  * 
Used in raising and pas-
sive constructions 1 2 
3 -NONE-  *PRO*
Used in control structures. 
The *PRO* cannot be 
substituted by an overt 
constituent. 
219 399
4 -NONE-  *pro* 
for dropped subject or 
object. 394 449
5 -NONE-  *RNR*
Used for right node rais-
ing (Cataphora) 0 36 
6 Others Other unknown empty categories 92 92 
Total (100 documents, 35089 words) 712 1720
Table 1: Statistics on different categories of  zero 
anaphora (AZA and ZA indicates anaphoric zero ana-
phor and zero anaphor respectively) 
885
Figure 2 illustrates an example sentence anno-
tated in CTB 6.0, where the special tag ?-NONE-? 
represents a null constituent and thus the occur-
rence of a zero anaphor. In our developed corpus, 
we need to annotate anaphoric zero anaphors using 
those null constituents with the special tag of ?-
NONE-?. 
Table 1 gives the statistics on all the six catego-
ries of zero anaphora. Since we do not consider 
zero cataphora in the current version, we simply 
redeem them non-anaphoric. It shows that among 
1720 zero anaphors, only 712 (about 40%) are 
anaphoric. This suggests the importance of ana-
phoricity determination in zero anaphora resolution. 
Table 3 further shows that, among 712 anaphoric 
zero anaphors, 598 (84%) are intra-sentential and 
no anaphoric zero anaphors have their antecedents 
occurring two sentences before. 
Sentence distance AZAs
0 598 
1 114 
>=2 0 
Table 3 Distribution of anaphoric zero anaphors over 
sentence distances 
Figure 3 shows an example in our corpus corre-
sponding to Figure 2. For a non-anaphoric zero 
anaphor, we replace the null constituent with ?E-i 
NZA?, where i indicates the category of zero 
anaphora, with ?1? referring to ?-NONE *T*? 
etc. For an anaphoric zero anaphor, we replace it 
with ?E-x-y-z-i AZA?, where x indicates the sen-
tence id of its antecedent, y indicates the position 
of the first word of its antecedent in the sentence, z 
indicates the position of the last word of its antece-
dent in the sentence, and i indicates the category id 
of the null constituent. 
 Figure 3: an example sentence annotated in our corpus 
4 Tree Kernel-based Framework 
This section presents the tree kernel-based unified 
framework for all the three sub-tasks in zero 
anaphora resolution. For each sub-task, different 
parse tree structures are constructed. In particular, 
the context-sensitive convolution tree kernel, as 
proposed in Zhou et al (2008), is employed to 
compute the similarity between two parse trees via 
the SVM toolkit SVMLight. 
In the tree kernel-based framework, we perform 
the three sub-tasks, zero anaphor detection, ana-
phoricity determination and antecedent identifica-
tion in a pipeline manner. That is, given a zero 
anaphor candidate Z, the zero anaphor detector is 
first called to determine whether Z is a zero ana-
phor or not. If yes, the anaphoricity determiner is 
then invoked to determine whether Z is an ana-
phoric zero anaphor. If yes, the antecedent identi-
fier is finally awaked to determine its antecedent. 
In the future work, we will explore better ways of 
integrating the three sub-tasks (e.g. joint learning). 
4.1 Zero anaphor detection 
At the first glance, it seems that a zero anaphor can 
occur between any two constituents in a parse tree. 
Fortunately, an exploration of our corpus shows 
that a zero anaphor always occurs just before a 
predicate1 phrase node (e.g. VP). This phenome-
non has also been employed in Zhao and Ng (2007) 
in generating zero anaphor candidates. In particular, 
if the predicate phrase node occurs in a coordinate 
structure or is modified by an adverbial node, we 
only need to consider its parent. As shown in Fig-
ure 1, zero anaphors may occur immediately to the 
left of??/guide, ??/avoid, ??/appear, ??
/according to, ?? /combine, ?? /promulgate, 
which cover the four true zero anaphors. Therefore, 
it is simple but reliable in applying above heuristic 
rules to generate zero anaphor candidates. 
Given a zero anaphor candidate, it is critical to 
construct a proper parse tree structure for tree ker-
nel-based zero anaphor detection. The intuition 
behind our parser tree structure for zero anaphor 
detection is to keep the competitive information 
                                                          
1 The predicate in Chinese can be categorized into verb predi-
cate, noun predicate and preposition predicate. In our corpus, 
about 93% of the zero anaphors are driven by verb predicates. 
In this paper, we only explore zero anaphors driven by verb 
predicates. 
886
about the predicate phrase node and the zero ana-
phor candidate as much as possible. In particular, 
the parse tree structure is constructed by first keep-
ing the path from the root node to the predicate 
phrase node and then attaching all the immediate 
verbal phrase nodes and nominal phrase nodes. 
Besides, for the sub-tree rooted by the predicate 
phrase node, we only keep those paths ended with 
verbal leaf nodes and the immediate verbal and 
nominal nodes attached to these paths. Figure 4 
shows an example of the parse tree structure corre-
sponding to Figure 1 with the zero anaphor candi-
date ?2 in consideration. 
During training, if a zero anaphor candidate has 
a counterpart in the same position in the golden 
standard corpus (either anaphoric or non-
anaphoric), a positive instance is generated. Oth-
erwise, a negative instance is generated. During 
testing, each zero anaphor candidate is presented to 
the learned zero anaphor detector to determine 
whether it is a zero anaphor or not. Besides, since a 
zero anaphor candidate is generated when a predi-
cate phrase node appears, there may be two or 
more zero anaphor candidates in the same position. 
However, there is normally one zero anaphor in the 
same position. Therefore, we just select the one 
with maximal confidence as the zero anaphor in 
the position and ignore others, if multiple zero 
anaphor candidates occur in the same position. 
 Figure 4: An example parse tree structure for zero ana-
phor detection with the predicate phrase node and the 
zero anaphor candidate ?2  in black 
4.2 Anaphoricity determination 
To determine whether a zero anaphor is anaphoric 
or not, we limit the parse tree structure between the 
previous predicate phrase node and the following 
predicate phrase node. Besides, we only keep those 
verbal phrase nodes and nominal phrase nodes. 
Figure 5 illustrates an example of the parse tree 
structure for anaphoricity determination, corre-
sponding to Figure 1 with the zero anaphor ?2 in 
consideration.   
VP
IPVV
?? NP-SBJ VP
NN
NP-OBJ
?? NP
??
VV
prevent
appear
phenomenon  
Figure 5: An example parse tree structure for anaphoric-
ity determination with the zero anaphor ?2 in consid-
eration 
4.3 Antecedent identification 
To identify an antecedent for an anaphoric zero 
anaphor, we adopt the Dynamic Expansion Tree, 
as proposed in Zhou et al (2008), which takes 
predicate- and antecedent competitor-related in-
formation into consideration. Figure 6 illustrates an 
example parse tree structure for antecedent identi-
fication, corresponding to Figure 1 with the ana-
phoric zero anaphor ? 2 and the antecedent 
candidate ?????/building action? in consid-
eration.  
 Figure 6: An example parse tree structure for antecedent 
identification with the anaphoric zero anaphor ?2 and 
the antecedent candidate ?????/building action? in 
consideration 
In this paper, we adopt a similar procedure as 
Soon et al (2001) in antecedent identification. Be-
887
sides, since all the anaphoric zero anaphors have 
their antecedents at most one sentence away, we 
only consider antecedent candidates which are at 
most one sentence away. In particular, a document-
level parse tree for an entire document is con-
structed by attaching the parse trees of all its sen-
tences to a new-added upper node, as done in Yang 
et al (2006), to deal with inter-sentential ones. 
5 Experimentation and Discussion 
We have systematically evaluated our tree kernel-
based unified framework on our developed Chi-
nese zero anaphora corpus, as described in Section 
3.2. Besides, in order to focus on zero anaphor 
resolution itself and compare with related work, all 
the experiments are done on golden parse trees 
provided by CTB 6.0. Finally, all the performances 
are achieved using 5-fold cross validation. 
5.1 Experimental results 
Zero anaphor detection 
Table 4 gives the performance of zero anaphor de-
tection, which achieves 70.05%, 83.24% and 76.08 
in precision, recall and F-measure, respectively. 
Here, the lower precision is much due to the simple 
heuristic rules used to generate zero anaphors can-
didates. In fact, the ratio of positive and negative 
instances reaches about 1:12. However, this ratio is 
much better than that (1:30) using the heuristic rule 
as described in Zhao and Ng (2007). It is also 
worth to point out that lower precision higher re-
call is much beneficial than higher precision lower 
recall as higher recall means less filtering of true 
zero anaphors and we can still rely on anaphoricity 
determination to filter out those false zero ana-
phors introduced by lower precision in zero ana-
phor detection. 
P% R% F 
70.05 83.24 76.08 
Table 4: Performance of zero anaphor detection 
Anaphoricity determination 
Table 5 gives the performance of anaphoricity de-
termination. It shows that anaphoricity determina-
tion on golden zero anaphors achieves very good 
performance of 89.83%, 84.21% and 86.93 in pre-
cision, recall and F-measure, respectively, although 
useful information, such as gender and number, is 
not available in anaphoricity determination. This 
indicates the critical role of the structural informa-
tion in anaphoricity determination of zero anaphors. 
It also shows that anaphoricity determination on 
automatic zero anaphor detection achieves 77.96%, 
53.97% and 63.78 in precision, recall and F-
measure, respectively. In comparison with ana-
phoricity determination on golden zero anaphors, 
anaphoricity determination on automatic zero ana-
phor detection lowers the performance by about 23 
in F-measure. This indicates the importance and 
the necessity for further research in zero anaphor 
detection. 
 P% R% F 
golden zero anaphors 89.83 84.21 86.93
zero anaphor detection 77.96 53.97 63.78
Table 5: Performance of anaphoricity determination 
Antecedent identification 
Table 6 gives the performance of antecedent iden-
tification given golden zero anaphors. It shows that 
antecedent identification on golden anaphoric zero 
anaphors achieves 88.93%, 68.36% and 77.29 in 
precision, recall and F-measure, respectively. It 
also shows that antecedent identification on auto-
matic anaphoricity determination achieves 80.38%, 
47.28% and 59.24 in precision, recall and F-
measure, respectively, with a decrease of about 8% 
in precision, about 21% in recall and about 18% in 
F-measure, in comparison with antecedent identifi-
cation on golden anaphoric zero anaphors. This 
indicates the critical role of anaphoricity determi-
nation in antecedent identification.  
 
 P% R% F 
golden anaphoric zero ana-
phors 
88.90 68.36 77.29
anaphoricity determination 80.38 47.28 59.54
Table 6: Performance of antecedent identification given 
golden zero anaphors 
Overall: zero anaphora resolution 
Table 7 gives the performance of overall zero 
anaphora resolution with automatic zero anaphor 
detection, anaphoricity determination and antece-
dent identification. It shows that our tree kernel-
based framework achieves 77.66%, 31.74% and 
45.06 in precision, recall and F-measure. In com-
parison with Table 6, it shows that the errors 
caused by automatic zero anaphor detection de-
crease the performance of overall zero anaphora 
resolution by about 14 in F-measure, in compari-
son with golden zero anaphors. 
888
 
P% R% F 
77.66 31.74 45.06 
Table 7: Performance of zero anaphora resolution 
Figure 7 shows the learning curve of zero 
anaphora resolution with the increase of the num-
ber of the documents in experimentation, with the 
horizontal axis the number of the documents used 
and the vertical axis the F-measure. It shows that 
the F-measure is about 42.5 when 20 documents 
are used in experimentation. This figure increases 
very fast to about 45 when 50 documents are used 
while further increase of documents only slightly 
improves the performance.  
auto ZA and AZA
41
42
43
44
45
46
20 30 40 50 60 70 80 90 100  Figure 7: Learning curve of zero anaphora resolution 
over the number of the documents in experimentation 
Table 8 shows the detailed performance of zero 
anaphora resolution over different sentence dis-
tance between a zero anaphor and its antecedent. It 
is expected that both the precision and the recall of 
intra-sentential resolution are much higher than 
those of inter-sentential resolution, largely due to 
the much more dependency of intra-sentential an-
tecedent identification on the parse tree structures.  
Sentence distance P% R% F 
0 85.12 33.28 47.85
1 46.55 23.64 31.36
2 - - - 
Table 8: Performance of zero anaphora resolution over 
sentence distances 
Table 9 shows the detailed performance of zero 
anaphora resolution over the two major zero 
anaphora categories, ?-NONE- *PRO*? and ?-
NONE- *pro*?. It shows that our tree kernel-based 
framework achieves comparable performance on 
them, both with high precision and low recall. This 
is in agreement with the overall performance. 
ID Category P% R% F 
3 -NONE-  *PRO* 79.37 34.23 47.83
4 -NONE-  *pro* 77.03 30.82 44.03
Table 9: Performance of zero anaphora resolution over 
major zero anaphora categories 
5.2 Comparison with previous work 
As a representative in Chinese zero anaphora reso-
lution, Zhao and Ng (2007) focused on anaphoric-
ity determination and antecedent identification 
using feature-based methods. In this subsection, we 
will compare our tree kernel-based framework with 
theirs in details. 
Corpus 
Zhao and Ng (2007) used a private corpus from 
Converse (2006). Although their corpus contains 
205 documents from CBT 3.0, it only deals with 
the zero anaphors under the zero anaphora cate-
gory of ?-NONE- *pro*? for dropped sub-
jects/objects. Furthermore, Zhao and Ng (2007) 
only considered zero anaphors with explicit noun 
phrase referents and discarded zero anaphors with 
split antecedents (i.e. split into two separate noun 
phrases) or referring to entities. As a result, their 
corpus is only about half of our corpus in the num-
ber of zero anaphors and anaphoric zero anaphors. 
Besides, our corpus deals with all the types of zero 
anaphors and all the categories of zero anaphora 
except zero cataphora. 
Method 
Zhao and Ng (2007) applied feature-based methods 
on anaphoricity determination and antecedent iden-
tification with most of features structural in nature. 
For zero anaphor detection, they used a very sim-
ple heuristic rule to generate zero anaphor candi-
dates. Although this rule can recover almost all the 
zero anaphors, it suffers from very low precision 
by introducing too many false zero anaphors and 
thus may lead to low performance in anaphoricity 
determination, much due to the imbalance between 
positive and negative training examples with the 
ratio up to about 1:30.  
In comparison, we propose a tree kernel-based 
unified framework for all the three sub-tasks in 
zero anaphora resolution. In particular, different 
parse tree structures are constructed for different 
sub-tasks. Besides, a context sensitive convolution 
tree kernel is employed to directly compute the 
similarity between the parse trees. 
For fair comparison with Zhao and Ng (2007), 
we duplicate their system and evaluate it on our 
developed Chinese zero anaphora corpus, using the 
same J48 decision tree learning algorithm in Weka 
and the same feature sets for anaphoricity determi-
nation and antecedent identification.  
889
Table 10 gives the performance of the feature-
based method, as described in Zhao and Ng (2007), 
in anaphoricity determination on our developed 
corpus. In comparison with the tree kernel-based 
method in this paper, the feature-based method 
performs about 16 lower in F-measure, largely due 
to the difference in precision (63.61% vs 89.83%), 
when golden zero anaphors are given. It also 
shows that, when our tree kernel-based zero ana-
phor detector is employed 2 , the feature-based 
method gets much lower precision with a gap of 
about 31%, although it achieves slightly higher 
recall.  
 P% R% F 
golden zero anaphors 63.61 79.71 70.76
zero anaphor detection 46.17 57.69 51.29
Table 10: Performance of the feature-based method 
(Zhao and Ng 2007) in anaphoricity determination on 
our developed corpus 
 P% R% F 
golden anaphoric zero ana-
phors 
77.45 51.97 62.20 
golden zero anaphpors and 
feature-based anaphoricity 
determination 
75.17 29.69 42.57 
overall: tree kernel-based 
zero anaphor detection and 
feature-based anaphoricity 
determination 
70.67 23.64 35.43 
Table 11: Performance of the feature-based method 
(Zhao and Ng 2007) in antecedent identification on our 
developed corpus  
Table 11 gives the performance of the feature-
based method, as described in Zhao and Ng (2007), 
in antecedent identification on our developed cor-
pus. In comparison with our tree kernel-based 
method, it shows that 1) when using golden ana-
phoric zero anaphors, the feature-based method 
performs about 11%, 17% and 15 lower in preci-
sion, recall and F-measure, respectively; 2) when 
golden zero anaphors are given and feature-based 
anaphoricity determination is applied, the feature-
based method performs about 5%, 18% and 17 
lower in precision, recall and F-measure, respec-
tively; and 3) when tree kernel-based zero anaphor 
detection and feature-based anaphoricity determi-
nation are applied, the feature-based method per-
                                                          
2 We do not apply the simple heuristic rule, as adopted in Zhao 
and Ng (2007), in zero anaphor detection, due to its much 
lower performance, for fair comparison on the other two sub-
tsaks.. 
forms about 7%, 8% and 10 lower in precision, 
recall and F-measure, respectively.  
In summary, above comparison indicates the 
critical role of the structural information in zero 
anaphora resolution, given the fact that most of 
features in the feature-based methods in Zhao and 
Ng (2007) are also structural, and the necessity of 
tree kernel methods in modeling such structural 
information, even if more feature engineering in 
the feature-based methods may improve the per-
formance to a certain extent. 
6 Conclusion and Further Work 
This paper proposes a tree kernel-based unified 
framework for zero anaphora resolution, which can 
be divided into three sub-tasks: zero anaphor de-
tection, anaphoricity determination and antecedent 
identification. 
The major contributions of this paper include: 1) 
We release a wide-coverage Chinese zero anaphora 
corpus of 100 documents, which adds a layer of 
annotation to the manually-parsed sentences in the 
Chinese Treebank (CTB) 6.0. 2) To our best 
knowledge, this is the first systematic work dealing 
with all the three sub-tasks in Chinese zero anaph-
ora resolution via a unified framework. 3) Em-
ployment of tree kernel-based methods indicates 
the critical role of the structural information in zero 
anaphora resolution and the necessity of tree kernel 
methods in modeling such structural information.  
In the future work, we will systematically evalu-
ate our framework on automatically-generated 
parse trees, construct more effective parse tree 
structures for different sub-tasks of zero anaphora 
resolution, and explore joint learning among the 
three sub-tasks.  
Besides, we only consider zero anaphors driven 
by a verb predicate phrase node in this paper. In 
the future work, we will consider other situations. 
Actually, among the remaining 7% zero anaphors, 
about 5% are driven by a preposition phrase (PP) 
node, and 2% are driven by a noun phrase (NP) 
node.  However, our preliminary experiments show 
that simple inclusion of those PP-driven and NP-
driven zero anaphors will largely increase the im-
balance between positive and negative instances, 
which significantly decrease the performance.  
Finally, we will devote more on further develop-
ing our corpus, with the ultimate mission of anno-
tating all the documents in CBT 6.0.    
890
Acknowledgments 
This research was supported by Projects 60873150,  
90920004 and 61003153 under the National Natu-
ral Science Foundation of China. 
References  
S. Converse. 2006. Pronominal Anaphora Resolution in 
Chinese. Ph.D. Thesis, Department of Computer and 
Information Science. University of Pennsylvania. 
M. Collins and N. Duffy. 2001. Convolution kernels for 
natural language. NIPS?2001:625-632.  
A. Ferrandez and J. Peral. 2000. A computational ap-
proach to zero-pronouns in Spanish. ACL'2000:166-
172. 
R. Iida, K. Inui, and Y. Matsumoto. 2006. Exploiting 
syntactic patterns as clues in zero-anaphora resolu-
tion. COLING-ACL'2006:625-632 
H. Isozaki and T. Hirao. 2003. Japanese zero pronoun 
resolution based on ranking rules and machine 
learning. EMNLP'2003:184-191 
F. Kong, G.D. Zhou and Q.M. Zhu. 2009 Employing the 
Centering Theory in Pronoun Resolution from the 
Semantic Perspective. EMNLP?2009: 987-996 
C. N. Li and S. A. Thompson. 1979. Third-person pro-
nouns and zero-anaphora in Chinese discourse. Syn-
tax and Semantics, 12:311-335. 
W. Li. 2004. Topic chains in Chinese discourse. Dis-
course Processes, 37(1):25-45. 
A. Moschitti. 2004. A Study on Convolution Kernels for 
Shallow Semantic Parsing, ACL?2004.  
L.H. Qian, G.D. Zhou, F. Kong, Q.M. Zhu and P.D. 
Qian. 2008. Exploiting constituent dependencies for 
tree kernel-based semantic relation extraction. 
COLING?2008:697-704  
K. Seki, A. Fujii, and T. Ishikawa. 2002. A probabilistic 
method for analyzing Japanese anaphora intergrat-
ing zero pronoun detection and resolution. 
COLING'2002:911-917 
R. Sasano. D. Kawahara and S. Kurohashi. 2008. A 
fully-lexicalized probabilistic model for Japanese 
zero anaphora resolution. COLING'2008:769-776 
W.M. Soon, H.T. Ng and D. Lim. 2001. A machine 
learning approach to coreference resolution of noun 
phrase. Computational Linguistics, 2001, 27(4):521-
544. 
V. Ng and C. Cardie 2002. Improving machine learning 
approaches to coreference resolution. ACL?2002: 
104-111 
X.F. Yang, G.D. Zhou, J. Su and C.L. Chew. 2003. 
Coreference Resolution Using Competition Learning 
Approach. ACL?2003:177-184 
X.F. Yang, J. Su and C.L. Tan 2008. A Twin-Candidate 
Model for Learning-Based Anaphora Resolution. 
Computational Linguistics 34(3):327-356 
N. Xue, F. Xia, F.D. Chiou and M. Palmer. 2005. The 
Penn Chinese TreeBank: Phrase structure annotation 
of a large corpus. Natural Language Engineering, 
11(2):207-238. 
X.F. Yang, J. Su and C.L. Tan. 2006. Kernel-based 
pronoun resolution with structured syntactic knowl-
edge. COLING-ACL'2006:41-48. 
D. Zelenko, A. Chinatsu and R. Anthony. 2003. Kernel 
methods for relation extraction. Journal of Machine 
Learning Research, 3(2003):1083-1106  
S. Zhao and H.T. Ng. 2007. Identification and Resolu-
tion of Chinese Zero Pronouns: A Machine Learning 
Approach. EMNLP-CoNLL'2007:541-550. 
S. Zhao and R. Grishman. 2005. Extracting relations 
with integrated information using kernel methods. 
ACL?2005:419-426  
G.D. Zhou, F. Kong and Q.M. Zhu. 2008. Context-
sensitive convolution tree kernel for pronoun resolu-
tion. IJCNLP'2008:25-31 
G.D. Zhou, M. Zhang, D.H. Ji and Q.M. Zhu. 2007. 
Tree kernel-based relation extraction with context-
sensitive structured parse tree information. EMNLP-
CoNLL?2007:728-736  
891
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 909?919,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Cache-based Document-level Statistical Machine Translation 
 
 
Zhengxian Gong1 Min Zhang2 Guodong Zhou1* 
1 School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
2 Institute for Infocomm Research, Singapore 138632 
{zhxgong, gdzhou}@suda.edu.cn mzhang@i2r.a-star.edu.sg 
  
 
Abstract 
Statistical machine translation systems are 
usually trained on a large amount of bilingual 
sentence pairs and translate one sentence at a 
time, ignoring document-level information. In 
this paper, we propose a cache-based approach 
to document-level translation. Since caches 
mainly depend on relevant data to supervise 
subsequent decisions, it is critical to fill the 
caches with highly-relevant data of a reasonable 
size. In this paper, we present three kinds of 
caches to store relevant document-level infor-
mation: 1) a dynamic cache, which stores bilin-
gual phrase pairs from the best translation 
hypotheses of previous sentences in the test 
document; 2) a static cache, which stores rele-
vant bilingual phrase pairs extracted from simi-
lar bilingual document pairs (i.e. source 
documents similar to the test document and 
their corresponding target documents) in the 
training parallel corpus; 3) a topic cache, which 
stores the target-side topic words related with 
the test document in the source-side. In particu-
lar, three new features are designed to explore 
various kinds of document-level information in 
above three kinds of caches. Evaluation shows 
the effectiveness of our cache-based approach 
to document-level translation with the perfor-
mance improvement of 0.81 in BLUE score 
over Moses. Especially, detailed analysis and 
discussion are presented to give new insights to 
document-level translation. 
1 Introduction 
During last decade, tremendous work has been 
done to improve the quality of statistical machine 
__________________ 
* Corresponding author. 
translation (SMT) systems. However, there is still 
a huge performance gap between the state-of-the-
art SMT systems and human translators. Bond 
(2002) suggested nine ways to improve machine 
translation by imitating the best practices of human 
translators (Nida, 1964), with parsing the entire 
document before translation as the first priority. 
However, most SMT systems still treat parallel 
corpora as a list of independent sentence-pairs and 
ignore document-level information.  
Document-level information can and should be 
used to help document-level machine translation. 
At least, the topic of a document can help choose 
specific translation candidates, since when taken 
out of the context from their document, some 
words, phrases and even sentences may be rather 
ambiguous and thus difficult to understand. Anoth-
er advantage of document-level machine transla-
tion is its ability in keeping a consistent translation.  
However, document-level translation has drawn 
little attention from the SMT research community.  
The reasons are manifold. First of all, most of pa-
rallel corpora lack the annotation of document 
boundaries (Tam, 2007). Secondly, although it is 
easy to incorporate a new feature into the classical 
log-linear model (Och, 2003), it is difficult to cap-
ture document-level information and model it via 
some simple features. Thirdly, reference transla-
tions of a test document written by human transla-
tors tend to have flexible expressions in order to 
avoid producing monotonous texts. This makes the 
evaluation of document-level SMT systems ex-
tremely difficult.  
Tiedemann (2010) showed that the repetition 
and consistency are very important when modeling 
natural language and translation. He proposed to 
employ cache-based language and translation 
models in a phrase-based SMT system for domain 
909
 adaptation. Especially, the cache in the translation 
model dynamically grows up by adding bilingual 
phrase pairs from the best translation hypotheses of 
previous sentences. One problem with the dynamic 
cache is that those initial sentences in a test docu-
ment may not benefit from the dynamic cache. 
Another problem is that the dynamic cache may be 
prone to noise and cause error propagation. This 
explains why the dynamic cache fails to much im-
prove the performance. 
This paper proposes a cache-based approach for 
document-level SMT using a static cache and a 
dynamic cache. While such a approach applies to 
both phrase-based and syntax-based SMT, this pa-
per focuses on phrase-based SMT. In particular, 
the static cache is employed to store relevant bilin-
gual phrase pairs extracted from similar bilingual 
document pairs (i.e. source documents similar to 
the test document and their target counterparts) in 
the training parallel corpus while the dynamic 
cache is employed to store bilingual phrase pairs 
from the best translation hypotheses of previous 
sentences in the test document. In this way, our 
cache-based approach can provide useful data at 
the beginning of the translation process via the 
static cache. As the translation process continues, 
the dynamic cache grows and contributes more and 
more to the translation of subsequent sentences. 
Our motivation to employ similar bilingual doc-
ument pairs in the training parallel corpus is simple: 
a human translator often collects similar bilingual 
document pairs to help translation. If there are 
translation pairs of sentences/phrases/words in 
similar bilingual document pairs, this makes the 
translation much easier. Given a test document, our 
approach imitates this procedure by first retrieving 
similar bilingual document pairs from the training 
parallel corpus, which has often been applied in 
IR-based adaptation of SMT systems (Zhao et 
al.2004; Hildebrand et al2005; Lu et al2007) and 
then extracting bilingual phrase pairs from similar 
bilingual document pairs to store them in a static 
cache. 
However, such a cache-based approach may in-
troduce many noisy/unnecessary bilingual phrase 
pairs in both the static and dynamic caches. In or-
der to resolve this problem, this paper employs a 
topic model to weaken those noisy/unnecessary 
bilingual phrase pairs by recommending the de-
coder to choose most likely phrase pairs according 
to the topic words extracted from the target-side 
text of similar bilingual document pairs. Just like a 
human translator, even with a big bilingual dictio-
nary, is often confused when he meets a source 
phrase which corresponds to several possible trans-
lations. In this case, some topic words can help 
reduce the perplexity. In this paper, the topic words 
are stored in a topic cache. In some sense, it has 
the similar effect of employing an adaptive lan-
guage model with the advantage of avoiding the 
interpolation of a global language model with a 
specific domain language model. 
The rest of this paper is organized as follows. 
Section 2 reviews the related work. Section 3 
presents our cache-based approach to document-
level SMT. Section 4 presents the experimental 
results. Session 5 gives new insights on cache-
based document-level translation. Finally, we 
conclude this paper in Section 6. 
2 Related work 
There are only a few studies on document-level 
SMT. Representative work includes Zhao et al 
(2006), Tam et al (2007), Carpuat (2009). 
Zhao et al (2006) assumed that the parallel sen-
tence pairs within a document pair constitute a 
mixture of hidden topics and each word pair fol-
lows a topic-specific bilingual translation model. It 
shows that the performance of word alignment can 
be improved with the help of document-level in-
formation, which indirectly improves the quality of 
SMT.  
Tam et al (2007) proposed a bilingual-LSA 
model on the basis of a parallel document corpus 
and built a topic-based language model for each 
language. By automatically building the corres-
pondence between the source and target language 
models, this method can match the topic-based 
language model and improve the performance of 
SMT. 
Carpuat (2009) revisited the ?one sense per dis-
course? hypothesis of Gale et al (1992) and gave a 
detailed comparison and analysis of the ?one trans-
lation per discourse? hypothesis. However, she 
failed to propose an effective way to integrate doc-
ument-level information into a SMT system. For 
example, she simply recommended some transla-
tion candidates to replace some target words in the 
post-process stage.  
In principle, the cache-based approach can be 
well suited for document-level translation. Basical-
910
 ly, the cache is analogous to ?cache memory? in 
hardware terminology, which tracks short-term 
fluctuation (Iyer et al, 1999). As the cache 
changes with different documents, the document-
level information should be capable of influencing 
SMT.  
Previous cache-based approaches mainly point 
to cache-based language modeling (Kuhn and Mori, 
1990), which uses a large global language model to 
mix with a small local model estimated from recent 
history data. However, applying such a language 
model in SMT is very difficult due to the risk of 
introducing extra noise (Raab, 2007).  
For cache-based translation modeling, Nepveu 
et al (2004) explored user-edited translations in 
the context of interactive machine translation. Tie-
demann (2010) proposed to fill the cache with bi-
lingual phrase pairs from the best translation 
hypotheses of previous sentences in the test docu-
ment. Both Nepveu et al (2004) and Tiedemann 
(2010) also explored traditional cache-based lan-
guage models and found that a cache-based lan-
guage model often contributes much more than a 
cache-based translation model. 
3 Cache-based document-level SMT 
Given a test document, our system works as fol-
lows:  
1) clears the static, topic and dynamic caches 
when switching to a new test document dx; 
2) retrieves a set of most similar bilingual docu-
ment pairs dds for dx from the training parallel 
corpus using the cosine similarity with tf-idf 
weighting; 
3) fills the static cache with bilingual phrase pairs 
extracted from dds;  
4) fills the topic cache with topic words extracted 
from the target-side documents of dds; 
5) for each sentence in the test document, trans-
lates it using cache-based SMT and conti-
nuously expands the dynamic cache with 
bilingual phrase pairs obtained from the best 
translation hypothesis of the  previous sen-
tences. 
In this way, our cache-based approach can pro-
vide useful data at the beginning of the translation 
process via the static cache. As the translation 
process continues, the dynamic cache grows and 
contributes more and more to the translation of 
subsequent sentences. Besides, the possibility of 
choosing noisy/unnecessary bilingual phrase pairs 
in both the static and dynamic caches is wakened 
with the help of the topic words in the topic cache. 
In particular, only the most similar document pair 
is used to construct the static cache and the topic 
cache unless specified. 
In this section, we first introduce the basic 
phrase-based SMT system and then present our 
cache-based approach to achieve document-level 
SMT with focus on constructing the caches (static, 
dynamic and topic) and designing their corres-
ponding features. 
3.1 Basic phrase-based SMT system 
It is well known that the translation process of 
SMT can be modeled as obtaining the best transla-
tion e of the source sentence f by maximizing fol-
lowing posterior probability (Brown et al, 1993): 
)()|(maxarg)|(maxarg ePefPfePe lm
ee
best ==  (1) 
where P(e|f) is a translation model and Plm is a lan-
guage model. 
Our system adopted Moses (a state-of-art 
phrase-based SMT system) as a baseline, which 
follows Koehn et al (2003) and mainly adopts six 
groups of popular features: 1) two phrase transla-
tion probabilities (two directions): Pphr(e|f) and 
Pphr(f|e); 2) two word translation probabilities (two 
directions) : Pw(e|f) and Pw(f|e); 3) one language 
model (target language): LM(e); 4) one phrase pe-
nalty (target language): PP(f); 5) one word penalty 
(target language):WP(e); 6) a lexicalized reorder-
ing model. Besides, the log-linear model as de-
scribed in (Och and Ney, 2003) is employed to 
linearly interpolate these features for obtaining the 
best translation according to the formula (2): 
)},(max{arg
1
fehe m
M
m
mbest ?
=
= l   (2) 
where hm(e , f) is a feature function, and ?m is the 
weight of hm(e , f)  optimized by a discriminative 
training method on a held-out development data. 
In principle, a phrase-based SMT system can 
provide the best phrase segmentation and align-
ment that cover a bilingual sentence pair. Here, a 
segmentation of a sentence into K phrases is de-
fined as: 
(f~e)?  ? (f , e , ~)      (3) 
 
where tuple (f , e ) refers to a phrase pair, and ~ 
indicates corresponding alignment information.  
911
 3.2 Dynamic Cache 
Our dynamic cache is mostly inspired by Tiede-
mann (2010), which adopts a dynamic cache to 
store relevant bilingual phrase pairs from the best 
translation hypotheses of previous sentences in the 
test document. In particular, a specific feature is 
incorporated S      to capture useful document-
level information in the dynamic cache: 
?
?
=
=
-?
=
?>>=<<= K
i ic
K
i
i
iicc
cccache ffI
efefeIfeS
1
1
)(
),,()|(
(4)
 
where ie-? is a decay factor to avoid the depen-
dence of the feature?s contribution on the cache 
size. Given <ec, fc> an existing phrase pair in the 
dynamic cache and <ei,fi> a phrase pair in a new 
hypothesis, if ( ei=ec ? fi=fc ) is true (i.e. full match-
ing), function I(.) returns 1 , otherwise 0.  
One problem with the dynamic cache in Tiede-
mann (2010) is that it continuously updates the 
weight of a phrase pair in the dynamic cache. This 
may cause noticeable computational burden with 
the increasing number of phrase pairs in the dy-
namic cache. In addition, as a source phrase (fc) 
may occur many times in the dynamic cache, the 
weights for related phrase pairs may degrade se-
verely and thus his decoder needs a decay factor, 
which is difficult to optimize. Finally, Tiedemann 
(2010) only allowed full matching. This largely 
lowers down the probability of hitting the dynamic 
cache and thus much affects its effectiveness. 
To overcome above problems, we only employ 
the bilingual phrase pairs in the dynamic cache to 
inform the decoder whether one bilingual phrase 
pair exists in the dynamic cache or not, which is 
slightly similar to (Nepveu et al 2004) ,thus avoid-
ing extra computational burden and the fine-tuning 
of the decay factor.  In particular, following new 
feature is incorporated to better explore the dynam-
ic cache:   = ? dpairmatch(e , f )        (5) 
where dpairmatch(  ,  )    = ???
?? 1   (e = e ? f = f )                       ?  e  = e ? f  = f ?? e ?> 3    ?  e = e  ? f = f  ?? e ?> 3 0    other                                               
Here, F  is called the dynamic cache feature. 
Assume (ec,fc ) is a phrase pair in the dynamic 
cache and (ei,fi) is a phrase pair candidate for a 
new hypothesis. Besides full matching, we intro-
duce a symbol of ?^? for sub-phrase, such as e   for 
a sub-phrase of ei and  e   for a sub-phrase of e  , to 
allow partial matching. Finally, F  measures the 
overall value of a target candidate f  by summing 
over the scores of K phrase pairs. 
Obviously, F  rewards both full matching and 
partial matching. In order to avoid too much noise, 
we put some constraints on the number of words in 
the target phrase of <ec,fc> or <ei,fi>, such as ? e ?> 3 , where " ?? "  measures the number of 
non-blank characters in a phrase. For example, if 
phrase pair ?, ??||| and reduced? occurs in the 
cache, phrase pair ?,|||and? is not rewarded because 
such shorter phrase pairs occur frequently and may 
largely degrade the effect of the cache. In accor-
dance, the dynamic cache only contains phrase 
pairs whose target phrases contain 4 or more non-
blank characters. 
3.3 Static Cache 
In Tiedemann (2010), initial sentences in a test 
document fail to benefit from the dynamic cache 
due to the lack of contents in the dynamic cache at 
the beginning of the translation process. To over-
come this problem, a static cache is included to 
store relevant bilingual phrase pairs extracted from 
similar bilingual document pairs in the training 
parallel corpus. In particular, a static cache feature F  is designed to capture useful information in the 
static cache in the same way as the dynamic cache 
feature, shown in Formula (5). 
For this purpose, all the document pairs in the 
training parallel corpus are aligned at the phrase 
level using 2-fold cross-validation. That is, we 
adopt 50% of the training parallel corpus to train a 
model using Moses and apply the model to enforce 
phrase alignment of the remaining training data, 
and vice versa. Here, the enforcement is done by 
guaranteeing the occurrence of the target phrase 
candidate of a source phrase in the sentence pair. 
Besides, all the words pairs trained on the whole 
training parallel corpus are included in both folds 
to ensure at least one possible translation. Finally, 
the phrase pairs in the best translation hypothesis 
of a sentence pair is retrieved from the decoder. In 
this way, we can extract a set of phrase pairs for 
each bilingual document pairs. 
Given a test document, we first find a set of sim-
ilar source documents by computing the Cosine 
similarity using the TF-IDF weighting scheme and 
their corresponding target documents, from the 
training parallel corpus. Then, the phrase pairs ex-
912
 tracted from these similar bilingual document pairs 
are collected into the static cache.  To avoid noise, 
we filter out those phrase pairs which occur less 
than two times in the training parallel corpus. 
 
?? ||| exports 
?? ||| slowdown 
?? ||| stock market 
?? ||| leading 
?? ||| exchange 
?? ||| vitality 
?? ||| speed up the 
???? ||| economists 
?? ?? ||| export growth 
?? ?? ||| various reasons 
?? ?? ||| a well-known international 
?? ??? ||| congressional committee 
? ?? ? ?? ||| pessimistic predictions 
?? ?? ? ?? ||| maintain a certain growth 
?? ?? ?? ||| a drop in the dollar exchange rate 
Table 1: Phrase pairs extracted from a document pair 
with an economic topic 
Similar to the dynamic cache, we only consider 
those phrase pairs whose target phrases contain 4 
or more non-blank characters to avoid noise. We 
do not deliberately remove long phrase pairs. It is 
possible to use these long phrase pairs if our test 
document is very similar to one training document 
pair.  Table 1 shows some bilingual phrase pairs 
extracted from a document pair, which reports a 
piece of news about ?impact on slowdown in US 
economic growth?. Obviously, these phrase pairs 
are closely related to economics. 
3.4 Topic Cache 
Both the dynamic and static caches may still intro-
duce noisy/unnecessary bilingual phrase pairs even 
with constraints on the length of phrases and their 
occurrence frequency in the training parallel cor-
pus. In order to resolve this problem, this paper 
adopts a topic cache to store relevant topic words 
and employs a topic cache feature to weaken those 
noisy/unnecessary phrase pairs. 
Given w  is a topic word in the topic cache, the 
topic cache feature F  is designed as follows:   =  topicexist(e , f )          (6) 
where topicexist(e , f ) =   1   (w ? e )                                    0    other                                            
Here, the target phrase which contains a topic word w  will be rewarded. w  is derived by a topic mod-
el, LDA (Latent Dirichlet Allocation). This is dif-
ferent from the previous work (Tam, 2007), which 
mainly interpolated a topic language model with a 
general language model and added additional two 
adaptive lexicon probabilities in his phrase table. 
In principle, LDA is a probabilistic model of 
text data, which provides a generative analog of 
PLSA (Blei et al, 2003), and is primarily meant to 
reveal hidden topics in text documents. Like most 
of the text mining techniques, LDA assumes that 
documents are made up of words and the ordering 
of the words within a document is unimportant (i.e. 
the ?bag-of-words? assumption).  
Figure 1 shows the principle of LDA, where ? is 
the parameter of the uniform Dirichlet prior on the 
per-document topic distributions, ? is the parame-
ter of the uniform Dirichlet prior on the per-topic 
word distribution, ?i is the topic distribution for 
document i, zij is the topic for the jth word in doc-
ument i, and wij is the specific word. Among all 
variables, wij is the only observable variable with 
all the other variables latent. In particular, K de-
notes the number of topics considered in the model 
and ? is a K*V (V is the dimension of the vocabu-
lary) Markov matrix each line of which denotes the 
word distribution of a topic. The inner plate over z 
and w illustrates the repeated sampling of topics 
and words until N words have been generated for document d. The plate surrounding ? illustrates the 
sampling of a distribution over topics for each 
document d for a total of M documents. The plate 
surrounding ? illustrates the repeated sampling of 
word distributions for each topic z until K topics 
have been generated. 
We use a LDA tool1 to build a topic model using 
the target-side documents in the training parallel 
corpus. Using LDA, we can obtain the topic distri-
bution of each word w, namely p(z|w) for topic z ?K. Moreover, using the obtained word topic dis-
tributions, we can infer the topic distribution of a 
new document, namely p(z|d) for each topic z ?K. 
Given a test document, we first find the most 
similar source document from the training data in 
                                                        
1 http://www.arbylon.net/projects/ 
Figure 1: LDA  
913
 the same way as done in the static cache. After that, 
we retrieve its corresponding target document. 
Then, the topic of the target document is deter-
mined by its major topic, with the maximum p(z|d). 
Finally, we load some topic words corresponding 
to this topic z into the topic cache. In particular, 
our LDA model deploy the setting of K=15, ?=0.5 
and ?=0.1. Besides, only top 1000 topic words are 
reserved for each topic. Table 2 shows top 10 topic 
words for five topics. 
 
Topic 1 Topic 2 Topic 3 Topic4 Topic5 
company  
corporation  
limited  
manager  
board  
branch  
companies  
ltd  
business  
personnel  
army  
armed  
military  
officers  
forces  
units  
troops  
force  
soldiers  
police  
party  
represents  
study  
theory  
leadership  
political  
cadres  
speech  
comrade  
central  
bush  
united  
adminis-
tration  
policy  
president  
clinton  
office  
secretary  
powell  
relations  
election  
olympic  
games  
votes  
bid  
gore  
presi-
dential  
party  
won  
speech 
Table 2: Topic words extracted from target-side doc-
uments  
 
4 Experimentation 
We have systematically evaluated our cache-based 
approach to document-level SMT on the Chinese-
English translation task. 
4.1 Experimental Setting 
Here, we use SRI language modeling toolkit to 
train a trigram general language model on English 
newswire text, mostly from the Xinhua portion of 
the Gigaword corpus (2007) and performed word 
alignment on the training parallel corpus using 
GIZA++(Och and Ney,2000) in two directions. For 
evaluation, the NIST BLEU script (version 13) 
with the default setting is used to calculate the 
Bleu score (Papineni et al 2002), which measures 
case-insensitive matching of n-grams with n up to 
4. To see whether an improvement is statistically 
significant, we also conduct significance tests us-
ing the paired bootstrap approach (Koehn, 2004)2. 
In this paper, ?***?, ?**?, and ?*? denote 
p-values less than or equal to 0.01, in-between 
(0.01, 0.05), and bigger than 0.05, which mean 
significantly better, moderately better and slightly 
better, respectively. 
                                                        
2 http://www.ark.cs.cmu.edu/MT 
In this paper, we use FBIS as the training data, 
the 2003 NIST MT evaluation test data as the de-
velopment data, and the 2005 NIST MT test data 
as the test data. Table 3 shows the statistics of 
these data sets (with document boundaries anno-
tated). 
 
   Corpus Sentences Documents 
Role Name 
Train FBIS 239413 10353 
Dev NIST2003 919 100 
Test NIST2005 1082 100 
Table 3: Corpus statistics 
In particular, the sizes of the static, topic and 
dynamic caches are fine-tuned to 2000, 1000 and 
5000 items, respectively. For the dynamic cache, 
we only keep those most recently-visited items, 
while for the static cache; we always keep the most 
frequently-occurring items. 
4.2 Experimental Results 
Table 4 shows the contribution of various caches in 
our cache-based document-level SMT system. The 
column of ?BLEU_W? means the BLEU score 
computed over the whole test set and ?BLEU_D? 
corresponds to the average BLEU score over sepa-
rated documents. 
 
System BLEU on 
Dev(%) 
BLEU on Test(%) 
BLEU_W NIST BLEU_D 
Moses 29.87 25.76 7.784 25.08 
Fd 29.90 26.03 (*) 7.852 25.39 
Fd+Fs 30.29 26.30 (**) 7.884 25.86 
Fd+Ft 30.11 26.24 (**) 7.871 25.74 
Fd+Fs+Ft 30.50 26.42 (***) 7.896 26.11 
Fd+Fs+Ft 
with merg-
ing 
- 26.57 (***) 7.901 26.32 
Table 4: Contribution of various caches in our cache-
based document-level SMT system. Note that signific-
ance tests are done against Moses. 
Contribution of dynamical cache (Fd)  
Table 4 shows that the dynamic cache slightly im-
proves the performance by 0.27 (*) in BLEU_W. 
This is similar to Tiedemann (2010). However, 
detailed analysis indicates that the dynamic cache 
does have negative effect on about one third of 
documents, largely due to the instability of the dy-
namic cache at the beginning of translating a doc-
ument. Figure 2 shows the distribution of the 
914
 BLEU_D difference of 100 test documents (sorted 
by BLEU_D). It shows that about 55% of test 
documents benefit from the dynamic cache. 
Contribution of static cache (Fs) 
Table 4 shows that the combination of the static 
cache with the dynamic cache further improves the 
performance by 0.27(*) in BLEU_W. This sug-
gests the effectiveness of the static cache in elimi-
nating the instability of the dynamic cache when 
translating first few sentences of a test document. 
Together, the dynamic and static caches much im-
prove the performance by 0.54 (**) in BLEU_W 
over Moses.  Figure 3 shows the distribution of the 
BLEU_D difference of 100 test documents (sorted 
by BLEU_D), with more positive effect on those 
borderline documents, compared to Figure 2. 
Contribution of topic cache (Ft) 
Table 4 shows that the topic cache has comparable 
effect on improving the performance as the static 
cache when combined with the dynamic cache 
(0.48 vs. 0.54 in BLEU_W). Figure 4 shows the 
effectiveness of combining the dynamic and topic 
caches (sorted by BLEU_D). 
However, detailed analysis shows that the topic 
cache and the static cache are quite complementary 
by contributing on different test documents, largely 
due to that while the static cache tends to keep 
translation consistent, the topic cache plays like a 
document-specific language model. This is justi-
fied by Table 4 that the combination of the dynam-
ic, static and topic caches significantly improve the 
performance by 0.66 (***) in BLEU_W, and by 
Figure 5 that about 75% of test documents benefit 
from the combination of the three caches (sorted 
by BLEU_D). 
Contribution of merging phrase pairs of similar 
document pairs 
Here, the number of similar documents we adopt is 
different from previous experiments. In the pre-
vious experiments, we only cache bilingual phrase 
pairs extracted from the most similar document. 
Here, we merge phrase pairs for several most simi-
lar documents (5 at most) which have the same 
topic. 
Figure 2: Contribution of employing the dynamic 
cache on different test documents 
-8.00%
-6.00%
-4.00%
-2.00%
0.00%
2.00%
4.00%
6.00%
8.00%
1 8 15 22 29 36 43 50 57 64 71 78 85 92 99
fd-moses
 
Figure 3: Contribution of combining the dynamic and 
static cache on different test documents 
-6.00%
-4.00%
-2.00%
0.00%
2.00%
4.00%
6.00%
1 8 15 22 29 36 43 50 57 64 71 78 85 92 99
fd+fs-moses
 Figure 4: Contribution of combining the dynamic 
and topic caches 
-6.00%
-4.00%
-2.00%
0.00%
2.00%
4.00%
6.00%
8.00%
10.00%
1 7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 97
fd+ft-moses
 
Figure 5: Contribution of combining the three caches 
-4.00%
-2.00%
0.00%
2.00%
4.00%
6.00%
8.00%
10.00%
1 9 17 25 33 41 49 57 65 73 81 89 97
fd+fs+ft-moses
915
  Table 4 shows that employing this trick can fur-
ther improve the performance by 0.15 in BLEU_W. 
As a result, the cache-based approach significantly 
improve the performance by 0.81 (***) in 
BLEU_W over Moses. 
5 Discussion 
In this section, we explore in more depth why the 
static cache can help the dynamic cache, some 
constrained factors which impact the effectiveness 
of our cache-based approach. 
Effectiveness of the static cache  
We investigate why the static cache affects the per-
formance. Basically, it is difficult for the dynamic 
cache to capture such similar information in the 
static cache. 
In principle, the static cache can both influence 
the initial and subsequent sentences; however sub-
sequent ones can be affected by multiple caches. In 
order to give an insight of the static cache, we eva-
luate its effectiveness on the first sentence for each 
test document. Figure 6 shows the contribution of 
the static cache on translating these first sentences 
(y-axis shows BLEU value of the first sentence for 
each test document). It notes that the most BLEU 
scores of them are zeros because of the length limi-
tation of first sentences. 
Furthermore, we count the hit (matching) fre-
quency of the static cache for each test documents. 
Since we use 1 or 0 for the static cache feature, it is 
easy to retrieve its effect for each test document. 
Our statistics shows that the hit frequency on static 
cache fluctuates between 5 and 18 for each test 
document. Without the static cache, the hit fre-
quency of the dynamic cache is 504 on whole test 
sets, this figure increases to 685 with the static 
cache. This means that the static cache significant-
ly enlarges the effectiveness of the dynamic cache 
by including more relevant phrase pairs to the dy-
namic cache, largely due to the positive impact of 
the static cache on the initial sentences of each test 
document. 
Size of topic cache  
Table 5 shows the impact of the topic cache when 
the number of the retained topic words for each 
topic increases from 500 to 2000. It shows that too 
more topic words actually harm the performance, 
due to the increase of noise. 1000 topic words 
seem a lot largely due to that we didn?t do stem-
ming for our topic modeling since we hope to in-
troduce some tense information of them in the 
future.  
 
Number of topic words BLEU_W 
500 26.27 
700 26.31 
1000 26.42 
1500 26.23 
2000 26.19 
Table 5: Impact of the topic cache size 
Influenced translations  
In order to explore how our cache-based system 
impacts on translation results, we manually in-
spected 5 documents respectively which is im-
proved or degraded in translation quality compared 
to the baseline Moses output. Those documents 
have 107 sentences in sum. 
The good effectiveness of each kind of cache 
can be observed by the example 1 and 2 showed in 
Table 6. Both the example 1 and 2 come from the 
same document whose ?BLEU_D? score exceeds 
Moses with 8.4 point. The example 1 benefits from 
the topic cache which contains the item of ?action?. 
The example 2 benefits from the static cache which 
contains a phrase pair of  ??? ||| promised to? 
while Moses use ?commitment? for ???? , which 
may be the reason for  missing the part of ?prime 
minister? in Moses output. Furthermore, due to the 
phrase pair of ??? ??||| the ceasefire agree-
ment? existing in our static cache, our decoder 
keeps using ?ceasefire? to translate ???? in the 
whole document while Moses randomly use ?cea-
sefire? or ?cease-fire? for this translation.  
 Figure 6: Contribution of the static cache on the first 
sentence of each test document  
(i.e. with empty dynamic cache)   
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 20 40 60 80 100 120
Fd FdFs
916
 1 ?? ?? ? ?? ?? ? ? ?? ? ?? , ?? ? ?? ?? ?? ? 
Moses: official forecasts said that preparatory work will be carried out in july and then launched a political maneuver . 
Ours:   official forecasts said that preparatory work will be carried out in july , then began a political action . 
Reference: officials expected that "preparations would take place until July, after which political action will begin".          
2 ?? ? ? ? , ????? ? , ??? ?? ?? ?? ? ? ? ???? ?? ?? ?? ?? , ?
? ? ?? ? ?? , ??? ? ? ?? ? ???? ? ? ?? ?? ? ? 
Moses: on this point , said that israeli commitment to the palestinian authorities to respect the cease-fire agreement , 
where they are well under control , israel will stop its military actions against palestinians . 
Ours: on this point , said that israeli prime minister promised to respect the ceasefire agreement , the palestinian au-
thorities to properly control their areas and israel will stop its military actions against palestinians . 
Reference:For this point , MENA said Israeli Prime Minister Sharon has promised to " stop Israeli military operations 
against the Palestinians insofar as they continue to respect the ceasefire deal and control their territory . " 
3 17 ? ? ?? 3000 ? ? ?? ? ??? ? ?? ?? ?? ? ? ??? ?? ? ???? ? ?? 
?? ????? ? ?? ?? ? ?? ?? ? ?? ?? ??? ?? ??? ? ?? ? ?? ?? 
? ?? ?? ? 
Moses: on the evening , nearly 3,000 residents in the downtown square of the weapons held by the municipal govern-
ment , watched a song and dance soiree , having colorful lighting disguise of ancient buildings around the square , sing-
ing and dancing famous artists staged different regions of ethnic song and dance . 
Ours: later on, nearly 3,000 residents in the downtown square to watch the government of having a song and dance 
performances were held under the disguise of colorful lighting around the square , a famous ancient buildings and local 
artists of different ethnic song and dance . 
Reference: On the night of the 17th , nearly 3,000 residents watched a wonderful gala of songs and dances , organized 
by the municipal government , at Plaza da Armas . Colorful lights lighted up ancient architecture around the plaza . 
Famous artists including singers and dancers staged performances of national songs and dances of different regions . 
4 ?? ? ?? ?? ? ? ?? ? ? ? 2.14 ???? ?? ? 2600 ? ???? ? ? ?? ? ?? 
? 800? ?? ? ? ? ?? ??? ? 31% ? 
Moses: at lima 's urban area from the beginning of 2600 square to 2.14 million square kilometers , while the popula-
tion has increased to 8 percent of the country 's total , about 31% . 
Ours: lima , the urban area from the beginning of 2600 square kilometers to 2.14 million square kilometers , but also 
increased to about 8 million population , the country 's total population of about 31% . 
Reference: The area of Lima city has expanded to more than 2,600 square kilometers from the original 2.14 square ki-
lometers when the city was founded , while the population has increased to around 8 million , roughly accounting for 
31% of the nation's total . 
Table 6: Positive and negative examples  
The example 3 and 4 also come from the same 
document however whose performance degrades 
with 2.17 point.  We don?t think the translation 
quality for example 4 in our system is worse than 
Moses. However, the translation quality for exam-
ple 3 in our system is very bad and especially 
showed on ?re-ordering?. We found this sentence 
did not match any item in our static cache and top-
ic cache. Although this phenomenon also happens 
in other documents, but this is the most typical 
negative example among these documents.  
Document-specific characteristics  
It seems that using the same weight for the whole 
test sets (all documents) is not very reasonable. 
Actually, if we can determine those negative doc-
uments which are not suitable for the cache-based 
approach, our cache-based approach may gain 
much improvement. Tiedemann (2010) explored 
the correlation to document length, baseline per-
formance and source document repetition. Howev-
er, it seems that there are no obvious rules to filter 
out those negative documents. Besides, there may 
be two more document-specific factors: repetition 
of the reference text and document style.  
Tiedemann (2010) only considered the repetition 
of the test text in the source side. Since BLEU 
score is computed against the reference text, the 
repetition in the reference text may greatly influ-
ence the performance of our cache-based approach 
to document-level SMT. As for document style, it 
is quite possible that a document may contain sev-
eral topics. Therefore, it may be useful to track 
such change over topics and refresh various caches 
when there is a topic change. We will leave the 
above issues to the future work. 
6 Conclusion 
We have shown that our cache-based approach 
significantly improves the performance with the 
help of various caches, such as the dynamic, static 
and topic caches, although the cache-based ap-
917
 proach may introduce some negative impact on 
BLEU scores for certain documents.  
In the future, we will further explore how to re-
flect document divergence during training and dy-
namically adjust cache weights according to 
different documents.  
There are many useful components in training 
documents, such as named entity, event and co-
reference. In this experiment, we only adopt the 
flat data in our cache. However, the structured data 
may improve the correctness of matching and thus 
effectively avoid noise. We will explore more ef-
fective ways to pick up various kinds of useful in-
formation from the training parallel corpus to 
expand our cache-based approach. Besides, we will 
resort to comparable corpora to enlarge our cache-
based approach to document-level SMT.  
Acknowledgments 
This research was supported by Projects 90920004, 
60873150, and 61003155 under the National Natu-
ral Science Foundation of China, Project 
20093201110006 under the Specialized Research 
Fund for the Doctoral Program of Higher Educa-
tion of China. 
References  
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent Dirichlet Allocation. Journal of machine 
Learning Research 3, pages 993?1022. 
Francis Bond. 2002. Toward a Science of Machine 
Translation. Asian Association of Machine Transla-
tion (AAMT) Journal, N0.22, Tokyo, Japan, pages 
12-20. 
PF Brown, SA Della Pietra, VJ Della Pietra, RL Merc-
er.1992.The Mathematics of Statistical Machine 
Translation: Parameter Estimation. Computational 
Linguistics. 19(2):263-309. 
Marine Carpuat. 2009. One Translation per Discourse. 
In Proc. of the NAACL HLT workshop on Semantic 
Evaluation, pages 19-26. 
John DeNero, Alexandre Buchard-C?ot?e, and Dan 
Klein. 2008. Sampling Alignment Structure under a 
Bayesian Translation Model. In Proc. of EMNLP 
2008, pages 314?323, Honolulu, October. 
William A. Gale, Kenneth W. Church, and David Ya-
rowsky. 1992. One Sense per Discourse. In Proceed-
ings of the workshop on Speech and Natural 
Language, Harriman, NY. 
Almut Silja Hildebrand, Matthias Eck, Stephan Vo-
gel,and Alex Waibel. 2005. Adaptation of the Trans-
lation Model for Statistical Machine Translation 
based on Information Retrieval. Proceedings of 
EAMT 2005:133-142. 
Rukmini M. Iyer and Mari Ostendorf. 1999. Modeling 
Long Distance Dependence in Language:Topic 
Mixtures Versus Dynamic Cache Models. IEEE 
Transactions on speech and audio processing, 7(1). 
Philipp Koehn, Franz Josef Och ,and DanielMarcu.2003. 
Statistical Phrase-Based Translation. Proceedings of 
the 2003 Conference of the North American Chapter 
of the Association for Computational Linguistics on 
Human Language Technology, pages 48-54. 
Philipp Koehn.2004.Statistical Significance Tests for 
Machine Translation Evaluation. In Proc. of EMNLP 
2004,  pages 388?395. 
Roland Kuhn and Renato De Mori. 1990. A Cache-
based Natural Language Model for Speech Recogni-
tion. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence,12(6):570-583. 
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving 
statistical machine translation performance by train-
ing data selection and optimization. In Proc. of  
EMNLP 2007,pages 343?350, Prague, Czech Repub-
lic, June. 
Daniel Marcu and William Wong. 2002. A phrase-based 
Joint Probability Model for Statistical Machine 
Translation. In Proc. of  EMNLP 2002, July. 
Laurent Nepveu, Guy Lapalme, Philippe Langlais and 
George Foster.2004. Adaptive Language and Trans-
lation Models for Interactive Machine Translation. In 
Proc. of EMNLP 2004, pages 190-197. 
Eugene A. Nida. 1964. Toward a Science of Translating. 
Leiden, Netherlands:E.J.Brill. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proc. of ACL, 
pages 160?167. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proc. of ACL, pages 
440?447. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: A Method for Automatic 
Evaluation of Machine Translation. In Proc. of 
ACL02, pages 311?318. 
Martin Raab. 2007. Language Modeling for Machine 
Translation. VDM Verlag, Saarbrucken, Germany. 
918
 G. Salton and C. Buckley. 1988. Term-weighting Ap-
proaches in Automatic Text Retrieval. Information 
Processing and management,24(5):513-523,1988. 
Yik-Cheung Tam, Ian Lane and Tanja Schultz. 2007. 
Bilingual ISA-based Adaptation for Statistical Ma-
chine Translation. Machine Translation, 28:187-207. 
Jorg Tiedemann. 2010. Context Adaptation in Statistical 
Machine Translation Using Models with Exponen-
tially Decaying Cache. In Proc. of the 2010 work-
shop on domain adaptation for Natural Language 
Processing, ACL 2010, pages 8-15. 
Joern Wuebker and Arne Mauser and Hermann Ney. 
2010. Training Phrase Translation Models with Leav-
ing-One-Out. In Proc. of ACL, pages 475-484. 
Bing Zhao, Matthias Eck, and Stephan Vogel. 
2004.Language Model Adaptation for Statistical Ma-
chine Translation with Structured Query Models. In 
COLING 2004, Geneva, August. 
Bing Zhao and Eric P. Xing .2006. BiTAM:Bilingual 
Topic Ad-Mixture Models for Word Alignment. In 
Proc. of ACL2006. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
919
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 139?148, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Active Learning for Imbalanced Sentiment Classification 
 
Shoushan Li?, Shengfeng Ju?, Guodong Zhou?? Xiaojun Li? 
?Natural Language Processing Lab 
School of Computer Science and Technology 
?College of Computer and 
Information Engineering 
Soochow University, Suzhou, 215006, China Zhejiang Gongshang University
{shoushan.li, shengfeng.ju}@gmail.com, Hangzhou, 310035, China 
gdzhou@suda.edu.cn lixj@mail.zjgsu.edu.cn
 
 
 
 
 
Abstract 
Active learning is a promising way for 
sentiment classification to reduce the 
annotation cost. In this paper, we focus on 
the imbalanced class distribution scenario 
for sentiment classification, wherein the 
number of positive samples is quite 
different from that of negative samples. 
This scenario posits new challenges to 
active learning. To address these 
challenges, we propose a novel active 
learning approach, named co-selecting, by 
taking both the imbalanced class 
distribution issue and uncertainty into 
account. Specifically, our co-selecting 
approach employs two feature subspace 
classifiers to collectively select most 
informative minority-class samples for 
manual annotation by leveraging a 
certainty measurement and an uncertainty 
measurement, and in the meanwhile, 
automatically label most informative 
majority-class samples, to reduce human-
annotation efforts. Extensive experiments 
across four domains demonstrate great 
potential and effectiveness of our proposed 
co-selecting approach to active learning for 
imbalanced sentiment classification. 1 
1 Introduction 
Sentiment classification is the task of identifying 
the sentiment polarity (e.g., positive or negative) of 
                                                          
*1 Corresponding author 
a natural language text towards a given topic (Pang 
et al2002; Turney, 2002) and has become the 
core component of many important applications in 
opinion analysis (Cui et al2006; Li et al2009; 
Lloret et al2009; Zhang and Ye, 2008). 
Most of previous studies in sentiment 
classification focus on learning models from a 
large number of labeled data. However, in many 
real-world applications, manual annotation is 
expensive and time-consuming. In these situations, 
active learning approaches could be helpful by 
actively selecting most informative samples for 
manual annotation. Compared to traditional active 
learning for sentiment classification, active 
learning for imbalanced sentiment classification 
faces some unique challenges.  
As a specific type of sentiment classification, 
imbalanced sentiment classification deals with the 
situation in which there are many more samples of 
one class (called majority class) than the other 
class (called minority class), and has attracted 
much attention due to its high realistic value in 
real-world applications (Li et al2011a). In 
imbalanced sentiment classification, since the 
minority-class samples (denoted as MI samples) 
are normally much sparse and thus more precious 
and informative for learning compared to the 
majority-class ones (denoted as MA samples), it is 
worthwhile to spend more on manually annotating 
MI samples to  guarantee both the quality and 
quantity of MI samples. Traditionally, uncertainty 
has been popularly used as a basic measurement in 
active learning (Lewis and Gale, 2004). Therefore, 
how to select most informative MI samples for 
manual annotation without violating the basic 
139
uncertainty requirement in active learning is 
challenging in imbalanced sentiment classification. 
In this paper, we address above challenges in 
active learning for imbalanced sentiment 
classification. The basic idea of our active learning 
approach is to use two complementary classifiers 
for collectively selecting most informative MI 
samples: one to adopt a certainty measurement for 
selecting most possible MI samples and the other 
to adopt an uncertainty measurement for selecting 
most uncertain MI samples from the most possible 
MI samples returned from the first classifier. 
Specifically, the two classifiers are trained with 
two disjoint feature subspaces to guarantee their 
complementariness. This also applies to selecting 
most informative MA samples. We call our novel 
active learning approach co-selecting due to its 
collectively selecting informative samples through 
two disjoint feature subspace classifiers. To further 
reduce the annotation efforts, we only manually 
annotate those most informative MI samples while 
those most informative MA samples are 
automatically labeled using the predicted labels 
provided by the first classifier.  
In principle, our active learning approach differs 
from existing ones in two main aspects. First, a 
certainty measurement and an uncertainty 
measurement are employed in two complementary 
subspace classifiers respectively to collectively 
select most informative MI samples for manual 
annotation. Second, most informative MA samples 
are automatically labeled to further reduce the 
annotation cost. Evaluation across four domains 
shows that our active learning approach is effective 
for imbalanced sentiment classification and 
significantly outperforms the state-of-the-art active 
learning alternatives, such as uncertainty sampling 
(Lewis and Gale, 2004) and co-testing (Muslea et 
al., 2006). 
The remainder of this paper is organized as 
follows. Section 2 overviews the related work on 
sentiment classification and active learning. 
Section 3 proposes our active learning approach 
for imbalanced sentiment classification. Section 4 
reports the experimental results. Finally, Section 5 
draws the conclusion and outlines the future work. 
2 Related Work 
In this section, we give a brief overview on 
sentiment classification and active learning. 
2.1 Sentiment Classification 
Sentiment classification has become a hot research 
topic in NLP community and various kinds of 
classification methods have been proposed, such as 
unsupervised learning methods (Turney, 2002), 
supervised learning methods (Pang et al2002), 
semi-supervised learning methods (Wan, 2009; Li 
et al2010), and cross-domain classification 
methods (Blitzer et al2007; Li and Zong, 2008; 
He et al2011). However, imbalanced sentiment 
classification is relatively new and there are only a 
few studies in the literature. 
Li et al2011a) pioneer the research in 
imbalanced sentiment classification and propose a 
co-training algorithm to perform semi-supervised 
learning for imbalanced sentiment classification 
with the help of a great amount of unlabeled 
samples. However, their semi-supervised approach 
to imbalanced sentiment classification suffers from 
the problem that their balanced selection strategy 
in co-training would generate many errors in late 
iterations due to the imbalanced nature of the 
unbalanced data. In comparison, our proposed 
active learning approach can effectively avoid this 
problem. By the way, it is worth to note that the 
experiments therein show the superiority of under-
sampling over other alternatives such as cost-
sensitive and one-class classification for 
imbalanced sentiment classification. 
Li et al2011b) focus on supervised learning 
for imbalanced sentiment classification and 
propose a clustering-based approach to improve 
traditional under-sampling approaches. However, 
the improvement of the proposed clustering-based 
approach over under-sampling is very limited. 
Unlike all the studies mentioned above, our 
study pioneers active learning on imbalanced 
sentiment classification. 
2.2 Active Learning 
Active leaning, as a standard machine learning 
problem, has been extensively studied in many 
research communities and several approaches have 
been proposed to address this problem (Settles, 
2009). Based on different sample selection 
strategies, they can be grouped into two main 
categories: (1) uncertainty sampling (Lewis and 
Gale, 2004) where the active learner iteratively 
select most uncertain unlabeled samples for 
manual annotation; and (2) committee-based 
140
sampling where the active learner selects those 
unlabeled samples which have the largest 
disagreement among several committee classifiers. 
Besides query by committee (QBC) as the first of 
such type (Freund et al1997), co-testing learns a 
committee of member classifiers from different 
views and selects those contention points (i.e., 
unlabeled examples on which the views predict 
different labels) for manual annotation (Muslea et 
al., 2006). 
However, most previous studies focus on the 
scenario of balanced class distribution and only a 
few recent studies address the active learning issue 
on imbalanced classification problems including 
Yang and Ma (2010), Zhu and Hovy (2007), 
Ertekin et al2007a) and Ertekin et al2007b)2. 
Unfortunately, they straightly adopt the uncertainty 
sampling as the active selection strategy to address 
active learning in imbalanced classification, which 
completely ignores the class imbalance problem in 
the selected samples.  
Attenberg and Provost (2010) highlights the 
importance of selecting samples by considering the 
proportion of the classes. Their simulation 
experiment on text categorization confirms that 
selecting class-balanced samples is more important 
than traditional active selection strategies like 
uncertainty. However, the proposed experiment is 
simulated and non real strategy is proposed to 
balance the class distribution of the selected 
samples. 
Doyle et al2011) propose a real strategy to 
select balanced samples. They first select a set of 
uncertainty samples and then randomly select 
balanced samples from the uncertainty-sample set. 
However, the classifier used for selecting balanced 
samples is the same as the one for supervising 
uncertainty, which makes the balance control 
unreliable (the selected uncertainty samples take 
very low confidences which are unreliable to 
correctly predict the class label for controlling the 
balance). Different from their study, our approach 
possesses two merits: First, two feature subspace 
classifiers are trained to finely integrate the 
certainty and uncertainty measurements. Second, 
the MA samples are automatically annotated, 
                                                          
2  Ertekin et al2007a) and Ertekin et al2007b) select 
samples closest to the hyperplane provided by the SVM 
classifier (within the margin). Their strategy can be seen as a 
special case of uncertainty sampling. 
which reduces the annotation cost in a further 
effort.  
3 Active Learning for Imbalanced 
Sentiment Classification 
Generally, active learning can be either stream-
based or pool-based (Sassano, 2002). The main 
difference between the two is that the former scans 
through the data sequentially and selects 
informative samples individually, whereas the 
latter evaluates and ranks the entire collection 
before selecting most informative samples at batch. 
As a large collection of samples can easily 
gathered once in sentiment classification, pool-
based active learning is adopted in this study. 
Figure 1 illustrates a standard pool-based active 
learning approach, where the most important issue 
is the sampling strategy, which evaluates the 
informativeness of one sample. 
 
Input: 
       Labeled data L; 
       Unlabeled pool U; 
Output: 
    New Labeled data L 
Procedure: 
Loop for N iterations: 
(1). Learn a classifier using current L  
(2). Use current classifier to label all unlabeled 
samples 
(3). Use the sampling strategy to select n most 
informative samples for manual annotation 
(4). Move newly-labeled samples from U to L 
 
 
Figure 1: Pool-based active learning 
3.1 Sampling Strategy: Uncertainty vs. 
Certainty 
As one of the most popular selection strategies in 
active learning, uncertainty sampling depends on 
an uncertainty measurement to select informative 
samples. Since sentiment classification is a binary 
classification problem, the uncertainty 
measurement of a document d can be simply 
defined as follows: 
{ , }
( ) min ( | )
y pos neg
Uncer d P y d
?
?  
Where ( | )P y d denotes the posterior probability of 
the document d belonging to the class y and {pos, 
141
neg} denotes the class labels of positive and 
negative. 
In imbalanced sentiment classification, MI 
samples are much sparse yet precious for learning 
and thus are believed to be more valuable for 
manual annotation. The key in active learning for 
imbalanced sentiment classification is to guarantee 
both the quality and quantity of newly-added MI 
samples. To guarantee the selection of MI samples, 
a certainty measurement is necessary. In this study, 
the certainty measurement is defined as follows: 
{ , }
( ) max ( | )
y pos neg
Cer d P y d
?
?  
Meanwhile, in order to balance the samples in 
the two classes, once an informative MI sample is 
manually annotated, an informative MA sample is 
automatically labeled. In this way, the annotated 
data become more balanced than a random 
selection strategy.  
However, the two sampling strategies discussed 
above are apparently contradicted: while the 
uncertainty measurement is prone to selecting the 
samples whose posterior probabilities are nearest 
to 0.5, the certainty measurement is prone to 
selecting the samples whose posterior probabilities 
are nearest to 1. Therefore, it is essential to find a 
solution to balance uncertainty sampling and 
certainty sampling in imbalanced sentiment 
classification,  
3.2 Co-selecting with Feature Subspace 
Classifiers 
In sentiment classification, a document is 
represented as a feature vector generated from the 
feature set ? ?1,..., mF f f? . When a feature subset, 
i.e., ? ?1 ,...,S S SrF f f?  ( r m? ), is used, the 
original m-dimensional feature space becomes an 
r-dimensional feature subspace. In this study, we 
call a classifier trained with a feature subspace a 
feature subspace classifier. 
Our basic idea of balancing both the uncertainty 
measurement and the certainty measurement is to 
train two subspace classifiers to adopt them 
respectively. In our implementation, we randomly 
select two disjoint feature subspaces, each of 
which is used to train a subspace classifier. On one 
side, one subspace classifier is employed to select 
some certain samples; on the other side, the other 
classifier is employed to select the most uncertain 
sample from those certain samples for manual 
annotation. In this way, the selected samples are 
certain in terms of one feature subspace for 
selecting more possible MI samples. Meanwhile, 
the selected sample remains uncertain in terms of 
the other feature subspace to introduce uncertain 
knowledge into current learning model. We name 
this approach as co-selecting because it 
collectively selects informative samples by two 
separate classifiers. Figure 2 illustrates the co-
selecting algorithm. In our algorithm, we strictly 
constrain the balance of the samples between the 
two classes, i.e., positive and negative. Therefore, 
once two samples are annotated with the same 
class label, they will not be added to the labeled 
data, as shown in step (7) in Figure 2. 
 
Input: 
Labeled data L with balanced samples over the two classes 
Unlabeled pool U  Output: 
    New Labeled data L 
Procedure: 
Loop for N iterations: 
(1). Randomly select a feature subset SF  with 
size r (with the proportion /r m? ? ) from F  
(2). Generate a feature subspace from SF  and 
train a corresponding feature subspace 
classifier CerC  with L 
(3). Generate another feature subspace from the 
complement set of SF , i.e., SF F?  and train 
a corresponding feature subspace classifier 
UncerC  with L 
(4). Use CerC  to select top certain k positive and k 
negative samples, denoted as a sample set 
1CER  
(5). Use UncerC  to select the most uncertain 
positive sample and negative sample from 
1CER   
(6). Manually annotate the two selected samples 
(7). If the annotated labels of the two selected 
samples are different from each other: 
      Add the two newly-annotated samples into L 
 
Figure 2: The co-selecting algorithm 
 
There are two parameters in the algorithm: the 
size of the feature subspace for training the first 
subspace classifier, i.e., ?  and the number of 
142
selected certain samples, i.e., k. Both of the two 
parameters will be empirically studied in our 
experiments. 
3.3 Co-selecting with Selected MA Samples 
Automatically Labeled 
 Input: 
Labeled data L with balanced samples over the two classes 
Unlabeled pool U MA and MI Label (positive or negative) 
Output: 
    New Labeled data L 
Procedure: 
Loop for N iterations: 
(1). Randomly select a proportion of features 
(with the proportion ? ) from F to get a 
feature subset SF  
(2). Generate a feature subspace from SF  and 
train a corresponding subspace classifier CerC  
with L 
(3). Generate another feature subspace from the 
complement set of SF , i.e., SF F?  and train 
a corresponding subspace classifier UncerC  
with L 
(4). Use CerC  to select top certain k positive and k 
negative samples, denoted as a sample set 
1CER  
(5). Use UncerC  to select the most uncertain 
positive sample and negative sample from 
1CER  
(6). Manually annotate the sample that is predicted 
as a MI sample by CerC  and automatically 
annotate the sample that is predicted as 
majority class 
(7). If the annotated labels of the two selected 
samples are different from each other: 
          Add the two newly-annotated samples into L 
Figure 3: The co-selecting algorithm with selected 
MA samples automatically labeled 
 
To minimize manual annotation, it is a good choice 
to automatically label those selected MA samples. 
In our co-selecting approach, automatically 
labeling those selected MA samples is easy and 
straightforward: the subspace classifier for 
monitoring the certainty measurement provides an 
ideal solution to annotate the samples that have 
been predicted as majority class. Figure 3 shows 
the co-selecting algorithm with those selected MA 
samples automatically labeled. The main 
difference from the original co-selecting is shown 
in Step (6) in Figure 3. Another difference is the 
input where a prior knowledge of which class is 
majority class or minority class should be known. 
In real applications, it is not difficult to know this. 
We first use a classifier trained with the initial 
labeled data to test all unlabeled data. If the 
predicted labels in the classification results are 
greatly imbalanced, we can assume that the 
unlabeled data is imbalanced, and consider the 
dominated class as majority class.  
4 Experimentation 
In this section, we will systematically evaluate our 
active learning approach for imbalanced sentiment 
classification and compare it with the state-of-the-
art active learning alternatives. 
4.1 Experimental Setting 
Dataset 
We use the same data as used by Li et al2011a). 
The data collection consists of four domains: Book, 
DVD, Electronic, and Kitchen ?Blitzer et al
2007). For each domain, we randomly select an 
initial balanced labeled data with 50 negative 
samples and 50 positive samples. For the unlabeled 
data, we randomly select 2000 negative samples, 
and 14580/12160/7140/7560 positive samples from 
the four domains respectively, keeping the same 
imbalanced ratio as the whole data. For the test 
data in each domain, we randomly extract 800 
negative samples and 800 positive samples.  
 
Classification algorithm 
The Maximum Entropy (ME) classifier 
implemented with the Mallet 3  tool is mainly 
adopted, except that in the margin-based active 
learning approach (Ertekin et al2007a) where 
SVM is implemented with light-SVM 4 . The 
features for classification are unigram words with 
Boolean weights. 
                                                          
3 http://mallet.cs.umass.edu/  
4 http://www.cs.cornell.edu/people/tj/svm_light/ 
143
 0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
Book DVD Electronic Kitchen
G-
me
an
Random SVM-based Uncertainty Certainty
Co-testing Self-selecting Co-selecting-basic Co-selecting-plus
 Figure 4: Performance comparison of different active learning approaches on imbalanced sentiment 
classification  
Evaluation metrics 
The popular geometric mean 
= rate rateG - mean TP TN?  is adopted, where rateTP  
is the true positive rate (also called positive recall 
or sensitivity) and rateTN  is the true negative rate 
(also called negative recall or specificity) (Kubat 
and Matwin, 1997). 
4.2 Experimental Results 
For thorough comparison, various kinds of active 
learning approaches are implemented including: 
? Random: randomly select the samples from the 
unlabeled data for manual annotation; 
? Margin-based: iteratively select samples 
closest to the hyperplane provided by the SVM 
classifier, which is suggested by Ertekin et al
(2007a) and Ertekin et al2007b). One sample 
is selected in each iteration; 
? Uncertainty: iteratively select samples using 
the uncertainty measurement according to the 
output of ME classifier. One sample is selected 
in each iteration; 
? Certainty: iteratively select class-balanced 
samples using the certainty measurement 
according to the output of ME classifier. One 
positive and negative sample (the positive and 
negative label is provided by the ME classifier) 
are selected in each iteration; 
? Co-testing: first get contention samples (i.e., 
unlabeled examples on which the member 
classifiers predict different labels) and then 
select the least confidence one among the 
hypotheses of different member classifiers, i.e., 
the aggressive strategy as described Muslea et 
al. (2006). Specifically, the member classifiers 
are two subspace classifiers trained by splitting 
the whole feature space into two disjoint 
subspaces of same size; 
? Self-selecting: first select k uncertainty samples 
and then randomly select a positive and 
negative sample from the uncertainty-sample 
set, which is suggested by Doyle et al2011). 
We call it self-selecting since only one 
classifier is involved to measure uncertainty 
and predict class labels. 
 
For those approaches involving random 
selection of features, we run 5 times for them and 
report the average results. Note that the samples 
selected by these approaches are imbalanced. To 
address the problem of classification on 
imbalanced data, we adopt the under-sampling 
strategy which has been shown effective for 
supervised imbalanced sentiment classification (Li 
et al2011a). Our active learning approach 
includes two versions: the co-selecting algorithm 
as described in Section 3.2 and the co-selecting 
with selected MA samples automatically labeled as 
described in Section 3.3. For clarity, we refer the 
former as co-selecting-basic and the latter as co-
selecting-plus in the following. 
144
 
Comparison with other active learning 
approaches 
Figure 4 compares different active learning 
approaches to imbalanced sentiment classification 
when 600 unlabeled samples are selected for 
annotation. Specifically, the parameters ? and k is 
set to be 1/16 and 50 respectively. Figure 4 
justifies that it is challenging to perform active 
learning in imbalanced sentiment classification: the 
approaches of margin-based, uncertainty-based 
and self-selecting perform no better than random 
selection while co-testing only outperforms 
random selection in two domains: DVD and 
Electronic with only a small improvement (about 
1%). In comparison, our approaches, both co-
selecting-basic and co-selecting-plus significantly 
outperform the random selection approach on all 
the four domains. It also shows that co-selecting-
plus is preferable over co-selecting-basic. This 
verifies the effectiveness of automatically labeling 
those selected MA samples in imbalanced 
sentiment classification.  
Specifically, we notice that only using the 
certainty measurement (i.e., certainty) performs 
worst, which reflects that only considering sample 
balance factor in imbalanced sentiment 
classification is not helpful. 
Figure 5 compares our approach to other active 
learning approaches by varying the number of the 
selected samples for manually annotation. For 
clarity, we only include random selection and co-
testing in comparison and do not show the 
performances of the other active learning 
approaches due to their similar behavior to random 
selection. From this figure, we can see that co-
testing is effective on Book and Electronic when 
less than 1500 samples are selected for manual 
annotation but it fails to outperform random 
selection in the other two domains. In contract, our 
co-selecting-plus approach is apparently more 
advantageous and significantly outperforms 
random selection across all domains (p-value<0.05) 
when less than 4800 samples are selected for 
manual annotation. 
 
Sensitiveness of the parameters ?   
The size of the feature subspace is an important 
parameter in our approach. Figure 6 shows the 
performance of co-selecting-plus with varying 
sizes of the feature subspaces for the first subspace 
Electronic
0.68
0.7
0.72
0.74
0.76
0.78
0.8
300 600 900 1200 1500 2400 4800 7000
Number of the manually annoated samples
Random Co-testing Co-selecting-plus
DVD
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
300 600 900 1200 1500 2400 4800 9600
Nubmer of the manually annotated samples
Random Co-testing Co-selecting-plus
Book
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
300 600 900 1200 1500 2400 4800 9600
Nubmer of the manually annotated samples
Random Co-testing Co-selecting-plus
Kitchen
0.7
0.72
0.74
0.76
0.78
0.8
0.82
300 600 900 1200 1500 2400 4800 7000
Number of the manually annoated samples
Random Co-testing Co-selecting-plus
Figure 5:  Performance comparison of three active learning approaches:  random selection, co-testing and co-selecting-plus, by varying the number of the selected samples for manually annotation 
145
classifier CerC . From Figure 6, we can see that a 
choice of the proportion ?  between 1/8 and 1/32 is 
recommended. This result also shows that the size 
of the feature subspace for selecting certain 
samples should be much less than that for selecting 
uncertain samples, which indicates the more 
important role of the uncertainty measurement in 
active learning. 
 
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
 1/2  1/4  1/8 1/16 1/32 1/64 1/128
Proportion of the Selected Features for Subspace
(r /m )
G-
me
an
Book DVD Electornic Kitchen   
Figure 6: Performance of co-selecting-plus over 
varying sizes of feature subspaces (? ) 
  
   
   
   
   
 
 Figure 7: Performance of co-selecting-plus over 
varying numbers of the selected certain samples (k) 
 
Sensitiveness of parameter k 
Figure 7 presents the performance of co-selecting-
plus with different numbers of the selected certain 
samples in each iteration, i.e., parameter k. 
Empirical studies suggest that setting k between 20 
and 100 could get a stable performance. Also, this 
figure demonstrates that using certainty as the only 
query strategy is much less effective (see the result 
when k=1). This once again verifies the importance 
of the uncertainty strategy in active learning. 
Number of MI samples selected for manual 
annotation 
In Table 1, we investigate the number of the MI 
samples selected for manual annotation using 
different active learning approaches when a total of 
600 unlabeled samples are selected for annotation. 
From this table, we can see that almost all the 
existing active learning approaches can only select 
a small amount of MI samples, taking similar 
imbalanced ratios as the whole unlabeled data. 
Although the certainty approach could select 
many MI samples for annotation, this approach 
performs worst due to its totally ignoring the 
uncertainty factor. When our approach is applied, 
especially co-selecting-plus, more MI samples are 
selected for manual annotation and finally included 
to learn the models. This greatly improves the 
effectiveness of our active learning approach.  
 
Table 1: The number of MI samples selected for 
manual annotation when 600 samples are annotated on the whole. 
 
 Book DVD Electronic Kitchen
Random 71 82 131 123 
SVM-based 65 72 135 106 
Uncertainty 78 93 137 136 
Certainty 160 200 236 227 
Co-testing 89 84 136 109 
Self-selecting 87 95 141 126 
Co-selecting-
basic 
101 112 179 174 
Co-selecting-
plus 
161 156 250 272 
 
Precision of automatically labeled MA samples 
In co-selecting-plus, all the added MA samples are 
automatically labeled by the first subspace 
classifier. It is encouraging to observe that 92.5%, 
91.25%, 92%, and 93.5% of automatically labeled 
MA samples are correctly annotated in Book, DVD, 
Electronic, and Kitchen respectively. This suggests 
that the subspace classifiers are able to predict the 
MA samples with a high precision. This indicates 
the rationality of automatically annotating MA 
samples. 
5 Conclusion  
In this paper, we propose a novel active learning 
approach, named co-selecting, to reduce the 
annotation cost for imbalanced sentiment 
classification. It first trains two complementary 
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
1 5 20 50 100 150
Number of the selected certainty samples
G-
me
an
Book DVD Electornic Kitchen
146
classifiers with two disjoint feature subspaces and 
then uses them to collectively select most 
informative MI samples for manual annotation, 
leaving most informative MA samples for 
automatic annotation. Empirical studies show that 
our co-selecting approach is capable of greatly 
reducing the annotation cost and in the meanwhile, 
significantly outperforms several active learning 
alternatives 
For the future work, we are interested in 
applying our co-selecting approach to active 
learning for other imbalanced classification tasks, 
especially those with much higher imbalanced ratio. 
 
Acknowledgments 
The research work described in this paper has been 
partially supported by three NSFC grants, 
No.61003155, No.60873150 and No.90920004, 
one National High-tech Research and 
Development Program of China 
No.2012AA011102, Open Projects Program of 
National Laboratory of Pattern Recognition, and 
the NSF grant of Zhejiang Province No.Z1110551. 
We also thank the three anonymous reviewers for 
their helpful comments. 
 
References  
Attenberg J. and F. Provost. 2010. Why Label when you 
can Search? Alternatives to Active Learning for 
Applying Human Resources to Build Classification 
Models Under Extreme Class Imbalance. In 
Proceeding of KDD-10, 423-432. 
Blitzer J., M. Dredze and F. Pereira. 2007. Biographies, 
Bollywood, Boom-boxes and Blenders: Domain 
Adaptation for Sentiment Classification. In 
Proceedings of ACL-07, 440-447. 
Cui H., V. Mittal, and M. Datar. 2006. Comparative 
Experiments on Sentiment Classification for Online 
Product Reviews. In Proceedings of AAAI-06, 
pp.1265-1270. 
Doyle S., J. Monaco, M. Feldman, J. Tomaszewski and 
A. Madabhushi. 2011. An Active Learning based 
Classification Strategy for the Minority Class 
Problem: Application to Histopathology Annotation. 
BMC Bioinformatics, 12: 424, 1471-2105. 
Ertekin S., J. Huang, L. Bottou and C. Giles. 2007a. 
Learning on the Border: Active Learning in 
Imbalanced Data Classification. In Proceedings of 
CIKM-07, 127-136. 
Ertekin S., J. Huang, L. Bottou and C. Giles. 2007b. 
Active Learning in Class Imbalanced Problem. In 
Proceedings of SIGIR-07, 823-824. 
Freund Y., H. Seung, E. Shamir and N. Tishby. 1997. 
Selective Sampling using the Query by Committee 
algorithm. Machine Learning, 28(2-3), 133-168. 
He Y., C. Lin and H. Alani. 2011. Automatically 
Extracting Polarity-Bearing Topics for Cross-
Domain Sentiment Classification. In Proceeding of 
ACL-11, 123-131. 
Lewis D. and W. Gale. 1994. Training Text Classifiers 
by Uncertainty Sampling. In Proceedings of SIGIR-
94, 3-12. 
Li F., Y. Tang, M. Huang and X. Zhu. 2009. Answering 
Opinion Questions with Random Walks on Graphs. 
In Proceedings of ACL-IJCNLP-09, 737-745. 
Li S. and C. Zong. 2008. Multi-domain Sentiment 
Classification. In Proceedings of ACL-08, short paper, 
pp.257-260. 
Li S., C. Huang, G. Zhou and S. Lee.  2010. Employing 
Personal/Impersonal Views in Supervised and Semi-
supervised Sentiment Classification.  In Proceedings 
of ACL-10,  pp.414-423. 
Li S., Z. Wang, G. Zhou and S. Lee. 2011a. Semi-
supervised Learning for Imbalanced Sentiment 
Classification. In Proceeding of IJCAI-11, 826-1831. 
Li S., G. Zhou, Z. Wang, S. Lee and R. Wang. 2011b. 
Imbalanced Sentiment Classification. In Proceedings 
of CIKM-11,  poster paper, 2469-2472. 
Lloret E., A. Balahur, M. Palomar, and A. Montoyo. 
2009. Towards Building a Competitive Opinion 
Summarization System. In Proceedings of NAACL-
09 Student Research Workshop and Doctoral 
Consortium, 72-77. 
Kubat M. and S. Matwin. 1997. Addressing the Curse of 
Imbalanced Training Sets: One-Sided Selection. In 
Proceedings of ICML-97, 179?186. 
Muslea I., S. Minton and C. Knoblock . 2006. Active 
Learning with Multiple Views. Journal of Artificial 
Intelligence Research, vol.27, 203-233. 
Pang B. and L. Lee. 2008. Opinion Mining and 
Sentiment Analysis: Foundations and Trends. 
Information Retrieval, vol.2(12), 1-135. 
Pang B., L. Lee and S. Vaithyanathan. 2002.Thumbs up? 
Sentiment Classification using Machine Learning 
Techniques. In Proceedings of EMNLP-02, 79-86.  
147
Settles B. 2009. Active Learning Literature Survey. 
Computer Sciences Technical Report 1648, 
University of Wisconsin, Madison, 2009. 
Turney P. 2002. Thumbs up or Thumbs down? 
Semantic Orientation Applied to Unsupervised 
Classification of reviews. In Proceedings of ACL-02, 
417-424.  
Wan X. 2009. Co-Training for Cross-Lingual Sentiment 
Classification. In Proceedings of ACL-IJCNLP-09, 
235?243. 
Yang Y. and G. Ma. 2010. Ensemble-based Active 
Learning for Class Imbalance Problem. J. Biomedical 
Science and Engineering,  vol.3,1021-1028. 
Zhang M. and X. Ye. 2008. A Generation Model to 
Unify Topic Relevance and Lexicon-based Sentiment 
for Opinion Retrieval. In Proceedings of SIGIR-08, 
411-418. 
Zhu J. and E. Hovy. 2007. Active Learning for Word 
Sense Disambiguation with Methods for Addressing 
the Class Imbalance Problem. In Proceedings of 
ACL-07, 783-793. 
148
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 276?285, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
N-gram-based Tense Models for Statistical Machine Translation
Zhengxian Gong1 Min Zhang2 Chewlim Tan3 Guodong Zhou1?
1 School of Computer Science and Technology, Soochow University, Suzhou, China 215006
2 Human Language Technology, Institute for Infocomm Research, Singapore 138632
3 School of Computing, National University of Singapore, Singapore 117417
{zhxgong, gdzhou}@suda.edu.cn mzhang@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
Tense is a small element to a sentence, how-
ever, error tense can raise odd grammars and
result in misunderstanding. Recently, tense
has drawn attention in many natural language
processing applications. However, most of
current Statistical Machine Translation (SMT)
systems mainly depend on translation model
and language model. They never consider and
make full use of tense information. In this pa-
per, we propose n-gram-based tense models
for SMT and successfully integrate them in-
to a state-of-the-art phrase-based SMT system
via two additional features. Experimental re-
sults on the NIST Chinese-English translation
task show that our proposed tense models are
very effective, contributing performance im-
provement by 0.62 BLUE points over a strong
baseline.
1 Introduction
For many NLP applications, such as event extraction
and summarization, tense has been regarded as a key
factor in providing temporal order. However, tense
information has been largely overlooked by current
SMT research. Consider the following example:
SRC:??B$?e?(J , ?U?Ny3??
,???N?I???m??'X"
REF:The embargo is a result of the Cold War and does not
reflect the present situation nor the partnership between China
and the EU.
MOSES: the embargo is the result of the cold war, not reflect
the present situation, it did not reflect the partnership with the
european union.
?*Corresponding author.
Although the translated text produced by Moses1
is understandable, it has very odd tense combination
from the grammatical aspect, i.e. with tense incon-
sistency (is/does in REF vs. is/did in Moses). Ob-
viously, slight modification, such as changing ?is?
into ?was?, can much improve the readability of the
translated text. It is also interesting to note that such
modification can much affect the evaluation. If we
change ?did? to ?does?, the BLEU-4 score increases
from 22.65 to 27.86 (as matching the 4-gram ?does
not reflect the? in REF). However, if we change ?is?
to ?was?, the BLEU score decreases from 22.65 to
21.44.
The above example seems special. To testify its
impact on SMT in wider range, we design a special
experiment based on the 2005 NIST MT data (see
section 6.1). This data has 4 references. We choose
one reference and modify its sentences with error
tense2. After that, we use other 3 references to mea-
sure this reference. The modified reference leads to
a sharp drop in BLEU-4 score, from 52.46 to 50.27
in all. So it is not a random phenomenon that tense
can affect translation results.
The key is how to detect tense errors and choose
correct tenses during the translation procedure. By
carefully comparing the references with Moses out-
put, we obtain the following useful observations,
Observation(1): to most simple sentences, coor-
dinate verbs should be translated with the same tense
while they have different tense in Moses output;
Observation(2): to some compound sentences,
1http://www.statmt.org/moses/
2Such changes are small by mainly modifying one auxiliary
verb for a sentence, such as ?is? was?, ?has? had?.
276
the subordinate clause should have the consisten-
t tense with its main clause while Moses fails;
Observation(3): the diversity of tense usage in a
document is common. However, in most cases, the
neighbored sentences tends to share the same main
tense. In some extreme examples, one tense (past or
present), even dominates the whole document.
One possible solution to model above observa-
tions is using rules. Dorr (2002) refers to six ba-
sic English tense structures and defines the possible
paired combinations of ?present, past, future?. But
the practical cases are very complicated, especial-
ly in news domain. There are a lot of complicat-
ed sentences in news articles. Our preliminary in-
vestigation shows that such six paired combinations
can only cover limited real cases in Chinese-English
SMT.
This paper proposes a simple yet effective method
to model above observations. For each target sen-
tence in the training corpus, we first parse it and ex-
tract its tense sequence. Then, a target-side tense
n-gram model is constructed. Such model can be
used to estimate the rationality of tense combina-
tion in a sentence and thus supervise SMT to reduce
tense inconsistency errors against Observations (1)
and (2) in the sentence-level. In comparison, Ob-
servation (3) actually reflects the tense distributions
among one document. After extracting each main
tense for each sentence, we build another tense n-
gram model in the document-level. For clarity, this
paper denotes document-level tense as ?inter-tense?
and sentence-level tense as ?intra-tense?.
After that, we propose to integrate such tense
models into SMT systems in a dynamic way. It
is well known there are many errors in the current
MT output (David et al2006). Unlike previously
making trouble with reference texts, the BLEU-4 s-
core cannot be influenced obviously by modifying
a small part of abnormal sentences in a static way.
In our system, both inter-tense and intra-tense mod-
el are integrated into a SMT system via additional
features and thus can supervise the decoding proce-
dure. During decoding, once some words with cor-
rect tense can be determined, with the help of lan-
guage model and other related features, the smal-
l component??tense??can affect surrounding words
and improve the performance of the whole sentence.
Our experimental results (see the examples in Sec-
tion 6.4) show the effectiveness of this way.
Rather than the rule-based model, our models are
fully statistical-based. So they can be easily scaled
up and integrated into either phrase-based or syntax-
based SMT systems. In this paper, we employ a
strong phrase-based SMT baseline system, as pro-
posed in Gong et al2011), which uses document as
translation unit, for better incorporating document-
level information.
The rest of this paper is organized as follows: Sec-
tion 2 reviews the related work. Section 3 and 4 are
related to tense models. Section 3 describes the pre-
processing work for building tense models. Section
4 presents how to build target-side tense models and
discuss their characteristics. Section 5 shows our
way of integrating such tense models into a SMT
system. Session 6 gives the experimental results. Fi-
nally, we conclude this paper in Section 7.
2 Related Work
In this section, we focus on related work on integrat-
ing the tense information into SMT. Since both inter-
and intra-tense models need to analyze and extract
tense information, we also give a brief overview on
tense prediction (or tagging).
2.1 Tense Prediction
The tense prediction task often needs to build a mod-
el based on a large corpus annotated with temporal
relations and thus its focus is on how to recognize,
interpret and normalize time expressions. As a rep-
resentative, Lapata and Lascarides (2006) proposed
a simple yet effective data-intensive approach. In
particular, they trained models on main and subor-
dinate clauses connected with some special tempo-
ral marker words, such as ?after? and ?before?, and
employed them in temporal inference.
Another typical task is cross-lingual tense pred-
ication. Some languages, such as English, are in-
flectional, whose verbs can express tense via certain
stems or suffix, while others, such as Chinese of-
ten lack inflectional forms. Take Chinese to English
translation as example, if Chinese text contains par-
ticle word ?
(Le)?, the nearest Chinese verb prefers
to be translated into English verb with the past tense.
Ye and Zhang (2005), Ye et al2007) and Liu et al
(2011) focus on labeling the tenses for keywords in
277
source-side language.
Ye and Zhang (2005) first built a small amoun-
t of manually-labeled data, which provide the tense
mapping from Chinese text to English text. Then,
they trained a CRF-based tense classifier to label
tense on Chinese documents. Ye et al2007) fur-
ther reported that syntactic features contribute most
to the marking of aspectual information. Liu et al
(2011) proposed a parallel mapping method to au-
tomatically generate annotated data. In particular,
they used English verbs to label tense information
for Chinese verbs via a parallel Chinese-English cor-
pus.
It is reasonable to label such source-side verb to
supervise the translation process since the tense of
English sentence is often determined by verbs. The
problem is that due to the diversity of English ver-
b inflection, it is difficult to map such Chinese tense
information into the English text. To our best knowl-
edge, although above works attempt to serve for
SMT, all of them fail to address how to integrate
them into a SMT system.
2.2 Machine Translation with Tense
Dorr (1992) described a two-level knowledge repre-
sentation model based on Lexical Conceptual Struc-
tures (LCS) for machine translation which integrates
the aspectual information and the lexical-semantic
information. Her system is based on an inter-lingual
model and does not belong to a SMT system.
Olsen et al2001) relied on LCS to generate
appropriately-tensed English translations for Chi-
nese. In particular, they addressed tense reconstruc-
tion on a binary taxonomy (present and past) for
Chinese text and reported that incorporating lexical
aspect features of telicity can obtain a 20% to 35%
boost in accuracy on tense realization.
Ye et al2006) showed that incorporating latent
features into tense classifiers can boost the perfor-
mance. They reported the tense resolution results
based on the best-ranked translation text produced
by a SMT system. However, they did not report the
variation of translation performance after introduc-
ing tense information.
3 Preprocessing for Tense Modeling
In this paper, tense modeling is done on the target-
side language. Since our experiments are done
on Chinese to English SMT, our tense models are
learned only from the English text. In the literature,
the taxonomy of English tenses typically includes
three basic tenses (i.e. present, past and future) plus
their combination with the progressive and perfec-
tive aspects. Here, we consider four basic tenses:
present, past, future and UNK (unknown) and ignore
the aspectual information. Furthermore, we assume
that one sentence has only one main tense but maybe
has many subordinate tenses.
This section describes the preprocessing work of
building tense models, which mainly involves how
to extract tense sequence via tense verbs.
3.1 Tense Verbs
Lapata et al006) used syntactic parse trees to find
clauses connected with special aspect markers and
collected them to train some special classifiers for
temporal inference. Inspired by their work, we use
the Stanford parser3 to parse tense sequence for each
sentence.
Take the following three typical sentences as ex-
amples, (a) is a simple sentence which contains two
coordinate verbs, while (b) and (c) are compound
sentences and (b) contains a quoted text.
(a) Japan?s constitution renounces the right to go to war and
prohibits the nation from having military forces except for self-
defense.
(b) ?We also hope Hong Kong will not be affected by diseases
like the severe acute respiratory syndrome again!? , added Ms.
Yang.
(c) Cheng said he felt at home in Hong Kong and he sincerely
wished Hong Kong more peaceful and more prosperous.
Figure 1 shows the parse tree with Penn Treebank
style for each sentence, which has strict level struc-
tures and POS tags for all the terminal words. Here,
the level structures mainly contribute to extract main
tense for each sentence (to be described in Section
3.2) and POS tags are utilized to detect tense verbs,
i.e. verbs with tense information.
Normally, POS tags in the parse tree can distin-
guish five different forms of verbs: the base form
(tagged as VB), and forms with overt endings D for
3http://nlp.stanford.edu/software/lex-parser.shtml
278
Figure 1: The Stanford parse trees with Penn Treebank style
past tense, G for present participle, N for past par-
ticiple, and Z for third person singular present. It is
worth noting that VB, VBG and VBN cannot deter-
mine the specific tenses by themselves. In addition,
the verbs with POS tag ?MD? need to be special-
ly considered to distinguish future tense from other
tenses.
Algorithm 1 illustrates how to determine what
tense a node has. If the return value is not ?UNK?,
the node belongs to a tense verb.
Algorithm 1 Determine the tense of a node.
Input:
The TreeNode of one parse tree, leafnode;
Output:
The tense, tense;
1: tense = ?UNK ??
2: Obtaining the POS tag lpostag from leafnode;
3: Obtaining the word lword from leafnode;
4: if (lpostag in [?V BP ??, ?V BZ ??]) then
5: tense = ?present??
6: else if (lpostag == ?V BD??]) then
7: tense = ?past??
8: else if (lpostag == ?MD??]) then
9: if (lword in [?will??, ?ll??, ?shall??]) then
10: tense = ?future??
11: else if (lword in [?would??, ?could??]) then
12: tense = ?past??
13: else
14: tense = ?present??
15: end if
16: end if
17: return tense;
3.2 Tense Extraction Based on Tense Verbs
As described in Section 1, the inter-tense
(document-level) refers to the main tense of
one sentence and the intra-tense (sentence-level)
corresponds to all tense sequence of one sentence.
This section introduces how to recognize the main
tense and extract all useful tense sequence for each
sentence.
The idea of determining the main tense is to find
the tense verb located in the top level of a parse tree.
According to the Penn Treebank style, the method
to determine the main tense can be described as fol-
lows:
(1) Traverse the parse tree top-down until a tree node
containing more than one child is identified, denot-
ed as Sm .
(2) Consider each child of Sm with tag ?VP?, recursive-
ly traverse such ?VP? node to find a tense verb. If
found, use it as the main tense and return the tense;
if not, go to step (3).
(3) Consider each child of Sm with tag ?S?, which ac-
tually corresponds to subordinate clause of this sen-
tence. Starting from the first subordinate clause, ap-
ply the similar policy of step (2) to find the tense
verb. If not found, search remaining subordinate
clauses.
(4) If no tense verb found, return ?UNK? as the main
tense.
Here, ?VP? nodes dominated by Sm directly are
preferred over those located in subordinate clauses.
This is to ensure that the main tense is decided by
the top-level tense verb.
279
Take Figure 1 as an example, the main tense of
sentence (a) and (b) can be determined only by step
(2). The tense verb of ?(VBZ renounces)? dominat-
ed in the ?VP? tag determines that (a) is in present
tense. Similarly the node ?(VBD added)? indicates
that (b) is in past tense. Sentence (c) needs to be fur-
ther treated by step (3) since there is no ?VP? nodes
dominated by Sm directly. The node ?(VBD said)?
located in the first subordinate clause shows its main
tense is ?past?.
The next task is to extract the tense sequence for
each sentence. They are determined by all tense
verbs in this sentence according to the strict top-
down order. For example, the tense sequence of
sentence (a), (b) and (c) are {present, present},
{present, future, past} and {past, past, past}. In or-
der to explore whether the main tense of intra-tense
model has an impact on SMT or not, we introduce
a special marker ?*? to denote the main tense. So
the tense sequence marked with main tense of (a),
(b) and (c) are {present*, present},{present, future,
past*} and {past*, past, past}. It is worth noting, the
intra-tense model (see Section 4) based on the latter
tense sequence is different to the former.
4 N-gram-based Tense Models
4.1 Tense N-gram Estimation
After applying the previous method to extract tense
for an English text corpus, we can obtain a big tense
corpus.
Given the current tense is indexed as ti, we call
the previous n ? 1 tenses plus the current tense as
tense n-gram.
Based on the tense corpus, tense n-gram statistics
can be done according to the Formula 1.
P (ti|t(i?(n?1)), ..., t(i?1)) =
count(t(i?(n?1)), . . . , t(i?1), ti)
count(t(i?(n?1)), ..., t(i?1))
(1)
Here, the function of ?count? return the tense n-gram
frequency. In order to avoid doing specific smooth-
ing work, we estimate tense n-gram probability us-
ing SRI language modeling (SRILM) tool (Stolcke,
2002).
To compute the probability of intra-tense n-gram,
we first extract all tense sequence for each sentence
and put them into a new file. Based on this new file,
we can get the intra-tense n-gram model via SRILM
tool.
To compute the probability of inter-tense n-gram,
we need to extract the main tense for each sentence
at first. Then, for each document, we re-organized
the main tenses of all sentences into a special line.
After putting all these special lines into a new file,
we can use SRILM to obtain the inter-tense n-gram
model.
4.2 Characteristic of Tense N-gram Models
We construct n-gram-based tense models on English
Gigaword corpus (LDC2003T05). This corpus is
used to build language model for most SMT sys-
tems. It includes 30221 documents (we remove such
files: file size is less than 1K or the number of con-
tinuous ?UNK? tenses is greater than 5).
Figure 2 shows the unigram and bigram probabil-
ities (Log10-style) for intra-tense and inter-tense.
The part (a) and (c) in Figure 2 refer to unigram.
The horizontal axis indicts tense type, and the ver-
tical axis shows its probabilities. The parts (a) and
(c) also indicate ?present? and ?past? are two main
tense types in news domain.
The part (b) and (d) refer to bigram. The horizon-
tal axis indicts history tense. Each different color-
ful bar indicts one current tense. The vertical axis
shows the transfer possibilities from a history tense
to a current tense.
The part (b)4 reflects transfer possibilities of tense
types in one sentence. It also slightly reflects some
linguistic information. For example, in one sen-
tence, the probability of co-occurrence of ?present
? present?, ?past ? past? and ?future ? present?
is more than other combinations, which can be a-
gainst tense inconsistency errors described in Obser-
vation (1) and (2) (see Section 1). However, it seem-
s strange that ?present? past? exceeds ?present?
future?. We checked our corpus and found a lot of
sentences like this??the bill has been . . . , he said. ?.
The part (d) shows tense type can be switched be-
tween two neighbored sentences. However, it shows
the strong tendency to use the same tense type for
4The co-occurrence of the ?UNK? tense and other tense
types in one sentence cannot happen, so the ?UNK? tense is
omitted.
280
Figure 2: statistics of intra-tense and inter-tense N-gram
neighbored sentences. This statistics conform to the
previous observation (3) very much.
5 Integrating N-gram-based Tense Models
into SMT
In this section, we discuss how to integrate the pre-
vious tense models into a SMT system.
5.1 Basic phrase-based SMT system
It is well known that the translation process of SMT
can be modeled as obtaining the best translation e
of the source sentence f by maximizing following
posterior probability(Brown et al1993):
ebest = argmax
e
P (e|f)
= argmax
e
P (f |e)Plm(e)
(2)
where P (e|f) is a translation model and Plm is a
language model.
Our baseline is a modified Moses, which follows
Koehn et al2003) and adopts similar six groups
of features. Besides, the log-linear model ( Och and
Ney, 2000) is employed to linearly interpolate these
features for obtaining the best translation according
to the formula 3:
ebest = argmax
e
M?
m=1
?mhm(e, f) (3)
where hm(e, f) is a feature function, and ?m is
the weight of hm(e, f) optimized by a discrimina-
tive training method on a held-out development da-
ta(Och, 2003).
5.2 The Workflow of Our System
Our system works as follows:
When a hypothesis has covered all source-side
words during the decoding procedure, the decoder
first obtains tense sequence for such hypothesis and
computes intra-tense feature Fs(see Section 5.3). At
the same time, it recognizes the main tense of this
hypothesis and associate the main tense of previous
sentence to compute inter-tense feature Fm (see Sec-
tion 5.3).
Next, the decoder uses such two additional feature
values to re-score this hypothesis automatically and
choose one hypothesis with highest score as the final
translation.
After translating one sentence, the decoder caches
its main tense and pass it to the next sentence.
When one document has been processed, the de-
coder cleans this cache.
In order to successfully implement above work-
flow, we should firstly design some related features,
then resolve another key problem of determining
tense (especially main tense) for SMT output. They
are described in Section 5.3 and 5.4 respectively.
5.3 Two Additional Features
Although the previous tense models show strong
tendency to use the consistent tenses for one sen-
tence or one document, other tense combinations al-
so can be permitted. So we should use such models
in a soft and dynamic way. We design two features:
inter-tense feature and intra-tense feature. And the
weight of each feature is tuned by the MERT script
in Moses packages.
Given main tense sequence of one documen-
t t1, . . . , tm, the inter-tense feature Fm is calculated
according to the following formula:
Fm =
m?
i=2
P (ti|ti?(n?1), . . . , t(i?1)) (4)
The P (?) of formula 4 can be estimated by the for-
mula 1. It is worth noting the first sentence of one
281
document often scares tense information since it cor-
responds to the title at most cases. To the first sen-
tence, the P (?) value is set to 14 (4 tense types).
Given tense sequence of one sentence
s1, . . . , se (e > 1), the intra-tense feature Fs
is calculated as follows:
Fs = e?1
?
?
?
?
e?
i=2
P (si|si?(n?1), . . . , s(i?1)) (5)
Here the square-root operator is used to avoid pun-
ishing translations with long tense sequence. It is
worth noting if the sentence only contains one tense,
the P (?) value of formula 5 is also set to 14 .
Since the average length of intra-tense sequence
is about 2.5, we mainly consider intra-tense bigram
model and thus n equals to 2. 5
5.4 Determining Tense For SMT Output
The current SMT systems often produce odd transla-
tions partly because of abnormal word ordering and
uncompleted text etc. For these abnormal translated
texts, the syntactic parser cannot work well in our
initial experiments, so the previous method to parse
main tense and tense sequence of regular texts can-
not be applied here too.
Fortunately, the solely utilization of Stanford POS
tagger for our SMT output is not bad although it has
the same issues described in Och et al2002). The
reason may be that phrase-based SMT contains short
contexts that POS tagger can utilize while the syntax
parser fails.
Once obtaining a completed hypothesis, the de-
coder will pass it to the Stanford POS tagger and ac-
cording to tense verbs to get alense sequence for
this hypothesis. However, since the POS tagger can
not return the information about level structures, the
decoder cannot recognize the main tense from such
tense sequence.
Liu et al2011) once used target-side verbs to la-
bel tense of source-side verbs. It is natural to consid-
er whether Chinese verbs can provide similar clues
in an opposite direction.
Since Chinese verbs have good correlation with
English verbs (described in section 6.2), we obtain
5In our experiment, the intra-tense bigram model can ob-
tain the comparable performance to the trigram model. And the
inter-tense trigram model can not exceed the bigram one.
Figure 3: trees for parallel sentences
main tense for SMT output according to such tense
verb, which corresponds to the ?VV?(Chinese POS
labels are different to English ones, ?VV? refers to
Chinese verb) node in the top level of the source-side
parse tree. Take Figure 3 as an example, the English
node ?(VBD announced)? is a tense verb which can
tell the main tense for this English sentence. The
Chinese verb ?(VV??)? in the top-level of the
Chinese parse tree is just the corresponding part for
this English verb.
So, before translating one sentence, the decoder
first parses it and records the location of one Chinese
?VV? node which located in the top-level, denotes
this location as Sarea.
Once a completed hypothesis is generated, ac-
cording to the phrase alignment information, the de-
coder can map Sarea into the English location Tarea
and obtain the main tense according to the POS tags
in Tarea .
If Tarea does not contain tense verb, such as the
verb POS tags in the list of {VB, VBN, VBG},
which cannot tell tense type by themselves, our sys-
tem permits to find main tense in the left/right 3
words neighbored to Tarea. And the proportion that
the top-level verb of Chinese has a verb correspon-
dence in English can reach to 83% in this way.
6 Experimentation
6.1 Experimental Setting for SMT
In our experiment, SRI language modeling toolk-
it was used to train a 5-Gram general language
model on the Xinhua portion of the Gigaword cor-
pus. Word alignment was performed on the train-
ing parallel corpus using GIZA++ ( Och and Ney,
2000) in two directions. For evaluation, the NIST
282
BLEU script (version 13) with the default setting is
used to calculate the BLEU score (Papineni et al
2002), which measures case-insensitive matching of
4-grams. To see whether an improvement is statisti-
cally significant, we also conduct significance tests
using the paired bootstrap approach (Koehn, 2004).
In this paper, ?***? and ?**? denote p-values equal
to 0.05, and bigger than 0.05, which mean signifi-
cantly better, moderately better respectively.
Corpus Sentences Documents
Role Name
Train FBIS 228455 10000
Dev NIST2003 919 100
Test NIST2005 1082 100
Table 1: Corpus statistics
We use FBIS as the training data, the 2003 NIST
MT evaluation test data as the development data, and
the 2005 NIST MT test data as the test data. Table 1
shows the statistics of these data sets (with document
boundaries annotated).
6.2 The Correlation of Chinese Verbs and
English Verbs
In this section, an additional experiment is designed
to show English Verbs have close correspondence
with Chinese Verbs.
We use the Stanford POS tagger to tag the Chi-
nese and English sentences in our training corpus
respectively at first. Then we utilize Giza++ to build
alignment for these special Word-POS pairs. Ac-
cording to the alignment results, we find the corre-
sponding relation for some special POS tags in two
languages.
Chinese Verb POS English POS Number
VV Verb VBD 89830
POS VBP 27276
VBZ 32588
MD 40378
VBG 86025
VBN 75019
VB 153596
In sum: 504712
Other Non-Verb 149318
Verb Corresponding Ratio 0.77169
Table 2: The Chinese and English Verb Pos Alignment
The ?Number? column of Table 2 shows the num-
bers of Chinese words with ?VV? tag correspond-
ing to English words with different verb POS tags.
We found Chinese verbs have more than 77% possi-
bilities to align to English verbs in total. However,
our method will fail when some special Chinese sen-
tences only contain noun predicates.
6.3 Experimental Results
All the experiment results are showed on the table 3.
Our Baseline is a modified Moses. The major modi-
fication is input and output module in order to trans-
late using document as unit. The performance of our
baseline exceeds the baseline reported by Gong et al
(2011) about 2 percent based on the similar training
and test corpus.
System BLEU BLEU on Test(%)
Dev(%) BLEU NIST
Moses Md(Baseline) 29.21 28.30 8.4528
Baseline+Fm 30.56 28.87(***) 8.7935
Baseline+Fs 31.28 28.61(**) 8.5645
Baseline+Fs(?) 31.04 28.74(**) 8.6271
Baseline+Fm+Fs 31.75 28.88(***) 8.7987
Baseline+Fm+Fs(*) 31.42 28.92(***) 8.8201
Table 3: The performance of using different feature com-
binations
The system denoted as ?Baseline+Fm? integrates
the inter-tense feature. The performance boosts
0.57(***) in BLEU score.
The system denoted as ?Baseline+Fs? integrates
the intra-tense feature into the baseline. The im-
provement is less than the inter-tense model, on-
ly 0.31(**). It seems the tenses in one sentence
has more flexible formats than the document-level
ones. It is worth noting, this method can gain high-
er performance on the develop data than the one of
?Baseline+Fm? while fail to improve the test data.
Maybe the related weight is tuned over-fit.
The system denoted as ?Baseline+Fs(*)? is s-
lightly different from ?Baseline+Fs?. This experi-
ment is to check whether the main tense has an im-
pact on intra-tense model or not (see Section 3.2).
Here, the intra-tense model based on the tense se-
quence with main tense marker is slightly different
to the model showed in Figure 2. The results are
slightly better than the previous system by 0.13.
Finally, we use the two features together
(Baseline+Fm+Fs). The best way improved the
performance by 0.62(***) in BLEU score over our
baseline.
283
6.4 Discussion
Table 4 shows special examples whose intra-tenses
are changed in our proposed system. The exam-
ple 1 and 2 show such modification can improve
the BLEU score but the example 3 obtains negative
impact. From these examples, we can see not only
tense verbs have changed but also their surrounding
words have subtle variation.
No. BLEU Translation sentence
1 8.64 Baseline: the gulf countries , the bahraini royal fam-
ily members by the military career of part of the
banned to their marriage stories like children , have
become the theme of television films .
19.71 Ours: the gulf country is a member of the bahraini
royal family , a risk by military career risks part of
the banned to their marriage like children , has be-
come a story of the television and film industry .
2 17.16 Baseline:economists said that the sharp appreciation
of the euro , in the recent investigation continues to
have an impact on economic confidence , it is esti-
mated that the economy is expected to rebound to
pick up .
24.25 Ours: economists said that the sharp appreciation of
the euro , in the recent investigation continued to
have an impact on economic confidence and there-
fore no reason to predict the economy expected to
pick up a rebound .
3 73.03 Baseline: the middle east news agency said that , af-
ter the concerns of all parties concerned in the mid-
dle east peace process , israel and palestine , egypt ,
the united states , russia and several european coun-
tries will hold a meeting in washington .
72.95 Ours: the middle east news agency said that after the
concerns of all parties in the middle east peace pro-
cess , israel and palestine , egypt , the united states ,
russia and several european countries held a meeting
in washington .
Table 4: Examples with tense variation using intra-tense
model
From the results showed on Table 3, the
document-level tense model seems more effective
than the sentence-level one. We manually choose
and analyzed 5 documents with significant improve-
ment in the test data. The part (a) of Figure 4 shows
the main tense distributions of one reference. The
main tense distributions for the baseline and our pro-
posed system are showed in the part (b) and (c) re-
spectively. These documents have different numbers
of sentences but all less than 10. The vertical axis in-
dicates different tense: 1 to ?past?, 2 to ?present?, 3
to ?future? and 4 to ?UNK?. It is obvious that our
system has closer distributions to the ones of this
reference.
The examples in Table 5 indicate the joint impact
of inter-tense and intra-tense model on SMT. Sen-
Src:
1)????K???^??? , |????; ??
??"
2)nVd")?|?+ECnd??Oc  ??W
?	?|? ,??Loc5????gONnV
d"??+<??\?! ?;"
Ref:
1)Israeli settlers blockaded a major road to protest a mortar attack
on the settlement area.
2)PLO leader Abbas had also been allowed to go to the West Bank
town of Bethlehem , which is the first time in the past four years
Israeli authorities have allowed a senior Palestinian leader to attend
Christmas celebrations.
Baseline:
1)israel has imposed a main road to protest by mortars attack .
2)the palestinian leader also visited the west bank cities and towns
to bethlehem , which in the past four years , the israeli authorities
allowed the palestinian leading figures attended the ceremony .
Ours:
1)israel has imposed a main road to protest against the mortars at-
tack .
2)leader of the palestinian liberation organization have also been
allowed to go to the west bank towns , bethlehem in the past four
years . this is the first time the israeli authorities allow palestinian
leading figures attended the ceremony .
Table 5: the joint impact of inter- tense and intra-tense
models on SMT
tence 1) and 2) are two neighbored sentences in one
document. Both the reference and our output tend
to use the same main tense type, but the former is in
?past? tense and the latter is in ?present? tense. The
baseline cannot show such tendency. Although our
main tense is different to the reference one, the con-
sistent tenses in document level bring better trans-
lation results than the baseline. And the tenses in
sentence level also show better consistency than the
baseline.
7 Conclusion
This paper explores document-level SMT from the
tense perspective. In particular, we focus on how to
build document-level and sentence-level tense mod-
els and how to integrate such models into a popular
SMT system.
Compared with the inter-tense model which great-
ly improves the performance of SMT, the intra-tense
model needs to be further explored. The reasons are
many folds, e.g. the failure to exclude quoted texts
when modeling intra-tense, since tenses in quoted
texts behave much diversely from normal texts. In
the future work, we will focus on modeling intra-
tense variation according to specific sentence types
and using more features to improve it.
284
Figure 4: the comparison of the inter-tense distributions for reference, baseline and our proposed system
Acknowledgments
This research is supported by part by NUS FRC
Grant R252-000-452-112, the National Natural Sci-
ence Foundation of China under grant No.90920004
and 61003155, the National High Technology Re-
search and Development Program of China (863
Program) under grant No.2012AA011102.
References
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra and R.L.
Mercer. 1992. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. Computa-
tional Linguistics, 19(2):263-311.
Vilar David, Jia Xu, DH`aro L. F., and Hermann Ney.
2006. Error Analysis of Machine Translation Output.
In Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 697-702,
Genoa, Italy.
Bonnie J. Dorr. 1992. A parameterized approach to
integrating aspect with lexical-semantics for machine
translation. In Proceedings of of ACL-2002, pages
257-264.
Bonnie J. Dorr and Terry Gaasterland. 2002. Constraints
on the Generation of Tense, Aspect, and Connecting
Words from Temporal Expressions. Technical Reports
from UMIACS.
Zhengxian Gong, Min Zhang and Guodong Zhou.
2011. Cached-based Document-level Statistical Ma-
chine Translation. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 909-919.
Philipp Koehn, Franz Josef Och ,and DanielMarcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of NAACL-2003, pages 48-54.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 388-395.
Mirella Lapata, Alex Lascarides. 2006. Learning
Sentence-internal Temporal Relations. Journal of Ar-
tificial Intelligence Research, 27:85-117.
Feifan Liu, Fei Liu and Yang Liu. 2011. Learning from
Chinese-English Parallel Data for Chinese Tense Pre-
diction. In Proceedings of IJCNLP-2011, pages 1116-
1124,Chiang Mai, Thailand.
Franz Josef Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models. In Proceedings of of ACL-
2000, pages 440-447.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
et.al. 2002. A smorgasbord of Features for Statistical
Machine Translation. In Proceedings of NAACL-2004,
pages 440-447.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
ACL-2003,pages 160-167, Sapporo, Japan,July.
Mari Olsen, David Traum,Carol Van Ess-Dykema and
Amy Weinberg. 2001. Implicit Cues for Explicit Gen-
eration: Using Telicity as a Cue for Tense Structure in
a Chinese to English MT System. In Proceedings of
MT Summit VIII, Santiago de Compostella, Spain.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: A Method for Automatic E-
valuation of Machine Translation. In Proceedings of
ACL-2002, pages 311-318.
Andreas Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In Proceedings of the Internation-
al Conference on Spoken Language Processing,pages
901-904.
Yang Ye and Zhu Zhang. 2005. Tense tagging for verbs
in cross-lingual context: A case study. In Proceedings
of IJCNLP-2005, pages 885-895.
Yang Ye, V.li Fossum, Steven Abney. 2006. Laten-
t features in Temporal Reference Translation. Fifth
SIGHAN Workshop on Chinese Language Processing,
pages 48-55.
Yang Ye, Karl-Michael Schnelder, Steven Abney. 2007.
Aspect Marker Generation in English-to-Chinese Ma-
chine Translation. Proceedings of MT Summit XI,
pages 521-527,Copenhagen, Denmark.
285
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1006?1016, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Employing Compositional Semantics and Discourse Consistency in 
Chinese Event Extraction 
Peifeng Li, Guodong Zhou, Qiaoming Zhu, Libin Hou 
School of Computer Science & Technology 
Soochow University, Suzhou, 215006, China 
{pfli, gdzhou, qmzhu, 20094227021}@suda.edu.cn 
 
Abstract 
Current Chinese event extraction systems suffer 
much from two problems in trigger 
identification: unknown triggers and word 
segmentation errors to known triggers. To 
resolve these problems, this paper proposes two 
novel inference mechanisms to explore special 
characteristics in Chinese via compositional 
semantics inside Chinese triggers and discourse 
consistency between Chinese trigger mentions. 
Evaluation on the ACE 2005 Chinese corpus 
justifies the effectiveness of our approach over 
a strong baseline. 
1 Introduction 
Event extraction, a classic information extraction 
task, is to identify instances of a predefined event 
type and can be typically divided into four subtasks: 
trigger identification, trigger type determination, 
argument identification and argument role 
determination. In the literature, most studies focus 
on English event extraction and have achieved 
certain success (e.g. Grishman et al 2005; Ahn, 
2006; Hardy et al 2006; Maslennikov and Chua, 
2007; Finkel et al 2005; Ji and Grishman, 2008; 
Patwardhan and Riloff, 2009, 2011; Liao and 
Grishman 2010; Hong et al 2011).  
In comparison, there are few successful stories 
regarding Chinese event extraction due to special 
characteristics in Chinese trigger identification. In 
particular, there are two major reasons for the low 
performance: unknown triggers 1  and word 
segmentation errors to known triggers. Table 1 
gives the statistics of unknown triggers and word 
segmentation errors to known triggers in both the 
                                                          
1 In this paper, a trigger word/phrase occurring in the training 
data is called a known trigger and otherwise, an unknown 
trigger.  
ACE 2005 Chinese and English corpora2 using 10-
fold cross-validation. In each validation, we leave 
10% trigger mentions as the test set and the 
remaining ones as the training set. If a mention in 
the test set doesn?t occurred in the training set, we 
regard it as an unknown trigger. It shows that these 
two cases cover almost 30% of Chinese trigger 
mentions while this figure reduces to only about 
9% in English. It also shows that given the same 
number of event mentions, there are 30% more 
different triggers in Chinese than that in English. 
This justifies the low performance (specifically, 
the recall) of a Chinese event extraction system, 
which normally extracts those known triggers 
occurring in the training data as candidate 
instances and uses a classifier to distinguish correct 
triggers from wrong ones. 
 
Language Chinese English 
%unknown triggers 33.7% 18.5% 
%unknown trigger mentions 20.9% 8.9% 
%word segmentation errors 
to known trigger mentions 
8.7% 0% 
#triggers 763 586 
Table 1. Statistics: a comparison between Chinese and 
English event extraction with regard to unknown 
triggers and word segmentation errors to known triggers. 
Note that word segmentation only applies to Chinese. 
 
In this paper, we propose two novel inference 
mechanisms to Chinese trigger identification by 
employing compositional semantics inside Chinese 
triggers and discourse consistency between 
Chinese trigger mentions.  
The first mechanism is motivated by the 
compositional nature of Chinese words, whose 
semantics can be often determined by the 
component characters. Hence, it is natural to infer 
                                                          
2  The whole Chinese ACE corpus has about 3300 event 
mentions. For the sake of fair comparison, we choose the same 
number of event mentions from the English corpus as the 
cross-validation data. 
1006
unknown triggers by employing compositional 
semantics inside Chinese triggers.  
The second mechanism is enlightened by the 
wide use of discourse consistency in natural 
languages, particularly for Chinese, due to its 
discourse-driven nature (Zhu, 1980). Very often, 
distinguishing true trigger mentions from pseudo 
ones is only possible with contextual information.  
The rest of this paper is organized as follows. 
Section 2 overviews the related work. Section 3 
introduces a state-of-the-art baseline system for 
Chinese event extraction. Sections 4 and 5 describe 
two novel inference mechanisms to Chinese trigger 
identification by employing compositional 
semantics inside Chinese triggers and discourse 
consistency between Chinese trigger mentions. 
Section 6 presents the experimental results. Section 
7 concludes the paper and points out future work. 
2 Related Work 
Almost all the existing studies on event extraction 
concern English. While earlier studies focus on 
sentence-level extraction (Grishman et al 2005; 
Ahn, 2006; Hardy et al 2006), later ones turn to 
employ high-level information, such as document 
(Maslennikov and Chua, 2007; Finkel et al 2005; 
Patwardhan and Riloff, 2009), cross-document (Ji 
and Grishman, 2008), cross-event (Liao and 
Grishman, 2010; Gupta and Ji, 2009) and cross-
entity (Hong et al 2011) information. 
2.1 Chinese Event Extraction  
Compared with tremendous efforts in English 
event extraction, there are only a few studies on 
Chinese event extraction.  
Tan et al(2008) modeled event extraction as a 
pipeline of classification tasks. Specially, they used 
a local feature selection approach to ensure the 
performance of trigger classification (trigger 
identification + trigger type determination) and 
applied multiple levels of patterns to improve the 
coverage of patterns in argument classification 
(argument identification + argument role 
determination). Chen and Ji (2009a) proposed a 
bootstrapping framework, which exploited extra 
information captured by an English event 
extraction system. Chen and Ji (2009b) applied 
various kinds of lexical, syntactic and semantic 
features to address the specific issues in Chinese. 
They also constructed a global errata table to 
record the inconsistency in the training set and 
used it to correct the inconsistency in the test set. Ji 
(2009) extracted cross-lingual predicate clusters 
using bilingual parallel corpora and a cross-lingual 
information extraction system, and then used the 
derived clusters to improve the performance of 
Chinese event extraction. 
2.2 Compositional Semantics 
Almost all the related studies on compositional 
semantics focus on how to combine words together 
to convey complex meanings, such as semantic 
parser (Zettlemoyer and Collins, 2007; Wong and 
Mooney, 2007; Liang et al 2011). However, the 
compositional semantics mentioned in this paper is 
more fined-grained and focuses on how to 
construct Chinese characters into a word and mine 
the semantics of words from the word structures, 
especially of verbs as event triggers.  
To our knowledge, there is only one paper 
associated with compositional semantics inside 
Chinese words. Li (2011) discussed the internal 
structures inside Chinese nouns and used it in word 
segmentation.  
2.3 Discourse Consistency 
Discourse consistency is an important hypothesis 
in natural languages and has been applied to many 
natural language processing applications, such as 
named entity recognition and coreference 
resolution. Specially, several studies have 
successfully incorporated trigger or entity 
consistency constraint into event extraction.  
Yarowsky (1995) and Yangarber et al
(Yangarber and Jokipii, 2005; Yangarber et al 
2007) applied cross-document inference to refine 
local extraction results for disease name, location 
and start/end time. Mann (2007) proposed some 
specific inference rules to improve extraction of 
personal information. Ji and Grishman (2008) 
employed a rule-based approach to propagate 
consistent triggers and arguments across topic-
related documents. Gupta and Ji (2009) used a 
similar approach to recover implicit time 
information for events. Liao and Grishman (2011) 
also used a similar approach and a self-training 
strategy to extract events. Liao and Grishman 
(2010) employed cross-event consistency 
information to improve sentence-level event 
extraction. Hong et al(2011) regarded entity type 
1007
consistency as a key feature to predict event 
mentions and adopted this inference method to 
improve the traditional event extraction system.  
3 Baseline 
As a baseline, we re-implement a state-of-the-art 
system, which consists of four typical components 
(trigger identification, trigger type determination, 
argument identification and argument role 
determination), in a pipeline way and employ the 
same set of features as described in Chen and Ji 
(2009b). 
Besides, the Maximum-Entropy (ME) model is 
employed to train individual component classifiers 
for the above four components. During testing, 
each word in the test set is first scanned for 
instances of known triggers from the training set. 
When an instance is found, the trigger identifier is 
applied to distinguish true trigger mentions from 
pseudo ones. If true, the trigger type determiner is 
then applied to recognize its event type. For any 
entity mentions in the sentence, the argument 
identifier is employed to assign possible arguments 
to them afterwards. Finally, the argument role 
determiner is introduced to assign a role to each 
argument.  
One problem with Chen and Ji?s system is its 
ignoring effective long-distance features. In order 
to resolve this problem and provide a stronger 
baseline, we introduce more refined and 
dependency features in four components:  
? Trigger Identification and Trigger Type 
Determination: 1) syntactic features: path to 
the root of the governing clause, 2) nearest 
entity information: entity type of left 
syntactically/physically nearest entity to the 
trigger + entity, entity type of right 
syntactically/physically nearest entity to the 
trigger mention in the sentence + entity; 3) 
dependency features: the subject and the object 
of the trigger when they are entities. 
? Argument Identification and Argument Role 
Determination: 1) basic features: POS of 
trigger; 2) neighboring words: left neighboring 
word of the entity + its POS, right neighbor 
word of the entity + its POS, left neighbor word 
of the trigger + its POS, right neighbor word of 
the trigger + its POS; 3) dependency feature: 
dependency path from the entity to the trigger; 
4) semantic role features: Arg0 and Arg1 which 
tagged by semantic role labeling tool (Li, et al 
2010). 
3.1 Experimental Setting 
The ACE 2005 Chinese corpus (only the training 
data is available) is used in all our experiments. 
The corpus contains 633 Chinese documents 
annotated with 8 predefined event types and 33 
predefined subtypes. Similar to previous studies, 
we treat these subtypes simply as 33 separate event 
types and do not consider the hierarchical structure 
among them. 
Following Chen and Ji (2009b), we randomly 
select 567 documents as the training set and the 
remaining 66 documents as the test set. Besides, 
we reserve 33 documents in the training set as the 
development set, and follow the setting of ACE 
diagnostic tasks and use the ground truth entities, 
times and values for our training and testing. 
For evaluation, we follow the standards as 
defined in Ji (2009):  
? A trigger is correctly identified if its position in 
the document matches a reference trigger; 
? A trigger type is correctly determined if its 
event type and position in the document match 
a reference trigger; 
? An argument is correctly identified if its 
involved event type and position in the 
document match any of the reference argument 
mentions; 
? An argument role is correctly determined if its 
involved event type, position in the document, 
and role match any of the reference argument 
mentions. 
Finally, all sentences in the corpus are divided 
into words using a word segmentation tool 
ICTCLAS3 with all entities annotated in the corpus 
kept. Besides, we use Stanford Parser (Levy and 
Manning, 2003, Chang, et al 2009) to create the 
constituent and dependency parse trees and employ 
the ME model to train individual component 
classifiers. 
3.2 Experimental Results 
Table 2 and 3 show the Precision (P), Recall (R) 
and F1-Measure (F) on the held-out test set. It 
shows that our baseline system outperforms Chen 
and Ji (2009b) by 1.8, 2.2, 3.9 and 2.3 in F1-
measure on trigger identification, trigger type 
                                                          
3 http://ictclas.org/ 
1008
determination, argument identification and 
argument role determination, respectively, with 
both gains in precision and recall. This is simply 
due to contribution of the newly-added refined and 
dependency features. 
 
Performance 
 
System 
Trigger 
Identification 
Trigger Type 
Determination 
P(%) R(%) F P(%) R(%) F 
Chen and Ji 
(2009b) 
71.5 51.2 59.7 66.5 47.7 55.6 
Our Baseline 75.2 52.0 61.5 70.3 49.0 57.8 
Table 2. Performance of trigger identification and 
trigger type determination  
 
Performance 
 
System 
Argument 
Identification 
Argument Role 
Determination 
P(%) R(%) F P(%) R(%) F 
Chen and Ji 
(2009b) 
56.1 38.2 45.4 53.1 36.2 43.1 
Our Baseline 58.4 42.7 49.3 55.2 38.6 45.4 
Table 3. Performance of argument identification and 
argument role determination 
 
For our baseline system, given the small 
performance gaps between trigger identification 
and trigger type determination (3.7 in F1-measure: 
61.5 vs. 57.8) and between argument identification 
and argument role determination (3.9 in F1-
measure: 49.3 vs. 45.4), the performance 
bottlenecks of our baseline system mainly exist in 
trigger identification and argument identification, 
particularly for the former one. While argument 
identification has the performance gap of 8.5 in 
F1-measure compared to trigger type 
determination (49.3 vs. 57.8), the former one, 
trigger identification, can only achieve the 
performance of 61.5 in F1-measure (in particular 
the recall with only 52.0). In this paper, we will 
focus on trigger identification to improve its 
performance, particularly for the recall, via 
compositional semantics inside Chinese triggers 
and discourse consistency between Chinese trigger 
mentions.  
4 Employing Compositional Semantics 
inside Chinese Triggers  
Language is perhaps the only communicative 
system in nature, which compositionally builds 
structured meanings from smaller pieces, and this 
compositionality is the cognitive mechanism that 
allows for what Humboldt called language?s 
?infinite use of finite means.? As usual, the lexical 
semantics is the smallest piece in most Chinese 
language processing applications. In this section, 
we introduce a more fine-grained semantics - the 
compositional semantics in Chinese verb structure 
- and unveil its effect and usage in Chinese 
language processing by employing it into Chinese 
event extraction. 
4.1 Compositional Semantics inside Chinese 
Triggers 
In English, a component character is just the basic 
unit to form a word instead of a semantics unit. In 
comparison, almost all Chinese characters have 
their own meanings and can be formed as SCWs 
(Single Character Words) themselves. If a Chinese 
word contains more than one character, its 
meaning can be often inferred from the meanings 
of its component characters (Yuan, 1998). Actually, 
it is the normal way of understanding a new 
Chinese word in everyday life of a Chinese native 
speaker. A general method to this problem is to 
systematically explore the morphological 
structures in Chinese words. In this paper, 
compositional semantics provides a simple but 
effective compromise to the general method and 
we leave the general method in the future work. 
Table 4 shows samples of such compositional 
semantics in Chinese words. For example, ???? 
is composed of two characters: ??? and ??? 
which have their own semantics and the semantics 
of ??? ? comes from that of its component 
characters ??? and ???.  
 
Words Characters 
?? (interview4) ? (meet) ?(meet) 
?? (shoot and kill) ?(shoot) ? (kill) 
??(come)  
?? (private letter) 
? (come) ? (to) 
?(private) ?(letter) 
Table 4. Examples of compositional semantics in 
Chinese words 
 
Therefore, it is natural to infer unknown triggers 
by employing compositional semantics inside 
Chinese triggers. Take following two sentences as 
examples: 
(1) 4?????????(Known trigger) 
                                                          
4  Most Chinese words have more than one sense. Here, we 
just give the one when it acts as a trigger. 
1009
(Four students were scratched by the glass.) 
(2)  1???????(Unknown trigger) 
(A passenger was stabbed.) 
where ???? is a known trigger and ???? is an 
unknown one.  
In above examples, the semantics of ???? 
(injure by scratching) can be largely determined 
from those of its component characters ?? ? 
(scratch) and ??? (injure) while the semantics of 
??? ? (injure by stabbing) from those of its 
component characters ??? (stab) and ??? (injure). 
Since these two triggers have similar internal 
structures, we can easily infer that ???? is a 
trigger of injure event if ???? is known as a 
trigger of injure event. Similarly, we can infer 
more triggers for injure event, such as ???? 
(injure by burning), ???? (injure by hitting), ??
? ? (injure by pressing), all with component 
character ??? (injure) as the head and the other 
component character as the way of causing injury.  
Since most triggers in Chinese event extraction 
are verbs 5 , we focus on the compositional 
semantics in the verb structure. Statistics on the 
training set shows that 3.3% triggers (e.g. ???
?? (open letter), ???? (event), ???? (patient's 
condition), etc.) don?t contain a BV and all of them 
are nouns. Normally, almost all verbs contain one 
or more single-character verbs as the basic element 
to construct a verb (we call it basic verb, shorted as 
BV) and the semantics of such a verb thus can be 
inferred from its BV. There are some studies on the 
Chinese verb structure in linguistics. However, 
their structures are much more complex and there 
are no annotated corpora available. We define 
following six main structures from our empirical 
observations: 
(1) BV (e.g. ??? (see), ??? (kill)) 
(2) BV + verb (e.g. ???? (meet)) 
(3) verb + BV (e.g. ???? (fire) ) 
(4) BV + complementation (e.g. ???? (kill) ) 
(5) BV + noun/adj. (e.g. ???? (go to home)) 
(6) noun/adj. +BV (e.g. ???? (shoot using 
gun)). 
                                                          
5 Actually, in the ACE 2005 Chinese (training) corpus, more 
than 90% of triggers are either verbs al or verbal nouns (those 
verbs which act as nouns). For simplicity, we don?t 
differentiate these two types in this paper. 
From above structures, a BV plays an important 
role in the verb structure and most of semantics of 
a verb can be interred from its contained BV and 
two words normally have very similar semantics if 
they have the same BV (e.g. ???? (meet) and 
???? (meet)). Actually, sometime the verb can 
be shortened to its contained BV (e.g. ?????
? ? and ??????? ? have the same 
semantics.).  
4.2 Inferring via Compositional Semantics 
inside Chinese Triggers 
Here a simple rule is employed to infer triggers via 
compositional semantics inside Chinese triggers: a 
verb is a trigger if it contains a BV which occurs 
as a known trigger or is contained in a known 
trigger. Table 5 shows the distribution of the set of 
triggers (contains the same BV 6 ) classified by 
number of triggers.  
From Table 5, we can find out that 85.3% of 
BVs occur in more than one trigger and 56.2% of 
them in more than 4 triggers. As for trigger 
mentions, these percentages become 89.1% and 
65.2% respectively. A extreme example is that 
85.2% (75/88) of triggers of Trial-Hearing event 
mentions contain ??? (trial) and 85.4% (117/138) 
of triggers of injure event mentions contains ??? 
(injure).  
 
Number  Distribution over 
Triggers 
Distribution over 
Trigger Mentions 
1 14.7% 10.9% 
2~4 29.1% 23.9% 
5~9 28.1% 32.9% 
>=10 28.1% 32.3% 
Table 5. Distribution of BVs in the number of 
triggers/trigger mentions  
 
In this paper, the inference is done as follows: 
? Add all single-character triggers into the BV set 
if it?s a verb; 
? Split all other triggers in the training set into a 
set of single characters and include all single 
characters into the BV set if it?s a verb; 
? For each word in the test set, it is identified as a 
trigger if it contains a BV. 
It is worthwhile to note that such inference 
works for unknown triggers and word 
                                                          
6 We didn?t tag BVs in the training set and regards all single-
character verbs contained in triggers as BVs. 
1010
segmentation errors to known triggers since in both 
cases, their BVs will always exist as either a SCW 
or a component of a word. 
4.3 Noise Filtering  
One problem with above inference is that while it 
is able to recover some true triggers and increase 
the recall, it may introduce many pseudo ones and 
harm the precision. To filter out those pseudo 
triggers, we propose following rules according to 
our intuition and statistics over the training set. 
Non-trigger Filtering 
A Chinese word will not be a trigger if it 
appears in the training set but never trigger an 
event. Statistics on the training set shows that this 
rule applies at 99.7% of cases. 
POS filtering 
A Chinese word will not be a trigger if it has a 
different POS from that of the same known 
trigger or similar known triggers 7  in the 
training set. In Chinese, a single-character verb 
has very high probability of composing words (e.g. 
??? (come), ??? (act as), ??? (combine), etc) 
with different POS from the single-character verb 
itself, such as preposition (e.g. ??? ? (for)), 
conjunction (e.g. ???? (and)), etc. Statistics on 
the training set shows that this rule applies at 
97.3% of cases.  
Verb structure filtering 
A Chinese word will not be a trigger if its verb 
structure is different from that of the same 
known trigger or similar known triggers in the 
training set. Figure 1 shows different distributions 
of three BVs over six verb structures as described 
in subsection 4.1. For example, we can find that all 
triggers including ??? (unbind) (e.g. ???? (fire), 
???? (fire), ???? (disband)) just have one verb 
structure (BV + verb) and those of ??? (kill) have 
4 structures. Obviously, we can use such 
distribution information to filter out pseudo 
triggers. For example, although both word ???? 
(console) and ???? (decompose) are constructed 
form verb ???, their verb structure (verb + BV) 
does not appear in the training set. Therefore, they 
will be filtered our via verb structure filtering. 
                                                          
7 Similar triggers are those ones which have the same BV and 
verb structure. 
Statistics on the training set shows that this rule 
applies at 95.5% of cases. 
 
0
0.2
0.4
0.6
0.8
1
BV verb+BV
BV+Verb
N/Adj+BV
BV+Comp
BV+N/Adj
?
?
?
Figure 1. Distribution of three BVs (??? (unbind), ??? 
(trial) and ??? (kill)) over six verb structures in 
constructing triggers 
5 Employing Discourse Consistency 
between Chinese Trigger Mentions  
Chinese event extraction may suffer much from the 
errors propagated from upstream processing such 
as part-of-speech tagging and parsing, especially 
word segmentation. To alleviate word 
segmentation errors to known triggers, Chen and Ji 
(2009b) constructed a global errata table to record 
the inconsistency in the training set and proved its 
effectiveness. In this paper, a merge and split 
method is applied to recover those known triggers. 
In this way, word segmentation errors can be 
alleviated to certain extent.  
For unknown triggers, we can merge two or 
more neighboring short words or single characters 
as a trigger candidate. In this paper, for each 
single-character verb in a document after word 
segmentation, this single-character verb can be 
merged with either previous SCW or next SCW to 
form a trigger candidate if this single-character 
verb has occurred in the training set with the same 
verb structure. 
Given above recovered triggers for both known 
and unknown triggers, the key issue here is how to 
distinguish true triggers from pseudo ones. In this 
paper, we employ discourse consistency between 
Chinese trigger mentions for Chinese event 
extraction. Previous studies on English event 
extraction have proved the effectiveness of both 
cross-entity and cross-document consistency.  
5.1 Discourse Consistency between Chinese 
Trigger Mentions  
As a discourse-driven language, the syntax of 
1011
Chinese is not as strict as English and sometime 
we must infer from the discourse-level information 
to understand the meaning of a sentence. Kim 
(2000) compared the use of overt subjects in 
English and Chinese and he found that overt 
subjects occupy over 96% in English, while this 
percentage drops to only 64% in Chinese. 
Similarly, argument missing is another issue in 
Chinese event extraction and almost 55% of 
arguments are missing in the ACE 2005 Chinese 
corpus. Normally, using a feature-based approach 
to distinguish true triggers from pseudo ones is 
very difficult from the sentence level if some of 
related arguments are missing from the trigger-
occurring sentence. Take following two contingent 
sentences as examples: 
(3) ????? 3????????????  
(The United States and the Democratic 
People's Republic of Korea finished missile 
talks in Kuala Lumpur.) 
(4) ???????? 
(The talks are serious.) 
While it is relatively easy to determine that 
mention ???? in sentence (3) indicates a meet 
event from the contained information in itself 
(there are many entities, such as agents, time and 
place in the sentence) and difficult to determine 
that mention ???? in sentence (4) is a meet event 
from the contained information in itself, we can 
easily infer from sentence (3) that sentence (4) also 
indicates a meet event, using discourse consistency: 
if one instance of a word is a trigger mention, other 
instances in the same discourse will be a trigger 
mention with high probability.  
 
Language Discourse-based Instance-based 
English 70.2% 87.5% 
Chinese 90.5% 95.4% 
Table 6. Comparison of discourse consistency between 
Chinese and English trigger mentions 
 
Table 6 compares the probabilities of discourse 
consistency between Chinese and English trigger 
mentions in the ACE 2005 Chinese and English 
corpora. A trigger may appear many times in a 
discourse. It?s considered discourse-consistent 
when all the appearances of a trigger have the 
same event type while instance-based consistency 
refers to pair-wired cases. It shows that within the 
discourse, there is a strong consistency in both 
Chinese and English between trigger mentions: if 
one instance of a word is a trigger, other instances 
in the same discourse will be a trigger of the same 
event type with very high probability. 
0.85
0.9
0.95
1
?
?
? ?
?
? ?
?
?
?
?
?
?
?
?
?
?
 
Figure 2. Probabilities of discourse-level consistency of 
top 10 frequent triggers 
It also shows that discourse consistency in 
Chinese triggers holds much more likely than the 
English counterpart. Figure 2 give the probabilities 
of discourse-level consistency of top 10 frequent 
triggers, which occupy 18% of event mentions in 
the ACE 2005 Chinese corpus. 
5.2 Inference via Discourse Consistency 
between Chinese Trigger Mentions  
Given a discourse and different mentions of a 
trigger returned by the trigger identifier, we can 
simply accept those mentions with high probability 
as true mentions of the trigger and discard those 
with low probability8. However, for those mentions 
in-between, an additional discourse-level trigger 
identifier is further employed to determine whether 
a trigger mention is true or not from the discourse 
level by augmenting the normal trigger identifier 
with several features to explore the consistency 
information between trigger mentions in the 
discourse (first three features) and the related 
information returned from the trigger type 
identifier (last two features).  
? Probability of the discourse consistency of the 
candidate trigger mention in the training set. If 
it doesn?t exist in the training set, we infer its 
probability from that of all of its similar triggers 
? Number of candidate trigger mentions being a 
trigger in the same discourse via trigger 
identification 
? Number of candidate trigger mentions being a 
non-trigger in the same discourse via trigger 
identification 
                                                          
8 The high and low probability thresholds are fine-tuned to 
95% and 5% respectively, using the development set. 
1012
? Event type of candidate trigger mention via 
trigger type determination 
? Confidence of trigger type determination 
6 Experiments 
In this section, we evaluate our two inference 
mechanisms in Chinese trigger identification and 
its application to overall Chinese event extraction, 
using the same experimental settings as described 
in Subsection 3.1. 
6.1 Chinese Trigger Identification 
Table 7 shows the impact of compositional 
semantics in trigger identification. Here, the 
baseline just extracts those triggers occurring in the 
training data. It justifies the effectiveness of our 
compositional semantics-based inference 
mechanism in recovering true triggers and its three 
filtering rules in removing pseudo triggers.  
 
                    Numbers 
Approaches 
Triggers Non-triggers 
Baseline 266 629 
+Compositional semantics 
without filtering 
334 1885 
+ Non-trigger filtering 328 1062 
+ POS filtering 325 974 
+ Verb structure filtering 302 444 
Gold 367 - 
Table 7. Impact of compositional semantics in trigger 
identification 
 
To reduce those pseudo triggers after above 
inference process, three rules are introduced.  
The first rule, the non-trigger filtering rule, 
filters out those pseudo ones in the test set which 
do not frequently occur as trigger mentions in the 
training set. In particular, to keep true triggers in 
our candidate set as many as possible, we just filter 
out those candidates which occur as non-triggers 
more than 5 times in the training set according to 
our validation on the development set. Table 7 
shows that 43.7% (823) of pseudo triggers are 
filtered out while only 1.8% (6) of true ones is 
wrongly filtered out.  
The second rule, the POS filtering rule, just 
filters out 8.3% (88) of pseudo triggers, due to 
POS errors in word segmentation and constituent 
parsing (e.g. 9.4% of candidate triggers have 
wrong POS tags in the development set.). Manual 
inspection shows that if we correct those wrong 
POS tags, that percentage will be increased to 
14.5%. 
The third rule, the verb structure filtering rule, is 
deployed in following steps: 1) keeping all 
candidates if they act as a trigger in the training set; 
2) if the candidate is a SCW, removing it when it 
does not occur as a BV in any triggers in the 
training set; 3) if the candidate is not a SCW, 
calculating the condition probability of its similar 
trigger words as triggers in the training set9 and 
then deleting all candidates whose conditional 
probabilities are less than a threshold ? , which is 
fine-tuned to 0.5. Figure 3 shows the effect on 
precision, recall and F1-measure of varying the 
threshold?  on the development set. 
0.5
0.6
0.7
0.8
0 0
.
1
0
.
2
0
.
3
0
.
4
0
.
5
0
.
6
0
.
7
P
R
F
 
Figure 3. Effect of threshold ?  on the development 
set 
 
                Performance 
System 
Trigger Identification 
P(%) R(%) F 
Baseline 75.2 52.0 61.5 
+Compositional semantics 
without filtering 
34.8 66.8 45.8 
+ Non-trigger filtering 49.4 66.5 56.7 
+ POS filtering 50.2 65.9 57.0 
+ Verb structure filtering 73.5 62.1 67.4 
+Discourse consistency 79.3 63.5 70.5 
Table 8. Contribution to Chinese triggers identification 
(incremental) 
 
Table 8 shows the contribution of employing 
compositional semantics and discourse consistency 
to trigger identification on the held-out test set. We 
can find out that our approach dramatically 
enhances F1-measure by 9.0 units, largely due to a 
dramatic increase of 11.5% in recall, benefiting 
from both compositional semantics and discourse 
consistency mechanisms. We expect that the 
precision will also increase since our filtering 
approach successfully filters out almost 30% more 
                                                          
9 If there are more than one BV in a candidate, we calculate 
the average one. 
1013
non-triggers and the number of non-trigger 
mentions is less than that of the baseline. 
Unfortunately, the resulting set of 444 non-trigger 
mentions (after all filtering) is not a subset of 
original 629 non-trigger ones. Our observation 
shows that our compositional semantics inference 
adds almost 10% new non-triggers into candidates 
which are very hard to distinguish.  
Table 8 also justifies the impact of the discourse 
consistency between trigger mentions in trigger 
identification and the effect of the additional 
discourse-level trigger identifier, with a big gain of 
5.8% in precision and a small gain of 1.4% in 
recall. 
6.2 Chinese Event Extraction 
Table 9 shows the contribution of trigger 
identification with compositional semantics and 
discourse consistency to overall event extraction 
on the held-out test set. In addition, we also report 
the performance of two human annotators (The 
human annotator 1 is a first year postgraduate 
student with no background to Chinese event 
extraction while the human annotator 2 is a third 
year postgraduate student working on Chinese 
event extraction) on 33 texts (a subset of the held-
out test set). From the results presented in Table 9, 
we can find that our approach can improve the F1-
measure for trigger identification by 9.0 units, 
trigger type determination by 9.1 units, argument 
identification by 6.0 units and argument role 
determination (i.e. overall event extraction) by 5.4 
units, largely due to the dramatic increase in recall 
of 11.5%, 11.2%, 7.5% and 7.2%.  
 
                        Performance 
 
System/Human 
Trigger 
Identification 
Trigger Type 
Determination 
Argument 
Identification 
Argument Role 
Determination 
P(%) R(%) F P(%) R(%) F P(%) R(%) F P(%) R(%) F 
Our Baseline 75.2 52.0 61.5 70.3 49.0 57.8 58.4 42.7 49.3 55.2 38.6 45.4 
+Compositional semantics 73.5 62.1 67.4 70.2 59.1 64.2 58.0 48.9 53.0 54.7 44.5 49.1 
+Discourse consistency 79.3 63.5 70.5 75.2 60.2 66.9 61.6 50.2 55.3 56.9 45.8 50.8 
Human annotator1(blind) 63.3 62.9 63.1 61.7 59.5 60.6 64.6 54.1 58.9 60.9 48.2 53.8 
Human annotator2(familiar) 72.6 74.3 73.4 69.1 70.2 69.6 71.5 65.9 68.6 66.4 54.6 59.9 
Inter-Annotator Agreement 45.8 42.9 44.3 45.3 42.5 43.8 60.4 49.7 54.5 55.1 45.9 50.1 
Table 9: Overall contribution to Chinese event extraction  
 
In addition, the results of two annotators show 
that Chinese event extraction is really challenging 
even for a well-educated human being. As shown 
in Table 9, the inter-annotator agreement on trigger 
identification and trigger type determination is 
even less than 45%. Although this figure is very 
low, it is not surprising: the results on the English 
ACE 2005 corpus show that the inter-annotator 
agreement on trigger identification is only about 
40% (Ji and Grishman, 2008). Detailed analysis 
shows that a human annotator tends to make more 
mistakes in trigger identification for two reasons. 
The first reason is that a human annotator always 
misses some event mentions when a sentence 
contains more than one event mention. The second 
reason is that it is hard to identify an event mention 
due to the failure of following specified annotation 
guidelines, as mentioned in Ji and Grishman 
(2008). Table 9 also shows the performance gaps 
of human annotators between trigger identification 
and trigger type determination is very small (2.5% 
and 3.8% in F1-measure). It ensures that trigger 
identification is the most important step in Chinese 
event extraction for a human being. For human 
annotators, it?s much easier to determine the event 
type of a trigger, identify its arguments and 
determine the role of each argument, all with more 
than 90% in accuracy, once a trigger is identified 
correctly.  
6.3 Discussion 
Compared with English, the word structures in 
Chinese are much more complex and diverse, 
causing a lot of troubles in Chinese language 
processing. We ensure that compositional 
semantics in Chinese words is very useful for 
many Chinese language processing applications, 
such as machine translation, semantic parser, etc. 
For example, many actions (e.g. ??? (hack), ??? 
(bite), ??? (kick), etc) can combine with ??? 
(injure) to form words and most of those words 
have similar semantics. The results in table 8 show 
its contribution in Chinese event extraction. 
Although our approach is simple, the result is 
1014
promising enough for further efforts in this 
direction.  
This paper shows that the compositional 
semantics in the verb structure provides an ideal 
way to expand the coverage of triggers. As a 
discourse-driven language, ellipsis is very common 
in Chinese, causing inference from the discourse-
level information is a fundamental requirement to 
understand the meaning of a clause, sentence or 
discourse. 
7 Conclusion 
In this paper we propose two novel inference 
mechanisms to Chinese trigger identification. In 
particular, compositional semantics inside Chinese 
triggers and discourse consistency between 
Chinese trigger mentions are used to resolve two 
critical issues in Chinese trigger identification: 
unknown triggers and word segmentation errors to 
known triggers. We give good reasons why this 
should be done, and present effective methods how 
this could be done. It shows that such novel 
inference mechanisms for Chinese event extraction 
are linguistically justified and pragmatically 
beneficial to real world applications.  
In future work, we will focus on how to 
introduce the discourse information into the 
individual classifiers to capture those long-distance 
features and joint learning of subtasks in Chinese 
event extraction. 
Acknowledgments 
The authors would like to thank three anonymous 
reviewers for their comments on this paper. This 
research was supported by the National Natural 
Science Foundation of China under Grant No. 
61070123 and No. 90920004, the National 863 
Project of China under Grant No. 2012AA011102. 
References 
David Ahn. 2006. The Stages of Event Extraction. In 
Proc. COLING/ACL 2006 Workshop on Annotating 
and Reasoning about Time and Events. Pages 1-8, 
Sydney, Australia. 
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and 
Christopher Manning. 2009. Discriminative 
Reordering with Chinese Grammatical Relations 
Features. In Proc. Third Workshop on Syntax and 
Structure in Statistical Translation, pages 51-59. 
Zheng Chen and Heng Ji. 2009a. Can One Language 
Bootstrap the Other: A Case Study on Event 
Extraction. In Proc. NAACL HLT Workshop on 
Semi-supervised Learning for Natural Language 
Processing, pages 66-74, Boulder, Colorado. 
Zheng Chen and Heng Ji. 2009b. Language Specific 
Issue and Feature Exploration in Chinese Event 
Extraction. In Proc. NAACL HLT 2009, pages 209-
212, Boulder, CO. 
Jenny Rose Finkel, Trond Grenager and Christopher 
Manning. 2005. Incorporating Non-local 
Information into Information Extraction Systems by 
Gibbs Sampling. In Proc. ACL 2005, pages 363-370, 
Ann Arbor, MI. 
Prashant Gupta and Heng Ji. 2009. Predicting Unknown 
Time Arguments based on Cross-Event Propagation. 
In Proc. ACL-IJCNLP 2009, pages 369-272, Suntec, 
Singapore. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System 
Description. In Proc. ACE 2005 Evaluation 
Workshop, Gaithersburg, MD. 
Hilda Hardy, Vika Kanchakouskaya and Tomek 
Strzalkowski. 2006. Automatic Event Classification 
Using Surface Text Features. In Proc. AAAI 2006 
Workshop on Event Extraction and Synthesis, pages 
36-41, Boston, MA. 
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, 
Guodong Zhou and Qiaoming Zhu. 2011. Using 
Cross-Entity Inference to Improve Event Extraction. 
In Proc. ACL 2011, pages 1127-1136, Portland, OR. 
Heng Ji. 2009. Cross-lingual Predicate Cluster 
Acquisition to Improve Bilingual Event Extraction 
by Inductive Learning. In Proc. NAACL HLT 
Workshop on Unsupervised and Minimally 
Supervised Learning of Lexical Semantics, pages 
27-35, Boulder, CO. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proc. ACL-08: HLT, pages 254-262, Columbus, OH. 
Young-Joo Kim. 2000. Subject/object drop in the 
acquisition of Korean: A Cross-linguistic 
Comparison. Journal of East Asian Linguistics, 9(4): 
325-351. 
Roger Levy and Christopher D. Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank? In 
Proc. ACL 2003, pages 439-446, Sapporo, Japan.  
Shasha Liao and Ralph Grishman. 2010. Using 
Document Level Cross-Event Inference to Improve 
Event Extraction. In Proc. ACL 2010, pages 789-
797, Uppsala, Sweden. 
Zhongguo Li. 2011. Parsing the Internal Structure of 
Words: A New Paradigm for Chinese Word 
Segmentation. In Proc. ACL 2011, pages 1405-1414, 
Portland, OR. 
Percy Liang, Michael I. Joedan and Dan Klein. 2011. 
Learning Dependency-Based Compositional 
1015
Semantics. In Proc. ACL 2011, pages 590-599, 
Portland, OR. 
Gideon Mann. 2007. Multi-document Relationship 
Fusion via Constraints on Probabilistic Databases. In 
Proc. HLT/NAACL 2007, pages 332-229, Rochester, 
NY. 
Mstislav Maslennikov and Tat-Seng Chua. 2007. A 
Multi Resolution Framework for Information 
Extraction from Free Text. In Proc. ACL 2007, 
pages 592-599, Prague, Czech Republic. 
Siddharth Patwardhan and Ellen Riloff. 2007. Effective 
Information Extraction with Semantic Affinity 
Patterns and Relevant Regions. In Proc. 
EMNLP/CoNLL 2007, pages 717-727, Prague, 
Czech Republic. 
Siddharth Patwardhan and Ellen Riloff. 2009. A Unified 
Model of Phrasal and Sentential Evidence for 
Information Extraction. In Proc. EMNLP 2009, 
pages 151-160, Singapore. 
Hongye Tan, Tiejun Zhao, Jiaheng Zheng. 2008. 
Identification of Chinese Event and Their Argument 
Roles. Proc. of the 2008 IEEE 8th International 
Conference on Computer and Information 
Technology Workshops, pages 14-19, Sydney, 
Australia. 
Yuk Wah Wong and Raymond J. Mooney. 2007. 
Learning Synchronous Grammars for Semantic 
Parsing with Lambda Calculus. In Proc. ACL 2007, 
pages 960-967, Prague, Czech Republic. 
Roman Yangarber, Clive Best, Peter von Etter, Flavio 
Fuart, David Horby and Ralf Steinberger. 2007. 
Combining Information about Epidemic Threats 
from Multiple Sources. In Proc. RANLP 2007 
workshop on Multi-source, Multilingual Information 
Extraction and Summarization. Borovets, pages 41-
48, Borovets, Bulgaria.  
Roman Yangarber and Lauri Jokipii. 2005. 
Redundancy-based Correction of Automatically 
Extracted Facts. In Proc. EMNLP 2005, pages 57-64, 
Vancouver, Canada.  
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proc. ACL 1995, pages 189-196, Cambridge, MA. 
Minglin Yuan. 1998. Studies on Valency in Modern 
Chinese. Chinese Commerce and Trade Press, 
Beijing, China. 
Luke S. Zettlemoyer and Michael Collins. 2007. Online 
Learning of Relaxed CCG Grammars for Parsing to 
Logical Form. In EMNLP/CoNLL 2007, pages 678-
687, Prague, Czech Republic. 
Dexi Zhu. 1980. Research on Chinese Modern 
Grammars. Chinese Commerce and Trade Press, 
Beijing, China.  
1016
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1445?1454, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Unified Dependency Parsing of Chinese Morphological
and Syntactic Structures
Zhongguo Li Guodong Zhou
Natural Language Processing Laboratory
School of Computer Science and Technology
Soochow University, Suzhou, Jiangsu Province 215006, China
{lzg, gdzhou}@suda.edu.cn
Abstract
Most previous approaches to syntactic pars-
ing of Chinese rely on a preprocessing step
of word segmentation, thereby assuming there
was a clearly defined boundary between mor-
phology and syntax in Chinese. We show
how this assumption can fail badly, leading
to many out-of-vocabulary words and incom-
patible annotations. Hence in practice the
strict separation of morphology and syntax in
the Chinese language proves to be untenable.
We present a unified dependency parsing ap-
proach for Chinese which takes unsegmented
sentences as input and outputs both morpho-
logical and syntactic structures with a single
model and algorithm. By removing the inter-
mediate word segmentation, the unified parser
no longer needs separate notions for words
and phrases. Evaluation proves the effective-
ness of the unified model and algorithm in
parsing structures of words, phrases and sen-
tences simultaneously. 1
1 Introduction
The formulation of the concept of words has baf-
fled linguists from ancient to modern times (Hock-
ett, 1969). Things are even worse for Chinese, partly
due to the fact that its written form does not delimit
words explicitly. While we have no doubt that there
are linguistic units which are definitely words (or
phrases, for that matter), it?s a sad truth that in many
cases we cannot manage to draw such a clear bound-
ary between morphology and syntax, for which we
now give two arguments.
1Corresponding author is Guodong Zhou.
The first argument is that many sub-word linguis-
tic units (such as suffixes and prefixes) are so pro-
ductive that they can lead to a huge number of out-
of-vocabulary words for natural language process-
ing systems. This phenomenon brings us into an
awkward situation if we adhere to a rigid separa-
tion of morphology and syntax. Consider charac-
ter ? ?someone? as an example. On the one hand,
there is strong evidence that it?s not a word as it can
never be used alone. On the other hand, taking it as a
mere suffix leads to many out-of-vocabulary words
because of the productivity of such characters. For
instance, Penn Chinese Treebank (CTB6) contains
??? ?one that fails? as a word but not ???
?one that succeeds?, even with the word ?? ?suc-
ceed? appearing 207 times. We call words like ?
?? ?one that succeeds? pseudo OOVs. By defini-
tion, pseudo OOVs are OOVs since they do not occur
in the training corpus, though their components are
frequently-seen words. Our estimation is that over
60% of OOVs in Chinese are of this kind (Section 2).
Of course, the way out of this dilemma is to parse
the internal structures of these words. That is to
say, we can still regard characters like ? as suf-
fixes, taking into account the fact that they cannot be
used alone. Meanwhile, pseudo OOVs can be largely
eliminated through analyzing their structures, thus
greatly facilitating syntactic and semantic analysis
of sentences. In fact, previous studies have revealed
other good reasons for parsing internal structures of
words (Zhao, 2009; Li, 2011).
The second argument is that in Chinese many lin-
guistic units can form both words and phrases with
exactly the same meaning and part-of-speech, which
1445
?? NN ?? NN ? NN
MOD MOD
? NN ? NN
MOD
Figure 1: Unified parsing of words and phrases.
causes lots of incompatible annotations in currently
available corpora. Take character? ?law? as an ex-
ample. It is head of both?? ?criminal law? and?
???? ?environmental protection law?, but CTB
treat it as a suffix in the former (with the annotation
being ??_NN) and a word in the later (the anno-
tation is??_NN??_NN?_NN). These annota-
tions are incompatible since in both cases the char-
acter ? ?law? bears exactly the same meaning and
usage (e.g. part-of-speech). We examined several
widely used corpora and found that about 90% of
affixes were annotated incompatibly (Section 2). In-
compatibility can be avoided through parsing struc-
tures of both words and phrases. Figure 1 conveys
this idea. A further benefit of unified parsing is to
reduce data sparseness. As an example, in CTB6?
?machine? appears twice in phrases but 377 times in
words (e.g. ??? ?accelerator?). Word structures
in Chinese can be excellent guide for parsing phrase
structures, and vice versa, due to their similarity.
The present paper makes two contributions in
light of these issues. Firstly, in order to get rid of
pseudo OOVs and incompatible annotations, we have
annotated structures of words in CTB6, after which
statistical models can learn structures of words as
well as phrases from the augmented treebank (Sec-
tion 4). Although previous authors have noticed
the importance of word-structure parsing (Li, 2011;
Zhao, 2009), no detailed description about annota-
tion of word structures has been provided in the liter-
ature. Secondly, we designed a unified dependency
parser whose input is unsegmented sentences and
its output incorporates both morphological and syn-
tactic structures with a single model and algorithm
(Section 5). By removing the intermediate step of
word segmentation, our unified parser no longer de-
pends on the unsound notion that there is a clear
boundary between words and phrases. Evaluation
(Section 6) shows that our unified parser achieves
satisfactory accuracies in parsing both morphologi-
cal and syntactic structures.
corpus OOV pseudo percent
CTB6 158 112 70.9
MSR 1,783 1,307 73.3
PKU 2,860 1,836 64.2
AS 3,020 2,143 71.0
CITYU 1,665 1,100 66.0
Table 1: Statistics of pseudo OOVs for five corpora.
2 Pseudo OOVs and Incompatible
Annotations
In this section we show the surprisingly pervasive
nature of pseudo OOVs and incompatible annota-
tions through analysis of five segmented corpora,
which are CTB6 and corpus by MSR, PKU, AS
and CITYU provided in SIGHAN word segmentation
Bakeoffs 2.
First we use the standard split of training and test-
ing data and extract all OOVs for each corpus, then
count the number of pseudo OOVs. Table 1 gives the
result. It?s amazing that for every corpus, over 60%
of OOVs are pseudo, meaning they can be avoided if
their internal structures were parsed. Reduction of
OOVs at such a large scale can benefit greatly down-
stream natural language processing systems.
We then sample 200 word types containing a pro-
ductive affix from each corpus, and check whether
the affix also occurs somewhere else in a phrase,
i.e, the affix is annotated as a word in the phrase.
The results are in Table 2. It?s clear and somewhat
shocking that most affixes are annotated incompat-
ibly. We believe it is not the annotators to blame,
rather the root cause lies deeply in the unique char-
acteristics of the Chinese language. This becomes
obvious in comparison with English, where suffix
like ?-ism? in ?capitalism? cannot be used alone as a
word in phrases. 3 Incompatible annotations can
be removed only through unified parsing of word
and phrase structures, as mentioned earlier and il-
lustrated in Figure 1.
2http://www.sighan.org/bakeoff2005/
3Actually English allows examples like ?pre- and post-war
imperialism? where a prefix like ?pre? can appear on its own as
long as the hyphen is present and it is in a coordination struc-
ture. Note that such examples are much rarer than what we
discuss in this paper for Chinese. We thank the reviewer very
much for pointing this out and providing this example for us.
1446
corpus incompatible percent
CTB6 190 95
MSR 178 89
PKU 192 96
AS 182 91
CITYU 194 97
Table 2: Statistics of incompatibly annotated affixes in
200 sampled words for five segmented corpora.
?? NR ? NN ?? VV ?? NN ? NN
MOD MOD
OBJ
SUBJ
Figure 2: Example output of unified dependency parsing
of Chinese morphological and syntactic structures.
3 Unified Parsing Defined
Given an unsegmented sentence ???????
? ?Gansu province attaches great importance to in-
surance industry?, the output of unified dependency
parser is shown in Figure 2. As can be seen, this
output contains information about word (such as?
?_VV) as well as phrase structures (such as ?
?_VV ??_NN ?_NN), which is what we mean
by ?unified? parsing. Now, it?s no longer vital to dif-
ferentiate between morphology and syntax for Chi-
nese. People could regard??? ?insurance indus-
try? as a word or phrase, but either way, there will be
no disagreements about its internal structure. From
the perspective of the unified parser, linguistic units
are given the same labels as long as they function
similarly (e.g, they have the same parts-of-speech).
As a bonus, output of unified parsing incorpo-
rates Chinese word segmentation, part-of-speech
tagging and dependency parsing. To achieve these
goals, previous systems usually used a pipelined
approach by combining several statistical models,
which was further complicated by different decod-
ing algorithms for each of these models. The present
paper shows that a single model does all these jobs.
Besides being much simpler in engineering such a
parser, this approach is also a lot more plausible for
modeling human language understanding.
4 Annotation of Word Structures
Unified parsing requires a corpus annotated with
both morphological and syntactic structures. Such
a corpus can be built with the least effort if we be-
gin with an existing treebank such as CTB6 already
annotated with syntactic structures. It only remains
for us to annotate internal structures of words in this
treebank.
4.1 Scope of Annotation
In order to get rid of pseudo OOVs and incompati-
ble annotations, internal structures are annotated for
two kinds of words. The first kind contains words
with a productive component such as suffix or pre-
fix. One example is??? ?speaker? whose suffix
is the very productive? ?person? (e.g, in CTB6 there
are about 400 words having this suffix). The second
kind includes words with compositional semantics.
Examples are ??? ?Monday? and ??? ?Sun-
day?. Though ?? ?week? is not very productive,
the meaning of words with this prefix is deducible
from semantics of their components.
Other compound words such as ?? ?research?
have no productive components and are not a cause
of pseudo OOVs. They are universally consid-
ered as words instead of phrases due to their non-
compositional semantics. Hence their structures are
not annotated in the present research. Meanwhile,
for single-morpheme words with no structures what-
soever, like??? ?Iraq? and?? ?bat?, annotation
of internal structures is of course unnecessary either.
Of all the 54, 214 word types in CTB6, 35% are
annotated, while the percentage is 24% for the 782,
901 word tokens. Around 80% of sentences contain
words whose structures need annotation. Our anno-
tations will be made publicly available for research
purposes.
4.2 From Part-of-speeches to Constituents
Of all 33 part-of-speech tags in CTB, annotation of
word structures is needed for nine tags: NN, VV, JJ,
CD, NT, NR, AD, VA and OD. Since part-of-speech
tags are preterminals and can only have one terminal
word as its child, POS tags of words become con-
stituent labels after annotation of word structures.
The mapping rules from POS tags to constituent la-
bels are listed in Table 3. Readers should note that
1447
POS tags constituent label
NR, NN, NT NP
JJ ADJP
AD ADVP
CD, OD QP
VV, VA VP
Table 3: Correspondence between POS tags and con-
stituent labels after annotation.
PP
????
P
?
NP
NN
???
? PP????
P
?
NP
????
NN
??
NN
?
Figure 3: Example annotation for the word ??? NN
in CTB6: POS tag NN changes to constituent label NP
after annotation.
such mapping is not arbitrary. The constraint is that
in the treebank the POS tag must somewhere be the
unique child of the constituent label. Figure 3 de-
picts an example annotation, in which we also have
an example of NP having a tag NN as its only child.
4.3 Recursive Annotation
Some words in CTB have very complex structures.
Examples include ??????? ?physicist ma-
joring in nuclear physics?, ????? ?anti-trust
laws? etc. Structures of these words are anno-
tated to their full possible depth. Existence of such
words are characteristic of the Chinese language,
since they are further demonstrations of the blurred
boundary between morphology and syntax. A full-
fledged parser is needed to analyze structures of
these words, which incidentally provides us with an-
other motivation for unified morphological and syn-
tactic parsing of Chinese.
5 Unified Dependency Parsing
All previous dependency parsers for Chinese take it
for granted that the input sentence is already seg-
mented into words (Li et al 2011). Most systems
even require words to be tagged with their part-of-
speeches (Zhang and Nivre, 2011). Hence current
off-the-shelf algorithms are inadequate for parsing
unsegmented sentences. Instead, a new unified pars-
ing algorithm is given in this section.
5.1 Transitions
To map a raw sentence directly to output shown in
Figure 2, we define four transitions for the unified
dependency parser. They act on a stack containing
the incremental parsing results, and a queue holding
the incoming Chinese characters of the sentence:
SHIFT: the first character in the queue is shifted into
the stack as the start of a new word. The queue
should not be empty.
LEFT: the top two words of the stack are connected
with an arc, with the top one being the head. There
should be at least two elements on the stack.
RIGHT: the top two words of the stack are con-
nected, but with the top word being the child. The
precondition is the same as that of LEFT.
APPEND: the first character in the queue is appended
to the word at the top of the stack. There are two
preconditions. First, the queue should not be empty.
Second, the top of the stack must be a word with no
arcs connected to other words (i.e, up to now it has
got neither children nor parent).
We see that these transitions mimic the general arc-
standard dependency parsing models. The first three
of them were used, for example, by Yamada and
Matsumoto (2003) to parse English sentences. The
only novel addition is APPEND, which is necessary
because we are dealing with raw sentences. Its sole
purpose is to assemble characters into words with
no internal structures, such as ??? ?Seattle?.
Thus this transition is the key for removing the need
of Chinese word segmentation and parsing unseg-
mented sentences directly.
To also output part-of-speech tags and depen-
dency labels, the transitions above can be aug-
mented accordingly. Hence we can change SHIFT to
SHIFT?X where X represents a certain POS tag. Also,
LEFT and RIGHT should be augmented with appro-
priate dependency relations, such as LEFT?SUBJ for
a dependency between verb and subject.
As a demonstration of the usage of these tran-
sitions, consider sentence ?????? ?I love
Seattle?. Table 4 lists all steps of the parsing pro-
cess. Readers interested in implementing their own
1448
step stack queue action
1 ?????? SHIFT?PN
2 ? PN ????? SHIFT?VV
3 ? PN? VV ???? APPEND
4 ? PN?? VV ??? LEFT?SUBJ
5 ? PN SUBJ?????? VV ??? SHIFT?NR
6 ? PN SUBJ?????? VV? NR ?? APPEND
7 ? PN SUBJ?????? VV?? NR ? APPEND
8 ? PN SUBJ?????? VV??? NR RIGHT?OBJ
9 ? PN SUBJ?????? VV OBJ?????? NR STOP
Table 4: Parsing process of a short sentence with the four transitions defined above.
unified dependency parsers are invited to study this
example carefully.
5.2 Model
Due to structural ambiguity, there might be quite a
lot of possibilities for parsing a given raw sentence.
Hence at each step in the parsing process, all four
transitions defined above may be applicable. To re-
solve ambiguities, each candidate parse is scored
with a global linear model defined as follows.
For an input sentence x, the parsing result F (x) is
the one with the highest score in all possible struc-
tures for this x:
F (x) = argmax
y?GEN(x)
Score(y) (1)
Here GEN(x) is a set of all possible parses for sen-
tence x, and Score(y) is a real-valued linear func-
tion:
Score(y) = ?(y) ? ~w (2)
where ?(y) is a global feature vector extracted from
parsing result y, and ~w is a vector of weighting pa-
rameters. Because of its linearity, Score(y) can be
computed incrementally, following the transition of
each parsing step. Parameter vector ~w is trained
with the generalized perceptron algorithm of Collins
(2002). The early-update strategy of Collins and
Roark (2004) is used so as to improve accuracy and
speed up the training.
5.3 Feature Templates
For a particular parse y, we now describe the way
of computing its feature vector ?(y) in the linear
Description Feature Templates
1 top of S S0wt; S0w; S0t
2 next top of S S1wt; S1w; S1t
3 S0 and S1 S1wtS0wt; S1wtS0w
S1wS0wt; S1wtS0t
S1tS0wt; S1wS0w; S1tS0t
4 char unigrams Q0; Q1; Q2; Q3
5 char bigrams Q0Q1; Q1Q2; Q2Q3
6 char trigrams Q0Q1Q2; Q1Q2Q3
7 ST+unigrams STwtQ0; STwQ0; STtQ0
8 ST+bigrams STwtQ0Q1; STwQ0Q1
STtQ0Q1
9 ST+trigrams STwtQ0Q1Q2
STwQ0Q1Q2; STtQ0Q1Q2
10 parent P of ST PtSTtQ0; PtSTtQ0Q1
PtSTtQ0Q1Q2
11 leftmost child STtLCtQ0; STtLCtQ0Q1
LC and STtLCtQ0Q1Q2
rightmost STtRCtQ0; STtRCtQ0Q1
child RC STtRCtQ0Q1Q2
Table 5: Transition-based feature templates. Q0 is the
first character in Q, etc. w = word, t = POS tag.
model of Equation (2). If S denotes the stack hold-
ing the partial results, and Q the queue storing the
incoming Chinese characters of a raw sentence, then
transition-based parsing features are extracted from
S and Q according to those feature templates in Ta-
ble 5.
Although we employ transition-based parsing,
nothing prevents us from using graph-based fea-
tures. As shown by Zhang and Clark (2011), depen-
1449
Description Feature Templates
1 parent word Pwt; Pw; Pt
2 child word Cwt; Cw; Ct
3 P and C PwtCwt; PwtCw; PwCwt
PtCwt; PwCw; PtCt
PwtCt
4 neighbor word PtPLtCtCLt; PtPLtCtCRt
of P and C PtPRtCtCLt; PtPRtCtCRt
left (L) or PtPLtCLt; PtPLtCRt
right (R) PtPRtCLt; PtPRtCRt
PLtCtCLt; PLtCtCRt
PRtCtCLt; PRtCtCRt
PtCtCLt; PtCtCRt
PtPLtCt; PtPRtCt
5 sibling(S) of C CwSw;CtSt; CwSt
CtSw; PtCtSt
6 leftmost and PtCtCLCt
rightmost child PtCtCRCt
7 left (la) and Ptla; Ptra
right (ra) Pwtla; Pwtra
arity of P Pwla; Pwra
Table 6: Graph-based feature templates for the unified
parser. Most of these templates are adapted from those
used by Zhang and Clark (2011). w = word; t = POS tag.
dency parsers using both transition-based and graph-
based features tend to achieve higher accuracy than
parsers which only make use of one kind of features.
Table 6 gives the graph-based feature templates used
in our parser. All such templates are instantiated at
the earliest possible time, in order to reduce as much
as possible situations where correct parses fall out of
the beam during decoding.
5.4 Decoding Algorithm
We use beam-search to find the best parse for a given
raw sentence (Algorithm 1). This algorithm uses
double beams. The first beam contains unfinished
parsing results, while the second holds completed
parses. Double beams are necessary because the
number of transitions might well be different for dif-
ferent parses, and those parses that finished earlier
are not necessarily better parses. During the search-
ing process, correct parse could fall off the beams,
resulting in a search error. However, in practice
beam search decoding algorithm works quite well.
In addition, it?s not feasible to use dynamic program-
ming because of the complicated features used in the
model.
The B in Algorithm 1 is the width of the two
beams. In our experiments we set B to 64. This
value of B was determined empirically by using the
standard development set of the data, with the goal
of achieving the highest possible accuracy within
reasonable time. Note that in line 20 of the algo-
rithm, the beam for completed parsers are pruned at
each iteration of the parsing process. The purpose
of this action is to keep this beam from growing too
big, resulting in a waste of memory space.
Algorithm 1 Beam Search Decoding
1: candidates? {STARTITEM()}
2: agenda? ?
3: completed? ?
4: loop
5: for all candidate in candidates do
6: for all legal action of candidate do
7: newc? EXPAND(candidate, action)
8: if COMPLETED(newc) then
9: completed.INSERT(newc)
10: else
11: agenda.INSERT(newc)
12: end if
13: end for
14: end for
15: if EMPTY(agenda) then
16: return TOP(completed)
17: end if
18: candidates? TOPB(agenda, B)
19: agenda? ?
20: completed? TOPB(completed, B)
21: end loop
6 Experiments and Evaluation
We describe the experiments carried out and our
method of evaluation of the unified dependency
parser. We used Penn2Malt 4 to convert constituent
trees of CTB to dependency relations. The head rules
for this conversion was given by Zhang and Clark
(2008). In all experiments, we followed the stan-
4http://w3.msi.vxu.se/
?
nivre/research/
Penn2Malt.html
1450
P R F
our method, labeled 78.54 80.93 79.72
our method, unlabeled 81.01 83.77 82.37
ZC2011, unlabeled N/A N/A 75.09
Table 7: Evaluation results on the original CTB5. N/A
means the value is not available to us. ZC2011 is Zhang
and Clark (2011).
dard split of the data into training, testing and devel-
opment data (Zhang and Clark, 2011). Though we
annotated structures of words in CTB6, most previ-
ously results were on CTB5, a subset of the former
treebank. Hence we report our results of evaluation
on CTB5 for better comparability.
6.1 Dependency Parsing of Morphological and
Syntactic Structures
If we look back at the Figure 2, it?s clear that a
dependency relation is correctly parsed if and only
if three conditions are met: Firstly, words at both
ends of the dependency are correctly segmented.
Secondly, part-of-speech tags are correct for both
words. Thirdly, the direction of the dependency re-
lation are correct. Of course, if labeled precision and
recall is to be measured, the label of the dependency
relation should also be correctly recovered. Let nc
be the number of dependencies correctly parsed with
respect to these criterion, no be the total number of
dependencies in the output, and nr the number of
dependencies in the reference. Then precision is de-
fined to be p = nc/no and recall is defined to be
r = nc/nr.
6.1.1 Results on the Original CTB5
We first train our unified dependency parser with
the original treebank CTB5. In this case, all words
are considered to be flat, with no internal structures.
The result are shown in Table 7. Note that on ex-
actly the same testing data, i.e, the original CTB5,
unified parser performs much better than the result
of a pipelined approach reported by Zhang and Clark
(2011). There are about 30% of relative error reduc-
tion for the unlabeled dependency parsing results.
This is yet another evidence of the advantage of joint
modeling in natural language processing, details of
which will be discussed in Section 7.
P R F
original dependencies 82.13 84.49 83.29
in CTB5
ZN2011 with Gold N/A N/A 84.40
segmentation & POS
original dependencies 85.71 87.18 86.44
plus word structures
Table 8: Evaluation results on CTB5 with word structures
annotated. All results are labeled scores.
6.1.2 Results on CTB with Structures of Words
Annotated
Then we train the parser with CTB5 augmented
with our annotations of internal structures of words.
For purpose of better comparability, we report re-
sults on both the original dependencies of CTB5 and
on the dependencies of CTB5 plus those of the in-
ternal structures of words. The results are shown
in Table 8. First, note that compared to another re-
sult by Zhang and Nivre (2011), whose input were
sentences with gold standard word segmentation and
POS tags, our F-score is only slightly lower even
with input of unsegmented sentences. This is un-
derstandable since gold-standard segmentation and
POS tags greatly reduced the uncertainty of parsing
results.
For the unified parser, the improvement of F-score
from 79.72% to 83.29% is attributed to the fact that
with internal structures of words annotated, parsing
of syntactic structures is also improved due to the
similarity of word and phrase structures mentioned
in Section 1, and also due to the fact that many
phrase level dependencies are now facing a much
less severe problem of data sparsity. The improve-
ment of F-score from 83.29% to 86.44% is attributed
to the annotation of word structures. Internal struc-
tures of words are be mostly local in comparison
with phrase and sentence structures. Therefore, with
the addition of word structures, the overall depen-
dency parsing accuracy naturally can be improved.
6.2 Chinese Word Segmentation
From the example in Figure 2, it is clear that output
of unified parser contains Chinese word segmenta-
tion information. Therefore, we can get results of
word segmentation for each sentence in the test sets,
1451
P R F
K2009 N/A N/A 97.87
This Paper 97.63 97.38 97.50
Table 9: Word segmentation results of our parser and
the best performance reported in literature on the same
dataset. K2009 is the result of Kruengkrai et al(2009).
P R F
K2009 N/A N/A 93.67
ZC2011 N/A N/A 93.67
This Paper 93.42 93.20 93.31
Table 10: Joint word segmentation and POS tagging
scores. K2009 is result of Kruengkrai et al(2009).
ZC2011 is result of Zhang and Clark (2011).
and evaluate their accuracies. For maximal compa-
rability, we train the unified parser on the original
CTB5 data used by previous studies. The result is
in Table 9. Despite the fact that the performance
of our unified parser does not exceed the best re-
ported result so far, which probably might be caused
by some minute implementation specific details, it?s
fair to say that our parser performs at the level of
state-of-the-art in Chinese word segmentation.
6.3 Joint Word Segmentation and POS Tagging
From Figure 2 we see that besides word segmenta-
tion, output of the unified parser also includes part-
of-speech tags. Therefore, it?s natural that we evalu-
ate the accuracy of joint Chinese word segmentation
and part of speech tagging, as reported in previous
literature (Kruengkrai et al 2009). The results are
in Table 10, in which for ease of comparison, again
we train the unified parser with the vanilla version
of CTB5. We can see that unified parser performs at
virtually the same level of accuracy compared with
previous best systems.
7 Related Work
Researchers have noticed the necessity of parsing
the internal structures of words in Chinese. Li
(2011) gave an method that could take raw sentences
as input and output phrase structures and internal
structures of words. This paper assumes that the in-
put are unsegmented, too, and our output also in-
cludes both word and phrase structures. There are
? ? ? ? ? ? ? ?
Figure 4: Example output of Zhao?s parser.
two key differences, though. The first is we output
dependency relations instead of constituent struc-
tures. Although dependencies can be extracted from
the constituent trees of Li (2011), the time complex-
ity of their algorithm is O(n5) while our parser runs
in linear time. Secondly, we specify the details of
annotating structures of words, with the annotations
being made publicly available.
Zhao (2009) presented a dependency parser which
regards each Chinese character as a word and then
analyzes the dependency relations between charac-
ters, using ordinary dependency parsing algorithms.
Our parser is different in two important ways. The
first is we output both part-of-speech tags and la-
beled dependency relations, both of which were ab-
sent in Zhao?s parser. More importantly, the AP-
PEND transition for handling flat words were unseen
in previous studies as far as we know. The difference
can best be described with an example: For the sen-
tence in Section 3, Zhao?s parser output the result in
Figure 4 while in contrast our output is Figure 2.
In recent years, considerable efforts have been
made in joint modeling and learning in natural lan-
guage processing (Lee et al 2011; Sun, 2011; Li et
al., 2011; Finkel and Manning, 2009; Kruengkrai et
al., 2009; Jiang et al 2008; Goldberg and Tsarfaty,
2008). Joint modeling can improve the performance
of NLP systems due to the obvious reason of being
able to make use of various levels of information si-
multaneously. However, the thesis of this paper, i.e,
unified parsing of Chinese word and phrase struc-
tures, bears a deeper meaning. As demonstrated in
Section 1 and by Li (2011), structures of words and
phrases usually have significant similarity, and the
distinction between them is very difficult to define,
even for expert linguists. But for real world applica-
tions, such subtle matters can safely be ignored if we
could analyzed morphological and syntactic struc-
tures in a unified framework. What applications re-
ally cares is structures instead of whether a linguistic
unit is a word or phrase.
1452
Another notable line of research closely related
to the present work is to annotate and parse the flat
structures of noun phrases (NP) (Vadas and Curran,
2007; Vadas and Curran, 2011). This paper dif-
fers from those previous work on parsing NPs in at
least two significant ways. First, we aim to parse
all kinds of words (e.g, nouns, verbs, adverbs, ad-
jectives etc) whose structures are not annotated by
CTB, and whose presence could cause lots of pseudo
OOVs and incompatible annotations. Second, the
problem we are trying to solve is a crucial obser-
vation specific to Chinese language, that is, in lots
of cases forcing a separation of words and phrases
leads to awkward situations for NLP systems. Re-
member that in Section 2 we demonstrated that all
corpora we examined had the problem of pseudo
OOVs and incompatible annotations. In comparison,
the problem Vadas and Curran (2007) tried to solve
is a lack of annotation for structures of NPs in cur-
rently available treebanks, or to put it in another way,
a problem more closely related to treebanks rather
than certain languages.
8 Discussion and Conclusion
Chinese word segmentation is an indispensable step
for traditional approaches to syntactic parsing of
Chinese. The purpose of word segmentation is to
decide what goes to words, with the remaining pro-
cessing (e.g, parsing) left to higher level structures
of phrases and sentences. This paper shows that it
could be very difficult to make such a distinction
between words and phrases. This difficulty cannot
be left unheeded, as we have shown quantitatively
that in practice it causes lots of real troubles such as
too many OOVs and incompatible annotations. We
showed how these undesirable consequences can be
resolved by annotation of the internal structures of
words, and by unified parsing of morphological and
syntactic structures in Chinese.
Unified parsing of morphological and syntactic
structures of Chinese can also be implemented with
a pipelined approach, in which we first segment in-
put sentences into words or affixes (i.e, with the
finest possible granularity), and then we do part-
of-speech tagging followed by dependency (or con-
stituent) parsing. However, a unified parsing ap-
proach using a single model as presented in this
paper offers several advantages over pipelined ap-
proaches. The first one is that joint modeling tends
to result in higher accuracy and suffer less from er-
ror propagation than do pipelined methods. Sec-
ondly, both the unified model and the algorithm
are conceptually much more simpler than pipelined
approaches. We only need one implementation of
the model and algorithm, instead of several ones in
pipelined approaches. Thirdly, our model and al-
gorithm might comes closer to modeling the pro-
cess of human language understanding, because hu-
man brain is more likely a parallel machine in un-
derstanding languages than an alternative pipelined
processor. Hence this work, together with previ-
ous studies by other authors like Li (2011) and Zhao
(2009), open up a possibly new direction for future
research efforts in parsing the Chinese language.
Acknowledgments
Reviewers of this paper offered many detailed and
highly valuable suggestions for improvement in pre-
sentation. The authors are supported by NSFC under
Grant No. 90920004 and National 863 Program un-
der Grant No. 2012AA011102.
References
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages 111?
118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1?8. Association for
Computational Linguistics, July.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
326?334, Boulder, Colorado, June. Association for
Computational Linguistics.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL-08: HLT,
pages 371?379, Columbus, Ohio, June. Association
for Computational Linguistics.
1453
C. F. Hockett. 1969. A Course in Modern Linguistics.
Macmillan.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint Chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL-08: HLT, pages 897?904, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 513?521, Suntec, Singapore,
August. Association for Computational Linguistics.
John Lee, Jason Naradowsky, and David A. Smith. 2011.
A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 885?894, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
Chinese POS tagging and dependency parsing. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1180?
1191, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for Chinese word segmenta-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1405?1414, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1385?1394, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 240?247, Prague,
Czech Republic, June. Association for Computational
Linguistics.
David Vadas and James R. Curran. 2011. Parsing noun
phrases in the penn treebank. Computational Linguis-
tics, 37(4):753?809.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal dependency analysis with support vector machines.
In Proceeding of the 8th International Workshop of
Parsing Technologies (IWPT), pages 195?206, Nancy,
France.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 562?571, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2011. Syntactic process-
ing using the generalized perceptron and beam search.
Computational Linguistics, 37(1):105?151.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Hai Zhao. 2009. Character-level dependencies in Chi-
nese: Usefulness and learning. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), pages 879?887, Athens, Greece, March.
Association for Computational Linguistics.
1454
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 715?725,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Collective Personal Profile Summarization with Social Networks 
 
 
Zhongqing Wang, Shoushan Li*, Kong Fang, and Guodong Zhou 
Natural Language Processing Lab, School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China  
{wangzq.antony, shoushan.li}@gmail.com,  
{kongfang, gdzhou}@suda.edu.cn 
 
  
 
Abstract 
Personal profile information on social media 
like LinkedIn.com and Facebook.com is at the 
core of many interesting applications, such as 
talent recommendation and contextual advertis-
ing. However, personal profiles usually lack or-
ganization confronted with the large amount of 
available information. Therefore, it is always a 
challenge for people to find desired information 
from them. In this paper, we address the task of 
personal profile summarization by leveraging 
both personal profile textual information and so-
cial networks. Here, using social networks is 
motivated by the intuition that, people with 
similar academic, business or social connections 
(e.g. co-major, co-university, and co-
corporation) tend to have similar experience and 
summaries. To achieve the learning process, we 
propose a collective factor graph (CoFG) model 
to incorporate all these resources of knowledge 
to summarize personal profiles with local textual 
attribute functions and social connection factors. 
Extensive evaluation on a large-scale dataset 
from LinkedIn.com demonstrates the effective-
ness of the proposed approach.* 
1 Introduction 
Web 2.0 has empowered people to actively interact 
with each other, forming social networks around 
mutually interesting information and publishing a 
large amount of useful user-generated content 
(UGC) online (Lappas et al, 2011; Tan et al, 
2011). One popular and important type of UGC is 
the personal profile, where people post detailed 
                                                 
* Corresponding author 
information on online portals about their education, 
experiences and other personal information. Social 
websites like Facebook.com and LinkedIn.com 
have created a viable business as profile portals, 
with the popularity and success partially attributed 
to their comprehensive personal profiles. 
Generally, online personal profiles provide val-
uable resources for businesses, especially for hu-
man resource managers to find talents, and help 
people connect with others of similar backgrounds 
(Yang et al, 2011a; Guy et al, 2010). However, as 
there is always large-scale information of experi-
ence and education fields, it is hardly for us to find 
useful information from the profile. Therefore, it is 
always a challenge for people to find desired in-
formation from them. For this regard, it is highly 
desirable to develop reliable methods to generate a 
summary of a person through his profile automati-
cally.  
To the best of our knowledge, this is the first re-
search that explores automatic summarization of 
personal profiles in social media. A straightfor-
ward approach is to consider personal profile 
summarization as a traditional document summari-
zation problem, which treating each personal pro-
file independently and generate a summary for 
each personal profile individually. For example, 
the well-known extraction and ranking approaches 
(e.g. PageRank, HITS) extract a certain amount of 
important sentences from a document according to 
some ranking measurements to form a summary 
(Wan and Yang, 2008; Wan, 2011).  
However, such straightforward approaches are 
not sufficient to benefit from the carrier of person-
al profiles. As the centroid of social networking, 
people are usually connected to others with similar 
715
background in social media (e.g. co-major, co-
corporation). Therefore, it is reasonable to lever-
age social connection to improve the performance 
of profile summarizing. For example if there are 
co-major, co-university, co-corporation or other 
academic and business relationships between two 
persons, we consider them sharing similar experi-
ence and having similar summaries. 
The remaining challenge is how to incorporate 
both the profile textual information and the con-
nection knowledge in the social networks. In this 
study, we propose a collective factor graph model 
(CoFG) to summarize the text of personal profile 
in social networks with local textual information 
and social connection information. The CoFG 
framework utilizes both the local textual attribute 
functions of an individual person and the social 
connection factor between different persons to col-
lectively summarize personal profile on one person. 
In this study, we treat the profile summarization 
as a supervised learning task. Specifically, we 
model each sentence of the profile as a vector. In 
the training phase, we use the vectors with the so-
cial connection between each person to build the 
CoFG model; while in the testing phase, we per-
form collective inference for the importance of 
each sentence and select a subset of sentences as 
the summary according to the trained model. Eval-
uation on a large-scale data from LinkedIn.com 
indicates that our proposed joint model and social 
connection information improve the performance 
of profile summarization. 
The remainder of our paper is structured as fol-
lows. We go over the related work in Section 2. In 
Section 3, we introduce the data we collected from 
LinkedIn.com and the annotated corpus we con-
structed. In Section 4, we present some motiva-
tional analysis. In Section 5, we explain our pro-
posed model and describe algorithms for parame-
ter estimation and prediction. In Section 6, we pre-
sent our experimental results. We sum up our work 
and discuss future directions in Section 7. 
2 Related Work 
In this section, we will introduce the related work 
on the traditional topic-based summarization, so-
cial-based summarization and factor graph model 
respectively. 
2.1 Topic-based Summarization 
Generally, traditional topic-based summarization 
can be categorized into two categories: extractive 
(Radev et al, 2004) and abstractive (Radev and 
McKeown, 1998) summarization. The former se-
lects a subset of sentences from original docu-
ment(s) to form a summary; the latter reorganizes 
some sentences to form a summary where several 
complex technologies, such as information fusion, 
sentence compression and reformulation are nec-
essarily employed (Wan and Yang, 2008; Celiky-
ilmaz and Hakkani-Tur, 2011; Wang and Zhou, 
2012). This study focuses on extractive summari-
zation.  
Radev et al (2004) proposed a centroid-based 
method to rank the sentences in a document set, 
using various kinds of features, such as the cluster 
centroid, position and TF-IDF features. Ryang and 
Abekawa (2012) proposed a reinforcement learn-
ing approach on text summarization, which models 
the summarization within a reinforcement learn-
ing-based framework.  
Compared to unsupervised approaches, super-
vised learning for summarization is relatively rare. 
A typical work is Shen et al, (2007) which present 
a Conditional Random Fields (CRF) based frame-
work to treat the summarization task as a sequence 
labeling problem. However, different from all ex-
isting studies, our work is the first attempt to con-
sider both textual information and social relation-
ship information for supervised summarization. 
2.2 Social-based Summarization 
As web 2.0 has empowered people to actively in-
teract with each other, studies focusing on social 
media have attracted much attention recently 
(Meeder et al, 2011; Rosenthal and McKeown, 
2011; Yang et al, 2011a). Social-based summari-
zation is exactly a special case of summarization 
where the social connection is employed to help 
obtaining the summarization. Although topic-
based summarization has been extensively studied, 
studies on social-based summarization are relative 
new and rare.  
Hu et al, (2011) proposed an unsupervised Pag-
eRank-based social summarization approach by 
incorporating both document context and user con-
text in the sentence evaluation process. Meng et al, 
(2012) proposed a unified optimization framework 
to produce opinion summaries of tweets through 
716
integrating information from dimensions of topic, 
opinion and insight, as well as other factors (e.g. 
topic relevancy, redundancy and language styles). 
Unlike all the above studies, this paper focuses 
on a novel task, profile summarization. Further-
more, we employ many other kinds of social in-
formation in profiles, such as co-major, and co-
corporation between two people. They are shown 
to be very effective for profile summarization.  
2.3 Factor Graph Model 
As social network has been investigated for sever-
al years (Leskovec et al, 2010; Tan et al, 2011; 
Lu et al, 2010; Guy et al, 2010) and Factor Graph 
Model (FGM) is a popular approach to describe 
the relationship of social network (Tang et al, 
2011a; Zhuang et al, 2012). Factor Graph Model 
builds a graph to represent the relationship of 
nodes on the social networks, and the factor func-
tions are always considered to represent the rela-
tionship of the nodes. 
Tang et al (2011a) and Zhuang et al (2012) 
formalized the problem of social relationship 
learning into a semi-supervised framework, and 
proposed Partially-labeled Pairwise Factor Graph 
Model (PLP-FGM) for learning to infer the type of 
social ties. Dong et al (2012) gave a formal defini-
tion of link recommendation across heterogeneous 
networks, and proposed a ranking factor graph 
model (RFG) for predicting links in social net-
works, which effectively improves the predictive 
performance. Yang et al, (2011b) generated sum-
maries by modeling tweets and social contexts into 
a dual wing factor graph (DWFG), which utilized 
the mutual reinforcement between Web documents 
and their associated social contexts.  
Different from all above researches, this paper 
proposes a pair-wise factor graph model to collec-
tively utilize both textual information and social 
connection factor to generate summary of profile. 
3 Data Collection and Statistics   
The personal profile summarization is a novel task 
and there exists no related data for accessing this 
issue. Therefore, in this study, we collect a data set 
containing personal summaries with the corre-
sponding knowledge, such as the self-introduction 
and personal profiles. In this section, we will in-
troduce this data set in detail. 
3.1 Data Collection  
We collect our data set from LinkedIn.com1 . It 
contains a large number of personal profiles gen-
erated by users, containing various kinds of infor-
mation, such as personal overview, summary, edu-
cation, experience, projects and skills.  
 
John Smith2  
Overview 
Current Applied Researcher at Apple Inc. 
Previous 
Senior Research Scientist at IBM 
? 
Education 
MIT, 
Georgia Institute of Technology,   
? 
Summary 
Machine learning researcher and engineer on 
many fields: 
Query understanding. Automatic Information 
extraction? 
Experience 
Applied Researcher 
Apple Inc., September 2012 ~  
Query recognition and relevance 
? 
Education 
MIT 
Ph.D., Electrical Engineering, 2002 ? 2008 
? 
Figure 1: An example of a profile webpage from 
LinkedIn.com 
 
In this study, the data set is crawled in the fol-
lowing ways. To begin with, 10 random people?s 
public profiles are selected as seed profiles, and 
then the profiles from their ?People Also Viewed? 
field were collected. The data is composed of 
3,182 public profiles3 in total. We do not collect 
personal names in public profiles to protect peo-
ple?s privacy. Figure 1 shows an example of a per-
son?s profile from LinkedIn.com. The profile in-
cludes following fields: 
? Overview: It gives a structure description of a 
person?s general information, such as cur-
rent/previous position and workplace, brief 
                                                 
1 http://www.linkedin.com 
2 The information of the example is a pseudo one. 
3 We collect all the data from LinkedIn.com at Dec 17, 
2012.  
717
education background and general technical 
background.  
? Summary: It summarizes a person?s work, 
experience and education.  
? Experience: It details a person?s work experi-
ence.  
? Education: It details a person?s education 
background.  
Among these fields, the Overview is required 
and the others are optional, such as Project, 
Course and Interest groups. However, compared 
with Overview, Summary, Experience, Education 
fields, they seem to be less important for summari-
zation of personal profiles. Thus, we ignore them 
in our study. 
3.2 Data Statistics of Major Fields 
We collected 3,182 personal profiles from 
LinkedIn.com. Table 1 shows the statistics of ma-
jor fields in our data collection. 
 
Field 
#Non-empty 
fields 
Average 
field 
length 
Overview 3,182 45.1 
Summary 921 25.8 
Experience 3,148 192.1 
Education 2,932 33.6 
Table 1: Statistics of major fields in our data set, i.e. the 
number of non-empty fields and the average length for 
each field 
 
From Table 1, we can see that, 
? The information of each profile is incom-
plete and inconsistent, That is, not all kinds 
of fields are available in each personal?s 
profile.  
? Most people provide their experience and 
education information. However, the Sum-
mary fields are popularly missing (Only 
about 30% of people provide it). This is 
mainly because writing summary is nor-
mally more difficult than other fields. 
Therefore, it is highly desirable to develop 
reliable automatic methods to generate a 
summary of a person through his/her pro-
file. 
? The length of the Experience field is the 
longest one, and work experience always 
could represent general information of 
people.  
3.3 Corpus Construction and Annotation  
Among the 921 profiles that contain the summary, 
we manually select 497 profiles with high quality 
summary to construct the corpus for our research. 
These high-quality summaries are all written by 
the authors themselves. Here, the quality is meas-
ured by manually checking that whether they are 
well capable of summarizing their profiles. That is, 
they are written carefully, and could give an over-
view of a person and represent the education and 
experience information of a person. 
After carefully seeing the profiles, we observe 
that the Experience field contains the most abun-
dant information of a person. Thus, we treat the 
text of Experience field as the source of summary 
for each profile. Besides, we collect social context 
information from Education and Experience field, 
and these social contexts are including by 
LinkedIn explicitly. Table 2 shows the average 
length of summary and experience fields we used 
for evaluating our summarization approach.  
 
Field 
Average 
length 
Summary 
(the summary of the 
profile) 
37.2 
Experience 
(the source text for the 
summarizing) 
372.0 
Table 2: Average length of the high-quality summary  
and corresponding experience fields 
 
From Table 2, we can see that,  
? Compared with the average length of 25.8 
in Table 1, summaries of high quality have 
longer length because they contain more in-
formation of the profiles.  
? The compression ratio of our proposed cor-
pus is 0.1 (37.2/372.0).  
4 Motivation and Analysis 
In this section, we propose the motivation of social 
connection to address the task of personal profile 
summarization. To preliminarily support the moti-
vation, some statistics of the social connection are 
provided. 
718
 Figure 2: An example of personal profile network.  
Red is for female, blue is for male, and the dotted line 
means the social connection between two persons. 
 
We first describe the social connections which 
we used. Figure 2 shows an example of social 
connection between people from the profiles of 
LinkedIn. We find that people are sometimes con-
nected by several social connections. For example, 
John and Lucy are connected by co_unvi relation-
ship, while Lily and Linda are connected by 
co_corp relationship. From LinkedIn, four kinds of 
social relationship between people are extracted 
from the Education field and Experience field. 
They are: 
? co_major denotes that two persons have the 
same major at school 
? co_univ denotes that two persons are graduat-
ed from the same university 
? co_title denotes that two persons have the 
same title at corporation. 
? co_corp denotes that two persons work at the 
same corporation. 
Our basic motivation of using social connection 
lies in the fact that ?connected? people will tend to 
hold related experience and similar summaries.  
We then give the statistics of edges of social 
connection. Table 3 shows basic statistics across 
these edges. From Table 3, we can see that the 
number of users is 497 while the number of social 
connection edges is 14,307. The latter is much 
larger than the former. The number of the edges 
from Education field is similar with the number of 
the edges from Experience filed. Among all the 
relationships, co_unvi is the most common one.  
 
 Numbers 
# users 497 
co_major 1,288 
co_unvi 6,015 
# education field 7,303 
co_title 3,228 
co_corp 3,776 
# experience field 7,004 
# total edges 14,307 
Table 3: The statistic of edges for our main datasets 
5 Collective Factor Graph Model 
In this section, we propose a collective factor 
graph (CoFG) model for learning and summarizing 
the text of personal profile with local textual in-
formation and social connection. 
5.1 Overview of Our Framework 
To generate summaries for profiles, a straightfor-
ward approach is to treat each personal profile in-
dependently and generating a summary for each 
personal profile individually. As we mentioned on 
Section 3.3, we use the sentences of Experience 
field as a text document and consider it as the 
source of summary for each profile. 
Instead, we formalize the problem of personal 
profile summarization in a pair-wise factor graph 
model and propose an approach referred to as 
Loopy Belief Propagation algorithm to learn the 
model for generating the summary of the profile. 
Our basic idea is to define the correlations using 
different types of factor functions. An objective 
function is defined based on the joint probability 
of the factor functions. Thus, the problem of col-
lective personal profile summarization model 
learning is cast as learning model parameters that 
maximizes the joint probability of the input con-
tinuous dynamic network. 
The overview of the proposed method is a su-
pervised framework (as shown in Figure 3).  First, 
we treat each sentence of the training data and test-
ing data as vectors with textual information (local 
textual attribute functions); Second, all the vectors 
are connected by social connection relationships 
(social connection factors) and we model these 
vectors and their relationships into the collective 
factor graph; third, we propose Loopy Belief Prop-
 
John 
Antony 
   Bill 
Lily  
Lucy  
       Linda 
 
 
 
 
 
co_major 
co_univ 
co_corp 
co_corp 
co_title 
co_title 
co_major 
co_univ 
719
agation algorithm to learn the model and predict 
the sentences of testing data; finally, we select a 
subset of sentences of each testing profile as the 
summary according to the models with top-n pre-
diction score. Thus, the core issues of our frame-
work are 1) how to define the collective factor 
graph model to connection profiles with social 
connection; 2) how to learn and predict the pro-
posed CoFG model; 3) how to predict the sentenc-
es from the testing data with the proposed CoFG 
model, and generate the summary by the predict 
scores. We will discuss these issues on the follow-
ing subsections. 
 
 
Figure 3: The overview of our proposed framework 
 
5.2 Model Definition 
Formally, given a network ( , , , )L UG V S S X? , 
each sentence 
is  is associated with an attribute 
vector 
ix  of the profile and a label iy  indicating 
whether the sentence is selected as a summary of 
the profile (The value of 
iy  is binary. 1 means that 
the sentence is selected as a summary sentence, 
whereas 0 stands for the opposite). V denotes the 
authors of the profiles, LS  denotes the labeled 
training data, and US denotes the unlabeled testing 
data. Let { }iX x? and { }iY y? . Then, we have the 
following formulation 
         
? ? ? ? ? ?? ?
, || , ,
P X G Y P YP Y X G P X G?
             (1) 
Here, G denotes all forms of network infor-
mation. This probabilistic formulation indicates 
that labels of skills depend on not only local at-
tributes X, but also the structure of the network G. 
According to Bayes? rule, we have 
         ? ? ? ? ? ?? ?
? ? ? ?
, |
| ,
,
                  | |
P X G Y P Y
P Y X G
P X G
P X Y P Y G
?
?
             (2) 
Where ( | )P Y G represents the probability of labels 
given the structure of the network and ( | )P X Y  
denotes the probability of generating attributes X
associated to their labels Y . We assume that the 
generative probability of attributes given the label 
of each edge is conditionally independent, thus we 
have 
? ? ? ? ? ?| , | |i iiP Y X G P Y G P x y? ?
    (3) 
Where ( | )i iP x y  is the probability of generating 
attributes 
ix given the label iy . Now, the problem 
becomes how to instantiate the probability 
( | )P Y G and ( | )i iP x y . We model them in a Mar-
kov random field, and thus according to the Ham-
mersley-Clifford theorem (Hammersley and 
Clifford, 1971), the two probabilities can be in-
stantiated as follows: 
? ? ? ?
11
1| exp ,
d
i i j j ij i
j
P x y f x yZ ??
? ?? ? ?? ??
       (4) 
? ? ? ?
( )2
1| exp ,
i j NB i
P Y G g i jZ ?
? ?? ? ?? ?? ?
       (5) 
                       
Where 
1 2 and Z Z  are normalization factors. Eq. 4 
indicates that we define an attribute function 
( , )i if x y  for each attribute ijx
 associated with 
sentence
is . j?  is the weight of the j
th attribute. Eq. 
5 represents that we define a set of correlation fac-
tor functions ( , )g i j  over each pair ( , )i j in the 
network. ( )NB i  denotes the set of social relation-
ship neighbors nodes of i.  
 
 
Training  
Set 
 
  
  
Social  
Connection 
Social  
Connection 
Testing  
Set 
  Sentence Scoring 
  Sentence Selection 
 Summarized Profile 
Profiles 
Profiles 
Collective Factor Graph 
Modeling 
  
720
 1 
3 
2 
  
  
 
 
 
  
 
 
 
 
 
 
 
f (v1,y1) 
y
2
 
y
1
 y3 
y
4
 
y
5
 
y
6
 
 S
1
 
 S
2
 
S
3
 
S
4
 
S
5
 
S
6
 
f (v
1
,y
2
) 
f (v
6
,y
6
) 
 
CoFG model 
Nodes of sentences 
with different people 
y1=0 
y
2
=1 
y
3
=1 
y
4
=0 
y
6
=? 
y
5
=? g (y
3
,y
5
) 
Figure 4: Graph representation of CoFG 
The left figure shows the personal profile network. Each dotted line indicates a social connection. Each dotted 
square denotes a person, and the grey square denotes the sentence selected in the summary, and the white square 
denotes a sentence that is not selected as the summary.. 
The right figure shows the CoFG model derived from left figure. Each eclipse denotes a sentence vector of a 
person, and each circle indicates the hidden variable yi. f(vi,yi) indicates the attribute factor function. g(yi,yj) indi-
cates the social connection factor function. 
 
4 
5 
6 
  
  
co_major 
co_corp 
  
Person A 
Person B 
Person C 
We now briefly introduce possible ways to de-
fine the attribute functions{ ( , )}ij i jf x y
, and factor 
function ( , )g i j  .  
Local textual attribute functions{ ( , )}ij i jf x y
: 
It denotes the attribute value associated with each 
sentence i. We define the local textual attribute as 
a feature (Lafferty et al, 2001). We can accumu-
late all the attribute functions and obtain local en-
tropy for a person: 
? ?
1
1 exp ,k k ik i
i k
f x yZ ?
? ?? ?? ???
              (6) 
The textual attributes include following features 
(Shen et al, 2007; Yang et al, 2011b):  
1) BOW: the bag-of-words of each sentence, we 
use unigram features as the basic textual fea-
tures for each sentence.  
2) Length: the number of terms of each sentence. 
3) Topic_words: these are the most frequent 
words in the sentence after the stop words are 
removed. 
4) PageRank_scores: as shown in the related 
work section, a document can be treated as a 
graph and applying a graph-based ranking al-
gorithm (Wan and Yang., 2008). We thus use 
the PageRank score to reflect the importance 
of each sentence. 
Social connection factor function ( , )i jg y y
: 
For the social correlation factor function, we de-
fine it through the pairwise network structure. That 
is, if the person of sentence i and the person of 
sentence j have a social relationship, a factor func-
tion for this social connection is defined (Tang et 
al., 2011a; Tang et al, 2011b), i.e., 
? ? ? ?? ?2, expi j ij i jg y y y y?? ?         (7) 
The person-person social relationships are de-
fined on Section 4, e.g. co_major, co_univ, co_title, 
and co_corp. We define that if two persons have at 
least one social connection edge, they have a so-
cial relationship. In addition, 
ij?  is the weight of 
the function, representing the influence degree of i 
on j. 
To better understand our model, one example of 
factor decomposition is given in Figure 4. In this 
example, there are six sentences from three pro-
files. Among them, four sentences are labeled (two 
are labeled with the category of ?1?, i.e,  1y ?  and 
the other two are labeled with the category of ?0?, 
i.e., 0y ? ) and two sentences are unlabeled (they 
are represented by y=?). We have six attribute 
functions. For example, 
1( , )if v y  denotes the set 
721
of local textual attribute functions of 
iy . We also 
have five pairwise relationships (e.g.,
2 4( , )y y ,
3 5( , )y y ) based on the structure of the input per-
sonal profile social network. For example, 
3 5( , )g y y  denotes social connection between 3y  
and 
5y , while they share the co_major relationship 
on the left figure. 
5.3 Model Learning 
We now address the problem of estimating the free 
parameters. The objective of learning the CoFG 
model is to estimate a parameter configuration 
({ },{ })? ? ??  to maximize the log-likelihood ob-
jective function ( ) log ( | , )L P Y X G?? ? , i.e., 
? ?* argmax L? ??                     (9) 
To solve the objective function, we adopt a gra-
dient descent method. We use ?  (the weight of 
the social connection factor function ( , )i jg y y
) as 
the example to explain how we learn the parame-
ters (the algorithm also applies to tune ?  by simp-
ly replacing ? with? ). Specifically, we first write 
the gradient of each 
k? with regard to the objective 
function (Eq. 9) :  
  ? ? ? ? ? ?( | , ), ,kP Y X G
k
L E g i j E g i j?
?
? ? ? ? ? ? ?? ? ? ?
   (10) 
Where [ ( , )]E g i j is the expectation of factor 
function ( , )g i j  given the data distribution (essen-
tially it can be considered as the average value of 
the factor function ( , )g i j over all pair in the train-
ing data); and 
( | , ) [ ( , )]k Y X GPE g i j?
is the expectation of 
factor function ( , )g i j under the distribution 
( | , )kP Y X G?
given by the estimated model. A 
similar gradient can be derived for parameter
ja
. 
We approximate the marginal distribution
( | , ) [ ( , )]k Y X GPE g i j?
 using LBP (Tang et al, 2011; 
Zhuang et al, 2012). With the marginal probabili-
ties, the gradient can be obtained by summing over 
all triads. It is worth noting that we need to per-
form the LBP process twice for each iteration: one 
is to estimate the marginal distribution of unknown 
variables ?iy ?  and the other is to estimate the 
marginal distribution over all pairs. In this way, 
the algorithm essentially performs a transfer learn-
ing over the complete network. Finally, with the 
obtained gradient, we update each parameter with 
a learning rate? . The learning algorithm is sum-
marized in Figure 5. 
 
Input: Network G , Learning rate ?   
Output: Estimated parameters ?   
Initialize 0? ?   
Repreat 
1) Perform LBP to calculate the 
marginal distribution of unknown 
variables, i.e., ? ?| ,i iP y x G   
2) Perform LBP to calculate the 
marginal distribution of each  
variables, i.e., ? ?( , ), | ,i j i jP y y X G  
3) Calculate the gradient of 
k? ac-
cording to Eq. 10 (for a  with a 
similar formula) 
4) Update parameter ?  with the 
learning rate ?  
               
? ?
new old
L ?? ? ? ?? ?  
Until Convergence 
Figure 5: The Learning Algorithm for CoFG model 
 
5.4 Model Prediction and Summary Gener-
ated 
We can see that in the learning process, the learn-
ing algorithm uses an additional loopy belief prop-
agation to infer the label of unknown relationships. 
With the estimated parameter ? , the summariza-
tion process is to find the most likely configuration 
of Y  for a given profile. This can be obtained by  
? ?* argmax | , ,Y L Y X G ??              (11) 
Finally, we select a subset of sentences of each 
testing profile as the summary according to the 
trained models with top-n prediction scores by *Y   
(Tang et al, 2011b; Dong et al 2012).  
6 Experimentation 
In this section, we describe the settings of our ex-
periment and present the experimental results of 
our proposed CoFG model. 
722
6.1 Experiment Settings 
In the experiment, we use the corpus collected 
from LinkedIn.com that contains 497 profiles (see 
more details in Section 3). The existing summaries 
in these profiles are served as the reference sum-
mary (the standard answers). As discussed in sub-
section 3.3, the average length of summary is 
about 40 words. Thus, we extract 40 words to con-
struct the summary for each profile. We use 200 
personal profiles as the testing data, and the re-
maining ones as the training data. 
We use the ROUGE-1.5.5 (Lin and Hovy, 2004) 
toolkit for evaluation, a popular tool that has been 
widely adopted by several evaluations such as 
DUC and TAC (Wan and Yang, 2008; Wan, 2011). 
We provide four of the ROUGE F-measure scores 
in the experimental results: ROUGE-2 (bigram-
based), ROUGE-L (based on longest common 
subsequences), ROUGE-W (based on weighted 
longest common subsequence, weight=1.2), and 
ROUGE-SU4 (based on skip bigram with a maxi-
mum skip distance of 4).  
6.2 Experimental Results 
We compare the proposed CoFG approach with 
three baselines illustrated as follows: 
? Random: we randomly select sentences of 
each profile to generate the summary for the 
profile. 
? HITS: we employ the HITS algorithm to per-
form profile summarization (Wan and Yang, 
2008). In detail, we first consider the words as 
hubs the sentences as authorities; Then, we 
rank the sentences with the authorities? scores 
for each profile individually; Finally, the 
highest ranked sentences are chosen to consti-
tute the summary. 
? PageRank: we employ the PageRank algo-
rithm to perform profile summarization (Wan 
and Yang, 2008). In detail, we first connect 
the sentences of the profile with cosine text-
based similar measure to construct a graph; 
Then, we apply PageRank algorithm to rank 
the sentence through the graph for each pro-
file individually; Finally, the highest ranked 
sentences are chosen to constitute the sum-
mary.  
?  MaxEnt: as a supervised learning approach, 
maximum entropy uses textual attribute as 
features to train a classification model. Then, 
the classification model is employed to pre-
dict which sentences can be selected to gener-
ate the summary. For the implementation of 
MaxEnt, we employ the tool of mallent 
toolkits4. 
Table 4 shows the comparison results of our ap-
proach (CoFG) and the baseline approaches. From 
Table 4, we can see that 1) either HITS or Pag-
eRank outperforms the approach of  random selec-
tion; 2) The supervised approach i.e. MaxEnt, out-
performs both the HITS algorithm and the Pag-
eRank approach; 3) CoFG model performs best 
and it greatly outperforms both the unsupervised 
and supervised learning baseline approaches in 
terms of the ROUGE-2 F-measure score. This re-
sult verifies the effectiveness of considering the 
social connection between the sentences in differ-
ent profiles, 
Figure 6 shows the performance of our proposed 
CoFG model with different sizes of training data. 
From Figure 6, we can see that CoFG model with 
social connection always performs better than 
MaxEnt, and the performance of our approach de-
scends slowly when the training dataset becomes 
small. Specifically, the performance of CoFG us-
ing only 10% training data achieves better perfor-
mance than MaxEnt using 100% training data. 
 
                                                 
4 http://mallet.cs.umass.edu/ 
 ROUGE-2 ROUGE-L ROUGE-W ROUGE-SU4 
Random 0.0219 0.1363 0.0831 0.0288 
HITS 0.0295 0.1499 0.0905 0.0355 
PageRank 0.0307 0.1574 0.0944 0.0383 
MaxEnt 0.0349 0.1659 0.0995 0.0377 
CoFG 0.0383 0.1696 0.1015 0.0415 
Table 4: Performances of different approaches to profile summarization in terms of different measurements 
723
 
Figure 6:  The performance of CoFG with different 
training data size 
 
Table 5 shows the contribution of the social 
edges with CoFG. Specifically, CoFG is our pro-
posed approach with both education and experi-
ence information, CoFG-edu means that the CoFG 
model considers the social edges of education field 
(co_major, co_univ) only, and CoFG-exp means 
that the CoFG model considers the social edges of 
work experience field (co_title, co_corp) only. 
MaxEnt can be considered as using textual infor-
mation only. 
 
 ROUGE-2 
MaxEnt 0.0349 
CoFG 0.0383 
CoFG-edu 0.0382 
CoFG-exp 0.0381 
Table 5: ROUGE-2 F-Measure score of the contribu-
tion of social edges 
 
From Table 5, we can see that all of our pro-
posed approaches, i.e., CoFG-edu, CoFG-exp, and 
CoFG, outperform the baseline approach, i.e., 
MaxEnt. However, the performance of CoFG-edu, 
CoFG-exp and CoFG are similar. This result is 
mainly due to the fact that the information of so-
cial connection is redundant. For example, two 
persons who are connected by co_major (educa-
tion field) might also be connected by co_corp 
(experience field).  
7 Conclusion and Future Work 
In this paper, we present a novel task named pro-
file summarization and propose a novel approach 
called collective factor graph model to address this 
task. One distinguishing feature of the proposed 
approach lies in its incorporating the social con-
nection. Empirical studies demonstrate that the 
social connection is effective for profile summari-
zation, which enables our approach outperform 
some competitive supervised and unsupervised 
baselines. 
The main contribution of this paper is to explore 
social context information to help generate the 
summary of the profiles, which represents an in-
teresting research direction in social network min-
ing. In the future work, we will explore more kinds 
of social context information and investigate better 
ways of incorporating them into profile summari-
zation and a wider range of social network mining. 
 
Acknowledgments 
 
This research work is supported by the National 
Natural Science Foundation of China 
(No.61273320, No.61272257, No.61331011 and 
No.61375073), and National High-tech Research 
and Development Program of China 
(No.2012AA011102). 
We thank Dr. Jie Tang and Honglei Zhuang for 
providing their software and useful suggestions 
about PGM. We acknowledge Dr. Xinfang Liu, 
Yunxia Xue and Yulai Shen for corpus construc-
tion and insightful comments. We also thank 
anonymous reviewers for their valuable sugges-
tions and comments.  
References  
Baeza-Yates R. and B. Ribeiro-Neto. 1999. Modern 
Information Retrieval. ACM Press and Addison Wes-
ley, 1999 
Celikyilmaz A. and D. Hakkani-Tur. 2011. Discovery 
of Topically Coherent Sentences for Extractive 
Summarization. In Proceeding of ACL-11. 
Dong Y., J. Tang, S. Wu, J. Tian, N. Chawla, J. Rao, 
and H. Cao. 2012. Link Prediction and Recommen-
dation across Heterogeneous Social Networks. In 
Proceedings of ICDM-12. 
Elson D., N. Dames and K. McKeown. 2010. Extracting 
Social Networks from Literary Fiction. In Proceed-
ing of ACL-10. 
Erkan G. and D. Radev. 2004. LexPageRank: Prestige 
in Multi-document Text Summarization. In Proceed-
ings of EMNLP-04. 
Guy I., N. Zwerdling, I.  Ronen, D. Carmel, E. Uziel. 
2010. Social Media Recommendation based on Peo-
ple and Tags. In Proceeding of SIGIR-10. 
0.030
0.032
0.034
0.036
0.038
0.040
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
R
O
U
G
E
-2
 
size of training data 
PageRank MaxEnt CFG
724
Hammersley J. and P. Clifford. 1971. Markov Field on 
Finite Graphs and Lattices, Unpublished manuscript. 
1971. 
Hu P., C. Sun, L. Wu, D. Ji and C. Teng. 1011. Social 
Summarization via Automatically Discovered Social 
Context. In Proceeding of IJCNLP-11. 
Lafferty J, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of ICML-01. 
Lappas T., K. Punera and T. Sarlos. 2011. Mining Tags 
Using Social Endorsement Networks. In Proceeding 
of SIGIR-11. 
Leskovec J., D. Huttenlocher and J. Kleinberg. 2010. 
Predicting Positive and Negative Links in Online So-
cial Networks. In Proceedings of WWW-10. 
Lin, C. 2004. ROUGE: a Package for Automatic Evalu-
ation of Summaries. In Proceedings of ACL-04 
Workshop on Text Summarization Branches Out. 
Lu Y., P. Tsaparas, A. Ntoulas and L. Polanyi. 2010. 
Exploiting Social Context for Review Quality Pre-
diction. In Proceeding of WWW-10. 
Meng X?F. Wei? X. Liu? M. Zhou? S. Li and H. 
Wang. 2012. Entity-Centric Topic-Oriented Opinion 
Summarization in Twitter. In Proceeding of KDD-12.  
Murphy K., Y. Weiss, and M. Jordan. 1999. Loopy Be-
lief Propagation for Approximate Inference: An Em-
pirical Study. In Proceedings of UAI-99. 
Radev D. and K. McKeown. 1998. Generating Natural 
Language Summaries from Multiple On-line Sources. 
Computational Linguistics, 24(3):469?500. 
Radev D., H. Jing, M. Stys, and D. Tam. 2004. Cen-
troid-based Summarization of Multiple Documents. 
Information Processing and Management. 40 (2004), 
919-938. 
Rosenthal S. and K. McKeown. 2011. Age Prediction in 
Blogs: A Study of Style, Content, and OnlineBehav-
ior in Pre- and Post-Social Media Generations. In 
Proceeding of ACL-11. 
Ryang S. and T. Abekawa. 2012. Framework of Auto-
matic Text Summarization Using Reinforcement 
Learning. In Proceeding of EMNLP-2012. 
Shen D., J. Sun, H. Li, Q. Yang and Zheng Chen. 2007. 
Document Summarization using Conditional Ran-
dom Fields. In Proceeding of IJCAI-07. 
Tan C., L. Lee, J. Tang, L. Jiang, M. Zhou and P. Li. 
2011. User-Level Sentiment Analysis Incorporating 
Social Networks. In Proceedings of KDD-11. 
Tang W., H. Zhuang, and J. Tang. 2011a. Learning to 
Infer Social Ties in Large Networks. In Proceedings 
of ECML/PKDD-11. 
Tang J., Y. Zhang, J. Sun, J. Rao, W. Yu, Y. Chen, and 
A. Fong. 2011b. Quantitative Study of Individual 
Emotional States in Social Networks. IEEE Transac-
tions on Affective Computing. vol.3(2), Pages 132-
144. 
Wan X. and J. Yang. 2008. Multi-document Summari-
zation using Cluster-based Link Analysis. In Pro-
ceedings of SIGIR-08. 
Wan X. 2011. Using Bilingual Information for Cross-
Language Document Summarization. In Proceedings 
of ACL-11. 
Wang H. and G. Zhou. 2012. Toward a Unified Frame-
work for Standard and Update Multi-Document 
Summarization. ACM Transactions on Asian Lan-
guage Information Processing. vol.11(2). 
Xing E, M. Jordan, and S. Russell. 2003. A Generalized 
Mean Field Algorithm for Variational Inference in 
Exponential Families. In Proceedings of UAI-03. 
Yang S., B. Long, A. Smola, N. Sadagopan, Z. Zheng 
and H. Zha. 2011a. Like like alike ? Joint Friend-
ship and Interest Propagation in Social Networks. In 
Proceeding of WWW-11. 
Yang Z., K. Cai, J. Tang, L. Zhang, Z. Su and J. Li. 
2011b. Social Context Summarization. In Proceed-
ing of SIGIR-11. 
Zhuang H, J. Tang, W. Tang, T. Lou, A. Chin, and X. 
Wang. 2012. Actively Learning to Infer Social Ties. 
In Proceedings of Data Mining and Knowledge Dis-
covery (DMKD-12), vol.25 (2), pages 270-297. 
725
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 968?976,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
  Tree Kernel-based Negation and Speculation Scope Detection with 
Structured Syntactic Parse Features 
 
 
Bowei Zou       Guodong Zhou       Qiaoming Zhu* 
Natural Language Processing Lab, School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China 
zoubowei@gmail.com, {gdzhou,qmzhu}@suda.edu.cn 
 
  
 
Abstract 
Scope detection is a key task in information ex-
traction. This paper proposes a new approach for 
tree kernel-based scope detection by using the 
structured syntactic parse information. In addi-
tion, we have explored the way of selecting 
compatible features for different part-of-speech 
cues. Experiments on the BioScope corpus show 
that both constituent and dependency structured 
syntactic parse features have the advantage in 
capturing the potential relationships between 
cues and their scopes. Compared with the state 
of the art scope detection systems, our system 
achieves substantial improvement.* 
1 Introduction 
The task of scope detection is to detect the linguis-
tic scope dominated by a specific cue. Current re-
searches in this field focus on two semantic as-
pects: negation and speculation. The negative 
scope detection is to detect the linguistic scope 
which is repudiated by a negative word (viz., nega-
tive cue, e.g., ?not?). In other side, the speculative 
scope detection is to detect the uncertain part in a 
sentence corresponding to the speculative word 
(viz., speculative cue, e.g., ?seems?). See the sen-
tence 1) below, the negative cue ?not? dominates 
the scope of ?not expensive?. Similarly, the specu-
lative cue ?possible? in sentence 2) dominates the 
uncertain scope ?the possible future scenarios?. 
1) The chair is [not expensive] but comfortable.  
2) Considering all that we have seen, what are now 
[the possible future scenarios]? 
                                                 
*	Corresponding	author	
The negative and speculative scope detection 
task consists of two basic stages. The first one is to 
identify the sentences involving negative or specu-
lative meaning. The second stage is to detect the 
linguistic scope of the cue in sentences (Velldal et 
al, 2012). In this paper, we focus on the second 
stage. That is, by given golden cues, we detect 
their linguistic scopes. 
We propose a tree kernel-based negation and 
speculation scope detection with structured syntac-
tic parse features. In detail, we regard the scope 
detection task as a binary classification issue, 
which is to classify the tokens in a sentence as be-
ing inside or outside the scope. In the basic 
framework, we focus on the analysis and applica-
tion of structured syntactic parse features as fol-
lows: 
Both constituent and dependency syntactic fea-
tures have been proved to be effective in scope 
detection (?zg?r et al 2009; ?vrelid et al 2010). 
However, these flat features are hardly to reflect 
the information implicit in syntactic parse tree 
structures. Our intuition is that the segments of the 
syntactic parse tree around a negative or specula-
tive cue is effective for scope detection. The relat-
ed structures normally underlay the indirect clues 
to identify the relations between cues and their 
scopes, e.g., in sentence 1), ?but something?, as a 
frequently co-occurred syntactic structure with 
?not something?, is an effective clue to determine 
the linguistic scope of ?not?. 
The tree kernel classifier (Moschitti, 2006) 
based on support vector machines uses a kernel 
function between two trees, affording a compari-
son between their substructures. Therefore, a tree 
kernel-based scope detection approach with struc-
tured syntactic parse tree is employed. The tree 
968
kernel has been already proved to be effective in 
semantic role labeling (Che et al 2006) and rela-
tion extraction (Zhou et al 2007). 
In addition, the empirical observation shows 
that features have imbalanced efficiency for scope 
classification, which is normally affected by the 
part-of-speech (abbr., POS) of cues. Hence, we 
build the discriminative classifiers for each kind of 
POS of cues, then explore and select the most 
compatible features for them. 
We construct a scope detection system by using 
the structured syntactic parse features based tree 
kernel classification. Compared with the state of 
the art scope detection systems, our system 
achieves the performance of accuracy 76.90% on 
negation and 84.21% on speculation (on Abstracts 
sub-corpus). Additionally, we test our system on 
different sub-corpus (Clinical Reports and Full 
Papers). The results show that our approach has 
better cross-domain performance. 
The rest of this paper is organized as follows: 
Section 2 reviews related work. Section 3 intro-
duces the corpus and corresponding usage in our 
experiments. Section 4 describes our approach and 
the experiments are presented in Section 5. Finally, 
there is a conclusion in Section 6. 
2 Related Work 
Most of the previous studies on negation and spec-
ulation scope detection task can be divided into 
two main aspects: the heuristic rule based methods 
and the machine learning based methods. We re-
spectively introduce the aspects in below. 
2.1 Heuristic Rule based Methods 
The initial studies for scope detection are to com-
pile effective heuristic rules (Chapman et al 2001; 
Goldin et al 2003). Recently, the heuristic rule 
based methods have further involved the syntactic 
features. 
Huang et al(2007) implemented a hybrid ap-
proach to automated negation scope detection. 
They combined the regular expression matching 
with grammatical parsing: negations are classified 
on the basis of syntactic categories and located in 
parse trees. Their hybrid approach is able to identi-
fy negated concepts in radiology reports even 
when they are located at some distance from the 
negative term. 
?zg?r et al(2009) hypothesized that the scope 
of a speculation cue can be characterized by its 
part-of-speech and the syntactic structure of the 
sentence and developed rules to map the scope of a 
cue to the nodes in the syntactic parse tree. By 
given golden speculation cues, their rule-based 
method achieves the accuracies of 79.89% and 
61.13% on the Abstracts and the Full-Papers sub-
corpus, respectively. 
?vrelid et al(2010) constructed a small set of 
heuristic rules which define the scope for each cue. 
In developing these rules, they made use of the 
information provided by the guidelines for scope 
annotation in the BioScope corpus, combined with 
manual inspection of the training data in order to 
further generalize over the phenomena discussed 
by Vincze et al(2008) and work out interactions of 
constructions for various types of cues. 
Apostolova et al(2011) presented a linguistical-
ly motivated rule-based system for the detection of 
negation and speculation scopes that performs on 
par with state-of-the-art machine learning systems. 
The rules are automatically extracted from the Bi-
oScope corpus and encode lexico-syntactic pat-
terns in a user-friendly format. While their system 
was developed and tested using a biomedical cor-
pus, the rule extraction mechanism is not domain-
specific. 
The heuristic rule based methods have bad ro-
bustness in detecting scopes crossing different 
meaning aspects (e.g., negative vs. speculative) 
and crossing different linguistic resources (e.g., 
Technical Papers vs. Clinical Reports). 
2.2 Machine Learning based Methods 
The machine learning based methods have been 
ignored until the release of the BioScope corpus 
(Szarvas et al 2008), where the large-scale data of 
manually annotated cues and corresponding scopes 
can support machine learning well. 
Morante et al(2008) formulated scope detection 
as a chunk classification problem. It is worth not-
ing that they also proposed an effective proper 
post-processing approach to ensure the consecu-
tiveness of scope. Then, for further improving the 
scope detection, Morante et al(2009a) applied a 
meta-learner that uses the predictions of the three 
classifiers (TiMBL/SVM/CRF) to predict the 
scope. 
For the competitive task in CoNLL?2010 (Far-
kas et al 2010), Morante et al(2010) used a 
969
memory-based classifier based on the k-nearest 
neighbor rule to determine if a token is the first 
token in a scope sequence, the last, or neither. 
Therefore, in order to guarantee that all scopes are 
continuous sequences of tokens they apply a first 
post-processing step that builds the sequence of 
scope. 
The existing machine learning based approaches 
substantially improve the robustness of scope de-
tection, and have nearly 80% accuracy. However, 
the approaches ignore the availability of the struc-
tured syntactic parse information. This information 
involves more clues which can well reflect the re-
lations between cues and scopes. S?nchez et al
(2010) employed a tree kernel based classifier with 
CCG structures to identify speculative sentences 
on Wikipedia dataset. However, in S?nchez?s ap-
proach, not all sentences are covered by the classi-
fier. 
3 Corpus 
We have employed the BioScope corpus (Szarvas 
et al 2008; Vincze et al 2008)1, an open resource 
from the biomedical domain, as the benchmark 
corpus. The corpus contains annotations at the to-
ken level for negative and speculative cues and at 
the sentence level for their linguistic scope (as 
shown in Figure 1). 
 (Note: <Sentence> denotes one sentence and the tag ?id? denotes its 
serial number; <xcope> denotes the scope of a cue; <cue> denotes the 
cue, the tag ?type? denotes the specific kind of cues and the tag ?ref? 
is the cue?s serial number.) 
Figure 1. An annotated sentence in BioScope. 
The BioScope corpus consists of three sub-
corpora: biological Full Papers from FlyBase and 
BMC Bioinformatics, biological paper Abstracts 
from the GENIA corpus (Collier et al 1999), and 
Clinical Reports. Among them, the Full Papers 
sub-corpus and the Abstracts sub-corpus come 
from the same genre. In comparison, the Clinical 
Reports sub-corpus consists of clinical radiology 
reports with short sentences. 
                                                 
1 http://www.inf.u-szeged.hu/rgai/bioscope 
In our experiments, if there is more than one cue 
in a sentence, we treat them as different cue and 
scope (two independent instances). The statistical 
data for our corpus is presented in Table 1 in be-
low. 
The average length of sentences in the negation 
portion is almost as long as that in speculation, 
while the average length of scope in negation is 
shorter than that in speculation. In addition, the 
length of sentence and scope in both Abstracts and 
Full Papers sub-corpora is comparative. But in 
Clinical Reports sub-corpus, it is shorter than that 
in Abstracts and Full Papers. Thus, looking for the 
effective features in short sentences is especially 
important for improving the robustness for scope 
detection. 
(Note: ?Av. Len? stands for average length.) 
Table 1. Statistics for our corpus in BioScope. 
4 Methodology 
We regard the scope detection task as a binary 
classification problem, which is to classify each 
token in sentence as being the element of the scope 
or not. Under this framework, we describe the flat 
syntactic features and employ them in our bench-
mark system. Then, we propose a tree kernel-
based scope detection approach using the struc-
tured syntactic parse features. Finally, we con-
struct the discriminative classifier for each kind of 
POS of cues, and select the most compatible fea-
tures for each classifier. 
4.1 Flat Syntactic Features 
In our benchmark classification system, the fea-
tures relevant to the cues or tokens are selected. 
Then, we have explored the constituent and de-
pendency syntactic features for scope detection. 
These features are all flat ones which reflect the 
characteristic of tokens, cues, scopes, and the rela-
tion between them. 
 Abstract Paper Clinical
Nega-
tion 
Sentences 1594 336 441 
Words 46849 10246 3613 
Scopes 1667 359 442 
Av. Len Sentence 29.39 30.49 8.19 
Av. Len Scope 9.62 9.36 5.28 
Specu-
lation
Sentences 2084 519 854 
Words 62449 16248 10241
Scopes 2693 682 1137 
Av. Len Sentence 29.97 31.31 11.99
Av. Len Scope 17.24 15.58 6.99 
<sentence id=?S26.8?> These findings <xcope id=?X26.8.2?> 
<cue type=?speculation? ref=?X26.8.2?> indicate that </cue> 
<xcope id=?X26.8.1?> corticosteroid resistance in bronchial 
asthma <cue type=?negation? ref=?X26.8.1?> can not </cue> 
be explained by abnormalities in corticosteroid receptor char-
acteristics </xcope></xcope> . </sentence> 
970
Basic Features: Table 2 shows the basic fea-
tures which directly relate to the characteristic of 
cues or tokens in our basic classification. 
Feature Remark 
B1 Cue. 
B2 Candidate token. 
B3 Part-of-speech of candidate token. 
B4 Left token of candidate token. 
B5 Right token of candidate token. 
B6 Positional relation between cue and token.
   Table 2. Basic features. 
Constituent Syntactic Features: For improv-
ing the basic classification, we employ 10 constit-
uent features belonging to two aspects. On the one 
hand, we regard the linguistic information of the 
neighbor locating around the candidate tokens as 
the coherent features (CS1~CS6 in Table 3). These 
features are used for detecting the close coopera-
tion of a candidate token co-occurring with its 
neighbors in a scope. On the other hand, we regard 
the linguistic characteristics of the candidate to-
kens themselves in a syntactic tree as the inherent 
features (CS7~CS10 in Table 3). These features 
are used for determining whether the token has the 
direct relationship with the cue or not. 
Features Remarks 
CS1 POS of left token. 
CS2 POS of right token. 
CS3 Syntactic category of left token. 
CS4 Syntactic category of right token. 
CS5 Syntactic path from left token to the cue. 
CS6 Syntactic path from right token to the cue. 
CS7 Syntactic category of the token. 
CS8 Syntactic path from the token to the cue. 
CS9 Whether the syntactic category of the token is 
the ancestor of the cue. 
CS10 Whether the syntactic category of the cue is the 
ancestor of the token. 
Table 3. Constituent syntactic features. 
Features Remarks 
DS1 Dependency direction (?head?or ?dependent?). 
DS2 Dependency syntactic path from the token to cue. 
DS3 The kind of dependency relation between the token 
and cue. 
DS4 Whether the token is the ancestor of the cue. 
DS5 Whether the cue is the ancestor of the token. 
Table 4. Dependency syntactic features. 
Dependency Syntactic Features: For the effec-
tiveness to obtain the syntactic information far 
apart from cues, we use 5 dependency syntactic 
features which emphasize the dominant relation-
ship between cues and tokens by dependency arcs 
as shown in Table 4. 
The features in Table 2, 3, and 4 have imbal-
anced classification for the scope classification. 
Therefore, we adopt the greedy feature selection 
algorithm as described in Jiang et al(2006) to pick 
up positive features incrementally according to 
their contributions. The algorithm repeatedly se-
lects one feature each time, which contributes most, 
and stops when adding any of the remaining fea-
tures fails to improve the performance. 
4.2 Structured Syntactic Features 
Syntactic trees involve not only the direct bridge 
(e.g., syntactic path) between cue and its scope but 
also the related structures to support the bridge 
(e.g., sub-tree). The related structures normally 
involve implicit clues which underlay the relation 
between cue and its scope. Therefore, we use the 
constituent and dependency syntactic structures as 
the supplementary features to further improve the 
benchmark system. 
Furthermore, we employ the tree kernel-based 
classifier to capture the structured information 
both in constituent and dependency parsing trees. 
The results of the constituent syntactic parser are 
typical trees which always consist of the syntactic 
category nodes and the terminal nodes. Thus, the 
constituent syntactic tree structures could be used 
in tree kernel-based classifier directly, but not for 
the dependency syntactic tree structures. As Figure 
2 shows, in sentence ?The chair is not expensive 
but comfortable.? the tree kernels cannot represent 
the relations on the arcs (e.g., ?CONJ? between 
?expensive? and ?comfortable?). It is hard to use 
the relations between tokens and cues in tree ker-
nels. 
 Figure 2: The dependency tree of sentence ?The 
chair is not expensive but comfortable.? 
971
 Figure 3. Two transformational rules. 
To solve the problem, we transform the depend-
ency tree into other two forms capable of being 
used directly as the compatible features in tree-
kernel based classification. The transformational 
rules are described as below: 
(1) Extracting the dependency relations to gen-
erate a tree of pure relations (named dependency 
relational frame), where the tokens on the nodes of 
original dependency tree are ignored and only the 
relation labels are used. E.g., the tokens ?chair?, 
?is?, etc in Figure 2 are all deleted and replaced by 
the corresponding relation labels. E.g., ?NSUBJ?, 
?COP?, etc are used as nodes in the dependency 
relational frame, see (1a) & (1b) in Figure 3. 
(2) Inserting the tokens which have been deleted 
in step (1) into the dependency relational frame 
and making them follow and link with their origi-
nal dependency relations. E.g., the tokens ?chair?, 
?is?, etc are added below the nodes ?NSUBJ?, 
?COP?, etc, see (2a) & (2b) in Figure 3. 
 
Figure 4. Two transformations for tree-kernel. 
Within the constituent and dependency syntactic 
trees, we have employed both the Completed Sub-
Tree and the Critical Path as the syntactic structure 
features for our classification. The former is a min-
imum sub-tree that involves the cues and the to-
kens, while the latter is the path from the cues to 
the tokens in the completed tree containing the 
primary structural information. Figure 4 shows 
them. 
4.3 Part-of-Speech Based Classification Op-
timization 
Motivating in part by the rule-based approach of 
?zg?r et al(2009), we infer that features have im-
balanced efficiency for scope classification, nor-
mally affected by the part-of-speech (POS) of cues.  
POS of Cues Number POS of Cues Number
CC 157 VB 31 
IN 115 VBD 131 
JJ 238 VBG 225 
MD 733 VBN 112 
NN 43 VBP 561 
RB 137 VBZ 207 
Table 5. Distribution of different POSs of specula-
tive cues in Abstracts sub-corpus. 
Table 5 shows the distribution for different 
POSs of cues in the Abstracts sub-corpus of Bio-
Scope for speculation detection task. The cues of 
different POS usually undertake different syntactic 
roles. Thus, there are different characteristics in 
triggering linguistic scopes. See the two examples 
below: 
3) TCF-1 contained a single DNA box in the [putative 
mammalian sex-determining gene SRY]. 
4) The circadian rhythm of plasma cortisol [either 
disappeared or was inverted]. 
The speculative cue ?putative? in sentence 3) is 
an adjective. The corresponding scope is its modi-
ficatory structure (?putative mammalian sex-
determining gene SRY?). In sentence 4), ?ei-
ther?or?? is a conjunction speculation cue. Its 
scope is the two connected components (?either 
disappeared or was inverted?). Thus, the effective 
features for the adjectival cue are normally the de-
pendency features, e.g., the features of DS1 and 
DS5 in Table 4, while the features for the conjunc-
tion cue are normally the constituent information, 
e.g., the features of CS9 in Table 3.  
In Table 5, considering the different function of 
verb voice, we cannot combine the ?VB(*)? POS. 
For instance, the POS of ?suggest? in sentence 5) 
is ?VBP? (the verb present tense). The correspond-
ing scope does not involve the sentence subject. 
972
The POS of ?suggested? in sentence 6) is ?VBN? 
(the past participle). The scope involves the sub-
ject ?An age-related decrease?. 
5) These results [suggest that the genes might be in-
volved in terminal granulocyte differentiation]. 
6) [An age-related decrease was suggested between 
subjects younger than 20 years]. 
As a result, we have built a discriminative clas-
sifier for each kind of POS of cues, and then ex-
plored and selected the most compatible features 
for each classifier. 
5 Experiments and Results 
5.1 Experimental Setting 
Considering the effectiveness of different features, 
we have split the Abstracts sub-corpus into 5 equal 
parts, within which 2 parts are used for feature 
selection (Feature Selection Data) and the rest for 
the scope detection experiments (Scope Detection 
Data). The Feature Selection Data are divided into 
5 equal parts, within which 4 parts for training and 
the rest for developing. In our scope detection ex-
periments, we divide the Scope Detection Data 
into 10 folds randomly, so as to perform 10-fold 
cross validation. As the experiment data is easily 
confusable, Figure 5 illustrates the allocation. 
Checking the validity of our method, we use the 
Abstracts sub-corpus in Section 5.2, 5.3 and 5.4, 
while in Section 5.5 we use all of the three sub-
corpora (Abstracts, Full Papers, and Clinical Re-
ports) to test the robustness of our system when 
applied to different text types within the same do-
main. 
 Figure 5. The allocation for experiment data. 
The evaluation is made using the precision, re-
call and their harmonic mean, F1-score. Addition-
ally, we report the accuracy in PCS (Percentage of 
Correct Scopes) applied in CoNLL?2010, within 
which a scope is fully correct if all tokens in a sen-
tence have been assigned to the correct scope class 
for a given cue. The evaluation in terms of preci-
sion and recall measures takes a token as a unit, 
whereas the evaluation in terms of PCS takes a 
scope as a unit. The key toolkits for scope classifi-
cation include: 
Constituent and Dependency Parser: All the 
sentences in BioScope corpus are tokenized and 
parsed using the Berkeley Parser (Petrov et al 
2007) 2  which have been trained on the GENIA 
TreeBank 1.0 (Tateisi et al 2005)3, a bracketed 
corpus in PTB style. 10-fold cross-validation on 
GTB1.0 shows that the parser achieves 87.12% in 
F1-score. On the other hand, we obtain the de-
pendency relations by the Stanford Dependencies 
Parser4. 
Support Vector Machine Classifier: SVMLight5 
is selected as our classifier, which provides a way 
to combine the tree kernels with the default and 
custom SVMLight kernels. We use the default pa-
rameter computed by SVMLight. 
Besides, according to the guideline of the Bio-
Scope corpus, scope must be a continuous chunk. 
The scope classifier may result in discontinuous 
blocks, as each token may be classified inside or 
outside the scope. Therefore, we perform the rule 
based post-processing algorithm proposed by Mo-
rante et al(2008) to obtain continuous scopes. 
5.2 Results on Flat Syntactic Features 
Relying on the results of the greedy feature selec-
tion algorithm (described in Section 4.1), we ob-
tain 9 effective features {B1, B3, B6, CS3, CS4, 
CS9, DS1, DS3, DS5} (see Table 2, 3 and 4) for 
negation scope detection and 13 effective features 
{B3, B4, B5, B6, CS1, CS5, CS6, CS8, CS9, CS10, 
DS1, DS4, DS5} for speculation. Table 6 lists the 
performances on the Scope Detection Data by per-
forming 10-fold cross validation. It shows that flat 
constituent and dependency syntactic features sig-
nificantly improve the basic scope detection by 
13.48% PCS for negation and 30.46% for specula-
tion (?2; p < 0.01). It demonstrates that the selected 
syntactic features are effective for scope detection. 
 
 
                                                 
2 http://code.google.com/p/berkeleyparser 
3 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://svmlight.joachims.org 
973
Negation 
Features P R F PCS
Basic 89.89 68.72 77.86 39.50
Con.  85.72 67.80 75.66 41.81
Dep.  90.31 69.01 78.19 40.08
Bas.&Con.  88.86 79.07 83.61 51.64
Bas.&Dep. 90.44 73.62 81.17 49.36
All 91.21 76.57 83.25 52.98
Speculation 
Features P R F PCS
Basic 89.67 86.86 88.24 40.09
Con.  96.43 87.46 91.72 66.57
Dep.  90.84 87.04 88.89 44.45
Bas.&Con.  95.66 92.08 93.83 69.59
Bas.&Dep. 92.39 88.27 90.28 67.49
All 95.71 92.09 93.86 70.55
(Note: ?Bas.? denotes basic features; ?Con.? denotes Constituent 
features; ?Dep.? denotes Dependency features; ?All? contains Basic, 
Constituent, and Dependency features being selected.) 
Table 6. Performance of flat syntactic features. 
The results also show that the speculative scope 
detection achieves higher performance (16.98% 
higher in PCS) (?2; p < 0.01) than the negation 
scope detection. The main reason is that although 
the average sentence length of negation and specu-
lation are comparable (29.97 vs. 29.39 words, in 
Table 1), the average length of speculation scopes 
is much longer than the negation (17.24 vs. 9.62 
words, in Table 1) in Abstracts sub-corpus. With 
the shorter scopes in training data, the classifier 
inevitably have more negative samples. Thus, by 
using a token as the basic unit in our classification, 
the imbalanced samples will seriously mislead the 
classifier and result in bias on the negative samples. 
In addition, both constituent and dependency 
flat features can improve the scope classification, 
for the reason that the constituent features usually 
provide the nearer syntactic information of the 
cues, and that the further syntactic information 
between cues and scopes have been obtained by 
the dependency features. 
5.3 Results on Structured Syntactic Parse 
Features 
Table 7 and Table 8 give the scope detection per-
formance using the different structured syntactic 
parse features on negation and speculation respec-
tively. Compared to the optimal system (using all 
of the selected flat features in Table 6) in Section 
5.2, the structured syntactic parse features at best 
improve the scope classification nearly 17.29% on 
negation (PCS=70.27%) and 12.32% on specula-
tion (PCS=82.87%) (?2; p < 0.01). It indicates that 
the structured syntactic parse features can provide 
more implicit linguistic information, as supple-
mentary clues, to support scope classification. 
The improvements also show that both the com-
pleted syntactic sub-trees and critical paths in con-
stituent and dependency parsing trees are effective. 
The reason is that the completed syntactic sub-
trees contain the surrounding information related 
to cues and tokens, while there are more direct 
syntactic information in the critical paths between 
cue and its scope. 
Features P R F PCS 
Con. CT 91.12 83.25 86.89 54.57 
Con. CT&CP 93.31 89.32 91.20 66.58 
Dep. T1 CT 87.29 84.37 85.81 53.07 
Dep. T1 CT&CP 90.03 86.77 88.37 59.53 
Dep. T2 CT 88.17 84.58 86.34 53.76 
Dep. T2 CT&CP 91.09 87.31 89.16 60.11 
All 93.84 91.94 92.88 70.27 
(Note: ?Con.? denotes Constituent features; ?Dep.? denotes Depend-
ency features; ?T1? use the transformational rule (1) in Section 4.2 to 
get the dependency tree; ?T2? use the transformational rule (2) in 
Section 4.2 to get the dependency tree; CT-?Completed syntactic sub-
Tree?; CP-?Critical Path?; ?All? contains Con CT&CP, Dep T1 
CT&CP and Dep T2 CT&CP) 
Table 7. Performance of structured syntactic parse 
features on negation. 
Features P R F PCS 
Con. CT 95.89 93.37 94.61 75.17 
Con. CT&CP 96.05 94.36 95.20 76.73 
Dep. T1 CT 93.24 90.77 91.99 72.31 
Dep. T1 CT&CP 94.28 92.30 93.28 73.75 
Dep. T2 CT 93.76 89.68 91.67 73.06 
Dep. T2 CT&CP 95.29 94.55 94.92 75.69 
All 96.93 96.86 96.89 82.87 
Table 8. Performance of structured syntactic parse 
features on speculation. 
5.4 Results on Part-of-Speech Based Classifi-
cation 
To confirm the assumption in Section 4.3, we have 
built a discriminative classifier for each kind of 
POS of cues. Considering that the features involv-
ing the global structured syntactic parse infor-
mation in Section 4.2 are almost effective to all 
instances, we only use the flat syntactic features in 
Section 4.1. 
Negation
System P R F PCS
All Features 91.21 76.57 83.25 52.98
POS Classifier 91.79 78.29 84.50 56.77
Specula-
tion 
System P R F PCS
All Features  95.71 92.09 93.86 70.55
POS Classifier 95.79 93.13 94.44 71.68
(Note: ?All Features? System is the optimal system in Section 5.2) 
Table 9. Performances of POS based classification. 
Table 9 shows the performance of POS based 
classification. Compared with the system which 
only uses one classifier for all cues in Section 5.2, 
974
the POS based classification improves 1.13% on 
PCS (?2; p < 0.01), as different POS kinds of cues 
involve respectively effective features with more 
related clues between cue and its scope. 
Table 10 lists the performance of each POS kind 
of cues in speculation scope classification. There 
are still some low performances in some kinds of 
POS of cues. We consider it caused by two reasons. 
Firstly, some kinds of POS of cues  (e.g. NN etc.) 
have fewer samples (just 43 samples shown in Ta-
ble 5). For this reason, the training for classifier is 
limited. Then, for these low performance kinds of 
POS of cues, we may have not found the effective 
features for them. Although there are some kinds 
of cues with low performance, the whole perfor-
mance of part-of-speech based classification is 
improved. 
Cue?s 
POS 
B1~B6 
  1   2   3   4   5   6 
CS1~CS10 
1   2   3   4    5   6   7   8   9 10 
DS1~DS5 
1   2   3   4   5 PCS 
CC ?    ? ?    ? ?   ? ? ? ? ? 38.45
IN ?    ? ?    ? ? ?  ? ?  ? ? ? 87.99
JJ ?     ?    ?  ?  ?    ? 31.83
MD    ? ? ?  ?  ?  ? ? ? ? ?  ? ? 79.84
NN ?     ? ? ?   ?   ?  ?  ? 65.83
RB      ? ? ?   ? ?    ?    ? 37.03
VB      ? ?       ? ?   44.29
VBD    ? ?    ? ? ? ? ? ? ? ?  ? 63.57
VBG    ? ?    ? ? ?  ? ? ? ?  ? ? 82.89
VBN    ? ?    ?  ?  ? ? ? ?  ? 66.38
VBP    ? ? ? ?   ? ?  ? ? ? ? ?  ? 81.91
VBZ    ? ? ? ?   ? ?  ? ? ? ? ?  ? 77.16
Table 10. Performance of each POS kind of cues 
in speculation scope classification. 
5.5 Results of Comparison Experiments 
To get the final performance of our approach, we 
train the classifiers respectively by different effec-
tive features in Section 4.1 for POS kinds of cues, 
and use the structured syntactic parse features in 
Section 4.2 on Abstracts sub-corpus by performing 
10-fold cross validation. 
Negation 
System Abstract Paper Clinical
Morante (2008) 57.33 N/A N/A 
Morante (2009a) 73.36 50.26 87.27 
Ours 76.90 61.19 85.31 
Specula-
tion 
System Abstract Paper Clinical
Morante (2009b) 77.13 47.94 60.59 
?zg?r (2009) 79.89 61.13 N/A 
Ours 84.21 67.24 72.92 
Table 11. Performance comparison of our system 
with the state-of-the-art ones in PCS. 
The results in Table 11 show that our system 
outperforms the state of the art ones both on nega-
tion and speculation scope detection. Results also 
show that the system is portable to different types 
of documents, although performance varies de-
pending on the characteristics of the corpus. 
In addition, on both negation and speculation, 
the results on Clinical Reports sub-corpus are bet-
ter than those on Full Papers sub-corpus. It is 
mainly due to that the clinical reports are easier to 
process than full papers and abstracts. The average 
length of sentence for negative clinical reports is 
8.19 tokens, whereas for abstracts it is 29.39 and 
for full papers 30.49. Shorter sentences imply 
shorter scopes. The more unambiguous sentence 
structure of short sentence can make the structured 
constituent and dependency syntactic features eas-
ier to be processed. 
6 Conclusion 
This paper proposes a new approach for tree ker-
nel-based scope detection by using the structured 
syntactic parse information. In particular, we have 
explored the way of selecting compatible features 
for different part-of-speech cues. Experiments 
show substantial improvements of our scope clas-
sification and better robustness. 
However, the results on the Full Papers and the 
Clinical Reports sub-corpora are lower than those 
on the Abstracts sub-corpus for both negation and 
speculation. That is because the structured syntac-
tic parse features contain some complicated and 
lengthy components, and the flat features cross 
corpus are sparse. Our future work will focus on 
the pruning algorithm for the syntactic structures 
and analyzing errors in depth in order to get more 
effective features for the scope detection on differ-
ent corpora. 
Acknowledgments 
This research is supported by the National Natural 
Science Foundation of China, No.61272260, 
No.61373097, No.61003152, the Natural Science 
Foundation of Jiangsu Province, No.BK2011282, 
the Major Project of College Natural Science 
Foundation of Jiangsu Province, No.11KJA520003 
and the Graduates Project of Science and Innova-
tion, No.CXZZ12_0818. Besides, thanks to Yu 
Hong and the three anonymous reviewers for their 
valuable comments on an earlier draft. 
 
975
References  
Emilia Apostolova, Noriko Tomuro and Dina Demner-
Fushman. 2011. Automatic Extraction of Lexico-
Syntactic Patterns for Detection of Negation and 
Speculation Scopes. In Proceedings of ACL-HLT 
short papers, pages 283-287. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. A 
Simple Algorithm for Identifying Negated Findings 
and Diseases in Discharge Summaries. Journal of 
Biomedical Informatics, 34 (5): 301-310. 
Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 
2006. A Hybrid Convolution Tree Kernel for Seman-
tic Role Labeling. In Proceedings of ACL, pages 73-
80. 
Nigel Collier, Hyun S. Park, Norihiro Ogata, et al 1999. 
The GENIA Project: Corpus-Based Knowledge Ac-
quisition and Information Extraction from Genome 
Research Papers. In Proceedings of EACL. 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, J?nos 
Csirik, and Gy?rgy Szarvas. 2010. The CoNLL-2010 
Shared Task: Learning to Detect Hedges and their 
Scope in Natural Language Text. In Proceedings of 
CoNLL: Shared Task, pages 1-12. 
Ilya M. Goldin and Wendy W. Chapman. 2003. Learn-
ing to Detect Negation with ?Not? in Medical Texts. 
In SIGIR Workshop: Text Analysis and Search for 
Bioinformatics. 
Yang Huang and Henry Lowe. 2007. A Novel Hybrid 
Approach to Automated Negation Detection in Clin-
ical Radiology Reports. Journal of the American 
Medical Informatics Association, 14(3):304-311. 
Zhengping Jiang and Hwee T. Ng. 2006. Semantic Role 
Labeling of NomBank: A Maximum Entropy Ap-
proach. In Proceedings of EMNLP, pages 138-145. 
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the Scope of Negation in Bio-
medical Texts. In Proceedings of EMNLP, pages 
715-724. 
Roser Morante and Walter Daelemans. 2009a. A Met-
alearning Approach to Processing the Scope of Ne-
gation. In Proceedings of CoNLL, pages 21-29. 
Roser Morante and Walter Daelemans. 2009b. Learning 
the Scope of Hedge Cues in Biomedical Texts. In 
Proceedings of the BioNLP Workshop, pages 28-36. 
Roser Morante, Vincent Van Asch and Walter Daele-
mans. 2010. Memory-Based Resolution of In-
Sentence Scopes of Hedge Cues. In Proceedings of 
CoNLL Shared Task, pages 40-47. 
Alessandro Moschitti. 2006. Making tree kernels practi-
cal for natural language learning. In Proceedings of 
the 11th Conference of the European Chapter of the 
Association for Computational Linguistics, pages 
113-120. 
Lilja ?vrelid, Erik Velldal, and Stephan Oepen. 2010. 
Syntactic Scope Resolution in Uncertainty Analysis. 
In Proceedings of COLING, pages 1379-1387. 
Arzucan ?zg?r and Dragomir R. Radev. 2009. Detect-
ing Speculations and their Scopes in Scientific Text. 
In Proceedings of EMNLP, pages 1398-1407. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. In Proceedings of NAACL, 
pages 404-411. 
Liliana M. S?nchez, Baoli Li, Carl Vogel. 2007. Ex-
ploiting CCG Structures with Tree Kernels for Spec-
ulation Detection. In Proceedings of the Fourteenth 
Conference on Computational Natural Language 
Learning: Shared Task, pages 126-131. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, and 
J?nos Csirik. 2008. The BioScope corpus: Annota-
tion for Negation, Uncertainty and their Scope in Bi-
omedical Texts. In Proceedings of BioNLP, pages 
38-45. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun?ichi Tsujii. 2005. Syntax Annotation for the 
GENIA Corpus. In Proceedings of IJCNLP, Com-
panion volume, pages 222-227. 
Erik Velldal, Lilja ?vrelid, Jonathon Read and Stephan 
Oepen. 2012. Speculation and Negation: Rules, 
Rankers, and the Role of Syntax. Computational 
Linguistics, 38(2):369-410. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra and J?nos Csirik. 2008. The BioScope 
corpus: biomedical texts annotated for uncertainty, 
negation and their scopes. BMC Bioinformatics, 
9(Suppl 11):S9. 
Guodong Zhou, Min Zhang, Donghong Ji, and Qi-
aoming Zhu. 2007. Tree Kernel-based Relation Ex-
traction with Context-Sensitive Structured Parse 
Tree Information. In Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages, 728-736. 
 
976
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 68?77,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Constituent-Based Approach to Argument Labeling
with Joint Inference in Discourse Parsing
Fang Kong
1?
Hwee Tou Ng
2
Guodong Zhou
1
1
School of Computer Science and Technology, Soochow University, China
2
Department of Computer Science, National University of Singapore
kongfang@suda.edu.cn nght@comp.nus.edu.sg gdzhou@suda.edu.cn
Abstract
Discourse parsing is a challenging task
and plays a critical role in discourse anal-
ysis. In this paper, we focus on label-
ing full argument spans of discourse con-
nectives in the Penn Discourse Treebank
(PDTB). Previous studies cast this task
as a linear tagging or subtree extraction
problem. In this paper, we propose a
novel constituent-based approach to argu-
ment labeling, which integrates the ad-
vantages of both linear tagging and sub-
tree extraction. In particular, the pro-
posed approach unifies intra- and inter-
sentence cases by treating the immediate-
ly preceding sentence as a special con-
stituent. Besides, a joint inference mech-
anism is introduced to incorporate glob-
al information across arguments into our
constituent-based approach via integer lin-
ear programming. Evaluation on PDT-
B shows significant performance improve-
ments of our constituent-based approach
over the best state-of-the-art system. It al-
so shows the effectiveness of our joint in-
ference mechanism in modeling global in-
formation across arguments.
1 Introduction
Discourse parsing determines the internal struc-
ture of a text and identifies the discourse rela-
tions between its text units. It has attracted in-
creasing attention in recent years due to its impor-
tance in text understanding, especially since the
release of the Penn Discourse Treebank (PDTB)
corpus (Prasad et al., 2008), which adds a layer of
discourse annotations on top of the Penn Treebank
?
The research reported in this paper was carried out while
Fang Kong was a research fellow at the National University
of Singapore.
(PTB) corpus (Marcus et al., 1993). As the largest
available discourse corpus, the PDTB corpus has
become the defacto benchmark in recent studies
on discourse parsing.
Compared to connective identification and dis-
course relation classification in discourse parsing,
the task of labeling full argument spans of dis-
course connectives is much harder and thus more
challenging. For connective identification, Lin et
al. (2014) achieved the performance of 95.76%
and 93.62% in F-measure using gold-standard and
automatic parse trees, respectively. For discourse
relation classification, Lin et al. (2014) achieved
the performance of 86.77% in F-measure on clas-
sifying discourse relations into 16 level 2 types.
However, for argument labeling, Lin et al. (2014)
only achieved the performance of 53.85% in F-
measure using gold-standard parse trees and con-
nectives, much lower than the inter-annotation a-
greement of 90.20% (Miltsakaki et al., 2004).
In this paper, we focus on argument labeling in
the PDTB corpus. In particular, we propose a nov-
el constituent-based approach to argument label-
ing which views constituents as candidate argu-
ments. Besides, our approach unifies intra- and
inter-sentence cases by treating the immediately
preceding sentence as a special constituent. Final-
ly, a joint inference mechanism is introduced to
incorporate global information across arguments
via integer linear programming. Evaluation on the
PDTB corpus shows the effectiveness of our ap-
proach.
The rest of this paper is organized as follows.
Section 2 briefly introduces the PDTB corpus.
Related work on argument labeling is reviewed
in Section 3. In Section 4, we describe our
constituent-based approach to argument labeling.
In Section 5, we present our joint inference mech-
anism via integer linear programming (ILP). Sec-
tion 6 gives the experimental results and analysis.
Finally, we conclude in Section 7.
68
2 Penn Discourse Treebank
As the first large-scale annotated corpus that fol-
lows the lexically grounded, predicate-argument
approach in D-LTAG (Lexicalized Tree Adjoin-
ing Grammar for Discourse) (Webber, 2004), the
PDTB regards a connective as the predicate of a
discourse relation which takes exactly two text s-
pans as its arguments. In particular, the text span
that the connective is syntactically attached to is
called Arg2, and the other is called Arg1.
Although discourse relations can be either ex-
plicitly or implicitly expressed in PDTB, this pa-
per focuses only on explicit discourse relations
that are explicitly signaled by discourse connec-
tives. Example (1) shows an explicit discourse re-
lation from the article wsj 2314 with connective
so underlined, Arg1 span italicized, and Arg2 s-
pan bolded.
(1) But its competitors have much broader busi-
ness interests and so are better cushioned
against price swings .
Note that a connective and its arguments can ap-
pear in any relative order, and an argument can be
arbitrarily far away from its corresponding con-
nective. Although the position of Arg2 is fixed
once the connective is located, Arg1 can occur in
the same sentence as the connective (SS), in a sen-
tence preceding that of the connective (PS), or in
a sentence following that of the connective (FS),
with proportions of 60.9%, 39.1%, and less than
0.1% respectively for explicit relations in the PDT-
B corpus (Prasad et al., 2008). Besides, out of
all PS cases where Arg1 occurs in some preced-
ing sentence, 79.9% of them are the exact imme-
diately preceding sentence. As such, in this paper,
we only consider the current sentence containing
the connective and its immediately preceding sen-
tence as the text span where Arg1 occurs, similar
to what was done in (Lin et al., 2014).
3 Related Work
For argument labeling in discourse parsing on the
PDTB corpus, the related work can be classified
into two categories: locating parts of arguments,
and labeling full argument spans.
As a representative on locating parts of argu-
ments, Wellner and Pustejovsky (2007) proposed
several machine learning approaches to identify
the head words of the two arguments for discourse
connectives. Following this work, Elwell and
Baldridge (2008) combined general and connec-
tive specific rankers to improve the performance
of labeling the head words of the two arguments.
Prasad et al. (2010) proposed a set of heuristics to
locate the position of the Arg1 sentences for inter-
sentence cases. The limitation of locating parts of
arguments, such as the positions and head word-
s, is that it is only a partial solution to argument
labeling in discourse parsing.
In comparison, labeling full argument spans can
provide a complete solution to argument labeling
in discourse parsing and has thus attracted increas-
ing attention recently, adopting either a subtree
extraction approach (Dinesh et al. (2005), Lin et
al. (2014)) or a linear tagging approach (Ghosh et
al. (2011)).
As a representative subtree extraction approach,
Dinesh et al. (2005) proposed an automatic tree
subtraction algorithm to locate argument spans for
intra-sentential subordinating connectives. How-
ever, only dealing with intra-sentential subordinat-
ing connectives is not sufficient since they con-
stitute only 40.93% of all cases. Instead, Lin et
al. (2014) proposed a two-step approach. First, an
argument position identifier was employed to lo-
cate the position of Arg1. For the PS case, it di-
rectly selects the immediately preceding sentence
as Arg1. For other cases, an argument node iden-
tifier was employed to locate the Arg1- and Arg2-
nodes. Next, a tree subtraction algorithm was used
to extract the arguments. However, as pointed out
in Dinesh et al. (2005), it is not necessarily the
case that a connective, Arg1, or Arg2 is dominated
by a single node in the parse tree (that is, it can be
dominated by a set of nodes). Figure 1 shows the
gold-standard parse tree corresponding to Exam-
ple (1). It shows that Arg1 includes three nodes:
[
CC
But], [
NP
its competitors], [
V P
have much
broader business interests], and Arg2 includes t-
wo nodes: [
CC
and], [
V P
are better cushioned a-
gainst price swings]. Therefore, such an argumen-
t node identifier has inherent shortcomings in la-
beling arguments. Besides, the errors propagat-
ed from the upstream argument position classifier
may adversely affect the performance of the down-
stream argument node identifier.
As a representative linear tagging approach,
Ghosh et al. (2011) cast argument labeling as a lin-
ear tagging task using conditional random fields.
Ghosh et al. (2012) further improved the perfor-
69
SVP
VP
VP
PP
NP
NNS
swings
NN
price
IN
against
VBN
cushioned
ADVP
RBR
better
VBP
are
RB
so
CC
and
VP
NP
NNS
interests
NN
business
ADJP
JJR
broader
RB
much
VBP
have
NP
NNS
competitors
PRP
its
CC
But
Figure 1: The gold-standard parse tree corresponding to Example (1)
mance with integration of the n-best results.
While the subtree extraction approach locates
argument spans based on the nodes of a parse tree
and is thus capable of using rich syntactic informa-
tion, the linear tagging approach works on the to-
kens in a sentence and is thus capable of capturing
local sequential dependency between tokens. In
this paper, we take advantage of both subtree ex-
traction and linear tagging approaches by propos-
ing a novel constituent-based approach. Further-
more, intra- and inter-sentence cases are unified
by treating the immediately preceding sentence as
a special constituent. Finally, a joint inference
mechanism is proposed to add global information
across arguments.
4 A Constituent-Based Approach to
Argument Labeling
Our constituent-based approach works by first
casting the constituents extracted from a parse tree
as argument candidates, then determining the role
of every constituent as part of Arg1, Arg2, or
NULL, and finally, merging all the constituents
for Arg1 and Arg2 to obtain the Arg1 and Arg2
text spans respectively. Obviously, the key to
the success of our constituent-based approach is
constituent-based argument classification, which
determines the role of every constituent argument
candidate.
As stated above, the PDTB views a connective
as the predicate of a discourse relation. Similar
to semantic role labeling (SRL), for a given con-
nective, the majority of the constituents in a parse
tree may not be its arguments (Xue and Palmer,
2004). This indicates that negative instances (con-
stituents marked NULL) may overwhelm positive
instances. To address this problem, we use a
simple algorithm to prune out these constituents
which are clearly not arguments to the connective
in question.
4.1 Pruning
The pruning algorithm works recursively in pre-
processing, starting from the target connective n-
ode, i.e. the lowest node dominating the connec-
tive. First, all the siblings of the connective node
are collected as candidates, then we move on to
the parent of the connective node and collect it-
s siblings, and so on until we reach the root of
the parse tree. In addition, if the target connec-
tive node does not cover the connective exactly,
the children of the target connective node are also
collected.
For the example shown in Figure 1, we can lo-
cate the target connective node [
RB
so] and return
five constituents ? [
V P
have much broader busi-
ness interests], [
CC
and], [
V P
are better cushioned
against price swings], [
CC
But], and [
NP
its com-
petitors] ? as argument candidates.
It is not surprising that the pruning algorithm
works better on gold parse trees than automatic
parse trees. Using gold parse trees, our pruning al-
gorithm can recall 89.56% and 92.98% (489 out of
546 Arg1s, 808 out of 869 Arg2s in the test data)
of the Arg1 and Arg2 spans respectively and prune
out 81.96% (16284 out of 19869) of the nodes in
the parse trees. In comparison, when automatic
parse trees (based on the Charniak parser (Char-
niak, 2000)) are used, our pruning algorithm can
recall 80.59% and 89.87% of the Arg1 and Arg2
spans respectively and prune out 81.70% (16190
70
Feature Description Example
CON-Str The string of the given connective (case-sensitive) so
CON-LStr The lowercase string of the given connective so
CON-Cat
The syntactic category of the given connective: sub-
ordinating, coordinating, or discourse adverbial
Subordinating
CON-iLSib Number of left siblings of the connective 2
CON-iRSib Number of right siblings of the connective 1
NT-Ctx
The context of the constituent. We use POS combi-
nation of the constituent, its parent, left sibling and
right sibling to represent the context. When there is
no parent or siblings, it is marked NULL.
VP-VP-NULL-CC
CON-NT-Path
The path from the parent node of the connective to
the node of the constituent
RB ? V P ? V P
CON-NT-Position
The position of the constituent relative to the connec-
tive: left, right, or previous
left
CON-NT-Path-iLsib
The path from the parent node of the connective to
the node of the constituent and whether the number
of left siblings of the connective is greater than one
RB ? V P ? V P :>1
Table 1: Features employed in argument classification.
out of 19816) of the nodes in the parse trees.
4.2 Argument Classification
In this paper, a multi-category classifier is em-
ployed to determine the role of an argument can-
didate (i.e., Arg1, Arg2, or NULL). Table 1 lists
the features employed in argument classification,
which reflect the properties of the connective and
the candidate constituent, and the relationship be-
tween them. The third column of Table 1 shows
the features corresponding to Figure 1, consider-
ing [
RB
so] as the given connective and [
V P
have
much broader business interests] as the constituent
in question.
Similar to Lin et al. (2014), we obtained the syn-
tactic category of the connectives from the list pro-
vided in Knott (1996). However, different from
Lin et al. (2014), only the siblings of the root path
nodes (i.e., the nodes occurring in the path of the
connective to root) are collected as the candidate
constituents in the pruning stage, and the value of
the relative position can be left or right, indicat-
ing that the constituent is located on the left- or
right-hand of the root path respectively. Besides,
we view the root of the previous sentence as a spe-
cial candidate constituent. For example, the value
of the feature CON-NT-Position is previous when
the current constituent is the root of the previous
sentence. Finally, we use the part-of-speech (POS)
combination of the constituent itself, its parent n-
ode, left sibling node and right sibling node to rep-
resent the context of the candidate constituent. In-
tuitively, this information can help determine the
role of the constituent.
For the example shown in Figure 1, we first em-
ploy the pruning algorithm to get the candidate
constituents, and then employ our argument clas-
sifier to determine the role for every candidate.
For example, if the five candidates are labeled as
Arg1, Arg2, Arg2, Arg1, and Arg1, respectively,
we merge all the Arg1 constituents to obtain the
Arg1 text span (i.e., But its competitors have much
broader business interests). Similarly, we merge
the two Arg2 constituents to obtain the Arg2 text s-
pan (i.e., and are better cushioned against price
swings).
5 Joint Inference via Integer Linear
Programming
In the above approach, decisions are always made
for each candidate independently, ignoring global
information across candidates in the final output.
For example, although an argument span can be
split into multiple discontinuous segments (e.g.,
the Arg2 span of Example (1) contains two dis-
continuous segments, and, are better cushioned
against price swings), the number of discontinu-
ous segments is always limited. Statistics on the
PDTB corpus shows that the number of discontin-
71
uous segments for both Arg1 and Arg2 is generally
(>= 99%) at most 2. For Example (1), from left
to right, we can obtain the list of constituent can-
didates: [
CC
But], [
NP
its competitors], [
V P
have
much broader business interests], [
CC
and], [
V P
are better cushioned against price swings]. If our
argument classifier wrongly determines the roles
as Arg1, Arg2, Arg1, Arg2, and Arg1 respectively,
we can find that the achieved Arg1 span contains
three discontinuous segments. Such errors may be
corrected from a global perspective.
In this paper, a joint inference mechanism is in-
troduced to incorporate various kinds of knowl-
edge to resolve the inconsistencies in argumen-
t classification to ensure global legitimate predic-
tions. In particular, the joint inference mechanism
is formalized as a constrained optimization prob-
lem, represented as an integer linear programming
(ILP) task. It takes as input the argument classi-
fiers? confidence scores for each constituent can-
didate along with a list of constraints, and outputs
the optimal solution that maximizes the objective
function incorporating the confidence scores, sub-
ject to the constraints that encode various kinds of
knowledge.
In this section, we meet the requirement of ILP
with focus on the definition of variables, the objec-
tive function, and the problem-specific constraints,
along with ILP-based joint inference integrating
multiple systems.
5.1 Definition of Variables
Given an input sentence, the task of argumen-
t labeling is to determine what labels should be
assigned to which constituents corresponding to
which connective. It is therefore natural that en-
coding the output space of argument labeling re-
quires various kinds of information about the con-
nectives, the argument candidates corresponding
to a connective, and their argument labels.
Given an input sentence s, we define following
variables:
(1) P : the set of connectives in a sentence.
(2) p ? P : a connective in P .
(3) C(p): the set of argument candidates corre-
sponding to connective p. (i.e., the parse tree
nodes obtained in the pruning stage).
(4) c ? C(p): an argument candidate.
(5) L: the set of argument labels {Arg1, Arg2,
NULL }.
(6) l ? L: an argument label in L.
In addition, we define the integer variables as
follows:
Z
l
c,p
? {0, 1} (1)
If Z
l
c,p
= 1, the argument candidate c, which
corresponds to connective p, should be assigned
the label l. Otherwise, the argument candidate c is
not assigned this label.
5.2 The Objective Function
The objective of joint inference is to find the best
arguments for all the connectives in one sentence.
For every connective, the pruning algorithm is first
employed to determine the set of corresponding
argument candidates. Then, the argument classifi-
er is used to assign a label to every candidate. For
an individual labeling Z
l
c,p
, we measure the quality
based on the confidence scores, f
l,c,p
, returned by
the argument classifier. Thus, the objective func-
tion can be defined as
max
?
l,c,p
f
l,c,p
Z
l
c,p
(2)
5.3 Constraints
As the key to the success of ILP-based joint infer-
ence, the following constraints are employed:
Constraint 1: The arguments corresponding
to a connective cannot overlap with the connec-
tive. Let c
1
, c
2
..., c
k
be the argument candidates
that correspond to the same connective and over-
lap with the connective in a sentence.
1
Then this
constraint ensures that none of them will be as-
signed as Arg1 or Arg2.
k
?
i=1
Z
NULL
c
i
,p
= k (3)
Constraint 2: There are no overlapping or em-
bedding arguments. Let c
1
, c
2
..., c
k
be the argu-
ment candidates that correspond to the same con-
nective and cover the same word in a sentence.
2
1
Only when the target connective node does not cover the
connective exactly and our pruning algorithm collects all the
children of the target connective node as part of constituent
candidates, such overlap can be introduced.
2
This constraint only works in system combination of
Section 5.4, where additional phantom candidates may intro-
duce such overlap.
72
Then this constraint ensures that at most one of
the constituents can be assigned as Arg1 or Arg2.
That is, at least k ? 1 constituents should be as-
signed the special label NULL.
k
?
i=1
Z
NULL
c
i
,p
? k ? 1 (4)
Constraint 3: For a connective, there is at least
one constituent candidate assigned as Arg2.
?
c
Z
Arg2
c,p
? 1 (5)
Constraint 4: Since we view the previous com-
plete sentence as a special Arg1 constituent candi-
date, denoted as m, there is at least one candidate
assigned as Arg1 for every connective.
?
c
Z
Arg1
c,p
+ Z
Arg1
m,p
? 1 (6)
Constraint 5: The number of discontinuous
constituents assigned as Arg1 or Arg2 should be at
most 2. That is, if argument candidates c
1
, c
2
..., c
k
corresponding to the same connective are discon-
tinuous, this constraint ensures that at most two
of the constituents can be assigned the same label
Arg1 or Arg2.
k
?
i=1
Z
Arg1
c
i
,p
? 2, and
k
?
i=1
Z
Arg2
c
i
,p
? 2 (7)
5.4 System Combination
Previous work shows that the performance of ar-
gument labeling heavily depends on the quality of
the syntactic parser. It is natural that combining
different argument labeling systems on differen-
t parse trees can potentially improve the overall
performance of argument labeling.
To explore this potential, we build two argu-
ment labeling systems ? one using the Berke-
ley parser (Petrov et al., 2006) and the other the
Charniak parser (Charniak, 2000). Previous s-
tudies show that these two syntactic parsers tend
to produce different parse trees for the same sen-
tence (Zhang et al., 2009). For example, our pre-
liminary experiment shows that applying the prun-
ing algorithm on the output of the Charniak parser
produces a list of candidates with recall of 80.59%
and 89.87% for Arg1 and Arg2 respectively, while
achieving recall of 78.6% and 91.1% for Arg1 and
Arg2 respectively on the output of the Berkeley
	






	
 













Figure 2: An example on unifying different candi-
dates.
parser. It also shows that combining these two can-
didate lists significantly improves recall to 85.7%
and 93.0% for Arg1 and Arg2, respectively.
In subsection 5.2, we only consider the con-
fidence scores returned by an argument classifier.
Here, we proceed to combine the probabilities pro-
duced by two argument classifiers. There are two
remaining problems to resolve:
? How do we unify the two candidate lists?
In principle, constituents spanning the same
sequence of words should be viewed as the
same candidate. That is, for different can-
didates, we can unify them by adding phan-
tom candidates. This is similar to the ap-
proach proposed by Punyakanok et al. (2008)
for the semantic role labeling task. For exam-
ple, Figure 2 shows the candidate lists gen-
erated by our pruning algorithm based on t-
wo different parse trees given the segment
?its competitors have much broader business
interests?. Dashed lines are used for phan-
tom candidates and solid lines for true can-
didates. Here, system A produces one can-
didate a1, with two phantom candidates a2
and a3 added. Analogously, phantom can-
didate b3 is added to the candidate list out-
put by System B. In this way, we can get the
unified candidate list: ?its competitors have
much broader business interests?, ?its com-
petitors?, ?have much broader business inter-
ests?.
? How do we compute the confidence score for
every decision? For every candidate in the
unified list, we first determine whether it is
a true candidate based on the specific parse
tree. Then, for a true candidate, we extrac-
t the features from the corresponding parse
73
tree. On this basis, we can determine the
confidence score using our argument classi-
fier. For a phantom candidate, we set the
same prior distribution as the confidence s-
core. In particular, the probability of the
?NULL? class is set to 0.55, following (Pun-
yakanok et al., 2008), and the probabilities of
Arg1 and Arg2 are set to their occurrence fre-
quencies in the training data. For the example
shown in Figure 2, since System A return-
s ?its competitors have much broader busi-
ness interests? as a true candidate, we can ob-
tain its confidence score using our argumen-
t classifier. For the two phantom candidates
? ?its competitors? and ?have much broader
business interests? ? we use the prior dis-
tributions directly. This applies to the candi-
dates for System B. Finally, we simply aver-
age the estimated probabilities to determine
the final probability estimate for every candi-
date in the unified list.
6 Experiments
In this section, we systematically evaluate our
constituent-based approach with a joint inference
mechanism to argument labeling on the PDTB
corpus.
6.1 Experimental settings
All our classifiers are trained using the OpenNLP
maximum entropy package
3
with the default pa-
rameters (i.e. without smoothing and with 100
iterations). As the PDTB corpus is aligned with
the PTB corpus, the gold parse trees and sentence
boundaries are obtained from PTB. Under the au-
tomatic setting, the NIST sentence segmenter
4
and
the Charniak parser
5
are used to segment and parse
the sentences, respectively. lp solve
6
is used for
our joint inference.
This paper focuses on automatically labeling
the full argument spans of discourse connec-
tives. For a fair comparison with start-of-the-
art systems, we use the NUS PDTB-style end-
to-end discourse parser
7
to perform other sub-
tasks of discourse parsing except argument label-
ing, which includes connective identification, non-
3
http://maxent.sourceforge.net/
4
http://duc.nist.gov/duc2004/software/duc2003
.breakSent.tar.gz
5
ftp://ftp.cs.brown.edu/pub/nlparser/
6
http://lpsolve.sourceforge.net/
7
http://wing.comp.nus.edu.sg/ linzihen/parser/
explicit discourse relation identification and clas-
sification.
Finally, we evaluate our system on two aspects:
(1) the dependence on the parse trees (GS/Auto,
using gold standard or automatic parse trees and
sentence boundaries); and (2) the impact of errors
propagated from previous components (noEP/EP,
using gold annotation or automatic results from
previous components). In combination, we have
four different settings: GS+noEP, GS+EP, Au-
to+noEP and Auto+EP. Same as Lin et al. (2014),
we report exact match results under these four set-
tings. Here, exact match means two spans match
identically, except beginning or ending punctua-
tion symbols.
6.2 Experimental results
We first evaluate the effectiveness of our
constituent-based approach by comparing our sys-
tem with the state-of-the-art systems, ignoring
the joint inference mechanism. Then, the con-
tribution of the joint inference mechanism to our
constituent-based approach, and finally the contri-
bution of our argument labeling system to the end-
to-end discourse parser are presented.
Effectiveness of our constituent-based ap-
proach
By comparing with two state-of-the-art argu-
ment labeling approaches, we determine the effec-
tiveness of our constituent-based approach.
Comparison with the linear tagging approach
As a representative linear tagging approach,
Ghosh et al. (2011; 2012; 2012) only reported the
exact match results for Arg1 and Arg2 using the
evaluation script for chunking evaluation
8
under
GS+noEP setting with Section 02?22 of the PDTB
corpus for training, Section 23?24 for testing, and
Section 00?01 for development. It is also worth
mentioning that an argument span can contain
multiple discontinuous segments (i.e., chunks), so
chunking evaluation only shows the exact match
of every argument segment but not the exact match
of every argument span. In order to fairly compare
our system with theirs, we evaluate our system us-
ing both the exact metric and the chunking eval-
uation. Table 2 compares the results of our sys-
tem without joint inference and the results report-
ed by Ghosh et al. (2012) on the same data split.
We can find that our system performs much bet-
8
http://www.cnts.ua.ac.be/conll2000/chunking/
conlleval.txt
74
ter than Ghosh?s on both Arg1 and Arg2, even on
much stricter metrics.
Systems Arg1 Arg2
ours using exact match 65.68 84.50
ours using chunking evaluation 67.48 88.08
reported by Ghosh et al. (2012) 59.39 79.48
Table 2: Performance (F1) comparison of our ar-
gument labeling approach with the linear tagging
approach as adopted in Ghosh et al. (2012)
Comparison with the subtree extracting ap-
proach
For a fair comparison, we also conduct our
experiments on the same data split of Lin et
al. (2014) with Section 02 to 21 for training, Sec-
tion 22 for development, and Section 23 for test-
ing. Table 3 compares our labeling system without
joint inference with Lin et al. (2014), a representa-
tive subtree extracting approach. From the results,
we find that the performance of our argument la-
beling system significantly improves under all set-
tings. This is because Lin et al. (2014) considered
all the internal nodes of the parse trees, whereas
the pruning algorithm in our approach can effec-
tively filter out those unlikely constituents when
determining Arg1 and Arg2.
Setting Arg1 Arg2
Arg1&2
ours
GS+noEP 62.84 84.07 55.69
GS+EP 61.46 81.30 54.31
Auto+EP 56.04 76.53 48.89
Lin?s
GS+noEP 59.15 82.23 53.85
GS+EP 57.64 79.80 52.29
Auto+EP 47.68 70.27 40.37
Table 3: Performance (F1) comparison of our ar-
gument labeling approach with the subtree extrac-
tion approach as adopted in Lin et al. (2014)
As justified above, by integrating the advan-
tages of both linear tagging and subtree extraction,
our constituent-based approach can capture both
rich syntactic information from parse trees and
local sequential dependency between tokens. The
results show that our constituent-based approach
indeed significantly improves the performance
of argument labeling, compared to both linear
tagging and subtree extracting approaches.
Contribution of Joint Inference
Same as Lin et al. (2014), we conduct our ex-
periments using Section 02 to 21 for training, Sec-
tion 22 for development, and Section 23 for test-
ing. Table 4 lists the performance of our argumen-
t labeling system without and with ILP inference
under four different settings, while Table 5 reports
the contribution of system combination. It shows
the following:
? On the performance comparison of Arg1 and
Arg2, the performance on Arg2 is much bet-
ter than that on Arg1 with the performance
gap up to 8% under different settings. This is
due to the fact that the relationship between
Arg2 and the connective is much closer. This
result is also consistent with previous studies
on argument labeling.
? On the impact of error propagation from con-
nective identification, the errors propagated
from connective identification reduce the per-
formance of argument labeling by less than
2% in both Arg1 and Arg2 F-measure under
different settings.
? On the impact of parse trees, using automat-
ic parse trees reduces the performance of ar-
gument labeling by about 5.5% in both Arg1
and Arg2 F-measure under different settings.
In comparison with the impact of error prop-
agation, parse trees have much more impact
on argument labeling.
? On the impact of joint inference, it improves
the performance of argument labeling, espe-
cially on automatic parse trees by about 2%.
9
? On the impact of system combination, the
performance is improved by about 1.5%.
Setting Arg1 Arg2
Arg1&2
without
Joint
Inference
GS+noEP 62.84 84.07 55.69
GS+EP 61.46 81.30 54.31
Auto+noEP 57.75 79.85 50.27
Auto+EP 56.04 76.53 48.89
with
Joint
Inference
GS+noEP 65.76 83.86 58.18
GS+EP 63.96 81.19 56.37
Auto+noEP 60.24 79.74 52.55
Auto+EP 58.10 76.53 50.73
Table 4: Performance (F1) of our argument label-
ing approach.
Contribution to the end-to-end discourse pars-
er
9
Unless otherwise specified, all the improvements in this
paper are significant with p < 0.001.
75
Systems Setting Arg1 Arg2
Arg1&2
Charniak
noEP 60.24 79.74 52.55
EP 58.10 76.53 50.73
Berkeley
noEP 60.78 80.07 52.98
EP 58.80 77.21 51.43
Combined
noEP 61.97 80.61 54.50
EP 59.72 77.55 52.52
Table 5: Contribution of System Combination in
Joint Inference.
Lastly, we focus on the contribution of our ar-
gument labeling approach to the overall perfor-
mance of the end-to-end discourse parser. This
is done by replacing the argument labeling mod-
el of the NUS PDTB-style end-to-end discourse
parser with our argument labeling model. Table 6
shows the results using gold parse trees and auto-
matic parse trees, respectively.
10
From the results,
we find that using gold parse trees, our argument
labeling approach significantly improves the per-
formance of the end-to-end system by about 1.8%
in F-measure, while using automatic parse trees,
the improvement significantly enlarges to 6.7% in
F-measure.
Setting New d-parser Lin et al.?s (2014)
GS 34.80 33.00
Auto 27.39 20.64
Table 6: Performance (F1) of the end-to-end dis-
course parser.
7 Conclusion
In this paper, we focus on the problem of auto-
matically labeling the full argument spans of dis-
course connectives. In particular, we propose a
constituent-based approach to integrate the advan-
tages of both subtree extraction and linear tagging
approaches. Moreover, our proposed approach in-
tegrates inter- and intra-sentence argument label-
ing by viewing the immediately preceding sen-
tence as a special constituent. Finally, a join-
t inference mechanism is introduced to incorpo-
rate global information across arguments into our
10
Further analysis found that the error propagated from
sentence segmentation can reduce the performance of the
end-to-end discourse parser. Retraining the NIST sentence
segmenter using Section 02 to 21 of the PDTB corpus, the
original NUS PDTB-style end-to-end discourse parser can
achieve the performance of 25.25% in F-measure, while the
new version (i.e. replace the argument labeling model with
our argument labeling model) can achieve the performance
of 30.06% in F-measure.
constituent-based approach via integer linear pro-
gramming.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
This research is also partially supported by Key
project 61333018 and 61331011 under the Nation-
al Natural Science Foundation of China.
References
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the First Meeting
of the North American Chapter of the Association
for Computational Linguistics, pages 132?139.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005.
Attribution and the (non-)alignment of syntactic and
discourse arguments of connectives. In Proceedings
of the Workshop on Frontiers in Corpus Annotation
II: Pie in the Sky, pages 29?36.
Robert Elwell and Jason Baldridge. 2008. Discourse
connective argument identification with connective
specific rankers. In Second IEEE International Con-
ference on Semantic Computing, pages 198?205.
Sucheta Ghosh, Richard Johansson, Giuseppe Riccar-
di, and Sara Tonelli. 2011. Shallow discourse pars-
ing with conditional random fields. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing, pages 1071?1079.
Sucheta Ghosh, Giuseppe Riccardi, and Richard Jo-
hansson. 2012. Global features for shallow dis-
course parsing. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 150?159.
Sucheta Ghosh. 2012. End-to-End Discourse Parsing
using Cascaded Structured Prediction. Ph.D. thesis,
University of Trento.
Alistair Knott. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. the-
sis, University of Edinburgh.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20(2):151?184.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
76
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. The Penn Discourse Tree-
bank. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation,
pages 2237?2240.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 433?
440.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the LREC 2008 Conference, pages
2961?2968.
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010. Exploiting scope for shallow discourse pars-
ing. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The important of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
Bonnie Webber. 2004. D-LTAG: extending lexicalized
TAG to discourse. Cognitive Science, 28(5):751?
779.
Ben Wellner and James Pustejovsky. 2007. Automat-
ically identifying the arguments of discourse con-
nectives. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 92?101.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 88?94.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1552?1560.
77
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2105?2114,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Building Chinese Discourse Corpus with Connective-driven 
Dependency Tree Structure 
 
Yancui Li1,2 Wenhe Feng2 Jing Sun1 Fang Kong1 Guodong Zhou1 
1Natural Language Processing Lab, School of Computer Science and Technology, Soochow 
University, Suzhou 215006, China 
2Henan Institute of Science and Technology, Xinxiang 453003, China 
{yancuili, wenhefeng}@gmail.com {20104027009, kongfang, gdzhou}@suda.edu.cn 
 
 
 
 
 
Abstract 
In this paper, we propose a Connective-
driven Dependency Tree (CDT) scheme 
to represent the discourse rhetorical 
structure in Chinese language, with ele-
mentary discourse units as leaf nodes 
and connectives as non-leaf nodes, large-
ly motivated by the Penn Discourse 
Treebank and the Rhetorical Structure 
Theory. In particular, connectives are 
employed to directly represent the hier-
archy of the tree structure and the rhetor-
ical relation of a discourse, while the nu-
clei of discourse units are globally de-
termined with reference to the depend-
ency theory. Guided by the CDT scheme, 
we manually annotate a Chinese Dis-
course Treebank (CDTB) of 500 docu-
ments. Preliminary evaluation justifies 
the appropriateness of the CDT scheme 
to Chinese discourse analysis and the 
usefulness of our manually annotated 
CDTB corpus. 
1 Introduction 
It is well-known that interpretation of a text re-
quires understanding of its rhetorical relation 
hierarchy since discourse units rarely exist in 
isolation. Such discourse structure is fundamen-
tal to many text-based applications, such as 
summarization (Marcu, 2000) and question-
answering (Verberne et al., 2007). Due to the 
wide and potential use of discourse structure, 
constructing discourse resources has been at-
tracting more and more attention in recent years. 
In comparison with English, there are much 
fewer discourse resources for Chinese which 
largely restricts the researches in Chinese dis-
course analysis. 
The general notion of discourse structure 
mainly consists of discourse unit, connective, 
structure, relation and nuclearity. However, pre-
vious studies on discourse failed to fully express 
these kinds of information. For example, the 
Rhetorical Structure Theory (RST) (Mann and 
Thompson, 1988) represents a discourse as a 
tree with phrases or clauses as elementary dis-
course units (EDUs). However, RST ignores the 
importance of connectives to a great extent. Fig-
ure 1 gives an example tree structure with four 
EDUs (e1-e4). In comparison, Penn Discourse 
Treebank (PDTB) (Prasad et al., 2008) adopts 
the predicate-argument view of discourse rela-
tion, with discourse connective as predicate and 
two text spans as its arguments. Example (1) 
shows an explicit reason relation signaled by the 
discourse connective ?particularly if? and an 
implicit result relation represented by the insert-
ed discourse connective ?so?, with Arg1 in ital-
ics and Arg2 in bold. However, as a connective 
and its arguments are determined in a local con-
textual window, it is normally difficult to deduce 
a complete discourse structure from such a con-
nective-argument scheme. In this sense, the 
PDTB at best only provides a partial solution to 
the discourse structure. 
[Catching up with commercial competitors in retail 
banking and financial services,] e1 [they argue,] e2 
[will be difficult,] e3 [particularly if market condi-
tions turn sour.]e4 e4
e2
e3
condition
same-unit
e1-e4
e1-e3
attributione1-e2
e1
 
Figure 1: An example of discourse structure in RST 
 
Example (1): An example of the connective-argument 
scheme in PDTB 
A)[Catching up with commercial competitors in retail 
banking and financial services will be difficult ]Arg1, 
they argue, will be difficult, particularly if [market 
2105
 conditions turn sour ] Arg2. (Contingency.Condition. 
Hypothetical) (0616) 
B) So much of the stuff poured into its Austin, Texas, 
offices [that its mail rooms there simply stopped de-
livering it.]Arg1 (Implicit = so)[Now, thousands of 
mailers, catalogs and sales pitches go straight into 
the trash.]Arg2 (Contingency.Cause. Result) (0989) 
Obviously, both RST and PDTB have their 
own advantages and disadvantages in represent-
ing different characteristics of the discourse 
structure. In this paper, we attempt to propose a 
new scheme to Chinese discourse structure, 
adopt advantages of the tree structure from RST 
and connective from PDTB. Meanwhile, the 
special characteristics of Chinese discourse 
structure are well addressed. 
First, it is difficult to define EDU in Chinese 
due to the frequent occurrence of the ellipsis of 
subjects, objects and predicates, and the lack of 
functional marks for EDU. Second, the connec-
tives in Chinese omit much more frequently than 
those in English with about 82.0% vs. 54.5% in 
Zhou and Xue (2012). In Example (2), there are 
even no explicit connectives. Third, previous 
studies have shown the difference in classifying 
Chinese discourse relations from English (Xing, 
2001; Huang and Liao, 2011). This suggests that 
the discourse relations defined for English (both 
RST and PDTB) are not readily suitable for 
Chinese. Finally, the nucleus of a Chinese dis-
course relation is normally not directly related to 
a particular relation type but should be dynami-
cally determined from the global meaning of a 
discourse. 
Example (2): An example of discourse with 4 EDUs 
[       ???                ??        ??     ?      ??     
According to reports,Dongguan Customs  total accept 
?? ??      ??   ???????]e1 [?     ??     
company contract record 8400 plus class, than  pilot 
?    ? ?   ???]e2 [   ??       ??   ???]e3 
before a slight increase, company responses well,  
[??             ??            ???]e4 
generally acknowledge acceptance. 
?[According to reports, Dongguan District Cus-
toms accepted more than 8400 records of company 
contracts,] e1 [a slight increase from before the pi-
lot.]e2 [Companies responded well,]e3 [generally 
acknowledging acceptance.]e4? 
In this paper, we present a Connective-driven 
Dependency Tree (CDT) discourse representa-
tion scheme, which takes advantage of both RST 
and PDTB, with elementary discourse units 
(limited to clauses) as leaf nodes and connec-
tives as non-leaf nodes. Especially, we define 
EDU from three aspects, and employ the con-
nective? level and semantic to indicate the rhe-
torical structure and the discourse relation. Be-
sides, the nuclearity of discourse units in a dis-
course relation is decided on the overall dis-
course meaning. On the basis, we adopt the CDT 
scheme to annotate a certain scale corpus, called 
Chinese Discourse Treebank (CDTB) thereafter 
in this paper. Evaluation shows the appropriate-
ness of the CDT scheme to Chinese discourse 
analysis. 
The rest of this paper is organized as follows. 
Section 2 overviews related work. In Section 3, 
we present the CDT discourse representation 
scheme. In Section 4, we describe the annotation 
of the CDTB corpus. Section 5 compares CDTB 
with other major discourse corpora. Section 6 
gives the experimental results on EDU recogni-
tion, the crucial step for discourse parsing. Final-
ly, conclusion is given in section 7. 
2 Related Work 
In the past decade, several discourse corpora for 
English have emerged, with the Rhetorical 
Structure Theory Discourse Treebank (RST-DT) 
(Carlson et al., 2003) and the Penn Discourse 
Treebank (PDTB) (Prasad et al., 2008) most 
prevalent. 
In the RST framework, a text is represented as 
a discourse tree, with non-overlapping text spans 
(either phrases or clauses) as leaves, and adja-
cent nodes are related through particular rhetori-
cal relations to form a discourse sub-tree, which 
is then related to other adjacent nodes in the tree 
structure. According to RST, there are two types 
of discourse relations, mononuclear and multi-
nuclear. Figure 1 shows an example of discourse 
tree representation, following the notational 
convention of RST. Among the four EDUs (e1-
e4), e1 and e2 are connected by a mononuclear 
relation ?attribution?, where e1 is the nucleus, 
the span (e1-e2) and the EDU e3 are further 
connected by a multi-nuclear relation ?same-
unit?, where they are equally salient. Annotated 
according to the RST framework, the RST-DT 
consists of 385 documents from the Wall Street 
Journal (WSJ). Besides, the original 24 dis-
course relations defined by Mann and Thompson 
(1988) are further divided into a set of 18 rela-
tion classes with 78 finer grained rhetorical rela-
tions in RST-DT. 
As the largest discourse corpus so far, the 
Penn Discourse Treebank (PDTB) contains over 
one million words from WSJ. With EDUs lim-
ited to clauses, the PDTB adopts the predicate-
2106
 argument view of discourse relations, with con-
nective as predicate and two text spans as its 
arguments. Example (1) shows two annotation 
tokens for the connective ?particularly if? and 
?so?. The current version of PDTB 2.0 annotates 
40600 tokens, including 18459 explicit relations 
of 100 distinct types (e.g. ?particularly if? and 
?if? are the same type) and 16224 implicit dis-
course relations of 102 distinct token types. Be-
sides, PDTB provides a three level hierarchy of 
relation tags with the first level consisting of 
four major relation classes (Temporal, Contin-
gency, Comparison, and Expansion), which are 
further divided into 16 types and 23 subtypes. 
In comparison, there are few researches on 
Chinese discourse annotation (Xue, 2005a; Chen, 
2006; Yue, 2008; Huang and Chen, 2011; Zhou 
and Xue, 2012), with no exception employing 
existing RST or PDTB frameworks. For exam-
ple, Zhou and Xue (2012) use the PDTB annota-
tion guidelines to annotate Chinese discourse 
with 98 files from Chinese Treebank (Xue et al., 
2005b) of Xinhua newswire. In particular, they 
adopt a lexically grounded approach and make 
some adaptation based on the linguistic and sta-
tistical characteristics of Chinese text, with Arg1 
and Arg2 defined semantically and the senses of 
discourse relations annotated besides connec-
tives and their lexical alternatives. The agree-
ment on relation types reaches 95.1% and the 
agreement on implicit relations with exact span 
match reaches 76.9%. 
Instead, Chen (2006) and Yue (2008) use RST 
to annotate Chinese discourse. Chen (2006) se-
lects comma as the segmentation signal of EDUs 
(in Example (2), ???(According to reports)? 
will be segmented as an EDU), and finds that 
RST fails to deal with some special features of 
Chinese. Yue (2008) manually annotates a set of 
97 texts according to RST and shows the cross-
lingual transferability of RST to Chinese. How-
ever, it also shows that EDUs in Chinese are 
much different from those in English, and many 
relation types in Chinese have no correspond-
ence to English, and vice versa. 
3 Connective-driven Dependency Tree 
An appropriate representation scheme is funda-
mental to linguistic resource construction. With 
reference to various theories and representation 
scheme on the tree structure and nuclearity of 
RST, the connective, relation and discourse 
structure of Chinese complex sentence (Xing, 
2001), the sentence-group theory (Cao, 1984), 
the connective treatment of PDTB, the conjunc-
tion dependent analysis (Feng and Ji, 2011) and 
the center theory of dependency grammar (Hays, 
1964), we propose a new discourse representa-
tion scheme for Chinese, called Connective-
driven Dependency Tree (CDT), with EDUs as 
leaf nodes and connectives as non-leaf nodes, to 
accommodate the special characteristics of the 
Chinese language in discourse structure. 
For instance, Example (3) consists of 2 sen-
tences, which is part of a paragraph from 
?chtb_0001?, and its corresponding CDT repre-
sentation is shown in Figure 2. Here, the number 
of ?|? in Example (3) stands for the level of 
EDUs in CDT and the numbers marked in Fig-
ure 2 (such as 1, 2 etc.) distinguish EDUs. While 
an arrow points to the main EDU or main dis-
course unit (called nucleus), the combination of 
different EDUs can be considered as EDUs in a 
higher level and the new discourse units can thus 
be combined into higher-level units from bottom 
to up. In this way, the discourse structure can be 
expressed as a tree structure via bottom-up com-
bination of EDUs.  
Obviously, such discourse structure is con-
structed by two kinds of basic units, EDUs (leaf 
nodes) and connectives (non-leaf nodes). On the 
one hand, connectives can represent the dis-
course structure by its hierarchical level in the 
tree. The discourse structure is independent on 
the connective level essentially, rather than the 
reverse. On the other hand, connectives them-
selves can represent the discourse relation. This 
is why we call the scheme ?Connective-driven?. 
As for the abstract discourse relation, we can 
construct a set of discourse relations, mapping a 
connective to discourse relation, according to the 
users? specific requirements. 
Example (3): CDT example from CTB 
1 ??    ??   ??    ?   ??    ??  ????? 
Pudong development open up is a promote Shanghai, construct 
???          ??????   ??       ??   ?     ??? 
modern economy, trade, financial century De cross-century 
???|| 2 (??)     ??             ????    ??      ?? 
project, therefore a large number arisen De previously never 
????   ?   ? ??????|  3 (??)?  ??  {??} 
encounter DE new situation, new problem.To this, Pudong not 
??    ?    ??    ??        ??      ????       ??    ? 
simply DE adopting ?does a period time, wait accumulate Le 
   ??     ?????    ??   ??? ?    ???|| 4{??} 
experience after re-enactment laws regulations De approach,but  
??  ??          ??     ?           ??      ?      ??      ?  
learn developed countries and Shenzhen etc. special zone DE 
??     ???|||| 5<??>??          ???       ???? 
experience lesson, Invite at home and abroad revlant expert 
???|||| 6<??>??????         ??  ?       ?? 
2107
 scholars,               actively, timely DI formulate and issuing 
???  ???||| 7 {?} ??    ??  ??    ?    ???  ? 
statutory file, make these economic activity as soon as appear bei 
 ??      ??  ???  
bring into legality track. 
?1 Pudong's development and opening up is a century-
spanning undertaking for vigorously promoting Shanghai and 
constructing a modern economic, trade, and financial center. || 
2 Because of this, new situations and new questions that have 
not been encountered before are emerging in great numbers. | 3 
In response to this, Pudong is not simply adopting an approach 
of "work for a short time and then draw up laws and regula-
tions only after experience has been accumulated.?|| 4 Instead, 
Pudong is taking advantage of the lessons from experience of 
developed countries and special regions such as Shenzhen, ||||5 
by hiring appropriate domestic and foreign specialists and 
scholars, ||||6 actively and promptly formulating and issuing 
regulatory documents. ||| 7 So these economic activities are 
incorporated into the sphere of influence of the legal system as 
soon as they appear.? 
1 2 3 4 5 6 7
??(therefore?
{can be deleted}
??  (for this) 
{can be deleted}
 ??(and)
 <inserted, bad language sense>
?(cause)
{cann?t be deleted}
??...??(is not... but)
{cann?t be deleted}
 
Figure 2: CDT representation of Example (3) 
3.1 Elementary Discourse Unit 
As the leaf nodes of CDT, EDUs are limited to 
clauses. In principle, EDUs play a crucial role to 
discourse analysis. Since from bottom-up dis-
course combination, EDUs are the start of dis-
course analysis, while from top-down discourse 
segmentation, they are the end of discourse 
analysis. Unfortunately, since there lacks obvi-
ous distinction between Chinese sentence struc-
ture and phrase structure, it is rather difficult to 
define Chinese EDU (clause). Till now, there is 
still no widely accepted definition in the Chinese 
linguistics community (Wang, 2010). Inspired 
by Li et al. (2013a), we give the definition of 
Chinese EDU from three perspectives. First, 
from the syntactic structure perspective, an EDU 
should contain at least one predicate and express 
at least one proposition. Second, from the func-
tional perspective, an EDU should be related to 
other EDUs with some propositional function, 
i.e. not act as a grammatical element of other 
EDUs. Finally, from the morphological perspec-
tive, an EDU should be segmented by some 
punctuation, e.g. comma, semicolon and period. 
We use punctuation because there usually has a 
pause between clauses (EDUs), which can be 
shown in written commas, semicolons etc 
(Huang and Liao, 2011). Normally, it is easy to 
handle complex sentences and special sentence 
patterns (e.g. serial predicate sentences). For 
Example (4), A) is a single sentence with serial 
predicate; B) is complex sentence with two 
EDUs (clauses): 
Example (4): EDU examples 
A) He opened the door and went out. (single sentence, 
serial predicate, one EDU) 
B) 1 He opened the door,| 2 and went out. (complex sen-
tence, two EDUs) 
Take as example, there exist 7 EDUs in Ex-
ample (3), each marked with a number in front. 
According to our definition, the fragment ???
????????? ?(?work for a short 
time?has been accumulated? ) in EDU 3 is not 
segmented as a EDU since: 1) it acts as a gram-
matical element of other EDUs and has no direct 
relationship with other EDUs on propositional 
function; 2) it is marked by a pair of quotation 
marks and does not end with any punctuation. In 
contrast, the fragment ???????????
? ?(?but learn developed?legality track.?) is 
segment as 4 EDUs since it meets the three crite-
ria in our EDU definition. 
3.2 Connective 
As non-leaf nodes in the CDT representation, 
connectives connect EDUs or discourse units. 
Thus, the main criterion of determining whether 
an expression is a connective is to check wheth-
er the two fragments it connects are EDUs (or 
discourse units). In our scheme, the list of ex-
plicit discourse connectives is judged by a data 
driven approach, i.e. with any discourse-like 
word or phrase marked as connective in the an-
notation practice, e.g. ???(therefore)?, ???
(to this)?, ???...??...(is not...but....)?, ??(so 
that), ????(just because)? in Example (3), 
?? ...??(first...then) ?, ????(and at the 
same time)? in Example (5). 
Example (5): Connective examples from CTB 
A) 1<?????>???????| 2??????
?????????|| 3????????????
??(chtb_0001) 
1<If ; As long as>The construction company enters the 
region, |2 first the appropriate bureau delivers these regu-
latory documents,|| 3 Then there is a specialized contin-
gent that carries out a supervisory inspection. 
B) 1?????, 2?? ? ??????????
?????(chtb_0031) 
2108
 1 The processing trade ?, | 2 and at the same time 
is important content in the economic and trade coop-
eration between Guangdong, Hong Kong, Macao and 
Taiwan. 
It is worthy of mention that from the part-of-
speech perspective, connectives are not 
necessarily conjunctions. For example, in 
Example (3) and (5), adverbs ??...??(first? 
then)?, verb phrases ??????(is not?but)?, 
and preposition phrases ??? (to this)? are 
determined as connectives. From the 
morphological perspective, a connective may 
contain more than one word, even discontinuous. 
As a common occurring phenomenon in Chinese 
discourse, there exist many paired Chinese 
connectives, e.g. ?????? (is not?but)? in 
Figure 2. Even in some paired connectives, such 
as ?????? (because?so)?, a word in a 
paired connective can appear independently as a 
connective. Please note that this may not be 
applied to other cases, e.g. ??????  (is 
not?but)? as appeared in Example (3). Moreo-
ver, in many cases whether an expression is a 
connective or not depends on its meaning, e.g., 
??  (in order to)? is a connective, while ?? 
(for)? is not. For the positional distribution, a 
connective may appear anywhere, i.e. in the be-
ginning, middle, or the end of the first or second 
EDU. Example (3) and (5) show some of cases 
in different positions. The above characteristics 
pose special challenges on connective determi-
nation in Chinese language.  
According to the appearance of a connective 
or not, a discourse relation can be either explicit 
or implicit. Previous studies have shown the dif-
ficulty of implicit relation recognition in English 
due to the omission of connectives (Pitler et al., 
2009; Lin et al., 2009). This becomes even 
worse in Chinese since compared with the im-
plicit ratio of 54.5% in English connectives, this 
ratio rises up to about 82% in Chinese (Zhou and 
Xue, 2012). It is worth noting that the majority 
of discourse relations in Chinese are implicit, so 
the insertion of a connective in an implicit posi-
tion can significantly ease the understanding of 
the discourse. That is, a connective driven repre-
sentation scheme is still applicable to a discourse 
with implicit connectives. To help determine 
implicit relations, two special strategies are pro-
posed. 
First, for each explicit connective, a decision 
is made whether or not it can be deleted without 
changing the rhetorical relation of a discourse. It 
should be emphasized that this constraint is 
largely semantic. The motivation behind the re-
moval of explicit connectives is to enlarge im-
plicit instances and help recognize implicit rela-
tions. As shown in Figure 2, we use the paired 
mark ?()? to indicate that a connective can be 
deleted, e.g. connectives ?(?? to this)?, ?(?? 
therefore)?, ?(??? just because)?, and the 
paired mark ?{}? to indicate that a connective 
cannot be deleted, e.g. connectives ?{? so 
that}?, ?{????? is not?but}?. 
Second, since a connective can be inserted to 
represent an implicit relation, our scheme tries to 
insert a connective which can be easily inter-
preted from the semantic perspective with little 
ambiguity into the most appropriate place. Most 
of the connective insertions for implicit relations 
occur between adjacent discourse spans. It is 
worth noting that not all implicit connectives are 
subjective to the language sense. To mark this 
difference, we cluster implicit connectives into 
two categories according to their language sens-
es, either ?good language intuition? or ?bad lan-
guage intuition?. In our scheme, we use the 
paired mark ?<>?to indicate inserted implicit 
connectives, e.g. connectives?<?? e.g.>?, ?<
? but>? with ?good language sense?, connec-
tive ?<?? and>? with ?bad language sense?, as 
shown in Figure 2. 
In some cases, it is possible that there exist 
several insertion options for an implicit connec-
tive due to the ambiguity in a discourse. For ex-
ample, in Example (5A), connectives ??? (if)? 
and ??? (as long as)? are inserted into the first 
level to show the two discourse relation options. 
As far as this happens, connectives are inserted 
and ordered according to annotators? first intui-
tion. 
3.3 Discourse Structure 
In Figure 2, the paragraph is organized as a tree 
structure, in which EDUs appear in the leaf 
nodes and the connectives appear in the non-leaf 
ones. The adoption of tree structure conforms to 
traditional Chinese discourse theories and prac-
tice. For example, a native Chinese speaker 
tends to determine the overall level boundary 
first and then the analysis goes on step by step to 
the individual clauses, when understanding a 
complex sentence. This process naturally forms 
a tree structure. Besides, tree structure is easier 
to formalize, compared with graph.  
More specifically, the hierarchical structure of 
connectives indicates the hierarchical structure 
of discourse units. Apparently, discourse struc-
2109
 ture analysis can be viewed as hierarchical anal-
ysis of connectives, with hierarchical connective 
structure reflecting hierarchical combination of 
discourse units. Essentially, the discourse hierar-
chy indicates the correlation degrees of semantic 
relations in the discourse, the deeper tree level of 
two discourse units, the higher correlation de-
gree of their semantic relation. Therefore, a dis-
course relation is the ultimate factor for the 
choice of hierarchical discourse structure. For a 
reference, please take Sentence 2 in Figure 2 as 
an example. 
3.4 Discourse Relation  
For discourse relation representation, a general 
approach is to assign an abstract relation type to 
a discourse relation directly, such as cause, con-
junction, condition, purpose, etc, as done in 
RST-DT and PDTB. In our CDT scheme, we 
avoid to directly assign an abstract relation type 
to a discourse relation. Instead, we use the con-
nective itself to express the discourse relation, as 
shown in Figure 2. In this way, the difficulty of 
pre-defining a set of acknowledged discourse 
relations and selecting an exact discourse rela-
tion can be avoided during the corpus annotation 
process. Since a Chinese discourse relation is 
largely controlled by connective (Xing, 2001), 
the key to determine a relation is to identify a 
suitable connective. Normally, most of relation 
annotations can easily map from connectives to 
abstract semantic classes of relations, if neces-
sary, with the help of the discourse context. The 
majority of discourse relations in Chinese are 
implicit, but it makes sense to insist on a con-
nective driven representation. With connective 
as a bridge, at least it makes discourse represen-
tation easier. 
For the abstraction of discourse relations, we 
leave it in a later separate stage. Of course, there 
are cases where a connective may represent 
more than one discourse relation. For example, 
connective ??? can denotes the continuous rela-
tion ?? (especially)? and the transitional rela-
tion ?? (however)?. Compared with annotating 
discourse relation directly, annotator's intuition 
is more accurate for specific connective. We 
don't object to label discourse relation, referring 
to the general work and Chinese analysis prac-
tice, give a set of relations (Figure 3), regarding 
it as connective's semantics, and then annotate 
the connective with it. In this way, we can obtain 
a general relation set and resolve the connec-
tive's polysemy problem. We believe that the 
connective itself is the foundation of discourse 
relation, and the relation set can be adjusted dy-
namically according to the application require-
ments. 
Figure 3 shows a three-level set of discourse 
relations example. In the first level, this set con-
tains four relations of causality, coordination, 
transition and explanation, which are further 
clustered into 17 sub-relations in the second lev-
el. For example, relation causality contains 6 
sub-relations, i.e. cause-result, inference, hypo-
thetical, purpose, condition and background. In 
the third level, the connectives are under each 
sub-relation. For example, cause-result relation 
can be represented by ?because?, 'therefore' etc. 
The numbers shown in the parentheses illustrate 
the distributions of different relations in our cor-
pus. For example, there are 1335 causality rela-
tions in the first level, including 686 cause-result 
relations, 38 inference relations, 70 hypothetical 
relations, 335 purpose relations, 72 condition 
relations and 134 background relations. 
causality(1335) coordination(4148)  
cause-result(686) 
because... 
inference(38) 
so that... 
hypothetical(70) 
if... 
purpose(335) 
in order to... 
condition(72) 
only? 
background(134) 
background... 
transition(217) 
transition (200) 
but... 
concessive(17) 
altough... 
coordination(3503) 
and... 
continue(517) 
first...second... 
progressive(59) 
in addition.. 
selectional(10) 
or... 
inverse(59) 
compared with... 
explanation(1617) 
explanation(911) 
which including... 
summary-
elaboration 
in a word... 
(234) 
example(252) 
e.g.... 
evaluation (220) 
evaluation ... 
Figure 3: A three-level set of discourse relations 
3.5 Nucleus and Satellite 
Once discourse units are determined, adjacent 
spans are linked together via connectives to 
build a hierarchical structure. As stated above, 
discourse relations may be either mononuclear 
or multi-nuclear. A mononuclear relation holds 
between a nucleus and a satellite unit. Normally, 
the nucleus usually reflects the intention focus of 
the discourse and is thus more salient in the dis-
course structure, while the satellite usually rep-
resents supportive information for the nucleus. 
In comparison, a multi-nuclear relation usually 
2110
 holds two or more discourse units of equal 
weight in the discourse structure.  
For nucleus determination, we adopt the de-
pendency grammar, and select the unit which 
can stand for the relationship with other dis-
course units in a discourse. As shown in Figure 
2, on the first level, discourse relation ??? (to 
this)? has the latter unit ????????
(Pudong?as soon as they appear.)? as nucleus and 
the former unit ??????? (Pudong ?new 
problem)? as satellite, since the latter unit agrees 
with the main purpose of the discourse, which 
emphasizes some methods for the progress of 
Pudong. Moreover, since the combination of 4, 5 
and 6 has the cause relation with 7, we choose 7 
as nucleus because it can stand for the combina-
tion of 4, 5, 6 and 7, and has the selection rela-
tionship with 3. 
4 Chinese Discourse Treebank 
Given above the CDT scheme, we choose 500 
Xinhua newswire documents from the Chinese 
Treebank (Xue et al., 2005b) in our Chinese 
Discourse Treebank (CDTB) annotation. In par-
ticular, we annotate one discourse tree for each 
paragraph. 
In this section, we address the key issues with 
the CDTB annotation, such as annotator training, 
tagging strategies, corpus quality, along with the 
statistics of the CDTB corpus. 
4.1 Annotator Training 
The annotator team consists of a Ph.D. in Chi-
nese linguistics as the supervisor (senior annota-
tor) and four undergraduate students in Chinese 
linguistics as annotators (two pairs). The annota-
tion is done in four phases. In the first phase, the 
annotators spend 3 months on learning the prin-
ciples of CDT and the use of our developed dis-
course annotation tool. In the second phase, the 
annotators spend 2 months on independently 
annotating the same 50 documents (about 260 
paraphrases), and another 2 months on cross-
checking to resolve the difference and to revise 
the guidelines. In the third phase, the annotators 
spend 9 months on annotating the remaining 450 
documents. In the final phase, the supervisor 
spends 3 months carefully proofread all 500 
documents. 
4.2 Tagging Strategies 
In the CDTB annotation, we employ a top-down 
strategy. That is, we determine the overall level 
first and then the analysis goes on step by step to 
the individual EDUs. This strategy is adopted in 
our annotation tool. The advantages of the top-
down strategy are three folds. First, such a strat-
egy can easily grasp the whole discourse struc-
ture. This conforms to the global nature of dis-
course analysis. Second, due to the lack of clear 
difference between Chinese sentence and phrase 
structure, such a strategy can largely avoid the 
error propagation in Chinese EDU segmentation. 
Since in such a top-down strategy, EDU seg-
mentation becomes an end question, and even if 
an EDU segmentation error happens, its impact 
is localized, i.e. with little impact on the whole 
discourse structure. Our annotation practice 
shows that such strategy is effective. Third, such 
a strategy accords with the cognitive of Chinese 
characteristics, and conforms to the mental pro-
cess of Chinese discourse understanding (Huang 
and Liao, 2011). However, we do not exclude 
the bottom-up strategy. In some cases, on the 
cognitive psychological process, annotator is 
combine top-down and bottom-up strategies. 
Take Example (3) as an example, an annotator 
first finds the first level, with the period at the 
end of sentence 1, and chooses discourse rela-
tion (either explicit or implicit), connective, and 
connective related information (e.g. whether can 
be added, deleted, and the language sense, etc.), 
nuclearity etc. Then, the annotator turns to sen-
tence 1 and marks the second comma as level 2 
with necessary information annotated, and goes 
on to sentence 2, recursively, until all EDUs are 
marked. In this way, a discourse tree with the 
CDT representation is constructed. 
4.3 Quality Assurance 
A number of steps are taken to ensure the quality 
of CDTB. These involve two tasks: checking the 
validity of the trees and tracking inter-annotator 
consistency. 
4.3.1 Tree validation 
We first manually check if a tree has a single 
root node and compare the tree with the docu-
ment to check for missing sentence or fragments 
from the end of text. Then we check the attached 
information such as connectives, relations and 
nuclearity in the tree. We also check the tree 
with a tree traversal program to find the errors 
undetected by the manual validation process. 
Finally, all of the trees work successfully.  
4.3.2 Consistency 
To ensure the quality of CDTB, we adopt the 
inter-annotator consistency using Agreement 
and kappa on 60 documents (chtb0041-chtb 
2111
 0100). Table 1 illustrates the inter-annotator 
consistency in details. 
As shown in Table 1, we measure the agree-
ment of EDU segmentation by determining 
whether punctuation (all period, comma etc. are 
considered) is treated as an EDU boundary. It 
shows that the agreement reaches 91.7% with 
Cohen's kappa value (Cohen, 1960) 0.91. This 
justifies the appropriateness of our EDU defini-
tion. Explicit or Implicit agreement 94.7% is 
calculate by the same EDU boundary (intersec-
tion) of two annotators. For the same explicit 
relation, the connective identification agreement 
is 82.3%, because this is strict measure when 
two annotators choose the same connective word. 
If we relax the measure to contain the same 
word, the agreement can reach 98%. For exam-
ple, one annotate ????(also?and)?, and the 
other annotate ??(and)? is wrong with our strict 
measure. 
 Agreement  Kappa 
EDU segmentation 
Explicit or Implicit 
Explicit connective identifi-
cation 
Implicit connective insertion 
Mononuclear or Multinuclear 
Nuclearity 
Structure 
91.7      0.91 
94.7      0.81 
82.3        -- 
 
74.6        -- 
80.8        -- 
82.4        -- 
77.4        -- 
Table 1: Inter-annotator consistency 
It is not surprising that the agreement on im-
plicit connective insertion with the same posi-
tion and the same connective only reaches 
74.6% since for some discourse relations, there 
may existing several connective alternatives. For 
example, both ?so? and ?therefore? can express 
the same causation relation. If we relax the con-
straint to the compatible connective, the agree-
ment on implicit connective insertion can reach 
up to 84.5%. 
Finally, it shows that the agreement on overall 
discourse structure (with the same connectives 
as non-leaf nodes, the same EDUs as leaf nodes) 
reaches 77.4%. This justifies the appropriateness 
of our CDT scheme, given the inherent ambigui-
ty in Chinese discourse structure. 
4.4 Corpus Statistics 
Currently, the CDTB corpus consists of 500 
newswire articles from Chinese Treebank, which 
are further divided into 2342 paragraphs with a 
CDT representation for one paragraph.  
? For EDUs, CDTB contains 10650 EDUs with 
an average of 4.5 EDUs per tree. On average, 
there are 2 EDUs per sentence and 22 Chi-
nese characters per EDU. 
? For discourse relations, CDTB contains 7310 
relations, of which 1812 are explicit relations 
(24.8%) and 5498 are implicit relations 
(75.2%). This indicates that implicit relations 
occur much more frequently in Chinese than 
in English, e.g. 75.2% in CDTB (Chinese) vs. 
~50% in PDTB (English). 
? With the deepest level of 9, most (98.5%) of 
discourse relations occur in level 1 (2342), 
level 2(2372), level 3(1532), level 4(712), 
and level 5(242). It also shows that 3557 
(48.7%) relations are mononuclear relations 
with 2110 nucleus ahead, while the remain-
ing 3754 relations are multi-nuclear. The 
numbers shown in the parentheses of Figure 
3 illustrate the distributions of different rela-
tions. In comparison with the top 2 most fre-
quently occurring relations in PDTB (Eng-
lish), i.e. the coordination and explanation re-
lations, there exist 3503 (47.9%) and 911 in-
stances respectively, with regard to the ab-
stract relation set as shown in Figure 3. 
? CDTB contains 282 connectives, among 
which 274 (140 can be deleted) appears as 
explicit connectives and 44 can be inserted in 
place of implicit connectives. Table 2 lists 
the top 10 frequent explicit connectives and 
implicit connectives. 
Explicit connectives Implicit connectives 
connectives   frequency connectives   frequency 
?(and)                     208 
??(among them)   154 
?(also)                     131 
?(however)              70 
?(but)                       69 
?(also)                      68 
?(so that)                  56 
?(in order to)            52 
?(in order to)            49 
??(meanwhile)       46 
??(so)                    368 
?(and)                      354 
??(and)                  259 
??(e.g)                   140 
?(in order to)             68 
?(in order to)             61 
??(then)                   55 
??(among them)      48 
?(while)                     47 
??(because)             32 
Table 2: The most frequent connectives in CDTB 
5 Comparison with other Discourse 
Banks 
Table 3 compares the difference of CDTB with 
RST-DT and PDTB from various perspectives, 
such as EDU, connective, relation, structure and 
nuclearity. 
2112
  
 RST-DB PDTB CDTB 
EDU 
Clear defined; start of 
combination; one relation 
has two or more EDUs 
Predicate-argument 
view; one relation 
has two arguments 
Clear defined from three aspects; end of 
top-down segmentation; one relation has 
two or more EDUs 
Connective -- 
Mark explicit con-
nectives and insert 
implicit connectives 
Mark whether an connective can be deleted 
without changing the rhetorical relation; 
insert implicit connective with good intui-
tion and bad intuition differentiated 
Relation 
Abstract set of relation 
types; annotate the rela-
tion types 
Abstract set of rela-
tion types; annotate 
connective and rela-
tion type 
Represent relation by connective; annotate 
connective and it?s attribute; mapping of 
connective to the set of discourse relations 
in a later stage 
Structure Complete tree 
Partial tree, deduced 
by connective and 
it?s argument 
Complete tree; top-down segmentation; 
structure can be represented by the connec-
tive hierarchy 
Nuclearity 
Determined by certain 
rhetorical relation 
-- 
Determined by the global meaning of a 
discourse 
Table 3: The comparison of RST-DT, PDTB and CDTB 
 
6 Preliminary Experimentation 
In order to evaluate the computability of CDTB, 
we give the experimental results on EDU recog-
nition, which is crucial in discourse parsing. Af-
ter excluding sentence end punctuations (such as 
period, question mark, and exclamatory mark), 
which are certainly EDU boundaries, there re-
mains 7625 punctuations as EDU boundaries 
(positive instances) and 4876 punctuations as 
non-EDU boundaries (negative instances). With 
various features as adopted in Xue and Yang 
(2011) and Li et al. (2013b), Table 4 shows the 
performance of EDU recognition on the CDTB 
corpus with 10-fold cross validation.  
 Gold standard parse      Automatic parse 
Accuracy  F1(+)  F1(-)  Accuracy  F1(+)  F1(-) 
MaxEnt 90.6     91.1   90.5 89.0    90.3    87.2 
C45 90.2    90.5    90.1 88.7    90.0    87.7 
NiveBayes 90.2    89.9   88.9 88.0    89.0    86.9 
Table 4: Performance of EDUs recognition 
As shown in Table 4, MaxEnt performs best, 
with accuracy up to 90.6% on gold standard 
parse tree, close to human agreement of 91.7%, 
and with accuracy up to 89% on automatic parse 
tree. This suggests the appropriateness of our 
definition of clause as EDU. Table 4 also gives 
the performance on both positive and negative 
instances. It shows better F1-measure on recog-
nizing positive instances than negative instances. 
7 Conclusions 
In this paper, we propose a Connective-driven 
Dependency Tree (CDT) structure as a represen-
tation scheme for Chinese discourse structure. 
CDT takes advantage of both RST and PDTB, 
and well adapts to the special characteristics of 
Chinese discourse. In particular, we describe 
CDT in detail from various perspectives, such as 
EDU, connective, structure, relation and nucle-
arity. Given the CDT scheme, we annotate 500 
documents in a top-down segmentation process 
to keep consistent with Chinese native?s cogni-
tive habit. Evaluation of the CDTB corpus on 
EDU recognition justifies the appropriateness of 
the CDT scheme to Chinese discourse structure 
and the usefulness of our CDTB corpus. 
In the future work, we will focus on enlarging 
the scale of the corpus annotation and develop-
ing a complete Chinese discourse parser. 
Acknowledgments 
This research is supported by the Project 
2012AA011102 under the National 863 High-
Tech Program of China, by the National Natural 
Science Foundation of China, No.61331011, 
No.61273320. 
Classifier 
2113
 The contact author of this paper, according to 
the meaning given to this role by Soochow Uni-
versity, is Guodong Zhou. The complete corpus 
is available for research purpose upon request. 
Reference 
Zheng Cao.1984. Primary exploration on sentence 
group. Zhejiang Education Press, Hangzhou,CN(in 
Chinese). 
Lynn Carlson, Daniel Marcu, and Mary Ellen Oku-
rowski. 2003. Building a Discourse-tagged Corpus 
in the Framework of Rhetorical Structure Theory. 
Springer Netherlands. 
LiPing Chen. 2006. English and Chinese discourse 
structure dimension theory and practice. Ph.D. 
thesis, Shanghai international studies university 
doctoral dissertation. 
Liping Chen. 2008. Chinese text structure annotation 
theory support?Journal of Nanjing university of 
aeronautics and astronautics, 10(3):69-71(in Chi-
nese). 
Jacob Cohen. 1960. A coefficient of agreement for 
nominal scales. Educational and Psychological 
Measurement, 20 (1): 37?46. 
Wenhe Feng and Donghong Ji. 2011. Parallel struc-
ture analysis of the coordination structure and the 
controller status of connective. Linguistic Sciences, 
2:168-181(in Chinese). 
David G Hays. 1964. Dependency theory: formalism 
and some observations. Language, 40(4):511-525. 
Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese 
Discourse Relation Recognition. In Proceedings of 
5th International Joint Conference on Natural 
Language Process, pages 1442?1446, Chiang Mai, 
Thailand, November 2011. 
Borong Huang and Xudong Liao. 2011. Morden Chi-
nese (volume two, updated 5th edition). Higher 
Education Press. Beijing,CN(in Chinese). 
Yancui Li, Wenhe Feng, and Guodong Zhou. 2013a. 
Elementary discourse unit in Chinese discourse 
structure analysis. In Chinese Lexical Semantics, 
pages 186-198, Wuhan, China, Springer Berlin 
Heidelberg. 
Yancui Li, Wenhe Feng, and Guodong Zhou et al. 
2013b. Research of Chinese Clause Identification 
Based on Comma. Acta Scientiarum Naturalium 
Universitatis Pekinensis, 49(1):7-14 (in Chinese 
with English abstract). 
Ziheng Lin, Min-Yan Kan, and Hwee Tou Ng. 2009. 
Recognizing implicit discourse relations in the 
Penn Discourse Treebank. In Proceedings of the 
2009 Conference on Empirical Methods in Natural 
Language Processing, pages 343?351, Singapore, 
6-7 August 2009. 
Daniel Marcu. 2000. The theory and practice of dis-
course parsing and summarization. MIT Press. 
William Mann and Sandra Thompson. 1988. Rhetori-
cal structure theory: Toward a functional theory of 
text organization. Text, 8(3):243-281. 
Emily Pitler, Annie Louis, and Ani Nenkova. Auto-
matic sense prediction for implicit discourse rela-
tions in text. 2009. In Proceedings of the 47th An-
nual Meeting of the ACL and the 4th IJCNLP of 
the AFNLP, pages 683?691, Suntec, Singapore, 2-
7 August 2009. 
Rashmi Prasad, Nikhil Dinesh, and Lee et al. 2008. 
The Penn Discourse TreeBank 2.0. In Pro-
cessdings of Sixth International Conference on 
Language Resources and Evaluation (LREC), 
pages 2961-2968, Marrakech, Morocco. 
Susan Verberne, Lou Boves, Nelleke Oostdijk, and 
Peter-Arno Coppen. 2007. Discourse-based an-
swering of why-questions. Traitement Automa-
tique des Langues, special issue on Computational 
Approaches to Discourse and Document Pro-
cessing, 47(2):21-41.  
Wenge Wang. 2010. The Current Research Situation 
of the Clause in Modern Chinese. Chinese Lan-
guage Learning, (1): 67?76(in Chinese). 
Fuyi Xing. 2003. Research of Chinese complex sen-
tence. The Commercial Press, Beijing, CN (in 
Chinese). 
Nianwen Xue. 2005a. Annotating the Discourse Con-
nectives in the Chinese Treebank. In Proceedings 
of the ACL Workshop on Frontiers in Corpus An-
notation, pages 84-91, Ann Arbor, Michigan. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha 
Palmer. 2005b. The Penn Chinese Treebank: 
Phrase Structure Annotation of a Large Corpus. 
Natural Language Engineering, 11(2): 207-238. 
Nianwen Xue and Yaqin Yang. 2011. Chinese sen-
tence segmentation as comma classification. In 
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics:Human 
Language Technologies, pages 631-635, Portland, 
Oregon,USA, June 2011. 
Ming Yue. 2008. Rhetorical Structure Annotation of 
Chinese News Commentaries. Journal of Chinese 
Information Processing, 22(4): 19-23(in Chinese 
with English abstract). 
Yuping Zhou and Nianwen Xue. 2012. PDTB-style 
discourse annotation of Chinese text. In Proceed-
ings of the 50th Annual Meeting of the Association 
for Computational Linguistics, pages 69-77, Jeju, 
Republic of Korea, 8-14 July 2012.  
 
2114
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 414?423,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Employing Personal/Impersonal Views in Supervised and 
Semi-supervised Sentiment Classification 
 
Shoushan Li??  Chu-Ren Huang?  Guodong Zhou?  Sophia Yat Mei Lee? 
 
?Department of Chinese and Bilingual 
Studies 
The Hong Kong Polytechnic University 
{shoushan.li,churenhuang, 
sophiaym}@gmail.com 
 
?
 Natural Language Processing Lab 
School of Computer Science and Technology 
Soochow University, China 
gdzhou@suda.edu.cn 
 
 
Abstract 
In this paper, we adopt two views, personal 
and impersonal views, and systematically 
employ them in both supervised and 
semi-supervised sentiment classification. Here, 
personal views consist of those sentences 
which directly express speaker?s feeling and 
preference towards a target object while 
impersonal views focus on statements towards 
a target object for evaluation. To obtain them, 
an unsupervised mining approach is proposed. 
On this basis, an ensemble method and a 
co-training algorithm are explored to employ 
the two views in supervised and 
semi-supervised sentiment classification 
respectively. Experimental results across eight 
domains demonstrate the effectiveness of our 
proposed approach. 
1 Introduction 
As a special task of text classification, sentiment 
classification aims to classify a text according to 
the expressed sentimental polarities of opinions 
such as ?thumb up? or ?thumb down? on the 
movies (Pang et al, 2002). This task has recently 
received considerable interests in the Natural 
Language Processing (NLP) community due to its 
wide applications. 
In general, the objective of sentiment 
classification can be represented as a kind of 
binary relation R, defined as an ordered triple (X, 
Y, G), where X is an object set including different 
kinds of people (e.g. writers, reviewers, or users), 
Y is another object set including the target 
objects (e.g. products, events, or even some 
people), and G is a subset of the Cartesian 
product X Y? . The concerned relation in 
sentiment classification is X ?s evaluation on Y, 
such as ?thumb up?, ?thumb down?, ?favorable?, 
and ?unfavorable?. Such relation is usually 
expressed in text by stating the information 
involving either a person (one element in X ) or a 
target object itself (one element in Y ). The first 
type of statement called personal view, e.g. ?I am 
so happy with this book?, contains X ?s 
?subjective? feeling and preference towards a 
target object, which directly expresses 
sentimental evaluation. This kind of information 
is normally domain-independent and serves as 
highly relevant clues to sentiment classification. 
The latter type of statement called impersonal 
view, e.g. ?it is too small?, contains Y ?s 
?objective? (i.e. or at least criteria-based) 
evaluation of the target object. This kind of 
information tends to contain much 
domain-specific classification knowledge. 
Although such information is sometimes not as 
explicit as personal views in classifying the 
sentiment of a text, speaker?s sentiment is 
usually implied by the evaluation result.  
It is well-known that sentiment classification 
is very domain-specific (Blitzer et al, 2007), so 
it is critical to eliminate its dependence on a 
large-scale labeled data for its wide applications. 
Since the unlabeled data is ample and easy to 
collect, a successful semi-supervised sentiment 
classification system would significantly 
minimize the involvement of labor and time. 
Therefore, given the two different views 
mentioned above, one promising application is to 
adopt them in co-training algorithms, which has 
been proven to be an effective semi-supervised 
learning strategy of incorporating unlabeled data 
to further improve the classification performance 
(Zhu, 2005). In addition, we would show that 
personal/impersonal views are linguistically 
marked and mining them in text can be easily 
performed without special annotation.  
414
In this paper, we systematically employ 
personal/impersonal views in supervised and 
semi-supervised sentiment classification. First, 
an unsupervised bootstrapping method is adopted 
to automatically separate one document into 
personal and impersonal views. Then, both views 
are employed in supervised sentiment 
classification via an ensemble of individual 
classifiers generated by each view. Finally, a 
co-training algorithm is proposed to incorporate 
unlabeled data for semi-supervised sentiment 
classification. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work of 
sentiment classification. Section 3 presents our 
unsupervised approach for mining personal and 
impersonal views. Section 4 and Section 5 
propose our supervised and semi-supervised 
methods on sentiment classification respectively. 
Experimental results are presented and analyzed 
in Section 6. Section 7 discusses on the 
differences between personal/impersonal and 
subjective/objective. Finally, Section 8 draws our 
conclusions and outlines the future work. 
2 Related Work 
Recently, a variety of studies have been reported 
on sentiment classification at different levels: 
word level (Esuli and Sebastiani, 2005), phrase 
level (Wilson et al, 2009), sentence level (Kim 
and Hovy, 2004; Liu et al, 2005), and document 
level (Turney, 2002; Pang et al, 2002). This 
paper focuses on the document-level sentiment 
classification. Generally, document-level 
sentiment classification methods can be 
categorized into three types: unsupervised, 
supervised, and semi-supervised. 
Unsupervised methods involve deriving a 
sentiment classifier without any labeled 
documents. Most of previous work use a set of 
labeled sentiment words called seed words to 
perform unsupervised classification. Turney 
(2002) determines the sentiment orientation of a 
document by calculating point-wise mutual 
information between the words in the document 
and the seed words of ?excellent? and ?poor?. 
Kennedy and Inkpen (2006) use a term-counting 
method with a set of seed words to determine the 
sentiment. Zagibalov and Carroll (2008) first 
propose a seed word selection approach and then 
apply the same term-counting method for Chinese 
sentiment classifications. These unsupervised 
approaches are believed to be 
domain-independent for sentiment classification. 
Supervised methods consider sentiment 
classification as a standard classification problem 
in which labeled data in a domain are used to 
train a domain-specific classifier. Pang et al 
(2002) are the first to apply supervised machine 
learning methods to sentiment classification. 
Subsequently, many other studies make efforts to 
improve the performance of machine 
learning-based classifiers by various means, such 
as using subjectivity summarization (Pang and 
Lee, 2004), seeking new superior textual features 
(Riloff et al, 2006), and employing document 
subcomponent information (McDonald et al, 
2007). As far as the challenge of 
domain-dependency is concerned, Blitzer et al 
(2007) present a domain adaptation approach for 
sentiment classification. 
Semi-supervised methods combine unlabeled 
data with labeled training data (often 
small-scaled) to improve the models. Compared 
to the supervised and unsupervised methods, 
semi-supervised methods for sentiment 
classification are relatively new and have much 
less related studies. Dasgupta and Ng (2009) 
integrate various methods in semi-supervised 
sentiment classification including spectral 
clustering, active learning, transductive learning, 
and ensemble learning. They achieve a very 
impressive improvement across five domains. 
Wan (2009) applies a co-training method to 
semi-supervised learning with labeled English 
corpus and unlabeled Chinese corpus for Chinese 
sentiment classification. 
3 Unsupervised Mining of Personal and 
Impersonal Views 
As mentioned in Section 1, the objective of 
sentiment classification is to classify a specific 
binary relation: X ?s evaluation on Y, where X is 
an object set including different kinds of persons 
and Y is another object set including the target 
objects to be evaluated. First of all, we focus on 
an analysis on sentences in product reviews 
regarding the two views: personal and 
impersonal views.  
The personal view consists of personal 
sentences (i.e. X ?s sentences) exemplified 
below: 
I. Personal preference: 
E1: I love this breadmaker! 
E2: I disliked it from the beginning. 
II. Personal emotion description: 
E3: Very disappointed! 
E4: I am happy with the product. 
III. Personal actions: 
415
E5: Do not waste your money. 
E6: I have recommended this machine to all my 
friends. 
The impersonal view consists of impersonal 
sentences (i.e.Y ?s sentences) exemplified below: 
I. Impersonal feature description: 
E7: They are too thin to start with. 
E8: This product is extremely quiet. 
II. Impersonal evaluation: 
E9: It's great. 
E10: The product is a waste of time and money. 
III. Impersonal actions: 
E11: This product not even worth a penny. 
E12: It broke down again and again. 
We find that the subject of a sentence presents 
important cues for personal/impersonal views, 
even though a formal and computable definition 
of this contrast cannot be found. Here, subject 
refers to one of the two main constituents in the 
traditional English grammar (the other 
constituent being the predicate) (Crystal, 2003)1. 
For example, the subjects in the above examples 
of E1, E7 and E11 are ?I?, ?they?, and ?this 
product? respectively. For automatic mining the 
two views, personal/impersonal sentences can be 
defined according to their subjects: 
Personal sentence: the sentence whose 
subject is (or represents) a person. 
Impersonal sentence: the sentence whose 
subject is not (does not represent) a person. 
In this study, we mainly focus on product 
review classification where the target object in 
the set Y  is not a person. The definitions need 
to be adjusted when the evaluation target itself is 
a person, e.g. the political sentiment 
classification by Durant and Smith (2007). 
Our unsupervised mining approach for mining 
personal and impersonal sentences consists of 
two main steps. First, we extract an initial set of 
personal and impersonal sentences with some 
heuristic rules: If the first word of one sentence 
is (or implies) a personal pronoun including ?I?, 
?we?, and ?do?, then the sentence is extracted as a 
personal sentence; If the first word of one 
sentence is an impersonal pronoun including 'it', 
'they', 'this', and 'these', then the sentence is 
extracted as an impersonal sentence. Second, we 
apply the classifier which is trained with the 
initial set of personal and impersonal sentences 
to classify the remaining sentences. This step 
aims to classify the sentences without pronouns 
                                                      
1
 The subject has the grammatical function in a sentence of 
relating its constituent (a noun phrase) by means of the verb to any 
other elements present in the sentence, i.e. objects, complements, 
and adverbials. 
(e.g. E3). Figure 1 shows the unsupervised 
mining algorithm. 
Input: 
The training data D
 
 
Output: 
    All personal and impersonal sentences, i.e. 
sentence sets personalS  and impersonalS . 
Procedure: 
(1). Segment all documents in D to sentences 
S using punctuations (such as periods and 
interrogation marks) 
(2). Apply the heuristic rules to classify the 
sentences S  with proper pronouns into, 1pS  
and  1iS  
(3). Train a binary classifier p if ?  with  1pS  and  
1iS  
(4). Use  p if ?  to classify the remaining sentences 
into  2pS  and  2iS  
(5). 1 2personal p pS S S= ? ,  1 2impersonal i iS S S= ?  
 
Figure 1: The algorithm for unsupervised mining 
personal and impersonal sentences from a training 
data 
4 Employing Personal/Impersonal 
Views in Supervised Sentiment 
Classification 
After unsupervised mining of personal and 
impersonal sentences, the training data is divided 
into two views: the personal view, which 
contains personal sentences, and the impersonal 
view, which contains impersonal sentences. 
Obviously, these two views can be used to train 
two different classifiers, 1f  and 2f , for 
sentiment classification respectively.  
Since our mining approach is unsupervised, 
there inevitably exist some noises. In addition, 
the sentences of different views may share the 
same information for sentiment classification. 
For example, consider the following two 
sentences: ?It is a waste of money.? and ?Do not 
waste your money.? Apparently, the first one 
belongs to the impersonal view while the second 
one belongs to personal view, according to our 
heuristic rules. However, these two sentences 
share the same word, ?waste?, which conveys 
strong negative sentiment information. This 
suggests that training a single-view classifier 3f  
with all sentences should help. Therefore, three 
base classifiers, 1f , 2f , and 3f , are eventually 
derived from the personal view, the impersonal 
416
view and the single view, respectively. Each base 
classifier provides not only the class label 
outputs but also some kinds of confidence 
measurements, e.g. posterior probabilities of the 
testing sample belonging to each class.  
Formally, each base classifier  ( 1,2,3)lf l =  
assigns a test sample (denoted as lx ) a posterior 
probability vector ( )lP x

:  
1 2( ) ( | ), ( | ) tl l lP x p c x p c x= < >

 
where 1( | )lp c x  denotes the probability that the 
-thl base classifier considers the sample 
belonging to 1c . 
In the ensemble learning literature, various 
methods have been presented for combining base 
classifiers. The combining methods are 
categorized into two groups (Duin, 2002): fixed 
rules such as voting rule, product rule, and sum 
rule (Kittler et al, 1998), and trained rules such 
as weighted sum rule (Fumera and Roli, 2005) 
and meta-learning approaches (Vilalta and Drissi, 
2002). In this study, we choose a fixed rule and a 
trained rule to combine the three base classifiers 
1f , 2f , and 3f .  
The chosen fixed rule is product rule which 
combine base classifiers by multiplying the 
posterior possibilities and using the multiplied 
possibility for decision, i.e. 
3
1
                 
  arg max ( | )
j
i l
i l
assign y c
where j p c x
=
?
= ?  
The chosen trained rule is stacking (Vilalta and 
Drissi, 2002; D?eroski and ?enko, 2004) where a 
meta-classifier is trained with the output of the 
base classifiers as the input. Formally, let 'x  
denote a feature vector of a sample from the 
development data. The output of the -thl base 
classifier lf on this sample is the probability 
distribution over the category set 1 2{ , }c c , i.e. 
1 2( ' ) ( | ' ), ( | ' )l l l lP x p c x p c x=< >

 
Then, a meta-classifier is trained using the 
development data with the meta-level feature 
vector 2 3metax R ??  
1 2 3( ' ), ( ' ), ( ' )meta l l lx P x P x P x= = ==< >
  
 
In our experiments, we perform stacking with 
4-fold cross validation to generate meta-training 
data where each fold is used as the development 
data and the other three folds are used to train the 
base classifiers in the training phase. 
5 Employing Personal/Impersonal 
Views in Semi-Supervised Sentiment 
Classification 
Semi-supervised learning is a strategy which 
combines unlabeled data with labeled training 
data to improve the models. Given the two-view 
classifiers 1f  and 2f  along with the single-view 
classifier 3f , we perform a co-training algorithm 
for semi-supervised sentiment classification. The 
co-training algorithm is a specific 
semi-supervised learning approach which starts 
with a set of labeled data and increases the 
amount of labeled data using the unlabeled data 
by bootstrapping (Blum and Mitchell, 1998). 
Figure 2 shows the co-training algorithm in our 
semi-supervised sentiment classification. 
Input: 
The labeled data L
 
containing personal 
sentence set L personalS ?  and impersonal sentence set 
L impersonalS ?  
The unlabeled data U  containing personal 
sentence set
 
U personalS ?  and impersonal sentence set 
U impersonalS ?  
Output: 
    New labeled data L  
Procedure: 
Loop for N iterations untilU ?=  
(1). Learn the first classifier 1f  with L personalS ?  
(2). Use 1f  to label samples from U with 
U personalS ?  
(3). Choose 1n  positive and 1n negative most 
confidently predicted samples 1A  
(4). Learn the second classifier 2f  with L impersonalS ?  
(5). Use 2f to label samples from U with 
U impersonalS ?   
(6). Choose 2n  positive and 2n negative most 
confidently predicted samples 2A   
(7). Learn the third classifier 3f  with L  
(8). Use 3f  to label samples from U  
(9). Choose 3n  positive and 3n  negative most 
confidently predicted samples 3A  
(10). Add samples 1 2 3A A A? ?  with the 
corresponding labels into L  
(11). Update L personalS ?  and L impersonalS ?  
 
Figure 2: Our co-training algorithm for 
semi-supervised sentiment classification 
417
After obtaining the new labeled data, we can 
either adopt one classifier (i.e. 3f ) or a 
combined classifier (i.e. 1 2 3f f f+ + ) in further 
training and testing. In our experimentation, we 
explore both of them with the former referred to 
as co-training and single classifier and the latter 
referred to as co-training and combined 
classifier. 
6 Experimental Studies 
We have systematically explored our method on 
product reviews from eight domains: book, DVD, 
electronic appliances, kitchen appliances, health, 
network, pet and software. 
6.1 Experimental Setting 
The product reviews on the first four domains 
(book, DVD, electronic, and kitchen appliances) 
come from the multi-domain sentiment 
classification corpus, collected from 
http://www.amazon.com/ by Blitzer et al (2007)2. 
Besides, we also collect the product views from 
http://www.amazon.com/ on other four domains 
(health, network, pet and software)3. Each of the 
eight domains contains 1000 positive and 1000 
negative reviews. Figure 3 gives the distribution 
of personal and impersonal sentences in the 
training data (75% labeled data of all data). It 
shows that there are more impersonal sentences 
than personal ones in each domain, in particular 
in the DVD domain, where the number of 
impersonal sentences is at least twice as many as 
that of personal sentences. This unusual 
phenomenon is mainly attributed to the fact that 
many objective descriptions, e.g. the movie plot 
introductions, are expressed in the DVD domain 
which makes the extracted personal and 
impersonal sentences rather unbalanced. 
We apply both support vector machine (SVM) 
and Maximum Entropy (ME) algorithms with the 
help of the SVM-light4 and Mallet5 tools. All 
parameters are set to their default values. We 
find that ME performs slightly better than SVM 
on the average. Furthermore, ME offers posterior 
probability information which is required for 
                                                      
2
 http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
3
 Note that the second version of multi-domain sentiment 
classification corpus does contain data from many other domains. 
However, we find that the reviews in the other domains contain 
many duplicated samples. Therefore, we re-collect the reviews from 
http://www.amazon.com/ and filter those duplicated ones. The new 
collection is here:  
http://llt.cbs.polyu.edu.hk/~lss/ACL2010_Data_SSLi.zip 
4
 http://svmlight.joachims.org/  
5
 http://mallet.cs.umass.edu/  
combination methods. Thus we apply the ME 
classification algorithm for further combination 
and co-training. In particular, we only employ 
Boolean features, representing the presence or 
absence of a word in a document. Finally, we 
perform t-test to evaluate the significance of the 
performance difference between two systems 
with different methods (Yang and Liu, 1999). 
Sentence Number in the Training Data
16134
8477 8337 8843
13097
29290
1485214414
12691 11941
1381814265 16441
1475315573
27714
0
10000
20000
30000
40000
Boo
k DVD
Ele
ctr
oni
c
Kit
che
n
Hea
lth
Net
wor
k Pet
Sof
twa
re
Nu
mb
er
Number of personal sentences
Number of impersonal sentences
Figure 3: Distribution of personal and impersonal 
sentences in the training data of each domain 
6.2 Experimental Results on Supervised 
Sentiment Classification 
4-fold cross validation is performed for 
supervised sentiment classification. For 
comparison, we generate two random views by 
randomly splitting the whole feature space into 
two parts. Each part is seen as a view and used to 
train a classifier. The combination (two random 
view classifiers along with the single-view 
classifier 3f ) results are shown in the last column 
of Table 1. The comparison between random two 
views and our proposed two views will clarify 
whether the performance gain comes truly from 
our proposed two-view mining, or simply from 
using the classifier combination strategy. 
Table 1 shows the performances of different 
classifiers, where the single-view classifier 3f  
which uses all sentences for training and testing, 
is considered as our baseline. Note that the 
baseline performances of the first four domains 
are worse than the ones reported in Blitzer et al 
(2007). But their experiment is performed with 
only one split on the data with 80% as the 
training data and 20% as the testing data, which 
means the size of their training data is larger than 
ours. Also, we find that our performances are 
similar to the ones (described as fully supervised 
results) reported in Dasgupta and Ng (2009) 
where the same data in the four domains are used 
and 10-fold cross validation is performed.  
418
Domain Personal 
View 
Classifier 
1f  
Impersonal 
View 
Classifier 
2f  
Single View 
Classifier 
(baseline) 
3f
 
Combination  
(Stacking) 
1 2 3f f f+ +  
Combination 
(Product rule) 
1 2 3f f f+ +  
Combination 
with two 
random views 
(Product rule) 
Book 0.7004 0.7474 0.7654 0.7919 0.7949 0.7546 
DVD 0.6931 0.7663 0.7884 0.8079 0.8165 0.8054 
Electronic 0.7414 0.7844 0.8074 0.8304 0.8364 0.8210 
Kitchen 0.7430 0.8030 0.8290 0.8555 0.8565 0.8152 
Health 0.7000 0.7370 0.7559 0.7780 0.7815 0.7548 
Network 0.7655 0.7710 0.8265 0.8360 0.8435 0.8312 
Pet 0.6940 0.7145 0.7390 0.7565 0.7665 0.7423 
Software 0.7035 0.7205 0.7470 0.7730 0.7715 0.7615 
AVERAGE 0.7176 0.7555 0.7823 0.8037 0.8084 0.7858 
 
Table 1: Performance of supervised sentiment classification 
 
From Table 1, we can see that impersonal view 
classifier 1f  consistently performs better than 
personal view classifier 2f . Similar to the 
sentence distributions, the difference in the 
classification performances between these two 
views in the DVD domain is the largest (0.6931 
vs. 0.7663). 
Both the combination methods (stacking and 
product rule) significantly outperform the 
baseline in each domain (p-value<0.01) with a 
decent average performance improvement of 
2.61%. Although the performance difference 
between the product rule and stacking is not 
significant, the product rule is obviously a better 
choice as it involves much easier implementation. 
Therefore, in the semi-supervised learning 
process, we only use the product rule to combine 
the individual classifiers. Finally, it shows that 
random generation of two views with the 
combination method of the product rule only 
slightly outperforms the baseline on the average 
(0.7858 vs. 0.7823) but performs much worse 
than our unsupervised mining of personal and 
impersonal views.  
6.3 Experimental Results on 
Semi-supervised Sentiment 
Classification 
We systematically evaluate and compare our 
two-view learning method with various 
semi-supervised ones as follows: 
Self-training, which uses the unlabeled data 
in a bootstrapping way like co-training yet limits 
the number of classifiers and the number of 
views to one. Only the baseline classifier 3f  is 
used to select most confident unlabeled samples 
in each iteration. 
Transductive SVM, which seeks the largest 
separation between labeled and unlabeled data 
through regularization (Joachims, 1999). We 
implement it with the help of the SVM-light tool. 
Co-training with random two-view 
generation (briefly called co-training with 
random views), where two views are generated 
by randomly splitting the whole feature space 
into two parts.  
In semi-supervised sentiment classification, 
the data are randomly partitioned into labeled 
training data, unlabeled data, and testing data 
with the proportion of 10%, 70% and 20% 
respectively. Figure 4 reports the classification 
accuracies in all iterations, where baseline 
indicates the supervised classifier 3f  trained on 
the 10% data; both co-training and single 
classifier and co-training and combined 
classifier refer to co-training using our proposed 
personal and impersonal views. But the former 
merely applies the baseline classifier 3f  trained 
the new labeled data to test on the testing data 
while the latter applies the combined classifier 
1 2 3f f f+ + . In each iteration, two top-confident 
samples in each category are chosen, i.e. 
1 2 3 2n n n= = = . For clarity, results of other 
methods (e.g. self-training, transductive SVM) 
are not shown in Figure 4 but will be reported in 
Figure 5 later.  
Figure 4 shows that co-training and 
combined classifier always outperforms 
co-training and single classifier. This again 
justifies the effectiveness of our two-view 
learning on supervised sentiment classification.
419
25 50 75 100 125
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
Domain: Book
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.58
0.6
0.62
0.64
0.66
0.68
0.7
Domain: DVD
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.7
0.72
0.74
0.76
0.78
0.8
Domain: Electronic
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.72
0.74
0.76
0.78
0.8
0.82
Domain: Kitchen
Iteration Number
Ac
cu
ra
cy
 
 
 
25 50 75 100 125
0.54
0.56
0.58
0.6
0.62
0.64
0.66
Domain: Health
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Domain: Network
Iteration Number
Ac
cu
ra
cy
 
 
Baseline
Co-traning and single classifier
Co-traning and combined classifier
25 50 75 100 125
0.58
0.6
0.62
0.64
0.66
0.68
Domain: Pet
Iteration Number
Ac
cu
ra
cy
 
 
25 50 75 100 125
0.62
0.64
0.66
0.68
0.7
0.72
Domain: Software
Iteration Number
Ac
cu
ra
cy
 
 
 
 
Figure 4: Classification performance vs. iteration numbers (using 10% labeled data as training data) 
 
One open question is whether the unlabeled 
data improve the performance. Let us set aside 
the influence of the combination strategy and 
focus on the effectiveness of semi-supervised 
learning by comparing the baseline and 
co-training and single classifier. Figure 4 
shows different results on different domains. 
Semi-supervised learning fails on the DVD 
domain while on the three domains of book, 
electronic, and software, semi-supervised 
learning benefits slightly (p-value>0.05). In 
contrast, semi-supervised learning benefits much 
on the other four domains (health, kitchen, 
network, and pet) from using unlabeled data and 
the performance improvements are statistically 
significant (p-value<0.01). Overall speaking, we 
think that the unlabeled data are very helpful as 
they lead to about 4% accuracy improvement on 
the average except for the DVD domain. Along 
with the supervised combination strategy, our 
approach can significantly improve the 
performance more than 7% on the average 
compared to the baseline. 
Figure 5 shows the classification results of 
different methods with different sizes of the 
labeled data: 5%, 10%, and 15% of all data, 
where the testing data are kept the same (20% of 
all data). Specifically, the results of other 
methods including self-training, transductive 
SVM, and random views are presented when 
10% labeled data are used in training. It shows 
that self-training performs much worse than our 
approach and fails to improve the performance of 
five of the eight domains. Transductive SVM 
performs even worse and can only improve the 
performance of the ?software? domain. Although 
co-training with random views outperforms the 
baseline on four of the eight domains, it performs 
worse than co-training and single classifier. 
This suggests that the impressive improvements 
are mainly due to our unsupervised two-view 
mining rather than the combination strategy.
420
Using 10% labeled data as training data
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Book DVD Electronic Kitchen Health Network Pet Software
Ac
cu
rac
y
Baseline Transductive SVM Self-training
Co-training with random views Co-training and single classifier Co-training and combined classifier
 
Using 5% labeled data as training data
0.69
0.747
0.584
0.525
0.67 0.6530.626
0.55
0.564
0.683
0.495
0.615
0.8675
0.7855
0.7
0.601
0.45
0.55
0.65
0.75
0.85
Boo
k DVD
Ele
ctr
oni
c
Kit
che
n
Hea
lth
Net
wor
k Pet
Sof
twa
re
Ac
cu
ra
cy
Using 15% labeled data as training data
0.763
0.6925
0.765
0.5925
0.679
0.564
0.677
0.7375
0.6625
0.735
0.655
0.615
0.8625
0.8325
0.782
0.716
0.45
0.55
0.65
0.75
0.85
Boo
k DVD
Ele
ctr
oni
c
Kit
che
n
Hea
lth
Net
wor
k Pet
Sof
twa
re
Ac
cu
ra
cy
 
Figure 5: Performance of semi-supervised sentiment classification when 5%, 10%, and 15% labeled data are used 
 
Figure 5 also shows that our approach is rather 
robust and achieves excellent performances in 
different training data sizes, although our 
approach fails on two domains, i.e. book and 
DVD, when only 5% of the labeled data are used. 
This failure may be due to that some of the 
samples in these two domains are too ambiguous 
and hard to classify. Manual checking shows that 
quite a lot of samples on these two domains are 
even too difficult for professionals to give a 
high-confident label. Another possible reason is 
that there exist too many objective descriptions 
in these two domains, thus introducing too much 
noisy information for semi-supervised learning. 
The effectiveness of different sizes of chosen 
samples in each iteration is also evaluated like 
1 2 3 6n n n= = = and 1 2 33, 6n n n= = = (This 
assignment is considered because the personal 
view classifier performs worse than the other two 
classifiers). Our experimental results are still 
unsuccessful in the DVD domain and do not 
show much difference on other domains. We also 
test the co-training approach without the 
single-view classifier 3f . Experimental results 
show that the inclusion of the single-view 
classifier 3f  slightly helps the co-training 
approach. The detailed discussion of the results 
is omitted due to space limit. 
6.4 Why our approach is effective? 
One main reason for the effectiveness of our 
approach on supervised learning is the way how 
personal and impersonal views are dealt with. As 
personal and impersonal views have different 
ways of expressing opinions, splitting them into 
two separations can filter some classification 
noises. For example, in the sentence of ?I have 
seen amazing dancing, and good dancing. This 
was TERRIBLE dancing!?. The first sentence is 
classified as a personal sentence and the second 
one is an impersonal sentence. Although the 
words ?amazing? and ?good? convey strong 
positive sentiment information, the whole text is 
negative. If we get the bag-of-words from the 
whole text, the classification result will be wrong. 
Rather, splitting the text into two parts based on 
different views allows correct classification as 
the personal view rarely contains impersonal 
words such as ?amazing? and ?good?. The 
classification result will thus be influenced by 
the impersonal view.  
In addition, a document may contain both 
personal and impersonal sentences, and each of 
them, to a certain extent, , provides classification 
evidence. In fact, we randomly select 50 
documents in the domain of kitchen appliances 
and find that 80% of the documents take both 
personal and impersonal sentences in which both 
of them express explicit opinions. That is to say, 
the two views provide different, complementary 
information for classification. This qualifies the 
success requirement of co-training algorithm to 
some extend. This might be the reason for the 
effectiveness of our approach on semi-supervised 
learning. 
421
7 Discussion on Personal/Impersonal vs. 
Subjective/Objective 
As mentioned in Section 1, personal view 
contains X ?s ?subjective? feeling, and 
impersonal view containsY ?s ?objective? (i.e. or 
at least criteria-based) evaluation of the target 
object. However, our technically-defined 
concepts of personal/impersonal are definitely 
different from subjective/objective: Personal 
view can certainly contain many objective 
expressions, e.g. ?I bought this electric kettle? and 
impersonal view can contain many subjective 
expressions, e.g. ?It is disappointing?.  
Our technically-defined personal/impersonal 
views are two different ways to describe 
opinions. Personal sentences are often used to 
express opinions in a direct way and their target 
object should be one of X. Impersonal ones are 
often used to express opinions in an indirect way 
and their target object should be one of Y. The 
ideal definition of personal (or impersonal) view 
given in Section 1 is believed to be a subset of 
our technical definition of personal (or 
impersonal) view. Thus impersonal view may 
contain both Y ?s objective evaluation (more 
likely to be domain independent) and subjective 
Y?s description. 
In addition, simply splitting text into 
subjective/objective views is not particularly 
helpful. Since a piece of objective text provides 
rather limited implicit classification information, 
the classification abilities of the two views are 
very unbalanced. This makes the co-training 
process unfeasible. Therefore, we believe that 
our technically-defined personal/impersonal 
views are more suitable for two-view learning 
compared to subjective/objective views. 
8 Conclusion and Future Work 
In this paper, we propose a robust and effective 
two-view model for sentiment classification 
based on personal/impersonal views. Here, the 
personal view consists of subjective sentences 
whose subject is a person, whereas the 
impersonal view consists of objective sentences 
whose subject is not a person. Such views are 
lexically cued and can be obtained without 
pre-labeled data and thus we explore an 
unsupervised learning approach to mine them.  
Combination methods and a co-training 
algorithm are proposed to deal with supervised 
and semi-supervised sentiment classification 
respectively. Evaluation on product reviews from 
eight domains shows that our approach 
significantly improves the performance across all 
eight domains on supervised sentiment 
classification and greatly outperforms the 
baseline with more than 7% accuracy 
improvement on the average across seven of 
eight domains (except the DVD domain) on 
semi-supervised sentiment classification. 
In the future work, we will integrate the 
subjectivity summarization strategy (Pang and 
Lee, 2004) to help discard noisy objective 
sentences. Moreover, we need to consider the 
cases when both X and Y appear in a sentence. 
For example, the sentence ?I think they're poor? 
should be an impersonal view but wrongly 
classified as a personal one according to our 
technical rules. We believe that these will help 
improve our approach and hopefully are 
applicable to the DVD domain. Another 
interesting and practical idea is to integrate 
active learning (Settles, 2009), another popular 
but principally different kind of semi-supervised 
learning approach, with our two-view learning 
approach to build high-performance systems 
with the least labeled data. 
Acknowledgments 
The research work described in this paper has 
been partially supported by Start-up Grant for 
Newly Appointed Professors, No. 1-BBZM in 
the Hong Kong Polytechnic University and two 
NSFC grants, No. 60873150 and No. 90920004. 
We also thank the three anonymous reviewers 
for their invaluable comments. 
References  
Blitzer J., M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain Adaptation for Sentiment 
Classification. In Proceedings of ACL-07. 
Blum A. and T. Mitchell. 1998. Combining labeled 
and unlabeled data with co-training. In 
Proceedings of COLT-98. 
Crystal D. 2003. The Cambridge Encyclopedia of the 
English Language. Cambridge University Press. 
Dasgupta S. and V. Ng. 2009. Mine the Easy and 
Classify the Hard: Experiments with Automatic 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Duin R. 2002. The Combining Classifier: To Train Or 
Not To Train? In Proceedings of 16th International 
Conference on Pattern Recognition (ICPR-02). 
Durant K. and M. Smith. 2007. Predicting the 
Political Sentiment of Web Log Posts using 
422
Supervised Machine Learning Techniques Coupled 
with Feature Selection. In Processing of Advances 
in Web Mining and Web Usage Analysis. 
D?eroski S. and B. ?enko. 2004. Is Combining 
Classifiers with Stacking Better than Selecting the 
Best One? Machine Learning, vol.54(3), 
pp.255-273, 2004. 
Esuli A. and F. Sebastiani. 2005. Determining the 
Semantic Orientation of Terms through Gloss 
Classification. In Proceedings of CIKM-05. 
Fumera G. and F. Roli. 2005. A Theoretical and 
Experimental Analysis of Linear Combiners for 
Multiple Classifier Systems. IEEE Trans. PAMI, 
vol.27, pp.942?956, 2005 
Joachims, T. 1999. Transductive Inference for Text 
Classification using Support Vector Machines. 
ICML1999. 
Kennedy A. and D. Inkpen. 2006. Sentiment 
Classification of Movie Reviews using Contextual 
Valence Shifters. Computational Intelligence, 
vol.22(2), pp.110-125, 2006. 
Kim S. and E. Hovy. 2004. Determining the 
Sentiment of Opinions. In Proceedings of 
COLING-04. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Liu B., M. Hu, and J. Cheng. 2005. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
In Proceedings of WWW-05. 
McDonald R., K. Hannan, T. Neylon, M. Wells, and J. 
Reynar. 2007. Structured Models for 
Fine-to-coarse Sentiment Analysis. In Proceedings 
of ACL-07. 
Pang B. and L. Lee. 2004. A Sentimental Education: 
Sentiment Analysis using Subjectivity 
Summarization based on Minimum Cuts. In 
Proceedings of ACL-04. 
Pang B., L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of 
EMNLP-02. 
Riloff E., S. Patwardhan, and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In Proceedings 
of EMNLP-06. 
Settles B. 2009. Active Learning Literature Survey. 
Technical Report 1648, Department of Computer 
Sciences, University of Wisconsin at Madison, 
Wisconsin. 
Turney P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. In Proceedings of 
ACL-02. 
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial Intelligence 
Review, 18(2): 77?95. 
Wan X. 2009. Co-Training for Cross-Lingual 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433, 
2009. 
Yang Y. and X. Liu. 1999. A Re-Examination of Text 
Categorization methods. In Proceedings of 
SIGIR-99. 
Zagibalov T. and J. Carroll. 2008. Automatic Seed 
Word Selection for Unsupervised Sentiment 
Classification of Chinese Test. In Proceedings of 
COLING-08.  
Zhu X. 2005. Semi-supervised Learning Literature 
Survey. Technical Report Computer Sciences 1530, 
University of Wisconsin ? Madison. 
 
423
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1108?1117,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Joint Syntactic and Semantic Parsing of Chinese 
Junhui Li  and  Guodong Zhou 
School of Computer Science & Technology
Soochow University 
Suzhou, China 215006 
{lijunhui, gdzhou}@suda.edu.cn
Hwee Tou Ng 
Department of Computer Science 
National University of Singapore 
13 Computing Drive, Singapore 117417
nght@comp.nus.edu.sg 
 
Abstract 
This paper explores joint syntactic and seman-
tic parsing of Chinese to further improve the 
performance of both syntactic and semantic 
parsing, in particular the performance of se-
mantic parsing (in this paper, semantic role 
labeling). This is done from two levels. Firstly, 
an integrated parsing approach is proposed to 
integrate semantic parsing into the syntactic 
parsing process. Secondly, semantic informa-
tion generated by semantic parsing is incorpo-
rated into the syntactic parsing model to better 
capture semantic information in syntactic 
parsing. Evaluation on Chinese TreeBank, 
Chinese PropBank, and Chinese NomBank 
shows that our integrated parsing approach 
outperforms the pipeline parsing approach on 
n-best parse trees, a natural extension of the 
widely used pipeline parsing approach on the 
top-best parse tree. Moreover, it shows that 
incorporating semantic role-related informa-
tion into the syntactic parsing model signifi-
cantly improves the performance of both syn-
tactic parsing and semantic parsing. To our 
best knowledge, this is the first research on 
exploring syntactic parsing and semantic role 
labeling for both verbal and nominal predi-
cates in an integrated way. 
1 Introduction 
Semantic parsing maps a natural language sen-
tence into a formal representation of its meaning. 
Due to the difficulty in deep semantic parsing, 
most previous work focuses on shallow semantic 
parsing, which assigns a simple structure (such 
as WHO did WHAT to WHOM, WHEN, 
WHERE, WHY, HOW) to each predicate in a 
sentence. In particular, the well-defined semantic 
role labeling (SRL) task has been drawing in-
creasing attention in recent years due to its im-
portance in natural language processing (NLP) 
applications, such as question answering (Nara-
yanan and Harabagiu, 2004), information extrac-
tion (Surdeanu et al, 2003), and co-reference 
resolution (Kong et al, 2009). Given a sentence 
and a predicate (either a verb or a noun) in the 
sentence, SRL recognizes and maps all the con-
stituents in the sentence into their corresponding 
semantic arguments (roles) of the predicate. In 
both English and Chinese PropBank (Palmer et 
al., 2005; Xue and Palmer, 2003), and English 
and Chinese NomBank (Meyers et al, 2004; Xue, 
2006), these semantic arguments include core 
arguments (e.g., Arg0 for agent and Arg1 for 
recipient) and adjunct arguments (e.g., 
ArgM-LOC for locative argument and 
ArgM-TMP for temporal argument). According 
to predicate type, SRL can be divided into SRL 
for verbal predicates (verbal SRL, in short) and 
SRL for nominal predicates (nominal SRL, in 
short).  
With the availability of large annotated cor-
pora such as FrameNet (Baker et al, 1998), 
PropBank, and NomBank in English, data-driven 
techniques, including both feature-based and 
kernel-based methods, have been extensively 
studied for SRL (Carreras and M?rquez, 2004; 
Carreras and M?rquez, 2005; Pradhan et al, 
2005; Liu and Ng, 2007). Nevertheless, for both 
verbal and nominal SRL, state-of-the-art systems 
depend heavily on the top-best parse tree and 
there exists a large performance gap between 
SRL based on the gold parse tree and the 
top-best parse tree. For example, Pradhan et al 
(2005) suffered a performance drop of 7.3 in 
F1-measure on English PropBank when using the 
top-best parse tree returned from Charniak?s 
parser (Charniak, 2001). Liu and Ng (2007) re-
ported a performance drop of 4.21 in F1-measure 
on English NomBank.  
Compared with English SRL, Chinese SRL 
suffers more seriously from syntactic parsing. 
Xue (2008) evaluated on Chinese PropBank and 
showed that the performance of Chinese verbal 
SRL drops by about 25 in F1-measure when re-
placing gold parse trees with automatic ones. 
Likewise, Xue (2008) and Li et al (2009) re-
ported a performance drop of about 12 in 
F1-measure in Chinese NomBank SRL. 
1108
While it may be difficult to further improve 
syntactic parsing, a promising alternative is to 
perform both syntactic and semantic parsing in 
an integrated way. Given the close interaction 
between the two tasks, joint learning not only 
allows uncertainty about syntactic parsing to be 
carried forward to semantic parsing but also al-
lows useful information from semantic parsing to 
be carried backward to syntactic parsing.  
This paper explores joint learning of syntactic 
and semantic parsing for Chinese texts from two 
levels. Firstly, an integrated parsing approach is 
proposed to benefit from the close interaction 
between syntactic and semantic parsing. This is 
done by integrating semantic parsing into the 
syntactic parsing process. Secondly, various se-
mantic role-related features are directly incorpo-
rated into the syntactic parsing model to better 
capture semantic role-related information in syn-
tactic parsing. Evaluation on Chinese TreeBank, 
Chinese PropBank, and Chinese NomBank 
shows that our method significantly improves the 
performance of both syntactic and semantic 
parsing. This is promising and encouraging. To 
our best knowledge, this is the first research on 
exploring syntactic parsing and SRL for verbal 
and nominal predicates in an integrated way.  
The rest of this paper is organized as follows. 
Section 2 reviews related work. Section 3 pre-
sents our baseline systems for syntactic and se-
mantic parsing. Section 4 presents our proposed 
method of joint syntactic and semantic parsing 
for Chinese texts. Section 5 presents the experi-
mental results. Finally, Section 6 concludes the 
paper. 
2 Related Work 
Compared to the large body of work on either 
syntactic parsing (Ratnaparkhi, 1999; Collins, 
1999; Charniak, 2001; Petrov and Klein, 2007), 
or SRL (Carreras and M?rquez, 2004; Carreras 
and M?rquez, 2005; Jiang and Ng, 2006), there is 
relatively less work on their joint learning.  
Koomen et al (2005) adopted the outputs of 
multiple SRL systems (each on a single parse 
tree) and combined them into a coherent predi-
cate argument output by solving an optimization 
problem. Sutton and McCallum (2005) adopted a 
probabilistic SRL system to re-rank the N-best 
results of a probabilistic syntactic parser. How-
ever, they reported negative results, which they 
blamed on the inaccurate probability estimates 
from their locally trained SRL model.  
As an alternative to the above pseudo-joint 
learning methods (strictly speaking, they are still 
pipeline methods), one can augment the syntactic 
label of a constituent with semantic information, 
like what function parsing does (Merlo and Mu-
sillo, 2005). Yi and Palmer (2005) observed that 
the distributions of semantic labels could poten-
tially interact with the distributions of syntactic 
labels and redefined the boundaries of constitu-
ents. Based on this observation, they incorpo-
rated semantic role information into syntactic 
parse trees by extending syntactic constituent 
labels with their coarse-grained semantic roles 
(core argument or adjunct argument) in the sen-
tence, and thus unified semantic parsing and 
syntactic parsing. The actual fine-grained seman-
tic roles are assigned, as in other methods, by an 
ensemble classifier. However, the results ob-
tained with this method were negative, and they 
concluded that semantic parsing on PropBank 
was too difficult due to the differences between 
chunk annotation and tree structure. Motivated 
by Yi and Palmer (2005), Merlo and Musillo 
(2008) first extended a statistical parser to pro-
duce a richly annotated tree that identifies and 
labels nodes with semantic role labels as well as 
syntactic labels. Then, they explored both 
rule-based and machine learning techniques to 
extract predicate-argument structures from this 
enriched output. Their experiments showed that 
their method was biased against these roles in 
general, thus lowering recall for them (e.g., pre-
cision of 87.6 and recall of 65.8).  
There have been other efforts in NLP on joint 
learning with various degrees of success. In par-
ticular, the recent shared tasks of CoNLL 2008 
and 2009 (Surdeanu et al, 2008; Hajic et al, 
2009) tackled joint parsing of syntactic and se-
mantic dependencies. However, all the top 5 re-
ported systems decoupled the tasks, rather than 
building joint models. Compared with the disap-
pointing results of joint learning on syntactic and 
semantic parsing, Miller et al (2000) and Finkel 
and Manning (2009) showed the effectiveness of 
joint learning on syntactic parsing and some 
simple NLP tasks, such as information extraction 
and name entity recognition. In addition, at-
tempts on joint Chinese word segmentation and 
part-of-speech (POS) tagging (Ng and Low, 
2004; Zhang and Clark, 2008) also illustrate the 
benefits of joint learning. 
 
1109
 3 Baseline: Pipeline Parsing on 
Top-Best Parse Tree 
In this section, we briefly describe our approach 
to syntactic parsing and semantic role labeling, 
as well as the baseline system with pipeline 
parsing on the top-best parse tree. 
3.1 Syntactic Parsing 
Our syntactic parser re-implements Ratnaparkhi 
(1999), which adopts the maximum entropy 
principle. The parser recasts a syntactic parse 
tree as a sequence of decisions similar to those 
of a standard shift-reduce parser and the parsing 
process is organized into three left-to-right 
passes via four procedures, called TAG, 
CHUNK, BUILD, and CHECK. 
First pass. The first pass takes a tokenized sen-
tence as input, and uses TAG to assign each 
word a part-of-speech.  
Second pass. The second pass takes the output 
of the first pass as input, and uses CHUNK to 
recognize basic chunks in the sentence.  
Third pass. The third pass takes the output of 
the second pass as input, and always alternates 
between BUILD and CHECK in structural pars-
ing in a recursive manner. Here, BUILD decides 
whether a subtree will start a new constituent or 
join the incomplete constituent immediately to 
its left. CHECK finds the most recently pro-
posed constituent, and decides if it is complete.  
3.2 Semantic Role Labeling 
Figure 1 demonstrates an annotation example of 
Chinese PropBank and NomBank. In the figure, 
the verbal predicate ???/provide? is annotated 
with three core arguments (i.e., ?NP (??
/Chinese ??/govt.)? as Arg0, ?PP (?/to ?
?/N. Korean ??/govt.)? as Arg2, and ?NP 
(???/RMB ??/loan)? as Arg1), while the 
nominal predicate ???/loan? is annotated with 
two core arguments (i.e., ?NP (??/Chinese ?
?/govt.)? as Arg1 and ?PP (?/to ??/N. Ko-
rean ??/govt.)? as Arg0), and an adjunct ar-
gument (i.e., ?NN ( ? ? ? /RMB)? as 
ArgM-MNR, denoting the manner of loan). It is 
worth pointing out that there is a (Chinese) 
NomBank-specific label in Figure 1, Sup (sup-
port verb) (Xue, 2006), to help introduce the 
arguments which occur outside the nominal pre-
dicate-headed noun phrase. In (Chinese) Nom-
Bank, a verb is considered to be a support verb 
only if it shares at least an argument with the 
nominal predicate. 
3.2.1 Automatic Predicate Recognition 
Automatic predicate recognition is a prerequisite 
for the application of SRL systems. For verbal 
predicates, it is very easy. For example, 99% of 
verbs are annotated as predicates in Chinese 
PropBank. Therefore, we can simply select any 
word with a part-of-speech (POS) tag of VV, 
VA, VC, or VE as verbal predicate. 
Unlike verbal predicate recognition, nominal 
predicate recognition is quite complicated. For 
Figure 1: Two predicates (Rel1 and Rel2) and their arguments in the style of Chinese PropBank and NomBank. 
?
to ?? 
N. Korean
??
govt.
?? 
provide
P 
NR NN
VV
NN NN 
NP
PP 
Arg0/Rel2 
Arg2/Rel1 
ArgM-MNR/Rel2 Rel2 
NP
VP
VP
???
RMB
??
loan 
? 
. 
NR NN
PU 
NP 
Arg1/Rel2
Arg0/Rel1
IP
?? 
Chinese 
?? 
govt. 
Sup/Rel2
Rel1
Chinese government provides RMB loan to North Korean government. 
Arg1/Rel1
TOP
1110
example, only 17.5% of nouns are annotated as 
predicates in Chinese NomBank. It is quite 
common that a noun is annotated as a predicate 
in some cases but not in others. Therefore, au-
tomatic predicate recognition is vital to nominal 
SRL. In principle, automatic predicate recogni-
tion can be cast as a binary classification (e.g., 
Predicate vs. Non-Predicate) problem. For no-
minal predicates, a binary classifier is trained to 
predict whether a noun is a nominal predicate or 
not. In particular, any word POS-tagged as NN 
is considered as a predicate candidate in both 
training and testing processes. Let the nominal 
predicate candidate be w0, and its left and right 
neighboring words/POSs be w-1/p-1and w1/p1, 
respectively. Table 1 lists the feature set used in 
our model. In Table 1, local features present the 
candidate?s contextual information while global 
features show its statistical information in the 
whole training set. 
 
Type Description 
w0, w-1, w1, p-1, p1 local 
features The first and last characters of the candidate
Whether w0 is ever tagged as a verb in the 
training data? Yes/No 
Whether w0 is ever annotated as a nominal 
predicate in the training data? Yes/No 
The most likely label for w0 when it occurs 
together with w-1 and w1. 
The most likely label for w0 when it occurs 
together with w-1. 
 
 
global 
features 
The most likely label for w0 when it occurs 
together with w1. 
Table 1: Feature set for nominal predicate recognition 
 
3.2.2 SRL for Chinese Predicates 
Our Chinese SRL models for both verbal and 
nominal predicates adopt the widely-used SRL 
framework, which divides the task into three 
sequential sub-tasks: argument pruning, argu-
ment identification, and argument classification. 
In particular, we follow Xue (2008) and Li et al 
(2009) to develop verbal and nominal SRL 
models, respectively. Moreover, we have further 
improved the performance of Chinese verbal 
SRL by exploring additional features, e.g., voice 
position that indicates the voice maker (BA, BEI) 
is before or after the constituent in focus, the 
rule that expands the parent of the constituent in 
focus, and the core arguments defined in the 
predicate?s frame file. For nominal SRL, we 
simply use the final feature set of Li et al (2009). 
As a result, our Chinese verbal and nominal SRL 
systems achieve performance of 92.38 and 72.67 
in F1-measure respectively (on golden parse 
trees and golden predicates), which are compa-
rable to Xue (2008) and Li et al (2009). For 
more details, please refer to Xue (2008) and Li 
et al (2009). 
3.3 Pipeline Parsing on Top-best Parse 
Tree 
Similar to most of the state-of-the-art systems 
(Pradhan et al, 2005; Xue, 2008; Li et al, 2009), 
the top-best parse tree is first returned from our 
syntactic parser and then fed into the SRL sys-
tem. Specifically, the verbal (nominal) SRL la-
beler is in charge of verbal (nominal) predicates, 
respectively. For each sentence, since SRL is 
only performed on one parse tree, only con-
stituents in it are candidates for semantic argu-
ments. Therefore, if no constituent in the parse 
tree can map the same text span to an argument 
in the manual annotation, the system will not get 
a correct annotation. 
4 Joint Syntactic and Semantic Parsing 
In this section, we first explore pipeline parsing 
on N-best parse trees, as a natural extension of 
pipeline parsing on the top-best parse tree. Then, 
joint syntactic and semantic parsing is explored 
for Chinese texts from two levels. Firstly, an 
integrated parsing approach to joint syntactic 
and semantic parsing is proposed. Secondly, 
various semantic role-related features are di-
rectly incorporated into the syntactic parsing 
model for better interaction between the two 
tasks. 
4.1 Pipeline Parsing on N-best Parse Trees 
The pipeline parsing approach employed in this 
paper is largely motivated by the general 
framework of re-ranking, as proposed in Sutton 
and McCallum (2005). The idea behind this ap-
proach is that it allows uncertainty about syntac-
tic parsing to be carried forward through an 
N-best list, and that a reliable SRL system, to a 
certain extent, can reflect qualities of syntactic 
parse trees. Given a sentence x, a joint parsing 
model is defined over a semantic frame F and a 
parse tree t in a log-linear way: 
( )
( ) ( ) ( )
, |
1 log | , log |
Score F t x
P F t x P t x? ?= ? +    (1) 
where P(t|x) is returned by a probabilistic syn-
tactic parsing model, e.g., our syntactic parser, 
and P(F|t, x) is returned by a probabilistic se-
mantic parsing model, e.g. our verbal & nominal 
1111
  
SRL systems. In our pipeline parsing approach, 
P(t|x) is calculated as the product of all involved 
decisions? probabilities in the syntactic parsing 
model, and P(F|t, x) is calculated as the product 
of all the semantic role labels? probabilities in a 
sentence (including both verbal and nominal 
SRL). That is to say, we only consider those 
constituents that are supposed to be arguments. 
Here, the parameter ?  is a balance factor in-
dicating the importance of the semantic parsing 
model. 
In particular, (F*, t*) with maximal Score(F, 
t|x) is selected as the final syntactic and seman-
tic parsing results. Given a sentence, N-best 
parse trees are generated first using the syntactic 
parser, and then for each parse tree, we predict 
the best SRL frame using our verbal and nomi-
nal SRL systems. 
4.2 Integrated Parsing 
Although pipeline parsing on N-best parse trees 
could relieve severe dependence on the quality 
of the top-best parse tree, there is still a potential 
drawback: this method suffers from the limited 
scope covered by the N-best parse trees since the 
items in the parse tree list may be too similar, 
especially for long sentences. For example, 
50-best parse trees can only represent a combi-
nation of 5 to 6 binary ambiguities since 2^5 < 
50 < 2^6. 
Ideally, we should perform SRL on as many 
parse trees as possible, so as to enlarge the 
search scope. However, pipeline parsing on all 
possible parse trees is time-consuming and thus 
unrealistic. As an alternative, we turn to inte-
grated parsing, which aims to perform syntactic 
and semantic parsing synchronously. The key 
idea is to construct a parse tree in a bottom-up 
way so that it is feasible to perform SRL at suit-
able moments, instead of only when the whole 
parse tree is built. Integrated parsing is practica-
ble, mostly due to the following two observa-
tions: (1) Given a predicate in a parse tree, its 
semantic arguments are usually siblings of the 
predicate, or siblings of its ancestor. Actually, 
this special observation has been widely em-
ployed in SRL to prune non-arguments for a 
verbal or nominal predicate (Xue, 2008; Li et al, 
2009). (2) SRL feature spaces (both in fea-
ture-based method and kernel-based method) 
mostly focus on the predicate-argument structure 
of a given (predicate, argument) pair. That is to 
say, once a predicate-argument structure is 
formed (i.e., an argument candidate is connected 
with the given predicate), there is enough con-
textual information to predict their SRL relation. 
As far as our syntactic parser is concerned, we 
invoke the SRL systems once a new constituent 
covering a predicate is complete with a ?YES? 
decision from the CHECK procedure. Algorithm 
Algorithm 1. The algorithm integrating syntactic parsing and SRL. 
Assume: 
  t: constituent which is complete with ?YES? decision of CHECK procedure 
  P: number of predicates 
  Pi: ith predicate 
  S: SRL result, set of predicates and its arguments 
BEGIN 
   srl_prob = 0.0; 
   FOR i=1 to P DO 
      IF t covers Pi THEN 
         T = number of children of t; 
         FOR j=1 to T DO 
             IF t?s jth child Chj does not cover Pi THEN 
                 Run SRL given predicate Pi and constituent Chj to get their semantic role 
lbl and its probability prob; 
                 IF lbl does not indicate non-argument THEN 
                    srl_prob += log( prob ); 
                    S = S ? {(Pi, Chj, lbl)}; 
                 END IF 
             END IF 
         END FOR 
      END IF 
   END FOR 
   return srl_prob; 
END 
1112
1 illustrates the integration of syntactic and se-
mantic parsing. For the example shown in Fig-
ure 2, the CHECK procedure predicts a ?YES? 
decision, indicating the immediately proposed 
constituent ?VP (?? /provide ??? /RMB 
??/loan)? is complete. So, at this moment, the 
verbal SRL system is invoked to predict the se-
mantic label of the constituent ?NP (???
/RMB ??/loan)?, given the verbal predicate 
?VV (??/provide)?. Similarly, ?PP (?/to ?
?/N. Korean ??/govt.)? would also be se-
mantically labeled as soon as ?PP (?/to ??/N. 
Korean ??/govt.)? and ?VP (??/provide ?
??/RMB ??/loan)? are merged into a big-
ger VP. In this way, both syntactic and semantic 
parsing are accomplished when the root node 
TOP is formed. It is worth pointing out that all 
features (Xue, 2008; Li et al, 2009) used in our 
SRL model can be instantiated and their values 
are same as the ones when the whole tree is 
available. In particular, the probability computed 
from the SRL model is interpolated with that of 
the syntactic parsing model in a log-linear way 
(with equal weights in our experiments). This is 
due to our hypothesis that the probability re-
turned from SRL model is helpful to joint syn-
tactic and semantic parsing, considering the 
close interaction between the two tasks. 
 
 
4.3 Integrating Semantic Role-related 
Features into Syntactic Parsing Model 
The integrated parsing approach as shown in 
Section 4.2 performs syntactic and semantic 
parsing synchronously. In contrast to traditional 
syntactic parsers where no semantic role-related 
information is used, it may be interesting to in-
vestigate the contribution of such information in 
the syntactic parsing model, due to the availabil-
ity of such information in the syntactic parsing 
process. In addition, it is found that 11% of pre-
dicates in a sentence are speculatively attached 
with two or more core arguments with the same 
label due to semantic parsing errors (partly 
caused by syntactic parsing errors in automatic 
parse trees). This is abnormal since a predicate 
normally only allows at most one argument of 
each core argument role (i.e., Arg0-Arg4). 
Therefore, such syntactic errors should be 
avoidable by considering those arguments al-
ready obtained in the bottom-up parsing process. 
On the other hand, taking those expected seman-
tic roles into account would help the syntactic 
parser. In terms of our syntactic parsing model, 
this is done by directly incorporating various 
semantic role-related features into the syntactic 
parsing model (i.e., the BUILD procedure) when 
the newly-formed constituent covers one or 
more predicates. 
For the example shown in Figure 2, once the 
constituent ?VP (?? /provide ??? /RMB 
??/loan)?, which covers a verbal predicate 
?VV (??/provide)?, is complete, the verbal 
SRL model would be triggered first to mark 
constituent ?NP (???/RMB ??/loan)? as 
ARG1, given predicate ?VV (??/provide)?. 
Then, the BUILD procedure is called to make 
the BUILD decision for the newly-formed con-
stituent ?VP (??/provide ???/RMB ??
/loan)?. Table 2 lists various semantic 
role-related features explored in our syntactic 
parsing model and their instantiations with re-
gard to the example shown in Figure 2. In Table 
2, feature sf4 gives the possible core semantic 
roles that the focus predicate may take, accord-
ing to its frame file; feature sf5 presents the se-
mantic roles that the focus predicate has already 
occupied; feature sf6 indicates the semantic 
roles that the focus predicate is expecting; and 
SF1-SF8 are combined features. Specifically, if 
the current constituent covers n predicates, then 
14 * n features would be instantiated. Moreover, 
we differentiate whether the focus predicate is 
verbal or nominal, and whether it is the head 
word of the current constituent. 
Feature Selection. Some features proposed 
above may not be effective in syntactic parsing. 
Here we adopt the greedy feature selection algo-
rithm as described in Jiang and Ng (2006) to 
select useful features empirically and incremen-
tally according to their contributions on the de-
velopment data. The algorithm repeatedly se-
lects one feature each time which contributes the 
most, and stops when adding any of the remain-
Figure 2: An application of CHECK with YES as the 
decision. Thus, VV (??/provide) and NP (???
/RMB ??/loan) reduce to a big VP. 
P NP 
PP 
Start_VP / NO 
VV NP
???
RMB
??
loan
NN NN?? 
provide 
?
to 
NR NN 
?? 
N. Korean 
?? 
govt. 
? ?
VP YES?
1113
ing features fails to improve the syntactic pars-
ing performance. 
 
Feat. Description 
sf1 Path: the syntactic path from C to P. (VP>VV)
sf2 Predicate: the predicate itself. (??/provide)
sf3 Predicate class (Xue, 2008): the class that P 
belongs to. (C3b) 
sf4 Possible roles: the core semantic roles P may 
take. (Arg0, Arg1, Arg2) 
sf5 Detected roles: the core semantic roles already 
assigned to P. (Arg1) 
sf6 Expected roles:  possible semantic roles P is 
still expecting. (Arg0, Arg2) 
SF1 For each already detected argument, its role 
label + its path from P. (Arg1+VV<VP>NP) 
SF2 sf1 + sf2. (VP>VV+??/provide) 
SF3 sf1 + sf3. (VP>VV+C3b) 
SF4 Combined possible argument roles. 
(Arg0+Arg1+Arg2) 
SF5 Combined detected argument roles. (Arg1) 
SF6 Combined expected argument roles. 
(Arg0+Arg2) 
SF7 For each expected semantic role, sf1 + its role 
label. (VP>VV+Arg0, VP>VV+Arg2) 
SF8 For each expected semantic role, sf2 + its role 
label. 
 (??/provide+Arg0, ??/provide+Arg2) 
Table 2: SRL-related features and their instantiations 
for syntactic parsing, with ?VP (??/provide ??
?/RMB ??/loan)? as the current constituent C 
and ???/provide? as the focus predicate P, based 
on Figure 2. 
5 Experiments and Results 
We have evaluated our integrated parsing ap-
proach on Chinese TreeBank 5.1 and corre-
sponding Chinese PropBank and NomBank.  
5.1 Experimental Settings 
This version of Chinese PropBank and Chinese 
NomBank consists of standoff annotations on 
the file (chtb 001 to 1151.fid) of Chinese Penn 
TreeBank 5.1. Following the experimental set-
tings in Xue (2008) and Li et al (2009), 648 
files (chtb 081 to 899.fid) are selected as the 
training data, 72 files (chtb 001 to 040.fid and 
chtb 900 to 931.fid) are held out as the test data, 
and 40 files (chtb 041 to 080.fid) are selected as 
the development data. In particular, the training, 
test, and development data contain 31,361 
(8,642), 3,599 (1,124), and 2,060 (731) verbal 
(nominal) propositions, respectively. 
For the evaluation measurement on syntactic 
parsing, we report labeled recall, labeled preci-
sion, and their F1-measure. Also, we report re-
call, precision, and their F1-measure for evalua-
tion of SRL on automatic predicates, combining 
verbal SRL and nominal SRL. An argument is 
correctly labeled if there is an argument in man-
ual annotation with the same semantic label that 
spans the same words. Moreover, we also report 
the performance of predicate recognition. To see 
whether an improvement in F1-measure is statis-
tically significant, we also conduct significance 
tests using a type of stratified shuffling which in 
turn is a type of compute-intensive randomized 
tests. In this paper, ?>>>?, ?>>?, and ?>? denote 
p-values less than or equal to 0.01, in-between 
(0.01, 0.05], and bigger than 0.05, respectively. 
We are not aware of any SRL system comb-
ing automatic predicate recognition, verbal SRL 
and nominal SRL on Chinese PropBank and 
NomBank. Xue (2008) experimented independ-
ently with verbal and nominal SRL and assumed 
correct predicates. Li et al (2009) combined 
nominal predicate recognition and nominal SRL 
on Chinese NomBank. The CoNLL-2009 shared 
task (Hajic et al, 2009) included both verbal and 
nominal SRL on dependency parsing, instead of 
constituent-based syntactic parsing. Thus the 
SRL performances of their systems are not di-
rectly comparable to ours. 
5.2 Results and Discussions 
Results of pipeline parsing on N-best parse 
trees. While performing pipeline parsing on 
N-best parse trees, 20-best (the same as the heap 
size in our syntactic parsing) parse trees are ob-
tained for each sentence using our syntactic 
parser as described in Section 3.1. The balance 
factor ?  is set to 0.5 indicating that the two 
components in formula (1) are equally important. 
Table 3 compares the two pipeline parsing ap-
proaches on the top-best parse tree and the 
N-best parse trees. It shows that the approach on 
N-best parse trees outperforms the one on the 
top-best parse tree by 0.42 (>>>) in F1-measure 
on SRL. In addition, syntactic parsing also bene-
fits from the N-best parse trees approach with 
improvement of 0.17 (>>>) in F1-measure. This 
suggests that pipeline parsing on N-best parse 
trees can improve both syntactic and semantic 
parsing. 
It is worth noting that our experimental results 
in applying the re-ranking framework in Chinese 
pipeline parsing on N-best parse trees are very 
encouraging, considering the pessimistic results 
of Sutton and McCallum (2005), in which the 
re-ranking framework failed to improve the per-
formance on English SRL. It may be because, 
1114
unlike Sutton and McCallum (2005), P(F, t|x) 
defined in this paper only considers those con-
stituents which are identified as arguments. This 
can effectively avoid the noises caused by the 
predominant non-argument constituents. More-
over, the huge performance gap between Chi-
nese semantic parsing on the gold parse tree and 
that on the top-best parse tree leaves much room 
for performance improvement. 
 
Method Task R (%) P (%) F1 
Syntactic 76.68 79.12 77.88
SRL 62.96 65.04 63.98
Predicate 94.18 92.28 93.22
V-SRL 65.33 68.52 66.88
V-Predicate 89.52 93.12 91.29
N-SRL 49.58 48.19 48.88
Pipeline on top 
-best parse tree 
N-Predicate 86.83 71.76 78.58
Syntactic 76.89 79.25 78.05
SRL 62.99 65.88 64.40
Predicate 94.07 92.22 93.13
V-SRL 65.41 69.09 67.20
V-Predicate 89.66 93.02 91.31
N-SRL 49.24 49.46 49.35
Pipeline on 20 
-best parse trees 
N-Predicate 86.65 72.15 78.74
Syntactic 77.14 79.01 78.07
SRL 62.67 67.67 65.07
Predicate 93.97 92.42 93.19
V-SRL 65.37 70.27 67.74
V-Predicate 90.08 92.87 91.45
N-SRL 48.02 52.83 50.31
Integrated 
parsing 
N-Predicate 85.41 73.23 78.85
Syntactic 77.47 79.58 78.51
SRL 63.14 68.17 65.56
Predicate 93.97 92.52 93.24
V-SRL 65.74 70.98 68.26
V-Predicate 89.86 93.17 91.49
N-SRL 48.80 52.67 50.66
Integrated 
parsing with 
semantic 
role-related 
features 
N-Predicate 85.85 72.78 78.78
Table 3: Syntactic and semantic parsing performance 
on test data (using gold standard word boundaries). 
?V-? denotes ?verbal? while ?N-?denotes ?nominal?. 
 
Results of integrated parsing. Table 3 also 
compares the integrated parsing approach with 
the two pipeline parsing approaches. It shows 
that the integrated parsing approach improves 
the performance of both syntactic and semantic 
parsing by 0.19 (>) and 1.09 (>>>) respectively 
in F1-measure over the pipeline parsing ap-
proach on the top-best parse tree. It is also not 
surprising to find out that the integrated parsing 
approach outperforms the pipeline parsing ap-
proach on 20-best parse trees by 0.67 (>>>) in 
F1-measure on SRL, due to its exploring a larger 
search space, although the integrated parsing 
approach integrates the SRL probability and the 
syntactic parsing probability in the same manner 
as the pipeline parsing approach on 20-best 
parse trees. However, the syntactic parsing per-
formance gap between the integrated parsing 
approach and the pipeline parsing approach on 
20-best parse trees is negligible.  
Results of integrated parsing with semantic 
role-related features. After performing the 
greedy feature selection algorithm on the devel-
opment data, features {SF3, SF2, sf5, sf6, SF4} 
as proposed in Section 4.3 are sequentially se-
lected for syntactic parsing. As what we have 
assumed, knowledge about the detected seman-
tic roles and expected semantic roles is helpful 
for syntactic parsing. Table 3 also lists the per-
formance achieved with those selected features. 
It shows that the integration of semantic 
role-related features in integrated parsing sig-
nificantly enhances both the performance of syn-
tactic and semantic parsing by 0.44 (>>>) and 
0.49 (>>) respectively in F1-measure. In addi-
tion, it shows that it outperforms the wide-
ly-used pipeline parsing approach on top-best 
parse tree by 0.63 (>>>) and 1.58 (>>>) in 
F1-measure on syntactic and semantic parsing, 
respectively. Finally, it shows that it outper-
forms the widely-used pipeline parsing approach 
on 20-best parse trees by 0.46 (>>>) and 1.16 
(>>>) in F1-measure on syntactic and semantic 
parsing, respectively. This is very encouraging, 
considering the notorious difficulty and 
complexity of both the syntactic and semantic 
parsing tasks. 
Table 3 also shows that our proposed method 
works well for both verbal SRL and nominal 
SRL. In addition, it shows that the performance 
of predicate recognition is very stable due to its 
high dependence on POS tagging results, rather 
than syntactic parsing results. Finally, it is not 
surprising to find out that the performance of 
predicate recognition when mixing verbal and 
nominal predicates is better than the perform-
ance of either verbal predicates or nominal 
predicates.  
5.3 Extending the Word-based Syntactic 
Parser to a Character-based Syntactic Parser 
The above experimental results on a word-based 
syntactic parser (assuming correct word seg-
mentation) show that both syntactic and seman-
tic parsing benefit from our integrated parsing 
approach. However, observing the great chal-
lenge of word segmentation in Chinese informa-
1115
tion processing, it is still unclear whether and 
how much joint learning benefits charac-
ter-based syntactic and semantic parsing. In this 
section, we extended the Ratnaparkhi parser 
(1999) to a character-based parser (with auto-
matic word segmentation), and then examined 
the effectiveness of joint learning.  
Given the three-pass process in the 
word-based syntactic parser, it is easy to extend 
it to a character-based parser for Chinese texts. 
This can be done by only replacing the TAG 
procedure in the first pass with a POSCHUNK 
procedure, which integrates Chinese word seg-
mentation and POS tagging in one step, follow-
ing the method described in (Ng and Low 2004). 
Here, each character is annotated with both a 
boundary tag and a POS tag. The 4 possible 
boundary tags include ?B? for a character that 
begins a word and is followed by another char-
acter, ?M? for a character that occurs in the 
middle of a word, ?E? for a character that ends a 
word, and ?S? for a character that occurs as a 
single-character word. For example, ????
/Beijing city/NR? would be decomposed into 
three units: ? ? /north/B_NR?, ? ?
/capital/M_NR?, and ??/city/E_NR?. Also, ??
/is/VC? would turn into ??/is/S_VC?. Through 
POSCHUNK, all characters in a sentence are 
first assigned with POS chunk labels which must 
be compatible with previous ones, and then 
merged into words with their POS tags. For ex-
ample, ??/north/B_NR?, ??/capital/M_NR?, 
and ??/city/E_NR? will be merged as ????
/Beijing/NR?, ??/is/S_VC? will become ??
/is/VC?. Finally the merged results of the PO-
SCHUNK are fed into the CHUNK procedure of 
the second pass. 
Using the same data split as the previous ex-
periments, word segmentation achieves perfor-
mance of 96.3 in F1-measure on the test data. 
Table 4 lists the syntactic and semantic parsing 
performance by adopting the character-based 
parser.  
Table 4 shows that integrated parsing benefits 
syntactic and semantic parsing when automatic 
word segmentation is considered. However, the 
improvements are smaller due to the extra noise 
caused by automatic word segmentation. For 
example, our experiments show that the per-
formance of predicate recognition drops from 
93.2 to 90.3 in F1-measure when replacing cor-
rect word segmentations with automatic ones. 
 
 
Method Task R (%) P (%) F1 
Syntactic 82.23 84.28 83.24Pipeline on top-best 
parse tree SRL 60.40 62.75 61.55
Syntactic 82.25 84.29 83.26Pipeline on 20-best 
parse trees SRL 60.17 63.63 61.85
Syntactic 82.51 84.31 83.40Integrated parsing  
with semantic 
role-related features
SRL 60.09 65.35 62.61
Table 4: Performance with the character-based pars-
er1 (using automatically recognized word bounda-
ries). 
6 Conclusion 
In this paper, we explore joint syntactic and se-
mantic parsing to improve the performance of 
both syntactic and semantic parsing, in particular 
that of semantic parsing. Evaluation shows that 
our integrated parsing approach outperforms the 
pipeline parsing approach on N-best parse trees, 
a natural extension of the widely-used pipeline 
parsing approach on the top-best parse tree. It 
also shows that incorporating semantic informa-
tion into syntactic parsing significantly improves 
the performance of both syntactic and semantic 
parsing. This is very promising and encouraging, 
considering the complexity of both syntactic and 
semantic parsing. 
To our best knowledge, this is the first suc-
cessful research on exploring syntactic parsing 
and semantic role labeling for verbal and nomi-
nal predicates in an integrated way.  
Acknowledgments 
The first two authors were financially supported 
by Projects 60683150, 60970056, and 90920004 
under the National Natural Science Foundation 
of China. This research was also partially sup-
ported by a research grant R-252-000-225-112 
from National University of Singapore Aca-
demic Research Fund. We also want to thank the 
reviewers for insightful comments. 
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of COLING-ACL 1998. 
Xavier Carreras and Lluis M?rquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2004.  
                                                          
1 POS tags are included in evaluating the perform-
ance of a character-based syntactic parser. Thus it 
cannot be directly compared with the word-based one 
where correct word segmentation is assumed. 
1116
Xavier Carreras and Lluis M?rquez. 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic 
Role Labeling. In Proceedings of CoNLL 2005.  
Eugene Charniak. 2001. Immediate-Head Parsing for 
Language Models. In Proceedings of ACL 2001. 
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania. 
Jenny Rose Finkel and Christopher D. Manning. 
2009. Joint Parsing and Named Entity Recognition. 
In Proceedings of NAACL 2009. 
Jan Hajic, Massimiliano Ciaramita, Richard Johans-
son, et al 2009. The CoNLL-2009 Shared Task: 
Syntactic and Semantic Dependencies in Multiple 
Languages. In Proceedings of CoNLL 2009. 
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic 
Role Labeling of NomBank: A Maximum Entropy 
Approach. In Proceedings of EMNLP 2006.  
Fang Kong, Guodong Zhou, and Qiaoming Zhu. 2009. 
Employing the Centering Theory in Pronoun 
Resolution from the Semantic Perspective. In 
Proceedings of EMNLP 2009.  
Peter Koomen, Vasin Punyakanok, Dan Roth, 
Wen-tau Yih. 2005. Generalized Inference with 
Multiple Semantic Role Labeling Systems. In 
Proceedings of CoNLL 2005. 
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu, 
and Peide Qian. 2009. Improving Nominal SRL in 
Chinese Language with Verbal SRL information 
and Automatic Predicate Recognition. In Pro-
ceedings of EMNLP 2009. 
Chang Liu and Hwee Tou Ng. 2007. Learning Pre-
dictive Structures for Semantic Role Labeling of 
NomBank. In Proceedings of ACL 2007. 
Paola Merlo and Gabriele Mussillo. 2005. Accurate 
Function Parsing. In Proceedings of EMNLP 2005. 
Paola Merlo and Gabriele Musillo. 2008. Semantic 
Parsing for High-Precision Semantic Role Label-
ling. In Proceedings of CoNLL 2008. 
Adam Meyers, Ruth Reeves, Catherine Macleod, 
Rachel Szekely, Veronika Zielinska, Brian Young, 
and Ralph Grishman. 2004. Annotating Noun Ar-
gument Structure for NomBank. In Proceedings of 
LREC 2004. 
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph 
Weischedel. 2000. A Novel Use of Statistical 
Parsing to Extract Information from Text. In Pro-
ceedings of ANLP 2000. 
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion Answering based on Semantic Structures. In 
Proceedings of COLING 2004. 
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese 
Part-of-Speech Tagging: One-at-a-Time or 
All-at-Once? Word-Based or Character-Based? In 
Proceedings of EMNLP 2004. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics, 
31, 71-106. 
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proceesings of 
NAACL 2007. 
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, 
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2005. Support Vector Learning for Semantic 
Argument Classification. Machine Learning, 2005, 
60:11-39.  
Adwait Ratnaparkhi. 1999. Learning to Parse Natural 
Language with Maximum Entropy Models. Ma-
chine Learning, 34, 151-175. 
Mihai Surdeanu, Sanda Harabagiu, John Williams 
and Paul Aarseth. 2003. Using Predi-
cate-Argument Structures for Information Extrac-
tion. In Proceedings of ACL 2003. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of 
Syntactic and Semantic Dependencies. In Pro-
ceedings of CoNLL 2008. 
Charles Sutton and Andrew McCallum. 2005. Joint 
Parsing and Semantic Role Labeling. In Proceed-
ings of CoNLL2005. 
Nianwen Xue and Martha Palmer. 2003. Annotating 
the Propositions in the Penn Chinese TreeBank. In 
Proceedings of the 2nd SIGHAN Workshop on 
Chinese Language Processing. 
Nianwen Xue. 2006. Annotating the Predi-
cate-Argument Structure of Chinese Nominaliza-
tions. In Proceedings of LREC 2006.  
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
Szu-ting Yi and Martha Palmer. 2005. The Integra-
tion of Syntactic Parsing and Semantic Role La-
beling. In Proceedings of CoNLL 2005. 
Yue Zhang and Stephen Clark. 2008. Joint Word 
Segmentation and POS Tagging Using a Single 
Perceptron. In Proceedings of ACL 2008. 
 
1117
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1127?1136,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Cross-Entity Inference to Improve Event Extraction 
Yu Hong     Jianfeng Zhang     Bin Ma     Jianmin Yao     Guodong Zhou     Qiaoming Zhu 
School of Computer Science and Technology, Soochow University, Suzhou City, China 
{hongy, jfzhang, bma, jyao, gdzhou, qmzhu}@suda.edu.cn 
 
 
Abstract 
Event extraction is the task of detecting certain 
specified types of events that are mentioned in 
the source language data. The state-of-the-art 
research on the task is transductive inference 
(e.g. cross-event inference). In this paper, we 
propose a new method of event extraction by 
well using cross-entity inference. In contrast to 
previous inference methods, we regard entity-
type consistency as key feature to predict event 
mentions. We adopt this inference method to 
improve the traditional sentence-level event ex-
traction system. Experiments show that we can 
get 8.6% gain in trigger (event) identification, 
and more than 11.8% gain for argument (role) 
classification in ACE event extraction. 
1 Introduction 
The event extraction task in ACE (Automatic Con-
tent Extraction) evaluation involves three challeng-
ing issues: distinguishing events of different types, 
finding the participants of an event and determin-
ing the roles of the participants. 
The recent researches on the task show the 
availability of transductive inference, such as that 
of the following methods: cross-document, cross-
sentence and cross-event inferences. Transductive 
inference is a process to use the known instances to 
predict the attributes of unknown instances. As an 
example, given a target event, the cross-event in-
ference can predict its type by well using the re-
lated events co-occurred with it within the same 
document. From the sentence: 
(1)He left the company. 
it is hard to tell whether it is a Transport event in 
ACE, which means that he left the place; or an 
End-Position event, which means that he retired 
from the company. But cross-event inference can 
use a related event ?Then he went shopping? within 
the same document to identify it as a Transport 
event correctly. 
As the above example might suggest, the avail-
ability of transductive inference for event extrac-
tion relies heavily on the known evidences of an 
event occurrence in specific condition. However, 
the evidence supporting the inference is normally 
unclear or absent. For instance, the relation among 
events is the key clue for cross-event inference to 
predict a target event type, as shown in the infer-
ence process of the sentence (1). But event relation 
extraction itself is a hard task in Information Ex-
traction. So cross-event inference often suffers 
from some false evidence (viz., misleading by un-
related events) or lack of valid evidence (viz., un-
successfully extracting related events). 
In this paper, we propose a new method of 
transductive inference, named cross-entity infer-
ence, for event extraction by well using the rela-
tions among entities. This method is firstly 
motivated by the inherent ability of entity types in 
revealing event types. From the sentences: 
(2)He left the bathroom. 
(3)He left Microsoft. 
it is easy to identify the sentence (2) as a Transport 
event in ACE, which means that he left the place, 
because nobody would retire (End-Position type) 
from a bathroom. And compared to the entities in 
sentence (1) and (2), the entity ?Microsoft? in (3) 
would give us more confidence to tag the ?left? 
event as an End-Position type, because people are 
used to giving the full name of the place where 
they retired. 
The cross-entity inference is also motivated by 
the phenomenon that the entities of the same type 
often attend similar events. That gives us a way to 
predict event type based on entity-type consistency. 
From the sentence: 
(4)Obama beats McCain. 
it is hard to identify it as an Elect event in ACE, 
which means Obama wins the Presidential Election, 
1127
or an Attack event, which means Obama roughs 
somebody up. But if we have the priori knowledge 
that the sentence ?Bush beats McCain? is an Elect 
event, and ?Obama? was a presidential contender 
just like ?Bush? (strict type consistency), we have 
ample evidence to predict that the sentence (4) is 
also an Elect event. 
Indeed above cross-entity inference for event-
type identification is not the only use of entity-type 
consistency. As we shall describe below, we can 
make use of it at all issues of event extraction: 
y For event type: the entities of the same type 
are most likely to attend similar events. And the 
events often use consistent or synonymous trigger. 
y For event argument (participant): the enti-
ties of the same type normally co-occur with simi-
lar participants in the events of the same type. 
y For argument role: the arguments of the 
same type, for the most part, play the same roles in 
similar events. 
With the help of above characteristics of entity, 
we can perform a step-by-step inference in this 
order:  
y Step 1: predicting event type and labeling 
trigger given the entities of the same type. 
y Step 2: identifying arguments in certain event 
given priori entity type, event type and trigger that 
obtained by step 1. 
y Step 3: determining argument roles in certain 
event given entity type, event type, trigger and ar-
guments that obtained by step 1 and step 2. 
On the basis, we give a blind cross-entity infer-
ence method for event extraction in this paper. In 
the method, we first regard entities as queries to 
retrieve their related documents from large-scale 
language resources, and use the global evidences 
of the documents to generate entity-type descrip-
tions. Second we determine the type consistency of 
entities by measuring the similarity of the type de-
scriptions. Finally, given the priori attributes of 
events in the training data, with the help of the en-
tities of the same type, we perform the step-by-step 
cross-entity inference on the attributes of test 
events (candidate sentences). 
In contrast to other transductive inference meth-
ods on event extraction, the cross-entity inference 
makes every effort to strengthen effects of entities 
in predicting event occurrences. Thus the inferen-
tial process can benefit from following aspects: 1) 
less false evidence, viz. less false entity-type con-
sistency (the key clue of cross-entity inference), 
because the consistency can be more precisely de-
termined with the help of fully entity-type descrip-
tion that obtained based on the related information 
from Web; 2) more valid evidence, viz. more enti-
ties of the same type (the key references for the 
inference), because any entity never lack its con-
geners. 
2 Task Description 
The event extraction task we addressing is that of 
the Automatic Content Extraction (ACE) evalua-
tions, where an event is defined as a specific occur-
rence involving participants. And event extraction 
task requires that certain specified types of events 
that are mentioned in the source language data be 
detected. We first introduce some ACE terminol-
ogy to understand this task more easily: 
y Entity: an object or a set of objects in one of 
the semantic categories of interest, referred to in 
the document by one or more (co-referential) entity 
mentions. 
y Entity mention: a reference to an entity (typi-
cally, a noun phrase). 
y Event trigger: the main word that most clear-
ly expresses an event occurrence (An ACE event 
trigger is generally a verb or a noun). 
y Event arguments: the entity mentions that 
are involved in an event (viz., participants). 
y Argument roles: the relation of arguments to 
the event where they participate. 
y Event mention: a phrase or sentence within 
which an event is described, including trigger and 
arguments. 
The 2005 ACE evaluation had 8 types of events, 
with 33 subtypes; for the purpose of this paper, we 
will treat these simply as 33 separate event types 
and do not consider the hierarchical structure 
among them. Besides, the ACE evaluation plan 
defines the following standards to determine the 
correctness of an event extraction: 
y A trigger is correctly labeled if its event type 
and offset (viz., the position of the trigger word in 
text) match a reference trigger. 
y An argument is correctly identified if its event 
type and offsets match any of the reference argu-
ment mentions, in other word, correctly recogniz-
ing participants in an event. 
y An argument is correctly classified if its role 
matches any of the reference argument mentions. 
Consider the sentence: 
1128
(5) It has refused in the last five years to revoke 
the license of a single doctor for committing medi-
cal errors.1
The event extractor should detect an End-
Position event mention, along with the trigger 
word ?revoke?, the position ?doctor?, the person 
whose license should be revoked, and the time dur-
ing which the event happened: 
 Event type End-Position 
Trigger revoke 
a single doctor Role=Person 
doctor Role=Position Arguments 
the last five years Role=Time-within 
Table 1: Event extraction example 
It is noteworthy that event extraction depends on 
previous phases like name identification, entity 
mention co-reference and classification. Thereinto, 
the name identification is another hard task in ACE 
evaluation and not the focus in this paper. So we 
skip the phase and instead directly use the entity 
labels provided by ACE. 
3 Related Work 
Almost all the current ACE event extraction sys-
tems focus on processing one sentence at a time 
(Grishman et al, 2005; Ahn, 2006; Hardyet al 
2006). However, there have been several studies 
using high-level information from a wider scope:  
Maslennikov and Chua (2007) use discourse 
trees and local syntactic dependencies in a pattern-
based framework to incorporate wider context to 
refine the performance of relation extraction. They 
claimed that discourse information could filter noi-
sy dependency paths as well as increasing the reli-
ability of dependency path extraction. 
Finkel et al (2005) used Gibbs sampling, a sim-
ple Monte Carlo method used to perform approxi-
mate inference in factored probabilistic models. By 
using simulated annealing in place of Viterbi de-
coding in sequence models such as HMMs, CMMs, 
and CRFs, it is possible to incorporate non-local 
structure while preserving tractable inference. 
They used this technique to augment an informa-
tion extraction system with long-distance depend-
ency models, enforcing label consistency and 
extraction template consistency constraints. 
Ji and Grishman (2008) were inspired from the 
hypothesis of ?One Sense Per Discourse? (Ya-
                                                          
1 Selected from the file ?CNN_CF_20030304.1900.02? in 
ACE-2005 corpus. 
rowsky, 1995); they extended the scope from a 
single document to a cluster of topic-related docu-
ments and employed a rule-based approach to 
propagate consistent trigger classification and 
event arguments across sentences and documents. 
Combining global evidence from related docu-
ments with local decisions, they obtained an appre-
ciable improvement in both event and event 
argument identification. 
Patwardhan and Riloff (2009) proposed an event 
extraction model which consists of two compo-
nents: a model for sentential event recognition, 
which offers a probabilistic assessment of whether 
a sentence is discussing a domain-relevant event; 
and a model for recognizing plausible role fillers, 
which identifies phrases as role fillers based upon 
the assumption that the surrounding context is dis-
cussing a relevant event. This unified probabilistic 
model allows the two components to jointly make 
decisions based upon both the local evidence sur-
rounding each phrase and the ?peripheral vision?. 
Gupta and Ji (2009) used cross-event informa-
tion within ACE extraction, but only for recovering 
implicit time information for events. 
Liao and Grishman (2010) propose document 
level cross-event inference to improve event ex-
traction. In contrast to Gupta?s work, Liao do not 
limit themselves to time information for events, but 
rather use related events and event-type consis-
tency to make predictions or resolve ambiguities 
regarding a given event. 
4 Motivation 
In event extraction, current transductive inference 
methods focus on the issue that many events are 
missing or spuriously tagged because the local in-
formation is not sufficient to make a confident de-
cision. The solution is to mine credible evidences 
of event occurrences from global information and 
regard that as priori knowledge to predict unknown 
event attributes, such as that of cross-document 
and cross-event inference methods.  
However, by analyzing the sentence-level base-
line event extraction, we found that the entities 
within a sentence, as the most important local in-
formation, actually contain sufficient clues for 
event detection. It is only based on the premise that 
we know the backgrounds of the entities before-
hand. For instance, if we knew the entity ?vesu-
vius? is an active volcano, we could easily identify 
1129
the word ?erupt?, which co-occurred with the en-
tity, as the trigger of a ?volcanic eruption? event 
but not that of a ?spotty rash?. 
In spite of that, it is actually difficult to use an 
entity to directly infer an event occurrence because 
we normally don?t know the inevitable connection 
between the background of the entity and the event 
attributes. But we can well use the entities of the 
same background to perform the inference. In de-
tail, if we first know entity(a) has the same back-
ground with entity(b), and we also know that 
entity(a), as a certain role, participates in a specific 
event, then we can predict that entity(b) might par-
ticiptes in a similar event as the same role. 
Consider the two sentences2 from ACE corpus: 
(5) American case for war against Saddam. 
(6) Bush should torture the al Qaeda chief op-
erations officer. 
The sentences are two event mentions which 
have the same attributes: 
Event type Attack 
Trigger war 
American Role=Attacker 
(5) 
Arguments 
Saddam Role=Target 
Event type Attack 
Trigger torture 
Bush Role=Attacker 
(6) 
Arguments 
...Qaeda chief ... Role=Target 
Table 2: Cross-entity inference example 
From the sentences, we can find that the entities 
?Saddam? and ?Qaeda chief? have the same back-
ground (viz., terrorist leader), and they are both the 
arguments of Attack events as the role of Target. 
So if we previously know any of the event men-
tions, we can infer another one with the help of the 
entities of the same background. 
In a word, the cross-entity inference, we pro-
posed for event extraction, bases on the hypothesis: 
Entities of the consistent type normally partici-
pate in similar events as the same role. 
As we will introduce below, some statistical da-
ta from ACE training corpus can support the hy-
pothesis, which show the consistency of event type 
and role in event mentions where entities of the 
same type occur. 
4.1 Entity Consistency and Distribution 
Within the ACE corpus, there is a strong entity 
consistency: if one entity mention appears in a type 
                                                          
2 They are extracted from the files ?CNN_CF_20030305.1900. 
00-1? and ?CNN_CF_20030303.1900.06-1? respectively. 
of event, other entity mentions of the same type 
will appear in similar events, and even use the 
same word to trigger the events. To see this we 
calculated the conditional probability (in the ACE 
corpus) of a certain entity type appearing in the 33 
ACE event subtypes. 
0
50
100
150
200
250
Be?Born
M
arry
D
ivorce
Injure
D
ie
Transport
Transfer?
Transfer?
Start?O
rg
M
erge?
D
eclare?
End?O
rg
A
ttack
D
em
onstr
M
eet
Phone?
Start?
End?
N
om
inate
Elect
A
rrest?Jail
Release?
Trial?
Charge?
Sue
Convict
Sentence
Fine
Execute
Extradite
A
cquit
A
ppeal
Pardon
Event typeF
re
qu
en
cy
Population?Center
Exploding
Air
 
Figure 1. Conditional probability of a certain entity 
type appearing in the 33 ACE event subtypes (Here 
only the probabilities of Population-Center, Ex-
ploding and Air entities as examples) 
0
50
100
150
200
250
Person
Place
Buyer
Seller
Beneficiary
Price
A
rtifact
O
rigin
D
estination
G
iver
Recipient
M
oney
O
rg
A
gent
Victim
Instrum
ent
Entity
A
ttacker
Target
D
efendant
A
djudicator
Prosecutor
Plaintiff
Crim
e
Position
Sentence
Vehicle
Tim
e?A
fter
Tim
e?Before
Tim
e?A
t?
Tim
e?A
t?End
Tim
e?
Tim
e?
Tim
e?H
olds
Tim
e?
RoleF
re
qu
en
cy
Population?Center
Exploding
Air
 
Figure 2. Conditional probability of an entity type 
appearing as the 34 ACE role types (Here only the 
probabilities of Population-Center, Exploding and 
Air entities as examples) 
As there are 33 event subtypes and 43 entity 
types, there are potentially 33*43=1419 entity-
event combinations. However, only a few of these 
appear with substantial frequency. For example, 
the Population-Center entities only occur in 4 
types of event mentions with the conditional prob-
ability more than 0.05. From Table 3, we can find 
that only Attack and Transport events co-occur 
frequently with Population-Center entities (see 
Figure 1 and Table 3). 
Event Cond.Prob. Freq. 
Transport 0.368 197 
Attack 0.295 158 
Meet 0.073 39 
Die 0.069 37 
Table 3: Events co-occurring with Population-
Center with the conditional probability > 0.05 
Actually we find that most entity types appear in 
more restricted event mentions than Population-
Center entity. For example, Air entity only co-
occurs with 5 event types (Attack, Transport, Die, 
Transfer-Ownership and Injure), and Exploding 
1130
entity co-occurs with 4 event types (see Figure 1). 
Especially, they only co-occur with one or two 
event types with the conditional probability more 
than 0.05. 
 Evnt.<=5 5<Evnt.<=10 Evnt.>10 
Freq. > 0 24 7 12 
Freq. >10 37 4 2 
Freq. >50 41 1 1 
Table 4: Distribution of entity-event combination 
corresponding to different co-occurrence frequency 
Table 4 gives the distributions of whole ACE 
entity types co-occurring with event types. We can 
find that there are 37 types of entities (out of 43 in 
total) appearing in less than 5 types of event men-
tions when entity-event co-occurrence frequency is 
larger than 10, and only 2 (e.g. Individual) appear-
ing in more than 10 event types. And when the fre-
quency is larger than 50, there are 41 (95%) entity 
types co-occurring with less than 5 event types. 
These distributions show the fact that most in-
stances of a certain entity type normally participate 
in events of the same type. And the distributions 
might be good predictors for event type detection 
and trigger determination. 
Air (Entity type) 
Attack 
event 
Fighter plane (subtype 1): 
?MiGs? ?enemy planes? ?warplanes? ?allied 
aircraft? ?U.S. jets? ?a-10 tank killer? ?b-1 
bomber? ?a-10 warthog? ?f-14 aircraft? 
?apache helicopter? 
Spacecraft (subtype 2): 
?russian soyuz capsule? ?soyuz? 
Civil aviation (subtype 3): 
?airliners? ?the airport? ?Hooters Air execu-
tive? 
Transport 
event 
Private plane (subtype 4): 
?Marine One? ?commercial flight? ?private 
plane? 
Table 5: Event types co-occurred with Air entities 
Besides, an ACE entity type actually can be di-
vided into more cohesive subtypes according to 
similarity of background of entity, and such a sub-
type nearly always co-occur with unique event 
type. For example, the Air entities can be roughly 
divided into 4 subtypes: Fighter plane, Spacecraft, 
Civil aviation and Private plane, within which the 
Fighter plane entities all appear in Attack event 
mentions, and other three subtypes all co-occur 
with Transport events (see Table 5). This consis-
tency of entities in a subtype is helpful to improve 
the precision of the event type predictor. 
4.2 Role Consistency and Distribution 
The same thing happens for entity-role combina-
tions: entities of the same type normally play the 
same role, especially in the event mentions of the 
same type. For example, the Population-Center 
entities occur in ACE corpus as only 4 role types: 
Place, Destination, Origin and Entity respectively 
with conditional probability 0.615, 0.289, 0.093, 
0.002 (see Figure 2). And They mainly appear in 
Transport event mentions as Place, and in Attack 
as Destination. Particularly the Exploding entities 
only occur as Instrument and Artifact respectively 
with the probability 0.986 and 0.014. They almost 
entirely appear in Attack events as Instrument. 
 Evnt.<=5 5<Evnt.<=10 Evnt.>10 
Freq. > 0 32 5 6 
Freq. >10 38 3 2 
Freq. >50 42 1 0 
Table 6: Distribution of entity-role combination 
corresponding to different co-occurrence frequency 
Table 6 gives the distributions of whole entity-
role combinations in ACE corpus. We can find that 
there are 38 entity types (out of 43 in total) occur 
as less than 5 role types when the entity-role co-
occurrence frequency is larger than 10. There are 
42 (98%) when the frequency is larger than 50, and 
only 2 (e.g. Individual) when larger than 10. The 
distributions show that the instances of an entity 
type normally occur as consistent role, which is 
helpful for cross-entity inference to predict roles. 
5 Cross-entity Approach  
In this section we present our approach to using 
blind cross-entity inference to improve sentence-
level ACE event extraction. 
Our event extraction system extracts events in-
dependently for each sentence, because the defini-
tion of event mention constrains them to appear in 
the same sentence. Every sentence that at least in-
volves one entity mention will be regarded as a 
candidate event mention, and a randomly selected 
entity mention from the candidate will be the star-
ing of the whole extraction process. For the entity 
mention, information retrieval is used to mine its 
background knowledge from Web, and its type is 
determined by comparing the knowledge with 
those in training corpus. Based on the entity type, 
the extraction system performs our step-by-step 
cross-entity inference to predict the attributes of 
1131
the candidate event mention: trigger, event type, 
arguments, roles and whether or not being an event 
mention. The main frame of our event extraction 
system is shown in Figure 3, which includes both 
training and testing processes. 
 
Figure 3. The frame of cross-entity inference for event extraction (including training and testing processes) 
In the training process, for every entity type in 
the ACE training corpus, a clustering technique 
(CLUTO toolkit)3 is used to divide it into different 
cohesive subtypes, each of which only contains the 
entities of the same background. For instance, the 
Air entities will be divided into Fighter plane, 
Spacecraft, Civil aviation, Private plane, etc (see 
Table 5). And for each subtype, we mine event 
mentions where this type of entities appear from 
ACE training corpus, and extract all the words 
which trigger the events to establish corresponding 
trigger list. Besides, a set of support vector ma-
chine (SVM) based classifiers are also trained: 
y Argument Classifier: to distinguish arguments 
of a potential trigger from non-arguments4; 
y Role Classifier: to classify arguments by ar-
gument role; 
y Reportable-Event Classifier (Trigger Classi-
fier): Given entity types, a potential trigger, an 
event type, and a set of arguments, to determine 
whether there is a reportable event mention. 
                                                          
3http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=h
tml&identifier=ADA439508 
4 It is noteworthy that a sentence may include more than one 
event (more than one trigger). So it is necessary to distinguish 
arguments of a potential trigger from that of others. 
In the test process, for each candidate event 
mention, our event extraction system firstly pre-
dicts its triggers and event types: given an ran-
domly selected entity mention from the candidate, 
the system determines the entity subtype it belong-
ing to and the corresponding trigger list, and then 
all non-entity words in the candidate are scanned 
for a instance of triggers from the list. When an 
instance is found, the system tags the candidate as 
the event type that the most frequently co-occurs 
with the entity subtype in the events that triggered 
by the instance. Secondly the argument classifier is 
applied to the remaining mentions in the candidate; 
for any argument passing that classifier, the role 
classifier is used to assign a role to it. Finally, once 
all arguments have been assigned, the reportable-
event classifier is applied to the candidate; if the 
result is successful, this event mention is reported. 
5.1 Further Division of Entity Type  
One of the most important pretreatments before 
our blind cross-entity inference is to divide the 
ACE entity type into more cohesive subtype. The 
greater consistency among backgrounds of entities 
in such a subtype might be good to improve the 
precision of cross-entity inference.  
1132
For each ACE entity type, we collect all entity 
mentions of the type from training corpus, and re-
gard each such mention as a query to retrieve the 
50 most relevant documents from Web. Then we 
select 50 key words that the most weighted by 
TFIDF in the documents to roughly describe back-
ground of entity. After establishing the vector 
space model (VSM) for each entity mention of the 
type, we adopt a clustering toolkit (CLUTO) to 
further divide the mentions into different subtypes. 
Finally, for each subtype, we describe its centroid 
by using 100 key words which the most frequently 
occurred in relevant documents of entities of the 
subtype. 
In the test process, for an entity mention in a 
candidate event mention, we determine its type by 
comparing its background against all centroids of 
subtypes in training corpus, and the subtype whose 
centroid has the most Cosine similarity with the 
background will be assigned to the entity. It is 
noteworthy that global information from the Web 
is only used to measure the entity-background con-
sistency and not directly in the inference process. 
Thus our event extraction system actually still per-
forms a sentence-level inference based on local 
information. 
5.2 Cross-Entity Inference 
Our event extraction system adopts a step-by-
step cross-entity inference to predict event. As dis-
cussed above, the first step is to determine the trig-
ger in a candidate event mention and tag its event 
type based on consistency of entity type. Given the 
domain of event mention that restrained by the 
known trigger, event type and entity subtype, the 
second step is to distinguish the most probable ar-
guments that co-occurring in the domain from the 
non-arguments. Then for each of the arguments, 
the third step can use the co-occurring arguments 
in the domain as important contexts to predict its 
role. Finally, the inference process determines 
whether the candidate is a reportable event men-
tion according to a confidence coefficient. In the 
following sections, we focus on introducing the 
three classifiers: argument classifier, role classifier 
and reportable-event classifier. 
5.2.1   Cross-Entity Argument Classifier 
For a candidate event mention, the first step 
gives its event type, which roughly restrains the 
domain of event mentions where the arguments of 
the candidate might co-occur. On the basis, given 
an entity mention in the candidate and its type (see 
the pretreatment process in section 5.1), the argu-
ment classifier could predict whether other entity 
mentions co-occur with it in such a domain, if yes, 
all the mentions will be the arguments of the can-
didate. In other words, if we know an entity of a 
certain type participates in some event, we will 
think of what entities also should participate in the 
event. For instance, when we know a defendant 
goes on trial, we can conclude that the judge, law-
yer and witness should appear in court. 
Argument Classifier 
Feature 1: an event type (an event-mention domain) 
Feature 2: an entity subtype 
Feature 3: entity-subtype co-occurrence in domain 
Feature 4: distance to trigger 
Feature 5: distances to other arguments 
Feature 6: co-occurrence with trigger in clause 
Role Classifier 
Feature 1 and Feature 2 
Feature 7: entity-subtypes of arguments 
Reportable-Event Classifier 
Feature 1 
Feature 8: confidence coefficient of trigger in domain 
Feature 9: confidence coefficient of role in domain 
Table 7: Features selected for SVM-based cross-
entity classifiers 
A SVM-based argument classifier is used to de-
termine arguments of candidate event mention. 
Each feature of this classifier is the conjunction of: 
y The subtype of an entity 
y The event type we are trying to assign an ar-
gument to 
y A binary indicator of whether this entity sub-
type co-occurs with other subtypes in such an 
event type (There are 266 entity subtypes, and so 
266 features for each instance) 
Some minor features, such as another binary indi-
cator of whether arguments co-occur with trigger 
in the same clause (see Table 7). 
5.2.2 Cross-Entity Role Classifier 
For a candidate event mention, the arguments 
that given by the second step (argument classifier) 
provide important contextual information for pre-
dicting what role the local entity (also one of the 
arguments) takes on. For instance, when citizens 
(Arg1) co-occur with terrorist (Arg2), most likely 
the role of Arg1 is Victim. On the basis, with the 
help of event type, the prediction might be more 
1133
precise. For instance, if the Arg1 and Arg2 co-
occur in an Attack event mention, we will have 
more confidence in the Victim role of Arg1. 
Besides, as discussed in section 4, entities of the 
same type normally take on the same role in simi-
lar events, especially when they co-occur with sim-
ilar arguments in the events (see Table 2). 
Therefore, all instances of co-occurrence model 
{entity subtype, event type, arguments} in training 
corpus could provide effective evidences for pre-
dicting the role of argument in the candidate event 
mention. Based on this, we trained a SVM-based 
role classifier which uses following features: 
y Feature 1 and Feature 2 (see Table 7) 
y Given the event domain that restrained by the 
entity and event types, an indicator of what sub-
types of arguments appear in the domain. (266 en-
tity subtypes make 266 features for each instance) 
5.2.3 Reportable-Event Classifier 
At this point, there are still two issues need to be 
resolved. First, some triggers are common words 
which often mislead the extraction of candidate 
event mention, such as ?it?, ?this?, ?what?, etc. 
These words only appear in a few event mentions 
as trigger, but when they once appear in trigger list, 
a large quantity of noisy sentences will be regarded 
as candidates because of their commonness in sen-
tences. Second, some arguments might be tagged 
as more than one role in specific event mentions, 
but as ACE event guideline, one argument only 
takes on one role in a sentence. So we need to re-
move those with low confidence. 
A confidence coefficient is used to distinguish 
the correct triggers and roles from wrong ones. The 
coefficient calculate the frequency of a trigger (or a 
role) appearing in specific domain of event men-
tions and that in whole training corpus, then com-
bines them to represent its confidence degree, just 
like TFIDF algorithm. Thus, the more typical trig-
gers (or roles) will be given high confidence. 
Based on the coefficient, we use a SVM-based 
classifier to determine the reportable events. Each 
feature of this classifier is the conjunction of: 
y An event type (domain of event mentions) 
y Confidence coefficients of triggers in domain 
y Confidence coefficients of roles in the domain. 
6 Experiments 
We followed Liao (2010)?s evaluation and ran-
domly select 10 newswire texts from the ACE 
2005 training corpus as our development set, 
which is used for parameter tuning, and then con-
duct a blind test on a separate set of 40 ACE 2005 
newswire texts. We use the rest of the ACE train-
ing corpus (549 documents) as training data for our 
event extraction system.  
To compare with the reported work on cross-
event inference (Liao, 2010) and its sentence-level 
baseline system, we cross-validate our method on 
10 separate sets of 40 ACE texts, and report the 
optimum, worst and mean performances (see Table 
8) on the data by using Precision (P), Recall (R) 
and F-measure (F). In addition, we also report the 
performance of two human annotators on 40 ACE 
newswire texts (a random blind test set): one 
knows the rules of event extraction; the other 
knows nothing about it. 
6.1 Main Results  
From the results presented in Table 8, we can 
see that using the cross-entity inference, we can 
improve the F score of sentence-level event extrac-
tion for trigger classification by 8.59%, argument 
classification by 11.86%, and role classification by 
11.9% (mean performance). Compared to the 
cross-event inference, we gains 2.87% improve-
ment for argument classification, and 3.81% for 
role classification (mean performance). Especially, 
our worst results also have better performances 
than cross-event inference. 
Nonetheless, the cross-entity inference has 
worse F score for trigger determination. As we can 
see, the low Recall score weaken its F score (see 
Table 8). Actually, we select the sentence which at 
least includes one entity mention as candidate 
event mention, but lots of event mentions in ACE 
never include any entity mention. Thus we have 
missed some mentions at the starting of inference 
process. 
In addition, the annotator who knows the rules 
of event extraction has a similar performance trend 
with systems: high for trigger classification, mid-
dle for argument classification, and low for role 
classification (see Table 8). But the annotator who 
never works in this field obtains a different trend: 
higher performance for argument classification. 
This phenomenon might prove that the step-by-
step inference is not the only way to predicate 
event mention because human can determine ar-
guments without considering triggers and event 
types. 
1134
                            Performance 
System/Human Trigger (%) Argument (%) Role (%) 
 P R F P R F P R F 
Sentence-level baseline 67.56 53.54 59.74 46.45 37.15 41.29 41.02 32.81 36.46
Cross-event inference 68.71 68.87 68.79 50.85 49.72 50.28 45.06 44.05 44.55
Cross-entity inference (optimum) 73.4 66.2 69.61 56.96 55.1 56 49.3 46.59 47.9 
Cross-entity inference (worst) 71.3 64.17 66.1 51.28 50.3 50.78 46.3 44.3 45.28
Cross-entity inference (mean) 72.9 64.3 68.33 53.4 52.9 53.15 51.6 45.5 48.36
Human annotation 1 (blind) 58.9 59.1 59.0 62.6 65.9 64.2 50.3 57.69 53.74
Human annotation 2 (know rules) 74.3 76.2 75.24 68.5 75.8 71.97 61.3 68.8 64.86
Table 8: Overall performance on blind test data
6.2 Influence of Clustering on Inference  
A main part of our blind inference system is the 
entity-type consistency detection, which relies 
heavily on the correctness of entity clustering and 
similarity measurement. In training, we used 
CLUTO clustering toolkit to automatically gener-
ate different types of entities based on their back-
ground-similarities. In testing, we use K-nearest 
neighbor algorithm to determine entity type. 
Fighter plane (subtype 1 in Air entities): 
?warplanes? ?allied aircraft? ?U.S. jets? ?a-10 tank killer? 
?b-1 bomber? ?a-10 warthog? ?f-14 aircraft? ?apache heli-
copter? ?terrorist? ?Saddam? ?Saddam Hussein? ?Bagh-
dad??
Table 9: Noises in subtype 1 of ?Air? entities (The 
blod fonts are noises) 
We obtained 129 entity subtypes from training 
set. By randomly inspecting 10 subtypes, we found 
nearly every subtype involves no less than 19.2% 
noises. For example, the subtype 1 of ?Air? in Ta-
ble 5 lost the entities of ?MiGs? and ?enemy 
planes?, but involved ?terrorist?, ?Saddam?, etc 
(See Table 9). Therefore, we manually clustered 
the subtypes and retry the step-by-step cross-entity 
inference. The results (denoted as ?Visible 1?) are 
shown in Table 10, within which, we additionally 
show the performance of the inference on the 
rough entity types provided by ACE (denoted as 
?Visible 2?), such as the type of ?Air?, ?Popula-
tion-Center?, ?Exploding?, etc., which normally 
can be divided into different more cohesive sub-
types. And the ?Blind? in Table 10 denotes the 
performances on our subtypes obtained by CLUTO. 
It is surprised that the performances (see Table 
10, F-score) on ?Visible 1? entity subtypes are just 
a little better than ?Blind? inference. So it seems 
that the noises in our blind entity types (CLUTO 
clusters) don?t hurt the inference much. But by re-
inspecting the ?Visible 1? subtypes, we found that 
their granularities are not enough small: the 89 
manual entity clusters actually can be divided into 
more cohesive subtypes. So the improvements of 
inference on noise-free ?Visible 1? subtypes are 
partly offset by loss on weakly consistent entities 
in the subtypes. It can be proved by the poor per-
formances on ?Visible 2? subtypes which are much 
more general than ?Visible 1?. Therefore, a rea-
sonable clustering method is important in our in-
ference process. 
F-score Trigger  Argument Role 
Blind 68.33 53.15 48.36 
Visible 1 69.15 53.65 48.83 
Visible 2 51.34 43.40 39.95 
Table 10: Performances on visible VS blind  
7 Conclusions and Future Work  
We propose a blind cross-entity inference method 
for event extraction, which well uses the consis-
tency of entity mention to achieve sentence-level 
trigger and argument (role) classification. Experi-
ments show that the method has better perform-
ance than cross-document and cross-event 
inferences in ACE event extraction. 
The inference presented here only considers the 
helpfulness of entity types of arguments to role 
classification. But as a superior feature, contextual 
roles can provide more effective assistance to role 
determination of local argument. For instance, 
when an Attack argument appears in a sentence, a 
Target might be there. So if we firstly identify 
simple roles, such as the condition that an argu-
ment has only a single role, and then use the roles 
as priori knowledge to classify hard ones, may be 
able to further improve performance.
Acknowledgments 
We thank Ruifang He. And we acknowledge the 
support of the National Natural Science Founda-
tion of China under Grant Nos. 61003152, 
60970057, 90920004. 
1135
References  
David Ahn. 2006. The stages of event extraction. In 
Proc. COLING/ACL 2006 Workshop on Annotating 
and Reasoning about Time and Events.Sydney, Aus-
tralia. 
Jenny Rose Finkel, Trond Grenager and Christopher 
Manning. 2005. Incorporating Non-local Information 
into Information Extraction Systems by Gibbs Sam-
pling. In Proc. 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 363?370, 
Ann Arbor, MI, June. 
Prashant Gupta and Heng Ji. 2009. Predicting Unknown 
Time Arguments based on Cross-Event Propagation. 
In Proc. ACL-IJCNLP 2009. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System Description. 
In Proc. ACE 2005 Evaluation Workshop, Gaithers-
burg, MD. 
Hilda Hardy, Vika Kanchakouskaya and Tomek Strzal-
kowski. 2006. Automatic Event Classification Using 
Surface Text Features. In Proc. AAAI06 Workshop on 
Event Extraction and Synthesis. Boston, MA. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proc. ACL-08: HLT, pages 254?262, Columbus, OH, 
June. 
Shasha Liao and Ralph Grishman. 2010. Using Docu-
ment Level Cross-Event Inference to Improve Event 
Extraction. In Proc. ACL-2010, pages 789-797, Upp-
sala, Sweden, July. 
Mstislav Maslennikov and Tat-Seng Chua. 2007. A 
Multi resolution Framework for Information Extrac-
tion from Free Text. In Proc. 45th Annual Meeting of 
the Association of Computational Linguistics, pages 
592?599, Prague, Czech Republic, June. 
Siddharth Patwardhan and Ellen Riloff. 2007. Effective 
Information Extraction with Semantic Affinity Pat-
terns and Relevant Regions. In Proc. Joint Confer-
ence on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, 2007, pages 717?727, Prague, Czech Re-
public, June. 
Siddharth Patwardhan and Ellen Riloff. 2009. A Unified 
Model of Phrasal and Sentential Evidence for Infor-
mation Extraction. In Proc. Conference on Empirical 
Methods in Natural Language Processing 2009, 
(EMNLP-09). 
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Proc. 
ACL 1995. Cambridge, MA. 
1136
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33?37,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Head-Driven Hierarchical Phrase-based Translation
Junhui Li Zhaopeng Tu? Guodong Zhou? Josef van Genabith
Centre for Next Generation Localisation
School of Computing, Dublin City University
? Key Lab. of Intelligent Info. Processing
Institute of Computing Technology, Chinese Academy of Sciences
?School of Computer Science and Technology
Soochow University, China
{jli,josef}@computing.dcu.ie
tuzhaopeng@ict.ac.cn gdzhou@suda.edu.cn
Abstract
This paper presents an extension of Chi-
ang?s hierarchical phrase-based (HPB) model,
called Head-Driven HPB (HD-HPB), which
incorporates head information in translation
rules to better capture syntax-driven infor-
mation, as well as improved reordering be-
tween any two neighboring non-terminals at
any stage of a derivation to explore a larger
reordering search space. Experiments on
Chinese-English translation on four NIST MT
test sets show that the HD-HPB model signifi-
cantly outperforms Chiang?s model with aver-
age gains of 1.91 points absolute in BLEU.
1 Introduction
Chiang?s hierarchical phrase-based (HPB) transla-
tion model utilizes synchronous context free gram-
mar (SCFG) for translation derivation (Chiang,
2005; Chiang, 2007) and has been widely adopted
in statistical machine translation (SMT). Typically,
such models define two types of translation rules:
hierarchical (translation) rules which consist of both
terminals and non-terminals, and glue (grammar)
rules which combine translated phrases in a mono-
tone fashion. Due to lack of linguistic knowledge,
Chiang?s HPB model contains only one type of non-
terminal symbol X , often making it difficult to se-
lect the most appropriate translation rules.1 What
is more, Chiang?s HPB model suffers from limited
phrase reordering combining translated phrases in a
monotonic way with glue rules. In addition, once a
1Another non-terminal symbol S is used in glue rules.
glue rule is adopted, it requires all rules above it to
be glue rules.
One important research question is therefore how
to refine the non-terminal category X using linguis-
tically motivated information: Zollmann and Venu-
gopal (2006) (SAMT) e.g. use (partial) syntactic
categories derived from CFG trees while Zollmann
and Vogel (2011) use word tags, generated by ei-
ther POS analysis or unsupervised word class in-
duction. Almaghout et al (2011) employ CCG-
based supertags. Mylonakis and Sima?an (2011) use
linguistic information of various granularities such
as Phrase-Pair, Constituent, Concatenation of Con-
stituents, and Partial Constituents, where applica-
ble. Inspired by previous work in parsing (Char-
niak, 2000; Collins, 2003), our Head-Driven HPB
(HD-HPB) model is based on the intuition that lin-
guistic heads provide important information about a
constituent or distributionally defined fragment, as
in HPB. We identify heads using linguistically mo-
tivated dependency parsing, and use their POS to
refine X. In addition HD-HPB provides flexible re-
ordering rules freely mixing translation and reorder-
ing (including swap) at any stage in a derivation.
Different from the soft constraint modeling
adopted in (Chan et al, 2007; Marton and Resnik,
2008; Shen et al, 2009; He et al, 2010; Huang et
al., 2010; Gao et al, 2011), our approach encodes
syntactic information in translation rules. However,
the two approaches are not mutually exclusive, as
we could also include a set of syntax-driven features
into our translation model. Our approach maintains
the advantages of Chiang?s HPB model while at the
same time incorporating head information and flex-
33
 ??/NR 
Ouzhou 
??/NN 
baguo 
??/AD 
lianming 
??/VV 
zhichi 
??/NR 
meiguo 
??/NN 
lichang 
root 
Eight European countries jointly support America?s stand 
Figure 1: An example word alignment for a Chinese-
English sentence pair with the dependency parse tree for
the Chinese sentence. Here, each Chinese word is at-
tached with its POS tag and Pinyin.
ible reordering in a derivation in a natural way. Ex-
periments on Chinese-English translation using four
NIST MT test sets show that our HD-HPB model
significantly outperforms Chiang?s HPB as well as a
SAMT-style refined version of HPB.
2 Head-Driven HPB Translation Model
Like Chiang (2005) and Chiang (2007), our HD-
HPB translation model adopts a synchronous con-
text free grammar, a rewriting system which gen-
erates source and target side string pairs simulta-
neously using a context-free grammar. Instead of
collapsing all non-terminals in the source language
into a single symbol X as in Chiang (2007), given a
word sequence f ij from position i to position j, we
first find heads and then concatenate the POS tags
of these heads as f ij?s non-terminal symbol. Specif-
ically, we adopt unlabeled dependency structure to
derive heads, which are defined as:
Definition 1. For word sequence f ij , word
fk (i ? k ? j) is regarded as a head if it is domi-
nated by a word outside of this sequence.
Note that this definition (i) allows for a word se-
quence to have one or more heads (largely due to
the fact that a word sequence is not necessarily lin-
guistically constrained) and (ii) ensures that heads
are always the highest heads in the sequence from a
dependency structure perspective. For example, the
word sequence ouzhou baguo lianming in Figure 1
has two heads (i.e., baguo and lianming, ouzhou is
not a head of this sequence since its headword baguo
falls within this sequence) and the non-terminal cor-
responding to the sequence is thus labeled as NN-
AD. It is worth noting that in this paper we only
refine non-terminal X on the source side to head-
informed ones, while still usingX on the target side.
According to the occurrence of terminals in
translation rules, we group rules in the HD-HPB
model into two categories: head-driven hierarchical
rules (HD-HRs) and non-terminal reordering rules
(NRRs), where the former have at least one terminal
on both source and target sides and the later have no
terminals. For rule extraction, we first identify ini-
tial phrase pairs on word-aligned sentence pairs by
using the same criterion as most phrase-based trans-
lation models (Och and Ney, 2004) and Chiang?s
HPB model (Chiang, 2005; Chiang, 2007). We
extract HD-HRs and NRRs based on initial phrase
pairs, respectively.
2.1 HD-HRs: Head-Driven Hierarchical Rules
As mentioned, a HD-HR has at least one terminal
on both source and target sides. This is the same
as the hierarchical rules defined in Chiang?s HPB
model (Chiang, 2007), except that we use head POS-
informed non-terminal symbols in the source lan-
guage. We look for initial phrase pairs that contain
other phrases and then replace sub-phrases with POS
tags corresponding to their heads. Given the word
alignment in Figure 1, Table 1 demonstrates the dif-
ference between hierarchical rules in Chiang (2007)
and HD-HRs defined here.
Similar to Chiang?s HPB model, our HD-HPB
model will result in a large number of rules causing
problems in decoding. To alleviate these problems,
we filter our HD-HRs according to the same con-
straints as described in Chiang (2007). Moreover,
we discard rules that have non-terminals with more
than four heads.
2.2 NRRs: Non-terminal Reordering Rules
NRRs are translation rules without terminals. Given
an initial phrase pair on the source side, there are
four possible positional relationships for their target
side translations (we use Y as a variable for non-
terminals on the source side while all non-terminals
on the target side are labeled as X):
? Monotone ?Y ? Y1Y2, X ? X1X2?;
? Discontinuous monotone
?Y ? Y1Y2, X ? X1 . . . X2?;
? Swap ?Y ? Y1Y2, X ? X2X1?;
? Discontinuous swap
?Y ? Y1Y2, X ? X2 . . . X1?.
34
phrase pairs hierarchical rule head-driven hierarchical rule
lichang, stand X?lichang, stand
NN?lichang,
X?stand
meiguo lichang1, America?s stand1 X?meiguo X1, America?s X1
NN?meiguo NN1,
X?America?s X1
zhichi meiguo, support America?s X?zhichi meiguo, support America?s
VV-NR?zhichi meiguo,
X?support America?s
zhichi meiguo1 lichang,
support America?s1 stand
X?X1 lichang,
X1 stand
VV?VV-NR1 lichang,
X?X1 stand
Table 1: Comparison of hierarchical rules in Chiang (2007) and HD-HRs. Indexed underlines indicate sub-phrases
and corresponding non-terminal symbols. The non-terminals in HD-HRs (e.g., NN, VV, VV-NR) capture the head(s)
POS tags of the corresponding word sequence in the source language.
Merging two neighboring non-terminals into a
single non-terminal, NRRs enable the translation
model to explore a wider search space. During train-
ing, we extract four types of NRRs and calculate
probabilities for each type. To speed up decoding,
we currently (i) only use monotone and swap NRRs
and (ii) limit the number of non-terminals in a NRR
to 2.
2.3 Features and Decoding
Given e for the translation output in the target lan-
guage, s and t for strings of terminals and non-
terminals on the source and target side, respectively,
we use a feature set analogous to the default feature
set of Chiang (2007), including:
? Phd-hr (t|s) and Phd-hr (s|t), translation probabili-
ties for HD-HRs;
? Plex (t|s) and Plex (s|t), lexical translation proba-
bilities for HD-HRs;
? Ptyhd-hr = exp (?1), rule penalty for HD-HRs;
? Pnrr (t|s), translation probability for NRRs;
? Ptynrr = exp (?1), rule penalty for NRRs;
? Plm (e), language model;
? Ptyword (e) = exp (?|e|), word penalty.
Our decoder is based on CKY-style chart parsing
with beam search and searches for the best deriva-
tion bottom-up. For a source span [i, j], it applies
both types of HD-HRs and NRRs. However, HD-
HRs are only applied to generate derivations span-
ning no more than K words ? the initial phrase
length limit used in training to extract HD-HRs ?
while NRRs are applied to derivations spanning any
length. Unlike in Chiang?s HPB model, it is pos-
sible for a non-terminal generated by a NRR to be
included afterwards by a HD-HR or another NRR.
3 Experiments
We evaluate the performance of our HD-HPB model
and compare it with our implementation of Chiang?s
HPB model (Chiang, 2007), a source-side SAMT-
style refined version of HPB (SAMT-HPB), and the
Moses implementation of HPB. For fair compari-
son, we adopt the same parameter settings for our
HD-HPB and HPB systems, including initial phrase
length (as 10) in training, the maximum number of
non-terminals (as 2) in translation rules, maximum
number of non-terminals plus terminals (as 5) on
the source, beam threshold ? (as 10?5) (to discard
derivations with a score worse than ? times the best
score in the same chart cell), beam size b (as 200)
(i.e. each chart cell contains at most b derivations).
For Moses HPB, we use ?grow-diag-final-and? to
obtain symmetric word alignments, 10 for the max-
imum phrase length, and the recommended default
values for all other parameters.
We train our model on a dataset with ?1.5M sen-
tence pairs from the LDC dataset.2 We use the
2002 NIST MT evaluation test data (878 sentence
pairs) as the development data, and the 2003, 2004,
2005, 2006-news NIST MT evaluation test data
(919, 1788, 1082, and 616 sentence pairs, respec-
tively) as the test data. To find heads, we parse the
source sentences with the Berkeley Parser3 (Petrov
and Klein, 2007) trained on Chinese TreeBank 6.0
and use the Penn2Malt toolkit4 to obtain (unlabeled)
dependency structures.
We obtain the word alignments by running
2This dataset includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06
3http://code.google.com/p/berkeleyparser/
4http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html/
35
GIZA++ (Och and Ney, 2000) on the corpus in both
directions and applying ?grow-diag-final-and? re-
finement (Koehn et al, 2003). We use the SRI lan-
guage modeling toolkit to train a 5-gram language
model on the Xinhua portion of the Gigaword corpus
and standard MERT (Och, 2003) to tune the feature
weights on the development data.
For evaluation, the NIST BLEU script (version
12) with the default settings is used to calculate the
BLEU scores. To test whether a performance differ-
ence is statistically significant, we conduct signifi-
cance tests following the paired bootstrap approach
(Koehn, 2004). In this paper,?**? and?*? de-
note p-values less than 0.01 and in-between [0.01,
0.05), respectively.
Table 2 lists the rule table sizes. The full rule ta-
ble size (including HD-HRs and NRRs) of our HD-
HPB model is ?1.5 times that of Chiang?s, largely
due to refining the non-terminal symbol X in Chi-
ang?s model into head-informed ones in our model.
It is also unsurprising, that the test set-filtered rule
table size of our model is only ?0.7 times that of Chi-
ang?s: this is due to the fact that some of the refined
translation rule patterns required by the test set are
unattested in the training data. Furthermore, the rule
table size of NRRs is much smaller than that of HD-
HRs since a NRR contains only two non-terminals.
Table 3 lists the translation performance with
BLEU scores. Note that our re-implementation of
Chiang?s original HPB model performs on a par with
Moses HPB. Table 3 shows that our HD-HPB model
significantly outperforms Chiang?s HPB model with
an average improvement of 1.91 in BLEU (and sim-
ilar improvements over Moses HPB).
Table 3 shows that the head-driven scheme out-
performs a SAMT-style approach (for each test set
p < 0.01), indicating that head information is more
effective than (partial) CFG categories. Taking lian-
ming zhichi in Figure 1 as an example, HD-HPB
labels the span VV, as lianming is dominated by
zhichi, effecively ignoring lianming in the transla-
tion rule, while the SAMT label is ADVP:AD+VV5
which is more susceptible to data sparsity. In addi-
tion, SAMT resorts to X if a text span fails to satisify
pre-defined categories. Examining initial phrases
5the constituency structure for lianming zhichi is (VP (ADVP
(AD lianming)) (VP (VV zhichi) ...)).
System Total MT 03 MT 04 MT 05 MT 06 Avg.
HPB 39.6 2.8 4.7 3.3 3.0 3.4
HD-HPB 59.5/0.6 1.9/0.1 3.4/0.2 2.3/0.2 2.0/0.1 2.4/0.2
Table 2: Rule table sizes (in million) of different mod-
els. Note: 1) For HD-HPB, the rule sizes separated by /
indicate HD-HRs and NRRs, respectively; 2) Except for
?Total?, the figures correspond to rules filtered on the cor-
responding test set.
System MT 03 MT 04 MT 05 MT 06 Avg.
Moses HPB 32.94* 35.16 32.18 29.88* 32.54
HPB 33.59 35.39 32.20 30.60 32.95
HD-HPB 35.50** 37.61** 34.56** 31.78** 34.86
SAMT-HPB 34.07 36.52** 32.90* 30.66 33.54
HD-HR+Glue 34.58** 36.55** 33.84** 31.06 34.01
Table 3: BLEU (%) scores of different models. Note:
1) SAMT-HPB indicates our HD-HPB model with non-
terminal scheme of Zollmann and Venugopal (2006);
2) HD-HR+Glue indicates our HD-HPB model replac-
ing NRRs with glue rules; 3) Significance tests for
Moses HPB, HD-HPB, SAMT-HPB, and HD-HR+Glue
are done against HPB.
extracted from the SAMT training data shows that
28% of them are labeled as X.
In order to separate out the individual contribu-
tions of the novel HD-HRs and NRRs, we carry out
an additional experiment (HD-HR+Glue) using HD-
HRs with monotonic glue rules only (adjusted to re-
fined rule labels, but effectively switching off the ex-
tra reordering power of full NRRs). Table 3 shows
that on average more than half of the improvement
over HPB (Chiang and Moses) comes from the re-
fined HD-HRs, the rest from NRRs.
Examining translation rules extracted from the
training data shows that there are 72,366 types of
non-terminals with respect to 33 types of POS tags.
On average each sentence employs 16.6/5.2 HD-
HRs/NRRs in our HD-HPB model, compared to
15.9/3.6 hierarchical rules/glue rules in Chiang?s
model, providing further indication of the impor-
tance of NRRs in translation.
4 Conclusion
We present a head-driven hierarchical phrase-based
(HD-HPB) translation model, which adopts head in-
formation (derived through unlabeled dependency
analysis) in the definition of non-terminals to bet-
ter differentiate among translation rules. In ad-
36
dition, improved and better integrated reordering
rules allow better reordering between consecutive
non-terminals through exploration of a larger search
space in the derivation. Experimental results on
Chinese-English translation across four test sets
demonstrate significant improvements of the HD-
HPB model over both Chiang?s HPB and a source-
side SAMT-style refined version of HPB.
Acknowledgments
This work was supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142) as part of the Cen-
tre for Next Generation Localisation (www.cngl.ie)
at Dublin City University. It was also partially
supported by Project 90920004 under the National
Natural Science Foundation of China and Project
2012AA011102 under the ?863? National High-
Tech Research and Development of China. We
thank the reviewers for their insightful comments.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2011. CCG
contextual labels in hierarchical phrase-based SMT. In
Proceedings of EAMT 2011, pages 281?288.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of ACL 2007, pages
33?40.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL 2000, pages 132?
139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857?868.
Zhongjun He, Yao Meng, and Hao Yu. 2010. Maxi-
mum entropy based phrase reordering for hierarchical
phrase-based translation. In Proceedings of EMNLP
2010, pages 555?563.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of EMNLP 2010, pages 138?
147.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT 2008, pages 1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proceedings of ACL-HLT 2011, pages 642?
652.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007, pages 404?411.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of EMNLP 2009, pages
72?80.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL 2006 - Workshop on Statistical
Machine Translation, pages 138?141.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proceedings of ACL-HLT 2011, pages
1?11.
37
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1477?1487,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Argument Inference from Relevant Event Mentions in Chinese 
Argument Extraction 
 
 
Peifeng Li, Qiaoming Zhu, Guodong Zhou* 
School of Computer Science & Technology 
Soochow University, Suzhou, 215006, China 
{pfli, qmzhu, gdzhou}@suda.edu.cn 
 
 
 
Abstract 
As a paratactic language, sentence-level 
argument extraction in Chinese suffers 
much from the frequent occurrence of 
ellipsis with regard to inter-sentence 
arguments. To resolve such problem, this 
paper proposes a novel global argument 
inference model to explore specific 
relationships, such as Coreference, 
Sequence and Parallel, among relevant 
event mentions to recover those inter-
sentence arguments in the sentence, 
discourse and document layers which 
represent the cohesion of an event or a 
topic. Evaluation on the ACE 2005 
Chinese corpus justifies the effectiveness 
of our global argument inference model 
over a state-of-the-art baseline. 
1 Introduction 
The task of event extraction is to recognize event 
mentions of a predefined event type and their 
arguments (participants and attributes). 
Generally, it can be divided into two subtasks: 
trigger extraction, which aims to identify 
trigger/event mentions and determine their event 
type, and argument extraction, which aims to 
extract various arguments of a specific event and 
assign the roles to them. In this paper, we focus 
on argument extraction in Chinese event 
extraction. While most of previous studies in 
Chinese event extraction deal with Chinese 
trigger extraction (e.g., Chen and Ji, 2009a; Qin 
et al, 2010; Li et al, 2012a, 2012b), there are 
only a few on Chinese argument extraction (e.g., 
Tan et al, 2008; Chen and Ji, 2009b). Following 
previous studies, we divide argument extraction 
into two components, argument identification 
and role determination, where the former 
recognizes the arguments in a specific event 
mention and the latter classifies these arguments 
by roles.  
With regard to methodology, most of previous 
studies on argument extraction recast it as a 
Semantic Role Labeling (SRL) task and focus on 
intra-sentence information to identify the 
arguments and their roles. However, argument 
extraction is much different from SRL in the 
sense that, while the relationship between a 
predicate and its arguments in SRL can be 
mainly decided from the syntactic structure, the 
relationship between an event trigger and its 
arguments are more semantics-based, especially 
in Chinese, as a paratactic (e.g., discourse-driven 
and pro-drop) language with the wide spread of 
ellipsis and the open flexible sentence structure. 
Therefore, some arguments of a specific event 
mention are far away from the trigger and how to 
recover those inter-sentence arguments becomes 
a challenging issue in Chinese argument 
extraction. Consider the following discourse 
(from ACE 2005 Chinese corpus) as a sample: 
D1: ??????????????? 20 ?
????????????(E1)?????
(E2)?????????????(E3)???
???? (The Palestinian National Authority 
denied any involvement in the bomb attack (E2) 
occurred in the Gaza Strip on the morning of the 
20th, which killed (E1) two Israelites. ? They 
claimed that they will be investigating this 
attack (E3).) - From CBS20001120.1000.0823 
In above discourse, there are three event 
mentions, one kill (E1) and two Attack (E2, E3). 
While it is relatively easy to identify 20??? 
(morning of 20th), ???? (Gaza Strip) and ?
?  (bomb) as the Time, Place and Instrument 
roles in E2 by a sentence-based argument 
1477
extractor, it is really challenging to recognize 
these entities as the arguments of its corefered 
mention E3 since to reduce redundancy in a 
Chinese discourse, the later Chinese sentences 
omit many of these entities already mentioned in 
previous sentences. Similarly, it is hard to 
recognize ?????? (two Israelites) as the 
Target role for event mention E2 and identify?
?  (bomb) as the Instrument role for event 
mention E1. An alternative way is to employ 
various relationships among relevant event 
mentions in a discourse to infer those inter-
sentence arguments. 
The contributions of this paper are: 
1) We propose a novel global argument 
inference model, in which various kinds of 
event relations are involved to infer more 
arguments on their semantic relations. 
2) Different from Liao and Grishman (2010) 
and Hong et al (2011), which only consider 
document-level consistency, we propose a 
more fine-gained consistency model to 
enforce the consistency in the sentence, 
discourse and document layers. 
3) We incorporate argument semantics into our 
global argument inference model to unify the 
semantics of the event and its arguments. 
The rest of this paper is organized as follows. 
Section 2 overviews the related work. Section 3 
describes a state-of-the-art Chinese argument 
extraction system as the baseline. Section 4 
introduces our global model in inferring those 
inter-sentence arguments. Section 5 reports 
experimental results and gives deep analysis. 
Finally, we conclude our work in Section 6. 
2 Related Work 
Almost all the existing studies on argument 
extraction concern English. While some apply 
pattern-based approaches (e.g., Riloff, 1996; 
Califf and Mooney, 2003; Patwardhan and Riloff, 
2007; Chambers and Jurafsky, 2011), the others 
use machine learning-based approaches (e.g., 
Grishman et al, 2005; Ahn, 2006; Patwardhan 
and Riloff, 2009; Lu and Roth, 2012), most of 
which rely on various kinds of features in the 
context of a sentence. In comparison, there are 
only a few studies exploring inter-sentence 
information or argument semantics (e.g., Liao 
and Grishman, 2010; Hong et al, 2011; Huang 
and Riloff, 2011, 2012). 
Compared with the tremendous work on 
English event extraction, there are only a few 
studies (e.g., Tan et al, 2008; Chen and Ji, 2009b; 
Fu et al, 2010; Qin et al, 2010; Li et al, 2012) 
on Chinese event extraction with focus on either 
feature engineering or trigger expansion, under 
the same framework as English trigger 
identification. In additional, there are only very 
few of them focusing on Chinese argument 
extraction and almost all aim to feature 
engineering and are based on sentence-level 
information and recast this task as an SRL-style 
task. Tan et al (2008) introduce multiple levels 
of patterns to improve the coverage in Chinese 
argument classification. Chen and Ji (2009b) 
apply various kinds of lexical, syntactic and 
semantic features to address the special issues in 
Chinese argument extraction. Fu et al (2010) use 
a feature weighting scheme to re-weight various 
features for Chinese argument extraction. Li et al 
(2012b) introduce more refined features to the 
system of Chen and Ji (2009b) as their baseline. 
Specially, several studies have successfully 
incorporated cross-document or document-level 
information and argument semantics into event 
extraction, most of them focused on English.  
Yangarber et al (2007) apply a cross-
document inference mechanism to refine local 
extraction results for the disease name, location 
and start/end time. Mann (2007) proposes some 
constraints on relationship rescoring to impose 
the discourse consistency on the CEO?s personal 
information. Chambers and Jurafsky (2008) 
propose a narrative event chain which are 
partially ordered sets of event mentions centered 
around a common protagonist and this chain can 
represent the relationship among the relevant 
event mentions in a document. 
Ji and Grishman (2008) employ a rule-based 
approach to propagate consistent triggers and 
arguments across topic-related documents. Liao 
and Grishman (2010) mainly focus on employing 
the cross-event consistency information to 
improve sentence-level trigger extraction and 
they also propose an inference method to infer 
the arguments following role consistency in a 
document. Hong et al (2011) employ the 
background information to divide an entity type 
into more cohesive subtypes to create the bridge 
between two entities and then infer arguments 
and their roles using cross-entity inference on the 
subtypes of entities. Huang and Rillof (2012) 
propose a sequentially structured sentence 
classifier which uses lexical associations and 
discourse relations across sentences to identify 
event-related document contexts and then apply 
it to recognize arguments and their roles on the 
relation among triggers and arguments. 
1478
3 Baseline 
In the task of event extraction as defined in ACE 
evaluations, an event is defined as a specific 
occurrence involving participants (e.g., Person, 
Attacker, Agent, Defendant) and attributes (e.g., 
Place, Time). Commonly, an event mention is 
triggered via a word (trigger) in a phrase or 
sentence which clearly expresses the occurrence 
of a specific event. The arguments are the entity 
mentions involved in an event mention with a 
specific role, the relation of an argument to an 
event where it participates. Hence, extracting an 
event consists of four basic steps, identifying an 
event trigger, determining its event type, 
identifying involved arguments (participants and 
attributes) and determining their roles. 
As the baseline, we choose a state-of-the-art 
Chinese event extraction system, as described in 
Li et al (2012b), which consists of four typical 
components: trigger identification, event type 
determination, argument identification and role 
determination. In their system, the former two 
components, trigger identification and event type 
determination, are processed in a joint model, 
where the latter two components are run in a 
pipeline way. Besides, the Maximum-Entropy 
(ME) model is employed to train individual 
component classifiers for above four components. 
This paper focuses on argument identification 
and role determination. In order to provide a 
stronger baseline, we introduce more refined 
features in such two components, besides those 
adopted in Li et al (2012b). Following is a list of 
features adopted in our baseline. 
1) Basic features: trigger, POS (Part Of Speech) 
of the trigger, event type, head word of the 
entity, entity type, entity subtype; 
2) Neighbouring features: left neighbouring 
word of the entity + its POS, right neighbour 
word of the entity + its POS, left neighbour 
word of the trigger + its POS, right neighbour 
word of the trigger + its POS;  
3) Dependency features: dependency path from 
the entity to the trigger, depth of the 
dependency path; 
4) Syntactic features: path from the trigger to the 
entity, difference of the depths of the trigger 
and entity, place of the entity (before trigger 
or after trigger), depth of the path from the  
trigger to the entity, siblings of the entity; 
5) Semantic features: semantic role of the entity 
tagged by an SRL tool (e.g., ARG0, ARG1) 
(Li et al, 2010), sememe of trigger in Hownet 
(Dong and Dong, 2006). 
4 Inferring Inter-Sentence Arguments 
on Relevant Event Mentions 
In this paper, a global argument inference model 
is proposed to infer those inter-sentence 
arguments and their roles, incorporating with 
semantic relations between relevant event 
mention pairs and argument semantics. 
4.1 Motivation 
It?s well-known that Chinese is a paratactic 
language, with an open flexible sentence 
structure and often omits the subject or the object, 
while English is a hypotactic language with a 
strict sentence structure and emphasizes on 
cohesion between clauses. Hence, there are two 
issues in Chinese argument extraction, associated 
with its nature of the paratactic language. 
The first is that many arguments of an event 
mention are out of the event mention scope since 
ellipsis is a common phenomenon in Chinese. 
We call them inter-sentence arguments in this 
paper. Table 1 gives the statistics of intra-
sentence and inter-sentence arguments in the 
ACE 2005 Chinese corpus and it shows that 
20.8% of the arguments are inter-sentence ones 
while this figure is less than 1% of the ACE 2005 
English corpus. The main reason of that 
difference is that some Chinese arguments are 
omitted in the same sentence of the trigger since 
Chinese is a paratactic language with the wide 
spread of ellipsis. Besides, a Chinese sentence 
does not always end with a full stop. In particular, 
a comma is used frequently as the stop sign of a 
sentence in Chinese. We detect sentence 
boundaries, relying on both full stop and comma 
signs, since in a Chinese document, comma can 
be also used to sign the end of a sentence. In 
particular, we detect sentence boundaries on full 
stop, exclamatory mark and question mark firstly. 
Then, we identify the sentence boundaries on 
comma, using a binary classifier with a set of 
lexical and constituent-based syntactic features, 
similar to Xue and Yang (2010). 
 
Category Number 
#Arguments 8032 
#Inter-sentence 1673(20.8%) 
#Intra-sentence 6359(79.2%) 
Table 1. Statistics: Chinese argument extraction 
with regard to intra- sentence and inter-sentence 
arguments. 
 
The second issue is that the Chinese word 
order in a sentence is rather agile for the open 
1479
flexible sentence structure. Hence, different word 
orders can often express the same semantics. For 
example, a Die event mention ?Three person 
died in this accident.? can be expressed in many 
different orders in Chinese, such as ??????
?????, ??????????, ??????
?????, etc. 
In a word, above two issues indicate that 
syntactic feature-based approaches are limited in 
identifying Chinese arguments and it will lead to 
low recall in argument identification. Therefore, 
employing those high level information to 
capture the semantic relation, not only the 
syntactic structure, between the trigger and its 
long distance arguments is the key to improve 
the performance of the Chinese argument 
identification. Unfortunately, it is really hard to 
find their direct relations since they always 
appear in different clauses or sentences. An 
alternative way is to link the different event 
mentions with their predicates (triggers) and use 
the trigger as a bridge to connect the arguments 
to the trigger in another event mention indirectly. 
Hence, the semantic relations among event 
mentions are helpful to be a bridge to identify 
those inter-sentence arguments. 
4.2 Relations of Event Mention Pairs 
In a discourse, most event mentions are 
surrounding a specific topic. It?s obvious that 
those mentions have the intrinsic relationships to 
reveal the essential structure of a discourse. 
Those relevant semantics-based relations are 
helpful to infer the arguments for a specific 
trigger mention when the syntactic relations in 
Chinese argument extraction are not as effective 
as that in English. In this paper, we divide the 
relations among relevant event mentions into 
three categories: Coreference, Sequence and 
Parallel. 
An event may have more than one mention in 
a document and coreference event mentions refer 
to the same event, as same as the definition in the 
ACE evaluations. Those coreference event 
mentions always have the same arguments and 
roles. Therefore, employing this relation can 
infer the arguments of an event mention from 
their Coreference ones. For example, we can 
recover the Time, Place and Instrument for E3 
via its Coreference mention E2 in discourse D1, 
mentioned in Section 1. 
Li et al (2012a) find out that sometimes two 
trigger mentions are within a Chinese word 
whose morphological structure is Coordination. 
Take the following sentence as a sample: 
D2: ?? 17 ????????????(E4)
? (E5)????? (A 12-year-old younger 
hijacked a bus and then stabbed (E4) a woman 
to death (E5).) - From ZBN20001218.0400.0005 
In D2, ??  (stab a person to death) is a 
trigger with the Coordination structure and can 
be divided into two single-morpheme words ? 
(stab) and ? (die) while the former triggers an 
Attack event and the latter refers to a Die one. 
It?s interesting that they share all arguments in 
this sentence. The relation between those event 
mentions whose triggers merge a Chinese word 
or share the subject and the object are Parallel. 
For the errors in the syntactic parsing, the second 
single-morpheme trigger is often assigned a 
wrong tag (e.g., NN, JJ) and this leads to the 
errors in the argument extraction. Therefore, 
inferring the arguments of the second single-
morpheme trigger from that of the first one based 
on Parallel relation is also an available way to 
recover arguments. 
Like that the topic is an axis in a discourse, the 
relations among those relevant event mentions 
with the different types is the bone to link them 
into a narration. There are a few studies on using 
the event relations in NLP (e.g., summarization 
(Li et al, 2006), learning narrative event chains 
(Chambers and Jurafsky, 2007)) to ensure its 
effectiveness. In this paper, we define two types 
of Sequence relations of relevant event mentions: 
Cause and Temporal for their high probabilities 
of sharing arguments.  
The Cause relation between the event 
mentions are similar to that in the Penn 
Discourse TreeBank 2.0 (Prasad et al, 2008). 
For example, an Attack event often is the cause 
of an Die or Injure event. Our Temporal relation 
is limited to those mentions with the same or 
relevant event types (e.g., Transport and Arrest) 
for the high probabilities of sharing arguments. 
Take the following discourse as a sample: 
D3: ??????(E6)??????????
????(E7)?????????????
(These prisoners left (E6) Tindouf, a western 
city of Algeria, and went (E7) to Agadir, a 
southwestern city of Morocco.) - From 
Xin20001215.2000.0158 
In D3, there are two Transport mentions and it 
is natural to infer ????  (Agadir) as the 
Destination role of E6 and??? (Tindouf) as 
the Origin role of E7 via their Sequence relation. 
1480
4.3 Identifying Relations of Event Mention 
Pairs 
Currently, there are only few studies focusing on 
such area (e.g., Ahn, 2006; Chamber and 
Jurafsky, 2007; Huang and Rillof, 2012; Do et al, 
2012) and their approaches cannot be introduced 
to our system directly for the language nature 
and the different goal. We try to achieve a higher 
accuracy in this stage so that our argument 
inference can recover more true arguments.  
Inspired by Li and Zhou (2012), we also use 
the morphological structure to identify the 
Parallel relation. Two parallel event mentions 
with the adjacent trigger mentions w1 and w2 must 
satisfy follows two conditions: 
1) Morph(w1,w2) is Coordination 
2) jiTwHMTwHM ji ??? )(,)( 21   
where Morph(w1,w2) is a function to recognize 
the morphological structure of joint word w1w2, 
HM(wi) is to identify the head morpheme 1  in 
word wi and Ti is the set of the head morphemes 
with ith event type. These constraints are 
enlightened by the fact that only Chinese words 
with Coordination structure can be divided into 
two new words and each word can trigger an 
event with the different event type 2 . The 
implementation of Morph(w1,w2) and HM(w) are 
described in Li and Zhou (2012). 
The Coreference relation is divided into two 
types: Noun-based Coreference (NC) and Event-
based Coreference (EC) while the former always 
uses a verbal noun to refer to an event mentioned 
in current or previous sentence and the latter is 
that an event is mentioned twice or more actually. 
For example, the relation between E2 and E3 in 
D1 is NC while the trigger of E3 is only a verbal 
noun without any direct arguments and it refers 
to E2. 
We adopt a simple rule to recognize those NC 
relations: for each event mention whose trigger is 
a noun and doesn?t act as the subject/object, we 
regard their relation as NC if there is another 
event mention with the same trigger in current or 
previous sentence. 
Inspired by Ahn (2006), we use the following 
conditions to infer the EC relations between two 
event mentions with the same event type: 
1) Their trigger mentions refer to the same 
trigger; 
2) They have at least one same or similar 
                                                          
1 It acts as the governing semantic element in a Chinese 
word. 
2 If they have the same event type, they will be regarded as 
a single event mention. 
subject/object; 
3) The score of cosine similarity of two event 
mentions is more than a threshold3. 
Finally, for the Sequence relation, instead of 
identifying and classifying the relations clearly 
and correctly, our goal is to identify whether 
there are relevant event mentions in a long 
sentence or two adjacent short sentences who 
share arguments. Algorithm 1 illustrates a 
knowledge-based approach to identify the 
Sequence event relation in a discourse for any 
two trigger mentions tri1 and tri2 as follows: 
 
Algorithm 1 
1: input: tri1 and tri2 and their type et1 and et2 
2:  output: whether their relation is Sequence 
3:  begin 
4:      hm1 ?HM(tri1);  hm2 ?HM(tri2) 
5:  MP ?FindAllMP(hm1,et1,hm2,et2) 
6:     for any mpi in MP 
7:         if ShareArg(mpi) is true then 
8:             return true   // Sequence 
9:        end if 
10:    end for 
11:    return false 
12:  end 
 
In algorithm 1, HM(tri) is to identify the head 
morpheme in trigger tri and FindAllMP(hm1, et1, 
hm2, et2) is to find all event mention pairs in the 
training set which satisfy the condition that their 
head morphemes are hm1 and hm2, and their 
event types are et1 and et2 respectively. Besides, 
ShareArg(mpi)is used to identify whether the 
event mention pair mpi sharing at least one 
argument. In this algorithm, since the relations 
on the event types are too coarse, we introduce a 
more fine-gained Sequence relation both on the 
event types and the head morphemes of the 
triggers which can divide an event type into 
many subtypes on the head morpheme. Li and 
Zhou (2012) have ensured the effectiveness of 
using head morpheme to infer the triggers and 
our experiment results also show it is helpful for 
identifying relevant event mentions which aims 
to the higher accuracy. 
4.4 Global Argument Inference Model 
Our global argument inference model is 
composed of two steps: 1) training two sentence-
based classifiers: argument identifier (AI) and 
role determiner (RD) that estimate the score of a 
candidate acts as an argument and belongs to a 
                                                          
3 The threshold is tuned to 0.78 on the training set. 
1481
specific role following Section 3. 2) Using the 
scores of two classifiers and the event relations 
in a sentence, a discourse or a document, we 
perform global optimization to infer those 
missing or long distance arguments and their 
roles.  
To incorporate those event relations with our 
global argument inference model, we regard a 
document as a tree and divide it into three layers: 
document, discourse and sentence. A document 
is composed of a set of the discourses while a 
discourse contains three sentences. Since almost 
all arguments (~98%) of a specific event mention 
in the ACE 2005 Chinese corpus appear in the 
sentence containing the specific event mention 
and its two adjacent sentences (previous and next 
sentences), we only consider these three 
sentences as a discourse to simplify the process 
of identifying the scope of a discourse.  
We incorporate different event relations into 
our model on the different layer and the goal of 
our global argument inference model is to 
achieve the maximized scores over a document 
on its three layers and two classifiers: AI and RD. 
The score of document D is defined as 
))1))(,(1(),(
()1(
))1))((1()(
((maxarg
,,
, ,,, ,,
, ,,, ,,,
^
><><
? ?>< ><?>< ><? ?
? ?>< ><?>< ><?
??++
?+
??+
=
? ? ? ? ?
? ? ? ?
mZmZDmZmZD
DiI iIjiS jiSkjiT kjiTZA Rm
ZZIZZI
DiI iIjiS jiSkjiT kjiTZAYX
YREfYREf
XEfXEf
D
?
?
(1) 
}1,0{.. ?ZXts                                          (2) 
}1,0{, ?>< mZY                                  (3) 
RmYX mZZ ??? >< ,                       (4) 
?
?Rm
mZZ YX ><= ,                               (5) 
where Ii is the ith discourses in document D; 
S<i,j> is the jth sentences in discourse Ii; T<i,j,k> is 
the kth event mentions in sentence S<i,j>; A<i,j,k,l> 
is the lth candidate arguments in event mention 
T<i,j,k>; Z is used to denote <i,j,k,l>; fI(EZ) is the 
score of AI identifying entity mention EZ as an 
argument, where EZ is the lth entity of the kth 
event mention of the jth sentence of the ith 
discourse in document D. fD(EZ, Rm) is the score 
of RD assigning role Rm to argument EZ. Finally, 
XZ and Y<Z,m> are the indicators denoting whether 
entity EZ is an argument and whether the role Rm 
is assigned to entity EZ respectively. Besides, Eq. 
4 and Eq. 5 are the inferences to enforce that:  
1) if an entity belongs to a role, it must be an 
argument; 
2) if a entity is an argument of a specific event 
mention, it must have a role. 
Parallel relation: Sentence-based 
optimization is used to incorporate the Parallel 
relation of two event mentions into our model 
and they share all arguments in a sentence. Since 
different event type may have different role set, 
each role in a specific event should be mapped to 
the corresponding role in its Parallel event when 
they have the different event type. For example, 
the argument ??? 17 ????? (A 12-year-
old younger) in D2 acts as the Attacker role in 
the Attack event and the Agent role in the Die 
event. We learn those role-pairs from the training 
set and Table 2 shows part of the role relations 
learning from the training set. 
 
Event type pair Role pair 
Attack-Die Attacker-Agent; Target-
Victim;? 
Injure-Die Agent-Agent; Victim-
Victim;? 
Transport-
Demonstrate 
Artifact-Entity; 
Destination-Place;? 
Table 2. Part of role-pairs for those event 
mention pairs with Parallel relation. 
 
To infer the arguments and their roles on the 
Parallel relation, we enforce the consistency on 
the role-pair as follows: 
><><?
><><><><
><><><><
><><
=?>?<
????
???????
=
',',,,,,'
',,',',,,,,,,
,',',,,,
',',',,,,,,
',
,
lkjilkjihethet
kjilkjikjilkji
jikjikjiijii
mlkjimlkji
EERPmm
TATA
STTISDI
YY
(6) 
where 
'hh etetRP ?  is the set of role-pairs between 
two Parallel event mention eth and eth? and 
><>< = ',',,,,, lkjilkji EE  means they refer to the 
same entity mention. With the transitivity 
between the indicators X and Y, Eq. 6 also 
enforces the consistency on X<i,j,k,l> and X<i,j,k?,l?>. 
Coreference relation: Since the NC and EC 
relcation between two event mentions are 
different in the event expression, we introduce 
the discourse-based optimization for the former 
and document-based optimization for the latter. 
For two NC mentions, we ensure that the 
succeeding mentions can inherit the arguments 
form the previous one. To enforce this 
consistency, we just replace all fI(EZ) and fD(EZ, 
Rm) of the succeeding event mention with that of 
the previous one, since the previous one have the 
more context information. 
As for two EC event mentions, algorithm 2 
shows how to create the constraints for our 
1482
global argument inference model to infer 
arguments and roles. 
 
Algorithm 2 
1: input: two event mentions T, T? and their 
arguments set A and A? 
2:  output: the constraints set C 
3:  begin 
4:       for each argument a in A do 
5:            a??FindSim(a) 
6:    if a??? then 
7:                 ),( 'aa YYyConsistencCC ??  
8:             end if 
9:        end for 
10: end 
 
In algorithm 2, the function FindSim(a) is 
used to find a similar candidate argument a? in 
A? for a. If it?s found, we enforce the consistency 
of argument a and a? in the role by using 
Consistency(Ya,Ya?) where Ya  and Ya? are the 
indicators in Eq. 1. To evaluate the similarity 
between two candidates a and a?, we regard them 
as similar ones when they are the same word or 
in the same entity coreference chain. We use a 
coreference resolution tool to construct the entity 
coreference chains, as described in Kong et al
(2010). 
Sequence relation: For any two event 
mentions in a discourse, we use the event type 
pair with their head morphemes (e.g., Attack:?
(burst) - Die:?(die), Trial-Hearing:?(trial) - 
Sentence:?(sentence)) to search the training set 
and then obtain the probabilities of sharing the 
arguments as mentioned in algorithm 1. We 
denoted Pro<et,et?,HM(tri),HM(tri?),Rm,Rm?> as the 
probability of the trigger mentions tri and tri? 
(their event types are et and et? respectively.) 
sharing an argument whose roles are Rm and Rm? 
respectively. We propose following discourse-
based constraint to enforce the consistency 
between the roles of two arguments, which are 
related semantically, temporally, causally or 
conditionally, based on the probability of sharing 
an argument and the absolute value of the 
difference between the scores of RD: 
?
?
>
>
=?
????
=
><><
><><><><
><><><><
><><
),(),(
),),'(),(,',(Pr
',
,?
'',',',,,,
'
',',',,,,',',',
,,,',,
',',',',,,,,
mlkjiDmlkjiD
mm
lkjilkjijikji
jikjiijijii
mlkjimlkji
REfREf
RRtriHMtriHMeteto
EERmmST
STISSDI
YY
??
????
? (7) 
where ? and ? are the thresholds learned from the 
development set; tri and tri? are triggers of kth 
and k?th event mention whose event types are et 
and et? in S<i,j> and S<i,j?> respectively. 
4.5 Incorporating Argument Semantics into 
Global Argument Inference Model 
We also introduce the argument semantics, 
which represent the semantic relations of 
argument-argument pair, argument-role pair and 
argument-trigger pair, to reflect the cohesion 
inside an event. Hong et al (2011) found out that 
there is a strong argument and role consistency in 
the ACE 2005 English corpus. Those 
consistencies also occur in Chinese and they 
reveal the relation between the trigger and its 
arguments, and also explore the relation between 
the argument and its role. Besides, those entities 
act as non-argument also have the consistency 
with high probabilities.  
To let the global argument inference model 
combine those knowledges of argument 
semantics, we compute the prior probabilities 
P(X<i,j>=1) and P(Y<i,j,m>=1) that entity enj 
occurrs in a specific event type eti as an 
argument and its role is Rm respectively. To 
overcome the sparsity of the entities, we cluster 
those entities into more cohesive subtype 
following Hong et al (2011). Hence, following 
the independence assumptions described by 
Berant et al (2011), we modify the fI(EZ) and 
fD(EZ,Rm)in Eq. 1 as follows: 
)0()|1(1(
)1()|1(
log)( ==?
===
ZZZ
ZZZ
ZI XPFXP
XPFXP
Ef     (8) 
)0()|1(1(
)1()|1(
log),(
,,,
,,,
==?
===
><><><
><><><
mZmZmZ
mZmZmZ
mZD XPFXP
XPFYP
REf (9) 
where )|1( ZZ FXP =  and )|1( ,, ><>< = mZmZ FYP  
are the probabilities from the AI and AD 
respectively while FZ and F<Z,m> are the feature 
vectors. Besides, )1( , =>< mZXP  and )1( =ZXP  
are the prior probabilities learning from the 
training set. 
5 Experimentation 
In this section, we first describe the experimental 
settings and the baseline, and then evaluate our 
global argument inference model incorporating 
with relevant event mentions and argument 
semantics to infer arguments and their roles. 
5.1 Experimental Settings and Baseline 
For fair comparison, we adopt the same 
experimental settings as the state-of-the-art event 
extraction system (Li et al 2012b) and all the 
1483
evaluations are experimented on the ACE 2005 
Chinese corpus. We randomly select 567 
documents as the training set and the remaining 
66 documents as the test set. Besides, we reserve 
33 documents in the training set as the 
development set and use the ground truth entities, 
times and values for our training and testing. As 
for evaluation, we also follow the standards as 
defined in Li et al (2012b). Finally, all the 
sentences in the corpus are divided into words 
using a Chinese word segmentation tool 
(ICTCLAS) 1  with all entities annotated in the 
corpus kept. We use Berkeley Parser 2  and 
Stanford Parser 3  to create the constituent and 
dependency parse trees.  Besides, the ME tool 
(Maxent) 4  is employed to train individual 
component classifiers and lp_solver5 is used to 
construct our global argument inference model. 
Besides, all the experiments on argument 
extraction are done on the output of the trigger 
extraction system as described in Li et al 
(2012b). Table 3 shows the performance of the 
baseline trigger extraction system and Line 1 in 
Table 4 illustrates the results of argument 
identification and role determination based on 
this system. 
 
Trigger 
identification 
Event type 
determination 
P(%) R(%) F1 P(%) R(%) F1 
74.4 71.9 73.1 71.4 68.9 70.2
Table 3. Performance of the baseline on trigger 
identification and event type determination. 
5.2 Inferring Arguments on Relevant Event 
Mentions and Argument Semantics 
We develop a baseline system as mentioned in 
Section 3 and Line 2 in Table 4 shows that it 
slightly improves the F1-measure by 0.9% over 
Li et al (2012b) due to the incorporation of more 
refined features. This result indicates the 
limitation of syntactic-based feature engineering. 
Before evaluating our global argument 
inference model, we should identify the event 
relations between two mentions in a sentence, a 
discourse or a document. The experimental 
results show that the accuracies of identifying 
NC, EC, Parallel and Sequence relation are 
80.0%, 72.4%, 88.5% and 87.7% respectively. 
Those results ensure that our simple methods are 
                                                          
1http://ictclas.org/  
2 http://code.google.com/p/berkeleyparser/ 
3 http://nlp.stanford.edu/software/lex-parser.shtml 
4 http://mallet.cs.umass.edu/ 
5 http://lpsolve.sourceforge.net/5.5/ 
effective. Our statistics on the development set 
shows almost 65% of the event mentions are 
involved in those Correfrence, Parallel and 
Sequence relations, which occupy 63%, 50%, 9% 
respectively6. Most of the exceptions are isolated 
event mentions. 
 
System 
Argument 
identification 
Argument role 
determination
P(%) R(%) F1 P(%) R(%) F1
Li et al(2012b) 59.1 57.2 58.1 55.8 52.1 53.9
Baseline 60.5 57.6 59.0 55.7 53.0 54.4
BIM 59.3 60.1 59.7 54.4 55.2 54.8
BIM+RE 60.2 65.6 62.8 55.0 60.0 57.4
BIM+RE+AS 62.9 66.1 64.4 57.2 60.2 58.7
Table 4. Performance comparison of argument 
extraction on argument identification and role 
determination. 
Once the classifier AI and RD are trained, we 
would like to apply our global argument 
inference model to infer more inter-sentence 
arguments and roles. To achieve an optimal 
solution, we formulate the global inference 
problem as an Integer Linear Program (ILP), 
which leads to maximize the objective function. 
ILP is a mathematical method for constraint-
based inference to find the optimal values for a 
set of variables that maximize an objective 
function in satisfying a certain number of 
constraints. In the literature, ILP has been widely 
used in many NLP applications (e.g., Barzilay 
and Lapata, 2006; Do et al, 2012; Li et al, 
2012b).  
For our systems, we firstly evaluate the 
performance of our basic global argument 
inference model (BIM) with the Eq. 2?5 which 
enforce the consistency on AI and RD and then 
introduce the inference on the relevant event 
mentions (RE) and argument semantics (AS) to 
BIM. Table 4 shows their results and we can find 
out that: 
1) BIM only slightly improves the performance 
in F1-measure, as the result of more increase 
in recall (R) than decrease in precision (P). 
This suggests that those constraints just 
enforcing the consistency on AI and RD is not 
effective enough to infer more arguments. 
2) Compared to the BIM, our model BIM+RE 
enhances the performance of argument 
identification and role determination by 3.1% 
and 2.6% improvement in F1-measure 
respectively. This suggests the effectiveness 
                                                          
6 20% of the mentions belongs to both Coreference and 
Sequence relations. 
1484
of our global argument inference model on 
the relevant event mentions to infer inter-
sentence arguments. Table 5 shows the 
contributions of the different event relations 
while the Sequence relation gains the highest 
improvement of argument identification and 
role determination in F1-measure respectively. 
 
Constraint 
Argument 
identification 
Argument role 
determination 
P(%) R(%) F1 P(%) R(%) F1
BIM 59.3 60.1 59.7 54.4 55.2 54.8
+Parallel +0.6 +0.7 +0.6 +0.4 +0.6 +0.5
+NC +0.0 +0.8 +0.4 -0.2 +0.6 +0.2
+EC +0.6 +1.2 +0.9 +0.5 +1.0 +0.7
+ Sequence -0.3 +2.8 +1.2 -0.2 +2.6 +1.1
Table 5. Contributions of different event 
relations on argument identification and role 
determination. (Incremental) 
3) Our model BIM+ER+AS gains 1.6% 
improvement for argument identification, and 
1.3% for role determination. The results 
ensure that argument semantics not only can 
improve the performance of argument 
identification, but also is helpful to assign a 
correct role to an argument in role 
determination. 
Table 3 shows 25.6% of trigger mentions 
introduced into argument extraction are pseudo 
ones. If we use the golden trigger extraction, our 
exploration shows that the precision and recall of 
argument identification can be up to 78.6% and 
88.3% respectively. Table 6 shows the 
performance comparison of argument extraction 
on AI and RD given golden trigger extraction. 
Compared to the Baseline, our system improves 
the performance of argument identification and 
role determination by 6.4% and 5.8% 
improvement in F1-measure respectively, largely 
due to the dramatic increase in recall of 10.9% 
and 10.4%. 
 
 
System 
Argument 
identification 
Argument role 
determination 
P(%) R(%) F1 P(%) R(%) F1
Baseline 76.2 77.4 76.8 70.4 72.0 71.2
Model2 78.6 88.3 83.2 72.3 82.4 77.0
Table 6. Performance comparison of argument 
identification and type determination. (Golden 
trigger extraction) 
5.3 Discussion 
The initiation of our paper is that syntactic 
features play an important role in current 
machine learning-based approaches for English 
event extraction, however, their effectiveness is 
much reduced in Chinese. So the improvement of 
our model for English event extraction is much 
less than that of Chinese. However, our model 
can be an effective complement of the sentence-
level English argument extraction systems since 
the performance of argument extraction is still 
low in English and using discourse-level 
information is a way to improve its performance, 
especially for those event mentions whose 
arguments spread in complex sentences. 
Moreover, our exploration shows that our 
global argument inference model can mine those 
arguments within a long distance which are un-
annotated as arguments of a special event 
mention in the corpus since the annotators just 
tagged arguments in a narrow scope or omitted a 
few arguments. Actually, they are the true ones 
to our knowledge and  are more than 30.6% of 
those pseudo arguments inferred by our model. 
This ensures that our global argument inference 
model and those relations among event mentions 
is helpful to argument extraction. 
6 Conclusion 
In this paper we propose a global argument 
inference model to extract those inter-sentence 
arguments due to the nature of Chinese that it is a 
discourse-driven pro-drop language with the 
wide spread of ellipsis and the open flexible 
sentence structure. In particular, we incorporate 
various kinds of event relations and the argument 
semantics into the model in the sentence, 
discourse and document layers which represent 
the cohesion of an event or a topic. The 
experimental results ensure that our global 
argument inference model outperforms the state-
of-the-art system. 
In future work, we will focus on introducing 
more semantic information and cross-document 
information into the global argument inference 
model to improve the performance of argument 
extraction. 
Acknowledgments 
The authors would like to thank three 
anonymous reviewers for their comments on this 
paper. This research was supported by the 
National Natural Science Foundation of China 
under Grant No. 61070123, No. 61272260 and 
No. 61273320, the National 863 Project of China 
under Grant No. 2012AA011102. The co-author 
tagged with ?*? is the corresponding author. 
1485
References  
David Ahn. 2006. The Stages of Event Extraction. In 
Proc. COLING/ACL 2006 Workshop on 
Annotating and Reasoning about Time and Events. 
Pages 1-8, Sydney, Australia. 
Regina Barzilay and Miralla Lapata. 2006. 
Aggregation via Set Partitioning for Natural 
Language Generation. In Proc. NAACL 2006, 
pages 359-366, New York City, NY. 
Jonathan Berant, Ido Dagan and Jacob Goldberger. 
2011. Global Learning of Typed Entailment Rules. 
In Proc. ACL 2011, pages 610-619, Portland, OR. 
Mary Elaine Califf and Raymond J. Mooney. 2003. 
Bottom-up Relational Learning of Pattern 
Matching rules for Information Extraction. Journal 
of Machine Learning Research, 4:177?210. 
Nathanael Chambers and Dan Jurafsky. 2008. 
Unsupervised Learning of Narrative Event Chains. 
In Proc. ACL 2008, pages 789-797, Columbus, OH. 
Nathanael Chambers and Dan Jurafsky. 2011. 
Template-based Information Extraction without the 
Templates. In Proc. ACL 2011, pages 976-986, 
Portland, OR. 
Zheng Chen and Heng Ji. 2009a. Can One Language 
Bootstrap the Other: A Case Study on Event 
Extraction. In Proc. NAACL/HLT 2009 Workshop 
on Semi-supervised Learning for Natural Language 
Processing, pages 66-74, Boulder, Colorado. 
Zheng Chen and Heng Ji. 2009b. Language Specific 
Issue and Feature Exploration in Chinese Event 
Extraction. In Proc. NAACL HLT 2009, pages 
209-212, Boulder, Colorado. 
Zhengdong Dong and Qiang Dong. 2006. HowNet 
and the Computation of Meaning. World Scientific 
Pub Co. Inc. 
Quang Xuan Do, Wei Lu and Dan Roth. 2012. Joint 
Inference for Event Timeline Construction. In Proc.  
EMNLP 2012, pages 677-687, Jeju, Korea. 
Jianfeng Fu, Zongtian Liu, Zhaoman Zhong and 
Jianfang Shan. 2010. Chinese Event Extraction 
Based on Feature Weighting. Information 
Technology Journal, 9: 184-187.  
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System 
Description. In Proc. ACE 2005 Evaluation 
Workshop, Gaithersburg, MD. 
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, 
Guodong Zhou and Qiaoming Zhu. 2011. Using 
Cross-Entity Inference to Improve Event Extraction. 
In Proc. ACL 2011, pages 1127-1136, Portland, 
OR. 
Ruihong Huang and Ellen Riloff. 2011. Peeling Back 
the Layers: Detecting Event Role Fillers in 
Secondary Contexts, In Proc. ACL 2011, pages 
1137-1147, Portland, OR. 
Ruihong Huang and Ellen Riloff. 2012. Modeling 
Textual Cohesion for Event Extraction. In Proc. 
AAAI 2012, pages 1664-1770, Toronto, Canada. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proc. ACL 2008, pages 254-262, Columbus, OH. 
Fang Kong, Guodong Zhou, Longhua Qian and 
Qiaoming Zhu. 2010. Dependency-driven 
Anaphoricity Determination for Coreference 
Resolution. In Proc. COLING 2010, pages 599-607, 
Beijing, China. 
Junhui Li, Guodong Zhou and Hwee Tou Ng. 2010. 
Joint Syntactic and Semantic Parsing of Chinese. 
In Proc. ACL 2010, pages 1108-1117, Uppsala, 
Sweden. 
Peifeng Li, Guodong Zhou, Qiaoming Zhu and Libin 
Hou. 2012a. Employing Compositional Semantics 
and Discourse Consistency in Chinese Event 
Extraction. In Proc. EMNLP 2012, pages 1006-
1016, Jeju, Korea. 
Peifeng Li, Qiaoming Zhu, Hongjun Diao and 
Guodong Zhou. 2012b. Joint Modeling of Trigger 
Identification and Event Type Determination in 
Chinese Event Extraction. In Proc. COLING 2012, 
pages 1635-1652, Mumbai, India. 
Peifeng Li and Guodong Zhou. 2012. Employing 
Morphological Structures and Sememes for 
Chinese Event Extraction. In Proc. COLING 2012, 
pages 1619-1634, Mumbai, India. 
Wenjie Li, Mingliu Wu, Qin Lu, Wei Xu and Chunfa 
Yuan. 2006. Extractive Summarization using Inter- 
and Intra- Event Relevance. In Proc. 
COLING/ACL 2006, pages 369-376, Sydney, 
Australia.  
Shasha Liao and Ralph Grishman. 2010. Using 
Document Level Cross-Event Inference to Improve 
Event Extraction. In Proc. ACL 2010, pages 789-
797, Uppsala, Sweden. 
Wei Lu and Dan Roth. 2012. Automatic Event 
Extraction with Structured Preference Modeling. 
In Proc. ACL 2012, pages 835-844, Jeju, Korea. 
Gideon Mann. 2007. Multi-document Relationship 
Fusion via Constraints on Probabilistic Databases. 
In Proc. HLT/NAACL 2007, pages 332-229,  
Rochester, NY. 
Siddharth Patwardhan and Ellen Riloff. 2007. 
Effective Information Extraction with Semantic 
Affinity Patterns and Relevant Regions. In Proc. 
EMNLP/CoNLL 2007, pages 717-727, Prague, 
Czech Republic. 
Siddharth Patwardhan and Ellen Riloff. 2009. A 
Unified Model of Phrasal and Sentential Evidence 
1486
for Information Extraction. In Proc. EMNLP 2009, 
pages 151-160, Singapore. 
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni 
Miltsakaki, Livio Robaldo, Aravind Joshi and 
Bonnie Webber. 2008. The Penn Discourse 
Treebank 2.0. In Proc. LREC 2008, pages 2961-
2968, Marrakech, Morocco. 
Bing Qin, Yanyan Zhao, Xiao Ding, Ting Liu and 
Guofu Zhai. 2010. Event Type Recognition Based 
on Trigger Expansion. Tsinghua Science and 
Technology, 15(3): 251-258, Beijing, China. 
Ellen Riloff. 1996. Automatically Generating 
Extraction Patterns from Untagged Text. In Proc. 
AAAI 1996, pages 1044?1049, Portland, OR. 
Hongye Tan, Tiejun Zhao, Jiaheng Zheng. 2008. 
Identification of Chinese Event and Their 
Argument Roles. In Proc. 2008 IEEE International 
Conference on Computer and Information 
Technology Workshops, pages 14-19, Sydney, 
Australia. 
Nianwen Xue and Yaqin Yang. 2010. Chinese 
Sentence Segmentation as Comma Classification. 
In Proc. ACL 2010, pages 631-635, Uppsala, 
Sweden. 
Roman Yangarber, Clive Best, Peter von Etter, Flavio 
Fuart, David Horby and Ralf Steinberger. 2007. 
Combining Information about Epidemic Threats 
from Multiple Sources. In Proc. RANLP 2007 
Workshop on Multi-source, Multilingual 
Information Extraction and Summarization, pages 
41-48, Borovets, Bulgaria. 
1487
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 511?515,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Modeling of News Reader?s and Comment Writer?s Emotions?
 
 
Huanhuan Liu?  Shoushan Li??*  Guodong Zhou?  Chu-Ren Huang?  Peifeng Li? 
 
?Natural Language Processing Lab 
Soochow University, China 
{huanhuanliu.suda,shoushan.li, 
churenhuang}@gmail.com 
 
?Department of CBS 
the Hong Kong Polytechnic University 
{gdzhou,pfli}@suda.edu.cn 
 
 
Abstract 
Emotion classification can be generally done 
from both the writer?s and reader?s 
perspectives. In this study, we find that two 
foundational tasks in emotion classification, 
i.e., reader?s emotion classification on the 
news and writer?s emotion classification on 
the comments, are strongly related to each 
other in terms of coarse-grained emotion 
categories, i.e., negative and positive. On the 
basis, we propose a respective way to jointly 
model these two tasks. In particular, a co-
training algorithm is proposed to improve 
semi-supervised learning of the two tasks. 
Experimental evaluation shows the 
effectiveness of our joint modeling 
approach.* 
1 Introduction 
Emotion classification aims to predict the emo-
tion categories (e.g., happy, angry, or sad) of a 
given text (Quan and Ren, 2009; Das and Ban-
dyopadhyay, 2009). With the rapid growth of 
computer mediated communication applications, 
such as social websites and miro-blogs, the re-
search on emotion classification has been attract-
ing more and more attentions recently from the 
natural language processing (NLP) community 
(Chen et al, 2010; Purver and Battersby, 2012). 
In general, a single text may possess two kinds 
of emotions, writer?s emotion and reader?s emo-
tion, where the former concerns the emotion ex-
pressed by the writer when writing the text and 
the latter concerns the emotion expressed by a 
reader after reading the text. For example, con-
sider two short texts drawn from a news and cor-
responding comments, as shown in Figure 1. On 
                                                 
* *  Corresponding author 
one hand, for the news text, while its writer just 
objectively reports the news and thus does not 
express his emotion in the text, a reader could 
yield sad or worried emotion. On the other hand, 
for the comment text, its writer clearly expresses 
his sad emotion while the emotion of a reader 
after reading the comments is not clear (Some 
may feel sorry but others might feel careless). 
 
News:  
Today's Japan earthquake could be 
     2011 quake aftershock. ?? 
News Writer?s emotion: None 
News Reader?s emotion: sad, worried 
Comments: 
(1) I hope everything is ok, so sad. I still can 
not forget last year. 
(2) My father-in-law got to experience this 
quake... what a suffering. 
Comment Writer?s emotion: sad 
Comment Reader?s emotion: Unknown 
Figure 1: An example of writer?s and reader?s 
emotions on a news and its comments 
 
Accordingly, emotion classification can be 
grouped into two categories: reader?s emotion 
and writer?s emotion classifications. Although 
both emotion classification tasks have been 
widely studied in recent years, they are always 
considered independently and treated separately.  
However, news and their corresponding com-
ments often appear simultaneously. For example, 
in many news websites, it is popular to see a 
news followed by many comments. In this case, 
because the writers of the comments are a part of 
the readers of the news, the writer?s emotions on 
the comments are exactly certain reflection of the 
reader?s emotions on the news. That is, the 
comment writer?s emotions and the news read-
er?s emotions are strongly related. For example, 
511
in Figure 1, the comment writer?s emotion ?sad? 
is among the news reader?s emotions. 
Above observation motivates joint modeling 
of news reader?s and comment writer?s emotions. 
In this study, we systematically investigate the 
relationship between the news reader?s emotions 
and the comment writer?s emotions. Specifically, 
we manually analyze their agreement in a corpus 
collected from a news website. It is interesting to 
find that such agreement only applies to coarse-
grained emotion categories (i.e., positive and 
negative) with a high probability and does not 
apply to fine-grained emotion categories (e.g., 
happy, angry, and sad). This motivates our joint 
modeling in terms of the coarse-grained emotion 
categories. Specifically, we consider the news 
text and the comment text as two different views 
of expressing either the news reader?s or com-
ment writer?s emotions. Given the two views, a 
co-training algorithm is proposed to perform 
semi-supervised emotion classification so that 
the information in the unlabeled data can be ex-
ploited to improve the classification performance. 
2 Related Work  
2.1 Comment Writer?s Emotion Classifica-
tion 
Comment writer?s emotion classification has 
been a hot research topic in NLP during the last 
decade (Pang et al, 2002; Turney, 2002; Alm et 
al., 2005; Wilson et al, 2009) and previous stud-
ies can be mainly grouped into two categories: 
coarse-grained and fine-grained emotion classifi-
cation. 
Coarse-grained emotion classification, also 
called sentiment classification, concerns only 
two emotion categories, such as like or dislike 
and positive or negative (Pang and Lee, 2008; 
Liu, 2012). This kind of emotion classification 
has attracted much attention since the pioneer 
work by Pang et al (2002) in the NLP communi-
ty due to its wide applications (Cui et al, 2006; 
Riloff et al, 2006; Dasgupta and Ng, 2009; Li et 
al., 2010; Li et al, 2011). 
In comparison, fine-grained emotion classifi-
cation aims to classify a text into multiple emo-
tion categories, such as happy, angry, and sad. 
One main group of related studies on this task is 
about emotion resource construction, such as 
emotion lexicon building (Xu et al, 2010; 
Volkova et al, 2012) and sentence-level or doc-
ument-level corpus construction (Quan and Ren, 
2009; Das and Bandyopadhyay, 2009). Besides, 
all the related studies focus on supervised learn-
ing (Alm et al, 2005; Aman and Szpakowicz, 
2008; Chen et al, 2010; Purver and Battersby, 
2012; Moshfeghi et al, 2011), and so far, we 
have not seen any studies on semi-supervised 
learning on fine-grained emotion classification.  
2.2 News Reader?s Emotion Classification 
While comment writer?s emotion classification 
has been extensively studied, there are only a 
few studies on news reader?s emotion classifica-
tion from the NLP and related communities.  
Lin et al (2007) first describe the task of read-
er?s emotion classification on the news articles 
and then employ some standard machine learning 
approaches to train a classifier for determining 
the reader?s emotion towards a news. Their fur-
ther study, Lin et al (2008) exploit more features 
and achieve a higher performance. 
Unlike all the studies mentioned above, our 
study is the first attempt on exploring the rela-
tionship between comment writer?s emotion 
classification and news reader?s emotion classifi-
cation.  
3 Relationship between News Reader?s 
and Comment Writer?s Emotions 
To investigate the relationship between news 
reader?s and comment writer?s emotions, we col-
lect a corpus of Chinese news articles and their 
corresponding comments from Yahoo! Kimo 
News (http://tw.news.yahoo.com), where each 
news article is voted with emotion tags from 
eight categories: happy, sad, angry, meaningless, 
boring, heartwarming, worried, and useful. 
These emotion tags on each news are selected by 
the readers of the news. Note that because the 
categories of ?useful? and ?meaningless? are not 
real emotion categories, we ignore them in our 
study. Same as previous studies of Lin et al 
(2007) and Lin et al (2008), we consider the 
voted emotions as reader?s emotions on the news, 
i.e., the news reader?s emotions. We only select 
the news articles with a dominant emotion (pos-
sessing more than 50% votes) in our data. Be-
sides, as we attempt to consider the comment 
writer?s emotions, the news articles without any 
comments are filtered. 
As a result, we obtain a corpus of 3495 news 
articles together with their comments and the 
numbers of the articles of happy, sad, angry, 
boring, heartwarming, and worried are 1405, 
230, 1673, 75, 92 and 20 respectively. For 
coarse-grained categories, happy and heartwarm-
ing are merged into the positive category while 
512
sad, angry, boring and worried are merged into 
the negative category. 
Besides the tags of the reader?s emotions, each 
news article is followed by some comments, 
which can be seen as a reflection of the writer?s 
emotions (Averagely, each news is followed by 
15 comments). In order to know the exact rela-
tionship between these two kinds of emotions, 
we select 20 news from each category and ask 
two human annotators, named A and B, to manu-
ally annotate the writer?s emotion (single-label) 
according to the comments of each news. Table 1 
reports the agreement on annotators and emo-
tions, measured with Cohen?s kappa (?) value 
(Cohen, 1960). 
 ?  Value 
(Fine-grained 
emotions) 
? Value 
(Coarse-grained 
emotions) 
Annotators 0.566 0.742 
Emotions 0.504 0.756 
Table 1: Agreement on annotators and emotions 
 
Agreement between two annotators: The 
annotation agreement between the two annota-
tors is 0.566 on the fine-grained emotion catego-
ries and 0.742 on the coarse-grained emotion 
categories.  
Agreement between news reader?s and 
comment writer?s emotions: We compare the 
news reader?s emotion (automatically extracted 
from the web page) and the comment writer?s 
emotion (manually annotated by annotator A). 
The annotation agreement between the two kinds 
of emotions is 0.504 on the fine-grained emotion 
categories and 0.756 on the coarse-grained emo-
tion categories. From the results, we can see that 
the agreement on the fine-grained emotions is a 
bit low while the agreement between the coarse-
grained emotions, i.e., positive and negative, is 
very high. We find that although some fine-
grained emotions of the comments are not con-
sistent with the dominant emotion of the news, 
they belong to the same coarse-grained category.  
In a word, the agreement between news read-
er?s and comment writer?s emotions on the 
coarse-grained emotions is very high, even high-
er than the agreement between the two annota-
tors (0.754 vs. 0.742).  
In the following, we focus on the coarse-
grained emotions in emotion classification. 
4 Joint Modeling of News Reader?s and 
Comment Writer?s Emotions 
Given the importance of both news reader?s and 
comment writer?s emotion classification as de-
scribed in Introduction and the close relationship 
between news reader?s and comment writer?s 
emotions as described in last section, we system-
atically explore their joint modeling on the two 
kinds of emotion classification. 
In semi-supervised learning, the unlabeled da-
ta is exploited to improve the models with a 
small amount of the labeled data. In our ap-
proach, we consider the news text and the com-
ment text as two different views to express the 
news or comment emotion and build the two 
classifiers 
NC  and CC . Given the two-view clas-
sifiers, we perform co-training for semi-
supervised emotion classification, as shown in 
Figure 2, on both news reader?s and comment 
writer?s emotion classification. 
 
 
Input:   
NewsL  the labeled data on the news 
CommentL the labeled data  on the comments 
NewsU the unlabeled data  on the news  
CommentU  the labeled data  on the comments 
Output: 
NewsL New labeled data on the news 
CommentL  New labeled data on the comments 
 
Procedure: 
 
Loop for N iterations until
NewsU ??  or CommentU ??  
(1). Learn classifier 
NC  with NewsL  
(2). Use 
NC  to label the samples from NewsU   
(3). Choose 
1n  positive and 1n negative news 1N  
most confidently predicted by 
NC  
(4). Choose corresponding comments 
1M (the 
comments of the news in 
1N ) 
(5). Learn classifier 
CC  with CommentL  
(6). Use 
CC  to label the samples from CommentU   
(7). Choose 
2n  positive and 2n negative comments 
2M  most confidently predicted by CC  
(8). Choose corresponding comments 
2N (the news 
of the comments in 
2M ) 
(9). 
1 2News NewsL L N N? ? ?  
1 2Comment CommentL L M M? ? ? 
(10). 
1 2News NewsU U N N? ? ?
1 2Comment CommentU U M M? ? ? 
 
Figure 2: Co-training algorithm for semi-
supervised emotion classification 
513
5 Experimentation 
5.1 Experimental Settings 
Data Setting: The data set includes 3495 news 
articles (1572 positive and 1923 negative) and 
their comments as described in Section 3. Alt-
hough the emotions of the comments are not giv-
en in the website, we just set their coarse-grained 
emotion categories the same as the emotions of 
their source news due to their close relationship, 
as described in Section 3. To make the data bal-
anced, we randomly select 1500 positive and 
1500 negative news with their comments for the 
empirical study. Among them, we randomly se-
lect 400 news with their comments as the test 
data. 
Features: Each news or comment text is treat-
ed as a bag-of-words and transformed into a bi-
nary vector encoding the presence or absence of 
word unigrams. 
Classification algorithm: the maximum en-
tropy (ME) classifier implemented with the pub-
lic tool, Mallet Toolkits*. 
5.2 Experimental Results 
News reader?s emotion classifier: The classifier 
trained with the news text. 
Comment writer?s emotion classifier: The 
classifier trained with the comment text. 
Figure 3 demonstrates the performances of the 
news reader?s and comment writer?s emotion 
classifiers trained with the 10 and 50 initial la-
beled samples plus automatically labeled data 
from co-training. Here, in each iteration, we pick 
2 positive and 2 negative most confident samples, 
i.e, 
1 2 2n n? ? . From this figure, we can see that 
our co-training algorithm is very effective: using 
only 10 labeled samples in each category 
achieves a very promising performance on either 
news reader?s or comment writer?s emotion clas-
sification. Especially, the performance when us-
ing only 10 labeled samples is comparable to that 
when using more than 1200 labeled samples on 
supervised learning of comment writer?s emotion 
classification. 
   For comparison, we also implement a self-
training algorithm for the news reader?s and 
comment writer?s emotion classifiers, each of 
which automatically labels the samples from the 
unlabeled data independently. For news reader?s 
emotion classification, the performances of self-
training are 0.783 and 0.79 when 10 and 50 ini-
                                                 
* http://mallet.cs.umass.edu/ 
tial labeled samples are used. For comment writ-
er?s emotion classification, the performances of 
self-training are 0.505 and 0.508. These results 
are much lower than the performances of our co-
training approach, especially on the comment 
writer?s emotion classification i.e., 0.505 and 
0.508 vs. 0.783 and 0.805. 
 
10 Initial Labeled Samples
0.5
0.6
0.7
0.8
0 400 800 1200 1600 2000 2400
Size of the added unlabeled data
A
c
c
u
r
a
c
y
 
50 Initial Labeled Samples
0.65
0.7
0.75
0.8
0.85
0.9
0 400 800 1200 1600 2000 2400
Size of the added unlabeled data data
A
c
c
u
r
a
c
y
The news reader's emotion
classifier (Co-training)
The comment writer's emotion
classifier (Co-training)
 Figure 3: Performances of the news reader?s and 
comment writer?s emotion classifiers using the 
co-training algorithm 
6 Conclusion 
In this paper, we focus on two popular emotion 
classification tasks, i.e., reader?s emotion classi-
fication on the news and writer?s emotion classi-
fication on the comments. From the data analysis, 
we find that the news reader?s and comment 
writer?s emotions are highly consistent to each 
other in terms of the coarse-grained emotion cat-
egories, positive and negative. On the basis, we 
propose a co-training approach to perform semi-
supervised learning on the two tasks. Evaluation 
shows that the co-training approach is so effec-
tive that using only 10 labeled samples achieves 
nice performances on both news reader?s and 
comment writer?s emotion classification.  
514
Acknowledgments 
This research work has been partially supported 
by two NSFC grants, No.61003155, and 
No.61273320, one National High-tech Research 
and Development Program of China 
No.2012AA011102, one General Research Fund 
(GRF) sponsored by the Research Grants Coun-
cil of Hong Kong No.543810, the NSF grant of 
Zhejiang Province No.Z1110551, and one pro-
ject supported by Zhejiang Provin-cial Natural 
Science Foundation of China, No.Y13F020030.  
References  
Alm C., D. Roth and R. Sproat. 2005. Emotions from 
Text: Machine Learning for Text-based Emotion 
Prediction. In Proceedings of EMNLP-05, pp.579-
586. 
Aman S. and S. Szpakowicz. 2008. Using Roget?s 
Thesaurus for Fine-grained Emotion Recognition. 
In Proceedings of IJCNLP-08, pp.312-318. 
Chen Y., S. Lee, S. Li and C. Huang. 2010. Emotion 
Cause Detection with Linguistic Constructions. In 
Proceeding of COLING-10, pp.179-187. 
Cohen J. 1960. A Coefficient of Agreement for Nom-
inal Scales. Educational and Psychological Meas-
urement, 20(1):37?46. 
 Cui H., V. Mittal and M. Datar. 2006. Comparative 
Experiments on Sentiment Classification for 
Online Product Comments. In Proceedings of 
AAAI-06, pp.1265-1270. 
Das D. and S. Bandyopadhyay. 2009. Word to Sen-
tence Level Emotion Tagging for Bengali Blogs. In 
Proceedings of ACL-09, pp.149-152. 
Dasgupta S. and V. Ng. 2009. Mine the Easy, Classify 
the Hard: A Semi-Supervised Approach to Auto-
matic Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09,  pp.701-709, 2009. 
Duin R. 2002. The Combining Classifier: To Train Or 
Not To Train? In Proceedings of 16th International 
Conference on Pattern Recognition (ICPR-02). 
Fumera G. and F. Roli. 2005. A Theoretical and Ex-
perimental Analysis of Linear Combiners for Mul-
tiple Classifier Systems. IEEE Trans. PAMI, vol.27, 
pp.942?956, 2005. 
Li S., Z. Wang, G. Zhou and S. Lee. 2011. Semi-
supervised Learning for Imbalanced Sentiment 
Classification. In Proceeding of IJCAI-11,  pp.826-
1831. 
Li S., C. Huang, G. Zhou and S. Lee.  2010. Employ-
ing Personal/Impersonal Views in Supervised and 
Semi-supervised Sentiment Classification. In Pro-
ceedings of ACL-10,  pp.414-423. 
Lin K., C. Yang and H. Chen. 2007. What Emotions 
do News Articles Trigger in Their Readers? In 
Proceeding of SIGIR-07, poster, pp.733-734. 
Lin K., C. Yang and H. Chen. 2008. Emotion Classi-
fication of Online News Articles from the Reader?s 
Perspective. In Proceeding of the International 
Conference on Web Intelligence and Intelligent 
Agent Technology, pp.220-226. 
 Liu B. 2012. Sentiment Analysis and Opinion Mining 
(Introduction and Survey). Morgan & Claypool 
Publishers, May 2012. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Moshfeghi Y., B. Piwowarski and J. Jose. 2011. Han-
dling Data Sparsity in Collaborative Filtering using 
Emotion and Semantic Based Features. In Proceed-
ings of SIGIR-11, pp.625-634. 
Pang B. and L. Lee. 2008. Opinion Mining and 
Sentiment Analysis: Foundations and Trends. 
Information Retrieval, vol.2(12), 1-135. 
Pang B., L. Lee and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of EMNLP-
02, pp.79-86. 
Purver M. and S. Battersby. 2012. Experimenting 
with Distant Supervision for Emotion Classifica-
tion. In Proceedings of EACL-12, pp.482-491. 
Quan C. and F. Ren. 2009. Construction of a Blog 
Emotion Corpus for Chinese Emotional Expression 
Analysis. In Proceedings of EMNLP-09, pp.1446-
1454. 
Riloff E., S. Patwardhan and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In Proceedings 
of EMNLP-06, pp.440-448. 
Turney P. 2002. Thumbs up or Thumbs down? 
Semantic Orientation Applied to Unsupervised 
Classification of comments. In Proceedings of 
ACL-02, pp.417-424.  
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial Intelligence 
Review, 18(2): 77?95. 
Volkova S., W. Dolan and T. Wilson. 2012. CLex: A 
Lexicon for Exploring Color, Concept and Emo-
tion Associations in Language. In Proceedings of 
EACL-12, pp.306-314. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433. 
Xu G., X. Meng and H. Wang. 2010. Build Chinese 
Emotion Lexicons Using A Graph-based 
Algorithm and Multiple Resources. In Proceeding 
of COLING-10, pp.1209-1217. 
515
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 522?530,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Negation Focus Identification with Contextual Discourse Information 
 
 
Bowei Zou        Qiaoming Zhu       Guodong Zhou* 
Natural Language Processing Lab, School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China 
zoubowei@gmail.com, {qmzhu, gdzhou}@suda.edu.cn 
 
  
 
Abstract 
Negative expressions are common in natural 
language text and play a critical role in in-
formation extraction. However, the perfor-
mances of current systems are far from satis-
faction, largely due to its focus on intra-
sentence information and its failure to con-
sider inter-sentence information. In this paper, 
we propose a graph model to enrich intra-
sentence features with inter-sentence features 
from both lexical and topic perspectives. 
Evaluation on the *SEM 2012 shared task 
corpus indicates the usefulness of contextual 
discourse information in negation focus iden-
tification and justifies the effectiveness of our 
graph model in capturing such global infor-
mation. * 
1 Introduction 
Negation is a grammatical category which com-
prises various kinds of devices to reverse the 
truth value of a proposition (Morante and 
Sporleder, 2012). For example, sentence (1) 
could be interpreted as it is not the case that he 
stopped. 
(1) He didn't stop. 
Negation expressions are common in natural 
language text. According to the statistics on bio-
medical literature genre (Vincze et al, 2008), 
19.44% of sentences contain negative expres-
sions. The percentage rises to 22.5% on Conan 
Doyle stories (Morante and Daelemans, 2012). It 
is interesting that a negative sentence may have 
both negative and positive meanings. For exam-
ple, sentence (2) could be interpreted as He 
stopped, but not until he got to Jackson Hole 
with positive part he stopped and negative part 
until he got to Jackson Hole. Moreover, a nega-
                                                 
* Corresponding author 
tive expression normally interacts with some 
special part in the sentence, referred as negation 
focus in linguistics. Formally, negation focus is 
defined as the special part in the sentence, which 
is most prominently or explicitly negated by a 
negative expression. Hereafter, we denote nega-
tive expression in boldface and negation focus 
underlined. 
(2) He didn't stop until he got to Jackson Hole. 
While people tend to employ stress or intona-
tion in speech to emphasize negation focus and 
thus it is easy to identify negation focus in 
speech corpora, such stress or intonation infor-
mation often misses in the dominating text cor-
pora. This poses serious challenges on negation 
focus identification. Current studies (e.g., Blanco 
and Moldovan, 2011; Rosenberg and Bergler, 
2012) sort to various kinds of intra-sentence in-
formation, such as lexical features, syntactic fea-
tures, semantic role features and so on, ignoring 
less-obvious inter-sentence information. This 
largely defers the performance of negation focus 
identification and its wide applications, since 
such contextual discourse information plays a 
critical role on negation focus identification. 
Take following sentence as an example. 
(3) Helen didn?t allow her youngest son to 
play the violin. 
In sentence (3), there are several scenarios on 
identification of negation focus, with regard to 
negation expression n?t, given different contexts: 
Scenario A: Given sentence But her husband did 
as next sentence, the negation focus should be 
Helen, yielding interpretation the person who 
didn?t allow the youngest son to play the violin is 
Helen but not her husband. 
Scenario B: Given sentence She thought that he 
didn?t have the artistic talent like her eldest son 
as next sentence, the negation focus should be 
the youngest son, yielding interpretation Helen 
522
thought that her eldest son had the talent to play 
the violin, but the youngest son didn?t. 
Scenario C: Given sentence Because of her 
neighbors? protests as previous sentence, the ne-
gation focus should be play the violin, yielding 
interpretation Helen didn?t allow her youngest 
son to play the violin, but it didn?t show whether 
he was allowed to do other things. 
In this paper, to well accommodate such con-
textual discourse information in negation focus 
identification, we propose a graph model to en-
rich normal intra-sentence features with various 
kinds of inter-sentence features from both lexical 
and topic perspectives. Besides, the standard 
PageRank algorithm is employed to optimize the 
graph model. Evaluation on the *SEM 2012 
shared task corpus (Morante and Blanco, 2012) 
justifies our approach over several strong base-
lines. 
The rest of this paper is organized as follows. 
Section 2 overviews the related work. Section 3 
presents several strong baselines on negation fo-
cus identification with only intra-sentence fea-
tures. Section 4 introduces our topic-driven 
word-based graph model with contextual dis-
course information. Section 5 reports the exper-
imental results and analysis. Finally, we con-
clude our work in Section 6. 
2 Related Work 
Earlier studies of negation were almost in lin-
guistics (e.g. Horn, 1989; van der Wouden, 
1997), and there were only a few in natural lan-
guage processing with focus on negation recog-
nition in the biomedical domain. For example, 
Chapman et al (2001) developed a rule-based 
negation recognition system, NegEx, to deter-
mine whether a finding mentioned within narra-
tive medical reports is present or absent. Since 
the release of the BioScope corpus (Vincze et al, 
2008), a freely available resource consisting of 
medical and biological texts, machine learning 
approaches begin to dominate the research on 
negation recognition (e.g. Morante et al, 2008; 
Li et al, 2010). 
Generally, negation recognition includes three 
subtasks: cue detection, which detects and identi-
fies possible negative expressions in a sentence, 
scope resolution, which determines the grammat-
ical scope in a sentence affected by a negative 
expression, and focus identification, which iden-
tifies the constituent in a sentence most promi-
nently or explicitly negated by a negative expres-
sion. This paper concentrates on the third subtask, 
negation focus identification. 
Due to the increasing demand on deep under-
standing of natural language text, negation 
recognition has been drawing more and more 
attention in recent years, with a series of shared 
tasks and workshops, however, with focus on cue 
detection and scope resolution, such as the Bi-
oNLP 2009 shared task for negative event detec-
tion (Kim et al, 2009) and the ACL 2010 Work-
shop for scope resolution of negation and specu-
lation (Morante and Sporleder, 2010), followed 
by a special issue of Computational Linguistics 
(Morante and Sporleder, 2012) for modality and 
negation. 
The research on negation focus identification 
was pioneered by Blanco and Moldovan (2011), 
who investigated the negation phenomenon in 
semantic relations and proposed a supervised 
learning approach to identify the focus of a nega-
tion expression. However, although Morante and 
Blanco (2012) proposed negation focus identifi-
cation as one of the *SEM?2012 shared tasks, 
only one team (Rosenberg and Bergler, 2012) 1 
participated in this task. They identified negation 
focus using three kinds of heuristics and 
achieved 58.40 in F1-measure. This indicates 
great expectation in negation focus identification. 
The key problem in current research on nega-
tion focus identification is its focus on intra-
sentence information and large ignorance of in-
ter-sentence information, which plays a critical 
role in the success of negation focus identifica-
tion. For example, Ding (2011) made a qualita-
tive analysis on implied negations in conversa-
tion and attempted to determine whether a sen-
tence was negated by context information, from 
the linguistic perspective. Moreover, a negation 
focus is always associated with authors? intention 
in article. This indicates the great challenges in 
negation focus identification. 
3 Baselines 
Negation focus identification in *SEM?2012 
shared tasks is restricted to verbal negations an-
notated with MNEG in PropBank, with only the 
constituent belonging to a semantic role selected 
as negation focus. Normally, a verbal negation 
expression (not or n?t) is grammatically associat-
ed with its corresponding verb (e.g., He didn?t 
stop). For details on annotation guidelines and 
                                                 
1 In *SEM?2013, the shared task is changed with focus on 
"Semantic Textual Similarity". 
523
examples for verbal negations, please refer to 
Blanco and Moldovan (2011). 
For comparison, we choose the state-of-the-art 
system described in Blanco and Moldovan 
(2011), which employed various kinds of syntac-
tic features and semantic role features, as one of 
our baselines. Since this system adopted C4.5 for 
training, we name it as BaselineC4.5. In order to 
provide a stronger baseline, besides those fea-
tures adopted in BaselineC4.5, we added more re-
fined intra-sentence features and adopted ranking 
Support Vector Machine (SVM) model for train-
ing. We name it as BaselineSVM. 
Following is a list of features adopted in the 
two baselines, for both BaselineC4.5 and Base-
lineSVM, 
? Basic features: first token and its part-of-
speech (POS) tag of the focus candidate; the 
number of tokens in the focus candidate; 
relative position of the focus candidate 
among all the roles present in the sentence; 
negated verb and its POS tag of the negative 
expression;  
? Syntactic features: the sequence of words 
from the beginning of the governing VP to 
the negated verb; the sequence of POS tags 
from the beginning of the governing VP to 
the negated verb; whether the governing VP 
contains a CC; whether the governing VP 
contains a RB. 
? Semantic features: the syntactic label of se-
mantic role A1; whether A1 contains POS 
tag DT, JJ, PRP, CD, RB, VB, and WP, as 
defined in Blanco and Moldovan (2011); 
whether A1 contains token any, anybody, an-
ymore, anyone, anything, anytime, anywhere, 
certain, enough, full, many, much, other, 
some, specifics, too, and until, as defined in 
Blanco and Moldovan (2011); the syntactic 
label of the first semantic role in the sentence; 
the semantic label of the last semantic role in 
the sentence; the thematic role for 
A0/A1/A2/A3/A4 of the negated predicate. 
and for BaselineSVM only, 
? Basic features: the named entity and its type 
in the focus candidate; relative position of the 
focus candidate to the negative expression 
(before or after). 
? Syntactic features: the dependency path and 
its depth from the focus candidate to the neg-
ative expression; the constituent path and its 
depth from the focus candidate to the nega-
tive expression; 
4 Exploring Contextual Discourse In-
formation for Negation Focus Identi-
fication 
While some of negation focuses could be identi-
fied by only intra-sentence information, others 
must be identified by contextual discourse in-
formation. Section 1 illustrates the necessity of 
such contextual discourse information in nega-
tion focus identification by giving three scenarios 
of different discourse contexts for negation ex-
pression n?t in sentence (3). 
For better illustration of the importance of 
contextual discourse information, Table 1 shows 
the statistics of intra- and inter-sentence infor-
mation necessary for manual negation focus 
identification with 100 instances randomly ex-
tracted from the held-out dataset of *SEM'2012 
shared task corpus. It shows that only 17 instanc-
es can be identified by intra-sentence information. 
It is surprising that inter-sentence information is 
indispensable in 77 instances, among which 42 
instances need only inter-sentence information 
and 35 instances need both intra- and inter-
sentence information. This indicates the great 
importance of contextual discourse information 
on negation focus identification. It is also inter-
esting to note 6 instances are hard to determine 
even given both intra- and inter-sentence infor-
mation. 
Info Number
#Intra-Sentence Only 17 
#Inter-Sentence Only 42 
#Both 35 
#Hard to Identify 6 
(Note: "Hard to Identify" means that it is hard for a 
human being to identify the negation focus even 
given both intra- and inter-sentence information.) 
Table 1. Statistics of intra- and inter-sentence 
information on negation focus identification. 
Statistically, we find that negation focus is al-
ways related with what authors repeatedly states 
in discourse context. This explains why contex-
tual discourse information could help identify 
negation focus. While inter-sentence information 
provides the global characteristics from the dis-
course context perspective and intra-sentence 
information provides the local features from lex-
ical, syntactic and semantic perspectives, both 
have their own contributions on negation focus 
identification. 
In this paper, we first propose a graph model 
to gauge the importance of contextual discourse 
524
information. Then, we incorporate both intra- 
and inter-sentence features into a machine learn-
ing-based framework for negation focus identifi-
cation. 
4.1 Graph Model 
Graph models have been proven successful in 
many NLP applications, especially in represent-
ing the link relationships between words or sen-
tences (Wan and Yang, 2008; Li et al, 2009). 
Generally, such models could construct a graph 
to compute the relevance between document 
theme and words. 
In this paper, we propose a graph model to 
represent the contextual discourse information 
from both lexical and topic perspectives. In par-
ticular, a word-based graph model is proposed to 
represent the explicit relatedness among words in 
a discourse from the lexical perspective, while a 
topic-driven word-based model is proposed to 
enrich the implicit relatedness between words, by 
adding one more layer to the word-based graph 
model in representing the global topic distribu-
tion of the whole dataset. Besides, the PageRank 
algorithm (Page et al, 1998) is adopted to opti-
mize the graph model. 
Word-based Graph Model: 
A word-based graph model can be defined as 
Gword (W, E), where W={wi} is the set of words in 
one document and E={eij|wi, wj ?W} is the set of 
directed edges between these words, as shown in 
Figure 1. 
 Figure 1. Word-based graph model. 
In the word-based graph model, word node wi 
is weighted to represent the correlation of the 
word with authors? intention. Since such correla-
tion is more from the semantic perspective than 
the grammatical perspective, only content words 
are considered in our graph model, ignoring 
functional words (e.g., the, to,?). Especially, the 
content words limited to those with part-of-
speech tags of JJ, NN, PRP, and VB. For sim-
plicity, the weight of word node wi is initialized 
to 1. 
In addition, directed edge eij is weighted to 
represent the relatedness between word wi and 
word wj in a document with transition probability 
P(j|i) from i to j, which is normalized as follows: 
???|?? ? ??????,???? ??????,????                    (1) 
where k represents the nodes in discourse, and 
Sim(wi,wj) denotes the similarity between wi and 
wj. In this paper, two kinds of information are 
used to calculate the similarity between words. 
One is word co-occurrence (if word wi and word 
wj occur in the same sentence or in the adjacent 
sentences, Sim(wi,wj) increases 1), and the other 
is WordNet (Miller, 1995) based similarity. 
Please note that Sim(wi,wi) = 0 to avoid self-
transition, and Sim(wi,wj) and Sim(wj,wi) may not 
be equal. 
Finally, the weights of word nodes are calcu-
lated using the PageRank algorithm as follows: 
???????????? ? 1 
?????????????? ? ? ? ???????????? ???? ???|?? ?
																																		?1 ? ??                                       (2) 
where d is the damping factor as in the PageRank 
algorithm. 
Topic-driven Word-based Graph Model 
While the above word-based graph model can 
well capture the relatedness between content 
words, it can only partially model the focus of a 
negation expression since negation focus is more 
directly related with topic than content. In order 
to reduce the gap, we propose a topic-driven 
word-based model by adding one more layer to 
refine the word-based graph model over the 
global topic distribution, as shown in Figure 2.  
 Figure 2. Topic-driven word-based graph model. 
525
Here, the topics are extracted from all the doc-
uments in the *SEM 2012 shared task using the 
LDA Gibbs Sampling algorithm (Griffiths, 2002). 
In the topic-driven word-based graph model, the 
first layer denotes the relatedness among content 
words as captured in the above word-based graph 
model, and the second layer denotes the topic 
distribution, with the dashed lines between these 
two layers indicating the word-topic model re-
turn by LDA. 
Formally, the topic-driven word-based two-
layer graph is defined as Gtopic (W, T, Ew, Et), 
where W={wi} is the set of words in one docu-
ment and T={ti} is the set of topics in all docu-
ments; Ew={ewij|wi, wj ?W} is the set of directed 
edges between words and Et ={etij|wi?W, tj ?T} 
is the set of undirected edges between words and 
topics; transition probability Pw(j|i) of ewij is de-
fined as the same as P(j|i) of the word-based 
graph model. Besides, transition probability Pt 
(i,m) of etij in the word-topic model is defined as: 
????, ?? ? ??????,???? ??????,????                 (3) 
where Rel(wi, tm) is the weight of word wi in top-
ic tm calculated by the LDA Gibbs Sampling al-
gorithm.  On the basis, the transition probability 
Pw (j|i) of ewij is updated by calculating as fol-
lowing: 
?????|?? ? ? ? ????|?? ? ?1 ? ?? ? ????,???????,??? ????,???????,???   
(4) 
where k represents all topics linked to both word 
wi and word wj, and ??[0,1] is the coefficient 
controlling the relative contributions from the 
lexical information in current document and the 
topic information in all documents. 
Finally, the weights of word nodes are calcu-
lated using the PageRank algorithm as follows: 
???????????? ? 1 
?????????????? ? ? ? ???????????? ???? ?????|?? ?
																																		?1 ? ??                                       (5) 
where d is the damping factor as in the PageRank 
algorithm. 
4.2 Negation Focus Identification via 
Graph Model 
Given the graph models and the PageRank opti-
mization algorithm discussed above, four kinds 
of contextual discourse information are extracted 
as inter-sentence features (Table 2). 
In particular, the total weight and the max 
weight of words in the focus candidate are calcu-
lated as follows: 
??????????? ? ? ?????????????????         (6) 
????????? ? max? ????????????????    (7) 
where i represents the content words in the focus 
candidate. These two kinds of weights focus on 
different aspects about the focus candidate with 
the former on the contribution of content words, 
which is more beneficial for a long focus candi-
date, and the latter biased towards the focus can-
didate which contains some critical word in a 
discourse. 
No Feature 
1 Total weight of words in the focus candi-date using the co-occurrence similarity. 
2 Max weight of words in the focus candi-date using the co-occurrence similarity. 
3 Total weight of words in the focus candi-date using the WordNet similarity. 
4 Max weight of words in the focus candi-date using the WordNet similarity. 
Table 2. Inter-sentence features extracted from 
graph model. 
For evaluating the contribution of contextual 
discourse information on negation focus identifi-
cation directly, we incorporate the four inter-
sentence features from the topic-driven word-
based graph model into a negation focus identifi-
er. 
5 Experimentation 
In this section, we describe experimental settings 
and systematically evaluate our negation focus 
identification approach with focus on exploring 
the effectiveness of contextual discourse infor-
mation. 
5.1 Experimental Settings 
Dataset 
In all our experiments, we employ the 
*SEM'2012 shared task corpus (Morante and 
Blanco, 2012)2 . As a freely downloadable re-
source, the *SEM shared task corpus is annotated 
on top of PropBank, which uses the WSJ section 
of the Penn TreeBank. In particular, negation 
focus annotation on this corpus is restricted to 
verbal negations (with corresponding mark 
                                                 
2 http://www.clips.ua.ac.be/sem2012-st-neg/ 
526
MNEG in PropBank). On 50% of the corpus an-
notated by two annotators, the inter-annotator 
agreement was 0.72 (Blanco and Moldovan, 
2011). Along with negation focus annotation, 
this corpus also contains other annotations, such 
as POS tag, named entity, chunk, constituent tree, 
dependency tree, and semantic role. 
In total, this corpus provides 3,544 instances 
of negation focus annotations. For fair compari-
son, we adopt the same partition as *SEM?2012 
shared task in all our experiments, i.e., with 
2,302 for training, 530 for development, and 712 
for testing. Although for each instance, the cor-
pus only provides the current sentence, the pre-
vious and next sentences as its context, we sort to 
the Penn TreeBank3 to obtain the corresponding 
document as its discourse context. 
Evaluation Metrics 
Same as the *SEM'2012 shared task, the evalua-
tion is made using precision, recall, and F1-score. 
Especially, a true positive (TP) requires an exact 
match for the negation focus, a false positive (FP) 
occurs when a system predicts a non-existing 
negation focus, and a false negative (FN) occurs 
when the gold annotations specify a negation 
focus but the system makes no prediction. For 
each instance, the predicted focus is considered 
correct if it is a complete match with a gold an-
notation. 
Beside, to show whether an improvement is 
significant, we conducted significance testing 
using z-test, as described in Blanco and Moldo-
van (2011). 
Toolkits 
In our experiments, we report not only the de-
fault performance with gold additional annotated 
features provided by the *SEM'2012 shared task 
corpus and the Penn TreeBank, but also the per-
formance with various kinds of features extracted 
automatically, using following toolkits: 
? Syntactic Parser: We employ the Stanford 
Parser4 (Klein and Manning, 2003; De Marn-
effe et al, 2006) for tokenization, constituent 
and dependency parsing. 
? Named Entity Recognizer: We employ the 
Stanford NER5 (Finkel et al, 2005) to obtain 
named entities. 
                                                 
3 http://www.cis.upenn.edu/~treebank/ 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://nlp.stanford.edu/ner/ 
? Semantic Role Labeler: We employ the se-
mantic role labeler, as described in Punyaka-
nok et al(2008). 
? Topic Modeler: For estimating transition 
probability Pt(i,m), we employ 
GibbsLDA++6, an LDA model using Gibbs 
Sampling technique for parameter estimation 
and inference. 
? Classifier: We employ SVMLight 7 with default 
parameters as our classifier. 
5.2 Experimental Results 
With Only Intra-sentence Information 
Table 3 shows the performance of the two base-
lines, the decision tree-based classifier as in 
Blanco and Moldovan (2011) and our ranking 
SVM-based classifier. It shows that our ranking 
SVM-based baseline slightly improves the F1-
measure by 2.52% over the decision tree-based 
baseline, largely due to the incorporation of more 
refined features.  
System P(%) R(%) F1 
BaselineC4.5 66.73 49.93 57.12
BaselineSVM 60.22 59.07 59.64
Table 3. Performance of baselines with only 
intra-sentence information. 
Error analysis of the ranking SVM-based 
baseline on development data shows that 72% of 
them are caused by the ignorance of inter-
sentence information. For example, among the 
42 instances listed in the category of ?#Inter-
Sentence Only? in Table 1, only 7 instances can 
be identified correctly by the ranking SVM-
based classifier. With about 4 focus candidates in 
one sentence on average, this percentage is even 
lower than random. 
With Only Inter-sentence Information 
For exploring the usefulness of pure contextual 
discourse information in negation focus identifi-
cation, we only employ inter-sentence features 
into ranking SVM-based classifier. First of all, 
we estimate two parameters for our topic-driven 
word-based graph model: topic number T for 
topic model and coefficient ? between Pw(j|i) and 
Pt (i,m) in Formula 4. 
Given the LDA Gibbs Sampling model with 
parameters ? = 50/T and ? = 0.1, we vary T from 
20 to 100 with an interval of 10 to find the opti-
                                                 
6 http://gibbslda.sourceforge.net/ 
7 http://svmlight.joachims.org 
527
mal T. Figure 3 shows the experiment results of 
varying T (with ? = 0.5) on development data. It 
shows that the best performance is achieved 
when T = 50 with 51.11 in F1). Therefore, we set 
T as 50 in our following experiments. 
 Figure 3. Performance with varying T. 
For parameter ?, a trade-off between the tran-
sition probability Pw(j|i) (word to word) and the 
transition probability Pt (i,m) (word and topic) to 
update P?w(j|i), we vary it from 0 to 1 with an 
interval of 0.1. Figure 4 shows the experiment 
results of varying ? (with T=50) on development 
data. It shows that the best performance is 
achieved when ? = 0.6, which are adopted here-
after in all our experiments. This indicates that 
direct lexical information in current document 
contributes more than indirect topic information 
in all documents on negation focus identification. 
It also shows that direct lexical information in 
current document and indirect topic information 
in all documents are much complementary on 
negation focus identification. 
 Figure 4. Performance with varying ?. 
System P(%) R(%) F1 
using word-based graph 
model  45.62 42.02 43.75
using topic-driven word-
based graph model 54.59 50.76 52.61
Table 4. Performance with only inter-sentence 
information. 
Table 4 shows the performance of negation 
focus identification with only inter-sentence fea-
tures. It also shows that the system with inter-
sentence features from the topic-driven word-
based graph model significantly improves the 
F1-measure by 8.86 over the system with inter-
sentence features from the word-based graph 
model, largely due to the usefulness of topic in-
formation. 
In comparison with Table 3, it shows that the 
system with only intra-sentence features achieves 
better performance than the one with only inter-
sentence features (59.64 vs. 52.61 in F1-
measure). 
With both Intra- and Inter-sentence In-
formation 
Table 5 shows that enriching intra-sentence fea-
tures with inter-sentence features significantly 
(p<0.01) improve the performance by 9.85 in F1-
measure than the better baseline. This indicates 
the usefulness of such contextual discourse in-
formation and the effectiveness of our topic-
driven word-based graph model in negation fo-
cus identification.  
System P(%) R(%) F1 
BaselineC4.5 with intra 
feat. only 66.73 49.93 57.12
BaselineSVM with intra 
feat. only 60.22 59.07 59.64
Ours with Both feat. 
using word-based GM 64.93 62.47 63.68
Ours  with  Both   feat. 
using    topic-driven 
word-based GM
71.67 67.43 69.49
(Note: ?feat.? denotes features; ?GM? denotes graph model.) 
Table 5. Performance comparison of systems on 
negation focus identification. 
System P(%) R(%) F1 
BaselineC4.5 with intra 
feat. only (auto) 60.94 44.62 51.52
BaselineSVM with intra 
feat. Only (auto) 53.81 51.67 52.72
Ours with Both feat. 
using word-based GM 
(auto) 
58.77 57.19 57.97
Ours  with  Both   feat. 
using    topic-driven 
word-based GM (auto) 
66.74 64.53 65.62
Table 6. Performance comparison of systems on 
negation focus identification with automatically 
extracted features. 
528
Besides, Table 6 shows the performance of 
our best system with all features automatically 
extracted using the toolkits as described in Sec-
tion 5.1. Compared with our best system employ-
ing gold additional annotated features (the last 
line in Table 5), the homologous system with 
automatically extracted features (the last line in 
Table 6) only decrease of less than 4 in F1-
measure. This demonstrates the achievability of 
our approach. 
In comparison with the best-reported perfor-
mance on the *SEM?2012 shared task (Rosen-
berg and Bergler, 2012), our system performs 
better by about 11 in F-measure.  
5.3 Discussion 
While this paper verifies the usefulness of con-
textual discourse information on negation focus 
identification, the performance with only inter-
sentence features is still weaker than that with 
only intra-sentence features. There are two main 
reasons. On the one hand, the former employs an 
unsupervised approach without prior knowledge 
for training. On the other hand, the usefulness of 
inter-sentence features depends on the assump-
tion that a negation focus relates to the meaning 
of which is most relevant to authors? intention in 
a discourse. If there lacks relevant information in 
a discourse context, negation focus will become 
difficult to be identified only by inter-sentence 
features.  
Error analysis also shows that some of the ne-
gation focuses are very difficult to be identified, 
even for a human being. Consider the sentence (3) 
in Section 1, if given sentence because of her 
neighbors' protests, but her husband doesn?t 
think so as its following context, both Helen and 
to play the violin can become the negation focus. 
Moreover, the inter-annotator agreement in the 
first round of negation focus annotation can only 
reach 0.72 (Blanco and Moldovan, 2011). This 
indicates inherent difficulty in negation focus 
identification. 
6 Conclusion 
In this paper, we propose a graph model to enrich 
intra-sentence features with inter-sentence fea-
tures from both lexical and topic perspectives. In 
this graph model, the relatedness between words 
is calculated by word co-occurrence, WordNet-
based similarity, and topic-driven similarity. 
Evaluation on the *SEM 2012 shared task corpus 
indicates the usefulness of contextual discourse 
information on negation focus identification and 
our graph model in capturing such global infor-
mation. 
In future work, we will focus on exploring 
more contextual discourse information via the 
graph model and better ways of integrating intra- 
and inter-sentence information on negation focus 
identification. 
Acknowledgments 
This research is supported by the National Natu-
ral Science Foundation of China, No.61272260, 
No.61331011, No.61273320, the Natural Science 
Foundation of Jiangsu Province, No. BK2011282, 
the Major Project of College Natural Science 
Foundation of Jiangsu Province, 
No.11KIJ520003, and the Graduates Project of 
Science and Innovation, No. CXZZ12_0818. The 
authors would like to thank the anonymous re-
viewers for their insightful comments and sug-
gestions. Our sincere thanks are also extended to 
Dr. Zhongqing Wang for his valuable discus-
sions during this study. 
Reference 
Eduardo Blanco and Dan Moldovan. 2011. Semantic 
Representation of Negation Using Focus Detection. 
In Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics, pages 
581-589, Portland, Oregon, June 19-24, 2011. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. 
A simple algorithm for identifying negated find-
ings and diseases in discharge summaries. Journal 
of Biomedical Informatics, 34:301-310. 
Marie-Catherine De Marneffe, Bill MacCartney and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 
In Proceedings of  LREC?2006. 
Yun Ding. 2011. Implied Negation in Discourse. 
Journal of Theory and Practice in Language Stud-
ies, 1(1): 44-51, Jan 2011. 
Jenny Rose Finkel, Trond Grenager, and Christopher 
Manning. 2005. Incorporating non-local infor-
mation into information extraction systems by 
gibbs sampling. In Proceedings of the 43rd Annual 
Meeting on Association for Computational Lin-
guistics, pages 363-370, Stroudsburg, PA, USA. 
Tom Griffiths. 2002. Gibbs sampling in the generative 
model of Latent Dirichlet Allocation. Tech. rep., 
Stanford University. 
Laurence R Horn. 1989. A Natural History of Nega-
tion. Chicago University Press, Chicago, IL. 
529
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan 
Zhu. 2009. Answering Opinion Questions with 
Random Walks on Graphs. In Proceedings of the 
47th Annual Meeting of the ACL and the 4th 
IJCNLP of the AFNLP, pages 737-745, Suntec, 
Singapore, 2-7 Aug 2009. 
Junhui Li, Guodong Zhou, Hongling Wang, and Qi-
aoming Zhu. 2010. Learning the Scope of Negation 
via Shallow Semantic Parsing. In Proceedings of 
the 23rd International Conference on Computa-
tional Linguistics. Stroudsburg, PA, USA: Associa-
tion for Computational Linguistics, 671-679. 
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, 
Yoshinobu Kano, and Jun'ichi Tsujii. 2009. Over-
view of BioNLP'09 Shared Task on Event Extrac-
tion. In Proceedings of the BioNLP'2009 Workshop 
Companion Volume for Shared Task. Stroudsburg, 
PA, USA: Association for Computational Linguis-
tics, 1-9. 
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 
41st Meeting of the Association for Computational 
Linguistics, pages 423-430. 
George A. Miller. 1995. Wordnet: a lexical database 
for english. Commun. ACM, 38(11):39-41. 
Roser Morante, Anthony Liekens and Walter Daele-
mans. 2008. Learning the Scope of Negation in Bi-
omedical Texts. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 715-724, Honolulu, Oc-
tober 2008. 
Roser Morante and Caroline Sporleder, editors. 2010. 
In Proceedings of the Workshop on Negation and 
Speculation in Natural Language Processing. Uni-
versity of Antwerp, Uppsala, Sweden. 
Roser Morante and Eduardo Blanco. 2012. *SEM 
2012 Shared Task: Resolving the Scope and Focus 
of Negation. In Proceedings of the First Joint Con-
ference on Lexical and Computational Semantics 
(*SEM), pages 265-274, Montreal, Canada, June 7-
8, 2012. 
Roser Morante and Caroline Sporleder. 2012. Modali-
ty and Negation: An Introduction to the Special Is-
sue. Computational Linguistics, 2012, 38(2): 223-
260. 
Roser Morante and Walter Daelemans. 2012. Conan 
Doyle-neg: Annotation of negation cues and their 
scope in Conan Doyle stories. In Proceedings of 
LREC 2012, Istambul. 
Lawrence Page, Sergey Brin, Rajeev Motwani, and 
Terry Winograd. 1998. The pagerank citation rank-
ing: Bringing order to the web. Technical report, 
Stanford University. 
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. 
The importance of syntactic parsing and inference 
in semantic role labeling. Computational Linguis-
tics, 34(2):257-287, June. 
Sabine Rosenberg and Sabine Bergler. 2012. UCon-
cordia: CLaC Negation Focus Detection at *Sem 
2012. In Proceedings of the First Joint Conference 
on Lexical and Computational Semantics (*SEM), 
pages 294-300, Montreal, Canada, June 7-8, 2012. 
Ton van der Wouden. 1997. Negative Contexts: Col-
location, Polarity, and Multiple Negation. 
Routledge, London. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for un-
certainty,negation and their scopes. BMC Bioin-
formatics, 9(Suppl 11):S9. 
Xiaojun Wan and Jianwu Yang. 2008. Multi-
document summarization using cluster-based link 
analysis. In Proceedings of the 31st annual inter-
national ACM SIGIR conference on Research and 
development in information retrieval, pages 299-
306. 
 
530
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 582?592,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Bilingual Active Learning for Relation Classification via Pseudo Paral-
lel Corpora 
 
Longhua Qian    Haotian Hui   Ya?nan Hu   Guodong Zhou*   Qiaoming Zhu 
Natural Language Processing Lab 
School of Computer Science and Technology, Soochow University 
1 Shizi Street, Suzhou, China 215006 
{qianlonghua,20134227019,20114227025,gdzhou,qmzhu}@suda.edu.cn 
 
  
 
Abstract 
Active learning (AL) has been proven ef-
fective to reduce human annotation ef-
forts in NLP. However, previous studies 
on AL are limited to applications in a 
single language. This paper proposes a 
bilingual active learning paradigm for re-
lation classification, where the unlabeled 
instances are first jointly chosen in terms 
of their prediction uncertainty scores in 
two languages and then manually labeled 
by an oracle. Instead of using a parallel 
corpus, labeled and unlabeled instances 
in one language are translated into ones 
in the other language and all instances in 
both languages are then fed into a bilin-
gual active learning engine as pseudo 
parallel corpora. Experimental results on 
the ACE RDC 2005 Chinese and English 
corpora show that bilingual active learn-
ing for relation classification signifi-
cantly outperforms monolingual active 
learning. 
1 Introduction 
Semantic relation extraction between named en-
tities (aka. entity relation extraction or more con-
cisely relation extraction) is an important subtask 
of Information Extraction (IE) as well as Natural 
Language Processing (NLP). With its aim to 
identify and classify the semantic relationship 
between two entities (ACE 2002-2007), relation 
extraction is of great significance to many NLP 
applications, such as question answering, infor-
mation fusion, social network construction, and 
knowledge mining and population etc. 
                                                 
* Corresponding author 
In the literature, the mainstream research on 
relation extraction adopts statistical machine 
learning methods, which can be grouped into 
supervised learning (Zelenko et al, 2003; Culotta 
and Soresen, 2004; Zhou et al, 2005; Zhang et 
al., 2006; Qian et al, 2008; Chan and Roth, 
2011), semi-supervised learning (Zhang et al, 
2004; Chen et al, 2006; Zhou et al, 2008; Qian 
et al, 2010) and unsupervised learning (Hase-
gawa et al, 2004; Zhang et al, 2005) in terms of 
the amount of labeled training data they need. 
Usually the extraction performance depends 
heavily on the quality and quantity of the labeled 
data, however, the manual annotation of a large-
scale corpus is labor-intensive and time-
consuming. In the last decade researchers have 
turned to another effective learning paradigm--
active learning (AL), which, given a small num-
ber of labeled instances and a large number of 
unlabeled instances, selects the most informative 
unlabeled instances to be manually annotated and 
add them into the training data in an iterative 
fashion. Essentially active learning attempts to 
decrease the quantity of labeled instances by en-
hancing their quality, gauged by their informa-
tiveness to the learner. Since its emergence, ac-
tive learning has been successfully applied to 
many tasks in NLP (Engelson and Dagan, 1996; 
Hwa, 2004; Tomanek et al, 2007; Settles and 
Craven, 2008).  
It is trivial to validate, as we will do later in 
this paper, that active learning can also alleviate 
the annotation burden for relation extraction in 
one language while retaining the extraction per-
formance. However, there are cases when we 
may exploit relation extraction in multiple lan-
guages and there are corpora with relation in-
stances annotated for more than one language, 
such as the ACE RDC 2005 English and Chinese 
corpora. Hu et al (2013) shows that supervised 
relation extraction in one language (e.g. Chinese) 
582
can be enhanced by relation instances translated 
from another language (e.g. English). This dem-
onstrates that there is some complementariness 
between relation instances in two languages, par-
ticularly when the training data is scarce. One 
natural question is: Can this characteristic be 
made full use of so that active learning can 
maximally benefit relation extraction in two lan-
guages? To the best of our knowledge, so far the 
issue of joint active learning in two languages 
has yet been addressed. Moreover, the success of 
joint bilingual learning may lend itself to many 
inherent multilingual NLP tasks such as POS 
tagging (Yarowsky and Ngai, 2001), name entity 
recognition (Yarowsky et al, 2001), sentiment 
analysis (Wan, 2009), and semantic role labeling 
(Sebastian and Lapata, 2009) etc. 
This paper proposes a bilingual active learn-
ing (BAL) paradigm to relation classification 
with a small number of labeled relation instances 
and a large number of unlabeled instances in two 
languages (non-parallel). Instead of using a par-
allel corpus which should have entity/relation 
alignment information and is thus difficult to 
obtain, this paper employs an off-the-shelf ma-
chine translator to translate both labeled and 
unlabeled instances from one language into the 
other language, forming pseudo parallel corpora. 
These translated instances along with the original 
instances are then fed into a bilingual active 
learning engine. Findings obtained from experi-
ments with relation classification on the ACE 
2005 corpora show that this kind of pseudo-
parallel corpora can significantly improve the 
classification performance for both languages in 
a BAL framework. 
The rest of the paper is organized as follows. 
Section 2 reviews the previous work on relation 
extraction while Section 3 describes our baseline 
systems. Section 4 elaborates on the bilingual 
active learning paradigm and Section 5 discusses 
the experimental results. Finally conclusions and 
directions for future work are presented in Sec-
tion 6. 
2 Related Work 
While there are many studies in monolingual 
relation extraction, there are only a few on multi-
lingual relation extraction in the literature. 
Monolingual relation extraction: A wide 
range of studies on relation extraction focus on 
monolingual resources. As far as representation 
of relation instances is concerned, there are fea-
ture-based methods (Zhao et al, 2004; Zhou et 
al., 2005; Chan and Roth, 2011) and kernel-
based methods (Zelenko et al, 2003; Zhang et al, 
2006; Qian et al, 2008), mainly for the English 
language. Both methods are also widely used in 
relation extraction in other languages, such as 
those in Chinese relation extraction (Che et al, 
2005; Li et al, 2008; Yu et al, 2010). 
Multilingual relation extraction: There are 
only two studies related to multilingual relation 
extraction. Kim et al (2010) propose a cross-
lingual annotation projection approach which 
uses parallel corpora to acquire a relation detec-
tor on the target language. However, the map-
ping of two entities involved in a relation in-
stance may leads to errors. Therefore, Kim and 
Lee (2012) further employ a graph-based semi-
supervised learning method, namely Label 
Propagation (LP), to indirectly propagate labels 
from the source language to the target language 
in an iterative fashion. Both studies transfer rela-
tion annotations via parallel corpora from the 
resource-rich language (English) to the resource-
poor language (Korean), but not vice versa. 
Based on a small number of labeled instances 
and a large number of unlabeled instances in 
both languages, our method differs from theirs in 
that we adopt a bilingual active learning para-
digm via machine translation and improve the 
performance for both languages simultaneously. 
Active Learning in NLP: Active learning 
has become an active research topic due to its 
potential to significantly reduce the amount of 
labeled training data while achieving comparable 
performance with supervised learning. It has 
been successfully applied to many NLP applica-
tions, such as POS tagging (Engelson and Dagan, 
1996; Ringger et al, 2007), word sense disam-
biguation (Chan and Ng, 2007; Zhu and Hovy, 
2007), sentiment detection (Brew et al, 2010; Li 
et al, 2012), syntactical parsing (Hwa, 2004; 
Osborne and Baldridge, 2004), and named entity 
recognition (Shen et al, 2004; Tomanek et al, 
2007; Tomanek and Hahn, 2009) etc.  
Different from these AL studies on a single 
task, Reichart et al (2008) introduce a multi-task 
active learning (MTAL) paradigm, where unla-
beled instances are selected for two annotation 
tasks (i.e. named entity and syntactic parse tree). 
They demonstrate that MTAL in the same lan-
guage outperforms one-sided and random selec-
tion AL. From a different perspective, we pro-
pose an active learning framework for the same 
task, but across two different languages. 
Another related study (Haffari and Sarkar, 
2009) deals with active learning for multilingual 
583
machine translation, which make use of multilin-
gual corpora to decrease human annotation ef-
forts by selecting highly informative sentences 
for a newly added language in multilingual paral-
lel corpora. While machine translation inherently 
deals with multilingual parallel corpora, our task 
focuses on relation extraction by pseudo parallel 
corpora in two languages. 
3 Baseline Systems 
This section first introduces the fundamental su-
pervised learning method, and then describes a 
baseline active learning algorithm. 
3.1 Supervised Learning 
We adopt the feature-based method for funda-
mental supervised relation classification, rather 
than the tree kernel-based method, since active 
learning needs a large number of iterations and 
the kernel-based method usually performs much 
slower than the feature-based one. Following is a 
list of our used features, much similar to Zhou et 
al. (2005): 
a) Lexical features of entities and their contexts 
WM1: bag-of-words in the 1st entity mention 
HM1: headword of M1 
WM2: bag-of-words in the 2nd entity mention 
HM2: headword of M2 
HM12: combination of HM1 and HM2 
WBNULL: when no word in between 
WBFL: the only one word in between 
WBF: the first word in between when at least 
two words in between 
WBL: the last word in between when at least 
two words in between 
WBO: other words in between except the first 
and last words when at least three words in 
between 
b) Entity type 
ET12: combination of entity types 
EST12: combination of entity subtypes 
EC12: combination of entity classes 
c) Mention level 
ML12: combination of entity mention levels 
MT12: combination of LDC mention types 
d) Overlap 
#WB: number of other mentions in between 
#MB: number of words in between 
M1>M2 or M1<M2: flag indicating whether 
M2/M1 is included in M1/M2. 
3.2 Active Learning Algorithm 
We use a pool-based active learning procedure 
with uncertainty sampling (Scheffer et al, 2001; 
Culotta and McCallum, 2005; Kim et al, 2006) 
for both Chinese and English relation classifica-
tion as illustrated in Fig. 1. During iterations a 
batch of unlabeled instances are chosen in terms 
of their informativeness to the current classifier, 
labeled by an oracle and in turn added into the 
labeled data to retrain the classifier. Due to our 
focus on the effectiveness of bilingual active 
learning on relation classification, we only use 
uncertainty sampling without incorporating more 
complex measures, such as diversity and repre-
sentativeness (Settles and Craven, 2008), and 
leave them for future work. 
Input: 
- L, labeled data set 
- U, unlabeled data set 
- n, batch size
Output:
- SVM, classifier 
Repeat:
    1. Train a single classifier SVM on L
2. Run the classifier on U
3. Find at most n instances in U that the classifier 
has the highest prediction uncertainty
    4. Have these instances labeled by an oracle
5. Add them into L
Until: certain number of instances are labeled or 
certain performance is reached
Algorithm uncertainty-based active learning
Figure 1. Pool-based active learning with uncer-
tainty sampling 
Since the SVMLIB package used in this paper 
can output probabilities assigned to the class la-
bels on an instance, we have three uncertainty 
metrics readily available, i.e., least confidence 
(LC), margin (M) and entropy (E). The NER 
experimental results on multiple corpora (Settles 
and Craven, 2008) show that there is no single 
clear winner among these three metrics. This 
conclusion is also validated by our preliminary 
experiments on the task of active learning rela-
tion extraction, thus we adopt the LC metric for 
simplicity. Specifically, with a sequence of K 
probabilities for a relation instance at some itera-
tion, denoted as {p1,p2,?pK} in the descending 
order, the LC metric of the relation instance can 
be simply picked as the first one, i.e. 
1pH
LC =     (1) 
Where K denotes the total number of relation 
classes. Note that this metric actually reflects 
prediction reliability (i.e. reverse uncertainty) 
rather than uncertainty in order to facilitate joint 
584
confidence calculation for two languages (cf. 
?4.4). Intuitively, the smaller the HLC is, the less 
confident the prediction is. 
4 Bilingual Active Learning for Rela-
tion Classification 
In this section, we elaborate on the bilingual ac-
tive learning for relation extraction. 
4.1 Problem Definition 
With Chinese and English (designated as c and e) 
as two languages used in our study, this paper 
intends to address the task of bilingual relation 
classification, i.e., assigning relation labels to 
candidate instances that have semantic relation-
ships. Suppose we have a small number of la-
beled instances in both languages, denoted as Lc 
and Le (non-parallel) respectively, and a large 
number of unlabeled instances in both languages, 
denoted as Uc and Ue (non-parallel). The test in-
stances in both languages are represented as Tc 
and Te. In order to take full advantage of bilin-
gual resources, we translate both labeled and 
unlabeled instances in one language to ones in 
the other language as follows: 
Lc ? Let 
Uc ? Uet 
Le ? Lct 
Ue ? Lct 
The objective is to learn SVM classifiers in 
both languages, denoted as SVMc and SVMe re-
spectively, in a BAL fashion to improve their 
classification performance. 
4.2 Bilingual Active Learning Framework 
Currently, AL is widely used in NLP tasks in a 
single language, i.e., during iterations unlabeled 
instances least confident only in one language 
are picked and manually labeled to augment the 
training data. The only exception is AL for ma-
chine translation (Haffari et al, 2009; Haffari 
and Sarkar, 2009), whose purpose is to select the 
most informative sentences in the source lan-
guage to be manually translated into the target 
language. Previous studies (Reichart et al, 2008; 
Haffari and Sarkar, 2009) show that multi-task 
active learning (MTAL) can yield promising 
overall results, no matter whether they are two 
different tasks or the task of machine translation 
on multiple language pairs. If a specific NLP 
task on two languages, such as relation classifi-
cation, can be regarded as two tasks, it is reason-
able to argue that these two tasks can benefit 
each other when jointly performed in the BAL 
framework. Yet, to our knowledge, this issue 
remains unexplored. 
An important issue for bilingual learning is 
how to obtain two language views for relation 
instances from multilingual resources. There are 
three solutions to this problem, i.e. parallel cor-
pora (Lu et al, 2011), translated corpora (aka. 
pseudo parallel corpora) (Wan 2009), and bilin-
gual lexicons (Oh et al, 2009). We adopt the one 
with pseudo parallel corpora, using the machine 
translation method to generate instances from 
one language to the other in the BAL paradigm, 
as depicted in Fig. 2. 
English View
Labeled 
Chinese Instances 
(Lc)
Labeled Translated 
English Instances 
(Let)
Labeled 
English Instances (Le)
Labeled Translated 
Chinese Instances 
(Lct)
Machine 
Translation
Machine 
Translation
Unlabeled 
Chinese Instances 
(Uc)
Unlabeled 
Translated Chinese 
Instances (Uct)
Unlabeled Translated
 English Instances (Uet)
Unlabeled 
English Instances 
(Ue)
Machine 
Translation
Machine 
Translation
Chinese View
Bilingual 
active learning
Test
Chinese Instances 
(Tc)
Test
English Instances 
(Te)
 
Figure 2. Framework of bilingual active learning 
In order to make full use of pseudo parallel 
corpora, translated labeled and unlabeled in-
stances are augmented in the following two ways: 
z For labeled Chinese instances (Lc) and Eng-
lish instances (Le), their translated counter-
parts (Let and Lct), along with their labels, are 
directly added into the labeled instances in the 
other language; 
z For unlabeled Chinese instances (Uc) and 
English instances (Ue), during an active learn-
ing iteration the top n unlabeled instances in 
Uc and Uet which are least confidently jointly 
585
predicted by SVMc and SVMe are labeled by 
an oracle and added to Lc and Le respectively. 
(cf. ?4.4) 
4.3 Instance Projection via MT 
Among the several off-the-shelf machine transla-
tion services, we select the Google Translator1 
because of its high quality and easy accessibility. 
Both the mentions of relation instances and the 
mentions of two involved entities are first trans-
lated into the other language via machine transla-
tion. Then, two entities in the original instance 
are aligned with their counterparts in the trans-
lated instance in order to form an aligned bilin-
gual relation instance pair. 
Instance translation 
All the positive instances in the ACE 2005 Chi-
nese and English corpora are translated to an-
other language respectively, i.e. Chinese to Eng-
lish and vice versa. The relation instance is rep-
resented as the word sequence between two enti-
ties. This word sequence, rather than the whole 
sentence, is then translated to another language 
by the Google Translator. The reason is that, al-
though this sequence loses partial contextual in-
formation of the relation instance, its translation 
quality is supposed to be better. Our preliminary 
experiments indicate that the addition of contex-
tual information fail to benefit the task. After 
translation, word segmentation is performed on 
Chinese instances translated from English while 
tokenization is needed for translated English in-
stances. 
Entity alignment 
The objective of entity alignment is to build a 
mapping from the entities in the original in-
stances to the entities in the translated instances. 
Put in another way, entity alignment automati-
cally marks the entity mentions in the translated 
instance, thereby the feature vector correspond-
ing to the translated instance can be constructed. 
Entity alignment is vital in cross-language rela-
tion extraction whose difficulty lies in the fact 
that the same entity mention as an isolated phrase 
and as an integral phrase in the relation instance 
can be translated to different phrases. For exam-
ple, the Chinese entity mention ???? (officer) 
is translated to ?officer? in isolation, it is, how-
ever, translated to ?officials? when in the relation 
instance ???? ??? (Syrian officials). 
                                                 
1 http://translate.google.com 
Input:
- Me, entity mention in English
- Re, relation instance in English
- Mct, translation of Me in Chinese
- Rc, translation of Re in Chinese
- L, a lexicon consisting of entries like (ei, ci, pi), 
where pi is the translation probability from ei to ci
- ?, probability threshold
Output:
- Mc, the counterpart of Me in Rc
Steps:
1. If Mct can be exactly found in Rc, then return 
Mct
2. If the rightmost part of Mct can be found in Rc, 
then this part can be returned
3. For very word we in Me,
a) If there exists a word wc in Rc and (we, wc, p) 
in L and p>?, then (we, wc) is a match of two words
b) Return a successive sequence of matching 
words wc
4. Return null
Algorithm entity alignment
 Figure 3. Entity alignment algorithm 
Therefore, we devise some heuristics to align 
entity mentions between Chinese and English. 
The basic idea is that the word sequence in one 
mention successively matches the word sequence 
in the other mention. Take entity alignment from 
English to Chinese as an example, given entity 
mention Me in relation instance Re in English and 
their respective translations Mct and Rc in Chi-
nese, the objective of entity alignment is to find 
Mc, the counterpart of Me in Rc. The procedure of 
entity alignment algorithm can be described in 
Fig. 3. 
In the algorithm, the probability threshold ? is 
empirically set to 0.002 where the precision and 
recall of entity alignment are balanced. Our lexi-
con is derived from the FBIS parallel corpus 
(#LDC2003E14), which is widely used in ma-
chine translation between English and Chinese. It 
should be noted that the process of relation trans-
lation and entity alignment are far from perfec-
tion, leading to reduction in the number of in-
stances being mapping to the other language, i.e. 
|Lc| > |Let| 
|Uc| > |Uet| 
|Le| > |Lct| 
|Ue| > |Lct| 
4.4 Bilingual Active Learning Algorithm 
The basic idea of our BAL paradigm is that, 
while unlabeled instances uncertain in one lan-
586
guage are informative to the learner in that lan-
guage, unlabeled instances jointly uncertain in 
both languages are informative to the learners in 
both languages, thus potentially improving clas-
sification performance for both languages more 
than their individual active learners do.  This 
idea is embodied in the BAL algorithm in Fig. 4, 
where n is the batch size, i.e., the number of in-
stances selected, labeled and augmented at each 
iteration. 
Figure 4. Bilingual active learning algorithm 
The key point of this algorithm lies in Step 5 
and Step 6, where unlabeled instances from Uc 
and Ue are selected and labeled respectively. 
Take Chinese for an example, when gauging the 
prediction uncertainty for an unlabeled instance 
in Uc, not only its own uncertainty measure Hc 
predicted by SVMc is considered, but also the 
uncertainty measure Het for its translation coun-
terpart in Uet, which is predicted by SVMe, is con-
sidered. Generally, in order to jointly consider 
these two measures, there are three methods to 
compute their means, namely, arithmetic mean, 
geometric mean and harmonic mean. Preliminary 
experiments show that among these three means, 
there is no single winner, so we simply take the 
geometric mean defined as follows:  
etcg HHH *=    (2) 
Considering that we adopt the LC measure as 
the uncertainty score, when an instance in Uc 
can?t find its translation counterpart in Uet due to 
translation error or entity alignment failure, Het is 
set to 1, i.e. the maximum. Since the bigger H is, 
the more confident the prediction is, the less 
likely the instance will be chosen, in this way we 
discourage the unlabeled instances without trans-
lation counterparts. 
5 Experimentation 
We have systematically evaluated our BAL para-
digm on the relation classification task using 
ACE RDC 2005 RDC Chinese and English cor-
pora. 
5.1 Experimental Settings 
Corpora and Preprocessing 
We use the ACE 2005 RDC Chinese and English 
corpora as the benchmark data (hereafter we re-
fer to them as the Chinese corpus (ACE2005c) 
and the English corpus (ACE2005e) respec-
tively). Both corpora have the same en-
tity/relation hierarchies, which define 7 entity 
types, 6 major relation types. However, the Chi-
nese corpus contains 633 documents and 9,147 
positive relation instances while the English cor-
pus only contains 498 files and 6,253 positive 
instances. Therefore, in order to balance the cor-
pus scale to fairly evaluate bilingual active learn-
ing impact on relation classification, we ran-
domly select 458 Chinese files and thus get 
6,268 positive instances, comparable to the Eng-
lish corpus. 
Preprocessing steps for both corpora include 
sentence splitting and tokenization (word seg-
mentation for Chinese using ICTCLAS2). Then, 
positive relation mentions with word sequences 
between two entities and their feature vectors are 
extracted from sentences while negative relation 
mentions are simply discarded because we focus 
on the task of relation classification. After entity 
and relation mentions in one language are trans-
                                                 
2 http://ictclas.org/ 
587
lated into the other language using the Google 
translator, entity alignment is performed between 
relation mentions and their translations. Finally 
4,747 Chinese relation mentions are successfully 
translated and aligned from English and vice 
versa, 4,936 English relation mentions are trans-
lated and aligned from Chinese. 
SVMLIB (Chang and Lin, 2011) is selected as 
our classifier since it supports multi-class classi-
fication. The training parameters C (SVM) is set 
to 2.4 according to our previous work on relation 
extraction (Qian et al, 2010). Relation classifica-
tion performance is evaluated using the standard 
Precision (P), Recall (R) and their harmonic av-
erage (F1) as well as deficiency measure (cf. lat-
ter in this section.). Overall performance scores 
are averaged over 10 runs. For each run, 1/40 
and 1/5 randomly selected instances are used as 
the training and test set respectively while the 
remaining instances are used as the unlabeled set 
for further labeling during active learning itera-
tions. 
Methods for Comparison 
For fair comparison, two baseline methods of 
supervised learning are included to augment their 
training sets with labeled instances during itera-
tions. However, these labeled instances are cho-
sen randomly from the corpus. 
SL-MO (Supervised Learning with monolin-
gual labeled instances): only the monolingual 
labeled instances are fed to the SVM classifiers 
for both Chinese and English relation classifica-
tion respectively. The initial training data only 
contain Lc and Le for Chinese and English respec-
tively.  
SL-CR (Supervised Learning with cross-
lingual labeled instances): in addition to mono-
lingual labeled instances (SL-MO), the training 
data for supervised learning contain labeled in-
stances translated from the other language. That 
is, the initial training data contain Lc and Lct for 
Chinese, or Le and Let for English. More impor-
tant, at each iteration not only the labeled in-
stances are added to the training data of its own 
language, but their translated instances are also 
added to the training data of the other language. 
AL-MO (Active Learning with monolingual 
instances): labeled and unlabeled data for active 
learning only contain monolingual instances. No 
translated instances are involved. That is, the 
data contain Lc and Uc for Chinese, or Le and Ue 
for English respectively. This is the normal ac-
tive learning method applied to a single language. 
AL-CR (Active Learning with cross-lingual 
instances): both the manually labeled instances 
and their translated ones are added to the respec-
tive training data. The initial training data con-
tain Lc and Lct for Chinese, or Le and Let for Eng-
lish. At each iteration, the n least confidently 
classified instances in Uc and Ue are labeled and 
added to the Chinese/English training data re-
spectively. Their translated instances in Uet and 
Uct are also added to the English/Chinese training 
data respectively. 
AL-BI (Active Learning with bilingual la-
beled and unlabeled instances): similar to AL-
CR with the exception that the unlabeled in-
stances are chosen not by uncertainty scores in 
one language, but by the joint uncertainty scores 
in two languages. (cf. ?4.4) 
Evaluation Metric 
Although learning curves are often used to evalu-
ate the performance for active learning, it is pref-
erable to quantitatively compare various active 
learning methods using a statistical metric defi-
ciency (Schein and Ungar, 2007) defined as: 
?
?
=
=
?
?= n
i in
n
i in
n
REFFREFF
ALFREFF
REFALdef
1
1
))()((
))()((
),(     (3) 
Where n is the number of iterations involved in 
active learning and Fi is the F1-score of relation 
classification at the ith iteration. REF is the base-
line active learning method and AL is an im-
proved variant of REF, such as AL-CR or AL-
BI. Essentially this deficiency metric measures 
the degree to which REF outperforms AL. Thus, 
smaller deficiency value (i.e. <1.0) indicates AL 
outperforms REF while a larger value (i.e. >1.0) 
indicates AL underperforms REF. 
5.2 Experimental Results and Analysis 
Comparison of overall deficiency 
Table 1 compares the deficiency scores of rela-
tion classification on the Chinese (ACE2005c) 
and English corpora (ACE2005e) for various 
learning methods, i.e., SL-CR, AL-MO, AL-CR 
and AL-BI. Particularly, SL-MO is used as the 
baseline system against which deficiency scores 
for other methods are computed. The batch size n 
is set to 100 and iterations stop after all the unla-
beled instances have run out of. Deficiency 
scores are averaged over 10 runs and the best 
ones are highlighted in bold font. Each run has a 
different test set and a different seed set. 
588
 
 (a) Chinese      (b) English 
Figure 5. Deficiency comparison for different batch sizes 
 
(a) Chinese      (b) English 
Figure 6. Learning curves for different methods 
 
The table shows that among the three active 
learning methods, bilingual active learning (AL-
BI) achieves the best performance for both Chi-
nese and English relation classification. This 
demonstrates that, bilingual active learning with 
jointly selecting the unlabeled instances can not 
only enhance relation classification for its own 
language, but also help relation classification for 
the other language due to the complementary 
nature of relation instances between Chinese and 
English. 
Corpora SL-CR AL-MO AL-CR AL-BI
ACE2005c 0.934 0.383 0.323 0.254
ACE2005e 0.779 0.405 0.298 0.160
Table 1. Deficiency comparison of different 
methods 
The table also shows the consistent utility of 
cross-lingual information for relation classifica-
tion for both languages. When cross-lingual in-
formation is augmented, SL-CR outperforms 
SL-MO and AL-CR outperforms AL-MO. 
Comparison of different batch sizes 
Figure 5(a) and 5(b) illustrate the deficiency 
scores for four learning methods (SL-CR, AL-
MO, AL-CR and AL-BI) against the SL-MO 
method with different batch sizes (n), where pre-
fixes ?C? and ?E? denote Chinese and English 
respectively. The horizontal axes denote the 
range of n (<=1000) while the vertical ones de-
note the deficiency scores. 
The figures show that the deficiency scores 
for three active learning methods run virtually 
parallel with each other while they increase mo-
notonously with the batch size n. This suggests 
that for both Chinese and English AL-BI consis-
tently performs best against other methods across 
a wide range of batch sizes, though the overall 
advantage of three active learning methods gen-
erally diminish. 
Comparison of learning curves 
In order to gain an intuition into how the per-
formance evolves when the labeled instances are 
added into the training data during iterations, we 
depict the learning curves for various learning 
methods on the Chinese and English corpora in 
Fig. 6(a) and 6(b) respectively. The horizontal 
axes denote learning iterations while the vertical 
ones denote F1-scores. For simplicity of illustra-
tion the F1-scores are collected from one of the 
10 runs. 
75
77
79
81
83
85
87
89
91
93
95
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48
C-SL-MO
C-SL-CR
C-AL-MO
C-AL-CR
C-AL-BI
75
77
79
81
83
85
87
89
91
93
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48
E-SL-MO
E-SL-CR
E-AL-MO
E-AL-CR
E-AL-BI
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
100 200 300 400 500 600 700 800 900 1000
C-SL-CR
C-AL-MO
C-AL-CR
C-AL-BI
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
100 200 300 400 500 600 700 800 900 1000
E-SL-CR
E-AL-MO
E-AL-CR
E-AL-BI
589
The figures clearly demonstrate the perform-
ance difference for both languages among five 
methods at the beginning of iterations while F1-
scores converge at the end of iterations. Particu-
larly at the very outset, AL-BI outperforms other 
methods, quickly jumps to a very high point 
comparable to its best performance. However, 
after the 10th iteration the performance scores for 
the three AL variants tend to show trivial differ-
ence probably because most highly informative 
instances have already been added to the training 
data. 
Comparison of annotation scale 
In order to better compare BAL with other AL 
methods Figure 7 zooms out partial data on three 
AL methods in Fig. 6 and rescale the data for 
AL-MO, where ?C? and ?E? denote Chinese and 
English respectively. Likewise, the vertical axis 
denotes F1-scores while the horizontal axis de-
notes the number of instances labeled for AL-
CR and AL-BI. However, for AL-MO that num-
ber is doubled. This figure tries to answer the 
question: to label n respective instances in both 
languages for BAL or to labeled 2n instances in 
just one language for monolingual AL, can the 
former rival the latter? 
80
82
84
86
88
90
92
94
100 200 300 400 500 600 700 800 900 1000
C-AL-MO (2n)
C-AL-CR
C-AL-BI
E-AL-MO (2n)
E-AL-CR
E-AL-BI
 
Figure 7. Comparison of annotation scale among 
three AL methods 
The figure shows that for both Chinese and 
English, when the number of instances (n) to be 
labeled is no greater than 400, AL-BI with n in-
stances can achieve comparable performance 
with AL-MO with 2n instances. It implies that 
when the labeled instances are limited, labeling 
instances, half in one language and half in the 
other for BAL, is competitive against labeling 
the same total number of instances in just one 
language for monolingual AL, not to mention 
that the former can generate two relation extrac-
tors on two languages. 
6 Conclusion 
This paper proposes a bilingual active learning 
paradigm for Chinese and English relation classi-
fication. Given a small number of relation in-
stances and a large number of unlabeled relation 
instances in both languages, we translate both the 
labeled and unlabeled instances in one language 
to the other as pseudo parallel corpora. After en-
tity alignment, these labeled and unlabeled in-
stances in both languages are fed into a bilingual 
active learning engine. Experiments with the task 
of relation classification on the ACE RDC 2005 
Chinese and English corpora show that bilingual 
active learning can significantly outperforms 
monolingual active learning for both Chinese and 
English simultaneously. Moreover, we demon-
strate that BAL across two languages can com-
pete against monolingual AL when the annota-
tion scale is limited, though the overall number 
of labeled instances remains the same. 
For future work, on one hand, we plan to 
combine uncertainty sampling with diversity and 
informativeness measures; on the other hand, we 
intend to combine BAL with semi-supervised 
learning to further reduce human annotation ef-
forts. 
Acknowledgments 
This research is supported by Grants 61373096, 
61305088, 61273320, and 61331011 under the 
National Natural Science Foundation of China; 
Project 2012AA011102 under the ?863? Na-
tional High-Tech Research and Development of 
China; Grant 11KJA520003 under the Education 
Bureau of Jiangsu, China. We would like to 
thank the excellent and insightful comments 
from the three anonymous reviewers. Thanks 
also go to my colleague Dr. Shoushan Li for his 
helpful suggestions. 
Reference 
ACE. 2002-2007. Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/ 
A. Brew, D. Greene, and P. Cunningham. 2010. Using 
crowdsourcing and active learning to track senti-
ment in online media. ECAI?2010: 145?150. 
Y.S. Chan and D. Roth. 2011. Exploiting Syntactico-
Semantic Structures for Relation Extraction. 
ACL?2011: 551-560 
Y.S. Chan and H.T. Ng. 2007. Domain adaptation 
with active learning for word sense disambiguation. 
ACL?2007. 
590
C.C. Chang and C.J. Lin. 2011. LIBSVM: a library 
for support vector machines. ACM Transactions on 
Intelligent Systems and Technology, 2(27):1-27. 
W.X. Che, T. Liu, and S. Li. 2005. Automatic Extrac-
tion of Entity Relation (in Chinese). Journal of 
Chinese Information Processing, 19(2): 1-6. 
J.X. Chen, D.H. Ji, and C. L. Tan. 2006. Relation Ex-
traction using Label Propagation-based Semi-
supervised Learning. ACL/COLING?2006: 129-136. 
A. Culotta and J. Sorensen. 2004. Dependency tree 
kernels for relation extraction. ACL?2004: 423-439.  
A. Culotta and A. McCallum. 2005. Reducing label-
ing effort for stuctured prediction tasks. AAAI?2005: 
746?751. 
S. P. Engelson and I. Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. ACL?1996: 319?326. 
G. Haffari, M. Roy, and A. Sarkar. 2009. Active 
learning for statistical phrase-based machine trans-
lation. NAACL?2009: 415?423. 
G. Haffari and A. Sarkar. 2009. Active learning for 
multilingual statistical machine translation. 
ACL/IJCNLP?2009: 181?189. 
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Dis-
covering Relations among Named Entities from 
Large Corpora. ACL?2004. 
Y.N. Hu, J.G. Shu, L.H. Qian, and Q.M. Qiao. 2013. 
Cross-lingual Relation Extraction based on Ma-
chine Translation (in Chinese). Journal of Chinese 
Information Processing, 27(5): 191-197. 
R. Hwa. 2004. Sample selection for statistical parsing. 
Computational Linguistics, 30(3): 253?276. 
S. Kim, M. Jeong, J. Lee, and G.G. Lee. 2010. A 
Cross-lingual Annotation Projection Approach for 
Relation Detection. COLING?2010: 564-571. 
S. Kim and G.G. Lee. 2012. A Graph-based Cross-
lingual Projection Approach for Weakly Super-
vised Relation Extraction. ACL?2012: 48-53. 
S. Kim, Y. Song, K. Kim, J.W. Cha, and G.G. Lee. 
2006. MMR-based active machine learning for bio 
named entity recognition. HLT-NAACL?2006: 69?
72. 
W.J. Li, P. Zhang, F.R. Wei, Y.X. Hou, and Q. Lu. 
2008. A Novel Feature-based Approach to Chinese 
Entity Relation Extraction. ACL?2008: 89-92. 
S.S. Li, S.F. Ju, G.D. Zhou, and X.J. Li. 2012. Active 
learning for imbalanced sentiment classifica-
tion. EMNLP-CoNLL?2012: 139-148. 
B. Lu, C.H. Tan, C. Cardie, and B.K. Tsou. 2011. 
Joint Bilingual Sentiment Classification with 
Unlabeled Parallel Corpora. ACL?2011: 320-330. 
J. Oh, K. Uchimoto, and K. Torisawa. 2009.  Bilin-
gual Co-Training for Monolingual Hyponymy-
Relation Acquisition. ACL?2009: 432-440. 
M. Osborne and J. Baldridge. 2004. Ensemble based 
active learning for parse selection. HLT-NAACL? 
2004: 89?96. 
L.H. Qian, G.D. Zhou, F. Kong, and Q.M. Zhu. 2010. 
Clustering-based Stratified Seed Sampling for 
Semi-Supervised Relation Classification. 
EMNLP2010: 346-355. 
L.H. Qian, G.D. Zhou, Q.M. Zhu, and P.D. Qian. 
2008. Exploiting constituent dependencies for tree 
kernel-based semantic relation extraction. COL-
ING?2008: 697-704.  
R. Reichart, K. Tomanek, U. Hahn, and A. Rappoport. 
2008. Multi-task active learning for linguistic an-
notations. ACL?2008: 861-869. 
E. Ringger, P. McClanahan, R. Haertel, G. Busby, M. 
Carmen, J. Carroll, K. Seppi, and D. Lonsdale. 
2007. Active learning for part-of-speech tagging: 
Accelerating corpus annotation. In Proceedings of 
the Linguistic Annotation Workshop at ACL?2007: 
101?108. 
T. Scheffer, C. Decomain, and S. Wrobel. 2001. Ac-
tive hidden Markov models for information extrac-
tion. In Proceedings of the International Confer-
ence on Advances in Intelligent Data Analysis 
(CAIDA), pages 309?318. 
A. I. Schein and L. H. Ungar. 2007. Active learning 
for logistic regression: an evaluation. Machine 
Learning, 68(3): 235-265. 
P. Sebastian and M. Lapata. 2009. Cross-lingual an-
notation projection of semantic roles. Journal of 
Artificial Intelligence Research, 36(1): 307-340. 
B. Settles and M. Craven. 2008. An Analysis of Ac-
tive Learning Strategies for Sequence Labeling 
Tasks. EMNLP?2008: 1070?1079. 
D. Shen, J. Zhang, J. Su, G.D. Zhou and C.-L. Tan. 
2004. Multi-criteria-based active learning for 
named entity recognition. ACL?2004. 
K. Tomanek and U. Hahn. 2009. Semi-Supervised 
Active Learning for Sequence Labeling. ACL-
IJCNLP?2009: 1039-1047. 
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts an-
notation costs and maintains reusability of anno-
tated data. EMNLP-CoNLL?2007: 486?495. 
X.J. Wan. 2009. Co-Training for Cross-Lingual Sen-
timent Classification. ACL-AFNLP?2009: 235-243. 
D. Yarowsky and G. Ngai. 2001. Inducing multilin-
gual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. NAACL?2001: 1-8. 
591
D. Yarowsky, G. Ngai, and R. Wicentorski. 2001. 
Inducing multilingual text analysis tools via robust 
projection across aligned corpora. HLT?2001:1-8. 
H.H. Yu, L.H. Qian, G.D. Zhou, and Q.M. Zhu. 2010. 
Chinese Semantic Relation Extraction based on 
Unified Syntactic and Entity Semantic Tree (in 
Chinese). Journal of Chinese Information Process-
ing, 24(5): 17-23. 
D. Zelenko, C. Aone, and A. Richardella. 2003. Ker-
nel Methods for Relation Extraction. Journal of 
Machine Learning Research, 3: 1083-1106. 
Z. Zhang. 2004. Weakly-supervised relation classifi-
cation for Information Extraction. CIKM?2004. 
M. Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L. 
Tan. 2005. Discovering Relations between Named 
Entities from a Large Raw Corpus Using Tree 
Similarity-Based Clustering. IJCNLP?2005: 378-
389.  
M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A 
Composite Kernel to Extract Relations between 
Entities with both Flat and Structured Features. 
ACL/COLING?2006: 825-832.  
S.B. Zhao and R. Grishman. 2005. Extracting rela-
tions with integrated information using kernel 
methods.  ACL?2005: 419-426. 
G.D. Zhou, J.H. Li, L.H. Qian, and Q.M. Zhu. 2008. 
Semi-Supervised Learning for Relation Extraction. 
IJCNLP?2008: 32-38. 
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. Ex-
ploring various knowledge in relation extraction. 
ACL?2005: 427-434.  
J.B. Zhu and E. Hovy. 2007. Active learning for word 
sense disambiguation with methods for addressing 
the class imbalance problem. EMNLP-
CoNLL?2007: 783-790. 
592
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 842?847,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Bilingual Event Extraction: a Case Study on Trigger Type Determina-
tion 
Zhu Zhu?  Shoushan Li?*  Guodong Zhou?  Rui Xia? 
 
?Natural Language Processing Lab 
Soochow University, China 
{zhuzhu0020, 
shoushan.li}@gmail.com, 
gdzhou@suda.edu.cn 
 
?Department of Computer Science 
 Nanjing University of Science and 
Technology 
rxia@njust.edu.cn 
 
 
Abstract 
Event extraction generally suffers from the 
data sparseness problem. In this paper, we 
address this problem by utilizing the labeled 
data from two different languages. As a pre-
liminary study, we mainly focus on the sub-
task of trigger type determination in event 
extraction. To make the training data in dif-
ferent languages help each other, we pro-
pose a uniform text representation with bi-
lingual features to represent the samples and 
handle the difficulty of locating the triggers 
in the translated text from both monolingual 
and bilingual perspectives. Empirical studies 
demonstrate the effectiveness of the pro-
posed approach to bilingual classification on 
trigger type determination. ? 
1 Introduction 
Event extraction is an increasingly hot and chal-
lenging research topic in the natural language 
processing (NLP) community (Ahn, 2006; Saun 
et al 2006; Zhao et al 2008). It aims to automat-
ically extract certain types of events with the ar-
guments to present the texts under a structured 
form. In event extraction, there are four primary 
subtasks, named trigger identification, trigger 
type determination, argument identification, and 
argument role determination (Chen and NG, 
2012). As an important technology in infor-
mation extraction, event extraction could be ap-
plied to many fields such as information retrieval, 
summarization, text mining, and question an-
swering. 
Recently, the dominative approach to event 
extraction is based on supervised learning where 
a set of labeled samples are exploited to train a 
model to extract the events. However, the availa-
                                                 
? *  Corresponding author 
ble labeled data are rather sparse due to various 
kinds of event categories. For example, the event 
taxonomy in ACE 2005 1  (Automatic Content 
Extraction) includes 8 types of events, with 33 
subtypes, such as ?Marry/Life? (subtype/type), 
and ?Transport/Movement?. Moreover, some 
subtypes such as ?Nominate/Personnel? and 
?Convict/Justice? contain less than 10 labeled 
samples in the English and Chinese corpus re-
spectively. Apparently, such a small scale of 
training data is difficult to yield a satisfying per-
formance. 
One possible way to alleviate the data sparse-
ness problem in event extraction is to conduct 
bilingual event extraction with training data from 
two different languages. This is motivated by the 
fact that labeled data from a language is highly 
possible to convey similar information in another 
language. For example, E1 is an event sample 
from the English corpus and E2 is another one in 
the Chinese corpus. Apparently, E1 and the Eng-
lish translation text of E2, share some important 
clues such as meet and Iraq which highly indi-
cates the event type of ?Meet/Contact?.  
 
E1: Bush arrived in Saint Petersburg on Sat-
urday, when he also briefly met German chancel-
lor Gerhard Schroeder, whose opposition to the 
Iraq war had soured his relationship with Wash-
ington, at a dinner hosted by Putin. 
E2: ?????????????????
???? ???????????????
????(U.S. president George W. Bush   will 
visit Germany in February and meet with   
Schroeder, Iran and Iraq will be the focus of the   
talks the two sides.) 
 
In this paper, we address the data sparseness 
problem in event extraction with a bilingual pro-
                                                 
1http://www.nist.gov/speech/tests/ace/2005 
842
cessing approach which aims to exploit bilingual 
training data to enhance the extraction perfor-
mance in each language. As a preliminary work, 
we mainly focus on the subtask of trigger type 
determination. Accordingly, our goal is to design 
a classifier which is trained with labeled data 
from two different languages and is capable of 
classifying the test data from both languages. 
Generally, this task possesses two main chal-
lenges.  
The first challenge is text representation, 
namely, how to eliminate the language gap be-
tween the two languages. To tackle this, we first 
employ Google Translate2, a state-of-the-art ma-
chine translation system, to gain the translation 
of an event instance, similar to what has been 
widely done by previous studies in bilingual 
classification tasks e.g., Wan (2008); Then, we 
uniformly represent each text with bilingual 
word features. That is, we augment each original 
feature vector into a novel one which contains 
the translated features.  
The second challenge is the translation for 
some specific features. It is well-known that 
some specific features, such as the triggers and 
their context features, are extremely important 
for determining the event types. For example, in 
E3, both trigger ?left? and named entity ?Sad-
dam? are important features to tell the event type, 
i.e., "Transport/Movement". When it is translated 
to Chinese, it is also required to know trigger ??
??(left) and named entity ????? (Saddam) 
in E4, the Chinese translation of E3.  
 
E3: Saddam's clan is said to have left for a 
small village in the desert. 
E4: Chinese translation: ? ?  ? ? ?
(Saddam) ?? ?? ??(left) ?? ? ? ?? 
? ??? 
 
However, it is normally difficult to know 
which words are the triggers and surrounding 
entities in the translated sentence. To tackle this 
issue, we propose to locate the trigger from both 
monolingual and bilingual perspectives in the 
translation text. Empirical studies demonstrate 
that adding the translation of these specific fea-
tures substantially improves the classification 
performance.  
The remainder of this paper is organized as 
follows. Section 2 overviews the related work on 
event extraction. Section 3 proposes our ap-
                                                 
2 www.google.com 
proach to bilingual event extraction. Section 4 
gives the experimental studies. In Section 5, we 
conclude our work and give some future work. 
2 Related Work  
In the NLP community, event extraction has 
been mainly studied in both English and Chinese. 
In English, various supervised learning ap-
proaches have been explored recently. Bethard 
and Martin (2006) formulate the event identifica-
tion as a classification problem in a word-
chunking paradigm, introducing a variety of lin-
guistically motivated features. Ahn (2006) pro-
poses a trigger-based method. It first identifies 
the trigger in an event, and then uses a multi-
classifier to implement trigger type determina-
tion. Ji and Grishman (2008) employ an ap-
proach to propagate consistent event arguments 
across sentences and documents. Liao and 
Grishman (2010) apply document level infor-
mation to improve the performance of event ex-
traction. Hong et al (2011) leverage cross-entity 
information to improve traditional event extrac-
tion, regarding entity type consistency as a key 
feature. More recently, Li et al (2013) propose a 
joint framework based on structured prediction 
which extracts triggers and arguments together. 
In Chinese, relevant studies in event extraction 
are in a relatively primary stage with focus on 
more special characteristics and challenges. Tan 
et al (2008) employ local feature selection and 
explicit discrimination of positive and negative 
features to ensure the performance of trigger type 
determination. Chen and Ji (2009) apply lexical, 
syntactic and semantic features in trigger label-
ing and argument labeling to improve the per-
formance. More recently, Li et al (2012) and Li 
et al (2013) introduce two inference mechanisms 
to infer unknown triggers and recover trigger 
mentions respectively with morphological struc-
tures.  
In comparison with above studies, we focus on 
bilingual event extraction. Although bilingual 
classification has been paid lots of attention in 
other fields (Wan 2008; Haghighi et al, 2008; 
Ismail et al, 2010; Lu et al, 2011?Li et al, 
2013), there is few related work in event extrac-
tion. The only one related work we find is Ji 
(2009) which proposes an inductive learning ap-
proach to exploit cross-lingual predicate clusters 
to improve the event extraction task with the 
main goal to get the event taggers from extra re-
sources, i.e., an English and Chinese parallel 
corpus. Differently, our goal is to make the la-
843
beled data from two languages help each other 
without any other extra resources, which is origi-
nal in the study of event extraction. 
3 The Proposed Approach 
Trigger type determination aims to determine the 
event type of a trigger given the trigger and its 
context (e.g., a sentence). Existing approaches to 
trigger type determination mainly focus on mon-
olingual classification. Figure 1 illustrates the 
framework for Chinese and English. 
In comparison, our approach exploits the cor-
pora from two different languages. Figure 2 illus-
trates the framework. As shown in the figure, we 
first get the translated corpora of Chinese and 
English origin corpora through machine transla-
tion. Then, we represent each text with bilingual 
features, which enables us to merge the training 
data from both languages so as to make them 
help each other. 
 
Figure 1: The framework of monolingual classifi-
cation for trigger type determination 
 
Figure 2: The framework of bilingual classification 
for trigger type determination 
3.1 Text Representation  
In a supervised learning approach, labeled data is 
trained to obtain a classifier. In this approach, the 
extracted features are the key components to 
make a successful classifier. Table 1 shows some 
typical kinds of features in a monolingual classi-
fication task for trigger type determination. To 
better understand these features, the real feature 
examples in E3 are given in the table. 
Given the feature definition, a monolingual 
sample x  is represented as the combination of all 
the features, i.e.,  
1 2, , , , _ , _ ,
_ , , _ , _
ne e e Tri POS Tri Tri conx POS con Ent Ent type Ent subtype
? ?? ? ?? ?
  (1) 
Features Feature examples in E3 
All words 
(
1 2, , ne e e ) 
Saddam, clan, is, ... , 
desert 
Trigger (Tri) left 
POS of the trigger 
(POS_Tri) 
VBN 
Trigger's context 
words (Tri_con) 
...,have, for,... 
POS of trigger's 
context words 
(POS_con) 
...,VB,IN,? 
Entities around trig-
ger (Ent) 
Saddam 
Entity type 
(Ent_type) 
PER 
Entity subtype 
(Ent_subtype) 
individual 
Table 1: The features and some feature examples for 
trigger type determination 
 
In bilingual classification, we represent a sam-
ple with bilingual features, which makes it possi-
ble to train with the data from two languages. To 
achieve this goal, we employ a single feature 
augmentation strategy to augment the monolin-
gual features into bilingual features, i.e.,  
,Chinese Englishx x x?
                      (2) 
Specifically, a sample x  is represented as fol-
lows: 
1 2
1 2
, , , , _ , _ ,
_ , , _ , _
, , , , , _ , _ ,
_ , , _ , _
m c c c
c c c
n e e e
e e e
c c c Tri POS Tri Tri con
POS con Ent Ent type Ent subtype
x
e e e Tri POS Tri Tri con
POS con Ent Ent type Ent subtype
? ?? ?
? ?? ?
? ?? ?? ? ?? ?? ?? ?? ?? ?? ?
  (3) 
Where the tokens with the ?c?/?e? subscript mean 
the features generated from the Chinese/English 
text. From the features, we can see that some 
Classifier Results 
Chinese event 
corpus 
Machine trans-
lation 
Translated 
samples 
Text representation 
Translated 
samples 
English event 
corpus 
Machine trans-
lation 
Text representation 
Samples with 
bilingual features 
Samples with 
bilingual features 
Trigger type determination 
for Chinese 
Trigger type determination 
for English 
Chinese event 
corpus 
Classifier 
English 
event corpus 
Classifier 
Results Results 
844
features, such as Tri_con and Ent, depend on the 
location of the trigger word. Therefore, locating 
the trigger in the translated text becomes crucial.  
3.2 Locating Translated Trigger 
Without loss of generality, we consider the case 
of translating a Chinese event sample into an 
English one. Formally, the word sequence of a 
Chinese event sample is denoted as 
1 2( , , , )c ns c c c? , while the sequence of the 
translated one is denoted as
1 2( , , )e ms e e e? . 
Then, the objective is to get the English trigger 
eTri  in es , given the Chinese trigger word  
cTri in cs . The objective function is given as fol-
lows:  
? ?_1 ,argmax k l ek l m P e Tri? ? ?
                 (4) 
Where 
_k le
 denotes the substring 
1( , , )k k le e e?  
in 
es  and 1 ,k l m? ? . 
In this paper, the above function could be 
solved in two perspectives: monolingual and bi-
lingual ones. The former uses the English train-
ing data alone to locate the trigger while the lat-
ter exploit the bilingual information to get the 
translated counterpart of the Chinese trigger. 
The monolingual perspective: The objective 
is to locate the trigger with the monolingual in-
formation. That is,  
? ?_1 ,argmax | ,k l e e ek l m P e Tri s R? ? ?
           (5) 
Where 
eR  denotes the training resource in Eng-
lish. In fact, this task is exactly the first subtask 
in event extraction named trigger identification, 
as mentioned in Introduction. For a simplified 
implementation, we first estimate the probabili-
ties of ? ?_k l eP e Tri?  in eR  with maximum like-
lihood estimation when 
_k l ee s?
.  
The bilingual perspective: The objective is to 
locate the trigger with the bilingual information. 
That is, 
? ?_1 ,argmax | , ,k l e e c ck l m P e Tri s s Tri? ? ?
        (6) 
Where 
cTri  is the trigger word in Chinese and es  
is the translated text towards 
cs . More generally, 
this can be solved from a standard word align-
ment model in machine translation (Och et al 
1999; Koehn et al 2003). However, training a 
word alignment requires a huge parallel corpus 
which is not available here.  
 For a simplified implementation, we first get 
the 
cTri ?s translation? denoted as cTritrans
?
with Google Translate. Then, we estimate 
? ?_k l eP e Tri?  as follows:  
? ? __ 0.9 ck l Trik l e if e transP e Tri others?
???? ? ???
    (7) 
Where 0.9 is an empirical value which makes the 
translation probability become a dominative fac-
tor when the translation of the trigger is found in 
the translated sentence. ?  is a small value which 
makes the sum of all probabilities equals 1.   
The final decision is made according to both 
the monolingual and bilingual perspectives, i.e., 
? ?
? ?
_
1 ,
_
arg max  | ,
              | , ,
k l e e e
k l m
k l e e c c
P e Tri s R
P e Tri s s Tri
? ?
?
? ?
        (8) 
Note that we reduce the computational cost by 
make the word length of the trigger less than 3, 
i.e., 3l k? ? . 
4 Experimentation 
4.1 Experimental Setting  
Data sets: The Chinese and English corpus for 
even extraction are from ACE2005, which in-
volves 8 types and 33 subtypes. All our experi-
ments are conducted on the subtype case. Due to 
the space limit, we only report the statistics for 
each type, as shown in Table 2. For each subtype, 
80% samples are used as training data while the 
rest are as test data. 
 
# Chinese English total 
Life 389 902 1291 
Movement 593 679 1272 
Transaction 147 379 526 
Business 144 137 281 
Conflict 514 1629 2143 
Contact 263 373 636 
Personnel 203 514 717 
Justice 457 672 1129 
total 2710 5285 7995 
Table 2: Statistics in each event type in both Chinese 
and English data sets 
 
Features: The features have been illustrated in 
Table 1 in Section 3.2.  
845
Classification algorithm: The maximum en-
tropy (ME) classifier is implemented with the 
public tool, Mallet Toolkits3 . 
Evaluation metric: The performance of event 
type recognition is evaluated with F-score. 
4.2 Experimental Results  
In this section, we evaluate the performance of 
our approach to bilingual classification on trigger 
type determination. For comparison, following 
approaches are implemented: 
? Monolingual: perform monolingual classi-
fication on the Chinese and English corpus 
individually, as shown in Figure 1. 
? Bilingual: perform bilingual classification 
with partial bilingual features, ignoring the 
context features (e.g., context words, con-
text entities) under the assumption that the 
trigger location task is not done. 
? Bilingual_location: perform bilingual clas-
sification by translating each sample into 
another language and using a uniform repre-
sentation with all bilingual features as 
shown in Section 3.2. This is exactly our 
approach. The number of the context words 
and entities before or after the trigger words 
is set as 3. 
0.658
0.706
0.677
0.679
0.678
0.734
0.62
0.64
0.66
0.68
0.7
0.72
0.74
Chinese Test Data English Test Data
F
-
s
c
o
r
e
Monolingual Bilingual Bilingual_location
 
Figure 3: Performance comparison of the three ap-
proaches on the Chinese and English test data 
 
Figure 3 shows the classification results of the 
three approaches on the Chinese and English test 
data. From this figure, we can see that Bilin-
gual_location apparently outperform Monolin-
gual, which verifies the effectiveness of using 
bilingual corpus. Specifically, the improvement 
by our approach in Chinese is impressive, reach-
ing 7.6%. The results also demonstrate the im-
portance of the operation of the trigger location, 
                                                 
3 http://mallet.cs.umass.edu/   
without which, bilingual classification can only 
slightly improve the performance, as shown in 
the English test data.  
The results demonstrate that our bilingual 
classification approaches are more effective for 
the Chinese data. This is understandable because 
the size of English data is much larger than that 
of Chinese data, 5285 vs. 2710, as shown in Ta-
ble 2. Specifically, after checking the results in 
each subtype, we find that some subtypes in Chi-
nese have very few samples while corresponding 
subtypes in English have a certain number sam-
ples. For example, the subtype of 
?Elect/Personnel? only contains 30 samples in 
the Chinese data while 161 samples can be found 
in the English data, which leads a very high im-
provement (15.4%) for the Chinese test data. In 
summary, our bilingual classification approach 
provides an effective way to handle the data 
sparseness problem in even extraction. 
5 Conclusion and Future Work 
This paper addresses the data sparseness problem 
in event extraction by proposing a bilingual clas-
sification approach. In this approach, we use a 
uniform text representation with bilingual fea-
tures and merge the training samples from both 
languages to enlarge the size of the labeled data. 
Furthermore, we handle the difficulty of locating 
the trigger from both the monolingual and bilin-
gual perspectives. Empirical studies show that 
our approach is effective in using bilingual cor-
pus to improve monolingual classification in 
trigger type determination.  
Bilingual event extraction is still in its early 
stage and many related research issues need to be 
investigated in the future work. For example, it is 
required to propose novel approaches to the bi-
lingual processing tasks in other subtasks of 
event extraction. Moreover, it is rather challeng-
ing to consider a whole bilingual processing 
framework when all these subtasks are involved 
together.  
Acknowledgments 
This research work has been partially supported 
by two NSFC grants, No.61375073, and 
No.61273320, one National High-tech Research 
and Development Program of China 
No.2012AA011102, one General Research Fund 
(GRF) project No.543810 and one Early Career 
Scheme (ECS) project No.559313 sponsored by 
the Research Grants Council of Hong Kong, the 
NSF grant of Zhejiang Province No.Z1110551. 
846
References  
Ahn D. 2006. The Stages of Event Extraction. In Pro-
ceedings of the Workshop on Annotating and Rea-
soning about Time and Events, pp.1~8. 
Bethard S. and J. Martin. 2006. Identification of 
Event Mentions and Their Semantic Class. In Pro-
ceedings of EMNLP-2006, pp.146-154. 
Chen C. and V. NG. 2012. Joint Modeling for Chi-
nese Event Extraction with Rich Linguistic Fea-
tures. In Proceedings of COLING-2012, pp. 529-
544. 
Chen Z. and H. Ji. 2009. Language Specific Issue and 
Feature Exploration in Chinese Event Extraction. 
In Proceedings of NAACL-2009, pp. 209-212. 
Haghighi A., P. Liang, T. Berg-Kirkpatrick and D. 
Klein. 2008. Learning Bilingual Lexicons from 
Monolingual Corpora. In Proceedings of ACL-
2008, pp. 771-779. 
Hong Y., J. Zhang., B. Ma., J. Yao., and G. Zhou. 
2011. Using Cross-Entity Inference to Improve 
Event Extraction. In Proceedings of ACL-2011, pp. 
1127?1136. 
Ismail A., and S. Manandhar. 2010. Bilingual Lexicon 
Extraction from Comparable Corpora Using In-
domain Terms. In Proceedings of COLING-2010, 
pp.481-489. 
Ji H. 2009. Cross-lingual Predicate Cluster Acquisi-
tion to Improve Bilingual Event Extraction by In-
ductive Learning. In Proceedings of the Workshop 
on Unsupervised and Minimally Supervised Learn-
ing of Lexical Semantics, pp. 27-35. 
Ji H, and R. Grishman. 2008. Refining Event Extrac-
tion through Cross-Document Inference. In Pro-
ceedings of ACL-2008, pp. 254-262. 
Koehn P., F. Och, and D. Marcu. 2003. Statistical 
Phrase-based Translation. In Proceedings of HTL-
NAACL-2003, pp. 127-133. 
Li P., and G. Zhou. 2012. Employing Morphological 
Structures and Sememes for Chinese Event Extrac-
tion. In Proceedings of COLING-2012, pp. 1619-
1634. 
Li P., Q. Zhu and G. Zhou. 2013. Using Composition-
al Semantics and Discourse Consistency to Im-
prove Chinese Trigger Identification. In Proceed-
ings of COLING-2013, pp. 399-415. 
Li Q, H Ji, and H. Liang. 2013. Joint Event Extraction 
via Structured Prediction with Global Features. In 
Proceedings of ACL-2013, pp. 73-82. 
Li S, R Wang, H Liu, and CR Huang. 2013. Active 
Learning for Cross-Lingual Sentiment Classifica-
tion. In Proceedings of Natural Language Pro-
cessing and Chinese Computing, pp. 236-246. 
Liao S and R. Grishman. 2010. Using Document Lev-
el Cross-event Inference to Improve Event Extrac-
tion. In Proceedings of ACL-2010, pp. 789-797. 
Lu B., C. Tan, C. Cardie and B. K. Tsou. 2011. Joint 
Bilingual Sentiment Classification with Unlabeled 
Parallel Corpora. In Proceedings of ACL-2011, pp. 
320-330.  
Och F., C. Tillmann, and H. Ney. 1999. Improved 
Alignment Models for Statistical Machine Transla-
tion. In Proceedings of EMNLP-1999, pp.20-28. 
Tan H., T. Zhao, and J. Zheng. 2008. Identification of 
Chinese Event and Their Argument Roles. In Pro-
ceedings of  CITWORKSHOPS-2008,  pp. 14-19. 
Wan X. 2008. Using Bilingual Knowledge and En-
semble Techniques for Unsupervised Chinese Sen-
timent Analysis. In  Proceedings of EMNLP-2008, 
pp. 553-561. 
Zhao Y., Y. Wang, B. Qin, et al 2008. Research on 
Chinese Event Extraction. In Proceedings of Jour-
nal of  Chinese Information, 22(01), pp. 3-8. 
847
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 253?257
Manchester, August 2008
Dependency Tree-based SRL with Proper Pruning and Extensive 
Feature Engineering 
Hongling Wang    Honglin Wang   Guodong Zhou   Qiaoming Zhu 
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology, 
Soochow University, Suzhou, China 215006 
{redleaf, 064227065055,gdzhou, qmzhu}@suda.edu.cn 
 
 
 
Abstract 
This paper proposes a dependency tree-
based SRL system with proper pruning and 
extensive feature engineering. Official 
evaluation on the CoNLL 2008 shared task 
shows that our system achieves 76.19 in la-
beled macro F1 for the overall task, 84.56 
in labeled attachment score for syntactic 
dependencies, and 67.12 in labeled F1 for 
semantic dependencies on combined test 
set, using the standalone MaltParser. Be-
sides, this paper also presents our unofficial 
system by 1) applying a new effective 
pruning algorithm; 2) including additional 
features; and 3) adopting a better depend-
ency parser, MSTParser. Unofficial evalua-
tion on the shared task shows that our sys-
tem achieves 82.53 in labeled macro F1, 
86.39 in labeled attachment score, and 
78.64 in labeled F1, using MSTParser on 
combined test set. This suggests that proper 
pruning and extensive feature engineering 
contributes much in dependency tree-based 
SRL.  
1 Introduction 
Although CoNLL 2008 shared task mainly 
evaluates joint learning of syntactic and semantic 
parsing, we focus on dependency tree-based se-
mantic role labeling (SRL). SRL refers to label 
the semantic roles of predicates (either verbs or 
nouns) in a sentence. Most of previous SRL sys-
tems (Gildea and Jurafsky, 2002; Gildea and 
Palmer, 2002; Punyakanok et al, 2005; Pradhan 
                                                 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
 
et al, 2004, 2005) work on constituent structure 
trees and has shown to achieve remarkable re-
sults. For example, Punyakanok et al (2005) 
achieved the best performance in the CoNLL 
2005 shared task with 79.44 in F-measure on the 
WSJ test set and 77.92 on the combined test set 
(WSJ +Brown). 
With rapid development of dependency pars-
ing in the last few years, more and more re-
searchers turn to dependency tree-based SRL 
with hope to advance SRL from viewpoint of 
dependency parsing. Hacioglu (2004) pioneered 
this work by formulating SRL as a classification 
problem of mapping various dependency rela-
tions into semantic roles. Compared with previ-
ous researches on constituent structure tree-based 
SRL which adopts constituents as labeling units, 
dependency tree-based SRL adopts dependency 
relations as labeling units. Due to the difference 
between constituent structure trees and depend-
ency trees, their feature spaces are expected to be 
somewhat different. 
In the CoNLL 2008 shared task, we extend the 
framework by Hacioglu (2004) with maximum 
entropy as our classifier. For evaluation, we will 
mainly report our official SRL performance us-
ing MaltParser (Nivre and Nilsson, 2005). Be-
sides, we will also present our unofficial system 
by 1) applying a new effective pruning algorithm; 
2) including additional features; and 3) adopting 
a better dependency parser, MSTParser (McDon-
ald, 2005). 
In the remainder of this paper, we will briefly 
describe our system architecture, present various 
features used by our models and report the per-
formance on CoNLL 2008 shared task (both offi-
cial and unofficial). 
253
2 System Description 
In CoNLL 2008 shared task, we adopt a standard 
three-stage process for SRL: pruning, argument 
identification and argument classification. To 
model the difference between verb and noun 
predicates, we carry out separate training and 
testing for verb and noun predicates respectively. 
In addition, we adopt OpenNLP maximum en-
tropy package 1  in argument identification and 
classification. 
2.1 Predicate identification 
Most of Previous SRL systems only consider 
given predicates. However, predicates are not 
given in CoNLL 2008 shared task and required 
to be determined automatically by the system. 
Therefore, the first step of the shared task is to 
identify the verb and noun predicates in a sen-
tence. Due to time limitation, a simple algorithm 
is developed to identify noun and verb predicates: 
1) For the WSJ corpus, we simply adopt the 
annotations provided by PropBank and 
NomBank. That is, we only consider the verb 
and noun predicates annotated in PropBank 
and NomBank respectively.  
2) For the Brown corpus, verb predicates are 
identified simply according to its POS tag 
and noun predicates are determined using a 
simple method that only those nouns which 
can also be used as verbs are identified. To 
achieve this goal, an English lexicon of about 
56K word is applied to identify noun predi-
cates.  
Evaluation on the test set of CoNLL 2008 
shared task shows that our simple predicate iden-
tification algorithm achieves the accuracies of 
98.6% and 92.7 in the WSJ corpus for verb and 
noun predicates respectively, with overall accu-
racy of 95.5%, while it achieves the accuracies of 
73.5% and 43.1% in the Brown corpus for verb 
and noun predicates respectively with overall 
accuracy of 61.8%. This means that the perform-
ance of predicate identification in the Brown 
corpus is much lower than the one in the WSJ 
corpus. This further suggests that much work is 
required to achieve reasonable predicate identifi-
cation performance in future work. 
2.2 Preprocessing 
Using the dependency relations returned by a 
dependency parser (either MaltParser or 
                                                 
1https://sourceforge.net/project/showfiles.php?group_id=59
61 
MSTParser in this paper), we can construct cor-
responding dependency tree for a given sentence. 
For example, Figure 1 shows the dependency 
tree of the sentence ?Meanwhile, overall evi-
dence on the economy remains fairly clouded.?. 
Here, W is composed of two parts: word and its 
POS tag with ?/? as a separator while R means a 
dependency relation and ARG represents a se-
mantic role. 
In Hacioglu (2004), a simple pruning algo-
rithm is applied to filter out unlikely dependency 
relation nodes in a dependency tree by only 
keeping the parent/children/grand-children of the 
predicate, the siblings of the predicates, and the 
children/grandchildren of the siblings. This paper 
extends the algorithm a little bit by including the 
nodes two more layers upward and downward 
with regard to the predicate?s parent, such as the 
predicate?s grandparent, the grandparent?s chil-
dren and the grandchildren?s children. For the 
example as shown in Figure 1, all the nodes in 
the entire tree are kept. Evaluation on the training 
set shows that our pruning algorithm signifi-
cantly reduces the training instances by 76.9%. 
This is at expanse of wrongly pruning 1.0% se-
mantic arguments for verb predicates. However, 
this figure increases to 43.5% for noun predicates 
due to our later observation that about half of 
semantic arguments of noun predicates distrib-
utes over ancestor nodes out of our consideration. 
This suggests that a specific pruning algorithm is 
necessary for noun predicates to include more 
ancestor nodes. 
2.3 Features 
Some of the features are borrowed from Ha-
cioglu (2004) with some additional features mo-
tivated by constituent structure tree-based SRL 
(Pradhan et al2005; Xue and Palmer, 2004). In 
the following, we explain these features and give 
examples with regard to the dependency tree as 
shown in Figure 1. We take the word evidence in 
Figure 1 as the predicate and the node ?on? as 
the node on focus.  
The following eight basic features are moti-
vated from constituent structure tree-based SRL:  
1)  Predicate: predicate lemma. (evidence) 
2) Predicate POS: POS of current predicate. 
(NN) 
3)  Predicate Voice: Whether the predicate (verb) 
is realized as an active or passive construc-
tion. If the predicate is a noun, the value is 
null and presented as ?_?. ( _ ) 
 
254
 
Figure 1. Example of a dependency tree augmented with semantic roles  
for the given predicate evidence. 
 
4)  Relation type: the dependency relation type 
of the current node. (NMOD) 
5) Path: the chain of relations from current rela-
tion node to the predicate. (NMOD->SBJ) 
6) Sub-categorization: The relation type of 
predicate and the left-to-right chain of the re-
lation label sequence of the predicate?s chil-
dren. (SBJ->NMOD-NMOD) 
7)  Head word: the head word in the relation, 
that is, the headword of the parent of the cur-
rent node. (evidence) 
8)  Position: the position of the headword of the 
current node with respect to the predicate po-
sition in the sentence, which can be before, 
after or equal. (equal) 
Besides, we also include following additional 
features borrowed from Hacioglu (2004): 
1) Family membership: the relationship be-
tween current node and the predicate node in 
the family tree, such as parent, child, sibling. 
(child) 
2)  Dependent word: the modifying word in the 
relation, that is, the word of current node. (on) 
3) POS of headword: the POS tag of the head-
word of current word. (NN) 
4)  POS of dependent word: the POS tag of cur-
rent word. (IN) 
5)  POS pattern of predicate's children: the 
left-to-right chain of the POS tag sequence of 
the predicate?s children. (JJ-IN) 
6)  Relation pattern of predicate?s children: 
the left-to-right chain of the relation label se-
quence of the predicate?s children. (NMOD-
NMOD) 
7)  POS pattern of predicate?s siblings: the 
left-to-right chain of the POS tag sequence of 
the predicate?s siblings. (RB-.-VBN-.) 
8)  Relation pattern of predicate?s siblings: the 
left-to-right chain of the relation label se-
quence of the predicate?s siblings. (TMP-P-
PRD-P) 
3 System Performance 
All  the training data are included in our system, 
which costs 70 minutes in training and 5 seconds 
on testing on a PC platform with a Pentium D 
3.0G CPU and 2G Memory. In particular, the 
argument identification stage filters out those 
nodes whose probabilities of not being semantic 
arguments are more than 0.98 for verb and noun 
predicates. 
   Labeled 
Macro F1 
Labeled 
F1 
LAS 
Test WSJ 78.39 70.41 85.50
Test Brown 59.89 42.67 77.06
Test WSJ+Brown 76.19 67.12 84.56
Table 1: Official performance using MaltParser 
(with the SRL model trained and tested on the 
automatic output of MaltParser) 
 
All the performance is returned on the test set 
using the CoNLL 2008 evaluation script 
eval08.pl provided by the organizers. Table 1 
shows the official performance using MaltParser 
(with the SRL model trained and tested on the 
automatic output of MaltParser provided by the 
task organizers) as the dependency parser. It 
shows that our system performs well on the WSJ 
corpus and badly on the Brown corpus largely 
due to bad performance on predicate identifica-
tion.  
4 Post-evaluation System 
To gain more insights into dependency tree-
based SRL, we improve the system with a new 
255
pruning algorithm and additional features, after 
submitting our official results. 
4.1 Effective pruning 
Our new pruning algorithm is motivated by the 
one proposed by Xue and Palmer (2004), which  
only keeps those siblings to a node on the path 
from current predicate to the root are included, 
for constituent structure tree-based SRL. Our 
pruning algorithm further cuts off the nodes 
which are not related with the predicate. Besides, 
it filters out those nodes which are punctuations 
or with ?symbol? dependency relations. Evalua-
tion on the Brown corpus shows that our pruning 
algorithm significantly reduces the training data 
by 75.5% at the expense of wrongly filtering out 
0.7% and 0.5% semantic arguments for verb and 
noun predicates respectively. This suggests that 
our new pruning algorithm significantly performs 
better than the old one in our official system, es-
pecially for the identification of noun predicates. 
Furthermore, the argument identification stage 
filters out those nodes whose probabilities of not 
being semantic arguments are more than 0.90 
and 0.85 for verb and noun predicates respec-
tively, since we that our original threshold of 
0.98 in the official system is too reserved. 
Finally, those rarely-occurred semantic roles 
which occur less than 200 in the training set are 
filtered out and thus not considered in our system, 
such as A5, AA, C-A0, C-AM-ADV, R-A2 and SU. 
4.2 Extensive Feature Engineering 
Motivated by constituent structure tree-based 
SRL, two more combined features are considered 
in our post-evaluation system:  
1) Predicate + Headword: (evidence + remain) 
2) Headword + Relation: (remain + Root) 
In order to better evaluate the contribution of 
various additional feature, we build a baseline 
system using hand-corrected dependency rela-
tions and the eight basic features, motivated by 
constituent structure tree-based SRL, as de-
scribed in Section 2.3. Table 2 shows the effect 
of various additional features by adding one in-
dividually to the baseline system. It shows that 
the feature of dependent word is most useful, 
which improves the labeled F1 score from 
81.38% to 84.84%. It also shows that the two 
features about predicate?s sibling deteriorate the 
performance. Therefore, we delete these two fea-
tures from remaining experiments. Although the 
combined feature of ?predicate+head word? is 
useful in constituent structure tree-based SRL, it 
slightly decrease the performance in dependency 
tree-based SRL. For convenience, we include it 
in our system. 
 P R F1 
Baseline 84.31 78.64 81.38
+ Family membership 84.70 78.87 81.68
+ Dependent word  86.74 83.01 84.84
+ POS of headword 84.44 78.55 81.38
+ POS of dependent 
word 
84.42 78.33 81.47
+ POS pattern of 
predicate's children 
84.35 78.73 81.47
+ Relation pattern of 
predicate?s children 
84.75 78.97 81.76
+ Relation pattern of 
predicate?s siblings 
84.29 78.52 81.30
+ POS pattern of 
predicate?s siblings 
83.75 78.32 80.95
+ Predicate  +  Head-
word 
83.30 78.94 81.30
+Headword + Relation 84.66 79.37 81.93
Table 2: Effects of various additional features 
4.3 Best performance 
Table 3 shows our system performance after ap-
plying above effective pruning strategy and addi-
tional features using the default MaltParser. Ta-
ble 3 also reports our performance using the 
state-of-the-art MSTParser. To show the impact 
of predicate identification in dependency tree-
based SRL, Table 4 report the performance on 
gold predicate identification, i.e. only using an-
notated predicates in the corpora. 
Comparison of Table 1 and Table 3 using the 
MaltParser shows that our new extension with 
effective pruning and extensive engineering sig-
nificantly improves the performance. It also 
shows that MSTParser-based SRL performs 
slightly better than MaltParser-based one, much 
less than the performance difference on depend-
ency parsing between them. This suggests that 
such difference between these two state-of-the-
art dependency parsers does not much affect cor-
responding SRL systems. This is also confirmed 
by the results in Table 4. 
Comparison of Table 3 and Table 4 in labeled 
F1 on the Brown test data shows that the system 
with gold predicate identification significantly 
outperforms the one with automatic predicate 
identification using our simple algorithm by 
about 22 in labeled F1. This suggests that the 
performance of predicate identification is critical 
to SRL.  
256
 
MSTParser MaltParser  
Labeled Macro 
F1 
Labeled F1 LAS Labeled Macro 
F1 
Labeled F1 LAS 
Test WSJ 84.50 81.95 87.01 83.69 81.82 85.50
Test Brown 67.61 53.69 81.46 65.09 53.03 77.06
Test 
WSJ+Brown 82.53 78.64 86.39 81.52 78.45 84.56
Table 3: Unofficial performance using MSTParser and MaltParser 
 with predicates automatically identified 
 
MSTParser MaltParser  
Labeled Macro 
F1 
Labeled F1 LAS Labeled Macro 
F1 
Labeled F1 LAS 
Test WSJ 84.75 82.45 87.01 84.04 82.52 85.50
Test Brown 78.31 75.07 81.46 75.72 74.28 77.06
Test 
WSJ+Brown 84.05 81.66 86.39 83.13 81.64 84.56
Table 4: Unofficial performance using MSTParser and MaltParser with gold predicate identification 
 
5 Conclusions 
This paper presents a dependency tree-based 
SRL system by proper pruning and extensive 
feature engineering. Evaluation on the CoNLL 
2008 shared task shows that proper pruning and 
extensive feature engineering contributes much. 
It also shows that SRL heavily depends on the 
performance of predicate identification. 
In future work, we will explore better ways in 
predicate identification. In addition, we will ex-
plore more on dependency parsing and further 
joint learning on syntactic and semantic parsing. 
Acknowledgment 
This research is supported by Project 60673041 
under the National Natural Science Foundation 
of China and Project 2006AA01Z147 under the 
?863? National High-Tech Research and Devel-
opment of China.  
References 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational Lin-
guistics, 28:3, pages 245-288. 
Gildea, Daniel and Martha Palmer. 2002. The Neces-
sity of Syntactic Parsing for Predicate Argument 
Recognition. In Proceedings of the 40th  Associa-
tion for Computational Linguistics,  2002.  
Hacioglu, Kadri. 2004. Semantic Role Labeling Using 
Dependency Trees. In Proceedings of the Interna-
tional Conference on Computational Linguistics 
(COLING). 2004. 
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, 
Jan Haji?. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In the pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in 
Natural Language Processing, 2005 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings 
of the 12th Conference on Computational Natural 
Language Learning (CoNLL-2008). 
Nivre, Joakim and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of 
the 43rd Annual Meeting of the Association for 
Computational Linguistics, pp. 99-106, 2005 
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, 
James H. Martin, Dan Jurafsky. 2004. Shallow 
Semantic Parsing Using Support Vector Machines. 
In Proceedings of (HLT-NAACL-2004), 2004. 
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, 
James H. Martin, Dan Jurafsky. 2005. Semantic 
role labeling using different syntactic views. In 
Proceedings of the 43rd  Association for Computa-
tional Linguistics (ACL-2005), 2005. 
Punyakanok, Vasin, Peter Koomen, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. In Pro-
ceedings of 9th Conference on Computational 
Natural Language Learning (CoNLL-2005).2005 
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Proceedings 
of Conference on Empirical Methods in Natural 
Language Processing (EMNLP), 2004. 
257
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 92?99,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Hedge Detection and Scope Finding by Sequence Labeling
with Normalized Feature Selection?
Shaodian Zhang12, Hai Zhao123?, Guodong Zhou3 and Bao-Liang Lu12
1Center for Brain-Like Computing and Machine Intelligence
Dept of Computer Science and Engineering, Shanghai Jiao Tong University
2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems
Shanghai Jiao Tong University
3School of Computer Science and Technology, Soochow University
zhangsd.sjtu@gmail.com, zhaohai@cs.sjtu.edu.cn
gdzhou@suda.edu.cn, blu@cs.sjtu.edu.cn
Abstract
This paper presents a system which adopts
a standard sequence labeling technique for
hedge detection and scope finding. For
the first task, hedge detection, we formu-
late it as a hedge labeling problem, while
for the second task, we use a two-step la-
beling strategy, one for hedge cue label-
ing and the other for scope finding. In par-
ticular, various kinds of syntactic features
are systemically exploited and effectively
integrated using a large-scale normalized
feature selection method. Evaluation on
the CoNLL-2010 shared task shows that
our system achieves stable and competi-
tive results for all the closed tasks. Fur-
thermore, post-deadline experiments show
that the performance can be much further
improved using a sufficient feature selec-
tion.
1 Introduction
Hedges are linguistic devices representing spec-
ulative parts of articles. Previous works such as
(Hyland, 1996; Marco and Mercer, 2004; Light et
al., 2004; Thompson et al, 2008) present research
on hedge mainly as a linguistic phenomenon.
Meanwhile, detecting hedges and their scopes au-
tomatically are increasingly important tasks in nat-
ural language processing and information extrac-
tion, especially in biomedical community. The
shared task of CoNLL-2010 described in Farkas
et al (2010) aims at detecting hedges (task 1)
and finding their scopes (task 2) for the literature
? This work is partially supported by the National
Natural Science Foundation of China (Grants 60903119,
60773090, 90820018 and 90920004), the National Basic Re-
search Program of China (Grant No. 2009CB320901), and
the National High-Tech Research Program of China (Grant
No.2008AA02Z315).
?corresponding author
from BioScope corpus (Szarvas et al, 2008) and
Wikipedia. This paper describes a system adopt-
ing sequence labeling which performs competitive
in the official evaluation, as well as further test.
In addition, a large-scale feature selection proce-
dure is applied in training and development. Con-
sidering that BioScope corpus is annotated by two
independent linguists according to a formal guide-
line (Szarvas, 2008), while Wikipedia weasels are
tagged by netizens who are diverse in background
and various in evaluation criterion, it is needed to
handle them separately. Our system selects fea-
tures for Wikipedia and BioScope corpus indepen-
dently and evaluate them respectively, leading to
fine performances for all of them.
The rest of the paper is organized as follows.
The next section presents the technical details of
our system of hedge detection and scope finding.
Section 3 gives information of features. Section
4 shows the evaluation results, including official
results and further ones after official outputs col-
lection. Section 5 concludes the paper.
2 Methods
Basically, the tasks are formulated as sequence la-
beling in our approach. The available label set dif-
fers between task 1 and 2. In addition, it is needed
to introduce an indicator in order to find scopes for
the multi-hedge sentences properly.
2.1 Hedge detection
The valid label set of task 1, hedge detection, con-
tains only two labels: ?Hedge? and ? ?, which
represent that a word is in a hedge cue or not
respectively. Since results of hedge detection in
this shared task are evaluated at sentence level, a
sentence will be classified as ?uncertain? in the
post-process if it has one or more words labeled
?Hedge? in it and otherwise ?certain?.
92
2.2 Scope finding
The second task is divided into two steps in our
system. The first step is quite the same as what
the system does in task 1: labeling the words as in
hedge cues or not. Then the scope of each hedge
will be labeled by taking advantage of the result
of the first step. A scope can be denoted by a
beginning word and an ending word to represent
the first and the last element. In scope finding the
available label set contains ?Begin?, ?End?, ?Mid-
dle? and ? ?, representing the first and last word in
the scope, in-scope and out-of-scope. As an exam-
ple, a sentence with hedge cue and scope labeling
is given in Table 1. Hedge cue ?indicating? with
its scope from ?indicating? itself to ?transcription?
are labeled. While evaluating outputs, only ?Be-
gin?s and ?End?s will be taken into consideration
and be treated as the head and tail tokens of the
scopes of specific hedge cues.
Furthermore ...
, ...
inhibition ...
can ...
be ...
blocked ...
by ...
actinomycin ...
D ...
, ...
indicating ... Hedge Begin
a ... Middle
requirement ... Middle
for ... Middle
de ... Middle
novo ... Middle
transcription ... End
. ...
Table 1: A sentence with hedge cue and scope la-
beling
It seems that the best labeling result of task 1
can be used directly to be the proper intermediate
representation of task 2. However, the complexity
of scope finding for multi-hedge sentences forces
us to modify the intermediate result of task 2 for
the sake of handling the sentences with more than
one hedge cue correctly. Besides, since task 1 is
a sentence classification task essentially, while the
goal of the first step of task 2 is to label the words
as accurately as possible, it is easy to find that
the optimal labeling results of task 1 may not be
optimal to be the intermediate representations for
task 2. This problem can be solved if sentence-
level hedge detection and intermediate representa-
tion finding are treated as two separate tasks with
independent feature selection procedures. The de-
tails of feature selection will be given in section
3.
2.3 Scope finding for multi-hedge cases
Sentences with more than one hedge cue are quite
common in both datasets of BioScope corpus and
Wikipedia. By counting hedges in every sentence,
we find that about one fourth of the sentences with
hedges have more than one hedge cue in all three
data sources (Table 2). In Morante and Daele-
mans (2009), three classifiers predict whether each
token is Begin, End or None and a postprocess-
ing is needed to associate Begins and Ends with
their corresponding hedge cues. In our approach,
in order to decrease ambiguous or illegal outputs
e.g. inequivalent numbers of Begins and Ends, a
pair of Begin and End without their correspond-
ing hedge cue between them, etc., sentences with
more than one hedge cue will be preprocessed by
making copies as many as the number of hedges
and be handled separately.
The sentence which is selected as a sample has
two hedge cues: ?suggesting? and ?may?, so our
system preprocesses the sentence into two single-
hedge ones, which is illustrated in Table 3. Now it
comes to the problem of finding scope for single-
hedge sentence. The two copies are labeled sep-
arately, getting one scope from ?suggesting? to
?mitogenesis? for the hedge cue ?suggesting? and
the other from ?IFN-alpha? to ?mitogenesis? for
?may?. Merging the two results will give the final
scope resolution of the sentence.
However, compared with matching Begins and
Ends in postprocessing given by Morante and
Daelemans (2009), the above method gives rise
to out of control of projections of the scopes,
i.e. scopes of hedges may partially overlap after
copies are merged. Since scopes should be in-
tact constituents of sentences, namely, subtrees in
syntax tree which never partly overlap with each
other, results like this are linguistically illegal and
should be discarded. We solve this problem by in-
troducing an instructional feature called ?Indica-
tor?. For sentences with more than one hedge cue,
namely more than one copy while finding scopes,
words inside the union of existing (labeled) scopes
will be tagged as ?Indicator? in unhandled copies
before every labeling. For example, after finding
scope for the first copy in Table 3 and words from
93
Dataset # Sentence # No-hedge ratio # One-hedge ratio # Multi-hedge ratio
Biomedical Abstracts 11871 9770 82.3% 1603 13.5% 498 4.2%
Biomedical Fulltexts 2670 2151 80.6% 385 14.4% 134 5.0%
Wikipedia 11111 8627 77.6% 1936 17.4% 548 4.9%
Table 2: Statistics of hedge amount
IFN-alpha IFN-alpha
also also
sensitized sensitized
T T
cells cells
to to
IL-2-induced IL-2-induced
proliferation proliferation
, ,
further further
suggesting Hedge suggesting
that that
IFN-alpha IFN-alpha
may may Hedge
be be
involved involved
in in
the the
regulation regulation
of of
T-cell T-cell
mitogenesis mitogenesis
. .
Table 3: An example of 2-hedge sentence before
scope finding
?suggesting? to ?mitogenesis? are put in the scope
of cue ?suggesting?, these words should be tagged
?Indicator? in the second copy, whose result is il-
lustrated in Table 4. If not in a scope, any word is
tagged ? ? as the indicator. The ?Indicator?s tag-
ging from ?suggesting? to ?mitogenesis? in Table
4 mean that no other than the situations of a) ?Be-
gin? is after or at ?suggesting? and ?End? is before
or at ?mitogenesis? b) Both ?Begin? and ?End? are
before ?suggesting? c) Both next ?Begin? and next
?End? are after ?mitogenesis? can be accepted. In
other words, new labeling should keep the projec-
tions of scopes in the result. Although it is only
an instructional indicator and does not have any
coerciveness, the evaluation result of experiment
shows it effective.
3 Feature selection
Since hedge and scope finding are quite novel
tasks and it is not easy to determine the effective
features by experience, a greedy feature selection
is conducted. As it mentioned in section 2, our
system divides scope finding into two sub-tasks:
IFN-alpha ...
also ...
sensitized ...
T ...
cells ...
to ...
IL-2-induced ...
proliferation ...
, ...
further ...
suggesting ... Indicator
that ... Indicator
IFN-alpha ... Indicator Begin
may ... Indicator Hedge Middle
be ... Indicator Middle
involved ... Indicator Middle
in ... Indicator Middle
the ... Indicator Middle
regulation ... Indicator Middle
of ... Indicator Middle
T-cell ... Indicator Middle
mitogenesis ... Indicator End
. ...
Table 4: Scope resolution with instructional fea-
ture: ?Indicator?
a) Hedge cue labeling
b) Scope labeling
The first one is the same as hedge detection task
in strategy, but quite distinct in target of feature
set, because hedge detection is a task of sentence
classification while the first step of scope find-
ing aims at high accuracy of labeling hedge cues.
Therefore, three independent procedures of fea-
ture selection are conducted for BioScope corpus
dataset. AsWikipedia is not involved in the task of
scope finding, it only needs one final feature set.
About 200 feature templates are initially con-
sidered for each task. We mainly borrow ideas and
are enlightened by following sources while initial-
izing feature template sets:
a) Previous papers on hedge detection and
scope finding (Light et al, 2004; Medlock,
2008; Medlock and Briscoe, 2008; Kilicoglu
and Bergler, 2008; Szarvas, 2008; Ganter
and Strube, 2009; Morante and Daelemans,
2009);
94
b) Related works such as named entity recog-
nition (Collins, 1999) and text chunking
(Zhang et al, 2001);
c) Some literature on dependency parsing
(Nivre and Scholz, 2004; McDonald et al,
2005; Nivre, 2009; Zhao et al, 2009c; Zhao
et al, 2009a);
3.1 Notations of Feature Template
A large amount of advanced syntactic features in-
cluding syntactic connections, paths, families and
their concatenations are introduced. Many of these
features come from dependency parsing, which
aims at building syntactic tree expressed by depen-
dencies between words. More details about de-
pendency parsing are given in Nivre and Scholz
(2004) and McDonald et al (2005). The parser
in Zhao et al (2009a) is used to construct de-
pendency structures in our system, and some of
the notations in this paper adopt those presented
in Zhao et al (2009c). Feature templates are from
various combinations or integrations of the follow-
ing basic elements.
Word Property. This part of features includes
word form (form), lemma (lemma), part-of-speech
tag (pos), syntactic dependency (dp) , syntactic de-
pendency label (dprel).
Syntactic Connection. This includes syntactic
head (h), left(right) farthest(nearest) child (lm, ln,
rm and rn) and high (low) support verb, noun or
preposition. Here we specify the last one as an
example, support verb(noun/preposition). From a
given word to the syntactic root along the syntac-
tic tree, the first verb/noun/preposition that is met
is called its low support verb/noun/preposition,
and the nearest one to the root(farthest to
the given word) is called as its high support
verb/noun/preposition. The concept of support
verb was broadly used (Toutanova et al, 2005;
Xue, 2006; Jiang and Ng, 2006), and it is extended
to nouns and prepositions in Zhao et al (2009b).
In addition, a slightly modified syntactic head, pp-
head, is introduced, it returns the left most sibling
of a given word if the word is headed by a prepo-
sition, otherwise it returns the original head.
Path. There are two basic types of path. One
is the linear path (linePath) in the sequence, the
other is the path in the syntactic parsing tree (dp-
Path). For example, m:n|dpPath represents the
dependency path from word m to n. Assuming
that the two paths from m and n to the root are
pm and pn, m:n|dpPathShare, m:n|dpPathPred
and m:n|dpPathArgu represent the common part
of pm and pn, part of pm which does not belong
to pn and part of pn which does not belong to pm,
respectively.
Family. A children set includes all syntactic
children(children) are used in the template nota-
tions.
Concatenation of Elements. For all collected
elements according to dpPath, children and so on,
we use three strategies to concatenate all those
strings to produce the feature value. The first is
seq, which concatenates all collected strings with-
out doing anything. The second is bag, which
removes all duplicated strings and sort the rest.
The third is noDup, which removes all duplicated
neighbored strings.
Hedge Cue Dictionary and Scope Indicator.
Hedge cues in the training set are collected and put
in a dictionary. Whether a word in the training or
testing set is in the dictionary (dic) is introduced
into feature templates. As the evaluation is non-
open, we do not put in any additional hedge cues
from other resources. An indicator (indicator) is
given for multi-hedge scope finding, as specified
in section 2.At last, in feature set for scope label-
ing, hedge represents that the word is in a hedge
cue.
At last, we take x as current token to be labeled,
and xm to denote neighbor words. m > 0 repre-
sents that it is a word goes mth after current word
and m < 0 for word ?mth before current word.
3.2 Feature template sets for each task
As optimal feature template subsets cannot be ex-
pected to be extracted from so large sets by hand,
greedy feature selections according to Zhao et al
(2009b) are applied. The normalized feature selec-
tion has been proved to be effective in quite a lot
of NLP tasks and can often successfully select an
optimal or very close to optimal feature set from a
large-scale superset. Although usually it needs 3
to 4 loops denoted by ?While? in the Algorithm 1
of Zhao et al (2009b) to get the best template set,
we only complete one before official outputs col-
lection because of time limitation, which to a large
extent hinders the performance of the system.
Three template sets are selected for BioScope
corpus. One with the highest accuracy for
sentence-level hedge detection (Set B), one with
the best performance for word-level hedge cue la-
95
beling (Set H) and another one with the maximal
F-score for scope finding (Set S). In addition, one
set is discovered for sentence-level hedge detec-
tion of Wikipedia (Set W)1 . Table 52 lists some
selected feature templates which are basic word or
hedging properties for the three sets of BioScope
corpus and Wikipedia. From the table we can see
it is clear that the combinations of lemma, POS
and word form of words in context, which are usu-
ally basic and common elements in NLP, are also
effective for hedge detection. And as we expected,
the feature that represents whether the word is in
the hedge list or not is very useful especially in
hedge cue finding, indicating that methods based
on a hedge cue lists (Light et al, 2004) or keyword
selection (Szarvas, 2008) are quite significant way
to accomplish such tasks.
Some a little complicated syntactic features
based on dependencies are systemically exploited
as features for tasks. Table 6 enumerates some of
the syntactic features which proves to be highly
effective. We noticed that lowSupportNoun, high-
SupportNoun and features derived from dpPath is
notably useful. It can be explained by the aware-
ness that hedge labeling and scope finding are to
process literatures in the level of semantics where
syntactic features are often helpful.
We continue our feature selection procedures
for BioScope corpus after official outputs collec-
tion and obtain feature template sets that bring bet-
ter performance. Table 7 gives some of the fea-
tures in the optimized sets for BioScope corpus
resolution. One difference between the new sets
and the old ones is the former contain more syntac-
tic elements, indicating that exploiting syntactic
feature is a correct choice. Another difference is
the new sets assemble more information of words
before or after the current word, especially words
linearly far away but close in syntax tree. Appear-
ance of combination of these two factors such as
x?1.lm.form seems to provide an evidence of the
insufficiency training and development of our sys-
tem submitted to some extent.
4 Evaluation results
Two tracks (closed and open challenges) are pro-
vided for CoNLL-2010 shared task. We partici-
pated in the closed challenge, select features based
1num in the set of Wikipedia represents the sequential
number of word in the sentence
2Contact the authors to get the full feature lists, as well as
entire optimized sets in post-deadline experiment
- x.lemma + x1.lemma + x?1.lemma
+ x.dic + x1.dic + x?1.dic
- x.lemma + x1.pos + x?1.pos + x.pos
+ x1.lemma + x?1.lemma
- x.form
Set B x.pos + x1.pos + x?1.pos + x2.pos
+ x?2.pos
- x.dic + x1.dic + x?1.dic
- x1.pos
- x.dic + x1.dic + x?1.dic + x2.dic
+ x?2.dic
- x.pos + x?1.pos
- x.dic
Set H x.dic + x.lemma + x.pos + x.form
- x.pos + x1.pos + x?1.pos + x2.pos
+ x?2.pos
- x?2.form + x?2.lemma
- x?1.form + x.form
- x.dic + x1.dic + x?1.dic
- x.dic + x1.dic + x?1.dic + x2.dic
+ x?2.dic + x3.dic + x?3.dic
- x.indicator
- x.hedge + x1.hedge + x?1.hedge
Set S x.lemma + x1.pos + x?1.pos + x.pos
+ x1.lemma + x?1.lemma
- x.pos + x.hedge + x.dp + x.dprel
- x1.pos
- x.pos + x1.pos + x?1.pos + x2.pos
+ x?2.pos
- x.lemma + x1.lemma + x?1.lemma
- + x.dic + x1.dic + x?1.dic
- x.lemma + x1.lemma + x?1.lemma
+x2.lemma + x?2.lemma + x.dic
+ x1.dic + x?1.dic + x2.dic + x?2.dic
- x.lemma + x1.lemma
Set W x.hedge + x1.hedge + x?1.hedge
+ x2.hedge + x?2.hedge + x3.hedge
+ x?3.hedge
- x.pos + x1.pos + x?1.pos +x2.pos
+ x?2.pos + x.dic + x1.dic + x?1.dic
+ x2.dic + x?2.dic
- x.pos + x.dic
- x.num + x.dic
Table 5: Selected feature template sets
96
- x.lowSupportNoun:x | dpPathArgu.dprel.seq
- x.lowSupportNoun:x|dpPathArgu.dprel.seq
+ x.lowSupportProp:x|dpPathArgu.dprel.seq
- x.lowSupoortNoun.pos
- x.pos + x.children.dprel.bag
- x.rm.dprel + x.form
Set B x.pphead.lemma
- x.form + x.children.dprel.bag
- x.lowSupportNoun:x?dpTreeRelation
- x.lowSupportProp.lemma
- x.form + x.children.dprel.noDup
- x.highSupportNoun:x|dpTreeRelation + x.form
- x.lowSupportVerb.form
- x.lowSupportProp:x|dpPathShared.dprel.seq
- x.lowSupportProp:x|dpPathShared.pos.seq
- x.highSupportNoun.pos
- x.highSupportNoun:x|dpTreeRelation
- x.highSupportNoun:x|dpPathArgu.dprel.seq
Set H + x.highSupportProp:x|dpPathArgu.dprel.seq
- xlowSupportProp.lemma
- x.rm.dprel
- x.lm.form
- x.lemma + x.pphead.form
- x.lowSupportVerb.form
- x.rm.lemma + x.rm.form
- x.children.dprel.noDup
- x.children.dprel.bag
- x.highSupportNoun:x|dpTreeRelation
- x.lemma + x.pphead.form
Set S x.highSupportNoun:x|dpTreeRelation + x.form
- x.lowSupportVerb.form
- x.lowSupportVerb.lemma
- x.h.children.dprel.bag
- x.highSupportVerb.form
- x.lm.form
- x.lemma + x.pphead.form
- x.lm.dprel + x.pos
- x.lowSupportProp:x|dpPathPred.dprel.seq
- x.pphead.lemma
Set W x.rm.lemma
- x.lowSupportProp:x|dpTreeRelation
- x.lowSupportVerb:x|dpPathPred.dprel.seq
- x.lowSupportVerb:x|dpPathPred.pos.seq
- x.lowSupportVerb:x|dpPathShared.pos.seq
- x.lowSupportProp:x|dpPathShared.pos.seq
- x.lowSupportProp.form
Table 6: Syntactic features
- x?1.lemma
- x.dic + x1.dic + x?1.dic + x2.dic
+ x?2.dic + x3.dic + x?3.dic
- x?1.pos + x1.pos
Set H x.rm.lemma
- x.rm.dprel
- x.lm.dprel + x.pos
- x.lowSupportNoun:x | dpPathArgu.dprel.seq
- x.lowSupportNoun:x|dpPathArgu.dprel.seq
+ x.lowSupportProp:x|dpPathArgu.dprel.seq
- x?1.lemma
- x.lemma + x1.lemma + x?1.lemma + x.dic
+ x1.dic + x?1.dic
- x.form + x.lemma + x.pos + x.dic
Set B x?2.form + x?1.form
- x.highSupportNoun:x|dpTreeRelation
- x.highSupportNoun:x|dpPathArgu.dprel.seq
- x.lowSupportProp:x|dpPathShared.dprel.seq
- x?1.lm.form
- x1.form
- x.pos + x.dic
- x.hedge + x1.hedge + x?1.hedge
- x.pos + x1.pos + x?1.pos + x2.pos + x?2.pos
Set S x.children.dprel.bag
- x.lemma + x.pphead.form
- x.highSupportVerb.form
- x.highSupportNoun:x|dpTreeRelation + x.form
- x.lowSupportNoun:x|dpTreeRelation + x.form
Table 7: Selected improved feature template sets
for BioScope corpus
on the in-domain data and evaluated our system
on the in-domain and cross-domain evaluation set.
All the experiments are implemented and run by
Maximum Entropy Markov Models (McCallum,
2000).
4.1 Official results
The official results for tasks are in Table 8, in
which three in-domain tests and cue matching
result for biomedical texts are listed. For the
first task for BioCorpus, our system gives F-score
0.8363 in in-domain test and for Wikipedia we
give F-score 0.5618 in closed evaluation. For the
second task, our system gives results in closed and
open test, with F-score 0.4425 and 0.4441 respec-
tively.
We compare the F-score of our system with the
best in the final result in Table 9. We rank pretty
high in Wikipedia hedge detection, while other
three are quite steady but not prominent. This is
mainly due to two reasons:
1. Feature selection procedures are not perfectly
conducted.
2. Abstracts and fulltexts in BioScope are mixed
to be the training set, which proves quite in-
appropriate when the evaluation set contains
97
only fulltext literature, since abstract and full-
text are quite different in terms of hedging.
Dataset F-score Best
Task1-closed 0.8363 0.8636
BioScope Task2-closed 0.4425 0.5732
Cue-matching 0.7853 0.8134
Wikipedia Task1-closed 0.5618 0.6017
Table 9: Comparing results with the best
4.2 Further results
Intact feature selection procedures for BioScope
corpus are conducted after official outputs collec-
tions. The results of evaluation with completely
selected features compared with the incomplete
one are given in Table 7. The system performs a
higher score on evaluation data (Table 10), which
is more competitive in both tasks on BioScope cor-
pus. The improvement for task 2 is significant, but
the increase of performance of hedge cue detec-
tion is less remarkable. We believe that a larger
fulltext training set and a more considerate train-
ing plan will help us to do better job in the future
work.
Dataset Complete Incomplete
Task1-closed 0.8522 0.8363
BioScope Task2-closed 0.5151 0.4425
Cue-matching 0.7990 0.7853
Table 10: Comparing improved outputs with the
best
5 Conclusion
We describe the system that uses sequence label-
ing with normalized feature selection and rich fea-
tures to detect hedges and find scopes for hedge
cues. Syntactic features which are derived from
dependencies are exploited, which prove to be
quite favorable. The evaluation results show that
our system is steady in performance and does
pretty good hedging and scope finding in both Bio-
Scope corpus and Wikipedia, especially when the
feature selection procedure is carefully and totally
conducted. The results suggest that sequence la-
beling and a feature-oriented method are effective
in such NLP tasks.
References
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore, 4,
August.
Ken Hyland. 1996. Writing without conviction: Hedg-
ing in science research articles. Applied Linguistics,
17:433?54.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic
role labeling of NomBank: A maximum entropy ap-
proach. In Proceedings of the EMNLP-2006, pages
138?145, Sydney, Australia.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9.
Marc Light, Xin Ying Qiu, and Padimini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proc. of the
BioLINK 2004, pages 17?24.
Chrysanne Di Marco and Robert E. Mercer. 2004.
Hedging in scientific articles as a means of classify-
ing citations. In Working Notes of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications, pages 50?54.
Andrew McCallum. 2000. Maximum entropy markov
models for information extraction and segmentation.
In Proceedings of ICML 2000, pages 591?598, Stan-
ford, California.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of HLT/EMNLP 05, pages 523?530, Vancouver,
Canada, October.
Ben Medlock and Ted Briscoe. 2008. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of 45th Annual Meeting
of the ACL, pages 992?999, Prague, Czech Repub-
lic, June.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41:636?654.
98
Dataset TP FP FN precision recall F-score
BioScope Task1-closed 669 141 121 0.8259 0.8468 0.8363
Task2-closed 441 519 592 0.4594 0.4269 0.4425
Cue-matching 788 172 259 0.8208 0.7526 0.7853
Wikipedia Task1-closed 991 303 1243 0.7658 0.4436 0.5618
Table 8: Official results of our submission for in-domain tasks
Roser Morante andWalter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the Workshop on BioNLP, pages 28?36,
Boulder, Colorado, June.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING-2004, pages 64?70, Geneva, Switzer-
land, August 23rd-27th.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of ACL-
IJCNLP 2009, pages 351?359, Suntec, Singapore,
2-7 August.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, and
Ja?nos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In Proceedings of BioNLP 2008,
pages 38?45, Columbus, Ohio, USA, June.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of ACL-08, pages 281?
289, Columbus, Ohio, USA, June.
Paul Thompson, Giulia Venturi, John McNaught,
Simonetta Montemagni, and Sophia Ananiadou.
2008. Categorising modality in biomedical texts. In
Proc. of the LREC 2008 Workshop on Building and
Evaluating Resources for Biomedical Text Mining,
pages 27?34, Marrakech.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL-2005, pages
589?596, Ann Arbor, USA.
Nianwen Xue. 2006. Semantic role labeling of nom-
inalized predicates in Chinese. In Proceedings of
the Human Language Technology Conference of the
NAACL (NAACL-2006), pages 431?438, New York
City, USA, June.
Tong Zhang, Fred Damerau, and David Johnson. 2001.
Text chunking using regularized winnow. In Pro-
ceedings of the 39th Annual Meeting on Associa-
tion for Computational Linguistics, pages 539?546,
Toulouse, France.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009a. Multi-
lingual dependency learning: Exploiting rich fea-
tures for tagging syntactic and semantic dependen-
cies. In Proceedings of CoNLL-2009, June 4-5,
Boulder, Colorado, USA.
Hai Zhao, Wenliang Chen, and Chunyu Kit. 2009b.
Semantic dependency parsing of NomBank and
PropBank: An efficient integrated approach via a
large-scale feature selection. In Proceedings of
EMNLP-2009, pages 30?39, Singapore.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009c. Multilingual dependency learning:
A huge feature engineering method to semantic de-
pendency parsing. In Proceedings of CoNLL-2009,
June 4-5, Boulder, Colorado, USA.
99
Soochow University: Description and Analysis of the Chinese 
Word Sense Induction System for CLP2010 
Hua Xu   Bing Liu   Longhua Qian?   Guodong Zhou 
Natural Language Processing Lab 
School of Computer Science and Technology 
Soochow University, Suzhou, China 215006 
Email: 
{20094227034,20084227065055,qianlonghua,gdzhou}@suda.edu.cn
 
                                                 
? Corresponding author 
Abstract 
Recent studies on word sense induction 
(WSI) mainly concentrate on European 
languages, Chinese word sense induction 
is becoming popular as it presents a new 
challenge to WSI. In this paper, we 
propose a feature-based approach using 
the spectral clustering algorithm to this 
problem. We also compare various 
clustering algorithms and similarity 
metrics. Experimental results show that 
our system achieves promising 
performance in F-score. 
1 Introduction 
Word sense induction (WSI) is an open problem 
of natural language processing (NLP), which 
governs the process of automatic discovery of 
the possible senses of a word. WSI is similar to 
word sense disambiguation (WSD) both in 
methods employed and in problem encountered. 
In the procedure of WSD, the senses are as-
sumed to be known and the task focuses on 
choosing the correct one for an ambiguous word 
in a context. The main difference between them 
is that the task of WSD generally requires large-
scale manually annotated lexical resources while 
WSI does not. As WSI doesn?t rely on the 
manually annotated corpus, it has become one of 
the most important topics in current NLP re-
search (Pantel and Lin, 2002; Neill, 2002; Rapp, 
2003). Typically, the input to a WSI algorithm is 
a target word to be disambiguated. The task of 
WSI is to distinguish which target words share 
the same meaning when they appear in different 
contexts. Such result can be at the very least 
used as empirically grounded suggestions for 
lexicographers or as input for WSD algorithm. 
Other possible uses include automatic thesaurus 
or ontology construction, machine translation or 
information retrieval. Compared with European 
languages, the study of WSI in Chinese is scarce. 
Furthermore, as Chinese has its special writing 
style and Chinese word senses have their own 
characteristics, the methods that work well in 
English may not perform effectively in Chinese 
and the usefulness of WSI in real-world applica-
tions has yet to be tested and proved. 
The core idea behind word sense induction is 
that contextual information provides important 
cues regarding a word?s meaning. The idea dates 
back to (at least) Firth (1957) (?You shall know 
a word by the company it keeps?), and under-
lies most WSD and lexicon acquisition work to 
date. For example, when the adverb phrase oc-
curring prior to the ambiguous word????, 
then the target word is more likely to be a verb 
and the meaning of which is ?to hold something?; 
Otherwise, if an adjective phrase locates in the 
same position, then it probably means ?confi-
dence? in English. Thus, the words surrounds 
the target word are main contributor to sense 
induction. 
The bake off task 4 on WSI in the first CIPS-
SIGHAN Joint Conference on Chinese Lan-
guage Processing (CLP2010) is intended to 
promote the exchange of ideas among partici-
pants and improve the performance of Chinese 
WSI systems. Generally, our WSI system also 
adopts a clustering algorithm to group the con-
texts of a target word. Differently, after generat-
ing feature vectors of words, we compute a simi-
larity matrix with each cell denoting the similar-
ity between two contexts. Furthermore, the set of 
similarity values of a context with other contexts 
is viewed as another kind of feature vector, 
which we refer to as similarity vector. Both fea-
ture vectors and similarity vectors can be sepa-
rately used as the input to clustering algorithms. 
Experimental results show our system achieves 
good performances on the development dataset 
as well as on the final test dataset provided by 
the CLP2010. 
2 System Description 
This section sequentially describes the architec-
ture of our WSI system and its main components. 
2.1 System Architecture 
Figure 1 shows the architecture of our WSI 
system. The first step is to preprocess the raw 
dataset for feature extraction. After that, we 
extract ?bag of words? from the sentence 
containing a target word (feature extraction) and 
transform them into high-dimension vectors 
(feature vector generation). Then, similarities of 
every two vectors could be computed based on 
the feature vectors (similarity measurement). the 
similarities of an instance can be viewed as 
another vector?similarity vector. Both feature 
vectors and similarity vectors can be served as 
the input for clustering algorithms. Finally, we 
perform three clustering algorithms, namely, k-
means, HAC and spectral clustering.  
Dataset
Preprocess
Feature
Extraction
Vector
Generation
Similarity
Measurement
Similarity
As VectorClustering
WSI
Results
 
Figure 1  Architecture of our Chinese 
WSI system 
2.2 Feature Engineering 
In the task of WSI, the target words with their 
topical context are first transformed into multi-
dimensional vectors with various features, and 
then applying clustering algorithm to detect the 
relevance of each other. 
Corpus Preprocessing 
For each raw file, we first extract each sentence 
embedded in the tag <instance>, including 
the <head> and </head> tags which are used 
to identify the ambiguous word. Then, we put all 
the sentences related to one target word into a 
file, ordered by their instance IDs. The next step 
is word segmentation, which segments each sen-
tence into a sequence of Chinese words and is 
unique for Chinese WSI. Here, we use the soft-
ware from Hylanda1 since it is ready to use and 
considered an efficient word segmentation tool. 
Finally, since we retain the <head> tag in the 
sentence, the <head> and </head> tags are 
usually separated after word segmentation, thus 
we have to restore them in order to correctly lo-
cate the target word during the process of feature 
extraction. 
Feature Extraction 
After word segmentation, for a context of a par-
ticular word, we extract all the words around it 
in the sentence and build a feature vector based 
on a ?bag-of-words? Boolean model. ?Bag-of-
words? means that we don?t consider the order 
of words. Meanwhile, in the Boolean model, 
each word in the context is used to generate a 
feature. This feature will be set to 1 if the word 
appears in the context or 0 if it does not. Finally, 
we get a number of feature vectors, each of them 
corresponds to an instance of the target word. 
One problem with this feature-based method is 
that, since the size of word set may be huge, the 
dimension is also very high, which might lead to 
data sparsity problem.  
Similarity measurement 
One commonly used metric for similarity meas-
urement is cosine similarity, which measures the 
angle between two feature vectors in a high-
dimensional space. Formally, the cosine similar-
ity can be computed as follows: 
cos ,ine similarity ?< > = ?
x yx y
x y
 
where ,x y are two vectors in the vector space 
and x , y are the lengths of  ,x y  respectively. 
                                                 
1 http://www.hylanda.com/
Some clustering algorithms takes feature vec-
tors as the input and use cosine similarity as the 
similarity measurement between two vectors. 
This may lead to performance degradation due 
to data sparsity in feature vectors. To avoid this 
problem, we compute the similarities of every 
two vectors and generate an  similarity 
matrix, where  is the number of all the in-
stances containing the ambiguous word. Gener-
ally, is usually much smaller than the dimen-
sion size and may alleviate the data sparsity 
problem. Moreover, we view every row of this 
matrix (i.e., an ordered set of similarities of an 
instance with other instances) as another kind of 
feature vector. In other words, each instance it-
self is regarded as a feature, and the similarity 
with this instance reflects the weight of the fea-
ture. We call this vector similarity vector, which 
we believe will more properly represent the in-
stance and achieve promising performance. 
*N N
N
N
2.3 Clustering Algorithm 
Clustering is a very popular technique which 
aims to partition a dataset into such subgroups 
that samples in the same group share more simi-
larities than those from different groups. Our 
system explores various cluster algorithms for 
Chinese WSI, including K-means, hierarchical 
agglomerative clustering (HAC), and spectral 
clustering (SC). 
K-means (KM) 
K-means is a very popular method for general 
clustering used to automatically partition a data 
set into k groups. K-means works by assigning 
multidimensional vectors to one of K clusters, 
where is given as a priori. The aim of the al-
gorithm is to minimize the variance of the vec-
tors assigned to each cluster.  
K
K-means proceeds by selecting k  initial clus-
ter centers and then iteratively refining them as 
follows: 
(1) Choose cluster centers to coincide with 
k randomly-chosen patterns or k  ran-
domly defined points. 
k
(2) Assign each pattern to the closest cluster 
center. 
(3) Recompute the cluster centers using the 
current cluster memberships. 
(4) If a convergence criterion is not met, go 
to step 2. 
Hierarchical Agglomerative Clustering (HAC) 
Different from K-means, hierarchical clustering 
creates a hierarchy of clusters which can be 
represented in a tree structure called a 
dendrogram. The root of the tree consists of a 
single cluster containing all objects, and the 
leaves correspond to individual object.  
Typically, hierarchical agglomerative 
clustering (HAC) starts at the leaves and 
successively merges two clusters together as 
long as they have the shortest distance among all 
the pair-wise distances between any two clusters.  
Given a specified number of clusters, the key 
problem is to determine where to cut the hierar-
chical tree into clusters. In this paper, we gener-
ate the final flat cluster structures greedily by 
maximizing the equal distribution of instances 
among different clusters. 
Spectral Clustering (SC) 
Spectral clustering refers to a class of techniques 
which rely on the eigen-structure of a similarity 
matrix to partition points into disjoint clusters 
with points in the same cluster having high simi-
larity and points in different clusters having low 
similarity.  
Compared to the ?traditional algorithms? such 
as K-means or single linkage, spectral clustering 
has many fundamental advantages. Results ob-
tained by spectral clustering often outperform 
the traditional approaches, spectral clustering is 
very simple to implement and can be solved ef-
ficiently by standard linear algebra methods. 
3 System Evaluation 
This section reports the evaluation dataset and 
system performance for our feature-based Chi-
nese WSI system. 
3.1  Dataset and Evaluation Metrics 
We use the CLP2010 bake off task 4 sample 
dataset as our development dataset. There are 
2500 examples containing 50 target words and 
each word has 50 sentences with different mean-
ings. The exact meanings of the target words are 
blind, only the number of the meanings is pro-
vided in the data. We compute the system per-
formance with the sample dataset because it con-
tains the answers of each candidate meaning. 
The test dataset provided by the CLP2010 is 
similar to the sample dataset. It contains 100 
target words and 5000 instances in total. How-
ever, it doesn?t provide the answers. 
The F-score measurement is the same as Zhao 
and Karypis (2005). Given a particular 
class rL of size and a particular cluster  of 
size , suppose  in the cluster  belong to
rn iS
in irn iS rL , 
then the value of this class and cluster is de-
fined to be 
F
2 ( , ) ( ,( , )
( , ) ( , )
r i r i
r i
r i r i
)R L S P L SF L S
R L S P L S
? ?= +  
( , ) /r i ir rR L S n n=  
( , ) /r i ir iP L S n n=  
where ( , )r iR L S is the recall value and  
is the precision value. The F-score of class 
( , )r iP L S
rL is 
the maximum value and F-score value follow: F
( ) max ( , )
ir S r i
F score L F L S? =  
1
( )
c
r
r
r
nF score F score L
n=
? = ??  
where  is the total number of classes and n  is 
the total size. 
c
3.2 Experiment Results 
Table 1 reports the F-score of our feature-based 
Chinese WSI for different feature sets with 
various window sizes using K-means clustering. 
Since there are different results for each run of 
K-means clustering algorithm, we perform 20 
trials and compute their average as the final 
results. The columns denote different window 
size n, that is, the n words before and after the 
target word are extracted as features. Particularly, 
the size of infinity (?) means that all the words 
in the sentence except the target word are 
considered. The rows represent various 
combinations of feature sets and similarity 
measurements, currently, four of which are 
considered as follows: 
F-All: all the words are considered as features 
and from them feature vectors are constructed. 
F-Stop: the top 150 most frequently occurring 
words in the total ?word bags? of the corpus are 
regarded as stop words and thus removed from 
the feature set. Feature vectors are then formed 
from these words. 
S-All: the feature set and the feature vector 
are the same as those of F-All, but instead the 
similarity vector is used for clustering (c.f. Sec-
tion 2.2). 
S-Stop: the feature set and the feature vector 
are the same as those of F-Stop, but instead the 
similarity vector is used for clustering. 
Table 1 Experimental results for differ-
ent feature sets with different window sizes us-
ing K-means clustering 
 
This table shows that S-Stop achieves the best 
performance of 0.7320 in F-score. This suggests 
that for K-means clustering, Chinese WSI can 
benefit much from removing stop words and 
adopting similarity vector. It also shows that: 
Feature/ 
Similarity 3 7 10 ? 
F-All 0.5949 0.6199 0.6320 0.6575
F-Stop 0.6384 0.6500 0.6493 0.6428
S-All 0.5856 0.6044 0.6186 0.6843
S-Stop 0.6532 0.6696 0.6804 0.7320
z As the window size increases, the perform-
ance is almost consistently enhanced. This 
indicates that all the words in the sentence 
more or less help disambiguate the target 
word. 
z Removing stop words consistently improves 
the F-score for both similarity metrics. This 
means some high frequent words do not help 
discriminate the meaning of the target words, 
and further work on feature selection is thus 
encouraged. 
z Similarity vector consistently outperforms 
feature vector for stop-removed features, but 
not so for all-words features. This may be 
due to the fact that, when the window size is 
limited, the influence of frequently occur-
ring stop words is relatively high, thus the 
similarity vector misrepresent the context of 
the target word. On the contrary, when stop 
words are removed or the context is wide, 
the similarity vector can better reflect the 
target word?s context, leading to better per-
formance. 
In order to intuitively explain why the simi-
larity vector is more discriminative than the fea-
ture vector, we take two sentences containing 
the Chinese word ???? (hold, grasp) as an ex-
ample (Figure 2). These two sentences have few 
common words, so clustering via feature vectors 
puts them into different classes. However, since 
the similarities of these two feature vectors with 
other feature vectors are much similar, cluster-
ing via similarity vectors group them into the 
same class.  
 
Figure 2  An example from the dataset 
 
According to the conclusion of the above ex-
periments, it is better to include all the words 
except stop words in the sentence as the features 
in the subsequent experiment. Table 2 lists the 
results using various clustering algorithms with 
this same experimental setting. It shows that the 
spectral clustering algorithm achieves the best 
performance of 0.7692 in F-score for Chinese 
WSI using the S-All setup. Additionally, there 
are some interesting findings: 
mi-
 of 
han 
ing 
this 
nto 
lus-
lly 
er-
re. 
ex-
der 
ers the density information, therefore S-All 
will not significantly improve the perform-
ance. 
 
Feature/ 
Similarity 
KM HAC SC 
F-All 0.6428 0.6280 0.7686 
S-All 0.7320 0.6332 0.7692 
Table 2 Experiments results using dif-
ferent clustering algorithms 
<lexelt item="??" snum="4"> 
<instance id="0012"> 
???????????????????
?????????????????
<head>??</head>???????????
?? 
 </instance>  
<instance id="0015">  
???????????????????
???????????????<head>?
?</head>???????????????
??????????????????? 
</instance>  
</lexelt> 
3.3 Final System Performance 
For the CLP2010 task 4 test dataset which con-
tains 100 target words and 5000 instances in to-
tal, we first extract all the words except stop 
words in a sentence containing the target word, 
then produce the feature vector for each context 
and generate the similarity matrix, finally we 
perform the spectral cluster algorithm. Probably 
because the distribution of the target word in the 
test dataset is different from that in the develop-
ment dataset, the F-score of our system on the 
test dataset is 0.7108, about 0.05 units lower 
than that we got on the sample dataset. 
4 Conclusions and Future Work 
In our Chinese WSI system, we extract all the 
words except stop words in the sentence, con-
struct feature vectors and similarity vectors, and 
apply the spectral clustering algorithm to this 
problem. Experimental results show that our 
simple and efficient system achieve a promising 
result. Moreover, we also compare various clus-
tering algorithms and similarity metrics. We find 
that although the spectral clustering algorithm 
outperforms other clustering algorithms, the K-
means clustering with similarity vectors can also 
achieve comparable results. 
For future work, we will incorporate more 
linguistic features, such as base chunking, parse 
tree feature as well as dependency information 
into our system to further improve the perform-
ance. 
Acknowledgement 
This research is supported by Project 60873150, 
60970056 and 90920004 under the National 
Natural Science Foundation of China. We would z Although SC performs best, KM with si
larity vectors achieves comparable results
0.7320 units in F-score, slightly lower t
that of SC. 
z HAC performs worst among all cluster
algorithms. An observation reveals that 
algorithm always groups the instances i
highly skewed clusters, i.e., one or two c
ters are extremely large while others usua
have only one instance in each cluster. 
z It is surprising that S-All slightly outp
forms F-All by only 0.0006 units in F-sco
The truth is that, as discussed in the first 
periment, KM using F-All doesn?t consi
instance density while S-All does. On the 
contrary, SC identifies the eign-structure in 
the instance space and thus already consid-
also like to thank other contributors in the NLP 
lab at Soochow University. 
References 
Jain A, Murty M. 1999.Flynn P. Data clustering : A 
Review [J]. ACM Computing Surveys,1999,31 
(3) :2642323 
F. Bach and M. Jordan.2004. Learning spectral clus-
tering. In Proc. of NIPS-16. MIT Press, 2004. 
Samuel Brody and Mirella Lapata. 2009. Bayesian 
word sense induction. In Proceedings of the 12th 
Conference of the European Chapter of the ACL 
(EACL 2009), pages 103?111. 
Neill, D. B. 2002. Fully Automatic Word Sense In-
duction by Semantic Clustering. Cambridge Uni-
versity, Master?s Thesis, M.Phil. in Computer 
Speech. 
Agirre, E. and Soroa, A. 2007.  Semeval-2007 task 02: 
Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations:7-12 
Ioannis P. Klapaftis and Suresh Manandhar. 2008. 
Word sense induction using graphs of collocations. 
In Proceedings of the 18th European Conference 
On Artificial Intelligence (ECAI-2008), Patras, 
Greece, July. IOS Press. 
Kannan, R., Vempala, S and Vetta, A. 2004. On clus-
terings: Good, bad and spectral. J. ACM, 51(3), 
497?515. 
Reinhard Rapp.2004. A practical solution to the 
problem of automatic word sense induction. Pro-
ceedings of the ACL 2004 on Interactive poster 
and demonstration sessions, p.26-es, July 21-26, 
2004, Barcelona, Spain 
Bordag, S. 2006. Word sense induction: Triplet-based 
clustering and automatic evaluation. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics (EACL, Trento, Italy). 137--144. 
Ying Zhao, and George Karypis.2005. Hierarchical 
Clustering Algorithms for Document Datasets. Da-
ta Mining and Knowledge Discovery, 10, 141?168. 
 
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 232?242,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Using Syntactic Head Information in Hierarchical Phrase-Based Translation
Junhui Li Zhaopeng Tu? Guodong Zhou? Josef van Genabith
Centre for Next Generation Localisation
School of Computing, Dublin City University
? Key Lab. of Intelligent Info. Processing
Institute of Computing Technology, Chinese Academy of Sciences
?School of Computer Science and Technology
Soochow University, China
{jli,josef}@computing.dcu.ie
tuzhaopeng@ict.ac.cn gdzhou@suda.edu.cn
Abstract
Chiang?s hierarchical phrase-based (HPB)
translation model advances the state-of-the-art
in statistical machine translation by expanding
conventional phrases to hierarchical phrases
? phrases that contain sub-phrases. How-
ever, the original HPB model is prone to over-
generation due to lack of linguistic knowl-
edge: the grammar may suggest more deriva-
tions than appropriate, many of which may
lead to ungrammatical translations. On the
other hand, limitations of glue grammar rules
in the original HPB model may actually pre-
vent systems from considering some reason-
able derivations. This paper presents a sim-
ple but effective translation model, called the
Head-Driven HPB (HD-HPB) model, which
incorporates head information in translation
rules to better capture syntax-driven informa-
tion in a derivation. In addition, unlike the
original glue rules, the HD-HPB model allows
improved reordering between any two neigh-
boring non-terminals to explore a larger re-
ordering search space. An extensive set of ex-
periments on Chinese-English translation on
four NIST MT test sets, using both a small
and a large training set, show that our HD-
HPB model consistently and statistically sig-
nificantly outperforms Chiang?s model as well
as a source side SAMT-style model.
1 Introduction
Chiang?s hierarchical phrase-based (HPB) transla-
tion model utilizes synchronous context free gram-
mar (SCFG) for translation derivation (Chiang,
2005; Chiang, 2007) and has been widely adopted
in statistical machine translation (SMT). Typically,
such models define two types of translation rules:
hierarchical (translation) rules which consist of both
terminals and non-terminals, and glue (grammar)
rules which combine translated phrases in a mono-
tone fashion. However, due to lack of linguistic
knowledge, Chiang?s HPB model contains only one
type of non-terminal symbol X , often making it
difficult to select the most appropriate translation
rules.1
One important research question is therefore how
to refine the non-terminal category X using linguis-
tically motivated information: Zollmann and Venu-
gopal (2006) (SAMT) e.g. use (partial) syntactic
categories derived from CFG trees while Zollmann
and Vogel (2011) use word tags, generated by ei-
ther POS analysis or unsupervised word class in-
duction. Almaghout et al (2011) employ CCG-
based supertags. Mylonakis and Sima?an (2011) use
linguistic information of various granularities such
as Phrase-Pair, Constituent, Concatenation of Con-
stituents, and Partial Constituents, where applica-
ble.
By contrast, and inspired by previous work in
parsing (Charniak, 2000; Collins, 2003), our Head-
Driven HPB (HD-HPB) model is based on the in-
tuition that linguistic heads provide important in-
formation about a constituent or distributionally de-
fined fragment, as in HPB. We identify heads using
linguistically motivated dependency parsing, and
use head information to refine X.
Furthermore, Chiang?s HPB model suffers from
limited phrase reordering by combining translated
1Another non-terminal symbol S is used in glue rules.
232
 (a) (b) 
zuotian chuxi huiyi 
attended a meeting yesterday 
X2 X1 
X1 X2 
S2 
S1 
S2 
S1 
zuotian chuxi huiyi 
attended a meeting yesterday 
X4 X3 
X3 X4 
X2 
X1 
X2 
S2 
X1 
S1 
S1 
X1 
Figure 1: Example of derivations disallowed in Chiang?s
HPB model. The rules with dotted lines are not covered
in Chiang?s model.
phrases in a monotonic way with glue rules. In
addition, once a glue rule is adopted, it requires
all rules above it to be glue rules. For exam-
ple, given a Chinese-English sentence pair (?
?/zuotian1 ??/chuxi2 ??/huiyi3, Attended2 a3
meeting3 yesterday1), a correct translation is impos-
sible via HPB derivations in Figure 1. For the deriva-
tion in Figure 1(a), swap reordering in the glue rule
(i.e., S1 ? ?S2X2, X2S2?) is disallowed and, even
if such a swap reordering is available, it lacks useful
information for rule selection. For the derivation in
Figure 1(b), the combination of two non-terminals
(i.e., X2 ? ?X3X4, X3X4?) is disallowed to form
a new non-terminal which in turn is a sub-phrase of
a hierarchical rule. These limitations prevent tra-
ditional HPB systems from even considering some
reasonable derivations.
To tackle the problem of glue rules, He (2010) ex-
tended the HPB model by using bracketing transduc-
tion grammar (Wu, 1996) instead of the monotone
glue rules, and trained an extra classifier for glue
rules to predict reorderings of neighboring phrases.
By contrast, our HD-HPB model refines the non-
terminal symbol X with syntactic head informa-
tion and provides flexible reordering rules, including
swap, which can mix freely with hierarchical trans-
lation rules for better interleaving of translation and
reordering in translation derivations.
Different from the soft constraint modeling
adopted in (Chan et al, 2007; Marton and Resnik,
2008; Shen et al, 2009; He et al, 2010; Huang et
al., 2010; Gao et al, 2011), our approach encodes
syntactic information in translation rules. However,
the two approaches are not mutually exclusive, as
we could also include a set of syntax-driven features
into our translation model. Our approach maintains
the advantages of Chiang?s HPB model while at the
same time incorporating head information and flex-
ible reordering in a derivation in a natural way. Ex-
periments on Chinese-English translation using four
NIST MT test sets show that our HD-HPB model
significantly outperforms Chiang?s HPB as well as a
SAMT-style refined version of HPB.
The paper is structured as follows: Section 2
describes the synchronous context-free grammar
(SCFG) in our HD-HPB translation model. Sec-
tion 3 presents our model and features, followed by
the decoding algorithm in Section 4. We report ex-
perimental results in Section 5. Finally we conclude
in Section 6.
2 Head-Driven HPB Translation Model
Like Chiang (2005) and Chiang (2007), our HD-
HPB translation model adopts a synchronous con-
text free grammar, a rewriting system which gen-
erates source and target side string pairs simultane-
ously using a context-free grammar. In particular,
each synchronous rule rewrites a non-terminal into
a pair of strings, s and t, where s (or t) contains ter-
minals and non-terminals from the source (or target)
language and there is a one-to-one correspondence
between the non-terminal symbols on both sides.
A good and informative inventory of non-terminal
symbols is always important, especially for a suc-
cessful SCFG-based translation model. Instead of
collapsing all non-terminals in the source language
into a single symbol X as in Chiang (2007), ideally
non-terminals should capture important information
of the word sequences they cover to be able to prop-
erly discriminate between similar and different word
sequences during translation. This motivates our
approach to provide syntax-enriched non-terminal
symbols. Given a word sequence f ij from position i
to position j, we refine the non-terminal symbol X
to reflect some of the internal syntactic structure of
233
?
?
/N
R
 
O
uz
ho
u 
?
?
/N
N 
ba
gu
o 
?
?
/A
D
li
an
mi
ng
?
?
/V
V
zh
ic
hi
 
?
?
/N
R 
me
ig
uo
 
?
/P du
i
?
?
/N
N
ce
li
e 
?
/N
R
yi
 
ro
ot
E
ig
ht
 
E
ur
op
ea
n 
co
un
tr
ie
s
jo
in
tly
su
pp
or
t
A
m
er
ic
a?
s
st
an
d
ag
ai
ns
t
Ir
aq
Figure 2: An example word alignment for a Chinese-English sentence pair with the dependency parse tree for the
Chinese sentence. Here, each Chinese word is attached with its POS tag and Pinyin.
the word sequence covered by X . A correct transla-
tion rule selection therefore not only maps terminals
into terminals, but is both constrained and guided
by syntactic information in the non-terminals. At
the same time, it is not clear whether an ?ideal? ap-
proach that captures a full syntactic analysis of the
string fragment covered by a non-terminal is feasi-
ble: the diversity of syntactic structures could make
training impossible and lead to serious data sparse-
ness issues. As a compromise, given a word se-
quence f ij , we first find heads and then concatenate
the POS tags of these heads as f ij?s non-terminal
symbol.2 Our approach is guided by the intuition
that linguistic heads provide important information
about a constituent or distributionally defined frag-
ment, as in HPB. Specifically, we adopt dependency
structure to derive heads, which are defined as:
Definition 1. For word sequence f ij , word
fk (i ? k ? j) is regarded as a head if it is domi-
nated by a word outside of this sequence.
Note that this definition (i) allows for a word se-
quence to have one or more heads (largely due to
the fact that a word sequence is not necessarily lin-
guistically constrained) and (ii) ensures that heads
are always the highest heads in the sequence from a
dependency structure perspective. For example, the
word sequence ouzhou baguo lianming in Figure 2
has two heads (i.e., baguo and lianming, ouzhou is
not a head of this sequence since its headword baguo
falls within this sequence) and the non-terminal cor-
responding to the sequence is thus labeled as NN-
AD. It is worth noting that in this paper we only
refine non-terminal X on the source side to head-
informed ones, while still usingX on the target side.
2Note that instead of POS tags, it is also possible to use other
types of syntactic information associated with heads to refine
non-terminal symbols (Section 5.5.2).
In our HD-HPB model, the SCFG is defined as
a tuple ??, N,?,?,<?, where ? is a set of source
language terminals,N is a set of non-terminals cate-
gorizing terminals in ?, ? is a set of target language
terminals, ? is a set of non-terminals categorizing
terminals in ?, and < is a set of translation rules.
A rule ? in < is in the form of ?Ps ? s, Pt ? t, ??,
where:
? Ps ? N and Pt ? ?;
? s ? (? ?N)+ and t ? (? ? ?)+
? ? is a bijection between non-terminals in s and t.
According to the occurrence of terminals in s and
t, we group the rules in the HD-HPB model into two
categories: head-driven hierarchical rules (HD-HRs)
and non-terminal reordering rules (NRRs), where
the former have at least one terminal on both source
and target sides and the later have no terminals. For
rule extraction, we first identify initial phrase pairs
on word-aligned sentence pairs by using the same
criterion as most phrase-based translation models
(Och and Ney, 2004) and Chiang?s HPB model (Chi-
ang, 2005; Chiang, 2007). We extract HD-HRs and
NRRs based on initial phrase pairs, respectively.
2.1 HD-HRs: Head-Driven Hierarchical Rules
As mentioned, a HD-HR has at least one terminal
on both source and target sides. This is the same
as the hierarchical rules defined in Chiang?s HPB
model (Chiang, 2007), except that we use head POS-
informed non-terminal symbols in the source lan-
guage. We look for initial phrase pairs that con-
tain other phrases and then replace sub-phrases with
their corresponding non-terminal symbols. Given
the word alignment as shown in Figure 2, Table 1
demonstrates the difference between hierarchical
rules in Chiang (2007) and HD-HRs defined here.
234
phrase pairs hierarchical rule head-driven hierarchical rule
celie, stand X?celie, stand
NN?celie,
X?stand
dui yi celie1, stand1 against Iraq X?dui yi X1, X1 against Iraq
NN?dui yi NN1,
X?X1 against Iraq
zhichi meiguo, support America?s X?zhichi meiguo, support America?s
VV-NR?zhichi meiguo,
X?support America?s
zhichi meiguo1 dui yi celie2,
support America?s1 stand2 against Iraq
X?X1 dui yi X2,
X1 X2 against Iraq
VV?VV-NR1 dui yi NN2,
X?X1 X2 against Iraq
Table 1: Comparison of hierarchical rules in Chiang (2007) and HD-HRs. Indexed underlines indicate sub-phrases
and corresponding non-terminal symbols. The non-terminals in HD-HRs (e.g., NN, VV, VV-NR) capture the head(s)
POS tags of the corresponding word sequence in the source language.
Similar to Chiang?s HPB model, our HD-HPB
model will result in a large number of rules causing
problems in decoding. To alleviate these problems,
we filter our HD-HRs according to the same con-
straints as described in Chiang (2007). Moreover,
we discard rules that have non-terminals with more
than four heads.
2.2 NRRs: Non-terminal Reordering Rules
NRRs are translation rules without terminals. Given
an initial phrase pair
?
f ij , e
i?
j?
?
, we check all other
initial phrase pairs
?
fkl , e
k?
l?
?
which satisfy k = j+1
(i.e., phrase fkl is located immediately to the right
of f ij in the source language). For their target
side translations, there are four possible positional
relationships: monotone, discontinuous monotone,
swap, and discontinuous swap. In order to differen-
tiate non-terminals from those in the target language
(i.e., X), we use Y as a variable for non-terminals in
the source language, and obtain four types of NRRs:
? Monotone ?Y ? Y1Y2, X ? X1X2?;
? Discontinuous monotone
?Y ? Y1Y2, X ? X1 . . . X2?;
? Swap ?Y ? Y1Y2, X ? X2X1?;
? Discontinuous swap
?Y ? Y1Y2, X ? X2 . . . X1?.
For example in Figure 2, the NRR for initial
phrase pairs ?zhichi meiguo, support America?s?
and ?dui yi celie, stand against Iraq? would be
?V V ? V V -NR1NN2, X ? X1X2?.
Merging two neighboring non-terminals into a
single non-terminal, NRRs enable the translation
model to explore a wider search space. During train-
ing, we extract four types of NRRs and calculate
probabilities for each type. To speed up decoding,
we currently (i) only use monotone and swap NRRs
and (ii) limit the number of non-terminals in a NRR
to 2.
3 Log-linear Model and Features
Following Och and Ney (2002), we depart from the
traditional noisy-channel approach and use a general
log-linear model. Let d be a derivation from sen-
tence f in the source language to sentence e in the
target language. The probability of d is defined as:
P (d) ?
?
i
?i (d)
?i (1)
where ?i are features defined on derivations and
?i are feature weights. In particular, we use a fea-
ture set analogous to the default feature set of Chi-
ang (2007), which includes:
? Phd-hr (t|s) and Phd-hr (s|t), translation probabili-
ties for HD-HRs;
? Plex (t|s) and Plex (s|t), lexical translation proba-
bilities for HD-HRs;
? Ptyhd-hr = exp (?1), rule penalty for HD-HRs;
? Pnrr (t|s), translation probability for NRRs;
? Ptynrr = exp (?1), rule penalty for NRRs;
? Plm (e), language model;
? Ptyword (e) = exp (?|e|), word penalty.
235
Algorithm 1: Decoding Algorithm
Input: Sentence f1n in the source language
Dependency structure of f1n
HD-HR rule set HDHR
NRR rule set NRR
Initial phrase length K
Output: Best derivation d?
1. set chart[i, j]=NIL (1 ? i ? j ? n);
2. for l from 1 to n do
3. for all i, j such that j ? i = l do
4. if l ? K do
5. for all derivations d derived from
HDHR spanning from i to j do
6. add d into chart[i, j]
7. for all derivations d derived from
NRR spanning from i to j do
8. add d into chart[i, j]
9. set d? as the top derivation of chart[1, n]
10.return d?
It is worth pointing out that we define translation
probabilities for NRRs only for the direction from
source language to target language, although trans-
lation probabilities for HD-HRs are defined for both
directions. This is mostly due to the fact that a NRR
excludes terminals and has only two options on the
target side (i.e., either X ? X1X2 or X ? X2X1).
4 Decoding
Our decoder is based on CKY-style chart parsing
with beam search. Given an input sentence f , it finds
a sentence e in the target language derived from the
best derivation d? among all possible derivations D:
d? = arg max
d?D
P (D) (2)
Algorithm 1 presents the decoding process. Given
a source sentence, it searches for the best deriva-
tion bottom-up. For a source span [i, j], it applies
both types of HD-HRs and NRRs. However, HD-
HRs are only applied to generate derivations span-
ning no more than K words ? the initial phrase
length limit used in training to extract HD-HRs ?
while NRRs are applied to derivations spanning any
length. Unlike in Chiang (2007), it is possible for
a non-terminal generated by a NRR to be included
afterwards by a HD-HR or another NRR. Similar to
Chiang (2007) in generating k-best derivations from
i to j, we make use of cube pruning (Huang and Chi-
ang, 2005) with an integrated language model for
each derivation.
5 Experiments
We evaluate the performance of our HD-HPB model
and compare it with our implementation of Chiang?s
HPB model (Chiang, 2007), a source-side SAMT-
style refined version of HPB (SAMT-HPB), and the
Moses implementation of HPB. For fair compari-
son, we adopt the same parameter settings for HD-
HPB, HPB and SAMT-HPB systems, including ini-
tial phrase length (as 10) in training, the maximum
number of non-terminals (as 2) in translation rules,
maximum number of non-terminals plus terminals
(as 5) on the source, prohibition of non-terminals
to be adjacent on the source, beam threshold ? (as
10?5) (to discard derivations with a score worse than
? times the best score in the same chart cell), beam
size b (as 200) (i.e. each chart cell contains at most
b derivations). For Moses HPB, we use ?grow-diag-
final-and? to obtain symmetric word alignments, 10
for the maximum phrase length, and the recom-
mended default values for all other parameters.
5.1 Experimental Settings
To examine the efficacy of our approach on training
datasets of different scales, we first train translation
models on a small-sized corpus, and then scale to a
larger one. We use the 2002 NIST MT evaluation
test data (878 sentence pairs) as the development
data, and the 2003, 2004, 2005, 2006-news NIST
MT evaluation test data (919, 1788, 1082, and 616
sentence pairs, respectively) as the test data. To find
heads, we parse the source sentences with the Berke-
ley Parser3 (Petrov and Klein, 2007) trained on Chi-
nese TreeBank 6.0 and use the Penn2Malt toolkit4
to obtain dependency structures.
We obtain the word alignments by running
GIZA++ (Och and Ney, 2000) on the corpus in
both directions, applying ?grow-diag-final-and? re-
finement (Koehn et al, 2003). We use the SRI lan-
guage modeling toolkit to train a 5-gram language
model on the Xinhua portion of the Gigaword corpus
and standard MERT (Och, 2003) to tune the feature
3http://code.google.com/p/berkeleyparser/
4http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html/
236
weights on the development data.
For evaluation, the NIST BLEU script (version
12) with the default settings is used to calculate the
NIST and the BLEU scores, which measures case-
insensitive matching of n-grams with n up to 4. To
test whether a performance difference is statistically
significant, we conduct significance tests following
the paired bootstrap approach (Koehn, 2004). In this
paper, ?**? and ?*? denote p-values less than
0.01 and in-between [0.01, 0.05), respectively.
5.2 Results on Small Data
To test the HD-HPB models, we firstly carried out
experiments using the FBIS corpus as training data,
which contains ?240K sentence pairs. Table 2 lists
the rule table sizes. The full rule table size (includ-
ing HD-HRs and NRRs) of our HD-HPB model is
about 1.5 times that of Chiang?s, largely due to re-
fining the non-terminal symbolX in Chiang?s model
into head-informed ones in our model. It is also
unsurprising, that the test set-filtered rule table size
of our model is only about 0.8 times that of Chi-
ang?s: this is due to the fact that some of the re-
fined translation rule patterns required by the test
set are unattested in the training data. Furthermore,
the rule table size of NRRs is much smaller than
that of HD-HRs since a NRR contains only two
non-terminals. Table 3 lists the translation perfor-
mance with NIST and BLEU scores. Note that our
re-implementation of Chiang?s original HPB model
performs on a par with Moses HPB. Table 3 shows
that our HD-HPB model significantly outperforms
Chiang?s HPB model with an average improvement
of 1.32 in BLEU and 0.16 in NIST (and similar im-
provements over Moses HPB).
Although HD-HPB has small size of phrase ta-
bles compared to HPB, it still consumes more time
in decoding (e.g., 15.1 vs. 11.0), mostly due to the
flexible reordering of NRRs.
5.3 Results on Large Data
We also conduct experiments on larger training
data with ?1.5M sentence pairs from the LDC
dataset.5 Table 4 lists the rule table sizes and Ta-
ble 5 presents translation performance with NIST
5This dataset includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06
and BLEU scores. It shows that our HD-HPB model
consistently outperforms Chiang?s HPB model with
an average improvement of 1.91 in BLEU and 0.35
in NIST (similar for Moses HPB). Compared to the
improvement achieved on the small data, it is en-
couraging to see that our HD-HPB model benefits
more from larger training data with little adverse ef-
fect on decoding time which increases only slightly
from 15.1 to 16.6 seconds per sentence.
5.4 Comparison with SAMT-HPB
Comparing the performance of SAMT-HPB with
regular HPB in Table 3 and Table 5, it is interest-
ing to see that in general the SAMT-style approach
leads to a deterioration of translation performance
for the small training set (e.g., 30.09 for SAMT-HPB
vs. 30.64 for HPB) while it comes into its own for
the large training set (e.g., 33.54 for SAMT-HPB vs.
32.95 for HPB), indicating that the SAMT-style ap-
proach is more prone to data sparseness than HPB
(or, indeed, HD-HPB).
Comparing the performance of SAMT-HPB with
HD-HPB, shows that our head-driven non-terminal
refining approach consistently outperforms the
SAMT-style approach on an extensive set of ex-
periments (for each test set p < 0.01), indicating
that head information is more effective than (par-
tial) CFG categories. To make the comparison fair,
it is important to note that our implementation of
source-side SAMT-HPB includes the same sophis-
ticated non-terminal re-ordering NRR rules as HD-
HPB (Section 2.2 ). Thus the performance differ-
ences reported here are not due to different reorder-
ing capabilities, but to the discriminative impact of
the head information in HD-HPB over SAMT-style
annotation. Taking lianming zhichi in Figure 2 as an
example, HD-HPB labels the span VV, as lianming
is dominated by zhichi, effecively ignoring lianming
in the translation rule, while the SAMT label is
ADVP:AD+VV6 which is more susceptible to data
sparsity (Table 2 and Table 4). In addition, SAMT
resorts to X if a text span fails to satisify pre-defined
categories. Examining initial phrases extracted from
the SAMT training data shows that 28% of them are
labeled as X. Finally, for Chinese syntactic analy-
6The constituency structure for lianming zhichi is (VP
(ADVP (AD lianming)) (VP (VV zhichi) ...)).
237
System Total Rules MT 03 MT 04 MT 05 MT 06 Avg.
HPB 39.6M 2.8M 4.7M 3.3M 3.0M 3.4M
HD-HPB 59.5/0.6M 1.9/0.1M 3.4/0.2M 2.3/0.2M 2.0/0.1M 2.4/0.2M
SAMT-HPB 70.1/0.4M 2.2/0.2M 4.0/0.2M 2.7/0.2M 2.3/0.2M 2.8/0.2M
Table 2: Rule table sizes of different models trained on small data. Note: 1) SAMT-HPB indicates our HD-HPB model
with the non-terminal scheme of Zollmann and Venugopal (2006); 2) For HD-HPB and SAMT-HPB, the rule sizes
separated by / indicate HD-HRs and NRRs, respectively; 2) Except for ?Total Rules?, the figures correspond to rules
filtered on the corresponding test set.
System
MT 03 MT 04 MT 05 MT 06 Avg.
Time
NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU
Moses HPB 7.377 29.67 8.209 33.60 7.571 29.49 6.773 28.90 7.483 30.42 NA
HPB 8.137 29.75 9.050 34.06 8.264 30.09 7.788 28.64 8.310 30.64 11.0
HD-HPB 8.308 31.01** 9.211 35.11** 8.426 31.57** 7.930 30.15** 8.469 31.96 15.1
SAMT-HPB 7.886 29.14* 8.703 33.32** 7.961 29.49* 7.307 28.41 7.964 30.09 17.3
HD-HR+Glue 7.966 29.51 8.826 33.68 8.116 29.84 7.474 28.51 8.095 30.39 5.4
Table 3: NIST and BLEU (%) scores of different models trained on small data. Note: 1) HD-HR+Glue indicates our
HD-HPB model replacing NRRs with glue rules; 2) Significance tests for Moses HPB, HD-HPB, SAMT-HPB and
HD-HR+Glue are done against HPB.
System Total Rules MT 03 MT 04 MT 05 MT 06 Avg.
HPB 206.8M 11.3M 17.6M 12.9M 10.4M 13.0M
HD-HPB 318.6/2.3M 7.3/0.3M 12.2/0.4M 8.5/0.3M 6.7/0.2M 8.7/0.3M
SAMT-HPB 371.0/1.1M 8.6/0.3M 14.3/0.4M 10.1/0.3M 7.9/0.3M 10.2/0.3M
Table 4: Rule table sizes of different models trained on large data.
System
MT 03 MT 04 MT 05 MT 06 Avg.
Time
NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU
Moses HPB 7.914 32.94* 8.429 35.16 7.962 32.18 6.483 29.88* 7.697 32.54 NA
HPB 8.583 33.59 9.114 35.39 8.465 32.20 7.532 30.60 8.423 32.95 13.7
HD-HPB 8.885 35.50** 9.494 37.61** 8.871 34.56** 7.839 31.78** 8.772 34.86 16.6
SAMT-HPB 8.644 34.07 9.245 36.52** 8.618 32.90* 7.543 30.66 8.493 33.54 19.1
HD-HR+Glue 8.831 34.58** 9.435 36.55** 8.821 33.84** 7.863 31.06 8.737 34.01 6.7
Table 5: NIST and BLEU (%) scores of different models trained on large data. Note: System labels and significance
testing as in Table 3.
238
sis, dependency structure is more reliable than con-
stituency structure. Moreover, SAMT-HPB takes
more time in decoding than HD-HPB due to larger
phrase tables.
5.5 Discussion
5.5.1 Individual Contribution of HD-HRs and
NRRs
Examining translation output shows that on aver-
age each sentence employs 16.6/5.2 HD-HRs/NRRs
in our HD-HPB model, compared to 15.9/3.6 hier-
archical rules/glue rules in Chiang?s model, provid-
ing further indication of the importance of NRRs in
translation. In order to separate out the individual
contributions of the novel HD-HRs and NRRs, we
carry out an additional experiment (HD-HR+Glue)
using HD-HRs with monotonic glue rules only (ad-
justed to refined rule labels, but effectively switching
off the extra reordering power of full NRRs) both
on the small and the large datasets, with interest-
ing results: Table 3 (HD-HR+Glue) shows that for
the small training set most of the improvement of
our full HD-HPB model comes from the NRRs, as
RR+Glue performs on the same level as Chiang?s
original and Moses HPB (the differences are not
statistically significant), perhaps indicating sparse-
ness for the refined HD-HRs given the small train-
ing set. Table 5 shows that for the large training
set, HD-HRs come into their own: on average more
than half of the improvement over HPB (Chiang and
Moses) comes from the refined HD-HRs, the rest
from NRRs.
It is not surprising that compared to the others
HD-HR+Glue takes much less time in decoding.
This is due to the fact that 1) compared to HPB, the
refined translation rule patterns on the source side
have fewer entries in phrase table; 2) compared to
HD-HPB, HD-HR+Glue switches off the extra re-
ordering of NRRs. The decoding time for HD-HPB
and HD-HR+Glue suggests that NRRs are more than
doubling the time required to decode.
5.5.2 Different Head Label Sets
Examining initial phrases extracted from the large
size training data shows that there are 63K types
of refined non-terminals with respect to 33 types of
POS tags. Considering the sparseness in translation
rules caused by this comparatively detained POS tag
set, we carry out an experiment with a reduced set
of non-terminal types by using a less granular POS
tag set (C-HPB). Moreover, due to the fact that con-
catenation of POS tags of heads mostly captures in-
ternal structure of a text span, it is interesting to ex-
amine the effect of other syntactic labels, in partic-
ular dependency labels, to try to better capture the
impact of the external context on the text span. To
this end, we replace the POS tag of head with its
incoming dependency label (DL-HPB), or the com-
bination of (the original fine-grained) POS tag and
its dependency label (POS-DL-HPB). For C-HPB
we use the coarse POS tag set obtained by group-
ing the 33 types of Chinese POS tags into 11 types
following Xia (2000). For example, we generalize
all verbal tags (e.g., VA, VC, VE, and VV ) and all
nominal tags (e.g., NR, NT, and NN) into Verb and
Noun, respectively. We use the dependency labels
in Penn2Malt which defines 9 types of dependency
labels for Chinese, including AMOD, DEP, NMOD,
P, PMOD, ROOT, SBAR, VC, and VMOD.7
Table 6 shows the results trained on large data.
Although the number of non-terminal types de-
creased sharply from 63K to 3K, using the coarse
POS tag set in C-HPB surprisingly lowers the per-
formance with 1.1 BLEU scores on average (e.g.,
33.75 vs. 34.86), indicating that grouping POS
tags using simple linguistic rules is inappropriate for
HD-HPB. We still believe that this initial negative
finding should be supplemented by future work on
groupping POS tags using machine learning tech-
niques considering contextual information.
Table 6 also shows that replacing POS tags
of heads with their dependency labels (DL-HPB)
substantially lowers the average performance from
34.86 on BLEU score to 32.54, probably due to
the very coarse granularity of the dependency la-
bels used. In addition, replacing non-terminal label
with more refined tags (e.g., combination of original
POS tag and dependency label) also lowers trans-
lation performance (POS-DL-HPB). Further experi-
ments with more fine-grained dependency labels are
required.
7Some other types of dependency labels (e.g., SUB, OBJ)
are generated from function tags which are not available in our
automatic parse trees.
239
V
V-
N
R
1 d
u
i 
yi
 
N
N
2 
V
V
?
 
,
 
X
?
 
X
1 
X
2 
ag
ai
n
st
 
Ir
aq
 
(b)
 
zh
ic
hi
 
m
ei
gu
o
1 
du
i y
i c
el
ie
2,
 
 
su
pp
o
rt
 
A
m
er
ic
a?
s 1
 
st
an
d 2
 
ag
ai
n
st
 
Ir
aq
 
 
V
V-
N
R
?
 
zh
ic
hi
 
m
ei
gu
o
 ,
 
X
?
 
su
pp
o
rt
 
A
m
er
ic
a?
s 
(a)
 
zh
ic
hi
 
m
ei
gu
o
,
 
su
pp
o
rt
 
A
m
er
ic
a?
s 
Figure 3: Examples of pharse pairs and their head-driven
translation rules with dependency relation, regarding Fig-
ure 2
System MT 03 MT 04 MT 05 MT 06 Avg.
HPB 33.59 35.39 32.20 30.60 32.95
HD-HPB 35.50 37.61 34.56 31.78 34.86
C-HPB 34.10 36.43 33.46 31.00 33.75
DL-HPB 32.81 35.19 32.27 29.89 32.54
POS-DL-HPB 34.08 36.78 33.14 30.43 33.61
HD-DEP-HPB 35.48 38.17 34.81 32.38 35.21
Table 6: BLEU (%) scores of models trained on large
data.
5.5.3 Encoding Full Dependency Relations in
Translation Rule
Xie et al (2011) present a dependency-to-string
translation model with a complete dependency struc-
ture on the source side and a moderate average im-
provement of 0.46 BLEU over the HPB baseline. By
contrast, in our HD-HPB approach, dependency in-
formation is used to identify heads in the strings cov-
ered by non-terminals in HD-HR rules, and to refine
non-terminal labels accordingly, with an average im-
provement of 1.91 in BLEU over the HPB baseline
(when trained on the large data). This raises the
question whether and to what extent complete (un-
labeled) dependency information between the string
and the heads in head-labeled non-terminal parts of
the source side of SCFGs in HD-HPB can further
improve results.
Given the source side of a translation rule (ei-
ther HD-HR or NRR), say Ps ? s1 . . . sm (where
each si is either a terminal or a head POS in a re-
fined non-terminal), in a further set of experiments
we keep the full unlabeled dependency relations be-
tween s1 . . . sm so as to capture contextual syntactic
information in translation rules. For example, on the
source side of Figure 3 (b) where VV-NR maps into
words zhichi and meiguo while NN maps into word
celie, we keep the full unlabeled dependency rela-
tions among words {zhichi, meiguo, dui, yi, celie}.
HD-DEP-HPB (Table 6) augments translation rules
in HD-HPB with full dependency relations on the
source side. This further boosts the performance
by 0.35 BLEU scores on average over HD-HPB and
outperforms the HPB baseline by 2.26 BLEU scores
on average.
5.5.4 Error Analysis
We carried out a manual error analysis compar-
ing the outputs of our HD-HPB system with those
of Chiang?s (both trained on the large data). We ob-
serve that improved BLEU score often correspond to
better topological ordering of phrases in the hierar-
chical structure of the source side, with a direct im-
pact on which words in a source sentence should be
translated first, and which later. As ungrammatical
translations are often due to inappropriate topologi-
cal orderings of phrases in the hierarchical structure,
guiding the translation through appropriate topolog-
ical ordering should improve translation quality. To
give an example, consider the following input sen-
tence from the 04 NIST MT test data and its two
translation results:
? Input: ??0 ??1 ?2 ?3 ??4 ????5 ?
?6 ?7 ??8 ??9
? HPB: chinese delegation to us dollar purchase of
more high technology equipment
? HD-HPB: chinese delegation went to the united
states to buy more us high - tech equipment
Figure 4 demonstrates the topological orderings
in the two hierarchical structures. In addition to dis-
fluency and some grammar errors (e.g., a main verb
is missing), the basic HPB system also makes mis-
takes in reordering (e.g., ??4 ????5 ??6
translated as dollar purchase of more). The poor
translation quality, unsurprisingly, is caused by in-
appropriate topological ordering (Figure 4(a)). By
comparison, the topological ordering reflected in the
hierarchical structure of our HD-HPB model bet-
ter respects syntactic structure (Figure 4(b)). Let
240
??
?
0?
??
?
1?
?? 2?
?? 3?
??
?
4?
??
??
?
5?
??
?
6?
?? 7?
??
?
8?
??
?
9?
X [4
?4]
?
X [6
?6]
?
X [4
?6]
?
X [3
?7]
?
X [3
?8]
?
X [2
?9]
?
X [1
?9]
?
X [0
?9]
?
S [0
?9]
?
(a)
.?T
op
olo
gic
al?
or
de
rin
gs
?of
?ph
ra
se
s?i
n?C
hia
ng
?s?
HP
B.?
(b
).?I
mp
ro
ve
d?t
op
olo
gic
al?
or
de
rin
gs
?of
?ph
ra
se
s?i
n?H
D?
HP
B.
1.?
S [0
?9]
??
?X [
0?9
],??
????
????
????
????
??X
[0?
9]
?
2.?
X [0
?9]
??
???
[0?
0]
?X [
1?9
],??
????
????
????
????
???c
hin
es
e?X
[1?
9]?
3.?
X [1
?9]
??
???
[1?
1]
?X [
2?9
],??
????
????
????
????
??d
ele
ga
tio
n?X
[2
?9
]?
4.?
X [2
?9]
??
?? [
2?2
]?X
[3
?8]
???
[9?
9],?
?
????
????
????
????
??to
?X [
3?8
]?e
qu
ipm
en
t?
5.?
X [3
?8]
??
?X [
3?7
]??
?,?
?
X [3
?7]
?te
ch
no
log
y?
6.?
X [3
?7]
??
?? [
3?3
]?X
[4
?6]
?? [
7?
7],?
?
us
?X [
4?6
]?h
igh
?
7.?
X [4
?6]
??
?X [
4?4
]??
??
? [5
?5]
?X [
6?6
],?
X [6
?6]
?X [
4?
4]?o
f?m
or
e?
8.?
X [4
?4]
??
???
[4?
4]
,??
pu
rch
as
e?
9.?
X [6
?6]
??
???
[6?
6]
,??
do
lla
r
1. ?
VV
[0
?9]
??
?N
N [
0?1
]?V
V [2
?9
],??
????
????
????
?X?
?
?X [
0?
1]?X
[2
?9]
?
2.?
NN
[0?
1]
??
???
[0?
0]?N
N [
1?
1]
,??
????
????
????
??X
??
?ch
ine
se
?X [
1?
1]??
3.?
NN
[1?
1]
??
???
[1
?1]
,??
????
????
????
??X
??
?de
leg
ati
on
?
4.?
VV
[2
?9]
??
?? [
2?2
]??
[3?
3]?
VV
[4?
9]
,???
????
?X?
?
?w
en
t?t
o?t
he
?un
ite
d?s
tat
es
?to
?X [
4?
9]?
5.?
VV
[4
?9]
??
?VV
?M
[4
?6
]??
[7?
7]
???
[8
?8]
?N
N [
9?9
],?
????
????
????
X??
?X [
4?6
]?h
igh
??t
ec
h?X
[9?
9]??
6.?
VV
?M
[4
?6]
??
???
[4
?4]
?M
[5?
6]
,?
????
????
????
????
??X
??
?bu
y?X
[5?
6]
??
7.?
M
[5
?6]
??
?CD
[5?
5]?M
[6
?6]
,??
????
????
????
X??
?X [
5?5
]?X
[6
?6]
?
8.?
CD
[5
?5]
??
???
??
[5
?5]
,??
????
????
????
?X?
?
?m
or
e?
9.?
M
[6
?6]
??
???
[6?
6]
,??
????
????
????
X??
?us
?
10
.?N
N [
9?9
]??
???
[9
?9]
,??
????
????
????
?X?
?
?eq
uip
me
nt
?
ro
ot
?? NR 0 
?? NN 1 
? VV 2
? NR 3
??
 
VV 4 
??
?? CD 5 
?? M 6 
? JJ 7
?? NN 8 
?? NN 9
CD
[5-5
]
M
[6-6
]
M
[5-6
]
VV-M
[4-6
]
VV
[4-9
]
VV
[2-9
] 
NN
 
VV
[0-9
]
[0-1
]
NN
NN
[1-1
]
[9-9
]
Figure 4: An example Chinese sentence and its two hierarchical structures. Note: subscript [i-j] represents spanning
from word i to word j on the source side.
us refer to the HD-HPB hierarchical structure on
the source side as translation parse tree and to the
treebank-based parser derived tree as syntactic parse
tree from which we obtain unlabeled dependency
structure. Examining the translation parse trees of
our HD-HPB model shows that phrases with 1/2/3/4
heads account for 64.9%/23.1%/8.8%/3.2%, respec-
tively. Compared to 37.9% of the phrases in the
translation parse trees of the HPB model, 43.2% of
the phrases of our HD-HPB model correspond to a
linguistically motivated constituent in the syntactic
parse tree with exactly the same text span. In sum,
therefore, instead of simply enforcing hard linguistic
constraints imposed by a full syntactic parse struc-
ture, our model opts for a successful mix of linguis-
tically motivated and combinatorial (matching sub-
phrases in HPB) constraints.
6 Conclusion
In this paper, we present a head-driven hierarchi-
cal phrase-based translation model, which adopts
head information (derived through unlabeled depen-
dency analysis) in the definition of non-terminals
to better differentiate among translation rules. In
addition, improved and better integrated reorder-
ing rules allow better reordering between consecu-
tive non-terminals through exploration of a larger
search space in the derivation. Our model main-
tains the strengths of Chiang?s HPB model while at
the same time it addresses the over-generation prob-
lem caused by using a uniform non-terminal symbol.
Experimental results on Chinese-English translation
across a wide range of training and test sets demon-
strate significant and consistent improvements of our
HD-HPB model over Chiang?s HPB model as well
as over a source side version of the SAMT-style
model.
Currently, we only consider head information in a
word sequence. In the future work, we will exploit
more syntactic and semantic information to system-
atically and automatically define the inventory of
non-terminals (in source and target). For example,
for a non-terminal symbol VV, we believe it will
benefit translation if we use fine-grained dependency
labels (subject, object etc.) used to link it to its gov-
erning head elsewhere in the translation rule.
Acknowledgments
This work was supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142) as part of the Cen-
tre for Next Generation Localisation (www.cngl.ie)
at Dublin City University. It was also partially
supported by Project 90920004 under the National
Natural Science Foundation of China and Project
2012AA011102 under the ?863? National High-
Tech Research and Development of China. We
thank the reviewers for their insightful comments.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2011. CCG
contextual labels in hierarchical phrase-based SMT. In
Proceedings of EAMT 2011, pages 281?288.
241
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of ACL 2007, pages
33?40.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL 2000, pages 132?
139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857?868.
Zhongjun He, Yao Meng, and Hao Yu. 2010. Maxi-
mum entropy based phrase reordering for hierarchical
phrase-based translation. In Proceedings of EMNLP
2010, pages 555?563.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT 2005, pages 53?64.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of EMNLP 2010, pages 138?
147.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT 2008, pages 1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proceedings of ACL-HLT 2011, pages 642?
652.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007, pages 404?411.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of EMNLP 2009, pages
72?80.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proceedings of ACL
1996, pages 152?158.
Fei Xia. 2000. The part-of-speech tagging guidelines for
the Penn Chinese Treebank (3.0). Technical Report
IRCS-00-07, University of Pennsylvania Institute for
Research in Cognitive Science Technical.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of EMNLP 2011, pages
216?226.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL 2006 - Workshop on Statistical
Machine Translation, pages 138?141.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proceedings of ACL-HLT 2011, pages
1?11.
242
