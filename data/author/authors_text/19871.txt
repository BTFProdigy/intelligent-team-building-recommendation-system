Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1854?1864, Dublin, Ireland, August 23-29 2014.
Rediscovering Annotation Projection for Cross-Lingual Parser Induction
J
?
org Tiedemann
Uppsala University
Department of Linguistics and Philology
firstname.lastname@lingfil.uu.se
Abstract
Previous research on annotation projection for parser induction across languages showed only
limited success and often required substantial language-specific post-processing to fix inconsis-
tencies and to lift the performance onto a useful level. Model transfer was introduced as another
quite successful alternative and much research has been devoted to this paradigm recently. In this
paper, we revisit annotation projection and show that the previously reported results are mainly
spoiled by the flaws of evaluation with incompatible annotation schemes. Lexicalized parsers
created on projected data are especially harmed by such discrepancies. However, recently de-
veloped cross-lingually harmonized annotation schemes remove this obstacle and restore the
abilities of syntactic annotation projection. We demonstrate this by applying projection strate-
gies to a number of European languages and a selection of human and machine-translated data.
Our results outperform the simple direct transfer approach by a large margin and also pave the
road to cross-lingual parsing without gold POS labels.
1 Introduction
Linguistic resources and tools exist only for a minority of the world?s languages. However, many NLP
applications require robust tools and the development of language-specific resources is expensive and
time consuming. Many of the common tools are based on data-driven techniques and they often re-
quire strong supervision to achieve reasonable results for real world applications. Fully unsupervised
techniques are not a good alternative yet for tasks like data-driven syntactic parsing and, therefore, cross-
lingual learning has been proposed as a possible solution to quickly create initial tools for otherwise
unsupported languages (Ganchev and Das, 2013).
In syntactic parsing, two main strategies have been explored in cross-lingual learning: annotation pro-
jection and model transfer. The first strategy relies on parallel corpora and automatic word alignment
that make it possible to map linguistics annotation from a source language to a new target language
(Yarowsky et al., 2001; Hwa et al., 2005; T?ackstr?om et al., 2013a). The basic idea is that existing tools
and models are used to process the source side of a parallel corpus and that projection heuristics guided
by alignment can be used to transfer the automatic annotation to the target language text. Using the
projected annotation assuming that it is sufficiently correct, models can then be trained for the target
language. However, directly projecting syntactic structure results in a rather poor performance when
applied to resources that were developed separately for individual languages (Hwa et al., 2005). Exten-
sive additional post-processing in form of transformation rules is required to achieve reasonable scores.
Furthermore, incompatible tagsets make it impossible to directly transfer labeled annotation to a new
language and previous literature on cross-lingual parsing via annotation projection is, therefore, bound
to the evaluation of unlabeled attachment scores (UAS). Less frequent, but also possible, is the scenario
where the source side of the corpus contains manual annotation (Agi?c et al., 2012). This addresses the
problem created by projecting noisy annotations, but it presupposes parallel corpora with manual anno-
tation, which are rarely available. Additionally, the problem of incompatible annotation still remains.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1854
The second strategy, model transfer instead relies on universal features and the transfer of model
parameters from one language to another. The main idea is to reduce the need of language-specific
information, e.g. using delexicalized parsers that ignore lexical information. Drawing from a harmonized
POS tagset (Petrov et al., 2012), transfer models have been used for a variety of languages. The advantage
over annotation projection approaches is that no parallel data is required (at least in the basic settings)
and that training can be performed on gold standard annotation. However, it requires a common feature
representation across languages (McDonald et al., 2013), which can be a strong bottleneck. There are
also several extensions to improve the performance of transfer models. One idea is to use multiple
source languages to increase the statistical ground for the learning process (McDonald et al., 2011;
Naseem et al., 2012), a strategy that can also be used in the case of annotation projection. Another idea
is to enhance models by cross-lingual word clusters (T?ackstr?om et al., 2012) and to use target language
adaptation techniques with prior knowledge of language properties and their relatedness when using
multiple sources in training (T?ackstr?om et al., 2013b). Based on the success of these techniques, model
transfer has dominated recent research on cross-lingual learning.
In this paper, we return to annotation projection as a powerful tool for porting syntactic parsers to new
languages. Building on the availability of cross-lingually harmonized data sets, we show that projection
performs well and outperforms direct transfer models by a large margin in contrast to previous findings
on projection with incompatible treebanks. In the following, we first revisit the projection algorithms
proposed earlier and discuss issues with transferring labels across languages. After that we report ex-
perimental results with various settings using human translations and machine-translated data. Finally,
we also look at parsing results without gold standard POS labeling, which is ultimately required when
porting parsers to new languages that lack appropriate resources.
2 Syntactic Annotation Projection
Hwa et al. (2005) propose a direct projection algorithm for syntactic dependency annotation. The algo-
rithm defines several heuristics to map source side annotations to target languages using word alignments
in a parallel corpus. The main difficulties with the projection arise with none-one-to-one links and un-
aligned tokens. Each of the following alignment types are adressed by the algorithm separately:
one-to-one: Copy relations R(s
i
, s
j
) between source words s
i
and s
j
to relations R(t
x
, t
y
)
if s
i
is aligned to t
x
and s
j
is aligned to t
y
and nothing else.
unaligned source: Create an empty (dummy) word in the target language sentence that takes
all relations (incoming and outgoing arcs) of the unaligned source language word.
one-to-many: Create an empty target word t
z
that acts as the parent of all aligned target
words t
x
, .., t
y
. Remove the alignments between s
i
and t
x
, .., t
y
and align s
i
to the new
empty word t
z
instead.
many-to-one: Delete all alignments between s
i
, .., s
j
and t
x
except the link between the head
of s
i
, .., s
j
and t
x
.
many-to-many: Perform the rule for one-to-many alignments first and then perform the rule
for many-to-one alignments.
unaligned target: Remove all unaligned target words.
In contrast to Hwa et al. (2005), we are also interested in labeled attachment and the projection of POS
annotation. Therefore, we copy labels through the alignment using the heuristics listed above. Figure 1
illustrates some of the cases discussed. There are some important implications due to the treatment of
complex alignment types. The direct projection algorithm frequently creates dummy nodes and relations
that have no correspondence in the source language. Here, we need to make some decisions on how to
project the annotation from source to target sentences.
First of all, we decided to name all additional tokens created by the algorithm with the same string
DUMMY. An alternative would be to invent unique names for each newly created token within each
sentence but this would blow up the vocabulary and would not add useful information to the data.
1855
src1    src2   src3    src4
trg1    trg2    trg3
DUMMY 
label 1
label 2
label 3
label 1
label 2
label 3
pos1      pos2      pos3      pos4
pos2      pos1      pos3        pos4
  src1    src2   src3    src4
trg1    trg2    trg3
label 1
label 2
label 3
label 1
label 2
pos2      pos1      pos4
pos1      pos2      pos3      pos4
src1    src2    src3
 trg1    trg2     trg3    trg4DUMMY 
dummy
dummy
label 2
label 2
label 1
label 1
pos1      dummy   dummy     pos2      pos3
pos1      pos2      pos3
Figure 1: Annotation projection heuristics for special alignment types: Unaligned source words (left
image), many-to-one alignments (center), one-to-many alignments (right image).
The second problem is related to the auxiliary relations that are created when treating one-to-many
alignments. In these cases, multiple words are attached to newly created dummy nodes. However, no
corresponding labels exist in the source language that would allow us to infer appropriate labels for
these additional attachments. One possibility would be to use a specific label from the existing set
of dependency relations, for example ?mwe?. However, one-to-many alignments do not always refer
to proper multi-word expressions but often represent other grammatical or structural differences like
the relation between the English preposition ?of? which is linked together with the determiner ?the?
to the German determiner ?der? in sentences like ?Resumption OF THE session? translated to German
?Wiederaufnahme DER Sitzung?. Therefore, we decided to label these additional dependency with a new
unique label dummy instead of selecting an existing one.
Yet another problem arises with the projection of POS annotation. Similar to the labeling of depen-
dency relations, we have to decide how to transfer POS tags to the target language in cases of one-
to-many alignments. In our implementation, we transfer the source language label only to the newly
created dummy node which dominates all target language words linked to the source language word
in the projected dependency tree. The daughter nodes, however, obtain the label dummy even as their
POS annotation. Alternatively, we may project the POS tag to all linked tokens according to the original
alignment but our guiding principle is to resolve link ambiguity first using the heuristics in the direct
projection algorithm and then to transfer annotation.
src1    src2   src3    src4
trg1    trg2    trg3
label 1
label 2
label 3
pos1      pos2      pos3      pos4
?
src1    src2   src3    src4
trg1    trg2    trg3
label 1
label 2
DUMMY 
dummy
dummy
label 2
label 3
label 3
DUMMY 
label 1
pos1      dummy     pos2      pos4           pos3
pos1      pos2      pos3      pos4
Figure 2: A complex example for annotation projection with many-to-many word alignments.
Finally, we also need to look at the interaction between the various projection heuristics. Figure 2
illustrates a complex case with many-to-many word alignments. Resolving the alignment ambiguity is
not entirely straightforward. In our implementation, we start by looking at all one-to-many alignments
and resolve them according to the definitions of the projection algorithm. In our example, this creates
a DUMMY node that dominates target words trg1 and trg2 and links between src1 and (trg1,trg2) are
deleted. We label the new relations with dummy. The next step considers many-to-one alignments,
1856
DET DET NOUN VERB ADP NOUN CONJ ADP DET NOUN ADJ .
Tous ses produits sont de qualite? et d? une fraicheur exemplaires .
All his products are high- quality DUMMY and DUMMY a cold mullet DUMMY copies .
DET DET NOUN VERB DUMMY ADP NOUN CONJ ADP DET DUMMY DUMMY NOUN ADJ .
det
poss
nsubj
root
adpmod
adpobj
cc
conj
det
adpobj
amod
p
det
poss
nsubj
root
adpmod
adpobj
cc
conj
det
DUMMY
DUMMY
adpobj
amod
DUMMY
p
Figure 3: A complete projection example from a translated treebank including transitive relations over
a DUMMY node that can safely be collapsed (which also removes the non-projectivity of the projected
tree). The resulting relation between quality and high- will be labeled as adpobj. Note that projection
errors appear due to the ambiguous alignments between de qualit?e and high- quality. Boxes indicate
phrases that are translated as units by the SMT engine.
which, using the remaining links, is source words (src2,src3) aligned to trg2. According to the algorithm
we delete the link between src3 and trg2 (because src2 dominates src3 in the source language tree) and
proceed. This, however, creates an unaligned source language word (src3), which we treat in the next
step. The unaligned token gives rise to the second DUMMY word, which is attached to trg3 as the result
of the alignment between src4 and trg3 and the relation between src4 and src3. Finally, we can map
all other relations according to the one-to-one alignment rule. This, however, creates a conflict with the
already existing dummy relation between the first DUMMY word and trg2. Mapping according to the
one-to-one rule turns the relation around and attaches the DUMMY word to trg2 and labels the relation
with label 1. Now, we could remove the second DUMMY node according to the rule about unaligned
target language words. However, this rule should not apply to these special nodes as they may play a
crucial role to keep elements connected in the final target language tree.
Another difficult case, which is not illustrated here, is when many-to-one alignments need to be re-
solved but the aligned source language words are siblings in the syntactic tree and no unique head can
be identified. In our implementation, we randomly pick a node but more linguistically informed guesses
would probably be better. Yet another difficult decision is the placement of the DUMMY nodes. We
decided to put them next to the head node they attach to. Other heuristics are possible and all placements
greatly influence the projectivity of the resulting tree structure. One final adjustment that we apply is
the removal of unary productions over DUMMY nodes. We collaps all relations that run with single at-
tachments via DUMMY nodes to reduce the number of these uninformative tokens. This may also have
positive effects on projectivity as we can see in the example in Figure 3.
3 Machine-Translated Treebanks
Another strategy for annotation projection is based on automatic translation. Machine translation models
can be used to create synthetic parallel data for projecting annotations from one language to another
(Tiedemann et al., 2014). Recent advances in machine translation (MT) are now making this a realistic
alternative. The use of direct treebank translation instead of existing parallel corpora has several impor-
tant advantages. First of all, we skip the use of an error-prone annotation step when producing the source
language side of the training data. Starting with a noisy source language annotation, we accumulate two
sources of errors in annotation projection. However, with direct translation we can start with the gold
standard annotation provided in the original treebank. Furthermore, we avoid problems of domain shifts
which is typically the case when applying a parser trained on one domain to texts (a parallel corpus in
1857
DELEXICALIZED
DE EN ES FR SV
DE 62.71 43.20 46.09 46.09 50.64
EN 46.62 77.66 55.65 56.46 57.68
ES 44.03 46.73 68.21 57.91 53.82
FR 43.91 46.75 59.65 67.51 52.01
SV 50.69 49.13 53.62 51.97 70.22
MCDONALD ET AL. (2013)
DE EN ES FR SV
DE 64.84 47.09 48.14 49.59 53.57
EN 48.11 78.54 56.86 58.20 57.04
ES 45.52 47.87 70.29 63.65 53.09
FR 45.96 47.41 62.56 73.37 52.25
SV 52.19 49.71 54.72 54.96 70.90
Table 1: Baselines ? labeled attachment score (LAS) for delexicalized transfer parsing; results of Mc-
Donald et al. (2013) included for reference.
our case) coming from another domain. Finally, we can also assume that machine translation produces
output which is closer to the original text than most human translations will be in any parallel corpus.
Even if this may sound as a disadvantage, for projection this is preferred. Being close to the original
source makes it easier to map annotation from one language to another as we expect a lower degree of
grammatical and structural divergences that originate in the linguistic freedom human translators can
apply. Furthermore, common statistical MT models inherently provide alignments between words and
phrases, which removes the requirement to apply yet another error-prone alignment step on the paral-
lel data. In the experiments below we, therefore, explore the translation strategy as yet another way of
applying annotation projection.
4 Experiments
In the following, we show our experimental results using annotation projection in several cross-lingual
scenarios. However, we start by presenting a delexicalized baseline, which is, to our knowledge, the
only previous model that has been presented for labeled dependency parsing across languages using
the recently created Universal Treebank. We will use this baseline as reference point even though our
projection models are not directly comparable with delexicalized direct transfer models. Note that all
results below are computed on the held-out test data sections of the Universal Treebank if not stated
otherwise.
4.1 Delexicalized Baselines
McDonald et al. (2013) present the Universal Treebank that comes with a harmonized syntactic anno-
tation scheme across six languages. This data set enables cross-lingual learning of labeled dependency
parsing models. McDonald et al. (2013) propose delexicalized models as a simple baseline for model
transfer and present encouraging labeled attachment scores (LAS) especially for closely related lan-
guages. As a reference, we have created similar baseline models using the same data set but a slightly
different setup, which is compatible with the experiments we present later. Table 1 summarizes the scores
in terms of LAS for all language pairs in the data set.
1
In our setup, we apply MaltParser (Nivre et al.,
2006) and optimize feature models and learning parameters using MaltOptimizer (Ballesteros and Nivre,
2012). For all cross-lingual experiments (columns represent target languages we test on), we always use
the same feature model and parameters as we have found for the source language treebank. Contrasting
our models with the scores from McDonald et al. (2013), we can see that they are comparable with some
differences that are due to the tools and learning parameters they apply which are along the lines of
Zhang and Nivre (2011).
4.2 Annotation Projection with Human Translations
Our first batch of projection experiments considers parallel data taken from the well-known Europarl
corpus, which is frequently used in research on statistical machine translation (SMT). It contains large
quantities of translated proceedings from the European Parliament for all but one language (namely
1
Note that we include punctuation in our evaluation. Ignoring punctuation leads to slightly higher scores but we do not
report those numbers here.
1858
UAS on CoNLL data
DE EN ES SV
DE ? 41.60 47.89 58.80
EN 49.67 ? 51.44 58.66
ES 46.14 37.78 ? 52.53
SV 57.99 51.57 57.25 ?
UAS on Universal Treebank data
DE EN ES SV
DE ? 56.21 65.18 70.27
EN 63.17 ? 68.02 70.40
ES 61.98 56.16 ? 71.06
SV 64.78 58.93 69.15 ?
Table 2: Unlabeled attachment scores for projected treebank models; comparing CoNLL data to Univer-
sal Treebank data for evaluation.
Korean) that are included in the Universal Treebank v1. The entire corpus (version 7) contains over
two million sentences in each language and we use increasing amounts of the corpus to investigate the
impact on cross-lingual parser induction. The corpus comes with automatic sentence alignments and
is quite clean with respect to translation quality and sentence alignment accuracy. It is, therefore, well
suited for our initial experiments with annotation projection even though the domain does not necessarily
match the one included in the treebank test sets.
Another important prerequisite for annotation projection is word alignment. Following the typical
setup, we rely on automatic word alignment produced by models developed for statistical machine trans-
lation. Similar to Hwa et al. (2005), we apply GIZA++ (Och and Ney, 2003) to align the corpus for all
language pairs in all translation directions using IBM model 4 Viterbi alignments. In contrast to Hwa et
al. (2005), we then use symmetrization heuristics to combine forward and backward alignments, which
is common practice in the SMT community. In particular, we apply the popular grow-diag-final-and
heuristics as implemented in the Moses toolbox (Koehn et al., 2007).
Let us first look at unlabeled attachment scores to compare results that can be achieved with harmo-
nized annotation in contrast to the ones that we can see on the cross-lingually incompatible data from the
CoNLL shared task (Buchholz and Marsi, 2006). Table 2 lists the scores that we obtain when applying
our implementation of the direct projection algorithm.
2
As expected, the performance on the CoNLL
data is rather poor, which confirms the findings of Hwa et al. (2005) even though our scores are signifi-
cantly above their results without post-correction. The scores on the Universal Treebank data, however,
are up to about 20 UAS points higher than the corresponding results on CoNLL data but without any of
the extensive post-processing transformations proposed by Hwa et al. (2005).
LAS on Universal Treebank data
DE EN ES FR SV
DE ? 49.44 56.58 58.75 61.04
EN 56.59 ? 60.07 62.78 62.15
ES 54.04 47.90 ? 65.50 61.45
FR 53.93 51.23 65.03 ? 58.71
SV 56.13 49.18 60.82 62.00 ?
data set: 40,000 sentences
 48
 50
 52
 54
 56
 58
 60
 62
 64
 0  5  10  15  20  25  30  35  40
nr of sentences (in thousands)
de
es
fr
sv
Figure 4: Annotation projection on Europarl data: LAS for induced parser models. The Figure to the
right plots the learning curves for increasing training data for projections from English to the other
languages.
Moreover, the real power of the harmonized annotation in the Universal Treebank comes from the pos-
sibility to obtain attachment labels. The table in Figure 4 shows the labeled attachment scores obtained
for training on 40,000 sentences
3
of each language pair. Next to the table in Figure 4 we also show the
2
We leave out French in this comparison as there is no French treebank in the CoNLL data.
3
Note that there may be repeated sentences in the data.
1859
with original source side annotation
DE EN ES FR SV
DE ? 53.02 54.96 58.20 59.65
EN 52.93 ? 61.25 64.58 63.82
ES 50.88 50.28 ? 66.17 60.48
FR 50.46 53.95 65.46 ? 59.05
SV 53.69 51.51 60.58 60.19 ?
jackknifing for source side annotation
DE EN ES FR SV
DE ? 50.27 54.91 56.00 57.91
EN 52.65 ? 61.28 63.86 63.72
ES 49.19 50.04 ? 64.43 59.65
FR 49.37 53.25 64.41 ? 57.78
SV 54.83 50.25 60.27 60.04 ?
Table 3: Cross-lingual parsing results (LAS) using translated treebanks (phrase-based model) and DCA-
based annotation projection. The table to the left contrasts the result with two-sample jackknifing experi-
ments where the source side dependencies are created by automatically parsing each half of the treebank
using a model trained on the other half of the training data.
learning curves for increasing amounts of training data using the example of data projected from English
to other languages. The figure illustrates that the LAS levels out at around 10,000 - 20,000 sentences and
this trend is essentially the same for all other languages as well.
4.3 Annotation Projection with Synthetic Machine-Translated Data
The next possibility we would like to explore is the use of synthetic parallel data. Annotating parallel
data with a statistical parser may lead to quite a lot of noise especially when the domain does not match
the original training data. Starting with noisy source language annotations, the projection algorithm may
transfer errors to the target language that can cause problems for the target language parsing model in-
duced from that data. Using machine translation and the original source language treebanks, we avoid
this kind of error propagation. Furthermore, we suspect that human translations are more difficult to
align on the word level then machine translated data which are inherently based on word alignments and,
therefore, tend to be more literal and consistent (Carpuat and Simard, 2012). Using statistical MT as
our translation model, we can also obtain such alignment as a given output from the decoding process,
which makes it unnecessary to run yet another error-prone process such as automatic word alignment.
Furthermore, the treebank data is too small to be used alone with generative statistical alignment mod-
els. Concatenating the data with larger parallel data would help but domain mismatches may, again,
negatively influence the alignment performance.
In the following, we show the cross-lingual scores obtained by translating all treebanks in the Universal
Treebank to all other languages. We leave out Korean here again, because no SMT training data is
included in Europarl for that language. The translation models are trained on the entire Europarl corpus
using a standard setup for phrase-based SMT and the Moses toolbox for training, tuning and decoding
(Koehn et al., 2007). For tuning we use MERT (Och, 2003) and the newstest 2012 data provided by the
annual workshop on statistical machine translation.
4
and for language modeling, we use a combination of
Europarl and News data provided from the same source. The language model is a standard 5-gram model
estimated from the monolingual data using modified Kneser-Ney smoothing without pruning (applying
KenLM tools (Heafield et al., 2013)).
Table 3 summarizes the labeled attachment scores obtained with our projection approach on synthetic
machine-translated data. The main observation we can make here is that this approach is very robust
with respect to the noise introduced by the translation engine. Automatic translation is a difficult task on
its own but we still achieve results that are similar to the ones from the projection approach on human
translated data. Note that our training data is now much smaller
5
compared to the data sizes used in
Section 4.2 and, still, we outperform those models in several cases. This seems to prove that it can be
a clear advantage to start with gold annotations in the source language and to have a close alignment
between source and target language. An indication for this effect is illustrated by the contrastive jack-
knifing experiments shown in Table 3. The scores are generally lower with two minor exceptions. Note
4
http://www.statmt.org/wmt14
5
Most treebanks includes 2,000-5,000 sentences, except English with about 40,000 sentences.
1860
that this experiment does not cover domain shift problems. Another trend that can be seen in our results
is that some languages such as German are more difficult to translate to (which can be confirmed by the
SMT literature) leading to lower cross-lingual parsing performance.
4.4 The Impact of Word Alignment
Crucial for the success of annotation projection is the quality of the word alignment used to map in-
formation from the source to the target language. Not only alignment errors cause problems but also
ambiguous alignments can lead to projection difficulties as we have discussed before. In the previous
sections, we relied on symmetrized word alignments that are common in the SMT community, which
are based on Viterbi alignments created by the final IBM model 4 in the typical training pipeline. Even
though this is a reasonable setup for training phrase-based SMT models (as presented in the previous
section), the chosen symmetrization heuristics (grow-diag-final-and) may not be well suited for accurate
annotation projection. In particular, it is known that these heuristics focus on recall and tend to add
many additional links that may not be useful for our projection task and even lead to some confusion as
depicted in the example in Figure 3.
In order to investigate the impact of word alignment, we, therefore, decided to look at other sym-
 50
 55
 60
 65
English Spanish French Swedish
LA
S
Source: German
 50
 55
 60
 65
German Spanish French Swedish
LA
S
Source: English
 50
 55
 60
 65
German English French Swedish
LA
S
Source: Spanish
 50
 55
 60
 65
German English Spanish Swedish
LA
S
Source: French
 50
 55
 60
 65
German English Spanish French
LA
S
Source: Swedish
trg-to-src
src-to-trg
intersect
grow
gdfa
Figure 5: The impact of word alignment symmetrization on projection and parsing accuracy. src-to-trg
and trg-to-src refer to the original directional Viterbi word alignments created by IBM model 4 in both
directions; intersect refers to the intersection of both IBM 4 alignments; grow and gdfa (grow-diag-final-
and) refer to popular symmetrization heuristics used in the SMT community.
1861
metrization heuristics and their effect on projection and the quality of the parser model trained on the
projected data. For this, we return to the setup of projecting annotations on human translations using
the Europarl corpus with the same settings as described in Section 4.2 (using 40,000 sentences for the
projection). We now compare five different word alignments based on IBM model 4 trained on the entire
corpus for each language pair. First of all, we look at the original directional word alignment from source
to target language and vice versa. We then include the intersection of these two directional link sets to
represent a symmetrization heuristics that produces very sparse but high precision word alignments. Fi-
nally, we also consider the grow heuristics that adds adjacent alignment points coming from the union of
directional alignment links to the sparse intersection of the same. In this way, the resulting word align-
ment covers most words while keeping precision at a rather high level. All of these alignment types are
then contrasted with the grow-diag-final-and heuristics that we use in our default setup.
Figure 5 plots the parsing performance across languages based on the projection with the various
alignment techniques listed above. A general observation is that the differences are rather small in most
cases. Projecting annotation using the direct correspondence assumption seems to be quite robust with
respect to alignment noise. In our experiments, no specific tendencies can be identified that would
allow to draw immediate conclusions and to give clear recommendations for our task. Somewhat sur-
prisingly we can see that the recall-oriented alignment heuristics (grow-diag-final-and) actually perform
quite well in many cases, leading either to the best performing model or to one that is very close to the
best result. However, in some cases, these models fall behind the ones based on alignment intersec-
tions (for instance Spanish-English) or directional word alignments (for example for Spanish-German,
French-English, Swedish-German). A striking difference can be seen in the annotations projected to
German. There, the target-to-source alignment performs pretty well and outperforms in two cases all
other alignment types in the down-stream task. Furthermore, the intersection falls far behind in three of
these cases, which indicates that both alignment directions are probably very different from each other
leading to a very sparse word alignment when intersecting them. One possible reason for the success of
the directional alignment might be that it favors the mapping to a compounding language such as Ger-
man that frequently requires many-to-one links. However, the same effect cannot be seen for the other
compounding language in our test set, Swedish.
4.5 Parsing Without Golden POS Labels
For a truly unsupported language, it does not make sense to assume a high quality POS tagger. Neverthe-
less, most cross-lingual experiments test their performance on data with human annotated golden POS
labels. This is similar to the tradition of monolingual parsing where test accuracy is measured with per-
fect tokenization and completely correct POS annotation. In practice, this would not be realistic where
new data needs to be parsed without proper tagging and unambiguous tokenization.
Direct transfer models are even more dependent on POS labels as those are the only source of infor-
mation they can work with when making attachment decisions. Annotation projection approaches, on
the other hand, are able to transfer POS information as well, which allows to train tagger models on
projected data. In this section, we would like to test the feasibility of such an idea to see if we can truly
port a parser to a new language without additional assumptions.
The first step is to train tagger models on our projected data sets. For this, we use the translated
treebanks and a simple word-by-word translation approach in which we translate single-word-phrases
only in our standard SMT model. The word-by-word translation model assures that we do not contam-
inate the data with DUMMY nodes and labels even though the translation quality lags behind the more
powerful phrase-based models with larger translation options. We train standard Markov taggers with
suffix backoff using HunPos (Hal?acsy et al., 2007) on each of the projected training data sets from the
Universal Treebank. Table 4 summarizes the performance of all tagger models tested on the test sets in
the treebank. The tagger all use the same universal POS tagset with its 12 labels as used in the Universal
Treebank (Petrov et al., 2012). As we can see, the performance of those taggers is not great but still
rather informative with overall accuracy values around 80%. The drop from source data to projected data
is about 10-15 absolute points, which is, however, quite dramatic. Assuming that this is the best we can
1862
POS DE EN ES FR SV
DE 95.24 73.15 69.31 72.41 79.01
EN 82.04 97.56 79.91 81.23 84.44
ES 77.27 77.43 95.37 83.97 78.26
FR 80.99 78.74 88.47 95.08 79.62
SV 78.40 71.45 70.11 66.77 95.86
DELEXICALIZED MODELS
LAS DE EN ES FR SV
DE ? 33.38 34.37 36.59 39.15
EN 36.55 ? 45.53 47.71 48.92
ES 35.07 39.87 ? 51.40 42.95
FR 35.89 40.40 51.55 ? 40.30
SV 37.87 39.80 43.62 41.61 ?
TRANSLATED TREEBANK MODELS
LAS DE EN ES FR SV
DE ? 41.29 42.16 46.26 46.79
EN 42.24 ? 50.54 53.63 53.78
ES 38.61 43.70 ? 57.58 47.01
FR 42.65 48.37 57.78 ? 45.55
SV 41.37 42.34 49.38 46.00 ?
Table 4: Top (POS): Accuracy of POS tagging models trained on translated treebanks (word-by-word
model). Bottom (LAS): Cross-lingual parser models tested on automatically POS tagged test sets. The
delexicalized baseline (left) and the translated treebank model using word-by-word translation (right).
achieve for the target language, we now have to look at the parsing performance when relying on such
noisy annotation.
Firstly, we look at the delexicalized baselines. The bottom-left part of Table 4 lists the labeled at-
tachment scores when gold POS labels are replaced with automatic tags created by the corresponding
projection tagger. The drop is huge and the original scores that were well above 50-70% go down to
not more than 30-40% LAS. Clearly, this was to be expected as proper POS labeling is crucial for these
models. Let us now look at the annotation projection approach using a translated treebank as our parallel
data set. Table 4 on the bottom-right lists the corresponding labeled attachment scores with automatic
POS tags. As expected, the performance is considerably lower than with golden POS labels, which are
still the most informative features in those models. However, the performance remains in a range of
above 40-50% LAS. Clearly, the lexical features help to keep the performance up at a higher level than
the delexicalized baselines. We believe, that this difference can be crucial when porting language tools
to new languages and that the models can be further optimized to rely less on golden POS tags.
5 Conclusions
In this paper we revisit annotation projection for cross-lingual parser induction. We show that annotation
can successfully be transfered to target languages if the annotation is harmonized across languages.
Despite previous negative results on diverse treebanks we demonstrate that direct projection works very
well for a number of languages and outperforms direct delexicalized transfer models by a large margin.
The approach is also quite robust with respect to word alignment. Furthermore, we show that machine
translation can be a useful alternative for this strategy and that projected data can also be used to induce
basic information such as POS labels in combination with syntactic parser models.
Acknowledgements
This work was supported by the Swedish Research Council (Vetenskapsr?adet), project 2012-916. I would
also like to thank Joakim Nivre,
?
Zeljko Agi?c and the anonymous reviewers for helpful comments and
suggestions.
References
?
Zeljko Agi?c, Danijela Merkler, and Da?sa Berovi?c. 2012. Slovene-Croatian Treebank Transfer Using Bilingual
Lexicon Improves Croatian Dependency Parsing. In Proceedings of IS-LTC 2012, pages 5?9.
1863
Miguel Ballesteros and Joakim Nivre. 2012. MaltOptimizer: An Optimization Tool for MaltParser. In Proceed-
ings of EACL 2012, pages 58?62.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In
Proceedings of CoNLL 2006, pages 149?164.
Marine Carpuat and Michel Simard. 2012. The trouble with smt consistency. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages 442?449, Montr?eal, Canada.
Kuzman Ganchev and Dipanjan Das. 2013. Cross-Lingual Discriminative Learning of Sequence Models with
Posterior Regularization. In Proceedings of EMNLP 2013, pages 1996?2006.
P?eter Hal?acsy, Andr?as Kornai, and Csaba Oravecz. 2007. Poster paper: Hunpos ? an open source trigram tagger.
In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions, pages 209?212, Prague, Czech Republic.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable Modified Kneser-Ney
Language Model Estimation. In Proceedings of ACL 2013, pages 690?696.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping Parsers via
Syntactic Projection across Parallel Texts. Natural Language Engineering, 11(3):311?325.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Christopher J. Dyer, Ond?rej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of
ACL 2007, pages 177?180.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-Source Transfer of Delexicalized Dependency Parsers.
In Proceedings of EMNLP 2011, pages 62?72.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,
Keith Hall, Slav Petrov, Hao Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria Bertomeu Castell?o, and Jungmee
Lee. 2013. Universal Dependency Annotation for Multilingual Parsing. In Proceedings of ACL 2013, pages
92?97.
Tahira Naseem, Regina Barzilay, and Amir Globerson. 2012. Selective Sharing for Multilingual Dependency
Parsing. In Proceedings of ACL 2012, pages 629?637.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. MaltParser: A Data-Driven Parser-Generator for Dependency
Parsing. In Proceedings of LREC 2006, pages 2216?2219.
Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?52.
Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL
2003, pages 160?167.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A Universal Part-of-Speech Tagset. In Proceedings of
LREC 2012, pages 2089?2096.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual Word Clusters for Direct Transfer
of Linguistic Structure. In Proceedings of NAACL 2012, pages 477?487.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013a. Token and Type
Constraints for Cross-lingual Part-of-speech Tagging. Transactions of the Association for Computational Lin-
guistics, 1:1?12.
Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre. 2013b. Target Language Adaptation of Discriminative
Transfer Parsers. In Proceedings of NAACL 2013, pages 1061?1071.
J?org Tiedemann,
?
Zeljko Agi?c, and Joakim Nivre. 2014. Treebank translation for cross-lingual parser induction.
In Proceedings of the 18th Conference Natural Language Processing and Computational Natural Language
Learning (CoNLL), Baltimore, Maryland, USA.
David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing Multilingual Text Analysis Tools via
Robust Projection Across Aligned Corpora. In Proceedings of HLT 2011, pages 1?8.
Yue Zhang and Joakim Nivre. 2011. Transition-based Dependency Parsing with Rich Non-local Features. In
Proceedings of ACL 2011, pages 188?193.
1864
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 130?140,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Treebank Translation for Cross-Lingual Parser Induction
J
?
org Tiedemann
Dep. of Linguistics and Philology
Uppsala University
jorg.tiedemann@lingfil.uu.se
?
Zeljko Agi
?
c
Linguistics Department
University of Potsdam
zagic@uni-potsdam.de
Joakim Nivre
Dep. of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
Cross-lingual learning has become a popu-
lar approach to facilitate the development
of resources and tools for low-density lan-
guages. Its underlying idea is to make
use of existing tools and annotations in
resource-rich languages to create similar
tools and resources for resource-poor lan-
guages. Typically, this is achieved by either
projecting annotations across parallel cor-
pora, or by transferring models from one or
more source languages to a target language.
In this paper, we explore a third strategy
by using machine translation to create syn-
thetic training data from the original source-
side annotations. Specifically, we apply
this technique to dependency parsing, us-
ing a cross-lingually unified treebank for
adequate evaluation. Our approach draws
on annotation projection but avoids the use
of noisy source-side annotation of an unre-
lated parallel corpus and instead relies on
manual treebank annotation in combination
with statistical machine translation, which
makes it possible to train fully lexicalized
parsers. We show that this approach signif-
icantly outperforms delexicalized transfer
parsing.
1 Introduction
The lack of resources and tools is a serious problem
for the majority of the world?s languages (Bender,
2013). Many applications require robust tools and
the development of language-specific resources is
expensive and time consuming. Furthermore, many
tasks such as data-driven syntactic parsing require
strong supervision to achieve reasonable results
for real-world applications, since the performance
of fully unsupervised methods lags behind by a
large margin in comparison with the state of the
art. Cross-lingual learning has been proposed as
one possible solution to quickly create initial tools
for languages that lack the appropriate resources
(Ganchev and Das, 2013). By and large, there
are two main strategies that have been proposed
in the literature: annotation projection and model
transfer.
1.1 Previous Cross-Lingual Approaches
Annotation projection relies on the mapping of lin-
guistic annotation across languages using paral-
lel corpora and automatic alignment as basic re-
sources (Yarowsky et al., 2001; Hwa et al., 2005;
T?ackstr?om et al., 2013a). Tools that exist for the
source language are used to annotate the source
side of the corpus and projection heuristics are then
applied to map the annotation through word align-
ment onto the corresponding target language text.
Target language tools can then be trained on the
projected annotation assuming that the mapping is
sufficiently correct. Less frequent, but also possi-
ble, is the scenario where the source side of the cor-
pus contains manual annotation (Agi?c et al., 2012).
This addresses the problem created by projecting
noisy annotations, but it presupposes parallel cor-
pora with manual annotation, which are rarely avail-
able, and expensive and time-consuming to pro-
duce.
Model transfer instead relies on universal fea-
tures and model parameters that can be transferred
from one language to another. Abstracting away
from all language-specific parameters makes it pos-
sible to train, e.g., delexicalized parsers that ignore
lexical information. This approach has been used
with success for a variety of languages, drawing
from a harmonized POS tagset (Petrov et al., 2012)
that is used as the main source of information. One
advantage compared to annotation projection is
that no parallel data is required. In addition, train-
ing can be performed on gold standard annotation.
However, model transfer assumes a common fea-
130
ture representation across languages (McDonald et
al., 2013), which can be a strong bottleneck. Sev-
eral extensions have been proposed to make the
approach more robust. First of all, multiple source
languages can be involved to increase the statistical
basis for learning (McDonald et al., 2011; Naseem
et al., 2012), a strategy that can also be used in
the case of annotation projection. Cross-lingual
word clusters can be created to obtain additional
universal features (T?ackstr?om et al., 2012). Tech-
niques for target language adaptation can be used
to improve model transfer with multiple sources
(T?ackstr?om et al., 2013b).
1.2 The Translation Approach
In this paper, we propose a third strategy, based
on automatically translating training data to a new
language in order to create annotated resources di-
rectly from the original source. Recent advances
in statistical machine translation (SMT) combined
with the ever-growing availability of parallel cor-
pora are now making this a realistic alternative. The
relation to annotation projection is obvious as both
involve parallel data with one side being annotated.
However, the use of direct translation brings two
important advantages. First of all, using SMT, we
do not accumulate errors from two sources: the tool
? e.g., tagger or parser ? used to annotate the source
language of a bilingual corpus and the noise com-
ing from alignment and projection. Instead, we use
the gold standard annotation of the source language
which can safely be assumed to be of much higher
quality than any automatic annotation obtained by
using a tool trained on that data. Moreover, using
SMT may help in bypassing domain shift problems,
which are common when applying tools trained
(and evaluated) on one resource to text from an-
other domain. Secondly, we can assume that SMT
will produce output that is much closer to the input
than manual translations in parallel texts usually
are. Even if this may seem like a short-coming
in general, in the case of annotation projection it
should rather be an advantage, because it makes it
more straightforward and less error-prone to trans-
fer annotation from source to target. Furthermore,
the alignment between words and phrases is inher-
ently provided as an output of all common SMT
models. Hence, no additional procedures have to be
performed on top of the translated corpus. Recent
research (Zhao et al., 2009; Durrett et al., 2012)
has attempted to address synthetic data creation
for syntactic parsing via bilingual lexica. We seek
to build on this work by utilizing more advanced
translation techniques.
Further in the paper, we first describe the tools
and resources used in our experiments (?2). We
elaborate on our approach to translating treebanks
(?3) and projecting syntactic annotations (?4) for a
new language. Finally, we provide empirical evalu-
ation of the suggested approach (?5) and observe
a substantial increase in parsing accuracy over the
delexicalized parsing baselines.
2 Resources and Tools
In our experiments, we rely on standard resources
and tools for both dependency parsing and ma-
chine translation without any special enhancements.
Since we are primarily trying to provide a proof
of concept for the use of SMT-derived synthetic
training data in dependency parsing, we believe it
is more important to facilitate reproducibility than
to tweak system components to obtain maximum
accuracy.
We use the Universal Dependency Treebank v1
(McDonald et al., 2013) for annotation projection,
parser training and evaluation. It is a collection
of data sets with consistent syntactic annotation
for six languages: English, French, German, Ko-
rean, Spanish, and Swedish.
1
The annotation is
based on Stanford Typed Dependencies for English
(De Marneffe et al., 2006) but has been adapted
and harmonized to allow adequate annotation of
typologically different languages. This is the first
collection of data sets that allows reliable evalua-
tion of labeled dependency parsing accuracy across
multiple languages (McDonald et al., 2013). We
use the dedicated training and test sets from the
treebank distribution in all our experiments. As ar-
gued in (McDonald et al., 2013), most cross-lingual
dependency parsing experiments up to theirs relied
on heterogeneous treebanks such as the CoNLL
datasets for syntactic dependency parsing (Buch-
holz and Marsi, 2006; Nivre et al., 2007a), mak-
ing it difficult to address challenges like consistent
cross-lingual analysis for downstream applications
and reliable cross-lingual evaluation of syntactic
parsers. More specifically, none of the previous
research could report full labeled parsing accura-
cies, but rather just unlabeled structural accuracies
across different attachment schemes. Following
the line of McDonald et al. (2013) regarding the
1
https://code.google.com/p/uni-dep-tb/
131
emphasized importance of homogenous data and
the assignment of labels, we only report labeled
attachment scores (LAS) in all our experiments.
As it is likely the first reliable cross-lingual pars-
ing evaluation, we also choose their results as the
baseline reference point for comparison with our
experiments.
For dependency parsing, we use MaltParser
(Nivre et al., 2006a)
2
due to its efficiency in both
training and parsing, and we facilitate MaltOpti-
mizer (Ballesteros and Nivre, 2012)
3
to bypass the
tedious task of manual feature selection. Malt-
Parser is a transition-based dependency parser
that has been evaluated on a number of different
languages with competitive results (Nivre et al.,
2006b; Nivre et al., 2007b; Hall et al., 2007) and it
is widely used for benchmarking and application
development. Although more accurate dependency
parsers exist for the task of monolingual supervised
parsing, it is not clear that these differences carry
over to the cross-lingual scenario, where baselines
are lower and more complex models are more likely
to overfit. The use of a transition-based parser also
facilitates comparison with delexicalized transfer
parsing, where transition-based parsers are domi-
nant so far (McDonald et al., 2011; McDonald et
al., 2013). We leave the exploration of additional
parsing approaches for future research.
For machine translation, we select the popular
Moses toolbox (Koehn et al., 2007) and the phrase-
based translation paradigm as our basic frame-
work. Phrase-based SMT has the advantage of
being straightforward and efficient in training and
decoding, while maintaining robustness and relia-
bility for many language pairs. More details about
the setup and the translation procedures are given
in Section 3 below. The most essential ingredient
for translation performance is the parallel corpus
used for training the translation models. For our
experiments we use the freely available and widely
used Europarl corpus v7 (Koehn, 2005).
4
It is com-
monly used for training SMT models and includes
parallel data for all languages represented in the
Universal Treebank except Korean, which we will,
therefore, leave out in our experiments. For tuning
we apply the newstest 2012 data provided by the an-
nual workshop on statistical machine translation.
5
For language modeling, we use a combination of
2
http://www.maltparser.org/
3
http://nil.fdi.ucm.es/maltoptimizer/
4
http://www.statmt.org/europarl/
5
http://www.statmt.org/wmt14
DE EN ES FR SV
DE 94 M 94 M 96 M 81 M
EN 2.0 M 103 M 105 M 89 M
ES 1.9 M 2.0 M 104 M 89 M
FR 1.9 M 2.0 M 2.0 M 91 M
SV 1.8 M 1.9 M 1.8 M 1.9 M
mono 22.9 M 17.1 M 6.3 M 6.3 M 2.3 M
Table 1: Parallel data and monolingual data used
for training the SMT models. Lower-left triangle
= number of sentence pairs; upper-right triangle
= number of tokens (source and target language
together); bottom row = number of sentences in
monolingual corpora.
Europarl and News data provided from the same
source. The statistics of the corpora are given in
Table 1.
3 Translating Treebanks
The main contribution of this paper is the empirical
study of automatic treebank translation for parser
transfer. We compare three different translation
approaches in order to investigate the influence of
several parameters. All of them are based on auto-
matic word alignment and subsequent extraction of
translation equivalents as common in phrase-based
SMT. In particular, word alignment is performed us-
ing GIZA++ (Och and Ney, 2003) and IBM model
4 as the final model for creating the Viterbi word
alignments for all parallel corpora used in our ex-
periments. For the extraction of translation tables,
we use the Moses toolkit with its standard settings
to extract phrase tables with a maximum of seven
tokens per phrase from a symmetrized word align-
ment. Symmetrization is done using the grow-diag-
final-and heuristics (Koehn et al., 2003). We tune
phrase-based SMT models using minimum error
rate training (Och, 2003) and the development data
for each language pair. The language model is a
standard 5-gram model estimated from the mono-
lingual data using modified Kneser-Ney smoothing
without pruning (applying KenLM tools (Heafield
et al., 2013)).
Our first translation approach is based on a very
simple word-by-word translation model. For this,
we select the most reliable translations of single
words from the phrase translation tables extracted
from the parallel corpora as described above. We
restrict the model to tokens with alphabetic char-
acters only using pre-defined Unicode character
132
sets. The selection of translation alternatives is
based on the Dice coefficient, which combines the
two essential conditional translation probabilities
given in the phrase table. The Dice coefficient is in
fact the harmonic mean of these two probabilities
and has successfully been used for the extraction of
translation equivalents before (Smadja et al., 1996):
Dice(s, t) =
2 p(s, t)
p(s) + p(t)
= 2
(
1
p(s|t)
+
1
p(t|s)
)
?1
Other association measures would be possible as
well but Smadja et al. (1996) argue that the Dice
coefficient is more robust with respect to low fre-
quency events than other common metrics such as
pointwise mutual information, which can be a seri-
ous issue with the unsmoothed probability estima-
tions in standard phrase tables. Our first translation
model then applies the final one-to-one correspon-
dences to monotonically translate treebanks word
by word. We refer to it as the LOOKUP approach.
Note that any bilingual dictionary could have been
used to perform the same procedure.
The second translation approach (WORD-BASED
MT) is slightly more elaborate but still restricts
the translation model to one-to-one word mappings.
For this, we extract all single word translation pairs
from the phrase tables and apply the standard beam-
search decoder implemented in Moses to translate
the original treebanks to all target languages. The
motivation for this model is to investigate the im-
pact of reordering and language models while still
keeping the projection of annotated data as simple
as possible. Note that the language model may
influence not only the word order but also the lex-
ical choice as we now allow multiple translation
options in our phrase table.
The final model implements translation based
on the entire phrase table using the standard ap-
proach to PHRASE-BASED SMT. We basically run
the Moses decoder with default settings and the pa-
rameters and models trained on our parallel corpora.
Note that it is important for the annotation trans-
fer to keep track of the alignment between phrases
and words of the input and output sentences. The
Moses decoder provides both, phrase segmentation
and word alignment (if the latter is coded into the
phrase tables). This will be important as we will
see in the annotation projection discussed below.
ORIGINAL
DE EN ES FR SV
14.0 0.00 7.90 13.3 4.20
WORD-BASED MT
DE EN ES FR SV
DE ? 49.1 62.6 52.8 60.4
EN 43.3 ? 27.6 34.8 0.00
ES 54.9 25.1 ? 12.3 18.3
FR 68.2 39.6 32.8 ? 57.8
SV 34.1 5.20 21.6 33.7 ?
PHRASE-BASED MT
DE EN ES FR SV
DE ? 51.5 57.3 58.8 46.8
EN 49.3 ? 50.3 61.7 14.6
ES 65.9 66.7 ? 62.8 49.0
FR 58.0 53.7 44.7 ? 38.2
SV 43.9 43.6 49.6 57.1 ?
Table 2: Non-projectivity in synthetic treebanks.
4 Transferring Annotation
The next step in preparing synthetic training data is
to project the annotation from the original treebank
to the target language. Given the properties of a
dependency tree, where every word has exactly one
syntactic head and dependency label, the annota-
tion transfer is trivial for the two initial translation
models. All annotation can simply be copied us-
ing the dictionary LOOKUP in which we enforce
a monotonic one-to-one word mapping between
source and target language.
In the second approach, we only have to keep
track of reordering, which is reported by the de-
coder when translating with our model. Note that
the mapping is strictly one-to-one (bijective) as
phrase-based SMT does not allow deletions or in-
sertions at any point. This also ensures that we
will always maintain a tree structure even though
reordering may have a strong impact on projectiv-
ity (see Table 2). An illustration of this type of
annotation transfer is shown in the left image of
Figure 1.
The third model, full PHRASE-BASED SMT, re-
quires the most attention when transferring anno-
tation across languages. Here we have to rely on
the alignment information and projection heuris-
tics similar to the ones presented in related work
(Hwa et al., 2005). In their work, Hwa et al. (2005)
define a direct projection algorithm that transfers
automatic annotation to a target language via word
alignment. The algorithm defines a number of
133
CO
NJ
NO
UN
PR
ON
VE
RB
AD
P
NO
UN
.
Qu
e
Die
u
lui
vie
nne
en
aid
e
!
Th
at
Go
d
hel
p
him
com
e
in
!
CO
NJ
NO
UN
NO
UN
PR
ON
VE
RB
AD
P
.
exp
l ns
ub
j io
bj
roo
t ad
pm
od
adp
ob
j
p
exp
lns
ub
j
adp
ob
j
iob
j
roo
tad
pm
od p
CO
NJ
NO
UN
PR
ON
VE
RB
AD
P
NO
UN
.
Qu
e
Die
u
lui
vie
nne
en
aid
e
!
Go
d
DU
MM
Y
hel
p
DU
MM
Y
DU
MM
Y
him
!
NO
UN
CO
NJ
VE
RB
AD
P
NO
UN
PR
ON
.
exp
l n
sub
j io
bj
roo
t
adp
mo
d
adp
ob
j
p
nsu
bj
exp
l
roo
ta
dpm
od
adp
ob
j
iob
j p
CO
NJ
NO
UN
PR
ON
VE
RB
AD
PN
OU
N
.
Qu
e
Die
u
lui
vie
nne
en
aid
e
!
Go
d
hel
p
him
!
NO
UN
VE
RB
PR
ON
.
exp
l ns
ub
j io
bj
roo
t ad
pm
od
adp
ob
j
p
nsu
bj
roo
t
iob
j
p
Figure 1: Transferring annotation from French to an English translation with a WORD-BASED translation
model (left) and with a PHRASE-BASED translation model (middle and right). Annotation projection using
the Direct Projection Algorithm by Hwa et al. (2005) (middle) and our approach (right).
heuristics to handle unaligned, one-to-many, many-
to-one and many-to-many alignments. As a side ef-
fect, this approach produces several dummy-nodes
in the target language to ensure a complete pro-
jection of the source language tree (see Hwa et al.
(2005) for more details).
In our approach, we try to make use of the addi-
tional information provided by the SMT decoder to
avoid dummy-nodes and relations that may nega-
tively influence the induced target language parser.
Compared to the annotation projection approach
of Hwa et al. (2005), the situation in our PHRASE-
BASED SMT setting is slightly different. Here, we
have two types of alignments that can be considered
when relating source and target language items: (i)
the alignment between phrases (pairs of consec-
utive n-grams) and (ii) the phrase-internal word
alignment on which phrase extraction is based. The
primary information used for annotation transfer
is still the latter which has the same properties as
described by Hwa et al. (2005) (except that we have
truly many-to-many alignments in our data which
were not available in their experiments).
Note that words may be unaligned in phrase-
based SMT as the phrase extraction algorithm used
in Moses includes unaligned adjacent tokens. How-
ever, for these unaligned words, we know to which
phrase they belong and can also identify the corre-
sponding phrase in the other language using phrase
alignment information. This makes it possible to
avoid the creation of dummy-nodes altogether and
instead to link unaligned words to existing nodes
based on the given phrase segmentation.
Similarly, we define heuristics for handling one-
to-many, many-to-one and many-to-many align-
ments that avoid the creation of dummy-nodes. The
main procedure is illustrated in Figure 2.
The key feature of this projection algorithm is
that ambiguous alignments are handled by attach-
ing words to the nodes that are highest up in the
dependency tree (the procedure find highest() re-
turns the node with minimum distance to the root
of the tree). This ensures that we avoid cycles
and isolated cliques in the graph. Furthermore,
unaligned words are attached to the head of the
target phrase they belong to, which seems to be the
most appropriate place without further knowledge.
The procedures in trg phrase() and in src phrase()
make use of the phrase segmentation used in the
translation process.
One complication is the search for the corre-
sponding target head word in cases where the
source language head is not aligned or aligned to
multiple target language words. Figure 3 shows
the head alignment procedure that we define in our
projection algorithm. Procedure find aligned() re-
turns the rightmost word of all words aligned to the
given source language word s. Other heuristics or
linguistically motivated rules based on POS tags
and general language properties would be possible
here as well. If s is not aligned, we move up in
the dependency tree until we hit ROOT or find an
aligned word. If we are at the root position we
return ROOT as this does not require further map-
pings. The effect of this algorithm is illustrated by
the right-hand side image in Figure 1.
5 Parsing Across Languages
In this section, we present the results of two ex-
perimental batches. First, we establish the base-
134
Input: source tree S, target sentence T ,
word alignment A, phrase segmentation P
Output: syntactic heads head[],
word attributes attr[]
1 treeSize = max distance to root(S) ;
2 attr = [] ;
3 head = [] ;
4 for t ? T do
5 if is unaligned trg(t,A) then
6 for t? ? in trg phrase(t,P) do
7 [s
x
,..,s
y
] = aligned to(t?) ;
8 ?s = find highest([s
x
,..,s
y
],S) ;
9
?
t = find aligned(?s,S,T,A) ;
10 attr[t] = DUMMY ;
11 head[t] =
?
t ;
12 end
13 else
14 [s
x
,..,s
y
] = aligned to(t) ;
15 s = find highest([s
x
,..,s
y
],S) ;
16 attr[t] = attr(s) ;
17 ?s = head of(s,S) ;
18
?
t = find aligned(?s,S,T,A) ;
19 if
?
t == t then
20 [s
x
,..,s
y
] = in src phrase(s,P) ;
21 s* = find highest([s
x
,..,s
y
],S) ;
22 ?s = head of(s*,S) ;
23
?
t = find aligned(?s,S,T,A) ;
24 head[t] =
?
t ;
25 end
26 end
27 end
Figure 2: Annotation projection algorithm.
lines by comparing monolingual supervised pars-
ing to delexicalized transfer parsing following the
approach of McDonald et al. (2013). Second, we
present the results obtained with parsers trained
on target language treebanks produced using ma-
chine translation and annotation projection. Here,
we also look at delexicalized models trained on
translated treebanks to show the effect of machine
translation without additional lexical features.
5.1 Baseline Results
First we present the baseline parsing scores. The
baselines we explore are: (i) the monolingual base-
line, i.e., training and testing using the same lan-
guage data from the Universal Dependency Tree-
bank and (ii) the delexicalized baseline, i.e., apply-
ing delexicalized parsers across languages.
For the monolingual baseline, MaltParser mod-
els are trained on the original treebanks with uni-
versal POS labels and lexical features but leaving
out other language-specific features if they exist in
the original treebanks. The delexicalized parsers
are trained on universal POS labels only for each
language and are then applied to all other languages
Input: node s, source tree S with root ROOT,
target sentence T , word alignment A
Output: node t*
1 if s == ROOT then
2 return ROOT ;
3 end
4 while is unaligned src(s,A) do
5 s = head of(s,S) ;
6 if s == ROOT then
7 return ROOT ;
8 end
9 end
10 p = 0 ;
11 t* = undef ;
12 for t? ? aligned(s,A) do
13 if position(t?,T) > p then
14 t* = t? ;
15 p = position(t?,T) ;
16 end
17 end
18 return t* ;
Figure 3: Procedure find aligned().
without modification. For all models, features and
options are optimized using MaltOptimizer. The
accuracy is given in Table 3 as a set of labeled at-
tachment scores (LAS). We include punctuation
in our evaluation. Ignoring punctuation generally
leads to slightly higher scores as we have noted in
our experiments but we do not report those num-
bers here. Note also that the columns represent the
target languages (used for testing), while the rows
denote the source languages (used in training), as
in McDonald et al. (2013).
From the table, we can see that the baseline
scores are compatible with the ones in the orig-
inal experiments presented by (McDonald et al.,
2013), included in Table 3 for reference. The dif-
ferences are due to parser selection, as they use a
transition-based parser with beam search and per-
ceptron learning along the lines of Zhang and Nivre
(2011) whereas we rely on greedy transition-based
parsing with linear support vector machines. In the
following, we will compare results to our baseline
as we have a comparable setup in those experi-
ments. However, most improvements shown below
also apply in comparison with (McDonald et al.,
2013).
5.2 Translated Treebanks
Now we turn to the experiments on translated tree-
banks. We consider two setups. First, we look at
the effect of translation when training delexical-
ized parsers. In this way, we can perform a direct
comparison to the baseline performance presented
135
MONOLINGUAL
DE EN ES FR SV
72.13 87.50 78.54 77.51 81.28
DELEXICALIZED
DE EN ES FR SV
DE 62.71 43.20 46.09 46.09 50.64
EN 46.62 77.66 55.65 56.46 57.68
ES 44.03 46.73 68.21 57.91 53.82
FR 43.91 46.75 59.65 67.51 52.01
SV 50.69 49.13 53.62 51.97 70.22
MCDONALD ET AL. (2013)
DE EN ES FR SV
DE 64.84 47.09 48.14 49.59 53.57
EN 48.11 78.54 56.86 58.20 57.04
ES 45.52 47.87 70.29 63.65 53.09
FR 45.96 47.41 62.56 73.37 52.25
SV 52.19 49.71 54.72 54.96 70.90
Table 3: Baselines ? labeled attachment score
(LAS) for monolingual and delexicalized transfer
parsing. Delexicalized transfer parsing results of
McDonald et al. (2013) included for reference.
above. The second setup then considers fully lexi-
calized models trained on translated treebanks. The
main advantage of the translation approach is the
availability of lexical information and this final
setup represents the real power of this approach.
In it, we compare lexicalized parsers trained on
translated treebanks with their delexicalized coun-
terparts and avoid a direct comparison with the
delexicalized baselines as they involve different
types of features.
5.3 Delexicalized Parsers
Table 4 presents the scores obtained by training
delexicalized parsing models on synthetic data cre-
ated by our translation approaches presented earlier.
Feature models and training options are the same
as for the delexicalized source language models
when training and testing on the target language
data. Note that we exclude the simple dictionary
LOOKUP approach here, because this approach
leads to identical models as the basic delexicalized
models. This is because words are translated one-
to-one without any reordering which leads to ex-
actly the same annotation sequences as the source
language treebank after projecting POS labels and
dependency relations.
From the table, we can see that all but one model
improve the scores obtained by delexicalized base-
line models. The improvements are quite substan-
tial up to +6.38 LAS. The boost in performance
WORD-BASED MT
DE EN ES FR SV
DE ? 48.12
(4.92)
50.84
(4.75)
52.92
(6.83)
55.52
(4.88)
EN 49.53
(2.91)
? 57.41
(1.76)
58.53
(2.07)
57.82
(0.14)
ES 45.48
(1.45)
48.46
(1.73)
? 58.29
(0.38)
55.25
(1.43)
FR 46.59
(2.68)
47.88
(1.13)
59.72
(0.07)
? 52.31
(0.30)
SV 52.16
(1.47)
49.14
(0.01)
56.50
(2.88)
56.71
(4.74)
?
PHRASE-BASED MT
DE EN ES FR SV
DE ? 45.43
(2.23)
47.26
(1.17)
49.14
(3.05)
53.37
(2.73)
EN 49.16
(2.54)
? 57.12
(1.47)
58.23
(1.77)
58.23
(0.55)
ES 46.75
(2.72)
46.82
(0.09)
? 58.22
(0.31)
54.14
(0.32)
FR 48.02
(4.11)
49.06
(2.31)
60.23
(0.58)
? 55.24
(3.23)
SV 50.96
(0.27)
46.12
?3.01
55.95
(2.33)
54.71
(2.74)
?
Table 4: Translated treebanks: labeled attachment
score (LAS) for delexicalized parsers trained on
synthetic data created by translation. Numbers in
superscript show the absolute improvement over
our delexicalized baselines.
is especially striking for the simpleWORD-BASED
translation model considering that the only differ-
ence to the baseline model is word order. The
impact of the more complex PHRASE-BASED trans-
lation model is, however, difficult to judge. In
14 out of 20 models it actually leads to a drop in
LAS when applying phrase-based translation in-
stead of single-word translation. This is somewhat
surprising but is probably related to the additional
ambiguity in annotation projection introduced by
many-to-many alignments. The largest drop can be
seen for Swedish translated to English, which even
falls behind the baseline performance when using
the PHRASE-BASED translation model.
5.4 Lexicalized Parsers
The final experiment is concerned with lexical
parsers trained on translated treebanks. The main
objective here is to test the robustness of fully lexi-
calized models trained on noisy synthetic data cre-
ated by simple automatic translation engines. Ta-
ble 5 lists the scores obtained by our models when
trained on treebanks translated with our three ap-
proaches (dictionary LOOKUP, WORD-BASED MT
and full PHRASE-BASED translation). Again, we
use the same feature model and training options as
for the source language model when training mod-
els for the target languages. This time, of course,
this refers to the features used by the lexicalized
baseline models.
The capacity of the parsing models increases due
to the lexical information which is now included.
In order to see the effect of lexicalization, we com-
136
DE
T
DE
T
NO
UN
VE
RB
AD
P
NO
UN
CO
NJ
AD
P
DE
T
NO
UN
AD
J
.
To
us
ses
pr
od
uit
s
so
nt
de
qu
ali
te?
et
d?
un
e
fra
ich
eu
re
xe
mp
lai
res
.
Al
l
his
pr
od
uc
ts
ar
e
hig
h-
qu
ali
ty
an
d
a
co
ld
mu
lle
t
co
pie
s
.
DE
T
DE
T
NO
UN
VE
RB
NO
UN
AD
P
CO
NJ
DE
T
NO
UN
NO
UN
AD
J
.
de
t p
os
s
ns
ub
j
ro
ot
ad
pm
od
ad
po
bj
cc
co
nj
de
t
ad
po
bj
am
od
p
de
tp
os
s
ns
ub
j
ro
ot
ad
po
bj
ad
pm
od
cc
de
t
ad
po
bj a
dp
ob
j
am
od
p
Figure 4: Problematic annotation projection with ambiguous word alignment.
pare the performance now with the corresponding
delexicalized models. Note that the LOOKUP ap-
proach relates to the delexicalized baseline models
without any translation.
As we can see, all models outperform their cor-
responding delexicalized version (with one excep-
tion), which demonstrates the ability of the training
procedure to pick up valuable lexical information
from the noisy translations. Again, we can see
substantial absolute improvements of up to +7.31
LAS showing the effectiveness of the translation
approach. Note that this also means that we outper-
form the delexicalized baselines in all cases by a
large margin, even if we should not directly com-
pare these models as they draw on different fea-
ture sets. Once again, we can also see that the
very simple methods are quite successful. Even the
very basic LOOKUP approach leads to significant
improvements with one minor exception. Surpris-
ingly, no gain can be seen with the PHRASE-BASED
translation approach. The translation quality is cer-
tainly better when manually inspecting the data.
However, the increased complexity of annotation
projection seems to pull down the parsers induced
on that kind of data. A question for future work
is whether the performance of those models can
be improved by better projection algorithms and
heuristics that lead to cleaner annotations of other-
wise better translations of the original treebanks.
One possible reason for this disappointing re-
sult could be the unreliable mapping of POS labels
across many-to-many alignments. Figure 4 illus-
trates a typical case of link ambiguity that leads to
erroneous projections. For example, the mapping
of the label ADP onto the English word quality is
due to the left-to-right procedure applied in our pro-
jection algorithm and the mapping of the NOUN
label to the English adjective cold is due to the
link to fraicheur. How much these errors effect our
parsing models trained on the projected treebanks
is difficult to estimate and further investigations are
required to pinpoint these issues and to find ways
of addressing problems that may occur in various
contexts.
Nevertheless, the overall results are very positive.
The experiments clearly show the potentials of the
translation approach. Note that this paper presents
the first attempt to study the effect of translation on
cross-lingual parser induction. Further optimiza-
tion of the translation process and the connected
annotation projection procedures should lead to
further improvements over our basic models.
6 Conclusions and Future Work
In this paper, we have addressed the problem of
cross-lingual parser induction by using statistical
machine translation to create synthetic training data.
Our SMT approach avoids the noisy source-side
137
LOOKUP
DE EN ES FR SV
DE ? 48.63
(5.43)
52.66
(6.57)
52.06
(5.97)
58.78
(8.14)
EN 48.59
(1.97)
? 57.79
(2.14)
57.80
(1.34)
62.21
(4.53)
ES 47.36
(3.33)
49.13
(2.40)
? 62.24
(4.33)
57.50
(3.68)
FR 47.57
(3.66)
54.06
(7.31)
66.31
(6.66)
? 57.73
(5.72)
SV 51.88
(1.19)
48.84
(0.29)
54.74
(1.12)
52.95
(0.98)
?
WORD-BASED MT
DE EN ES FR SV
DE ? 51.86
(3.74)
55.90
(5.06)
57.77
(4.85)
61.65
(6.13)
EN 53.80
(4.27)
? 60.76
(3.35)
63.32
(4.79)
62.93
(5.11)
ES 49.94
(4.46)
49.93
(1.47)
? 65.60
(7.31)
59.22
(3.97)
FR 52.07
(5.48)
54.44
(6.56)
65.63
(5.91)
? 57.67
(5.36)
SV 53.18
(1.02)
50.91
(1.77)
60.82
(4.32)
59.14
(2.43)
?
PHRASE-BASED MT
DE EN ES FR SV
DE ? 50.89
(5.46)
52.54
(5.28)
54.99
(5.85)
59.46
(6.09)
EN 53.71
(4.55)
? 60.70
(3.58)
62.89
(4.66)
64.01
(5.78)
ES 49.59
(2.84)
48.35
(1.53)
? 64.88
(6.66)
58.99
(4.85)
FR 51.83
(3.81)
53.81
(4.75)
65.55
(5.32)
? 59.01
(3.77)
SV 53.22
(2.26)
49.06
(2.94)
58.41
(2.46)
58.04
(3.33)
?
Table 5: Translated treebanks: labeled attachment score (LAS) for lexicalized parsers trained on synthetic
data. Numbers in superscript show the absolute improvements over the delexicalized models based on the
same translation strategy.
annotations of traditional annotation projection and
makes it possible to train fully lexicalized target lan-
guage models that significantly outperform delexi-
calized transfer parsers. We have also demonstrated
that translation leads to better delexicalized models
that can directly be compared with each other as
they are based on the same feature space.
We have compared three SMT methods for syn-
thesizing training data: LOOKUP-based translation,
WORD-BASED translation and full PHRASE-BASED
translation. Our experiments show that even noisy
data sets and simple translation strategies can be
used to achieve positive results. For all three ap-
proaches, we have recorded substantial improve-
ments over the state of the art in labeled cross-
lingual parsing (McDonald et al., 2013). According
to our results, simple word-by-word translations
are often sufficient to create reasonable translations
to train lexicalized parsers on. More elaborated
phrase-based models together with advanced anno-
tation projection strategies do not necessarily lead
to any improvements.
As future work, we want to improve our model
by (i) studying the impact of other SMT properties
and improve the quality of treebank translation,
(ii) implementing more sophisticated methods for
annotation projection and (iii) using n-best lists
provided by SMT models to introduce additional
synthetic data using a single resource. We also aim
at (iv) applying our approach to transfer parsing
for closely related languages (see Agi?c et al. (2012)
and Zeman and Resnik (2008) for related work),
(v) testing it in a multi-source transfer scenario
(McDonald et al., 2011) and, finally, (vi) comparing
different dependency parsing paradigms within our
experimental framework.
Multi-source approaches are especially appeal-
ing using the translation approach. However, initial
experiments (which we omit in this presentation)
revealed that simple concatenation is not sufficient
to obtain results that improve upon the single-best
translated treebanks. A careful selection of appro-
priate training examples and their weights given
to the training procedure seems to be essential to
benefit from different sources.
7 Acknowledgements
This work was supported by the Swedish Research
Council (Vetenskapsr?adet) through the project on
Discourse-Oriented Machine Translation (2012-
916).
138
References
?
Zeljko Agi?c, Danijela Merkler, and Da?sa Berovi?c.
2012. Slovene-Croatian Treebank Transfer Using
Bilingual Lexicon Improves Croatian Dependency
Parsing. In Proceedings of IS-LTC 2012, pages 5?
9.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOp-
timizer: An Optimization Tool for MaltParser. In
Proceedings of EACL 2012, pages 58?62.
Emily M. Bender. 2013. Linguistic Fundamentals for
Natural Language Processing: 100 Essentials from
Morphology and Syntax. Morgan & Claypool Pub-
lishers.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
In Proceedings of CoNLL 2006, pages 149?164.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of LREC 2006, pages 449?454.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic Transfer Using a Bilingual Lexicon. In Pro-
ceedings of EMNLP-CoNLL 2012, pages 1?11.
Kuzman Ganchev and Dipanjan Das. 2013. Cross-
Lingual Discriminative Learning of Sequence Mod-
els with Posterior Regularization. In Proceedings of
EMNLP 2013, pages 1996?2006.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ulsen Eryi?git,
Be?ata Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single Malt or Blended? A Study in Mul-
tilingual Parser Optimization. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 933?939.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Mod-
ified Kneser-Ney Language Model Estimation. In
Proceedings of ACL 2013, pages 690?696.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping Parsers via Syntactic Projection across Parallel
Texts. Natural Language Engineering, 11(3):311?
325.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase Based Translation. In Pro-
ceedings of NAACL-HLT 2003, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Christopher J. Dyer, Ond?rej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Proceedings of ACL 2007, pages
177?180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit 2005, pages 79?86.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-Source Transfer of Delexicalized Dependency
Parsers. In Proceedings of EMNLP 2011, pages 62?
72.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013.
Universal Dependency Annotation for Multilingual
Parsing. In Proceedings of ACL 2013, pages 92?97.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective Sharing for Multilingual Depen-
dency Parsing. In Proceedings of ACL 2012, pages
629?637.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a.
MaltParser: A Data-Driven Parser-Generator for De-
pendency Parsing. In Proceedings of LREC 2006,
pages 2216?2219.
Joakim Nivre, Johan Hall, Jens Nilsson, G?ulsen Eryi?git,
and Svetoslav Marinov. 2006b. Labeled Pseudo-
Projective Dependency Parsing with Support Vector
Machines. In Proceedings of CoNLL 2006, pages
221?225.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 Shared Task on
Dependency Parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915?932.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?uls?en Eryi?git, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007b. MaltParser: A
Language-Independent System for Data-Driven De-
pendency Parsing. Natural Language Engineering,
13(2):95?135.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?52.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL 2003, pages 160?167.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A Universal Part-of-Speech Tagset. In Proceedings
of LREC 2012, pages 2089?2096.
Frank Smadja, Vasileios Hatzivassiloglou, and Kath-
leen R. McKeown. 1996. Translating Colloca-
tions for Bilingual Lexicons: A Statistical Approach.
Computational Linguistics, 22(1):1?38.
139
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. In Proceedings of
NAACL 2012, pages 477?487.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013a. Token and
Type Constraints for Cross-lingual Part-of-speech
Tagging. Transactions of the Association for Com-
putational Linguistics, 1:1?12.
Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre.
2013b. Target Language Adaptation of Discrimi-
native Transfer Parsers. In Proceedings of NAACL
2013, pages 1061?1071.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing Multilingual Text Analysis
Tools via Robust Projection Across Aligned Corpora.
In Proceedings of HLT 2011, pages 1?8.
Daniel Zeman and Philip Resnik. 2008. Cross-
Language Parser Adaptation between Related Lan-
guages. In Proceedings of IJCNLP 2008, pages 35?
42.
Yue Zhang and Joakim Nivre. 2011. Transition-based
Dependency Parsing with Rich Non-local Features.
In Proceedings of ACL 2011, pages 188?193.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross Language Dependency Parsing Using a
Bilingual Lexicon. In Proceedings of ACL-IJCNLP
2009, pages 55?63.
140
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122?129,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Anaphora Models and Reordering for Phrase-Based SMT
Christian Hardmeier Sara Stymne J
?
org Tiedemann Aaron Smith Joakim Nivre
Uppsala University
Department of Linguistics and Philology
firstname.lastname@lingfil.uu.se
Abstract
We describe the Uppsala University sys-
tems for WMT14. We look at the integra-
tion of a model for translating pronomi-
nal anaphora and a syntactic dependency
projection model for English?French. Fur-
thermore, we investigate post-ordering and
tunable POS distortion models for English?
German.
1 Introduction
In this paper we describe the Uppsala University
systems for WMT14. We present three different
systems. Two of them are based on the document-
level decoder Docent (Hardmeier et al., 2012; Hard-
meier et al., 2013a). In our English?French sys-
tem we extend Docent to handle pronoun anaphora,
and in our English?German system we add part-
of-speech phrase-distortion models to Docent. For
German?English we also have a system based on
Moses (Koehn et al., 2007). Again the focus is
on word order, this time by using pre- and post-
reordering.
2 Document-Level Decoding
Traditional SMT decoders translate texts as bags
of sentences, assuming independence between sen-
tences. This assumption allows efficient algorithms
for exploring a large search space based on dy-
namic programming (Och et al., 2001). Because of
the dynamic programming assumptions it is hard to
directly include discourse-level and long-distance
features into a traditional SMT decoder.
In contrast to this very popular stack decoding
approach, our decoder Docent (Hardmeier et al.,
2012; Hardmeier et al., 2013a) implements a search
procedure based on local search. At any stage of
the search process, its search state consists of a
complete document translation, making it easy for
feature models to access the complete document
with its current translation at any point in time. The
search algorithm is a stochastic variant of standard
hill climbing. At each step, it generates a successor
of the current search state by randomly applying
one of a set of state changing operations to a ran-
dom location in the document, and accepts the new
state if it has a better score than the previous state.
The operations are to change the translation of a
phrase, to change the word order by swapping the
positions of two phrases or moving a sequence of
phrases, and to resegment phrases. The initial state
can either be initialized randomly, or be based on
an initial run from Moses. This setup is not limited
by dynamic programming constraints, and enables
the use of the full translated target document to
extract features.
3 English?French
Our English?French system is a phrase-based SMT
system with a combination of two decoders, Moses
(Koehn et al., 2007) and Docent (Hardmeier et al.,
2013a). The fundamental setup is loosely based
on the system submitted by Cho et al. (2013) to
the WMT 2013 shared task. Our phrase table is
trained on data taken from the News commentary,
Europarl, UN, Common crawl and 10
9
corpora.
The first three of these corpora were included in-
tegrally into the training set after filtering out sen-
tences of more than 80 words. The Common crawl
and 10
9
data sets were run through an additional
filtering step with an SVM classifier, closely fol-
lowing Mediani et al. (2011). The system includes
three language models, a regular 6-gram model
with modified Kneser-Ney smoothing (Chen and
Goodman, 1998) trained with KenLM (Heafield,
2011), a 4-gram bilingual language model (Niehues
et al., 2011) with Kneser-Ney smoothing trained
with KenLM and a 9-gram model over Brown clus-
ters (Brown et al., 1992) with Witten-Bell smooth-
ing (Witten and Bell, 1991) trained with SRILM
(Stolcke, 2002).
122
The latest version released in March is equipped with . . . It is sold at . . .
La derni`ere version lanc?ee en mars est dot?ee de . . . ? est vendue . . .
Figure 1: Pronominal Anaphora Model
Our baseline system achieved a cased BLEU
score of 33.2 points on the newstest2014 data set.
Since the anaphora model used in our submission
suffered from a serious bug, we do not discuss the
results of the primary submission in more detail.
3.1 Pronominal Anaphora Model
Our pronominal anaphora model is an adaptation
of the pronoun prediction model described by Hard-
meier et al. (2013b) to SMT. The model consists
of a neural network that discriminatively predicts
the translation of a source language pronoun from
a short list of possible target language pronouns us-
ing features from the context of the source language
pronouns and from the translations of possibly re-
mote antecedents. The objective of this model is to
handle situations like the one depicted in Figure 1,
where the correct choice of a target-language pro-
noun is subject to morphosyntactic agreement with
its antecedent. This problem consists of several
steps. To score a pronoun, the system must decide
if a pronoun is anaphoric and, if so, find potential
antecedents. Then, it can predict what pronouns
are likely to occur in the translation. Our pronoun
prediction model is trained on both tasks jointly,
including anaphora resolution as a set of latent vari-
ables. At test time, we split the network in two
parts. The anaphora resolution part is run sepa-
rately as a preprocessing step, whereas the pronoun
prediction part is integrated into the document-level
decoder with two additional feature models.
The features correspond to two copies of the neu-
ral network, one to handle the singular pronoun it
and one to handle the plural pronoun they. Each net-
work just predicts a binary distinction between two
cases, il and elle for the singular network and ils
and elles for the plural network. Unlike Hardmeier
et al. (2013b), we do not use an OTHER category to
capture cases that should not be translated with any
of these options. Instead, we treat all other cases in
the phrase table and activate the anaphora models
only if one of their target pronouns actually occurs
in the output.
To achieve this, we generate pronouns in two
steps. In the phrase table training corpus, we re-
place all pronouns that should be handled by the
classifier, i.e. instances of il and elle aligned to it
and instances of ils and elles aligned to they, with
special placeholders. At decoding time, if a place-
holder is encountered in a target language phrase,
the applicable pronouns are generated with equal
translation model probability, and the anaphora
model adds a score to discriminate between them.
To reduce the influence of the language model
on pronoun choice and give full control to the
anaphora model, our primary language model is
trained on text containing placeholders instead of
pronouns. Since all output pronouns can also be
generated without the interaction of the anaphora
model if they are not aligned to a source language
pronoun, we must make sure that the language
model sees training data for both placeholders and
actual pronouns. However, for the monolingual
training corpora we have no word alignments to
decide whether or not to replace a pronoun by a
placeholder. To get around this problem, we train a
6-gram placeholder language model on the target
language side of the Europarl and News commen-
tary corpora. Then, we use the Viterbi n-gram
model decoder of SRILM (Stolcke, 2002) to map
pronouns in the entire language model training set
to placeholders where appropriate. No substitu-
tions are made in the bilingual language model or
the Brown cluster language model.
3.2 Dependency Projection Model
Our English?French system also includes a depen-
dency projection model, which uses source-side
dependency structure to model target-side relations
between words. This model assigns a score to each
dependency arc in the source language by consider-
ing the target words aligned to the head and the de-
pendent. In Figure 2, for instance, there is an nsub-
jpass arc connecting dominated to production. The
head is aligned to the target word domin?ee, while
the dependent is aligned to the set {production,de}.
The score is computed by a neural network taking
as features the head and dependent words and their
part-of-speech tags in the source language, the tar-
get word sets aligned to the head and dependent,
the label of the dependency arc, the distance be-
tween the head and dependent word in the source
language as well as the shortest distance between
any pair of words in the aligned sets. The network
is a binary classifier trained to discriminate positive
examples extracted from human-made reference
123
Domestic meat production is dominated by chicken .
amod
nn
nsubjpass
auxpass prep pobj
punct
La production int?erieure de viande est domin?ee par le poulet .
Figure 2: Dependency projection model
translations from negative examples extracted from
n-best lists generated by a baseline SMT system.
4 English?German
For English?German we have two systems, one
based on Moses, and one based on Docent. In both
cases we have focused on word order, particularly
for verbs and particles.
Both our systems are trained on the same data
made available by WMT. The Common crawl data
was filtered using the method of Stymne et al.
(2013). We use factored models with POS tags
as a second output factor for German. The possi-
bility to use language models for different factors
has been added to our Docent decoder. Language
models include an in-domain news language model,
an out-of-domain model trained on the target side
of the parallel training data and a POS language
model trained on tagged news data. The LMs are
trained in the same way as for English?French.
All systems are tuned using MERT (Och, 2003).
Phrase-tables are filtered using entropy-based prun-
ing (Johnson et al., 2007) as implemented in Moses.
All BLEU scores are given for uncased data.
4.1 Pre-Ordered Alignment and
Post-Ordered Translation
The use of syntactic reordering as a separate pre-
processing step has already a long tradition in sta-
tistical MT. Handcrafted rules (Collins et al., 2005;
Popovi?c and Ney, 2006) or data-driven models (Xia
and McCord, 2004; Genzel, 2010; Rottmann and
Vogel, 2007; Niehues and Kolss, 2009) for pre-
ordering training data and system input have been
explored in numerous publications. For certain
language pairs, such as German and English, this
method can be very effective and often improves
the quality of standard SMT systems significantly.
Typically, the source language is reordered to better
match the syntax of the target language when trans-
lating between languages that exhibit consistent
word order differences, which are difficult to handle
by SMT systems with limited reordering capabil-
ities such as phrase-based models. Preordering is
often done on the entire training data as well to op-
timize translation models for the pre-ordered input.
Less common is the idea of post-ordering, which
refers to a separate step after translating source lan-
guage input to an intermediate target language with
corrupted (source-language like) word order (Na et
al., 2009; Sudoh et al., 2011).
In our experiments, we focus on the translation
from English to German. Post-ordering becomes
attractive for several reasons: One reason is the
common split of verb-particle constructions that
can lead to long distance dependencies in German
clauses. Phrase-based systems and n-gram lan-
guage models are not able to handle such relations
beyond a certain distance and it is desirable to keep
them as connected units in the phrase translation
tables. Another reason is the possible distance of
finite and infinitival verbs in German verb phrases
that can lead to the same problems described above
with verb-particle constructions. The auxiliary or
modal verb is placed at the second position but
the main verb appears at the end of the associated
verb phrase. The distances can be arbitrarily long
and long-range dependencies are quite frequent.
Similarly, negation particles and adverbials move
away from the inflected verb forms in certain con-
structions. For more details on specific phenomena
in German, we refer to (Collins et al., 2005; Go-
jun and Fraser, 2012). Pre-ordering, i.e. moving
English words into German word order does not
seem to be a good option as we loose the con-
nection between related items when moving par-
ticles and main verbs away from their associated
elements. Hence, we are interested in reordering
the target language German into English word or-
der which can be beneficial in two ways: (i) Re-
ordering the German part of the parallel training
data makes it possible to improve word alignment
(which tends to prefer monotonic mappings) and
subsequent phrase extraction which leads to better
translation models. (ii) We can explore a two-step
procedure in which we train a phrase-based SMT
model for translating English into German with
English word order first (which covers many long-
distance relations locally) and then apply a second
system that moves words into place according to
correct German syntax (which may involve long-
range distortion).
For simplicity, we base our experiments on hand-
124
crafted rules for some of the special cases discussed
above. For efficiency reasons, we define our rules
over POS tag patterns rather than on full syntac-
tic parse trees. We rely on TreeTagger and apply
rules to join verbs in discontinuous verb phrases
and to move verb-finals in subordinate clauses, to
move verb particles, adverbials and negation par-
ticles. Table 1 shows two examples of reordered
sentences together with the original sentences in
English and German. Our rules implement rough
heuristics to identify clause boundaries and word
positions. We do not properly evaluate these rules
but focus on the down-stream evaluation of the MT
system instead.
It is therefore dangerous to extrapolate from short-term trends.
Daher ist es gef?ahrlich, aus kurzfristigen Trends Prognosen abzuleiten.
Daher ist gef?ahrlich es, abzuleiten aus kurzfristigen Trends Prognosen.
The fall of Saddam ushers in the right circumstances.
Der Sturz von Saddam leitet solche richtigen Umst?ande ein.
Der Sturz von Saddam ein leitet solche richtigen Umst?ande.
Table 1: Two examples of pre-ordering outputs.
The first two lines are the original English and
German sentences and the third line shows the re-
ordered sentence.
We use three systems based on Moses to com-
pare the effect of reordering on alignment and trans-
lation. All systems are case-sensitive phrase-based
systems with lexicalized reordering trained on data
provided by WMT. Word alignment is performed
using fast align (Dyer et al., 2013). For tuning we
use newstest2011. Additionally, we also test paral-
lel data from OPUS (Tiedemann, 2012) filtered by
a method adopted from Mediani et al. (2011).
To contrast our baseline system, we trained a
phrase-based model on parallel data that has been
aligned on data pre-ordered using the reordering
rules for German, which has been restored to the
original word order after word alignment and be-
fore phrase extraction (similar to (Carpuat et al.,
2010; Stymne et al., 2010)). We expect that the
word alignment is improved by reducing crossings
and long-distance links. However, the translation
model as such has the same limitations as the base-
line system in terms of long-range distortions. The
final system is a two-step model in which we apply
translation and language models trained on pre-
ordered target language data to perform the first
step, which also includes a reordered POS language
model. The second step is also treated as a transla-
tion problem as in Sudoh et al. (2011), and in our
case we use a phrase-based model here with lexical-
ized reordering and a rather large distortion limit
of 12 words. Another possibility would be to apply
another rule set that reverts the misplaced words
to the grammatically correct positions. This, how-
ever, would require deeper syntactic information
about the target language to, for example, distin-
guish main from subordinate clauses. Instead, our
model is trained on parallel target language data
with the pre-ordered version as input and the orig-
inal version as output language. For this model,
both sides are tagged and a POS language model
is used again as one of the target language factors
in decoding. Table 2 shows the results in terms of
BLEU scores on the newstest sets from 2013 and
2014.
newstest2013 newstest2014
baseline 19.3 19.1
pre 19.4 19.3
post 18.6 18.7
baseline+OPUS 19.5 19.3
pre+OPUS 19.5 19.3
post+OPUS 19.7 18.8
Table 2: BLEU4 scores for English-German sys-
tems (w/o OPUS): Standard phrase-based (base-
line); phrase-based with pre-ordered parallel cor-
pus used for word alignment (pre); two-step phrase-
based with post-reordering (post)
The results show that pre-ordering has some ef-
fect on word alignment quality in terms of support-
ing better phrase extractions in subsequent steps.
Our experiments show a consistent but small im-
provement for models trained on data that have
been prepared in this way. In contrast, the two-step
procedure is more difficult to judge in terms of au-
tomatic metrics. On the 2013 newstest data we can
see another small improvement in the setup that
includes OPUS data but in most cases the BLEU
scores go down, even below the baseline. The
short-comings of the two-step procedure are ob-
vious. Separating translation and reordering in a
pipeline adds the risk of error propagation. Fur-
thermore, reducing the second step to single-best
translations is a strong limitation and using phrase-
based models for the final reordering procedure is
probably not the wisest decision. However, manual
inspections reveals that many interesting phenom-
ena can be handled even with this simplistic setup.
Table 3 illustrates this with a few selected out-
comes of our three systems. They show how verb-
particle constructions with long-range distortion
125
reference Schauspieler Orlando Bloom hat sich zur Trennung von seiner Frau , Topmodel Miranda Kerr , ge?au?ert .
baseline Schauspieler Orlando Bloom hat die Trennung von seiner Frau , Supermodel Miranda Kerr .
pre-ordering Schauspieler Orlando Bloom hat angek?undigt , die Trennung von seiner Frau , Supermodel Miranda Kerr .
post-ordering Schauspieler Orlando Bloom hat seine Trennung von seiner Frau angek?undigt , Supermodel Miranda Kerr .
reference Er gab bei einer fr?uheren Befragung den Kokainbesitz zu .
baseline Er gab den Besitz von Kokain in einer fr?uheren Anh?orung .
pre-ordering Er r?aumte den Besitz von Kokain in einer fr?uheren Anh?orung .
post-ordering Er r?aumte den Besitz von Kokain in einer fr?uheren Anh?orung ein .
reference Borussia Dortmund k?undigte daraufhin harte Konsequenzen an .
baseline Borussia Dortmund k?undigte an , es werde schwere Folgen .
pre-ordering Borussia Dortmund hat angek?undigt , dass es schwerwiegende Konsequenzen .
post-ordering Borussia Dortmund k?undigte an , dass es schwere Folgen geben werde .
Table 3: Selected translation examples from the newstest 2014 data; the human reference translation; the
baseline system, pre-ordering for word alignment and two-step translation with post-ordering.
such as ?r?aumte ... ein? can be created and how
discontinuous verb phrases can be handled (?hat ...
angek?undigt?) with the two-step procedure. The
model is also often better in producing verb finals
in subordinate clauses (see the final example with
?geben werde?). Note that many of these improve-
ments do not get any credit by metrics like BLEU.
For example the acceptable expression ?r?aumte ein?
which is synonymous to ?gab zu? obtains less credit
then the incomplete baseline translation. Interest-
ing is also to see the effect of pre-ordering when
used for alignment only in the second system. The
first example in Table 3, for example, includes a
correct main verb which is omitted in the baseline
translation, probably because it is not extracted as
a valid translation option.
4.2 Part-of-Speech Phrase-Distortion Models
Traditional SMT distortion models consist of two
parts. A distance-based distortion cost is based
on the position of the last word in a phrase, com-
pared to the first word in the next phrase, given the
source phrase order. A hard distortion limit blocks
translations where the distortion is too large. The
distortion limit serves to decrease the complexity
of the decoder, thus increasing its speed.
In the Docent decoder, the distortion limit is not
implemented as a hard limit, but as a feature, which
could be seen as a soft constraint. We showed in
previous work (Stymne et al., 2013) that it was
useful to relax the hard distortion limit by either
using a soft constraint, which could be tuned, or
removing the limit completely. In that work we
still used the standard parametrization of distortion,
based on the positions of the first and last words in
phrases.
Our Docent decoder, however, always provides
us with a full target translation that is step-wise im-
proved, which means that we can apply distortion
measures on the phrase-level without resorting to
heuristics, which, for instance, are needed in the
case of the lexicalized reordering models in Moses
(Koehn et al., 2005). Because of this it is possible
to use phrase-based distortion, where we calculate
distortion based on the order of phrases, not on the
order of some words. It is possible to parametrize
phrase-distortion in different ways. In this work we
use the phrase-distortion distance and a soft limit
on the distortion distance, to mimic the word-based
distortion. In our experiments we always set the
soft limit to a distance of four phrases. In addition
we use a measure based on how many crossings
a phrase order gives rise to. We thus have three
phrase-distortion features.
As captured by lexicalized reordering models,
different phrases have different tendencies to move.
To capture this to some extent, we also decided
to add part-of-speech (POS) classes to our mod-
els. POS has previously successfully been used
in pre-reordering approaches (Popovi?c and Ney,
2006; Niehues and Kolss, 2009). The word types
that are most likely to move long distances in
English?German translation are verbs and parti-
cles. Based on this observation we split phrases
into two classes, phrases that only contains verbs
and particles, and all other phrases. For these two
groups we use separate phrase-distortion features,
thus having a total of six part-of-speech phrase-
distortion features. All of these features are soft,
and are optimized during tuning.
In our system we initialize Docent by running
Moses with a standard distortion model and lexi-
calized reordering, and then continuing the search
with Docent including our part-of-speech phrase-
distortion features. Tuning was done separately for
the two components, first for the Moses component,
and then for the Docent component initialized by
126
reference Laut Dmitrij Kislow von der Organisation ?Pravo na oryzhie? kann man eine Pistole vom Typ Makarow f?ur 100 bis 300 Dollar kaufen.
baseline Laut Dmitry Kislov aus der Rechten zu Waffen, eine Makarov Gun-spiele erworben werden k?onnen f?ur 100-300 Dollar.
POS+phrase Laut Dmitry Kislov von die Rechte an Waffen, eine Pistole Makarov f?ur 100-300 Dollar erworben werden k?onnen.
reference Die Waffen gelangen ?uber mehrere Kan?ale auf den Schwarzmarkt.
baseline Der ?Schwarze? Markt der Waffen ist wieder aufgef ?ullt ?uber mehrere Kan?ale.
POS+phrase Der ?Schwarze? Markt der Waffen durch mehrere Kan?ale wieder aufgef ?ullt ist.
reference Mehr Kameras k?onnten m?oglicherweise das Problem l?osen...
baseline M?oglicherweise k?onnte das Problem l?osen, eine gro?e Anzahl von Kameras...
POS+phrase M?oglicherweise, eine gro?e Anzahl von Kameras k?onnte das Problem l?osen...
Table 4: Selected translation examples from the newstest2013 data; the human reference translation; the
baseline system (Moses with lexicalized reordering) and the system with a POS+phrase distortion model.
Moses with lexicalized reordering with its tuned
weights. We used newstest2009 for tuning. The
training data was lowercased for training and de-
coding, and recasing was performed using a sec-
ond Moses run trained on News data. As baselines
we present two Moses systems, without and with
lexicalized reordering, in addition to standard dis-
tortion features.
Table 5 shows results with our different distor-
tion models. Overall the differences are quite small.
The clearest difference is between the two Moses
baselines, where the lexicalized reordering model
leads to an improvement. With Docent, both the
word distortion and phrase distortion without POS
do not help to improve on Moses, with a small de-
crease in scores on one dataset. This is not very
surprising, since lexical distortion is currently not
supported by Docent, and the distortion models are
thus weaker than the ones implemented in Moses.
For our POS phrase distortion, however, we see a
small improvement compared to Moses, despite the
lack of lexicalized distortion. This shows that this
distortion model is actually useful, and can even
successfully replace lexicalized reordering. In fu-
ture work, we plan to combine this method with a
lexicalized reordering model, to see if the two mod-
els have complementary strengths. Our submitted
system uses the POS phrase-distortion model.
System Distortion newstest2013 newstest2014
Moses word 19.4 19.3
Moses word+LexReo 19.6 19.6
Docent word 19.5 19.6
Docent phrase 19.5 19.6
Docent POS+phrase 19.7 19.7
Table 5: BLEU4 scores for English?German sys-
tems with different distortion models.
If we inspect the translations, most of the differ-
ences between the Moses baseline and the system
with POS+phrase distortion are actually due to lex-
ical choice. Table 4 shows some examples where
there are word order differences. The result is quite
mixed with respect to the placement of verbs. In
the first example, both systems put the verbs to-
gether but in different positions, instead of splitting
them like the reference suggests. In the second
example, our system erroneously put the verbs at
the end, which would be fine if the sentence had
been a subordinate clause. In the third example,
the baseline system has the correct placement of
the auxiliary ?k?onnte?, while our system is bet-
ter at placing the main verb ?l?osen?. In general,
this indicates that our system is able to support
long-distance distortion as it is needed in certain
cases but sometimes overuses this flexibility. A
better model would certainly need to incorporate
syntactic information to distinguish main from sub-
ordinate clauses. However, this would add a lot of
complexity to the model.
5 Conclusion
We have described the three Uppsala University
systems for WMT14. In the English?French sys-
tem we extend our document-level decoder Do-
cent (Hardmeier et al., 2013a) to handle pronoun
anaphora and introduced a dependency projection
model. In our two English?German system we
explore different methods for handling reordering,
based on Docent and Moses. In particular, we look
at post-ordering as a separate step and tunable POS
phrase distortion.
Acknowledgements
This work forms part of the Swedish strategic re-
search programme eSSENCE. We also acknowl-
edge the use of the Abel cluster, owned by the
University of Oslo and the Norwegian metacenter
for High Performance Computing (NOTUR) and
operated by the Department for Research Comput-
ing at USIT, under project nn9106k. Finally, we
would also like to thank Eva Pettersson, Ali Basirat,
and Eva Martinez for help with human evaluation.
127
References
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving Arabic-to-English statistical ma-
chine translation by reordering post-verbal subjects
for alignment. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 178?183, Uppsala, Swe-
den.
Stanley F. Chen and Joshua Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical report, Computer Sci-
ence Group, Harvard University, Cambridge, Mas-
sachusetts, USA.
Eunah Cho, Thanh-Le Ha, Mohammed Mediani, Jan
Niehues, Teresa Herrmann, Isabel Slawik, and Alex
Waibel. 2013. The Karlsruhe Institute of Technol-
ogy translation systems for the WMT 2013. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 104?108, Sofia, Bulgaria.
Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 531?540, Ann Arbor, Michi-
gan, USA.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of the 2013
Conference of the NAACL: Human Language Tech-
nologies, pages 644?648, Atlanta, Georgia, USA.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 376?
384, Beijing, China.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English?to?
German SMT. In Proceedings of the 13th Confer-
ence of the EACL, pages 726?735, Avignon, France.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1179?
1190, Jeju Island, Korea.
Christian Hardmeier, Sara Stymne, J?org Tiedemann,
and Joakim Nivre. 2013a. Docent: A document-
level decoder for phrase-based statistical machine
translation. In Proceedings of the 51st Annual Meet-
ing of the ACL, Demonstration session, pages 193?
198, Sofia, Bulgaria.
Christian Hardmeier, J?org Tiedemann, and Joakim
Nivre. 2013b. Latent anaphora resolution for cross-
lingual pronoun prediction. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 380?391, Seattle,
Washington, USA. Association for Computational
Linguistics.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland. Association
for Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The
KIT English?French translation systems for IWSLT
2011. In Proceedings of the International Workshop
on Spoken Language Translation, pages 73?78, San
Francisco, California, USA.
Hwidong Na, Jin-Ji Li, Jungi Kim, and Jong-Hyeok
Lee. 2009. Improving fluency by reordering tar-
get constituents using MST parser in English-to-
Japanese phrase-based SMT. In Proceedings of
MT Summit XII, pages 276?283, Ottawa, Ontario,
Canada.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider context by using bilin-
gual language models in machine translation. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 198?206, Edinburgh, Scot-
land. Association for Computational Linguistics.
128
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for Statisti-
cal Machine Translation. In Proceedings of the ACL
2001 Workshop on Data-Driven Machine Transla-
tion, pages 55?62, Toulouse, France.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
42nd Annual Meeting of the ACL, pages 160?167,
Sapporo, Japan.
Maja Popovi?c and Hermann Ney. 2006. POS-based re-
orderings for statistical machine translation. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC?06), pages
1278?1283, Genoa, Italy.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
POS-based distortion model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 171?180, Sk?ovde, Sweden.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of the
Seventh International Conference on Spoken Lan-
guage Processing, pages 901?904, Denver, Col-
orado, USA.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and OOVs: Two problems for translation
between German and English. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 183?188, Uppsala,
Sweden.
Sara Stymne, Christian Hardmeier, J?org Tiedemann,
and Joakim Nivre. 2013. Tunable distortion limits
and corpus cleaning for SMT. In Proceedings of the
Eighth Workshop on Statistical Machine Translation,
pages 225?231, Sofia, Bulgaria.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Post-ordering
in statistical machine translation. In Proceedings of
MT Summit XIII, pages 316?323, Xiamen. China.
J?org Tiedemann. 2012. Parallel data, tools and in-
terfaces in OPUS. In Proceedings of the 8th In-
ternational Conference on Language Resources and
Evaluation (LREC?12), pages 2214?2218, Istanbul,
Turkey.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085?
1094.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 508?514, Geneva, Switzerland.
129
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 275?286,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Estimating Word Alignment Quality for SMT Reordering Tasks
Sara Stymne J
?
org Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
firstname.lastname@lingfil.uu.se
Abstract
Previous studies of the effect of word
alignment on translation quality in SMT
generally explore link level metrics only
and mostly do not show any clear connec-
tions between alignment and SMT qual-
ity. In this paper, we specifically inves-
tigate the impact of word alignment on
two pre-reordering tasks in translation, us-
ing a wider range of quality indicators
than previously done. Experiments on
German?English translation show that re-
ordering may require alignment models
different from those used by the core trans-
lation system. Sparse alignments with
high precision on the link level, for trans-
lation units, and on the subset of cross-
ing links, like intersected HMM models,
are preferred. Unlike SMT performance
the desired alignment characteristics are
similar for small and large training data
for the pre-reordering tasks. Moreover,
we confirm previous research showing that
the fuzzy reordering score is a useful and
cheap proxy for performance on SMT re-
ordering tasks.
1 Introduction
Word alignment is a key component in all state-of-
the-art statistical machine translation (SMT) sys-
tems, and there has been some work exploring the
connection between word alignment quality and
translation quality (Och and Ney, 2003; Fraser and
Marcu, 2007; Lambert et al., 2012). The standard
way to evaluate word alignments in this context is
by using metrics like alignment error rate (AER)
and F-measure on the link level, and the general
conclusion appears to be that translation quality
benefits from alignments with high recall (rather
than precision), at least for large training data. Al-
though many other ways of measuring alignment
quality have been proposed, such as working on
translation units (Ahrenberg et al., 2000; Ayan and
Dorr, 2006; S?gaard and Kuhn, 2009) or using link
degree and related measures (Ahrenberg, 2010),
these methods have not been used to study the re-
lation between alignment and translation quality,
with the exception of Lambert et al. (2012).
Word alignment is also used for many other
tasks besides translation, including term bank
creation (Merkel and Foo, 2007), cross-lingual
annotation projection for part-of-speech tagging
(Yarowsky et al., 2001), semantic roles (Pado and
Lapata, 2005), pronoun anaphora (Postolache et
al., 2006), and cross-lingual clustering (T?ackstr?om
et al., 2012). Even within SMT itself, there are
tasks such as reordering that often make crucial
use of word alignments. For instance, source lan-
guage reordering commonly relies on rules learnt
automatically from word-aligned data (e.g., Xia
and McCord (2004)). As far as we know, no one
has studied the impact of alignment quality on
these additional tasks, and it seems to be tacitly
assumed that alignments that are good for transla-
tion are also good for other tasks.
In this paper we set out to explore the impact
of alignment quality on two pre-reordering tasks
for SMT. In doing so, we employ a wider range of
quality indicators than is customary, and for refer-
ence these indicators are used also to assess over-
all translation quality. To allow an in-depth explo-
ration of the connections between several aspects
of word alignment and reordering, we limit our
study to one language pair, German?English. We
think this is a suitable language pair for studying
reordering since it has both short range and long
range reorderings. Our main focus is on using rel-
atively large training data, 2M sentences, but we
also report results with small training data, 170K
sentences. The main conclusion of our study is
that alignments that are optimal for translation are
not necessarily optimal for reordering, where pre-
275
cision is of greater importance than recall. For
SMT the best alignments are different depending
on corpus size, but for the reordering tasks results
are stable across training data size.
In section 2 we discuss previous work related
to word alignment and SMT. In section 3, we in-
troduce the word alignment quality indicators we
use, and show experimental results for a number
of alignment systems on an SMT task. In sec-
tion 4, we turn to reordering for SMT and use
the same quality indicators to study the impact of
alignment quality on reordering quality. In section
5 we briefly describe results using small training
data. In section 6, we conclude and suggest direc-
tions for future work.
2 Word Alignment and SMT
Word alignment is the task of relating words
in one language to words in the translation in
another language, see an example in Figure 1.
Word alignment models can be learnt automati-
cally from large corpora of sentence aligned data.
Brown et al. (1993) proposed the so-called IBM
models, which are still widely used. These five
models estimate alignments from corpora using
the expectation-maximization algorithm, and each
model adds some complexity. Model 4 is com-
monly used in SMT systems. There have been
many later suggestions of alternatives to these
models. These are often alternatives to model 2,
such as the HMM model (Vogel et al., 1996) and
fast align (Dyer et al., 2013).
All these generative models produce directional
alignments where one word in the source can be
linked to many target words (1?m links) but not
vice versa. It is generally desirable to also allow
n?1 and n?m links, and to achieve this it is com-
mon practice to perform word alignment in both
directions and to symmetrize them using some
heuristic. A number of common symmetrization
strategies are described in Table 1 (Koehn et al.,
2005). There are also other alternatives, such as
the refined method (Och and Ney, 2003), or link
deletion from the union (Fossum et al., 2008).
There is also a wide range of alternative ap-
proaches to word alignment. For example, various
discriminative models have been proposed in the
literature (Liu et al., 2005; Moore, 2005; Taskar
et al., 2005). Their advantage is that they may
integrate a wide range of features that may lead
to improved alignment quality. However, most of
Symmetrization Description
int: intersection A
TS
?A
ST
uni: union A
TS
?A
ST
gd: grow-diag intersection plus adjacent links
from the union if both linked
words are unaligned
gdf: grow-diag-final gd with links from the union
added in a final step if either
linked word is unaligned
gdfa:
grow-diag-final-and
gd with links from the union
added in a final step if both linked
words are unaligned
Table 1: Symmetrization strategies for word align-
ments A
TS
and A
ST
in two directions
these models require external tools (for creating
linguistic features) and manually aligned training
data, which we do not have for our data sets (be-
sides the data we need for evaluation). Investigat-
ing these types of models are outside the scope of
our current work.
Word alignments are used as an important
knowledge source for training SMT systems. In
word-based SMT, the parameters of the gener-
ative word alignment models are essentially the
translation model of the system. In phrase-based
SMT (PBSMT) (Koehn et al., 2003), which is
among the state-of-the-art systems today, word
alignments are used as a basis for extracting
phrases and estimating phrase alignment probabil-
ities. Similarly, word alignments are also used for
estimating rule probabilities in various kinds of hi-
erarchical and syntactic SMT (Chiang, 2007; Ya-
mada and Knight, 2002; Galley et al., 2004).
Intrinsic evaluation of word alignment is gener-
ally based on a comparison to a gold standard of
human alignments. Based on the gold standard,
metrics like precision, recall and F-measure can
be calculated for each alignment link, see Eqs. 1?
2, where A are hypothesized alignment links and
G are gold standard links. Another common met-
ric is alignment error rate (AER) (Och and Ney,
2000), which is based on a distinction between
sure, S, and possible, P , links in the gold stan-
dard. 1?AER is identical to balanced F-measure
when the gold standard does not make a distinc-
tion between S and P.
Precision(A,G) =
|G ?A|
|A|
(1)
Recall(A,G) =
|G ?A|
|G|
(2)
AER = 1?
|P ?A|+ |S ?A|
|S|+ |A|
(3)
276
Crossing = 8
SKDT =
?
8/66 ? 0.65
6 1?1 links
3 multi links
0 null links
Figure 1: An example alignment illustrating n?1, 1?m and crossing links.
The relation between word alignment qual-
ity and PBSMT has been studied by some re-
searchers. Och and Ney (2000) looked at the im-
pact of IBM and HMM models on the alignment
template approach (Och et al., 1999) in terms of
AER. They found that AER correlates with human
evaluation of sentence level quality, but not with
word error rate. Fraser and Marcu (2007) found
that there is no correlation between AER and Bleu
(Papineni et al., 2002), especially not when the P -
set is large. They found that a balanced F-measure
is a better indicator of Bleu, but that a weighted
F-measure is even better (see Eq. 4) mostly with
a higher weight for recall than for precision. This
weight, however, needs to be optimized for each
data set, language pair, and gold standard align-
ment separately.
F(A,G, ?) =
(
?
Precision(A,G)
+
1? ?
Recall(A,G)
)
?1
(4)
Ayan and Dorr (2006) on the other hand found
some evidence for the importance of precision
over recall. However, they used much smaller
training data than Fraser and Marcu (2007). They
also suggested using a measure called consistent
phrase error-rate (CPER), but found that it was
hard to assess the impact of alignment on MT, both
with AER and CPER. Lambert et al. (2012) per-
formed a study where they investigated the effect
of word alignment on MT using a large number of
word alignment indicators. They found that there
was a difference between large and small datasets
in that alignment precision was more important
with small data sets, and recall more important
with large data sets. Overall they did not find any
indicator that was significant over two language
pairs and different corpus sizes. There were more
significant indicators for large datasets, however.
Most researchers who propose new alignment
models perform both a gold standard evalua-
tion and an SMT evaluation (Liang et al., 2006;
Ganchev et al., 2008; Junczys-Dowmunt and Sza?,
2012; Dyer et al., 2013). The relation between the
two types of evaluation is often quite weak. Sev-
eral of these studies only show AER on their gold
standard, despite its well-known shortcomings.
Even though many studies have shown some
relation between translation quality and AER or
weighted F-measure, it has rarely been investi-
gated thoroughly in its own right, and, as far as we
are aware, not for other tasks than SMT. Further-
more, most of these studies considers nothing else
but link level agreement. In this paper we take a
broader view on alignment quality and explore the
effect of other types of quality indicators as well.
3 Word Alignment Quality Indicators
We investigate four groups of quality indicators.
The first group is the classic group where met-
rics are calculated on the alignment link level,
which has been used in several studies. In our
experiments we use a gold standard that does not
make use of distinctions between sure and possible
links, as suggested by Fraser and Marcu (2007).
With this, we can calculate the standard metrics
P(recision) R(ecall) and F(-measure). We will
mainly use balanced F-measure, but occasionally
also report weighted F-measure. As noted before,
1?AER is equivalent to balanced F when only
sure links are used, and will thus not be reported
separately.
S?gaard and Kuhn (2009) and S?gaard and Wu
(2009) suggested working on the translation unit
(TU) level, instead of the link level. A translation
unit, or cept (Goutte et al., 2004), is defined as
a maximally connected subgraph of an alignment.
In Figure 1, the twelve links form nine translation
units. S?gaard and Wu (2009) suggest the metric
TUER, translation unit error rate, shown in Eq. 5,
where A
U
are hypothesized translation units, and
G
U
are gold standard translation units.
1
They use
TUER to establish lower bounds for the cover-
age of alignments from different formalisms, not
to evaluate SMT. While they only use TUER, it
1
TUER is similar to CPER (Ayan and Dorr, 2006), which
measures the error rate of extracted phrases. Due to how
phrase extraction handle null links, there are differences,
however.
277
is also possible to define Precision, Recall and F-
measure over translation units in the same way as
for alignment links. We will use these three mea-
sures to get a broader picture of TUs in alignment
evaluation. Also in this case, 1?TUER is equiva-
lent to F-measure.
TUER(A,G) = 1?
2|A
U
?G
U
|
|A
U
|+ |G
U
|
(5)
The TU metrics are quite strict, since they re-
quire exact matching of TUs. Tiedemann (2005)
suggested the MWU metrics for word alignment
evaluation, which also consider partial matches
of annotated multi-word units, which is a similar
concept to TUs. In those metrics, precision and
recall grow proportionally to the number of cor-
rectly aligned words within translation units. Pro-
posed links are in this way scored according to
their overlap with translation units in the gold stan-
dard. Precision and recall are defined in Eqs. 6?7,
where overlap(X
U
, Y ) is the number of source
and target words in X
U
that overlap with transla-
tion units in Y normalized by the size of X
U
(in
terms of source and target words). Note, that TUs
need to overlap in source and target. Otherwise,
their overlap will be counted as zero.
P
MWU
=
?
A
U
?A
overlap(A
U
, G)
|A|
(6)
R
MWU
=
?
G
U
?G
overlap(G
U
, A)
|G|
(7)
There have also been attempts at classifying
alignments in other ways, not related to a gold
standard. Ahrenberg (2010) proposed several
ways to categorize human alignments, including
link degree, reordering of links, and structural cor-
respondence. He used these indicators to profile
hand-aligned corpora from different domains. We
will not use structural correspondence, which re-
quires a dependency parser, and which we believe
is error prone when performed automatically. We
will use what we call link degree, i.e., how many
alignment links each word obtains. Ahrenberg
(2010) used a fine-grained scheme of the percent-
age for different degrees, including isomorphism
1?1, deletion 0?1, reduction m?1, and paraphrase
m?n. Similar link degree classes were used by
Lambert et al. (2012). In this work we will re-
duce these classes into three: 1?1 links, null links,
which combine the 0?1 and 1?0 cases, and multi
links where there are many words on at least one
side.
Ahrenberg (2010) also proposed to measure re-
orderings. He does this by calculating the percent-
age of links with crossings of different lengths. To
define this he only considers adjacent links in the
source using the distance between corresponding
target words, which means that his metric becomes
a directional measure. Reorderings of alignments
was also used by Genzel (2010), who used cross-
ing score, the number of crossing links, to rank
reordering rules. This is non-directional and sim-
pler to calculate than Ahrenberg (2010)?s metrics,
and implicitly covers length since a long distance
reordering leads to a higher number of pairwise
crossing links. Birch and Osborne (2011) sug-
gest using squared Kendall ? distance (SKTD), see
Eq. 8, where n is the number of links, as a basis
of LR-score, an MT metric that takes reordering
into account. They found that squaring ? better
explained reordering, than using only ? . In this
study we will use both, crossing score and SKTD.
Figure 1 shows these scores for an example sen-
tence. These two measures only tell us how much
reordering there is. To quantify this relative to the
gold standard we also report the absolute differ-
ence between the number of gold standard cross-
ings and system crossings, which we call Crossd-
iff. To account for the quality of crossings, to some
extent, we will also report precision, recall, and F-
measure for the subset of translation units that are
involved in a crossing.
SKTD =
?
|crossing link pairs|
(n
2
? n)/2
(8)
3.1 Alignment Experiments
We perform all our experiments for German?
English. The alignment indicators are calculated
on a corpus of 987 hand aligned sentences (Pado
and Lapata, 2005). The gold standard contains
explicit null links, which the symmetrized auto-
matic alignments do not. To allow a straightfor-
ward comparison we consistently remove all null
links when comparing system alignments to the
gold standard.
For creating the automatic alignments we used
GIZA++ (Och and Ney, 2003) to compute direc-
tional alignments for model 2?4 and the HMM
model, and fast align (fa) (Dyer et al., 2013) as
newer alternatives to model 2. These models re-
quire large amounts of data to be estimated reli-
ably. To achieve this we concatenated the gold
standard with the large SMT training data (see
278
A
l
i
g
n
m
e
n
t
l
i
n
k
s
T
r
a
n
s
l
a
t
i
o
n
u
n
i
t
s
M
W
U
L
i
n
k
d
e
g
r
e
e
L
i
n
k
c
r
o
s
s
i
n
g
s
T
o
t
a
l
P
R
F
T
o
t
a
l
P
R
F
P
R
F
1
-
1
n
u
l
l
m
u
l
t
i
T
o
t
a
l
S
K
T
D
P
R
F
C
r
o
s
s
d
i
f
f
g
o
l
d
2
2
6
2
9
?
?
?
1
7
0
6
8
?
?
?
?
?
?
.
5
4
2
.
3
2
8
.
1
3
0
3
0
1
6
3
.
2
9
2
?
?
?
0
2
-
i
n
t
1
5
3
6
2
.
8
5
0
.
5
7
7
.
6
8
7
1
5
3
6
2
.
7
0
1
.
6
3
1
.
6
6
4
.
8
4
9
.
7
1
2
.
7
7
4
.
5
0
0
.
5
0
0
.
0
0
0
1
0
0
6
4
.
2
6
7
.
5
5
1
.
4
6
3
.
5
0
3
2
0
0
9
9
3
-
i
n
t
1
6
5
7
3
.
8
6
0
.
6
3
0
.
7
2
7
1
6
5
7
3
.
7
0
7
.
6
8
6
.
6
9
7
.
8
5
7
.
7
7
6
.
8
1
4
.
5
6
1
.
4
3
9
.
0
0
0
1
2
6
8
2
.
2
7
4
.
5
5
3
.
5
2
1
.
5
3
7
1
7
4
8
1
4
-
i
n
t
1
6
5
2
9
.
9
0
3
.
6
6
0
.
7
6
3
1
6
5
2
9
.
7
4
3
.
7
2
0
.
7
3
1
.
9
0
1
.
8
1
3
.
8
5
5
.
5
5
9
.
4
4
1
.
0
0
0
1
1
2
2
9
.
2
5
1
.
6
6
3
.
5
2
2
.
5
8
4
1
8
9
3
4
H
M
M
-
i
n
t
1
4
8
7
1
.
9
2
2
.
6
0
6
.
7
3
1
1
4
8
7
1
.
7
6
8
.
6
6
9
.
7
1
5
.
9
2
0
.
7
5
0
.
8
2
7
.
4
7
6
.
5
2
4
.
0
0
0
8
0
7
7
.
2
2
1
.
7
0
9
.
4
1
7
.
5
2
5
2
2
0
8
6
f
a
-
i
n
t
1
5
9
9
7
.
8
5
7
.
6
0
6
.
7
1
0
1
5
9
9
7
.
6
9
6
.
6
5
2
.
6
7
3
.
8
5
4
.
7
4
2
.
7
9
4
.
5
3
1
.
4
6
9
.
0
0
0
9
7
2
4
.
2
4
6
.
5
6
8
.
4
7
1
.
5
1
5
2
0
4
3
9
2
-
g
d
2
2
8
8
2
.
7
0
2
.
7
1
0
.
7
0
6
1
6
5
1
1
.
5
9
9
.
5
7
9
.
5
8
9
.
8
0
6
.
8
2
7
.
8
1
6
.
5
2
4
.
2
8
9
.
1
8
6
2
1
8
2
3
.
2
7
0
.
4
4
6
.
4
4
4
.
4
4
5
8
3
4
0
3
-
g
d
2
1
9
6
1
.
7
5
7
.
7
3
4
.
7
4
5
1
7
6
4
4
.
6
5
0
.
6
7
2
.
6
6
1
.
8
1
7
.
8
5
5
.
8
3
6
.
6
0
8
.
2
7
0
.
1
2
2
2
1
8
8
6
.
2
7
8
.
4
9
2
.
5
2
3
.
5
0
7
8
2
7
7
4
-
g
d
2
2
7
5
4
.
7
6
8
.
7
7
2
.
7
7
0
1
7
6
1
1
.
6
7
0
.
6
9
2
.
6
8
1
.
8
3
9
.
8
8
6
.
8
6
2
.
6
0
5
.
2
4
7
.
1
4
8
2
1
9
6
6
.
2
5
9
.
5
8
3
.
5
1
7
.
5
4
8
8
1
9
7
H
M
M
-
g
d
1
9
4
3
0
.
8
1
2
.
6
9
8
.
7
5
1
1
5
8
3
1
.
7
0
9
.
6
5
8
.
6
8
2
.
8
7
8
.
8
2
0
.
8
4
8
.
4
9
9
.
4
0
7
.
0
9
4
1
4
3
3
4
.
2
3
1
.
6
2
1
.
4
1
1
.
4
9
5
1
5
8
2
9
f
a
-
g
d
2
3
1
4
8
.
7
0
2
.
7
1
9
.
7
1
0
1
7
0
4
3
.
5
8
9
.
5
8
8
.
5
8
8
.
8
0
2
.
8
3
9
.
8
2
0
.
5
4
8
.
2
5
8
.
1
9
4
1
8
5
7
8
.
2
4
2
.
4
5
4
.
4
4
7
.
4
5
0
1
1
5
8
5
2
-
g
d
f
a
2
3
8
4
0
.
6
8
7
.
7
2
4
.
7
0
5
1
7
4
6
9
.
5
7
5
.
5
8
8
.
5
8
2
.
7
8
0
.
8
4
1
.
8
0
9
.
5
9
0
.
2
1
6
.
1
9
4
2
5
6
1
6
.
2
7
9
.
4
1
9
.
4
7
3
.
4
4
4
6
7
1
8
3
-
g
d
f
a
2
3
0
4
9
.
7
3
6
.
7
4
9
.
7
4
2
1
8
7
3
2
.
6
2
1
.
6
8
1
.
6
5
0
.
7
8
6
.
8
7
0
.
8
2
6
.
6
8
4
.
1
8
8
.
1
2
8
2
7
1
1
9
.
2
9
4
.
4
5
1
.
5
6
1
.
5
0
0
4
5
4
7
4
-
g
d
f
a
2
3
7
0
4
.
7
5
1
.
7
8
7
.
7
6
9
1
8
5
6
1
.
6
4
5
.
7
0
1
.
6
7
2
.
8
1
3
.
9
0
1
.
8
5
5
.
6
7
3
.
1
7
2
.
1
5
4
2
6
9
7
7
.
2
7
5
.
5
2
9
.
5
6
2
.
5
4
5
3
0
4
4
H
M
M
-
g
d
f
a
2
0
5
5
4
.
7
9
9
.
7
2
6
.
7
6
1
1
6
9
5
5
.
6
8
5
.
6
8
1
.
6
8
3
.
8
5
7
.
8
5
1
.
8
5
4
.
5
6
5
.
3
3
7
.
0
9
8
1
7
3
9
9
.
2
4
6
.
5
8
4
.
4
7
5
.
5
2
4
1
2
7
6
4
f
a
-
g
d
f
a
2
3
7
1
7
.
6
9
3
.
7
2
6
.
7
1
0
1
7
6
1
2
.
5
7
5
.
5
9
4
.
5
8
4
.
7
8
5
.
8
4
6
.
8
1
5
.
5
8
7
.
2
1
4
.
1
9
9
2
0
3
8
4
.
2
4
7
.
4
3
9
.
4
6
5
.
4
5
2
9
7
7
9
2
-
g
d
f
2
9
0
5
0
.
5
9
1
.
7
5
8
.
6
6
4
1
7
0
8
9
.
5
1
1
.
5
1
2
.
5
1
2
.
7
6
1
.
8
7
6
.
8
1
4
.
6
2
5
.
0
0
2
.
3
7
3
5
9
5
9
2
.
3
3
8
.
3
2
1
.
4
3
8
.
3
7
0
2
9
4
2
9
3
-
g
d
f
2
6
5
7
5
.
6
6
0
.
7
7
5
.
7
1
3
1
8
3
5
4
.
5
8
8
.
6
3
2
.
6
0
9
.
7
7
8
.
8
9
1
.
8
3
1
.
7
1
2
.
0
6
4
.
2
2
5
5
0
8
3
4
.
3
4
4
.
3
8
7
.
5
5
2
.
4
5
5
2
0
6
7
1
4
-
g
d
f
2
6
5
2
9
.
6
9
3
.
8
1
2
.
7
4
8
1
8
2
6
9
.
6
2
8
.
6
7
3
.
6
5
0
.
8
1
0
.
9
2
2
.
8
6
2
.
7
0
6
.
0
7
0
.
2
2
3
4
7
2
1
6
.
3
2
2
.
4
5
9
.
5
8
5
.
5
1
4
1
7
0
5
3
H
M
M
-
g
d
f
2
3
8
8
6
.
7
2
5
.
7
6
5
.
7
4
4
1
6
6
6
0
.
6
5
1
.
6
3
5
.
6
4
3
.
8
5
1
.
8
8
7
.
8
6
9
.
5
7
9
.
2
5
1
.
1
6
9
3
6
8
8
1
.
3
0
9
.
4
7
3
.
4
9
9
.
4
8
6
6
7
1
8
f
a
-
g
d
f
2
6
7
2
4
.
6
3
3
.
7
4
8
.
6
8
6
1
7
4
5
4
.
5
2
4
.
5
3
6
.
5
3
0
.
7
6
9
.
8
6
5
.
8
1
4
.
5
8
9
.
1
0
1
.
3
1
0
3
4
3
0
9
.
3
7
9
.
3
5
1
.
4
4
5
.
3
9
2
4
1
4
6
2
-
u
n
i
3
0
7
1
2
.
5
6
6
.
7
6
9
.
6
5
2
1
5
8
6
4
.
5
0
3
.
4
6
8
.
4
8
5
.
7
7
4
.
8
6
9
.
8
1
8
.
5
8
4
.
0
0
2
.
4
1
3
7
1
2
2
3
.
3
4
9
.
3
0
5
.
3
9
6
.
3
4
5
4
1
0
6
0
3
-
u
n
i
2
8
0
9
3
.
6
3
6
.
7
8
9
.
7
0
4
1
7
3
9
1
.
5
9
2
.
6
0
3
.
5
9
7
.
7
9
1
.
8
8
9
.
8
3
7
.
6
8
4
.
0
6
7
.
2
4
9
6
1
8
2
3
.
3
5
5
.
3
8
1
.
5
2
3
.
4
4
1
3
1
6
6
0
4
-
u
n
i
2
7
9
2
0
.
6
7
0
.
8
2
7
.
7
4
0
1
7
4
1
1
.
6
3
6
.
6
4
9
.
6
4
2
.
8
2
6
.
9
2
1
.
8
7
1
.
6
8
2
.
0
7
4
.
2
4
4
5
7
4
0
8
.
3
3
3
.
4
5
6
.
5
6
4
.
5
0
4
2
7
2
4
5
H
M
M
-
u
n
i
2
4
7
1
2
.
7
0
7
.
7
7
2
.
7
3
8
1
5
9
8
0
.
6
4
9
.
6
0
8
.
6
2
8
.
8
5
7
.
8
8
1
.
8
6
9
.
5
6
1
.
2
6
0
.
1
8
0
4
2
2
6
4
.
3
1
9
.
4
5
9
.
4
7
5
.
4
6
7
1
2
1
0
1
f
a
-
u
n
i
2
7
9
5
1
.
6
1
2
.
7
5
6
.
6
7
6
1
6
3
8
5
.
5
1
2
.
4
9
1
.
5
0
4
.
7
8
1
.
8
6
7
.
8
2
2
.
5
4
8
.
1
1
1
.
3
4
6
3
8
2
8
5
.
3
9
6
.
3
3
6
.
4
0
7
.
3
6
8
8
1
2
2
T
a
b
l
e
2
:
V
a
l
u
e
s
f
o
r
a
l
i
g
n
m
e
n
t
q
u
a
l
i
t
y
i
n
d
i
c
a
t
o
r
s
f
o
r
t
h
e
d
i
f
f
e
r
e
n
t
a
l
i
g
n
m
e
n
t
s
,
w
h
e
r
e
2
?
4
,
H
M
M
,
a
n
d
f
a
a
r
e
a
l
i
g
n
m
e
n
t
m
o
d
e
l
s
,
a
n
d
s
y
m
m
e
t
r
i
z
a
t
i
o
n
s
t
r
a
t
e
g
i
e
s
r
e
f
e
r
t
o
T
a
b
l
e
1
279
Section 3.2) of 2M sentences during alignment.
For symmetrization we used all methods in Table
1, as implemented in the Moses toolkit (Koehn et
al., 2007) and in fast align (Dyer et al., 2013).
Based on the automatically aligned gold stan-
dard, we calculated all alignment indicators for all
settings. The complete results can be found in
Table 2, where we have ordered the symmetriza-
tion methods with the most sparse, intersection, on
top. Overall we can see that while several of the
alignment methods create a much higher number
of alignment links than the gold standard, they do
not produce many more translation units. This is
very interesting and indicates why link level statis-
tics may not be accurate enough to predict the per-
formance of certain downstream applications. As
expected, the metric scores for translation units
are lower than for link level metrics. This is
partly due to the fact that these measures do not
count any partially correct links; the MWU met-
rics which considers partial matches often have
higher scores than link level metrics. Another
finding is that the number of crossings vary a lot
with more than twice as many as the reference for
model2+union, and less than three times as many
for HMM+intersection. The HMM and fa models
have fewer reorderings than the IBM models.
We are now interested in the relation between
alignment evaluation on the link level and on the
translation unit level, which has not been thor-
oughly investigated before. Table 3 shows the cor-
relations between the various metrics. Both preci-
sion and F-measure at the link level have signifi-
cant correlations to all TU metrics. Link level re-
call, on the other hand, is significantly negatively
correlated with TU precision, but not significantly
correlated to any other TU metric, not even TU re-
call. Link level precision is thus highly important
for matching translation units. We can also note
here that while there is a trade-off between preci-
sion and recall on link level, this is not the case for
translation units, which can have both high pre-
cision and high recall. The same is not true for
MWU, that allows partial matching, where we also
see at least some precision/recall trade-off.
3.2 SMT Experiments
For reference, we first study the impact of align-
ment on SMT performance. Our SMT system
is a standard PBSMT system trained on WMT13
Translation unit
Link level ? P R F
P .95 .77 .90
R ?.57 ?.22 ?.42
F .70 .90 .83
Table 3: Pearson correlations between gold stan-
dard word alignment evaluation on the link level
and on translation unit level. Significant correla-
tions are marked with bold (< 0.01).
data.
2
We trained a German?English system on
2M sentences from Europarl and News Commen-
tary. We used the target side of the parallel corpus
and the SRILM toolkit (Stolcke, 2002) to train a 5-
gram language model. For training the translation
model and for decoding we used the Moses toolkit
(Koehn et al., 2007). We applied a standard feature
set consisting of a language model feature, four
translation model features, word penalty, phrase
penalty, and distortion cost. For tuning we used
minimum error-rate training (Och, 2003). In or-
der to minimize the risk of tuning influencing the
results, we used a fixed set of weights for each
experiment, tuned on a model 4+gdfa alignment.
3
For tuning we used newstest2009 with 2525 sen-
tences, and for testing we used newstest2013 with
3000 sentences. Evaluation was performed using
the Bleu metric (Papineni et al., 2002). The same
system setup was used for the SMT systems with
reordering.
Table 4 shows the results on the SMT task.
Model 3 and 4 with gd/gdfa symmetrization yield
the highest scores. There is a larger difference be-
tween systems with different symmetrization than
between systems with different alignment models.
The sparse intersection symmetrization gives the
poorest results. The top row in Table 5 shows
correlations between Bleu and all word alignment
quality indicators. There are significant correla-
tions with link level recall. A weighted link level
F-measure with ? = 0.3 gives a significant corre-
lation of .72, which confirms the results of Fraser
and Marcu (2007). There are no significant corre-
lations with the TU metrics but a positive correla-
tion with the number of TUs. For the MWU met-
rics the correlations are similar to the link level,
2
http://www.statmt.org/wmt13/
translation-task.html
3
This could have disfavored the other alignments, so we
also performed control experiments where we ran separate
tunings for each alignment. While the absolute results varied
somewhat, the correlations with alignment indicators were
stable.
280
m2 m3 m4 HMM fa
inter 18.1 19.1 19.3 18.8 18.9
gd 20.4 20.9 20.9 20.5 20.6
gdfa 20.4 20.7 20.8 20.5 20.5
gdf 19.4 19.7 20.1 19.9 20.0
union 19.2 19.6 19.8 19.7 20.0
Table 4: Baseline Bleu scores for different sym-
metrization heuristics
suggesting that they measure similar things. Intu-
itively it seems important for SMT to match full
translation units, but it might be the case that the
phrase extraction strategy is robust as long as there
are partial matches. There are no significant cor-
relations with link degree or link crossings, ex-
cept a negative correlation with Crossdiff, which
means that it is good to have a similar number of
crossings as the baseline. These results confirm
results from previous studies that link level mea-
sures, especially recall and weighted F-measure
show some correlation with SMT quality whereas
precision does not.
4 Reordering Tasks for SMT
Reordering is an important part of any SMT sys-
tem. One way to address it is to add reorder-
ing models to standard PBSMT systems, for in-
stance lexicalized reordering models (Koehn et al.,
2005), or to directly model reordering in hierarchi-
cal (Chiang, 2007) or syntactic translation models
(Yamada and Knight, 2002). Another type of ap-
proach is preordering, where the source side is re-
ordered to mimic the target side before translation.
There have also been approaches where reordering
is modeled as part of the evaluation of MT systems
(Birch and Osborne, 2011).
We can distinguish two main types of ap-
proaches to preordering in SMT, either by using
hand-written rules, which often operate on syn-
tactic trees (Collins et al., 2005), or by reordering
rules that are learnt automatically based on a word
aligned corpus (Xia and McCord, 2004). The lat-
ter approach is of interest to us, since it is based
on word alignments.
There has been much work on automatic learn-
ing of reordering rules, which can be based on dif-
ferent levels of annotation, such as part-of-speech
tags (Rottmann and Vogel, 2007; Niehues and
Kolss, 2009; Genzel, 2010), chunks (Zhang et
al., 2007) or parse trees (Xia and McCord, 2004).
In general, all these approaches lead to improve-
ments of translation quality. The reordering is
always applied on the translation input. It can
also be applied on the source side of the train-
ing corpora, which sometimes improves the results
(Rottmann and Vogel, 2007), but sometimes does
not make a difference (Stymne, 2012). When pre-
ordering is performed on the translation input, it
can be presented to the decoder as a 1-best reorder-
ing (Xia and McCord, 2004), as an n-best list (Li
et al., 2007), or as a lattice of possible reorderings
(Rottmann and Vogel, 2007; Zhang et al., 2007).
In the preordering studies cited above it is often
not even stated which alignment model was used.
A few authors mention the alignment tool that has
been applied but no comparison between different
alignment models is performed in any of the pa-
pers we are aware of. Li et al. (2007), for exam-
ple, simply state that they used GIZA++ and gdf
symmetrization and that they removed less proba-
ble multi links. Lerner and Petrov (2013) use the
intersection of HMM alignments and claims that
model 4 did not add much value. Genzel (2010)
did mention that using a standard model 4 was
not successful for his rule learning approach. In-
stead he used filtered model-1-alignments, which
he claims was more successful. However, there
are no further analyses or comparisons between
the alignments reported in any of these papers.
Another type of approach to reordering is to
only reorder the data in order to improve word
alignments, and to restore the original word or-
der before training the SMT system. This type
of approach has the advantage that no modifica-
tions are needed for the translation input. This ap-
proach has also been used both with hand-written
rules (Carpuat et al., 2010; Stymne et al., 2010)
and with rules based on initial word alignments on
non-reordered texts (Holmqvist et al., 2009). For
the latter approach a small study of the effect of gd
and gdfa symmetrizations was presented, which
only showed small variations in quality scores
(Holmqvist et al., 2012).
Below we present the two tasks that we study
in this paper: part-of-speech-based reordering for
creating input lattices for SMT and alignment-
based reordering for improving phrase-tables. We
evaluate the performance of these tasks in rela-
tion to the use of different alignment models and
symmetrization heuristics. For these tasks we are
mainly interested in the full translation task, for
which we report Bleu scores. In addition we also
show fuzzy reordering score (FRS), which focuses
281
Alignment links Translation units MWU
Total P R F Total P R F P R F
SMT, Bleu .33 ?.25 .56 .46 .65 ?.20 .16 ?.02 ?.29 .59 .44
POSReo, FRS ?.80 .87 ?.49 .75 ?.23 .90 .81 .89 .82 ?.45 .22
POSReo, Bleu ?.64 .74 ?.27 .85 .05 .80 .80 .86 .67 ?.23 .35
AlignReo, FRS ?.77 .88 ?.43 .84 ?.11 .90 .88 .92 .81 ?.37 .31
AlignReo, Bleu ?.81 .83 ?.58 .61 ?.24 .75 .64 .72 .71 ?.53 .04
Link degree Link crossings
1-1 null multi Total SKTD P R F Crossdiff
SMT, Bleu .33 ?.30 .21 ?.05 ?.14 ?.09 .25 .07 ?.63
POSReo, FRS ?.41 .84 ?.89 ?.81 ?.70 .90 .21 .86 ?.41
POSReo, Bleu ?.17 .66 ?.80 ?.71 ?.60 .79 .42 .89 ?.49
AlignReo, FRS ?.32 .77 ?.86 ?.80 ?.73 .94 .27 .92 ?.38
AlignReo, Bleu ?.57 .83 ?.79 ?.93 ?.91 .86 ?.07 .69 ?.52
Table 5: Pearson correlations between different alignment characteristics and scores for the translation
and reordering tasks. Significant correlations are marked with bold (< 0.01).
only on the reordering component (Talbot et al.,
2011). It compares a system reordering to a refer-
ence reordering, by measuring how many chunks
that have to be moved to get an identical word or-
der, see Eq. 9, where C is the number of con-
tiguously aligned chunks, and M the number of
words. To find the reference ordering we apply
the method of Holmqvist et al. (2009), described
in Section 4.2, to the gold standard alignment.
FRS = 1?
C ? 1
M ? 1
(9)
4.1 Part-of-Speech-Based Reordering
Our first reordering task is a part-of-speech-based
preordering method described by Rottmann and
Vogel (2007) and Niehues and Kolss (2009),
which was successfully used for German?English
translation. Rules are learnt from a word aligned
POS-tagged corpus. Based on the alignments, tag
patterns are identified that give rise to specific re-
orderings. These patterns are then scored based
on relative frequency.
4
The rules are then applied
to the translation input to create a reordering lat-
tice, with normalized edge scores based on rule
scores. In our experiments we only use rules with
a score higher than 0.2, to limit the size of the lat-
tices. For calculating FRS, we pick the highest
scoring 1-best word order from the lattices.
We learn rules from our entire SMT training
corpus varying alignment models and symmetriza-
tion. To investigate only the effect of word align-
ment for creating reordering rules, we do not
4
Note that we do not use words (Rottmann and Vogel,
2007) or wild cards (Niehues and Kolss, 2009) in our rules.
m2 m3 m4 HMM fa
inter .577 .575 .581 .596 .567
gd .555 .559 .570 .589 .546
gdfa .540 .540 .559 .579 .539
gdf .439 .499 .542 .560 .495
union .442 .492 .544 .563 .486
Table 6: Fuzzy reordering scores for part-of-
speech-based reordering for different alignments
m2 m3 m4 HMM fa
inter 21.4 21.6 21.8 21.6 21.6
gd 21.5 21.6 21.6 21.7 21.5
gdfa 21.4 21.5 21.7 21.7 21.4
gdf 20.3 21.0 21.4 21.5 21.0
union 20.3 21.5 21.6 21.5 20.8
Table 7: Bleu scores for part-of-speech-based re-
ordering for different alignments
change the SMT system, which is trained based
on model 4+gdfa alignments. The only thing that
varies for the translation task is thus the input lat-
tice given to this SMT system.
The results are shown in Tables 6 and 7. Most
Bleu scores are better than using the same SMT
system without preordering, with a Bleu score of
20.8. The results on FRS and Bleu are highly cor-
related at .94, despite the fact that we use a lattice
as SMT input, and the 1-best order for FRS. For
both metrics sparse symmetrization like intersec-
tion and gd performs best. Model 4 and HMM
perform best with similar Bleu scores, but FRS is
better for the HMM model.
Table 5 shows the correlations with the word
alignment indicators, in the rows labeled POSReo.
There are strong correlations with all TU metrics,
contrary to the SMT task. There are also signifi-
cant correlations with link level precision and bal-
282
anced F-measure. The correlation with weighted
link level F-measure is even higher, .91 for ? =
0.6. This is an indication that this algorithm is
more sensitive to precision than the SMT task. As
for the SMT task, the correlation patterns are simi-
lar for the MWU metrics as for link level. For link
degree, null alignments are correlated, but there is
a negative correlation for multi links. The correla-
tions with the number of crossings and SKTD are
negative, which means that it is better to have a
low number of crossings. This may seem counter-
intuitive, but note in Table 1 that many alignments
have a much higher number of crossings than the
baseline. The precision of the crossing links is
highly correlated with performance on this task,
while the recall is not. This tells us that it is impor-
tant that the crossings we find in the alignment are
good, but that it is less important that we find all
crossings. This makes sense since the rule learner
can then learn at least a subset of all existing cross-
ings well.
4.2 Reordering for Alignment
In our second reordering task we investigate
alignment-based reordering for improving phrase-
tables (Holmqvist et al., 2009; Holmqvist et al.,
2012). This strategy first performs a word align-
ment, based on which the source text is reordered
to remove all crossings. A second alignment is
trained on the reordered data, which is then re-
stored to the original order before training the
full SMT system. In Holmqvist et al. (2012) it
was shown that this strategy leads to improve-
ments in link level recall and F-measure as well
as small translation improvements for English?
Swedish. It also led to small improvements for
German?English translation.
Similar to the previous experiments, we now
vary alignment models and symmetrization that
are used for reordering during the first step. The
second step is kept the same using model 4+gdfa
in order to focus on the reordering step in our com-
parisons. Tables 8 and 9 show the results of these
experiments. In this case the reordering strat-
egy was not successful, always producing lower
Bleu scores than the baseline of 20.8. However,
there are some interesting differences in these out-
comes. On this task as well, FRS and Bleu scores
are highly correlated at .89, which was expected,
since this method directly uses the reordered data
to train phrase tables. For the best systems, the
m2 m3 m4 HMM fa
inter .583 .604 .669 .654 .598
gd .548 .583 .646 .642 .561
gdfa .532 .564 .633 .645 .553
gdf .422 .482 .571 .574 .474
union .395 .455 .552 .545 .452
Table 8: Fuzzy reordering scores for alignment-
based reordering for different alignments
m2 m3 m4 HMM fa
inter 19.5 19.5 19.9 20.2 19.4
gd 19.3 19.5 19.8 20.2 19.3
gdfa 19.1 19.2 19.6 20.0 19.2
gdf 18.3 18.2 18.6 19.0 18.9
union 17.4 17.8 18.4 18.8 18.8
Table 9: Bleu scores for alignment-based reorder-
ing for different alignments
FRS scores are higher than for the previous task,
see Table 6, which shows that reordering directly
based on alignments is easier than learning and ap-
plying rules based on them, given suitable align-
ments. On this task, again, the sparser alignments
are the most successful on both tasks. Here, how-
ever, the HMM model gives the best Bleu scores,
and similar FRS scores to model 4.
Table 5 shows the correlations with the word
alignment indicators, in the rows labeled Align-
Reo. The correlation patterns are very similar
to the previous task. A few more indicators are
significantly negatively correlated with alignment-
based reordering than with the other reordering
tasks and metrics. The performance on our two
reordering tasks are significantly correlated at .76.
Again alignments with good scores on TU met-
rics, link level precision and crossing link preci-
sion are preferable. For this task, the best correla-
tion with weighted link level F-measure is .86 for
? = 0.8. Again, we thus see that sparse align-
ments with high precision on all measures includ-
ing the crossing subset, are important.
5 Small Training Data
Since previous work has suggested that training
data size influences the relation between align-
ment and SMT quality for small and large training
data (Lambert et al., 2012), we investigated this is-
sue also for our reordering tasks. We repeated all
our experiments on a small dataset, only the News
Commentary data from WMT13, with 170K sen-
tences. Due to space constraints we cannot show
all results in the paper, but the main findings are
283
summarized in this section.
To acquire alignment results we realigned the
gold standard concatenated with the smaller data,
to reflect the actual quality of alignment with a
small dataset. As expected the quality scores tend
to be lower with less data. Overall the same sys-
tems tend to perform good on each metric with the
small and large data, even though there is some
variation in the ranking between systems. On the
SMT task as well, the Bleu scores are lower, as
expected. In this case fast align is doing best fol-
lowed by model 4 and 3. The best symmetrization
is again gd and gdfa. There are also some differ-
ences in the correlation profile. Link recall and
number of translation units are no longer signifi-
cantly correlated, whereas the number of crossings
and SKTD are. The highest correlation for link
level F-measure is .60 for balanced F-measure,
showing that precision is equally important to re-
call with less data.
For the reordering tasks the scores are again
lower. The POS-based reorderings again help over
the baseline SMT, whereas the alignment-based
reordering leads to slightly lower scores. The cor-
relation profile look exactly the same for Bleu
for POS-based reordering. FRS for both tasks
and Bleu for alignment-based reordering have the
same correlation profiles as Bleu for alignment-
based reordering on large data. There are thus
very small differences in the word alignment qual-
ity indicators that are relevant with large and small
training data, while there are some differences on
the SMT task. For weighted link level F-measure,
the highest correlations are found with ? = 0.6?
0.7 on the different metrics, again showing that
precision is more important than recall. For FRS
on both tasks and Bleu for alignment-based re-
ordering, model4 and HMM with intersection and
gd still perform best. For Bleu for POS-based re-
ordering, gdfa and model 3 also give good results.
6 Conclusion and Future Work
We have shown that the best combination of align-
ment and symmetrization models for SMT are not
the best models for reordering tasks in our ex-
perimental setting. For SMT, high recall is more
important than precision with large training data,
while precision and recall are of equal impor-
tance with small training data. This finding sup-
ports previous research (Fraser and Marcu, 2007;
Lambert et al., 2012). Translation unit metrics
are not predictive of SMT performance. For the
large data condition model 3 and 4 with gd and
gdfa symmetrization gave the best results, whereas
fast align with gd and gdfa was best with small
training data.
For the two preordering tasks we investigated,
however, link level weighted F-measure that gave
more weight to precision was important, as well as
all TU metrics. It was also important to have high
precision for the crossing subset of TUs. Hence,
it is more important to reliably find some cross-
ings than to find all crossings. This make sense
since the extracted rules or performed reorderings
are likely good in such cases, even if we are not
able to find all possible reorderings. In conclu-
sion, based on this study, we recommend intersec-
tion symmetrization with model 4 and HMM for
SMT reordering tasks.
We have studied two relatively different re-
ordering tasks with two training data sizes, but
found that they to a large extent prefer the same
types of alignments. Moreover, the results on
these two reordering tasks correlates strongly with
FRS, which is much cheaper to calculate than
SMT metrics that may even require retraining of
full SMT systems. This is consistent with Tal-
bot et al. (2011) who suggested FRS for preorder-
ing tasks. We thus would encourage developers
of alignment methods to not only give results for
SMT, but also for FRS, as a proxy for reordering
tasks. Furthermore, it is also useful to give results
on TU metrics in addition to link level metrics to
complement the evaluation.
In this paper, we have looked at existing genera-
tive alignment and symmetrization models. In fu-
ture work, we would also like to investigate other
models, including the removal of low-confidence
links, which has previously been proposed for pre-
reordering (Li et al., 2007; Genzel, 2010). Given
the results, it also seems motivated to develop
or adapt the existing models in general, to bet-
ter fit the properties of specific auxiliary tasks.
Furthermore, we need to validate our findings on
other language pairs, especially for non-related
languages with even more diverse word order.
Acknowledgments
This work was supported by the Swedish strategic
research programme eSSENCE.
284
References
Lars Ahrenberg, Magnus Merkel, Anna S?agvall Hein,
and J?org Tiedemann. 2000. Evaluation of word
alignment systems. In Proceedings of LREC, vol-
ume III, pages 1255?1261, Athens, Greece.
Lars Ahrenberg. 2010. Alignment-based profiling of
Europarl data in an English-Swedish parallel corpus.
In Proceedings of LREC, pages 3398?3404, Valetta,
Malta.
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In Proceedings of
Coling and ACL, pages 9?16, Sydney, Australia.
Alexandra Birch and Miles Osborne. 2011. Reorder-
ing metrics for MT. In Proceedings of ACL, pages
1027?1035, Portland, Oregon, USA.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving Arabic-to-English statistical ma-
chine translation by reordering post-verbal subjects
for alignment. In Proceedings of ACL, Short Papers,
pages 178?183, Uppsala, Sweden.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):202?228.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, USA.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of NAACL,
pages 644?648, Atlanta, Georgia, USA.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using syntax to improve word alignment pre-
cision for syntax-based machine translation. In Pro-
ceedings of WMT, pages 44?52, Columbus, Ohio.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL, pages 273?280, Boston,
Massachusetts, USA.
Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations? In
Proceedings of ACL, pages 986?993, Columbus,
Ohio, USA.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of Coling, pages 376?384,
Beijing, China.
Cyril Goutte, Kenji Yamada, and Eric Gaussier. 2004.
Aligning words using matrix factorisation. In Pro-
ceedings of ACL, pages 502?509, Barcelona, Spain.
Maria Holmqvist, Sara Stymne, Jody Foo, and Lars
Ahrenberg. 2009. Improving alignment for SMT
by reordering and augmenting the training corpus.
In Proceedings of WMT, pages 120?124, Athens,
Greece.
Maria Holmqvist, Sara Stymne, Lars Ahrenberg, and
Magnus Merkel. 2012. Alignment-based reordering
for SMT. In Proceedings of LREC, Istanbul, Turkey.
Marcin Junczys-Dowmunt and Arkadiusz Sza?. 2012.
SyMGiza++: Symmetrized word alignment models
for statistical machine translation. In International
Joint Conference of Security and Intelligent Infor-
mation Systems, pages 379?390, Warsaw, Poland.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL, pages 48?54, Edmonton, Al-
berta, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL, Demonstration Session, pages
177?180, Prague, Czech Republic.
Patrik Lambert, Simon Petitrenaud, Yanjun Ma, and
Andy Way. 2012. What types of word alignment
improve statistical machine translation? Machine
Translation, 26(4):289?323.
Uri Lerner and Slav Petrov. 2013. Source-side clas-
sifier preordering for machine translation. In Pro-
ceedings of EMNLP, pages 513?523, Seattle, Wash-
ington, USA.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the ACL, pages 720?727, Prague, Czech
Republic.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL,
pages 104?111, New York City, New York, USA.
285
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings of
ACL, pages 459?466, Ann Arbor, Michigan, USA.
Magnus Merkel and Jody Foo. 2007. Terminology
extraction and term ranking for standardizing term
banks. In Proceedings of the 16th Nordic Confer-
ence on Computational Linguistics, pages 349?354,
Tartu, Estonia.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of
HLT and EMNLP, pages 81?88, Vancouver, British
Columbia, Canada.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of WMT, pages 206?214, Athens, Greece.
Franz Josef Och and Hermann Ney. 2000. A com-
parison of alignment models for statistical machine
translation. In Proceedings of Coling, pages 1086?
1090, Saarbr?ucken, Germany.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for sta-
tistical machine translation. In Proceedings of the
Joint Conference of EMNLP and Very Large Cor-
pora, pages 20?28, College Park, Maryland, USA.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167, Sapporo, Japan.
Sebastian Pado and Mirella Lapata. 2005. Cross-
linguistic projection of role-semantic information.
In Proceedings of HLT and EMNLP, pages 859?866,
Vancouver, British Columbia, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318, Philadelphia, Pennsylvania,
USA.
Oana Postolache, Dan Cristea, and Constantin Or?asan.
a. 2006. Transferring coreference chains through
word alignment. In Proceedings of LREC, pages
889?892, Genoa, Italy.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
POS-based distortion model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 171?180, Sk?ovde, Sweden.
Anders S?gaard and Jonas Kuhn. 2009. Empirical
lower bounds on alignment error rates in syntax-
based machine translation. In Proceedings of the
Third Workshop on Syntax and Structure in Statis-
tical Translation, pages 19?27, Boulder, Colorado,
USA.
Anders S?gaard and Dekai Wu. 2009. Empirical lower
bounds on translation unit error rate for the full class
of inversion transduction grammars. In Proceedings
of 11th International Conference on Parsing Tech-
nologies, pages 33?36, Paris, France.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901?904, Denver, Colorado, USA.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and OOVs: Two problems for translation
between German and English. In Proceedings of
WMT and MetricsMATR, pages 183?188, Uppsala,
Sweden.
Sara Stymne. 2012. Clustered word classes for pre-
ordering in statistical machine translation. In Pro-
ceedings of ROBUS-UNSUP 2012: Joint Workshop
on Unsupervised and Semi-Supervised Learning in
NLP, pages 28?34, Avignon, France.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
NAACL, pages 477?487, Montr?eal, Quebec, Canada.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason
Katz-Brown, Masakazu Seno, and Franz Och. 2011.
A lightweight evaluation framework for machine
translation reordering. In Proceedings of WMT,
pages 12?21, Edinburgh, Scotland.
Ben Taskar, Lacoste-Julien Simon, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of HLT and EMNLP,
pages 73?80, Vancouver, British Columbia, Canada.
J?org Tiedemann. 2005. Optimisation of word
alignment clues. Natural Language Engineering,
11(03):279?293. Special Issue on Parallel Texts.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical
translation. In Proceedings of Coling, pages 836?
841, Copenhagen, Denmark.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of Coling, pages
508?514, Geneva, Switzerland.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings of ACL,
pages 303?310, Philadelphia, Pennsylvania, USA.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the First International Conference
on Human Language Technology, pages 1?8, San
Diego, California, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Improved chunk-level reordering for statistical ma-
chine translation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 21?28, Trento, Italy.
286
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 132?134,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Word?s Vector Representations meet Machine Translation
Eva Mart??nez Garcia
Cristina Espan?a-Bonet
TALP Research Center
Univesitat Polite`cnica de Catalunya
emartinez@lsi.upc.edu
cristinae@lsi.upc.edu
Jo?rg Tiedemann
Uppsala University
Department of Linguistics
and Philology
jorg.tiedemann@lingfil.uu.se
Llu??s Ma`rquez
Qatar Computing Research Institute
Qatar Foundation
lluism@lsi.upc.edu
Abstract
Distributed vector representations of
words are useful in various NLP tasks.
We briefly review the CBOW approach
and propose a bilingual application of
this architecture with the aim to improve
consistency and coherence of Machine
Translation. The primary goal of the bilin-
gual extension is to handle ambiguous
words for which the different senses are
conflated in the monolingual setup.
1 Introduction
Machine Translation (MT) systems are nowadays
achieving a high-quality performance. However,
they are typically developed at sentence level
using only local information and ignoring the
document-level one. Recent work claims that
discourse-wide context can help to translate indi-
vidual words in a way that leads to more coherent
translations (Hardmeier et al., 2013; Hardmeier et
al., 2012; Gong et al., 2011; Xiao et al., 2011).
Standard SMT systems use n-gram models to
represent words in the target language. How-
ever, there are other word representation tech-
niques that use vectors of contextual information.
Recently, several distributed word representation
models have been introduced that have interesting
properties regarding to the semantic information
that they capture. In particular, we are interested
in the word2vec package available in (Mikolov et
al., 2013a). These models proved to be robust
and powerful for predicting semantic relations be-
tween words and even across languages. However,
they are not able to handle lexical ambiguity as
they conflate word senses of polysemous words
into one common representation. This limitation is
already discussed in (Mikolov et al., 2013b) and in
(Wolf et al., 2014), in which bilingual extensions
of the word2vec architecture are proposed. In con-
trast to their approach, we are not interested in
monolingual applications but instead like to con-
centrate directly on the bilingual case in connec-
tion with MT.
We built bilingual word representation mod-
els based on word-aligned parallel corpora by
an application of the Continuous Bag-of-Words
(CBOW) algorithm to the bilingual case (Sec-
tion 2). We made a twofold preliminary evalua-
tion of the acquired word-pair representations on
two different tasks (Section 3): predicting seman-
tically related words (3.1) and cross-lingual lexical
substitution (3.2). Section 4 draws the conclusions
and sets the future work in a direct application of
these models to MT.
2 Semantic Models using CBOW
The basic architecture that we use to build our
models is CBOW (Mikolov et al., 2013a). The
algorithm uses a neural network (NN) to predict
a word taking into account its context, but without
considering word order. Despite its drawbacks, we
chose to use it since we presume that the transla-
tion task applies the same strategy as the CBOW
architecture, i.e., from a set of context words try to
predict a translation of a specific given word.
In the monolingual case, the NN is trained using
a monolingual corpus to obtain the corresponding
projection matrix that encloses the vector repre-
sentations of the words. In order to introduce the
semantic information in a bilingual scenario, we
use a parallel corpus and automatic word align-
ment to extract a training corpus of word pairs:
(w
i,S
|w
i,T
). This approach is different from (Wolf
et al., 2014) who build an independent model for
each language. With our method, we try to cap-
ture simultaneously the semantic information as-
sociated to the source word and the information
in the target side of the translation. In this way,
we hope to better capture the semantic informa-
tion that is implicitly given by translating a text.
132
Model Accuracy Known words
mono en 32.47 % 64.67 %
mono es 10.24 % 44.96 %
bi en-es 23.68 % 13.74 %
Table 1: Accuracy on the Word Relationship set.
3 Experiments
The semantic models are built using a combination
of freely available corpora for English and Span-
ish (EuropalV7, United Nations and Multilingual
United Nations, and Subtitles2012). They can
be found in the Opus site (Tiedemann, 2012).We
trained vectors to represent word pairs forms us-
ing this corpora with the word2vec CBOW imple-
mentation. We built a training set of almost 600
million words and used 600-dimension vectors in
the training. Regarding to the alignments, we only
used word-to-word ones to avoid noise.
3.1 Accuracy of the Semantic Model
We first evaluate the quality of the models based
on the task of predicting semantically related
words. A Spanish native speaker built the bilin-
gual test set similarly to the process done to the
training data from a list of 19, 544 questions intro-
duced by (Mikolov et al., 2013c). In our bilingual
scenario, the task is to predict a pair of words given
two pairs of related words. For instance, given the
pair Athens|Atenas Greece|Grecia and
the question London|Londres, the task is to
predict England|Inglaterra.
Table 1 shows the results, both overall accuracy
and accuracy over the known words for the mod-
els. Using the first 30, 000 entries of the model
(the most frequent ones), we obtain 32% of ac-
curacy for English (mono en) and 10% for Span-
ish (mono es). We chose these parameters for our
system to obtain comparable results to the ones
in (Mikolov et al., 2013a) for a CBOW architec-
ture but trained with 783 million words (50.4%).
Decay for the model in Spanish can be due to the
fact that it was built from automatic translations.
In the bilingual case (bi en-es), the accuracy is
lower than for English probably due to the noise
in translations and word alignment.
3.2 Cross-Lingual Lexical Substitution
Another way to evaluate the semantic models is
through the effect they have in translation. We im-
plemented the Cross-Lingual Lexical Substitution
task carried out in SemEval-2010 (Task2, 2010)
and applied it to a test set of news data from the
News Commentary corpus of 2011.
We identify those content words which are
translated in more than one way by a baseline
translation system (Moses trained with Europarl
v7). Given one of these content words, we take the
two previous and two following words and look
for their vector representations using our bilingual
models. We compute a linear combination of these
vectors to obtain a context vector. Then, to chose
the best translation option, we calculate a score
based on the similarity among the vector of every
possible translation option seen in the document
and the context vector.
In average there are 615 words per document
within the test set and 7% are translated in more
than one way by the baseline system. Our bilin-
gual models know in average 87.5% of the words
and 83.9% of the ambiguous ones, so although
there is a good coverage for this test set, still, some
of the candidates cannot be retranslated or some
of the options cannot be used because they are
missing in the models. The accuracy obtained af-
ter retranslation of the known ambiguous words
is 62.4% and this score is slightly better than the
result obtained by using the most frequent transla-
tion for ambiguous words (59.8%). Even though
this improvement is rather modest, it shows poten-
tial benefits of our model in MT.
4 Conclusions
We implemented a new application of word vec-
tor representations for MT. The system uses word
alignments to build bilingual models with the final
aim to improve the lexical selection for words that
can be translated in more than one sense.
The models have been evaluated regarding their
accuracy when trying to predict related words
(Section 3.1) and also regarding its possible effect
within a translation system (Section 3.2). In both
cases one observes that the quality of the transla-
tion and alignments previous to building the se-
mantic models are bottlenecks for the final perfor-
mance: part of the vocabulary, and therefore trans-
lation pairs, are lost in the training process.
Future work includes studying different kinds
of alignment heuristics. We plan to develop
new features based on the semantic models to
use them inside state-of-the-art SMT systems like
Moses (Koehn et al., 2007) or discourse-oriented
decoders like Docent (Hardmeier et al., 2013).
133
References
Z. Gong, M. Zhang, and G. Zhou. 2011. Cache-based
document-level statistical machine translation. In
Proc. of the 2011 Conference on Empirical Methods
in NLP, pages 909?919, UK.
C. Hardmeier, J. Nivre, and J. Tiedemann. 2012.
Document-wide decoding for phrase-based statisti-
cal machine translation. In Proc. of the Joint Con-
ference on Empirical Methods in NLP and Compu-
tational Natural Language Learning, pages 1179?
1190, Korea.
C. Hardmeier, S. Stymne, J. Tiedemann, and J. Nivre.
2013. Docent: A document-level decoder for
phrase-based statistical machine translation. In
Proc. of the 51st ACL Conference, pages 193?198,
Bulgaria.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proc. of the 45th
ACL Conference, pages 177?180, Czech Republic.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a.
Efficient estimation of word representations in vec-
tor space. In Proceedings of Workshop at ICLR.
http://code.google.com/p/word2vec.
T. Mikolov, Q. V. Le, and I. Sutskever. 2013b. Ex-
ploiting similarities among languages for machine
translation. In arXiv.
T. Mikolov, I. Sutskever, G. Corrado, and J. Dean.
2013c. Distributed representations of words and
phrases and their compositionality. In Proceedings
of NIPS.
Task2. 2010. Cross-lingual lexi-
cal substitution task, semeval-2010.
http://semeval2.fbk.eu/semeval2.php?location=tasksT24.
J. Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and in-
terfaces. In N. Nicolov and K. Bontcheva and G.
Angelova and R. Mitkov (eds.) Recent Advances in
Natural Language Processing (vol V), pages 237?
248, Amsterdam/Philadelphia. John Benjamins.
J. Tiedemann. 2012. Parallel data, tools and interfaces
in opus. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC?2012). http://opus.lingfil.uu.se/.
L. Wolf, Y. Hanani, K. Bar, and N. Derschowitz. 2014.
Joint word2vec networks for bilingual semantic rep-
resentations. In Poster sessions at CICLING.
T. Xiao, J. Zhu, S. Yao, and H. Zhang. 2011.
Document-level consistency verification in machine
translation. In Proc. of Machine Translation Summit
XIII, pages 131?138, China.
134
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 13?24,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Cross-lingual Dependency Parsing of Related Languages with Rich
Morphosyntactic Tagsets
?
Zeljko Agi
?
c J
?
org Tiedemann Kaja Dobrovoljc
zagic@uni-potsdam.de jorg.tiedemann@lingfil.uu.se kaja.dobrovoljc@trojina.si
Simon Krek Danijela Merkler Sara Mo?ze
simon.krek@ijs.si dmerkler@ffzg.hr s.moze@wlv.ac.uk
Abstract
This paper addresses cross-lingual depen-
dency parsing using rich morphosyntac-
tic tagsets. In our case study, we experi-
ment with three related Slavic languages:
Croatian, Serbian and Slovene. Four dif-
ferent dependency treebanks are used for
monolingual parsing, direct cross-lingual
parsing, and a recently introduced cross-
lingual parsing approach that utilizes sta-
tistical machine translation and annota-
tion projection. We argue for the benefits
of using rich morphosyntactic tagsets in
cross-lingual parsing and empirically sup-
port the claim by showing large improve-
ments over an impoverished common fea-
ture representation in form of a reduced
part-of-speech tagset. In the process, we
improve over the previous state-of-the-art
scores in dependency parsing for all three
languages.
1 Introduction
A large majority of human languages are under-
resourced in terms of text corpora and tools avail-
able for applications in natural language process-
ing (NLP). According to recent surveys (Bender,
2011; Uszkoreit and Rehm, 2012; Bender, 2013),
this is especially apparent with syntactically anno-
tated corpora, i.e., treebanks ? both dependency-
based ones and others. In this paper, we fo-
cus on dependency parsing (K?ubler et al., 2009),
but the claims should hold in general. The lack
of dependency treebanks is due to the fact that
they are expensive and time-consuming to con-
struct (Abeill?e, 2003). Since dependency parsing
of under-resourced languages nonetheless draws
substantial interest in the NLP research commu-
nity, over time, we have seen a number of research
efforts directed towards their processing despite
the absence of training data for supervised learn-
ing of parsing models. We give a brief overview of
the major research directions in the following sub-
section. Here, we focus on supervised learning of
dependency parsers, as the performance of unsu-
pervised approaches still falls far behind the state
of the art in supervised parser induction.
1.1 Related Work
There are two basic strategies for data-driven pars-
ing of languages with no dependency treebanks:
annotation projection and model transfer. Both
fall into the general category of cross-lingual de-
pendency parsing as they attempt to utilize ex-
isting dependency treebanks or parsers from a
resource-rich language (source) for parsing the
under-resourced (target) language.
Annotation projection: In this approach, de-
pendency trees are projected from a source lan-
guage to a target language using word alignments
in parallel corpora. It is based on a presumption
that source-target parallel corpora are more read-
ily available than dependency treebanks. The ap-
proach comes in two varieties. In the first one, par-
allel corpora are exploited by applying the avail-
able state-of-the-art parsers on the source side
and subsequent projection to the target side us-
ing word alignments and heuristics for resolving
possible link ambiguities (Yarowsky et al., 2001;
Hwa et al., 2005). Since dependency parsers typ-
ically make heavy use of various morphological
and other features, the apparent benefit of this ap-
proach is the possibility of straightforward pro-
jection of these features, resulting in a feature-
rich representation for the target language. On the
downside, the annotation projection noise adds up
to dependency parsing noise and errors in word
alignment, influencing the quality of the resulting
target language parser.
The other variety is rare, since it relies on paral-
lel corpora in which the source side is a depen-
13
dency treebank, i.e., it is already manually an-
notated for syntactic dependencies (Agi?c et al.,
2012). This removes the automatic parsing noise,
while the issues with word alignment and annota-
tion heuristics still remain.
Model transfer: In its simplest form, transfer-
ring a model amounts to training a source lan-
guage parser and running it directly on the target
language. It is usually coupled with delexicaliza-
tion, i.e., removing all lexical features from the
source treebank for training the parser (Zeman and
Resnik, 2008; McDonald et al., 2013). This in turn
relies on the same underlying feature model, typi-
cally drawing from a shared part-of-speech (POS)
representation such as the Universal POS Tagset of
Petrov et al. (2012). Negative effects of using such
an impoverished shared representation are typi-
cally addressed by adapting the model to better fit
the target language. This includes selecting source
language data points appropriate for the target lan-
guage (S?gaard, 2011; T?ackstr?om et al., 2013),
transferring from multiple sources (McDonald et
al., 2011) and using cross-lingual word clusters
(T?ackstr?om et al., 2012). These approaches need
no projection and enable the usage of source-side
gold standard annotations, but they all rely on
a shared feature representation across languages,
which can be seen as a strong bottleneck. Also,
while most of the earlier research made use of
heterogenous treebanks and thus yielded linguisti-
cally implausible observations, research stemming
from an uniform dependency scheme across lan-
guages (De Marneffe and Manning, 2008; Mc-
Donald et al., 2013) made it possible to perform
more consistent experiments and to assess the ac-
curacy of dependency labels.
Other approaches: More recently, Durrett et
al. (2012) suggested a hybrid approach that in-
volves bilingual lexica in cross-lingual phrase-
based parsing. In their approach, a source-side
treebank is adapted to a target language by ?trans-
lating? the source words to target words through
a bilingual lexicon. This approach is advanced
by Tiedemann et al. (2014), who utilize full-
scale statistical machine translation (SMT) sys-
tems for generating synthetic target language tree-
banks. This approach relates to annotation pro-
jection, while bypassing the issue of dependency
parsing noise as gold standard annotations are pro-
jected. The SMT noise is in turn mitigated by
better word alignment quality for synthetic data.
The influence of various projection algorithms in
this approach is further investigated by Tiedemann
(2014). This line of cross-lingual parsing research
substantially improves over previous work.
1.2 Paper Overview
All lines of previous cross-lingual parsing research
left the topics of related languages and shared rich
feature representations largely unaddressed, with
the exception of Zeman and Resnik (2008), who
deal with phrase-based parsing test-cased on Dan-
ish and Swedish treebanks, utilizing a mapping
over relatively small POS tagsets.
In our contribution, the goal is to observe the
properties of cross-lingual parsing in an envi-
ronment of relatively free-word-order languages,
which are related and characterized by rich mor-
phology and very large morphosyntactic tagsets.
We experiment with four different small- and
medium-size dependency treebanks of Croatian
and Slovene, and cross-lingually parse into Croa-
tian, Serbian and Slovene. Along with monolin-
gual and direct transfer parsing, we make use of
the SMT framework of Tiedemann et al. (2014).
We are motivated by:
? observing the performance of various ap-
proaches to cross-lingual dependency parsing
for closely related languages, including the very
recent treebank translation approach by Tiede-
mann et al. (2014);
? doing so by using rich morphosyntactic tagsets,
in contrast to virtually all other recent cross-
lingual dependency parsing experiments, which
mainly utilize the Universal POS tagset of
Petrov et al. (2012);
? reliably testing for labeled parsing accuracy in
an environment with heterogenous dependency
annotation schemes; and
? improving the state of the art for Croatian,
Slovene and Serbian dependency parsing across
these heterogenous schemes.
In Section 2, we describe the language resources
used: treebanks, tagsets and test sets. Section 3
describes the experimental setup, which includes
a description of parsing, machine translation and
annotation projection. In Section 4, we discuss the
results of the experiments, and we conclude the
discussion by sketching the possible directions for
future research in Section 5.
14
Figure 1: Histogram of edge distances in the tree-
banks. Edge distance is measured in tokens be-
tween heads and dependents. Distance of 1 de-
notes adjacent tokens.
Figure 2: Histogram of average tree depths.
2 Resources
We make use of the publicly available language re-
sources for Croatian, Serbian and Slovene. These
include dependency treebanks, test sets annotated
for morphology and dependency syntax, and a
morphosyntactic feature representation drawing
from the Multext East project (Erjavec, 2012).
A detailed assessment of the current state of de-
velopment for morphosyntactic and syntactic pro-
cessing of these languages is given by Agi?c et al.
(2013) and Uszkoreit and Rehm (2012). Here, we
provide only a short description.
2.1 Treebanks
We use two Croatian and two Slovene dependency
treebanks.
1
One for each language is based on the
Prague Dependency Treebank (PDT) (B?ohmov?a
et al., 2003) annotation scheme, while the other
two introduced novel and more simplified syntac-
tic tagsets. All four treebanks use adaptations of
1
No treebanks of Serbian were publicly available at the
time of conducting this experiment.
Feature hr PDT hr SET sl PDT sl SSJ
Sentences 4,626 8,655 1,534 11,217
Tokens 117,369 192,924 28,750 232,241
Types 25,038 37,749 7,128 48,234
Parts of speech 13 13 12 13
MSDs 821 685 725 1,142
Syntactic tags 26 15 26 10
Table 1: Basic treebank statistics.
the Multext East version 4 tagset (Erjavec, 2012)
for the underlying morphological annotation layer,
which we shortly describe further down. Basic
statistics for the treebanks are given in Table 1.
hr PDT: This treebank is natively referred to
as the Croatian Dependency Treebank (HOBS)
(Tadi?c, 2007; Berovi?c et al., 2012). Its most recent
instance, HOBS 2.0 (Agi?c et al., 2014) slightly de-
parts from the PDT scheme. Thus, in this exper-
iment, we use the older version, HOBS 1.0, and
henceforth refer to it as hr PDT for consistency and
more clear reference to its annotation.
2
hr SET: The SETIMES.HR dependency treebank
of Croatian has a 15-tag scheme. It is targeted
towards high parsing accuracy, while maintaining
a clear distinction between all basic grammatical
categories of Croatian. Its publicly available 1.0
release consists of approximately 2,500 sentences
(Agi?c and Merkler, 2013), while release 2.0 has
just under 4,000 sentences (Agi?c and Ljube?si?c,
2014) of newspaper text. Here, we use an even
newer, recently developed version with more than
8,500 sentences from multiple domains.
3
sl PDT: The PDT-based Slovene Dependency
Treebank (D?zeroski et al., 2006) is built on top of
a rather small portion of Orwell?s novel 1984 from
the Multext East project (Erjavec, 2012). Even if
the project was discontinued, it is still heavily used
as part of the venerable CoNLL 2006 and 2007
shared task datasets (Buchholz and Marsi, 2006;
Nivre et al., 2007).
4
sl SSJ: The Slovene take on simplifying syntac-
tic annotations resulted in the 10-tag strong JOS
Corpus of Slovene (Erjavec et al., 2010). Similar
to hr SET, this new annotation scheme is loosely
2
HOBS is available through META-SHARE (Tadi?c and
V?aradi, 2012).
3
http://nlp.ffzg.hr/resources/corpora/
setimes-hr/
4
http://nl.ijs.si/sdt/
15
PDT-based, but considerably reduced to facilitate
manual annotation. The initial 100,000 token cor-
pus has recently doubled in size, as described by
Dobrovoljc et al. (2012). We use the latter version
in our experiment.
5
The statistics in Table 1 show a variety of tree-
bank sizes and annotations. Figure 1 illustrates the
structural complexity of the treebanks by provid-
ing a histogram of egdes by token distance. While
adjacent edges expectedly dominate the distribu-
tions, it is interesting to see that almost 30% of
all edges in sl SSJ attach to root, resulting in an
easily parsable flattened tree structure. Knowing
that relations denoting attributes account for more
than one third of all non-root dependents in the re-
mainder, one can expect dependency parsing per-
formance comparable to CoNLL-style chunking
(Tjong Kim Sang and Buchholz, 2000). This is
further supported by the distributions of sentences
in the four treebanks by average tree depth in Fig-
ure 2. We can see that virtually all sl SSJ trees have
average depths of 1 to 3, while the other treebanks
exhibit the more common structural properties of
dependency trees.
In these terms of complexity, the Croatian tree-
banks are richer than their Slovene counterparts.
In sl SSJ, attributes and edges to root account for
more than 60% of all dependencies. Even in the
other three treebanks, 20-30% of the edges are la-
beled as attributes, while the rest is spread more
evenly between the basic syntactic categories such
as predicates, subject and objects. More detailed
and more linguistically motivated comparisons of
the three annotation guidelines fall outside the
scope of our paper. Instead, we refer to the pre-
viously noted publications on the respective tree-
banks, and to (Agi?c and Merkler, 2013; Agi?c et
al., 2013) for comparisons between PDT and SET
in parsing Croatian and Serbian.
2.2 Morphosyntactic Tagset
All four treebanks were manually created: they
are sentence- and token-split, lemmatized, mor-
phosyntactically tagged and syntactically anno-
tated. In morphosyntactic annotation, they all
make use of the Multext East version 4 (MTE
4) guidelines (Erjavec, 2012).
6
MTE 4 is a po-
sitional tagset in which morphosyntactic descrip-
tors of word forms are captured by a morphosyn-
5
http://eng.slovenscina.eu/
tehnologije/ucni-korpus
6
http://nl.ijs.si/ME/V4/
tactic tag (MSD) created by merging atomic at-
tributes in the predefined positions. This is illus-
trated in Table 2 through an example verb tag. The
first character of the tag denotes the part of speech
(POS), while each of the following characters en-
codes a specific attribute in a specific position.
Both the positions and the attributes are language-
dependent in MTE 4, but the attributes are still
largely shared between these three languages due
to their relatedness.
The Slovene treebanks closely adhere to the
specification, while each of the Croatian treebanks
implements slight adaptations of the tagset to-
wards Croatian specifics. In hr PDT, the adaptation
is governed by and documented in the Croatian
Morphological Lexicon (Tadi?c and Fulgosi, 2003),
and the modifications in hr SET were targeted to
more closely match the ones for Slovene.
7
2.3 Test Sets
Recent research by McDonald et al. (2013) has
uncovered the downsides of experimenting with
parsing using heterogenous dependency annota-
tions, while at the same time providing possi-
bly the first reliable results in cross-lingual pars-
ing. They did so by creating the uniformly anno-
tated Universal Dependency Treebanks collection
based on Stanford Typed Dependencies (De Marn-
effe and Manning, 2008), which in turn also en-
abled measuring both labeled (LAS) and unla-
beled (UAS) parsing accuracy.
Having four treebanks with three different an-
notation schemes, we seek to enable reliable ex-
perimentation through our test sets. Along with
Croatian and Slovene, which are represented in the
training sets, we introduce Serbian as a target-only
language in the test data. Following the CoNLL
shared tasks setup (Buchholz and Marsi, 2006;
Nivre et al., 2007), our test sets have 200 sentences
(approx. 5,000 tokens) per language, split 50:50
between newswire and Wikipedia text. Each test
set is manually annotated for morphosyntax, fol-
lowing the MTE 4 guidelines for the respective
languages, and checked by native speakers for va-
lidity. On top of that, all test sets are annotated
with all three dependency schemes: PDT, SET and
SSJ. This enables observing LAS in a heteroge-
nous experimental environment, as we test each
monolingual and cross-lingual parser on an anno-
7
http://nlp.ffzg.hr/data/tagging/
msd-hr.html
16
Language MSD tag Attribute-value pairs
hr Vmn Category = Verb, Type = main, Vform = infinitive
sl Vmen Category = Verb, Type = main, Aspect = perfective, VForm = infinitive
sr Vmn----an-n---e Category = Verb, Type = main, VForm = infinitive, Voice = active,
Negative = no, Clitic = no, Aspect = perfective
Table 2: Illustration of the Multext East version 4 tagset for Croatian, Serbian and Slovene. The attributes
are language-dependent, as well as their positions in the tag, which are also dependent on the part of
speech, denoted by position zero in the tag.
tation layer matching its training set. In contrast,
the MTE 4 tagsets are not adjusted, i.e., each test
set only has a single language-specific MTE 4 an-
notation. We rely on their underlying similarities
in feature representations to suffice for improved
cross-lingual parsing performance.
3 Experiment Setup
This section describes the experiment settings. We
list the general workflow of the experiment and
then provide the details on the parser setup and
the more advanced approaches used for target lan-
guage adaptation of the models.
3.1 Workflow
The experiment consists of three work packages:
(1) monolingual parsing, (2) direct cross-lingual
parsing, and (3) cross-lingual parsing using syn-
thetic training data from SMT. In the first one, we
train dependency parsers on the four treebanks and
test them on the corresponding languages, thus
assessing the monolingual parsing performance.
The second stage observes the effects of directly
applying the parsers from the first stage across the
languages. Finaly, in the third work package, we
use four different approaches to automatic transla-
tion to create synthetic training data. We translate
the Croatian treebanks to Slovene and vice versa,
project the annotations using two different projec-
tion algorithms, and train and apply the adapted
parsers across the languages. The details are in-
cluded in the two following subsections.
Two general remarks apply to our experiment.
First, we perform cross-lingual parsing, and not
cross-annotation-scheme parsing. Thus, we do not
compare the dependency parsing scores between
the annotation schemes, but rather just between
the in-scheme parsers. Second, we use Serbian as
a test-set-only language. As there are no treebanks
of Serbian, we cannot use it as a source language,
and we leave SMT and annotation projection into
Serbian for future work.
3.2 Dependency Parsing
In all experiments, we use the graph-based de-
pendency parser by Bohnet (2010) with default
settings. We base our parser choice on its state-
of-the-art performance across various morpholog-
ically rich languages in the SPMLR 2013 shared
task (Seddah et al., 2013). While newer contribu-
tions targeted at joint morphological and syntactic
analysis (Bohnet and Kuhn, 2012; Bohnet et al.,
2013) report slightly higher scores, we chose the
former one for speed and robustness, and because
we use gold standard POS/MSD annotations. The
choice of gold standard preprocessing is motivated
by previous research in parsing Croatian and Ser-
bian (Agi?c et al., 2013), and by insight of Sed-
dah et al. (2013), who report a predictable linear
decrease in accuracy for automatic preprocessing.
This decrease amounts to approximately 3 points
LAS for Croatian and Serbian across various test
cases in (Agi?c et al., 2013).
We observe effects of (de)lexicalization and of
using full MSD tagset as opposed to only POS tags
in all experiments. Namely, in all work packages,
we compare parsers trained with {lexicalized,
delexicalized} ? {MSD, POS} features. In lexi-
calized parsers, we use word forms and features,
while we exclude lemmas from all experiments ?
both previous research using MSTParser (McDon-
ald et al., 2005) and our own test runs show no
use for lemmas as features in dependency parsing.
Delexicalized parsers are stripped of all lexical
features, i.e., word forms are omitted from training
and testing data. Full MSD parsers use both the
POS information and the sub-POS features in the
form of atomic attribute-value pairs, while POS-
only parsers are stripped of the MSD features ?
they use just the POS information. The delexi-
calized POS scenario is thus very similar to the
17
direct transfer by McDonald et al. (2013), since
MTE 4 POS is virtually identical to Universal POS
(Petrov et al., 2012).
8
3.3 Treebank Translation and Annotation
Projection
For machine translation, we closely adhere to the
setup implemented by Tiedemann et al. (2014) in
their treebank translation experiments. Namely,
our translations are based on automatic word
alignment and subsequent extraction of translation
equivalents as common in phrase-based SMT. We
perform word alignment by using GIZA++ (Och
and Ney, 2003), while utilizing IBM model 4 for
creating the Viterbi word alignments for parallel
corpora. For the extraction of translation tables,
we use the de facto standard SMT toolbox Moses
(Koehn et al., 2007) with default settings. Phrase-
based SMT models are tuned using minimum er-
ror rate training (Och, 2003). Our monolingual
language modeling using KenLM tools
9
(Heafield,
2011) produces standard 5-gram language mod-
els using modified Kneser-Ney smoothing without
pruning.
For building the translation models, we use
the OpenSubtitles parallel resources from OPUS
10
(Tiedemann, 2009) for the Croatian-Slovene pair.
Even if we expect this to be a rather noisy paral-
lel resource, we justify the choice by (1) the fact
that no other parallel corpora
11
of Croatian and
Slovene exist, other than Orwell?s 1984 from the
Multext East project, which is too small for SMT
training and falls into a very narrow domain, and
(2) evidence from (Tiedemann et al., 2014) that the
SMT-supported cross-lingual parsing approach is
very robust to translation noise.
For translating Croatian treebanks into Slovene
and vice versa, we implement and test four dif-
ferent methods of translation. They are coupled
with approaches to annotation projection from the
source side gold dependency trees to the target
translations via the word alignment information
available from SMT.
8
A mapping from Slovene MTE 4 to Universal
POS is available at https://code.google.com/p/
universal-pos-tags/ as an example.
9
https://kheafield.com/code/kenlm/
10
http://opus.lingfil.uu.se/
11
We note the Croatian-Slovene parallel corpus project de-
scribed by Po?zgaj Had?zi and Tadi?c (2000), but it appears that
the project was not completed and the corpus itself is not pub-
licly available.
LOOKUP: The first approach to translation in
our experiment is the dictionary lookup approach.
We simply select the most reliable translations of
single words in the source language into the tar-
get language by looking up the phrase translation
tables extracted from the parallel corpus. This is
very similar to what Agi?c et al. (2012) did for the
Croatian-Slovene pair. However, their approach
involved both translating and testing on the same
small corpus (Orwell?s novel), while here we ex-
tract the translations from full-blown SMT phrase
tables on a much larger scale. The trees projec-
tion from source to target is trivial since the num-
ber and the ordering of words between them does
not change. Thus, the dependencies are simply
copied.
CHAR: By this acronym, we refer to an ap-
proach known as character-based statistical ma-
chine translation. It is shown to perform very
well for closely related languages (Vilar et al.,
2007; Tiedemann, 2012; Tiedemann and Nakov,
2013). The motivation for character-level transla-
tion is the ability of such models to better gener-
alize the mapping between similar languages es-
pecially in cases of rich productive morphology
and limited amounts of training data. With this,
character-level models largely reduce the num-
ber of out-of-vocabulary words. In a nutshell,
our character-based model performs word-to-word
translation using character-level modeling. Simi-
lar to LOOKUP, this is also a word-to-word trans-
lation model, which also requires no adaptation of
the source dependency trees ? they are once again
simply copied to target sentences.
WORD: Our third take on SMT is slightly more
elaborate but still restricts the translation model
to one-to-one word mappings. In particular, we
extract all single word translation pairs from the
phrase tables and apply the standard beam-search
decoder implemented in Moses to translate the
original treebanks to all target languages. Thus,
we allow word reordering and use a language
model while still keeping the projection of anno-
tated data as simple as possible. The language
model may influence not only the word order but
also the lexical choice as we now allow multiple
translation options in our phrase table. Also note
that this approach may introduce additional non-
projectivity in the projected trees. This system
is the overall top-performer in (Tiedemann et al.,
18
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Vl
ad
a
pla
nir
a
otv
ori
ti
inf
orm
ati
vn
e
ure
de
Vl
ad
an
ac?r
tuj
eo
dp
rtj
ei
nfo
rm
aci
jsk
ep
isa
rne
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Sb
Pre
d
Atv
Atr
Obj
Sb
Pre
d
Atv
Atr
Obj
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Vl
ad
a
pla
nir
a
otv
ori
ti
inf
orm
ati
vn
e
ure
de
Vl
ad
an
ac?r
tuj
eo
dp
rtj
e
pis
arn
e
inf
orm
ati
vn
e
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Sb
Pre
d
Atv
Atr
Obj
Sb
Pre
d
Atv
Atr
Obj
Nc
fs
n
Vm
ip
3s
Vm
n
Af
pm
pa
Nc
mp
a
Vl
ad
a
pla
nir
a
otv
ori
ti
inf
orm
ati
vn
e
ure
de
Vl
ad
an
ac?r
tuj
e
,
da
bo
od
prl
a
DU
MM
Y
inf
orm
aci
jsk
ep
isa
rne
Nc
fs
n
Vm
ip
3s
--
du
mm
y
du
mm
y
du
mm
y
Vm
n
Af
pm
pa
Nc
mp
a
Sb
Pre
d
Pre
d
Atv
Obj
Sb
Pre
d
Atv
dum
mydu
mmyd
umm
y
Obj
Atr
Figure 3: An illustration of the projections. Left side = CHAR, middle = WORD, right side = PHRASE. As
illustrated, WORD might introduce reorderings, while PHRASE can enter dummy nodes and edges to the
dependency trees. The sentence: The government plans to open information offices. See (Tiedemann et
al., 2014; Tiedemann, 2014) for detailed insight into projection algorithms.
2014), where reordering played an important role
in adapting the models to the target languages. We
test whether it holds for related languages as well.
PHRASE: This model implements translation
based on the entire phrase table using the standard
approach to phrase-based SMT. We basically run
the Moses decoder with default settings and the
parameters and models trained on our parallel cor-
pus. Here, we can have many-to-many word align-
ments, which require a more elaborate approach to
the projection of the source side dependency an-
notatio s. It is important for the annotation trans-
fer to keep track of the alignment between phrases
and words of the input and output sentences. The
Moses decoder provides both, phrase seg enta-
tion and word alignment. We use the annotation
projection algorithm of Hwa et al. (2005). As
illustrated in Figure 3, it resolves many-to-many
alignments by introducing dummy nodes to the
dependency trees. We use the implementation by
Tiedemann (2014), which addresses certain issues
with algorithm choices for ambiguous alignments
which were left unaccounted for in the original
work. Since this paper does not focus on the intri-
cacies of annotation projection, but rather on ap-
plying it in an environment of related languages
and rich MSD tagsets, we refer the reader to re-
lated work regarding the details.
We translate from Croatian to Slovene and vice
versa using four different treebanks and these
four different methods of translation and annota-
tion projection. As we stated in the experiment
overview, for each of these, we also experiment
with (de)lexicalization and MSD vs. POS, and we
test on all three languages. The three experimental
batches ? monolingual, direct and SMT-supported
transfer ? produce a large number of observations,
all of which we assess in the following section.
4 Results and Discussion
We split our discussion of the parsing results into
the following three subsections. We first observe
the performance of monolingual parsers. Sec-
ondly, we measure the quality of these when ap-
plied directly on the other two languages. Finally,
we look into the accuracy of parsers trained on
SMT-generated artificial treebank data when ap-
plied across the test languages.
4.1 Monolingual Parsing
Accuracies of parsers trained and applied on train-
ing and testing data belonging to the same lan-
guage ? i.e., our monolingual parsers ? are pro-
vided in the g ayed out sections of Table 3.
Parsing Croatian using hr PDT yields a high
score of 69.45 LAS, better than the former state
of the art on this test set (Agi?c et al., 2013) simply
due to applying a newer generation parser. This
score is provided by a lexicalized model with the
full MSD feature set. Replacing MSD with POS or
delexicalizing this model results in a 3-point drop
in LAS, while applying both replacements sub-
stantially decreases the score ? by more than 11
points LAS. We observe virtually the same pattern
for the other Croatian treebank, hr SET, where this
latter drop is even more significant, at 14 points.
Incidentally, 76.36 points LAS is also the new
state of the art for hr SET parsing, owing to the
recent enlargement of the treebank.
The Slovene parsers exhibit effectively the same
behavior as the Croatian ones. The lexicalized
MSD models of sl PDT and sl SSJ both record new
state-of-the-art scores, although the latter one on a
different test set than in previous research (Dobro-
voljc et al., 2012). At over 92 points LAS, sl SSJ
19
lexicalized delexicalized
hr sl sr hr sl sr
MSD POS MSD POS MSD POS MSD POS MSD POS MSD POS
hr PDT 69.45 66.95 60.09 50.19 69.42 66.96 66.03 57.79 57.98 42.66 66.79 57.41
SET 76.36 73.02 68.65 59.52 76.08 73.37 72.52 62.31 68.16 55.17 72.71 62.04
sl PDT 51.19 47.99 76.46 73.33 52.46 49.64 49.58 42.59 71.96 62.99 50.41 44.11
SSJ 78.50 74.18 92.38 88.93 78.94 75.96 75.23 66.23 87.19 77.92 75.25 67.47
Table 3: Monolingual and direct cross-lingual parsing accuracy, expressed by the labeled accuracy metric
(LAS). Scores are split for lexicalized and delexicalized, full MSD and POS only parsers. Monolingual
scores are in grey. Row indices represent source languages and treebanks.
expectedly shows to be the easiest to parse, most
likely due to the relatively flat tree structure and its
small label set.
We note the following general pattern of fea-
ture importance. Dropping MSD features seems
to carry the most weight in all models, followed
by lexicalization. Dropping MSD is compensated
in part by lexical features paired with POS, while
dropping both MSD and word forms severely de-
grades all models. At this point, it is very impor-
tant to note that at 60-70 points LAS, these de-
creased scores closely resemble those of McDon-
ald et al. (2013) for the six languages in the Uni-
versal Treebanks. This observation is taken further
in the next subsection.
4.2 Direct Cross-lingual Parsing
The models used for monolingual parsing are here
directly applied on all languages but the treebank
source language, thus constituting a direct cross-
lingual parsing scenario. Its scores are also given
in Table 3, but now in the non-grey parts.
Croatian models are applied to Slovene and Ser-
bian test sets. For hr PDT, the highest score is
60.09 LAS on Slovene and 69.42 LAS on Serbian,
the latter noted as the state of the art for Serbian
PDT parsing. Comparing the cross-lingual score to
monolingual Slovene, the difference is substantial
as expected and comparable to the drops observed
by McDonald et al. (2013) in their experiments.
Our ranking of feature significance established in
the monolingual experiments holds here as well,
or rather, the absolute differences are even more
pronounced. Most notably, the difference between
the lexicalized MSD model and the delexicalized
POS model is 17 points LAS in favor of the for-
mer one on Slovene. hr SET appears to be more
resilient to delexicalization and tagset reduction
when applied on Slovene and Serbian, most likely
due to the treebank?s size, well-balanced depen-
dency label set and closer conformance with the
official MTE 4 guidelines. That said, the feature
patterns still hold. Also, 76.08 LAS for Serbian is
the new state of the art for SET parsing.
Slovene PDT is an outlier due to its small size,
as its training set is just over 1,500 sentences. Still,
the scores maintain the level of those in related
research, and the feature rankings hold. Perfor-
mance of parsing Croatian and Serbian using sl
SSJ is high, arguably up to the level of usability
in down-stream applications. These are the first
recorded scores in parsing the two languages us-
ing SSJ, and they reach above 78 points LAS for
both. Even if the scores are not comparable across
the annotation schemes due to their differences, it
still holds that the SSJ scores are the highest ab-
solute parsing scores recorded in the experiment.
This might hold significance in applications that
require robust parsing for shallow syntax.
Generally, the best transfer scores are quite
high in comparison with those on Universal Tree-
banks (McDonald et al., 2013; Tiedemann et al.,
2014). This is surely due to the relatedness of
the three languages. However, even for these ar-
guably closely related languages, the performance
of delexicalized models that rely only on POS fea-
tures ? averaging at around 55 points LAS ? is vir-
tually identical to that on more distant languages
test-cased in related work. We see this as a very
strong indicator of fundamental limitations of us-
ing linguistically impoverished shared feature rep-
resentations in cross-lingual parsing.
4.3 Cross-lingual Parsing with Treebank
Translation
Finally, we discuss what happens to parsing per-
formance when we replace direct cross-lingual ap-
plication of parsers with training models on trans-
lated treebanks. We take a treebank, Croatian or
Slovene, and translate it into the other language.
20
Target Approach PDT SET SSJ
hr monolingual 69.45 76.36 ?
direct 51.19 ? 78.50
translated 67.55 ? 74.68 ? 79.51 ?
sl monolingual 76.46 ? 92.38
direct 60.09 68.65 ?
translated 72.35 ? 70.52 ? 88.71 ?
sr monolingual ? ? ?
direct 69.42 76.08 78.94
translated 68.11 ? 74.31 ? 79.81 ??
Legend: ? CHAR ? LOOKUP ? PHRASE ? WORD
Table 4: Parsing score (LAS) summary for the top-
performing systems with respect to language and
approach to parser induction. All models are MSD
+ lexicalized.
We then train a parser on the translation and ap-
ply it on all three target test sets. We do this for all
the treebanks, and in all variations regarding trans-
lation and projection methods, morphological fea-
tures and lexicalization.
All scores for this evaluation stage are given in
Table 5 for completeness. The table contains 192
different LAS scores, possibly constituting a te-
dious read. Thus, in Table 4 we provide a sum-
mary of information on the top-performing parsers
from all three experimental stages, which includes
treebank translation.
We can see that the best models based on
translating the treebanks predominantly stem from
word-to-word SMT, i.e., from WORD transla-
tion models that basically enrich the lexical fea-
ture space and perform word reordering, enabling
straightforward copying of syntactic structures
from translation sources to translation targets. Fol-
lowing them are the CHAR and LOOKUP models,
expectedly leaving ? although not too far behind
? PHRASE behind given the similarities of the lan-
guage pair. Since Croatian and Slovene are related
languages, the differences between the models are
not as substantial as in (Tiedemann et al., 2014),
but WORD models still turn out to be the most ro-
bust ones, even if word reordering might not be so
frequent in this language pair as in the data from
(McDonald et al., 2013). Further, when compar-
ing the best SMT-supported models to monolin-
gual parsers, we see that the models with trans-
lation come really close to monolingual perfor-
mance. In comparison with direct transfer, models
trained on translated treebanks manage to outper-
form them in most cases, especially for the more
distant language pairs. For example, the sl ? hr
SSJ WORD model is 1 point LAS better on Croat-
ian than the directly applied Slovene model, and
the same holds for testing on Serbian with the
same dataset. On the other side, directly applied
models from Croatian SET outperform the trans-
lated ones for Serbian. For PDT, the translated
models are substantially better between Croatian
and Slovene since sl PDT is an outlier in terms
of size and dataset selection, while direct trans-
fer from Croatian seems to work better for Serbian
than the translated models.
Reflecting on the summary in Table 4 more
generally, by and large, we see high parsing ac-
curacies. Averages across the formalisms reach
well beyond 70 points LAS. We attribute this to
the relatedness of the languages selected for this
case study, as well as to the quality of the un-
derlying language resources. From another view-
point, the table clearly shows the prominence of
lexical and especially rich morphosyntactic tagset
features throughout the experiment. Across our
monolingual, direct and SMT-supported parsing
experiments, these features are represented in the
best systems, and dropping them incurs significant
decreases in accuracy.
5 Conclusions and Future Work
In this contribution, we addressed the topic of
cross-lingual dependency parsing, i.e., applying
dependency parsers from typically resource-rich
source languages to under-resourced target lan-
guages. We used three Slavic languages ? Croat-
ian, Slovene and Serbian ? as a test case for related
languages in different stages of language resource
development. As these are relatively free-word-
order languages with rich morphology, we were
able to test the cross-lingual parsers for perfor-
mance when using training features drawing from
large morphosyntactic tagsets ? typically consist-
ing of over 1,000 different tags ? in contrast to
impoverished common part-of-speech representa-
tions. We tested monolingual parsing, direct cross-
lingual parsing and a very recent promising ap-
proach with artificial creation of training data via
machine translation. In the experiments, we ob-
served state-of-the-art results in dependency pars-
ing for all three languages. We strongly argued
and supported the case for using common rich rep-
resentations of morphology in dependency parsing
21
lexicalized delexicalized
hr sl sr hr sl sr
MSD POS MSD POS MSD POS MSD POS MSD POS MSD POS
CHAR hr ? sl PDT 66.92 60.25 61.49 55.57 67.83 62.04 66.56 57.63 58.34 43.04 66.89 57.65
SET 73.65 64.64 70.52 66.11 72.95 64.44 72.98 62.98 69.03 54.81 72.74 62.73
sl ? hr PDT 51.96 48.14 72.35 63.71 53.11 49.47 49.58 42.59 71.96 62.99 50.41 44.11
SSJ 78.69 75.45 88.21 78.88 79.25 77.09 75.23 66.23 87.19 77.92 75.25 67.47
LOOKUP hr ? sl PDT 67.55 59.96 60.81 56.54 67.78 61.41 66.56 57.63 58.34 43.04 66.89 57.65
SET 73.58 64.98 69.93 68.09 73.70 64.25 72.52 62.72 68.47 55.27 72.71 62.73
sl ? hr PDT 51.74 49.15 72.02 63.08 53.49 51.33 49.58 42.59 71.96 62.99 50.41 44.11
SSJ 79.25 77.06 88.10 78.53 79.81 77.23 75.23 66.23 87.19 77.92 75.25 67.47
WORD hr ? sl PDT 67.33 59.24 61.80 57.14 68.11 61.13 65.84 57.12 58.17 42.99 67.12 57.70
SET 73.26 65.87 69.98 68.98 73.63 65.85 72.71 62.29 68.50 55.06 73.14 62.40
sl ? hr PDT 51.67 49.58 71.47 63.51 54.62 51.82 50.25 43.17 71.27 62.79 50.79 44.07
SSJ 79.51 76.89 88.71 79.69 79.81 78.03 75.95 67.19 86.92 77.28 75.89 68.18
PHRASE hr ? sl PDT 67.28 58.90 60.53 56.79 67.92 61.36 65.77 55.06 58.18 45.41 66.16 55.79
SET 74.68 65.29 69.42 68.55 74.31 65.17 73.36 60.77 68.16 58.42 72.15 61.55
sl ? hr PDT 49.92 46.82 68.18 58.18 52.15 49.42 47.73 41.08 68.51 55.29 48.93 42.59
SSJ 79.29 78.09 88.24 78.75 79.32 78.85 75.33 68.10 86.59 75.66 75.91 68.67
Table 5: Parsing scores (LAS) for cross-lingual parsers trained on translated treebanks. Scores are
split for lexicalized and delexicalized, full MSD and POS only parsers, and with respect to the trans-
lation/projection approaches. Row indices represent source languages and treebanks, and indicate the
direction of applying SMT (e.g., hr ? sl denotes a Croatian treebank translated to Slovene).
for morphologically rich languages. Through our
multilayered test set annotation, we also facilitated
a reliable cross-lingual evaluation in a heteroge-
nous testing environment. We list our most impor-
tant observations:
? Even for closely related languages, using only
the basic POS features ? which are virtually
identical to the widely-used Universal POS of
Petrov et al. (2012) ? substantially decreases
parsing accuracy up to the level comparable with
results of McDonald et al. (2013) across the Uni-
versal Treebanks language groups.
? Adding MSD features heavily influences all the
scores in a positive way. This has obvious im-
plications for improving over McDonald et al.
(2013) on the Universal Treebanks dataset.
? Other than that, we show that it is possible
to cross-lingually parse Croatian, Serbian and
Slovene using all three syntactic annotation
schemes, and with high accuracy. A treebank for
Serbian does not exist, but we accurately parse
Serbian by using PDT, SET and SSJ-style annota-
tions. We parse Croatian using SSJ (transferred
from Slovene) and Slovene using SSJ (trans-
ferred from Croatian). This clearly indicates the
possibilities of uniform downstream pipelining
for any of the schemes.
? We show clear benefits of using the SMT ap-
proach for transferring SSJ parsers to Croatian
and SET parsers to Slovene. We observe these
benefits regardless of the low-quality, out-of-
domain SMT training data (OpenSubs).
Given the current interest for cross-lingual depen-
dency parsing in the natural language processing
community, we will seek to further test our obser-
vations on shared morphological features by us-
ing other pairs of languages of varying relatedness,
drawing from datasets such as Google Universal
Treebanks (McDonald et al., 2013) or HamleDT
(Zeman et al., 2012; Rosa et al., 2014). The goal
of cross-lingual processing in general is to enable
improved general access to under-resourced lan-
guages. With this in mind, seeing how we intro-
duced a test case of Serbian as a language cur-
rently without a treebank, we hope to explore other
options for performing cross-lingual experiments
on actual under-resourced languages, rather than
in an exclusive group of resource-rich placehold-
ers, possibly by means of down-stream evaluation.
Acknowledgments The second author was sup-
ported by the Swedish Research Council (Veten-
skapsr?adet), project 2012-916. The fifth author is
funded by the EU FP7 STREP project XLike.
22
References
Anne Abeill?e. 2003. Treebanks: Building and Using
Parsed Corpora. Springer.
?
Zeljko Agi?c and Nikola Ljube?si?c. 2014. The SE-
Times.HR Linguistically Annotated Corpus of Croa-
tian. In Proc. LREC, pages 1724?1727.
?
Zeljko Agi?c and Danijela Merkler. 2013. Three
Syntactic Formalisms for Data-Driven Dependency
Parsing of Croatian. LNCS, 8082:560?567.
?
Zeljko Agi?c, Danijela Merkler, and Da?sa Berovi?c.
2012. Slovene-Croatian Treebank Transfer Using
Bilingual Lexicon Improves Croatian Dependency
Parsing. In Proc. IS-LTC, pages 5?9.
?
Zeljko Agi?c, Danijela Merkler, and Da?sa Berovi?c.
2013. Parsing Croatian and Serbian by Using Croat-
ian Dependency Treebanks. In Proc. SPMRL, pages
22?33.
?
Zeljko Agi?c, Da?sa Berovi?c, Danijela Merkler, and
Marko Tadi?c. 2014. Croatian Dependency Tree-
bank 2.0: New Annotation Guidelines for Improved
Parsing. In Proc. LREC, pages 2313?2319.
Emily Bender. 2011. On achieving and evaluating
language-independence in nlp. Linguistic Issues in
Language Technology, 6(3):1?26.
Emily Bender. 2013. Linguistic Fundamentals for
Natural Language Processing: 100 Essentials from
Morphology and Syntax. Morgan & Claypool Pub-
lishers.
Da?sa Berovi?c,
?
Zeljko Agi?c, and Marko Tadi?c. 2012.
Croatian Dependency Treebank: Recent Develop-
ment and Initial Experiments. In Proc. LREC, pages
1902?1906.
Alena B?ohmov?a, Jan Haji?c, Eva Haji?cov?a, and Barbora
Hladk?a. 2003. The Prague Dependency Treebank.
In Treebanks, pages 103?127.
Bernd Bohnet and Jonas Kuhn. 2012. The Best of
Both Worlds ? A Graph-based Completion Model
for Transition-based Parsers. In Proc. EACL, pages
77?87.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky,
Rich?ard Farkas, Filip Ginter, and Jan Hajic. 2013.
Joint Morphological and Syntactic Analysis for
Richly Inflected Languages. TACL, 1:415?428.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proc. COL-
ING, pages 89?97.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proc. CoNLL, pages 149?164.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The Stanford Typed Dependencies Rep-
resentation. In Proc. COLING, pages 1?8.
Kaja Dobrovoljc, Simon Krek, and Jan Rupnik. 2012.
Skladenjski raz?clenjevalnik za sloven?s?cino. In Proc.
IS-LTC, pages 42?47.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic Transfer Using a Bilingual Lexicon. In Proc.
EMNLP-CoNLL, pages 1?11.
Sa?so D?zeroski, Toma?z Erjavec, Nina Ledinek, Petr Pa-
jas, Zdenek
?
Zabokrtsky, and Andreja
?
Zele. 2006.
Towards a Slovene Dependency Treebank. In Proc.
LREC, pages 1388?1391.
Toma?z Erjavec, Darja Fi?ser, Simon Krek, and Nina
Ledinek. 2010. The JOS Linguistically Tagged Cor-
pus of Slovene. In Proc. LREC, pages 1806?1809.
Toma?z Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic Resources for Central and Eastern European
Languages. Language Resources and Evaluation,
46(1):131?142.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proc. WSMT, pages
187?197.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping Parsers via Syntactic Projection across Parallel
Texts. Natural Language Engineering, 11(3):311?
325.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
ACL, pages 177?180.
Sandra K?ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan & Claypool
Publishers.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective Dependency Pars-
ing Using Spanning Tree Algorithms. In Proc. HLT-
EMNLP, pages 523?530.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-Source Transfer of Delexicalized Dependency
Parsers. In Proc. EMNLP, pages 62?72.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria
Bertomeu Castell?o, and Jungmee Lee. 2013.
Universal Dependency Annotation for Multilingual
Parsing. In Proc. ACL, pages 92?97.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In Proc. CoNLL, pages 915?932.
23
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. ACL,
pages 160?167.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A Universal Part-of-Speech Tagset. In Proc. LREC,
pages 2089?2096.
Vesna Po?zgaj Had?zi and Marko Tadi?c. 2000. Croatian-
Slovene Parallel Corpus. In Proc. IS-LTC.
Rudolf Rosa, Jan Ma?sek, David Mare?cek, Martin
Popel, Daniel Zeman, and Zden?ek
?
Zabokrtsk?y.
2014. HamleDT 2.0: Thirty Dependency Treebanks
Stanfordized. In Proc. LREC, pages 2334?2341.
Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, Marie
Candito, Jinho D. Choi, Rich?ard Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepi?orkowski, Ryan Roth, Wolfgang
Seeker, Yannick Versley, Veronika Vincze, Marcin
Woli?nski, Alina Wr?oblewska, and Eric Villemonte
de la Clergerie. 2013. Overview of the SPMRL
2013 Shared Task: Cross-framework Evaluation of
Parsing Morphologically Rich Languages. In Proc.
SPMRL, pages 146?182.
Anders S?gaard. 2011. Data Point Selection for Cross-
language Adaptation of Dependency Parsers. In
Proc. ACL, pages 682?686.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. In Proc. NAACL,
pages 477?487.
Oscar T?ackstr?om, Ryan McDonald, and Joakim Nivre.
2013. Target Language Adaptation of Discrimina-
tive Transfer Parsers. In Proc. NAACL, pages 1061?
1071.
Marko Tadi?c and Sanja Fulgosi. 2003. Building the
Croatian Morphological Lexicon. In Proc. BSNLP,
pages 41?46.
Marko Tadi?c and Tam?as V?aradi. 2012. Central and
South-East European Resources in META-SHARE.
Proc. COLING, pages 431?438.
Marko Tadi?c. 2007. Building the Croatian Depen-
dency Treebank: The Initial Stages. Suvremena
lingvistika, 63:85?92.
J?org Tiedemann and Preslav Nakov. 2013. Analyzing
the Use of Character-Level Translation with Sparse
and Noisy Datasets. In Proc. RANLP, pages 676?
684.
J?org Tiedemann,
?
Zeljko Agi?c, and Joakim Nivre. 2014.
Treebank Translation for Cross-Lingual Parser In-
duction. In Proc. CoNLL, pages 130?140.
J?org Tiedemann. 2009. News from OPUS: A Collec-
tion of Multilingual Parallel Corpora with Tools and
Interfaces. In Proc. RANLP, volume 5, pages 237?
248.
J?org Tiedemann. 2012. Character-Based Pivot Trans-
lations for Under-Resourced Languages and Do-
mains. In Proc. EACL, pages 141?151.
J?org Tiedemann. 2014. Rediscovering Annotation
Projection for Cross-Lingual Parser Induction. In
Proc. COLING.
Erik F Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the CoNLL-2000 Shared Task:
Chunking. In Proc. CoNLL, pages 127?132.
Hans Uszkoreit and Georg Rehm. 2012. Language
White Paper Series. Springer.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can We Translate Letters? In Proc. WMT,
pages 33?39.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing Multilingual Text Analy-
sis Tools via Robust Projection Across Aligned Cor-
pora. In Proc. HLT, pages 1?8.
Daniel Zeman and Philip Resnik. 2008. Cross-
Language Parser Adaptation between Related Lan-
guages. In Proc. IJCNLP, pages 35?42.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan Step?anek, Zdenek
Zabokrtsk`y, and Jan Hajic. 2012. HamleDT: To
Parse or Not to Parse? In Proc. LREC, pages 2735?
2741.
24
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58?67,
Dublin, Ireland, August 23 2014.
A Report on the DSL Shared Task 2014
Marcos Zampieri
1
, Liling Tan
2
, Nikola Ljube
?
si
?
c
3
, J
?
org Tiedemann
4
Saarland University, Germany
1,2
University of Zagreb, Croatia
3
Uppsala University, Sweden
4
marcos.zampieri@uni-saarland.de, liling.tan@uni-saarland.de
jorg.tiedemann@lingfil.uu.se, nljubesi@ffzg.hr
Abstract
This paper summarizes the methods, results and findings of the Discriminating between Similar
Languages (DSL) shared task 2014. The shared task provided data from 13 different languages
and varieties divided into 6 groups. Participants were required to train their systems to discrimi-
nate between languages on a training and development set containing 20,000 sentences from each
language (closed submission) and/or any other dataset (open submission). One month later, a test
set containing 1,000 unidentified instances per language was released for evaluation. The DSL
shared task received 22 inscriptions and 8 final submissions. The best system obtained 95.7%
average accuracy.
1 Introduction
Discriminating between similar languages is one of the bottlenecks of state-of-the-art language iden-
tification systems. Although in recent years systems have been trained to discriminate between more
languages
1
, they still struggle to discriminate between similar languages such as Croatian and Serbian or
Malay and Indonesian.
From an NLP point of view, the difficulty systems face when discriminating between closely related
languages is similar to the problem of discriminating between standard national language varieties (e.g.
American English and British English or Brazilian Portuguese and European Portuguese), henceforth
varieties. Recent studies show that language varieties can be discriminated automatically using words or
characters as features (Zampieri and Gebre, 2012; Lui and Cook, 2013) . However, due to performance
limitations, state-of-the-art general-purpose language identification systems do not distinguish texts from
different national varieties, modelling pluricentric languages as unique classes.
To evaluate how state-of-the-art systems perform in identifying similar languages and varieties, we
decided to organize the Discriminating between Similar Languages (DSL)
2
shared task. This shared task
was organized within the scope of the workshop on Applying NLP Tools to Similar Languages, Varieties
and Dialects (VarDial) in the 2014 edition of COLING.
The motivation behind the DSL shared task is two-fold. Firstly, we have observed an increase of
interest in the topic. This is reflected by a number of papers that have been published about this task in
recent years starting with Ranaivo-Malanc?on (2006) for Malay and Indonesian and Ljube?si?c et al. (2007)
for South Slavic languages. In the DSL shared task we tried to include (depending on the availability of
data) languages that have been studied in previous experiments, such as Croatian, English, Indonesian,
Malay, Portuguese and Spanish.
The second aspect that motivated us to organize this shared task is that, to our knowledge, no shared
task focusing on the discrimination of similar languages has been organized previously. The most sim-
ilar shared tasks to DSL are the DEFT 2010 shared task (Grouin et al., 2010), in which systems were
required to classify French journalistic texts with respect to their geographical location as well as the
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Brown (2013) reports results on a system trained to recognize more than 1,100 languages
2
http://corporavm.uni-koeln.de/vardial/sharedtask.html
58
decade in which they were published. Other related shared tasks include the ALTW 2010 multilingual
language identification shared task, a general-purpose language identification task containing data from
74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task
(Tetreault et al., 2013) where participants were provided English essays written by foreign students of 11
different mother tongues (Blanchard et al., 2013). Participants had to train their systems to identify the
native language of the writer of each text.
2 Related Work
Among the first studies to investigate the question of discriminating between similar languages is the
study published by Ranaivo-Malanc?on (2006). The author presents a semi-supervised model to dis-
tinguish between Indonesian and Malay, two closely related languages from the Austronesian family
represented in the DSL shared task. The study uses the frequency and rank of character trigrams derived
from the most frequent words in each language, lists of exclusive words, and the format of numbers
(Malay uses decimal points whereas Indonesian uses commas). The author compares the performance
of this method with the performance obtained by TextCat (Cavnar and Trenkle, 1994).
Ljube?si?c et al. (2007) proposed a computational model for the identification of Croatian texts in
comparison to Slovene and Serbian, reporting 99% recall and precision in three processing stages. The
approach includes a ?black list?, which increases the performance of the algorithm. Tiedemann and
Ljube?si?c (2012) improved this method and applied it to Bosnian, Croatian and Serbian texts. The study
reports significantly higher performance compared to general purpose language identification methods.
The methods applied to discriminate between texts from different language varieties and dialects are
similar to those applied to similar languages
3
. One of the methods proposed to identify language varieties
is by Huang and Lee (2008). This study presented a bag-of-words approach to classify Chinese texts from
the mainland and Taiwan with results of up to 92% accuracy.
Another study that focused on language varieties is the one published by Zampieri and Gebre (2012).
In this study, the authors proposed a log-likelihood estimation method along with Laplace smoothing to
identify two varieties of Portuguese (Brazilian and European). Their approach was trained and tested in
a binary setting using journalistic texts with accuracy results above 99.5% for character n-grams. The
algorithm was later adapted to classify Spanish texts using not only the classical word and character
n-grams but also POS distribution (Zampieri et al., 2013).
The aforementioned study by Lui and Cook (2013) investigates computational methods to discriminate
between texts from three different English varieties (Canadian, Australian and British) across different
domains. The authors state that the results obtained suggest that each variety contains characteristics that
are consistent across multiple domains, which enables algorithms to distinguish them regardless of the
data source.
Zaidan and Callison-Burch (2013) propose computational methods for the identification of Arabic
language varieties
4
using character and word n-grams. The authors built their own dataset using crowd-
sourcing and investigated annotators? behaviour, agreement and performance when manually tagging
instances with the correct label (variety).
3 Methods
In the following subsections we will describe the methodology adopted for the DSL shared task. Due to
the lack of comparable resources, the first decision we had to take was to create a dataset that could be
used in the shared task and also redistributed to be used in other experiments. We opted for the creation
of a corpus collection based on existing datasets as discussed in 3.1 (Tan et al., 2014).
Groups interested in participating in the DSL shared task had to register themselves in the shared
task website to receive the training and test data. Each group could participate in one or two types of
3
In the DSL shared task and in this paper we did not distinguish between language varieties and similar languages. More
on this discussion can be found in Clyne (1992) and Chamber and Trudgill (1998).
4
Zaidan and Callison-Burch (2013) use the terms ?varieties? and ?dialects? interchangeably whereas Lui and Cook (2013)
use the term ?national dialect? to refer to what previous work describes as ?national variety?.
59
submission as follows:
? Closed Submission: Using only the DSL corpus collection for training.
? Open Submission: Using any other dataset including or not the DSL collection for training.
In the open submission we did not make any distinction between systems using the DSL corpus col-
lection and those that did not. This is different from the types of submissions for the NLI shared task
2013. The NLI shared task offered proposed two types of open submissions: open submission 1 - any
dataset including the aforementioned TOEFL11 dataset (Blanchard et al., 2013) and open submission 2
- any dataset excluding TOEFL11.
For each of these submission types, participants were allowed to submit up to three runs, resulting in
a maximum of six runs in total (three closed submissions and three open submissions).
3.1 Data
As previously mentioned, we decided to compile our own dataset for the shared task. The dataset was
entitled DSL corpus collection and its compilation was motivated by the absence of a resource that
allowed us to evaluate systems on discriminating similar languages. The methods behind the compilation
of this collection and the preliminary baseline experiments are described in Tan et al. (2014).
The DSL corpus collection consists of 18,000 randomly sampled training sentences, 2,000 develop-
ment sentences and 1,000 test sentences for each language (or variety) containing at least 20 tokens
5
each.
The languages are presented in table 1 with their ISO 639-1 language codes
6
. For language varieties the
country code is appended to the ISO code (e.g. en-GB refers to the British variety of English).
Group Language/Variety Code
Bosnian bs
A Croatian hr
Serbian sr
Indonesian id
B Malay my
Czech cz
C Slovak sk
Brazilian Portuguese pt-BR
D European Portuguese pt-PT
Argentine Spanish es-AR
E Castilian Spanish es-ES
British English en-GB
F American English en-US
Table 1: Language Groups - DSL 2014 Shared Task
For this collection, randomly sampled sentences from journalistic corpora (and corpora collections) were
selected for each of the 13 classes. Journalistic corpora were preferred because they represent standard
language, which is an important factor to be considered when working with language varieties. Other data
sources (e.g. Wikipedia) do not make any distinction between language varieties and they are therefore
not suitable for the purpose of the shared task. A number of studies mentioned in the related work section
use journalistic texts for similar reasons (Huang and Lee, 2008; Grouin et al., 2010; Zampieri and Gebre,
2012)
Given what has been said in this section, we consider the collection to be a suitable comparable corpora
from this task, which was compiled to avoid bias in classification towards source, register and topics. The
5
We considered a token as orthographic units delimited by white spaces.
6
http://www.loc.gov/standards/iso639-2/php/English_list.php
60
DSL corpus collection was distributed in tab delimited format; the first column contains a sentence in
the language/variety, the second column states its group and the last column refers to its language code.
7
3.1.1 Problems with Group F
There are no major problems to report regarding the organization of the shared task nor with the com-
pilation of the DSL corpus collection apart from some issues in the Group F data. The organizers and
a couple of teams participating in the shared task observed very poor performance when distinguishing
instances from group F (British English - American British). For example, the baseline experiments
described in Tan et al. (2014) report a very low 0.59 F-measure for Group F (the lowest score) and 0.84
for Group E (the second lowest score). Some of the teams asked human annotators to try to distinguish
the sentences manually and they concluded that some instances were probably misclassified.
We decided to look more carefully at the data and noticed that the instances were originally tagged
based on the websites (newspapers) that they were retrieved from and not the country of the original
publication. There are, however, many cases of cross citation and republication of texts that the original
data sources did not take into account (e.g. British texts that were later republished by an American
website). As the DSL is a corpus collection and manually checking all 20,000 training and development
instances per language was not feasible, we assumed that the original sources
8
from which the texts were
retrieved provided the correct country of origin. The assumption was correct for all language groups but
English.
To illustrate the issues above we present next some misclassified examples. Two particular cases raised
by the UMich team are the following:
(1) I think they can afford to give North another innings and some time in Shield cricket and take
another middle order batsman. (en-US)
(2) ATHENS, Ohio (AP) Albuquerque will continue its four-game series in Nashville Thursday night
when it takes on the Sounds behind starter Les Walrond (3-4, 4.50) against Gary Glover, who is
making his first Triple-A start after coming down from Milwaukee. (en-GB)
Example number one was tagged as American English because it was retrieved from the online edition
of The New York Times but it was in fact first published in Australia. The second example is a text
published by Associated Press describing an event that took place in Ohio, United States, but it was
tagged as British English because it was retrieved by the UK Yahoo! sports section.
Our solution was to exclude the language group F from the final scores and perform a manual check in
all its 1,000 test instances
9
, thus giving the chance to participants to train their algorithms on other data
sources (open submission).
3.2 Schedule
The DSL shared task spanned from March 20
th
when the training set was released, to June 6
th
when
participants could submit a paper (up to 10 pages) describing their system. We provided one month
between the release of the training and the test set. The schedule of the DSL shared task 2014 can be
seen below.
Event Date
Training Set Release March 20
th
, 2014
Test Set Release April 21
st
, 2014
Submissions Due April 23
rd
, 2014
Results Announced April 30
th
, 2014
Paper Submission June 6
th
, 2014
Table 2: DSL 2014 Shared Task Schedule
7
To obtain the data please visit: https://bitbucket.org/alvations/dslsharedtask2014
8
See Tan et al. (2014) for a complete description of the data sources of the DSL corpus collection.
9
Our manual check suggests that about 25% of the instances in the English dataset was likely to have been misclassified.
61
4 Results
This section summarises the results obtained by all participants of the shared task who submitted final
results.
10
The DSL shared task included 22 enrolled teams from different countries (e.g. Australia,
Estonia, Holland, Germany, United Kingdom and United States). From the 22 enrolled teams, eight of
them submitted their final results. Most of the groups opted to exclusively use the DSL corpus collection
and therefore participated solely in the closed submission track. Two of them compiled comparable
datasets and also participated in the open submission.
Given that the dataset contained misclassified instances, group F (English) was not taken into account
to compute the final shared task scores. In the next subsections we report results in terms of macro-
average F-measure and accuracy.
4.1 Closed Submission
Table 3 presents the best F-measure and Accuracy results obtained by the eight teams that submitted their
results for the closed submission track ordered by accuracy.
Team Macro-avg F-score Overall Accuracy
NRC-CNRC 0.957 0.957
RAE 0.947 0.947
UMich 0.932 0.932
UniMelb-NLP 0.918 0.918
QMUL 0.925 0.906
LIRA 0.721 0.766
UDE 0.657 0.681
CLCG 0.405 0.453
Table 3: Open Submission - Results
In the closed submissions, we observed a group of five teams whose systems (best runs) obtained re-
sults over 90% accuracy. This is comparable to what is described in the state-of-the-art literature for
discriminating similar languages and language varieties (Tiedemann and Ljube?si?c, 2012; Lui and Cook,
2013). These five teams submitted system descriptions that allowed us to look in more detail at successful
approaches for this task. System descriptions will be discussed in section 5.
Three of the eight teams obtained substantially lower scores, from 45.33% to 76.64% accuracy. These
three groups unfortunately did not submit system description papers. From our point of view, this would
create an interesting opportunity to look more carefully at the weaknesses of approaches that did not
obtain good results in this task.
4.2 Open Submission
Only two systems submitted results for the open submission track and their F-measure and Accuracy
results are presented in table 3.
Team Macro-avg F-score Overall Accuracy
UniMelb-NLP 0.878 0.880
UMich 0.858 0.859
Table 4: Closed Submission - Results
The UniMelb-NLP (Lui et al., 2014) group used data from different corpora such as the BNC, EU-
ROPARL and Open Subtitles whereas UMich (King et al., 2014) compiled journalistic corpora from dif-
ferent sizes for each language ranging from 695,597 tokens for Malay to 20,288,294 tokens for British
English.
10
Visit https://bitbucket.org/alvations/dslsharedtask2014/downloads/dsl-results.html
for more detail on the shared task results or at the aforementioned DSL shared task website.
62
Comparing the results of the closed to the open submissions, we observed that the UniMelb-NLP sub-
mission was outperformed by UMich system by about 1.5% accuracy in the closed submission, but in
the open submission they scored 2.1% better than UMich. This difference can be explained by investi-
gating these two factors: 1) the quality and amount of the collected training data; 2) the robustness of
the method to obtain correct predictions across different datasets and domains as previously discussed
by Lui and Cook (2013) for English varieties.
4.3 Accuracy per Language Group
In this subsection we look more carefully at the performance of systems in discriminating each class
within groups A to E. Table 5 presents the accuracy scores obtained per language group for each team
sorted alphabetically. The best score per group is displayed in bold.
CLCG LIRA NRC-CNRC QMUL RAE UDE UMich UniMelb-NLP
A 0.338 0.333 0.936 0.879 0.919 0.785 0.919 0.915
B 0.503 0.982 0.996 0.935 0.994 0.892 0.992 0.972
C 0.500 1.000 1.000 0.962 1.000 0.493 0.999 1.000
D 0.496 0.892 0.956 0.905 0.948 0.493 0.926 0.896
E 0.503 0.843 0.910 0.865 0.888 0.694 0.876 0.807
Table 5: Language Groups A to E - Accuracy Results
The top 5 systems plus the LIRA team obtained very good results for groups B (Malay and Indonesian)
and C (Czech and Slovak). Four out of eight systems obtained perfect performance when discriminating
Czech and Slovak texts. Perfect performance was not achieved by any of the systems when distinguishing
Malay from Indonesian texts, but even so, results were fairly high and the best result was 99.6% accuracy
obtained by the NRC-CNRC group. The perfect results obtained by four groups when distinguishing
texts from group C suggest that Czech and Slovak texts are not as similar as we assumed before the
shared task, and that they therefore possess strong systemic and/or orthographic differences that allow
well-trained classifiers to perform perfectly. Figure 1 presents the accuracy results of the top 5 groups.
Figure 1: Language Groups A to E Accuracy - Top 5 Systems
Distinguishing between languages from group A (Bosnian, Croatian and Serbian), the only group con-
taining 3 languages, proved to be a challenging task as discussed in previous research (Ljube?si?c et al.,
2007; Tiedemann and Ljube?si?c, 2012). The best result was again obtained by the NRC-CNRC group
with 93.5% accuracy. The groups containing texts written in different language varieties, namely D (Por-
tuguese) and E (Spanish) were the most difficult to discriminate, particularly the Spanish varieties. These
results also corroborate the findings of previous studies (Zampieri et al., 2013).
63
The QMUL system that was the 5
th
best system in the closed submission track did not outperform any
of the other top 5 systems in groups A, B or C. However, the system did better when distinguishing texts
from the two most difficult language groups (D and E), outperforming the UniMelb-NLP submission on
two occasions. The simplicity of the approach proposed by the QMUL, which the author describes as
?a simple baseline? (Purver, 2014) may be an explanation for the regular performance across different
language groups.
4.4 Results Group F
To document the problems in the group F (British and American English) dataset we included the results
of both the open and closed submissions for this language group in table 6. As previously mentioned,
submitting group F results was optional and we did not include these results in the final shared task
results. Six out of eight systems decided to submit their predictions as closed submissions and the two
groups participating in the open submission track also submitted their group F results.
Team F-score Accuracy Type
UMich 0.639 0.639 Open
UniMelb- NLP 0.581 0.583 Open
NRC-CNRC 0.522 0.524 Closed
LIRA 0.450 0.493 Closed
RAE 0.451 0.481 Closed
UMich 0.463 0.464 Closed
UDE 0.451 0.451 Closed
UniMelb-NLP 0.435 0.435 Closed
Table 6: Group F - Accuracy Results
The results confirm the problems in the DSL dataset discussed in section 3.1.1. After a careful manual
check of the 1,000 test instances, open submissions scores were still substantially lower than the other
groups: 69.9% and 58.3% accuracy. Closed submissions proved to be impossible and only one of the six
systems scored slightly above the 50% baseline.
It should be investigated more carefully in future research whether the poor results for group F reflect
only the problems in the dataset or also the actual difficulty in discriminating between these two varieties
of English. Moderate differences in orthography (e.g. neighbour (UK) and neighbor (US)) as well
as lexical choices (e.g. rubbish (UK) and garbage (US) or trousers (UK) and pants (US)) are present
in texts from these two varieties and these can be informative features for algorithms to discriminate
between them. Discriminating between other English varieties already proved to be a challenging yet
feasible task in previous research (Lui and Cook, 2013).
5 System Descriptions
All eight systems that submitted their final results to the shared task were invited to submit papers de-
scribing their systems and the top 5 systems in the closed track submitted their papers, namely: NRC-
CNRC, RAE, UMich, UniMelb-NLP and QMUL.
The best scores were obtained by the NRC-CNRC (Goutte et al., 2014) team which proposed a two-
step approach to predict first the language group than the language of each instance. The language group
was predicted in a 6-way classification using a probabilistic model similar to a Naive Bayes classifier,
and later the method applied SVM classifiers to discriminate within each group: binary for groups B-F
and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian).
An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called
?white lists? inspired by the ?blacklist? classifier (Tiedemann and Ljube?si?c, 2012). These lists are word
lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc?on (2006)
proposed to discriminate between Malay and Indonesian.
64
Two groups used Information Gain (IG) to select the best features for classification, namely UMich
(King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit
open submissions. The UniMelb-NLP team tried different classification methods and features (including
delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf
general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has
been widely used for general-purpose language identification and its performance is regarded superior
to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hier-
archically firstly identifying the language group that a sentence belongs to and subsequently the specific
language, achieving performance comparable to the state-of-the-art, but still slightly below the other
three systems.
The QMUL team (Purver, 2014) proposed a linear SVM classifier using words and characters as fea-
tures. The author investigated the influence of the cost parameter c (from 1.0 to 100.0), in the classifiers?
performance. The cost parameter c is responsible for the trade-off between maximum margin and clas-
sification errors. According to the system description the optimal parameter for this task lies between
30.0 and 50.0. Purver (2014) also notes that the linear SVM classifier performs well with word uni-gram
language models in comparison to methods using character n-grams. This observation corroborates the
findings of previous experiments that rely on words as important features to distinguish similar languages
and varieties (Huang and Lee, 2008; Zampieri, 2013)
The features and algorithms presented so far, as well as the system paper descriptions, are summarised
in table7.
11
Team Algorithm Features System Paper
NRC-CNRC Prob. Class. and Linear SVM Words 1-2, Char. 2-6 (Goutte et al., 2014)
RAE MaxEnt Words 1-2, Char. 1-5, ?Whitelist? (Porta and Sancho, 2014)
UMich Naive Bayes Words 1-2, Char. 2-6 (IG Feat. Selection) (King et al., 2014)
UniMelb-NLP langid.py Words, Char., POS (IG Feat. Selection) (Lui et al., 2014)
QMUL Linear SVM Words 1, Char. 1-3 (Purver, 2014)
Table 7: Top 5 Systems - Features and Algorithms at a Glance
6 Conclusion
Shared tasks are an interesting way of comparing algorithms, computational methods and features using
the same dataset. Given what has been presented in this paper, we believe that the DSL shared task filled
an important gap in language identification and will allow other researchers to look in more detail at the
problem of discriminating similar languages. Accurate methods for discriminating similar languages can
help to improve performance not only in language identification but also in a number of NLP tasks and
applications such as part-of-speech-tagging, spell checking and machine translation.
The best system obtained 95.71% accuracy and F-measure for a set of 11 languages and varieties
divided into 5 groups (A to E), using only the DSL corpus collection. Systems that performed best
modelled their algorithms to perform two-step predictions: first the language group, then the actual class
and used characters and words as features. As we regard the corpus to be a balanced sample of the
news domain, the results obtained confirm the assumption that similar languages and varieties possess
systemic characteristics that can be modelled by algorithms in order to distinguish languages from other
similar languages or varieties using lexical or orthographical features.
Another lesson learned from this shared task is regarding the compilation of group F (English) data.
Researchers, including us, often rely on previously annotated meta-data which sometimes may contain
inaccurate information and errors. Corpus collection for this purpose should be thoroughly checked
(manually if possible). The issues with the group F might have discouraged some of the participants to
continue in the shared task (particularly those who were interested only in the discrimination of English
varieties).
11
UniMelb-NLP experimented different methods in their 6 runs. In this report we commented on the algorithm that achieved
the best performance.
65
6.1 Future Perspectives
The shared task was a very fruitful and positive experience for the organizers. We would like to organize
a second edition of the shared task containing, for example, new language groups for which we could
not find suitable corpora before the 2014 edition. This includes, most notably, the cases of Dutch and
Flemish or the varieties of French and German which could not be included in the DSL shared task due
to the lack of available data.
The DSL corpus collection is freely available and can be used as a gold standard for language iden-
tification or to train algorithms for other NLP tasks involving similar languages. We would like to use
the dataset to investigate, for example, lexical variation between similar languages and varieties as pro-
posed by Piersman et al. (2010) and Soares da Silva (2010) or syntactic variation using annotated data
as discussed in Anstein (2013).
At present, we are investigating the influence of the length of texts in the discrimination of similar
languages. It is a well known fact that the longer texts are, the more likely they are to contain features
that allow algorithms to identify their language. However, this variable was not explored within the
scope of the DSL shared task and we are using the DSL dataset and the results for this purpose. Another
direction that our work may take is the linguistic analysis of the most informative features in classification
as was done recently by Diwersy et al. (2014).
Acknowledgements
The authors would like to thank all participants of the DSL shared task for their comments and sugges-
tions throughout the organization of this shared task. We would also like to thank Joel Tetreault and
Binyam Gebrekidan Gebre for their valuable feedback on this report.
References
Stefanie Anstein. 2013. Computational approaches to the comparison of regional variety corpora : prototyping a
semi-automatic system for German. Ph.D. thesis, University of Stuttgart.
Timothy Baldwin and Marco Lui. 2010. Multilingual language identification: ALTW 2010 shared task data. In
Proceedings of Australasian Language Technology Association Workshop, pages 4?7.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Educational Testing Service.
Ralf Brown. 2013. Selecting and weighting n-grams to identify 1100 languages. In Proceedings of the 16th In-
ternational Conference on Text Speech and Dialogue (TSD2013), Lecture Notes in Artificial Intelligence (LNAI
8082), pages 519?526, Pilsen, Czech Republic. Springer.
William Cavnar and John Trenkle. 1994. N-gram-based text catogorization. 3rd Symposium on Document Analy-
sis and Information Retrieval (SDAIR-94).
Jack Chambers and Peter Trudgill. 1998. Dialectology (2nd Edition). Cambridge University Press.
Michael Clyne. 1992. Pluricentric Languages: Different Norms in Different Nations. CRC Press.
Sascha Diwersy, Stefan Evert, and Stella Neumann. 2014. A semi-supervised multivariate approach to the study
of language variation. Linguistic Variation in Text and Speech, within and across Languages.
Cyril Goutte, Serge L?eger, and Marine Carpuat. 2014. The NRC system for discriminating similar languages. In
Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),
Dublin, Ireland.
Cyril Grouin, Dominic Forest, Lyne Da Sylva, Patrick Paroubek, and Pierre Zweigenbaum. 2010. Pr?esentation et
r?esultats du d?efi fouille de texte DEFT2010 o`u et quand un article de presse a-t-il ?et?e ?ecrit? Actes du sixi`eme
D
?
Efi Fouille de Textes.
Chu-ren Huang and Lung-hao Lee. 2008. Contrastive approach towards text source classification based on top-
bag-of-word similarity. In Proceedings of PACLIC 2008, pages 404?410.
66
Ben King, Dragomir Radev, and Steven Abney. 2014. Experiments in sentence language identification with
groups of similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages,
Varieties and Dialects (VarDial), Dublin, Ireland.
Nikola Ljube?si?c, Nives Mikelic, and Damir Boras. 2007. Language identification: How to distinguish similar
languages? In Proceedings of the 29th International Conference on Information Technology Interfaces.
Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings
of the 50th Meeting of the ACL.
Marco Lui and Paul Cook. 2013. Classifying English documents by national dialect. In Proceedings of Aus-
tralasian Language Tchnology Workshop, pages 5?15.
Marco Lui, Ned Letcher, Oliver Adams, Long Duong, Paul Cook, and Timothy Baldwin. 2014. Exploring methods
and resources for discriminating similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools
to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.
Yves Piersman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation
between language varieties. Natural Language Engineering, 16:469?491.
Jordi Porta and Jos?e-Luis Sancho. 2014. Using maximum entropy models to discriminate between similar lan-
guages and varieties. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties
and Dialects (VarDial), Dublin, Ireland.
Matthew Purver. 2014. A simple baseline for discriminating similar language. In Proceedings of the 1st Workshop
on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.
Bali Ranaivo-Malanc?on. 2006. Automatic identification of close languages - case study: Malay and Indonesian.
ECTI Transactions on Computer and Information Technology, 2:126?134.
Augusto Soares da Silva. 2010. Measuring and parameterizing lexical convergence and divergence between
European and Brazilian Portuguese: endo/exogeneousness and foreign and normative influence. Advances in
Cognitive Sociolinguistics.
Liling Tan, Marcos Zampieri, Nikola Ljube?si?c, and J?org Tiedemann. 2014. Merging comparable data sources
for the discrimination of similar languages: The DSL corpus collection. In Proceedings of The Workshop on
Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification shared
task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,
Atlanta, GA, USA, June. Association for Computational Linguistics.
J?org Tiedemann and Nikola Ljube?si?c. 2012. Efficient discrimination between closely related languages. In
Proceedings of COLING 2012, pages 2619?2634, Mumbai, India.
Omar F Zaidan and Chris Callison-Burch. 2013. Arabic dialect identification. Computational Linguistics.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case
of Portuguese. In Proceedings of KONVENS2012, pages 233?237, Vienna, Austria.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS
distribution for the identification of Spanish varieties. In Proceedings of TALN2013, pages 580?587, Sable
d?Olonne, France.
Marcos Zampieri. 2013. Using bag-of-words to distinguish similar languages: How efficient are they?
In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics
(CINTI2013), pages 37?41, Budapest, Hungary.
67
