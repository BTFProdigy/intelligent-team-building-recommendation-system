Proceedings of the 12th Conference of the European Chapter of the ACL, pages 719?727,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Word Lattices for Multi-Source Translation
Josh Schroeder, Trevor Cohn, and Philipp Koehn
School of Informatics
University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
Scotland, United Kingdom
{jschroe1, tcohn, pkoehn}@inf.ed.ac.uk
Abstract
Multi-source statistical machine transla-
tion is the process of generating a single
translation from multiple inputs. Previous
work has focused primarily on selecting
from potential outputs of separate transla-
tion systems, and solely on multi-parallel
corpora and test sets. We demonstrate how
multi-source translation can be adapted for
multiple monolingual inputs. We also ex-
amine different approaches to dealing with
multiple sources, including consensus de-
coding, and we present a novel method
of input combination to generate lattices
for multi-source translation within a single
translation model.
1 Introduction
Multi-source statistical machine translation was
first formally defined by Och and Ney (2001)
as the process of translating multiple meaning-
equivalent source language texts into a single tar-
get language. Multi-source translation is of par-
ticular use when translating a document that has
already been translated into several languages, ei-
ther by humans or machines, and needs to be fur-
ther translated into other target languages. This
situation occurs often in large multi-lingual organ-
isations such as the United Nations and the Euro-
pean Parliament, which must translate their pro-
ceedings into the languages of the member in-
stitutions. It is also common in multi-national
companies, which need to translate product and
marketing documentation for their different mar-
kets. Clearly, any existing translations for a docu-
ment can help automatic translation into other lan-
guages. These different versions of the input can
resolve deficiencies and ambiguities (e.g., syntac-
tic and semantic ambiguity) present in a single in-
put, resulting in higher quality translation output.
In this paper, we present three models of multi-
source translation, with increasing degrees of so-
phistication, which we compare empirically on a
number of different corpora. We generalize the
definition of multi-source translation to include
any translation case with multiple inputs and a sin-
gle output, allowing for, e.g., multiple paraphrased
inputs in a single language. Our methods include
simple output selection, which treats the multi-
source translation task as many independent trans-
lation steps followed by selection of one of their
outputs (Och and Ney, 2001), and output combina-
tion, which uses consensus decoding to construct
a string from n-gram fragments of the translation
outputs (Bangalore et al, 2001). We also present
a novel method, input combination, in which we
compile the input texts into a compact lattice, over
which we perform a single decoding pass. We
show that as we add additional inputs, the simplest
output selection method performs quite poorly rel-
ative to a single input translation system, while the
latter two methods are able to make better use of
the additional inputs.
The paper is structured as follows. ?2 presents
the three methods for multi-source translation in
detail: output selection, output combination, and
our novel lattice-based method for input combina-
tion. We report experiments applying these tech-
niques to three different corpora, with both mono-
lingual inputs (?3) and multilingual inputs (?4).
We finish in ?5 by analyzing the benefits and draw-
backs of these approaches.
2 Approaches to Multi-Source
Translation
We now present three ways to combine multiple
inputs into a single output translation, in the con-
text of related work for each technique.
719
2.1 Output Selection
The most straightforward approach to multi-
source translation, proposed by Och and Ney
(2001), is to independently translate each of the
N source languages and then select a single
translation from the outputs. Given N sources
sN1 = s1, . . . , sN , first translate each with a sep-
arate translation system, p1, . . . , pN , to obtain N
target translations, tN1 = t1, . . . , tN . Och and Ney
present two approaches for selecting a single tar-
get from these outputs.
The first, PROD, finds the maximiser of the
product, argmaxt?tN1 p(t)
?N
n=1 pn(sn|t), where
p(t) is the language model probability. For rea-
sons of tractability, the maximisation is performed
only over targets generated by the translation sys-
tems, tN1 , not the full space of all translations.
The PROD method requires each model to pro-
vide a model score for each tn generated by the
other models. However, this is often impossible
due to the models? highly divergent output spaces
(Schwartz, 2008), and therefore the technique can-
not be easily applied.
The second approach, MAX, solves
argmaxt?tN1 max
N
n=1 p(t)pn(sn|t), which is
much easier to calculate. As with PROD, the
translation models? outputs are used for the
candidate translations. While different models
may have different score ranges, Och and Ney
(2001) state that there is little benefit in weighting
these scores to normalise the output range. In their
experiments, they show that MAX used on pairs or
triples of language inputs can outperform a model
with single language input, but that performance
degrades as more languages are added.
These methods limit the explored space to a full
translation output of one of the inputs, and there-
fore cannot make good use of the full diversity of
the translations. In this paper we present MAX
scores as a baseline for output selection, and ap-
proximate an oracle using the BLEU metric as an
upper bound for the output selection technique.
2.2 Output Combination
Consensus decoding as a form of system combi-
nation is typically used to integrate the outputs of
multiple translation systems into a single synthetic
output that seeks to combine the best fragments
from each component system. Multi-source trans-
lation can be treated as a special case of consen-
sus decoding. Indeed, several authors have seen
the  dog barked very loudly
a big dog barked  loudly
sub insert ? shift delete ?
Table 1: Example minimum TER edit script.
0 1thea 2?big 3dog 4barked 5very? 6loudly
Figure 1: Conversion of TER script from Table 1
to a confusion network.
improvements in translation quality by perform-
ing multi-source translation using generic system
combination techniques (Matusov et al, 2006;
Paulik et al, 2007).
One class of approaches to consensus decoding
focuses on construction of a confusion network
or lattice1 from translation outputs, from which
new sentences can be created using different re-
orderings or combinations of translation fragments
(e.g., Bangalore et al (2001); Rosti et al (2007b)).
These methods differ in the types of lattices used,
their means of creation, and scoring method used
to extract the best consensus output from the lat-
tice. The system used in this paper is a variant of
the one proposed in Rosti et al (2007a), which we
now describe in detail.
The first step in forming a lattice is to align the
inputs. Consensus decoding systems often use the
script of edit operations that minimises the transla-
tion edit rate (TER; Snover et al (2006)). TER is
a word-based measure of edit distance which also
allows n-gram shifts when calculating the best
match between a hypothesis and reference. Be-
cause TER describes the correspondence between
the hypothesis and reference as a sequence of in-
sertions, substitutions, deletions, and shifts, the
edit script it produces can be used to create a con-
fusion network.
Consider a reference of ?The dog barked very
loudly? and a hypothesis ?A big dog loudly
barked.? The TER alignment is shown in Ta-
ble 1, along with the edit operations. Note that the
matching ?barked? tokens are labelled shift, as one
needs to be shifted for this match to occur. Using
the shifted hypothesis, we can form a confusion
1Different authors refer to ?lattices,? ?confusion net-
works,? ?word sausages,? etc. to describe these data struc-
tures, and specific terminology varies from author to author.
We define a lattice as a weighted directed acyclic graph, and
a confusion network as a special case where each node n in
the ordered graph has word arcs only to node n + 1.
720
  
 
 
 Confusion Network 1
 Confusion Network 2
 Confusion Network 3
 
Figure 2: Structure of a lattice of confusion net-
works for consensus decoding.
network as in Figure 1. Additional sentences can
be added by aligning them to the reference as well.
Each link is weighted by the number of component
sentences sharing that particular word at the given
location.
Similar to Rosti et al (2007a), we let each hy-
pothesis take a turn as the ?reference? for TER,
using it as a skeleton for a confusion network. We
then form a lattice of confusion networks (Fig-
ure 2), assigning a prior weight to each confusion
network based on the average TER of the selected
skeleton with the other hypotheses. This allows
each system to set the word order for a component
confusion network, but at the cost of a more com-
plex lattice structure.
We can score pathsP through these lattices with
the assistance of a language model. Formally, the
path score is given by:
w(P) = ? log pLM (t(P))
+
?
d?P
[
N?
n=1
?n log pn(d|sn)
+ ??(d, ) + ?(1? ?(d, ))
]
where pLM is the language model probability of
the target string specified by the lattice path, t(P),
pn(d|sn) is the proportion of system n?s k-best
outputs that use arc d in path P , and the last two
terms count the number of epsilon and non-epsilon
transitions in the path. The model parameters are
?1, . . . , ?n, ?, ?, ?, which are trained using Pow-
ell?s search to maximise the BLEU score for the
highest scoring path, argmaxP w(P).
2.3 Input Combination
Loosely defined, input combination refers to find-
ing a compact single representation of N transla-
tion inputs. The hope is that the new input pre-
serves as many of the salient differences between
the inputs as possible, while eliminating redundant
information. Lattices are well suited to this task.
0 1
?watchit 2
?out's 3?for 4
thepursepicka?
5
robberthief
snatcherburglarcrookpocket
6.
Figure 3: A monolingual confusion network.
Thicker lines indicate higher probability word
arcs.
When translating speech recognition output,
previous work has shown that representing the
ambiguity in the recognized text via confusion
networks leads to better translations than simply
translating the single best hypothesis of the speech
recognition system (Bertoldi et al, 2007). The ap-
plication of input lattices to other forms of input
ambiguity has been limited to encoding input re-
orderings, word segmentation, or morphological
segmentation, all showing improvements in trans-
lation quality (Costa-jussa` et al, 2007; Xu et al,
2005; Dyer et al, 2008). However, these appli-
cations encode the ambiguity arising from a sin-
gle input, while in this work we combine distinct
inputs into a more compact and expressive single
input format.
When given many monolingual inputs, we can
apply TER and construct a confusion network as
in Section 2.2.2 In this application of confusion
networks, arc weights are calculated by summing
votes from each input for a given word, and nor-
malizing all arcs leaving a node to sum to 1.
Figure 3 shows an example of a TER-derived
input from IWSLT data. Because the decoder will
handle reordering, we select the input with the
lowest average TER against the other inputs to
serve as the skeleton system, and do not create a
lattice with multiple skeletons.
The problem becomes more complex when we
consider cases of multi-lingual multi-source trans-
lation. We cannot easily apply TER across lan-
guages because there is no clear notion of an exact
match between words. Matusov et al (2006) pro-
pose using a statistical word alignment algorithm
as a more robust way of aligning (monolingual)
outputs into a confusion network for system com-
2Barzilay and Lee (2003) construct lattices over para-
phrases using an iterative pairwise multiple sequence align-
ment (MSA) algorithm. Unlike our approach, MSA does not
allow reordering of inputs.
721
bination. We take a similar approach for multi-
lingual lattice generation.
Our process consists of four steps: (i) Align
words for each of the N(N ? 1) pairs of inputs;
(ii) choose an input (or many inputs) to be the
lattice skeleton; (iii) extract all minimal consis-
tent alignments between the skeleton and the other
inputs; and (iv) add links to the lattice for each
aligned phrase pair.
A multi-parallel corpus such as Europarl
(Koehn, 2005) is ideally suited for training this
setup, as training data is available for each pair of
input languages needed by the word aligner. We
used the GIZA++ word alignment tool (Och and
Ney, 2003) for aligning inputs, trained on a por-
tion of the Europarl training data for each pair.
We select a skeleton input based on which
single-language translation system performs the
best when translating a development set. For our
Europarl test condition, this was French.
We define a minimal consistent alignment
(MCA) as a member of the set of multi-word
alignment pairs that can be extracted from a many-
to-many word alignment between skeleton sen-
tence x and non-skeleton sentence y with the fol-
lowing restrictions: (i) no word in x or y is used
more than once in the set of MCAs; (ii) words
and phrases selected from y cannot be aligned to
null; and (iii) no smaller MCA can be decomposed
from a given pair. This definition is similar to
that of minimal translation units as described in
Quirk and Menezes (2006), although they allow
null words on either side.
Different word alignment approaches will result
in different sets of MCAs. For input lattices, we
want sets of MCAs with as many aligned words
as possible, while minimising the average num-
ber of words in each pair in the set. Experiments
with GIZA++ on the Europarl data showed that
the ?grow-diag-final-and? word alignment sym-
metrization heuristic had the best balance between
coverage and pair length: over 85% of skeleton
words were part of a non-null minimal pair, and
the average length of each pair was roughly 1.5
words. This indicates that our lattices will pre-
serve most of the input space while collapsing eas-
ily alignable sub-segments.
Once a set of phrase alignments has been found,
we construct a lattice over the skeleton sentence
x. For each additional input yn we add a set of
links and nodes for each word in x to any relevant
? podr?a darnos las cifras correspondientes a espa?a y grecia ?
pouvez-vous nous donner les
chiffres pour  l' espagne et la gr?ce ?
siffrorna f?rkan ni ge oss spanien och grekland ?
Figure 4: A multi-lingual alignment between
French, Spanish and Swedish, showing the min-
imal consistent alignments. The lattice generated
by this alignment is shown in Figure 5.
words in yn, rejoining at the last word in x that
is covered by the pair. Figures 4 and 5 show an
example of the alignments and lattice generated by
using a French skeleton with Spanish and Swedish
sentences.
Once a lattice is created, we can submit it to a
phrase-based decoder in place of text input. The
decoder traverses lattice nodes in a manner simi-
lar to how words are traversed in text translation.
Instead of one input word represented by each lo-
cation in the coverage vector as in text input, with
lattices there are a set of possible input word arcs,
each with its own translation possibilities. The
concept of compatible coverage vectors for the lo-
cations of translated words becomes the notion of
reachability between frontier nodes in the lattice
(Dyer et al, 2008).
It is possible to construct multi-skeleton lat-
tices by connecting up a set of N lattices, each
built around a different skeleton xn, in much the
same manner as multiple confusion networks can
be connected to form a lattice in output combina-
tion. With sufficient diversity in the input order-
ing of each skeleton, the decoder need not perform
reordering. Because of the size and complexity
of these multi-skeleton lattices, we attempt only
monotonic decoding. In this scenario, as in con-
sensus decoding, we hope to exploit the additional
word order information provided by the alternative
skeletons.
3 Experiments: Monolingual Input
We start our experimental evaluation by translat-
ing multiple monolingual inputs into a foreign lan-
guage. This is a best-case scenario for testing
and analytic purposes because we have a single
translation model from one source language to one
target language. While translating from multiple
monolingual inputs is not a common use for ma-
chine translation, it could be useful in situations
where we have a number of paraphrases of the in-
put text, e.g., cross-language information retrieval
and summarization.
722
0 5pouvez-vous1? 2kan 7
darnos6nouspodr?a 3ni 4ge oss 9lasles 8siffrornadonner 11
chiffres10cifras 12f?r 13l' 14espa?aspanienapourcorrespondientes espagne 15yetoch 17la 18grecia 16grekland gr?ce 19???
Figure 5: A multi-lingual lattice input for French, Spanish, and Swedish from Europarl dev2006.
Data sets for this condition are readily available
in the form of test sets created for machine trans-
lation evaluation, which contains multiple target
references for each source sentence. By flipping
these test sets around, we create multiple mono-
lingual inputs (the original references) and a sin-
gle reference output (the original source text). We
examine two datasets: the BTEC Italian-English
corpus (Takezawa et al, 2002), and the Multiple
Translation Chinese to English (MTC) corpora,3
as used in past years? NIST MT evaluations.
All of our translation experiments use the
Moses decoder (Koehn et al, 2007), and are eval-
uated using BLEU-4. Moses is a phrase-based
decoder with features for lexicalized reordering,
distance-based reordering, phrase and word trans-
lation probabilities, phrase and word counts, and
an n-gram language model.
3.1 English to Italian
We use the portion of the BTEC data made avail-
able for the Italian-English translation task at
IWSLT 2007, consisting of approximately 24,000
sentences. We also use the Europarl English-
Italian parallel corpus to supplement our train-
ing data with approximately 1.2 million out-of-
domain sentences. We train a 5-gram language
model over both training corpora using SRILM
(Stolcke, 2002) with Kneser-Ney smoothing and
linear interpolation, the interpolation weight cho-
sen to minimise perplexity on the Italian side of
the development tuning set.
For multiple translation data, we use IWSLT
test sets devset1-3 which have sixteen English
translations for each Italian sentence. The Ital-
ian version of the BTEC corpus was created af-
ter the original Japanese-English version, and only
the first English translation was used to generate
the Italian data. The other fifteen versions of each
English sentence were generated as paraphrases
of the primary English translation. We explore
translation conditions using only the fifteen para-
phrased inputs (?Para.? in Table 2), as well as us-
ing all sixteen English inputs (?All?).
3LDC2002T01, LDC2003T17, LDC2004T07 and
LDC2006T04.
All Para.
BEST 40.06 24.02
ORACLE 51.64 47.27
MAX 29.32 23.94
SYSCOMB 32.89 30.39
CN INPUT 31.86 27.62
Table 2: BLEU scores on the BTEC test set for
translating English inputs into Italian.
We tune our translation models on devset1, sys-
tem combination on devset2 and report results on
devset3 for each condition.
When tuning the single input ?Para.? and ?All?
baselines, we include all relevant copies of the 506
lines of devset1 English data, and repeat the Ital-
ian reference fifteen or sixteen times on the target
side, resulting in a total of 7,590 and 8,096 sen-
tence pairs respectively.
The results for devset3 are shown in Table 2.
For comparison, we show the BEST score any in-
put produced, as well as an approximated ORA-
CLE output selection generated by choosing the
best BLEU-scoring output for each sentence using
a greedy search. Our output combination method,
SYSCOMB, uses no system-specific weights to
distinguish the inputs. For SYSCOMB and MAX,
we translated all versions of the English input sep-
arately, and we use the top ten distinct hypothe-
ses from each input sentence for n-best input to
SYSCOMB.
For input combination, CN INPUT, we used the
TER-based monolingual input lattice approach de-
scribed in Section 2.3, choosing as a skeleton the
input with the lowest average TER score when
compared with the other inputs (assessed sepa-
rately for each sentence). Each input was given
equal probability in the confusion network links.
Note that the quality of output from translat-
ing the primary English input is much higher than
from translating any of the paraphrases. The pri-
mary input sentence scores a BLEU of 40.06, while
the highest scoring paraphrased input manages
only a 24.02. When we look at ?Para.? the dif-
ference in the scores when using a single input
723
(BEST) versus all the inputs (SYSCOMB and CN
INPUT) is striking ? clearly there is considerable
information in the other inputs which can radically
improve the translation output. Removing the pri-
mary input from ORACLE reinforces this observa-
tion: the score drops by only 4.37 BLEU despite
the nearly 16 BLEU drop for the single best input.
Interestingly, the output selection technique,
MAX, performs at a similar level to the combina-
tion techniques when we include the primary in-
put, but degrades when given only the lower qual-
ity translations of paraphrased input under condi-
tion ?Para.? In previous work on multi-lingual out-
put selection, the MAX score degraded after two
or three outputs were combined, but even with-
out the primary reference it maintains a score near
the best single paraphrased input when combining
fifteen outputs. One possible explanation for this
is that the inputs are all being translated with the
same translation model, so comparing their scores
can give a more accurate ranking of their relative
translation quality according to the model. The
input combination method, CN INPUT, performs
better than MAX and only slightly worse than the
output combination approach.
3.2 English to Chinese
We can add an extra dimension to monolingual
multi-source translation by considering inputs of
differing quality. A multi-source translation sys-
tem can exploit features indicating the origin of the
input to improve output quality. For these exper-
iments, we use the MTC English-Chinese corpus,
parts 1?4. This data was translated from Chinese
into English by four teams of annotators, denoted
E01?E04. This allows us to examine the results
for translating the same team?s work over multiple
years.
We train on the news domain portion of the of-
ficial NIST data4 (excluding the UN and Hong
Kong data) for both the translation model and the
5-gram Chinese language model.
While we still have a single translation model,
all of our inputs are now of a traceable origin and
are known to have quality differences when judged
by human evaluators. With this information we
can tune one of two ways: We can create a set of
all input systems and replicate the reference as we
did for English to Italian translation (?All tuned?),
4http://www.nist.gov/speech/tests/mt/
2008
Team Tuning Part 3 Part 4
E01 All 16.18 15.52
E01 Self 16.02 15.63
E02 All 14.29 14.00
E02 Self 13.88 14.05
E03 All 14.99 15.06
E03 Self 15.10 14.94
E04 All 14.03 12.65
E04 Self 14.03 12.59
Table 3: BLEU scores using single inputs from
each different team on the MTC. Bold indicates
the better score between All and Self tuning.
Approach Tuning Part 3 Part 4
MAX All 15.06 15.08
MAX Self 14.97 13.75
SYSCOMB All 16.82 16.24
SYSCOMB Self 16.87 16.45
Table 4: BLEU scores for multi-source translations
of MTC test sets. Better score for each output-
based multi-source method is shown in bold.
or we can tune each input using only the version of
the tuning data generated by the same translation
team (?Self tuned?).5 For example, we can tune
a system with the MTC Part 2 data provided by
translation team E01, and then decode E01?s trans-
lations of parts 3 and 4 with the weights obtained
in tuning. The results for each system are shown
in Table 3. Despite the different tuning conditions,
there is no clear advantage to tuning to all inputs
versus tuning to each input separately ? on aver-
age we see a 0.06 BLEU score advantage by using
?All? weights.
With four different inputs to our multi-source
translation system, and two ways of weighting the
features for each input, how can we best utilize
these systems in output selection and combina-
tion? We perform system combination and MAX
selection and obtain the scores shown in Table 4.
The consensus decoding approach uses system-
specific features as described in Section 2.2 to dis-
tinguish between E01-E04.
As with English to Italian, output combination
performs the best of the multi-source techniques.
MAX performs better with translations generated
by ?All? weights than with ?Self?, and the con-
5Note that in the ?Self tuned? setting we have only a quar-
ter as much tuning data as for ?All tuned?.
724
Input Language test2006 test2007
French (FR) 29.72 30.21
Spanish (ES) 29.55 29.62
Swedish (SV) 29.33 29.44
Portuguese (PT) 28.75 28.79
Danish (DA) 27.20 27.48
Greek (EL) 26.93 26.78
Italian (IT) 26.82 26.51
German (DE) 24.04 24.41
Dutch (NL) 23.79 24.28
Finnish (FI) 18.96 18.85
Table 5: BLEU scores for individual translation
systems into English trained on Europarl, from
best to worst.
verse is true for SYSCOMB. Given the robust per-
formance of MAX when translation scores origi-
nated from the same translation model in English
to Italian, it is not surprising that it favors the
case where all the outputs are scored by the same
model (?All tuned?). On the other hand, diversity
amongst the system outputs has been shown to be
important to the performance of system combina-
tion techniques (Macherey and Och, 2007). This
may give an indication as to why the ?Self tuned?
data produced higher scores in consensus decod-
ing ? the outputs will be more highly divergent due
to their different tuning conditions.
4 Experiments: Multilingual Input
Multilingual cases are the traditional realm of
multi-source translation. We no longer have di-
rectly comparable translation models; instead each
input language has a separate set of rules for trans-
lating to the output language. However, the avail-
ability of (and demand for) multi-parallel corpora
makes this form of multi-source translation of
great practical use.
4.1 Lattice Inputs
As described in Section 2.3, lattices can be used
to provide a compact format for translating multi-
lingual inputs to a multi-source translation system.
We trim all non-skeleton node paths to a maximum
length of four to reduce complexity when decod-
ing. Such long paths are mostly a result of errors in
the original word alignments, and therefore prun-
ing these links is largely innocuous.
We train on the Europarl corpus and use the
FR SV ES DA PT IT EL NL DE FI
BLE
U
0.15
0.20
0.25
0.30
0.35
0.40
0.45 OracleMAXSolo
Figure 6: Performance for multilingual multi-
source translation (test2005) as each language in-
put is added, showing Oracle target selection,
MAX score, or just a single language input (Solo).
in-domain test sets provided for previous years?
Workshops on Statistical Machine Translation.
Because of the computational complexity of deal-
ing with so many models, we train on only the first
100,000 sentences of each parallel corpus. Sin-
gle system baseline scores for each language are
shown in Table 5.
Besides comparing the different multi-source
translation methods discussed above, in this task
we also want to examine what happens when we
use different numbers of input languages. To de-
termine the best order to add languages, we per-
formed a greedy search over oracle BLEU scores
for test set test2005. We started with the best scor-
ing single system, French to English, and in each
iteration picked one additional system that would
maximise BLEU if we always selected the trans-
lation system output closest to the reference. The
results are shown in Figure 6.
The oracle selection order differs from the or-
der of the best performing systems, which could
be due to the high scoring systems having very
similar output while lower scoring systems exhibit
greater diversity. Interestingly, the order of the
languages chosen iterates between the Roman and
Germanic language families and includes Greek
early on. This supports our claim that diversity
is important. Note though that Finnish, which is
also in a separate language family, is selected last,
most likely due to difficulties in word alignment
and translation stemming from its morphological
complexity (Birch et al, 2008). This finding might
also carry over to phrase-table triangulation (Cohn
and Lapata, 2007), where multi-parallel data is
used in training to augment a standard translation
725
Approach test2006 test2007
French Only 29.72 30.21
French + Swedish
MAX 29.86 30.13
LATTICE 29.33 29.97
MULTILATTICE 29.55 29.88
SYSCOMB 31.32 31.77
French + Swedish + Spanish
MAX 30.18 30.33
LATTICE 29.98 30.45
MULTILATTICE 30.50 30.50
SYSCOMB 33.77 33.87
6 Languages
MAX 28.37 28.33
LATTICE 30.22 30.91
MULTILATTICE 30.59 30.59
SYSCOMB 35.47 36.03
Table 6: BLEU scores for multi-source translation
systems into English trained on Europarl. Single
source French decoding is shown as a baseline.
system.
We choose to evaluate translation perfor-
mance at three combination levels: two lan-
guages (French and Swedish), three languages
(+Spanish), and six languages (+Danish, Por-
tuguese, Italian). For each combination we ap-
ply MAX, SYSCOMB, French skeleton lattice in-
put translation LATTICE, and monotone decoding
over multiple skeleton lattices, MULTILATTICE.
Results are shown in Table 6.
To enable the decoder used in LATTICE and
MULTILATTICE to learn weights for different
sources, we add a feature to the phrase table for
each of the languages being translated. This fea-
ture takes as its value the number of words on the
source side of the phrase. By weighting this fea-
ture up or down for each language, the decoder can
prefer word links from specific languages.
As seen in previous work in multi-source trans-
lation, MAX output selection performs well with
two or three languages but degrades as more lan-
guages are added to the input. Conversely, our
lattice input method shows upward trends: LAT-
TICE is comparable with MAX on three inputs and
scores increase in the six language case.
Given the higher scores for output combination
over input combination, what differences can we
observe between the systems? Both systems have
features that indicate the contributions of each in-
put language to the final output. With input com-
bination, we are forced by the decoder to take the
maximum scoring path through the lattice, but in
output combination we have the aggregate vote of
word confidences generated by each system. If we
could combine word arc scores across inputs, as in
output combination, we might get a more robust
solution for taking advantage of the available sim-
ilarities on the target side of the translation. This
points to a direction for future research.
Other differences between the systems may ex-
plain the score gap between our input and output
combination approaches. Consensus decoding al-
lows you to mix and match fragments that aren?t
necessarily stored as fragments in the phrase table.
Another difference is the richer space of reorder-
ings in TER-based lattices, due to the ability of the
metric to handle long-distance alignments.
5 Conclusion
We analyzed three approaches for dealing with
multi-source translation. While MAX is mostly a
poor performer, the upper bound of output selec-
tion is stunning. The very positive results for out-
put system combination across all data conditions
are quite promising. Output combination achieves
these results while the using the limited expres-
sive power of n-best inputs. The potential of using
a more expressive format ? such as lattices that
represent the joint search space of multiple mod-
els ? is high. Our first attempts at adapting lattices
to multi-source translation input show promise for
future development. We have only scratched the
surface of methods for constructing input lattices,
and plan to actively continue research into improv-
ing these methods.
Acknowledgments
Thanks to Chris Callison-Burch for many insight-
ful discussions, and to Chris Dyer for his imple-
mentation of lattice decoding in Moses.
This work was supported by the EuroMatrix
project funded by the European Commission (6th
Framework Programme), and has made use of
the resources provided by the Edinburgh Com-
pute and Data Facility (http://www.ecdf.
ed.ac.uk/), which is partially supported by the
eDIKT initiative (http://www.edikt.org).
We also acknowledge the support of the EPSRC
(grant GR/T04557/01).
726
References
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proceed-
ings of ASRU, pages 351?354, Trento, Italy, Decem-
ber.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings
of NAACL: HLT, pages 16?23, Edmonton, Canada,
May.
Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In Proceedings of IEEE ICASSP, pages
1297?1300, Honolulu, Hawaii, USA, April.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting success in machine translation. In
Proceedings of EMNLP, pages 745?754, Honolulu,
Hawaii, USA, October.
Trevor Cohn and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use
of multi-parallel corpora. In Proceedings of ACL,
pages 728?735, Prague, Czech Republic, June.
Marta Ruiz Costa-jussa`, Josep M. Crego, Patrik Lam-
bert, Maxim Khalilov, Jose? A. R. Fonollosa, Jose? B.
Mario, and Rafael E. Banchs. 2007. Ngram-based
statistical machine translation enhanced with mul-
tiple weighted reordering hypotheses. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 167?170, Prague, Czech Repub-
lic, June.
Christopher J. Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL: HLT, pages 1012?
1020, Columbus, Ohio, USA, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Christopher J. Dyer, Ondr?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of ACL: Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit X, pages 79?86, Phuket, Thailand,
September.
Wolfgang Macherey and Franz J. Och. 2007. An
empirical study on computing consensus transla-
tions from multiple machine translation systems.
In Proceedings of EMNLP-CoNLL, pages 986?995,
Prague, Czech Republic, June.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation for multi-
ple machine translation systems using enhanced hy-
pothesis alignment. In Proceedings of EACL, pages
33?40, Trento, Italy, April.
Franz Josef Och and Hermann Ney. 2001. Statis-
tical multi-source translation. In Proceedings of
MT Summit VIII, pages 253?258, Santiago de Com-
postela, Spain, September.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?52.
Matthias Paulik, Kay Rottmann, Jan Niehues, Al-
mut Silja Hildebrand, and Stephan Vogel. 2007.
The ISL phrase-based MT system for the 2007 ACL
workshop on statistical machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 197?202, Prague, Czech
Republic, June.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
statistical machine translation. In Proceedings of
ACL: HLT, Main Conference, pages 9?16, New
York, New York, USA, June.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007a. Improved word-level system
combination for machine translation. In Proceed-
ings of ACL, pages 312?319, Prague, Czech Repub-
lic, June.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bon-
nie J. Dorr. 2007b. Combining output from mul-
tiple machine translation systems. In Proceedings
of NAACL: HLT, pages 228?235, Rochester, New
York, USA, April.
Lane Schwartz. 2008. Multi-source translation meth-
ods. In Proceedings of AMTA, pages 279?288,
Waikiki, Hawaii, USA, October.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted human an-
notation. In Proceedings of AMTA, pages 223?231,
Boston, Massachusetts, USA, August.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901?904, Denver, Colorado, USA, October.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sug-
aya, Hirofumi Yamamoto, and Seiichi Yamamoto.
2002. Toward a broad-coverage bilingual corpus for
speech translation of travel conversations in the real
world. In Proceedings of LREC, pages 147?152,
Las Palmas, Canary Islands, Spain, May.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated Chinese word segmentation
in statistical machine translation. In Proceedings of
IWSLT, Pittsburgh, Pennsylvania, USA, October.
727
Proceedings of the 43rd Annual Meeting of the ACL, pages 255?262,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Scaling Phrase-Based Statistical Machine Translation
to Larger Corpora and Longer Phrases
Chris Callison-Burch Colin Bannard
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
{chris,colin}@linearb.co.uk
Josh Schroeder
Linear B Ltd.
39 B Cumberland Street
Edinburgh EH3 6RA
josh@linearb.co.uk
Abstract
In this paper we describe a novel data
structure for phrase-based statistical ma-
chine translation which allows for the re-
trieval of arbitrarily long phrases while si-
multaneously using less memory than is
required by current decoder implementa-
tions. We detail the computational com-
plexity and average retrieval times for
looking up phrase translations in our suf-
fix array-based data structure. We show
how sampling can be used to reduce the
retrieval time by orders of magnitude with
no loss in translation quality.
1 Introduction
Statistical machine translation (SMT) has an advan-
tage over many other statistical natural language
processing applications in that training data is reg-
ularly produced by other human activity. For some
language pairs very large sets of training data are
now available. The publications of the European
Union and United Nations provide gigbytes of data
between various language pairs which can be eas-
ily mined using a web crawler. The Linguistics
Data Consortium provides an excellent set of off
the shelf Arabic-English and Chinese-English paral-
lel corpora for the annual NIST machine translation
evaluation exercises.
The size of the NIST training data presents a prob-
lem for phrase-based statistical machine translation.
Decoders such as Pharaoh (Koehn, 2004) primarily
use lookup tables for the storage of phrases and their
translations. Since retrieving longer segments of hu-
man translated text generally leads to better trans-
lation quality, participants in the evaluation exer-
cise try to maximize the length of phrases that are
stored in lookup tables. The combination of large
corpora and long phrases means that the table size
can quickly become unwieldy.
A number of groups in the 2004 evaluation exer-
cise indicated problems dealing with the data. Cop-
ing strategies included limiting the length of phrases
to something small, not using the entire training data
set, computing phrases probabilities on disk, and fil-
tering the phrase table down to a manageable size
after the testing set was distributed. We present a
data structure that is easily capable of handling the
largest data sets currently available, and show that it
can be scaled to much larger data sets.
In this paper we:
? Motivate the problem with storing enumerated
phrases in a table by examining the memory re-
quirements of the method for the NIST data set
? Detail the advantages of using long phrases in
SMT, and examine their potential coverage
? Describe a suffix array-based data structure
which allows for the retrieval of translations
of arbitrarily long phrases, and show that it re-
quires far less memory than a table
? Calculate the computational complexity and
average time for retrieving phrases and show
how this can be sped up by orders of magnitude
with no loss in translation accuracy
2 Related Work
Koehn et al (2003) compare a number of differ-
ent approaches to phrase-based statistical machine
255
length num uniq
(mil)
average #
translations
avg trans
length
1 .88 8.322 1.37
2 16.5 1.733 2.35
3 42.6 1.182 3.44
4 58.7 1.065 4.58
5 65.0 1.035 5.75
6 66.4 1.022 6.91
7 65.8 1.015 8.07
8 64.3 1.012 9.23
9 62.2 1.010 10.4
10 59.9 1.010 11.6
Table 1: Statistics about Arabic phrases in the NIST-
2004 large data track.
translation including the joint probability phrase-
based model (Marcu and Wong, 2002) and a vari-
ant on the alignment template approach (Och and
Ney, 2004), and contrast them to the performance of
the word-based IBM Model 4 (Brown et al, 1993).
Most relevant for the work presented in this paper,
they compare the effect on translation quality of us-
ing various lengths of phrases, and the size of the
resulting phrase probability tables.
Tillmann (2003) further examines the relationship
between maximum phrase length, size of the trans-
lation table, and accuracy of translation when in-
ducing block-based phrases from word-level align-
ments. Venugopal et al (2003) and Vogel et al
(2003) present methods for achieving better transla-
tion quality by growing incrementally larger phrases
by combining smaller phrases with overlapping seg-
ments.
3 Scaling to Long Phrases
Table 1 gives statistics about the Arabic-English par-
allel corpus used in the NIST large data track. The
corpus contains 3.75 million sentence pairs, and has
127 million words in English, and 106 million words
in Arabic. The table shows the number of unique
Arabic phrases, and gives the average number of
translations into English and their average length.
Table 2 gives estimates of the size of the lookup
tables needed to store phrases of various lengths,
based on the statistics in Table 1. The number of
unique entries is calculated as the number unique
length entries
(mil)
words
(mil)
memory
(gigs)
including
alignments
1 7.3 10 .1 .11
2 36 111 .68 .82
3 86 412 2.18 2.64
4 149 933 4.59 5.59
5 216 1,645 7.74 9.46
6 284 2,513 11.48 14.07
7 351 3,513 15.70 19.30
8 416 4,628 20.34 25.05
9 479 5,841 25.33 31.26
10 539 7,140 30.62 37.85
Table 2: Estimated size of lookup tables for the
NIST-2004 Arabic-English data
length coverage length coverage
1 93.5% 6 4.70%
2 73.3% 7 2.95%
3 37.1% 8 2.14%
4 15.5% 9 1.99%
5 8.05% 10 1.49%
Table 3: Lengths of phrases from the training data
that occur in the NIST-2004 test set
phrases times the average number of translations.
The number of words in the table is calculated as the
number of unique phrases times the phrase length
plus the number of entries times the average transla-
tion length. The memory is calculated assuming that
each word is represented with a 4 byte integer, that
each entry stores its probability as an 8 byte double
and that each word alignment is stored as a 2 byte
short. Note that the size of the table will vary de-
pending on the phrase extraction technique.
Table 3 gives the percent of the 35,313 word long
test set which can be covered using only phrases of
the specified length or greater. The table shows the
efficacy of using phrases of different lengths. The ta-
ble shows that while the rate of falloff is rapid, there
are still multiple matches of phrases of length 10.
The longest matching phrase was one of length 18.
There is little generalization in current SMT imple-
mentations, and consequently longer phrases gener-
ally lead to better translation quality.
256
3.1 Why use phrases?
Statistical machine translation made considerable
advances in translation quality with the introduction
of phrase-based translation. By increasing the size
of the basic unit of translation, phrase-based ma-
chine translation does away with many of the prob-
lems associated with the original word-based for-
mulation of statistical machine translation (Brown
et al, 1993), in particular:
? The Brown et al (1993) formulation doesn?t
have a direct way of translating phrases; instead
they specify a fertility parameter which is used
to replicate words and translate them individu-
ally.
? With units as small as words, a lot of reordering
has to happen between languages with different
word orders. But the distortion parameter is a
poor explanation of word order.
Phrase-based SMT overcomes the first of these
problems by eliminating the fertility parameter
and directly handling word-to-phrase and phrase-to-
phrase mappings. The second problem is alleviated
through the use of multi-word units which reduce
the dependency on the distortion parameter. Less
word re-ordering need occur since local dependen-
cies are frequently captured. For example, common
adjective-noun alternations are memorized. How-
ever, since this linguistic information is not encoded
in the model, unseen adjective noun pairs may still
be handled incorrectly.
By increasing the length of phrases beyond a
few words, we might hope to capture additional
non-local linguistic phenomena. For example, by
memorizing longer phrases we may correctly learn
case information for nouns commonly selected by
frequently occurring verbs; we may properly han-
dle discontinuous phrases (such as French negation,
some German verb forms, and English verb particle
constructions) that are neglected by current phrase-
based models; and we may by chance capture some
agreement information in coordinated structures.
3.2 Deciding what length of phrase to store
Despite the potential gains from memorizing longer
phrases, the fact remains that as phrases get longer
length coverage length coverage
1 96.3% 6 21.9%
2 94.9% 7 11.2%
3 86.1% 8 6.16%
4 65.6% 9 3.95%
5 40.9% 10 2.90%
Table 4: Coverage using only repeated phrases of
the specified length
there is a decreasing likelihood that they will be re-
peated. Because of the amount of memory required
to store a phrase table, in current implementations a
choice is made as to the maximum length of phrase
to store.
Based on their analysis of the relationship be-
tween translation quality and phrase length, Koehn
et al (2003) suggest limiting phrase length to three
words or less. This is entirely a practical sugges-
tion for keeping the phrase table to a reasonable
size, since they measure minor but incremental im-
provement in translation quality up to their maxi-
mum tested phrase length of seven words.1
Table 4 gives statistics about phrases which oc-
cur more than once in the English section of the Eu-
roparl corpus (Koehn, 2002) which was used in the
Koehn et al (2003) experiments. It shows that the
percentage of words in the corpus that can be cov-
ered by repeated phrases falls off rapidly at length
6, but that even phrases up to length 10 are able to
cover a non-trivial portion of the corpus. This draws
into question the desirability of limiting phrase re-
trieval to length three.
The decision concerning what length of phrases
to store in the phrase table seems to boil down to
a practical consideration: one must weigh the like-
lihood of retrieval against the memory needed to
store longer phrases. We present a data structure
where this is not a consideration. Our suffix array-
based data structure allows the retrieval of arbitrar-
ily long phrases, while simultaneously requiring far
less memory than the standard table-based represen-
tation.
1While the improvements to translation quality reported in
Koehn et al (2003) are minor, their evaluation metric may not
have been especially sensitive to adding longer phrases. They
used the Bleu evaluation metric (Papineni et al, 2002), but
capped the n-gram precision at 4-grams.
257
01
2
3
4
5
6
7
8
9
spain declined to confirm that spain declined to aid morocco
declined to confirm that spain declined to aid morocco
to confirm that spain declined to aid morocco
confirm that spain declined to aid morocco
that spain declined to aid morocco
spain declined to aid morocco
declined to aid morocco
to aid morocco
aid morocco
morocco
spain declined to confirm that spain declined aidto morocco
0 1 2 3 4 5 6 87 9
s[0]
s[1]
s[2]
s[3]
s[4]
s[5]
s[6]
s[7]
s[8]
s[9]
Initialized, unsorted
Suffix Array
Suffixes denoted by s[i]
Corpus
Index of
words:
Figure 1: An initialized, unsorted suffix array for a
very small corpus
4 Suffix Arrays
The suffix array data structure (Manber and Myers,
1990) was introduced as a space-economical way of
creating an index for string searches. The suffix ar-
ray data structure makes it convenient to compute
the frequency and location of any substring or n-
gram in a large corpus. Abstractly, a suffix array is
an alphabetically-sorted list of all suffixes in a cor-
pus, where a suffix is a substring running from each
position in the text to the end. However, rather than
actually storing all suffixes, a suffix array can be
constructed by creating a list of references to each
of the suffixes in a corpus. Figure 1 shows how a
suffix array is initialized for a corpus with one sen-
tence. Each index of a word in the corpus has a cor-
responding place in the suffix array, which is identi-
cal in length to the corpus. Figure 2 shows the final
state of the suffix array, which is as a list of the in-
dices of words in the corpus that corresponds to an
alphabetically sorted list of the suffixes.
The advantages of this representation are that it is
compact and easily searchable. The total size of the
suffix array is a constant amount of memory. Typ-
ically it is stored as an array of integers where the
array is the same length as the corpus. Because it is
organized alphabetically, any phrase can be quickly
located within it using a binary search algorithm.
Yamamoto and Church (2001) show how to use
suffix arrays to calculate a number of statistics that
are interesting in natural language processing appli-
cations. They demonstrate how to calculate term fre-
8
3
6
1
9
5
0
4
7
2
to aid morocco
to confirm that spain declined to aid morocco
morocco
spain declined to aid morocco
declined to confirm that spain declined to aid morocco
declined to aid morocco
confirm that spain declined to aid morocco
aid morocco
that spain declined to aid morocco
spain declined to confirm that spain declined to aid morocco
Sorted
Suffix Array
Suffixes denoted by s[i]
s[0]
s[1]
s[2]
s[3]
s[4]
s[5]
s[6]
s[7]
s[8]
s[9]
Figure 2: A sorted suffix array and its corresponding
suffixes
quency / inverse document frequency (tf / idf) for all
n-grams in very large corpora, as well as how to use
these frequencies to calculate n-grams with high mu-
tual information and residual inverse document fre-
quency. Here we show how to apply suffix arrays to
parallel corpora to calculate phrase translation prob-
abilities.
4.1 Applied to parallel corpora
In order to adapt suffix arrays to be useful for sta-
tistical machine translation we need a data structure
with the following elements:
? A suffix array created from the source language
portion of the corpus, and another created from
the target language portion of the corpus,
? An index that tells us the correspondence be-
tween sentence numbers and positions in the
source and target language corpora,
? An alignment a for each sentence pair in the
parallel corpus, where a is defined as a subset
of the Cartesian product of the word positions
in a sentence e of length I and a sentence f of
length J :
a ? {(i, j) : i = 1...I; j = 1...J}
? A method for extracting the translationally
equivalent phrase for a subphrase given an
aligned sentence pair containing that sub-
phrase.
The total memory usage of the data structure is
thus the size of the source and target corpora, plus
the size of the suffix arrays (identical in length to the
258
corpora), plus the size of the two indexes that cor-
relate sentence positions with word positions, plus
the size of the alignments. Assuming we use ints
to represent words and indices, and shorts to repre-
sent word alignments, we get the following memory
usage:
2 ? num words in source corpus ? sizeof(int)+
2 ? num words in target corpus ? sizeof(int)+
2 ? number sentence pairs ? sizeof(int)+
number of word alignments ? sizeof(short)
The total amount of memory required to store the
NIST Arabic-English data using this data structure
is
2 ? 105,994,774 ? sizeof(int)+
2 ? 127,450,473 ? sizeof(int)+
2 ? 3,758,904 ? sizeof(int)+
92,975,229 ? sizeof(short)
Or just over 2 Gigabytes.
4.2 Calculating phrase translation
probabilities
In order to produce a set of phrase translation prob-
abilities, we need to examine the ways in which
they are calculated. We consider two common ways
of calculating the translation probability: using the
maximum likelihood estimator (MLE) and smooth-
ing the MLE using lexical weighting.
The maximum likelihood estimator for the proba-
bility of a phrase is defined as
p(f? |e?) =
count(f? , e?)
?
f? count(f? , e?)
(1)
Where count(f? , e?) gives the total number of times
the phrase f? was aligned with the phrase e? in the
parallel corpus. We define phrase alignments as fol-
lows. A substring e? consisting of the words at po-
sitions l...m is aligned with the phrase f? by way of
the subalignment
s = a ? {(i, j) : i = l...m, j = 1...J}
The aligned phrase f? is the subphrase in f which
spans from min(j) to max(j) for j|(i, j) ? s.
The procedure for generating the counts that are
used to calculate the MLE probability using our suf-
fix array-based data structures is:
1. Locate all the suffixes in the English suffix ar-
ray which begin with the phrase e?. Since the
suffix array is sorted alphabetically we can eas-
ily find the first occurrence s[k] and the last oc-
currence s[l]. The length of the span in the suf-
fix array l?k+1 indicates the number of occur-
rences of e? in the corpus. Thus the denominator
?
f? count(f? , e?) can be calculated as l ? k + 1.
2. For each of the matching phrases s[i] in the
span s[k]...s[l], look up the value of s[i] which
is the word index w of the suffix in the English
corpus. Look up the sentence number that in-
cludes w, and retrieve the corresponding sen-
tences e and f , and their alignment a.
3. Use a to extract the target phrase f? that aligns
with the phrase e? that we are searching for. In-
crement the count for < f?, e? >.
4. Calculate the probability for each unique
matching phrase f? using the formula in Equa-
tion 1.
A common alternative formulation of the phrase
translation probability is to lexically weight it as fol-
lows:
plw(f? |e?, s) =
n?
i=1
1
|{i|(i, j) ? s}|
?
?(i,j)?s
p(fj |ei)
(2)
Where n is the length of e?.
In order to use lexical weighting we would need
to repeat steps 1-4 above for each word ei in e?. This
would give us the values for p(fj |ei). We would fur-
ther need to retain the subphrase alignment s in or-
der to know the correspondence between the words
(i, j) ? s in the aligned phrases, and the total num-
ber of foreign words that each ei is aligned with
(|{i|(i, j) ? s}|). Since a phrase alignment < f?, e? >
may have multiple possible word-level alignments,
we retain a set of alignments S and take the maxi-
mum:
259
p(f? |e?, S) = p(f? |e?) ? argmax
s?S
plw(f? |e?, s) (3)
Thus our suffix array-based data structure can be
used straightforwardly to look up all aligned trans-
lations for a given phrase and calculate the proba-
bilities on-the-fly. In the next section we turn to
the computational complexity of constructing phrase
translation probabilities in this way.
5 Computational Complexity
Computational complexity is relevant because there
is a speed-memory tradeoff when adopting our data
structure. What we gained in memory efficiency
may be rendered useless if the time it takes to cal-
culate phrase translation probabilities is unreason-
ably long. The computational complexity of looking
up items in a hash table, as is done in current table-
based data structures, is extremely fast. Looking up
a single phrase can be done in unit time, O(1).
The computational complexity of our method has
the following components:
? The complexity of finding all occurrences of
the phrase in the suffix array
? The complexity of retrieving the associated
aligned sentence pairs given the positions of the
phrase in the corpus
? The complexity of extracting all aligned
phrases using our phrase extraction algorithm
? The complexity of calculating the probabilities
given the aligned phrases
The methods we use to execute each of these, and
their complexities are as follow:
? Since the array is sorted, finding all occur-
rences of the English phrase is extremely fast.
We can do two binary searches: one to find the
first occurrence of the phrase and a second to
find the last. The computational complexity is
therefore bounded by O(2 log(n)) where n is
the length of the corpus.
? We use a similar method to look up the sen-
tences ei and fi and word-level alignment ai
phrase freq O time (ms)
respect for the
dead
3 80 24
since the end of
the cold war
19 240 136
the parliament 1291 4391 1117
of the 290921 682550 218369
Table 5: Examples of O and calculation times for
phrases of different frequencies
that are associated with the position wi in the
corpus of each phrase occurrence e?i. The com-
plexity is O(k ? 2 log(m)) where k is the num-
ber of occurrences of e? and m is the number of
sentence pairs in the parallel corpus.
? The complexity of extracting the aligned phrase
for a single occurrence of e?i is O(2 log(|ai|) to
get the subphrase alignment si, since we store
the alignments in a sorted array. The complex-
ity of then getting f?i from si is O(length(f?i)).
? The complexity of summing over all aligned
phrases and simultaneously calculating their
probabilities is O(k).
Thus we have a total complexity of:
O(2 log(n) + k ? 2 log(m) (4)
+
e?1...e?k?
ai,f?i|e?i
(2 log(|ai|) + length(f?i)) + k) (5)
for the MLE estimation of the translation probabil-
ities for a single phrase. The complexity is domi-
nated by the k terms in the equation, when the num-
ber of occurrences of the phrase in the corpus is
high. Phrases with high frequency may cause exces-
sively long retrieval time. This problem is exacer-
bated when we shift to a lexically weighted calcula-
tion of the phrase translation probability. The com-
plexity will be multiplied across each of the compo-
nent words in the phrase, and the component words
themselves will be more frequent than the phrase.
Table 5 shows example times for calculating the
translation probabilities for a number of phrases. For
frequent phrases like of the these times get unaccept-
ably long. While our data structure is perfect for
260
overcoming the problems associated with storing the
translations of long, infrequently occurring phrases,
it in a way introduces the converse problem. It has
a clear disadvantage in the amount of time it takes
to retrieve commonly occurring phrases. In the next
section we examine the use of sampling to speed up
the calculation of translation probabilities for very
frequent phrases.
6 Sampling
Rather than compute the phrase translation proba-
bilities by examining the hundreds of thousands of
occurrences of common phrases, we instead sam-
ple from a small subset of the occurrences. It is
unlikely that we need to extract the translations of
all occurrences of a high frequency phrase in order
to get a good approximation of their probabilities.
We instead cap the number of occurrences that we
consider, and thus give a maximum bound on k in
Equation 5.
In order to determine the effect of different lev-
els of sampling, we compare the translation quality
against cumulative retrieval time for calculating the
phrase translation probabilities for all subphrases in
an evaluation set. We translated a held out set of
430 German sentences with 50 words or less into
English. The test sentences were drawn from the
01/17/00 proceedings of the Europarl corpus. The
remainder of the corpus (1 million sentences) was
used as training data to calculate the phrase trans-
lation probabilities. We calculated the translation
quality using Bleu?s modified n-gram precision met-
ric (Papineni et al, 2002) for n-grams of up to length
four. The framework that we used to calculate the
translation probabilities was similar to that detailed
in Koehn et al (2003). That is:
e? = argmax
eI1
p(eI1|f
I
1) (6)
= argmax
eI1
pLM (e
I
1) ? (7)
I?
i=1
p(f?i|e?i)d(ai ? bi?1)plw(f?i|e?i,a) (8)
Where pLM is a language model probability and d is
a distortion probability which penalizes movement.
Table 6 gives a comparison of the translation qual-
ity under different levels of sampling. While the ac-
sample size time quality
unlimited 6279 sec .290
50000 1051 sec .289
10000 336 sec .291
5000 201 sec .289
1000 60 sec .288
500 35 sec .288
100 10 sec .288
Table 6: A comparison of retrieval times and trans-
lation quality when the number of translations is
capped at various sample sizes
curacy fluctuates very slightly it essentially remains
uniformly high for all levels of sampling. There are
a number of possible reasons for the fact that the
quality does not decrease:
? The probability estimates under sampling are
sufficiently good that the most probable trans-
lations remain unchanged,
? The interaction with the language model prob-
ability rules out the few misestimated probabil-
ities, or
? The decoder tends to select longer or less fre-
quent phrases which are not affected by the
sampling.
While the translation quality remains essentially
unchanged, the cumulative time that it takes to cal-
culate the translation probabilities for all subphrases
in the 430 sentence test set decreases radically. The
total time drops by orders of magnitude from an hour
and a half without sampling down to a mere 10 sec-
onds with a cavalier amount of sampling. This sug-
gests that the data structure is suitable for deployed
SMT systems and that no additional caching need
be done to compensate for the structure?s computa-
tional complexity.
7 Discussion
The paper has presented a super-efficient data struc-
ture for phrase-based statistical machine translation.
We have shown that current table-based methods are
unwieldily when used in conjunction with large data
sets and long phrases. We have contrasted this with
our suffix array-based data structure which provides
261
a very compact way of storing large data sets while
simultaneously allowing the retrieval of arbitrarily
long phrases.
For the NIST-2004 Arabic-English data set,
which is among the largest currently assembled for
statistical machine translation, our representation
uses a very manageable 2 gigabytes of memory. This
is less than is needed to store a table containing
phrases with a maximum of three words, and is ten
times less than the memory required to store a table
with phrases of length eight.
We have further demonstrated that while compu-
tational complexity can make the retrieval of trans-
lation of frequent phrases slow, the use of sampling
is an extremely effective countermeasure to this.
We demonstrated that calculating phrase translation
probabilities from sets of 100 occurrences or less re-
sults in nearly no decrease in translation quality.
The implications of the data structure presented
in this paper are significant. The compact rep-
resentation will allow us to easily scale to paral-
lel corpora consisting of billions of words of text,
and the retrieval of arbitrarily long phrases will al-
low experiments with alternative decoding strate-
gies. These facts in combination allow for an even
greater exploitation of training data in statistical ma-
chine translation.
References
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation. Unpublished
Draft.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA.
Udi Manber and Gene Myers. 1990. Suffix arrays:
A new method for on-line string searches. In The
First Annual ACM-SIAM Symposium on Dicrete Algo-
rithms, pages 319?327.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?450, De-
cember.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proceed-
ings of EMNLP.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venugopal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical machine translation sys-
tem. In Proceedings of MT Summit 9.
Mikio Yamamoto and Kenneth Church. 2001. Using suf-
fix arrays to compute term frequency and document
frequency for all substrings in a corpus. Compuata-
tional Linguistics, 27(1):1?30.
262
Proceedings of the Second Workshop on Statistical Machine Translation, pages 136?158,
Prague, June 2007. c?2007 Association for Computational Linguistics
(Meta-) Evaluation of Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb clsp jhu edu
Cameron Fordyce
CELCT
fordyce celct it
Philipp Koehn
University of Edinburgh
pkoehn inf ed ac uk
Christof Monz
Queen Mary, University of London
christof dcs qmul ac uk
Josh Schroeder
University of Edinburgh
j schroeder ed ac uk
Abstract
This paper evaluates the translation quality
of machine translation systems for 8 lan-
guage pairs: translating French, German,
Spanish, and Czech to English and back.
We carried out an extensive human evalua-
tion which allowed us not only to rank the
different MT systems, but also to perform
higher-level analysis of the evaluation pro-
cess. We measured timing and intra- and
inter-annotator agreement for three types of
subjective evaluation. We measured the cor-
relation of automatic evaluation metrics with
human judgments. This meta-evaluation re-
veals surprising facts about the most com-
monly used methodologies.
1 Introduction
This paper presents the results for the shared trans-
lation task of the 2007 ACL Workshop on Statistical
Machine Translation. The goals of this paper are
twofold: First, we evaluate the shared task entries
in order to determine which systems produce trans-
lations with the highest quality. Second, we analyze
the evaluation measures themselves in order to try to
determine ?best practices? when evaluating machine
translation research.
Previous ACL Workshops on Machine Transla-
tion were more limited in scope (Koehn and Monz,
2005; Koehn and Monz, 2006). The 2005 workshop
evaluated translation quality only in terms of Bleu
score. The 2006 workshop additionally included a
limited manual evaluation in the style of NIST ma-
chine translation evaluation workshop. Here we ap-
ply eleven different automatic evaluation metrics,
and conduct three different types of manual evalu-
ation.
Beyond examining the quality of translations pro-
duced by various systems, we were interested in ex-
amining the following questions about evaluation
methodologies: How consistent are people when
they judge translation quality? To what extent do
they agree with other annotators? Can we im-
prove human evaluation? Which automatic evalu-
ation metrics correlate most strongly with human
judgments of translation quality?
This paper is organized as follows:
? Section 2 gives an overview of the shared task.
It describes the training and test data, reviews
the baseline system, and lists the groups that
participated in the task.
? Section 3 describes the manual evaluation. We
performed three types of evaluation: scoring
with five point scales, relative ranking of trans-
lations of sentences, and ranking of translations
of phrases.
? Section 4 lists the eleven different automatic
evaluation metrics which were also used to
score the shared task submissions.
? Section 5 presents the results of the shared task,
giving scores for each of the systems in each of
the different conditions.
? Section 6 provides an evaluation of the dif-
ferent types of evaluation, giving intra- and
136
inter-annotator agreement figures for the man-
ual evaluation, and correlation numbers for the
automatic metrics.
2 Shared task overview
This year?s shared task changed in some aspects
from last year?s:
? We gave preference to the manual evaluation of
system output in the ranking of systems. Man-
ual evaluation was done by the volunteers from
participating groups and others. Additionally,
there were three modalities of manual evalua-
tion.
? Automatic metrics were also used to rank the
systems. In total eleven metrics were applied,
and their correlation with the manual scores
was measured.
? As in 2006, translation was from English, and
into English. English was again paired with
German, French, and Spanish. We additionally
included Czech (which was fitting given the lo-
cation of the WS).
Similar to the IWSLT International Workshop on
Spoken Language Translation (Eck and Hori, 2005;
Paul, 2006), and the NIST Machine Translation
Evaluation Workshop (Lee, 2006) we provide the
shared task participants with a common set of train-
ing and test data for all language pairs. The major
part of data comes from current and upcoming full
releases of the Europarl data set (Koehn, 2005).
2.1 Description of the Data
The data used in this year?s shared task was similar
to the data used in last year?s shared task. This year?s
data included training and development sets for the
News Commentary data, which was the surprise out-
of-domain test set last year.
The majority of the training data for the Spanish,
French, and German tasks was drawn from a new
version of the Europarl multilingual corpus. Addi-
tional training data was taken from the News Com-
mentary corpus. Czech language resources were
drawn from the News Commentary data. Additional
resources for Czech came from the CzEng Paral-
lel Corpus (Bojar and Z?abokrtsky?, 2006). Overall,
there are over 30 million words of training data per
language from the Europarl corpus and 1 million
words from the News Commentary corpus. Figure 1
provides some statistics about the corpora used this
year.
2.2 Baseline system
To lower the barrier of entrance to the competition,
we provided a complete baseline MT system, along
with data resources. To summarize, we provided:
? sentence-aligned training corpora
? development and dev-test sets
? language models trained for each language
? an open source decoder for phrase-based SMT
called Moses (Koehn et al, 2006), which re-
places the Pharaoh decoder (Koehn, 2004)
? a training script to build models for Moses
The performance of this baseline system is similar
to the best submissions in last year?s shared task.
2.3 Test Data
The test data was again drawn from a segment of
the Europarl corpus from the fourth quarter of 2000,
which is excluded from the training data. Partici-
pants were also provided with three sets of parallel
text to be used for system development and tuning.
In addition to the Europarl test set, we also col-
lected editorials from the Project Syndicate web-
site1, which are published in all the five languages
of the shared task. We aligned the texts at a sentence
level across all five languages, resulting in 2,007
sentences per language. For statistics on this test set,
refer to Figure 1.
The News Commentary test set differs from the
Europarl data in various ways. The text type are ed-
itorials instead of speech transcripts. The domain is
general politics, economics and science. However, it
is also mostly political content (even if not focused
on the internal workings of the European Union) and
opinion.
2.4 Participants
We received submissions from 15 groups from 14
institutions, as listed in Table 1. This is a slight
1http://www.project-syndicate.com/
137
Europarl Training corpus
Spanish? English French? English German? English
Sentences 1,259,914 1,288,901 1,264,825
Foreign words 33,159,337 33,176,243 29,582,157
English words 31,813,692 32,615,285 31,929,435
Distinct foreign words 345,944 344,287 510,544
Distinct English words 266,976 268,718 250,295
News Commentary Training corpus
Spanish? English French? English German? English Czech? English
Sentences 51,613 43,194 59,975 57797
Foreign words 1,263,067 1,028,672 1,297,673 1,083,122
English words 1,076,273 906,593 1,238,274 1,188,006
Distinct foreign words 84,303 68,214 115,589 142,146
Distinct English words 70,755 63,568 76,419 74,042
Language model data
English Spanish French German
Sentence 1,407,285 1,431,614 1,435,027 1,478,428
Words 34,539,822 36,426,542 35,595,199 32,356,475
Distinct words 280,546 385,796 361,205 558,377
Europarl test set
English Spanish French German
Sentences 2,000
Words 53,531 55,380 53,981 49,259
Distinct words 8,558 10,451 10,186 11,106
News Commentary test set
English Spanish French German Czech
Sentences 2,007
Words 43,767 50,771 49,820 45,075 39,002
Distinct words 10,002 10,948 11,244 12,322 15,245
Figure 1: Properties of the training and test sets used in the shared task. The training data is drawn from the
Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple
languages.
138
ID Participant
cmu-uka Carnegie Mellon University, USA (Paulik et al, 2007)
cmu-syntax Carnegie Mellon University, USA (Zollmann et al, 2007)
cu Charles University, Czech Republic (Bojar, 2007)
limsi LIMSI-CNRS, France (Schwenk, 2007)
liu University of Linko?ping, Sweden(Holmqvist et al, 2007)
nrc National Research Council, Canada (Ueffing et al, 2007)
pct a commercial MT provider from the Czech Republic
saar Saarland University & DFKI, Germany (Chen et al, 2007)
systran SYSTRAN, France & U. Edinburgh, UK (Dugast et al, 2007)
systran-nrc National Research Council, Canada (Simard et al, 2007)
ucb University of California at Berkeley, USA (Nakov and Hearst, 2007)
uedin University of Edinburgh, UK (Koehn and Schroeder, 2007)
umd University of Maryland, USA (Dyer, 2007)
upc University of Catalonia, Spain (Costa-Jussa` and Fonollosa, 2007)
upv University of Valencia, Spain (Civera and Juan, 2007)
Table 1: Participants in the shared task. Not all groups participated in all translation directions.
increase over last year?s shared task where submis-
sions were received from 14 groups from 11 insti-
tutions. Of the 11 groups that participated in last
year?s shared task, 6 groups returned this year.
This year, most of these groups follow a phrase-
based statistical approach to machine translation.
However, several groups submitted results from sys-
tems that followed a hybrid approach.
While building a machine translation system is a
serious undertaking we hope to attract more new-
comers to the field by keeping the barrier of entry
as low as possible. The creation of parallel corpora
such as the Europarl, the CzEng, and the News Com-
mentary corpora should help in this direction by pro-
viding freely available language resources for build-
ing systems. The creation of an open source baseline
system should also go a long way towards achieving
this goal.
For more on the participating systems, please re-
fer to the respective system description in the pro-
ceedings of the workshop.
3 Human evaluation
We evaluated the shared task submissions using both
manual evaluation and automatic metrics. While
automatic measures are an invaluable tool for the
day-to-day development of machine translation sys-
tems, they are an imperfect substitute for human
assessment of translation quality. Manual evalua-
tion is time consuming and expensive to perform,
so comprehensive comparisons of multiple systems
are rare. For our manual evaluation we distributed
the workload across a number of people, including
participants in the shared task, interested volunteers,
and a small number of paid annotators. More than
100 people participated in the manual evaluation,
with 75 of those people putting in at least an hour?s
worth of effort. A total of 330 hours of labor was in-
vested, nearly doubling last year?s all-volunteer ef-
fort which yielded 180 hours of effort.
Beyond simply ranking the shared task submis-
sions, we had a number of scientific goals for the
manual evaluation. Firstly, we wanted to collect
data which could be used to assess how well au-
tomatic metrics correlate with human judgments.
Secondly, we wanted to examine different types of
manual evaluation and assess which was the best.
A number of criteria could be adopted for choos-
ing among different types of manual evaluation: the
ease with which people are able to perform the task,
their agreement with other annotators, their reliabil-
ity when asked to repeat judgments, or the number
of judgments which can be collected in a fixed time
period.
There are a range of possibilities for how human
139
evaluation of machine translation can be done. For
instance, it can be evaluated with reading compre-
hension tests (Jones et al, 2005), or by assigning
subjective scores to the translations of individual
sentences (LDC, 2005). We examined three differ-
ent ways of manually evaluating machine translation
quality:
? Assigning scores based on five point adequacy
and fluency scales
? Ranking translated sentences relative to each
other
? Ranking the translations of syntactic con-
stituents drawn from the source sentence
3.1 Fluency and adequacy
The most widely used methodology when manually
evaluatingMT is to assign values from two five point
scales representing fluency and adequacy. These
scales were developed for the annual NIST Machine
Translation Evaluation Workshop by the Linguistics
Data Consortium (LDC, 2005).
The five point scale for adequacy indicates how
much of the meaning expressed in the reference
translation is also expressed in a hypothesis trans-
lation:
5 = All
4 = Most
3 = Much
2 = Little
1 = None
The second five point scale indicates how fluent
the translation is. When translating into English the
values correspond to:
5 = Flawless English
4 = Good English
3 = Non-native English
2 = Disfluent English
1 = Incomprehensible
Separate scales for fluency and adequacy were
developed under the assumption that a translation
might be disfluent but contain all the information
from the source. However, in principle it seems that
people have a hard time separating these two as-
pects of translation. The high correlation between
people?s fluency and adequacy scores (given in Ta-
bles 17 and 18) indicate that the distinction might be
false.
?
people
's
Iraq
to
services
basic
other
and
,
care
health
,
food
provide
cannot
it
if
occupation
its
sustain
US
the
Can
?k
?
n
n
e
n
a
n
b
i
e
t
e
n
D
i
e
n
s
t
l
e
i
s
t
u
n
g
e
n
g
r
u
n
d
l
e
g
e
n
d
e
a
n
d
e
r
e
u
n
d
G
e
s
u
n
d
h
e
i
t
s
f
?
r
s
o
r
g
e
,N
a
h
r
u
n
g
n
i
c
h
t
V
o
l
k
i
r
a
k
i
s
c
h
e
n
d
e
m
s
i
e
w
e
n
n
,U
S
A
d
i
e
K
?
n
n
e
n
a
u
f
r
e
c
h
t
e
r
h
a
l
t
e
n
B
e
s
e
t
z
u
n
g
 
 
i
h
r
e
R
e
f
e
r
e
n
c
e
 
t
r
a
n
s
l
a
t
i
o
n
NP
NP
NP
VP
NP
VP
S
S
CNP
NP
Constituents selected 
for evaluation
Target phrases
highlighted via
word alignments
Parsed source
sentence
Figure 2: In constituent-based evaluation, the source
sentence was parsed, and automatically aligned with
the reference translation and systems? translations
Another problem with the scores is that there are
no clear guidelines on how to assign values to trans-
lations. No instructions are given to evaluators in
terms of how to quantify meaning, or how many
grammatical errors (or what sort) separates the dif-
ferent levels of fluency. Because of this many judges
either develop their own rules of thumb, or use the
scales as relative rather than absolute. These are
borne out in our analysis of inter-annotator agree-
ment in Section 6.
3.2 Ranking translations of sentences
Because fluency and adequacy were seemingly diffi-
cult things for judges to agree on, and because many
people from last year?s workshop seemed to be using
them as a way of ranking translations, we decided to
try a separate evaluation where people were simply
140
asked to rank translations. The instructions for this
task were:
Rank each whole sentence translation
from Best to Worst relative to the other
choices (ties are allowed).
These instructions were just as minimal as for flu-
ency and adequacy, but the task was considerably
simplified. Rather than having to assign each trans-
lation a value along an arbitrary scale, people simply
had to compare different translations of a single sen-
tence and rank them.
3.3 Ranking translations of syntactic
constituents
In addition to having judges rank the translations
of whole sentences, we also conducted a pilot
study of a new type of evaluation methodology,
which we call constituent-based evaluation. In our
constituent-based evaluation we parsed the source
language sentence, selected constituents from the
tree, and had people judge the translations of those
syntactic phrases. In order to draw judges? attention
to these regions, we highlighted the selected source
phrases and the corresponding phrases in the transla-
tions. The corresponding phrases in the translations
were located via automatic word alignments.
Figure 2 illustrates the constituent based evalu-
ation when applied to a German source sentence.
The German source sentence is parsed, and vari-
ous phrases are selected for evaluation. Word align-
ments are created between the source sentence and
the reference translation (shown), and the source
sentence and each of the system translations (not
shown). We parsed the test sentences for each of
the languages aside from Czech. We used Cowan
and Collins (2005)?s parser for Spanish, Arun and
Keller (2005)?s for French, Dubey (2005)?s for Ger-
man, and Bikel (2002)?s for English.
The word alignments were created with Giza++
(Och and Ney, 2003) applied to a parallel corpus
containing 200,000 sentence pairs of the training
data, plus sets of 4,007 sentence pairs created by
pairing the test sentences with the reference transla-
tions, and the test sentences paired with each of the
system translations. The phrases in the translations
were located using techniques from phrase-based
statistical machine translation which extract phrase
pairs fromword alignments (Koehn et al, 2003; Och
and Ney, 2004). Because the word-alignments were
created automatically, and because the phrase ex-
traction is heuristic, the phrases that were selected
may not exactly correspond to the translations of the
selected source phrase. We noted this in the instruc-
tions to judges:
Rank each constituent translation from
Best to Worst relative to the other choices
(ties are allowed). Grade only the high-
lighted part of each translation.
Please note that segments are selected au-
tomatically, and they should be taken as
an approximate guide. They might in-
clude extra words that are not in the actual
alignment, or miss words on either end.
The criteria that we used to select which con-
stituents were to be evaluated were:
? The constituent could not be the whole source
sentence
? The constituent had to be longer three words,
and be no longer than 15 words
? The constituent had to have a corresponding
phrase with a consistent word alignment in
each of the translations
The final criterion helped reduce the number of
alignment errors.
3.4 Collecting judgments
We collected judgments using a web-based tool.
Shared task participants were each asked to judge
200 sets of sentences. The sets consisted of 5 sys-
tem outputs, as shown in Figure 3. The judges
were presented with batches of each type of eval-
uation. We presented them with five screens of ade-
quacy/fluency scores, five screens of sentence rank-
ings, and ten screens of constituent rankings. The
order of the types of evaluation were randomized.
In order to measure intra-annotator agreement
10% of the items were repeated and evaluated twice
by each judge. In order to measure inter-annotator
agreement 40% of the items were randomly drawn
from a common pool that was shared across all
141
http://www.statmt.org/wmt07/shared-task/judge/do_task.php
WMT07 Manual Evaluation
Rank Segments
You have judged 25 sentences for WMT07 German-English News Corpus, 190 sentences total taking 64.9 seconds per sentence.
Source: K?nnen die USA ihre Besetzung aufrechterhalten, wenn sie dem irakischen Volk nicht Nahrung, Gesundheitsf?rsorge und andere 
grundlegende Dienstleistungen anbieten k?nnen?
Reference: Can the US sustain its occupation if it cannot provide food, health care, and other basic services to Iraq's people?
Translation Rank
The United States can maintain its employment when it the Iraqi people not food, health care and other 
basic services on offer?.
1
Worst
2 3 4 5
Best
The US can maintain its occupation, if they cannot offer the Iraqi people food, health care and other basic 
services?
1
Worst
2 3 4 5
Best
Can the US their occupation sustained if it to the Iraqi people not food, health care and other basic 
services can offer?
1
Worst
2 3 4 5
Best
Can the United States maintain their occupation, if the Iraqi people do not food, health care and other 
basic services can offer?
1
Worst
2 3 4 5
Best
The United States is maintained, if the Iraqi people, not food, health care and other basic services can 
offer?
1
Worst
2 3 4 5
Best
Annotator: ccb Task: WMT07 German-English News Corpus
Instructions: 
Rank each constituent translation from Best to Worst relative to the other choices (ties are allowed). Grade 
only the highlighted part of each translation.
Please note that segments are selected automatically, and they should be taken as an approximate guide. 
They might include extra words on either end that are not in the actual alignment, or miss words.
 
Figure 3: For each of the types of evaluation, judges were shown screens containing up to five different
system translations, along with the source sentence and reference translation.
annotators so that we would have items that were
judged by multiple annotators.
Judges were allowed to select whichever data set
they wanted, and to evaluate translations into what-
ever languages they were proficient in. Shared task
participants were excluded from judging their own
systems.
Table 2 gives a summary of the number of judg-
ments that we collected for translations of individ-
ual sentences. Since we had 14 translation tasks and
four different types of scores, there were 55 differ-
ent conditions.2 In total we collected over 81,000
judgments. Despite the large number of conditions
we managed to collect more than 1,000 judgments
for most of them. This provides a rich source of data
for analyzing the quality of translations produced by
different systems, the different types of human eval-
uation, and the correlation of automatic metrics with
human judgments.3
2We did not perform a constituent-based evaluation for
Czech to English because we did not have a syntactic parser
for Czech. We considered adapting our method to use Bojar
(2004)?s dependency parser for Czech, but did not have the time.
3The judgment data along with all system translations are
available at http://www.statmt.org/wmt07/
4 Automatic evaluation
The past two ACL workshops on machine trans-
lation used Bleu as the sole automatic measure of
translation quality. Bleu was used exclusively since
it is the most widely used metric in the field and
has been shown to correlate with human judgments
of translation quality in many instances (Dodding-
ton, 2002; Coughlin, 2003; Przybocki, 2004). How-
ever, recent work suggests that Bleu?s correlation
with human judgments may not be as strong as pre-
viously thought (Callison-Burch et al, 2006). The
results of last year?s workshop further suggested that
Bleu systematically underestimated the quality of
rule-based machine translation systems (Koehn and
Monz, 2006).
We used the manual evaluation data as a means of
testing the correlation of a range of automatic met-
rics in addition to Bleu. In total we used eleven
different automatic evaluation measures to rank the
shared task submissions. They are:
? Meteor (Banerjee and Lavie, 2005)?Meteor
measures precision and recall of unigrams
when comparing a hypothesis translation
142
Language Pair Test Set Adequacy Fluency Rank Constituent
English-German Europarl 1,416 1,418 1,419 2,626
News Commentary 1,412 1,413 1,412 2,755
German-English Europarl 1,525 1,521 1,514 2,999
News Commentary 1,626 1,620 1,601 3,084
English-Spanish Europarl 1,000 1,003 1,064 1,001
News Commentary 1,272 1,272 1,238 1,595
Spanish-English Europarl 1,174 1,175 1,224 1,898
News Commentary 947 949 922 1,339
English-French Europarl 773 772 769 1,456
News Commentary 729 735 728 1,313
French-English Europarl 834 833 830 1,641
News Commentary 1,041 1,045 1,035 2,036
English-Czech News Commentary 2,303 2,304 2,331 3,968
Czech-English News Commentary 1,711 1,711 1,733 0
Totals 17,763 17,771 17,820 27,711
Table 2: The number of items that were judged for each task during the manual evaluation
against a reference. It flexibly matches words
using stemming and WordNet synonyms. Its
flexible matching was extended to French,
Spanish, German and Czech for this workshop
(Lavie and Agarwal, 2007).
? Bleu (Papineni et al, 2002)?Bleu is currently
the de facto standard in machine translation
evaluation. It calculates n-gram precision and
a brevity penalty, and can make use of multi-
ple reference translations as a way of capturing
some of the allowable variation in translation.
We use a single reference translation in our ex-
periments.
? GTM (Melamed et al, 2003)?GTM general-
izes precision, recall, and F-measure to mea-
sure overlap between strings, rather than over-
lap between bags of items. An ?exponent? pa-
rameter which controls the relative importance
of word order. A value of 1.0 reduces GTM to
ordinary unigram overlap, with higher values
emphasizing order.4
? Translation Error Rate (Snover et al, 2006)?
4The GTM scores presented here are an F-measure with a
weight of 0.1, which counts recall at 10x the level of precision.
The exponent is set at 1.2, which puts a mild preference towards
items with words in the correct order. These parameters could
be optimized empirically for better results.
TER calculates the number of edits required to
change a hypothesis translation into a reference
translation. The possible edits in TER include
insertion, deletion, and substitution of single
words, and an edit which moves sequences of
contiguous words.
? ParaEval precision and ParaEval recall (Zhou
et al, 2006)?ParaEval matches hypothesis and
reference translations using paraphrases that
are extracted from parallel corpora in an unsu-
pervised fashion (Bannard and Callison-Burch,
2005). It calculates precision and recall using a
unigram counting strategy.
? Dependency overlap (Amigo? et al, 2006)?
This metric uses dependency trees for the hy-
pothesis and reference translations, by comput-
ing the average overlap between words in the
two trees which are dominated by grammatical
relationships of the same type.
? Semantic role overlap (Gime?nez and Ma`rquez,
2007)?This metric calculates the lexical over-
lap between semantic roles (i.e., semantic argu-
ments or adjuncts) of the same type in the the
hypothesis and reference translations. It uni-
formly averages lexical overlap over all seman-
tic role types.
143
? Word Error Rate over verbs (Popovic and Ney,
2007)?WER? creates a new reference and a
new hypothesis for each POS class by extract-
ing all words belonging to this class, and then
to calculate the standardWER.We show results
for this metric over verbs.
? Maximum correlation training on adequacy and
on fluency (Liu and Gildea, 2007)?a lin-
ear combination of different evaluation metrics
(Bleu, Meteor, Rouge, WER, and stochastic it-
erative alignment) with weights set to maxi-
mize Pearson?s correlation with adequacy and
fluency judgments. Weights were trained on
WMT-06 data.
The scores produced by these are given in the ta-
bles at the end of the paper, and described in Sec-
tion 5. We measured the correlation of the automatic
evaluation metrics with the different types of human
judgments on 12 data conditions, and report these in
Section 6.
5 Shared task results
The results of the human evaluation are given in Ta-
bles 9, 10, 11 and 12. Each of those tables present
four scores:
? FLUENCY and ADEQUACY are normalized ver-
sions of the five point scores described in Sec-
tion 3.1. The tables report an average of the
normalized scores.5
? RANK is the average number of times that a
system was judged to be better than any other
system in the sentence ranking evaluation de-
scribed in Section 3.2.
? CONSTITUENT is the average number of times
that a system was judged to be better than any
other system in the constituent-based evalua-
tion described in Section 3.3.
There was reasonably strong agreement between
these four measures at which of the entries was the
best in each data condition. There was complete
5Since different annotators can vary widely in how they as-
sign fluency and adequacy scores, we normalized these scores
on a per-judge basis using the method suggested by Blatz et al
(2003) in Chapter 5, page 97.
SYSTRAN (systran) 32%
University of Edinburgh (uedin) 20%
University of Catalonia (upc) 15%
LIMSI-CNRS (limsi) 13%
University of Maryland (umd) 5%
National Research Council of Canada?s
joint entry with SYSTRAN (systran-nrc)
5%
Commercial Czech-English system (pct) 5%
University of Valencia (upv) 2%
Charles University (cu) 2%
Table 3: The proportion of time that participants?
entries were top-ranked in the human evaluation
University of Edinburgh (uedin) 41%
University of Catalonia (upc) 12%
LIMSI-CNRS (limsi) 12%
University of Maryland (umd) 9%
Charles University (cu) 4%
Carnegie Mellon University (cmu-syntax) 4%
Carnegie Mellon University (cmu-uka) 4%
University of California at Berkeley (ucb) 3%
National Research Council?s joint entry
with SYSTRAN (systran-nrc)
2%
SYSTRAN (systran) 2%
Saarland University (saar) 0.8%
Table 4: The proportion of time that participants?
entries were top-ranked by the automatic evaluation
metrics
agreement between them in 5 of the 14 conditions,
and agreement between at least three of them in 10
of the 14 cases.
Table 3 gives a summary of how often differ-
ent participants? entries were ranked #1 by any of
the four human evaluation measures. SYSTRAN?s
entries were ranked the best most often, followed
by University of Edinburgh, University of Catalonia
and LIMSI-CNRS.
The following systems were the best perform-
ing for the different language pairs: SYSTRAN
was ranked the highest in German-English, Uni-
versity of Catalonia was ranked the highest in
Spanish-English, LIMSI-CNRS was ranked high-
est in French-English, and the University of Mary-
land and a commercial system were the highest for
144
Evaluation type P (A) P (E) K
Fluency (absolute) .400 .2 .250
Adequacy (absolute) .380 .2 .226
Fluency (relative) .520 .333 .281
Adequacy (relative) .538 .333 .307
Sentence ranking .582 .333 .373
Constituent ranking .693 .333 .540
Constituent ranking .712 .333 .566
(w/identical constituents)
Table 5: Kappa coefficient values representing the
inter-annotator agreement for the different types of
manual evaluation
Czech-English.
While we consider the human evaluation to be
primary, it is also interesting to see how the en-
tries were ranked by the various automatic evalua-
tion metrics. The complete set of results for the auto-
matic evaluation are presented in Tables 13, 14, 15,
and 16. An aggregate summary is provided in Table
4. The automatic evaluation metrics strongly favor
the University of Edinburgh, which garners 41% of
the top-ranked entries (which is partially due to the
fact it was entered in every language pair). Signif-
icantly, the automatic metrics disprefer SYSTRAN,
which was strongly favored in the human evaluation.
6 Meta-evaluation
In addition to evaluating the translation quality of
the shared task entries, we also performed a ?meta-
evaluation? of our evaluation methodologies.
6.1 Inter- and Intra-annotator agreement
We measured pairwise agreement among annotators
using the kappa coefficient (K) which is widely used
in computational linguistics for measuring agree-
ment in category judgments (Carletta, 1996). It is
defined as
K =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance. We define chance
agreement for fluency and adequacy as 15 , since they
are based on five point scales, and for ranking as 13
Evaluation type P (A) P (E) K
Fluency (absolute) .630 .2 .537
Adequacy (absolute) .574 .2 .468
Fluency (relative) .690 .333 .535
Adequacy (relative) .696 .333 .544
Sentence ranking .749 .333 .623
Constituent ranking .825 .333 .738
Constituent ranking .842 .333 .762
(w/identical constituents)
Table 6: Kappa coefficient values for intra-annotator
agreement for the different types of manual evalua-
tion
since there are three possible out comes when rank-
ing the output of a pair of systems: A > B, A = B,
A < B.
For inter-annotator agreement we calculated
P (A) for fluency and adequacy by examining all
items that were annotated by two or more annota-
tors, and calculating the proportion of time they as-
signed identical scores to the same items. For the
ranking tasks we calculated P (A) by examining all
pairs of systems which had been judged by two or
more judges, and calculated the proportion of time
that they agreed that A > B, A = B, or A < B.
For intra-annotator agreement we did similarly, but
gathered items that were annotated on multiple oc-
casions by a single annotator.
Table 5 gives K values for inter-annotator agree-
ment, and Table 6 gives K values for intra-annoator
agreement. These give an indication of how often
different judges agree, and how often single judges
are consistent for repeated judgments, respectively.
The interpretation of Kappa varies, but according to
Landis and Koch (1977) 0??.2 is slight, .21??.4
is fair, .41??.6 is moderate, .61??.8 is substantial
and the rest almost perfect.
The K values for fluency and adequacy should
give us pause about using these metrics in the fu-
ture. When we analyzed them as they are intended to
be?scores classifying the translations of sentences
into different types?the inter-annotator agreement
was barely considered fair, and the intra-annotator
agreement was only moderate. Even when we re-
assessed fluency and adequacy as relative ranks the
agreements increased only minimally.
145
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 0.08
 0  10  20  30  40  50  60
n
u
m
 
s
e
n
t
e
n
c
e
s
 
t
a
k
i
n
g
 
t
h
i
s
 
l
o
n
g
 
(
%
)
time to judge one sentence (seconds)
constituent ranksentence rankfluency+adequacy scoring
Figure 4: Distributions of the amount of time it took
to judge single sentences for the three types of man-
ual evaluation
The agreement on the other two types of man-
ual evaluation that we introduced were considerably
better. The both the sentence and constituent ranking
had moderate inter-annotator agreement and sub-
stantial intra-annotator agreement. Because the con-
stituent ranking examined the translations of short
phrases, often times all systems produced the same
translations. Since these trivially increased agree-
ment (since they would always be equally ranked)
we also evaluated the inter- and intra-annotator
agreement when those items were excluded. The
agreement remained very high for constituent-based
evaluation.
6.2 Timing
We used the web interface to collect timing infor-
mation. The server recorded the time when a set of
sentences was given to a judge and the time when
the judge returned the sentences. We divided the
time that it took to do a set by the number of sen-
tences in the set. The average amount of time that it
took to assign fluency and adequacy to a single sen-
tence was 26 seconds.6 The average amount of time
it took to rank a sentence in a set was 20 seconds.
The average amount of time it took to rank a high-
lighted constituent was 11 seconds. Figure 4 shows
the distribution of times for these tasks.
6Sets which took longer than 5 minutes were excluded from
these calculations, because there was a strong chance that anno-
tators were interrupted while completing the task.
These timing figures are promising because they
indicate that the tasks which the annotators were the
most reliable on (constituent ranking and sentence
ranking) were also much quicker to complete than
the ones that they were unreliable on (assigning flu-
ency and adequacy scores). This suggests that flu-
ency and adequacy should be replaced with ranking
tasks in future evaluation exercises.
6.3 Correlation between automatic metrics and
human judgments
To measure the correlation of the automatic metrics
with the human judgments of translation quality we
used Spearman?s rank correlation coefficient ?. We
opted for Spearman rather than Pearson because it
makes fewer assumptions about the data. Impor-
tantly, it can be applied to ordinal data (such as the
fluency and adequacy scales). Spearman?s rank cor-
relation coefficient is equivalent to Pearson correla-
tion on ranks.
After the raw scores that were assigned to systems
by an automatic metric and by one of our manual
evaluation techniques have been converted to ranks,
we can calculate ? using the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher value for ? is
making predictions that are more similar to the hu-
man judgments than an automatic evaluation metric
with a lower ?.
Table 17 reports ? for the metrics which were
used to evaluate translations into English.7. Table
7 summarizes the results by averaging the correla-
tion numbers by equally weighting each of the data
conditions. The table ranks the automatic evalua-
tion metrics based on how well they correlated with
human judgments. While these are based on a rela-
tively few number of items, and while we have not
performed any tests to determine whether the dif-
ferences in ? are statistically significant, the results
7The Czech-English conditions were excluded since there
were so few systems
146
are nevertheless interesting, since three metrics have
higher correlation than Bleu:
? Semantic role overlap (Gime?nez and Ma`rquez,
2007), which makes its debut in the proceed-
ings of this workshop
? ParaEval measuring recall (Zhou et al, 2006),
which has a model of allowable variation in
translation that uses automatically generated
paraphrases (Callison-Burch, 2007)
? Meteor (Banerjee and Lavie, 2005) which also
allows variation by introducing synonyms and
by flexibly matches words using stemming.
Tables 18 and 8 report ? for the six metrics which
were used to evaluate translations into the other lan-
guages. Here we find that Bleu and TER are the
closest to human judgments, but that overall the cor-
relations are much lower than for translations into
English.
7 Conclusions
Similar to last year?s workshop we carried out an ex-
tensive manual and automatic evaluation of machine
translation performance for translating from four
European languages into English, and vice versa.
This year we substantially increased the number of
automatic evaluation metrics and were also able to
nearly double the efforts of producing the human
judgments.
There were substantial differences in the results
results of the human and automatic evaluations. We
take the human judgments to be authoritative, and
used them to evaluate the automatic metrics. We
measured correlation using Spearman?s coefficient
and found that three less frequently used metrics
were stronger predictors of human judgments than
Bleu. They were: semantic role overlap (newly in-
troduced in this workshop) ParaEval-recall and Me-
teor.
Although we do not claim that our observations
are indisputably conclusive, they again indicate that
the choice of automatic metric can have a signifi-
cant impact on comparing systems. Understanding
the exact causes of those differences still remains an
important issue for future research.
metric A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
O
V
E
R
A
L
L
Semantic
role overlap
.774 .839 .803 .741 .789
ParaEval-
Recall
.712 .742 .768 .798 .755
Meteor .701 .719 .745 .669 .709
Bleu .690 .722 .672 .602 .671
1-TER .607 .538 .520 .514 .644
Max adequ-
correlation
.651 .657 .659 .534 .626
Max fluency
correlation
.644 .653 .656 .512 .616
GTM .655 .674 .616 .495 .610
Dependency
overlap
.639 .644 .601 .512 .599
ParaEval-
Precision
.639 .654 .610 .491 .598
1-WER of
verbs
.378 .422 .431 .297 .382
Table 7: Average corrections for the different auto-
matic metrics when they are used to evaluate trans-
lations into English
metric A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
O
V
E
R
A
L
L
Bleu .657 .445 .352 .409 .466
1-TER .589 .419 .361 .380 .437
Max fluency
correlation
.534 .419 .368 .400 .430
Max adequ-
correlation
.498 .414 .385 .409 .426
Meteor .490 .356 .279 .304 .357
1-WER of
verbs
.371 .304 .359 .359 .348
Table 8: Average corrections for the different auto-
matic metrics when they are used to evaluate trans-
lations into the other languages
147
This year?s evaluation also measured the agree-
ment between human assessors by computing the
Kappa coefficient. One striking observation is
that inter-annotator agreement for fluency and ad-
equacy can be called ?fair? at best. On the other
hand, comparing systems by ranking them manually
(constituents or entire sentences), resulted in much
higher inter-annotator agreement.
Acknowledgments
This work was supported in part by the EuroMa-
trix project funded by the European Commission
(6th Framework Programme), and in part by the
GALE program of the US Defense Advanced Re-
search Projects Agency, Contract No. HR0011-06-
C-0022.
We are grateful to Jesu?s Gime?nez, Dan Melamed,
Maja Popvic, Ding Liu, Liang Zhou, and Abhaya
Agarwal for scoring the entries with their automatic
evaluation metrics. Thanks to Brooke Cowan for
parsing the Spanish test sentences, to Josh Albrecht
for his script for normalizing fluency and adequacy
on a per judge basis, and to Dan Melamed, Rebecca
Hwa, Alon Lavie, Colin Bannard andMirella Lapata
for their advice about statistical tests.
References
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??s
Ma`rquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of COLING-ACL06.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of ACL.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, Michigan.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL-2005.
Dan Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings
of HLT.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. CLSP Summer Workshop Final
Report WS2003, Johns Hopkins University.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2006. CzEng:
Czech-English Parallel Corpus, Release version 0.5.
Prague Bulletin of Mathematical Linguistics, 86.
Ondr?ej Bojar. 2004. Problems of inducing large
coverage constraint-based dependency grammar for
Czech. In Constraint Solving and Language Process-
ing, CSLP 2004, volume LNAI 3438. Springer.
Ondr?ej Bojar. 2007. English-to-Czech factored machine
translation. In Proceedings of the ACL-2007 Work-
shop on Statistical Machine Translation (WMT-07),
Prague.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In Proceedings of EACL.
Chris Callison-Burch. 2007. Paraphrasing and Transla-
tion. Ph.D. thesis, University of Edinburgh, Scotland.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison. 2007.
Multi-engine machine translation with an open-source
decoder for statistical machine translation. In Pro-
ceedings of the ACL-2007 Workshop on Statistical Ma-
chine Translation (WMT-07), Prague.
Jorge Civera and Alfons Juan. 2007. Domain adaptation
in statistical machine translation with mixture mod-
elling. In Proceedings of the ACL-2007 Workshop on
Statistical Machine Translation (WMT-07), Prague.
Marta R. Costa-Jussa` and Jose? A.R. Fonollosa. 2007.
Analysis of statistical and morphological classes to
generate weighted reordering hypotheses on a statisti-
cal machine translation system. In Proceedings of the
ACL-2007 Workshop on Statistical Machine Transla-
tion (WMT-07), Prague.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality. In
Proceedings of MT Summit IX.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of Spanish. In
Proceedings of EMNLP 2005.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Human Language Technology: Notebook
Proceedings, pages 128?132, San Diego.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In Proceedings of ACL.
148
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Proceedings of the ACL-2007
Workshop on Statistical Machine Translation (WMT-
07), Prague.
Christopher J. Dyer. 2007. The ?noisier channel?: trans-
lation from morphologically complex languages. In
Proceedings of the ACL-2007 Workshop on Statistical
Machine Translation (WMT-07), Prague.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In Proceedings of
International Workshop on Spoken Language Transla-
tion.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguistic fea-
tures for automatic evaluation of heterogenous mt sys-
tems. In Proceedings of ACL Workshop on Statistical
Machine Translation.
Maria Holmqvist, Sara Stymne, and Lars Ahrenberg.
2007. Getting to know Moses: Initial experiments
on German-English factored translation. In Proceed-
ings of the ACL-2007 Workshop on Statistical Machine
Translation (WMT-07), Prague.
Douglas Jones, Wade Shen, Neil Granoien, Martha Her-
zog, and Clifford Weinstein. 2005. Measuring trans-
lation quality by testing english speakers with a new
defense language proficiency test for arabic. In Pro-
ceedings of the 2005 International Conference on In-
telligence Analysis.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between European lan-
guages. In Proceedings of ACL 2005 Workshop on
Parallel Text Translation.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the ACL-2007 Workshop on
Statistical Machine Translation (WMT-07), Prague.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, Chris
Callison-Burch, Alexandra Constantin, Brooke
Cowan, Chris Dyer, Marcello Federico, Evan Herbst,
Hieu Hoang, Christine Moran, Wade Shen, and
Richard Zens. 2006. Factored translation models.
CLSP Summer Workshop Final Report WS-2006,
Johns Hopkins University.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159?174.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels
of correlation with human judgments. In Proceedings
of the Workshop on Statistical Machine Translation,
Prague, June. Association for Computational Linguis-
tics.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Audrey Lee. 2006. NIST 2006 machine translation eval-
uation official results. Official release of automatic
evaluation scores for all submissions, November.
Ding Liu and Daniel Gildea. 2007. Source-language fea-
tures and maximum correlation training for machine
translation evaluation. In Proceedings of NAACL.
Dan Melamed, Ryan Green, and Jospeh P. Turian. 2003.
Precision and recall of machine translation. In Pro-
ceedings of HLT/NAACL.
Preslav Nakov and Marti Hearst. 2007. UCB system de-
scription for the WMT 2007 shared task. In Proceed-
ings of the ACL-2007 Workshop on Statistical Machine
Translation (WMT-07), Prague.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Michael Paul. 2006. Overview of the IWSLT 2006
evaluation campaign. In Proceedings of International
Workshop on Spoken Language Translation.
Matthias Paulik, Kay Rottmann, Jan Niehues, Silja
Hildebrand, and Stephan Vogel. 2007. The ISL
phrase-based MT system for the 2007 ACL Workshop
on Statistical Machine Translation. In Proceedings of
the ACL-2007 Workshop on Statistical Machine Trans-
lation (WMT-07), Prague.
149
Maja Popovic and Hermann Ney. 2007. Word error rates:
Decomposition over POS classes and applications for
error analysis. In Proceedings of ACL Workshop on
Statistical Machine Translation.
Mark Przybocki. 2004. NIST 2004 machine translation
evaluation results. Confidential e-mail to workshop
participants, May.
Holger Schwenk. 2007. Building a statistical machine
translation system for French using the Europarl cor-
pus. In Proceedings of the ACL-2007 Workshop on
Statistical Machine Translation (WMT-07), Prague.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and
Roland Kuhn. 2007. Rule-based translation with sta-
tistical phrase-based post-editing. In Proceedings of
the ACL-2007 Workshop on Statistical Machine Trans-
lation (WMT-07), Prague.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Statistical Machine
Translation in the Americas.
Nicola Ueffing, Michel Simard, Samuel Larkin, and
Howard Johnson. 2007. NRC?s PORTAGE system for
WMT 2007. In Proceedings of the ACL-2007 Work-
shop on Statistical Machine Translation (WMT-07),
Prague.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-
evaluating machine translation results with paraphrase
support. In Proceedings of EMNLP.
Andreas Zollmann, Ashish Venugopal, Matthias Paulik,
and Stephan Vogel. 2007. The syntax augmented MT
(SAMT) system for the shared task in the 2007 ACL
Workshop on Statistical Machine Translation. In Pro-
ceedings of the ACL-2007 Workshop on Statistical Ma-
chine Translation (WMT-07), Prague.
system A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
German-English Europarl
cmu-uka 0.511 0.496 0.395 0.206
liu 0.541 0.55 0.415 0.234
nrc 0.474 0.459 0.354 0.214
saar 0.334 0.404 0.119 0.104
systran 0.562 0.594 0.530 0.302
uedin 0.53 0.554 0.43 0.187
upc 0.534 0.533 0.384 0.214
German-English News Corpus
nrc 0.459 0.429 0.325 0.245
saar 0.278 0.341 0.108 0.125
systran 0.552 0.56 0.563 0.344
uedin 0.508 0.536 0.485 0.332
upc 0.536 0.512 0.476 0.330
English-German Europarl
cmu-uka 0.557 0.508 0.416 0.333
nrc 0.534 0.511 0.328 0.321
saar 0.369 0.383 0.172 0.196
systran 0.543 0.525 0.511 0.295
uedin 0.569 0.576 0.389 0.350
upc 0.565 0.522 0.438 0.3
English-German News Corpus
nrc 0.453 0.4 0.437 0.340
saar 0.186 0.273 0.108 0.121
systran 0.542 0.556 0.582 0.351
ucb 0.415 0.403 0.332 0.289
uedin 0.472 0.445 0.455 0.303
upc 0.505 0.475 0.377 0.349
Table 9: Human evaluation for German-English sub-
missions
150
system A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
Spanish-English Europarl
cmu-syntax 0.552 0.568 0.478 0.152
cmu-uka 0.557 0.564 0.392 0.139
nrc 0.477 0.489 0.382 0.143
saar 0.328 0.336 0.126 0.075
systran 0.525 0.566 0.453 0.156
uedin 0.593 0.610 0.419 0.14
upc 0.587 0.604 0.5 0.188
upv 0.562 0.573 0.326 0.154
Spanish-English News Corpus
cmu-uka 0.522 0.495 0.41 0.213
nrc 0.479 0.464 0.334 0.243
saar 0.446 0.46 0.246 0.198
systran 0.525 0.503 0.453 0.22
uedin 0.546 0.534 0.48 0.268
upc 0.566 0.543 0.537 0.312
upv 0.435 0.459 0.295 0.151
English-Spanish Europarl
cmu-uka 0.563 0.581 0.391 0.23
nrc 0.546 0.548 0.323 0.22
systran 0.495 0.482 0.329 0.224
uedin 0.586 0.638 0.468 0.225
upc 0.584 0.578 0.444 0.239
upv 0.573 0.587 0.406 0.246
English-Spanish News Corpus
cmu-uka 0.51 0.492 0.45 0.277
nrc 0.408 0.392 0.367 0.224
systran 0.501 0.507 0.481 0.352
ucb 0.449 0.414 0.390 0.307
uedin 0.429 0.419 0.389 0.266
upc 0.51 0.488 0.404 0.311
upv 0.405 0.418 0.250 0.217
Table 10: Human evaluation for Spanish-English
submissions
system A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
French-English Europarl
limsi 0.634 0.618 0.458 0.290
nrc 0.553 0.551 0.404 0.253
saar 0.384 0.447 0.176 0.157
systran 0.494 0.484 0.286 0.202
systran-nrc 0.604 0.6 0.503 0.267
uedin 0.616 0.635 0.514 0.283
upc 0.616 0.619 0.448 0.267
French-English News Corpus
limsi 0.575 0.596 0.494 0.312
nrc 0.472 0.442 0.306 0.241
saar 0.280 0.372 0.183 0.159
systran 0.553 0.534 0.469 0.288
systran-nrc 0.513 0.49 0.464 0.290
uedin 0.556 0.586 0.493 0.306
upc 0.576 0.587 0.493 0.291
English-French Europarl
limsi 0.635 0.627 0.505 0.259
nrc 0.517 0.518 0.359 0.206
saar 0.398 0.448 0.155 0.139
systran 0.574 0.526 0.353 0.179
systran-nrc 0.575 0.58 0.512 0.225
uedin 0.620 0.608 0.485 0.273
upc 0.599 0.566 0.45 0.256
English-French News Corpus
limsi 0.537 0.495 0.44 0.363
nrc 0.481 0.484 0.372 0.324
saar 0.243 0.276 0.086 0.121
systran 0.536 0.546 0.634 0.440
systran-nrc 0.557 0.572 0.485 0.287
ucb 0.401 0.391 0.316 0.245
uedin 0.466 0.447 0.485 0.375
upc 0.509 0.469 0.437 0.326
Table 11: Human evaluation for French-English
submissions
151
system A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
Czech-English News Corpus
cu 0.468 0.478 0.362 ?
pct 0.418 0.388 0.220 ?
uedin 0.458 0.471 0.353 ?
umd 0.550 0.592 0.627 ?
English-Czech News Corpus
cu 0.523 0.510 0.405 0.440
pct 0.542 0.541 0.499 0.381
uedin 0.449 0.433 0.249 0.258
Table 12: Human evaluation for Czech-English sub-
missions
152
system M
E
T
E
O
R
B
L
E
U
1-
T
E
R
G
T
M
PA
R
A
E
V
A
L
-R
E
C
A
L
L
PA
R
A
E
V
A
L
-P
R
E
C
IS
IO
N
D
E
P
E
N
D
E
N
C
Y
-O
V
E
R
L
A
P
S
E
M
A
N
T
IC
-R
O
L
E
-O
V
E
R
L
A
P
1-
W
E
R
-O
F
-V
E
R
B
S
M
A
X
-C
O
R
R
-F
L
U
E
N
C
Y
M
A
X
-C
O
R
R
-A
D
E
Q
U
A
C
Y
German-English Europarl
cmu-uka 0.559 0.247 0.326 0.455 0.528 0.531 0.259 0.182 0.848 1.91 1.910
liu 0.559 0.263 0.329 0.460 0.537 0.535 0.276 0.197 0.846 1.91 1.910
nrc 0.551 0.253 0.324 0.454 0.528 0.532 0.263 0.185 0.848 1.88 1.88
saar 0.477 0.198 0.313 0.447 0.44 0.527 0.228 0.157 0.846 1.76 1.710
systran 0.560 0.268 0.342 0.463 0.543 0.541 0.261 0.21 0.849 1.91 1.91
systran-2 0.501 0.154 0.238 0.376 0.462 0.448 0.237 0.154 ? 1.71 1.73
uedin 0.56 0.277 0.319 0.480 0.536 0.562 0.298 0.217 0.855 1.96 1.940
upc 0.541 0.250 0.343 0.470 0.506 0.551 0.27 0.193 0.846 1.89 1.88
German-English News Corpus
nrc 0.563 0.221 0.333 0.454 0.514 0.514 0.246 0.157 0.868 1.920 1.91
saar 0.454 0.159 0.288 0.413 0.405 0.467 0.193 0.120 0.86 1.700 1.64
systran 0.570 0.200 0.275 0.418 0.531 0.472 0.274 0.18 0.858 1.910 1.93
systran-2 0.556 0.169 0.238 0.397 0.511 0.446 0.258 0.163 ? 1.86 1.88
uedin 0.577 0.242 0.339 0.459 0.534 0.524 0.287 0.181 0.871 1.98 1.970
upc 0.575 0.233 0.339 0.455 0.527 0.516 0.265 0.171 0.865 1.96 1.96
English-German Europarl
cmu-uka 0.268 0.189 0.251 ? ? ? ? ? 0.884 1.66 1.63
nrc 0.272 0.185 0.221 ? ? ? ? ? 0.882 1.660 1.630
saar 0.239 0.174 0.237 ? ? ? ? ? 0.881 1.61 1.56
systran 0.198 0.123 0.178 ? ? ? ? ? 0.866 1.46 1.42
uedin 0.277 0.201 0.273 ? ? ? ? ? 0.889 1.690 1.66
upc 0.266 0.177 0.195 ? ? ? ? ? 0.88 1.640 1.62
English-German News Corpus
nrc 0.257 0.157 0.25 ? ? ? ? ? 0.891 1.590 1.560
saar 0.162 0.098 0.212 ? ? ? ? ? 0.881 1.400 1.310
systran 0.223 0.143 0.266 ? ? ? ? ? 0.887 1.55 1.500
ucb 0.256 0.156 0.249 ? ? ? ? ? 0.889 1.59 1.56
ucb-2 0.252 0.152 0.229 ? ? ? ? ? ? 1.57 1.55
uedin 0.266 0.166 0.266 ? ? ? ? ? 0.891 1.600 1.58
upc 0.256 0.167 0.266 ? ? ? ? ? 0.89 1.590 1.56
Table 13: Automatic evaluation scores for German-English submissions
153
system M
E
T
E
O
R
B
L
E
U
1-
T
E
R
G
T
M
PA
R
A
E
V
A
L
-R
E
C
PA
R
A
E
V
A
L
-P
R
E
C
D
E
P
E
N
D
E
N
C
Y
S
E
M
A
N
T
IC
-R
O
L
E
1-
W
E
R
-O
F
-V
E
R
B
S
M
A
X
-C
O
R
R
-F
L
U
M
A
X
-C
O
R
R
-A
D
E
Q
Spanish-English Europarl
cmu-syntax 0.602 0.323 0.414 0.499 0.59 0.588 0.338 0.254 0.866 2.10 2.090
cmu-syntax-2 0.603 0.321 0.408 0.494 0.593 0.584 0.336 0.249 ? 2.09 2.09
cmu-uka 0.597 0.32 0.42 0.501 0.581 0.595 0.336 0.247 0.867 2.09 2.080
nrc 0.596 0.313 0.402 0.484 0.581 0.581 0.321 0.227 0.867 2.04 2.04
saar 0.542 0.245 0.32 0.432 0.531 0.511 0.272 0.198 0.854 1.870 1.870
systran 0.593 0.290 0.364 0.469 0.586 0.550 0.321 0.238 0.858 2.02 2.03
systran-2 0.535 0.202 0.288 0.406 0.524 0.49 0.263 0.187 ? 1.81 1.84
uedin 0.6 0.324 0.414 0.499 0.584 0.589 0.339 0.252 0.868 2.09 2.080
upc 0.600 0.322 0.407 0.492 0.593 0.583 0.334 0.253 0.865 2.08 2.08
upv 0.594 0.315 0.400 0.493 0.582 0.581 0.329 0.249 0.865 2.060 2.06
Spanish-English News Corpus
cmu-uka 0.64 0.299 0.428 0.497 0.617 0.575 0.339 0.246 0.89 2.17 2.17
cmu-uka-2 0.64 0.297 0.427 0.496 0.616 0.574 0.339 0.246 ? 2.17 2.17
nrc 0.641 0.299 0.434 0.499 0.615 0.584 0.329 0.238 0.892 2.160 2.160
saar 0.607 0.244 0.338 0.447 0.587 0.512 0.303 0.208 0.879 2.04 2.05
systran 0.628 0.259 0.35 0.453 0.611 0.523 0.325 0.221 0.877 2.08 2.10
systran-2 0.61 0.233 0.321 0.438 0.602 0.506 0.311 0.209 ? 2.020 2.050
uedin 0.661 0.327 0.457 0.512 0.634 0.595 0.363 0.264 0.893 2.25 2.24
upc 0.654 0.346 0.480 0.528 0.629 0.616 0.363 0.265 0.895 2.240 2.23
upv 0.638 0.283 0.403 0.485 0.614 0.562 0.334 0.234 0.887 2.15 2.140
English-Spanish Europarl
cmu-uka 0.333 0.311 0.389 ? ? ? ? ? 0.889 1.98 2.00
nrc 0.322 0.299 0.376 ? ? ? ? ? 0.886 1.92 1.940
systran 0.269 0.212 0.301 ? ? ? ? ? 0.878 1.730 1.760
uedin 0.33 0.316 0.399 ? ? ? ? ? 0.891 1.980 1.990
upc 0.327 0.312 0.393 ? ? ? ? ? 0.89 1.960 1.98
upv 0.323 0.304 0.379 ? ? ? ? ? 0.887 1.95 1.97
English-Spanish News Corpus
cmu-uka 0.368 0.327 0.469 ? ? ? ? ? 0.903 2.070 2.090
cmu-uka-2 0.355 0.306 0.461 ? ? ? ? ? ? 2.04 2.060
nrc 0.362 0.311 0.448 ? ? ? ? ? 0.904 2.04 2.060
systran 0.335 0.281 0.439 ? ? ? ? ? 0.906 1.970 2.010
ucb 0.374 0.331 0.464 ? ? ? ? ? ? 2.09 2.11
ucb-2 0.375 0.325 0.456 ? ? ? ? ? ? 2.09 2.110
ucb-3 0.372 0.324 0.457 ? ? ? ? ? ? 2.08 2.10
uedin 0.361 0.322 0.479 ? ? ? ? ? 0.907 2.08 2.09
upc 0.361 0.328 0.467 ? ? ? ? ? 0.902 2.06 2.08
upv 0.337 0.285 0.432 ? ? ? ? ? 0.900 1.98 2.000
Table 14: Automatic evaluation scores for Spanish-English submissions
154
system M
E
T
E
O
R
B
L
E
U
1-
T
E
R
G
T
M
PA
R
A
E
V
A
L
-R
E
C
PA
R
A
E
V
A
L
-P
R
E
C
D
E
P
E
N
D
E
N
C
Y
S
E
M
A
N
T
IC
-R
O
L
E
1-
W
E
R
-O
F
-V
E
R
B
S
M
A
X
-C
O
R
R
-F
L
U
M
A
X
-C
O
R
R
-A
D
E
Q
French-English Europarl
limsi 0.604 0.332 0.418 0.504 0.589 0.591 0.344 0.259 0.865 2.100 2.10
limsi-2 0.602 0.33 0.417 0.504 0.587 0.592 0.302 0.257 ? 2.05 2.05
nrc 0.594 0.312 0.403 0.488 0.578 0.58 0.324 0.244 0.861 2.05 2.050
saar 0.534 0.249 0.354 0.459 0.512 0.546 0.279 0.202 0.856 1.880 1.88
systran 0.549 0.211 0.308 0.417 0.525 0.501 0.277 0.201 0.849 1.850 1.890
systran-nrc 0.594 0.313 0.404 0.492 0.578 0.580 0.330 0.248 0.862 2.06 2.060
uedin 0.595 0.318 0.424 0.505 0.574 0.599 0.338 0.254 0.865 2.08 2.08
upc 0.6 0.319 0.409 0.495 0.588 0.583 0.337 0.255 0.861 2.08 2.080
French-English News Corpus
limsi 0.595 0.279 0.405 0.478 0.563 0.555 0.289 0.235 0.875 2.030 2.020
nrc 0.587 0.257 0.389 0.470 0.557 0.546 0.301 0.22 0.876 2.020 2.020
saar 0.503 0.206 0.301 0.418 0.475 0.476 0.245 0.169 0.864 1.80 1.78
systran 0.568 0.202 0.28 0.415 0.554 0.472 0.292 0.198 0.866 1.930 1.96
systran-nrc 0.591 0.269 0.398 0.475 0.558 0.547 0.323 0.226 0.875 2.050 2.06
uedin 0.602 0.27 0.392 0.471 0.569 0.545 0.326 0.233 0.875 2.07 2.07
upc 0.596 0.275 0.400 0.476 0.567 0.552 0.322 0.233 0.876 2.06 2.06
English-French Europarl
limsi 0.226 0.306 0.366 ? ? ? ? ? 0.891 1.940 1.96
nrc 0.218 0.294 0.354 ? ? ? ? ? 0.888 1.930 1.96
saar 0.190 0.262 0.333 ? ? ? ? ? 0.892 1.86 1.87
systran 0.179 0.233 0.313 ? ? ? ? ? 0.885 1.79 1.83
systran-nrc 0.220 0.301 0.365 ? ? ? ? ? 0.892 1.940 1.960
uedin 0.207 0.262 0.301 ? ? ? ? ? 0.886 1.930 1.950
upc 0.22 0.299 0.379 ? ? ? ? ? 0.892 1.940 1.960
English-French News Corpus
limsi 0.206 0.255 0.354 ? ? ? ? ? 0.897 1.84 1.87
nrc 0.208 0.257 0.369 ? ? ? ? ? 0.9 1.87 1.900
saar 0.151 0.188 0.308 ? ? ? ? ? 0.896 1.65 1.65
systran 0.199 0.243 0.378 ? ? ? ? ? 0.901 1.860 1.90
systran-nrc 0.23 0.290 0.408 ? ? ? ? ? 0.903 1.940 1.98
ucb 0.201 0.237 0.366 ? ? ? ? ? 0.897 1.830 1.860
uedin 0.197 0.234 0.340 ? ? ? ? ? 0.899 1.87 1.890
upc 0.212 0.263 0.391 ? ? ? ? ? 0.900 1.87 1.90
Table 15: Automatic evaluation scores for French-English submissions
155
system M
E
T
E
O
R
B
L
E
U
1-
T
E
R
G
T
M
PA
R
A
E
V
A
L
-R
E
C
PA
R
A
E
V
A
L
-P
R
E
C
D
E
P
E
N
D
E
N
C
Y
S
E
M
A
N
T
IC
-R
O
L
E
1-
W
E
R
-O
F
-V
E
R
B
S
M
A
X
-C
O
R
R
-F
L
U
M
A
X
-C
O
R
R
-A
D
E
Q
Czech-English News Corpus
cu 0.545 0.215 0.334 0.441 0.502 0.504 0.245 0.165 0.867 1.87 1.88
cu-2 0.558 0.223 0.344 0.447 0.510 0.514 0.254 0.17 ? 1.90 1.910
uedin 0.54 0.217 0.340 0.445 0.497 0.51 0.243 0.160 0.865 1.860 1.870
umd 0.581 0.241 0.355 0.460 0.531 0.526 0.273 0.184 0.868 1.96 1.97
English-Czech News Corpus
cu 0.429 0.134 0.231 ? ? ? ? ? ? 1.580 1.53
cu-2 0.430 0.132 0.219 ? ? ? ? ? ? 1.58 1.520
uedin 0.42 0.119 0.211 ? ? ? ? ? ? 1.550 1.49
Table 16: Automatic evaluation scores for Czech-English submissions
156
ADEQUACY
FLUENCY
RANK
CONSTITUENT
METEOR
BLEU
1-TER
GTM
PARAEVAL-REC
PARAEVAL-PREC
DEPENDENCY
SEMANTIC-ROLE
1-WER-OF-VS
MAX-CORR-FLU
MAX-CORR-ADEQ
G
erm
an-E
nglish
N
ew
s
C
orpus
adequacy
1
0.900
0.900
0.900
0.600
0.300
-0.025
0.300
0.700
0.300
0.700
0.700
-0.300
0.300
0.600
fl
uency
?
1
1.000
1.000
0.700
0.400
-0.025
0.400
0.900
0.400
0.900
0.900
-0.100
0.400
0.700
rank
?
?
1
1.000
0.700
0.400
-0.025
0.400
0.900
0.400
0.900
0.900
-0.100
0.400
0.700
constituent
?
?
?
1
0.700
0.400
-0.025
0.400
0.900
0.400
0.900
0.900
-0.100
0.400
0.700
G
erm
an-E
nglish
E
uroparl
adequacy
1
0.893
0.821
0.750
0.599
0.643
0.787
0.68
0.750
0.643
0.464
0.750
0.206
0.608
0.447
fl
uency
?
1
0.964
0.537
0.778
0.858
0.500
0.821
0.821
0.787
0.571
0.93
0.562
0.821
0.661
rank
?
?
1
0.500
0.902
0.821
0.393
0.714
0.858
0.643
0.464
0.858
0.652
0.893
0.769
constituent
?
?
?
1
0.456
0.464
0.714
0.18
0.750
0.250
0.214
0.43
0.117
0.214
0.126
Spanish-E
nglish
N
ew
s
C
orpus
adequacy
1
1.000
0.964
0.893
0.643
0.68
0.68
0.68
0.68
0.68
0.634
0.714
0.571
0.68
0.68
fl
uency
?
1
0.964
0.893
0.643
0.68
0.68
0.68
0.68
0.68
0.634
0.714
0.571
0.68
0.68
rank
?
?
1
0.858
0.714
0.750
0.750
0.750
0.750
0.750
0.741
0.787
0.608
0.750
0.750
constituent
?
?
?
1
0.787
0.821
0.821
0.821
0.714
0.821
0.599
0.750
0.750
0.714
0.714
Spanish-E
nglish
E
uroparl
adequacy
1
0.93
0.452
0.333
0.596
0.810
0.62
0.690
0.542
0.714
0.762
0.739
0.489
0.638
0.638
fl
uency
?
1
0.571
0.524
0.596
0.787
0.43
0.500
0.732
0.524
0.690
0.810
0.346
0.566
0.566
rank
?
?
1
0.643
0.739
0.596
0.43
0.262
0.923
0.406
0.500
0.739
0.168
0.542
0.542
constituent
?
?
?
1
0.262
0.143
-0.143
-0.143
0.816
-0.094
0.000
0.477
-0.226
0.042
0.042
F
rench-E
nglish
N
ew
s
C
orpus
adequacy
1
0.964
0.964
0.858
0.787
0.750
0.68
0.68
0.787
0.571
0.321
0.787
0.456
0.68
0.554
fl
uency
?
1
1.000
0.93
0.750
0.787
0.714
0.714
0.750
0.608
0.214
0.858
0.367
0.608
0.482
rank
?
?
1
0.93
0.750
0.787
0.714
0.714
0.750
0.608
0.214
0.858
0.367
0.608
0.482
constituent
?
?
?
1
0.858
0.858
0.787
0.787
0.858
0.643
0.393
0.964
0.349
0.750
0.661
F
rench-E
nglish
E
uroparl
adequacy
1
0.884
0.778
0.991
0.982
0.956
0.902
0.902
0.812
0.902
0.956
0.956
0.849
0.964
0.991
fl
uency
?
1
0.858
0.893
0.849
0.821
0.93
0.93
0.571
0.93
0.858
0.821
0.787
0.849
0.858
rank
?
?
1
0.821
0.670
0.68
0.858
0.858
0.43
0.858
0.787
0.68
0.893
0.741
0.714
constituent
?
?
?
1
0.956
0.93
0.93
0.93
0.750
0.93
0.964
0.93
0.893
0.956
0.964
Table
17:
C
orrelation
of
the
autom
atic
evaluation
m
etrics
w
ith
the
hum
an
judgm
ents
w
hen
translating
into
E
nglish
157
A
D
E
Q
U
A
C
Y
F
L
U
E
N
C
Y
R
A
N
K
C
O
N
S
T
IT
U
E
N
T
M
E
T
E
O
R
B
L
E
U
1-
T
E
R
1-
W
E
R
-O
F
-V
S
M
A
X
-C
O
R
R
-F
L
U
M
A
X
-C
O
R
R
-A
D
E
Q
English-German News Corpus
adequacy 1 0.943 0.83 0.943 0.187 0.43 0.814 0.243 0.33 0.187
fluency ? 1 0.714 0.83 0.100 0.371 0.758 0.100 0.243 0.100
rank ? ? 1 0.771 0.414 0.258 0.671 0.414 0.414 0.414
constituent ? ? ? 1 0.13 0.371 0.671 0.243 0.243 0.13
English-German Europarl
adequacy 1 0.714 0.487 0.714 0.487 0.600 0.314 0.371 0.487 0.487
fluency ? 1 0.543 0.43 0.258 0.200 -0.085 0.03 0.258 0.258
rank ? ? 1 0.03 -0.37 -0.256 -0.543 -0.485 -0.37 -0.37
constituent ? ? ? 1 0.887 0.943 0.658 0.83 0.887 0.887
English-Spanish News Corpus
adequacy 1 0.714 0.771 0.83 0.314 0.658 0.487 0.03 0.314 0.600
fluency ? 1 0.943 0.887 -0.200 0.03 0.143 0.200 -0.085 0.258
rank ? ? 1 0.943 -0.029 0.087 0.258 0.371 -0.029 0.371
constituent ? ? ? 1 -0.143 0.143 0.200 0.314 -0.085 0.258
English-Spanish Europarl
adequacy 1 0.83 0.943 0.543 0.658 0.943 0.943 0.943 0.83 0.658
fluency ? 1 0.771 0.543 0.714 0.771 0.771 0.771 0.83 0.714
rank ? ? 1 0.600 0.600 0.887 0.887 0.887 0.771 0.600
constituent ? ? ? 1 0.43 0.43 0.43 0.43 0.371 0.43
English-French News Corpus
adequacy 1 0.952 0.762 0.452 0.690 0.787 0.690 0.709 0.596 0.686
fluency ? 1 0.810 0.477 0.62 0.739 0.714 0.792 0.62 0.780
rank ? ? 1 0.762 0.239 0.381 0.500 0.757 0.596 0.601
constituent ? ? ? 1 -0.048 0.096 0.143 0.411 0.333 0.304
English-French Europarl
adequacy 1 0.964 0.750 0.93 0.608 0.528 0.287 -0.07 0.652 0.376
fluency ? 1 0.858 0.893 0.643 0.562 0.214 -0.07 0.652 0.376
rank ? ? 1 0.750 0.821 0.76 0.393 0.214 0.830 0.697
constituent ? ? ? 1 0.571 0.473 0.18 -0.07 0.652 0.447
Table 18: Correlation of the automatic evaluation metrics with the human judgments when translating out
of English
158
Proceedings of the Second Workshop on Statistical Machine Translation, pages 224?227,
Prague, June 2007. c?2007 Association for Computational Linguistics
Experiments in Domain Adaptation for Statistical Machine Translation
Philipp Koehn and Josh Schroeder
pkoehn@inf.ed.ac.uk, j.schroeder@ed.ac.uk
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW
Scotland, United Kingdom
Abstract
The special challenge of the WMT 2007
shared task was domain adaptation. We
took this opportunity to experiment with
various ways of adapting a statistical ma-
chine translation systems to a special do-
main (here: news commentary), when
most of the training data is from a dif-
ferent domain (here: European Parliament
speeches). This paper also gives a descrip-
tion of the submission of the University of
Edinburgh to the shared task.
1 Our framework: the Moses MT system
The open source Moses (Koehn et al, 2007) MT
system was originally developed at the University
of Edinburgh and received a major boost through a
2007 Johns Hopkins workshop. It is now used at
several academic institutions as the basic infrastruc-
ture for statistical machine translation research.
The Moses system is an implementation of the
phrase-based machine translation approach (Koehn
et al, 2003). In this approach, an input sentence is
first split into text chunks (so-called phrases), which
are then mapped one-to-one to target phrases using
a large phrase translation table. Phrases may be re-
ordered, but typically a reordering limit (in our ex-
periments a maximum movement over 6 words) is
used. See Figure 1 for an illustration.
Phrase translation probabilities, reordering prob-
abilities and language model probabilities are com-
bined to give each possible sentence translation a
score. The best-scoring translation is searched for by
the decoding algorithm and outputted by the system
as the best translation. The different system compo-
nents hi (phrase translation probabilities, language
Figure 1: Phrase-based statistical machine transla-
tion model: Input is split into text chunks (phrases)
which are mapped using a large phrase translation
table. Phrases are mapped one-to-one, and may be
reordered.
model, etc.) are combined in a log-linear model to
obtain the score for the translation e for an input sen-
tence f:
score(e, f) = exp
?
i
?i hi(e, f) (1)
The weights of the components ?i are set by a
discriminative training method on held-out develop-
ment data (Och, 2003). The basic components used
in our experiments are: (a) two phrase translation
probabilities (both p(e|f) and p(f |e)), (b) two word
translation probabilities (both p(e|f) and p(f |e)),
(c) phrase count, (d) output word count, (e) language
model, (f) distance-based reordering model, and (g)
lexicalized reordering model.
For a more detailed description of this model,
please refer to (Koehn et al, 2005).
2 Domain adaption
Since training data for statistical machine translation
is typically collected opportunistically from wher-
ever it is available, the application domain for a ma-
chine translation system may be very different from
the domain of the system?s training data.
For the WMT 2007 shared task, the challenge was
to use a large amount of out-of-domain training data
224
(about 40 million words) combined with a much
smaller amount of in-domain training data (about 1
million words) to optimize translation performance
on that particular domain. We carried out these ex-
periments on French?English.
2.1 Only out-of-domain training data
The first baseline system is trained only on the out-
of-domain Europarl corpus, which has the following
corpus statistics:
French English
Sentences 1,257,419
Words 37,489,556 33,787,890
2.2 Only in-domain training data
The second baseline system is trained only on the
in-domain NewsCommentary corpus. This corpus
is much smaller:
French English
Sentences 42,884
Words 1,198,041 1,018,503
2.3 Combined training data
To make use of all the training data, the straight-
forward way is to simply concatenate the two train-
ing corpora and use the combined data for both
translation model and language model training. In
our situation, however, the out-of-domain training
data overwhelms the in-domain training data due to
the sheer relative size. Hence, we do not expect the
best performance from this simplistic approach.
2.4 In-domain language model
One way to force a drift to the jargon of the target
domain is the use of the language model. In our next
setup, we used only in-domain data for training the
language model. This enables the system to use all
the translation knowledge from the combined cor-
pus, but it gives a preference to word choices that
are dominant in the in-domain training data.
2.5 Interpolated language model
Essentially, the goal of our subsequent approaches is
to make use of all the training data, but to include a
preference for the in-domain jargon by giving more
weight to the in-domain training data. This and the
next approach explore methods to bias the language
model, while the final approach biases the transla-
tion model.
0.60.2 0.3 0.4 0.5 0.7 0.8
157
158
159
160
161
162
163
164
weight
perplexity
Figure 2: Interpolating in-domain and out-of-
domain language models: effect of interpolation
weight on perplexity of LM on development set.
We trained two language models, one for each the
out-of-domain and the in-domain training data. Lan-
guage modeling software such as the SRILM toolkit
we used (Stolke, 2002) allows the interpolation of
these language models. When interpolating, we give
the out-of-domain language model a weight in re-
spect to the in-domain language model.
Since we want to obtain a language model that
gives us the best performance on the target domain,
we set this weight so that the perplexity of the de-
velopment set from that target domain is optimized.
We searched for the optimal weight setting by sim-
ply testing a set of weights and focusing on the most
promising range of weights.
Figure 2 displays all the weights we explored dur-
ing this process and the corresponding perplexity of
the resulting language model on the development set
(nc-dev2007). The optimal weight can be picked out
easily from this very smooth curve.
2.6 Two language models
The log-linear modeling approach of statistical ma-
chine translation enables a straight-forward combi-
nation of the in-domain and out-of-domain language
models. We included them as two separate fea-
tures, whose weights are set with minimum error
rate training. The relative weight for each model is
set directly by optimizing translation performance.
2.7 Two translation models
Finally, besides biasing the language model to a spe-
cific target domain, we may also bias the translation
model. Here, we take advantage of a feature of the
Moses decoder?s factored translation model frame-
work. In factored translation models, the representa-
225
Method %BLEU
Large out-of-domain training data 25.11
Small in-domain training data 25.88
Combined training data 26.69
In-domain language model 27.46
Interpolated language model 27.12
Two language models 27.30
Two translation models 27.64
Table 1: Results of domain adaptation experiments
tion of words is extended to a vector of factors (e.g.,
surface form, lemma, POS, morphology).
The mapping of an input phrase to an output
phrase is decomposed into several translation and
generation steps, each using a different translation
or generation table, respectively. Such a decomposi-
tion is called a decoding path.
A more recent feature of the factored translation
model framework is the possible use of multiple al-
ternative decoding paths. This alternate decoding
path model was developed by Birch et al (2007).
For our purposes, we use two decoding paths, each
consisting of only one translation step. One decod-
ing path is the in-domain translation table, and the
other decoding path is the out-of-domain translation
table. Again, respective weights are set with mini-
mum error rate training.
3 Domain adaptation results
Table 1 shows results of our domain adaptation ex-
periments on the development test set (nc-devtest-
2007). The results suggest that the language model
is a useful tool for domain adaptation. While train-
ing on all the data is essential for good performance,
using an in-domain language model alone already
gives fairly high performance (27.46). The perfor-
mance with the interpolated language model (27.12)
and two language models (27.30) are similar. All
perform better than the three baseline approaches.
The results also suggest that higher performance
can be obtained by using two translation models
through the Moses decoder?s alternative decoding
path framework. We saw our best results under this
condition (27.64).
4 WMT 2007 shared task submissions
We participated in all categories. Given the four lan-
guage pairs, with two translation directions and (ex-
cept for Czech) two test domains, this required us to
build 14 translation systems.
We had access to a fairly large computer cluster to
carry out our experiments over the course of a few
weeks. However, speed issues with the decoder and
load issues on the crowded cluster caused us to take
a few shortcuts. Also, a bug crept in to our English?
French experiments where we used the wrong deto-
kenizer, resulting drop of 2?3 points in %BLEU.
4.1 Tuning
Minimum error rate training is the most time-
consuming aspects of the training process. Due to
time constraints, we did not carry out this step for all
but the Czech systems (a new language for us). For
the other systems, we re-used weight settings from
our last year?s submission.
One of the most crucial outcomes of tuning is a
proper weight setting for output length, which is es-
pecially important for the BLEU score. Since the
training corpus and tokenization changed, our re-
used weights are not always optimal in this respect.
But only in one case we felt compelled to manually
adjust the weight for the word count feature, since
the original setup led to a output/reference length ra-
tio of 0.88 on the development test set.
4.2 Domain adaptation
For the Europarl test sets, we did not use any do-
main adaptation techniques, but simply used either
just the Europarl training data or the combined data
? whatever gave the higher score on the develop-
ment test set, although scores differed by only about
0.1?0.2 %BLEU.
In order to be able to re-use the old weights, we
were limited to domain adaptation methods that did
not change the number of components. We decided
to use the interpolated language model method de-
scribed in Section 2.5. For the different language
pairs, optimal interpolation weights differed:
Language pair Weight for Europarl LM
French?English 0.43
Spanish?English 0.41
German?English 0.40
English?French 0.51
English?Spanish 0.42
English?German 0.45
226
Language pair Europarl NewsCommentary
%BLEU Length NIST %BLEU Length NIST
French?English 32.66 0.96 7.94 28.27 1.03 7.50
Spanish?English 33.26 1.00 7.82 34.17 1.06 8.35
German?English 28.49 0.94 7.32 25.45 1.01 7.19
Czech?English ? ? ? 22.68 0.98 6.96
English?French 26.76 1.08 6.66 24.38 1.02 6.73
English?Spanish 32.55 0.98 7.66 33.59 0.94 8.46
English?German 20.59 0.97 6.18 17.06 1.00 6.04
English?Czech ? ? ? 12.34 1.02 4.85
Table 2: Test set performance of our systems: BLEU and NIST scores, and output/reference length ratio.
4.3 Training and decoding parameters
We tried to improve performance by increasing
some of the limits imposed on the training and de-
coding setup. During training, long sentences are
removed from the training data to speed up the
GIZA++ word alignment process. Traditionally, we
worked with a sentence length limit of 40. We found
that increasing this limit to about 80 gave better re-
sults without causing undue problems with running
the word alignment (GIZA++ increasingly fails and
runs much slower with long sentences).
We also tried to increase beam sizes and the
limit on the number of translation options per cov-
erage span (ttable-limit). This has shown to be suc-
cessful in our experiments with Arabic?English and
Chinese?English systems. Surprisingly, increasing
the maximum stack size to 1000 (from 200) and
ttable-limit to 100 (from 20) has barely any ef-
fect on translation performance. The %BLEU score
changed only by less than 0.05, and often worsened.
4.4 German?English system
The German?English language pair is especially
challenging due to the large differences in word or-
der. Collins et al (2005) suggest a method to reorder
the German input before translating using a set of
manually crafted rules. In our German?English sub-
missions, this is done both to the training data and
the input to the machine translation system.
5 Conclusions
Our submission to the WMT 2007 shared task is a
fairly straight-forward use of the Moses MT system
using default parameters. In a sense, we submitted
a baseline performance of this system. BLEU and
NIST scores for all our systems on the test sets are
displayed in Table 2. Compared to other submitted
systems, these are very good scores, often the best
or second highest scores for these tasks.
We made a special effort in two areas: We ex-
plored domain adaptation methods for the News-
Commentary test sets and we used reordering rules
for the German?English language pair.
Acknowledgments
This work was supported in part under the GALE program
of the Defense Advanced Research Projects Agency, Contract
No. HR0011-06-C-0022 and in part under the EuroMatrix
project funded by the European Commission (6th Framework
Programme).
References
Birch, A., Osborne, M., and Koehn, P. (2007). CCG supertags
in factored statistical machine translation. In Proceedings
of the Workshop on Statistical Machine Translation, Prague.
Association for Computational Linguistics.
Collins, M., Koehn, P., and Kucerova, I. (2005). Clause re-
structuring for statistical machine translation. In Proceed-
ings of the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 531?540, Ann Arbor,
Michigan. Association for Computational Linguistics.
Koehn, P., Axelrod, A., Mayne, A. B., Callison-Burch, C., Os-
borne, M., and Talbot, D. (2005). Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evaluation. In
Proc. of the International Workshop on Spoken Language
Translation.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico,
M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R.,
Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open source toolkit for statistical machine transla-
tion. In Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics, demonstation session.
Koehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase
based translation. In Proceedings of the Joint Conference on
Human Language Technologies and the Annual Meeting of
the North American Chapter of the Association of Computa-
tional Linguistics (HLT-NAACL).
Och, F. J. (2003). Minimum error rate training in statistical
machine translation. In Hinrichs, E. and Roth, D., editors,
Proceedings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167.
Stolke, A. (2002). SRILM - an extensible language modeling
toolkit. In Proceedings of the International Conference on
Spoken Language Processing.
227
Proceedings of the Third Workshop on Statistical Machine Translation, pages 70?106,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Further Meta-Evaluation of Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb cs jhu edu
Cameron Fordyce
camfordyce gmail com
Philipp Koehn
University of Edinburgh
pkoehn inf ed ac uk
Christof Monz
Queen Mary, University of London
christof dcs qmul ac uk
Josh Schroeder
University of Edinburgh
j schroeder ed ac uk
Abstract
This paper analyzes the translation qual-
ity of machine translation systems for 10
language pairs translating between Czech,
English, French, German, Hungarian, and
Spanish. We report the translation quality
of over 30 diverse translation systems based
on a large-scale manual evaluation involv-
ing hundreds of hours of effort. We use the
human judgments of the systems to analyze
automatic evaluation metrics for translation
quality, and we report the strength of the cor-
relation with human judgments at both the
system-level and at the sentence-level. We
validate our manual evaluation methodol-
ogy by measuring intra- and inter-annotator
agreement, and collecting timing informa-
tion.
1 Introduction
This paper presents the results the shared tasks of the
2008 ACL Workshop on Statistical Machine Trans-
lation, which builds on two past workshops (Koehn
andMonz, 2006; Callison-Burch et al, 2007). There
were two shared tasks this year: a translation task
which evaluated translation between 10 pairs of Eu-
ropean languages, and an evaluation task which ex-
amines automatic evaluation metrics.
There were a number of differences between this
year?s workshop and last year?s workshop:
? Test set selection ? Instead of creating our test
set by reserving a portion of the training data,
we instead hired translators to translate a set of
newspaper articles from a number of different
sources. This out-of-domain test set contrasts
with the in-domain Europarl test set.
? New language pairs ? We evaluated the qual-
ity of Hungarian-English machine translation.
Hungarian is a challenging language because it
is agglutinative, has many cases and verb con-
jugations, and has freer word order. German-
Spanish was our first language pair that did not
include English, but was not manually evalu-
ated since it attracted minimal participation.
? System combination ? Saarland University
entered a system combination over a number
of rule-based MT systems, and provided their
output, which were also treated as fully fledged
entries in the manual evaluation. Three addi-
tional groups were invited to apply their system
combination algorithms to all systems.
? Refined manual evaluation ? Because last
year?s study indicated that fluency and ade-
quacy judgments were slow and unreliable, we
dropped them from manual evaluation. We re-
placed them with yes/no judgments about the
acceptability of translations of shorter phrases.
? Sentence-level correlation ? In addition to
measuring the correlation of automatic evalu-
ation metrics with human judgments at the sys-
tem level, we also measured how consistent
they were with the human rankings of individ-
ual sentences.
The remainder of this paper is organized as fol-
lows: Section 2 gives an overview of the shared
70
translation task, describing the test sets, the mate-
rials that were provided to participants, and a list of
the groups who participated. Section 3 describes the
manual evaluation of the translations, including in-
formation about the different types of judgments that
were solicited and how much data was collected.
Section 4 presents the results of the manual eval-
uation. Section 5 gives an overview of the shared
evaluation task, describes which automatic metrics
were submitted, and tells how they were evaluated.
Section 6 presents the results of the evaluation task.
Section 7 validates the manual evaluation methodol-
ogy.
2 Overview of the shared translation task
The shared translation task consisted of 10 language
pairs: English to German, German to English, En-
glish to Spanish, Spanish to English, English to
French, French to English, English to Czech, Czech
to English, Hungarian to English, and German to
Spanish. Each language pair had two test sets drawn
from the proceedings of the European parliament, or
from newspaper articles.1
2.1 Test data
The test data for this year?s task differed from previ-
ous years? data. Instead of only reserving a portion
of the training data as the test set, we hired people
to translate news articles that were drawn from a va-
riety of sources during November and December of
2007. We refer to this as the News test set. A total
of 90 articles were selected, 15 each from a variety
of Czech-, English-, French-, German-, Hungarian-
and Spanish-language news sites:2
Hungarian: Napi (3 documents), Index (2),
Origo (5), Ne?pszabadsa?g (2), HVG (2),
Uniospez (1)
Czech: Aktua?lne? (1), iHNed (4), Lidovky (7),
Novinky (3)
French: Liberation (4), Le Figaro (4), Dernieres
Nouvelles (2), Les Echos (3), Canoe (2)
1For Czech news editorials replaced the European parlia-
ment transcripts as the second test set, and for Hungarian the
newspaper articles was the only test set.
2For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
Original source language avg. BLEU
Hungarian 8.8
German 11.0
Czech 15.2
Spanish 17.3
English 17.7
French 18.6
Table 1: Difficulty of the test set parts based on the
original language. For each part, we average BLEU
scores from the Edinburgh systems for 12 language
pairs of the shared task.
Spanish: Cinco Dias (7), ABC.es (3), El Mundo (5)
English: BBC (3), Scotsman (3), Economist (3),
Times (3), New York Times (3)
German: Financial Times Deutschland (3), Su?d-
deutsche Zeitung (3), Welt (3), Frankfurter All-
gemeine Zeitung (3), Spiegel (3)
The translations were created by the members of
EuroMatrix consortium who hired a mix of profes-
sional and non-professional translators. All trans-
lators were fluent or native speakers of both lan-
guages, and all translations were proofread by a na-
tive speaker of the target language. All of the trans-
lations were done directly, and not via an intermedi-
ate language. So for instance, each of the 15 Hun-
garian articles were translated into Czech, English,
French, German and Spanish. The total cost of cre-
ating the 6 test sets consisting of 2,051 sentences
in each language was approximately 17,200 euros
(around 26,500 dollars at current exchange rates, at
slightly more than 10c/word).
Having a test set that is balanced in six differ-
ent source languages and translated across six lan-
guages raises some interesting questions. For in-
stance, is it easier, when the machine translation sys-
tem translates in the same direction as the human
translator? We found no conclusive evidence that
shows this. What is striking, however, that the parts
differ dramatically in difficulty, based on the orig-
inal source language. For instance the Edinburgh
French-English system has a BLEU score of 26.8 on
the part that was originally Spanish, but a score of on
9.7 on the part that was originally Hungarian. For
average scores for each original language, see Ta-
ble 1.
71
In order to remain consistent with previous eval-
uations, we also created a Europarl test set. The
Europarl test data was again drawn from the tran-
scripts of EU parliamentary proceedings from the
fourth quarter of 2000, which is excluded from the
Europarl training data. Our rationale behind invest-
ing a considerable sum to create the News test set
was that we believe that it more accurately repre-
sents the quality of systems? translations than when
we simply hold out a portion of the training data
as the test set, as with the Europarl set. For in-
stance, statistical systems are heavily optimized to
their training data, and do not perform as well on
out-of-domain data (Koehn and Schroeder, 2007).
Having both the News test set and the Europarl test
set alows us to contrast the performance of systems
on in-domain and out-of-domain data, and provides
a fairer comparison between systems trained on the
Europarl corpus and systems that were developed
without it.
2.2 Provided materials
To lower the barrier of entry for newcomers to the
field, we provided a complete baseline MT system,
along with data resources. We provided:
? sentence-aligned training corpora
? language model data
? development and dev-test sets
? Moses open source toolkit for phrase-based sta-
tistical translation (Koehn et al, 2007)
The performance of this baseline system is similar
to the best submissions in last year?s shared task.
The training materials are described in Figure 1.
2.3 Submitted systems
We received submissions from 23 groups from 18
institutions, as listed in Table 2. We also eval-
uated seven additional commercial rule-based MT
systems, bringing the total to 30 systems. This is
a significant increase over last year?s shared task,
where there were submissions from 15 groups from
14 institutions. Of the 15 groups that participated in
last year?s shared task, 11 groups returned this year.
One of the goals of the workshop was to attract sub-
missions from newcomers to the field, and we are
please to have attracted many smaller groups, some
as small as a single graduate student and her adviser.
The 30 submitted systems represent a broad
range of approaches to statistical machine transla-
tion. These include statistical phrase-based and rule-
based (RBMT) systems (which together made up the
bulk of the entries), and also hybrid machine trans-
lation, and statistical tree-based systems. For most
language pairs, we assembled a solid representation
of the state of the art in machine translation.
In addition to individual systems being entered,
this year we also solicited a number of entries which
combined the results of other systems. We invited
researchers at BBN, Carnegie Mellon University,
and the University of Edinburgh to apply their sys-
tem combination algorithms to all of the systems
submitted to shared translation task. We designated
the translations of the Europarl set as the develop-
ment data for combination techniques which weight
each system.3 CMU combined the French-English
systems, BBN combined the French-English and
German-English systems, and Edinburgh submitted
combinations for the French-English and German-
English systems as well as a multi-source system
combination which combined all systems which
translated from any language pair into English for
the News test set. The University of Saarland also
produced a system combination over six commercial
RBMT systems (Eisele et al, 2008). Saarland gra-
ciously provided the output of these systems, which
we manually evaluated alongside all other entries.
For more on the participating systems, please re-
fer to the respective system descriptions in the pro-
ceedings of the workshop.
3 Human evaluation
As with last year?s workshop, we placed greater em-
phasis on the human evaluation than on the auto-
matic evaluation metric scores. It is our contention
that automatic measures are an imperfect substitute
for human assessment of translation quality. There-
fore, rather than select an official automatic eval-
uation metric like the NIST Machine Translation
Workshop does (Przybocki and Peterson, 2008), we
define the manual evaluation to be primary, and use
3Since the performance of systems varied significantly be-
tween the Europarl and News test sets, such weighting might
not be optimal. However this was a level playing field, since
none of the individual systems had development data for the
News set either.
72
Europarl Training Corpus
Spanish? English French? English German? English German? Spanish
Sentences 1,258,778 1,288,074 1,266,520 1,237,537
Words 36,424,186 35,060,653 38,784,144 36,046,219 33,404,503 35,259,758 32,652,649 35,780,165
Distinct words 149,159 96,746 119,437 97,571 301,006 96,802 298,040 148,206
News Commentary Training Corpus
Spanish? English French? English German? English German? Spanish
Sentences 64,308 55,030 72,291 63,312
Words 1,759,972 1,544,633 1,528,159 1,329,940 1,784,456 1,718,561 1,597,152 1,751,215
Distinct words 52,832 38,787 42,385 36,032 84,700 40,553 78,658 52,397
Hunglish Training Corpus CzEng Training Corpus
Hungarian? English
Sentences 1,517,584
Words 26,082,667 31,458,540
Distinct words 717,198 192,901
Czech? English
Sentences 1,096,940
Words 15,336,783 17,909,979
Distinct words 339,683 129,176
Europarl Language Model Data
English Spanish French German
Sentence 1,412,546 1,426,427 1,438,435 1,467,291
Words 34,501,453 36,147,902 35,680,827 32,069,151
Distinct words 100,826 155,579 124,149 314,990
Europarl test set
English Spanish French German
Sentences 2,000
Words 60,185 61,790 64,378 56,624
Distinct words 6,050 7,814 7,361 8,844
News Commentary test set
English Czech
Sentences 2,028
Words 45,520 39,384
Distinct words 7,163 12,570
News Test Set
English Spanish French German Czech Hungarian
Sentences 2,051
Words 43,482 47,155 46,183 41,175 36,359 35,513
Distinct words 7,807 8,973 8,898 10,569 12,732 13,144
Figure 1: Properties of the training and test sets used in the shared task. The training data is drawn from the
Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple
languages. For Czech and Hungarian we use other available parallel corpora. Note that the number of
words is computed based on the provided tokenizer and that the number of distinct words is the based on
lowercased tokens.
73
ID Participant
BBN-COMBO BBN system combination (Rosti et al, 2008)
CMU-COMBO Carnegie Mellon University system combination (Jayaraman and Lavie, 2005)
CMU-GIMPEL Carnegie Mellon University Gimpel (Gimpel and Smith, 2008)
CMU-SMT Carnegie Mellon University SMT (Bach et al, 2008)
CMU-STATXFER Carnegie Mellon University Stat-XFER (Hanneman et al, 2008)
CU-TECTOMT Charles University TectoMT (Zabokrtsky et al, 2008)
CU-BOJAR Charles University Bojar (Bojar and Hajic?, 2008)
CUED Cambridge University (Blackwood et al, 2008)
DCU Dublin City University (Tinsley et al, 2008)
LIMSI LIMSI (De?chelotte et al, 2008)
LIU Linko?ping University (Stymne et al, 2008)
LIUM-SYSTRAN LIUM / Systran (Schwenk et al, 2008)
MLOGIC Morphologic (Nova?k et al, 2008)
PCT a commercial MT provider from the Czech Republic
RBMT1?6 Babelfish, Lingenio, Lucy, OpenLogos, ProMT, SDL (ordering anonymized)
SAAR University of Saarbruecken (Eisele et al, 2008)
SYSTRAN Systran (Dugast et al, 2008)
UCB University of California at Berkeley (Nakov, 2008)
UCL University College London (Wang and Shawe-Taylor, 2008)
UEDIN University of Edinburgh (Koehn et al, 2008)
UEDIN-COMBO University of Edinburgh system combination (Josh Schroeder)
UMD University of Maryland (Dyer, 2007)
UPC Universitat Politecnica de Catalunya, Barcelona (Khalilov et al, 2008)
UW University of Washington (Axelrod et al, 2008)
XEROX Xerox Research Centre Europe (Nikoulina and Dymetman, 2008)
Table 2: Participants in the shared translation task. Not all groups participated in all language pairs.
74
the human judgments to validate automatic metrics.
Manual evaluation is time consuming, and it re-
quires a monumental effort to conduct it on the
scale of our workshop. We distributed the work-
load across a number of people, including shared
task participants, interested volunteers, and a small
number of paid annotators. More than 100 people
participated in the manual evaluation, with 75 peo-
ple putting in more than an hour?s worth of effort,
and 25 putting in more than four hours. A collective
total of 266 hours of labor was invested.
We wanted to ensure that we were using our anno-
tators? time effectively, so we carefully designed the
manual evaluation process. In our analysis of last
year?s manual evaluation we found that the NIST-
style fluency and adequacy scores (LDC, 2005) were
overly time consuming and inconsistent.4 We there-
fore abandoned this method of evaluating the trans-
lations.
We asked people to evaluate the systems? output
in three different ways:
? Ranking translated sentences relative to each
other
? Ranking the translations of syntactic con-
stituents drawn from the source sentence
? Assigning absolute yes or no judgments to the
translations of the syntactic constituents.
The manual evaluation software asked for re-
peated judgments from the same individual, and had
multiple people judge the same item, and logged the
time it took to complete each judgment. This al-
lowed us to measure intra- and inter-annotator agree-
ment, and to analyze the average amount of time it
takes to collect the different kinds of judgments. Our
analysis is presented in Section 7.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rela-
tively intuitive and straightforward task. We there-
fore kept the instructions simple. The instructions
for this task were:
4It took 26 seconds on average to assign fluency and ade-
quacy scores to a single sentence, and the inter-annotator agree-
ment had a Kappa of between .225?.25, meaning that annotators
assigned the same scores to identical sentences less than 40% of
the time.
Rank each whole sentence translation
from Best to Worst relative to the other
choices (ties are allowed).
Ranking several translations at a time is a variant
of force choice judgments where a pair of systems
is presented and an annotator is asked ?Is A better
than B, worse than B, or equal to B.? In our exper-
iments, annotators were shown five translations at a
time, except for the Hungarian and Czech language
pairs where there were fewer than five system sub-
missions. In most cases there were more than 5 sys-
tems submissions. We did not attempt to get a com-
plete ordering over the systems, and instead relied
on random selection and a reasonably large sample
size to make the comparisons fair.
?
people
's
Iraq
to
services
basic
other
and
,
care
health
,
food
provide
cannot
it
if
occupation
its
sustain
US
the
Can
?k
?
n
n
e
n
a
n
b
i
e
t
e
n
D
i
e
n
s
t
l
e
i
s
t
u
n
g
e
n
g
r
u
n
d
l
e
g
e
n
d
e
a
n
d
e
r
e
u
n
d
G
e
s
u
n
d
h
e
i
t
s
f
?
r
s
o
r
g
e
,N
a
h
r
u
n
g
n
i
c
h
t
V
o
l
k
i
r
a
k
i
s
c
h
e
n
d
e
m
s
i
e
w
e
n
n
,U
S
A
d
i
e
K
?
n
n
e
n
a
u
f
r
e
c
h
t
e
r
h
a
l
t
e
n
B
e
s
e
t
z
u
n
g
 
 
i
h
r
e
R
e
f
e
r
e
n
c
e
 
t
r
a
n
s
l
a
t
i
o
n
NP
NP
NP
VP
NP
VP
S
S
CNP
NP
Constituents selected 
for evaluation
Target phrases
highlighted via
word alignments
Parsed source
sentence
Figure 2: In constituent-based evaluation, the source
sentence was parsed, and automatically aligned with
the reference translation and systems? translations
75
Language Pair Test Set Constituent Rank Yes/No Judgments Sentence Ranking
English-German Europarl 2,032 2,034 1,004
News 2,170 2,221 1,115
German-English Europarl 1,705 1,674 819
News 1,938 1,881 1,944
English-Spanish Europarl 1,200 1,247 615
News 1,396 1,398 700
Spanish-English Europarl 1,855 1,921 948
News 2,063 1,939 1,896
English-French Europarl 1,248 1,265 674
News 1,741 1,734 843
French-English Europarl 1,829 1,841 909
News 2,467 2,500 2,671
English-Czech News 2,069 2,070 1,045
Commentary 1,840 1,815 932
Czech-English News 0 0 1,400
Commentary 0 0 1,731
Hungarian-English News 0 0 937
All-English News 0 0 4,868
Totals 25,553 25,540 25,051
Table 3: The number of items that were judged for each task during the manual evaluation. The All-English
judgments were reused in the News task for individual language pairs.
3.2 Ranking translations of syntactic
constituents
We continued the constituent-based evaluation that
we piloted last year, wherein we solicited judgments
about the translations of short phrases within sen-
tences rather than whole sentences. We parsed the
source language sentence, selected syntactic con-
stituents from the tree, and had people judge the
translations of those syntactic phrases. In order to
draw judges? attention to these regions, we high-
lighted the selected source phrases and the corre-
sponding phrases in the translations. The corre-
sponding phrases in the translations were located via
automatic word alignments.
Figure 2 illustrates how the source and reference
phrases are highlighted via automatic word align-
ments. The same is done for sentence and each
of the system translations. The English, French,
German and Spanish test sets were automatically
parsed using high quality parsers for those languages
(Bikel, 2002; Arun and Keller, 2005; Dubey, 2005;
Bick, 2006).
The word alignments were created with Giza++
(Och and Ney, 2003) applied to a parallel corpus
containing the complete Europarl training data, plus
sets of 4,051 sentence pairs created by pairing the
test sentences with the reference translations, and
the test sentences paired with each of the system
translations. The phrases in the translations were
located using standard phrase extraction techniques
(Koehn et al, 2003). Because the word-alignments
were created automatically, and because the phrase
extraction is heuristic, the phrases that were selected
may not exactly correspond to the translations of the
selected source phrase. We noted this in the instruc-
tions to judges:
Rank each constituent translation from
Best to Worst relative to the other choices
(ties are allowed). Grade only the high-
lighted part of each translation.
Please note that segments are selected au-
tomatically, and they should be taken as
an approximate guide. They might in-
clude extra words that are not in the actual
alignment, or miss words on either end.
76
The criteria that we used to select which con-
stituents to evaluate were:
? The constituent could not be the whole source
sentence
? The constituent had to be longer three words,
and be no longer than 15 words
? The constituent had to have a corresponding
phrase with a consistent word alignment in
each of the translations
The final criterion helped reduce the number of
alignment errors, but may have biased the sample
to phrases that are more easily aligned.
3.3 Yes/No judgments for the translations of
syntactic constituents
This year we introduced a variant on the constituent-
based evaluation, where instead of asking judges
to rank the translations of phrases relative to each
other, we asked them to indicate which phrasal trans-
lations were acceptable and which were not.
Decide if the highlighted part of each
translation is acceptable, given the refer-
ence. This should not be a relative judg-
ment against the other system translations.
The instructions also contained the same caveat
about the automatic alignments as above. For each
phrase the judges could click on ?Yes?, ?No?, or
?Not Sure.? The number of times people clicked on
?Not Sure? varied by language pair and task. It was
selected as few as 5% of the time for the English-
Spanish News task to as many as 12.5% for the
Czech-English News task.
3.4 Collecting judgments
We collected judgments using a web-based tool that
presented judges with batches of each type of eval-
uation. We presented them with five screens of sen-
tence rankings, ten screens of constituent rankings,
and ten screen of yes/no judgments. The order of the
types of evaluation were randomized.
In order to measure intra-annotator agreement
10% of the items were repeated and evaluated twice
by each judge. In order to measure inter-annotator
agreement 40% of the items were randomly drawn
from a common pool that was shared across all
annotators so that we would have items that were
judged by multiple annotators.
Judges were allowed to select whichever data set
they wanted, and to evaluate translations into what-
ever languages they were proficient in. Shared task
participants were excluded from judging their own
systems.
In addition to evaluation each language pair indi-
vidually, we also combined all system translations
into English for the News test set, taking advantage
of the fact that our test sets were parallel across all
languages. This allowed us to gather interesting data
about the difficulty of translating from different lan-
guages into English.
Table 3 gives a summary of the number of judg-
ments that we collected for translations of individ-
ual sentences. We evaluated 14 translation tasks
with three different types of judgments for most of
them, for a total of 46 different conditions. In to-
tal we collected over 75,000 judgments. Despite the
large number of conditions we managed to collect
between 1,000?2,000 judgments for the constituent-
based evaluation, and several hundred to several
thousand judgments for the sentence ranking tasks.
4 Translation task results
Tables 4, 5, and 6 summarize the results of the hu-
man evaluation of the quality of the machine trans-
lation systems. Table 4 gives the results for the man-
ual evaluation which ranked the translations of sen-
tences. It shows the average number of times that
systems were judged to be better than or equal to
any other system. Table 5 similarly summarizes
the results for the manual evaluation which ranked
the translations of syntactic constituents. Table 6
shows how many times on average a system?s trans-
lated constituents were judged to be acceptable in
the Yes/No evaluation. The bolded items indicate
the system that performed the best for each task un-
der that particular evaluate metric.
Table 7 summaries the results for the All-English
task that we introduced this year. Appendix C gives
an extremely detailed pairwise comparison between
each of the systems, along with an indication of
whether the differences are statistically significant.
The highest ranking entry for the All-English task
77
was the University of Edinburgh?s system combina-
tion entry. It uses a technique similar to Rosti et
al. (2007) to perform system combination. Like the
other system combination entrants, it was tuned on
the Europarl test set and tested on the News test set,
using systems that submitted entries to both tasks.
The University of Edinburgh?s system combi-
nation went beyond other approaches by combin-
ing output from multiple languages pairs (French-
English, German-English and Spanish-English),
resulting in 37 component systems. Rather
than weighting individual systems, it incorporated
weighted features that indicated which language the
system was originally translating from. This entry
was part of ongoing research in multi-lingual, multi-
source translation. Since there was no official multi-
lingual system combination track, this entry should
be viewed only as a contrastive data point.
We analyzed the All-English judgments to see
which source languages were preferred more often,
thinking that this might be a good indication of how
challenging it is for current MT systems to trans-
late from each of the languages into English. For
this analysis we collapsed all of the entries derived
from one source language into an equivalence class,
and judged them against the others. Therefore, all
French systems were judged against all German sys-
tems, and so on. We found that French systems were
judged to be better than or equal to other systems
69% of the time, Spanish systems 64% of the time,
German systems 47% of the time, Czech systems
39% of the time, and Hungarian systems 29% of the
time.
We performed a similar analysis by collapsing the
RBMT systems into one equivalence class, and the
other systems into another. We evaluated how well
these two classes did on the sentence ranking task
for each language pair and test set, and found that
RBMT was a surprisingly good approach in many
of the conditions. RBMT generally did better on the
News test set and for translations into German, sug-
gesting that SMT?s forte is in test sets where it has
appropriate tuning data and for language pairs with
less reordering than between German and English.
BBN-COMBO
CMU-COMBO
CMU-GIMPEL
CMU-SMT
CMU-STATXFER
CU-BOJAR
CU-TECTOMT
CUED
CUED-CONTR
DCU
LIMSI
LIU
LIUM-SYSTRAN
LIUM-SYS-CONTR
MORPHOLOGIC
PC-TRANSLATOR
RBMT2
RBMT3
RBMT4
RBMT5
RBMT6
SAAR
SAAR-CONTR
SYSTRAN
UCB
UCL
UEDIN
UEDIN-COMBO
UMD
UPC
UW
XEROX
C
zech-E
nglish
C
om
m
entary
.745
.445
.603
.717
C
zech-E
nglish
N
ew
s
.675
.583
.646
E
nglish-C
zech
C
om
m
entary
.714
.488
.663
.486
E
nglish-C
zech
N
ew
s
.634
.494
.715
.502
E
nglish-F
rench
E
uroparl
.791
.775
.416
.608
.263
.436
.744
.786
.444
.766
E
nglish-F
rench
N
ew
s
.667
.655
.602
.780
.734
.657
.511
.573
.545
.317
E
nglish-G
erm
an
E
uroparl
.612
.584
.581
.615
.583
.681
.471
.432
.527
.386
.667
E
nglish-G
erm
an
N
ew
s
.361
.426
.787
.664
.752
.667
.555
.463
.444
E
nglish-S
panish
E
uroparl
.667
.737
.554
.560
.413
.436
.717
.500
.714
.593
.735
E
nglish-S
panish
N
ew
s
.494
.537
.683
.674
.724
.715
.548
.586
.481
.601
F
rench-E
nglish
E
uroparl
.415
.697
.642
.776
.792
.400
.504
.484
.323
.577
.753
.465
.524
.707
F
rench-E
nglish
N
ew
s
.659
.592
.379
.549
.643
.632
.660
.693
.581
.575
.654
.565
.540
.639
.614
.608
G
erm
an-E
nglish
E
uroparl
.364
.485
.614
.627
.596
.610
.543
.537
.677
.416
.679
G
erm
an-E
nglish
N
ew
s
.514
.354
.518
.559
.742
.725
.731
.668
.590
.607
.649
.548
.441
H
ungarian-E
nglish
N
ew
s
.853
.321
S
panish-E
nglish
E
uroparl
.714
.676
.677
.780
.427
.488
.350
.470
.671
.425
.660
.687
S
panish-E
nglish
N
ew
s
.567
.563
.674
.583
.667
.768
.577
.613
.669
.543
.561
.602
Table
4:
S
um
m
ary
results
for
the
sentence
ranking
judgm
ents.
T
he
num
bers
reportthe
percentof
tim
e
thateach
system
w
as
judged
to
be
greater
than
or
equalto
any
other
system
.
B
old
indicates
the
highestscore
for
thattask.
78
C
zech-E
nglish
C
om
m
entary
C
zech-E
nglish
N
ew
s
E
nglish-C
zech
C
om
m
entary
.732
.538
.609
.614
E
nglish-C
zech
N
ew
s
.663
.615
.674
.610
E
nglish-F
rench
E
uroparl
.876
.881
.561
.675
.546
.561
.807
.656
.870
E
nglish-F
rench
N
ew
s
.649
.760
.716
.768
.763
.671
.725
.746
.661
.437
E
nglish-G
erm
an
E
uroparl
.774
.750
.812
.577
.585
.582
.508
.518
.770
.690
.822
E
nglish-G
erm
an
N
ew
s
.649
.570
.720
.682
.748
.602
.563
.610
.556
E
nglish-S
panish
E
uroparl
.825
.855
.561
.592
.458
.573
.849
.592
.818
.775
.790
E
nglish-S
panish
N
ew
s
.721
.694
.694
.754
.570
.644
.696
.653
.625
.595
F
rench-E
nglish
E
uroparl
.626
.907
.854
.906
.917
.523
.648
.697
.517
.783
.865
.713
.741
.894
F
rench-E
nglish
N
ew
s
.506
.745
.787
.801
.765
.780
.652
.655
.726
.615
.640
.735
.773
G
erm
an-E
nglish
E
uroparl
.554
.752
.795
.580
.640
.643
.579
.587
.843
.601
.832
G
erm
an-E
nglish
N
ew
s
.502
.715
.674
.772
.755
.740
.674
.640
.757
.775
.744
H
ungarian-E
nglish
N
ew
s
S
panish-E
nglish
E
uroparl
.847
.846
.868
.854
.455
.561
.469
.567
.893
.646
.865
.870
S
panish-E
nglish
N
ew
s
.715
.760
.818
.739
.644
.608
.699
.700
.760
.706
.758
.763
Table
5:
S
um
m
ary
results
for
the
constituent
ranking
judgm
ents.
T
he
num
bers
report
the
percent
of
tim
e
that
each
system
w
as
judged
to
be
greater
than
or
equalto
any
other
system
.
B
old
indicates
the
highestscore
for
thattask.
BBN-COMBO
CMU-COMBO
CMU-GIMPEL
CMU-SMT
CMU-STATXFER
CU-BOJAR
CU-TECTOMT
CUED
CUED-CONTR
DCU
LIMSI
LIU
LIUM-SYSTRAN
LIUM-SYS-CONTR
MORPHOLOGIC
PC-TRANSLATOR
RBMT2
RBMT3
RBMT4
RBMT5
RBMT6
SAAR
SAAR-CONTR
SYSTRAN
UCB
UCL
UEDIN
UEDIN-COMBO
UMD
UPC
UW
XEROX
C
zech-E
nglish
C
om
m
entary
C
zech-E
nglish
N
ew
s
E
nglish-C
zech
C
om
m
entary
.594
.427
.506
.409
E
nglish-C
zech
N
ew
s
.540
.422
.518
.441
E
nglish-F
rench
E
uroparl
.745
.843
.490
.504
.442
.351
.701
.596
.750
E
nglish-F
rench
N
ew
s
.730
.748
.589
.593
.640
.576
.591
.586
.633
.302
E
nglish-G
erm
an
E
uroparl
.822
.794
.788
.692
.571
.665
.447
.466
.774
.611
.849
E
nglish-G
erm
an
N
ew
s
.559
.494
.689
.689
.750
.553
.598
.544
.518
E
nglish-S
panish
E
uroparl
.804
.872
.582
.598
.635
.600
.806
.714
.888
.903
.785
E
nglish-S
panish
N
ew
s
.459
.532
.638
.759
.599
.623
.639
.568
.493
.366
F
rench-E
nglish
E
uroparl
.612
.833
.876
.886
.891
.535
.620
.712
.540
.717
.860
.811
.734
.910
F
rench-E
nglish
N
ew
s
.554
.736
.788
.805
.789
.696
.628
.640
.762
.663
.638
.701
.718
G
erm
an-E
nglish
E
uroparl
.534
.803
.831
.759
.744
.667
.633
.630
.823
.492
.856
G
erm
an-E
nglish
N
ew
s
.470
.725
.638
.717
.731
.738
.589
.684
.669
.716
.632
H
ungarian-E
nglish
N
ew
s
S
panish-E
nglish
E
uroparl
.882
.857
.853
.902
.648
.562
.590
.550
.869
.730
.879
.857
S
panish-E
nglish
N
ew
s
.635
.638
.694
.675
.610
.651
.594
.635
.697
.635
.622
.707
Table
6:
S
um
m
ary
results
for
the
Yes/N
o
judgm
ents
for
constituenttranslations
judgm
ents.
T
he
num
bers
reportthe
percentof
each
system
?s
transla-
tions
thatw
ere
judged
to
be
acceptable.
B
old
indicates
the
highestscore
for
thattask.
79
UEDIN-COMBOxx .717 SAARfr .584
LIUM-SYSTRAN-Cfr .708 SAAR-Cde .574
RBMT5fr .706 RBMT4de .573
UEDIN-COMBOfr .704 CUEDes .572
LIUM-SYSTRANfr .702 RBMT3de .552
RBMT4es .699 CMU-SMTes .548
LIMSIfr .699 UCBes .547
BBN-COMBOfr .695 LIMSIes .537
SAARes .678 RBMT6de .509
CUED-CONTRASTes .674 RBMT5de .493
CMU-COMBOfr .661 LIMSIde .469
UEDINes .654 LIUde .447
CUEDfr .652 SAARde .445
CUED-CONTRASTfr .638 CMU-STATXFRfr .444
RBMT4fr .637 UMDcz .429
UPCes .633 BBN-COMBOde .407
RBMT3es .628 UEDINde .402
RBMT2de .627 MORPHOLOGIChu .387
SAAR-CONTRASTfr .624 DCUcz .380
UEDINfr .616 UEDIN-COMBOde .327
RBMT6fr .615 UEDINcz .293
RBMT6es .615 CMU-STATXFERde .280
RBMT3fr .612 UEDINhu .188
Table 7: The average number of times that each
system was judged to be better than or equal to all
other systems in the sentence ranking task for the
All-English condition. The subscript indicates the
source language of the system.
5 Shared evaluation task overview
The manual evaluation data provides a rich source
of information beyond simply analyzing the qual-
ity of translations produced by different systems. In
particular, it is especially useful for validating the
automatic metrics which are frequently used by the
machine translation research community. We con-
tinued the shared task which we debuted last year,
by examining how well various automatic metrics
correlate with human judgments.
In addition to examining how well the automatic
evaluation metrics predict human judgments at the
system-level, this year we have also started to mea-
sure their ability to predict sentence-level judg-
ments.
The automatic metrics that were evaluated in this
year?s shared task were the following:
? Bleu (Papineni et al, 2002)?Bleu remains the
de facto standard in machine translation eval-
uation. It calculates n-gram precision and a
brevity penalty, and can make use of multi-
ple reference translations as a way of capturing
some of the allowable variation in translation.
We use a single reference translation in our ex-
periments.
? Meteor (Agarwal and Lavie, 2008)?Meteor
measures precision and recall for unigrams and
applies a fragmentation penalty. It uses flex-
ible word matching based on stemming and
WordNet-synonymy. A number of variants are
investigated here: meteor-baseline and meteor-
ranking are optimized for correlation with ad-
equacy and ranking judgments respectively.
mbleu and mter are Bleu and TER computed
using the flexible matching used in Meteor.
? Gimenez and Marquez (2008) measure over-
lapping grammatical dependency relationships
(DP), semantic roles (SR), and discourse repre-
sentations (DR). The authors further investigate
combining these with other metrics including
TER, Bleu, GTM, Rouge, and Meteor (ULC
and ULCh).
? Popovic and Ney (2007) automatically eval-
uate translation quality by examining se-
quences of parts of speech, rather than
words. They calculate Bleu (posbleu) and
F-measure (pos4gramFmeasure) by matching
part of speech 4grams in a hypothesis transla-
tion against the reference translation.
In addition to the above metrics, which scored
the translations on both the system-level5 and the
sentence-level, there were a number of metrics
which focused on the sentence-level:
? Albrecht and Hwa (2008) use support vector re-
gression to score translations using past WMT
manual assessment data as training examples.
The metric uses features derived from target-
side language models and machine-generated
translations (svm-pseudo-ref) as well as refer-
ence human translations (svm-human-ref).
? Duh (2008) similarly used support vector ma-
chines to predict an ordering over a set of
5We provide the scores assigned to each system by these
metrics in Appendix A.
80
system translations (svm-rank). Features in-
cluded in Duh (2008)?s training were sentence-
level BLEU scores and intra-set ranks com-
puted from the entire set of translations.
? USaar?s evaluation metric (alignment-prob)
uses Giza++ to align outputs of multiple sys-
tems with the corresponding reference transla-
tions, with a bias towards identical one-to-one
alignments through a suitably augmented cor-
pus. The Model4 log probabilities in both di-
rections are added and normalized to a scale
between 0 and 1.
5.1 Measuring system-level correlation
To measure the correlation of the automatic metrics
with the human judgments of translation quality at
the system-level we used Spearman?s rank correla-
tion coefficient ?. We converted the raw scores as-
signed each system into ranks. We assigned a rank-
ing to the systems for each of the three types of man-
ual evaluation based on:
? The percent of time that the sentences it pro-
duced were judged to be better than or equal to
the translations of any other system.
? The percent of time that its constituent transla-
tions were judged to be better than or equal to
the translations of any other system.
? The percent of time that its constituent transla-
tions were judged to be acceptable.
We calculated ? three times for each automatic met-
ric, comparing it to each type of human evaluation.
Since there were no ties ? can be calculated using
the simplified equation:
? = 1 ?
6
?
d2i
n(n2 ? 1)
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher value for ? is
making predictions that are more similar to the hu-
man judgments than an automatic evaluation metric
with a lower ?.
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
O
V
E
R
A
L
L
meteor-ranking .81 .72 .77 .76
ULCh .68 .79 .82 .76
meteor-baseline .77 .75 .74 .75
posbleu .77 .8 .66 .74
pos4gramFmeasure .75 .62 .82 .73
ULC .66 .67 .84 .72
DR .79 .55 .76 .70
SR .79 .53 .76 .69
DP .57 .79 .65 .67
mbleu .61 .77 .56 .65
mter .47 .72 .68 .62
bleu .61 .59 .44 .54
svm-rank .21 .24 .35 .27
Table 8: Average system-level correlations for the
automatic evaluation metrics on translations into En-
glish
5.2 Measuring consistency at the sentence-level
Measuring sentence-level correlation under our hu-
man evaluation framework was made complicated
by the fact that we abandoned the fluency and ad-
equacy judgments which are intended to be abso-
lute scales. Some previous work has focused on
developing automatic metrics which predict human
ranking at the sentence-level (Kulesza and Shieber,
2004; Albrecht and Hwa, 2007a; Albrecht and Hwa,
2007b). Such work generally used the 5-point flu-
ency and adequacy scales to combine the transla-
tions of all sentences into a single ranked list. This
list could be compared against the scores assigned
by automatic metrics and used to calculate corre-
lation coefficients. We did not gather any absolute
scores and thus cannot compare translations across
different sentences. Given the seemingly unreliable
fluency and adequacy assignments that people make
even for translations of the same sentences, it may
be dubious to assume that their scoring will be reli-
able across sentences.
The data points that we have available consist of a
set of 6,400 human judgments each ranking the out-
put of 5 systems. It?s straightforward to construct a
ranking of each of those 5 systems using the scores
81
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
O
V
E
R
A
L
L
posbleu .57 .78 .80 .72
bleu .54 .79 .6 .64
meteor-ranking .55 .74 .55 .61
meteor-baseline .42 .78 .57 .59
pos4gramFmeasure .37 .49 .79 .55
mter .54 .50 .55 .53
svm-rank .55 .56 .46 .52
mbleu .63 .47 .43 .51
Table 9: Average system-level correlations for the
automatic evaluation metrics on translations into
French, German and Spanish
assigned to their translations of that sentence by the
automatic evaluation metrics. When the automatic
scores have been retrieved, we have 6,400 pairs of
ranked lists containing 5 items. How best to treat
these is an open discussion, and certainly warrants
further thought. It does not seem like a good idea
to calculate ? for each pair of ranked list, because
5 items is an insufficient number to get a reliable
correlation coefficient and its unclear if averaging
over all 6,400 lists would make sense. Furthermore,
many of the human judgments of 5 contained ties,
further complicating matters.
Therefore rather than calculating a correlation co-
efficient at the sentence-level we instead ascertained
how consistent the automatic metrics were with the
human judgments. The way that we calculated con-
sistency was the following: for every pairwise com-
parison of two systems on a single sentence by a per-
son, we counted the automatic metric as being con-
sistent if the relative scores were the same (i.e. the
metric assigned a higher score to the higher ranked
system). We divided this by the total number of pair-
wise comparisons to get a percentage. Because the
systems generally assign real numbers as scores, we
excluded pairs that the human annotators ranked as
ties.
6 Evaluation task results
Tables 8 and 9 report the system-level ? for each au-
tomatic evaluation metric, averaged over all trans-
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
DP .514 .527 .536
DR .500 .511 .530
SR .498 .489 .511
ULC .559 .554 .561
ULCh .562 .542 .542
alignment-prob .517 .538 .535
mbleu .505 .516 .544
meteor-baseline .512 .520 .542
meteor-ranking .512 .517 .539
mter .436 .471 .480
pos4gramFmeasure .495 .517 .52
posbleu .435 .43 .454
svm-human-ref .542 .541 .552
svm-pseudo-ref .538 .538 .543
svm-rank .493 .499 .497
Table 10: The percent of time that each automatic
metric was consistent with human judgments for
translations into English
lations directions into English and out of English6
For the into English direction the Meteor score with
its parameters tuned on adequacy judgments had
the strongest correlation with ranking the transla-
tions of whole sentences. It was tied with the com-
bined method of Gimenez and Marquez (2008) for
the highest correlation over all three types of human
judgments. Bleu was the second to lowest ranked
overall, though this may have been due in part to the
fact that we were using test sets which had only a
single reference translation, since the cost of creat-
ing multiple references was prohibitively expensive
(see Section 2.1).
In the reverse direction, for translations out of En-
glish into the other languages, Bleu does consider-
ably better, placing second overall after the part-of-
speech variant on it proposed by Popovic and Ney
(2007). Yet another variant of Bleu which utilizes
Meteor?s flexible matching has the strongest corre-
lation for sentence-level ranking. Appendix B gives
a break down of the correlations for each of the lan-
6Tables 8 and 9 exclude the Spanish-English News Task,
since it had a negative correlation with most of the automatic
metrics. See Tables 19 and 20.
82
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
mbleu 0.520 0.521 0.52
meteor-baseline 0.514 0.494 0.520
meteor-ranking 0.522 0.501 0.534
mter 0.454 0.441 0.457
pos4gramFmeasure 0.515 0.525 0.512
posbleu 0.436 0.446 0.416
svm-rank 0.514 0.531 0.51
Table 11: The percent of time that each automatic
metric was consistent with human judgments for
translations into other languages
guage pairs and test sets.
Tables 10 and 11 report the consistency of the au-
tomatic evaluation metrics with human judgments
on a sentence-by-sentence basis, rather than on the
system level. For the translations into English the
ULC metric (which itself combines many other met-
rics) had the strongest correlation with human judg-
ments, correctly predicting the human ranking of a
each pair of system translations of a sentence more
than half the time. This is dramatically higher than
the chance baseline, which is not .5, since it must
correctly rank a list of systems rather than a pair. For
the reverse direction meteor-ranking performs very
strongly. The svn-rank which had the lowest over-
all correlation at the system level does the best at
consistently predicting the translations of syntactic
constituents into other languages.
7 Validation and analysis of the manual
evaluation
In addition to scoring the shared task entries, we also
continued on our campaign for improving the pro-
cess of manual evaluation.
7.1 Inter- and Intra-annotator agreement
We measured pairwise agreement among annotators
using the kappa coefficient (K) which is widely used
in computational linguistics for measuring agree-
ment in category judgments (Carletta, 1996). It is
defined as
K =
P (A) ? P (E)
1 ? P (E)
Evaluation type P (A) P (E) K
Sentence ranking .578 .333 .367
Constituent ranking .671 .333 .506
Constituent (w/identicals) .678 .333 .517
Yes/No judgments .821 .5 .642
Yes/No (w/identicals) .825 .5 .649
Table 12: Kappa coefficient values representing the
inter-annotator agreement for the different types of
manual evaluation
Evaluation type P (A) P (E) K
Sentence ranking .691 .333 .537
Constituent ranking .825 .333 .737
Constituent (w/identicals) .832 .333 .748
Yes/No judgments .928 .5 .855
Yes/No (w/identicals) .930 .5 .861
Table 13: Kappa coefficient values for intra-
annotator agreement for the different types of man-
ual evaluation
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance. We define chance
agreement for ranking tasks as 13 since there are
three possible outcomes when ranking the output of
a pair of systems: A > B, A = B, A < B, and for
the Yes/No judgments as 12 since we ignored those
items marked ?Not Sure?.
For inter-annotator agreement we calculated
P (A) for the yes/no judgments by examining all
items that were annotated by two or more annota-
tors, and calculating the proportion of time they as-
signed identical scores to the same items. For the
ranking tasks we calculated P (A) by examining all
pairs of systems which had been judged by two or
more judges, and calculated the proportion of time
that they agreed that A > B, A = B, or A < B.
For intra-annotator agreement we did similarly, but
gathered items that were annotated on multiple oc-
casions by a single annotator.
Table 12 givesK values for inter-annotator agree-
ment, and Table 13 gives K values for intra-
annotator agreement. These give an indication of
how often different judges agree, and how often sin-
gle judges are consistent for repeated judgments, re-
83
spectively. The interpretation of Kappa varies, but
according to Landis and Koch (1977), 0?.2 is slight,
.2? .4 is fair, .4? .6 is moderate, .6? .8 is substan-
tial and the rest almost perfect. The inter-annotator
agreement for the sentence ranking task was fair, for
the constituent ranking it was moderate and for the
yes/no judgments it was substantial.7 For the intra-
annotator agreement K indicated that people had
moderate consistency with their previous judgments
on the sentence ranking task, substantial consistency
with their previous constituent ranking judgments,
and nearly perfect consistency with their previous
yes/no judgments.
These K values indicate that people are able to
more reliably make simple yes/no judgments about
the translations of short phrases than they are to
rank phrases or whole sentences. While this is an
interesting observation, we do not recommend do-
ing away with the sentence ranking judgments. The
higher agreement on the constituent-based evalua-
tion may be influenced based on the selection cri-
teria for which phrases were selected for evalua-
tion (see Section 3.2). Additionally, the judgments
of the short phrases are not a great substitute for
sentence-level rankings, at least in the way we col-
lected them. The average correlation coefficient be-
tween the constituent-based judgments with the sen-
tence ranking judgments is only ? = 0.51. Tables
19 and 20 give a detailed break down of the cor-
relation of the different types of human judgments
with each other on each translation task. It may
be possible to select phrases in such a way that the
constituent-based evaluations are a better substitute
for the sentence-based ranking, for instance by se-
lecting more of constituents from each sentence, or
attempting to cover most of the words in each sen-
tence in a phrase-by-phrase manner. This warrants
further investigation. It might also be worthwhile to
refine the instructions given to annotators about how
to rank the translations of sentences to try to improve
their agreement, which is currently lower than we
would like it to be (although it is substantially bet-
ter than the previous fluency and adequacy scores,
7Note that for the constituent-based evaluations we verified
that the high K was not trivially due to identical phrasal trans-
lations. We excluded screens where all five phrasal translations
presented to the annotator were identical, and report both num-
bers.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  10  20  30  40  50  60
p
e
r
c
e
n
t
 
o
f
 
s
e
n
t
e
n
c
e
s
 
t
a
k
i
n
g
 
t
h
i
s
 
l
o
n
g
time to judge one sentence (seconds)
yes/no judgmentsconstituent ranksentence rank
Figure 3: Distributions of the amount of time it took
to judge single sentences for the three types of man-
ual evaluation
which had a K < .25 in last year?s evaluation).
7.2 Timing
We used the web interface to collect timing infor-
mation. The server recorded the time when a set of
sentences was given to a judge and the time when
the judge returned the sentences. It took annotators
an average of 18 seconds per sentence to rank a list
of sentences.8 It took an average of 10 seconds per
sentence for them to rank constituents, and an av-
erage of 8.5 seconds per sentence for them to make
yes/no judgments. Figure 3 shows the distribution
of times for these tasks.
These timing figures indicate that the tasks which
the annotators were the most reliable on (yes/no
judgments and constituent ranking) were also much
quicker to complete than the ones they were less re-
liable on (ranking sentences). Given that they are
faster at judging short phrases, they can do propor-
tionally more of them. For instance, we could collect
211 yes/no judgments in the same amount of time
that it would take us to collect 100 sentence ranking
judgments. However, this is partially offset by the
fact that many of the translations of shorter phrases
are identical, which means that we have to collect
more judgments in order to distinguish between two
systems.
8Sets which took longer than 5 minutes were excluded from
these calculations, because there was a strong chance that anno-
tators were interrupted while completing the task.
84
7.3 The potential for re-usability of human
judgments
One strong advantage of the yes/no judgments over
the ranking judgments is their potential for reuse.
We have invested hundreds of hours worth of effort
evaluating the output of the translation systems sub-
mitted to this year?s workshop and last year?s work-
shop. While the judgments that we collected pro-
vide a wealth of information for developing auto-
matic evaluation metrics, we cannot not re-use them
to evaluate our translation systems after we update
their parameters or change their behavior in anyway.
The reason for this is that altered systems will pro-
duce different translations than the ones that we have
judged, so our relative rankings of sentences will no
longer be applicable. However, the translations of
short phrases are more likely to be repeated than the
translations of whole sentences.
Therefore if we collect a large number of yes/no
judgments for short phrases, we could build up a
database that contains information about what frag-
mentary translations are acceptable for each sen-
tence in our test corpus. When we change our sys-
tem and want to evaluate it, we do not need to man-
ually evaluate those segments that match against the
database, and could instead have people evaluate
only those phrasal translations which are new. Ac-
cumulating these judgments over time would give
a very reliable idea of what alternative translations
were allowable. This would be useful because it
could alleviate the problems associated with Bleu
failing to recognize allowable variation in translation
when multiple reference translations are not avail-
able (Callison-Burch et al, 2006). A large database
of human judgments might also be useful as an
objective function for minimum error rate training
(Och, 2003) or in other system development tasks.
8 Conclusions
Similar to previous editions of this workshop we car-
ried out an extensive manual and automatic evalua-
tion of machine translation performance for trans-
lating from European languages into English, and
vice versa. One important aspect in which this year?s
shared task differed from previous years was the in-
troduction of an additional newswire test set that
was different in nature to the training data. We
also added new language pairs to our evaluation:
Hungarian-English and German-Spanish.
As in previous years we were pleased to notice an
increase in the number of participants. This year we
received submissions from 23 groups from 18 insti-
tutions. In addition, we evaluated seven commercial
rule-based MT systems.
The goal of this shared-task is two-fold: First we
want to compare state-of-the-art machine translation
systems, and secondly we aim to measure to what
extent different evaluation metrics can be used to as-
sess MT quality.
With respect to MT quality we noticed that the in-
troduction of test sets from a different domain did
have an impact on the ranking of systems. We ob-
served that rule-based systems generally did better
on the News test set. Overall, it cannot be con-
cluded that one approach clearly outperforms other
approaches, as systems performed differently on the
various translation tasks. One general observation is
that for the tasks where statistical combination ap-
proaches participated, they tended to score relatively
high, in particular with respect to Bleu.
With respect to measuring the correlation between
automated evaluation metrics and human judgments
we found that using Meteor and ULCh (which uti-
lizes a variety of metrics, including Meteor) resulted
in the highest Spearman correlation scores on aver-
age, when translating into English. When translat-
ing from English into French, German, and Spanish,
Bleu and posbleu resulted in the highest correlations
with human judgments.
Finally, we investigated inter- and intra-annotator
agreement of human judgments using Kappa coef-
ficients. We noticed that ranking whole sentences
results in relatively low Kappa coefficients, mean-
ing that there is only fair agreement between the as-
sessors. Constituent ranking and acceptability judg-
ments on the other hand showmoderate and substan-
tial inter-annotator agreement, respectively. Intra-
annotator agreement was substantial to almost per-
fect, except for the sentence ranking assessment
where agreement was only moderate. Although it
is difficult to draw exact conclusions from this, one
might wonder whether the sentence ranking task is
simply too complex, involving too many aspects ac-
cording to which translations can be ranked.
The huge wealth of the data generated by this
85
workshop, including the human judgments, system
translations and automatic scores, is available at
http://www.statmt.org/wmt08/ for other
researchers to analyze.
Acknowledgments
This work was supported in parts by the EuroMatrix
project funded by the European Commission (6th
Framework Programme), the GALE program of the
US Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022, and the US Na-
tional Science Foundation under grant IIS-0713448.
We are grateful to Abhaya Agarwal, John Hen-
derson, Rebecca Hwa, Alon Lavie, Mark Przybocki,
Stuart Shieber, and David Smith for discussing dif-
ferent possibilities for calculating the sentence-level
correlation of automatic evaluation metrics with hu-
man judgments in absence of absolute scores. Any
errors in design remain the responsibility of the au-
thors.
Thank you to Eckhard Bick for parsing the Span-
ish test set. See http://beta.visl.sdu.dk for
more information about the constraint-based parser.
Thanks to Greg Hanneman and Antti-Veikko Rosti
for applying their system combination algorithms to
our data.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor,
M-BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine transla-
tion output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio, June. Association for Computational
Linguistics.
Joshua Albrecht and Rebecca Hwa. 2007a. A
re-examination of machine learning approaches for
sentence-level mt evaluation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2007), Prague, Czech Repub-
lic.
Joshua Albrecht and Rebecca Hwa. 2007b. Regres-
sion for sentence-level mt evaluation with pseudo ref-
erences. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
2007), Prague, Czech Republic.
Joshua Albrecht and Rebecca Hwa. 2008. The role of
pseudo references in MT evaluation. In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, pages 187?190, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of ACL.
Amittai Axelrod, Mei Yang, Kevin Duh, and Katrin
Kirchhoff. 2008. The University of Washington ma-
chine translation system for ACL WMT 2008. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 123?126, Columbus, Ohio, June.
Association for Computational Linguistics.
Nguyen Bach, Qin Gao, and Stephan Vogel. 2008. Im-
proving word alignment with language model based
confidence scores. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 151?
154, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Eckhard Bick. 2006. A constraint grammar-based parser
for Spanish. In Proceedings of the 4th Workshop on
Information and Human Language Technology (TIL-
2006), Ribeiro Preto, Brazil.
Dan Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceed-
ings of Second International Conference on Human
Language Technology Research (HLT-02), San Diego,
California.
Graeme Blackwood, Adria` de Gispert, Jamie Brunning,
and William Byrne. 2008. European language transla-
tion with weighted finite state transducers: The CUED
MT system for the 2008 ACL workshop on SMT. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 131?134, Columbus, Ohio,
June. Association for Computational Linguistics.
Ondr?ej Bojar and Jan Hajic?. 2008. Phrase-based and
deep syntactic English-to-Czech statistical machine
translation. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 143?146,
Columbus, Ohio, June. Association for Computational
Linguistics.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2006), Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
86
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
He?le`ne Bonneau-Maynard, Olivier Galibert, Jean-Luc
Gauvain, Philippe Langlais, and Franc?ois Yvon. 2008.
Limsi?s statistical translation systems for WMT?08. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 107?110, Columbus, Ohio,
June. Association for Computational Linguistics.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In Proceedings of ACL.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2008.
Can we relearn an RBMT system? In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, pages 175?178, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Kevin Duh. 2008. Ranking vs. regression in ma-
chine translation evaluation. In Proceedings of the
Third Workshop on Statistical Machine Translation,
pages 191?194, Columbus, Ohio, June. Association
for Computational Linguistics.
Christopher J. Dyer. 2007. The ?noisier channel?: trans-
lation from morphologically complex languages. In
Proceedings of the ACL-2007 Workshop on Statistcal
Machine Translation (WMT-07), Prague, Czech Re-
public.
Andreas Eisele, Christian Federmann, Herve? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using moses to integrate multiple
rule-based machine translation engines into a hybrid
system. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 179?182, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Jesus Gimenez and Lluis Marquez. 2008. A smorgas-
bord of features for automatic MT evaluation. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 195?198, Columbus, Ohio, June.
Association for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 9?17, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Greg Hanneman, Edmund Huber, Abhaya Agarwal,
Vamshi Ambati, Alok Parlikar, Erik Peterson, and
Alon Lavie. 2008. Statistical transfer systems for
French-English and German-English machine transla-
tion. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 163?166, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proceedings of the 10th Annual Confer-
ence of the European Association for Machine Trans-
lation, pages 143?152, Budapest, Hungary, May.
Maxim Khalilov, Adolfo Herna?ndez H., Marta R. Costa-
jussa`, Josep M. Crego, Carlos A. Henr??quez Q., Pa-
trik Lambert, Jose? A. R. Fonollosa, Jose? B. Marin?o,
and Rafael E. Banchs. 2008. The TALP-UPC Ngram-
based statistical machine translation system for ACL-
WMT 2008. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 127?130,
Columbus, Ohio, June. Association for Computational
Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation, New
York, New York.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Proceedings of the ACL-2007 Workshop on Statist-
cal Machine Translation (WMT-07), Prague, Czech
Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
of the North American chapter of the Association for
Computational Linguistics (HLT/NAACL-2003), Ed-
monton, Alberta.
Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, Chris
Callison-Burch, Alexandra Constantin, Brooke
Cowan, Chris Dyer, Marcello Federico, Evan Herbst,
Hieu Hoang, Christine Moran, Wade Shen, and
Richard Zens. 2007. Open source toolkit for statisti-
cal machine translation: Factored translation models
and confusion network decoding. CLSP Summer
Workshop Final Report WS-2006, Johns Hopkins
University.
Philipp Koehn, Abhishek Arun, and Hieu Hoang. 2008.
Towards better machine translation quality for the
German-English language pairs. In Proceedings of
the Third Workshop on Statistical Machine Transla-
tion, pages 139?142, Columbus, Ohio, June. Associ-
ation for Computational Linguistics.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
87
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation, Baltimore, MD, October 4?6.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159?174.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Preslav Nakov. 2008. Improving English-Spanish sta-
tistical machine translation: Experiments in domain
adaptation, sentence paraphrasing, tokenization, and
recasing. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 147?150,
Columbus, Ohio, June. Association for Computational
Linguistics.
Vassilina Nikoulina and Marc Dymetman. 2008. Using
syntactic coupling features for discriminating phrase-
based translations (wmt-08 shared translation task). In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 159?162, Columbus, Ohio,
June. Association for Computational Linguistics.
Attila Nova?k, La?szlo? Tihanyi, and Ga?bor Pro?sze?ky. 2008.
The MetaMorpho translation system. In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, pages 111?114, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: Amethod for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL-2002), Philadelphia, Pennsylvania.
Maja Popovic and Hermann Ney. 2007. Word error rates:
Decomposition over POS classes and applications for
error analysis. In Proceedings of ACL Workshop on
Machine Translation, Prague, Czech Republic.
Mark Przybocki and Kay Peterson, editors. 2008. Pro-
ceedings of the 2008 NIST Open Machine Translation
Evaluation Workshop. Arlington, Virginia, March.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system combi-
nation for machine translation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2007), Prague, Czech Repub-
lic.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183?186, Columbus, Ohio,
June. Association for Computational Linguistics.
Holger Schwenk, Jean-Baptiste Fouet, and Jean Senel-
lart. 2008. First steps towards a general purpose
French/English statistical machine translation system.
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 119?122, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of morphological analysis in transla-
tion between German and English. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 135?138, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
John Tinsley, Yanjun Ma, Sylwia Ozdowska, and Andy
Way. 2008. MaTrEx: The DCU MT system for WMT
2008. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 171?174, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zhuoran Wang and John Shawe-Taylor. 2008. Kernel
regression framework for machine translation: UCL
system description for WMT 2008 shared translation
task. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, pages 155?158, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zdenek Zabokrtsky, Jan Ptacek, and Petr Pajas. 2008.
TectoMT: Highly modular MT system with tectogram-
matics used as transfer layer. In Proceedings of the
Third Workshop on Statistical Machine Translation,
pages 167?170, Columbus, Ohio, June. Association
for Computational Linguistics.
88
A Automatic scores for each system
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
S
V
M
-R
A
N
K
English-Czech News Commentary Task
CU-BOJAR 0.15 0.21 0.43 0.35 0.28 4.57
CU-BOJAR-CONTRAST-1 0.04 0.11 0.32 0.25 0.18 0.90
CU-BOJAR-CONTRAST-2 0.14 0.2 0.42 0.34 0.27 2.86
CU-TECTOMT 0.09 0.15 0.37 0.29 0.23 2.13
PC-TRANSLATOR 0.08 0.14 0.35 0.28 0.19 2.09
UEDIN 0.12 0.18 0.4 0.32 0.25 2.28
English-Czech News Task
CU-BOJAR 0.11 0.18 0.37 0.3 0.18 4.72
CU-BOJAR-CONTRAST-1 0.02 0.10 0.26 0.2 0.12 0.80
CU-BOJAR-CONTRAST-2 0.09 0.16 0.35 0.28 0.15 2.65
CU-TECTOMT 0.06 0.13 0.32 0.25 0.16 2.14
PC-TRANSLATOR 0.08 0.14 0.33 0.26 0.14 2.40
UEDIN 0.08 0.15 0.34 0.27 0.15 2.13
Table 14: Automatic evaluation metric for translations into Czech
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
M
E
T
E
O
R
-R
M
T
E
R
P
O
S
F
4G
-A
M
P
O
S
F
4G
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
English-French News Task
LIMSI 0.2 0.26 0.16 0.34 0.33 0.48 0.44 0.43 9.74
LIUM-SYSTRAN 0.20 0.26 0.16 0.35 0.34 0.49 0.44 0.44 7.38
LIUM-SYSTRAN-CONTRAST 0.20 0.26 0.16 0.35 0.34 0.48 0.44 0.44 7.02
RBMT1 0.13 0.19 0.12 0.28 0.24 0.42 0.37 0.35 5.46
RBMT3 0.17 0.23 0.14 0.31 0.31 0.45 0.4 0.40 5.60
RBMT4 0.19 0.24 0.15 0.33 0.32 0.48 0.43 0.43 6.80
RBMT5 0.17 0.23 0.14 0.32 0.31 0.47 0.42 0.42 6.15
RBMT6 0.16 0.22 0.13 0.32 0.3 0.46 0.40 0.41 5.60
SAAR 0.15 0.22 0.15 0.33 0.28 0.46 0.41 0.42 6.12
SAAR-CONTRAST 0.17 0.23 0.15 0.33 0.30 0.47 0.42 0.41 5.50
UEDIN 0.16 0.23 0.14 0.32 0.32 0.44 0.39 0.38 4.79
XEROX 0.13 0.2 0.12 0.29 0.29 0.41 0.34 0.34 3.91
XEROX-CONTRAST 0.13 0.2 0.12 0.29 0.29 0.41 0.35 0.35 3.86
English-French Europarl Task
LIMSI 0.32 0.36 0.24 0.42 0.44 0.56 0.53 0.53 8.84
LIUM-SYSTRAN 0.32 0.36 0.24 0.42 0.45 0.56 0.53 0.53 7.46
LIUM-SYSTRAN-CONTRAST 0.31 0.36 0.23 0.42 0.44 0.56 0.52 0.53 6.69
RBMT1 0.15 0.20 0.13 0.29 0.26 0.44 0.4 0.37 3.89
RBMT3 0.18 0.24 0.15 0.34 0.33 0.47 0.42 0.43 4.13
RBMT4 0.2 0.25 0.17 0.35 0.35 0.5 0.45 0.45 4.70
RBMT5 0.12 0.16 0.09 0.22 0.06 0.37 0.32 0.32 3.01
RBMT6 0.17 0.23 0.14 0.33 0.32 0.47 0.42 0.42 3.93
SAAR 0.26 0.29 0.21 0.41 0.34 0.53 0.49 0.48 7.75
SAAR-CONTRAST 0.28 0.32 0.23 0.41 0.39 0.55 0.51 0.52 6.45
UCL 0.24 0.28 0.19 0.37 0.41 0.49 0.44 0.42 4.16
UEDIN 0.30 0.35 0.23 0.42 0.43 0.54 0.51 0.51 6.56
Table 15: Automatic evaluation metric for translations into French
89
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
English-German News Task
LIMSI 0.11 0.18 0.19 0.45 0.22 0.36 0.29 0.28 7.83
LIU 0.10 0.17 0.18 0.44 0.24 0.36 0.28 0.27 4.03
RBMT1 0.12 0.18 0.18 0.44 0.22 0.39 0.33 0.32 5.42
RBMT2 0.13 0.19 0.20 0.46 0.24 0.4 0.33 0.33 5.76
RBMT3 0.12 0.18 0.19 0.44 0.24 0.39 0.32 0.32 4.70
RBMT4 0.14 0.19 0.2 0.46 0.25 0.41 0.35 0.34 5.58
RBMT5 0.11 0.17 0.17 0.43 0.21 0.38 0.31 0.31 4.49
RBMT6 0.10 0.16 0.17 0.43 0.2 0.37 0.3 0.29 4.81
SAAR 0.13 0.19 0.19 0.44 0.27 0.38 0.31 0.3 4.04
SAAR-CONTRAST 0.12 0.18 0.18 0.43 0.26 0.37 0.3 0.28 3.71
UEDIN 0.12 0.17 0.18 0.45 0.23 0.37 0.30 0.29 4.37
English-German Europarl Task
CMU-GIMPEL 0.20 0.24 0.27 0.54 0.32 0.43 0.37 0.37 9.54
LIMSI 0.20 0.24 0.27 0.53 0.32 0.43 0.37 0.37 6.97
LIU 0.2 0.24 0.27 0.53 0.32 0.43 0.38 0.37 6.95
RBMT1 0.11 0.16 0.16 0.42 0.19 0.38 0.32 0.32 5.01
RBMT2 0.12 0.17 0.19 0.46 0.21 0.39 0.32 0.31 5.93
RBMT3 0.11 0.16 0.17 0.43 0.21 0.38 0.31 0.30 4.75
RBMT4 0.12 0.17 0.18 0.45 0.22 0.41 0.34 0.33 5.42
RBMT5 0.1 0.14 0.16 0.42 0.19 0.39 0.32 0.31 4.42
RBMT6 0.09 0.14 0.15 0.42 0.18 0.38 0.30 0.29 4.40
SAAR 0.20 0.25 0.26 0.53 0.32 0.43 0.38 0.37 6.67
SAAR-CONTRAST 0.2 0.24 0.26 0.52 0.31 0.43 0.37 0.37 6.35
UCL 0.16 0.20 0.23 0.49 0.31 0.4 0.33 0.31 5.12
UEDIN 0.21 0.25 0.27 0.54 0.32 0.44 0.38 0.38 7.02
English-Spanish News Task
CMU-SMT 0.19 0.24 0.25 0.34 0.32 0.32 0.25 0.26 8.34
LIMSI 0.19 0.25 0.26 0.34 0.34 0.33 0.26 0.26 5.92
RBMT1 0.16 0.22 0.23 0.32 0.30 0.31 0.23 0.23 5.36
RBMT3 0.19 0.24 0.25 0.33 0.34 0.33 0.26 0.26 5.42
RBMT4 0.21 0.26 0.26 0.34 0.35 0.34 0.28 0.28 6.36
RBMT5 0.18 0.24 0.25 0.33 0.32 0.33 0.26 0.26 5.84
RBMT6 0.19 0.24 0.24 0.33 0.33 0.32 0.25 0.26 5.42
SAAR 0.20 0.27 0.26 0.34 0.37 0.34 0.28 0.28 5.04
SAAR-CONTRAST 0.2 0.26 0.25 0.34 0.37 0.34 0.27 0.27 4.86
UCB 0.20 0.26 0.26 0.34 0.34 0.33 0.26 0.27 5.70
UEDIN 0.18 0.25 0.25 0.33 0.35 0.33 0.26 0.26 4.30
UPC 0.18 0.23 0.24 0.32 0.35 0.32 0.25 0.24 3.97
English-Spanish Europarl Task
CMU-SMT 0.32 0.36 0.33 0.42 0.45 0.40 0.35 0.36 0.10
LIMSI 0.31 0.36 0.33 0.42 0.45 0.4 0.35 0.35 7.80
RBMT1 0.16 0.22 0.24 0.32 0.31 0.32 0.25 0.25 4.47
RBMT3 0.20 0.25 0.25 0.34 0.35 0.33 0.27 0.27 4.66
RBMT4 0.21 0.25 0.26 0.34 0.36 0.34 0.28 0.28 4.85
RBMT5 0.18 0.24 0.25 0.34 0.33 0.34 0.27 0.27 5.03
RBMT6 0.18 0.23 0.25 0.33 0.33 0.33 0.26 0.26 4.57
SAAR 0.31 0.35 0.33 0.41 0.44 0.40 0.35 0.35 7.59
SAAR-CONTRAST 0.30 0.34 0.33 0.41 0.44 0.4 0.34 0.35 7.42
UCL 0.25 0.29 0.29 0.37 0.43 0.36 0.29 0.29 4.67
UEDIN 0.32 0.36 0.33 0.42 0.45 0.40 0.35 0.35 7.25
UPC 0.30 0.34 0.32 0.40 0.46 0.4 0.35 0.34 6.18
UW 0.32 0.36 0.33 0.42 0.45 0.40 0.35 0.35 7.36
UW-CONTRAST 0.32 0.35 0.33 0.42 0.45 0.40 0.35 0.36 7.21
Table 16: Automatic evaluation metric for translations into German and Spanish
90
D
P
D
R
S
R
U
L
C
U
L
C
H
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
Spanish-English Europarl Task
CMU-SMT 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.48 9.72
CUED 0.33 0.43 0.25 0.29 0.33 0.32 0.38 0.59 0.48 0.50 0.51 0.47 0.47 7.41
CUED-CONTRAST 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.48 7.00
DCU 0.34 0.43 0.25 0.29 0.33 0.32 0.38 0.59 0.48 0.50 0.51 0.47 0.48 6.78
LIMSI 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.48 6.73
RBMT3 0.26 0.37 0.19 0.22 0.27 0.19 0.26 0.51 0.41 0.36 0.45 0.4 0.39 5.46
RBMT4 0.26 0.37 0.19 0.22 0.27 0.18 0.26 0.52 0.42 0.36 0.45 0.39 0.38 5.57
RBMT5 0.25 0.36 0.18 0.22 0.27 0.18 0.25 0.51 0.41 0.36 0.44 0.39 0.38 4.74
RBMT6 0.24 0.34 0.18 0.21 0.26 0.17 0.25 0.51 0.41 0.36 0.44 0.38 0.37 4.71
SAAR 0.34 0.44 0.26 0.29 0.33 0.32 0.39 0.59 0.48 0.51 0.52 0.49 0.48 6.30
SAAR-CONTRAST 0.33 0.43 0.25 0.28 0.33 0.30 0.37 0.59 0.48 0.47 0.51 0.47 0.46 7.33
UCL 0.29 0.4 0.21 0.25 0.29 0.25 0.32 0.55 0.43 0.47 0.47 0.42 0.4 4.02
UEDIN 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.50 0.52 0.48 0.48 6.61
UPC 0.33 0.43 0.25 0.28 0.33 0.32 0.38 0.59 0.48 0.5 0.52 0.48 0.48 6.82
French-English News Task
BBN-COMBO 0.27 0.37 0.2 0.23 0.28 0.21 n/a n/a n/a n/a n/a n/a n/a n/a
CMU-COMBO 0.26 0.36 0.18 0.22 0.27 0.19 n/a n/a n/a n/a n/a n/a n/a n/a
CMU-COMBO-CONTRAST n/a n/a n/a n/a n/a 0.19 n/a n/a n/a n/a n/a n/a n/a n/a
CMU-STATXFER 0.21 0.32 0.14 0.19 0.23 0.14 0.22 0.48 0.39 0.28 0.38 0.32 0.30 9.91
CMU-STATXFER-CONTRAST 0.21 0.30 0.14 0.18 0.23 0.14 0.21 0.47 0.38 0.26 0.38 0.31 0.29 6.47
CUED 0.25 0.35 0.17 0.21 0.26 0.18 0.27 0.51 0.41 0.37 0.41 0.35 0.34 6.34
CUED-CONTRAST 0.26 0.37 0.18 0.22 0.27 0.19 0.28 0.52 0.42 0.38 0.42 0.37 0.36 6.29
LIMSI 0.26 0.37 0.18 0.22 0.27 0.20 0.28 0.51 0.40 0.40 0.43 0.38 0.37 5.75
LIUM-SYSTRAN 0.27 0.38 0.19 0.23 0.27 0.21 0.29 0.51 0.41 0.41 0.44 0.39 0.38 6.32
LIUM-SYSTRAN-CONTRAST 0.27 0.38 0.19 0.23 0.28 0.21 0.29 0.51 0.41 0.41 0.44 0.39 0.38 5.93
RBMT3 0.24 0.36 0.17 0.21 0.26 0.16 0.24 0.49 0.40 0.29 0.42 0.36 0.34 7.61
RBMT4 0.25 0.37 0.17 0.21 0.26 0.17 0.25 0.49 0.4 0.33 0.42 0.36 0.35 6.17
RBMT5 0.25 0.37 0.18 0.22 0.27 0.18 0.25 0.51 0.41 0.33 0.43 0.37 0.36 6.97
RBMT6 0.24 0.36 0.17 0.21 0.26 0.16 0.24 0.49 0.39 0.30 0.41 0.35 0.34 6.51
SAAR 0.24 0.14 0.17 0.19 0.22 0.15 0.24 0.47 0.37 0.39 0.39 0.32 0.31 3.22
SAAR-CONTRAST 0.26 0.36 0.18 0.22 0.27 0.17 0.27 0.51 0.41 0.36 0.41 0.35 0.35 6.01
UEDIN 0.25 0.36 0.17 0.21 0.26 0.18 0.26 0.51 0.41 0.35 0.42 0.36 0.35 5.97
UEDIN-COMBO 0.26 0.36 0.18 0.23 0.27 n/a n/a n/a n/a n/a n/a n/a n/a n/a
French-English Europarl Task
CMU-STATXFER 0.24 0.34 0.18 0.22 0.26 0.2 0.26 0.52 0.42 0.37 0.42 0.36 0.35 9.85
CMU-STATXFER-CONTRAST 0.25 0.34 0.19 0.22 0.26 0.2 0.26 0.53 0.42 0.38 0.42 0.36 0.35 7.10
CUED 0.34 0.44 0.26 0.29 0.33 0.32 0.38 0.59 0.48 0.50 0.51 0.47 0.47 0.11
CUED-CONTRAST 0.34 0.44 0.26 0.29 0.34 0.32 0.39 0.59 0.48 0.51 0.51 0.47 0.47 9.34
DCU 0.33 0.43 0.25 0.28 0.33 0.31 0.37 0.58 0.47 0.49 0.50 0.46 0.46 9.16
LIMSI 0.34 0.44 0.26 0.29 0.34 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.48 9.59
LIUM-SYSTRAN 0.35 0.45 0.27 0.3 0.34 0.33 0.39 0.59 0.48 0.51 0.52 0.48 0.49 9.75
LIUM-SYSTRAN-CONTRAST 0.34 0.44 0.26 0.29 0.34 0.33 0.39 0.59 0.48 0.50 0.52 0.48 0.48 9.23
RBMT3 0.25 0.36 0.10 0.20 0.24 0.17 0.25 0.51 0.41 0.35 0.43 0.37 0.36 7.36
RBMT4 0.27 0.36 0.19 0.22 0.27 0.18 0.26 0.51 0.41 0.37 0.43 0.38 0.37 5.92
RBMT5 0.27 0.38 0.21 0.23 0.28 0.20 0.28 0.53 0.43 0.4 0.45 0.4 0.39 7.20
RBMT6 0.24 0.35 0.18 0.21 0.26 0.16 0.24 0.5 0.40 0.35 0.42 0.36 0.35 5.96
SAAR 0.32 0.41 0.23 0.27 0.31 0.27 0.33 0.54 0.43 0.49 0.49 0.44 0.41 4.76
SAAR-CONTRAST 0.33 0.43 0.25 0.28 0.33 0.3 0.36 0.58 0.48 0.47 0.51 0.47 0.46 0.10
SYSTRAN 0.3 0.4 0.23 0.26 0.30 0.26 0.34 0.55 0.45 0.46 0.48 0.43 0.43 7.01
UCL 0.3 0.40 0.22 0.26 0.3 0.26 0.32 0.55 0.44 0.47 0.47 0.42 0.41 6.35
UEDIN 0.34 0.44 0.26 0.29 0.33 0.33 0.39 0.59 0.48 0.50 0.52 0.48 0.48 9.41
Table 17: Automatic evaluation metric for translations into English
91
D
P
D
R
S
R
U
L
C
U
L
C
H
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
Czech-English News Commentary Task
DCU 0.25 0.34 0.18 0.22 0.27 0.21 0.29 0.54 0.44 0.42 0.42 0.36 0.36 2.45
SYSTRAN 0.19 0.28 0.12 0.17 0.21 0.15 0.23 0.45 0.36 0.34 0.36 0.29 0.29 0.76
UEDIN 0.24 0.31 0.16 0.21 0.25 0.22 0.30 0.54 0.44 0.43 0.41 0.35 0.35 1.37
UMD 0.26 0.34 0.19 0.23 0.28 0.24 0.33 0.56 0.45 0.49 0.44 0.39 0.38 1.41
Czech-English News Task
DCU 0.19 0.30 0.13 0.17 0.22 0.12 0.22 0.45 0.35 0.32 0.36 0.28 0.28 1.78
UEDIN 0.19 0.28 0.12 0.17 0.21 0.12 0.21 0.44 0.34 0.32 0.35 0.27 0.27 0.65
UMD 0.2 0.29 0.12 0.18 0.22 0.13 0.22 0.44 0.34 0.36 0.36 0.29 0.27 0.52
German-English News Task
BBN-COMBO 0.23 0.34 0.14 0.21 0.25 0.18 n/a n/a n/a n/a n/a n/a n/a n/a
CMU-STATXFER 0.16 0.27 0.09 0.15 0.19 0.11 0.18 0.43 0.34 0.25 0.33 0.25 0.24 7.84
LIMSI 0.22 0.33 0.13 0.19 0.23 0.17 0.25 0.47 0.37 0.36 0.4 0.33 0.32 5.58
LIU 0.21 0.32 0.06 0.18 0.22 0.15 0.24 0.48 0.38 0.33 0.38 0.31 0.31 5.51
RBMT1 0.22 0.33 0.14 0.19 0.23 0.14 0.22 0.44 0.35 0.28 0.37 0.31 0.30 6.13
RBMT2 0.24 0.37 0.17 0.21 0.26 0.15 0.24 0.5 0.40 0.31 0.4 0.33 0.32 7.14
RBMT3 0.24 0.37 0.16 0.21 0.26 0.16 0.24 0.49 0.4 0.32 0.41 0.34 0.34 6.97
RBMT4 0.25 0.38 0.17 0.21 0.27 0.16 0.25 0.50 0.40 0.34 0.41 0.35 0.34 7.03
RBMT5 0.23 0.36 0.15 0.20 0.25 0.15 0.23 0.48 0.39 0.32 0.4 0.33 0.32 5.94
RBMT6 0.22 0.34 0.14 0.19 0.24 0.14 0.22 0.47 0.38 0.31 0.39 0.32 0.31 5.65
SAAR 0.22 0.33 0.14 0.2 0.24 0.15 0.24 0.47 0.37 0.36 0.39 0.32 0.31 4.67
SAAR-CONTRAST 0.24 0.35 0.16 0.21 0.25 0.17 0.26 0.5 0.4 0.36 0.4 0.33 0.33 5.80
SAAR-CONTRAST-2 0.21 0.33 0.14 0.19 0.23 0.15 0.24 0.47 0.37 0.36 0.39 0.32 0.31 4.80
UEDIN 0.23 0.34 0.09 0.19 0.23 0.16 0.25 0.48 0.39 0.35 0.4 0.33 0.33 5.72
German-English Europarl Task
CMU-STATXFER 0.2 0.31 0.12 0.19 0.22 0.17 0.23 0.49 0.39 0.34 0.39 0.32 0.31 7.11
LIMSI 0.28 0.38 0.18 0.24 0.28 0.27 0.33 0.55 0.44 0.43 0.47 0.42 0.42 8.04
LIU 0.28 0.39 0.09 0.23 0.26 0.27 0.33 0.55 0.44 0.44 0.47 0.43 0.43 7.46
RBMT1 0.21 0.3 0.14 0.18 0.22 0.12 0.19 0.42 0.33 0.27 0.36 0.30 0.28 4.61
RBMT2 0.24 0.35 0.16 0.20 0.25 0.14 0.23 0.49 0.39 0.32 0.39 0.33 0.32 5.42
RBMT3 0.24 0.35 0.16 0.20 0.25 0.15 0.23 0.48 0.39 0.32 0.40 0.34 0.33 5.43
RBMT4 0.24 0.36 0.15 0.20 0.25 0.14 0.23 0.49 0.39 0.34 0.41 0.34 0.34 5.11
RBMT5 0.23 0.34 0.15 0.2 0.24 0.14 0.22 0.48 0.38 0.33 0.4 0.33 0.32 4.55
RBMT6 0.22 0.33 0.13 0.18 0.23 0.13 0.21 0.47 0.37 0.31 0.38 0.31 0.31 4.08
SAAR 0.29 0.39 0.19 0.25 0.28 0.27 0.33 0.55 0.44 0.43 0.47 0.42 0.42 7.32
SAAR-CONTRAST 0.28 0.37 0.18 0.24 0.28 0.26 0.32 0.54 0.43 0.43 0.47 0.42 0.42 6.77
UCL 0.24 0.36 0.16 0.22 0.25 0.2 0.25 0.49 0.39 0.41 0.42 0.35 0.32 4.26
UEDIN 0.30 0.41 0.20 0.26 0.3 0.28 0.34 0.56 0.45 0.45 0.48 0.44 0.44 7.96
Spanish-English News Task
CMU-SMT 0.24 0.35 0.17 0.21 0.25 0.18 0.26 0.48 0.38 0.39 0.41 0.35 0.34 8.00
CUED 0.25 0.36 0.17 0.21 0.26 0.19 0.28 0.50 0.40 0.38 0.42 0.36 0.36 6.03
CUED-CONTRAST 0.26 0.37 0.18 0.22 0.27 0.21 0.3 0.52 0.42 0.39 0.44 0.38 0.38 6.27
LIMSI 0.26 0.37 0.18 0.22 0.27 0.20 0.28 0.50 0.4 0.41 0.43 0.38 0.37 4.93
RBMT3 0.25 0.38 0.17 0.22 0.27 0.18 0.26 0.50 0.41 0.32 0.43 0.38 0.36 7.54
RBMT4 0.26 0.38 0.18 0.22 0.27 0.18 0.26 0.51 0.42 0.32 0.44 0.39 0.37 7.81
RBMT5 0.26 0.38 0.08 0.20 0.25 0.2 0.27 0.51 0.42 0.33 0.44 0.38 0.37 6.89
RBMT6 0.25 0.36 0.17 0.21 0.26 0.18 0.25 0.51 0.41 0.33 0.43 0.37 0.36 6.83
SAAR 0.26 0.37 0.19 0.22 0.27 0.19 0.29 0.51 0.41 0.39 0.43 0.37 0.37 5.23
SAAR-CONTRAST 0.26 0.37 0.18 0.22 0.27 0.19 0.28 0.51 0.41 0.37 0.42 0.37 0.36 5.95
UCB 0.25 0.35 0.17 0.21 0.26 0.19 0.27 0.5 0.39 0.39 0.42 0.36 0.35 4.40
UEDIN 0.24 0.35 0.17 0.21 0.26 0.18 0.27 0.50 0.40 0.36 0.41 0.35 0.34 5.07
UEDIN-COMBO 0.27 0.36 0.19 0.23 0.27 n/a n/a n/a n/a n/a n/a n/a n/a n/a
UPC 0.25 0.36 0.17 0.21 0.26 0.19 0.26 0.49 0.39 0.4 0.43 0.37 0.36 4.38
Table 18: Automatic evaluation metric for translations into English
92
B Break down of correlation for each task
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
D
P
D
R
S
R
U
L
C
U
L
C
H
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
All-English News Task
RANK 1 n/a n/a 0.83 0.73 0.83 0.83 0.87 0.71 0.7 0.82 0.79 0.41 0.79 0.8 0.80 0.25
French-English News Task
RANK 1 0.69 0.63 0.92 0.83 0.89 0.90 0.90 0.81 0.80 0.88 0.80 0.57 0.87 0.9 0.9 ?
0.21
CONST ? 1 0.81 0.83 0.52 0.81 0.86 0.81 0.93 0.9 0.76 0.64 0.73 0.69 0.72 0.85 ?
0.52
YES/NO ? ? 1 0.71 0.57 0.76 0.77 0.74 0.79 0.75 0.67 0.59 0.62 0.66 0.67 0.79 ?
0.26
French-English Europarl Task
RANK 1 0.95 0.9 0.94 0.95 0.93 0.95 0.93 0.92 0.90 0.88 0.87 0.92 0.94 0.94 0.91 0.50
CONST ? 1 0.91 0.97 0.97 0.98 0.98 0.97 0.97 0.96 0.97 0.95 0.96 0.97 0.97 0.96 0.56
YES/NO ? ? 1 0.94 0.94 0.94 0.96 0.96 0.96 0.97 0.92 0.93 0.92 0.95 0.95 0.97 0.47
German-English News Task
RANK 1 0.56 0.56 0.85 0.93 0.92 0.85 0.95 0.12 0.09 0.83 0.89 ?
0.11
0.63 0.60 0.58 0.36
CONST ? 1 0.48 0.54 0.48 0.59 0.66 0.57 0.64 0.65 0.61 0.55 0.51 0.57 0.63 0.56 ?
0.02
YES/NO ? ? 1 0.68 0.61 0.69 0.73 0.67 0.60 0.41 0.54 0.56 0.33 0.79 0.83 0.70 0.08
German-English Europarl Task
RANK 1 0.63 0.81 0.76 0.59 0.46 0.57 0.60 0.30 0.39 0.40 0.66 0.25 0.53 0.53 0.64 0.35
CONST ? 1 0.78 0.87 0.92 0.51 0.83 0.86 0.69 0.69 0.76 0.80 0.69 0.88 0.88 0.88 0.61
YES/NO ? ? 1 0.88 0.77 0.48 0.77 0.78 0.66 0.67 0.64 0.86 0.58 0.74 0.74 0.85 0.78
Spanish-English News Task
RANK 1 ?
0.07
0.44 0.75 0.76 0.68 0.71 0.81 0.19 0.01 0.66 0.63 ?
0.12
0.73 0.76 0.66 0.36
CONST ? 1 0.66 ?
0.03
?
0.44
0.29 0.29 0.14 0.45 0.66 ?
0.11
?
0.33
0.77 ?
0.37
?
0.34
0.16 ?
0.58
YES/NO ? ? 1 0.29 0.05 0.73 0.64 0.55 0.48 0.47 0.09 ?
0.11
0.71 0.06 0.1 0.39 ?
0.43
Spanish-English Europarl Task
RANK 1 0.69 0.76 0.78 0.73 0.73 0.8 0.77 0.78 0.79 0.83 0.84 0.77 0.73 0.73 0.80 0.87
CONST ? 1 0.68 0.76 0.77 0.75 0.69 0.73 0.64 0.67 0.64 0.68 0.73 0.78 0.78 0.73 0.56
YES/NO ? ? 1 0.94 0.93 0.95 0.96 0.95 0.98 0.97 0.91 0.91 0.95 0.94 0.94 0.98 0.69
Table 19: Correlation of automatic evaluation metrics with the three types of human judgments for transla-
tion into English
93
R
A
N
K
C
O
N
S
T
Y
E
S
/N
O
B
L
E
U
M
B
L
E
U
M
E
T
E
O
R
-B
A
S
E
L
IN
E
M
E
T
E
O
R
-R
A
N
K
IN
G
M
T
E
R
P
O
S
F
4G
R
A
M
-A
M
P
O
S
F
4G
R
A
M
-G
M
P
O
S
B
L
E
U
S
V
M
-R
A
N
K
English-French News Task
RANK 1 0.55 0.48 0.73 0.62 0.3 0.47 0.56 0.69 0.69 0.66 0.72
CONST ? 1 0.35 0.49 0.47 0.39 0.49 0.24 0.59 0.59 0.58 0.45
YES/NO ? ? 1 0.81 0.92 0.71 0.73 0.78 0.73 0.73 0.76 0.76
English-French Europarl Task
RANK 1 0.98 0.88 0.95 0.95 0.95 0.95 0.90 0.97 0.97 0.93 0.93
CONST ? 1 0.94 0.98 0.98 0.98 0.98 0.93 1 1 0.97 0.91
YES/NO ? ? 1 0.97 0.97 0.97 0.97 0.92 0.95 0.95 0.92 0.83
English-German News Task
RANK 1 0.57 0.71 0.58 0.42 0.43 0.13 0.25 0.90 0.90 0.90 0.32
CONST ? 1 0.78 0.75 0.83 0.82 0.55 0.60 0.72 0.72 0.72 0.58
YES/NO ? ? 1 0.62 0.54 0.51 0.36 0.23 0.75 0.75 0.75 0.76
English-German Europarl Task
RANK 1 0.28 0.57 0.36 0.36 0.42 0.39 0.26 0.38 0.38 0.50 0.56
CONST ? 1 0.87 0.88 0.88 0.91 0.90 0.93 0.88 0.88 0.80 0.85
YES/NO ? ? 1 0.89 0.89 0.96 0.96 0.84 0.86 0.86 0.87 0.98
English-Spanish News Task
RANK 1 ?
0.30
0.49 ?
0.04
?
0.47
?
0.25
?
0.29
?
0.33
?
0.19
?
0.19
?
0.07
0.02
CONST ? 1 0.43 0.79 0.61 0.64 0.56 0.2 0.59 0.59 0.55 0.56
YES/NO ? ? 1 0.55 0.41 0.43 0.31 0.13 0.65 0.65 0.72 0.16
English-Spanish Europarl Task
RANK 1 0.90 0.63 0.8 0.83 0.84 0.83 0.73 0.79 0.79 0.76 0.80
CONST ? 1 0.73 0.84 0.86 0.81 0.8 0.74 0.84 0.83 0.84 0.86
YES/NO ? ? 1 0.68 0.75 0.66 0.67 0.90 0.67 0.66 0.73 0.68
Table 20: Correlation of automatic evaluation metrics with the three types of human judgments for transla-
tion into other languages
94
C Pairwise system comparisons by human judges
The following tables show pairwise comparisons between systems for each language pair, test set, and
manual evaluation type. The numbers in each of the tables? cells indicate the percent of that the system in
that column was judged to be better than the system in that row. Bolding indicates the winner of the two
systems. The difference between 100 and the sum of the complimentary cells is the percent of time that the
two systems were judged to be equal.
Because there were so many systems and data conditions the significance of each pairwise comparison
needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine differ-
ences (rather than differences that are attributable to chance). In the following tables ? indicates statistical
significance at p <= 0.05 and ? indicates statistical significance at p <= 0.01, according to the Sign Test.
B
B
N
-C
M
B
C
M
U
-C
M
B
C
M
U
-X
F
R
C
U
E
D
C
U
E
D
-C
L
IM
S
I
L
IU
M
-S
Y
S
L
IU
M
-S
Y
S
-C
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
U
E
D
IN
-C
M
B
BBN-CMB 0.32 0.18? 0.21 0.42 0.37 0.29 0.24 0.33 0.48 0.48 0.32 0.29 0.44 0.48 0.21
CMU-CMB 0.50 0.26 0.29 0.42 0.4 0.44 0.48 0.49 0.38 0.45 0.55 0.32 0.34 0.34 0.46
CMU-XFR 0.67? 0.44 0.60? 0.75? 0.58 0.73? 0.62 0.59 0.54 0.77? 0.48 0.54 0.65? 0.71? 0.58
CUED 0.46 0.41 0.20? 0.47 0.56 0.47 0.51? 0.41 0.54 0.57 0.37 0.43 0.61 0.39 0.15
CUED-C 0.27 0.22 0.08? 0.20 0.31 0.54 0.52? 0.32 0.52 0.50 0.31 0.40 0.38 0.30 0.52
LIMSI 0.34 0.4 0.29 0.31 0.41 0.23? 0.52 0.38 0.50 0.39 0.49 0.42 0.32 0.26 0.30
LIUM-SYS 0.37 0.32 0.13? 0.39 0.27 0.60? 0.24 0.44 0.46 0.46 0.33 0.24? 0.25 0.30 0.19
LI-SYS-C 0.40 0.26 0.24 0.20? 0.13? 0.30 0.24 0.44 0.42 0.43 0.35 0.21? 0.30 0.30 0.31
RBMT3 0.46 0.43 0.26 0.38 0.46 0.48 0.39 0.39 0.41 0.44 0.26 0.36 0.50 0.68? 0.44
RBMT4 0.36 0.33 0.31 0.36 0.39 0.35 0.50 0.45 0.45 0.49 0.40 0.35 0.57 0.51 0.53
RBMT5 0.37 0.33 0.12? 0.32 0.33 0.33 0.39 0.46 0.25 0.22 0.21 0.37 0.44 0.49 0.57
RBMT6 0.50 0.33 0.37 0.34 0.50 0.39 0.44 0.50 0.48 0.37 0.55 0.42 0.48 0.41 0.41
SAAR 0.50 0.46 0.37 0.38 0.44 0.52 0.6? 0.54? 0.44 0.53 0.44 0.29 0.34 0.52 0.50
SAAR-C 0.31 0.47 0.23? 0.30 0.24 0.51 0.50 0.47 0.25 0.31 0.33 0.35 0.26 0.47 0.38
UED 0.35 0.37 0.13? 0.39 0.55 0.50 0.50 0.43 0.24? 0.37 0.36 0.41 0.31 0.47 0.36
UED-CMB 0.57 0.36 0.16 0.46 0.38 0.30 0.63 0.39 0.39 0.37 0.35 0.53 0.27 0.48 0.36
> OTHERS 0.43 0.37 0.22 0.34 0.41 0.44 0.45 0.45 0.4 0.42 0.47 0.37 0.34 0.43 0.44 0.42
? OTHERS 0.66 0.59 0.38 0.55 0.64 0.63 0.66 0.69 0.58 0.58 0.65 0.57 0.54 0.64 0.61 0.61
Table 21: Sentence-level ranking for the French-English News Task.
C
M
U
-X
F
R
C
U
E
D
D
C
U
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
S
Y
S
T
R
A
N
U
C
L
U
E
D
IN
CMU-XFR 0.53 0.50 0.74? 0.79? 0.55 0.46 0.50 0.36 0.73 0.92? 0.36 0.44 0.77?
CUED 0.29 0.42 0.29 0.48 0.16? 0.53 0.16? 0.18? 0.18? 0.55 0.06? 0.21 0.38
DCU 0.46 0.29 0.38 0.47 0.37 0.27 0.24 0.29 0.35 0.55 0.18 0.25 0.50
LIMSI 0.11? 0.21 0.44 0.11 0.12? 0.17 0.29 0.05? 0.30 0.32 0.19 0.29 0.33
LIUM-SYS 0.14? 0.16 0.24 0.32 0.06? 0.13 0.22 0.12? 0.14? 0.33 0.20? 0.26 0.32
RBMT3 0.36 0.79? 0.58 0.88? 0.72? 0.40 0.57 0.21 0.67 0.72? 0.50 0.54 0.67
RBMT4 0.50 0.40 0.64 0.67 0.56 0.40 0.42 0.21? 0.52 0.67 0.33 0.47 0.75
RBMT5 0.38 0.79? 0.60 0.57 0.56 0.24 0.42 0.26 0.48 0.72? 0.50 0.46 0.60
RBMT6 0.54 0.79? 0.67 0.77? 0.82? 0.47 0.79? 0.53 0.71? 0.83? 0.56 0.47 0.77?
SAAR 0.27 0.59? 0.57 0.47 0.71? 0.22 0.29 0.48 0.18? 0.50 0.35 0.23 0.50
SAAR-C 0.04? 0.15 0.31 0.39 0.48 0.14? 0.24 0.21? 0.08? 0.21 0.17? 0.20 0.57
SYSTRAN 0.50 0.81? 0.65 0.52 0.64? 0.38 0.62 0.33 0.32 0.41 0.71? 0.56 0.55
UCL 0.31 0.64 0.56 0.57 0.47 0.46 0.40 0.39 0.27 0.55 0.60 0.44 0.47
UED 0.24? 0.43 0.35 0.33 0.42 0.28 0.25 0.33 0.15? 0.29 0.26 0.25 0.27
> OTHERS 0.32 0.50 0.5 0.54 0.55 0.28 0.4 0.35 0.21 0.41 0.59 0.32 0.35 0.55
? OTHERS 0.42 0.7 0.64 0.78 0.79 0.40 0.50 0.48 0.32 0.58 0.75 0.47 0.52 0.71
Table 22: Sentence-level ranking for the French-English Europarl Task.
95
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
X
E
R
O
X
LIMSI 0.29 0.25 0.60? 0.52 0.48 0.13 0.30 0.13? 0.17?
LIUM-SYSTRAN 0.36 0.41 0.51 0.41 0.53 0.22 0.26 0.27 0.04?
RBMT3 0.56 0.34 0.48 0.52 0.40 0.31 0.53 0.37 0.11?
RBMT4 0.13? 0.36 0.31 0.29 0.19? 0.26 0.15? 0.17? 0.09?
RBMT5 0.33 0.35 0.29 0.42 0.26 0.17? 0.32 0.17? 0.12?
RBMT6 0.42 0.38 0.37 0.43? 0.44 0.32 0.32 0.28 0.11?
SAAR 0.56 0.52 0.51 0.56 0.69? 0.41 0.33 0.46 0.3
SAAR-CONTRAST 0.55 0.44 0.33 0.63? 0.56 0.46 0.21 0.41 0.22?
UEDIN 0.48? 0.48 0.41 0.60? 0.65? 0.53 0.41 0.43 0.09?
XEROX 0.63? 0.74? 0.78? 0.74? 0.71? 0.75? 0.44 0.64? 0.63?
> OTHERS 0.44 0.43 0.41 0.54 0.53 0.43 0.28 0.37 0.32 0.13
? OTHERS 0.67 0.66 0.60 0.78 0.73 0.66 0.51 0.57 0.55 0.32
Table 23: Sentence-level ranking for the English-French News Task.
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
C
L
U
E
D
IN
LIMSI 0.23 0.21? 0.32 0.10? 0.15? 0.35 0.27 0.15? 0.17
LIUM-SYSTRAN 0.28 0.39 0.11? 0.21? 0.22 0.40 0.19? 0.15
RBMT3 0.75? 0.59 0.38 0.39 0.49 0.70? 0.81? 0.47 0.81?
RBMT4 0.64 0.36 0.28 0.24? 0.18 0.61 0.48 0.42 0.50
RBMT5 0.85? 0.89? 0.49 0.62? 0.67? 0.78? 0.91? 0.63? 0.93?
RBMT6 0.85? 0.62? 0.26 0.42 0.24? 0.83? 0.82? 0.47 0.68?
SAAR 0.41 0.52 0.17? 0.30 0.11? 0.06? 0.41 0.11? 0.41
SAAR-CONTRAST 0.47 0.40 0.11? 0.26 0.03? 0.06? 0.32 0.27 0.26
UCL 0.80? 0.70? 0.42 0.47 0.22? 0.44 0.71? 0.61 0.78?
UEDIN 0.46 0.41 0.11? 0.33 0.04? 0.15? 0.32 0.36 0.03?
> OTHERS 0.62 0.54 0.26 0.4 0.17 0.27 0.56 0.6 0.32 0.54
? OTHERS 0.79 0.78 0.42 0.61 0.26 0.44 0.74 0.79 0.44 0.77
Table 24: Sentence-level ranking for the English-French Europarl Task.
B
B
N
-C
M
B
C
M
U
-X
F
R
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
U
E
D
IN
-C
M
B
BBN-COMBO 0.1? 0.22 0.37 0.62? 0.69? 0.74? 0.66? 0.41 0.63? 0.60? 0.35 0.40
CMU-STATXFER 0.71? 0.44 0.54 0.76? 0.79? 0.73? 0.74? 0.80? 0.62? 0.65? 0.54? 0.37
LIMSI 0.44 0.24 0.41 0.67? 0.65? 0.69? 0.54 0.50 0.50 0.63 0.38 0.22
LIU 0.37 0.27 0.34 0.55? 0.56 0.61? 0.50 0.45 0.48 0.56 0.32 0.34
RBMT2 0.21? 0.14? 0.31? 0.20? 0.27 0.43 0.29 0.34 0.30 0.13? 0.25? 0.24?
RBMT3 0.18? 0.13? 0.19? 0.27 0.56 0.37 0.33 0.32 0.29 0.29 0.19? 0.17?
RBMT4 0.22? 0.12? 0.17? 0.18? 0.46 0.51 0.3 0.31 0.18? 0.26? 0.28 0.17?
RBMT5 0.22? 0.12? 0.32 0.36 0.58 0.51 0.40 0.29 0.23? 0.37 0.3 0.28
RBMT6 0.55 0.08? 0.40 0.4 0.51 0.51 0.47 0.51 0.49 0.52 0.22? 0.43
SAAR 0.23? 0.21? 0.40 0.39 0.52 0.50 0.61? 0.53? 0.38 0.50? 0.26? 0.13?
SAAR-CONTRAST 0.23? 0.19? 0.3 0.37 0.71? 0.37 0.60? 0.37 0.33 0.17? 0.48 0.13?
UEDIN 0.23 0.13? 0.38 0.3 0.68? 0.65? 0.55 0.59 0.64? 0.67? 0.38 0.42
UEDIN-COMBO 0.35 0.41 0.59 0.50 0.72? 0.66? 0.83? 0.56 0.52 0.50? 0.67? 0.38
> OTHERS 0.32 0.17 0.34 0.35 0.61 0.56 0.57 0.49 0.45 0.41 0.46 0.33 0.28
? OTHERS 0.51 0.35 0.52 0.56 0.74 0.73 0.73 0.67 0.59 0.61 0.65 0.55 0.44
Table 25: Sentence-level ranking for the German-English News Task.
96
C
M
U
-X
F
R
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
CMU-STATXFER 0.57? 0.77? 0.53 0.71? 0.69? 0.50 0.58 0.82? 0.46 0.75?
LIMSI 0.17? 0.35 0.71? 0.63 0.76? 0.50 0.59 0.52 0.23 0.67?
LIU 0.14? 0.35 0.50 0.29 0.67 0.3 0.42 0.35 0.27 0.57
RBMT2 0.27 0.24? 0.46 0.39 0.33 0.36 0.42 0.50 0.33 0.46
RBMT3 0.23? 0.3 0.57 0.45 0.40 0.31 0.38 0.56 0.32 0.55
RBMT4 0.22? 0.19? 0.29 0.50 0.48 0.39 0.48 0.41 0.32 0.61
RBMT5 0.40 0.40 0.56 0.54 0.57 0.52 0.3 0.48 0.29? 0.54
RBMT6 0.27 0.32 0.48 0.46 0.53 0.44 0.51 0.55 0.36 0.61
SAAR 0.12? 0.19 0.30 0.44 0.41 0.48 0.32 0.42 0.20? 0.40
UCL 0.35 0.54 0.46 0.63 0.61 0.68 0.68? 0.61 0.63? 0.65?
UEDIN 0.22? 0.17? 0.32 0.42 0.42 0.36 0.41 0.27 0.40 0.23?
> OTHERS 0.24 0.32 0.46 0.51 0.51 0.53 0.43 0.43 0.53 0.30 0.58
? OTHERS 0.36 0.49 0.61 0.63 0.6 0.61 0.54 0.54 0.68 0.42 0.68
Table 26: Sentence-level ranking for the German-English Europarl Task.
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
E
D
IN
LIMSI 0.44 0.8? 0.67? 0.81? 0.76? 0.63? 0.53 0.47?
LIU 0.29 0.80? 0.68? 0.81? 0.62? 0.63? 0.25 0.31
RBMT2 0.13? 0.07? 0.35 0.33 0.32? 0.20? 0.17? 0.09?
RBMT3 0.18? 0.27? 0.50 0.52 0.45 0.29? 0.26 0.21?
RBMT4 0.09? 0.12? 0.47 0.30 0.42 0.22? 0.15? 0.17?
RBMT5 0.12? 0.26? 0.59? 0.42 0.40 0.33 0.28 0.24?
RBMT6 0.25? 0.22? 0.6? 0.61? 0.63? 0.50 0.36 0.33
SAAR 0.28 0.63 0.66? 0.56 0.7? 0.62 0.46 0.45
UEDIN 0.24? 0.42 0.75? 0.66? 0.73? 0.68? 0.51 0.36
> OTHERS 0.19 0.28 0.64 0.54 0.61 0.54 0.40 0.3 0.27
? OTHERS 0.36 0.43 0.79 0.66 0.75 0.67 0.56 0.46 0.44
Table 27: Sentence-level ranking for the English-German News Task.
C
M
U
-G
IM
P
E
L
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
CMU-GIMPEL 0.29 0.28 0.41 0.49 0.56 0.44 0.24? 0.09? 0.24? 0.52
LIMSI 0.45 0.31 0.48 0.45 0.54 0.40 0.35 0.40 0.29? 0.47
LIU 0.34 0.47 0.56 0.44 0.65? 0.37 0.30 0.31 0.19? 0.50
RBMT2 0.51 0.48 0.41 0.41 0.48 0.22? 0.24? 0.62 0.26? 0.43
RBMT3 0.40 0.50 0.47 0.47 0.60 0.33 0.3? 0.11 0.26? 0.50
RBMT4 0.39 0.37 0.27? 0.41 0.35 0.22? 0.14? 0.25 0.33 0.46
RBMT5 0.49 0.47 0.54 0.64? 0.60 0.64? 0.32 0.47 0.45 0.64?
RBMT6 0.71? 0.50 0.58 0.57? 0.65? 0.74? 0.46 0.41 0.36 0.60
SAAR 0.73? 0.40 0.39 0.39 0.78 0.58 0.47 0.35 0.31 0.50
UCL 0.61? 0.6? 0.67? 0.59? 0.68? 0.64 0.53 0.51 0.62 0.70?
UEDIN 0.25 0.27 0.30 0.52 0.41 0.49 0.26? 0.31 0.25 0.23?
> OTHERS 0.47 0.43 0.43 0.51 0.51 0.59 0.36 0.3 0.37 0.3 0.54
? OTHERS 0.61 0.58 0.58 0.62 0.58 0.68 0.47 0.43 0.53 0.39 0.67
Table 28: Sentence-level ranking for the English-German Europarl Task.
97
C
M
U
-S
M
T
C
U
E
D
C
U
E
D
-C
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
B
U
E
D
IN
U
P
C
CMU-SMT 0.41 0.62? 0.33 0.54? 0.57? 0.42 0.46 0.46 0.29 0.34 0.37
CUED 0.29 0.24 0.27 0.54? 0.76? 0.61? 0.50 0.39 0.46 0.26 0.42
CUED-CONTRAST 0.19? 0.24 0.23 0.47 0.48 0.28 0.41 0.37 0.26 0.26 0.33
LIMSI 0.33 0.30 0.51 0.41 0.56? 0.47 0.41 0.46 0.33 0.37 0.43
RBMT3 0.19? 0.23? 0.37 0.43 0.39 0.28 0.3 0.33 0.39 0.30 0.49
RBMT4 0.19? 0.14? 0.27 0.21? 0.27 0.21? 0.30 0.27 0.17? 0.29? 0.23?
RBMT5 0.37 0.19? 0.56 0.35 0.47 0.57? 0.56 0.43 0.24? 0.35 0.52
RBMT6 0.41 0.30 0.29 0.39 0.43 0.50 0.25 0.46 0.34 0.44 0.46
SAAR 0.29 0.25 0.43 0.32 0.50 0.42 0.33 0.31 0.2? 0.26 0.3
UCB 0.29 0.36 0.52 0.49 0.46 0.61? 0.6? 0.41 0.56? 0.39 0.28
UEDIN 0.39 0.37 0.52 0.30 0.50 0.61? 0.58 0.39 0.46 0.24 0.44
UPC 0.26 0.36 0.47 0.35 0.40 0.59? 0.32 0.42 0.46 0.33 0.41
> OTHERS 0.29 0.28 0.43 0.34 0.45 0.55 0.39 0.40 0.42 0.29 0.34 0.39
? OTHERS 0.57 0.56 0.67 0.58 0.67 0.77 0.58 0.61 0.67 0.54 0.56 0.60
Table 29: Sentence-level ranking for the Spanish-English News Task.
C
M
U
-S
M
T
C
U
E
D
D
C
U
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
U
P
C
CMU-SMT 0.36 0.38 0.37 0.10? 0.20? 0.14? 0.32 0.39 0.22 0.25 0.38
CUED 0.40 0.38 0.53 0.33 0.30 0.30 0.20? 0.32 0.08? 0.36 0.29
DCU 0.34 0.38 0.46 0.32 0.19? 0.26? 0.21? 0.32 0.33 0.25 0.46
LIMSI 0.31 0.30 0.21 0.05? 0.09? 0.15? 0.18? 0.24 0.10? 0.19 0.48
RBMT3 0.83? 0.62 0.58 0.73? 0.56 0.25 0.37 0.60? 0.31 0.66? 0.78?
RBMT4 0.73? 0.54 0.76? 0.74? 0.28 0.38 0.24 0.53 0.29 0.56 0.65?
RBMT5 0.79? 0.55 0.67? 0.75? 0.58 0.57 0.59? 0.70? 0.44 0.71? 0.67
RBMT6 0.52 0.77? 0.66? 0.68? 0.42 0.49 0.18? 0.55 0.41 0.54 0.71
SAAR 0.43 0.42 0.41 0.47 0.20? 0.32 0.17? 0.30 0.22? 0.35 0.32
UCL 0.56 0.71? 0.56 0.70? 0.42 0.57 0.33 0.44 0.59? 0.81? 0.67
UEDIN 0.28 0.46 0.39 0.31 0.29? 0.42 0.25? 0.39 0.35 0.15? 0.40
UPC 0.44 0.39 0.43 0.36 0.07? 0.23? 0.24 0.29 0.27 0.20 0.40
> OTHERS 0.50 0.5 0.49 0.53 0.28 0.36 0.24 0.32 0.44 0.26 0.45 0.51
? OTHERS 0.71 0.68 0.68 0.78 0.43 0.49 0.35 0.47 0.67 0.43 0.66 0.69
Table 30: Sentence-level ranking for the Spanish-English Europarl Task.
C
M
U
-S
M
T
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
B
U
E
D
IN
U
P
C
CMU-SMT 0.39 0.57 0.52? 0.62? 0.56? 0.50 0.41 0.42 0.56?
LIMSI 0.42 0.56 0.53 0.63? 0.58 0.32 0.39 0.35 0.35
RBMT3 0.23 0.3 0.34 0.46 0.50 0.39 0.17 0.21? 0.06?
RBMT4 0.25? 0.30 0.47 0.31 0.35 0.38 0.36 0.32 0.19
RBMT5 0.21? 0.20? 0.28 0.42 0.42 0.29? 0.24 0.17? 0.23
RBMT6 0.23? 0.23 0.31 0.41 0.42 0.23? 0.19 0.24? 0.24
SAAR 0.36 0.52 0.39 0.43 0.67? 0.54? 0.36 0.29 0.42
UCB 0.37 0.39 0.52 0.39 0.49 0.52 0.46 0.27 0.25
UEDIN 0.35 0.48 0.62? 0.48 0.64? 0.61? 0.50 0.47 0.53?
UPC 0.11? 0.41 0.63? 0.48 0.50 0.57 0.42 0.63 0.06?
> OTHERS 0.28 0.36 0.47 0.45 0.52 0.51 0.38 0.34 0.27 0.33
? OTHERS 0.49 0.54 0.68 0.67 0.72 0.72 0.55 0.59 0.48 0.60
Table 31: Sentence-level ranking for the English-Spanish News Task.
98
C
M
U
-S
M
T
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
U
P
C
U
W
CMU-SMT 0.28 0.47 0.33 0.17? 0.26 0.50 0.25 0.48? 0.44 0.28
LIMSI 0.38 0.19? 0.33 0.16? 0.23 0.33 0.14? 0.14 0.35 0.32
RBMT3 0.42 0.62? 0.42 0.36 0.29 0.54 0.28 0.39 0.50 0.75?
RBMT4 0.46 0.47 0.42 0.19 0.31 0.61 0.50 0.40 0.50 0.57
RBMT5 0.70? 0.64? 0.59 0.48 0.35 0.65? 0.52 0.64 0.61 0.63?
RBMT6 0.63 0.58 0.47 0.56 0.50 0.78? 0.32 0.58 0.33 0.71?
SAAR 0.33 0.40 0.33 0.30 0.23? 0.19? 0.20 0.27 0.24 0.33
UCL 0.46 0.64? 0.41 0.46 0.36 0.41 0.60 0.65? 0.42 0.57?
UEDIN 0.09? 0.29 0.48 0.45 0.28 0.27 0.41 0.19? 0.25 0.17
UPC 0.22 0.40 0.50 0.43 0.28 0.40 0.52 0.26 0.56 0.58
UW 0.44 0.32 0.06? 0.29 0.17? 0.21? 0.33 0.14? 0.33 0.33
> OTHERS 0.43 0.46 0.4 0.4 0.26 0.28 0.53 0.28 0.46 0.4 0.49
? OTHERS 0.67 0.74 0.55 0.56 0.41 0.44 0.72 0.50 0.71 0.59 0.74
Table 32: Sentence-level ranking for the English-Spanish Europarl Task.
DCU UEDIN UMD
DCU 0.26? 0.4
UEDIN 0.37? 0.46?
UMD 0.4 0.31?
> OTHERS 0.38 0.28 0.43
? OTHERS 0.68 0.58 0.65
Table 33: Sentence-level ranking for the Czech-English News Task.
DCU SYSTRAN UEDIN UMD
DCU 0.21? 0.19? 0.37
SYSTRAN 0.59? 0.47? 0.61?
UEDIN 0.42? 0.27? 0.50?
UMD 0.38 0.18? 0.29?
> OTHERS 0.46 0.22 0.31 0.49
? OTHERS 0.75 0.45 0.60 0.72
Table 34: Sentence-level ranking for the Czech-English Commentary Task.
CU-BOJAR CU-TECTOMT PC-TRANSLATOR UEDIN
CU-BOJAR 0.32? 0.51? 0.27?
CU-TECTOMT 0.52? 0.58? 0.42
PC-TRANSLATOR 0.35? 0.25? 0.26?
UEDIN 0.5? 0.40 0.59?
> OTHERS 0.45 0.32 0.56 0.32
? OTHERS 0.63 0.49 0.72 0.50
Table 35: Sentence-level ranking for the English-Czech News Task.
CU-BOJAR CU-TECTOMT PC-TRANSLATOR UEDIN
CU-BOJAR 0.28? 0.38 0.19?
CU-TECTOMT 0.58? 0.53? 0.43
PC-TRANSLATOR 0.45 0.3? 0.26?
UEDIN 0.60? 0.37 0.56?
> OTHERS 0.54 0.32 0.49 0.29
? OTHERS 0.71 0.49 0.66 0.49
Table 36: Sentence-level ranking for the English-Czech Commentary Task.
99
MLOGIC UEDIN
MORPHOLOGIC 0.15?
UEDIN 0.68?
> OTHERS 0.68 0.15
? OTHERS 0.85 0.32
Table 37: Sentence-level ranking for the Hungarian-English News Task.
C
M
U
-X
F
R
C
U
E
D
C
U
E
D
-C
L
IM
S
I
L
IU
M
-S
Y
S
L
IU
M
-S
Y
S
-C
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
CMU-XFR 0.37 0.49? 0.62? 0.57? 0.61? 0.49 0.49 0.48? 0.41 0.56? 0.39 0.46?
CUED 0.28 0.21 0.30 0.30 0.13 0.28 0.18 0.27 0.28 0.31 0.34 0.18
CUED-C 0.2? 0.11 0.30? 0.19 0.33 0.18? 0.21 0.24 0.2? 0.2? 0.17? 0.24
LIMSI 0.13? 0.20 0.13? 0.27 0.22 0.23 0.24 0.2 0.20? 0.16? 0.23 0.22
LIUM-SYS 0.18? 0.17 0.27 0.17 0.20 0.18? 0.41 0.29 0.24 0.26 0.22 0.26
LI-SYS-C 0.18? 0.28 0.24 0.25 0.07 0.33 0.2? 0.27 0.18? 0.23 0.25 0.19
RBMT3 0.28 0.34 0.52? 0.28 0.40? 0.37 0.27 0.46? 0.27 0.30 0.39 0.34
RBMT4 0.29 0.40 0.34 0.31 0.39 0.43? 0.33 0.34 0.34 0.27 0.41 0.31
RBMT5 0.22? 0.24 0.34 0.3 0.27 0.43 0.14? 0.24 0.13? 0.32 0.32 0.32
RBMT6 0.3 0.41 0.50? 0.39? 0.33 0.58? 0.3 0.33 0.37? 0.33 0.52? 0.37
SAAR 0.27? 0.33 0.43? 0.37? 0.4 0.42 0.41 0.36 0.32 0.41 0.23 0.41
SAAR-C 0.28 0.32 0.38? 0.27 0.27 0.45 0.23 0.21 0.20 0.23? 0.18 0.19
UED 0.19? 0.15 0.20 0.25 0.29 0.19 0.28 0.27 0.19 0.24 0.21 0.26
> OTHERS 0.24 0.27 0.33 0.32 0.32 0.37 0.29 0.28 0.30 0.27 0.29 0.31 0.29
? OTHERS 0.51 0.75 0.79 0.80 0.77 0.78 0.65 0.66 0.73 0.62 0.64 0.74 0.77
Table 38: Constituent ranking for the French-English News Task
C
M
U
-X
F
R
C
U
E
D
D
C
U
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
S
Y
S
T
R
A
N
U
C
L
U
E
D
IN
CMU-XFR 0.42? 0.4? 0.37? 0.54? 0.16? 0.21 0.41 0.23 0.49? 0.42? 0.34 0.45 0.50?
CUED 0.03? 0.13 0.08 0.14 0.13? 0.13? 0.08? 0.05? 0.08 0.04 0.15 0.11 0.07
DCU 0.09? 0.08 0.10 0.12 0.06? 0.20 0.31 0.16? 0.14 0.22 0.13 0.10 0.16
LIMSI 0.1? 0.05 0.19 0.05 0.04? 0.08? 0.19 0.11? 0.18 0.09 0.05? 0.05?
LIUM-SYS 0.03? 0.14 0.19 0.07 0 0.08? 0.03? 0.05? 0.03? 0.09 0.15 0.14 0.08
RBMT3 0.44? 0.61? 0.50? 0.58? 0.56? 0.41? 0.38 0.32 0.37 0.53? 0.44 0.50? 0.58?
RBMT4 0.39 0.44? 0.43 0.45? 0.35? 0.12? 0.31 0.23 0.42 0.39 0.33 0.32 0.35
RBMT5 0.19 0.47? 0.29 0.35 0.37? 0.18 0.17 0.23 0.35 0.33 0.19 0.46 0.40
RBMT6 0.36 0.65? 0.54? 0.48? 0.55? 0.26 0.40 0.50 0.50? 0.52? 0.47? 0.60? 0.44
SAAR 0.07? 0.25 0.24 0.18 0.37? 0.23 0.36 0.23 0.12? 0.12 0.23 0.13 0.37?
SAAR-C 0.09? 0.18 0.12 0.16 0.16 0.09? 0.18 0.2 0.06? 0.12 0.09 0.14 0.15
SYSTRAN 0.34 0.40 0.21 0.38? 0.23 0.25 0.36 0.22 0.15? 0.23 0.28 0.31 0.30?
UCL 0.25 0.34 0.28 0.31? 0.19 0.11? 0.24 0.23 0.11? 0.24 0.31 0.34 0.37?
UED 0.10? 0.10 0.16 0.05 0.08 0.03? 0.15 0.14 0.18 0.07? 0.13 0.07? 0.11?
> OTHERS 0.2 0.32 0.27 0.28 0.28 0.12 0.22 0.25 0.15 0.26 0.27 0.22 0.25 0.28
? OTHERS 0.63 0.91 0.85 0.91 0.92 0.52 0.65 0.7 0.52 0.78 0.87 0.71 0.74 0.89
Table 39: Constituent ranking for the French-English Europarl Task
100
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
X
E
R
O
X
LIMSI 0.27 0.43 0.43 0.29 0.53? 0.32 0.37 0.30 0.14?
LIUM-SYSTRAN 0.09 0.33 0.36 0.18 0.35 0.16? 0.25 0.22 0.13?
RBMT3 0.36 0.33 0.22 0.31 0.28 0.4 0.26 0.26? 0.20?
RBMT4 0.25 0.26 0.30 0.23 0.16? 0.28 0.26 0.24 0.13?
RBMT5 0.31 0.33 0.22 0.28 0.17 0.27 0.25 0.23 0.13?
RBMT6 0.26? 0.30 0.31 0.38? 0.32 0.33 0.36 0.39 0.25?
SAAR 0.32 0.41? 0.35 0.38 0.32 0.28 0.14 0.23 0.11?
SAAR-CONTRAST 0.25 0.26 0.36 0.30 0.33 0.36 0.05 0.22 0.13?
UEDIN 0.29 0.34 0.45? 0.4 0.33 0.40 0.31 0.35 0.13?
XEROX 0.66? 0.55? 0.61? 0.65? 0.58? 0.51? 0.53? 0.57? 0.45?
> OTHERS 0.31 0.34 0.38 0.38 0.33 0.33 0.3 0.31 0.29 0.15
? OTHERS 0.65 0.76 0.72 0.77 0.76 0.67 0.73 0.75 0.66 0.44
Table 40: Constituent ranking for the English-French News Task
L
IM
S
I
L
IU
M
-S
Y
S
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
LIMSI 0.14 0.09? 0.10? 0.24 0.11? 0.13 0.08? 0.12
LIUM-SYSTRAN 0.19? 0.19? 0.15 0.12? 0.06 0.06? 0.09
RBMT3 0.65? 0.59? 0.33 0.43 0.32 0.50? 0.39 0.46?
RBMT4 0.53? 0.47? 0.19 0.27 0.18? 0.33 0.38 0.39
RBMT5 0.48 0.38 0.32 0.48 0.47 0.55? 0.44 0.51?
RBMT6 0.54? 0.49? 0.32 0.41? 0.26 0.52? 0.45 0.58?
SAAR 0.21 0.17 0.23? 0.25 0.21? 0.17? 0.19 0.13
UCL 0.37? 0.33? 0.38 0.35 0.36 0.32 0.34 0.31?
UEDIN 0.12 0.11 0.17? 0.23 0.13? 0.13? 0.07 0.07?
> OTHERS 0.38 0.36 0.25 0.30 0.26 0.24 0.33 0.27 0.34
? OTHERS 0.88 0.88 0.56 0.68 0.55 0.56 0.81 0.66 0.87
Table 41: Constituent ranking for the English-French Europarl Task
C
M
U
-X
F
E
R
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
S
A
A
R
-C
U
E
D
IN
CMU-STATXFER 0.47? 0.44 0.52? 0.53? 0.57? 0.49? 0.41 0.49 0.58? 0.49?
LIMSI 0.17? 0.18 0.35 0.34 0.40 0.33 0.43 0.19 0.28 0.19
LIU 0.25 0.3 0.37 0.35 0.44 0.28 0.40 0.21 0.33 0.32?
RBMT2 0.19? 0.26 0.30 0.19 0.32 0.16? 0.20 0.26 0.23 0.21
RBMT3 0.22? 0.36 0.26 0.23 0.24 0.23 0.14? 0.15 0.28 0.29
RBMT4 0.20? 0.35 0.23 0.21 0.24 0.22 0.19? 0.36 0.32 0.31
RBMT5 0.26? 0.28 0.38 0.34? 0.31 0.35 0.26 0.3 0.43? 0.35
RBMT6 0.38 0.37 0.39 0.34 0.44? 0.4? 0.30 0.28 0.26 0.38
SAAR 0.29 0.22 0.37 0.29 0.10 0.28 0.19 0.22 0.26 0.18
SAAR-CONTRAST 0.18? 0.33 0.29 0.19 0.22 0.24 0.15? 0.26 0.18 0.23
UEDIN 0.11? 0.3 0.13? 0.23 0.35 0.3 0.2 0.37 0.30 0.31
> OTHERS 0.22 0.33 0.3 0.31 0.32 0.35 0.25 0.29 0.28 0.33 0.30
? OTHERS 0.50 0.72 0.67 0.77 0.76 0.74 0.67 0.64 0.76 0.78 0.74
Table 42: Constituent ranking for the German-English News Task
101
C
M
U
-X
F
R
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
CMU-STATXFER 0.51? 0.51? 0.38 0.38 0.41 0.37 0.44 0.48? 0.39 0.6?
LIMSI 0.18? 0.22 0.3 0.30 0.23 0.22? 0.32 0.27 0.18? 0.29
LIU 0.14? 0.22 0.26? 0.32 0.22? 0.16? 0.31 0.20 0.08? 0.12
RBMT2 0.38 0.51 0.52? 0.40 0.32 0.25 0.31 0.51 0.40 0.7?
RBMT3 0.32 0.42 0.45 0.28 0.46 0.16 0.20? 0.56? 0.38 0.43
RBMT4 0.32 0.45 0.52? 0.31 0.24 0.13? 0.30 0.49? 0.44 0.48?
RBMT5 0.44 0.57? 0.53? 0.34 0.31 0.43? 0.19 0.54? 0.39 0.54?
RBMT6 0.33 0.51 0.48 0.33 0.47? 0.33 0.33 0.47? 0.42 0.51?
SAAR 0.12? 0.1 0.15 0.26 0.09? 0.19? 0.17? 0.23? 0.11? 0.14
UCL 0.30 0.43? 0.49? 0.40 0.40 0.30 0.41 0.39 0.38? 0.51?
UEDIN 0.11? 0.16 0.12 0.18? 0.25 0.2? 0.18? 0.23? 0.14 0.12?
> OTHERS 0.27 0.40 0.41 0.31 0.32 0.32 0.25 0.3 0.41 0.30 0.44
? OTHERS 0.55 0.75 0.8 0.58 0.64 0.64 0.58 0.59 0.84 0.60 0.83
Table 43: Constituent ranking for the German-English Europarl Task
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
E
D
IN
LIMSI 0.29 0.46 0.45 0.37 0.36 0.29? 0.33 0.22
LIU 0.32 0.53? 0.45? 0.51? 0.5? 0.38 0.31 0.36
RBMT2 0.33 0.32? 0.29 0.29 0.20? 0.25? 0.28 0.28?
RBMT3 0.34 0.3? 0.4 0.33 0.3? 0.34 0.20? 0.27?
RBMT4 0.26 0.25? 0.31 0.3 0.23? 0.23? 0.20? 0.21?
RBMT5 0.46 0.33? 0.55? 0.46? 0.40? 0.32 0.32 0.29?
RBMT6 0.52? 0.40 0.47? 0.44 0.53? 0.40 0.27 0.37
SAAR 0.38 0.3 0.39 0.42? 0.44? 0.40 0.44 0.34
UEDIN 0.30 0.24 0.53? 0.52? 0.51? 0.56? 0.45 0.36
> OTHERS 0.36 0.31 0.46 0.41 0.42 0.37 0.33 0.28 0.29
? OTHERS 0.65 0.57 0.72 0.68 0.75 0.60 0.56 0.61 0.56
Table 44: Constituent ranking for the English-German News Task
C
M
U
-G
IM
P
E
L
L
IM
S
I
L
IU
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
CMU-GIMPEL 0.12 0.27 0.21? 0.30 0.21? 0.27? 0.21? 0.22 0.22 0.23
LIMSI 0.22 0.22 0.34 0.29? 0.29? 0.23? 0.29? 0.2 0.21 0.19
LIU 0.18 0.2 0.20? 0.25? 0.17? 0.16? 0.12? 0.28 0.21 0.18
RBMT2 0.54? 0.41 0.62? 0.28 0.33 0.35 0.28 0.61? 0.43 0.47?
RBMT3 0.47 0.47? 0.47? 0.4 0.33 0.32 0.28 0.56? 0.47 0.48?
RBMT4 0.52? 0.57? 0.52? 0.42 0.32 0.27? 0.28 0.47 0.45 0.39
RBMT5 0.49? 0.57? 0.65? 0.42 0.38 0.48? 0.31 0.76? 0.51 0.52?
RBMT6 0.51? 0.54? 0.60? 0.41 0.39 0.40 0.41 0.51? 0.53? 0.51?
SAAR 0.24 0.29 0.17 0.26? 0.22? 0.25 0.20? 0.21? 0.31 0.12
UCL 0.28 0.32 0.29 0.33 0.38 0.32 0.32 0.29? 0.19 0.30
UEDIN 0.1 0.13 0.22 0.2? 0.18? 0.22 0.21? 0.18? 0.15 0.17
> OTHERS 0.37 0.37 0.42 0.32 0.30 0.31 0.28 0.25 0.39 0.35 0.35
? OTHERS 0.77 0.75 0.81 0.58 0.59 0.58 0.51 0.52 0.77 0.69 0.82
Table 45: Constituent ranking for the English-German Europarl Task
102
C
M
U
-S
M
T
C
U
E
D
C
U
E
D
-C
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
B
U
E
D
IN
U
P
C
CMU-SMT 0.19 0.17 0.26 0.38 0.27 0.45 0.32 0.35 0.27 0.26 0.2
CUED 0.21 0.21 0.24 0.24 0.2 0.34 0.25 0.27 0.18 0.26 0.21
CUED-CONTRAST 0.17 0.08 0.12 0.24 0.23? 0.27 0.25 0.21 0.12 0.11 0.26
LIMSI 0.17 0.25 0.26 0.34 0.18? 0.33 0.33 0.31 0.17 0.26 0.23
RBMT3 0.29 0.31 0.35 0.37 0.21 0.4 0.31 0.32 0.43 0.42 0.52?
RBMT4 0.38 0.34 0.54? 0.47? 0.35 0.24 0.32 0.46? 0.37 0.40 0.53
RBMT5 0.24 0.31 0.40 0.33 0.25 0.18 0.31 0.33 0.32 0.28 0.38
RBMT6 0.33 0.29 0.28 0.33 0.26 0.27 0.16 0.26 0.3 0.39 0.41
SAAR 0.26 0.27 0.33 0.26 0.21 0.12? 0.25 0.24 0.20 0.28 0.20
UCB 0.25 0.30 0.23 0.27 0.31 0.27 0.40 0.34 0.28 0.32 0.26
UEDIN 0.19 0.20 0.19 0.24 0.27 0.33 0.31 0.27 0.21 0.21 0.25
UPC 0.1 0.21 0.17 0.2 0.22? 0.28 0.4 0.24 0.29 0.30 0.2
> OTHERS 0.24 0.25 0.28 0.28 0.28 0.23 0.33 0.29 0.3 0.26 0.3 0.32
? OTHERS 0.72 0.76 0.82 0.74 0.64 0.61 0.7 0.70 0.76 0.71 0.76 0.76
Table 46: Constituent ranking for the Spanish-English News Task
C
M
U
-S
M
T
C
U
E
D
D
C
U
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
U
P
C
CMU-SMT 0.2 0.20 0.1 0.1? 0.18? 0.04? 0.18? 0.16 0.17 0.19 0.19
CUED 0.18 0.13 0.19 0.14? 0.12? 0.1? 0.2? 0.13 0.12? 0.22 0.12
DCU 0.15 0.13 0.11 0.09? 0.10? 0.13? 0.09? 0.19 0.15? 0.14 0.15
LIMSI 0.03 0.15 0.16 0.19? 0.18? 0.15? 0.19? 0.19 0.08? 0.07 0.22
RBMT3 0.7? 0.73? 0.59? 0.49? 0.19 0.36 0.22 0.62? 0.55? 0.68? 0.73?
RBMT4 0.55? 0.62? 0.51? 0.55? 0.23 0.22 0.17 0.56? 0.43 0.56? 0.44?
RBMT5 0.60? 0.61? 0.53? 0.61? 0.32 0.38 0.28 0.63? 0.53 0.7? 0.59?
RBMT6 0.52? 0.48? 0.51? 0.49? 0.23 0.26 0.19 0.49? 0.53? 0.52? 0.50?
SAAR 0.14 0.10 0.12 0.15 0.10? 0.12? 0.05? 0.07? 0.14? 0.05 0.18
UCL 0.38 0.37? 0.46? 0.45? 0.28? 0.32 0.29 0.24? 0.38? 0.38? 0.36
UEDIN 0.06 0.14 0.14 0.18 0.15? 0.16? 0.05? 0.16? 0.15 0.10? 0.21
UPC 0.19 0.12 0.20 0.12 0.07? 0.17? 0.09? 0.14? 0.04 0.17 0.14
> OTHERS 0.32 0.33 0.32 0.32 0.17 0.2 0.15 0.17 0.33 0.28 0.34 0.35
? OTHERS 0.85 0.85 0.87 0.85 0.46 0.56 0.47 0.57 0.89 0.65 0.87 0.87
Table 47: Constituent ranking for the Spanish-English Europarl Task
C
M
U
-S
M
T
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
B
U
E
D
IN
U
P
C
CMU-SMT 0.20 0.36 0.37 0.24? 0.36 0.32 0.21 0.17 0.27
LIMSI 0.23 0.4 0.46? 0.33 0.39 0.31 0.23 0.17 0.18
RBMT3 0.33 0.35 0.22 0.19? 0.3 0.31 0.49 0.34 0.22
RBMT4 0.30 0.25? 0.25 0.17? 0.17? 0.24 0.19? 0.34 0.30
RBMT5 0.53? 0.42 0.50? 0.41? 0.35 0.50? 0.44 0.37 0.29
RBMT6 0.36 0.35 0.34 0.39? 0.32 0.35 0.36 0.37 0.38
SAAR 0.33 0.36 0.38 0.28 0.24? 0.38 0.29 0.22? 0.24
UCB 0.32 0.29 0.35 0.54? 0.33 0.45 0.31 0.19 0.29
UEDIN 0.29 0.33 0.36 0.42 0.42 0.39 0.45? 0.30 0.44
UPC 0.36 0.42 0.50 0.49 0.42 0.44 0.51 0.21 0.26
> OTHERS 0.34 0.33 0.38 0.39 0.29 0.35 0.36 0.31 0.27 0.29
? OTHERS 0.72 0.69 0.69 0.75 0.57 0.64 0.7 0.65 0.63 0.6
Table 48: Constituent ranking for the English-Spanish News Task
103
C
M
U
-S
M
T
L
IM
S
I
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
B
M
T
6
S
A
A
R
U
C
L
U
E
D
IN
U
P
C
U
W
CMU-SMT 0.13 0.10? 0.21? 0.2? 0.2? 0.26 0.22 0.13 0.16 0.14
LIMSI 0.17 0.24 0.16? 0.20? 0.13? 0.21 0.06? 0.09 0.14 0.08
RBMT3 0.64? 0.45 0.24 0.30 0.21 0.57? 0.56 0.58? 0.32 0.58?
RBMT4 0.54? 0.52? 0.42 0.26 0.24 0.50? 0.35 0.43 0.47 0.44
RBMT5 0.61? 0.68? 0.46 0.44 0.37 0.64? 0.50 0.63? 0.62? 0.54
RBMT6 0.57? 0.48? 0.39 0.33 0.25 0.52? 0.33 0.54? 0.46 0.46
SAAR 0.19 0.14 0.07? 0.19? 0.09? 0.14? 0.13? 0.17 0.26 0.18
UCL 0.43 0.46? 0.29 0.37 0.38 0.42 0.49? 0.37? 0.48 0.40
UEDIN 0.15 0.11 0.24? 0.20 0.13? 0.17? 0.30 0.14? 0.20 0.20
UPC 0.26 0.05 0.35 0.25 0.16? 0.23 0.34 0.21 0.23 0.10
UW 0.14 0.14 0.17? 0.22 0.23 0.2 0.32 0.20 0.20 0.35
> OTHERS 0.37 0.32 0.28 0.26 0.22 0.23 0.42 0.27 0.35 0.35 0.33
? OTHERS 0.83 0.86 0.56 0.59 0.46 0.57 0.85 0.59 0.82 0.78 0.79
Table 49: Constituent ranking for the English-Spanish Europarl Task
CU-BOJAR CU-TECTOMT PC-TRANSLATOR UEDIN
CU-BOJAR 0.33 0.41 0.28?
CU-TECTOMT 0.37 0.42? 0.36
PC-TRANSLATOR 0.34 0.31? 0.32?
UEDIN 0.37? 0.37 0.43?
> OTHERS 0.36 0.34 0.42 0.32
? OTHERS 0.66 0.62 0.67 0.61
Table 50: Constituent ranking for the English-Czech News Task
CU-BOJAR CU-TECTOMT PC-TRANSLATOR UEDIN
CU-BOJAR 0.25? 0.33? 0.22?
CU-TECTOMT 0.50? 0.44? 0.45
PC-TRANSLATOR 0.47? 0.3? 0.40
UEDIN 0.39? 0.37 0.39
> OTHERS 0.45 0.31 0.39 0.36
? OTHERS 0.73 0.54 0.61 0.61
Table 51: Constituent ranking for the English-Czech Commentary Task
104
French?English English?French
Europarl YES NO
CMU-XFR 0.61 0.39
CUED 0.83 0.17
DCU 0.88 0.12
LIMSI 0.89 0.11
LIUM-SYS 0.89 0.11
RBMT3 0.54 0.47
RBMT4 0.62 0.38
RBMT5 0.71 0.29
RBMT6 0.54 0.46
SAAR 0.72 0.28
SAAR-C 0.86 0.14
SYSTRAN 0.81 0.19
UCL 0.73 0.27
UEDIN 0.91 0.09
News YES NO
CMU-XFR 0.55 0.45
CUED 0.74 0.26
CUED-C 0.79 0.21
LIMSI 0.81 0.2
LIUM-SYS 0.79 0.21
LI-SYS-C 0.7 0.30
RBMT3 0.63 0.37
RBMT4 0.64 0.36
RBMT5 0.76 0.24
RBMT6 0.66 0.34
SAAR 0.64 0.36
SAAR-C 0.70 0.3
UEDIN 0.72 0.28
Europarl YES NO
LIMSI 0.75 0.26
LIUM-SYS 0.84 0.16
RBMT3 0.49 0.51
RBMT4 0.50 0.5
RBMT5 0.44 0.56
RBMT6 0.35 0.65
SAAR 0.70 0.3
UCL 0.6 0.40
UEDIN 0.75 0.25
News YES NO
LIMSI 0.73 0.27
LIUM-SYS 0.75 0.25
RBMT3 0.59 0.41
RBMT4 0.59 0.41
RBMT5 0.64 0.36
RBMT6 0.58 0.42
SAAR 0.59 0.41
SAAR-C 0.59 0.41
UEDIN 0.63 0.37
XEROX 0.30 0.7
German?English English?German
Europarl YES NO
CMU-XFER 0.53 0.47
LIMSI 0.80 0.2
LIU 0.83 0.17
RBMT2 0.76 0.24
RBMT3 0.74 0.26
RBMT4 0.67 0.33
RBMT5 0.63 0.37
RBMT6 0.63 0.37
SAAR 0.82 0.18
UCL 0.49 0.51
UEDIN 0.86 0.14
News YES NO
CMU-XFER 0.47 0.53
LIMSI 0.73 0.28
LIU 0.64 0.36
RBMT2 0.72 0.28
RBMT3 0.73 0.27
RBMT4 0.74 0.26
RBMT5 0.59 0.41
RBMT6 0.68 0.32
SAAR 0.67 0.33
SAAR-C 0.72 0.28
UEDIN 0.63 0.37
Europarl YES NO
CMU-GIMPEL 0.82? 0.18
LIMSI 0.79? 0.21
LIU 0.79? 0.21
RBMT2 0.69? 0.31
RBMT3 0.57 0.43
RBMT4 0.67? 0.34
RBMT5 0.45 0.55
RBMT6 0.47 0.53
SAAR 0.77? 0.23
UCL 0.61? 0.39
UEDIN 0.85? 0.15
News YES NO
LIMSI 0.56 0.44
LIU 0.49 0.51
RBMT2 0.69 0.31
RBMT3 0.69 0.31
RBMT4 0.75 0.25
RBMT5 0.55 0.45
RBMT6 0.6 0.40
SAAR 0.54 0.46
UEDIN 0.52 0.48
Spanish?English English?Spanish
Europarl YES NO
CMU-SMT 0.88 0.12
CUED 0.86 0.14
DCU 0.85 0.15
LIMSI 0.90 0.1
RBMT3 0.65 0.35
RBMT4 0.56 0.44
RBMT5 0.59 0.41
RBMT6 0.55 0.45
SAAR 0.87 0.13
UCL 0.73 0.27
UEDIN 0.88 0.12
UPC 0.86 0.14
News YES NO
CMU-SMT 0.64 0.37
CUED 0.64 0.36
CUED-C 0.69 0.31
LIMSI 0.68 0.33
RBMT3 0.61 0.39
RBMT4 0.65 0.35
RBMT5 0.59 0.41
RBMT6 0.64 0.37
SAAR 0.7 0.30
UCB 0.64 0.37
UEDIN 0.62 0.38
UPC 0.71 0.29
Europarl YES NO
CMU-SMT 0.80 0.2
LIMSI 0.87 0.13
RBMT3 0.58 0.42
RBMT4 0.6 0.40
RBMT5 0.64 0.37
RBMT6 0.60 0.40
SAAR 0.81 0.19
UCL 0.71 0.29
UEDIN 0.89 0.11
UPC 0.90 0.1
UW 0.79 0.22
News YES NO
CMU-SMT 0.46 0.54
LIMSI 0.53 0.47
RBMT3 0.64 0.36
RBMT4 0.76 0.24
RBMT5 0.6 0.40
RBMT6 0.62 0.38
SAAR 0.64 0.36
UCB 0.57 0.43
UEDIN 0.49 0.51
UPC 0.37 0.63
English?Czech
Commentary YES NO
CU-BOJAR 0.59 0.41
CU-TECTO 0.43 0.57
PC-TRANS 0.51 0.49
UEDIN 0.41 0.59
News YES NO
CU-BOJAR 0.54 0.46
CU-TECTO 0.42 0.58
PC-TRANS 0.52 0.48
UEDIN 0.44 0.56
Table 52: Yes/No Acceptability of Constituents
105
BBN-CMB-DE
BBN-CMB-FR
CMU-CMB-FR
CMU-SMT-ES
CMU-XFR-DE
CMU-XFR-FR
CUED-C-ES
CUED-C-FR
CUED-ES
CUED-FR
DCU-CZ
LIMSI-DE
LIMSI-ES
LIMSI-FR
LIU-DE
LIUM-S-C-FR
LIUM-SYS-FR
MLOGIC-HU
RBMT2-DE
RBMT3-DE
RBMT3-ES
RBMT3-FR
RBMT4-DE
RBMT4-ES
RBMT4-FR
RBMT5-DE
RBMT5-ES
RBMT5-FR
RBMT6-DE
RBMT6-ES
RBMT6-FR
SAAR-C-DE
SAAR-C-FR
SAAR-DE
SAAR-ES
SAAR-FR
UCB-ES
UED-CMB-DE
UED-CMB-FR
UED-CMB-XX
UED-CZ
UED-DE
UED-ES
UED-FR
UED-HU
UMD-CZ
UPC-ES
B
B
N
-C
M
B
-D
E
.50
.40
1
.20
.50
1
?
.64
1
.73
.31
.69
?
.71
.38
.70
.60
.60
.80
.77
?
.60
.63
.89
?
1
.57
.62
.83
.60
.17
.57
.55
.41
.70
.58
.71
.82
.75
.40
.33
1
.25
.36
.85
?
.50
.40
.60
B
B
N
-C
M
B
-F
R
.38
.14
.38
.09
?
.13
?
.33
.63
.20
.25
.13
.13
.60
.31
.46
.43
.27
.13
.67
.25
.46
.33
.38
.22
.43
.07
?
.33
.42
.50
.36
.25
.46
.40
.06
?
.30
.33
.50
.80
.14
?
.20
.67
.33
.25
.13
.42
C
M
U
-C
M
B
-F
R
.60
.71
.54
.09
?
.60
.29
.13
.57
.33
.23
.33
.33
.46
.44
.58
.40
.20
.54
.27
.50
.67
.11
.14
.44
.11
.25
.60
.09
?
.40
.29
.29
.25
.56
.20
.56
.25
.14
.38
.11
?
.11
.22
.36
.44
C
M
U
-S
M
T-E
S
.50
.31
.50
.17
.75
.46
.64
.43
.25
.54
.60
.83
?
.40
.50
.17
.14
.46
.50
.64
.73
.80
.67
.64
.33
.33
.67
.46
.50
.57
.50
.39
.36
.64
.70
.17
.50
.33
.14
.25
.33
.13
.38
.43
C
M
U
-X
F
R
-D
E
.60
.82
?
.91
?
.50
.78
.56
.89
?
.42
.73
?
.55
.27
.33
.88
?
.57
.73
?
.92
?
.75
.80
?
.82
.75
?
.67
.75
.86
.78
.91
?
.89
?
.79
?
.81
?
.80
.80
?
.67
.90
?
.64
.73
?
.80
.64
.33
1
.83
.11
.20
1
?
.90
?
.33
.50
.85
?
C
M
U
-X
F
R
-F
R
.50
.75
?
.67
.11
.70
.80
?
.88
?
.71
.50
.75
.50
.60
.71
.67
.50
.67
.60
.40
.43
.60
.67
.29
.25
.64
.75
.38
.75
.38
.50
.67
.18
.57
.44
.73
.33
.50
.75
.80
.69
.64
.50
.33
1
C
U
E
D
-C
-E
S
0
.56
.59
.22
.20
.18
.21
.19
0
.29
.15
.47
.14
.39
.50
.25
.39
.36
.43
.46
.33
.31
.56
.50
.07
?
.73
?
.31
.42
.42
.43
.50
.42
.40
.27
.18
.50
.38
.29
.10
?
.22
.33
.43
.33
C
U
E
D
-C
-F
R
.29
.13
.38
.39
.11
?
.10
?
.73
.50
.25
.36
.57
.40
.36
.11
?
.70
?
.60
.40
.58
.36
.56
.20
.39
.50
.60
.10
?
.50
.50
.30
.55
.46
.33
0
.17
.20
.39
.13
.88
0
.29
.39
.36
0
.40
.50
C
U
E
D
-E
S
.80
.29
.18
.25
0
.29
.38
.64
.25
.20
.14
.78
.25
.36
.88
.25
.36
.39
.69
.71
.58
.83
?
.67
.30
.50
.60
.47
.67
.43
.40
.20
.38
.50
.50
.57
.25
.50
.50
.08
?
0
.57
.20
.50
.67
C
U
E
D
-F
R
.18
.25
.22
.43
.09
?
.29
.69
.38
.27
.11
.10
?
.47
.33
.64
.15
.50
.38
.57
.50
.42
.43
.33
.50
.22
.46
.46
.33
.58
.43
.50
.56
.18
.44
.25
.38
.20
.25
0
.33
.13
.44
0
.10
?
.50
D
C
U
-C
Z
.39
.75
.69
.67
.36
.50
.90
?
.46
.58
1
.44
.22
.91
?
.56
.60
.85
?
1
.77
?
.78
.86
.75
.62
.57
.83
?
.30
.55
.80
.67
.77
?
.80
?
.79
?
.50
.33
.80
?
.89
?
.73
.17
.50
.60
.50
.54
.78
.80
.13
.38
.39
L
IM
S
I-D
E
.17
.63
.67
.39
.27
1
.71
.43
.80
.78
.38
.33
.57
.50
.77
?
1
?
.29
.50
.78
.50
.67
.71
.88
.71
?
.33
.57
.89
?
.30
.60
.80
.43
.78
?
.27
.36
.17
.44
1
1
.13
.50
.67
.50
.40
.30
.43
L
IM
S
I-E
S
.08
?
.40
.67
.30
.17
.25
.54
.60
.57
.80
?
.56
.50
.50
.43
.55
.50
.33
.43
.10
?
.67
.50
.63
.39
.69
.29
.75
.50
.29
.60
.82
.63
.20
.22
.55
.33
.29
.25
1
.75
.17
.25
.57
.50
.64
.50
L
IM
S
I-F
R
.14
.38
.18
.08
?
0
.40
.27
.36
.11
.35
.09
?
.29
.25
.23
.63
.30
.25
.38
.56
.36
.44
.22
.25
.50
.10
?
.31
.20
.20
.56
.40
.17
.20
.38
.50
.36
.33
1
.33
.50
.25
.08
?
.27
.24
.25
.29
.50
L
IU
-D
E
.50
.55
.33
.60
.40
.86
.89
?
.38
.44
.22
.25
.57
.54
.73
?
.80
1
.22
.55
.86
.83
.67
.67
.67
.25
.89
?
.71
.50
.60
.58
.60
.60
.60
.75
.71
.67
.33
1
1
.22
.22
.43
.67
.40
.69
L
IU
M
-S-C
-F
R
.20
.29
.17
.50
0
.14
.46
0
.43
.21
.20
.08
?
.18
.13
.09
?
.25
.11
?
.18
.39
.50
.27
.27
.46
.50
.13
?
.31
.55
.33
.46
.50
.42
.25
.33
.59
?
.33
.33
.33
.50
.25
.22
.18
0
.44
.60
L
IU
M
-S-F
R
.20
.36
.20
.67
.08
?
.11
.50
.20
.13
.62
.15
?
0
.38
.50
.20
.25
.14
.42
.36
.17
.43
.13
.60
.30
.25
.33
.52
.25
.18
.43
.39
.29
.20
.44
.16
?
.44
.50
.60
.08
?
.17
?
.23
.17
0
.16
?
.46
M
L
O
G
IC
-H
U
.40
.75
.60
.71
.25
.50
.63
.60
.75
.50
.43
.67
.50
.89
?
.71
.88
.67
.44
.86
?
1
?
.50
.75
.67
.83
.63
.63
.54
.63
.67
.50
.86
.33
.63
.33
.75
1
.40
1
1
.44
.25
.80
R
B
M
T2-D
E
.33
.39
.46
.07
?
.33
.46
.33
.64
.38
.08
?
.50
.36
.50
.33
.55
.58
.13
.17
.67
.38
.38
.70
.55
.22
.46
.46
.46
.43
.17
.10
.42
.43
.67
.29
.33
.40
.40
1
.10
?
.31
.54
.36
.14
.07
?
.56
R
B
M
T3-D
E
.08
?
.75
.64
.50
.18
.20
.64
.64
.31
.43
.22
.11
.80
?
.44
.36
.62
.64
.33
.67
.55
.46
.35
.90
?
.40
.14
.80
?
.40
.38
.38
.60
.25
.53
.44
.31
.56
.63
.17
.80
.60
.22
.55
.60
.17
.20
.67
R
B
M
T3-E
S
.40
.55
.50
.18
.13
?
.50
.36
.22
.23
.50
.14
.42
.33
.50
.14
.50
.83
.44
.33
.27
.39
.64
.50
.50
.36
.33
.31
.27
.46
.33
.09
.43
.23
.50
.46
.29
.75
.75
.25
.25
.36
.50
0
.20
.78
R
B
M
T3-F
R
.25
.58
.22
.27
.33
.29
.27
.40
.29
.33
.13
.33
.43
.50
.17
.55
.50
0
.56
.31
.62
.75
.63
.33
.13
.44
.38
.27
.60
.09
?
.57
.63
.17
.50
.38
.40
1
.11
.25
.73
.50
.25
.53
R
B
M
T4-D
E
.11
?
.50
.78
.20
.40
.67
.62
.25
.14
.15
.29
.13
.78
.25
.73
.75
.50
.55
.18
.22
.43
.50
.57
.29
.25
.50
.63
.42
.33
.25
.67
.58
.50
.60
.67
.14
.38
.50
.75
0
.22
.63
R
B
M
T4-E
S
.44
.57
.22
.14
.17
.38
.38
.08
?
.56
.14
.13
.31
.50
.33
.46
.30
0
.10
.10
?
.50
.38
.56
.29
.43
.25
.33
.38
.25
.25
.46
.33
.14
.09
.25
.33
.11
1
1
0
.09
?
.39
.50
0
.38
.38
R
B
M
T4-F
R
.43
.29
.22
.27
.11
.57
.22
.27
.33
.33
.08
?
.14
?
.23
.33
.22
.25
.60
.50
.46
.30
.50
.47
.57
.41
.67
.46
.58
.54
.43
.38
.36
.63
.14
.86
.36
.25
.60
.39
.50
.50
.33
.20
.38
R
B
M
T5-D
E
.23
.71
?
.67
.50
.09
?
.50
.38
.80
?
.40
.56
.60
.44
.57
.80
?
.63
.75
?
.75
.25
.67
.43
.83
.75
.29
.57
.17
.50
1
?
.13
.67
.53
.40
.71
.20
.67
.47
.38
.86
.25
.27
.25
.70
.67
.20
.38
.50
R
B
M
T5-E
S
.17
.67
.75
.27
.11
?
.27
.73
?
.50
.13
.46
.27
.43
.62
.11
?
.62
.50
.22
.36
.10
?
.43
.44
.43
.63
.46
.36
.50
.29
.44
.17
.57
.27
.29
.25
.60
.11
.33
.60
.67
.44
0
.30
.43
.17
.25
.58
R
B
M
T5-F
R
.30
.42
.10
.11
.14
?
.17
.09
?
.42
.30
.39
.20
.11
?
.33
.50
.21
.27
.29
.15
.40
.67
.38
.71
.33
.17
0
.40
.50
.40
.20
.25
.56
.07
?
.50
.31
.14
.50
.22
.40
.57
.54
.29
R
B
M
T6-D
E
.67
.25
.91
?
.36
.06
?
.50
.54
.70
.47
.53
.33
.50
.57
.70
.40
.67
.58
.25
.39
.38
.69
.73
.50
.50
.46
.50
.43
.50
.13
.50
.46
.62
.50
.46
.50
.46
.40
.33
.80
.29
.20
.57
.50
.29
.86
R
B
M
T6-E
S
.29
.55
.60
.50
.25
.25
.46
.22
.33
.08
?
.40
.20
.44
.55
.64
.38
.29
.63
.40
.30
.30
.25
.57
.33
.22
.60
.63
.67
.64
.42
.38
.67
.71
.46
0
.50
.22
.25
.50
.33
0
.13
.67
R
B
M
T6-F
R
.36
.63
.57
.43
.20
?
.63
.50
.83
.43
.36
.10
?
.18
.40
.17
.43
.50
.39
.67
.30
.39
.73
?
.38
.63
.38
.27
.83
.50
.38
.17
.29
.67
.22
.40
.33
.38
.33
.50
.13
.25
.63
.33
.14
.25
S
A
A
R
-C
-D
E
.41
.55
.67
.30
.25
.50
.50
.54
.60
.40
.14
?
.57
.25
.83
.30
.50
.62
.25
.70
.50
.47
.36
.50
.46
.55
.33
.14
.75
.36
.27
.59
.21
.13
.50
.36
.42
.33
1
.20
.33
.54
.88
.69
0
.44
S
A
A
R
-C
-F
R
.20
.40
.50
.54
0
.33
.50
.33
.60
.44
.38
0
.60
.60
.40
.42
.43
.17
.50
.33
.64
.13
.67
.67
.25
.46
.22
.31
.42
.27
.79
.18
.56
.18
.17
.60
.25
.09
.86
.50
.18
.44
.44
S
A
A
R
-D
E
.33
.77
?
.63
.43
.14
.64
.50
.91
?
.44
.82
.58
.73
.67
.50
.40
.67
.70
.33
.43
.44
.36
.67
.67
.86
.86
.60
.50
.86
?
.29
.56
.44
.53
.73
.33
.83
?
.31
.67
.86
?
.43
.33
.83
?
.56
.17
.78
.55
S
A
A
R
-E
S
.29
.60
.44
.18
.07
?
.29
.40
.67
.17
.44
.10
?
.46
.27
.50
.13
.12
?
.50
.14
.33
.46
.62
.36
.25
.55
.14
.17
.25
.40
.39
.17
.50
.42
.44
.47
.50
.11
.33
.11
?
.33
.39
.18
.08
.33
S
A
A
R
-F
R
.18
.44
.60
.30
.20
.56
.73
.60
.50
.58
0
.50
.33
.64
.29
.42
.63
?
.67
.57
.33
.25
.38
.42
.58
.43
.41
.30
.54
.50
.29
.33
.36
.36
.17
?
.43
.33
.40
.60
.33
.27
.20
.64
?
.31
.25
.58
U
C
B
-E
S
.33
.44
.33
.18
.18
.55
.62
.14
.63
.18
.44
.57
.44
.33
.67
.44
.25
.56
.25
.46
.60
.50
.56
.83
.63
.56
.57
.36
.46
.50
.50
.75
.50
.56
.58
.88
.71
.57
.14
.42
.67
.44
.31
U
E
D
-C
M
B
-D
E
.20
.67
.75
1
.67
.33
1
.88
.75
1
.83
.71
.75
.67
.67
1
.44
.60
.67
.57
1
1
.78
1
.67
.44
1
.60
1
?
1
.67
1
.83
1
.40
.13
.67
.75
?
.25
.50
.40
1
.50
.75
.50
U
E
D
-C
M
B
-F
R
.67
1
.57
.50
.50
.13
.50
.40
.33
.50
.25
.40
.20
.25
.50
.40
.50
.14
.20
.33
.33
1
.67
.40
.33
.33
.29
.38
.33
.33
.50
U
E
D
-C
M
B
-X
X
.50
.67
.25
.13
1
.50
.50
.40
.25
.20
.40
.13
.33
.40
.75
.33
1
.50
.50
.60
.50
0
.67
.56
.14
0
.38
0
.33
.25
U
E
D
-C
Z
.75
.79
?
.89
?
.86
.56
.83
.71
.77
?
.92
?
1
?
.20
.88
.67
.63
.67
1
.69
?
.60
.80
?
.80
.75
.67
.86
.75
?
.62
.46
.56
.67
.71
.78
.88
.67
.64
.43
.78
?
.73
.86
.50
1
1
?
.55
1
?
.80
?
0
.46
.90
?
U
E
D
-D
E
.27
.80
.78
1
.20
.80
?
.71
1
?
.67
.31
.25
.75
.92
?
.56
.67
.83
?
.62
.44
.67
.67
.63
.82
?
.25
.75
1
?
.40
.40
.75
.75
.23
1
.67
.67
.60
.57
.50
1
.36
.64
.67
.11
.25
.67
U
E
D
-E
S
.15
?
.17
.56
.42
0
.31
.56
.46
.50
.88
.11
.33
.29
.46
.29
.67
.69
.31
.36
.46
.27
.50
.54
.33
.30
.60
.43
.29
.50
.38
.13
.14
.17
?
.23
0
.17
.40
.83
.80
0
.27
.46
0
.13
U
E
D
-F
R
.50
.56
.75
.67
0
.18
.56
.50
.14
.44
.20
.17
.42
.53
.22
.64
.58
.44
.57
.30
.50
.38
.17
.42
.50
.22
.57
.31
.38
.44
.42
.23
.40
.33
.46
.39
.33
.75
1
.10
?
.11
.46
.31
.38
U
E
D
-H
U
1
.75
.67
.88
.67
.50
.75
1
?
.80
1
?
.38
.60
.83
.75
1
1
?
.86
?
.50
.86
.75
1
?
.80
1
?
1
?
.80
.80
.83
1
1
1
?
1
.80
.82
.67
1
1
.83
.50
.33
.67
.86
?
.67
1
?
1
.83
.40
U
M
D
-C
Z
.40
.50
.64
.50
.50
.67
.29
.50
.50
.80
?
.25
.40
.27
.71
.60
.44
.79
?
.79
?
.70
.80
.63
.67
.50
.63
.44
.63
.60
.43
.75
.64
.91
?
.56
.11
.54
.67
.44
.25
.67
1
.27
.50
.88
.69
.17
.62
U
P
C
-E
S
.40
.42
.44
.21
.15
?
.44
.38
.25
.38
.54
.43
.30
.50
.23
.40
.55
.33
.33
.22
.47
.13
.50
.50
.38
.25
.43
.14
.33
.63
.56
.44
.36
.53
.25
.39
.50
.50
.50
.10
?
.33
.63
.50
.40
.31
>
O
T
H
E
R
S
.29
.54
.51
.41
.15
.35
.51
.52
.43
.51
.25
.34
.39
.55
.33
.57
.57
.3
.5
.44
.52
.49
.48
.56
.50
.35
.46
.57
.39
.50
.49
.46
.49
.32
.51
.45
.4
.22
.56
.58
.19
.28
.53
.48
.11
.3
.52
?
O
T
H
E
R
S
.41
.7
.66
.55
.28
.44
.67
.64
.57
.65
.38
.47
.54
.7
.45
.71
.70
.39
.63
.55
.63
.61
.57
.7
.64
.49
.60
.71
.51
.62
.62
.57
.62
.45
.68
.58
.55
.33
.70
.72
.29
.40
.65
.62
.19
.43
.63
Table
53:
S
entence-levelranking
for
the
A
ll-E
nglish
N
ew
s
Task.
106
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 1?28,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Findings of the 2009 Workshop on Statistical Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb cs jhu edu
Philipp Koehn
University of Edinburgh
pkoehn inf ed ac uk
Christof Monz
University of Amsterdam
christof science uva nl
Josh Schroeder
University of Edinburgh
j schroeder ed ac uk
Abstract
This paper presents the results of the
WMT09 shared tasks, which included a
translation task, a system combination
task, and an evaluation task. We con-
ducted a large-scale manual evaluation of
87 machine translation systems and 22
system combination entries. We used the
ranking of these systems to measure how
strongly automatic metrics correlate with
human judgments of translation quality,
for more than 20 metrics. We present a
new evaluation technique whereby system
output is edited and judged for correctness.
1 Introduction
This paper presents the results of the shared tasks
of the 2009 EACL Workshop on Statistical Ma-
chine Translation, which builds on three previ-
ous workshops (Koehn and Monz, 2006; Callison-
Burch et al, 2007; Callison-Burch et al, 2008).
There were three shared tasks this year: a transla-
tion task between English and five other European
languages, a task to combine the output of multiple
machine translation systems, and a task to predict
human judgments of translation quality using au-
tomatic evaluation metrics. The performance on
each of these shared task was determined after a
comprehensive human evaluation.
There were a number of differences between
this year?s workshop and last year?s workshop:
? Larger training sets ? In addition to annual
increases in the Europarl corpus, we released
a French-English parallel corpus verging on 1
billion words. We also provided large mono-
lingual training sets for better language mod-
eling of the news translation task.
? Reduced number of conditions ? Previ-
ous workshops had many conditions: 10
language pairs, both in-domain and out-of-
domain translation, and three types of man-
ual evaluation. This year we eliminated
the in-domain Europarl test set and defined
sentence-level ranking as the primary type of
manual evaluation.
? Editing to evaluate translation quality ?
Beyond ranking the output of translation sys-
tems, we evaluated translation quality by hav-
ing people edit the output of systems. Later,
we asked annotators to judge whether those
edited translations were correct when shown
the source and reference translation.
The primary objectives of this workshop are to
evaluate the state of the art in machine transla-
tion, to disseminate common test sets and pub-
lic training data with published performance num-
bers, and to refine evaluation methodologies for
machine translation. All of the data, translations,
and human judgments produced for our workshop
are publicly available.1 We hope they form a
valuable resource for research into statistical ma-
chine translation, system combination, and auto-
matic evaluation of translation quality.
2 Overview of the shared translation and
system combination tasks
The workshop examined translation between En-
glish and five other languages: German, Spanish,
French, Czech, and Hungarian. We created a test
set for each language pair by translating news-
paper articles. We additionally provided training
data and a baseline system.
1http://statmt.org/WMT09/results.html
1
2.1 Test data
The test data for this year?s task was created by
hiring people to translate news articles that were
drawn from a variety of sources during the pe-
riod from the end of September to mid-October
of 2008. A total of 136 articles were selected, in
roughly equal amounts from a variety of Czech,
English, French, German, Hungarian, Italian and
Spanish news sites:2
Hungarian: hvg.hu (10), Napi (2), MNO (4),
Ne?pszabadsa?g (4)
Czech: iHNed.cz (3), iDNES.cz (4), Li-
dovky.cz (3), aktua?lne?.cz (2), Novinky (1)
French: dernieresnouvelles (1), Le Figaro (2),
Les Echos (4), Liberation (4), Le Devoir (9)
Spanish: ABC.es (11), El Mundo (12)
English: BBC (11), New York Times (6), Times
of London (4),
German: Su?ddeutsche Zeitung (3), Frankfurter
Allgemeine Zeitung (3), Spiegel (8), Welt (3)
Italian: ADN Kronos (5), Affari Italiani (2),
ASCA (1), Corriere della Sera (4), Il Sole 24
ORE (1), Il Quotidiano (1), La Republica (8)
Note that Italian translation was not one of this
year?s official translation tasks.
The translations were created by the members
of EuroMatrix consortium who hired a mix of
professional and non-professional translators. All
translators were fluent or native speakers of both
languages. Although we made efforts to proof-
read all translations, many sentences still contain
minor errors and disfluencies. All of the transla-
tions were done directly, and not via an interme-
diate language. For instance, each of the 20 Hun-
garian articles were translated directly into Czech,
English, French, German, Italian and Spanish.
The total cost of creating the test sets consisting
of roughly 80,000 words across 3027 sentences in
seven languages was approximately 31,700 euros
(around 39,800 dollars at current exchange rates,
or slightly more than $0.08/word).
Previous evaluations additionally used test sets
drawn from the Europarl corpus. Our rationale be-
hind discontinuing the use of Europarl as a test set
was that it overly biases towards statistical systems
that were trained on this particular domain, and
2For more details see the XML test files. The docid
tag gives the source and the date for each document in the
test set, and the origlang tag indicates the original source
language.
that European Parliament proceedings were less of
general interest than news stories. We focus on a
single task since the use of multiple test sets in the
past spread our resources too thin, especially in the
manual evaluation.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
train language models, and development sets to
tune parameters. Some statistics about the train-
ing materials are given in Figure 1.
109 word parallel corpus
To create the large French-English parallel cor-
pus, we conducted a targeted web crawl of bilin-
gual web sites. These sites came from a variety of
sources including the Canadian government, the
European Union, the United Nations, and other
international organizations. The crawl yielded on
the order of 40 million files, consisting of more
than 1TB of data. Pairs of translated documents
were identified using a set of simple heuristics to
transform French URLs into English URLs (for in-
stance, by replacing fr with en). Documents that
matched were assumed to be translations of each
other.
All HTML and PDF documents were converted
into plain text, which yielded 2 million French
files paired with their English equivalents. Text
files were split so that they contained one sen-
tence per line and had markers between para-
graphs. They were sentence-aligned in batches of
10,000 document pairs, using a sentence aligner
that incorporates IBM Model 1 probabilities in ad-
dition to sentence lengths (Moore, 2002). The
document-aligned corpus contained 220 million
segments with 2.9 billion words on the French side
and 215 million segments with 2.5 billion words
on the English side. After sentence alignment,
there were 177 million sentence pairs with 2.5 bil-
lion French words and 2.2 billion English words.
The sentence-aligned corpus was cleaned to re-
move sentence pairs which consisted only of num-
bers or paragraph markers, or where the French
and English sentences were identical. The later
step helped eliminate documents that were not
actually translated, which was necessary because
we did not perform language identification. After
cleaning, the parallel corpus contained 105 million
sentence pairs with 2 billion French words and 1.8
billion English words.
2
Europarl Training Corpus
Spanish? English French? English German? English
Sentences 1,411,589 1,428,799 1,418,115
Words 40,067,498 41,042,070 44,692,992 40,067,498 39,516,645 37,431,872
Distinct words 154,971 108,116 129,166 107,733 320,180 104,269
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 74,512 64,223 82,740 79,930
Words 2,052,186 1,799,312 1,831,149 1,560,274 2,051,369 1,977,200 1,733,865 1,891,559
Distinct words 56,578 41,592 46,056 38,821 92,313 43,383 105,280 41,801
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
Hunglish Training Corpus CzEng Training Corpus
Hungarian? English
Sentences 1,517,584
Words 26,114,985 31,467,693
Distinct words 717,198 192,901
Czech? English
Sentences 1,096,940
Words 15,336,783 17,909,979
Distinct words 339,683 129,176
Europarl Language Model Data
English Spanish French German
Sentence 1,658,841 1,607,419 1,676,435 1,713,715
Words 44,983,136 45,382,287 50,577,097 41,457,414
Distinct words 117,577 162,604 138,621 348,197
News Language Model Data
English Spanish French German Czech Hungarian
Sentence 21,232,163 1,626,538 6,722,485 10,193,376 5,116,211 4,209,121
Words 504,094,159 48,392,418 167,204,556 185,639,915 81,743,223 86,538,513
Distinct words 1,141,895 358,664 660,123 1,668,387 929,318 1,313,578
News Test Set
English Spanish French German Czech Hungarian Italian
Sentences 2525
Words 65,595 68,092 72,554 62,699 55,389 54,464 64,906
Distinct words 8,907 10,631 10,609 12,277 15,387 16,167 11,046
News System Combination Development Set
English Spanish French German Czech Hungarian Italian
Sentences 502
Words 11,843 12,499 12,988 11,235 9,997 9,628 11,833
Distinct words 2,940 3,176 3,202 3,471 4,121 4,133 3,318
Figure 1: Statistics for the training and test sets used in the translation task. The number of words is
based on the provided tokenizer and the number of distinct words is the based on lowercased tokens.
3
In addition to cleaning the sentence-aligned par-
allel corpus we also de-duplicated the corpus, re-
moving all sentence pairs that occured more than
once in the parallel corpus. Many of the docu-
ments gathered in our web crawl were duplicates
or near duplicates, and a lot of the text is repeated,
as with web site navigation. We further elimi-
nated sentence pairs that varied from previous sen-
tences by only numbers, which helped eliminate
template web pages such as expense reports. We
used a Bloom Filter (Talbot and Osborne, 2007) to
do de-duplication, so it may have discarded more
sentence pairs than strictly necessary. After de-
duplication, the parallel corpus contained 28 mil-
lion sentence pairs with 0.8 billion French words
and 0.7 billion English words.
Monolingual news corpora
We have crawled the news sources that were the
basis of our test sets (and a few more additional
sources) since August 2007. This allowed us to
assemble large corpora in the target domain to be
mainly used as training data for language mod-
eling. We collected texts from the beginning of
our data collection period to one month before the
test set period, segmented these into sentences and
randomized the order of the sentences to obviate
copyright concerns.
2.3 Baseline system
To lower the barrier of entry for newcomers to the
field, we provided Moses, an open source toolkit
for phrase-based statistical translation (Koehn et
al., 2007). The performance of this baseline sys-
tem is similar to the best submissions in last year?s
shared task. Twelve participating groups used the
Moses toolkit for the development of their system.
2.4 Submitted systems
We received submissions from 22 groups from
20 institutions, as listed in Table 1, a similar
turnout to last year?s shared task. Of the 20
groups that participated with regular system sub-
missions in last year?s shared task, 12 groups re-
turned this year. A major hurdle for many was
a DARPA/GALE evaluation that occurred at the
same time as this shared task.
We also evaluated 7 commercial rule-based MT
systems, and Google?s online statistical machine
translation system. We note that Google did not
submit an entry itself. Its entry was created by
the WMT09 organizers using Google?s online sys-
tem.3 In personal correspondence, Franz Och
clarified that the online system is different from
Google?s research system in that it runs at faster
speeds at the expense of somewhat lower transla-
tion quality. On the other hand, the training data
used by Google is unconstrained, which means
that it may have an advantage compared to the re-
search systems evaluated in this workshop, since
they were trained using only the provided materi-
als.
2.5 System combination
In total, we received 87 primary system submis-
sions along with 42 secondary submissions. These
were made available to participants in the sys-
tem combination shared task. Based on feedback
that we received on last year?s system combina-
tion task, we provided two additional resources to
participants:
? Development set: We reserved 25 articles
to use as a dev set for system combina-
tion (details of the set are given in Table
1). These were translated by all participating
sites, and distributed to system combination
participants along with reference translations.
? n-best translations: We requested n-best
lists from sites whose systems could produce
them. We received 25 100-best lists accom-
panying the primary system submissions, and
5 accompanying the secondary system sub-
missions.
In addition to soliciting system combination en-
tries for each of the language pairs, we treated sys-
tem combination as a way of doing multi-source
translation, following Schroeder et al (2009). For
the multi-source system combination task, we pro-
vided all 46 primary system submissions from any
language into English, along with an additional 32
secondary systems.
Table 2 lists the six participants in the system
combination task.
3 Human evaluation
As with past workshops, we placed greater em-
phasis on the human evaluation than on the auto-
matic evaluation metric scores. It is our contention
3http://translate.google.com
4
ID Participant
CMU-STATXFER Carnegie Mellon University?s statistical transfer system (Hanneman et al, 2009)
COLUMBIA Columbia University (Carpuat, 2009)
CU-BOJAR Charles University Bojar (Bojar et al, 2009)
CU-TECTOMT Charles University Tectogramatical MT (Bojar et al, 2009)
DCU Dublin City University (Du et al, 2009)
EUROTRANXP commercial MT provider from the Czech Republic
GENEVA University of Geneva (Wehrli et al, 2009)
GOOGLE Google?s production system
JHU Johns Hopkins University (Li et al, 2009)
JHU-TROMBLE Johns Hopkins University Tromble (Eisner and Tromble, 2006)
LIMSI LIMSI (Allauzen et al, 2009)
LIU Linko?ping University (Holmqvist et al, 2009)
LIUM-SYSTRAN University of Le Mans / Systran (Schwenk et al, 2009)
MORPHO Morphologic (Nova?k, 2009)
NICT National Institute of Information and Comm. Tech., Japan (Paul et al, 2009)
NUS National University of Singapore (Nakov and Ng, 2009)
PCTRANS commercial MT provider from the Czech Republic
RBMT1-5 commercial systems from Learnout&Houspie, Lingenio, Lucy, PROMT, SDL
RWTH RWTH Aachen (Popovic et al, 2009)
STUTTGART University of Stuttgart (Fraser, 2009)
SYSTRAN Systran (Dugast et al, 2009)
TALP-UPC Universitat Politecnica de Catalunya, Barcelona (R. Fonollosa et al, 2009)
UEDIN University of Edinburgh (Koehn and Haddow, 2009)
UKA University of Karlsruhe (Niehues et al, 2009)
UMD University of Maryland (Dyer et al, 2009)
USAAR University of Saarland (Federmann et al, 2009)
Table 1: Participants in the shared translation task. Not all groups participated in all language pairs.
ID Participant
BBN-COMBO BBN system combination (Rosti et al, 2009)
CMU-COMBO Carnegie Mellon University system combination (Heafield et al, 2009)
CMU-COMBO-HYPOSEL CMU system comb. with hyp. selection (Hildebrand and Vogel, 2009)
DCU-COMBO Dublin City University system combination
RWTH-COMBO RWTH Aachen system combination (Leusch et al, 2009)
USAAR-COMBO University of Saarland system combination (Chen et al, 2009)
Table 2: Participants in the system combination task.
5
Language Pair Sentence Ranking Edited Translations Yes/No Judgments
German-English 3,736 1,271 4,361
English-German 3,700 823 3,854
Spanish-English 2,412 844 2,599
English-Spanish 1,878 278 837
French-English 3,920 1,145 4,491
English-French 1,968 332 1,331
Czech-English 1,590 565 1,071
English-Czech 7,121 2,166 9,460
Hungarian-English 1,426 554 1,309
All-English 4,807 0 0
Multisource-English 2,919 647 2184
Totals 35,786 8,655 31,524
Table 3: The number of items that were judged for each task during the manual evaluation.
that automatic measures are an imperfect substi-
tute for human assessment of translation quality.
Therefore, we define the manual evaluation to be
primary, and use the human judgments to validate
automatic metrics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct it on the scale of
our workshop. We distributed the workload across
a number of people, including shared-task partic-
ipants, interested volunteers, and a small number
of paid annotators. More than 160 people partic-
ipated in the manual evaluation, with 100 people
putting in more than an hour?s worth of effort, and
30 putting in more than four hours. A collective
total of 479 hours of labor was invested.
We asked people to evaluate the systems? output
in two different ways:
? Ranking translated sentences relative to each
other. This was our official determinant of
translation quality.
? Editing the output of systems without dis-
playing the source or a reference translation,
and then later judging whether edited transla-
tions were correct.
The total number of judgments collected for the
different modes of annotation is given in Table 3.
In all cases, the output of the various translation
outputs were judged on equal footing; the output
of system combinations was judged alongside that
of the individual system, and the constrained and
unconstrained systems were judged together.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the in-
structions simple:
Rank translations from Best to Worst rel-
ative to the other choices (ties are al-
lowed).
In our the manual evaluation, annotators were
shown at most five translations at a time. For most
language pairs there were more than 5 systems
submissions. We did not attempt to get a com-
plete ordering over the systems, and instead relied
on random selection and a reasonably large sample
size to make the comparisons fair.
Relative ranking is our official evaluation met-
ric. Individual systems and system combinations
are ranked based on how frequently they were
judged to be better than or equal to any other sys-
tem. The results of this are reported in Section 4.
Appendix A provides detailed tables that contain
pairwise comparisons between systems.
3.2 Editing machine translation output
We experimented with a new type of evaluation
this year where we asked judges to edit the output
of MT systems. We did not show judges the refer-
ence translation, which makes our edit-based eval-
uation different than the Human-targeted Trans-
lation Error Rate (HTER) measure used in the
DARPA GALE program (NIST, 2008). Rather
than asking people to make the minimum number
of changes to the MT output in order capture the
same meaning as the reference, we asked them to
6
edit the translation to be as fluent as possible with-
out seeing the reference. Our hope was that this
would reflect people?s understanding of the out-
put.
The instructions that we gave our judges were
the following:
Correct the translation displayed, mak-
ing it as fluent as possible. If no correc-
tions are needed, select ?No corrections
needed.? If you cannot understand the
sentence well enough to correct it, select
?Unable to correct.?
Each translated sentence was shown in isolation
without any additional context. A screenshot is
shown in Figure 2.
Since we wanted to prevent judges from see-
ing the reference before editing the translations,
we split the test set between the sentences used
in the ranking task and the editing task (because
they were being conducted concurrently). More-
over, annotators edited only a single system?s out-
put for one source sentence to ensure that their un-
derstanding of it would not be influenced by an-
other system?s output.
3.3 Judging the acceptability of edited output
Halfway through the manual evaluation period, we
stopped collecting edited translations, and instead
asked annotators to do the following:
Indicate whether the edited transla-
tions represent fully fluent and meaning-
equivalent alternatives to the reference
sentence. The reference is shown with
context, the actual sentence is bold.
In addition to edited translations, unedited items
that were either marked as acceptable or as incom-
prehensible were also shown. Judges gave a sim-
ple yes/no indication to each item. A screenshot is
shown in Figure 3.
3.4 Inter- and Intra-annotator agreement
In order to measure intra-annotator agreement
10% of the items were repeated and evaluated
twice by each judge. In order to measure inter-
annotator agreement 40% of the items were ran-
domly drawn from a common pool that was shared
across all annotators so that we would have items
that were judged by multiple annotators.
INTER-ANNOTATOR AGREEMENT
Evaluation type P (A) P (E) K
Sentence ranking .549 .333 .323
Yes/no to edited output .774 .5 .549
INTRA-ANNOTATOR AGREEMENT
Evaluation type P (A) P (E) K
Sentence ranking .707 .333 .561
Yes/no to edited output .866 .5 .732
Table 4: Inter- and intra-annotator agreement for
the two types of manual evaluation
We measured pairwise agreement among anno-
tators using the kappa coefficient (K) which is de-
fined as
K =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance.
For inter-annotator agreement we calculated
P (A) for the yes/no judgments by examining all
items that were annotated by two or more anno-
tators, and calculating the proportion of time they
assigned identical scores to the same items. For
the ranking tasks we calculated P (A) by examin-
ing all pairs of systems which had been judged by
two or more judges, and calculated the proportion
of time that they agreed that A > B, A = B, or
A < B. Intra-annotator agreement was computed
similarly, but we gathered items that were anno-
tated on multiple occasions by a single annotator.
Table 4 gives K values for inter-annotator and
intra-annotator agreement. These give an indi-
cation of how often different judges agree, and
how often single judges are consistent for repeated
judgments, respectively. The interpretation of
Kappa varies, but according to Landis and Koch
(1977), 0 ? .2 is slight, .2 ? .4 is fair, .4 ? .6 is
moderate, .6? .8 is substantial and the rest almost
perfect.
Based on these interpretations the agreement for
yes/no judgments is moderate for inter-annotator
agreement and substantial for intra-annotator
agreement, but the inter-annotator agreement for
sentence level ranking is only fair.
We analyzed two possible strategies for improv-
ing inter-annotator agreement on the ranking task:
First, we tried discarding initial judgments to give
7
Edit MT Output
You have judged 19 sentences for WMT09 Multisource-English News Editing, 468 sentences total taking 74.4 seconds per sentence.
Original: They are often linked to other alterations sleep as nightmares, night terrors, the nocturnal enuresis (pee in bed) or the sleepwalking, but it is not 
always the case.
Edit:
Reset Edit
    Edited.
    No corrections needed.
    Unable to correct.
Annotator: ccb Task: WMT09 Multisource-English News Editing
Instructions: 
Correct the translation displayed, making it as fluent as possble. If no corrections are needed, select "No corrections needed." If you cannot understand
the sentence well enough to correct it, select "Unable to correct."
They are often linked to other sleep disorders, such as nightmares, night terrors, the nocturnal enuresis (bedwetting) or sleepwalking, but this is 
not always the case.
http://www.statmt.org/wmt09/judge/do_task.php
WMT09 Manual Evaluation
Figure 2: This screenshot shows an annotator editing the output of a machine translation system.
http://www.statmt.org/wmt09/judge/do_task.php
WMT09 Manual Evaluation
Judge Edited MT Output
You have judged 84 sentences for WMT09 French-English News Edit Acceptance, 459 sentences total taking 64.9 seconds per sentence.
Source: Au m?me moment, les gouvernements belges, hollandais et luxembourgeois ont en parti nationalis? le conglom?rat europ?en financier, Fortis. 
Les analystes de Barclays Capital ont d?clar? que les n?gociations fr?n?tiques de ce week end, conclues avec l'accord de sauvetage" semblent ne pas avoir 
r?ussi ? faire revivre le march?". 
Alors que la situation ?conomique se d?t?riorasse, la demande en mati?res premi?res, p?trole inclus, devrait se ralentir. 
"la prospective d'?quit? globale, de taux d'int?r?t et d'?change des march?s, est devenue incertaine" ont ?crit les analystes de Deutsche Bank dans une 
lettre ? leurs investisseurs." 
"nous pensons que les mati?res premi?res ne pourront ?chapper ? cette contagion. 
Reference: Meanwhile, the Belgian, Dutch and Luxembourg governments partially nationalized the European financial conglomerate Fortis. 
Analysts at Barclays Capital said the frantic weekend negotiations that led to the bailout agreement "appear to have failed to revive market sentiment." 
As the economic situation deteriorates, the demand for commodities, including oil, is expected to slow down. 
"The outlook for global equity, interest rate and exchange rate markets has become increasingly uncertain," analysts at Deutsche Bank wrote in a note to 
investors. 
"We believe commodities will be unable to escape the contagion.
Translation Verdict
While the economic situation is deteriorating, demand for commodities, including oil, should decrease.
Yes No
While the economic situation is deteriorating, the demand for raw materials, including oil, should slow down.
Yes No
Alors que the economic situation deteriorated, the request in rawmaterial enclosed, oil, would have to slow down.
Yes  No
While the financial situation damaged itself, the first matters affected, oil included, should slow down themselves.
Yes  No
While the economic situation is depressed, demand for raw materials, including oil, will be slow.
Yes No
Annotator: ccb Task: WMT09 French-English News Edit Acceptance
Instructions: 
Indicate whether the edited translations represent fully fluent and meaning-equivalent alternatives to the reference sentence. 
The reference is shown with context, the actual sentence is bold.
Figure 3: This screenshot shows an annotator judging the acceptability of edited translations.
8
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0.40
0.41
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.51
0.52
0.53
0.54
0.55
0.56
0.57
0.58
0.59
0.60
0.61
Inter-annotator agreement
Intra-annotator agreement
Proportion of judgments retained
Figure 4: The effect of discarding every annota-
tors? initial judgments, up to the first 50 items
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0.40
0.41
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.51
0.52
0.53
0.54
0.55
0.56
0.57
0.58
0.59
0.60
0.61
Inter-annotator agreement
Intra-annotator agreement
Proportion of judgments retained
Figure 5: The effect of removing annotators with
the lowest agreement, disregarding up to 40 anno-
tators
9
annotators a chance to learn to how to perform
the task. Second, we tried disregarding annota-
tors who have very low agreement with others, by
throwing away judgments for the annotators with
the lowest judgments.
Figures 4 and 5 show how the K values im-
prove for intra- and inter-annotator agreement un-
der these two strategies, and what percentage of
the judgments are retained as more annotators are
removed, or as the initial learning period is made
longer. It seems that the strategy of removing the
worst annotators is the best in terms of improv-
ing inter-annotator K, while retaining most of the
judgments. If we remove the 33 judges with the
worst agreement, we increase the inter-annotator
K from fair to moderate, and still retain 60% of
the data.
For the results presented in the rest of the paper,
we retain all judgments.
4 Translation task results
We used the results of the manual evaluation to
analyze the translation quality of the different sys-
tems that were submitted to the workshop. In our
analysis, we aimed to address the following ques-
tions:
? Which systems produced the best translation
quality for each language pair?
? Did the system combinations produce better
translations than individual systems?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
Table 6 shows best individual systems. We de-
fine the best systems as those which had no other
system that was statistically significantly better
than them under the Sign Test at p ? 0.1.4 Multi-
ple systems are listed for many language pairs be-
cause it was not possible to draw a statistically sig-
nificant difference between the systems. Commer-
cial translation software (including Google, Sys-
tran, Morphologic, PCTrans, Eurotran XP, and
anonymized RBMT providers) did well in each of
the language pairs. Research systems that utilized
4In one case this definition meant that the system that was
ranked the highest overall was not considered to be one of
the best systems. For German-English translation RBMT5
was ranked highest overall, but was statistically significantly
worse than RBMT2.
only the provided data did as well as commercial
vendors in half of the language pairs.
The table also lists the best systems among
those which used only the provided materials.
To determine this decision we excluded uncon-
strained systems which employed significant ex-
ternal resources. Specifically, we ruled out all of
the commercial systems, since Google has access
to significantly greater data sources for its statisti-
cal system, and since the commercial RBMT sys-
tems utilize knowledge sources not available to
other workshop participants. The remaining sys-
tems were research systems that employ statisti-
cal models. We were able to draw distinctions
between half of these for each of the language
pairs. There are some borderline cases, for in-
stance LIMSI only used additional monolingual
training resources, and LIUM/Systran used addi-
tional translation dictionaries as well as additional
monolingual resources.
Table 5 summarizes the performance of the
system combination entries by listing the best
ranked combinations, and by indicating whether
they have a statistically significant difference with
the best individual systems. In general, system
combinations performed as well as the best indi-
vidual systems, but not statistically significantly
better than them. Moreover, it was hard to draw
a distinction between the different system combi-
nation strategies themselves. There are a number
of possibilities as to why we failed to find signifi-
cant differences:
? The number of judgments that we collected
were not sufficient to find a difference. Al-
though we collected several thousand judg-
ments for each language pair, most pairs of
systems were judged together fewer than 100
times.
? It is possible that the best performing indi-
vidual systems were sufficiently better than
the other systems and that it is difficult to im-
prove on them by combining them.
? Individual systems could have been weighted
incorrectly during the development stage,
which could happen if the automatic evalu-
ation metrics scores on the dev set did not
strongly correlate with human judgments.
? The lack of distinction between different
combinations could be due to the fact that
10
Language Pair Best system combinations Entries Significantly different than
best individual systems?
German-English RWTH-COMBO, BBN-COMBO,
CMU-COMBO, USAAR-COMBO
5 BBN-COMBO>GOOGLE, SYSTRAN,
USAAR-COMBO<RMBT2,
no difference for others
English-German USAAR-COMBO 1 worse than 3 best systems
Spanish-English CMU-COMBO, USAAR-COMBO,
BBN-COMBO
3 each better than one of the RBMT
systems, but there was no difference
with GOOGLE, TALP-UPC
English-Spanish USAAR-COMBO 1 no difference
French-English CMU-COMBO-HYPOSEL,
DCU-COMBO, CMU-COMBO
5 no difference
English-French USAAR-COMBO, DCU-COMBO 2 USAAR-COMBO>UKA,
DCU-COMBO>SYSTRAN, LIMSI,
no difference with others
Czech-English CMU-COMBO 2 no difference
Hungarian-English CMU-COMBO-HYPOSEL,
CMU-COMBO
3 both worse than MORPHO
Multisource-English RWTH-COMBO 3 n/a
Table 5: A comparison between the best system combinations and the best individual systems. It was
generally difficult to draw a statistically significant differences between the two groups, and between the
combinations themselves.
there is significant overlap in the strategies
that they employ.
Improved system combination warrants further in-
vestigation. We would suggest collecting addi-
tional judgments, and doing oracle experiments
where the contributions of individual systems are
weighted according to human judgments of their
quality.
Understandability
Our hope is that judging the acceptability of edited
output as discussed in Section 3 gives some indi-
cation of how often a system?s output was under-
standable. Figure 6 gives the percentage of times
that each system?s edited output was judged to
be acceptable (the percentage also factors in in-
stances when judges were unable to improve the
output because it was incomprehensible).
The edited output of the best perform-
ing systems under this evaluation model were
deemed acceptable around 50% of the time
for French-English, English-French, English-
Spanish, German-English, and English-German.
For Spanish-English the edited output of the best
system was acceptable around 40% of the time, for
English-Czech it was 30% and for Czech-English
and Hungarian-English it was around 20%.
This style of manual evaluation is experimental
and should not be taken to be authoritative. Some
caveats about this measure:
? Editing translations without context is diffi-
cult, so the acceptability rate is probably an
underestimate of how understandable a sys-
tem actually is.
? There are several sources of variance that are
difficult to control for: some people are better
at editing, and some sentences are more dif-
ficult to edit. Therefore, variance in the un-
derstandability of systems is difficult to pin
down.
? The acceptability measure does not strongly
correlate with the more established method of
ranking translations relative to each other for
all the language pairs.5
Please also note that the number of corrected
translations per system are very low for some
language pairs, as low as 23 corrected sentences
per system for the language pair English?French.
5The Spearman rank correlation coefficients for how the
two types of manual evaluation rank systems are .67 for de-
en, .67 for fr-en, .06 for es-en, .50 for cz-en, .36 for hu-en,
.65 for en-de, .02 for en-fr, -.6 for en-es, and .94 for en-cz.
11
French?English
625?836 judgments per system
System C? ?others
GOOGLE ? no .76
DCU ? yes .66
LIMSI ? no .65
JHU ? yes .62
UEDIN ? yes .61
UKA yes .61
LIUM-SYSTRAN no .60
RBMT5 no .59
CMU-STATXFER ? yes .58
RBMT1 no .56
USAAR no .55
RBMT3 no .54
RWTH ? yes .52
COLUMBIA yes .50
RBMT4 no .47
GENEVA no .34
English?French
422?517 judgments per system
System C? ?others
LIUM-SYSTRAN ? no .73
GOOGLE ? no .68
UKA ?? yes .66
SYSTRAN ? no .65
RBMT3 ? no .65
DCU ?? yes .65
LIMSI ? no .64
UEDIN ? yes .60
RBMT4 no .59
RWTH yes .58
RBMT5 no .57
RBMT1 no .54
USAAR no .48
GENEVA no .38
Hungarian?English
865?988 judgments per system
System C? ?others
MORPHO ? no .75
UMD ? yes .66
UEDIN yes .45
German?English
651?867 judgments per system
System C? ?others
RBMT5 no .66
USAAR ? no .65
GOOGLE ? no .65
RBMT2 ? no .64
RBMT3 no .64
RBMT4 no .62
STUTTGART ?? yes .61
SYSTRAN ? no .60
UEDIN ? yes .59
UKA ? yes .58
UMD ? yes .56
RBMT1 no .54
LIU ? yes .50
RWTH yes .50
GENEVA no .33
JHU-TROMBLE yes .13
English?German
977?1226 judgments per system
System C? ?others
RBMT2 ? no .66
RBMT3 ? no .64
RBMT5 ? no .64
USAAR no .58
RBMT4 no .58
RBMT1 no .57
GOOGLE no .54
UKA ? yes .54
UEDIN ? yes .51
LIU ? yes .49
RWTH ? yes .48
STUTTGART yes .43
Czech?English
1257?1263 judgments per system
System C? ?others
GOOGLE ? no .75
UEDIN ? yes .57
CU-BOJAR ? yes .51
Spanish?English
613?801 judgments per system
System C? ?others
GOOGLE ? no .70
TALP-UPC ?? yes .59
UEDIN ? yes .56
RBMT1 ? no .55
RBMT3 ? no .55
RBMT5 ? no .55
RBMT4 ? no .53
RWTH ? yes .51
USAAR no .51
NICT yes .37
English?Spanish
632?746 judgments per system
System C? ?others
RBMT3 ? no .66
UEDIN ?? yes .66
GOOGLE ? no .65
RBMT5 ? no .64
RBMT4 no .61
NUS ? yes .59
TALP-UPC yes .58
RWTH yes .51
RBMT1 no .25
USAAR no .48
English?Czech
4626?4784 judgments per system
System C? ?others
PCTRANS ? no .67
EUROTRANXP ? no .67
GOOGLE no .66
CU-BOJAR ? yes .61
UEDIN yes .53
CU-TECTOMT yes .48
Systems are listed in the order of how often their translations were ranked higher than or equal to any
other system. Ties are broken by direct comparison.
C? indicates constrained condition, meaning only using the supplied training data and possibly standard
monolingual linguistic tools (but no additional corpora).
? indicates a win in the category, meaning that no other system is statistically significantly better at
p-level?0.1 in pairwise comparison.
? indicates a constrained win, no other constrained system is statistically better.
For all pairwise comparisons between systems, please check the appendix.
Table 6: Official results for the WMT09 translation task, based on the human evaluation (ranking trans-
lations relative to each other)
12
Given these low numbers, the numbers presented
in Figure 6 should not be read as comparisons be-
tween systems, but rather viewed as indicating the
state of machine translation for different language
pairs.
5 Shared evaluation task overview
In addition to allowing us to analyze the transla-
tion quality of different systems, the data gath-
ered during the manual evaluation is useful for
validating the automatic evaluation metrics. Last
year, NIST began running a similar ?Metrics
for MAchine TRanslation? challenge (Metrics-
MATR), and presented their findings at a work-
shop at AMTA (Przybocki et al, 2008).
In this year?s shared task we evaluated a number
of different automatic metrics:
? Bleu (Papineni et al, 2002)?Bleu remains
the de facto standard in machine translation
evaluation. It calculates n-gram precision and
a brevity penalty, and can make use of multi-
ple reference translations as a way of captur-
ing some of the allowable variation in trans-
lation. We use a single reference translation
in our experiments.
? Meteor (Agarwal and Lavie, 2008)?Meteor
measures precision and recall for unigrams
and applies a fragmentation penalty. It uses
flexible word matching based on stemming
and WordNet-synonymy. meteor-ranking is
optimized for correlation with ranking judg-
ments.
? Translation Error Rate (Snover et al,
2006)?TER calculates the number of ed-
its required to change a hypothesis transla-
tion into a reference translation. The possi-
ble edits in TER include insertion, deletion,
and substitution of single words, and an edit
which moves sequences of contiguous words.
Two variants of TER are also included: TERp
(Snover et al, 2009), a new version which in-
troduces a number of different features, and
(Bleu ? TER)/2, a combination of Bleu and
Translation Edit Rate.
? MaxSim (Chan and Ng, 2008)?MaxSim
calculates a similarity score by comparing
items in the translation against the reference.
Unlike most metrics which do strict match-
ing, MaxSim computes a similarity score
for non-identical items. To find a maxi-
mum weight matching that matches each sys-
tem item to at most one reference item, the
items are then modeled as nodes in a bipar-
tite graph.
? wcd6p4er (Leusch and Ney, 2008)?a mea-
sure based on cder with word-based substitu-
tion costs. Leusch and Ney (2008) also sub-
mitted two contrastive metrics: bleusp4114,
a modified version of BLEU-S (Lin and
Och, 2004), with tuned n-gram weights, and
bleusp, with constant weights. wcd6p4er
is an error measure and bleusp is a quality
score.
? RTE (Pado et al, 2009)?The RTE metric
follows a semantic approach which applies
recent work in rich textual entailment to the
problem of MT evaluation. Its predictions are
based on a regression model over a feature
set adapted from an entailment systems. The
features primarily model alignment quality
and (mis-)matches of syntactic and semantic
structures.
? ULC (Gime?nez and Ma`rquez, 2008)?ULC
is an arithmetic mean over other automatic
metrics. The set of metrics used include
Rouge, Meteor, measures of overlap between
constituent parses, dependency parses, se-
mantic roles, and discourse representations.
The ULC metric had the strongest correlation
with human judgments in WMT08 (Callison-
Burch et al, 2008).
? wpF and wpBleu (Popovic and Ney, 2009) -
These metrics are based on words and part of
speech sequences. wpF is an n-gram based F-
measure which takes into account both word
n-grams and part of speech n-grams. wp-
BLEU is a combnination of the normal Blue
score and a part of speech-based Bleu score.
? SemPOS (Kos and Bojar, 2009) ? the Sem-
POS metric computes overlapping words, as
defined in (Gime?nez and Ma`rquez, 2007),
with respect to their semantic part of speech.
Moreover, it does not use the surface repre-
sentation of words but their underlying forms
obtained from the TectoMT framework.
13
re
f
b
b
n
-
c
g
o
o
g
l
e
c
m
u
-
c
o
m
b
o
c
u
-
b
o
j
a
r
u
e
d
i
n
0.140.160.180.230.250.98
Czech-English
r
e
f
g
o
o
g
l
e
c
m
u
-
s
t
a
t
x
b
b
n
-
c
o
m
b
o
c
o
l
u
m
b
i
a
c
m
u
-
c
o
m
b
o
c
m
u
-
c
o
m
b
o
-
h
y
p
d
c
u
-
c
o
m
b
o
u
k
a
r
w
t
h
r
b
m
t
4
l
i
m
s
i
r
b
m
t
1
l
i
u
m
-
s
y
s
t
r
n
u
s
a
a
r
-
c
o
m
b
o
d
c
u
u
e
d
i
n
j
h
u
r
b
m
t
3
r
b
m
t
5
u
s
a
a
r
g
e
n
e
v
a
0.210.280.280.280.290.300.310.330.330.340.340.340.350.380.390.400.410.410.470.500.520.85
French-English
r
e
f
r
w
t
h
-
c
c
m
u
-
c
o
m
b
o
b
b
n
-
c
o
m
b
o
r
b
m
t
5
u
s
a
a
r
-
c
g
o
o
g
l
e
c
m
u
-
c
m
b
-
h
u
s
a
a
r
r
b
m
t
3
s
t
u
t
t
g
a
r
t
r
b
m
t
4
r
b
m
t
1
u
k
a
r
w
t
h
u
m
d
u
e
d
i
n
r
b
m
t
2
s
y
s
t
r
a
n
l
i
u
g
e
n
e
v
a
j
h
u
-
t
r
o
m
b
l
e
0.030.060.200.210.250.260.260.270.280.300.300.310.310.320.330.340.350.360.370.410.470.83
German-English
r
e
f
u
s
a
a
r
-
c
g
o
o
g
l
e
r
b
m
t
5
r
w
t
h
t
a
l
p
-
u
p
c
u
e
d
i
n
u
s
a
a
r
r
b
m
t
3
n
u
s
r
b
m
t
4
r
b
m
t
1
0.080.100.190.210.270.270.280.320.330.380.520.69
English-Spanish
r
e
f
u
s
a
a
r
-
c
g
o
o
g
l
e
r
b
m
t
5
r
b
m
t
3
b
b
n
-
c
m
b
u
e
d
i
n
t
a
l
p
-
u
p
c
r
b
m
t
1
r
w
t
h
c
m
u
-
c
r
b
m
t
4
n
i
c
t
u
s
a
a
r
0.230.260.270.280.280.280.280.310.310.360.370.380.410.88
Spanish-English
r
e
f
g
o
o
g
l
e
p
c
t
r
a
n
s
e
u
r
o
t
r
a
n
x
p
u
e
d
i
n
c
u
-
b
o
j
a
r
c
u
-
t
e
c
t
o
m
t
0.190.210.230.260.320.320.91
English-Czech
r
e
f
g
o
o
g
l
e
u
k
a
l
i
m
s
i
u
s
a
a
r
-
c
r
b
m
t
3
r
b
m
t
5
u
e
d
i
n
u
s
a
a
r
r
b
m
t
4
l
i
u
m
-
s
y
s
r
w
t
h
d
c
u
-
c
m
b
d
c
u
s
y
s
t
r
a
n
r
b
m
t
1
g
e
n
e
v
a
0.080.100.220.270.300.310.320.340.370.400.400.430.440.450.480.490.79
English-French
r
e
f
m
o
r
p
h
o
c
m
u
-
c
m
b
-
h
c
m
u
-
c
o
m
b
o
u
m
d
u
e
d
i
n
b
b
n
-
c
m
b
0.110.120.150.190.210.220.93
Hungarian-English
r
e
f
r
b
m
t
5
r
b
m
t
3
g
o
o
g
l
e
r
b
m
t
1
r
b
m
t
2
u
s
a
a
r
u
s
a
a
r
-
c
m
b
r
b
m
t
4
r
w
t
h
u
k
a
u
e
d
i
n
s
t
u
t
t
g
a
r
t
l
i
u
0.120.180.190.260.280.310.310.320.330.350.370.420.470.85
English-German
r
e
f
r
w
t
h
b
b
n
c
m
u
0.250.270.320.90
Multsource-English
Figure 6: The percent of time that each system?s edited output was judged to be an acceptable translation.
These numbers also include judgments of the system?s output when it was marked either incomprehen-
sible or acceptable and left unedited. Note that the reference translation was edited alongside the system
outputs. Error bars show one positive and one negative standard deviation for the systems in that lan-
guage pair.
14
5.1 Measuring system-level correlation
We measured the correlation of the automatic met-
rics with the human judgments of translation qual-
ity at the system-level using Spearman?s rank cor-
relation coefficient ?. We converted the raw scores
assigned to each system into ranks. We assigned
a human ranking to the systems based on the per-
cent of time that their translations were judged to
be better than or equal to the translations of any
other system in the manual evaluation.
When there are no ties ? can be calculated using
the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all sys-
tems are ranked in the same order) and ?1 (where
the systems are ranked in the reverse order). Thus
an automatic evaluation metric with a higher abso-
lute value for ? is making predictions that are more
similar to the human judgments than an automatic
evaluation metric with a lower absolute ?.
5.2 Measuring sentence-level consistency
Because the sentence-level judgments collected
in the manual evaluation are relative judgments
rather than absolute judgments, it is not possi-
ble for us to measure correlation at the sentence-
level in the same way that previous work has done
(Kulesza and Shieber, 2004; Albrecht and Hwa,
2007a; Albrecht and Hwa, 2007b).
Rather than calculating a correlation coefficient
at the sentence-level we instead ascertained how
consistent the automatic metrics were with the hu-
man judgments. The way that we calculated con-
sistency was the following: for every pairwise
comparison of two systems on a single sentence by
a person, we counted the automatic metric as being
consistent if the relative scores were the same (i.e.
the metric assigned a higher score to the higher
ranked system). We divided this by the total num-
ber of pairwise comparisons to get a percentage.
Because the systems generally assign real num-
bers as scores, we excluded pairs that the human
annotators ranked as ties.
de
-e
n
(2
1
sy
st
em
s)
fr
-e
n
(2
1
sy
st
em
s)
es
-e
n
(1
3
sy
st
em
s)
cz
-e
n
(5
sy
st
em
s)
hu
-e
n
(6
sy
st
em
s)
A
ve
ra
ge
ulc .78 .92 .86 1 .6 .83
maxsim .76 .91 .98 .7 .66 .8
rte (absolute) .64 .91 .96 .6 .83 .79
meteor-rank .64 .93 .96 .7 .54 .75
rte (pairwise) .76 .59 .78 .8 .83 .75
terp -.72 -.89 -.94 -.7 -.37 -.72
meteor-0.6 .56 .93 .87 .7 .54 .72
meteor-0.7 .55 .93 .86 .7 .26 .66
bleu-ter/2 .38 .88 .78 .9 -.03 .58
nist .41 .87 .75 .9 -.14 .56
wpF .42 .87 .82 1 -.31 .56
ter -.43 -.83 -.84 -.6 -.01 -.54
nist (cased) .42 .83 .75 1 -.31 .54
bleu .41 .88 .79 .6 -.14 .51
bleusp .39 .88 .78 .6 -.09 .51
bleusp4114 .39 .89 .78 .6 -.26 .48
bleu (cased) .4 .86 .8 .6 -.31 .47
wpbleu .43 .86 .8 .7 -.49 .46
wcd6p4er -.41 -.89 -.76 -.6 .43 -.45
Table 7: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation into English.
en
-d
e
(1
3
sy
st
em
s)
en
-f
r
(1
6
sy
st
em
s)
en
-e
s
(1
1
sy
st
em
s)
en
-c
z
(5
sy
st
em
s)
A
ve
ra
ge
terp .03 -.89 -.58 -.4 -.46
ter -.03 -.78 -.5 -.1 -.35
bleusp4114 -.3 .88 .51 .1 .3
bleusp -.3 .87 .51 .1 .29
bleu -.43 .87 .36 .3 .27
bleu (cased) -.45 .87 .35 .3 .27
bleu-ter/2 -.37 .87 .44 .1 .26
wcd6p4er .54 -.89 -.45 -.1 -.22
nist (cased) -.47 .84 .35 .1 .2
nist -.52 .87 .23 .1 .17
wpF -.06 .9 .58 n/a n/a
wpbleu .07 .92 .63 n/a n/a
Table 8: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation out of English.
15
SemPOS .4 BLEUtecto .3
Meteor .4 BLEU .3
GTM(e=0.5)tecto .4 NISTlemma .1
GTM(e=0.5)lemma .4 NIST .1
GTM(e=0.5) .4 BLEUlemma .1
WERtecto .3 WERlemma -.1
TERtecto .3 WER -.1
PERtecto .3 TERlemma -.1
F-measuretecto .3 TER -.1
F-measurelemma .3 PERlemma -.1
F-measure .3 PER -.1
NISTtecto -.3
Table 9: The system-level correlation for auto-
matic metrics ranking five English-Czech systems
6 Evaluation task results
6.1 System-level correlation
Table 7 shows the correlation of automatic met-
rics when they rank systems that are translating
into English. Note that TERp, TER and wcd6p4er
are error metrics, so a negative correlation is bet-
ter for them. The strength of correlation varied for
the different language pairs. The automatic met-
rics were able to rank the French-English systems
reasonably well with correlation coefficients in the
range of .8 and .9. In comparison, metrics per-
formed worse for Hungarian-English, where half
of the systems had negative correlation. The ULC
metric once again had strongest correlation with
human judgments of translation quality. This was
followed closely by MaxSim and RTE, with Me-
teor and TERp doing respectably well in 4th and
5th place. Notably, Bleu and its variants were the
worst performing metrics in this translation direc-
tion.
Table 8 shows correlation for metrics which op-
erated on languages other than English. Most of
the best performing metrics that operate on En-
glish do not work for foreign languages, because
they perform some linguistic analysis or rely on
a resource like WordNet. For translation into for-
eign languages TERp was the best system overall.
The wpBleu and wpF metrics also did extremely
well, performing the best in the language pairs that
they were applied to. wpBleu and wpF were not
applied to Czech because the authors of the met-
ric did not have a Czech tagger. English-German
proved to be the most problematic language pair
to automatically evaluate, with all of the metrics
having a negative correlation except wpBleu and
TER.
Table 9 gives detailed results for how well vari-
ations on a number of automatic metrics do for
the task of ranking five English-Czech systems.6
These systems were submitted by Kos and Bojar
(2009), and they investigate the effects of using
Prague Dependency Treebank annotations during
automatic evaluation. They linearizing the Czech
trees and evaluated either the lemmatized forms of
the Czech (lemma) read off the trees or the Tec-
togrammatical form which retained only lemma-
tized content words (tecto). The table also demon-
strates SemPOS, Meteor, and GTM perform better
on Czech than many other metrics.
6.2 Sentence-level consistency
Tables 10 and 11 show the percent of times that the
metrics? scores were consistent with human rank-
ings of every pair of translated sentences.7 Since
we eliminated sentence pairs that were judged to
be equal, the random baseline for this task is 50%.
Many metrics failed to reach the baseline (includ-
ing most metrics in the out-of-English direction).
This indicates that sentence-level evaluation of
machine translation quality is very difficult. RTE
and ULC again do the best overall for the into-
English direction. They are followed closely by
wpF and wcd6p4er, which considerably improve
their performance over their system-level correla-
tions.
We tried a variant on measuring sentence-level
consistency. Instead of using the scores assigned
to each individual sentence, we used the system-
level score and applied it to every sentence that
was produced by that system. These can be
thought of as a metric?s prior expectation about
how a system should preform, based on their per-
formance on the whole data set. Tables 12 and 13
show that using the system-level scores in place
of the sentence-level scores results in considerably
higher consistency with human judgments. This
suggests an interesting line of research for improv-
ing sentence-level predictions by using the perfor-
mance on a larger data set as a prior.
7 Summary
As in previous editions of this workshop we car-
ried out an extensive manual and automatic eval-
uation of machine translation performance for
translating from European languages into English,
6PCTRANS was excluded from the English-Czech systems
because its SGML file was malformed.
7Not all metrics entered into the sentence-level task.
16
fr
-e
n
(6
26
8
pa
ir
s)
de
-e
n
(6
38
2
pa
ir
s)
es
-e
n
(4
10
6
pa
ir
s)
cz
-e
n
(2
25
1
pa
ir
s)
hu
-e
n
(2
19
3
pa
ir
s)
xx
-e
n
(1
95
2
pa
ir
s)
O
ve
ra
ll
(2
31
52
pa
ir
s)
ulc .55 .56 .51 .50 .51 .51 .54
rte (absolute) .54 .56 .51 .50 .55 .51 .53
wpF .54 .55 .50 .47 .48 .51 .52
wcd6p4er .54 .54 .49 .48 .48 .50 .52
maxsim .53 .55 .49 .47 .50 .49 .52
bleusp .54 .55 .49 .47 .46 .50 .51
bleusp4114 .53 .55 .48 .47 .46 .50 .51
rte (pairwise) .49 .48 .52 .53 .55 .52 .51
terp .52 .53 .48 .46 .45 .48 .50
meteor-0.6 .50 .53 .46 .48 .47 .47 .49
meteor-rank .50 .52 .46 .48 .47 .47 .49
meteor-0.7 .49 .52 .46 .48 .47 .47 .49
ter .48 .47 .43 .41 .40 .42 .45
wpbleu .46 .45 .46 .39 .35 .45 .44
Table 10: Sentence-level consistency of the auto-
matic metrics with human judgments for transla-
tions into English. Italicized numbers fall below
the random-choice baseline.
en
-f
r
(2
96
7
pa
ir
s)
en
-d
e
(6
56
3
pa
ir
s)
en
-e
s
(3
24
9
pa
ir
s)
en
-c
z
(1
12
42
pa
ir
s)
O
ve
ra
ll
(2
40
21
pa
ir
s)
wcd6p4er .57 .47 .52 .49 .50
bleusp4114 .57 .46 .54 .49 .50
bleusp .57 .46 .53 .48 .49
ter .50 .41 .45 .37 .41
terp .51 .39 .48 .27 .36
wpF .57 .46 .54 n/a .51
wpbleu .53 .37 .46 n/a .43
Table 11: Sentence-level consistency of the auto-
matic metrics with human judgments for transla-
tions out of English. Italicized numbers fall below
the random-choice baseline.
fr
-e
n
(6
26
8
pa
ir
s)
de
-e
n
(6
38
2
pa
ir
s)
es
-e
n
(4
10
6
pa
ir
s)
cz
-e
n
(2
25
1
pa
ir
s)
hu
-e
n
(2
19
3
pa
ir
s)
O
ve
ra
ll
(2
12
00
pa
ir
s)
Oracle .61 .63 .59 .61 .67 .62
rte (absolute) .60 .61 .59 .57 .65 .61
ulc .61 .62 .58 .61 .59 .60
maxsim .61 .62 .59 .57 .61 .60
meteor-rank .61 .61 .59 .57 .61 .60
meteor-0.6 .61 .61 .58 .57 .60 .60
rte (pairwise) .56 .61 .57 .59 .64 .59
terp .60 .61 .59 .57 .56 .59
meteor-0.7 .61 .61 .58 .57 .55 .59
ter .60 .59 .57 .55 .51 .58
wpF .60 .59 .57 .61 .46 .58
bleusp .61 .59 .56 .55 .48 .57
bleusp4114 .61 .59 .56 .55 .46 .57
wcd6p4er .61 .59 .57 .55 .44 .57
wpbleu .60 .59 .57 .57 .43 .57
Table 12: Consistency of the automatic met-
rics when their system-level ranks are treated as
sentence-level scores. Oracle shows the consis-
tency of using the system-level human ranks that
are given in Table 6.
en
-f
r
(2
96
7
pa
ir
s)
en
-d
e
(6
56
3
pa
ir
s)
en
-e
s
(3
24
9
pa
ir
s)
en
-c
z
(1
12
42
pa
ir
s)
O
ve
ra
ll
(2
40
21
pa
ir
s)
Oracle .62 .59 .63 .60 .60
terp .62 .50 .59 .53 .54
ter .61 .51 .58 .50 .53
bleusp .62 .48 .59 .50 .52
bleusp4114 .63 .48 .59 .50 .52
wcd6p4er .62 .46 .58 .50 .52
wpbleu .63 .51 .60 n/a .56
wpF .63 .50 .59 n/a .55
Table 13: Consistency of the automatic met-
rics when their system-level ranks are treated as
sentence-level scores. Oracle shows the consis-
tency of using the system-level human ranks that
are given in Table 6.
17
and vice versa.
The number of participants remained stable
compared to last year?s WMT workshop, with
22 groups from 20 institutions participating in
WMT09. This year?s evaluation also included 7
commercial rule-based MT systems and Google?s
online statistical machine translation system.
Compared to previous years, we have simpli-
fied the evaluation conditions by removing the in-
domain vs. out-of-domain distinction focusing on
news translations only. The main reason for this
was eliminating the advantage statistical systems
have with respect to test data that are from the
same domain as the training data.
Analogously to previous years, the main focus
of comparing the quality of different approaches
is on manual evaluation. Here, also, we reduced
the number of dimensions with respect to which
the different systems are compared, with sentence-
level ranking as the primary type of manual eval-
uation. In addition to the direct quality judgments
we also evaluated translation quality by having
people edit the output of systems and have as-
sessors judge the correctness of the edited output.
The degree to which users were able to edit the
translations (without having access to the source
sentence or reference translation) served as a mea-
sure of the overall comprehensibility of the trans-
lation.
Although the inter-annotator agreement in the
sentence-ranking evaluation is only fair (as mea-
sured by the Kappa score), agreement can be im-
proved by removing the first (up to 50) judgments
of each assessor, focusing on the judgments that
were made once the assessors are more familiar
with the task. Inter-annotator agreement with re-
spect to correctness judgments of the edited trans-
lations were higher (moderate), which is proba-
bly due to the simplified evaluation criterion (bi-
nary judgments versus rankings). Inter-annotator
agreement for both conditions can be increased
further by removing the judges with the worst
agreement. Intra-annotator agreement on the other
hand was considerably higher ranging between
moderate and substantial.
In addition to the manual evaluation criteria we
applied a large number of automated metrics to
see how they correlate with the human judgments.
There is considerably variation between the differ-
ent metrics and the language pairs under consid-
eration. As in WMT08, the ULC metric had the
highest overall correlation with human judgments
when translating into English, with MaxSim and
RTE following closely behind. TERp and wpBleu
were best when translating into other languages.
Automatically predicting human judgments at
the sentence-level proved to be quite challeng-
ing with many of the systems performing around
chance. We performed an analysis that showed
that if metrics? system-level scores are used in
place of their scores for individual sentences, that
they do quite a lot better. This suggests that prior
probabilities ought to be integrated into sentence-
level scoring.
All data sets generated by this workshop, in-
cluding the human judgments, system translations
and automatic scores, are publicly available for
other researchers to analyze.8
Acknowledgments
This work was supported in parts by the EuroMa-
trix project funded by the European Commission
(6th Framework Programme), the GALE program
of the US Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022, and
the US National Science Foundation under grant
IIS-0713448.
We are grateful to Holger Schwenk and Preslav
Nakov for pointing out the potential bias in our
method for ranking systems when self-judgments
are excluded. We analyzed the results and found
that this did not hold. We would like to thank
Maja Popovic for sharing thoughts about how to
improve the manual evaluation. Thanks to Cam
Fordyce for helping out with the manual evalua-
tion again this year.
An extremely big thanks to Sebastian Pado for
helping us work through the logic of segment-level
scoring of automatic evaluation metric.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor, M-
BLEU and M-TER: Evaluation metrics for high-
correlation with human rankings of machine trans-
lation output. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 115?118,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Joshua Albrecht and Rebecca Hwa. 2007a. A re-
examination of machine learning approaches for
8http://www.statmt.org/wmt09/results.
html
18
sentence-level MT evaluation. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2007), Prague, Czech Re-
public.
Joshua Albrecht and Rebecca Hwa. 2007b. Regres-
sion for sentence-level MT evaluation with pseudo
references. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2007), Prague, Czech Republic.
Alexandre Allauzen, Josep Crego, Aure?lien Max, and
Fran cois Yvon. 2009. LIMSI?s statistical transla-
tion systems for WMT?09. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Mar-
tin Popel, Jan Pta?c?ek, Jan Rous?, and Zdene?k
Z?abokrtsky?. 2009. English-Czech MT in 2008. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation (WMT07), Prague, Czech Repub-
lic.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08), Colmbus, Ohio.
Marine Carpuat. 2009. Toward using morphology
in French-English phrase-based SMT. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, Athens, Greece, March. Association for
Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2008. An automatic
metric for machine translation evaluation based on
maximum similary. In In the Metrics-MATR Work-
shop of AMTA-2008, Honolulu, Hawaii.
Yu Chen, Michael Jellinghaus, Andreas Eisele,
Yi Zhang, Sabine Hunsicker, Silke Theison, Chris-
tian Federmann, and Hans Uszkoreit. 2009. Com-
bining multi-engine translations with moses. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Jinhua Du, Yifan He, Sergio Penkale, and Andy Way.
2009. MATREX: The DCU MT system for WMT
2009. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
Lo??c Dugast, Jean Senellart, and Philipp Koehn.
2009. Statistical post editing and dictionary ex-
traction: Systran/Edinburgh submissions for ACL-
WMT2009. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
Chris Dyer, Hendra Setiawan, Yuval Marton, and
Philip Resnik. 2009. The University of Mary-
land statistical machine translation system for the
fourth workshop on machine translation. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Jason Eisner and Roy W. Tromble. 2006. Local
search with very large-scale neighborhoods for op-
timal permutations in machine translation. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American chapter of the Associ-
ation for Computational Linguistics (HLT/NAACL-
2006), New York, New York.
Christian Federmann, Silke Theison, Andreas Eisele,
Hans Uszkoreit, Yu Chen, Michael Jellinghaus, and
Sabine Hunsicker. 2009. Translation combina-
tion using factored word substitution. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, Athens, Greece, March. Association for
Computational Linguistics.
Alexander Fraser. 2009. Experiments in morphosyn-
tactic processing for translating to and from German.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguis-
tic features for automatic evaluation of heterogenous
MT systems. In Proceedings of ACL Workshop on
Machine Translation.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. A smor-
gasbord of features for automatic MT evaluation.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198.
Greg Hanneman, Vamshi Ambati, Jonathan H. Clark,
Alok Parlikar, and Alon Lavie. 2009. An
improved statistical transfer system for French-
English machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Kenneth Heafield, Greg Hanneman, and Alon Lavie.
2009. Machine translation system combination
with flexible word ordering. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Almut Silja Hildebrand and Stephan Vogel. 2009.
CMU system combination for WMT?09. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
19
Maria Holmqvist, Sara Stymne, Jody Foo, and Lars
Ahrenberg. 2009. Improving alignment for SMT by
reordering and augmenting the training corpus. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2009. Edin-
burgh?s submission to all tracks of the WMT2009
shared task with reordering and speed improvements
to Moses. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL
2006 Workshop on Statistical Machine Translation,
New York, New York.
Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, Chris
Callison-Burch, Alexandra Constantin, Brooke
Cowan, Chris Dyer, Marcello Federico, Evan
Herbst, Hieu Hoang, Christine Moran, Wade Shen,
and Richard Zens. 2007. Open source toolkit for
statistical machine translation: Factored translation
models and confusion network decoding. CLSP
Summer Workshop Final Report WS-2006, Johns
Hopkins University.
Kamil Kos and Ondr?ej Bojar. 2009. Evaluation of Ma-
chine Translation Metrics for Czech as the Target
Language. Prague Bulletin of Mathematical Lin-
guistics, 92. in print.
Alex Kulesza and Stuart M. Shieber. 2004. A learn-
ing approach to improving sentence-level MT evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation, Baltimore, MD, October 4?6.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Gregor Leusch and Hermann Ney. 2008. BLEUSP,
PINVWER, CDER: Three improved MT evaluation
measures. In In the Metrics-MATR Workshop of
AMTA-2008, Honolulu, Hawaii.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2009. The RWTH system combination system for
WMT 2009. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2004), Barcelona, Spain.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
the 5th Biennial Conference of the Association for
Machine Translation in the Americas (AMTA-2002),
Tiburon, California.
Preslav Nakov and Hwee Tou Ng. 2009. NUS
at WMT09: Domain adaptation experiments for
English-Spanish machine translation of news com-
mentary text. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, Athens,
Greece, March. Association for Computational Lin-
guistics.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
translation system for the EACL-WMT 2009. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
NIST. 2008. Evaluation plan for gale go/no-go phase
3 / phase 3.5 translation evaluations. June 18, 2008.
Attila Nova?k. 2009. Morphologic?s submission for
the WMT 2009 shared task. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Sebastian Pado, Michel Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Machine transla-
tion evaluation with textual entailment features. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2002),
Philadelphia, Pennsylvania.
Michael Paul, Andrew Finch, and Eiichiro Sumita.
2009. NICT@WMT09: Model adaptation and
transliteration for Spanish-English SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Maja Popovic and Hermann Ney. 2009. Syntax-
oriented evaluation measures for machine transla-
tion output. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, Athens, Greece,
March. Association for Computational Linguistics.
20
Maja Popovic, David Vilar, Daniel Stein, Evgeny Ma-
tusov, and Hermann Ney. 2009. The RWTH ma-
chine translation system for WMT 2009. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Mark Przybocki, Kay Peterson, and Se-
bastien Bronsart. 2008. Official results
of the NIST 2008 ?Metrics for MAchine
TRanslation? challenge (MetricsMATR08).
http://nist.gov/speech/tests/metricsmatr/2008/results/.
Jose? A. R. Fonollosa, Maxim Khalilov, Marta R. Costa-
jussa?, Jose? B. Marin?o, Carlos A. Henra?quez Q.,
Adolfo Herna?ndez H., and Rafael E. Banchs. 2009.
The TALP-UPC phrase-based translation system
for EACL-WMT 2009. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hy-
pothesis alignment with flexible matching for build-
ing confusion networks: BBN system description
for WMT09 system combination task. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, Athens, Greece, March. Association for
Computational Linguistics.
Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009. Word lattices for multi-source translation.
In 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-
2009), Athens, Greece.
Holger Schwenk, Sadaf Abdul Rauf, Loic Barrault, and
Jean Senellart. 2009. SMT and SPE machine trans-
lation systems for WMT?09. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA-2006), Cambridge, Massachusetts.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy,
or HTER? exploring different human judgments
with a tunable MT metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale lms on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), Prague, Czech Repub-
lic.
Eric Wehrli, Luka Nerima, and Yves Scherrer.
2009. Deep linguistic multilingual translation
and bilingual dictionaries. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March. Association for Com-
putational Linguistics.
21
A Pairwise system comparisons by human judges
Tables 14?24 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row. Bolding indicates the winner of the two systems. The difference between
100 and the sum of the complimentary cells is the percent of time that the two systems were judged to
be equal.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
B Automatic scores
Tables 26 and 25 give the automatic scores for each of the systems.
G
E
N
E
V
A
G
O
O
G
L
E
JH
U
-T
R
O
M
B
L
E
L
IU
R
B
M
T
1
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
S
T
U
T
T
G
A
R
T
S
Y
S
T
R
A
N
U
E
D
IN
U
K
A
U
M
D
U
S
A
A
R
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
R
W
T
H
-C
O
M
B
O
U
S
A
A
R
-C
O
M
B
O
GENEVA .76? .08? .63? .54 .69? .73? .83? .78? .49? .77? .75? .74? .57? .74? .69? .75? .84? .60 .84? .71?
GOOGLE .15? .03? .23? .50 .43 .24? .39 .42 .39 .43 .33 .27? .29? .38 .48 .57? .44 .32 .35 .36
JHU-TROMBLE .75? .90? .77? .81? .84? .91? .94? .88? .79? .83? .83? .93? .89? .92? .90? .94? .90? .95? .91? .83?
LIU .29? .65? .12? .49 .63 .63? .57 .63? .41 .49 .46 .50 .49 .50 .41 .66? .53 .59? .62? .53
RBMT1 .32 .43 .11? .46 .42 .46 .50 .61? .34 .46 .58 .51 .42 .42 .56 .47 .53 .49 .58 .54
RBMT2 .25? .46 .09? .37 .45 .33 .45 .23? .3 .28 .47 .42 .31? .34 .39 .49 .61 .4 .32 .29?
RBMT3 .17? .59? .02? .26? .35 .46 .27 .45 .27 .36 .46 .42 .43 .26? .49 .4 .48 .58 .29 .31
RBMT4 .12? .47 .07? .37 .4 .45 .52 .60? .39 .39 .45 .39 .31? .29? .44 .54 .45 .37 .43 .30
RBMT5 .13? .34 .07? .30? .24? .57? .41 .29? .31 .50 .34 .3 .28? .43 .30 .49 .57 .3 .49 .21
RWTH .21? .55 .10? .41 .49 .55 .46 .46 .60 .44 .57 .48 .51? .41 .56 .64? .54 .56? .74? .59?
STUTTGART .17? .43 .13? .39 .43 .55 .39 .36 .33 .34 .38 .42 .52 .42 .49 .49 .28 .35 .56 .46
SYSTRAN .11? .63 .06? .42 .37 .47 .50 .32 .58 .34 .55 .36 .44 .35 .43 .61? .46 .41 .33 .44
UEDIN .10? .50? .03? .35 .49 .46 .39 .52 .55 .29 .39 .52 .35 .33 .42 .58? .43 .56 .59? .55
UKA .29? .58? .04? .32 .47 .63? .55 .54? .64? .24? .28 .39 .50 .29 .50 .48 .36 .57? .45 .45
UMD .16? .53 .08? .38 .49 .43 .63? .68? .49 .38 .39 .41 .50 .49 .46 .54 .44 .38 .46 .50
USAAR .19? .44 ? .41 .34 .49 .4 .44 .33 .36 .33 .45 .39 .32 .41 .46 .41 .31 .42 .11
BBN-COMBO .14? .31? .06? .26? .44 .44 .48 .36 .38 .23? .35 .26? .29? .34 .36 .37 .32 .23? .38 .32
CMU-COMBO .10? .36 .07? .37 .37 .36 .48 .40 .30 .28 .53 .41 .4 .43 .28 .34 .50 .33 .53 .44
CMU-COMBO-H .3 .46 ? .10? .39 .43 .40 .48 .57 .27? .41 .47 .28 .26? .38 .49 .65? .46 .41 .47
RWTH-COMBO .06? .38 ? .19? .36 .54 .43 .43 .30 .10? .33 .56 .22? .27 .23 .42 .32 .31 .41 .29
USAAR-COMBO .20? .55 .17? .3 .39 .57? .45 .59 .32 .27? .33 .47 .32 .33 .27 .16 .55 .44 .4 .50
> OTHERS .22 .51 .06 .38 .44 .52 .49 .49 .50 .33 .44 .48 .44 .42 .41 .47 .56 .48 .46 .51 .43
>= OTHERS .33 .65 .13 .50 .54 .64 .64 .62 .66 .50 .61 .60 .59 .58 .56 .65 .68 .63 .62 .70 .62
Table 14: Sentence-level ranking for the WMT09 German-English News Task
22
G
O
O
G
L
E
L
IU
R
B
M
T
1
R
B
M
T
2
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
S
T
U
T
T
G
A
R
T
U
E
D
IN
U
K
A
U
S
A
A
R
U
S
A
A
R
-C
O
M
B
O
GOOGLE .34? .56 .51 .55? .44 .56? .37 .41 .42 .45 .45 .43
LIU .58? .62? .55? .55? .61? .59? .37 .38 .47 .43 .58? .44
RBMT1 .39 .33? .56? .44 .50? .57? .41 .32? .37? .35? .45 .42
RBMT2 .35 .34? .34? .43 .37? .40 .25? .25? .31? .36? .37? .32?
RBMT3 .31? .35? .41 .35 .37? .41 .24? .25? .33? .43 .49 .36?
RBMT4 .48 .33? .33? .56? .55? .47 .37 .35? .34? .45 .44 .38
RBMT5 .36? .35? .33? .50 .53 .33 .36? .32? .35? .31? .25? .32?
RWTH .51 .46 .50 .60? .65? .51 .60? .38 .47 .48 .52 .54
STUTTGART .50 .47 .62? .65? .64? .57? .62? .46 .52? .54? .66? .53
UEDIN .50 .37 .53? .64? .62? .60? .55? .45 .28? .41 .53 .35
UKA .47 .42 .57? .58? .46 .44 .62? .35 .32? .36 .46 .41
USAAR .46 .36? .46 .55? .42 .42 .48? .42 .28? .39 .44 .41
USAAR-COMBO .37 .45 .54 .55? .55? .53 .61? .39 .40 .39 .46 .52
> OTHERS .44 .38 .48 .55 .53 .47 .54 .37 .33 .39 .42 .48 .41
>= OTHERS .54 .49 .57 .66 .64 .58 .64 .48 .43 .51 .54 .58 .52
Table 15: Sentence-level ranking for the WMT09 English-German News Task
G
O
O
G
L
E
N
IC
T
R
B
M
T
1
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
T
A
L
P
-U
P
C
U
E
D
IN
U
S
A
A
R
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
U
S
A
A
R
-C
O
M
B
O
GOOGLE .21? .40 .40 .41 .38 .23? .35 .31? .25? .36 .14 .21
NICT .74? .52 .53 .63? .64? .55? .61? .65? .59? .62? .78? .66?
RBMT1 .56 .40 .34 .44 .46 .35 .48 .42 .42 .57? .52 .54
RBMT3 .40 .39 .40 .34 .36 .42 .4 .55 .50 .57? .48 .62?
RBMT4 .55 .32? .41 .46 .47 .39 .49 .49 .48 .54 .57? .54
RBMT5 .54 .30? .35 .44 .38 .45 .50 .49 .23 .51 .51 .66?
RWTH .64? .29? .50 .53 .53 .49 .42 .46 .43 .44 .51 .58?
TALP-UPC .48 .24? .44 .47 .41 .36 .39 .36 .32? .47 .45 .50
UEDIN .61? .16? .48 .42 .41 .46 .44 .43 .44 .49 .51 .41
USAAR .69? .28? .47 .44 .38 .35 .43 .60? .48 .64? .58? .56?
BBN-COMBO .35 .20? .32? .36? .39 .37 .36 .39 .32 .31? .50 .40
CMU-COMBO .19 .15? .33 .39 .32? .37 .36 .31 .37 .21? .35 .31
USAAR-COMBO .23 .20? .42 .31? .39 .25? .27? .35 .35 .32? .36 .29
> OTHERS .50 .26 .42 .42 .42 .42 .39 .44 .43 .37 .49 .49 .50
>= OTHERS .70 .37 .55 .55 .53 .55 .51 .59 .56 .51 .64 .70 .69
Table 16: Sentence-level ranking for the WMT09 Spanish-English News Task
G
O
O
G
L
E
N
U
S
R
B
M
T
1
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
T
A
L
P
-U
P
C
U
E
D
IN
U
S
A
A
R
U
S
A
A
R
-C
O
M
B
O
GOOGLE .39 .21? .49 .36 .48 .34? .39 .33 .36? .21
NUS .50 .11? .62? .51 .51 .35 .25 .47 .36 .43
RBMT1 .76? .80? .79? .79? .83? .64? .76? .80? .67? .64?
RBMT3 .42 .31? .16? .30? .43 .34 .29? .56 .24? .32
RBMT4 .47 .32 .11? .52? .49 .38 .36 .51 .39 .38
RBMT5 .42 .40 .11? .49 .35 .31? .39 .47 .18? .47
RWTH .59? .52 .26? .54 .51 .61? .46 .56? .39 .55?
TALP-UPC .49 .41 .17? .63? .52 .51 .29 .45? .39 .41
UEDIN .50 .32 .17? .36 .37 .46 .30? .29? .32? .36
USAAR .58? .56 .23? .67? .53 .47? .51 .49 .61? .58?
USAAR-COMBO .31 .45 .21? .54 .49 .50 .30? .43 .43 .33?
> OTHERS .50 .45 .17 .56 .47 .53 .38 .42 .52 .37 .43
>= OTHERS .65 .59 .25 .66 .61 .64 .51 .58 .66 .48 .61
Table 17: Sentence-level ranking for the WMT09 English-Spanish News Task
23
C
M
U
-S
T
A
T
X
F
E
R
C
O
L
U
M
B
IA
D
C
U
G
E
N
E
V
A
G
O
O
G
L
E
JH
U
L
IM
S
I
L
IU
M
-S
Y
S
T
R
A
N
R
B
M
T
1
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
U
E
D
IN
U
K
A
U
S
A
A
R
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
D
C
U
-C
O
M
B
O
U
S
A
A
R
-C
O
M
B
O
CMU-STATXFER .37 .44 .17? .63? .47 .46 .58? .34 .32 .25? .42 .48 .46 .28 .38 .58? .47 .39 .41 .35
COLUMBIA .56 .56? .37 .71? .48 .56? .35 .45 .28? .38 .42 .41 .33 .58 .50 .64? .52 .64? .71? .58?
DCU .27 .29? .15? .67? .45 .33 .34 .29 .31 .29 .27? .24 .37 .21? .39 .61? .4 .36 .37 .1
GENEVA .76? .54 .73? .71? .65? .73? .62? .66? .76? .46 .79? .57 .74? .72? .67? .69? .52 .71? .67? .64?
GOOGLE .23? .17? .12? .13? .21? .35 .09? .20? .27? .31? .44 .16? .21? .33 .27? .28 .30 .34 .37 .16?
JHU .40 .26 .38 .22? .60? .31 .44 .27 .37 .29? .41 .33 .37 .48 .48 .53 .47 .31 .47 .29
LIMSI .4 .16? .38 .19? .56 .49 .29 .37 .27 .20? .38 .23? .33 .29 .38 .61? .47 .31 .36 .26?
LIUM-SYSTRAN .23? .30 .42 .33? .61? .27 .45 .48 .31 .41 .44 .32 .35 .41 .39 .54? .61? .24 .67? .36
RBMT1 .53 .23 .42 .19? .57? .46 .51 .45 .47 .33 .46 .33 .41 .30 .61 .77? .51 .41 .50 .41
RBMT3 .57 .63? .55 .15? .69? .44 .57 .52 .41 .22? .38 .51 .43 .43 .31 .57? .46 .47 .38 .55
RBMT4 .58? .35 .51 .36 .67? .60? .63? .35 .41 .59? .40 .55 .50 .71? .52? .63? .65? .65? .66? .38
RBMT5 .42 .49 .54? .09? .38 .49 .49 .37 .27 .29 .34 .38 .39 .51 .18 .42 .58 .48 .50 .60?
RWTH .38 .39 .45 .32 .63? .46 .51? .34 .56 .39 .32 .52 .48 .46 .46 .66? .62? .61? .66? .54?
UEDIN .41 .21 .31 .19? .68? .46 .42 .35 .41 .38 .31 .46 .33 .34 .41 .41 .35 .44 .63? .37
UKA .40 .31 .54? .19? .51 .37 .44 .33 .52 .51 .17? .27 .32 .49 .34 .39 .53 .36 .44 .29
USAAR .44 .43 .52 .26? .62? .48 .46 .30 .30 .58 .17? .24 .44 .47 .41 .65? .52 .70? .55 .41
BBN-COMBO .21? .21? .12? .23? .26 .32 .28? .23? .12? .26? .22? .49 .09? .34 .23 .19? .44 .49? .28 .21?
CMU-COMBO .41 .36 .4 .28 .30 .35 .47 .21? .29 .42 .23? .31 .17? .49 .25 .42 .31 .37 .29 .25
CMU-COMBO-H .24 .21? .38 .23? .37 .39 .31 .24 .31 .41 .28? .31 .14? .33 .34 .24? .18? .3 .29 .27
DCU-COMBO .41 .13? .42 .20? .37 .29 .50 .19? .44 .49 .23? .46 .20? .21? .37 .39 .31 .26 .46 .19?
USAAR-COMBO .41 .25? .18 .28? .66? .53 .52? .48 .41 .38 .53 .17? .21? .42 .42 .47 .58? .58 .47 .63?
> OTHERS .40 .31 .41 .23 .56 .43 .46 .36 .37 .41 .30 .40 .33 .41 .40 .40 .50 .47 .46 .49 .36
>= OTHERS .58 .5 .66 .34 .76 .62 .65 .60 .56 .54 .47 .59 .52 .61 .61 .55 .73 .66 .71 .67 .57
Table 18: Sentence-level ranking for the WMT09 French-English News Task
D
C
U
G
E
N
E
V
A
G
O
O
G
L
E
L
IM
S
I
L
IU
M
-S
Y
S
T
R
A
N
R
B
M
T
1
R
B
M
T
3
R
B
M
T
4
R
B
M
T
5
R
W
T
H
S
Y
S
T
R
A
N
U
E
D
IN
U
K
A
U
S
A
A
R
D
C
U
-C
O
M
B
O
U
S
A
A
R
-C
O
M
B
O
DCU .12? .39 .47 .44 .33 .44 .27 .45 .24? .49 .24 .46 .26? .39 .33
GENEVA .62? .73? .69? .80? .50? .71? .50? .52? .56? .66? .46? .56? .57 .74? .84?
GOOGLE .46 .15? .28 .42 .26 .44 .26? .34 .29? .44 .24 .32 .29 .36 .32
LIMSI .25 .16? .45 .48 .23? .43 .30 .45 .27 .42 .34 .4 .36 .53? .38
LIUM-SYSTRAN .24 ? .45 .32 .17? .29 .17? .21? .38 .29 .17? .35 .17? .41 .41
RBMT1 .39 .25? .51 .51? .53? .46 .40 .29 .52 .36 .60? .63? .41 .44 .60?
RBMT3 .36 .11? .37 .37 .52 .24 .25? .27? .31 .44 .43 .32 .27? .53 .44
RBMT4 .36 .19? .58? .37 .57? .23 .61? .42 .32 .50 .22 .39 .44 .53 .56?
RBMT5 .41 .17? .53 .39 .61? .38 .58? .30 .41 .52? .41 .48 .13 .54 .60
RWTH .59? .21? .63? .50 .47 .29 .44 .37 .31 .37 .35 .51 .16? .50? .57?
SYSTRAN .35 .20? .33 .39 .38 .40 .22 .29 .26? .44 .47 .33 .32 .60? .45
UEDIN .38 .11? .41 .28 .77? .33? .51 .44 .49 .32 .37 .30 .31 .56 .56?
UKA .36 .09? .46 .4 .45 .23? .50 .39 .29 .29 .47 .26 .19? .41 .56?
USAAR .66? .27 .52 .49 .70? .31 .61? .29 .32 .64? .62 .51 .61? .76? .65?
DCU-COMBO .32 .11? .30 .18? .45 .22 .29 .33 .29 .13? .27? .26 .41 .12? .21
USAAR-COMBO .40 ? .39 .17 .26 .17? .28 .20? .28 .20? .39 .04? .06? .08? .39
> OTHERS .41 .15 .47 .39 .52 .29 .45 .32 .35 .35 .45 .34 .42 .28 .51 .49
>= OTHERS .65 .38 .68 .64 .73 .54 .65 .59 .57 .58 .65 .60 .66 .48 .74 .77
Table 19: Sentence-level ranking for the WMT09 English-French News Task
C
U
-B
O
JA
R
G
O
O
G
L
E
U
E
D
IN
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
CU-BOJAR .54? .44 .45? .52?
GOOGLE .28? .32? .18? .23
UEDIN .38 .51? .38 .45?
BBN-COMBO .31? .39? .32 .38?
CMU-COMBO .28? .29 .27? .24?
> OTHERS .31 .43 .34 .31 .40
>= OTHERS .51 .75 .57 .65 .73
Table 20: Sentence-level ranking for the WMT09 Czech-English News Task
24
C
U
-B
O
JA
R
C
U
-T
E
C
T
O
M
T
E
U
R
O
T
R
A
N
X
P
G
O
O
G
L
E
P
C
T
R
A
N
S
U
E
D
IN
CU-BOJAR .31? .45? .43? .48? .30?
CU-TECTOMT .51? .54? .56? .58? .42?
EUROTRANXP .35? .26? .39 .38 .29?
GOOGLE .31? .30? .42 .43? .26?
PCTRANS .33? .27? .36 .38? .30?
UEDIN .42? .37? .52? .50? .53?
> OTHERS .38 .30 .46 .45 .48 .31
>= OTHERS .61 .48 .67 .66 .67 .53
Table 21: Sentence-level ranking for the WMT09 English-Czech News Task
M
O
R
P
H
O
U
E
D
IN
U
M
D
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
MORPHO .21? .28? .24? .27? .28?
UEDIN .70? .59? .45? .55? .50?
UMD .61? .26? .21? .29 .38
BBN-COMBO .67? .23? .48? .41? .52?
CMU-COMBO .59? .25? .35 .29? .42
CMU-COMBO-HYPOSEL .55? .15? .34 .27? .34
> OTHERS .62 .22 .41 .29 .37 .42
>= OTHERS .75 .45 .66 .54 .62 .68
Table 22: Sentence-level ranking for the WMT09 Hungarian-English News Task
G
O
O
G
L
E
C
Z
G
O
O
G
L
E
E
S
G
O
O
G
L
E
F
R
R
B
M
T
2 D
E
R
B
M
T
3 D
E
R
B
M
T
3 E
S
R
B
M
T
3 F
R
R
B
M
T
5 E
S
R
B
M
T
5 F
R
B
B
N
-C
O
M
B
O
C
Z
B
B
N
-C
O
M
B
O
D
E
B
B
N
-C
O
M
B
O
E
S
B
B
N
-C
O
M
B
O
F
R
B
B
N
-C
O
M
B
O
H
U
B
B
N
-C
O
M
B
O
X
X
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
D
E
C
M
U
-C
O
M
B
O
-H
Y
P
O
S
E
L
H
U
C
M
U
-C
O
M
B
O
C
Z
C
M
U
-C
O
M
B
O
H
U
C
M
U
-C
O
M
B
O
X
X
D
C
U
-C
O
M
B
O
F
R
R
W
T
H
-C
O
M
B
O
D
E
R
W
T
H
-C
O
M
B
O
X
X
U
S
A
A
R
-C
O
M
B
O
E
S
GOOGLECZ .61? .54? .47 .52 .51 .47 .61? .42 .38 .52 .55 .54 .11? .51 .48 .34 .49 .32 .53 .52 .50 .59 .53
GOOGLEES .33
? .42 .37 .38 .41 .35 .49 .45 .11? .39 .25 .36 .18? .26? .36 .22? .32 .18? .38 .4 .4 .38 .22
GOOGLEFR .27
? .42 .26? .36 .43 .47 .33 .35 .29? .23? .50 .23 .14? .29? .21? .11? .17? .22? .39 .48 .32 .36 .27
RBMT2DE .33 .49 .61? .41 .43 .25? .52 .38 .33 .41 .4 .55 .20? .66? .62? .18? .55 .35 .35 .58 .54 .61? .57?
RBMT3DE .37 .60 .54 .41 .42 .38 .45 .61 .48 .39 .40 .63? .32 .43 .25? .35 .35 .25? .56 .69? .46 .49 .46
RBMT3ES .34 .52 .46 .51 .54 .43 .36 .38 .30? .54 .41 .47 .25? .50 .42 .26? .43 .27? .52 .57 .47 .46 .26?
RBMT3FR .40 .58 .37 .63? .53 .57 .54 .50 .36 .64? .44 .55 .13? .60 .64? .4 .53 .31 .46 .48 .44 .52 .42
RBMT5ES .29
? .41 .55 .31 .48 .36 .33 .39 .16? .44 .50 .68? .23? .35 .48 .38 .37 .41 .60? .51 .51 .65? .32
RBMT5FR .47 .52 .45 .50 .33 .51 .34 .42 .29 .59 .44 .49 ? .49 .61? .28? .19? .35 .58? .60? .27 .59 .57
BBN-COMBOCZ .41 .74? .65? .55 .44 .67? .56 .80? .46 .46 .58 .70? .22? .73? .63? .32 .38 .48 .65? .72? .66? .70? .58
BBN-COMBODE .39 .54 .58? .41 .49 .44 .31? .44 .28 .49 .49 .52 .16? .52 .36 .22? .38 .33? .41 .68? .34 .52 .56
BBN-COMBOES .38 .40 .41 .43 .47 .55 .46 .25 .51 .31 .43 .44 .20? .50 .42 .30? .32 .29? .36 .62 .47 .44 .38
BBN-COMBOFR .38 .52 .35 .36 .27? .53 .40 .26? .33 .24? .44 .36 .12? .47 .47 .32 .44 .27? .41 .42 .33 .60? .35
BBN-COMBOHU .84? .75? .78? .60? .57 .70? .71? .62? .84? .65? .72? .63? .85? .78? .69? .60? .71? .50 .85? .78? .87? .86? .75?
BBN-COMBOXX .4 .54? .63? .34? .50 .47 .32 .45 .39 .20? .39 .45 .41 .14? .24? .21? .3 .21? .46 .40 .47 .41 .41
CMU-CMB-HYPDE .48 .43 .68? .29? .64? .46 .31? .30 .30? .23? .41 .39 .32 .19? .74? .21? .32 .31 .50 .74? .38 .56? .53
CMU-CMB-HYPHU .63 .75? .78? .70? .55 .63? .46 .58 .59? .50 .61? .70? .59 .13? .68? .69? .65? .39 .75? .71? .82? .80? .68?
CMU-COMBOCZ .32 .59 .81? .36 .50 .46 .41 .50 .60? .28 .54 .52 .47 .20? .55 .56 .26? .13? .55 .69? .57 .66? .55
CMU-COMBOHU .62 .76? .69? .58 .68? .67? .59 .54 .54 .48 .67? .64? .70? .32 .74? .60 .50 .77? .66? .72? .61 .82? .82?
CMU-COMBOXX .4 .50 .33 .51 .37 .43 .44 .29? .24? .32? .56 .43 .39 .13? .39 .39 .16? .30 .32? .39 .4 .46 .4
DCU-COMBOFR .44 .57 .29 .32 .25? .29 .26 .35 .27? .19? .23? .38 .42 .15? .34 .20? .12? .19? .17? .50 .55 .49 .30?
RWTH-COMBODE .41 .43 .52 .37 .39 .53 .50 .35 .53 .25? .40 .47 .54 .10? .47 .41 .07? .38 .30 .53 .38 .56 .49
RWTH-COMBOXX .31 .38 .44 .26? .41 .39 .31 .26? .32 .18? .29 .44 .19? .10? .36 .25? .11? .28? .15? .39 .42 .28 .44
USAAR-COMBOES .37 .37 .54 .21? .4 .58? .39 .47 .31 .32 .34 .28 .55 .11? .38 .38 .20? .38 .18? .44 .67? .43 .44
> OTHERS .41 .54 .54 .43 .45 .49 .41 .44 .44 .32 .46 .46 .50 .16 .51 .45 .26 .40 .29 .52 .57 .48 .55 .47
>= OTHERS .52 .67 .70 .55 .55 .57 .52 .58 .58 .43 .57 .59 .62 .27 .62 .58 .37 .52 .36 .63 .68 .59 .69 .62
Table 23: Sentence-level ranking for the WMT09 All-English News Task
25
B
B
N
-C
O
M
B
O
C
M
U
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
BBN-COMBO .37 .40?
CMU-COMBO .41 .44?
RWTH-COMBO .32? .34?
> OTHERS .36 .35 .42
>= OTHERS .62 .58 .67
Table 24: Sentence-level ranking for the WMT09 Multisource-English News Task
26
R
A
N
K
B
L
E
U
B
L
E
U
-C
A
S
E
D
B
L
E
U
-T
E
R
B
L
E
U
S
P
B
L
E
U
S
P
41
14
M
A
X
S
IM
M
E
T
E
O
R
-0
.6
M
E
T
E
O
R
-0
.7
M
E
T
E
O
R
-R
A
N
K
IN
G
N
IS
T
N
IS
T
-C
A
S
E
D
R
T
E
-A
B
S
O
L
U
T
E
R
T
E
-P
A
IR
W
IS
E
T
E
R
T
E
R
P
U
L
C
W
C
D
6P
4E
R
W
P
F
W
P
B
L
E
U
German-English News Task
BBN-COMBO 0.68 0.24 0.22 ?0.17 0.29 0.31 0.51 0.55 0.6 0.41 7.08 6.78 0.13 0.1 0.54 0.63 0.31 0.45 0.36 0.31
CMU-COMBO 0.63 0.22 0.21 ?0.19 0.28 0.29 0.49 0.54 0.58 0.4 6.95 6.71 0.12 0.09 0.56 0.66 0.29 0.47 0.35 0.29
CMU-COMBO-HYPOSEL 0.62 0.23 0.21 ?0.19 0.28 0.3 0.49 0.54 0.57 0.4 6.79 6.5 0.11 0.09 0.57 0.66 0.29 0.47 0.35 0.3
GENEVA 0.33 0.1 0.09 ?0.33 0.17 0.18 0.38 0.43 0.44 0.30 4.88 4.65 0.03 0.04 0.71 0.86 0.22 0.58 0.25 0.17
GOOGLE 0.65 0.21 0.20 ?0.2 0.27 0.28 0.48 0.54 0.57 0.39 6.85 6.65 0.11 0.11 0.56 0.65 0.29 0.48 0.35 0.28
JHU-TROMBLE 0.13 0.07 0.06 ?0.38 0.09 0.1 0.34 0.43 0.41 0.29 4.90 4.25 0.02 0.02 0.81 1 0.19 0.61 0.22 0.12
LIU 0.50 0.19 0.18 ?0.22 0.25 0.27 0.46 0.51 0.54 0.38 6.35 6.02 0.06 0.05 0.61 0.72 0.27 0.49 0.33 0.26
RBMT1 0.54 0.14 0.13 ?0.29 0.20 0.21 0.43 0.50 0.53 0.37 5.30 5.07 0.04 0.04 0.67 0.76 0.26 0.55 0.29 0.22
RBMT2 0.64 0.17 0.16 ?0.26 0.23 0.24 0.48 0.52 0.55 0.38 6.06 5.75 0.1 0.12 0.63 0.70 0.29 0.51 0.31 0.24
RBMT3 0.64 0.17 0.16 ?0.25 0.23 0.25 0.48 0.52 0.55 0.38 5.98 5.71 0.09 0.09 0.61 0.68 0.29 0.51 0.32 0.25
RBMT4 0.62 0.16 0.14 ?0.27 0.21 0.23 0.45 0.5 0.52 0.36 5.65 5.36 0.06 0.07 0.65 0.72 0.27 0.52 0.30 0.23
RBMT5 0.66 0.16 0.15 ?0.26 0.22 0.24 0.47 0.51 0.54 0.37 5.76 5.52 0.07 0.06 0.63 0.70 0.28 0.52 0.31 0.24
RWTH 0.50 0.19 0.18 ?0.21 0.25 0.26 0.45 0.50 0.53 0.36 6.44 6.24 0.06 0.03 0.60 0.74 0.27 0.49 0.33 0.26
RWTH-COMBO 0.7 0.23 0.22 ?0.18 0.29 0.30 0.50 0.55 0.59 0.41 7.06 6.81 0.11 0.07 0.54 0.63 0.30 0.46 0.36 0.31
STUTTGART 0.61 0.2 0.18 ?0.22 0.26 0.27 0.48 0.52 0.56 0.38 6.39 6.11 0.1 0.06 0.60 0.69 0.29 0.49 0.33 0.27
SYSTRAN 0.6 0.19 0.17 ?0.22 0.24 0.26 0.47 0.52 0.55 0.38 6.40 6.08 0.08 0.07 0.60 0.71 0.28 0.5 0.33 0.26
UEDIN 0.59 0.20 0.19 ?0.22 0.26 0.27 0.47 0.52 0.55 0.38 6.47 6.24 0.07 0.04 0.61 0.70 0.27 0.49 0.34 0.27
UKA 0.58 0.21 0.2 ?0.20 0.27 0.28 0.47 0.52 0.56 0.38 6.66 6.43 0.08 0.04 0.58 0.69 0.28 0.48 0.34 0.28
UMD 0.56 0.21 0.19 ?0.19 0.26 0.28 0.47 0.52 0.56 0.38 6.74 6.42 0.08 0.04 0.56 0.69 0.28 0.48 0.34 0.27
USAAR 0.65 0.17 0.15 ?0.26 0.23 0.24 0.47 0.51 0.54 0.38 5.89 5.64 0.06 0.05 0.64 0.71 0.28 0.52 0.31 0.24
USAAR-COMBO 0.62 0.17 0.16 ?0.25 0.23 0.24 0.47 0.51 0.55 0.38 5.99 6.85 0.07 0.06 0.64 0.70 0.28 0.51 0.32 0.25
Spanish-English News Task
BBN-COMBO 0.64 0.29 0.27 ?0.13 0.34 0.35 0.53 0.57 0.62 0.43 7.64 7.35 0.16 0.13 0.51 0.61 0.33 0.42 0.4 0.35
CMU-COMBO 0.7 0.28 0.27 ?0.13 0.33 0.35 0.53 0.58 0.62 0.43 7.65 7.46 0.21 0.2 0.51 0.60 0.34 0.42 0.40 0.36
GOOGLE 0.70 0.29 0.28 ?0.13 0.34 0.35 0.53 0.58 0.62 0.43 7.68 7.50 0.23 0.22 0.5 0.59 0.34 0.42 0.41 0.36
NICT 0.37 0.22 0.22 ?0.19 0.27 0.29 0.48 0.54 0.57 0.39 6.91 6.74 0.1 0.1 0.60 0.71 0.3 0.46 0.36 0.3
RBMT1 0.55 0.19 0.18 ?0.24 0.25 0.26 0.49 0.54 0.57 0.40 6.07 5.93 0.11 0.12 0.62 0.69 0.3 0.49 0.34 0.28
RBMT3 0.55 0.20 0.2 ?0.22 0.26 0.27 0.50 0.54 0.58 0.41 6.24 6.08 0.13 0.14 0.60 0.65 0.31 0.48 0.36 0.29
RBMT4 0.53 0.2 0.19 ?0.22 0.25 0.27 0.48 0.53 0.57 0.4 6.20 6.03 0.10 0.11 0.60 0.67 0.3 0.48 0.35 0.28
RBMT5 0.55 0.20 0.2 ?0.22 0.26 0.27 0.5 0.54 0.58 0.40 6.26 6.10 0.12 0.11 0.6 0.65 0.31 0.48 0.36 0.29
RWTH 0.51 0.24 0.23 ?0.16 0.3 0.31 0.49 0.54 0.58 0.4 7.12 6.95 0.11 0.08 0.56 0.68 0.31 0.45 0.37 0.32
TALP-UPC 0.59 0.26 0.25 ?0.15 0.31 0.33 0.51 0.56 0.6 0.41 7.28 7.02 0.13 0.11 0.54 0.64 0.32 0.44 0.38 0.33
UEDIN 0.56 0.26 0.25 ?0.15 0.32 0.33 0.51 0.56 0.60 0.42 7.25 7.04 0.16 0.1 0.55 0.64 0.32 0.43 0.39 0.34
USAAR 0.51 0.2 0.19 ?0.22 0.25 0.27 0.48 0.54 0.57 0.4 6.31 6.14 0.11 0.09 0.62 0.67 0.3 0.48 0.34 0.28
USAAR-COMBO 0.69 0.29 0.27 ?0.13 0.34 0.35 0.53 0.58 0.62 0.43 7.58 7.25 0.20 0.13 0.51 0.6 0.34 0.42 0.4 0.35
French-English News Task
BBN-COMBO 0.73 0.31 0.3 ?0.11 0.36 0.38 0.54 0.59 0.64 0.45 7.88 7.58 0.14 0.12 0.2 0.20 0.36 0.40 0.41 0.37
CMU-COMBO 0.66 0.3 0.29 ?0.12 0.35 0.36 0.53 0.58 0.63 0.44 7.72 7.57 0.15 0.12 0.24 0.26 0.35 0.41 0.41 0.37
CMU-COMBO-HYPOSEL 0.71 0.28 0.26 ?0.14 0.33 0.35 0.53 0.57 0.61 0.43 7.40 7.15 0.1 0.08 0.31 0.33 0.34 0.42 0.4 0.35
CMU-STATXFER 0.58 0.24 0.23 ?0.18 0.29 0.31 0.49 0.54 0.58 0.40 6.89 6.75 0.08 0.07 0.38 0.42 0.31 0.46 0.37 0.32
COLUMBIA 0.50 0.23 0.22 ?0.18 0.29 0.30 0.49 0.54 0.58 0.40 6.85 6.68 0.07 0.07 0.36 0.39 0.31 0.46 0.36 0.31
DCU 0.66 0.27 0.25 ?0.15 0.32 0.34 0.52 0.56 0.61 0.42 7.29 6.94 0.09 0.07 0.32 0.34 0.33 0.43 0.38 0.34
DCU-COMBO 0.67 0.31 0.31 ?0.11 0.36 0.37 0.54 0.59 0.64 0.44 7.84 7.69 0.14 0.12 0.21 0.22 0.35 0.41 0.42 0.38
GENEVA 0.34 0.14 0.14 ?0.29 0.21 0.22 0.43 0.49 0.52 0.36 5.32 5.15 0.05 0.05 0.54 0.52 0.26 0.53 0.29 0.22
GOOGLE 0.76 0.31 0.30 ?0.10 0.36 0.37 0.54 0.58 0.63 0.44 8 7.84 0.17 0.13 0.17 0.2 0.36 0.41 0.42 0.38
JHU 0.62 0.27 0.23 ?0.15 0.32 0.33 0.51 0.56 0.6 0.41 7.23 6.68 0.08 0.05 0.33 0.36 0.32 0.43 0.37 0.32
LIMSI 0.65 0.26 0.25 ?0.16 0.30 0.32 0.51 0.56 0.60 0.42 7.02 6.87 0.09 0.07 0.35 0.36 0.33 0.44 0.38 0.33
LIUM-SYSTRAN 0.60 0.27 0.26 ?0.15 0.32 0.33 0.51 0.56 0.60 0.42 7.26 7.10 0.10 0.06 0.33 0.36 0.33 0.43 0.39 0.35
RBMT1 0.56 0.18 0.18 ?0.25 0.24 0.25 0.48 0.53 0.57 0.4 5.89 5.73 0.07 0.06 0.51 0.45 0.3 0.50 0.34 0.26
RBMT3 0.54 0.2 0.19 ?0.22 0.25 0.27 0.48 0.53 0.56 0.39 6.12 5.96 0.07 0.06 0.45 0.45 0.30 0.49 0.35 0.28
RBMT4 0.47 0.19 0.18 ?0.24 0.24 0.26 0.48 0.52 0.56 0.39 5.97 5.83 0.07 0.06 0.46 0.45 0.3 0.49 0.34 0.27
RBMT5 0.59 0.19 0.19 ?0.24 0.25 0.26 0.49 0.54 0.57 0.40 6.03 5.9 0.09 0.07 0.46 0.43 0.31 0.49 0.35 0.28
RWTH 0.52 0.25 0.24 ?0.16 0.30 0.32 0.5 0.55 0.59 0.40 7.09 6.94 0.07 0.03 0.35 0.39 0.32 0.44 0.38 0.32
UEDIN 0.61 0.25 0.24 ?0.16 0.31 0.32 0.50 0.55 0.59 0.41 7.04 6.85 0.08 0.04 0.35 0.38 0.32 0.44 0.38 0.33
UKA 0.61 0.26 0.25 ?0.15 0.31 0.33 0.51 0.55 0.6 0.41 7.17 7.00 0.08 0.04 0.34 0.37 0.32 0.44 0.38 0.34
USAAR 0.55 0.19 0.18 ?0.24 0.24 0.26 0.48 0.54 0.57 0.4 6.08 5.92 0.07 0.06 0.46 0.44 0.3 0.49 0.34 0.26
USAAR-COMBO 0.57 0.26 0.25 ?0.16 0.31 0.33 0.51 0.55 0.59 0.41 7.13 6.85 0.08 0.02 0.33 0.35 0.32 0.44 0.38 0.33
Czech-English News Task
BBN-COMBO 0.65 0.22 0.20 ?0.19 0.27 0.29 0.47 0.52 0.56 0.39 6.74 6.45 0.24 0.3 0.52 0.60 0.29 0.47 0.34 0.29
CMU-COMBO 0.73 0.22 0.20 ?0.2 0.27 0.29 0.47 0.53 0.57 0.39 6.72 6.46 0.34 0.34 0.53 0.60 0.29 0.47 0.35 0.29
CU-BOJAR 0.51 0.16 0.15 ?0.26 0.22 0.24 0.43 0.5 0.52 0.36 5.84 5.54 0.26 0.28 0.61 0.69 0.26 0.52 0.31 0.24
GOOGLE 0.75 0.21 0.20 ?0.19 0.26 0.28 0.46 0.52 0.55 0.38 6.82 6.61 0.32 0.33 0.53 0.62 0.29 0.47 0.35 0.28
UEDIN 0.57 0.2 0.19 ?0.23 0.25 0.27 0.45 0.50 0.54 0.37 6.2 6 0.22 0.25 0.56 0.63 0.27 0.49 0.33 0.27
Hungarian-English News Task
BBN-COMBO 0.54 0.14 0.13 ?0.29 0.19 0.21 0.38 0.45 0.46 0.32 5.46 5.2 0.16 0.18 0.71 0.83 0.23 0.55 0.27 0.2
CMU-COMBO 0.62 0.14 0.13 ?0.29 0.19 0.21 0.39 0.46 0.47 0.32 5.52 5.24 0.28 0.22 0.71 0.82 0.23 0.55 0.28 0.2
CMU-COMBO-HYPOSEL 0.68 0.14 0.12 ?0.29 0.19 0.21 0.39 0.45 0.46 0.32 5.51 5.16 0.25 0.25 0.71 0.82 0.23 0.55 0.27 0.2
MORPHO 0.75 0.1 0.09 ?0.36 0.15 0.17 0.39 0.45 0.46 0.32 4.75 4.55 0.34 0.49 0.79 0.83 0.23 0.6 0.26 0.17
UEDIN 0.45 0.12 0.11 ?0.32 0.18 0.19 0.37 0.42 0.43 0.30 4.95 4.74 0.12 0.12 0.75 0.87 0.21 0.58 0.27 0.19
UMD 0.66 0.13 0.12 ?0.28 0.18 0.2 0.36 0.44 0.45 0.30 5.41 5.12 0.21 0.13 0.68 0.85 0.22 0.55 0.27 0.18
Table 25: Automatic evaluation metric scores for translations into English
27
R
A
N
K
B
L
E
U
B
L
E
U
-C
A
S
E
D
B
L
E
U
-T
E
R
B
L
E
U
S
P
B
L
E
U
S
P
41
14
N
IS
T
N
IS
T
-C
A
S
E
D
T
E
R
T
E
R
P
W
C
D
6P
4E
R
W
P
F
W
P
B
L
E
U
English-German News Task
GOOGLE 0.54 0.15 0.14 ?0.29 0.20 0.22 5.36 5.25 0.62 0.74 0.54 0.3 0.23
LIU 0.49 0.14 0.13 ?0.29 0.2 0.21 5.35 5.18 0.65 0.78 0.54 0.3 0.23
RBMT1 0.57 0.11 0.11 ?0.32 0.17 0.19 4.69 4.59 0.67 0.81 0.57 0.28 0.21
RBMT2 0.66 0.13 0.13 ?0.30 0.19 0.21 5.08 4.99 0.62 0.75 0.55 0.30 0.23
RBMT3 0.64 0.12 0.12 ?0.29 0.2 0.21 4.8 4.71 0.62 0.76 0.54 0.31 0.25
RBMT4 0.58 0.11 0.10 ?0.33 0.17 0.18 4.66 4.57 0.7 0.84 0.57 0.27 0.2
RBMT5 0.64 0.13 0.12 ?0.3 0.19 0.20 5.03 4.94 0.64 0.79 0.55 0.3 0.23
RWTH 0.48 0.14 0.13 ?0.28 0.2 0.21 5.51 5.41 0.62 0.78 0.53 0.3 0.23
STUTTGART 0.43 0.12 0.12 ?0.31 0.18 0.20 5.06 4.82 0.67 0.79 0.55 0.29 0.21
UEDIN 0.51 0.15 0.15 ?0.27 0.21 0.23 5.53 5.42 0.63 0.77 0.53 0.31 0.24
UKA 0.54 0.15 0.15 ?0.27 0.21 0.22 5.6 5.48 0.62 0.75 0.52 0.31 0.24
USAAR 0.58 0.12 0.11 ?0.33 0.18 0.19 4.83 4.71 0.69 0.8 0.57 0.28 0.21
USAAR-COMBO 0.52 0.16 0.15 ?0.27 0.21 0.23 5.6 5.39 0.62 0.75 0.52 0.31 0.24
English-Spanish News Task
GOOGLE 0.65 0.28 0.27 ?0.15 0.33 0.34 7.27 7.07 0.36 0.42 0.42 0.37 0.31
NUS 0.59 0.25 0.23 ?0.17 0.30 0.31 6.96 6.67 0.48 0.59 0.44 0.34 0.28
RBMT1 0.25 0.15 0.14 ?0.27 0.20 0.22 5.32 5.17 0.55 0.66 0.51 0.24 0.16
RBMT3 0.66 0.18 0.17 ?0.18 0.28 0.3 5.79 5.63 0.49 0.59 0.45 0.33 0.27
RBMT4 0.61 0.21 0.2 ?0.20 0.26 0.28 6.47 6.28 0.52 0.64 0.47 0.31 0.25
RBMT5 0.64 0.22 0.21 ?0.2 0.27 0.29 6.53 6.34 0.52 0.64 0.46 0.32 0.26
RWTH 0.51 0.22 0.21 ?0.18 0.27 0.29 6.83 6.63 0.50 0.65 0.46 0.32 0.26
TALP-UPC 0.58 0.25 0.23 ?0.17 0.3 0.31 6.96 6.69 0.47 0.58 0.44 0.34 0.28
UEDIN 0.66 0.25 0.24 ?0.17 0.30 0.31 6.94 6.73 0.48 0.59 0.44 0.34 0.29
USAAR 0.48 0.20 0.19 ?0.21 0.26 0.27 6.36 6.16 0.54 0.66 0.47 0.30 0.24
USAAR-COMBO 0.61 0.28 0.26 ?0.14 0.33 0.34 7.36 6.97 0.39 0.48 0.42 0.36 0.31
English-French News Task
DCU 0.65 0.24 0.22 ?0.19 0.29 0.30 6.69 6.39 0.63 0.72 0.47 0.38 0.34
DCU-COMBO 0.74 0.28 0.27 ?0.15 0.33 0.34 7.29 7.12 0.58 0.67 0.44 0.42 0.38
GENEVA 0.38 0.15 0.14 ?0.27 0.20 0.22 5.59 5.39 0.68 0.82 0.53 0.32 0.25
GOOGLE 0.68 0.25 0.24 ?0.17 0.30 0.31 6.90 6.71 0.62 0.7 0.46 0.40 0.36
LIMSI 0.64 0.25 0.24 ?0.17 0.3 0.31 6.94 6.77 0.60 0.71 0.46 0.4 0.35
LIUM-SYSTRAN 0.73 0.26 0.24 ?0.17 0.31 0.32 7.02 6.83 0.61 0.71 0.45 0.40 0.36
RBMT1 0.54 0.18 0.17 ?0.23 0.24 0.26 6.12 5.96 0.65 0.76 0.5 0.35 0.29
RBMT3 0.65 0.22 0.20 ?0.20 0.27 0.28 6.48 6.29 0.63 0.72 0.48 0.38 0.33
RBMT4 0.59 0.18 0.17 ?0.24 0.24 0.25 6.02 5.86 0.66 0.77 0.50 0.35 0.3
RBMT5 0.57 0.20 0.19 ?0.21 0.26 0.27 6.31 6.15 0.63 0.74 0.49 0.36 0.31
RWTH 0.58 0.22 0.21 ?0.19 0.27 0.28 6.67 6.51 0.62 0.75 0.48 0.38 0.32
SYSTRAN 0.65 0.23 0.22 ?0.19 0.28 0.29 6.7 6.47 0.63 0.74 0.47 0.39 0.34
UEDIN 0.60 0.24 0.23 ?0.18 0.29 0.30 6.75 6.57 0.62 0.71 0.47 0.39 0.35
UKA 0.66 0.24 0.23 ?0.18 0.29 0.30 6.82 6.65 0.61 0.71 0.46 0.39 0.35
USAAR 0.48 0.19 0.18 ?0.23 0.24 0.26 6.16 5.98 0.66 0.76 0.5 0.34 0.29
USAAR-COMBO 0.77 0.27 0.25 ?0.15 0.32 0.33 7.24 6.93 0.59 0.69 0.44 0.41 0.37
English-Czech News Task
CU-BOJAR 0.61 0.14 0.13 ?0.28 0.21 0.23 5.18 4.96 0.63 0.82 0.01 n/a n/a
CU-TECTOMT 0.48 0.07 0.07 ?0.35 0.14 0.16 4.17 4.03 0.71 0.96 0.01 n/a n/a
EUROTRANXP 0.67 0.1 0.09 ?0.33 0.16 0.18 4.38 4.26 0.7 0.93 0.01 n/a n/a
GOOGLE 0.66 0.14 0.13 ?0.30 0.20 0.22 4.96 4.84 0.66 0.82 0.01 n/a n/a
PCTRANS 0.67 0.09 0.09 ?0.34 0.17 0.18 4.34 4.19 0.71 0.90 0.01 n/a n/a
UEDIN 0.53 0.14 0.13 ?0.29 0.21 0.22 5.04 4.9 0.64 0.84 0.01 n/a n/a
English-Hungarian News Task
MORPHO 0.79 0.08 0.08 ?0.37 0.15 0.16 4.04 3.92 0.83 1 0.6 n/a n/a
UEDIN 0.32 0.1 0.09 ?0.33 0.17 0.18 4.48 4.32 0.78 1 0.56 n/a n/a
Table 26: Automatic evaluation metric scores for translations out of English
28
