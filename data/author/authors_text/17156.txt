Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1411?1422, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A New Minimally-Supervised Framework
for Domain Word Sense Disambiguation
Stefano Faralli and Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
{faralli,navigli}@di.uniroma1.it
Abstract
We present a new minimally-supervised
framework for performing domain-driven
Word Sense Disambiguation (WSD). Glos-
saries for several domains are iteratively ac-
quired from the Web by means of a boot-
strapping technique. The acquired glosses are
then used as the sense inventory for fully-
unsupervised domain WSD. Our experiments,
on new and gold-standard datasets, show that
our wide-coverage framework enables high-
performance results on dozens of domains at
a coarse and fine-grained level.
1 Introduction
Domain information pervades most of the text we
read every day. If we just think of the Web, the vast
majority of its textual content is domain oriented.
A case in point is Wikipedia, which provides ency-
clopedic coverage for a huge number of knowledge
domains (Medelyan et al 2009), but most blogs,
Web sites and newspapers also provide a great deal
of information focused on specific areas of knowl-
edge. When it comes to automatic text understand-
ing, then, it is crucial to take into account the domain
specificity of a piece of text, so as to perform a fo-
cused and as-precise-as-possible analysis which, in
its turn, can enable domain-aware applications such
as question answering and information extraction.
Domain knowledge also has the potential to improve
open-text applications such as summarization (Cey-
lan et al 2010) and machine translation (Foster et
al., 2010).
Research in Word Sense Disambiguation (Nav-
igli, 2009, WSD), the task aimed at the automatic
labeling of text with word senses, has been ori-
ented towards domain text understanding for sev-
eral years now. Many approaches have been devised,
including the identification of domain-specific pre-
dominant senses (McCarthy et al 2007; Lapata and
Keller, 2007), the development of domain resources
(Magnini and Cavaglia`, 2000; Magnini et al 2002),
their application to WSD (Gliozzo et al 2004), and
the effective use of link analysis algorithms such as
Personalized PageRank (Agirre et al 2009; Nav-
igli et al 2011). More recently, semi-supervised ap-
proaches to domain WSD have been proposed which
aim at decreasing the amount of supervision needed
to carry out the task (Khapra et al 2010).
High-performance domain WSD, however, has
been hampered by the widespread use of a general-
purpose sense inventory, i.e., WordNet (Miller et
al., 1990; Fellbaum, 1998). Unfortunately WordNet
does not contain many specialized terms, making
it difficult to use it in work on arbitrary special-
ized domains. While Wikipedia has recently been
considered a valid alternative (Mihalcea, 2007), it
is mainly focused on covering named entities and,
strictly speaking, does not contain a formal wide-
coverage sense inventory (not even in disambigua-
tion pages, which are often incomplete, especially
in the lexicographic sense).
In this paper we provide three main contributions:
? We tackle the above issues by introducing
a new framework based on the minimally-
supervised acquisition of specialized glossaries
for dozens of domains.
1411
? In turn, we use the acquired domain glossaries
as a sense inventory for domain WSD. As a re-
sult, we redefine the domain WSD task as one
of picking out the most appropriate gloss (fine-
grained setting) or domain (coarse-grained set-
ting) from a multi-domain glossary.
? We show that our framework represents a con-
siderable departure from the common usage
of a general-purpose sense inventory such as
WordNet, in that, thanks to the wide cov-
erage of domain meanings, it enables high-
performance unsupervised WSD on many do-
mains in the range of 69-80% F1.
Furthermore, our approach can be customized to
any set of domains of interest, and new senses, i.e.,
glosses, can be added at any time (either manually or
automatically) to the multi-domain sense inventory.
2 Related Work
Domain WSD has been the focus of much interest
in the last few years. An important research direc-
tion identifies distributionally similar neighbors in
raw text as cues for determining the predominant
sense of a target word by means of a semantic simi-
larity measure (McCarthy et al 2004; Koeling et al
2005; McCarthy et al 2007). Other distributional
methods include the use of a word-category cooccur-
rence matrix, where categories are coarse senses ob-
tained from an existing thesaurus (Mohammad and
Hirst, 2006), and synonym-based word occurrence
counts (Lapata and Keller, 2007). Domain-informed
methods have also been proposed which make use of
domain labels as cues for disambiguation purposes
(Gliozzo et al 2004).
Domain-driven approaches have been shown to
obtain the best performance among the unsupervised
alternatives (Strapparava et al 2004), especially
when domain kernels are coupled with a syntag-
matic one (Gliozzo et al 2005). However, their per-
formance is typically lower than supervised systems.
On the other hand, supervised systems fall short
of carrying out high-performance WSD within do-
mains, the main reason being the need for retraining
on each new specific knowledge domain. Nonethe-
less, the knowledge acquisition bottleneck can be
relieved by means of domain adaptation (Chan and
Ng, 2006; Chan and Ng, 2007; Agirre and de La-
calle, 2009) or by effectively injecting a general-
purpose corpus into a smaller domain-specific train-
ing set (Khapra et al 2010).
However, as mentioned above, most work on
domain WSD uses WordNet as a sense inven-
tory. But even if WordNet senses have been en-
riched with topically-distinctive words and concepts
(Agirre and de Lacalle, 2004; Cuadros and Rigau,
2008), manually-developed domain labels (Magnini
et al 2002), and disambiguated semantic relations
(Navigli, 2005), the main obstacle of being stuck
with an open-ended fine-grained sense inventory re-
mains. Recent results on the SPORTS and FINANCE
gold standard dataset (Koeling et al 2005) show
that domain WSD can achieve accuracy in the 50-
60% ballpark when a state-of-the-art algorithm such
as Personalized PageRank is paired with a distribu-
tional approach (Agirre et al 2009) or with seman-
tic model vectors acquired for many domains (Nav-
igli et al 2011).
In this paper, we take domain WSD to the next
level by proposing a new framework based on
the minimally-supervised acquisition of large do-
main sense inventories thanks to which high per-
formance can be attained on virtually any domain
using unsupervised algorithms. Glossary acquisi-
tion approaches in the literature are mostly fo-
cused on pattern-based definition extraction (Fujii
and Ishikawa, 2000; Hovy et al 2003; Fahmi and
Bouma, 2006, among others) and lattice-based su-
pervised models (Navigli and Velardi, 2010) start-
ing from an initial terminology, while we jointly
bootstrap the lexicon and the definitions for sev-
eral domains with minimal supervision and without
the requirement of domain-specific corpora. To do
so, we adapt bootstrapping techniques (Brin, 1998;
Agichtein and Gravano, 2000; Pasca et al 2006) to
the novel task of domain glossary acquisition from
the Web.
Approaches to domain sense modeling have al-
ready been proposed which go beyond the WordNet
sense inventory (Duan and Yates, 2010). Distinc-
tive collocations are extracted from corpora and used
as features to bootstrap a supervised WSD system.
Experiments in the biomedical domain show good
performance, however only in-domain ambiguity is
addressed. In contrast, our approach tackles cross-
1412
Figure 1: The bootstrapping process for glossary acquisition.
domain ambiguity, by working with virtually any set
of domains and minimizing the requirements by har-
vesting domain terms and definitions from the Web,
bootstrapped using a small number of seeds.
The existing approach closest to ours is that of
Huang and Riloff (2010), who devised a bootstrap-
ping approach to induce semantic class taggers from
domain text. The semantic classes are associated
with arbitrary NPs and must be established before-
hand. Our objective, instead, is to perform domain
disambiguation at the word level. To do this, we re-
define the domain WSD problem as one of selecting
the most suitable gloss from those available in our
full-fledged multi-domain glossary.
3 A Minimally-Supervised Framework for
Domain WSD
In this section we present our new framework for
performing domain WSD. The framework consists
of two phases: glossary bootstrapping (Section 3.1)
and domain WSD (Section 3.2).
3.1 Phase 1: Bootstrapping Domain Glossaries
The objective of the first phase is to acquire a multi-
domain glossary from the Web with minimal super-
vision. We initially select a set D of domains of
interest. For each individual domain d ? D we start
with an empty set of HTML patterns Pd (i.e., Pd :=
?), used for gloss harvesting. During this phase we
iteratively populate the pattern set by means of six
steps, described in the next six subsections and de-
picted in Figure 1. The final output of this phase will
be a glossary Gd consisting of domain terms and
their automatically-harvested glosses.
3.1.1 Step 1: Initial seed selection
First, given the domain d, we manually
pick out K hypernymy relation seeds Sd =
{(t1, h1), . . . , (tK , hK)}, where the pair (ti, hi)
contains a domain term ti and its generalization hi
(e.g., (firewall, security system)). The only con-
straint we impose is that the selected relations must
be distinctive for the domain d of interest. The cho-
sen hypernymy relations have to be as topical and
representative as possible for the given domain (e.g.,
(compiler, computer program) is an appropriate pair
for computer science, while (byte, unit of measure-
ment) is not, as it might cause the extraction of sev-
eral glossaries of various units and measures). Note
that this is the only human intervention in the entire
glossary acquisition process.
We now set the iteration counter k to 1 and start
the first iteration of the process (steps 2-5). After
each iteration k, we keep track of the set of glosses
Gkd, acquired during iteration k.
3.1.2 Step 2: Seed queries
For each seed pair (ti, hi), we submit the follow-
ing three queries to a Web search engine: ?ti? ?hi?
glossary1, ?ti? ?hi? definition, ?ti? ?hi?
dictionary and collect the 64 top-ranking results
for each query2. Each resulting page is a candidate
glossary for the domain d identified by our relation
seeds Sd.
3.1.3 Step 3: Pattern and glossary extraction
We initialize the glossary for iteration k as fol-
lows: Gkd := ?. Next, from each resulting page,
we harvest all the text snippets s starting with
ti and ending with hi (e.g., firewall</b> -- a
<i>security system), i.e., s = ti . . . hi. For each
such text snippet s, we perform five substeps:
a) extraction of the term/gloss separator: we
1In what follows, we use the typewriter font for key-
words and term/gloss separators.
2We use the Google AJAX API, which returns 64 results.
1413
Term Gloss Hypernym # seeds Gloss score
dynamic
packet filter
A firewall facility that monitors the state of connections and uses this
information to determine which network packets to allow through the firewall
firewall 2 0.75
peripheral Hardware that extends the capabilities of the computer, such as a printer,
modem, or scanner.
hardware 1 0.83
die An integrated circuit chip cut from a finished wafer. integrated circuit 1 0.75
constructor a method used to help create a new object and initialise its data method 0 1.00
schema In database terminology, a schema is the organization of the tables, the fields in
each table, and the relationships between fields and tables.
database 0 0.78
Table 1: Examples of extracted terms, glosses and hypernyms (seeds are in bold, domain terms are underlined).
start from ti and move right until we extract
the longest sequence pM of HTML tags and
non-alphanumeric characters, which we call the
term/gloss separator, between ti and the glossary
definition (e.g., ?</b> --? between ?firewall?
and ?a? in the above example);
b) gloss extraction: we expand the snippet s to
the right of hi in search of the entire gloss of
ti, i.e., until we reach a non-formatting tag el-
ement (e.g., <span>, <p>, <div>), while ig-
noring formatting elements such as <b>, <i>
and <a> which are typically included within a
definition sentence. As a result, we obtain the
sequence ti pM glosss(ti) pR, where glosss(ti)
is our gloss for seed term ti in snippet s (which
includes hi by construction) and pR is the non-
formatting HTML tag element to the right of
the extracted gloss. For example, we extend the
above definition for firewall to: ?a <i>security
system</i> for protecting against illegal entry
to a local area network.?.
c) pattern instance extraction: we extract the fol-
lowing pattern instance:
pL ti pM glosss(ti) pR,
where pL and pR are, respectively, the left bound-
ary of ti and the right boundary of glosss(ti), and
pM is the term/gloss separator extracted at step
3(a). The two boundaries pL and pR are obtained
by extracting the longest sequence of HTML
tags and non-alphanumeric characters obtained
when moving to the left of ti and the right of
glosss(ti), respectively3. For the above exam-
ple, we extract the following pattern instance:
3The minimum and maximum length of both pL and pR are
set to 4 and 50 characters, respectively, as a result of a tuning
phase described in Section 4.1.
pL = ?<p><b>?, ti = ?firewall?, pM = ?</b>
--?, glosss(ti) = ?a <i>security system</i>
for protecting against illegal entry to a local area
network.?, pR =?</p>?.
d) pattern extraction: we generalize the above pat-
tern instance to the following pattern:
pL ? pM ? pR,
i.e., we replace ti and glosss(ti) with *. In the
above example, we obtain the following pattern:
<p><b> ? </b> -- ? </p>.
Finally, we add the generalized pattern to the set
of patterns Pd, i.e., we set Pd := Pd ? {pL ?
pM ? pR}. We also add the first sentence of
the retrieved definition glosss(ti) to our glossary
Gkd, i.e., G
k
d := G
k
d ? {(ti, first(glosss(ti)))},
where first(g) returns the first sentence of gloss
g.
e) pattern matching: we look for additional pairs
of terms/glosses in the Web page containing the
snippet s by matching the page against the gen-
eralized pattern pL ? pM ? pR. We then add
to Gkd the new (term, gloss) pairs matching the
generalized pattern.
As a result of this step, we obtain a glossary Gkd
for the terms discovered at iteration k.
3.1.4 Step 4: Gloss ranking and filtering
Importantly not all the extracted definitions per-
tain to the domain of interest. In order to rank by
domain pertinence the glosses obtained at iteration
k, we define the terminology T k?11 of the terms
accumulated up until iteration k ? 1 as follows:
T k?11 :=
?k?1
i=1 T
i, where T i := {t : ?(t, g) ? Gid}.
1414
Gloss Domain
Measures undertaken to return a degraded ecosystem?s functions and values, including its hydrology, plant and. . . BIOLOGY
The renewing or repairing of a natural system so that its functions and qualities are comparable to its original. . . GEOGRAPHY
The reign of Charles II in England. ROYALTY
A goal of criminal sentencing that attempts to make the victim ?whole again.? LAW
The process and work of improving the degraded quality of the sound or image in terms of video and audio preservation. MEDIA
A process used by radio astronomers to eliminate the smoothing effect observed in radio maps that is caused by. . . PHYSICS
Table 2: Examples of glosses harvested for the term restoration.
For the base step k = 1, we define T 01 := T
1, i.e.,
we use the first-iteration terminology itself. To rank
the glosses, we first transform each acquired gloss
g to its bag-of-words representation Bag(g), which
contains all the single- and multi-word expressions
in g. We then score each gloss g by the ratio of do-
main terms found in its bag of words:
score(g) =
|Bag(g) ? T k?11 |
|Bag(g)|
. (1)
In Table 1 we show some glosses in the computer
science domain (second column, domain terms are
underlined) together with their score (last column).
Next, we use a threshold ? (tuned on a held-out do-
main, described in Section 4.1) to remove from Gkd
those glosses g whose score(g) < ?.
3.1.5 Step 5: Seed selection for next iteration
We now aim at selecting the new set of hyper-
nymy relation seeds to be used to start the next it-
eration. We perform three substeps:
a) Hypernym extraction: for each newly-acquired
term/gloss pair (t, g) ? Gkd, we automatically ex-
tract a candidate hypernym h from the textual
gloss g. To do this we use a simple unsupervised
heuristic which just selects the first term in the
gloss. More sophisticated, supervised approaches
could have been used for hypernym extraction
from glosses (Navigli and Velardi, 2010). How-
ever, note that, for the purposes of our glossary
extraction task, it is not crucial to extract ac-
curate hypernyms, but rather to harvest terms h
which are very likely to occur in the glosses of t.
We show an example of hypernym extraction for
some terms in Table 1 (we report the term in col-
umn 1, the gloss in column 2 and the hypernyms
extracted by our hypernym extraction technique
in column 3).
b) (Term, Hypernym)-ranking: we sort all the
glosses in Gkd by the number of seed terms found
in each gloss. In the case of ties (i.e., glosses with
the same number of seed terms), we further sort
the glosses by the score shown in Formula 1. We
show the number of seed terms and the scores
for some glosses in Table 1 (columns 4 and 5,
respectively), where seed terms are in bold and
domain terms (i.e., in T k?11 ) are underlined.
c) New seed selection: as new seeds we select the
(term, hypernym) pairs corresponding to the K
top-ranking glosses.
If k equals the maximum number of iterations, we
stop. Else, we increment the iteration counter (i.e.,
k := k + 1) and jump to step (2) of our glossary
bootstrapping algorithm after replacing Sd with the
new set of seeds.
The output of the glossary bootstrapping phase is
a domain glossary Gd :=
?
i=1,...,maxG
i
d, where
max is the total number of iterations.
3.1.6 Step 6: Increasing Coverage
Given the nature of Web domain glossaries one
can rarely find terms and definitions for general
terms (e.g., jurisprudence for the LAW domain). In
order to cover this gap, we apply domain filtering
(see Section 3.1.4) to all the glosses contained in a
general-purpose dictionary (we use WordNet). We
then add the surviving term/gloss pairs to Gd.
3.2 Phase 2: Domain WSD
Now that we have acquired a glossary for each do-
main in our set D, we can create a multi-domain
glossary G := {((t, g), d) : d ? D, (t, g) ? Gd}.
Our glossary G is thus a set of term/gloss pairs
for many domains. Note that one pair might indi-
vidually belong to more than one domain, as glos-
sary bootstrapping is performed separately for each
domain. In Table 2 we show an example of the
1415
glosses acquired for the term restoration. We ob-
serve that 5 out of 6 senses are not available in Word-
Net (namely: the BIOLOGY, GEOGRAPHY, LAW, ME-
DIA and PHYSICS senses). Many of them are domain-
specific meanings for the general concept of ?the
act of restoring?, with the BIOLOGY and GEOGRA-
PHY senses being very similar. However, this is a
perfectly acceptable phenomenon as any of the two
senses, i.e., glosses, would be equally valid when
disambiguating a domain text dealing with ecosys-
tem restoration.
3.2.1 Gloss-driven WSD
We redefine the task of domain WSD as one of
selecting the most suitable gloss, if one exists, for
an input term t. For instance, consider the sentence:
?He performed the restoration of heavily corrupted
images?. An appropriate option for this occurrence
would be the MEDIA sense of restoration in Table 2.
Our gloss-driven WSD paradigm has the desir-
able property of automatically providing two levels
of sense granularity: a domain, coarse-grained level,
similar in spirit to Word Domain Disambiguation
(Sanfilippo et al 2006), in which the sense inven-
tory of a term t is just the set of domains for which t
is covered (e.g., BIOLOGY, GEOGRAPHY, ROYALTY, LAW,
MEDIA, PHYSICS in the example of Table 2), and a
fine-grained level, which requires the selection of
the gloss which best describes the sense denoted
by the given word occurrence. A second desirable
property of our gloss-driven WSD paradigm is that
it relies on a flexible framework, which allows for
the bootstrapping of new domain glossaries or the
expansion of existing ones. However, while these
two properties ? i.e., double level of granularity dis-
tinctions and flexibility ? are naturally inherent in
the gloss-driven paradigm, the same cannot be said
for mainstream open-text WSD in which general-
purpose static dictionaries are typically used.
In order to evaluate our framework for domain
WSD, we propose two fully unsupervised algo-
rithms for gloss-driven domain WSD. Ideally, high
performance could be obtained using state-of-the-art
supervised WSD systems. However, in order to train
such systems, a wide-coverage sense-labeled corpus
should be available for each domain, a heavy task
which we leave to future work. Instead, our objec-
tive is to show that high-performance domain WSD
can be enabled with little effort by our framework.
3.2.2 Algorithm 1: WSD with Personalized
PageRank
Domain Glossaries as Graphs For each domain
d ? D, we create an undirected graph Nd =
(Vd, Ed) as follows: Vd is the set of concepts identi-
fied by term/gloss pairs in the domain glossary Gd,
i.e., Vd := Gd; Ed is the set of edges between pairs
of concepts, where an edge {(t, g), (t?, g?)} exists if
and only if t? is such that t? 6= t and t? occurs in the
bag of words of the gloss g of t. In other words, t is
connected to all the domain senses of words used in
its definition g.
Graph-based WSD Given an input text, for each
domain d ? D, we produce its bag of domain con-
tent words Cd = {w1, w2, . . . , wn} by perform-
ing tokenization, lemmatization and compounding
based on the lexicon of domain d. Then, given a
target word t, we use Cd \ {t} as the context to dis-
ambiguate t within the domain d. In order to carry
out domain WSD, i.e., to pick out the most suit-
able sense of t across domains, we apply a state-of-
the-art graph-based algorithm, namely Personalized
PageRank (Haveliwala, 2002, PPR), to each domain
graph Nd. PPR is a variant of the popular PageRank
algorithm (Brin and Page, 1998) in which the damp-
ing probability mass is concentrated on a selected
number of graph nodes, instead of being uniformly
distributed across all nodes. Specifically, following
Agirre and Soroa (2009) we concentrate the proba-
bility mass on the nodes (t?, g?) ? Vd for which the
term t? is a context word, i.e., t? ? Cd. Next, for each
domain d ? D, we run PPR for a given number of
iterations and obtain as output a probability distribu-
tion PPVd over the graph nodes. Finally, we select
the most suitable gloss of t as follows:
SensePPR(t) = argmax
g:?d?D,(t,g)?Vd
PPVd(t, g) (2)
where PPVd(t, g) is the PPR probability for the
term/gloss pair (t, g) and SensePPR(t) contains the
best interpretation of t across all the domains D.
3.2.3 Algorithm 2: PPR Boosted with Domain
Distribution Information
The words in a given text do not typically deal
with a single domain. Instead, they touch different
1416
ART BIOLOGY BUSINESS CHEMISTRY COMPUTING EDUCATION ENGINEERING ENVIRONMENT FOOD & DRINK GEOGRAPHY
GEOLOGY HEALTH HISTORY LANGUAGE LAW LITERATURE MATHS MEDIA METEOROLOGY MUSIC
PHILOSOPHY PHYSICS POLITICS PSYCHOLOGY RELIGION ROYALTY SPORTS TOURISM VIDEOGAMES WARFARE
Table 3: List of the 30 domains used in our experiments.
COMPUTING FOOD ENVIRONMENT BUSINESS
chip circuit timbale dish sewage waste eurobond bond
destructor method brioche bread acid rain rain asset play stock
compiler program macaroni pasta ecosystem system income stock security
html language pizza dish air monitoring sampling financial intermediary institution
firewall security system ice cream dessert global warming temperature derivative financial product
remote lan access process pasteurized milk milk fermentation decomposition arbitrage pricing theory economic theory
relational database tabular database salted butter butter attainment area area banker?s draft bill of exchange
admin console user interface prosecco wine fugitive dust matter working capital cash
Table 4: Hypernymy relation seeds used to bootstrap glossary acquisition in four of the 30 domains.
areas of knowledge which are intertwined with each
other within the discourse. For example, a text deal-
ing with VIDEOGAMES will often concern domains
such as BUSINESS, COMPUTING, SPORTS, etc. Given an
input text, we can capture its relevance for each do-
main by calculating the following domain score:
?d =
|Cd|
?
d??D |Cd? |
(3)
where, as above, Cd is the set of content words from
the input text which are covered by domain d. We
thus propose a second algorithm which synergisti-
cally combines the spreading effect of PPR with the
domain distribution information. The best sense for
a given term t is calculated as follows:
SenseDomPPR(t) = argmax
g:?d?D,(t,g)?Vd
?dPPVd(t, g)
(4)
that is, we select as the most suitable gloss for t the
one which maximizes the product of its domain rel-
evance score by its domain PPVd value. Note that
the same gloss can occur in multiple domains and
that it might obtain different scores depending on the
domain. Again, since the approach is gloss-driven,
we do not see this as a problem, but rather as a natu-
ral characteristic of our framework.
4 Experimental Setup
4.1 Domains
We selected 30 domains starting from the Wikipedia
featured articles4. We show the domain labels in Ta-
4http://en.wikipedia.org/wiki/Wikipedia:Featured articles
Table 5: Statistics on the multi-domain acquired glossary.
From the Web From WordNet From both Total
Terms 74,295 83,904 18,313 176,512
Glosses 153,920 68,731 596 223,247
ble 3 (some labels have been conveniently short-
ened, e.g., PHYSICS should read PHYSICS & ASTRON-
OMY). We manually identified 8 hypernym/hyponym
seeds for each domain, totalizing 240 seeds. We
used two criteria for selecting a seed: i) it covers a
separate segment of the domain, and ii) it has to be
specialized enough to avoid ambiguity. We show the
seeds used in four of our domains in Table 4. We
bootstrapped our glossary acquisition technique (cf.
Section 3.1) on each domain and performed 5 itera-
tions. For increasing the coverage of domain terms
we used WordNet glosses (see Section 3.1.6). As a
result, we obtained 30 domain glossaries. We also
kept aside a 31st domain, namely FASHION, which
we employed for tuning the minimum and maximum
length of both pL and pR in Section 3.1.3 and the
threshold ? used to filter out non-domain glosses in
Section 3.1.4.
In Table 5 we show the statistics for the ac-
quired multi-domain glossary by distinguishing
Web-derived and WordNet terms and glosses.
4.2 Sense Inventory
Our sense inventory is given by the 30-domain
glossary obtained as a result of our glossary boot-
strapping phase. Overall we collected 176,512 and
223,247 distinct terms and glosses, respectively,
with an important contribution from both the Web
1417
and WordNet (see Table 5). The average num-
ber of glosses per term in our inventory is 1.9 (3.6
glosses on polysemous terms). However, note that
a monosemous word in our domain sense inventory
does not necessarily make disambiguation easier,
as i) we might have missed other domain-specific
senses, ii) an uncovered, non-domain sense might fit
a word occurrence (in this case, the domain WSD
algorithms might be (wrongly) biased towards re-
turning the only possible choice if a non-zero dis-
ambiguation score is calculated for it).
In order to determine the suitability of our multi-
domain sense inventory, we compared it with the
latest version of WordNet Domains (Magnini et al
2002, WND 3.2), a well-known resource which
provides domain labels for almost 65,000 nomi-
nal WordNet synsets (we removed all the synsets
tagged with the FACTOTUM label, which indicates no
domain specificity). Since WND uses about 160
finer-grained domain labels, we manually mapped
them to our 30 labels when possible (e.g. SOCCER
and SWIMMING were mapped to SPORTS), totalizing
62,100 domain-labeled synsets.
We calculated the coverage of our sense inventory
against WND at the synset and the sense level, for
each non-FACTOTUM synset. Given a WordNet synset
S, let d =
?
s?S ds be the union of the domains ds
provided for each synonym s ? S by our sense in-
ventory (ds = ? if not present), and let d? be the do-
main labels assigned to S by WND. A synset is cov-
ered if d and d? intersect. At the sense level, instead,
we consider a synonym s ? S to be covered if ds and
d? intersect. Our synset and sense coverage is 65.9%
(40,969/62,100) and 63.7% (71,950/112,875), re-
spectively. We also calculated an extra-coverage of
203.2% (229,384/112,875), that is the fraction of do-
main senses which are not available in WND, but
we are able to provide in our sense inventory (see
e.g. the example in Table 2) over the total number of
senses in WND. While coverage and extra-coverage
provide a good indicator of the completeness of our
sense inventory, we need to calculate its precision to
determine its correctness. To do so, we randomly
sampled 500 domain glosses of terms for which no
WordNet sense was tagged with the same domain in
WND. A manual validation of this sample resulted
in an 87.0% (435/500) estimate of the precision of
our sense inventory.
4.3 Datasets
A dataset for 30 domains We used the Giga-
word corpus (Graff and Cieri, 2003) to extract a 6-
paragraph text snippet for each of the 30 domains.
As a result, we obtained a domain dataset made up
of 180 paragraphs to which we applied tokeniza-
tion, lemmatization and compounding, totaling 1432
domain content words overall (47.7 content words
per domain on average). The average polysemy of
the words in the dataset was of 9.7 glosses and 4.4
domains per word. Each content word was manu-
ally tagged with the most suitable glosses from our
multi-domain glossary (3.9 glosses, i.e., senses per
word were assigned on average). The annotation
task was performed by two annotators with adjudi-
cation.
Sports and Finance We also experimented with
the gold standard produced by Koeling et al(2005).
The dataset covers two domains: SPORTS and FI-
NANCE. The dataset comprises 41 ambiguous words
(with an average polysemy of 6.7 senses), many
of which express different meanings in the two do-
mains. In each domain, and for each word, around
100 sentences were sense-annotated with WordNet.
Environment Finally, we also carried out an ex-
periment on the ENVIRONMENT dataset from the
Semeval-2010 domain WSD task (Agirre et al
2010). The dataset includes 1,398 content words (of
which 1,032 content nouns) tagged with WordNet
senses.
4.4 Systems
We applied the two algorithms proposed in Section
3.2, namely vanilla PPR and domain-boosted PPR.
For both versions of PPR we employed UKB, a
readily-available implementation of PPR for WSD5,
successfully experimented by Agirre and Soroa
(2009) and Agirre et al(2009).
4.5 Baselines
Random baseline We compared our algorithms
with the random baseline, which associates a ran-
dom gloss among those available for each word oc-
currence according to a uniform distribution.
5http://ixa2.si.ehu.es/ukb/
1418
Predominant domain We also compared our al-
gorithms with a predominant sense baseline which
assigns to each word occurrence the domain label
with the highest domain score ?d among those avail-
able for the word (cf. Formula 3). Note that this is
a strong baseline, because it aims at identifying the
domain covered by the majority of terms in the input
text, however it can disambiguate only at a coarse-
grained level, i.e., at the domain level.
5 Experimental Results
30 domains We ran our WSD systems and the
baselines on our 30-domain dataset, on a sentence-
by-sentence basis. We calculated results at the two
levels of granularity enabled by our WSD frame-
work: a coarse-grained setting where systems out-
put the most appropriate domain label for each word
item to be disambiguated; a fine-grained setting
where systems are required to output the most suit-
able gloss for the input word. The results are shown
in Table 6. Domain PPR outperforms Vanilla PPR
by some points in precision, recall and F1 in both the
coarse-grained and the fine-grained setting, achiev-
ing an F1 around 80% and 69%, respectively (dif-
ferences in recall performance are statistically sig-
nificant using a ?2 test). The predominant domain
baseline, available only in the coarse-grained set-
ting, lags behind Domain PPR by more than 3 points
in precision and 2 in recall. While these differences
are not statistically significant, the variance across
domains is much higher, thus suggesting lower reli-
ability of the method.
These results were obtained in a fully unsuper-
vised setting in which no structured knowledge was
provided, unlike previous applications of PPR to
WSD (Agirre et al 2009; Agirre and Soroa, 2009)
which relied on the underlying WordNet graph, a
manually created resource. Furthermore, our graph
contains ?noisy? semantic relations, as we connect
each gloss to all the senses of its gloss words (cf.
Section 3.2.2). Finally, we note that the results
shown in Table 6 could never have been obtained
with WordNet. In fact, drawing on our domain map-
ping, we calculated that the correct domain sense is
not in WordNet for about 68% of the words in the
dataset. Instead, the results in Table 6 show that our
framework enables high-performance unsupervised
Coarse-grained Fine-grained
P R F1 P R F1
Vanilla PPR 76.7 74.3? 75.5 66.1 64.1? 65.1
Domain PPR 81.2 78.7? 79.9 69.7 67.6? 68.6
Predom. domain 77.9 76.8 77.3 - - -
Random baseline 42.5 42.5 42.5 44.1 44.1 44.1
Table 6: Performance results on the 30-domain dataset
(? differences between the two systems are statistically
significant using a ?2 test, p < 0.05).
WSD thanks to the wide coverage of domain mean-
ings.
As regards the random baseline, this performs
42.5% and 44.1% in the two settings. Despite the
higher polysemy of glosses (9.7 glosses vs. 4.4 do-
mains per word in the dataset), the performance is
higher in the fine-grained setting because often there
is more than one gloss covering the same meaning of
a domain word.
Sports, Finance and Environment For the
SPORTS, FINANCE and ENVIRONMENT datasets (cf. Sec-
tion 4.3) we did not have gloss-based sense annota-
tions, so we could not perform a fine-grained evalu-
ation. Therefore, we first studied the different sys-
tems at a coarse level on the basis of the domain dis-
tribution of the senses returned for the word items
in the dataset. We show the 3 most frequent domain
labels for each system and each dataset in Figure 2.
The figure seems to confirm our results showing Do-
main PPR as being more robust than its Vanilla ver-
sion. Next, to get a more accurate evaluation, we
randomly sampled 200 sentences from each dataset
and manually validated the coarse-grained senses,
i.e., domain assignments, output by each system on
this set of sentences. We remark that several words
in the datasets did not pertain to the domain of inter-
est. For instance, will and share do not have any
sports sense in WordNet, while the same applies
to half and chip for the business domain. Table 7
shows the results of our validation, where a domain
output by a system was considered correct if a suit-
able gloss existed for that domain in our inventory.
The results show that our framework enables
coarse-grained recall in the 70-80% ballpark even
on difficult gold standard datasets for which fine-
grained recall with WordNet struggles to surpass the
50-60% range. For instance, the best performance
1419
Vanilla PPR Domain PPR Pred. dom. Vanilla PPR Domain PPR Pred. dom. Vanilla PPR Domain PPR Pred. dom.
FINANCE SPORTS ENVIRONMENT
Figure 2: Frequency of the most common domain labels returned by our 3 systems on standard domain datasets.
FINANCE SPORTS ENVIRONMENT
P R F1 P R F1 P R F1
Vanilla PPR 57.8 56.5 57.1 65.5 63.2 64.3 81.5 77.9 79.7
Domain PPR 77.8 76.1 76.9 72.1 71.3 71.7 83.1 79.4 81.2
Predom. domain 80.0 78.3 79.1 72.6 70.1 71.3 72.7 70.6 71.6
Table 7: Coarse-grained performance results on gold-standard domain datasets.
on the ENVIRONMENT dataset was around 60% re-
call (Kulkarni et al 2010) using a semi-supervised
WSD system, trained on the domain. Similarly, both
the FINANCE and SPORTS datasets are notoriously dif-
ficult gold standards on which state-of-the-art recall
using WordNet is lower than 60% (Navigli et al
2011).
Interestingly, the predominant domain baseline
shows a bias towards BUSINESS, thus performing best
on the FINANCE dataset. This is because of the large
number of terms covered in our domain glossary,
and consequently the high overlap with cue words
in context. On the other two domains, we observe
performance in line with our 30-domain experiment.
6 Conclusion
We have here presented a new framework for do-
main Word Sense Disambiguation. We depart from
the use of general-purpose sense inventories like
WordNet and propose a bootstrapping approach to
the acquisition of sense inventories for virtually any
domain. While we selected 30 domains for this
study, nothing would prevent us from using a smaller
or larger set of these domains, or a set of completely
different domains.
Our work provides three main contributions:
i) we propose a new, flexible approach to glossary
bootstrapping which harvests hundreds of thou-
sands of term/gloss pairs; the resulting multi-
domain glossary is shown to have wide cov-
erage across domains and to include a large
amount of terms not available in WordNet;
ii) we propose a novel framework for fully-
unsupervised domain WSD which uses the
multi-domain glossary as our sense inventory;
iii) we show that high performance can be achieved
by means of simple, unsupervised WSD algo-
rithms (around 80% and 69% in a coarse- and
fine-grained setting, respectively).
Note that our aim here has not been to determine
which system performs best, but rather to show that
a reliable, full-fledged framework for domain WSD
can be set up with minimal supervision. Addition-
ally, our framework can be applied to any language
of interest, provided enough glossaries are available
online, by simply translating the keywords used for
our queries.
The multi-domain glossary (and sense inven-
tory) together with the seeds used for bootstrapping
are available from http://lcl.uniroma1.
it/dwsd.
Acknowledgments
The authors gratefully acknowl-
edge the support of the ERC Start-
ing Grant MultiJEDI No. 259234.
1420
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digi-
tal Libraries (DL 2000), pages 85?94, San Antonio,
Texas, United States.
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proceedings of the 4th International
Conference on Language Resources and Evaluation,
LREC 2004, pages 1123?1126, Lisbon, Portugal.
Eneko Agirre and Oier Lopez de Lacalle. 2009. Su-
pervised domain adaption for WSD. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics, EACL
2009, pages 42?50, Athens, Greece.
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing PageRank for Word Sense Disambiguation. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL 2009, pages 33?41, Athens, Greece.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2009. Knowledge-based WSD on specific domains:
performing better than generic supervised WSD. In
Proceedings of the 21st International Joint Conference
on Artificial Intelligence (IJCAI), pages 1501?1506,
Pasadena, California.
Eneko Agirre, Oier Lo?pez de Lacalle, Christiane Fell-
baum, Shu-Kai Hsieh, Maurizio Tesconi, Monica
Monachini, Piek Vossen, and Roxanne Segers. 2010.
Semeval-2010 task 17: All-words word sense disam-
biguation on a specific domain. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 75?80, Uppsala, Sweden.
Sergey Brin and Michael Page. 1998. Anatomy of a
large-scale hypertextual web search engine. In Pro-
ceedings of the 7th Conference on World Wide Web,
WWW 2007, pages 107?117, Brisbane, Australia.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In Proceedings of the In-
ternational Workshop on The World Wide Web and
Databases (WebDB 1998), pages 172?183, London,
UK.
Hakan Ceylan, Rada Mihalcea, Umut O?zertem, Elena
Lloret, and Manuel Palomar. 2010. Quantifying the
limits and success of extractive summarization sys-
tems across domains. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 903?911, Los Angeles, California.
Yee Seng Chan and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for word sense dis-
ambiguation. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL 2006, pages 89?96, Sydney, Aus-
tralia.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, ACL 2007,
pages 49?56, Prague, Czech Republic.
Montse Cuadros and German Rigau. 2008. KnowNet:
building a large net of knowledge from the Web.
In Proceedings of the 22nd International Conference
on Computational Linguistics, COLING 2008, pages
161?168, Manchester, U.K.
Weisi Duan and Alexander Yates. 2010. Extracting
glosses to disambiguate word senses. In Proceedings
of Human Language Technologies: Conference of the
North American Chapter of the Association of Com-
putational Linguistics, NAACL 2010, pages 627?635,
Los Angeles, California, USA.
Ismail Fahmi and Gosse Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Proceed-
ings of the EACL 2006 workshop on Learning Struc-
tured Information in Natural Language Applications,
pages 64?71, Trento, Italy.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2010), pages 451?
459, Cambridge, Massachusetts.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the world wide web as an encyclopedia: extracting
term descriptions from semi-structured texts. In Pro-
ceedings of the 38th Annual Meeting on Association
for Computational Linguistics, ACL 2000, pages 488?
495, Hong Kong.
Alfio Gliozzo, Carlo Strapparava, and Ido Dagan. 2004.
Unsupervised and supervised exploitation of semantic
domains in lexical disambiguation. Computer Speech
and Language, 18(3):275?299.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL 2005, pages
403?410, Ann Arbor, Michigan.
David Graff and Christopher Cieri. 2003. English Giga-
word, LDC2003T05. In Linguistic Data Consortium,
Philadelphia.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of 11th International Conference on
World Wide Web, WWW 2002, pages 517?526, Hon-
olulu, Hawaii.
1421
Eduard Hovy, Andrew Philpot, Judith Klavans, Ulrich
Germann, and Peter T. Davis. 2003. Extending meta-
data definitions by automatically extracting and orga-
nizing glossary definitions. In Proceedings of the 2003
Annual National Conference on Digital Government
Research, pages 1?6, Boston, MA.
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
2010, pages 275?285, Uppsala, Sweden.
Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, and
Pushpak Bhattacharyya. 2010. All words domain
adapted WSD: Finding a middle ground between su-
pervision and unsupervision. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1532?1541, Sweden.
Rob Koeling, Diana McCarthy, and John Carroll. 2005.
Domain-specific sense distributions and predominant
sense acquisition. In Proceedings of the Human Lan-
guage Technology Conference and the 2005 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 419?426, Vancouver, B.C., Canada.
Anup Kulkarni, Mitesh Khapra, Saurabh Sohoney, and
Pushpak Bhattacharyya. 2010. CFILT: Resource con-
scious approaches for all-words domain specific WSD.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (Semeval-2010), pages 421?426,
Stroudsburg, PA, USA.
Mirella Lapata and Frank Keller. 2007. An information
retrieval approach to sense ranking. In Proceedings of
Human Language Technologies 2007: The Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT-NAACL 2007, pages
348?355, Rochester, USA.
Bernardo Magnini and Gabriela Cavaglia`. 2000. In-
tegrating subject field codes into WordNet. In Pro-
ceedings of the 2nd Conference on Language Re-
sources and Evaluation, LREC 2000, pages 1413?
1418, Athens, Greece.
Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,
and Alfio Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Language
Engineering, 8:359?373.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
ACL 2004, pages 280?287, Barcelona, Spain.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553?590.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. Int. J. Hum.-Comput. Stud., 67(9):716?
754.
Rada Mihalcea. 2007. Using Wikipedia for automatic
Word Sense Disambiguation. In Proceedings of Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT-NAACL, pages 196?
203, Rochester, N.Y.
George A. Miller, R.T. Beckwith, Christiane D. Fell-
baum, D. Gross, and K. Miller. 1990. WordNet: an
online lexical database. International Journal of Lexi-
cography, 3(4):235?244.
Saif Mohammad and Graeme Hirst. 2006. Determining
word sense dominance using a thesaurus. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL 2006, pages 121?128, Trento, Italy.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
2010, pages 1318?1327, Uppsala, Sweden.
Roberto Navigli, Stefano Faralli, Aitor Soroa, Oier de La-
calle, and Eneko Agirre. 2011. Two birds with one
stone: Learning semantic models for text categoriza-
tion and Word Sense Disambiguation. In Proceed-
ings of the 20th ACM Conference on Information and
Knowledge Management, CIKM 2011, pages 2317?
2320, Glasgow, UK.
Roberto Navigli. 2005. Semi-automatic extension of
large-scale linguistic knowledge bases. In Proceed-
ings of the 18th Internationa Florida AI Research Sym-
posium Conference (FLAIRS), 15?17 May 2005, pages
548?553, Clearwater Beach, Florida.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1?69.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and searching
the world wide web of facts - step one: the one-million
fact extraction challenge. In Proceedings of the 21st
National Conference on Artificial intelligence (AAAI
2006), pages 1400?1405, Boston, MA.
Antonio Sanfilippo, Stephen Tratz, and Michelle Gre-
gory. 2006. Word domain disambiguation via word
sense disambiguation. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Companion Volume: Short Papers, NAACL 2006,
pages 141?144, New York, USA.
Carlo Strapparava, Alfio Gliozzo, and Claudio Giuliano.
2004. Pattern abstraction and term similarity for
Word Sense Disambiguation: IRST at Senseval-3.
In Proceedings of the 3rd International Workshop on
the Evaluation of Systems for the Semantic Analy-
sis of Text (SENSEVAL-3), pages 229?234, Barcelona,
Spain.
1422
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 170?181,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Growing Multi-Domain Glossaries from a Few Seeds
using Probabilistic Topic Models
Stefano Faralli and Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
{faralli,navigli}@di.uniroma1.it
Abstract
In this paper we present a minimally-
supervised approach to the multi-domain ac-
quisition of wide-coverage glossaries. We start
from a small number of hypernymy rela-
tion seeds and bootstrap glossaries from the
Web for dozens of domains using Probabilis-
tic Topic Models. Our experiments show that
we are able to extract high-precision glos-
saries comprising thousands of terms and def-
initions.
1 Introduction
Dictionaries, thesauri and glossaries are useful
sources of information for students, scholars and ev-
eryday readers, who use them to look up words of
which they either do not know, or have forgotten,
the meaning. With the advent of the Web an increas-
ing number of dictionaries and technical glossaries
has been made available online, thereby speeding
up the definition search process. However, finding
definitions is not always immediate, especially if the
target term pertains to a specialized domain. Indeed,
not even well-known services such as Google Define
are able to provide definitions for scientific or tech-
nical terms such as taxonomy learning or distant su-
pervision in AI or figure-four leglock and suspended
surfboard in wrestling.
Domain-specific knowledge of a definitional na-
ture is not only useful for humans, it is also use-
ful for machines (Hovy et al, 2013). Examples
include Natural Language Processing tasks such as
Question Answering (Cui et al, 2007), Word Sense
Disambiguation (Duan and Yates, 2010; Faralli and
Navigli, 2012) and ontology learning (Velardi et al,
2013). Unfortunately, most of the Web dictionar-
ies and glossaries available online comprise just a
few hundred definitions, and they therefore provide
only a partial view of a domain. This is also the
case with manually compiled glossaries created by
means of collaborative efforts, such as Wikipedia.1
The coverage issue is addressed by online aggrega-
tion services such as Google Define, which bring to-
gether definitions from several online dictionaries.
However, these services do not classify textual def-
initions by domain: they just present the collected
definitions for all the possible meanings of a given
term.
In order to automatically obtain large domain
glossaries, in recent years computational approaches
have been developed which extract textual defi-
nitions from corpora (Navigli and Velardi, 2010;
Reiplinger et al, 2012) or the Web (Velardi et al,
2008; Fujii and Ishikawa, 2000). The methods in-
volving corpora start from a given set of terms (pos-
sibly automatically extracted from a domain cor-
pus) and then harvest textual definitions for these
terms from the input corpus using a supervised sys-
tem. Web-based methods, instead, extract text snip-
pets from Web pages which match pre-defined lex-
ical patterns, such as ?X is a Y?, along the lines
of Hearst (1992). These approaches typically per-
form with high precision and low recall, because
they fall short of detecting the high variability of the
syntactic structure of textual definitions. To address
the low-recall issue, recurring cue terms occurring
1See http://en.wikipedia.org/wiki/Portal:
Contents/Glossaries
170
within dictionary and encyclopedic resources can be
automatically extracted and incorporated into lexical
patterns (Saggion, 2004). However, this approach is
term-specific and does not scale to arbitrary termi-
nologies and domains.
The goal of the new approach outlined in this pa-
per is to enable the automatic harvesting of large-
scale, full-fledged domain glossaries for dozens of
domains, an outcome which should be very use-
ful for both human activities and automatic tasks.
We present ProToDoG (Probabilistic Topics for
multi-Domain Glossaries), a framework for growing
multi-domain glossaries which has three main nov-
elties:
i) minimal human supervision: a small set of
hypernymy relation seeds for each domain is
used to bootstrap the multi-domain acquisition
process;
ii) jointness: our approach harvests terms and
glosses at the same time;
iii) probabilistic topic models are leveraged for
a simultaneous, high-precision multi-domain
classification of the extracted definitions, with
substantial performance improvements over
our previous work on glossary bootstrapping,
i.e., GlossBoot (De Benedictis et al, 2013).
ProToDog is able to harvest definitions from the
Web and thus drop the requirement of large corpora
for each domain. Moreover, apart from the need to
select a few seeds, it avoids the use of training data
or manually defined sets of lexical patterns. It is thus
applicable to virtually any language of interest.
2 ProToDoG
Given a set of domains D = {d1, ..., dn}, for each
domain d ? D ProToDoG harvests a domain glos-
sary Gd containing pairs of the kind (t, g) where t
is a domain term and g is its textual definition, i.e.,
gloss. We show the pseudocode of ProToDoG in Al-
gorithm 1.
Step 1. Initial seed selection: Algorithm 1 takes
as input a set of domains D and, for each domain
d ? D, a small set of hypernymy relation seeds
Sd = {(t1, h1), . . . , (t|Sd|, h|Sd|)}, where the seed
Algorithm 1 ProToDoG
Input: the set of domains D,
a set Sd of hypernymy seeds for each domain
d ? D
Output: a multi-domain glossary G
1: k ? 1
2: repeat
3: for each domain d ? D do
4: Gkd ? ?
5: for each seed (tj , hj) ? Sd do
6: pages? webSearch(tj , hj , ?glossary?)
7: Gkd ? G
k
d ? extractGlossary(pages)
8: end for
9: end for
10: create a topic model using glossaries from previ-
ous iterations
11: infer topic assignments for iteration-k glosses
12: filter out non-domain glosses for each domain
13: for each d ? D do
14: Sd ? seedSelectionForNextIteration(Gkd)
15: end for
16: k ? k + 1
17: until k > max
18: for each domain d ? D do
19: recover filtered glosses into Gmax+1d
20: Gd ?
?
j=1,...,max+1G
j
d
21: end for
22: return G = {(Gd, d) : d ? D}
pair (tj , hj) contains a term tj and its generalization
hj (e.g., (linux, operating system)). This is the only
human input to the entire glossary acquisition pro-
cess. The selection of the input seeds plays a key
role in the bootstrapping process, in that the pattern
and gloss extraction process will be driven by them.
The chosen hypernymy relations thus have to be as
topical and representative as possible for the domain
of interest (e.g., (compiler, computer program) is an
appropriate pair for computer science, while (byte,
unit of measurement) is not, as it might cause the
extraction of out-of-domain glossaries of units and
measures).
The algorithm first sets the iteration counter k to
1 (line 1) and starts the first iteration of the glos-
sary bootstrapping process (lines 2-17), each involv-
ing steps 2-4, described below. After each iteration
k, for each domain d we keep track of the set of
glosses Gkd acquired during that iteration. After the
last iteration, we perform step (5) of gloss recovery
(lines 18-21).
171
Step 2. Web search and glossary extraction (lines
3-9): For each domain d, we first initialize the do-
main glossary for iteration k: Gkd := ? (line 4).
Then, for each seed pair (tj , hj) ? Sd, we submit
the following query to a Web search engine: ?tj?
?hj? glossary and collect the top-ranking results
for each query (line 6).2 Each resulting page is a
candidate glossary for the domain d.
We then call the extractGlossary function (line
7) which extracts terms and glosses from the re-
trieved pages as follows. From each candidate page,
we harvest all the text snippets s starting with tj and
ending with hj (e.g., ?linux</b> ? an<i>operating
system?), i.e., s = tj . . . hj . For each such text snip-
pet s, we extract the following pattern instance:
pL tj pM glosss(tj) pR,
where:
? pM is the longest sequence of HTML tags and
non-alphanumeric characters between tj and
the glossary definition (e.g., ?</b> ?? between
?linux? and ?an? in the above example);
? glosss(tj) is the gloss of tj obtained by mov-
ing to the right of pM until we reach a non-
formatting tag element (e.g., <span>, <p>,
<div>), while ignoring formatting elements
such as <b>, <i> and <a> which are typi-
cally included within a definition sentence;
? pL and pR are the longest sequences of HTML
tags on the left of tj and the right of glosss(tj),
respectively.
For instance, given the HTML snippet
?. . .<p><b>linux</b> ? an <i>operating
system</i> developed by Linus Torvalds</p>. . . ?
we extract the following pattern instance: pL =
?<p><b>?, tj = ?linux?, pM = ?</b> ??,
glosss(tj) = ?an <i>operating system</i>
developed by Linus Torvalds?, pR =?</p>?.
Then we generalize the above pattern instance by
replacing tj and glosss(tj) with *, obtaining:
pL ? pM ? pR,
For the above example, we obtain the following
pattern:
2We use the Google Ajax API, which returns the 64 top-
ranking search results.
<p><b> * </b> ? * </p>.
We add the first sentence of the retrieved gloss
glosss(tj) to our glossary Gkd, i.e., G
k
d := G
k
d ?
{(tj , first(glosss(tj)))}, where first(g) returns
the first sentence of gloss g. Finally, we look for ad-
ditional pairs of terms/glosses in the Web page con-
taining the snippet s by matching the page against
the generalized pattern pL ? pM ? pR, and adding
them to Gkd.
As a result of step (2), for each domain d ? D
we obtain a glossary Gkd for the terms discovered at
iteration k.
Step 3. Topic modeling and gloss filtering (lines
10-12): Unfortunately, not all (term, gloss) pairs
in a glossaryGkd will pertain to the domain d. For in-
stance, we might end up retrieving interdisciplinary
or even unrelated glossaries. In order to address this
fuzziness, we model domains with a Probabilistic
Topic Model (PTM) (Blei et al, 2003; Steyvers and
Griffiths, 2007). PTMs model a given text document
as a mixture of topics. In our case topics are do-
mains and we, first, create a topic model from the
domain glossaries acquired before the current iter-
ation k, then, second, use the topic model to esti-
mate the domain assignment of each new pair (term,
gloss) in our glossaries Gkd, i.e., obtained at iteration
k, third, filter out non-domain glosses.
Creating the topic model (line 10): For a given
iteration k and domain d, we first define the ter-
minology accumulated up until iteration k ? 1 for
that domain as the set T 1,k?1d :=
?k?1
j=1 T
j
d , where
T jd is the set of terms acquired at iteration j, i.e.,
T jd := {t : ?(t, g) ? G
j
d}.
3 Then we define:
? W :=
?
d?D T
1,k?1
d as the entire terminology
acquired up until iteration k?1 for all domains,
i.e., the full set of terms independently of their
domain;
? M :=
?
d?D
?k?1
j=1 G
j
d as the multi-domain
glossary acquired up until iteration k ? 1, i.e.,
the full set of pairs (term, gloss) independently
of their domain;4
3For the first iteration, i.e., when k = 1, we define T 1,0d :=
{t : ?(t, g) ? G1d}, i.e., we use the terminology resulting from
step (2) of the first iteration.
4For k = 1, M :=
?
d?D G
1
d.
172
? Two count matrices, i.e., the word-domain ma-
trix CWD and the gloss-domain matrix CMD,
such that: CWDw,d counts the number of times
w ?W is assigned to domain d ? D, i.e., it oc-
curs in the glosses of domain d; CMD(t,g),d counts
the number of words in g assigned to domain d.
At this point, as shown by Steyvers and Grif-
fiths (2007), we can estimate the probability ?(d)w for
word w, and the probability ?(t,g)d for a term/gloss
pair (t, g), of belonging to domain d:
?(d)w =
CWDw,d + ?
?|W |
w?=1 C
WD
w?,d + |W |?
; (1)
?(t,g)d =
CMD(t,g),d + ?
?|D|
d?=1 C
MD
(t,g),d? + |D|?
(2)
where ? and ? are smoothing factors.5 The two
above probabilities represent the core of our topic
model of the domain knowledge acquired up until
iteration k ? 1.
Probabilistic modeling of iteration-k glosses (line
11): We now utilize the above topic model to es-
timate the probabilities in Formulas 1 and 2 for the
newly acquired glosses at iteration k. To this end we
define M ? :=
?
d?DG
k
d as the union of the (term,
gloss) pairs at iteration k and W ? :=
?
d?D T
k
d
?
W
as the union of terms acquired at iteration k, but also
occurring in W (i.e., the entire terminology up un-
til iteration k ? 1). Then we apply Gibbs sampling
(Blei et al, 2003; Phan et al, 2008) to estimate the
probability of each pair (t, g) ? M ? of pertaining to
a domain d by computing:
?
?(t,g)
d =
RM
?D
(t,g),d + ?
?|D|
d?=1R
M ?D
(t,g),d? + |D|?
(3)
where the gloss-domain matrixRM
?D is initially de-
fined by counting random domain assignments for
each word w? in the bag of words of each (term,
gloss) pair ? M ?. Next, the domain assignment
counts in RM
?D are iteratively updated using Gibbs
sampling.6
5As experienced by Steyvers and Griffiths (2007), the values
of ? = 50/|D| and ? = 0.01 work well with many different
text collections.
6For the PTM part of ProToDoG we used the JGibbLDA
Filtering out non-domain glosses (line 12): Now,
for each domain d ? D, for each pair (t, g) ? Gkd we
have a probability ?
?(t,g)
d of belonging to d. We mark
(t, g) as a non-domain item if ?
?(t,g)
d < ?, where ? is
a confidence threshold, or if ?
?(t,g)
d is not maximum
among all domains in D. Non-domain pairs are re-
moved from Gkd and stored into a set Ad for possible
recovery after the last iteration (see step (5)).
Step 4. Seed selection for next iteration (lines
13-15): For each domain d ? D, we now select
the new set of hypernymy relation seeds to be used
to start the next iteration. First, for each newly-
acquired term/gloss pair (t, g) ? Gkd, we automat-
ically extract a candidate hypernym h from the tex-
tual gloss g. To do this we use a simple heuristic
which just selects the first content term in the gloss.7
Then we sort all the glosses in Gkd by the number of
seed terms found in each gloss. In the case of ties
(i.e., glosses with the same number of seed terms),
we further sort the glosses by ?
?(t,g)
d . Finally we se-
lect the (term, hypernym) pairs corresponding to the
|Sd| top-ranking glosses as the new set of seeds for
the next iteration.
Next, we increment k (line 16 of Algorithm 1)
and if the maximum number of iterations is reached
we jump to step (5). Otherwise, we go back to step
(2) of our glossary bootstrapping algorithm with the
new set of seeds Sd.
Step 5. Gloss recovery (line 19): After all iter-
ations, the entire multi-domain terminology W (cf.
step (3)) may contain several new terms which were
not present when a given gloss g was filtered out.
So, thanks to the last-iteration topic model, the gloss
g might come back into play because its words are
now important cues for a domain. To reassess the
domain pertinence of (term, gloss) pairs in Ad for
each d, we just reapply the entire step (3) by setting
Gmax+1d := Ad for each d ? D. As a result, we
library, a Java Implementation of Latent Dirichlet Allocation
(LDA) using Gibbs Sampling for Parameter Estimation and In-
ference, available at: http://jgibblda.sourceforge.
net/
7While more complex strategies could be devised, e.g.,
lattice-based hypernym extraction (Navigli and Velardi, 2010),
we found that this heuristic works well because, even when it is
not a hypernym, the first term acts as a cue word for the defined
term.
173
obtain an updated glossary Gmax+1d which contains
all the recovered glosses.
Final output: For each domain d ? D the final
output of ProToDoG is a domain glossary Gd :=?
j=1,...,max+1G
j
d. Finally the algorithm aggregates
all glossaries Gd into a multi-domain glossary G
(line 22).
3 Experimental Setup
3.1 Domains
For our experiments we selected 30 different do-
mains ranging from Arts to Warfare, mostly follow-
ing the domain classification of Wikipedia featured
articles (full list at http://lcl.uniroma1.
it/protodog). The set includes several techni-
cal domains, such as Chemistry, Geology, Meteorol-
ogy, Mathematics, some of which are highly inter-
disciplinary. For instance, the Environment domain
covers terms from fields such as Chemistry, Biology,
Law, Politics, etc.
3.2 Gold Standard
Since our evaluations required considerable human
effort, in what follows we calculated all perfor-
mances on a random set of 10 domains, shown in the
top row of Table 1. For each of these 10 domains we
selected well-reputed glossaries on the Web as gold
standards, including the Reuters glossary of finance,
the Utah computing glossary and many others (full
list at the above URL). We show the size of our 10
gold-standard datasets in Table 1.
3.3 Evaluation measures
We evaluated the quality of both terms and glosses,
as jointly extracted by ProToDoG.
3.3.1 Terms
For each domain we calculated coverage, extra-
coverage and precision of the acquired terms T .
Coverage is the ratio of extracted terms in T also
contained in the gold standard T? over the size of T? .
Extra-coverage is calculated as the ratio of the ad-
ditional extracted terms in T \ T? over the number of
gold standard terms T? . Finally, precision is the ra-
tio of extracted terms in T deemed to be within the
domain. To calculate precision we randomly sam-
pled 5% of the retrieved terms and asked two human
annotators to manually tag their domain pertinence
(with adjudication in case of disagreement; ? = .62,
indicating substantial agreement). Note that by ran-
domly sampling on the entire set T we calculate the
precision of both terms in T ? T? , i.e., in the gold
standard, and terms in T \ T? , i.e., not in the gold
standard, but which are not necessarily outside the
domain.
3.3.2 Glosses
We calculated the precision of the extracted
glosses as the ratio of glosses which were both well-
formed textual definitions and specific to the tar-
get domain. Precision was determined on a random
sample of 5% of the acquired glosses for each do-
main. The annotation was made by two annotators,
with ? = .675, indicating substantial agreement.
The annotators were provided with specific guide-
lines available on the ProToDoG Web site (see URL
above).
3.4 Comparison
We compared ProToDog against:
? BoW: a bag-of-words variant in which step
(3) is replaced by a simple bag-of-words scor-
ing approach which assigns a score to each
term/gloss pair (t, g) ? Gkd as follows:
score(g) =
|Bag(g) ? T 1,k?1d |
|Bag(g)|
. (4)
where Bag(g) contains all content words in
g. At iteration k, we filter out those glosses
whose score(g) < ?, where ? is a thresh-
old tuned in the same manner as ? (see Sec-
tion 3.5). This approach essentially implements
GlossBoot, our previous work on domain glos-
sary bootstrapping (De Benedictis et al, 2013).
? Wikipedia: since Wikipedia is the largest
collaborative resource, covering hundreds
of fields of knowledge, we devised a simple
heuristic for producing multi-domain glos-
saries from Wikipedia, so as to compare their
performance against our gold standards. For
each target domain we manually selected one
174
45%
50%
55%
60%
65%
70%
75%
80%
85%
 2  4  6  8  10  12  14  16  18  20
iteration
BotanyFashion
Figure 1: Harmonic mean of precision and coverage for
Botany and Fashion (tuning domains) over 20 iterations
(|Sd|=5, ?=0.03).
or more Wikipedia categories representing
the domain (for instance, Category:Arts
for Arts, Category:Business for Fi-
nance, etc.). Then, for each domain d,
we picked out all the Wikipedia pages
tagged either with the categories selected
for d or their direct subcategories (e.g.,
Category:Creative works) or sub-
subcategories (e.g., Category:Genres).
From each page we extracted a (page title,
gloss) pair, where the gloss was obtained by
extracting the first sentence of the Wikipedia
page, as done, e.g., in BabelNet (Navigli and
Ponzetto, 2012). Since subcategories might
have more parents and might thus belong to
multiple domains, we discarded pages assigned
to more than 2 domains.
3.5 Parameter tuning
In order to choose the optimal values of the parame-
ters of ProToDoG (number |Sd| of seeds per domain,
number max of iterations, and filtering threshold ?)
and BoW (? threshold) we selected two extra do-
mains, i.e., Botany and Fashion, not used in our
tests, together with the corresponding gold standard
Web glossaries.
As regards the number of seeds, we defined an
initial pool of 10 seeds for each of the two tun-
ing domains and studied the average performance
of 5 random sets of x seeds (from the initial pool),
when x = 1, 3, 5, 7, 9. As regards the number of
iterations, we explored all values between 1 and
20. Finally, for the filtering thresholds ? and ?
for ProToDoG PTM and its BoW variant, we tried
values of ? ? {0, 0.03, 0.06, . . . , 0.6} and ? ?
{0, 0.05, 0.1, . . . , 1.0}, respectively.
Given the high number of possible parameter
value configurations, we first explored the entire
search space automatically by calculating the cov-
erage of ProToDoG PTM (and BoW) with each con-
figuration against our tuning gold standards. Then
we identified as optimal candidates those ?fron-
tier? configurations for which, when moving from
a lower-coverage configuration, coverage reached a
maximum. We then calculated the precision of each
optimal candidate configuration by manually vali-
dating a 3% random sample of the resulting glos-
saries for the two tuning domains. The optimal con-
figuration for ProToDoG was |Sd| = 5, max = 5,
? = 0.03, while for BoW was ? = 0.1.
In Figure 1 we show the performance trend over
iterations for our two tuning domains when |Sd| = 5
and ? = 0.03. Performance is calculated as the
harmonic mean of precision and coverage of the ac-
quired glossary after each iteration, from 1 to 20. We
can see that after 5 iterations performance decreases
for Botany (a highly interdisciplinary domain) due
to lower precision, while it remains stable for Fash-
ion due to the lack of newly-acquired glosses.
3.6 Seed Selection
For each domain d we manually selected five seed
hypernymy relations as the seed sets Sd input to Al-
gorithm 1 (see Section 3.5). The seeds were selected
by the authors on the basis of just two conditions: i)
the seeds should cover different aspects of the do-
main and, indeed, should identify the domain im-
plicitly; ii) at least 10,000 results should be returned
by the search engine when querying it with the seeds
plus the glossary keyword (see line 6 of Algo-
rithm 1). The seed selection was not fine-tuned (i.e.,
it was not adjusted to improve performance), so it
might well be that better seeds would provide better
results (see (Kozareva and Hovy, 2010a)). However,
such a study is beyond the scope of this paper.
175
A
rt
B
us
in
es
s
C
he
m
is
tr
y
C
om
pu
ti
ng
E
nv
ir
on
m
en
t
Fo
od
L
aw
M
us
ic
P
hy
si
cs
S
po
rt
Gold t/g 394 1777 164 421 713 946 180 218 315 146
PTM t 4253 7370 2493 3412 3009 1526 1836 1647 3847 1696
g 7386 9795 3841 4186 3552 2175 4141 2729 5197 2938
BoW t 4012 7639 1174 3127 3644 1827 1773 1166 4471 1990
g 5923 8999 1414 3662 4334 2601 4024 1249 6956 3425
Wiki t,g 107.1k 48.4k 8137 32.0k 23.6k 5698 13.5k 84.1k 33.8k 267.5k
Table 1: Size of the gold standard and the automatically-acquired glossaries for 10 of the 30 selected domains (t:
number of terms, g: number of glosses).
4 Results and Discussion
4.1 Terms
The size of the extracted terminologies for the 10 do-
mains after five iterations is reported in Table 1 (the
output for all 30 domains is available at the above
URL, cf. Section 3.1). ProToDoG PTM and its BoW
variant extract thousands of terms and glosses for
each domain, whereas the number of glosses ob-
tained from Wikipedia (cf. Section 3.4) varies de-
pending upon the domain, from thousands to hun-
dreds of thousands. Note that there is no overlap
between the glossaries extracted by ProToDoG and
the set of Wikipedia articles, since the latter are not
organized as glossaries.
In Table 2 we show the percentage results in
terms of precision (P), coverage (C), and extra-
coverage (X, see Section 3.3 for definitions) for
ProToDoG PTM and its BoW variant and for the
Wikipedia glossary. With the exception of the
Food domain, ProToDoG achieves the best pre-
cision. The Wikipedia glossary has fluctuating
precision values, ranging between 25% and 90%,
due to the heterogeneous nature of subcategories.
ProToDog achieves the best coverage of gold stan-
dard terms on 6 of the 10 domains, with the BoW
variant obtaining slightly higher coverage on 3 do-
mains and +10% on the Food domain. The cov-
erage of Wikipedia glossaries, instead, with the
sole exception of Sport, is much lower, despite the
use of (sub)subcategories (cf. Section 3.4). Both
ProToDoG PTM and BoW achieve very high extra-
coverage percentages, meaning that they are able to
go substantially beyond our domain gold standards,
but it is the Wikipedia glossary which achieves the
highest extra-coverage values. To get a better in-
sight into the quality of extra-coverage we calcu-
lated the percentage of named entities (i.e., encyclo-
pedic) among the terms extracted by each of the dif-
ferent approaches. Comparing results across the (E)
columns of Table 2 it can be seen that high percent-
ages of the terms extracted by Wikipedia are named
entities, which is in marked contrast to the 0%-1%
extracted by ProToDog. This is as should be ex-
pected for an encyclopedia, whose coverage focuses
on people, places, brands, etc. rather than concepts.
To summarize, ProToDoG PTM outperforms both
BoW and Wikipedia in terms of precision, while
at the same time achieving both competitive cov-
erage and extra-coverage. The Wikipedia glossary
suffers from fluctuating precision values across do-
mains and overly encyclopedic coverage of terms.
4.2 Glosses
We show the results of gloss evaluation in Table 2
(last two columns) for ProToDoG PTM and BoW
(we do not report the precision values for Wikipedia,
as they are slightly lower than those obtained for
terms). Precision ranges between 89% and 99%
for ProToDoG PTM and between 82% and 97%
for BoW. We observe that these results are strongly
correlated with the precision of the extracted terms
(cf. Table 2), because the retrieved glosses of do-
main terms are usually in-domain too, and follow a
definitional style since they come from glossaries.
Note, however, that the gloss precision could also be
176
terms glosses
PTM BoW Wiki PTM BoW
P C X E P C X E P C X E P P
Art 92 26 1053 1 86 25 992 0 81 19 23.4k 67 93 87
Business 95 41 374 0 90 43 387 0 37 15 2692 31 96 91
Chemistry 99 77 1410 0 95 73 643 0 49 18 12.9k 3 98 96
Computing 95 43 767 0 93 40 702 0 81 30 7506 36 96 94
Environment 91 29 393 0 84 28 482 0 25 9 3302 12 89 82
Food 91 21 1404 0 97 31 1621 0 81 9 3997 25 92 95
Law 98 89 931 0 95 87 897 0 35 34 7406 16 99 97
Music 94 98 660 0 93 84 453 0 90 50 37.1k 84 96 95
Physics 97 43 1178 0 91 46 1373 0 68 25 10.6k 10 95 89
Sport 98 22 1139 1 96 23 1339 1 87 44 178.2k 83 97 96
Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.
A
rt
B
us
in
es
s
C
he
m
is
tr
y
C
om
pu
tin
g
E
nv
ir
on
m
en
t
Fo
od
L
aw
M
us
ic
Ph
ys
ic
s
Sp
or
t
Google Define 76 80 93 86 88 91 96 96 98 84
ProToDoG 27 41 81 40 37 19 85 98 47 27
Table 3: Number of domain glosses (from a random sam-
ple of 100 gold standard terms per domain) retrieved us-
ing Google Define and ProToDoG.
higher than term precision, thanks to many pertinent
glosses being extracted for the same term (cf. Table
1).
In Table 4 we show an excerpt of the multi-
domain glossary extracted by ProToDoG for the Art,
Business and Sport domains.
5 Comparative Evaluation
5.1 Comparison with Google Define
We performed a comparison with Google Define,8
a state-of-the-art definition search service. This
service inputs a term query and outputs a list of
glosses. First, we randomly sampled 100 terms
from our gold standard for each domain. Next, for
each domain, we manually calculated the fraction
of terms for which at least one in-domain defini-
tion was provided by Google Define and ProToDoG.
8Accessible from Google search with the define: key-
word.
Table 3 shows the coverage results. In this exper-
iment, Google Define outperforms our system on
9 of the 10 analyzed domains. However, we note
that when searching for domain-specific knowledge
only, Google Define: i) needs to know the domain
term to be defined in advance, while ProToDoG
jointly acquires domain terms and glosses starting
from just a few seeds; ii) does not discriminate be-
tween glosses pertaining to the target domain and
glosses pertaining to other fields or senses, whereas
ProToDog extracts terms and glosses specific to each
domain of interest.
5.2 Comparison with TaxoLearn
We also compared ProToDoG with the output of
a state-of-the-art taxonomy learning framework,
called TaxoLearn (Navigli et al, 2011). We did
this because i) TaxoLearn extracts terms and glosses
from domain corpora in order to create a domain tax-
onomy; ii) it is one of the few systems which extracts
both terms and glosses from specialized corpora; iii)
the extracted glossaries are available online.9 There-
fore we compared the performance of ProToDoG on
two domains for which glossaries were extracted by
TaxoLearn, i.e. AI and Finance. The glossaries were
harvested from large collections of scholarly arti-
cles. For ProToDoG we selected 10 seeds to cover
all the fields of AI, while for the financial domain
we selected the same 5 seeds used in the Business
9http://ontolearn.org and http://lcl.
uniroma1.it/taxolearn
177
Art
rock art includes pictographs (designs painted on stone surfaces) and petroglyphs (designs
pecked or incised on stone surfaces).
impressionism Late 19th-century French school dedicated to defining transitory visual impressions
painted directly from nature, with light and color of primary importance.
point Regarding paper, a unit of thickness equating 1/1000 inch.
Business
hyperinflation Extremely rapid or out of control inflation.
interbank rate The rate of interest charged by a bank on a loan to another bank.
points Amount of discount on a mortgage loan stated as a percentage; one point equals one
percent of the face amount of the loan; a discount of one point raises the net yield on
the loan by one-eighth of one percent.
Sport
gross score The actual number of strokes taken by a player for hole or round before the player?s
handicap is deducted.
obstructing preventing the opponent from going around a player by standing in the path of move-
ment.
points a team statistic indicating its degree of success, calculated as follows: 2 points for a
win (3 in the 1994 World Cup), 1 point for a tie, 0 points for a loss.
Table 4: An excerpt of the resulting multi-domain glossary obtained with ProToDoG.
domain of our experiments above (cf. Section 3).
We show the number of extracted terms and
glosses for ProToDoG and TaxoLearn in Table 5.
We also show the precision values calculated on a
random sample of 5% of terms and glosses. As can
be clearly seen, on both domains ProToDoG extracts
a number of terms and glosses which is an order
of magnitude greater than those obtained by Tax-
oLearn, while at the same time obtaining consider-
ably higher precision.
6 Related Work
Current approaches to automatic glossary acquisi-
tion suffer from two main issues: i) the poor avail-
ability of large domain-specific corpora from which
terms and glosses are extracted at different times;
ii) the focus on individual domains. ProToDog ad-
dresses both issues by providing a joint multi-
domain approach to term and glossary extraction.
Among the approaches which extract unre-
stricted textual definitions from open text, Fujii and
Ishikawa (2000) determine the definitional nature of
text fragments by using an n-gram model, whereas
Klavans and Muresan (2001) apply pattern match-
ing techniques at the lexical level guided by cue
phrases such as ?is called? and ?is defined as?.
More recently, a domain-independent supervised ap-
proach, named Word-Class Lattices (WCLs), was
presented which learns lattice-based definition clas-
sifiers applied to candidate sentences containing the
input terms (Navigli and Velardi, 2010). To avoid
the burden of manually creating a training dataset,
definitional patterns can be extracted automatically.
Faralli and Navigli (2013) utilized Wikipedia as
a huge source of definitions and simple, yet ef-
fective heuristics to automatically annotate them.
Reiplinger et al (2012) experimented with two dif-
ferent approaches for the acquisition of lexical-
syntactic patterns. The first approach bootstraps pat-
terns from a domain corpus and then manually re-
fines the acquired patterns. The second approach, in-
stead, automatically acquires definitional sentences
by using a more sophisticated syntactic and seman-
tic processing. The results show high precision
in both cases. However, all the above approaches
need large domain corpora, the poor availability of
which hampers the creation of wide-coverage glos-
saries for several domains. To avoid the need to
use a large corpus, domain terminologies can be ob-
tained by using Doubly-Anchored Patterns (DAPs)
178
AI Finance
# terms P # glosses P # terms P # glosses P
ProToDoG 4983 83% 5326 84% 7370 95% 9795 96%
TaxoLearn 427 77% 834 79% 2348 86% 1064 88%
Table 5: Number and precision of terms and glosses extracted by ProToDoG and TaxoLearn in the Artificial Intelli-
gence (AI) and Finance domains.
which, given a (term, hypernym) pair, extract from
the Web sentences matching manually-defined pat-
terns like ?<hypernym> such as <term>, and *?
(Kozareva and Hovy, 2010b). This term extrac-
tion process is further extended by harvesting new
hypernyms using the corresponding inverse pat-
terns (called DAP?1) like ?* such as <term1>, and
<term2>?. Similarly to ProToDoG, this approach
drops the requirement of a domain corpus and starts
from a small number of (term, hypernym) seeds.
However, while DAPs have proven useful in the in-
duction of domain taxonomies (Kozareva and Hovy,
2010b), they cannot be applied to the glossary learn-
ing task because the extracted sentences are not for-
mal definitions. In contrast, ProToDoG performs
the novel task of multi-domain glossary acquisition
from the Web by bootstrapping the extraction pro-
cess with a few (term, hypernym) seeds. Bootstrap-
ping techniques (Brin, 1998; Agichtein and Gra-
vano, 2000; Pas?ca et al, 2006) have been success-
fully applied to several tasks, including learning se-
mantic relations (Pantel and Pennacchiotti, 2006),
extracting surface text patterns for open-domain
question answering (Ravichandran and Hovy, 2002),
semantic tagging (Huang and Riloff, 2010) and un-
supervised Word Sense Disambiguation (Yarowsky,
1995). ProToDoG synergistically integrates boot-
strapping with probabilistic topic models so as to
keep the glossary acquisition process within the tar-
get domains as much as possible.
7 Conclusions
In this paper we have presented ProToDoG, a new,
minimally-supervised approach to multi-domain
glossary acquisition. Starting from a small set of hy-
pernymy seeds which identify each domain of inter-
est, we apply a bootstrapping approach which itera-
tively obtains generalized patterns from Web glos-
saries and then applies them to the extraction of
term/gloss pairs. To our knowledge, ProToDoG is
the first approach to large-scale probabilistic glos-
sary learning which jointly acquires thousands of
terms and glosses for dozens of domains with mini-
mal supervision.
At the core of ProToDoG lies our glossary boot-
strapping approach, thanks to which we can drop
the requirements of existing techniques such as the
ready availability of domain corpora, which often do
not contain enough definitions (cf. Table 5), and the
manual definition of lexical patterns, which typically
extract sentence snippets instead of formal glosses.
ProToDoG will be made available to the re-
search community. Beyond the immediate usabil-
ity of the output glossaries (we show an excerpt
in Table 4), we also wish to show the benefit
of ProToDoG in gloss-driven approaches to taxon-
omy learning (Navigli et al, 2011; Velardi et al,
2013) and Word Sense Disambiguation (Duan and
Yates, 2010; Faralli and Navigli, 2012). The 30-
domain glossaries and gold standards created for
our experiments are available from http://lcl.
uniroma1.it/protodog.
We remark that the terminologies covered with
ProToDoG are not only precise, but are also one
order of magnitude greater than those covered in
individual online glossaries. As future work, we
plan to study the ability of ProToDoG to acquire
domain glossaries at different levels of specificity
(i.e., domains vs. subdomains). Finally, we will
adapt ProToDoG to other languages, by translating
the glossary keyword used in step (2), along the
lines of (De Benedictis et al, 2013).
Acknowledgments
The authors gratefully acknowledge
the support of the ?MultiJEDI? ERC
Starting Grant No. 259234.
179
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the 5th ACM conference on Digital
Libraries, pages 85?94, San Antonio, Texas, USA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Sergey Brin. 1998. Extracting patterns and relations
from the World Wide Web. In Proceedings of the
International Workshop on The World Wide Web and
Databases, pages 172?183, London, UK.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007. Soft
pattern matching models for definitional question an-
swering. ACM Transactions on Information Systems,
25(2):8.
Flavio De Benedictis, Stefano Faralli, and Roberto Nav-
igli. 2013. GlossBoot: Bootstrapping Multilingual
Domain Glossaries from the Web. In Proceedings of
the 51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 528?538, Sofia, Bulgaria.
Weisi Duan and Alexander Yates. 2010. Extracting
glosses to disambiguate word senses. In Proceedings
of Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 627?
635, Los Angeles, CA, USA.
Stefano Faralli and Roberto Navigli. 2012. A New
Minimally-supervised Framework for Domain Word
Sense Disambiguation. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 1411?1422, Jeju, Korea.
Stefano Faralli and Roberto Navigli. 2013. A Java
Framework for Multilingual Definition and Hypernym
Extraction. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics, Sys-
tem Demonstrations, pages 103?108, Sofia, Bulgaria.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the World Wide Web as an encyclopedia: extracting
term descriptions from semi-structured texts. In Pro-
ceedings of the 38th Annual Meeting on Association
for Computational Linguistics, pages 488?495, Hong
Kong.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 539?545, Nantes, France.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-structured
content and Artificial Intelligence: The story so far.
Artificial Intelligence, 194:2?27.
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
275?285, Uppsala, Sweden.
Judith Klavans and Smaranda Muresan. 2001. Evalu-
ation of the DEFINDER system for fully automatic
glossary construction. In Proceedings of the American
Medical Informatics Association (AMIA) Symposium,
pages 324?328, Washington, D.C., USA.
Zornitsa Kozareva and Eduard H. Hovy. 2010a. Not all
seeds are equal: Measuring the quality of text min-
ing seeds. In Proceedings of Human Language Tech-
nologies: The 11th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 618?626, Los Angeles, Cali-
fornia, USA.
Zornitsa Kozareva and Eduard H. Hovy. 2010b. A semi-
supervised method to learn and construct taxonomies
using the web. In Proceedings of Empirical Methods
in Natural Language Processing, pages 1110?1118,
Cambridge, MA, USA.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1318?1327, Uppsala, Sweden.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexi-
cal taxonomies from scratch. In Proceedings of the
22th International Joint Conference on Artificial Intel-
ligence, pages 1872?1877, Barcelona, Spain.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and similarities on
the Web: Fact extraction in the fast lane. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 809?
816, Sydney, Australia.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging Generic Patterns for Automatically Har-
vesting Semantic Relations. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics (COLING-ACL), Syd-
ney, Australia, pages 113?120, Sydney, Australia.
Xuan-Hieu Phan, Le-Minh Nguyen, and Susumu
Horiguchi. 2008. Learning to classify short and sparse
text & web with hidden topics from large-scale data
collections. In Proceedings of the 17th international
180
conference on World Wide Web, WWW ?08, pages 91?
100, New York, NY, USA.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering sys-
tem. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, pages 41?
47, Philadelphia, PA, USA.
Melanie Reiplinger, Ulrich Scha?fer, and Magdalena Wol-
ska. 2012. Extracting glossary sentences from schol-
arly articles: A comparative evaluation of pattern
bootstrapping and deep analysis. In Proceedings of
the ACL-2012 Special Workshop on Rediscovering 50
Years of Discoveries, pages 55?65, Jeju Island, Korea.
Horacio Saggion. 2004. Identifying definitions in text
collections for question answering. In Proceedings
of the Fourth International Conference on Language
Resources and Evaluation, pages 1927?1930, Lisbon,
Portugal.
Mark Steyvers and Tom Griffiths, 2007. Probabilistic
Topic Models. Lawrence Erlbaum Associates.
Paola Velardi, Roberto Navigli, and Pierluigi D?Amadio.
2008. Mining the web to create specialized glossaries.
IEEE Intelligent Systems, 23(5):18?25.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algorithm
for taxonomy induction. Computational Linguistics,
39(3):665?707.
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA, USA.
181
OntoLearn Reloaded: A Graph-Based
Algorithm for Taxonomy Induction
Paola Velardi?
Sapienza University of Rome
Stefano Faralli?
Sapienza University of Rome
Roberto Navigli?
Sapienza University of Rome
In 2004 we published in this journal an article describing OntoLearn, one of the first systems
to automatically induce a taxonomy from documents and Web sites. Since then, OntoLearn has
continued to be an active area of research in our group and has become a reference work within
the community. In this paper we describe our next-generation taxonomy learning methodol-
ogy, which we name OntoLearn Reloaded. Unlike many taxonomy learning approaches in the
literature, our novel algorithm learns both concepts and relations entirely from scratch via the
automated extraction of terms, definitions, and hypernyms. This results in a very dense, cyclic
and potentially disconnected hypernym graph. The algorithm then induces a taxonomy from
this graph via optimal branching and a novel weighting policy. Our experiments show that we
obtain high-quality results, both when building brand-new taxonomies and when reconstructing
sub-hierarchies of existing taxonomies.
1. Introduction
Ontologies have proven useful for different applications, such as heterogeneous data
integration, information search and retrieval, question answering, and, in general, for
fostering interoperability between systems. Ontologies can be classified into three main
types (Sowa 2000), namely: i) formal ontologies, that is, conceptualizations whose cat-
egories are distinguished by axioms and formal definitions, stated in logic to support
complex inferences and computations; ii) prototype-based ontologies, which are based
on typical instances or prototypes rather than axioms and definitions in logic; iii) lexical-
ized (or terminological) ontologies, which are specified by subtype-supertype relations
and describe concepts by labels or synonyms rather than by prototypical instances.
Here we focus on lexicalized ontologies because, in order to enable natural
language applications such as semantically enhanced information retrieval and ques-
tion answering, we need a clear connection between our formal representation of the
? Dipartimento di Informatica, Sapienza Universita` di Roma, Via Salaria, 113, 00198 Roma Italy.
E-mail: {velardi,faralli,navigli}@di.uniroma1.it.
Submission received: 17 December 2011; revised submission received: 28 July 2012; accepted for publication:
10 October 2012.
doi:10.1162/COLI a 00146
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
domain and the language used to express domain meanings within text. And, in turn,
this connection can be established by producing full-fledged lexicalized ontologies for
the domain of interest. Manually constructing ontologies is a very demanding task,
however, requiring a large amount of time and effort, even when principled solutions
are used (De Nicola, Missikoff, and Navigli 2009). A quite recent challenge, referred
to as ontology learning, consists of automatically or semi-automatically creating a
lexicalized ontology using textual data from corpora or the Web (Gomez-Perez and
Manzano-Mancho 2003; Biemann 2005; Maedche and Staab 2009; Petasis et al 2011). As
a result of ontology learning, the heavy requirements of manual ontology construction
can be drastically reduced.
In this paper we deal with the problem of learning a taxonomy (i.e., the backbone
of an ontology) entirely from scratch. Very few systems in the literature address this
task. OntoLearn (Navigli and Velardi 2004) was one of the earliest contributions in this
area. In OntoLearn taxonomy learning was accomplished in four steps: terminology
extraction, derivation of term sub-trees via string inclusion, disambiguation of domain
terms using a novel Word Sense Disambiguation algorithm, and combining the sub-
trees into a taxonomy. The use of a static, general-purpose repository of semantic
knowledge, namely, WordNet (Miller et al 1990; Fellbaum 1998), prevented the system
from learning taxonomies in technical domains, however.
In this paper we present OntoLearn Reloaded, a graph-based algorithm for learning
a taxonomy from the ground up. OntoLearn Reloaded preserves the initial step of
our 2004 pioneering work (Navigli and Velardi 2004), that is, automated terminology
extraction from a domain corpus, but it drops the requirement for WordNet (thereby
avoiding dependence on the English language). It also drops the term compositionality
assumption that previously led to us having to use a Word Sense Disambiguation
algorithm?namely, SSI (Navigli and Velardi 2005)?to structure the taxonomy. Instead,
we now exploit textual definitions, extracted from a corpus and the Web in an iterative
fashion, to automatically create a highly dense, cyclic, potentially disconnected hyper-
nym graph. An optimal branching algorithm is then used to induce a full-fledged tree-
like taxonomy. Further graph-based processing augments the taxonomy with additional
hypernyms, thus producing a Directed Acyclic Graph (DAG).
Our system provides a considerable advancement over the state of the art in
taxonomy learning:
 First, excepting for the manual selection of just a few upper nodes, this
is the first algorithm that has been experimentally shown to build from
scratch a new taxonomy (i.e., both concepts and hypernym relations)
for arbitrary domains, including very technical ones for which
gold-standard taxonomies do not exist.
 Second, we tackle the problem with no simplifying assumptions: We cope
with issues such as term ambiguity, complexity of hypernymy patterns,
and multiple hypernyms.
 Third, we propose a novel algorithm to extract an optimal branching
from the resulting hypernym graph, which?after some recovery
steps?becomes our final taxonomy. Taxonomy induction is the
main theoretical contribution of the paper.
 Fourth, the evaluation is not limited, as it is in most papers, to the number
of retrieved hypernymy relations that are found in a reference taxonomy.
666
Velardi, Faralli, and Navigli OntoLearn Reloaded
Instead, we also analyze the extracted taxonomy in its entirety;
furthermore, we acquire two ?brand new? taxonomies in the
domains of ARTIFICIAL INTELLIGENCE and FINANCE.
 Finally, our taxonomy-building workflow is fully implemented and
the software components are either freely available from our Web
site,1 or reproducible.
In this paper we extend our recent work on the topic (Navigli, Velardi, and Faralli
2011) as follows: i) we describe in full detail the taxonomy induction algorithm; ii) we
enhance our methodology with a final step aimed at creating a DAG, rather than a strict
tree-like taxonomical structure; iii) we perform a large-scale multi-faceted evaluation
of the taxonomy learning algorithm on six domains; and iv) we contribute a novel
methodology for evaluating an automatically learned taxonomy against a reference
gold standard.
In Section 2 we illustrate the related work. We then describe our taxonomy-
induction algorithm in Section 3. In Section 4 we present our experiments, and discuss
the results. Evaluation is both qualitative (on new ARTIFICIAL INTELLIGENCE and
FINANCE taxonomies), and quantitative (on WordNet and MeSH sub-hierarchies). Sec-
tion 5 is dedicated to concluding remarks.
2. Related Work
Two main approaches are used to learn an ontology from text: rule-based and distri-
butional approaches. Rule-based approaches use predefined rules or heuristic patterns
to extract terms and relations. These approaches are typically based on lexico-syntactic
patterns, first introduced by Hearst (1992). Instances of relations are harvested from text
by applying patterns aimed at capturing a certain type of relation (e.g., X is a kind of Y).
Such lexico-syntactic patterns can be defined manually (Berland and Charniak 1999;
Kozareva, Riloff, and Hovy 2008) or obtained by means of bootstrapping techniques
(Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006). In the latter case,
a number of term pairs in the wanted relation are manually picked and the relation is
sought within text corpora or the Web. Other rule-based approaches learn a taxonomy
by applying heuristics to collaborative resources such as Wikipedia (Suchanek, Kasneci,
and Weikum 2008; Ponzetto and Strube 2011), also with the supportive aid of computa-
tional lexicons such as WordNet (Ponzetto and Navigli 2009).
Distributional approaches, instead, model ontology learning as a clustering or
classification task, and draw primarily on the notions of distributional similarity (Pado
and Lapata 2007; Cohen and Widdows 2009), clustering of formalized statements (Poon
and Domingos 2010), or hierarchical random graphs (Fountain and Lapata 2012). Such
approaches are based on the assumption that paradigmatically-related concepts2 appear
in similar contexts and their main advantage is that they are able to discover relations
that do not explicitly appear in the text. They are typically less accurate, however, and
the selection of feature types, notion of context, and similarity metrics vary considerably
depending on the specific approach used.
1 http://lcl.uniroma1.it/ontolearn reloaded and http://ontolearn.org.
2 Because we are concerned with lexical taxonomies, in this paper we use the words concepts and terms
interchangeably.
667
Computational Linguistics Volume 39, Number 3
Recently, Yang and Callan (2009) presented a semi-supervised taxonomy induc-
tion framework that integrates contextual, co-occurrence, and syntactic dependencies,
lexico-syntactic patterns, and other features to learn an ontology metric, calculated
in terms of the semantic distance for each pair of terms in a taxonomy. Terms are
incrementally clustered on the basis of their ontology metric scores. In their work, the
authors assume that the set of ontological concepts C is known, therefore taxonomy
learning is limited to finding relations between given pairs in C. In the experiments,
they only use the word senses within a particular WordNet sub-hierarchy so as to avoid
any lexical ambiguity. Their best experiment obtains a 0.85 precision rate and 0.32 recall
rate in replicating is-a links on 12 focused WordNet sub-hierarchies, such as PEOPLE,
BUILDING, PLACE, MILK, MEAL, and so on.
Snow, Jurafsky, and Ng (2006) propose the incremental construction of taxonomies
using a probabilistic model. In their work they combine evidence from multiple
supervised classifiers trained on very large training data sets of hyponymy and cousin
relations. Given the body of evidence obtained from all the relevant word pairs in
a lexico-syntactic relation, the taxonomy learning task is defined probabilistically as
the problem of finding the taxonomy that maximizes the probability of having that
evidence (a supervised logistic regression model is used for this). Rather than learning
a new taxonomy from scratch, however, this approach aims at attaching new concepts
under the appropriate nodes of an existing taxonomy (i.e., WordNet). The approach is
evaluated by manually assessing the quality of the single hypernymy edges connecting
leaf concepts to existing ones in WordNet, with no evaluation of a full-fledged struc-
tured taxonomy and no restriction to a specific domain. A related, weakly supervised
approach aimed at categorizing named entities, and attaching them to WordNet leaves,
was proposed by Pasca (2004). Other approaches use formal concept analysis (Cimiano,
Hotho, and Staab 2005), probabilistic and information-theoretic measures to learn tax-
onomies from a folksonomy (Tang et al 2009), and Markov logic networks and syntactic
parsing applied to domain text (Poon and Domingos 2010).
The work closest to ours is that presented by Kozareva and Hovy (2010). From an
initial given set of root concepts and basic level terms, the authors first use Hearst-like
lexico-syntactic patterns iteratively to harvest new terms from the Web. As a result a
set of hyponym?hypernym relations is obtained. Next, in order to induce taxonomic
relations between intermediate concepts, the Web is searched again with surface pat-
terns. Finally, nodes from the resulting graph are removed if the out-degree is below
a threshold, and edges are pruned by removing cycles and selecting the longest path
in the case of multiple paths between concept pairs. Kozareva and Hovy?s method has
some limitations, which we discuss later in this paper. Here we note that, in evalu-
ating their methodology, the authors discard any retrieved nodes not belonging to a
WordNet sub-hierarchy (they experiment on PLANTS, VEHICLES, and ANIMALS), thus
it all comes down to Yang and Callan?s (2009) experiment of finding relations between a
pre-assigned set of nodes.
In practice, none of the algorithms described in the literature was actually applied
to the task of creating a new taxonomy for an arbitrary domain of interest truly from
scratch. Instead, what is typically measured is the ability of a system to reproduce as
far as possible the relations of an already existing taxonomy (a common test is WordNet
or the Open Directory Project3), when given the set of domain concepts. Evaluating
against a gold standard is, indeed, a reasonable validation methodology. The claim to be
3 http://www.dmoz.org/.
668
Velardi, Faralli, and Navigli OntoLearn Reloaded
Figure 1
The OntoLearn Reloaded taxonomy learning workflow.
?automatically building? a taxonomy needs also to be demonstrated on new domains
for which no a priori knowledge is available, however. In an unknown domain, tax-
onomy induction requires the solution of several further problems, such as identifying
domain-appropriate concepts, extracting appropriate hypernym relations, and detect-
ing lexical ambiguity, whereas some of these problems can be ignored when evaluating
against a gold standard (we will return to this issue in detail in Section 4). In fact,
the predecessor of OntoLearn Reloaded, that is, OntoLearn (Navigli and Velardi 2004),
suffers from a similar problem, in that it relies on the WordNet taxonomy to establish
paradigmatic connections between concepts.
3. The Taxonomy Learning Workflow
OntoLearn Reloaded starts from an initially empty directed graph and a corpus for the
domain of interest (e.g., an archive of artificial intelligence papers). We also assume
that a small set of upper terms (entity, abstraction, etc.), which we take as the end
points of our algorithm, has been manually defined (e.g., from a general purpose taxon-
omy like WordNet) or is available for the domain.4 Our taxonomy-learning workflow,
summarized in Figure 1, consists of five steps:
1. Initial Terminology Extraction (Section 3.1): The first step applies a term
extraction algorithm to the input domain corpus in order to produce an
initial domain terminology as output.
2. Definition & Hypernym Extraction (Section 3.2): Candidate definition
sentences are then sought for the extracted domain terminology. For each
term t, a domain-independent classifier is used to select well-formed
definitions from the candidate sentences and extract the corresponding
hypernyms of t.
4 Although very few domain taxonomies are available, upper (core) concepts have been defined in several
domains, such as MEDICINE, ART, ECONOMY, and so forth.
669
Computational Linguistics Volume 39, Number 3
3. Domain Filtering (Section 3.3): A domain filtering technique is applied
to filter out those definitions that do not pertain to the domain of interest.
The resulting domain definitions are used to populate the directed graph
with hypernymy relations connecting t to the extracted hypernym h.
Steps (2) and (3) are then iterated on the newly acquired hypernyms,
until a termination condition occurs.
4. Graph Pruning (Section 3.4): As a result of the iterative phase we obtain
a dense hypernym graph that potentially contains cycles and multiple
hypernyms for most nodes. In this step we combine a novel weighting
strategy with the Chu-Liu/Edmonds algorithm (Chu and Liu 1965;
Edmonds 1967) to produce an optimal branching (i.e., a tree-like
taxonomy) of the initial noisy graph.
5. Edge Recovery (Section 3.5): Finally, we optionally apply a recovery
strategy to reattach some of the hypernym edges deleted during the
previous step, so as to produce a full-fledged taxonomy in the form
of a DAG.
We now describe in full detail the five steps of OntoLearn Reloaded.5
3.1 Initial Terminology Extraction
Domain terms are the building blocks of a taxonomy. Even though in many cases an
initial domain terminology is available, new terms emerge continuously, especially
in novel or scientific domains. Therefore, in this work we aim at fully automatizing
the taxonomy induction process. Thus, we start from a text corpus for the domain
of interest and extract domain terms from the corpus by means of a terminology
extraction algorithm. For this we use our term extraction tool, TermExtractor,6 that
implements measures of domain consensus and relevance to harvest the most relevant
terms for the domain from the input corpus.7 As a result, an initial domain terminol-
ogy T(0) is produced that includes both single- and multi-word expressions (such as,
respectively, graph and flow network). We add one node to our initially empty graph
Gnoisy = (Vnoisy, Enoisy) for each term in T(0)?that is, we set Vnoisy := T(0) and Enoisy := ?.
In Table 1 we show an excerpt of our ARTIFICIAL INTELLIGENCE and FINANCE
terminologies (cf. Section 4 for more details). Note that our initial set of domain terms
(and, consequently, nodes) will be enriched with the new hypernyms acquired during
the subsequent iterative phase, described in the next section.
3.2 Definition and Hypernym Extraction
The aim of our taxonomy induction algorithm is to learn a hypernym graph by means of
several iterations, starting from T(0) and stopping at very general terms U, that we take
as the end point of our algorithm. The upper terms are chosen from WordNet topmost
5 A video of the first four steps of OntoLearn Reloaded is available at
http://www.youtube.com/watch?v=-k3cOEoI Dk.
6 http://lcl.uniroma1.it/termextractor.
7 TermExtractor has already been described in Sclano and Velardi (2007) and in Navigli and Velardi (2004);
therefore the interested reader is referred to these papers for additional details.
670
Velardi, Faralli, and Navigli OntoLearn Reloaded
Table 1
An excerpt of the terminology extracted for the ARTIFICIAL INTELLIGENCE and FINANCE
domains.
ARTIFICIAL INTELLIGENCE
acyclic graph parallel corpus flow network
adjacency matrix parse tree pattern matching
artificial intelligence partitioned semantic network pagerank
tree data structure pathfinder taxonomic hierarchy
FINANCE
investor shareholder open economy
bid-ask spread profit maximization speculation
long term debt shadow price risk management
optimal financing policy ratings profit margin
synsets. In other words, U contains all the terms in the selected topmost synsets. In
Table 2 we show representative synonyms of the upper-level synsets that we used for
the ARTIFICIAL INTELLIGENCE and FINANCE domains. Seeing that we use high-level
concepts, the set U can be considered domain-independent. Other choices are of course
possible, especially if an upper ontology for a given domain is already available.
For each term t ? T(i) (initially, i = 0), we first check whether t is an upper term (i.e.,
t ? U). If it is, we just skip it (because we do not aim at extending the taxonomy beyond
an upper term). Otherwise, definition sentences are sought for t in the domain corpus
and in a portion of the Web. To do so we use Word-Class Lattices (WCLs) (Navigli and
Velardi 2010, introduced hereafter), which is a domain-independent machine-learned
classifier that identifies definition sentences for the given term t, together with the
corresponding hypernym (i.e., lexical generalization) in each sentence.
For each term in our set T(i), we then automatically extract definition candidates
from the domain corpus, Web documents, and Web glossaries, by harvesting all the
sentences that contain t. To obtain on-line glossaries we use a Web glossary extraction
system (Velardi, Navigli, and D?Amadio 2008). Definitions can also be obtained via a
lightweight bootstrapping process (De Benedictis, Faralli, Navigli 2013).
Finally, we apply WCLs and collect all those sentences that are classified as defini-
tional. We show some terms with their definitions in Table 3 (first and second column,
respectively). The extracted hypernym is shown in italics.
Table 2
The set of upper concepts used in OntoLearn Reloaded for AI and FINANCE (only representative
synonyms from the corresponding WordNet synsets are shown).
ability#n#1 abstraction#n#6 act#n#2 code#n#2
communication#n#2 concept#n#1 data#n#1 device#n#1
discipline#n#1 entity#n#1 event#n#1 expression#n#6
research#n#1 instrumentality#n#1 knowledge#n#1 knowledge domain#n#1
language#n#1 methodology#n#2 model#n#1 organization#n#1
person#n#1 phenomenon#n#1 process#n#1 property#n#2
quality#n#1 quantity#n#1 relation#n#1 representation#n#2
science#n#1 system#n#2 technique#n#1 theory#n#1
671
Computational Linguistics Volume 39, Number 3
Table 3
Some definitions for the ARTIFICIAL INTELLIGENCE domain (defined term in bold, extracted
hypernym in italics).
Term Definition Weight Domain?
adjacency matrix an adjacency matrix is a zero-one matrix 1.00 
flow network in graph theory, a flow network is a directed graph 0.57 
flow network global cash flow network is an online company that
specializes in education and training courses in
teaching the entrepreneurship
0.14 ?
Table 4
Example definitions (defined terms are marked in bold face, their hypernyms in italics).
[In arts, a chiaroscuro]DF [is]VF [a monochrome picture]GF.
[In mathematics, a graph]DF [is]VF [a data structure]GF [that consists of . . . ]REST.
[In computer science, a pixel]DF [is]VF [a dot]GF [that is part of a computer image]REST.
[Myrtales]DF [are an order of]VF [ flowering plants]GF [placed as a basal group . . . ]REST.
3.2.1 Word-Class Lattices. We now describe our WCL algorithm for the classification of
definitional sentences and hypernym extraction. Our model is based on a formal notion
of textual definition. Specifically, we assume a definition contains the following fields
(Storrer and Wellinghoff 2006):
 The DEFINIENDUM field (DF): this part of the definition includes the
definiendum (that is, the word being defined) and its modifiers
(e.g., ?In computer science, a pixel?);
 The DEFINITOR field (VF): which includes the verb phrase used to
introduce the definition (e.g., ?is?);
 The DEFINIENS field (GF): which includes the genus phrase (usually
including the hypernym, e.g., ?a dot?);
 The REST field (RF): which includes additional clauses that further
specify the differentia of the definiendum with respect to its genus
(e.g., ?that is part of a computer image?).
To train our definition extraction algorithm, a data set of textual definitions was
manually annotated with these fields, as shown in Table 4.8 Furthermore, the single-
or multi-word expression denoting the hypernym was also tagged. In Table 4, for each
sentence the definiendum and its hypernym are marked in bold and italics, respectively.
Unlike other work in the literature dealing with definition extraction (Hovy et al 2003;
Fahmi and Bouma 2006; Westerhout 2009; Zhang and Jiang 2009), we covered not only
a variety of definition styles in our training set, in addition to the classic X is a Y pattern,
but also a variety of domains. Therefore, our WCL algorithm requires no re-training
when changing the application domain, as experimentally demonstrated by Navigli and
Velardi (2010). Table 5 shows some non-trivial patterns for the VF field.
8 Available on-line at: http://lcl.uniroma1.it/wcl.
672
Velardi, Faralli, and Navigli OntoLearn Reloaded
Table 5
Some nontrivial patterns for the VF field.
is a term used to describe is a specialized form of
is the genus of was coined to describe
is a term that refers to a kind of is a special class of
can denote is the extension of the concept of
is commonly used to refer to is defined both as
Starting from the training set, the WCL algorithm learns generalized definitional
models as detailed hereafter.
Generalized sentences. First, training and test sentences are part-of-speech tagged with the
TreeTagger system, a part-of-speech tagger available for many languages (Schmid 1995).
The first step in obtaining a definitional pattern is word generalization. Depending on
its frequency we define a word class as either a word itself or its part of speech. Formally,
let T be the set of training sentences. We first determine the set F of words in T whose
frequency is above a threshold ? (e.g., the, a, an, of ). In our training sentences, we replace
the defined term with the token ?TARGET? (note that ?TARGET? ? F).
Given a new sentence s = t1, t2, . . . , tn, where ti is the i-th token of s, we generalize
its words ti to word classes t?i as follows:
t?i =
{
ti if ti ? F
POS(ti) otherwise
that is, a word ti is left unchanged if it occurs frequently in the training corpus (i.e.,
ti ? F); otherwise it is replaced with its part of speech (POS(ti)). As a result we obtain a
generalized sentence s?. For instance, given the first sentence in Table 4, we obtain the
corresponding generalized sentence: ?In NNS, a ?TARGET? is a JJ NN,? where NN and
JJ indicate the noun and adjective classes, respectively. Generalized sentences are dou-
bly beneficial: First, they help reduce the annotation burden, in that many differently
lexicalized sentences can be caught by a single generalized sentence; second, thanks
to their reduction of the definition variability, they allow for a higher-recall definition
model.
Star patterns. Let T again be the set of training sentences. In this step we associate a
star pattern ?(s) with each sentence s ? T . To do so, let s ? T be a sentence such that
s = t1, t2, . . . , tn, where ti is its i-th token. Given the set F of most frequent words in T ,
the star pattern ?(s) associated with s is obtained by replacing with * all the tokens ti 
? F,
that is, all the tokens that are non-frequent words. For instance, given the sentence ?In
arts, a chiaroscuro is a monochrome picture,? the corresponding star pattern is ?In *, a
?TARGET? is a *,? where ?TARGET? is the defined term.
Sentence clustering. We then cluster the sentences in our training set T on the basis of
their star pattern. Formally, let ? = (?1, . . . ,?m) be the set of star patterns associated
with the sentences in T . We create a clustering C = (C1, . . . , Cm) such that Ci = {s ? T :
?(s) = ?i}, that is, Ci contains all the sentences whose star pattern is ?i.
As an example, assume ?3 = ?In *, a ?TARGET? is a *.? The first three sentences
reported in Table 4 are all grouped into cluster C3. We note that each cluster Ci contains
673
Computational Linguistics Volume 39, Number 3
sentences whose degree of variability is generally much lower than for any pair of
sentences in T belonging to two different clusters.
Word-class lattice construction. The final step consists of the construction of a WCL for
each sentence cluster, using the corresponding generalized sentences. Given such a
cluster Ci ? C, we apply a greedy algorithm that iteratively constructs the WCL.
Let Ci = {s1, s2, . . . , s|Ci|} and consider its first sentence s1 = t1, t2, . . . , tn. Initially, we
create a directed graph G = (V, E) such that V = {t1, . . . , tn} and E = {(t1, t2), (t2, t3), . . . ,
(tn?1, tn)}. Next, for each j = 2, . . . , |Ci|, we determine the alignment between sentence sj
and each sentence sk ? Ci such that k < j according to the following dynamic program-
ming formulation (Cormen, Leiserson, and Rivest 1990, pages 314?319):
Ma,b = max {Ma?1,b?1 + Sa,b, Ma,b?1, Ma?1,b}, (1)
where a ? {0, . . . , |sk|} and b ? {0, . . . , |sj|}, Sa,b is a score of the matching between the
a-th token of sk and the b-th token of sj, and M0,0, M0,b and Ma,0 are initially set to 0 for
all values of a and b.
The matching score Sa,b is calculated on the generalized sentences s?k and s
?
j as
follows:
Sa,b =
{
1 if t?k,a = t
?
j,b
0 otherwise
where t?k,a and t
?
j,b are the a-th and b-th tokens of s
?
k and s
?
j , respectively. In other words, the
matching score equals 1 if the a-th and the b-th tokens of the two generalized sentences
have the same word class.
Finally, the alignment score between sk and sj is given by M|sk|,|sj|, which calculates
the minimal number of misalignments between the two token sequences. We repeat this
calculation for each sentence sk (k = 1, . . . , j ? 1) and choose the one that maximizes its
alignment score with sj. We then use the best alignment to add sj to the graph G: We add
to the set of nodes V the tokens of s?j for which there is no alignment to s
?
k and we add to
E the edges (t?1, t
?
2), . . . , (t
?
|sj|?1, t
?
|sj|).
Example. Consider the first three definitions in Table 4. Their star pattern is ?In *,
a ?TARGET? is a *.? The corresponding WCL is built as follows: The first part-
of-speech tagged sentence, ?In/IN arts/NN , a/DT ?TARGET?/NN is/VBZ a/DT
monochrome/JJ picture/NN,? is considered. The corresponding generalized sentence is
?In NN1 , a ?TARGET? is a JJ NN2.? The initially empty graph is thus populated with one
node for each word class and one edge for each pair of consecutive tokens, as shown in
Figure 2a. Note that we use a rectangle to denote the hypernym token NN2 . We also add
to the graph a start node and an end node ?, and connect them to the corresponding
initial and final sentence tokens. Next, the second sentence, ?In mathematics, a graph
is a data structure that consists of...,? is aligned to the first sentence. The alignment
is perfect, apart from the NN3 node corresponding to ?data.? The node is added to
the graph together with the edges ?a?? NN3 and NN3 ? NN2 (Figure 2b, node and
edges in bold). Finally, the third sentence in Table 4, ?In computer science, a pixel is a
dot that is part of a computer image,? is generalized as ?In NN4 NN1 , a ?TARGET?
is a NN2.? Thus, a new node NN4 is added, corresponding to ?computer? and new
674
Velardi, Faralli, and Navigli OntoLearn Reloaded
Figure 2
The Word-Class Lattice construction steps on the first three sentences in Table 4. We show in
bold the nodes and edges added to the lattice graph as a result of each sentence alignment step.
The support of each word class is reported beside the corresponding node.
edges are added that connect node ?In? to NN4 and NN4 to NN1. Figure 2c shows the
resulting lattice.
Variants of the WCL model. So far we have assumed that our WCL model learns lattices
from the training sentences in their entirety (we call this model WCL-1). We also consid-
ered a second model that, given a star pattern, learns three separate WCLs, one for each
of the three main fields of the definition, namely: definiendum (DF), definitor (VF), and
definiens (GF). We refer to this latter model as WCL-3. Note that our model does not
take into account the REST field, so this fragment of the training sentences is discarded.
The reason for introducing the WCL-3 model is that, whereas definitional patterns are
highly variable, DF, VF, and GF individually exhibit a lower variability, thus WCL-3
improves the generalization power.
Once the learning process is over, a set of WCLs is produced. Given a test sentence
s, the classification phase for the WCL-1 model consists of determining whether there
exists a lattice that matches s. In the case of WCL-3, we consider any combination of
definiendum, definitor, and definiens lattices. Given that different combinations might
match, for each combination of three WCLs we calculate a confidence score as follows:
score(s, lDF, lVF, lGF ) = coverage ? log2(support + 1) (2)
where s is the candidate sentence, lDF, lVF, and lGF are three lattices (one for
each definition field), coverage is the fraction of sentence tokens covered by the
675
Computational Linguistics Volume 39, Number 3
third lattice, and support is the total number of sentences in the corresponding star
pattern.
WCL-3 selects, if any, the combination of the three WCLs that best fits the sentence
in terms of coverage and support from the training set. In fact, choosing the most
appropriate combination of lattices impacts the performance of hypernym extraction.
Given its higher performance (Navigli and Velardi 2010), in OntoLearn Reloaded we
use WCL-3 for definition classification and hypernym extraction.
3.3 Domain Filtering and Creation of the Hypernym Graph
The WCLs described in the previous section are used to identify definitional sentences
and harvest hypernyms for the terms obtained as a result of the terminology extraction
phase. In this section we describe how to filter out non-domain definitions and create a
dense hypernym graph for the domain of interest.
Given a term t, the common case is that several definitions are found for it (e.g.,
the flow network example provided at the beginning of this section). Many of these
will not pertain to the domain of interest, however, especially if they are obtained
from the Web or if they define ambiguous terms. For instance, in the COMPUTER
SCIENCE domain, the cash flow definition of flow network shown in Table 3 was not
pertinent. To discard these non-domain sentences, we weight each definition candidate
d(t) according to the domain terms that are contained therein using the following
formula:
DomainWeight(d(t)) =
|Bd(t) ? D|
|Bd(t)|
(3)
where Bd(t) is the bag of content words in the definition candidate d(t) and D is given
by the union of the initial terminology T(0) and the set of single words of the terms in
T(0) that can be found as nouns in WordNet. For example, given T(0) = { greedy algo-
rithm, information retrieval, minimum spanning tree }, our domain terminology D = T(0) ?
{ algorithm, information, retrieval, tree }. According to Equation (3), the domain weight
of a definition is normalized by the total number of content words in the definition, so
as to penalize longer definitions. Domain filtering is performed by keeping only those
definitions d(t) whose DomainWeight(d(t)) ? ?, where ? is an empirically tuned thresh-
old.9 In Table 3 (third column), we show some values calculated for the corresponding
definitions (the fourth column reports a check mark if the domain weight is above
the threshold, an ? otherwise). Domain filtering performs some implicit form of Word
Sense Disambiguation (Navigli 2009), as it aims at discarding senses of hypernyms
which do not pertain to the domain.
Let Ht be the set of hypernyms extracted with WCLs from the definitions of term t
which survived this filtering phase. For each t ? T(i), we add Ht to our graph Gnoisy =
(Vnoisy, Enoisy), that is, we set Vnoisy := Vnoisy ? Ht. For each t, we also add a directed
edge (h, t)10 for each hypernym h ? Ht, that is, we set Enoisy := Enoisy ? {(h, t)}. As a result
9 Empirically set to 0.38, as a result of tuning on several data sets of manually annotated definitions in
different domains.
10 In what follows, (h, t) or h ? t reads ?t is-a h.?
676
Velardi, Faralli, and Navigli OntoLearn Reloaded
of this step, the graph contains our domain terms and their hypernyms obtained from
domain-filtered definitions. We now set:
T(i+1) :=
?
t?T(i)
Ht \
i
?
j=1
T(j) (4)
that is, the new set of terms T(i+1) is given by the hypernyms of the current set of terms
T(i) excluding those terms that were already processed during previous iterations of
the algorithm. Next, we move to iteration i + 1 and repeat the last two steps, namely,
we perform definition/hypernym extraction and domain filtering on T(i+1). As a result
of subsequent iterations, the initially empty graph is increasingly populated with new
nodes (i.e., domain terms) and edges (i.e., hypernymy relations).
After a given number of iterations K, we obtain a dense hypernym graph Gnoisy
that potentially contains more than one connected component. Finally, we connect all
the upper term nodes in Gnoisy to a single top node . As a result of this connecting
step, only one connected component of the noisy hypernym graph?which we call
the backbone component?will contain an upper taxonomy consisting of upper
terms in U.
The resulting graph Gnoisy potentially contains cycles and multiple hypernyms for
the vast majority of nodes. In order to eliminate noise and obtain a full-fledged taxon-
omy, we perform a step of graph pruning, as described in the next section.
3.4 Graph Pruning
At the end of the iterative hypernym harvesting phase, described in Sections 3.2 and 3.3,
the result is a highly dense, potentially disconnected, hypernymy graph (see Section 4
for statistics concerning the experiments that we performed). Wrong nodes and edges
might stem from errors in any of the definition/hypernym extraction and domain filter-
ing steps. Furthermore, for each node, multiple ?good? hypernyms can be harvested.
Rather than using heuristic rules, we devised a novel graph pruning algorithm, based
on the Chu-Liu/Edmonds optimal branching algorithm (Chu and Liu 1965; Edmonds
1967), that exploits the topological graph properties to produce a full-fledged taxonomy.
The algorithm consists of four phases (i.e., graph trimming, edge weighting, optimal
branching, and pruning recovery) that we describe hereafter with the help of the noisy
graph in Figure 3a, whose grey nodes belong to the initial terminology T(0) and whose
bold node is the only upper term.
3.4.1 Graph Trimming. We first perform two trimming steps. First, we disconnect ?false?
roots, i.e., nodes which are not in the set of upper terms and with no incoming edges
(e.g., image in Figure 3a). Second, we disconnect ?false? leaves, namely, leaf nodes which
are not in the initial terminology and with no outgoing edges (e.g., output in Figure 3a).
We show the disconnected components in Figure 3b.
3.4.2 Edge Weighting. Next, we weight the edges in our noisy graph Gnoisy. A policy based
only on graph connectivity (e.g., in-degree or betweenness, see Newman [2010] for a
complete survey) is not sufficient for taxonomy learning.11 Consider again the graph in
11 As also remarked by Kozareva and Hovy (2010), who experimented with in-degree.
677
Computational Linguistics Volume 39, Number 3
Figure 3
A noisy graph excerpt (a), its trimmed version (b), and the final taxonomy resulting from
pruning (c).
Figure 3: In choosing the best hypernym for the term token sequence, a connectivity-based
measure might select collection rather than list, because the former reaches more nodes.
In taxonomy learning, however, longer hypernymy paths should be preferred (e.g., data
structure ? collection ? list ? token sequence is better than data structure ? collection ?
token sequence).
We thus developed a novel weighting policy aimed at finding the best trade-off
between path length and the connectivity of traversed nodes. It consists of three steps:
i) Weight each node v by the number of nodes belonging to the initial
terminology that can be reached from v (potentially including v itself).12
Let w(v) denote the weight of v (e.g., in Figure 3b, node collection reaches
list and token sequence, thus w(collection) = 2, whereas w(graph) = 3).
All weights are shown in the corresponding nodes in Figure 3b.
ii) For each node v, consider all the paths from an upper root r to v.
Let ?(r, v) be the set of such paths. Each path p ? ?(r, v) is weighted
by the cumulative weight of the nodes in the path, namely:
?(p) =
?
v??p
w(v?) (5)
iii) Assign the following weight to each incoming edge (h, v) of v (i.e., h is one
of the direct hypernyms of v):
w(h, v) = max
r?U
max
p??(r,h)
?(p) (6)
This formula assigns to edge (h, v) the value ?(p) of the highest-weighting
path p from h to any upper root ? U. For example, in Figure 3b, w(list) = 2,
w(collection) = 2, w(data structure) = 5. Therefore, the set of paths ?(data
structure, list) = { data structure ? list, data structure ? collection ? list },
whose weights are 7 (w(data structure) + w(list)) and 9 (w(data structure) +
w(collection) + w(list)), respectively. Hence, according to Formula 6, w(list,
token sequence) = 9. We show all edge weights in Figure 3b.
12 Nodes in a cycle are visited only once.
678
Velardi, Faralli, and Navigli OntoLearn Reloaded
3.4.3 Optimal Branching. Next, our goal is to move from a noisy graph to a tree-like
taxonomy on the basis of our edge weighting strategy. A maximum spanning tree
algorithm cannot be applied, however, because our graph is directed. Instead, we need
to find an optimal branching, that is, a rooted tree with an orientation such that every
node but the root has in-degree 1, and whose overall weight is maximum. To this end,
we first apply a pre-processing step: For each (weakly) connected component in the
noisy graph, we consider a number of cases, aimed at identifying a single ?reasonable?
root node to enable the optimal branching to be calculated. Let R be the set of candidate
roots, that is, nodes with no incoming edges. We perform the following steps:
i) If |R| = 1 then we select the only candidate as root.
ii) Else if |R| > 1, if an upper term is in R, we select it as root, else we choose
the root r ? R with the highest weight w according to the weighting
strategy described in Section 3.4.2. We also disconnect all the unselected
roots, that is, those in R \ {r}.
iii) Else (i.e., if |R| = 0), we proceed as for step (ii), but we search candidates
within the entire connected component and select the highest weighting
node. In contrast to step (ii), we remove all the edges incoming to the
selected node.
This procedure guarantees not only the selection but also the existence of a single
root node for each component, from which the optimal branching algorithm can start.
We then apply the Chu-Liu/Edmonds algorithm (Chu and Liu 1965; Edmonds 1967) to
each component Gi = (Vi, Ei) of our directed weighted graph Gnoisy in order to find an
optimal branching. The algorithm consists of two phases: a contraction phase and an
expansion phase. The contraction phase is as follows:
1. For each node which is not a root, we select the entering edge with the
highest weight. Let S be the set of such |Vi| ? 1 edges;
2. If no cycles are formed in S, go to the expansion phase. Otherwise,
continue;
3. Given a cycle in S, contract the nodes in the cycle into a pseudo-node k,
and modify the weight of each edge entering any node v in the cycle from
some node h outside the cycle, according to the following equation:
w(h, k) = w(h, v) + (w(x(v), v) ? minv(w(x(v), v))) (7)
where x(v) is the predecessor of v in the cycle and w(x(v), v) is the weight
of the edge in the cycle which enters v;
4. Select the edge entering the cycle which has the highest modified weight
and replace the edge which enters the same real node in S by the new
selected edge;
5. Go to step 2 with the contracted graph.
The expansion phase is applied if pseudo-nodes have been created during step 3.
Otherwise, this phase is skipped and Ti = (Vi, S) is the optimal branching of component
679
Computational Linguistics Volume 39, Number 3
Gi (i.e., the i-th component of Gnoisy). During the expansion phase, pseudo-nodes are
replaced with the original cycles. To break the cycle, we select the real node v into which
the edge selected in step 4 enters, and remove the edge entering v belonging to the
cycle. Finally, the weights on the edges are restored. For example, consider the cycle
in Figure 4a. Nodes pagerank, map, and rank are contracted into a pseudo-node, and
the edges entering the cycle from outside are re-weighted according to Equation (7).
According to the modified weights (Figure 4b), the selected edge, that is, (table, map),
is the one with weight w = 13. During the expansion phase, the edge (pagerank, map) is
eliminated, thus breaking the cycle (Figure 4c).
The tree-like taxonomy resulting from the application of the Chu-Liu/Edmonds
algorithm to our example in Figure 3b is shown in Figure 3c.
3.4.4 Pruning Recovery. The weighted directed graph Gnoisy input to the Chu-Liu/
Edmonds algorithm might contain many (weakly) connected components. In this case,
an optimal branching is found for each component, resulting in a forest of taxonomy
trees. Although some of these components are actually noisy, others provide an impor-
tant contribution to the final tree-like taxonomy. The objective of this phase is to recover
from excessive pruning, and re-attach some of the components that were disconnected
during the optimal branching step. Recall from Section 3.3 that, by construction, we
have only one backbone component, that is, a component which includes an upper tax-
onomy. Our aim is thus to re-attach meaningful components to the backbone taxonomy.
To this end, we apply Algorithm 1. The algorithm iteratively merges non-backbone trees
to the backbone taxonomy tree T0 in three main steps:
 Semantic reconnection step (lines 7?9 in Algorithm 1): In this step we
reuse a previously removed ?noisy? edge, if one is available, to reattach a
non-backbone component to the backbone. Given a root node rTi of a
non-backbone tree Ti (i > 0), if an edge (v, rTi ) existed in the noisy graph
Gnoisy (i.e., the one obtained before the optimal branching phase), with
v ? T0, then we connect the entire tree Ti to T0 by means of this edge.
Figure 4
A graph excerpt containing a cycle (a); Edmonds? contraction phase: a pseudo-node enclosing
the cycle with updated weights on incoming edges (b); and Edmonds? expansion phase: the
cycle is broken and weights are restored (c).
680
Velardi, Faralli, and Navigli OntoLearn Reloaded
Algorithm 1 PruningRecovery(G, Gnoisy)
Require: G is a forest
1: repeat
2: Let F := {T0, T1, . . . , T|F|} be the forest of trees in G = (V, E)
3: Let T0 ? F be the backbone taxonomy
4: E? ? E
5: for all T in F \ {T0} do
6: rT ? rootOf (T)
7: if ?v ? T0 s.t. (v, rT ) ? Gnoisy then
8: E ? E ? {(v, rT )}
9: break
10: else
11: if out-degree(rT ) = 0 then
12: if ?v ? T0 s.t. v is the longest right substring of rT then
13: E := E ? {(v, rT )}
14: break
15: else
16: E ? E \ {(rT, v) : v ? V}
17: break
18: until E = E?
 Reconnection step by lexical inclusion (lines 11?14): Otherwise, if Ti is a
singleton (the out-degree of rTi is 0) and there exists a node v ? T0 such
that v is the longest right substring of rTi by lexical inclusion,
13 we connect
Ti to the backbone tree T0 by means of the edge (v, rTi ).
 Decomposition step (lines 15?17): Otherwise, if the component Ti is not a
singleton (i.e., if the out-degree of the root node rTi is > 0) we disconnect
rTi from Ti. At first glance, it might seem counterintuitive to remove edges
during pruning recovery. Reconnecting by lexical inclusion within a
domain has already been shown to perform well in the literature (Vossen
2001; Navigli and Velardi 2004), but we want to prevent any cascading
errors on the descendants of the root node, and at the same time free up
other pre-existing ?noisy? edges incident to the descendants.
These three steps are iterated on the newly created components, until no change
is made to the graph (line 18). As a result of our pruning recovery phase we return the
enriched backbone taxonomy. We show in Figure 5 an example of pruning recovery that
starts from a forest of three components (including the backbone taxonomy tree on top,
Figure 5a). The application of the algorithm leads to the disconnection of a tree root,
that is, ordered structure (Figure 5a, lines 15?17 of Algorithm 1), the linking of the trees
rooted at token list and binary search tree to nodes in the backbone taxonomy (Figures 5b
and 5d, lines 7?9), and the linking of balanced binary tree to binary tree thanks to lexical
inclusion (Figure 5c, lines 11?14 of the algorithm).
13 Similarly to our original OntoLearn approach (Navigli and Velardi 2004), we define a node?s string
v = wnwn?1 . . .w2w1 to be lexically included in that of a node v? = w?mw
?
m?1 . . .w
?
2w
?
1 if m > n and
wj = w?j for each j ? {1, . . . , n}.
681
Computational Linguistics Volume 39, Number 3
Figure 5
An example starting with three components, including the backbone taxonomy tree on the
top and two other trees on the bottom (a). As a result of pruning recovery, we disconnect ordered
structure (a); we connect token sequence to token list by means of a ?noisy? edge (b); we connect
binary tree to balanced binary tree by lexical inclusion (c); and finally binary tree to binary search
tree by means of another ?noisy? edge (d).
3.5 Edge Recovery
The goal of the last phase was to recover from the excessive pruning of the optimal
branching phase. Another issue of optimal branching is that we obtain a tree-like tax-
onomic structure, namely, one in which each node has only one hypernym. This is not
fully appropriate in taxonomy learning, because systematic ambiguity and polysemy
often require a concept to be paradigmatically related to more than one hypernym. In
fact, a more appropriate structure for a conceptual hierarchy is a DAG, as in WordNet.
For example, two equally valid hypernyms for backpropagation are gradient descent search
682
Velardi, Faralli, and Navigli OntoLearn Reloaded
procedure and training algorithm, so two hypernym edges should correctly be incident to
the backpropagation node.
We start from our backbone taxonomy T0 obtained after the pruning recovery
phase described in Section 3.4.4. In order to obtain a DAG-like taxonomy we apply
the following step: for each ?noisy? edge (v, v?) ? Enoisy such that v, v? are nodes in T0
but the edge (v, v?) does not belong to the tree, we add (v, v?) to T0 if:
i) it does not create a cycle in T0;
ii) the absolute difference between the length of the shortest path from v to
the root rT0 and that of the shortest path from v
? to rT0 is within an interval
[m, M]. The aim of this constraint is to maintain a balance between the
height of a concept in the tree-like taxonomy and that of the hypernym
considered for addition. In other words, we want to avoid the connection
of an overly abstract concept with an overly specific one.
In Section 4, we experiment with three versions of our OntoLearn Reloaded algo-
rithm, namely: one version that does not perform edge recovery (i.e., which learns a
tree-like taxonomy [TREE], and two versions that apply edge recovery (i.e., which learn
a DAG) with different intervals for constraint (ii) above (DAG[1, 3] and DAG[0, 99]; note
that the latter version virtually removes constraint (ii)). Examples of recovered edges
will be presented and discussed in the evaluation section.
3.6 Complexity
We now perform a complexity analysis of the main steps of OntoLearn Reloaded. Given
the large number of steps and variables involved we provide a separate discussion of
the main costs for each individual step, and we omit details about commonly used data
structures for access and storage, unless otherwise specified. Let Gnoisy = (Vnoisy, Enoisy)
be our noisy graph, and let n = |Vnoisy| and m = |Enoisy|.
1. Terminology extraction: Assuming a part-of-speech tagged corpus as
input, the cost of extracting candidate terms by scanning the corpus with a
maximum-size window is in the order of the word size of the input
corpus. Thus, the application of statistical measures to our set of candidate
terms has a computational cost that is on the order of the square of the
number of term candidates (i.e., the cost of calculating statistics for each
pair of terms).
2. Definition and hypernym extraction: In the second step, we first retrieve
candidate definitions from the input corpus, which costs on the order of
the corpus size.14 Each application of a WCL classifier to an input
candidate sentence s containing a term t costs on the order of the word
length of the sentence, and we have a constant number of such classifiers.
So the cost of this step is given by the sum of the lengths of the candidate
sentences in the corpus, which is lower than the word size of the corpus.
14 Note that this corpus consists of both free text and Web glossaries (cf. Section 3.2).
683
Computational Linguistics Volume 39, Number 3
3. Domain filtering and creation of the graph: The cost of domain filtering
for a single definition is in the order of its word length, so the running time
of domain filtering is in the order of the sum of the word size of the
acquired definitions. As for the hypernym graph creation, using an
adjacency-list representation of the graph Gnoisy, the dynamic addition of a
newly acquired hypernymy edge costs O(n), an operation which has to be
repeated for each (hypernymy, term) pair.
4. Graph pruning, consisting of the following steps:
 Graph trimming: This step requires O(n) time in order to identify
false leaves and false roots by iterating over the entire set of nodes.
 Edge weighting: i) We perform a DFS (O(n + m)) to weight all the
nodes in the graph; ii) we collect all paths from upper roots to any
given node, totalizing O(n!) paths in the worst case (i.e., in a
complete graph). In real domains, however, the computational cost
of this step will be much lower. In fact, over our six domains, the
average number of paths per node ranges from 4.3 (n = 2107,
ANIMALS) to 3175.1 (n = 2616, FINANCE domain): In the latter,
worst case, in practice, the number of paths is in the order of n, thus
the cost of this step, performed for each node, can be estimated by
O(n2) running time; iii) assigning maximum weights to edges costs
O(m) if in the previous step we keep track of the maximum value
of paths ending in each node h (see Equation (6)).
 Optimal branching: Identifying the connected components of our
graph costs O(n + m) time, identifying root candidates and
selecting one root per component costs O(n), and finally applying
the Chu-Liu/Edmonds algorithm costs O(m ? log2n) for sparse
graphs, O(n2) for dense ones, using Tarjan?s implementation
(Tarjan 1977).
5. Pruning recovery: In the worst case, m iterations of Algorithm 1 will be
performed, each costing O(n) time, thus having a total worst-case cost of
O(mn).
6. Edge recovery: For each pair of nodes in T0 we perform i) the
identification of cycles (O(n + m)) and ii) the calculation of the shortest
paths to the root (O(n + m)). By precomputing the shortest path for each
node, the cost of this step is O(n(n + m)) time.
Therefore, in practice, the computational complexity of OntoLearn Reloaded is
polynomial in the main variables of the problem, namely, the number of words in the
corpus and nodes in the noisy graph.
4. Evaluation
Ontology evaluation is a hard task that is difficult even for humans, mainly because
there is no unique way of modeling the domain of interest. Indeed several different
taxonomies might model a particular domain of interest equally well. Despite this
difficulty, various evaluation methods have been proposed in the literature for assessing
684
Velardi, Faralli, and Navigli OntoLearn Reloaded
the quality of a taxonomy. These include Brank, Mladenic, and Grobelnik (2006) and
Maedche, Pekar, and Staab (2002):
a) automatic evaluation against a gold standard;
b) manual evaluation performed by domain experts;
c) structural evaluation of the taxonomy;
d) application-driven evaluation, in which a taxonomy is assessed on the
basis of the improvement its use generates within an application.
Other quality indicators have been analyzed in the literature, such as accuracy,
completeness, consistency (Vo?lker et al 2008), and more theoretical features (Guarino
and Welty 2002) like essentiality, rigidity, and unity. Methods (a) and (b) are by far the
most popular ones. In this section, we will discuss in some detail the pros and cons of
these two approaches.
Gold standard evaluation. The most popular approach for the evaluation of lexicalized
taxonomies (adopted, e.g., in Snow, Jurafsky, and Ng 2006; Yang and Callan 2009;
and Kozareva and Hovy 2010) is to attempt to reconstruct an existing gold standard
(Maedche, Pekar, and Staab 2002), such as WordNet or the Open Directory Project.
This method is applicable when the set of taxonomy concepts are given, and the
evaluation task is restricted to measuring the ability to reproduce hypernymy links
between concept pairs. The evaluation is far more complex when learning a specialized
taxonomy entirely from scratch, that is, when both terms and relations are unknown.
In reference taxonomies, even in the same domain, the granularity and cotopy15 of an
abstract concept might vary according to the scope of the taxonomy and the expertise
of the team who created it (Maedche, Pekar, and Staab 2002). For example, both the
terms chiaroscuro and collage are classified under picture, image, icon in WordNet, but in
the Art & Architecture Thesaurus (AA&T)16 chiaroscuro is categorized under perspective
and shading techniques whereas collage is classified under image-making processes and
techniques. As long as common-sense, non-specialist knowledge is considered, it is still
feasible for an automated system to replicate an existing classification, because the
Web will provide abundant evidence for it. For example, Kozareva and Hovy (2010,
K&H hereafter) are very successful at reproducing the WordNet sub-taxonomy for
ANIMALS, because dozens of definitional patterns are found on the Web that classify,
for example, lion as a carnivorous feline mammal, or carnivorous, or feline. As we show
later in this section, however, and as also suggested by the previous AA&T example,
finding hypernymy patterns in more specialized domains is far more complex. Even in
simpler domains, however, it is not clear how to evaluate the concepts and relations not
found in the reference taxonomy. Concerning this issue, Zornitsa Kozareva comments
that: ?When we gave sets of terms to annotators and asked them to produce a taxonomy,
people struggled with the domain terminology and produced quite messy organization.
Therefore, we decided to go with WordNet and use it as a gold truth? (personal
communication). Accordingly, K&H do not provide an evaluation of the nodes and
relations other than those for which the ground truth is known. This is further clarified
in a personal communication: ?Currently we do not have a full list of all is-a outside
15 The cotopy of a concept is the set of its hypernyms and hyponyms.
16 http://www.getty.edu/vow/AATHierarchy.
685
Computational Linguistics Volume 39, Number 3
WordNet. [...] In the experiments, we work only with the terms present in WordNet
[...] The evaluation is based only on the WordNet relations. However, the harvesting
algorithm extracts much more. Currently, we do not know how to evaluate the Web
taxonomization.?
To conclude, gold standard evaluation has some evident drawbacks:
 When both concepts and relations are unknown, it is almost impossible to
replicate a reference taxonomy accurately.
 In principle, concepts not in the reference taxonomy can be either wrong
or correct; therefore the evaluation is in any case incomplete.
Another issue in gold standard evaluation is the definition of an adequate evalu-
ation metric. The most common measure used in the literature to compare a learned
with a gold-standard taxonomy is the overlapping factor (Maedche, Pekar, and Staab
2002). Given the set of is-a relations in the two taxonomies, the overlapping factor
simply computes the ratio between the intersection and union of these sets. Therefore
the overlapping factor gives a useful global measure of the similarity between the
two taxonomies. It provides no structural comparison, however: Errors or differences
in grouping concepts in progressively more general classes are not evidenced by this
measure.
Comparison against a gold standard has been analyzed in a more systematic way
by Zavitsanos, Paliouras, and Vouros (2011) and Brank, Mladenic, and Grobelnik (2006).
They propose two different strategies for escaping the ?naming? problem that we have
outlined. Zavitsanos, Paliouras, and Vouros (2011) propose transforming the ontology
concepts and their properties into distributions over the term space of the source data
from which the ontology has been learned. These distributions are used to compute
pairwise concept similarity between gold standard and learned ontologies.
Brank, Mladenic, and Grobelnik (2006) exploit the analogy between ontology learn-
ing and unsupervised clustering, and propose OntoRand, a modified version of the
Rand Index (Rand 1971) for computing the similarity between ontologies. Morey and
Agresti (1984) and Carpineto and Romano (2012), however, demonstrated a high de-
pendency of the Rand Index (and consequently of OntoRand itself) upon the number of
clusters, and Fowlkes and Mallows (1983) show that the Rand Index has the undesirable
property of converging to 1 as the number of clusters increases, even in the unrealistic
case of independent clusterings. These undesired outcomes have also been experienced
by Brank, Mladenic, and Grobelnik (2006, page 5), who note that ?the similarity of an
ontology to the original one is still as high as 0.74 even if only the top three levels of
the ontology have been kept.? Another problem with the OntoRand formula, as also
remarked in Zavitsanos, Paliouras, and Vouros (2011), is the requirement of comparing
ontologies with the same set of instances.
Manual evaluation. Comparison against a gold standard is important because it repre-
sents a sort of objective evaluation of an automated taxonomy learning method. As
we have already remarked, however, learning an existing taxonomy is not particularly
interesting in itself. Taxonomies are mostly needed in novel, often highly technical do-
mains for which there are no gold standards. For a system to claim to be able to acquire
a taxonomy from the ground up, manual evaluation seems indispensable. Nevertheless,
none of the taxonomy learning systems surveyed in Section 2 performs such evaluation.
Furthermore, manual evaluation should not be limited to an assessment of the acquired
686
Velardi, Faralli, and Navigli OntoLearn Reloaded
hypernymy relations ?in isolation,? but must also provide a structural assessment
aimed at identifying common phenomena and the overall quality of the taxonomic
structure. Unfortunately, as already pointed out, manual evaluation is a hard task.
Deciding whether or not a concept belongs to a given domain is more or less feasible
for a domain expert, but assessing the quality of a hypernymy link is far more complex.
On the other hand, asking a team of experts to blindly reconstruct a hierarchy, given a
set of terms, may result in the ?messy organization? reported by Zornitsa Kozareva. In
contrast to previous approaches to taxonomy induction, OntoLearn Reloaded provides
a natural solution to this problem, because is-a links in the taxonomy are supported by
one or more definition sentences from which the hypernymy relation was extracted. As
shown later in this section, definitions proved to be a very helpful feature in supporting
manual analysis, both for hypernym evaluation and structural assessment.
The rest of this section is organized as follows. We first describe the experimen-
tal set-up (Section 4.1): OntoLearn Reloaded is applied to the task of acquiring six
taxonomies, four of which attempt to replicate already existing gold standard sub-
hierarchies in WordNet17 and in the MeSH medical ontology,18 and the other two are
new taxonomies acquired from scratch. Next, we present a large-scale multi-faceted
evaluation of OntoLearn Reloaded focused on three of the previously described eval-
uation methods, namely: comparison against a gold standard, manual evaluation, and
structural evaluation. In Section 4.2 we introduce a novel measure for comparing an
induced taxonomy against a gold standard one. Finally, Section 4.3 is dedicated to a
manual evaluation of the six taxonomies.
4.1 Experimental Set-up
We now provide details on the set-up of our experiments.
4.1.1 Domains. We applied OntoLearn Reloaded to the task of acquiring six taxonomies:
ANIMALS, VEHICLES, PLANTS, VIRUSES, ARTIFICIAL INTELLIGENCE, and FINANCE.
The first four taxonomies were used for comparison against three WordNet sub-
hierarchies and the viruses sub-hierarchy of MeSH. The ANIMALS, VEHICLES, and
PLANTS domains were selected to allow for comparison with K&H, who experimented
on the same domains. The ARTIFICIAL INTELLIGENCE and FINANCE domains are ex-
amples of taxonomies truly built from the ground up, for which we provide a thorough
manual evaluation. These domains were selected because they are large, interdisci-
plinary, and continuously evolving fields, thus representing complex and specialized
use cases.
4.1.2 Definition Harvesting. For each domain, definitions were sought in Wikipedia and
in Web glossaries automatically obtained by means of a Web glossary extraction system
(Velardi, Navigli, and D?Amadio 2008). For the ARTIFICIAL INTELLIGENCE domain we
also used a collection consisting of the entire IJCAI proceedings from 1969 to 2011 and
the ACL archive from 1979 to 2010. In what follows we refer to this collection as the ?AI
corpus.? For FINANCE we used a combined corpus from the freely available collection
of Journal of Financial Economics from 1995 to 2012 and from Review Of Finance from 1997
to 2012 for a total of 1,575 papers.
17 http://wordnet.princeton.edu.
18 http://www.nlm.nih.gov/mesh/.
687
Computational Linguistics Volume 39, Number 3
4.1.3 Terminology. For the ANIMALS, VEHICLES, PLANTS, and VIRUSES domains, the
initial terminology was a fragment of the nodes of the reference taxonomies,19 sim-
ilarly to, and to provide a fair comparison with, K&H. For the AI domain instead,
the initial terminology was selected using our TermExtractor tool20 on the AI corpus.
TermExtractor extracted over 5,000 terms from the AI corpus, ranked according to a
combination of relevance indicators related to the (direct) document frequency, domain
pertinence, lexical cohesion, and other indicators (Sclano and Velardi 2007). We manu-
ally selected 2,218 terms from the initial set, with the aim of eliminating compounds
like order of magnitude, empirical study, international journal, that are frequent but not
domain relevant. For similar reasons a manual selection of terms was also applied to the
terminology automatically extracted for the FINANCE domain, obtaining 2,348 terms21
from those extracted by TermExtractor. An excerpt of extracted terms was provided in
Table 1.
4.1.4 Upper Terms. Concerning the selection of upper terms U (cf. Section 3.2), again
similarly to K&H, we used just one concept for each of the four domains focused
upon: ANIMALS, VEHICLES, PLANTS, and VIRUSES. For the AI and FINANCE domains,
which are more general and complex, we selected from WordNet a core taxonomy of
32 upper concepts U (resulting in 52 terms) that we used as a stopping criterion for
our iterative definition/hypernym extraction and filtering procedure (cf. Section 3.2).
The complete list of upper concepts was given in Table 2. WordNet upper concepts are
general enough to fit most domains, and in fact we used the same set U for AI and
FINANCE. Nothing, however, would have prevented us from using a domain-specific
core ontology, such as the CRM-CIDOC core ontology for the domain of ART AND
ARCHITECTURE.22
4.1.5 Algorithm Versions and Structural Statistics. For each of the six domains we ran the
three versions of our algorithm: without pruning recovery (TREE), with [1, 3] recovery
(DAG[1, 3]), and with [0, 99] recovery (DAG[0, 99]), for a total of 18 experiments. We
remind the reader that the purpose of the recovery process was to reattach some of the
edges deleted during the optimal branching step (cf. Section 3.5).
Figure 6 shows an excerpt of the AI tree-like taxonomy under the node data structure.
Notice that, even though the taxonomy looks good overall, there are still a few errors,
such as ?neuron is a neural network? and overspecializations like ?network is a digraph.?
Figure 7 shows a sub-hierarchy of the FINANCE tree-like taxonomy under the concept
value.
In Table 6 we give the structural details of the 18 taxonomies extracted for our six
domains. In the table, edge and node compression refers to the number of surviving
nodes and edges after the application of optimal branching and recovery steps to the
noisy hypernymy graph. To clarify the table, consider the case of VIRUSES, DAG[1, 3]:
we started with 281 initial terms, obtaining a noisy graph with 1,174 nodes and 1,859
edges. These were reduced to 297 nodes (i.e., 1,174?877) and 339 edges (i.e., 1,859?1,520)
after pruning and recovery. Out of the 297 surviving nodes, 222 belonged to the initial
19 For ANIMALS, VEHICLES, and PLANTS we used precisely the same seeds as K&H.
20 http://lcl.uniroma1.it/termextractor.
21 These dimensions are quite reasonable for large technical domains: as an example, The Economist?s
glossary of economic terms includes on the order of 500 terms (http://www.economist.com/
economics-a-to-z/).
22 http://cidoc.mediahost.org/standard crm(en)(E1).xml.
688
Velardi, Faralli, and Navigli OntoLearn Reloaded
Figure 6
An excerpt of the ARTIFICIAL INTELLIGENCE taxonomy.
terminology; therefore the coverage over the initial terms is 0.79 (222/281). This means
that, for some of the initial terms, either no definitions were found, or the definition
was rejected in some of the processing steps. The table also shows, as expected, that the
term coverage is much higher for ?common-sense? domains like ANIMALS, VEHICLES,
and PLANTS, is still over 0.75 for VIRUSES and AI, and is a bit lower for FINANCE
(0.65). The maximum and average depth of the taxonomies appears to be quite variable,
with VIRUSES and FINANCE at the two extremes. Finally, Table 6 reports in the last
column the number of glosses (i.e., domain definitional sentences) obtained in each
run. We would like to point out that providing textual glosses for the retrieved domain
hypernyms is a novel feature that has been lacking in all previous approaches to
ontology learning, and which can also provide key support to much-needed manual
validation and enrichment of existing semantic networks (Navigli and Ponzetto 2012).
4.2 Evaluation Against a Gold Standard
In this section we propose a novel, general measure for the evaluation of a learned
taxonomy against a gold standard. We borrow the Brank, Mladenic, and Grobelnik
689
Computational Linguistics Volume 39, Number 3
Figure 7
An excerpt of the FINANCE taxonomy.
(2006) idea of exploiting the analogy with unsupervised clustering but, rather than
representing the two taxonomies as flat clusterings, we propose a measure that takes
into account the hierarchical structure of the two analyzed taxonomies. Under this
perspective, a taxonomy can be transformed into a hierarchical clustering by replacing
each label of a non-leaf node (e.g., perspective and shading techniques) with the transitive
closure of its hyponyms (e.g., cangiatismo, chiaroscuro, foreshortening, hatching).
4.2.1 Evaluation Model. Techniques for comparing clustering results have been surveyed
in Wagner and Wagner (2007), although the only method for comparing hierarchical
clusters, to the best of our knowledge, is that proposed by Fowlkes and Mallows (1983).
Suppose that we have two hierarchical clusterings H1 and H2, with an identical set of n
objects. Let k be the maximum depth of both H1 and H2, and Hij a cut of the hierarchy,
where i ? {0, . . . , k} is the cut level and j ? {1, 2} selects the clustering of interest. Then,
for each cut i, the two hierarchies can be seen as two flat clusterings Ci1 and C
i
2 of the n
concepts. When i = 0 the cut is a single cluster incorporating all the objects, and when
i = k we obtain n singleton clusters. Now let:
 n11 be the number of object pairs that are in the same cluster in both Ci1
and Ci2;
 n00 be the number of object pairs that are in different clusters in both Ci1
and Ci2;
 n10 be the number of object pairs that are in the same cluster in Ci1 but
not in Ci2;
690
Velardi, Faralli, and Navigli OntoLearn Reloaded
Table 6
Structural evaluation of three versions of our taxonomy-learning algorithm on six different
domains.
Experiment Term Coverage Depth |V| |E| V Compress. E Compress. Glosses
A
I
TREE 75.51% 12 max 2,387 2,386 43.00% 67.31% 1,249(1,675/2,218) 6.00 avg (1,801/4,188) (4,915/7,301)
DAG [1,3] 75.51% 19 max 2,387 3,554 43.00% 51.32% 2,081(1,675/2,218) 8.27 avg (1,801/4,188) (3,747/7,301)
DAG [0,99] 75.51% 20 max 2,387 3,994 43.00% 45.29% 2,439(1,675/2,218) 8.74 avg (1,801/4,188) (3,307/7,301)
FI
N
A
N
C
E
TREE 65.20% 14 max 2,038 2,037 22.09% 47.99% 1,064(1,533/2,348) 6.83 avg (578/2,616) (1,880/3,917)
DAG [1,3] 65.20% 38 max 2,038 2,524 22.09% 35.56% 1,523(1,533/2,348) 18.82 avg (578/2,616) (1,393/3,917)
DAG [0,99] 65.20% 65 max 2,038 2,690 22.09% 31.32% 1,677(1,533/2,348) 33.54 avg (578/2,616) (1,227/3,917)
V
IR
U
SE
S
TREE 79.00% 5 max 297 296 74.70% 84.07% 172(222/281) 2.13 avg (877/1,174) (1,563/1,859)
DAG [1,3] 79.00% 5 max 297 339 74.70% 81.76% 212(222/281) 2.20 avg (877/1,174) (1,520/1,859)
DAG [0,99] 79.00% 5 max 297 360 74.70% 80.63% 233(222/281) 2.32 avg (877/1,174) (1,563/1,859)
A
N
IM
A
L
S
TREE 93.56% 10 max 900 899 57.28% 66.96% 724(640/684) 4.35 avg (1,207/2,107) (1,822/2,721)
DAG [1,3] 93.56% 16 max 900 1,049 57.28% 61.44% 872(640/684) 5.21 avg (1,207/2,107) (1,672/2,721)
DAG [0,99] 93.56% 16 max 900 1,116 57.28% 58.98% 939(640/684) 5.39 avg (1,207/2,107) (1,605/2,721)
P
L
A
N
T
S
TREE 96.57% 19 max 710 709 72.69% 84.53% 638(535/554) 5.85 avg (1,890/2,600) (3,877/4,586)
DAG [1,3] 96.57% 19 max 710 922 72.69% 79.89% 851(535/554) 6.65 avg (1,890/2,600) (3,664/4,586)
DAG [0,99] 96.57% 19 max 710 1,242 72.69% 72.91% 1,171(535/554) 6.54 avg (1,890/2,600) (3,344/4,586)
V
E
H
IC
L
E
S
TREE 95.72% 8 max 169 168 71.50% 80.48% 150(112/117) 3.44 avg (424/593) (693/861)
DAG [1,3] 95.72% 8 max 169 200 71.50% 76.77% 182(112/117) 3.94 avg (424/593) (661/861)
DAG [0,99] 95.72% 10 max 169 231 71.50% 73.17% 213(112/117) 4.48 avg (424/593) (630/861)
 n01 be the number of object pairs that are in the same cluster in Ci2 but not
in Ci1;
The generalized Fowlkes and Mallows (F&M) measure of cluster similarity for the
cut i (i ? {0, . . . , k}), as reformulated by Wagner and Wagner (2007), is defined as:
Bi1,2 =
ni11
?
(ni11 + n
i
10) ? (ni11 + ni01)
. (8)
Note that the formula can be interpreted as the geometric mean of precision and
recall of an automated method in clustering the same concept pairs as in a gold-standard
691
Computational Linguistics Volume 39, Number 3
clustering. This formula has a few undesirable properties: first, the value of Bi1,2 gets
close to its maximum 1.0 as we approach the root of the hierarchy (i = 0); second, the
two hierarchies need to have the same maximum depth k; third, the hierarchies need to
have the same number of initial objects and a crisp classification.
In order to apply the F&M measure to the task of comparing a learned and a gold-
standard taxonomy, we need to mitigate these problems. Equation (8) copes with the
third problem without modifications. In fact, if the sets of objects in H1 and H2 are
different, the integers n10 and n01 can be considered as also including objects that belong
to one hierarchy and not to the other. In this case, the value of B01,2 will provide a measure
of the overlapping objects in the learned taxonomy and the gold standard one. In order
to take into account multiple (rather than crisp) classifications, again, there is no need
to change the formula, which is still meaningful if an object is allowed to belong to
more than one cluster. As before, mismatches between H1 and H2 would result in higher
values of n10 and n01 and lower Bi1,2.
A more serious problem with Equation (8) is that the lower the value of i, the higher
the value of the formula, whereas, ideally, we would like to reward similar clusterings
when the clustering task is more difficult and fine-grained, that is, for cuts that are close
to the leaf nodes. To assign a reward to ?early? similarity values, we weight the values
of Bi1,2 with a coefficient
i+1
k . We can then compute a cumulative measure of similarity
with the following formula:
B1,2 =
?k?1
i=0
i+1
k B
i
1,2
?k?1
i=0
i+1
k
=
?k?1
i=0
i+1
k B
i
1,2
k+1
2
. (9)
Finally, to solve the problem of different depths of the two hierarchies, we define a
policy that penalizes a learned taxonomy that is less structured than the gold standard
one, and rewards?or at least does not penalize?the opposite case.
As an example, consider Figure 8, which shows two taxonomies H1 and H2, with
non-identical sets of objects {a, b, c, d, e, f} and {a, b, c, d, e, g}. In the figure each edge is
labeled by its distance from the root node (the value i in the F&M formula). Notice that
H1 and H2 have multiple classifications (i.e., multiple hypernyms in our case) for the
object e, thus modeling the common problem of lexical ambiguity and polysemy. Let
us suppose that H1 is the learned taxonomy, and H2 the gold standard one. We start
comparing the clusterings at cut 0 and stop at cut kr ? 1, where kr is the depth of the
Figure 8
Two hierarchical clusters of n non-identical objects.
692
Velardi, Faralli, and Navigli OntoLearn Reloaded
gold standard taxonomy. This means that if the learned taxonomy is less structured
we replicate the cut kl ? 1 for kr ? kl times (where kl is the maximum depth of the
learned taxonomy), whereas if it is more structured we stop at cut kr ? 1. In contrast to
previous evaluation models, our aim is to reward (instead of penalize) more structured
taxonomies provided they still match the gold standard one.
Table 7 shows the cuts from 0 to 3 of H1 and H2 and the values of Bi1,2. For i = 2 the
B value is 0, if H2 is the learned taxonomy, and is not defined, if H2 is the gold standard.
Therefore, when computing the cumulative Equation (9), we obtain the desired effect of
penalizing less the structured learned taxonomies. Note that, when the two hierarchies
have different depths, the value k ? 1 in Equation (9) is replaced by kr ? 1.
Finally, we briefly compare our evaluation approach with the OntoRand index,
introduced by Brank, Mladenic, and Grobelnik (2006). The Rand Index measures the
similarity between two clusterings Cl and Cr by the formula:
R(Cl, Cr) =
2(n11 + n00)
n(n ? 1) (10)
where n11, n00, and n have the same meaning described earlier. In Brank, Mladenic,
and Grobelnik (2006), a clustering is obtained from an ontology by associating each
ontology instance to its concept. The set of clusters is hence represented by the set of
leaf concepts in the hierarchy, namely, according to our notation, the clustering Ck?1i . In
order to take into account the hierarchical structure, they define the OntoRand formula.
This measure, rather than summing up to 1 or 0, depending on whether or not two
given instances i and j belong to the same cluster in the compared ontologies, returns a
real number in [0, 1] depending upon the distance between i and j in terms of common
ancestors. In other terms, if i and j do not belong to the same concept but have a very
close common ancestor, the OntoRand measure returns a value still close to 1.
Our measure has several advantages over the OntoRand index:
i) It allows for a comparison at different levels of depth of the hierarchy,
and the cumulative similarity measure penalizes the contribution of the
highest cuts of the hierarchy.
ii) It does not require that the two hierarchies have the same depth, nor that
they have the same number of leaf nodes.
iii) The measure can be extended to lattices (e.g., it is not required that each
object belongs precisely to one cluster).
Table 7
Application of the evaluation method to the hierarchies of Figure 8. The values of Bi1,2 are shown
both when H1 and H2 are the learned taxonomy (penultimate and last column, respectively).
i C1 C2 n11 n10 n01 H1 H2
Bi1,2
0 {a,b,c,d,e,f} {a,b,c,d,e,g} 10 5 5 0.67 0.67
1 {a,b,c,d,e},{e,f} {a,b,c,d,e},{e},{g} 10 1 0 0.95 0.95
2 {a,b},{c,d},{e},{f} {a},{b},{c},{d},{e},{g} 0 2 0 n.a. 0
3 {a},{b},{c},{d},{e},{f} {a},{b},{c},{d},{e},{g} 0 0 0 n.a. n.a.
693
Computational Linguistics Volume 39, Number 3
iv) It is not dependent, as the Rand Index is, on the number n00, the value of
which has the undesirable effect of producing an ever higher similarity as
the number of singleton clusters grows (Morey and Agresti 1984).
4.2.2 Results. This section presents the results of the F&M evaluation model for gold
standard evaluation, therefore we focus on four domains and do not consider AI and
FINANCE. The three WordNet sub-hierarchies are also compared with the taxonomies
automatically created by Kozareva and Hovy (2010) in the same domains, kindly made
available by the authors. It is once more to be noted that Kozareva and Hovy, during hy-
pernym extraction, reject all the nodes not belonging to WordNet, whereas we assume
no a-priori knowledge of the domain, apart from adopting the same set of seed terms
used by K&H.
Figure 9 shows, for each domain (ANIMALS, PLANTS, VEHICLES, and VIRUSES), and
for each cut level of the hierarchy, a plot of Bi1,2 values multiplied by the penalty factor.
As far as the comparison with K&H is concerned, we notice that, though K&H obtain
better performance in general, OntoLearn has higher coverage over the domain, as is
shown by the highest values for i = 0, and has a higher depth of the derived hierarchy,
especially with DAG[0, 99]. Another recurrent phenomenon is that K&H curves grace-
fully degrade from the root to the leaf nodes, possibly with a peak in the intermediate
levels, whereas OntoLearn has a hollow in the mid-high region (see the region 4?6 for
ANIMALS and 1?2 for the other three hierarchies) and often a relative peak in the lowest
Figure 9
Gold standard evaluation of our three versions of OntoLearn Reloaded against WordNet
(ANIMALS, PLANTS, and VEHICLES) and MeSH (VIRUSES). A comparison with K&H is also
shown for the first three domains.
694
Velardi, Faralli, and Navigli OntoLearn Reloaded
levels. In the manual evaluation section we explain this phenomenon, which also occurs
in the ARTIFICIAL INTELLIGENCE taxonomy.
The generally decreasing values of Bi1,2 in Figure 9 show that, as expected, mim-
icking the clustering criteria of a taxonomy created by a team of experts proves very
difficult at the lowest levels, while performance grows at the highest levels. At the
lowest taxonomy levels there are two opposing phenomena: overgeneralization and
overspecialization. For example, macaque has monkey as a direct hypernym in WordNet,
and we find short-tailed monkey as a direct hypernym of macaque. An opposite case is
ganoid, which is a taleostan in WordNet and simply a fish in our taxonomy. The first
case does not reward the learned taxonomy (though, unlike for the overlapping factor
[Maedche, Pekar, and Staab 2002], it does not cause a penalty), whereas the second is
quite penalizing. More of these examples will be provided in Section 4.3.
Finally, in Table 8 we show the cumulative B1,2 values for the four domains, ac-
cording to Equation (9). Here, except for the VEHICLES domain, the unconstrained
DAG[0, 99] performs best.
4.3 Manual Evaluation
This section is dedicated to the manual assessment of the learned ontologies. The
section is divided in three parts: Section 4.3.1 is concerned with the human validation of
hypernymy relations, Section 4.3.2 examines the global learned taxonomic structure in
the search for common phenomena across the six domains, and finally Section 4.3.3 in-
vestigates the possibility of enhancing our hypernymy harvesting method with K&H?s
Hearst-like patterns, applying their method to the AI domain and manually evaluating
the extracted hypernyms.
4.3.1 Hypernym Evaluation. To reduce subjectivity in taxonomy evaluation, we asked
three annotators, only one of whom was a co-author, to validate, for each of the three
experiments of each of the six domains, a random sample of hypernymy relations. For
each relation the definition(s) supporting the relation were also provided. This was
especially helpful for domains like VIRUSES, but also PLANTS and ANIMALS, in which
the annotators were not expert. The size of each random sample was 300 for the (larger)
AI and FINANCE domains and 100 for the others.
Each evaluator was asked to tag incorrect relations, regardless of whether the error
was due to the selection of non-domain definitions (e.g., for VEHICLES: ?a driver is a
golf club with a near vertical face that is used for hitting long shots from the tee?), to
a poor definition (e.g., for AI: ?a principle is a fundamental essence, particularly one
producing a given quality?) or to a wrong selection of the hypernym. As an example of
the latter, in the PLANTS domain, we extracted the hypernym species from the sentence:
?geranium is a genus of 422 species of flowering annual, biennial, and perennial plants
Table 8
Values of B1,2 for the domains of VIRUSES, ANIMALS, PLANTS, and VEHICLES.
Experiment VIRUSES ANIMALS PLANTS VEHICLES
TREE 0.093 0.064 0.059 0.065
DAG [1,3] 0.101 0.062 0.072 0.069
DAG [0,99] 0.115 0.097 0.080 0.103
K&H n.a. 0.067 0.068 0.158
695
Computational Linguistics Volume 39, Number 3
Table 9
Precision of hypernym edges on six domains (calculated on a majority basis) and inter-annotator
agreement on the corresponding sample of relations.
Experiment # of Sample Precision ?
AI
TREE 300 80.3% [241/300] 0.45
DAG [1,3] 300 73.6% [221/300] 0.42
DAG [0,99] 300 73.0% [219/300] 0.41
FINANCE
TREE 300 93.6% [281/300] 0.40
DAG [1,3] 300 93.0% [279/300] 0.43
DAG [0,99] 300 92.6% [278/300] 0.41
VIRUSES
TREE 100 99.0% [99/100] 0.49
DAG [1,3] 100 99.0% [99/100] 0.39
DAG [0,99] 100 99.0% [99/100] 0.32
ANIMALS
TREE 100 92.0% [92/100] 0.53
DAG [1,3] 100 92.0% [92/100] 0.36
DAG [0,99] 100 90.0% [90/100] 0.56
PLANTS
TREE 100 89.0% [89/100] 0.49
DAG [1,3] 100 85.0% [85/100] 0.53
DAG [0,99] 100 97.0% [97/100] 0.26
VEHICLES
TREE 100 92.0% [92/100] 0.64
DAG [1,3] 100 92.0% [92/100] 0.49
DAG [0,99] 100 91.0% [91/100] 0.44
? Interpretation
< 0 Poor agreement
0.01?0.20 Slight agreement
0.21?0.40 Fair agreement
0.41?0.60 Moderate agreement
0.61?0.80 Substantial agreement
0.81?1.00 Almost perfect agreement
that are commonly known as the cranesbills? since, in the WCL verb set, we have ?is
a * species of? and ?is a * genus of?, but not the concatenation of these two patterns.
Annotators could mark with ? a hyponym?hypernym pair for which they felt uncertain.
Though it would have been useful to distinguish between the different types of error,
we found that regarding many error types there was, anyway, very low inter-annotator
agreement. Indeed the annotation task would appear to be intrinsically complex and
controversial. In any case, an assessment of the definition and hypernym extraction
tasks in isolation was already provided by Navigli and Velardi (2010).
Table 9 summarizes the results. Precision of each classification was computed on a
majority basis, and we used Fleiss? kappa statistics (Fleiss 1971) to measure the inter-
annotator agreement. In general, the precision is rather good, though it is lower for the
AI domain, probably due to its high ?vitality? (many new terms continuously arise, and
for some of them it is difficult to find good quality definitions). In general, precision is
higher in focused domains (VIRUSES, ANIMALS, PLANTS, and VEHICLES) than in wide-
range domains (AI and FINANCE). The former domains, however, have just one quite
696
Velardi, Faralli, and Navigli OntoLearn Reloaded
?narrow? upper concept (virus for VIRUSES, etc.), whereas AI and FINANCE have several
upper concepts (e.g., person or abstraction), and furthermore they are less focused. This
means that there is an inherently higher ambiguity and this may be seen as justifying
the lower performance. In Table 9 we also note that TREE structures achieve in general a
higher precision, except for PLANTS, whereas the DAG has the advantage of improving
recall (see also Section 4.2.2).
Note that high precision here does not contradict the results shown in Section 4.2.2:
In this case, each single relation is evaluated in isolation, therefore overgenerality or
overspecificity do not imply a penalty, provided the relation is judged to be correct.
Furthermore, global consistency is not considered here: for example, distance metric
learning ? parametric technique, and eventually ends up in technique, whereas belief
network learning ? machine learning algorithm ends up in algorithm and then in procedure.
In isolation, these hypernymy patterns are acceptable, but within a taxonomic structure
one would like to see a category node grouping all terms denoting machine learning
algorithms. This behavior should be favored by the node weighting strategy described
in Section 3.4, aimed at attracting nodes with multiple hypernyms towards the most
populated category nodes. As in the previous example, however, there are category
nodes that are almost equally ?attractive? (e.g., algorithm and technique), and, further-
more, the taxonomy induction algorithm can only select among the set of hypernyms
extracted during the harvesting phase. Consequently, when no definition suggests that
distance metric learning is a hyponym of machine learning algorithm, or of any other
concept connected to machine learning algorithm, there is no way of grouping distance
metric learning and belief network learning in the desired way. This task must be postponed
to manual post-editing.
Concerning the kappa statistics, we note that the values range from moderate to
substantial in most cases. These numbers are apparently low, but the task of evaluating
hypernymy relations is quite a complex one. Similar kappa values were obtained in
Yang and Callan (2008) in a human-guided ontology learning task.
4.3.2 Structural Assessment. In addition to the manual evaluation summarized in
Table 9, a structural assessment was performed to identify the main sources of error.
To this end, one of the authors analyzed the full AI and FINANCE taxonomies and a
sample of the other four domains in search of recurring errors. In general, our optimal
branching algorithm and weighting schema avoids many of the problems highlighted in
well-known studies on taxonomy acquisition from dictionaries (Ide and Ve?ronis 1993),
like circularity, over-generalization, and so forth. There are new problems to be faced,
however.
The main sources of error are the following:
 Ambiguity of terms, especially at the intermediate levels
 Low quality of definitions
 Hypernyms described by a clause rather than by a single- or multi-word
expression
 Lack of an appropriate WCL to analyze the definition
 Difficulty of extracting the correct hypernym string from phrases with
identical syntactic structure
We now provide examples for each of these cases.
697
Computational Linguistics Volume 39, Number 3
Figure 10
Error distribution of the TREE version of our algorithm on the ARTIFICIAL INTELLIGENCE
domain.
Ambiguity. Concerning ambiguity of terms, consider Figures 10 and 11, which show the
distribution of errors at the different levels of the learned AI and FINANCE taxonomies
for the TREE experiment. The figures provide strong evidence that most errors are
located in the intermediate levels of the taxonomy. As we move from leaf nodes to
the upper ontology, the extracted terms become progressively more general and con-
sequently more ambiguous. For these terms the domain heuristics may turn out to be
inadequate, especially if the definition is a short sentence.
But why are these errors frequent at the intermediate levels and not at the highest
levels? To understand this, consider the following example from the AI domain: For
the term classifier the wrong hypernym is selected from the sentence ?classifier is a
person who creates classifications.? In many cases, wrong hypernyms do not accumulate
sufficient weight and create ?dead-end? hypernymy chains, which are pruned during
the optimal branching step. But, unfortunately, a domain appropriate definition is
Figure 11
Error distribution of the TREE version of our algorithm on the FINANCE domain.
698
Velardi, Faralli, and Navigli OntoLearn Reloaded
found for person: ?person is the more general category of an individual,? due to the
presence of the domain word category. On the other hand, this new sentence produces
an attachment that, in a sense, recovers the error, because category is a ?good? domain
concept that eventually ends up in subsequent iterations to the upper node abstraction.
Therefore, what happens is that the upper taxonomy nodes, with the help of the domain
heuristic, mitigate the ?semantic drift? caused by out-of-domain ambiguity, recovering
the ambiguity errors of the intermediate levels. This phenomenon is consistently found
in all domains, as shown by the hollow that we noticed in the graphs of Section 4.2.2.
An example in the ANIMALS domain is represented by the hypernymy sequence
fawn ? color ? race ? breed ? domestic animal, where the wrong hypernym color was
originated by the sentence ?fawn is a light yellowish brown color that is usually used in
reference to a dog?s coat color.? Only in VIRUSES is the phenomenon mitigated by the
highly specific and very focused nature of the domain.
In addition to out-of-domain ambiguity, we have two other phenomena: in-domain
ambiguity and polysemy. In-domain ambiguity is rare, but not absent (Agirre et al
2010; Faralli and Navigli 2012). Consider the example of Figure 12a, from the VEHICLES
domain: tractor has two definitions corresponding to two meanings, which are both
correct. The airplane meaning is ?tractor is an airplane where the propeller is located in
front of the fuselage,? whereas the truck meaning is ?tractor is a truck for pulling a semi-
trailer or trailer.? Here the three hyponyms of tractor (see the figure) all belong to the
truck sense. We leave to future developments the task of splitting in-domain ambiguous
nodes in the appropriate way.
Another case is systematic polysemy, which is shown in Figure 13. The graph in
the figure, from the AI domain, captures the fact that a semantic network, as well as
its hyponyms, are both a methodology and a representation. Another example is shown
in Figure 12b for the PLANTS domain, where systematic polysemy can be observed
for terms like olive, orange, and breadfruit, which are classified as evergreen tree and
fruit. Polysemy, however, does not cause errors, as it does for in-domain ambiguity,
because hyponyms of polysemous concepts inherit the polysemy: In the two graphs
of Figures 13 and 12b, both partitioned semantic network and tangerine preserve the
polysemy of their ancestors. Note that in-domain ambiguity and polysemy are only
captured by the DAG structure; therefore this can be seen as a further advantage (in
addition to higher recall) of the DAG model over and against the more precise TREE
structure.
Figure 12
An example of in-domain ambiguity (a) and an example of systematic polysemy (b). Dashed
edges were added to the graph as a result of the edge recovery phase (see Section 3.5).
699
Computational Linguistics Volume 39, Number 3
Figure 13
An example of systematic polysemy. Dashed edges were added to the graph as a result of the
edge recovery phase (see Section 3.5).
Low quality of definitions. Often textual definitions, especially if extracted from the
Web, do not have a high quality. Examples are: ?artificial intelligence is the next big
development in computing? or ?aspectual classification is also a necessary prerequi-
site for interpreting certain adverbial adjuncts.? These sentences are definitions on a
syntactic ground, but not on a semantic ground. As will be shown in Section 4.3.3,
this problem is much less pervasive than for Hearst-like lexico-syntactic patterns,
although, neither domain heuristics nor the graph pruning could completely eliminate
the problem. We can also include overgeneralization in this category of problems: Our
algorithm prefers specific hypernyms to general hypernyms, but for certain terms no
specific definitions are found. The elective way to solve this problem would be to assign
a quality confidence score to the definition source (document or Web page), for example,
by performing an accurate and stable classification of its genre (Petrenz and Webber
2011).
Hypernym is a clause. There are cases in which, although very descriptive and good
quality definitions are found, it is not possible to summarize the hypernym with a
term or multi-word expression. For example ?anaphora resolution is the process of
determining whether two expressions in natural language refer to the same real world
entity.? OntoLearn extracts process of determining which ends up in procedure, process.
This is not completely wrong, however, and in some case is even fully acceptable, as
for ?summarizing is a process of condensing or expressing in short something you
have read, watched or heard?: here, process of condensing is an acceptable hypernym.
An example for FINANCE is: ?market-to-book ratio is book value of assets minus book
value of equity plus market value of equity,? where we extracted book value, rather than
the complete formula. Another example is: ?roa is defined as a ratio of operating income
to book value of assets,? from which we extracted ratio, which is, instead, acceptable.
Lack of an appropriate definitional pattern. Though we acquired hundreds of different
definitional patterns, there are still definitions that are not correctly parsed. We already
700
Velardi, Faralli, and Navigli OntoLearn Reloaded
mentioned the geranium example in the PLANTS domain. An example in the AI domain
is ?execution monitoring is the robot?s process of observing the world for discrepancies
between the actual world and its internal representation of it,? where the extracted
hypernym is robot because we have no WCL with a Saxon genitive.
Wrong hypernym string. This is the case in which the hypernym is a superstring or
substring of the correct one, like: ?latent semantic analysis is a machine learning proce-
dure.? Here, the correct hypernym is machine learning procedure, but OntoLearn extracts
machine because learning is POS tagged as a verb. In general, it is not possible to evaluate
the extent of the hypernym phrase except case-by-case. The lattice learner acquired a
variety of hypernymy patterns, but the precision of certain patterns might be quite low.
For example, the hypernymy pattern ?* of *? is acceptable for ?In grammar, a lexical
category is a linguistic category of words? or ?page rank is a measure of site popularity?
but not for ?page rank is only a factor of the amount of incoming and outgoing links
to your site? nor for ?pattern recognition is an artificial intelligence area of considerable
importance.? The same applies to the hypernymy pattern ADJ NN: important algorithm is
wrong, although greedy algorithm is correct.
4.3.3 Evaluation of Lexico-Syntactic Patterns. As previously remarked, Kozareva and Hovy
(2010) do not actually apply their algorithm to the task of creating a new taxonomy, but
rather they try to reproduce three WordNet taxonomies, under the assumption that the
taxonomy nodes are known (cf. Section 4). Therefore, there is no evidence of the preci-
sion of their method on new domains, where the category nodes are unknown. On the
other hand, if Hearst?s patterns, which are at the basis of K&H?s hypernymy harvesting
algorithm, could show adequate precision, we would use them in combination with
our definitional patterns. This section investigates the matter.
As briefly summarized in Section 2, K&H create a hypernym graph in three steps.
Given a few root concepts (e.g., animal) and basic level concepts or instances (e.g.,
lion), they:
1) harvest new basic and intermediate concepts from the Web in an iterative
fashion, using doubly anchored patterns (DAP) like ??root? such as ?seed?
and ?? and inverse DAP (i.e., DAP?1) like ?? such as ?term1? and ?term2??.
The procedure is iterated until no new terms can be found;
2) rank the nodes extracted with DAP by out-degree and those extracted
with inverse DAP by in-degree, so as to prune out less promising terms;
3) induce the final taxonomic structure by positioning the intermediate nodes
between basic level and root terms using a concept positioning procedure
based on a variety of Hearst-like surface patterns. Finally, they eliminate
cycles, as well as nodes with no predecessor or no successor, and they
select the longest path in the case of multiple paths between node pairs.
In this section we apply their method23 to the domain of AI in order to manually
analyze the quality of the extracted relations. To replicate the first two steps of K&H
algorithm we fed the algorithm with a growing set of seed terms randomly selected
from our validated terminology, together with their hierarchically related root terms
23 We followed the exact procedure described in Figure 2 of Kozareva & Hovy (2010).
701
Computational Linguistics Volume 39, Number 3
Table 10
K&H performance on the AI domain.
number of root/seed pairs 1 10 100 1,000
# new concepts 131 163 227 247
# extracted is-a relations 114 146 217 237
correct and in-domain 21.05% 24.65% 18.89% 18.56%
(24/114) (36/146) (41/217) (44/237)
in the upper taxonomy (e.g., unsupervised learning is a method or maximum entropy is a
measure). We then performed the DAP and DAP?1 steps iteratively until no more terms
could be retrieved, and we manually evaluated the quality of the harvested concepts
and taxonomic relations using the same thresholding formula described in K&H.24 We
give the results in Table 10.
As we said earlier, our purpose here is mainly to evaluate the quality of Hearst
patterns in more technical domains, and the efficacy of DAP and DAP?1 steps in
retrieving domain concepts and relations. Therefore, replicating step (3) above is not
useful in this case since, rather than adding new nodes, step (3) is aimed, as in our
optimal branching and pruning recovery steps, at reorganizing and trimming the final
graph.
Table 10 should be compared with the first three rows (AI) of Table 9: It shows that
in the absence of a priori knowledge on the domain concepts the quantity and quality
of the is-a links extracted by the K&H algorithm is much lower than those extracted by
OntoLearn Reloaded. First, the number of new nodes found by the K&H algorithm is
quite low: For the same domain of ARTIFICIAL INTELLIGENCE, our method, as shown in
Table 9, is able to extract from scratch 2,387 ? 52 = 2,335 nodes,25 in comparison with the
247 new nodes of Table 10, obtained with 1,000 seeds. Second, many nodes extracted by
the K&H algorithm, like fan speed, guidelines, chemical engineering, and so on, are out-of-
domain and many hypernym relations are incorrect irrespective of their direction, like
computer program is a slow and data mining is a contemporary computing problem. Third, the
vast majority of the retrieved hypernyms are overgeneral, like discipline, method, area,
problem, technique, topic, and so forth, resulting in an almost flat hypernymy structure. A
high in-degree threshold and a very high number of seeds do not mitigate the problem,
demonstrating that Hearst-like patterns are not very good at harvesting many valid
hypernym relations in specialized domains.26
Following this evaluation, we can outline several advantages of our method over
K&H?s work (and, as a consequence, over Hearst?s patterns):
i) We obtain higher precision and recall when no a priori knowledge is
available on the taxonomy concepts, because hypernyms are extracted
from expert knowledge on the Web (i.e., technical definitions rather than
patterns reflecting everyday language).
24 The technique is based on the in-degree and out-degree of the graph nodes.
25 Remember that the 52 domain-independent upper terms are manually defined (cf. Section 4.1.4).
26 This result is in line with previous findings in a larger, domain-balanced experiment (Navigli and Velardi
2010) in which we have shown that WCLs outperform Hearst patterns and other methods in the task of
hypernym extraction.
702
Velardi, Faralli, and Navigli OntoLearn Reloaded
ii) We cope better with sense ambiguity via the domain filtering step.27
iii) We use a principled algorithmic approach to graph pruning and cycle
removal.28
iv) Thanks to the support provided by textual definitions, we are able to
cope with the problem of manually evaluating the retrieved concepts
and relations, even in the absence of a reference taxonomy.
4.3.4 Summary of Findings. We here summarize the main findings of our manifold
evaluation experiments:
i) With regard to the two versions of our graph pruning algorithm, we found
that TREE structures are more precise, whereas DAGs have a higher recall.
ii) Errors are mostly concentrated in the mid-level of the hierarchy, where
concepts are more ambiguous and the ?attractive? power of top nodes is
less influential. This was highlighted by our quantitative (F&M) model
and justified by means of manual analysis.
iii) The quality and number of definitions is critical for high performance.
Less-focused domains in which new terms continuously emerge are the
most complex ones, because it is more difficult to retrieve high-quality
definitions for them.
iv) Definitions, on the other hand, are a much more precise and high-coverage
source of knowledge for hypernym extraction than (Hearst-like) patterns
or contexts, because they explicitly represent expert knowledge on a
given domain. Furthermore, they are a very useful support for manual
validation and structural analysis.
5. Conclusions
In this paper we presented OntoLearn Reloaded, a graph-based algorithm for learning
a taxonomy from scratch using highly dense, potentially disconnected, hypernymy
graphs. The algorithm performs the task of eliminating noise from the initial graph
remarkably well on arbitrary, possibly specialized, domains, using a weighting scheme
that draws both on the topological properties of the graph and on some general prin-
ciples of taxonomic structures. OntoLearn Reloaded provides a considerable advance-
ment over the state of the art in taxonomy learning. First, it is the first algorithm that
experimentally demonstrates its ability to build a new taxonomy from the ground up,
without any a priori assumption on the domain except for a corpus and a set of (possibly
general) upper terms. The majority of existing systems start from a set of concepts
and induce hypernymy links between these concepts. Instead, we automatically learn
both concepts and relations via term extraction and iterative definition and hypernym
27 In the authors? words (Kozareva and Hovy 2010, page 1,115): ?we found that the learned terms in the
middle ranking do not refer to the meaning of vehicle as a transportation device, but to the meaning of
vehicle as media (i.e., seminar, newspapers), communication and marketing.?
28 Again in the authors? words (Kozareva and Hovy 2010, page 1,115): ?we found that in-degree is not
sufficient by itself. For example, highly frequent but irrelevant hypernyms such as meats and others are
ranked at the top of the list, while low frequent but relevant ones such as protochordates, hooved-mammals,
homeotherms are discarded.?
703
Computational Linguistics Volume 39, Number 3
extraction. Second, we cope with issues such as term ambiguity, complexity, and
multiplicity of hypernymy patterns. Third, we contribute a multi-faceted evaluation,
which includes a comparison against gold standards, plus a structural and a manual
evaluation. Taxonomy induction was applied to the task of creating new ARTIFICIAL
INTELLIGENCE and FINANCE taxonomies and four taxonomies for gold-standard
comparison against WordNet and MeSH.29
Our experimental analysis shows that OntoLearn Reloaded greatly simplifies the
task of acquiring a taxonomy from scratch: Using a taxonomy validation tool,30 a team
of experts can correct the errors and create a much more acceptable taxonomy in a
matter of hours, rather than man-months, also thanks to the automatic acquisition of
textual definitions for our concepts. As with any automated and unsupervised learning
tool, however, OntoLearn does make errors, as we discussed in Section 4. The accuracy
of the resulting taxonomy is clearly related to the number and quality of discovered
definitional patterns, which is in turn related to the maturity and generality of a domain.
Even with good definitions, problems might arise due to in- and out-domain ambiguity,
the latter being probably the major source of errors, together with complex definitional
structures. Although we believe that there is still room for improvement to OntoLearn
Reloaded, certain errors would appear unavoidable, especially for less focused and
relatively dynamic domains like ARTIFICIAL INTELLIGENCE and FINANCE, in which
new terms arise continuously and have very few, or no definitions on the Web.
Future work includes the addition of non-taxonomical relations along the lines of
ReVerb (Etzioni et al 2011) and WiSeNet (Moro and Navigli 2012), and a more sophis-
ticated rank-based method for scoring textual definitions. Finally, we plan to tackle
the issue of automatically discriminating between in-domain ambiguity and systematic
polysemy (as discussed in Section 4.3.2).
Acknowledgments
Stefano Faralli and Roberto Navigli
gratefully acknowledge the support of the
ERC Starting Grant MultiJEDI No. 259234.
The authors wish to thank Jim McManus for
his valuable comments on the paper, and
Zornitsa Kozareva and Eduard Hovy for
making their data available.
References
Agirre, Eneko, Oier Lo?pez de Lacalle,
Christiane Fellbaum, Shu-Kai Hsieh,
Maurizio Tesconi, Monica Monachini,
Piek Vossen, and Roxanne Segers.
2010. SemEval-2010 Task 17: All-words
Word Sense Disambiguation on a
specific domain. In Proceedings of the
5th International Workshop on Semantic
Evaluation (SemEval-2010), pages 75?80,
Uppsala.
Berland, Matthew and Eugene Charniak.
1999. Finding parts in very large
corpora. In Proceedings of the 27th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 57?64, College
Park, MD.
Biemann, Chris. 2005. Ontology learning
from text?A survey of methods.
LDV-Forum, 20(2):75?93.
Brank, Janez, Dunja Mladenic, and
Marko Grobelnik. 2006. Gold standard
based ontology evaluation using instance
assignment. In Proceedings of 4th Workshop
Evaluating Ontologies for the Web (EON),
Edinburgh.
Carpineto, Claudio and Giovanni Romano.
2012. Consensus Clustering Based on
a New Probabilistic Rand Index with
Application to Subtopic Retrieval.
IEEE Transactions on Pattern Analysis and
Machine Intelligence, 34(12):2315?2326.
Chu, Yoeng-Jin and Tseng-Hong Liu.
1965. On the shortest arborescence
of a directed graph. Science Sinica,
14:1396?1400.
Cimiano, Philipp, Andreas Hotho, and
Steffen Staab. 2005. Learning concept
29 Data sets are available at: http://lcl.uniroma1.it/ontolearn reloaded.
30 For example, http://lcl.uniroma1.it/tav/.
704
Velardi, Faralli, and Navigli OntoLearn Reloaded
hierarchies from text corpora using
formal concept analysis. Journal of
Artificial Intelligence Research,
24(1):305?339.
Cohen, Trevor and Dominic Widdows.
2009. Empirical distributional semantics:
Methods and biomedical applications.
Journal of Biomedical Informatics,
42(2):390?405.
Cormen, Thomas H., Charles E. Leiserson,
and Ronald L. Rivest. 1990. Introduction
to Algorithms. MIT Electrical Engineering
and Computer Science. MIT Press,
Cambridge, MA.
De Benedictis, Flavio, Stefano Faralli, and
Roberto Navigli. 2013. GlossBoot:
Bootstrapping Multilingual Domain
Glossaries from the Web. In Proceedings
of the 51st Annual Meeting of the
Association for Computational Linguistics
(ACL), Sofia.
De Nicola, Antonio, Michele Missikoff,
and Roberto Navigli. 2009. A software
engineering approach to ontology
building. Information Systems,
34(2):258?275.
Edmonds, Jack. 1967. Optimum branchings.
Journal of Research of the National Bureau of
Standards, 71B:233?240.
Etzioni, Oren, Anthony Fader, Janara
Christensen, Stephen Soderland, and
Mausam. 2011. Open information
extraction: The second generation. In
Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI),
pages 3?10, Barcelona.
Fahmi, Ismail and Gosse Bouma. 2006.
Learning to identify definitions using
syntactic features. In Proceedings of
the EACL 2006 workshop on Learning
Structured Information in Natural
Language Applications, pages 64?71,
Trento.
Faralli, Stefano and Roberto Navigli.
2012. A new minimally supervised
framework for domain Word Sense
Disambiguation. In Proceedings of
the 2012 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP-CoNLL),
pages 1,411?1,422, Jeju.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fleiss, Joseph L. 1971. Measuring
nominal scale agreement among
many raters. Psychological Bulletin,
76(5):378?382.
Fountain, Trevor and Mirella Lapata. 2012.
Taxonomy induction using hierarchical
random graphs. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics: Human Language
Technologies (HLT-NAACL), pages 466?476,
Montre?al.
Fowlkes, Edward B. and Colin L. Mallows.
1983. A method for comparing two
hierarchical clusterings. Journal of the
American Statistical Association,
78(383):553?569.
Girju, Roxana, Adriana Badulescu, and
Dan Moldovan. 2006. Automatic discovery
of part-whole relations. Computational
Linguistics, 32(1):83?135.
Gomez-Perez, Asuncio?n and David
Manzano-Mancho. 2003. A survey of
ontology learning methods and
techniques. OntoWeb Delieverable 1.5.
Universidad Polite?cnica de Madrid.
Guarino, Nicola and Chris Welty. 2002.
Evaluating ontological decisions with
OntoClean. Communications of the ACM,
45(2):61?65.
Hearst, Marti A. 1992. Automatic acquisition
of hyponyms from large text corpora.
In Proceedings of the 14th International
Conference on Computational Linguistics
(COLING), pages 539?545, Nantes.
Hovy, Eduard, Andrew Philpot,
Judith Klavans, Ulrich Germann, and
Peter T. Davis. 2003. Extending metadata
definitions by automatically extracting
and organizing glossary definitions. In
Proceedings of the 2003 Annual National
Conference on Digital Government Research,
pages 1?6, Boston, MA.
Ide, Nancy and Jean Ve?ronis. 1993.
Extracting knowledge bases from
machine-readable dictionaries: Have
we wasted our time? In Proceedings
of the Workshop on Knowledge Bases and
Knowledge Structures, pages 257?266,
Tokyo.
Kozareva, Zornitsa and Eduard Hovy. 2010.
A semi-supervised method to learn and
construct taxonomies using the Web.
In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 1,110?1,118,
Cambridge, MA.
Kozareva, Zornitsa, Ellen Riloff, and
Eduard Hovy. 2008. Semantic class
learning from the Web with hyponym
pattern linkage graphs. In Proceedings
of the 46th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 1,048?1,056, Columbus, OH.
705
Computational Linguistics Volume 39, Number 3
Maedche, Alexander, Viktor Pekar, and
Steffen Staab. 2002. Ontology learning
part one?on discovering taxonomic
relations from the Web. In N. Zhong,
J. Liu, and Y. Y. Yao, editors, Web
Intelligence. Springer Verlag, Berlin,
pages 301?322.
Maedche, Alexander and Steffen Staab.
2009. Ontology learning. In Steffen Staab
and Rudi Studer, editors, Handbook on
Ontologies. Springer, Berlin, pages 245?268.
Miller, George A., R. T. Beckwith,
Christiane D. Fellbaum, D. Gross, and
K. Miller. 1990. WordNet: An online
lexical database. International Journal of
Lexicography, 3(4):235?244.
Morey, Leslie C. and Alan Agresti. 1984. The
measurement of classification agreement:
An adjustment to the Rand statistic for
chance agreement. Educational and
Psychological Measurement, 44:33?37.
Moro, Andrea and Roberto Navigli. 2012.
WiSeNet: Building a Wikipedia-based
semantic network with ontologized
relations. In Proceedings of the 21st
ACM Conference on Information and
Knowledge Management (CIKM 2012),
pages 1,672?1,676, Maui, HI.
Navigli, Roberto. 2009. Word Sense
Disambiguation: A survey. ACM
Computing Surveys, 41(2):1?69.
Navigli, Roberto, and Simone Paolo
Ponzetto. 2012. BabelNet: The automatic
construction, evaluation and application of
a wide-coverage multilingual semantic
network. Artificial Intelligence 193,
pp. 217?250.
Navigli, Roberto and Paola Velardi. 2004.
Learning domain ontologies from
document warehouses and dedicated
websites. Computational Linguistics,
30(2):151?179.
Navigli, Roberto and Paola Velardi. 2005.
Structural semantic interconnections:
A knowledge-based approach to Word
Sense Disambiguation. IEEE Transactions
on Pattern Analysis and Machine Intelligence,
27(7):1075?1088.
Navigli, Roberto and Paola Velardi. 2010.
Learning Word-Class Lattices for
definition and hypernym extraction.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 1,318?1,327,
Uppsala.
Navigli, Roberto, Paola Velardi, and Stefano
Faralli. 2011. A graph-based algorithm for
inducing lexical taxonomies from scratch.
In Proceedings of the 22nd International Joint
Conference on Artificial Intelligence (IJCAI),
pages 1,872?1,877, Barcelona.
Newman, Mark E. J. 2010. Networks: An
Introduction. Oxford University Press.
Pado, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pantel, Patrick and Marco Pennacchiotti.
2006. Espresso: Leveraging generic
patterns for automatically harvesting
semantic relations. In Proceedings of
44th Annual Meeting of the Association for
Computational Linguistics joint with 21st
Conference on Computational Linguistics
(COLING-ACL), pages 113?120, Sydney.
Pasca, Marius. 2004. Acquisition of
categorized named entities for web search.
In Proceedings of the 13th ACM International
Conference on Information and Knowledge
Management (CIKM), pages 137?145,
Washington, DC.
Petasis, Georgios, Vangelis Karkaletsis,
Georgios Paliouras, Anastasia Krithara,
and Elias Zavitsanos. 2011. Ontology
population and enrichment: State of the
art. In Georgios Paliouras, Constantine
Spyropoulos, and George Tsatsaronis,
editors, Knowledge-Driven Multimedia
Information Extraction and Ontology
Evolution, volume 6050 of Lecture Notes
in Computer Science. Springer, Berlin /
Heidelberg, pages 134?166.
Petrenz, Philipp and Bonnie L. Webber.
2011. Stable classification of text genres.
Computational Linguistics, 37(2):385?393.
Ponzetto, Simone Paolo and Roberto Navigli.
2009. Large-scale taxonomy mapping for
restructuring and integrating Wikipedia.
In Proceedings of the 21st International Joint
Conference on Artificial Intelligence (IJCAI),
pages 2,083?2,088, Pasadena, CA.
Ponzetto, Simone Paolo and Michael Strube.
2011. Taxonomy induction based on a
collaboratively built knowledge repository.
Artificial Intelligence, 175:1737?1756.
Poon, Hoifung and Pedro Domingos. 2010.
Unsupervised ontology induction from
text. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 296?305, Uppsala.
Rand, William M. 1971. Objective criteria for
the evaluation of clustering methods.
Journal of the American Statistical
Association, 66(336):846?850.
Schmid, Helmut. 1995. Improvements in
part-of-speech tagging with an application
to German. In Proceedings of the ACL
SIGDAT-Workshop, pages 47?50, Dublin.
706
Velardi, Faralli, and Navigli OntoLearn Reloaded
Sclano, Francesco and Paola Velardi. 2007.
TermExtractor: A Web application to
learn the shared terminology of emergent
Web communities. In Proceedings of the 3th
International Conference on Interoperability
for Enterprise Software and Applications
(I-ESA), pages 287?290, Funchal.
Snow, Rion, Dan Jurafsky, and Andrew Ng.
2006. Semantic taxonomy induction from
heterogeneous evidence. In Proceedings of
44th Annual Meeting of the Association for
Computational Linguistics joint with 21st
Conference on Computational Linguistics
(COLING-ACL), pages 801?808, Sydney.
Sowa, John F. 2000. Knowledge Representation:
Logical, Philosophical, and Computational
Foundations. Brooks Cole Publishing Co.,
Pacific Grove, CA.
Storrer, Angelika and Sandra Wellinghoff.
2006. Automated detection and annotation
of term definitions in German text corpora.
In Proceedings of the 5th International
Conference on Language Resources and
Evaluation (LREC), pages 2,373?2,376,
Genova.
Suchanek, Fabian M., Gjergji Kasneci,
and Gerhard Weikum. 2008. YAGO:
A large ontology from Wikipedia and
WordNet. Journal of Web Semantics,
6(3):203?217.
Tang, Jie, Ho Fung Leung, Qiong Luo,
Dewei Chen, and Jibin Gong. 2009.
Towards ontology learning from
folksonomies. In Proceedings of the
21st International Joint Conference on
Artificial Intelligence (IJCAI),
pages 2,089?2,094, Pasadena, CA.
Tarjan, Robert Endre. 1977. Finding optimum
branchings. Networks, 7(1):25?35.
Velardi, Paola, Roberto Navigli, and Pierluigi
D?Amadio. 2008. Mining the Web to create
specialized glossaries. IEEE Intelligent
Systems, 23(5):18?25.
Vo?lker, Johanna, Denny Vrandec?ic?,
York Sure, and Andreas Hotho. 2008.
AEON?An approach to the automatic
evaluation of ontologies. Journal of
Applied Ontology, 3(1-2):41?62.
Vossen, Piek. 2001. Extending, trimming
and fusing WordNet for technical
documents. In Proceedings of the North
American Chapter of the Association
for Computational Linguistics Workshop
on WordNet and Other Lexical
Resources: Applications, Extensions
and Customizations (NAACL),
pages 125?131, Pittsburgh, PA.
Wagner, Silke and Dorothea Wagner. 2007.
Comparing clusterings: An overview.
Technical Report 2006-04, Faculty of
Informatics, Universita?t Karlsruhe (TH).
Westerhout, Eline. 2009. Definition extraction
using linguistic and structural features.
In Proceedings of the RANLP Workshop
on Definition Extraction, pages 61?67,
Borovets.
Yang, Hui and Jamie Callan. 2008.
Human-guided ontology learning.
In Proceedings of Human-Computer
Interaction and Information Retrieval
(HCIR), pages 26?29, Redmond, WA.
Yang, Hui and Jamie Callan. 2009. A
metric-based framework for automatic
taxonomy induction. In Proceedings of
the 47th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 271?279, Suntec.
Zavitsanos, Elias, Georgios Paliouras,
and George A. Vouros. 2011. Gold
standard evaluation of ontology learning
methods through ontology transformation
and alignment. IEEE Transactions on
Knowledge and Data Engineering,
23(11):1635?1648.
Zhang, Chunxia and Peng Jiang. 2009.
Automatic extraction of definitions.
In Proceedings of 2nd IEEE International
Conference on Computer Science and
Information Technology (ICCSIT),
pages 364?368, Beijing.
707

Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 528?538,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
GlossBoot: Bootstrapping Multilingual Domain Glossaries from the Web
Flavio De Benedictis, Stefano Faralli and Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
flavio.debene@gmail.com,{faralli,navigli}@di.uniroma1.it
Abstract
We present GlossBoot, an effective
minimally-supervised approach to ac-
quiring wide-coverage domain glossaries
for many languages. For each language
of interest, given a small number of
hypernymy relation seeds concerning a
target domain, we bootstrap a glossary
from the Web for that domain by means of
iteratively acquired term/gloss extraction
patterns. Our experiments show high
performance in the acquisition of domain
terminologies and glossaries for three
different languages.
1 Introduction
Much textual content, such as that available on
the Web, contains a great deal of information fo-
cused on specific areas of knowledge. However,
it is not infrequent that, when reading a domain-
specific text, we humans do not know the mean-
ing of one or more terms. To help the human
understanding of specialized texts, repositories of
textual definitions for technical terms, called glos-
saries, are compiled as reference resources within
each domain of interest. Interestingly, electronic
glossaries have been shown to be key resources
not only for humans, but also in Natural Language
Processing (NLP) tasks such as Question Answer-
ing (Cui et al, 2007), Word Sense Disambiguation
(Duan and Yates, 2010; Faralli and Navigli, 2012)
and ontology learning (Navigli et al, 2011; Ve-
lardi et al, 2013).
Today large numbers of glossaries are available
on the Web. However most such glossaries are
small-scale, being made up of just some hundreds
of definitions. Consequently, individual glossaries
typically provide a partial view of a given domain.
Moreover, there is no easy way of retrieving the
subset of Web glossaries which appertains to a do-
main of interest. Although online services such
as Google Define allow the user to retrieve defi-
nitions for an input term, such definitions are ex-
tracted from Web glossaries and put together for
the given term regardless of their domain. As a re-
sult, gathering a large-scale, full-fledged domain
glossary is not a speedy operation.
Collaborative efforts are currently producing
large-scale encyclopedias, such as Wikipedia,
which are proving very useful in NLP (Hovy et al,
2013). Interestingly, wikipedias also include man-
ually compiled glossaries. However, such glos-
saries still suffer from the same above-mentioned
problems, i.e., being incomplete or over-specific,1
and hard to customize according to a user?s needs.
To automatically obtain large domain glos-
saries, over recent years computational ap-
proaches have been developed which extract tex-
tual definitions from corpora (Navigli and Velardi,
2010; Reiplinger et al, 2012) or the Web (Fujii
and Ishikawa, 2000). The former methods start
from a given set of terms (possibly automatically
extracted from a domain corpus) and then har-
vest textual definitions for these terms from the
input corpus using a supervised system. Web-
based methods, instead, extract text snippets from
Web pages which match pre-defined lexical pat-
terns, such as ?X is a Y?, along the lines of Hearst
(1992). These approaches typically perform with
high precision and low recall, because they fall
short of detecting the high variability of the syn-
tactic structure of textual definitions. To address
the low-recall issue, recurring cue terms occurring
within dictionary and encyclopedic resources can
be automatically extracted and incorporated into
lexical patterns (Saggion, 2004). However, this
approach is term-specific and does not scale to ar-
bitrary terminologies and domains.
In this paper we propose GlossBoot, a novel
approach which reduces human intervention to a
bare minimum and exploits the Web to learn a
1http://en.wikipedia.org/wiki/Portal:Contents/Glossaries
528
  
Pattern and glossary extractionInitial seedselection  Gloss ranking and filteringSeedqueries Seedselection
initialseeds
newseeds
 search results domainglossary Gk
finalglossary 1 2 3 4 5
Figure 1: The GlossBoot bootstrapping process for glossary learning.
full-fledged domain glossary. Given a domain and
a language of interest, we bootstrap the glossary
learning process with just a few hypernymy rela-
tions (such as computer is-a device), with the only
condition that the (term, hypernym) pairs must be
specific enough to implicitly identify the domain
in the target language. Hence we drop the require-
ment of a large domain corpus, and also avoid the
use of training data or a manually defined set of
lexical patterns. To the best of our knowledge, this
is the first approach which jointly acquires large
amounts of terms and glosses from the Web with
minimal supervision for any target domain and
language.
2 GlossBoot
Our objective is to harvest a domain glossary G
containing pairs of terms/glosses in a given lan-
guage. To this end, we automatically populate a
set of HTML patterns P which we use to extract
definitions from Web glossaries. Initially, both
P := ? and G := ?. We incrementally populate
the two sets by means of an initial seed selection
step and four iterative steps (cf. Figure 1):
Step 1. Initial seed selection: first, we manu-
ally select a set of K hypernymy relation seeds
S = {(t1, h1), . . . , (tK , hK)}, where the pair (ti,
hi) contains a term ti and its generalization hi
(e.g., (firewall, security system)). This is the only
human input to the entire glossary learning pro-
cess. The selection of the input seeds plays a key
role in the bootstrapping process, in that the pat-
tern and gloss extraction process will be driven by
these seeds. The chosen hypernymy relations thus
have to be as topical and representative as pos-
sible for the domain of interest (e.g., (compiler,
computer program) is an appropriate pair for com-
puter science, while (byte, unit of measurement)
is not, as it might cause the extraction of several
glossaries of units and measures).
We now set the iteration counter k to 1 and start
the first iteration of the glossary bootstrapping pro-
cess (steps 2-5). After each iteration k, we keep
track of the set of glosses Gk, acquired during it-
eration k.
Step 2. Seed queries: for each seed pair (ti, hi),
we submit the following query to a Web search
engine: ?ti? ?hi? glossaryKeyword2 (where
glossaryKeyword is the term in the target lan-
guage referring to glossary (i.e., glossary for En-
glish, glossaire for French etc.)) and collect the
top-ranking results for each query.3 Each result-
ing page is a candidate glossary for the domain
implicitly identified by our relation seeds S.
Step 3. Pattern and glossary extraction: we
initialize the glossary for iteration k as follows:
Gk := ?. Next, from each resulting page, we har-
vest all the text snippets s starting with ti and end-
ing with hi (e.g., ?firewall</b> ? a <i>security
system? where ti = firewall and hi = security sys-
tem), i.e., s = ti . . . hi. For each such text snippet
s, we perform the following substeps:
(a) extraction of the term/gloss separator: we
start from ti and move right until we extract
the longest sequence pM of HTML tags and
non-alphanumeric characters, which we call the
term/gloss separator, between ti and the glossary
definition (e.g., ?</b> -? between ?firewall? and
?a? in the above example).
(b) gloss extraction: we expand the snippet s
to the right of hi in search of the entire gloss
of ti, i.e., until we reach a block element (e.g.,
<span>, <p>, <div>), while ignoring format-
ting elements such as <b>, <i> and <a> which
are typically included within a definition sen-
tence. As a result, we obtain the sequence
ti pM glosss(ti) pR, where glosss(ti) is our gloss
for seed term ti in snippet s (which includes hi by
construction) and pR is the HTML block element
2In what follows we use the typewriter font for
keywords and term/gloss separators.
3We use the Google Ajax APIs, which return the 64 top-
ranking search results.
529
Generalized pattern HTML text snippet
<strong> * </strong> - * </span> <strong>Interrupt</strong> - The suspension of normal
program execution to perform a higher priority service rou-
tine as requested by a peripheral device. </span>
<dt> * </dt><dd> * </dd> <dt>Netiquette</dt><dd>The established conventions
of online politeness are called netiquette.</dd>
<h3> * </h3><p> * </p> <h3>Compiler</h3><p>A program that translates
source code, such as C++ or Pascal, into directly executable
machine code.</p>
<span> * </span> - * </p> <span>Signature</span> - A function?s name and param-
eter list. </p>
<span> * </span>: * <span> <span>Blog</span>: Short for ?web log?, a blog is an
online journal. <span>
Table 1: Examples of generalized patterns together with matching HTML text snippets.
Figure 2: An example of decomposition during pattern extraction for a snippet matching the seed pair
(firewall, security system).
to the right of the extracted gloss. In Figure 2 we
show the decomposition of our example snippet
matching the seed (firewall, security system).
(c) pattern instance extraction: we extract the
following pattern instance:
pL ti pM glosss(ti) pR,
where pL is the longest sequence of HTML tags
and non-alphanumeric characters obtained when
moving to the left of ti (see Figure 2).
(d) pattern extraction: we generalize the above
pattern instance to the following pattern:
pL ? pM ? pR,
i.e., we replace ti and glosss(ti) with *. For the
above example, we obtain the following pattern:
<p><b> * </b> - * </p>.
Finally, we add the generalized pattern to the set
of patterns P , i.e., P := P ? {pL ? pM ? pR}.
We also add the first sentence of the retrieved gloss
glosss(ti) to our glossary Gk, i.e., Gk := Gk ?
{(ti, first(glosss(ti)))}, where first(g) returns
the first sentence of gloss g.
(e) pattern matching: finally, we look for addi-
tional pairs of terms/glosses in the Web page con-
taining the snippet s by matching the page against
the generalized pattern pL ? pM ? pR. We then
add toGk the new (term, gloss) pairs matching the
generalized pattern. In Table 1 we show some non-
trivial generalized patterns together with matching
HTML text snippets.
As a result of step 3, we obtain a glossary Gk
for the terms discovered at iteration k.
Step 4. Gloss ranking and filtering: impor-
tantly, not all the extracted definitions pertain to
the domain of interest. In order to rank the glosses
obtained at iteration k by domain pertinence, we
assume that the terms acquired at previous itera-
tions belong to the target domain, i.e., they are do-
main terms at iteration k. Formally, we define the
terminology T k?11 of the domain terms accumu-
lated up until iteration k ? 1 as follows: T k?11 :=?k?1
i=1 T i, where T i := {t : ?(t, g) ? Gi}. For thebase step k = 1, we define T 01 := {t : ?(t, g) ?
G1}, i.e., we use the first-iteration terminology it-
self.
To rank the glosses, we first transform each ac-
quired gloss g to its bag-of-word representation
Bag(g), which contains all the single- and multi-
word expressions in g. We use the lexicon of the
target language?s Wikipedia together with T k?11 in
order to obtain the bag of content words.4 Then we
4In fact Wikipedia is only utilized in the multi-word iden-
tification phase. We do not use Wikipedia for discovering
new terms.
530
Term Gloss Hypernym # Seeds Score
dynamic packet filter A firewall facility that can monitor the state of ac-
tive connections and use this information to determine
which network packets to allow through the firewall
firewall 2 0.75
die An integrated circuit chip cut from a finished wafer. integrated circuit 1 0.75
constructor a method used to help create a new object and ini-
tialise its data
method 0 1.00
Table 2: Examples of extracted terms, glosses and hypernyms (seeds are in bold, domain terms, i.e., in
T k?11 , are underlined, non-domain terms in italics).
calculate the domain score of a gloss g as follows:
score(g) = |Bag(g) ? T
k?1
1 |
|Bag(g)| . (1)
Finally, we use a threshold ? (whose tuning is
described in the experimental section) to remove
from Gk those glosses g whose score(g) < ?.
In Table 2 we show some glosses in the com-
puter science domain (second column, domain
terms are underlined) together with their scores
(last column).
Step 5. Seed selection for next iteration: we
now aim at selecting the new set of hypernymy
relation seeds to be used to start the next iteration.
We perform three substeps:
(a) Hypernym extraction: for each newly-
acquired term/gloss pair (t, g) ? Gk, we automati-
cally extract a candidate hypernym h from the tex-
tual gloss g. To do this we use a simple unsuper-
vised heuristic which just selects the first term in
the gloss.5 We show an example of hypernym ex-
traction for some terms in Table 2 (we report the
term in column 1, the gloss in column 2 and the
hypernyms extracted by the first term hypernym
extraction heuristic in column 3).
(b) (Term, Hypernym)-ranking: we sort all the
glosses in Gk by the number of seed terms found
in each gloss. In the case of ties (i.e., glosses with
the same number of seed terms), we further sort
the glosses by the score given in Formula 1. We
show an example of rank for some glosses in Table
2, where seed terms are in bold, domain terms (i.e.,
in T k?11 ) are underlined, and non-domain terms
are shown in italics.
5While more complex strategies could be used, such as
supervised classifiers (Navigli and Velardi, 2010), we found
that this heuristic works well because, even when it is not a
hypernym, the first term plays the role of a cue word for the
defined term.
(c) New seed selection: we select the (term, hy-
pernym) pairs corresponding to the K top-ranking
glosses.
Finally, if k equals the maximum number of it-
erations, we stop. Else, we increment the iteration
counter (i.e., k := k + 1) and jump to step (2) of
our glossary bootstrapping algorithm after replac-
ing S with the new set of seeds.
The output of glossary bootstrapping is a do-
main glossary G := ?i=1,...,maxGi, whichincludes a domain terminology T := {t :
?(t, g) ? G} (i.e., T := Tmax1 ) and a set of
glosses glosses(t) for each term t ? T (i.e.,
glosses(t) := {g : ?(t, g) ? G}).
3 Experimental Setup
3.1 Domains and Gold Standards
For our experiments we focused on four differ-
ent domains, namely, Computing, Botany, Envi-
ronment, and Finance, and on three languages,
namely, English, French and Italian. Note that not
all the four domains are clear-cut. For instance, the
Environment domain is quite interdisciplinary, in-
cluding terms from fields such as Chemistry, Biol-
ogy, Law, Politics, etc.
For each domain and language we selected
as gold standards well-reputed glossaries on
the Web, such as: the Utah computing glos-
sary,6 the Wikipedia glossary of botanical terms,7
a set of Wikipedia glossaries about environ-
ment,8 and the Reuters glossary for Finance9
(full list at http://lcl.uniroma1.it/
glossboot/). We report the size of the four
gold-standard datasets in Table 4.
6http://www.math.utah.edu/?wisnia/glossary.html
7http://en.wikipedia.org/wiki/Glossary of botanical terms
8http://en.wikipedia.org/wiki/List of environmental issues,
http://en.wikipedia.org/wiki/Glossary of environmental science,
http://en.wikipedia.org/wiki/Glossary of climate change
9http://glossary.reuters.com/index.php/Main Page
531
Computing Botany Environment Financechip circuit leaf organ sewage waste eurobond bonddestructor method grass plant acid rain rain asset play stockcompiler program cultivar variety ecosystem system income stock securityscanner device gymnosperm plant air monitoring sampling financial intermediary institutionfirewall security system flower reproductive organ global warming temperature derivative financial product
Table 3: Hypernymy relation seeds used to bootstrap glossary learning in the four domains for the English
language.
3.2 Seed Selection
For each domain and language we manually se-
lected five seed hypernymy relations, shown for
the English language in Table 3. The seeds
were selected by the authors on the basis of
just two conditions: i) the seeds should cover
different aspects of the domain and should, in-
deed, identify the domain implicitly, ii) at least
10,000 results should be returned by the search
engine when querying it with the seeds plus the
glossaryKeyword (see step (2) of GlossBoot).
The seed selection was not fine-tuned (i.e., it was
not adjusted to improve performance), so it might
well be that better seeds would provide better
results (see, e.g., (Kozareva and Hovy, 2010b)).
However, this type of consideration is beyond the
scope of this paper.
3.2.1 Evaluation measures
We performed experiments to evaluate the quality
of both terms and glosses, as jointly extracted by
GlossBoot.
Terms. For each domain and language we cal-
culated coverage, extra-coverage and precision of
the acquired terms T . Coverage is the ratio of ex-
tracted terms in T also contained in the gold stan-
dard T? to the size of T? . Extra-coverage is calcu-
lated as the ratio of the additional extracted terms
in T \ T? over the number of gold standard terms
T? . Finally, precision is the ratio of extracted terms
in T deemed to be within the domain. To calcu-
late precision we randomly sampled 5% of the re-
trieved terms and asked two human annotators to
manually tag their domain pertinence (with adju-
dication in case of disagreement; ? = .62, indicat-
ing substantial agreement). Note that by sampling
on the entire set T , we calculate the precision of
both terms in T ? T? , i.e., in the gold standard, and
terms in T \ T? , i.e., not in the gold standard, which
are not necessarily outside the domain.
Glosses. We calculated the precision of the ex-
tracted glosses as the ratio of glosses which were
both well-formed textual definitions and specific
Botany Comput. Environ. Finance
EN
Gold std. terms 772 421 713 1777
GlossBoot terms 5598 3738 4120 5294
glosses 11663 4245 5127 6703
FR
Gold std. terms 662 278 117 109
GlossBoot terms 3450 3462 1941 1486
glosses 5649 3812 2095 1692
IT
Gold std. terms 205 244 450 441
GlossBoot terms 1965 3356 1630 3601
glosses 2678 5891 1759 5276
Table 4: Size of the gold-standard and
automatically-acquired glossaries for the four
domains in the three languages of interest.
to the target domain. Precision was determined on
a random sample of 5% of the acquired glosses for
each domain and language. The annotation was
made by two annotators, with ? = .675, indicat-
ing substantial agreement.
3.3 Parameter tuning
We tuned the minimum and maximum length of
both pL and pR (see step (3) of GlossBoot) and
the threshold ? that we use to filter out non-domain
glosses (see step (4) of GlossBoot) using an extra
domain, i.e., the Arts domain. To do this, we cre-
ated a development dataset made up of the full set
of 394 terms from the Tate Gallery glossary,10 and
bootstrapped our glossary extraction method with
just one seed, i.e., (fresco, painting). We chose an
optimal value of ? = 0.1 on the basis of a har-
monic mean of coverage and precision. Note that,
since precision also concerns terms not in the gold
standard, we had to manually validate a sample of
the extracted terms for each of the 21 tested values
of ? ? {0, 0.05, 0.1, . . . , 1.0}.
4 Results and Discussion
4.1 Terms
The size of the extracted terminologies for the four
domains after five iterations are reported in Table
4. In Table 5 we show examples of the possi-
ble scenarios for terms: in-domain extracted terms
10http://www.tate.org.uk/collections/glossary/
532
In-domain In-domain Out-of-domain In-domain
(in gold std, ? T? ? T ) (not in gold std, ? T \ T? ) (not in gold std, ? T \ T? ) (missed, ? T? \ T )
Computing software, inheritance, mi-
croprocessor
clipboard, even parity, su-
doer
gs1-128 label, grayscale,
quantum dots
openwindows, sun mi-
crosystems, hardwired
Botany pollinium, stigma, spore vegetation, dichogamous,
fertilisation
ion, free radicals, mana-
mana
nomenclature, endemism,
insectivorous
Environment carcinogen, footprint, solar
power
frigid soil, biosafety, fire
simulator
epidermis, science park,
alum
g8, best practice,
polystyrene
Finance cash, bond, portfolio trustor, naked option, mar-
ket price
precedent, immigration,
heavy industry
co-location, petrodollars,
euronext
Table 5: Examples of extracted (and missed) terms.
Botany Comput. Environ. Finance
EN
Precision 95% 98% 94% 98%
Coverage 85% 40% 35% 32%
Extra-coverage 640% 848% 542% 266%
FR
Precision 80% 97% 83% 98%
Coverage 97% 27% 14% 26%
Extra-coverage 425% 1219% 1646% 1350%
IT
Precision 89% 98% 76% 99%
Coverage 42% 27% 11% 73%
Extra-coverage 511% 1349% 356% 746%
Table 6: Precision, coverage and extra-coverage of
the term extraction phase after 5 iterations.
which are also found in the gold standard (col-
umn 2), in-domain extracted terms but not in the
gold standard (column 3), out-of-domain extracted
terms (column 4), and domain terms in the gold
standard but not extracted by our approach (col-
umn 5).
A quantitative evaluation is provided in Table
6, which shows the percentage results in terms of
precision, coverage, and extra-coverage after 5 it-
erations of GlossBoot. For the English language
we observe good coverage (between 32% and 40%
on three domains, with a high peak of 85% cover-
age on Botany) and generally very high precision
values. Moreover for the French and the Italian
languages we observe a peak in the Botany and Fi-
nance domains respectively, while the lowest per-
formances in terms of precision and coverage are
observed for Environment, i.e., the most interdis-
ciplinary domain.
In all three languages GlossBoot provides very
high extra coverage of domain terms, i.e., addi-
tional terms which are not in the gold standard but
are returned by our system. The figures, shown in
Table 6, range between 266% (4726/1777) for the
English Finance domain and 1646% (1926/117)
for the French Environment domain. These re-
sults, together with the generally high precision
values, indicate the larger extent of our boot-
strapped glossaries compared to our gold stan-
dards.
Botany Computing Environm. Finance
Min Max Min Max Min Max Min Max
26% 68% 8% 39% 5% 33% 14% 30%
Table 7: Coverage ranges for single-seed term ex-
traction for the English language.
Number of seeds. Although the choice of se-
lecting five hypernymy relation seeds is quite arbi-
trary, it shows that we can acquire a reliable termi-
nology with minimal human intervention. Now, an
obvious question arises: what if we bootstrapped
GlossBoot with fewer hypernym seeds, e.g., just
one seed? To answer this question we replicated
our English experiments on each single (term, hy-
pernym) pair in our seed set. In Table 7 we show
the coverage ranges ? i.e., the minimum and max-
imum coverage values ? for the five seeds on each
domain. We observe that the maximum coverage
can attain values very close to those obtained with
five seeds. However, the minimum coverage val-
ues are much lower. So, if we adopt a 1-seed boot-
strapping policy there is a high risk of acquiring
a poorer terminology unless we select the single
seed very carefully, whereas we have shown that
just a few seeds can cope with domain variabil-
ity. Similar considerations can be made regarding
different seed set sizes (we also tried 2, 3 and 4).
So five is not a magic number, just one which can
guarantee an adequate coverage of the domain.
Number of iterations. In order to study the cov-
erage trend over iterations we selected 5 seeds for
our tuning domain (i.e., Arts, see Section 3.3).
Figure 3 shows the size (left graph), coverage,
extra-coverage and precision (middle graph) of the
acquired glossary after each iteration, from 1 to
20. As expected, (extra-)coverage grows over iter-
ations, while precision drops. Stopping at iteration
5, as we do, is optimal in terms of the harmonic
mean of precision and coverage (right graph in
Figure 3).
533
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 2  4  6  8  10  12  14  16  18  20
iteration
Number of terms and glosses extracted over iterations
termsglosses 10%
100%
1000%
 2  4  6  8  10  12  14  16  18  20
iteration
Coverage, extra-coverage and precision over iterations
precisioncoverageextra-coverage 30%
32%
34%
36%
38%
40%
 2  4  6  8  10  12  14  16  18  20
iteration
Harmonic mean of precision and coverage over iterations
harmonic mean of precision and coverage
Figure 3: Size, coverage and precision trends for Arts (tuning domain) over 20 iterations for English.
Botany Comput. Environm. Finance
EN 96% 94% 97% 97%
FR 88% 89% 88% 95%
IT 94% 98% 83% 99%
Table 8: Precision of the glosses for the four do-
mains and for the three languages.
4.2 Glosses
We show the results of gloss evaluation in Ta-
ble 8. Precision ranges between 83% and 99%,
with three domains performing above 92% on av-
erage across languages, and the Environment do-
main performing relatively worse because of its
highly interdisciplinary nature (89% on average).
We observe that these results are strongly corre-
lated with the precision of the extracted terms (cf.
Table 6), because the retrieved glosses of domain
terms are usually in-domain too, and follow a def-
initional style because they come from glossaries.
Note, however, that the gloss precision can also
be higher than term precision, because many perti-
nent glosses might be extracted for the same term,
cf. Table 4.
5 Comparative Evaluation
5.1 Comparison with Google Define
We performed a comparison with Google De-
fine,11 a state-of-the-art definition search service.
This service inputs a term query and outputs a list
of glosses. First, we randomly sampled 100 terms
from our gold standard for each domain and each
of the three languages. Next, for each domain and
language, we manually calculated the fraction of
terms for which an in-domain definition was pro-
vided by Google Define and GlossBoot. Table 9
shows the coverage results.
Google Define outperforms our system on all
four domains (with a few exceptions). However
11Accessible from Google search by means of the
define: keyword.
Botany Comput. Environm. Finance
EN Google Define 90% 87% 84% 82%GlossBoot 77% 47% 44% 51%
FR Google Define 40% 48% 36% 82%GlossBoot 88% 42% 22% 32%
IT Google Define 52% 74% 78% 80%GlossBoot 64% 38% 44% 92%
Table 9: Number of domain glosses (from a ran-
dom sample of 100 gold standard terms per do-
main) retrieved using Google Define and Gloss-
Boot.
we note that Google Define: i) requires knowing
the domain term to be defined in advance, whereas
we jointly acquire thousands of terms and glosses
starting from just a few seeds; ii) does not discrim-
inate between glosses pertaining to the target do-
main and glosses concerning other fields or senses,
whereas we extract domain-specific glosses.
5.2 Comparison with TaxoLearn
We also compared GlossBoot with a recent ap-
proach to glossary learning embedded into a
framework for graph-based taxonomy learning
from scratch, called TaxoLearn (Navigli et al,
2011). Since this approach requires the manual
selection of a domain corpus to automatically ex-
tract terms and glosses, we decided to keep a level
playing field and experimented with the same do-
main used by the authors, i.e., Artificial Intelli-
gence (AI). TaxoLearn was applied to the entire
set of IJCAI 2009 proceedings, resulting in the ex-
traction of 427 terms and 834 glosses.12 As re-
gards GlossBoot, we selected 10 seeds to cover all
the fields of AI, obtaining 5827 terms and 6716
glosses after 5 iterations, one order of magnitude
greater than TaxoLearn.
As for the precision of the extracted terms, we
randomly sampled 50% of them for each system.
We show in Table 10 (first row) the estimated term
12Available at: http://lcl.uniroma1.it/taxolearn
534
GlossBoot TaxoLearn
Term Precision 82.3% (2398/2913) 77.0% (164/213)
Gloss Precision 82.8% (2780/3358) 78.9% (329/417)
Table 10: Estimated term and gloss precision of
GlossBoot and TaxoLearn for the Artificial Intel-
ligence domain.
precision for GlossBoot and TaxoLearn. The pre-
cision value for GlossBoot is lower than the preci-
sion values of the four domains in Table 6, due
to the AI domain being highly interdisciplinary.
TaxoLearn obtained a lower precision because it
acquires a full-fledged taxonomy for the domain,
thus also including higher-level concepts which do
not necessarily pertain to the domain.
We performed a similar evaluation for the pre-
cision of the acquired glosses, by randomly sam-
pling 50% of them for each system. We show in
Table 10 (second row) the estimated gloss preci-
sion of GlossBoot and TaxoLearn. Again, Gloss-
Boot outperforms TaxoLearn, retrieving a larger
amount of glosses (6716 vs. 834) with higher pre-
cision. We remark, however, that in TaxoLearn
glossary extraction is a by-product of the taxon-
omy learning process.
Finally, we note that we cannot compare with
approaches based on lexical patterns (such as
(Kozareva and Hovy, 2010a)), because they are
not aimed at learning glossaries, but just at re-
trieving sentence snippets which contain pairs of
terms/hypernyms (e.g., ?supervised systems such
as decision trees?).
6 Related Work
There are several techniques in the literature for
the automated acquisition of definitional knowl-
edge. Fujii and Ishikawa (2000) use an n-gram
model to determine the definitional nature of text
fragments, whereas Klavans and Muresan (2001)
apply pattern matching techniques at the lexical
level guided by cue phrases such as ?is called?
and ?is defined as?. Cafarella et al (2005) de-
veloped a Web search engine which handles more
general and complex patterns like ?cities such as
ProperNoun(Head(NP ))? in which it is possi-
ble to constrain the results with syntactic proper-
ties. More recently, a domain-independent super-
vised approach was presented which learns Word-
Class Lattices (WCLs), i.e. lattice-based definition
classifiers that are applied to candidate sentences
containing the input terms (Navigli and Velardi,
2010). WCLs have been shown to perform with
high precision in several domains (Velardi et al,
2013).
To avoid the burden of manually creating a
training dataset, definitional patterns can be ex-
tracted automatically. Reiplinger et al (2012) ex-
perimented with two different approaches for the
acquisition of lexical-syntactic patterns. The first
approach involves bootstrapping patterns from a
domain corpus, and then manually refining the ac-
quired patterns. The second approach, instead,
involves automatically acquiring definitional sen-
tences by using a more sophisticated syntactic and
semantic processing. The results shows high pre-
cision in both cases.
However, these approaches to glossary learning
extract unrestricted textual definitions from open
text. In order to filter out non-domain definitions,
Velardi et al (2008) automatically extract a do-
main terminology from an input corpus which they
later use for assigning a domain score to each har-
vested definition and filtering out non-domain can-
didates. The extraction of domain terms from cor-
pora can be performed either by means of statis-
tical measures such as specificity and cohesion
(Park et al, 2002), or just TF*IDF (Kim et al,
2009).
To avoid the use of a large domain corpus, ter-
minologies can be obtained from the Web by using
Doubly-Anchored Patterns (DAPs) which, given a
(term, hypernym) pair, harvest sentences match-
ing manually-defined patterns like ?<hypernym>
such as <term>, and *? (Kozareva et al, 2008).
Kozareva and Hovy (2010a) further extend this
term extraction process by harvesting new hy-
pernyms using the corresponding inverse patterns
(called DAP?1) like ?* such as <term1>, and
<term2>?. Similarly to our approach, they drop
the requirement of a domain corpus and start
from a small number of (term, hypernym) seeds.
However, while Doubly-Anchored Patterns have
proven useful in the induction of domain tax-
onomies (Kozareva and Hovy, 2010a), they cannot
be applied to the glossary learning task, because
the extracted sentences are not formal definitions.
In contrast, GlossBoot performs the novel task
of multilingual glossary learning from the Web by
bootstrapping the extraction process with a few
(term, hypernym) seeds. Bootstrapping techniques
(Brin, 1998; Agichtein and Gravano, 2000; Pas?ca
et al, 2006) have been successfully applied to
several tasks, including high-precision semantic
lexicon extraction from large corpora (Riloff and
Jones, 1999; Thelen and Riloff, 2002; McIntosh
535
Domain Term Gloss
EN
Botany deciduous losing foliage at the end of the growing season.
Computing information space The abstract concept of everything accessible using networks: the Web.
Finance discount The difference between the lower price paid for a security and the security?s
face amount at issue.
FR
Botany insectivore Qui capture des insectes et en absorbe les matie`res nutritives.
Computing notebook C?est l?appellation d?un petit portable d?une taille proche d?une feuille A4.
Environment e?cosyste`me Ensemble des e?tres vivants et des e?le?ments non vivants d?un milieu qui sont
lie?s vitalement entre eux.
IT
Computing link Collegamento tra diverse pagine web, puo` essere costituito da immagini o
testo.
Environment effetto serra Riscaldamento dell?atmosfera terrestre dovuto alla presenza di gas
nell?atmosfera (anidride carbonica, metano e vapore acqueo) che osta-
colano l?uscita delle radiazioni infrarosse emesse dal suolo terreste verso
l?alto.
Finance spread Indica la differenza tra la quotazione di acquisto e quella di vendita.
Table 11: An excerpt of the domain glossaries acquired for the three languages.
and Curran, 2008; McIntosh and Curran, 2009),
learning semantic relations (Pantel and Pennac-
chiotti, 2006), extracting surface text patterns for
open-domain question answering (Ravichandran
and Hovy, 2002), semantic tagging (Huang and
Riloff, 2010) and unsupervised Word Sense Dis-
ambiguation (Yarowsky, 1995). By exploiting the
(term, hypernym) seeds to bootstrap the itera-
tive acquisition of extraction patterns from Web
glossary pages, we can cover the high variabil-
ity of textual definitions, including both sentences
matching the above-mentioned lexico-syntactic
patterns (e.g., ?a corpus is a collection of docu-
ments?) and glossary-style definitions (e.g., ?cor-
pus: a collection of document?) independently of
the target domain and language.
7 Conclusions
In this paper we have presented GlossBoot, a
new, minimally-supervised approach to multilin-
gual glossary learning. Starting from a few hyper-
nymy relation seeds which implicitly identify the
domain of interest, we apply a bootstrapping ap-
proach which iteratively obtains HTML patterns
from Web glossaries and then applies them to the
extraction of term/gloss pairs. To our knowledge,
GlossBoot is the first approach to large-scale glos-
sary learning which jointly acquires thousands of
terms and glosses for a target domain and language
with minimal supervision.
The gist of GlossBoot is our glossary bootstrap-
ping approach, thanks to which we can drop the
requirements of existing techniques such as the
availability of domain text corpora, which often
do not contain enough definitions, and the man-
ual specification of lexical patterns, which typi-
cally extract sentence snippets, instead of formal
glosses.
GlossBoot will be made available to the re-
search community as open-source software. Be-
yond the immediate usability of its output and
its effective use for domain Word Sense Disam-
biguation (Faralli and Navigli, 2012), we wish
to show the benefit of GlossBoot in gloss-driven
approaches to ontology learning (Navigli et al,
2011; Velardi et al, 2013) and semantic network
enrichment (Navigli and Ponzetto, 2012). In Ta-
ble 11 we show an excerpt of the acquired glos-
saries. All the glossaries and gold standards cre-
ated for our experiments are available from the au-
thors? Web site http://lcl.uniroma1.it/
glossboot/.
We remark that the terminologies covered with
GlossBoot are not only precise, but also one or-
der of magnitude greater than those covered in
individual online glossaries. As future work we
plan to study the ability of GlossBoot to acquire
domain glossaries at different levels of specificity
(i.e., domains vs. subdomains). We also plan to
exploit the acquired HTML patterns for imple-
menting an open-source glossary crawler, along
the lines of Google Define.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
536
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM confer-
ence on Digital Libraries, pages 85?94, San Anto-
nio, Texas, USA.
Sergey Brin. 1998. Extracting patterns and relations
from the World Wide Web. In Proceedings of the
International Workshop on The World Wide Web and
Databases, pages 172?183, London, UK.
Michael J. Cafarella, Doug Downey, Stephen Soder-
land, and Oren Etzioni. 2005. KnowItNow: Fast,
scalable information extraction from the web. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 563?570, Van-
couver, British Columbia, Canada.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007.
Soft pattern matching models for definitional ques-
tion answering. ACM Transactions on Information
Systems, 25(2):1?30.
Weisi Duan and Alexander Yates. 2010. Extracting
glosses to disambiguate word senses. In Proceed-
ings of Human Language Technologies: The 11th
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 627?635, Los Angeles, CA, USA.
Stefano Faralli and Roberto Navigli. 2012. A
New Minimally-supervised Framework for Domain
Word Sense Disambiguation. In Proceedings of
the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 1411?
1422, Jeju, Korea.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the World Wide Web as an encyclopedia: extract-
ing term descriptions from semi-structured texts. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, pages 488?495,
Hong Kong.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 539?545, Nantes, France.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2?27.
Ruihong Huang and Ellen Riloff. 2010. Induc-
ing domain-specific semantic class taggers from (al-
most) nothing. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 275?285, Uppsala, Sweden.
Su Nam Kim, Timothy Baldwin, and Min-Yen Kan.
2009. An unsupervised approach to domain-specific
term extraction. In Proceedings of the Australasian
Language Technology Workshop, pages 94?98, Syd-
ney, Australia.
Judith Klavans and Smaranda Muresan. 2001. Evalu-
ation of the DEFINDER system for fully automatic
glossary construction. In Proceedings of the Amer-
ican Medical Informatics Association (AMIA) Sym-
posium, pages 324?328, Washington, D.C., USA.
Zornitsa Kozareva and Eduard Hovy. 2010a. A
semi-supervised method to learn and construct tax-
onomies using the Web. In Proceedings of Empiri-
cal Methods in Natural Language Processing, pages
1110?1118, Cambridge, MA, USA.
Zornitsa Kozareva and Eduard H. Hovy. 2010b. Not
all seeds are equal: Measuring the quality of text
mining seeds. In Proceedings of Human Lan-
guage Technologies: The 11th Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 618?626, Los
Angeles, California, USA.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the Web with
hyponym pattern linkage graphs. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics, pages 1048?1056, Colum-
bus, Ohio, USA.
Tara McIntosh and James R. Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceed-
ings of the Australasian Language Technology Asso-
ciation Workshop, pages 97?105, CSIRO ICT Cen-
tre, Tasmania.
Tara McIntosh and James R. Curran. 2009. Reducing
semantic drift with bagging and distributional sim-
ilarity. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 396?404, Suntec,
Singapore.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Roberto Navigli and Paola Velardi. 2010. Learning
Word-Class Lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1318?1327, Uppsala, Sweden.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexi-
cal taxonomies from scratch. In Proceedings of the
22th International Joint Conference on Artificial In-
telligence, pages 1872?1877, Barcelona, Spain.
537
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006. Names and similari-
ties on the web: Fact extraction in the fast lane. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 809?816, Sydney, Australia.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging Generic Patterns for Auto-
matically Harvesting Semantic Relations. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL), Sydney, Australia, pages 113?120,
Sydney, Australia.
Youngja Park, Roy J. Byrd, and Branimir K. Boguraev.
2002. Automatic glossary extraction: beyond termi-
nology identification. In Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics, pages 1?7, Taipei, Taiwan.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
41?47, Philadelphia, Pennsylvania.
Melanie Reiplinger, Ulrich Scha?fer, and Magdalena
Wolska. 2012. Extracting glossary sentences from
scholarly articles: A comparative evaluation of pat-
tern bootstrapping and deep analysis. In Proceed-
ings of the ACL-2012 Special Workshop on Redis-
covering 50 Years of Discoveries, pages 55?65, Jeju
Island, Korea.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In Proceedings of the sixteenth na-
tional conference on Artificial intelligence and the
eleventh Innovative applications of artificial intelli-
gence conference, pages 474?479, Menlo Park, CA,
USA.
Horacio Saggion. 2004. Identifying definitions in text
collections for question answering. In Proceedings
of the Fourth International Conference on Language
Resources and Evaluation, pages 1927?1930, Lis-
bon, Portugal.
Michael Thelen and Ellen Riloff. 2002. A bootstrap-
ping method for learning semantic lexicons using
extraction pattern contexts. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 214?221, Salt Lake City,
UT, USA.
Paola Velardi, Roberto Navigli, and Pierluigi
D?Amadio. 2008. Mining the Web to create
specialized glossaries. IEEE Intelligent Systems,
23(5):18?25.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. OntoLearn Reloaded: A graph-based algo-
rithm for taxonomy induction. Computational Lin-
guistics, 39(3).
David Yarowsky. 1995. Unsupervised Word Sense
Disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 189?
196, Cambridge, MA, USA.
538
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 103?108,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Java Framework for Multilingual Definition and Hypernym Extraction
Stefano Faralli and Roberto Navigli
Dipartimento di Informatica
Sapienza Universita` di Roma
{faralli,navigli}@di.uniroma1.it
Abstract
In this paper we present a demonstra-
tion of a multilingual generalization of
Word-Class Lattices (WCLs), a super-
vised lattice-based model used to identify
textual definitions and extract hypernyms
from them. Lattices are learned from a
dataset of automatically-annotated defini-
tions from Wikipedia. We release a Java
API for the programmatic use of multilin-
gual WCLs in three languages (English,
French and Italian), as well as a Web ap-
plication for definition and hypernym ex-
traction from user-provided sentences.
1 Introduction
Electronic dictionaries and domain glossaries are
definition repositories which prove very useful not
only for lookup purposes, but also for automatic
tasks such as Question Answering (Cui et al,
2007; Saggion, 2004), taxonomy learning (Navigli
et al, 2011; Velardi et al, 2013), domain Word
Sense Disambiguation (Duan and Yates, 2010;
Faralli and Navigli, 2012), automatic acquisition
of semantic predicates (Flati and Navigli, 2013),
relation extraction (Yap and Baldwin, 2009) and,
more in general, knowledge acquisition (Hovy et
al., 2013). Unfortunately, constructing and updat-
ing such resources requires the effort of a team of
experts. Moreover, they are of no help when deal-
ing with new words or usages, or, even worse, new
domains. Nonetheless, raw text often contains
several definitional sentences, that is, it provides
within itself formal explanations for terms of inter-
est. Whilst it is not feasible to search texts manu-
ally for definitions in several languages, the task of
extracting definitional information can be autom-
atized by means of Machine Learning (ML) and
Natural Language Processing (NLP) techniques.
Many approaches (Snow et al, 2004; Kozareva
and Hovy, 2010, inter alia) build upon lexico-
syntactic patterns, inspired by the seminal work
of Hearst (1992). However, these methods suf-
fer from two signifiicant drawbacks: on the one
hand, low recall (as definitional sentences occur in
highly variable syntactic structures), and, on the
other hand, noise (because the most frequent def-
initional pattern ? X is a Y ? is inherently very
noisy). A recent approach to definition and hyper-
nym extraction, called Word-Class Lattices (Nav-
igli and Velardi, 2010, WCLs), overcomes these
issues by addressing the variability of definitional
sentences and providing a flexible way of automat-
ically extracting hypernyms from them. To do so,
lattice-based classifiers are learned from a training
set of textual definitions. Training sentences are
automatically clustered by similarity and, for each
such cluster, a lattice classifier is learned which
models the variants of the definition template de-
tected. A lattice is a directed acyclic graph, a
subclass of non-deterministic finite state automata.
The purpose of the lattice structure is to preserve
(in a compact form) the salient differences among
distinct sequences.
In this paper we present a demonstration of
Word-Class Lattices by providing a Java API and
a Web application for online usage. Since multi-
linguality is a key need in today?s information so-
ciety, and because WCLs have been tested over-
whelmingly only with the English language, we
provide experiments for three different languages,
namely English, French and Italian. To do so, in
contrast to Navigli and Velardi (2010), who cre-
ated a manually annotated training set of defini-
tions, we provide a heuristic method for the au-
tomatic acquisition of reliable training sets from
Wikipedia, and use them to determine the robust-
ness and generalization power of WCLs. We show
high performance in definition and hypernym ex-
traction for our three languages.
2 Word-Class Lattices
In this section we briefly summarize Word-Class
Lattices, originally introduced by Navigli and Ve-
lardi (2010).
2.1 Definitional Sentence Generalization
WCL relies on a formal notion of textual defi-
nition. Specifically, given a definition, e.g.: ?In
computer science, a closure is a first-class func-
tion with free variables that are bound in the lex-
ical environment?, we assume that it contains the
103
[In geography, a country]DF [is]V F [a political division]GF .
[In finance, a bond]DF [is]V F [a negotiable certificate]GF [that that acknowledges. . . ]REST .
[In poetry, a foot]DF [is]V F [a measure]GF [, consisting. . . ]REST .
Table 1: Example definitions (defined terms are marked in bold face, their hypernyms in italics).
In
geography
finance
poetry
NN1 , a ?TARGET?
foot
bond
country
a
political
negotiable
JJ NN2
division
certificate
measure
Figure 1: The DF and GF Word-Class Lattices for the sentences in Table 1.
following fields (Storrer and Wellinghoff, 2006):
definiendum (DF), definitor (VF), definiens (GF)
and rest (REST), where DF is the part of the
definition including the word being defined (e.g.,
?In computer science, a closure?), VF is the verb
phrase used to introduce the definition (e.g., ?is?),
GF usually includes the hypernym (e.g., ?a first-
class function?, hypernym marked in italics) and
RF includes additional clauses (e.g., ?with free
variables that are bound in the lexical environ-
ment?).
Consider a set of training sentences T , each
of which is automatically part-of-speech tagged
and manually bracketed with the DF, VF, GF and
REST fields (examples are shown in Table 1). We
first identify the set of most frequent words F
(e.g., the, a, is, of, refer, etc.). Then we add
the symbol ?TARGET? to F and replace in T the
terms being defined with ?TARGET?. We then use
the set of frequent words F to generalize words to
?word classes?.
We define a word class as either a word itself
or its part of speech. Given a sentence s =
w1, w2, . . . , w|s|, where wi is the i-th word of s,we generalize its words wi to word classes ?i as
follows:
?i =
{
wi if wi ? F
POS(wi) otherwise
that is, a word wi is left unchanged if it occurs fre-
quently in the training corpus (i.e., wi ? F ) or is
transformed to its part of speech tag (POS(wi))
otherwise. As a result, we obtain a generalized
sentence s? = ?1, ?2, . . . , ?|s|. For instance,given the first sentence in Table 1, we obtain the
corresponding generalized sentence: ?In NN, a
?TARGET? is a JJ NN?, where NN and JJ indicate
the noun and adjective classes, respectively.
2.2 Learning
The WCL learning algorithm consists of 3 steps:
? Star patterns: each sentence in the training
set is preprocessed and generalized to a star
pattern by replacing with * all the words wi 6?
F , i.e., non-frequent words. For instance, ?In
geography, a country is a political division?
is transformed to ?In *, a ?TARGET? is a *?;
? Sentence clustering: the training sentences
are then clustered based on the star patterns
they belong to;
? Word-Class Lattice construction: for each
sentence cluster, a WCL is created separately
for each DF, VF and GF field by means of a
greedy alignment algorithm. In Figure 1 we
show the resulting lattices for the DF and GF
fields built for the cluster of sentences of Ta-
ble 1. Note that during the construction of the
lattice the nodes associated with the hyper-
nym words in the learning sentences (i.e., di-
vision, certificate and measure) are marked as
hypernyms in order to determine the hyper-
nym of a test sentence at classification time
(see (Navigli and Velardi, 2010) for details).
2.3 Classification
Once the learning process is over, a set of WCLs
is produced for the DF, VF and GF fields. Given
a test sentence s, we consider all possible combi-
nations of definiendum, definitor and definiens lat-
tices and select the combination of the three WCLs
that best fits the sentence, if such a combination
exists. In fact, choosing the most appropriate
combination of lattices impacts the performance
of hypernym extraction. The best combination
of WCLs is selected by maximizing the follow-
ing confidence score: score(s, lDF , lV F , lGF ) =
coverage ? log(support+1) where s is the candi-
date sentence, lDF , lV F and lGF are three latticesone for each definition field, coverage is the frac-
tion of words of the input sentence covered by the
three lattices, and support is the sum of the num-
ber of sentences in the star patterns corresponding
to the GF lattice. Finally, when a sentence is clas-
sified as a definition, its hypernym is extracted by
104
# Wikipedia pages # definitions extracted
English (EN) 3,904,360 1,552,493
French (FR) 1,617,359 447,772
Italian (IT) 1,008,044 291,259
Table 2: The number of Wikipedia pages and the
resulting automatically annotated definitions.
selecting the words in the input sentence that are
marked as hypernyms in the WCL selected for GF.
3 Multilingual Word-Class Lattices
In order to enable multilinguality, thereby extract-
ing definitions and hypernyms in many languages,
we provide here a heuristic method for the creation
of multilingual training datasets from Wikipedia,
that we apply to three languages: English, French
and Italian. As a result, we are able to fully au-
tomatize the definition and hypernym extraction
by utilizing collaboratively-curated encyclopedia
content.
3.1 Automatic Learning of Multilingual
WCLs
The method consists of four steps:
1. candidate definition extraction: we iterate
through the collection of Wikipedia pages for
the language of interest. For each article we
extract the first paragraph, which usually, but
not always, contains a definitional sentence
for the concept expressed by the page title.
We discard all those pages for which the title
corresponds to a special page (i.e., title in the
form ?List of [. . . ]?, ?Index of [. . . ]?, ?[. . . ]
(disambiguation)? etc.).
2. part-of-speech tagging and phrase chunk-
ing: for each candidate definition we per-
form part-of-speech tagging and chunking,
thus automatically identifying noun, verb,
and prepositional phrases (we use TreeTag-
ger (Schmid, 1997)).
3. automatic annotation: we replace all the oc-
currences in the candidate definition of the
target term (i.e., the title of the page) with
the marker ?TARGET?, we then tag as hyper-
nym the words associated with the first hy-
perlink occurring to the right of ?TARGET?.
Then we tag as VF (i.e., definitor field,
see Section 2.1) the verb phrase found be-
tween ?TARGET? and the hypernym, if such
a phrase exists. Next we tag as GF (i.e.,
definiens field) the phrase which contains the
hypernym and as DF (i.e., definiendum field)
the phrase which starts at the beginning of
the sentence and ends right before the start
of the VP tag. Finally we mark as REST the
remaining phrases after the phrase already
tagged as GF. For example, given the sen-
tence ?Albert Einstein was a German-born
theoretical physicist.?, we produce the fol-
lowing sentence annotation: ?[Albert Ein-
stein]DF [was]V F [a German-born theoreti-cal physicist]GF .? (target term marked inbold and hypernym in italics).
4. filtering: we finally discard all the candidate
definitions for which not all fields could be
found during the previous step (i.e., either the
?TARGET?, hypernym or any DF, VF, GF,
REST tag is missing).
We applied the above four steps to the En-
glish, French and Italian dumps of Wikipedia1.
The numbers are shown in Table 2: starting with
3,904,360 Wikipedia pages for English, 1,617,359
for French and 1,008,044 for Italian (first column),
we obtained 1,552,493, 447,772, and 291,259 au-
tomatically tagged sentences, respectively, for the
three languages (second column in the Table).
Since we next had to use these sentences for train-
ing our WCLs, we took out a random sample
of 1000 sentences for each language which we
used for testing purposes. We manually annotated
each of these sentences as definitional or non-
definitional2 and, in the case of the former, also
with the correct hypernym.
3.2 Evaluation
We tested the newly acquired training dataset
against two test datasets. The first dataset was
our random sampling of 1000 Wikipedia test sen-
tences which we had set aside for each language
(no intersection with the training set, see previous
section). The second dataset was the same one
used in Navigli and Velardi (2010), made up of
sentences from the ukWaC Web corpus (Ferraresi
et al, 2008) and used to estimate the definition and
hypernym extraction performance on an open text
corpus.
3.3 Results
Table 3 shows the results obtained on definition
(column 2-4) and hypernym extraction (column 5-
7) in terms of precision (P), recall (R) and accu-
racy (A) on our first dataset. Note that accuracy
also takes into account candidate definitions in
the test set which were tagged as non-definitional
(see Section 3.1). In the Table we compare the
performance of our English WCL trained from
Wikipedia sentences using our automatic proce-
dure against the original performance of WCL
1We used the 21-09-2012 (EN), 17-09-2012 (FR), 21-09-
2012 (IT) dumps.
2Note that the first sentence of a Wikipedia page might
seldom be non-definitional, such as ?Basmo fortress is lo-
cated in the north-western part . . . ?.
105
Definition Extraction Hypernym Extraction
P R A P R A
EN 98.5 78.3 81.0 98.5 77.4 80.0
FR 98.7 83.3 84.0 98.6 78.0 79.0
IT 98.8 87.3 87.0 98.7 83.2 83.0
EN (2010) 100.0 59.0 66.0 100.0 58.3 65.0
Table 3: Precision (P), recall (R) and accuracy
(A) of definition and hypernym extraction when
testing on our dataset of 1000 randomly sam-
pled Wikipedia first-paragraph sentences. EN
(2010) refers to the WCL learned from the origi-
nal manually-curated training set from Navigli and
Velardi (2010), while EN, FR and IT refer to WCL
trained, respectively, with one of the three training
sets automatically acquired from Wikipedia.
P R
EN 98.9 57.6
EN (2010) 94.8 56.5
Table 4: Estimated WCL definition extraction
precision (P) and recall (R), testing a sample of
ukWaC sentences.
trained on 1,908 manually-selected training sen-
tences3. It can be seen that the automatically ac-
quired training set considerably improves the per-
formance, as it covers higher variability. We note
that the recall in both definition and hypernym ex-
traction is higher for French and Italian. We at-
tribute this behavior to the higher complexity and,
again, variability of English Wikipedia pages, and
specifically first-sentence definitions. We remark
that the presented results were obtained without
any human effort, except for the independent col-
laborative editing and hyperlinking of Wikipedia
pages, and that the overall performances can be
improved by manually checking the automatically
annotated training datasets.
We also replicated the experiment carried out
by Navigli and Velardi (2010), testing WCLs with
a subset (over 300,000 sentences) of the ukWaC
Web corpus. As can be seen in Table 4, the
estimated precision and recall for WCL defini-
tion extraction with the 2010 training set were
94.8% and 56.5%, respectively, while with our au-
tomatically acquired English training set we ob-
tained a higher precision of 98.9% and a recall of
57.6%. This second experiment shows that learn-
ing WCLs from hundreds of thousands of defini-
tion candidates does not overfit to Wikipedia-style
definitional sentences.
After looking at the automatically acquired
training datasets, we noted some erroneous an-
notations mainly due to the following factors: i)
some Wikipedia pages do not start with defini-
3Available from http://lcl.uniroma1.it/wcl
1 // select the language of interest
2 Language targetLanguage = Language.EN;
3 // open the training set
4 Dataset ts = new AnnotatedDataset(
5 trainingDatasetFile,
6 targetLanguage);
7 // obtain an instance of the WCL classifier
8 WCLClassifier c = new WCLClassifier(targetLanguage);
9 c.train(ts);
10 // create a sentence to be tested
11 Sentence sentence = Sentence.createFromString(
12 "WCL",
13 "WCL is a kind of classifier.",
14 targetLanguage);
15 // test the sentence
16 SentenceAnnotation sa = c.test(sentence);
17 // print the hypernym
18 if (sa.isDefinition())
19 System.out.println(sa.getHyper());
Figure 2: An example of WCL API usage.
tional sentences; ii) they may contain more than
one verbal phrase between the defined term and
the hypernym; iii) the first link after the verbal
phrase does not cover, or partially covers, the
correct hypernym. The elimination of the above
wrongly acquired definitional patterns can be im-
plemented with some language-dependent heuris-
tics or can be done by human annotators. In any
case, given the presence of a high number of cor-
rect annotated sentences, these wrong definitional
patterns have a very low impact on the definition
and hypernym extraction precision as shown in the
above experiments (see Table 3 and Table 4).
4 Multilingual WCL API
Together with the training and test sets of the
above experiments, we also release here our im-
plementation of Word-Class Lattices, available as
a Java API. As a result the WCL classifier can eas-
ily be used programmatically in any Java project.
In Figure 2 we show an example of the API usage.
After the selection of the target language (line 2),
we load the training dataset for the target language
(line 4). Then an instance of WCLClassifier is
created (line 8) and the training phase is launched
on the input training corpora (line 9). Now the
classifier is ready to be tested on any given sen-
tence in the target language (lines 11-16). If the
classifier output is positive (line 18) we can print
the extracted hypernym (line 19). The output of
the presented code is the string ?classifier? which
corresponds to the hypernym extracted by WCL
for the input sentence ?WCL is a kind of classi-
fier?.
4.1 Web user interface
We also release a Web interface to enable online
usage of our WCLs for the English, French and
Italian languages. In Figure 3 we show a screen-
shot of our Web interface. The user can type the
106
Figure 3: A screenshot of the WCL Web interface.
term of interest, the candidate definition, select
the language of interest and, after submission, in
the case of positive response from WCL, obtain
the corresponding hypernym and a graphical rep-
resentation of the lattices matching the given sen-
tence, as shown in the bottom part of the Figure.
The graphical representation shows the concate-
nation of the learned lattices which match the DF,
VF, GF parts of the given sentence (see Section
2). We also allow the user not to provide the term
of interest: in this case all the nouns in the sen-
tence are considered as candidate defined terms.
The Web user interface is part of a client-server ap-
plication, created with the JavaServer Pages tech-
nology. The server side produces an HTML page
(like the one shown in Figure 3), using the WCL
API (see Section 4) to process and test the submit-
ted definition candidate.
5 Related Work
A great deal of work is concerned with the lan-
guage independent extraction of definitions. Much
recent work uses symbolic methods that depend
on lexico-syntactic patterns or features, which are
manually created or semi-automatically learned as
recently done in (Zhang and Jiang, 2009; Wester-
hout, 2009). A fully automated method is, instead,
proposed by Borg et al (2009), where higher
performance (around 60-70% F1-measure) is ob-
tained only for specific domains and patterns. Ve-
lardi et al (2008), in order to improve precision
while keeping pattern generality, prune candidates
using more refined stylistic patterns and lexical fil-
ters. Cui et al (2007) propose the use of prob-
abilistic lexico-semantic patterns, for definitional
question answering in the TREC contest4. How-
ever, the TREC evaluation datasets cannot be con-
sidered true definitions, but rather text fragments
providing some relevant fact about a target term.
4Text REtrieval Conferences: http://trec.nist.
gov
Hypernym extraction methods vary from simple
lexical patterns (Hearst, 1992; Oakes, 2005) to sta-
tistical and machine learning techniques (Agirre
et al, 2000; Caraballo, 1999; Dolan et al, 1993;
Sanfilippo and Poznanski, 1992; Ritter et al,
2009). Extraction heuristics can be adopted in
many languages (De Benedictis et al, 2013),
where given a definitional sentence the hypernym
is identified as the first occuring noun after the
defined term. One of the highest-coverage meth-
ods is proposed by Snow et al (2004). They first
search sentences that contain two terms which are
known to be in a taxonomic relation (term pairs are
taken from WordNet (Miller et al, 1990)); then
they parse the sentences, and automatically ex-
tract patterns from the parse trees. Finally, they
train a hypernym classifier based on these features.
Lexico-syntactic patterns are generated for each
sentence relating a term to its hypernym, and a de-
pendency parser is used to represent them.
6 Conclusion
In this demonstration we provide three main con-
tributions: 1) a general method for obtaining large
training sets of annotated definitional sentences
for many languages from Wikipedia, thanks to
which we can release three new training sets for
English, French and Italian; 2) an API to program-
matically use WCLs in Java projects; 3) a Web ap-
plication which enables online use of multilingual
WCLs: http://lcl.uniroma1.it/wcl/.
Acknowledgments
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
107
References
Eneko Agirre, Olatz Ansa, Eduard H. Hovy, and David
Mart??nez. 2000. Enriching very large ontologies using the
WWW. In ECAI Workshop on Ontology Learning, Berlin,
Germany.
Claudia Borg, Mike Rosner, and Gordon Pace. 2009. Evo-
lutionary algorithms for definition extraction. In Proceed-
ings of the 1st Workshop on Definition Extraction, pages
26?32, Borovets, Bulgaria.
Sharon A. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics: Proceedings of the Confer-
ence, pages 120?126, Maryland, USA.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007. Soft pat-
tern matching models for definitional question answering.
ACM Transactions on Information Systems, 25(2):1?30.
Flavio De Benedictis, Stefano Faralli, and Roberto Navigli.
2013. GlossBoot: Bootstrapping Multilingual Domain
Glossaries from the Web. In Proceedings of 51st Annual
Meeting of the Association for Computational Linguistics,
Sofia, Bulgaria.
William Dolan, Lucy Vanderwende, and Stephen D. Richard-
son. 1993. Automatically deriving structured knowledge
bases from on-line dictionaries. In Proceedings of the
First Conference of the Pacific Association for Computa-
tional Linguistics, pages 5?14, Vancouver, Canada.
Weisi Duan and Alexander Yates. 2010. Extracting glosses
to disambiguate word senses. In Proceedings of Human
Language Technologies: The 11th Annual Conference of
the North American Chapter of the Association for Com-
putational Linguistics, pages 627?635, Los Angeles, CA,
USA.
Stefano Faralli and Roberto Navigli. 2012. A new
minimally-supervised framework for Domain Word Sense
Disambiguation. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning,
pages 1411?1422, Jeju, Korea.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia
Bernardini. 2008. Introducing and evaluating ukWaC, a
very large web-derived corpus of English. In Proceedings
of the 4th Web as Corpus Workshop (WAC-4), pages 47?
54, Marrakech, Morocco.
Tiziano Flati and Roberto Navigli. 2013. SPred: Large-scale
Harvesting of Semantic Predicates. In Proceedings of 51st
Annual Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics, pages
539?545, Nantes, France.
Eduard Hovy, Roberto Navigli, and Simone Paolo Ponzetto.
2013. Collaboratively built semi-structured content and
artificial intelligence: The story so far. Artificial Intelli-
gence, 194:2?27.
Zornitsa Kozareva and Eduard Hovy. 2010. Learning argu-
ments and supertypes of semantic relations using recur-
sive patterns. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics (ACL),
Uppsala, Sweden, pages 1482?1491, Uppsala, Sweden.
George A. Miller, R.T. Beckwith, Christiane D. Fellbaum,
D. Gross, and K. Miller. 1990. WordNet: an online
lexical database. International Journal of Lexicography,
3(4):235?244.
Roberto Navigli and Paola Velardi. 2010. Learning Word-
Class Lattices for definition and hypernym extraction. In
Proceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1318?1327, Up-
psala, Sweden.
Roberto Navigli, Paola Velardi, and Stefano Faralli. 2011.
A graph-based algorithm for inducing lexical taxonomies
from scratch. In Proceedings of the 22th International
Joint Conference on Artificial Intelligence, pages 1872?
1877, Barcelona, Spain.
Michael P. Oakes. 2005. Using Hearst?s rules for the auto-
matic acquisition of hyponyms for mining a pharmaceu-
tical corpus. In RANLP Text Mining Workshop?05, pages
63?67, Borovets, Bulgaria.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discovery.
In Proceedings of the 2009 AAAI Spring Symposium on
Learning by Reading and Learning to Read, pages 88?93,
Palo Alto, California.
Horacio Saggion. 2004. Identifying definitions in text col-
lections for question answering. In Proceedings of the
Fourth International Conference on Language Resources
and Evaluation, pages 1927?1930, Lisbon, Portugal.
Antonio Sanfilippo and Victor Poznanski. 1992. The ac-
quisition of lexical knowledge from combined machine-
readable dictionary sources. In Proceedings of the third
Conference on Applied Natural Language Processing,
pages 80?87, Trento, Italy.
Helmut Schmid. 1997. Probabilistic part-of-speech tagging
using decision trees. In Daniel Jones and Harold Somers,
editors, New Methods in Language Processing, Studies in
Computational Linguistics, pages 154?164. UCL Press,
London, GB.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning syntactic patterns for automatic hypernym dis-
covery. In Lawrence K. Saul, Yair Weiss, and Le?on Bot-
tou, editors, Proc. of NIPS 2004, pages 1297?1304, Cam-
bridge, Mass. MIT Press.
Angelika Storrer and Sandra Wellinghoff. 2006. Automated
detection and annotation of term definitions in German
text corpora. In LREC 2006, pages 275?295, Genoa, Italy.
Paola Velardi, Roberto Navigli, and Pierluigi D?Amadio.
2008. Mining the Web to create specialized glossaries.
IEEE Intelligent Systems, 23(5):18?25.
Paola Velardi, Stefano Faralli, and Roberto Navigli. 2013.
OntoLearn Reloaded: A graph-based algorithm for taxon-
omy induction. Computational Linguistics, 39(3).
Eline Westerhout. 2009. Definition extraction using linguis-
tic and structural features. In Proceedings of the RANLP
2009 Workshop on Definition Extraction, page 61?67,
Borovets, Bulgaria.
Willy Yap and Timothy Baldwin. 2009. Experiments on
pattern-based relation learning. In Proceedings of the 18th
ACM Conference on Information and Knowledge Man-
agement (CIKM 2009), pages 1657?1660, Hong Kong,
China, 2009.
Chunxia Zhang and Peng Jiang. 2009. Automatic extraction
of definitions. In Proceedings of 2nd IEEE International
Conference on Computer Science and Information Tech-
nology, pages 364?368, Beijing, China.
108
