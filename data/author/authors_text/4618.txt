The Annotation Graph Toolkit:
Software Components for
Building Linguistic Annotation Tools
Kazuaki Maeda, Steven Bird, Xiaoyi Ma and Haejoong Lee
Linguistic Data Consortium, University of Pennsylvania
3615 Market St., Philadelphia, PA 19104-2608 USA
fmaeda, sb, xma, haejoongg@ldc.upenn.edu
ABSTRACT
Annotation graphs provide an efficient and expressive data model
for linguistic annotations of time-series data. This paper reports
progress on a complete software infrastructure supporting the rapid
development of tools for transcribing and annotating time-series
data. This general-purpose infrastructure uses annotation graphs
as the underlying model, and allows developers to quickly create
special-purpose annotation tools using common components. An
application programming interface, an I/O library, and graphical
user interfaces are described. Our experience has shown us that it
is a straightforward task to create new special-purpose annotation
tools based on this general-purpose infrastructure.
Keywords
transcription, coding, annotation graph, interlinear text, dialogue
annotation
1. INTRODUCTION
Annotation graphs (AGs) provide an efficient and expressive
data model for linguistic annotations of time-series data [2]. This
paper reports progress on a complete software infrastructure sup-
porting the rapid development of tools for transcribing and anno-
tating time-series data. This general-purpose infrastructure uses
annotation graphs as the underlying model, and allows developers
to quickly create special-purpose annotation tools using common
components. This work is being done in cooperation with the
developers of other widely used annotation systems, Transcriber
and Emu [1, 3].
The infrastructure is being used in the development of a series
of annotation tools at the Linguistic Data Consortium. Several
such tools are shown in the paper: one for dialogue annotation,
one for telephone conversation transcription, and one for interlinear
transcription aligned to speech.
This paper will cover the following points: the application pro-
gramming interfaces for manipulating annotation graph data and
importing data from other formats; the model of inter-component
.
communication which permits easy reuse of software components;
and the design of the graphical user interfaces, which have been
tailored to be maximally ergonomic for the tasks.
The project homepage is: [http://www.ldc.upenn.edu/
AG/]. The software tools and software components described in
this paper are available through a CVS repository linked from this
homepage.
2. ARCHITECTURE
2.1 General Architecture
Existing annotation tools are based on a two level model (Fig-
ure 1 Top). The systems we demonstrate are based around a three
level model, in which annotation graphs provide a logical level
independent of application and physical levels (Figure 1 Bottom).
The application level represents special-purpose tools built on top
of the general-purpose infrastructure at the logical level.
The system is built from several components which instantiate
this model. Figure 2 shows the architecture of the tools currently
being developed. Annotation tools, such as the ones discussed
below, must provide graphical user interface components for signal
visualization and annotation. The communication between compo-
nents is handled through an extensible event language. An appli-
cation programming interface for annotation graphs (AG-API) has
been developed to support well-formed operations on annotation
graphs. This permits applications to abstract away from file format
issues, and deal with annotations purely at the logical level.
2.2 The Annotation Graph API
The complete IDL definition of the AG-API is provided in the
appendix (also online). Here we describe a few salient features of
the API.
The API provides access to internal objects (signals, anchors,
annotations etc) using identifiers. Identifiers are strings which con-
tain internal structure. For example, an AG identifier is quali-
fied with an AGSet identifier: AGSetId:AGId. Annotations and
anchors are doubly qualified: AGSetId:AGId:AnnotationId,
AGSetId:AGId:AnchorId. Thus, it is possible to determine from
any given identifiers, its membership in the overall data structure.
The functioning of the API will now be illustrated with a series
of examples. Suppose we have already constructed an AG and now
wish to create a new anchor. We might have the following API call:
CreateAnchor( "agSet12:ag5", 15.234, "sec" );
This call would construct a new anchor object and return its
identifier: agSet12:ag5:anchor34. Alternatively, if we already
Physical
Level
Application
Level
Query
Systems
Evaluation
Software
Annotation
ToolsExtraction
Systems
Visualization
& Exploration
Conversion
Tools
RDB
Format XML Tab delimited
flat files
Automatic
Aligners
Physical
Level
Application
Level
Logical
Level
Tab delimited
flat files
RDB
Format
XML
Query
Systems
Automatic
Aligners
Conversion
Tools
Extraction
Systems
Visualization
& Exploration
Evaluation
Software
Annotation
Tools
AG-API
Figure 1: The Two and Three-Level Architectures for Speech
Annotation
Figure 2: Architecture for Annotation Systems
have an anchor identifier that we wish to use for this new anchor
(e.g. because we are reading previously created annotation data
from a file and do not wish to assign new identifiers), then we could
have the following API call:
CreateAnchor( "agset12:ag5:anchor34", 15.234, "sec" );
This call will return agset12:ag5:anchor34.
Once a pair of anchors have been created it is possible to create
an annotation which spans them:
CreateAnnotation( "agSet12:ag5",
"agSet12:ag5:anchor34",
"agSet12:ag5:anchor35",
"phonetic" );
This call will construct an annotation object and return an iden-
tifier for it, e.g. agSet12:ag5:annotation41. We can now add
features to this annotation:
SetFeature( "agSet12:ag5:annotation41",
"date", "1999-07-02" );
The implementation maintains indexes on all the features, and
also on the temporal information and graph structure, permitting
efficient search using a family of functions such as:
GetAnnotationSetByFeature( "agSet12:ag5",
"date", "1999-07-02" );
2.3 A File I/O Library
A file I/O library (AG-FIO) to support creation and export of AG
data has been developed. This will eventually handle all widely
used annotation formats. Formats currently supported by the AG-
FIO library include the TIMIT, BU, Treebank, AIF (ATLAS Inter-
change Format), Switchboard and BAS Partitur formats.
2.4 Inter-component Communication
Figure 3 shows the structure of an annotation tool in terms of
components and their inter-communications.
Main program - a small script
Waveform
display
Transcription
editor
Internal
representation
File input
/ output
AG-GUI-API
AG-GUI-API AG-API
AG-FIO-API
Figure 3: The Structure of an Annotation Tool
The main program is typically a small script which sets up the
widgets and provides callback functions to handle widget events.
In this example there are four other components which are reused
by several annotation tools. The AG and AG-FIO components
have already been described. The waveform display component
(of which there may be multiple instances) receives instructions to
pan and zoom, to play a segment of audio data, and so on. The tran-
scription editor is an annotation component which is specialized for
a particular coding task. Most tool customization is accomplished
by substituting for this component.
Both GUI components and the main program support a com-
mon API for transmitting and receiving events. For example, GUI
components have a notion of a ?current region? ? the timespan
which is currently in focus. A waveform component can change
an annotation component?s idea of the current region by sending a
SetRegion event (Figure 4). The same event can also be used in
the reverse direction. The main program routes the events between
GUI components, calling the AG-API to update the internal repre-
sentation as needed. With this communication mechanism, it is a
straightforward task to add new commands, specific to the annota-
tion task.
Main program
Waveform display AG-API Transcription editor
User types Control-G Update Display
SetRegion t1 t2 AG::SetAnchorOffset SetRegion t1 t2
Update
Internal Representation
Figure 4: Inter-component Communication
2.5 Reuse of Software Components
The architecture described in this paper allows rapid develop-
ment of special-purpose annotation tools using common compo-
nents. In particular, our model of inter-component communica-
tion facilitates reuse of software components. The annotation tools
described in the next section are not intended for general purpose
annotation/transcription tasks; the goal is not to create an ?emacs
for linguistic annotation?. Instead, they are special-purpose tools
based on the general purpose infrastructure. These GUI com-
ponents can be modified or replaced when building new special-
purpose tools.
3. GRAPHICAL USER INTERFACES
3.1 A Spreadsheet Component
The first of the annotation/transcription editor components we
describe is a spreadsheet component. In this section, we show two
tools that use the spreadsheet component: a dialogue annotation
tool and a telephone conversation transcription tool.
Dialogue annotation consists of assigning a field-structured record
to each utterance in each speaker turn. A key challenge is to
handle overlapping speaker turns and back-channel cues without
disrupting the structure of individual speaker contributions. The
tool solves these problems and permits annotations to be aligned
to a (multi-channel) recording. The records are displayed in a
spreadsheet. Clicking on a row of the spreadsheet causes the corre-
sponding extent of audio signal to be highlighted. As an extended
recording is played back, annotated sections are highlighted (both
waveform and spreadsheet displays).
Figure 5 shows the tool with a section of the TRAINS/DAMSL
corpus [4]. Figure 6 shows another tool designed for transcribing
telephone conversations. This latter tool is a version of the dialogue
annotation tool, with the columns changed to accommodate the
needed fields: in this case, speaker turns and transcriptions. Both
of these tools are for two-channel audio files. The audio channel
corresponding to the highlighted annotation in the spreadsheet is
also highlighted.
3.2 An Interlinear Transcription Component
Interlinear text is a kind of text in which each word is anno-
tated with phonological, morphological and syntactic information
(displayed under the word) and each sentence is annotated with a
free translation. Our tool permits interlinear transcription aligned
to a primary audio signal, for greater accuracy and accountability.
Whole words and sub-parts of words can be easily aligned with
the audio. Clicking on a piece of the annotation causes the corre-
sponding extent of audio signal to be highlighted. As an extended
recording is played back, annotated sections are highlighted (both
waveform and interlinear text displays).
The following screenshot shows the tool with some interlinear
text from Mawu (a Manding language of the Ivory Coast, West
Africa).
Figure 7: Interlinear Transcription Tool
3.3 A Waveform Display Component
The tools described above utilize WaveSurfer and Snack devel-
oped by Ka?re Sjo?lander and Jonas Beskow [7, 8]. WaveSurfer
allows developers to specify event callbacks through a plug-in
architecture. We have developed a plug-in for WaveSurfer that
enables the inter-component communication described in this paper.
In addition to waveforms, it is also possible to show spectrograms
and pitch contours of a speech file if the given annotation task
requires phonetic analysis of the speech data.
4. FUTURE WORK
4.1 More GUI Components
In addition to the software components discussed in this paper,
we plan to develop more components to support various annotation
tasks. For example, a video component is being developed, and it
will have an associated editor for gestural coding. GUI components
for Conversation Analysis (CA) [6] and CHAT [5] are also planned.
4.2 An Annotation Graph Server
We are presently designing a client-side component which presents
the same AG-API to the annotation tool, but translates all calls
Figure 5: Dialogue Annotation Tool for the TRAINS/DAMSL Corpus
Figure 6: Telephone Conversation Transcription Tool for the CALLFRIEND Spanish Corpus
into SQL and then transmits them to a remote SQL server (see
Figure 8). A centralized server could house a potentially large
quantity of annotation data, permitting multiple clients to collabo-
ratively construct annotations of shared data. Existing methods for
authentication and transaction processing will be be used to ensure
the integrity of the data.
AG-API
Mapping to SQL
SQL
RDB server and
persistent storage
Main program - a small script
Waveform
display
Transcription
editor
File input
/ output
AG-GUI-API
AG-GUI-API
AG-FIO-API
network
Figure 8: Annotation Tool Connecting to Annotation Server
4.3 Timeline for Development
A general distribution (Version 1.0) of the tools is planned for the
early summer, 2001. Additional components and various improve-
ments will be added to future releases. Source code will be
available through a source code distribution service, SourceForge
([http://sourceforge.net/projects/agtk/]). Further
schedule for updates will be posted on our web site: [http:
//www.ldc.upenn.edu/AG/].
5. CONCLUSION
This paper has described a comprehensive infrastructure for
developing annotation tools based on annotation graphs. Our expe-
rience has shown us that it is a simple matter to construct new
special-purpose annotation tools using high-level software compo-
nents. The tools can be quickly created and deployed, and replaced
by new versions as annotation tasks evolve. The components and
tools reported here are all being made available under an open
source license.
6. ACKNOWLEDGMENT
This material is based upon work supported by the National
Science Foundation under Grant No. 9978056 and 9983258.
7. REFERENCES
[1] C. Barras, E. Geoffrois, Z. Wu, and M. Liberman. Transcriber:
development and use of a tool for assisting speech corpora
production. Speech Communication, 33:5?22, 2001.
[2] S. Bird and M. Liberman. A formal framework for linguistic
annotation. Speech Communication, 33:23?60, 2001.
[3] S. Cassidy and J. Harrington. Multi-level annotation of
speech: An overview of the emu speech database management
system. Speech Communication, 33:61?77, 2001.
[4] D. Jurafsky, E. Shriberg, and D. Biasca. Switchboard
SWBD-DAMSL Labeling Project Coder?s Manual, Draft 13.
Technical Report 97-02, University of Colorado Institute of
Cognitive Science, 1997. [http://stripe.colorado.
edu/?jurafsky/manual.august1.html].
[5] B. MacWhinney. The CHILDES Project: Tools for Analyzing
Talk. Mahwah, NJ: Lawrence Erlbaum., second edition, 1995.
[http://childes.psy.cmu.edu/].
[6] E. Schegloff. Reflections on studying prosody in
talk-in-interaction. Language and Speech, 41:235?60, 1998.
[http://www.sscnet.ucla.edu/soc/faculty/
schegloff/prosody/].
[7] K. Sjo?lander. The Snack sound toolkit, 2000.
[http://www.speech.kth.se/snack/].
[8] K. Sjo?lander and J. Beskow. WaveSurfer ? an open source
speech tool. In Proceedings of the 6th International
Conference on Spoken Language Processing, 2000.
[http://www.speech.kth.se/wavesurfer/].
APPENDIX
A. IDL DEFINITION FOR FLAT AG API
interface AG {
typedef string Id; // generic identifier
typedef string AGSetId; // AGSet identifier
typedef string AGId; // AG identifier
typedef string AGIds;
// AG identifiers (space separated list)
typedef string AnnotationId;
// Annotation identifier
typedef string AnnotationType; // Annotation type
typedef string AnnotationIds;
// Annotation identifiers (list)
typedef string AnchorId; // Anchor identifier
typedef string AnchorIds;
// Anchor identifiers (list)
typedef string TimelineId; // Timeline identifier
typedef string SignalId; // Signal identifier
typedef string SignalIds;
// Signal identifiers (list)
typedef string FeatureName; // feature name
typedef string FeatureNames; // feature name (list)
typedef string FeatureValue; // feature value
typedef string Features;
// feature=value pairs (list)
typedef string URI;
// a uniform resource identifier
typedef string MimeClass; // the MIME class
typedef string MimeType; // the MIME type
typedef string Encoding; // the signal encoding
typedef string Unit; // the unit for offsets
typedef string AnnotationRef;
// an annotation reference
typedef float Offset; // the offset into a signal
//// AGSet ////
// Id is AGSetId or AGId
AGId CreateAG( in Id id
in TimelineId timelineId );
boolean ExistsAG( in AGId agId );
void DeleteAG( in AGId agId );
AGIds GetAGIds( in AGSetId agSetId );
//// Signals ////
TimelineId CreateTimeline( in URI uri,
in MimeClass mimeClass,
in MimeType mimeType,
in Encoding encoding,
in Unit unit,
in Track track );
TimelineId CreateTimeline( in TimelineId timelineId,
in URI uri,
in MimeClass mimeClass,
in MimeType mimeType,
in Encoding encoding,
in Unit unit,
in Track track);
boolean ExistsTimeline( in TimelineId timelineId );
void DeleteTimeline( in TimelineId timelineId );
// Id may be TimelineId or SignalId
SignalId CreateSignal( in Id id,
in URI uri,
in MimeClass mimeClass,
in MimeType mimeType,
in Encoding encoding,
in Unit unit,
in Track track );
boolean ExistsSignal( in SignalId signalId );
void DeleteSignal( in SignalId signalId );
SignalIds GetSignals( in TimelineId timelineId );
MimeClass
GetSignalMimeClass( in SignalId signalId );
MimeType
GetSignalMimeType( in SignalId signalId );
Encoding GetSignalEncoding( in SignalId signalId );
string GetSignalXlinkType( in SignalId signalId );
string GetSignalXlinkHref( in SignalId signalId );
string GetSignalUnit( in SignalId signalId );
Track GetSignalTrack( in SignalId signalId );
//// Annotation ////
// Id may be AGId or AnnotationId
AnnotationId CreateAnnotation( in Id id,
in AnchorId anchorId1,
in AnchorId anchorId2,
in AnnotationType annotationType );
boolean ExistsAnnotation
(in AnnotationId annotationId );
void DeleteAnnotation
(in AnnotationId annotationId );
AnnotationId CopyAnnotation
(in AnnotationId annotationId );
AnnotationIds SplitAnnotation
(in AnnotationId annotationId );
AnnotationIds NSplitAnnotation(
in AnnotationId annotationId, in short N );
AnchorId
GetStartAnchor( in AnnotationId annotationId);
AnchorId GetEndAnchor(
in AnnotationId annotationId);
void SetStartAnchor( in AnnotationId annotationId,
in AnchorId anchorId );
void SetEndAnchor( in AnnotationId annotationId,
in AnchorId anchorId );
Offset
GetStartOffset( in AnnotationId annotationId );
Offset GetEndOffset(
in AnnotationId annotationId );
void SetStartOffset( in AnnotationId annotationId,
in Offset offset );
void SetEndOffset( in AnnotationId annotationId,
in Offset offset );
// this might be necessary to package up an id
// into a durable reference
AnnotationRef GetRef( in Id id );
//// Features ////
// this is for both the content of an annotation,
// and for the metadata associated with AGSets,
// AGs, Timelines and Signals.
void SetFeature( in Id id,
in FeatureName featureName,
in FeatureValue featureValue );
boolean ExistsFeature( in Id id,
in FeatureName featureName );
void DeleteFeature( in Id id,
in FeatureName featureName );
string GetFeature( in Id id,
in FeatureName featureName );
void UnsetFeature( in Id id,
in FeatureName featureName );
FeatureNames GetFeatureNames( in Id id );
void SetFeatures( in Id id,
in Features features );
Features GetFeatures( in Id id );
void UnsetFeatures( in Id id );
//// Anchor ////
// Id may be AGId or AnchorId
AnchorId CreateAnchor( in Id id,
in Offset offset,
in Unit unit,
in SignalIds signalIds );
AnchorId CreateAnchor( in Id id,
in SignalIds signalIds );
AnchorId CreateAnchor( in Id id );
boolean ExistsAnchor( in AnchorId anchorId );
void DeleteAnchor( in AnchorId anchorId );
void SetAnchorOffset( in AnchorId anchorId,
in Offset offset );
Offset GetAnchorOffset( in AnchorId anchorId );
void UnsetAnchorOffset( in AnchorId anchorId );
AnchorId SplitAnchor( in AnchorId anchorId );
AnnotationIds GetIncomingAnnotationSet(
in AnchorId anchorId );
AnnotationIds GetOutgoingAnnotationSet(
in AnchorId anchorId );
//// Index ////
AnchorIds GetAnchorSet( in AGId agId );
AnchorIds GetAnchorSetByOffset( in AGId agId,
in Offset offset,
in float epsilon );
AnchorIds GetAnchorSetNearestOffset(
in AGId agId,
in Offset offset );
AnnotationIds
GetAnnotationSetByFeature( in AGId agId,
in FeatureName featureName );
AnnotationIds
GetAnnotationSetByOffset( in AGId agId,
in Offset offset );
AnnotationIds
GetAnnotationSetByType( in AGId agId,
in AnnotationType annotationType );
//// Ids ////
// Id may be AGId, AnnotationId, AnchorId
AGSetId GetAGSetId( in Id id );
// Id may be AnnotationId or AnchorId
AGId GetAGId( in Id id );
// Id may be AGId or SignalId
TimelineId GetTimelineId( in Id id );
};
Annotation Tools Based on the Annotation Graph API
Steven Bird, Kazuaki Maeda, Xiaoyi Ma and Haejoong Lee
Linguistic Data Consortium, University of Pennsylvania
3615 Market Street, Suite 200, Philadelphia, PA 19104-2608, USA
fsb,maeda,xma,haejoongg@ldc.upenn.edu
Abstract
Annotation graphs provide an efficient
and expressive data model for linguistic
annotations of time-series data. This
paper reports progress on a complete
open-source software infrastructure
supporting the rapid development of
tools for transcribing and annotating
time-series data. This general-
purpose infrastructure uses annotation
graphs as the underlying model, and
allows developers to quickly create
special-purpose annotation tools using
common components. An application
programming interface, an I/O library,
and graphical user interfaces are
described. Our experience has shown
us that it is a straightforward task to
create new special-purpose annotation
tools based on this general-purpose
infrastructure.
1 Introduction
In the past, standardized file formats and coding
practices have greatly facilitated data sharing and
software reuse. Yet it has so far proved impossible
to work out universally agreed formats and codes
for linguistic annotation. We contend that this is a
vain hope, and that the interests of sharing and
reuse are better served by agreeing on the data
models and interfaces.
Annotation graphs (AGs) provide an efficient
and expressive data model for linguistic anno-
tations of time-series data (Bird and Liberman,
Figure 1: Architecture for Annotation Systems
2001). Recently, the LDC has been develop-
ing a complete software infrastructure supporting
the rapid development of tools for transcribing
and annotating time-series data, in cooperation
with NIST and MITRE as part of the ATLAS
project, and with the developers of other widely
used annotation systems, Transcriber and Emu
(Bird et al, 2000; Barras et al, 2001; Cassidy and
Harrington, 2001).
The infrastructure is being used in the devel-
opment of a series of annotation tools at the Lin-
guistic Data Consortium. Two tools are shown in
the paper: one for dialogue annotation and one
for interlinear transcription. In both cases, the
transcriptions are time-aligned to a digital audio
signal.
This paper will cover the following points: the
application programming interfaces for manipu-
lating annotation graph data and importing data
from other formats; the model of inter-component
communication which permits easy reuse of soft-
ware components; and the design of the graphical
user interfaces.
2 Architecture
2.1 General architecture
Figure 1 shows the architecture of the tools
currently being developed. Annotation tools,
such as the ones discussed below, must provide
graphical user interface components for signal
visualization and annotation. The communication
between components is handled through an
extensible event language. An application
programming interface for annotation graphs
has been developed to support well-formed
operations on annotation graphs. This permits
applications to abstract away from file format
issues, and deal with annotations purely at the
logical level.
2.2 The annotation graph API
The application programming interface provides
access to internal objects (signals, anchors,
annotations etc) using identifiers, represented
as formatted strings. For example, an AG
identifier is qualified with an AGSet identifier:
AGSetId:AGId. Annotations and anchors are
doubly qualified: AGSetId:AGId:AnnotationId,
AGSetId:AGId:AnchorId. Thus, the identifier
encodes the unique membership of an object in
the containing objects.
We demonstrate the behavior of the API with
a series of simple examples. Suppose we have
already constructed an AG and now wish to create
a new anchor. We might have the following API
call:
CreateAnchor("agSet12:ag5", 15.234, "sec");
This call would construct a new anchor object
and return its identifier: agSet12:ag5:anchor34.
Alternatively, if we already have an anchor iden-
tifier that we wish to use for the new anchor (e.g.
because we are reading previously created anno-
tation data from a file and do not wish to assign
new identifiers), then we could have the following
API call:
CreateAnchor("agset12:ag5:anchor34",
15.234, "sec");
This call will return agset12:ag5:anchor34.
Once a pair of anchors have been created it
is possible to create an annotation which spans
them:
CreateAnnotation("agSet12:ag5",
"agSet12:ag5:anchor34",
"agSet12:ag5:anchor35",
"phonetic" );
This call will construct an annotation
object and return an identifier for it, e.g.
agSet12:ag5:annotation41. We can now add
features to this annotation:
SetFeature("agSet12:ag5:annotation41",
"date", "1999-07-02" );
The implementation maintains indexes on all
the features, and also on the temporal information
and graph structure, permitting efficient search
using a family of functions such as:
GetAnnotationSetByFeature(
"agSet12:ag5", "date", "1999-07-02");
2.3 A file I/O library
A file I/O library (AG-FIO) supports input and
output of AG data to existing formats. Formats
currently supported by the AG-FIO library
include the TIMIT, BU, Treebank, AIF (ATLAS
Interchange Format), Switchboard and BAS
Partitur formats. In time, the library will handle
all widely-used signal annotation formats.
2.4 Inter-component communication
Figure 2 shows the structure of an annotation tool
in terms of components and their communication.
The main program is typically a small script
which sets up the widgets and provides callback
functions to handle widget events. In this
example there are four other components which
Main program - a small script
Waveform
display
Transcription
editor
Internal
representation
File input
/ output
AG-GUI-API
AG-GUI-API AG-API
AG-FIO-API
Figure 2: The Structure of an Annotation Tool
Main program
Waveform display AG-API Transcription editor
User types Control-G Update Display
SetRegion t1 t2 AG::SetAnchorOffset SetRegion t1 t2
Update
Internal Representation
Figure 3: Inter-component Communication
are reused by several annotation tools. The AG
and AG-FIO components have already been
described. The waveform display component (of
which there may be multiple instances) receives
instructions to pan and zoom, to play a segment
of audio data, and so on. The transcription
editor is an annotation component which is
specialized for a particular coding task. Most tool
customization is accomplished by substituting for
this component.
Both GUI components and the main program
support a common API for transmitting and
receiving events. For example, GUI components
have a notion of a ?current region? ? the
timespan which is currently in focus. A
waveform component can change an annotation
component?s idea of the current region by
sending a SetRegion event (Figure 3). The
same event can also be used in the reverse
direction. The main program routes the events
between GUI components, calling the annotation
graph API to update the internal representation as
needed. With this communication mechanism, it
is straightforward to add new commands, specific
to the annotation task.
2.5 Reuse of software components
The architecture described in this paper allows
rapid development of special-purpose annotation
tools using common components. In particular,
our model of inter-component communication
facilitates reuse of software components.
The annotation tools described in the next
section are not intended for general purpose
annotation/transcription tasks; the goal is not
to create an ?emacs for linguistic annotation?.
Instead, they are special-purpose tools based on
the general purpose infrastructure. These GUI
Figure 4: Dialogue Annotation Tool for the
TRAINS/DAMSL Corpus
components can be modified or replaced when
building new special-purpose tools.
3 Graphical User Interfaces
3.1 A spreadsheet component
Dialogue annotation typically consists of assign-
ing a field-structured record to each utterance in
each speaker turn. A key challenge is to handle
overlapping turns and back-channel cues without
disrupting the structure of individual speaker con-
tributions. The tool side-steps these problems by
permitting utterances to be independently aligned
to a (multi-channel) recording. The records are
displayed in a spreadsheet; clicking on a row of
the spreadsheet causes the corresponding extent
of audio signal to be highlighted. As an extended
recording is played back, annotated sections are
highlighted, in both the waveform and spread-
sheet displays.
Figure 4 shows the tool with a section of the
TRAINS/DAMSL corpus (Jurafsky et al, 1997).
Note that the highlighted segment in the audio
channel corresponds to the highlighted annotation
in the spreadsheet.
3.2 An interlinear transcription component
Interlinear text is a kind of text in which
each word is annotated with phonological,
morphological and syntactic information
(displayed under the word) and each sentence
is annotated with a free translation. Our tool
Figure 5: Interlinear Transcription Tool
permits interlinear transcription aligned to a
primary audio signal, for greater accuracy and
accountability. Whole words and sub-parts of
words can be easily aligned with the audio.
Clicking on a piece of the annotation causes
the corresponding extent of audio signal to be
highlighted. As an extended recording is played
back, annotated sections are highlighted (both
waveform and interlinear text displays).
The screenshot in Figure 5 shows the tool with
some interlinear text from Mawu (a Manding lan-
guage of the Ivory Coast, West Africa).
3.3 A waveform display component
The tools described above utilize WaveSurfer
and Snack (Sjo?lander, 2000; Sjo?lander and
Beskow, 2000). We have developed a plug-in
for WaveSurfer to support the inter-component
communication described in this paper.
4 Available Software and Future Work
The Annotation Graph Toolkit, version 1.0, con-
tains a complete implementation of the annota-
tion graph model, import filters for several for-
mats, loading/storing data to an annotation server
(MySQL), application programming interfaces in
C++ and Tcl/tk, and example annotation tools for
dialogue, ethology and interlinear text. The sup-
ported formats are: xlabel, TIMIT, BAS Parti-
tur, Penn Treebank, Switchboard, LDC Callhome,
CSV and AIF level 0. All software is distributed
under an open source license, and is available
from http://www.ldc.upenn.edu/AG/.
Future work will provide Python and Perl inter-
faces, more supported formats, a query language
and interpreter, a multichannel transcription tool,
and a client/server model.
5 Conclusion
This paper has described a comprehensive infras-
tructure for developing annotation tools based on
annotation graphs. Our experience has shown us
that it is a simple matter to construct new special-
purpose annotation tools using high-level soft-
ware components. The tools can be quickly cre-
ated and deployed, and replaced by new versions
as annotation tasks evolve.
Acknowledgements
This material is based upon work supported by the
National Science Foundation under Grant Nos.
9978056, 9980009, and 9983258.
References
Claude Barras, Edouard Geoffrois, Zhibiao Wu, and Mark
Liberman. 2001. Transcriber: development and use of a
tool for assisting speech corpora production. Speech
Communication, 33:5?22.
Steven Bird and Mark Liberman. 2001. A formal
framework for linguistic annotation. Speech
Communication, 33:23?60.
Steven Bird, David Day, John Garofolo, John Henderson,
Chris Laprun, and Mark Liberman. 2000. ATLAS: A
flexible and extensible architecture for linguistic annotation.
In Proceedings of the Second International Conference on
Language Resources and Evaluation. Paris: European
Language Resources Association.
Steve Cassidy and Jonathan Harrington. 2001. Multi-level
annotation of speech: An overview of the emu speech
database management system. Speech Communication,
33:61?77.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca.
1997. Switchboard SWBD-DAMSL Labeling Project
Coder?s Manual, Draft 13. Technical Report 97-02,
University of Colorado Institute of Cognitive Science.
[stripe.colorado.edu/?jurafsky/manual.august1.html].
Ka?re Sjo?lander and Jonas Beskow. 2000. Wavesurfer ? an
open source speech tool. In Proceedings of the 6th
International Conference on Spoken Language Processing.
http://www.speech.kth.se/wavesurfer/.
Ka?re Sjo?lander. 2000. The Snack sound toolkit.
http://www.speech.kth.se/snack/.
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 1?5,
Baltimore, Maryland, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Aikuma: A Mobile App for Collaborative Language Documentation
Steven Bird
1,2
, Florian R. Hanke
1
, Oliver Adams
1
, and Haejoong Lee
2
1
Dept of Computing and Information Systems, University of Melbourne
2
Linguistic Data Consortium, University of Pennsylvania
Abstract
Proliferating smartphones and mobile
software offer linguists a scalable, net-
worked recording device. This paper de-
scribes Aikuma, a mobile app that is de-
signed to put the key language documen-
tation tasks of recording, respeaking, and
translating in the hands of a speech com-
munity. After motivating the approach we
describe the system and briefly report on
its use in field tests.
1 Introduction
The core of a language documentation consists
of primary recordings along with transcriptions
and translations (Himmelmann, 1998; Woodbury,
2003). Many members of a linguistic community
may contribute to a language documentation, play-
ing roles that depend upon their linguistic com-
petencies. For instance, the best person to pro-
vide a text could be a monolingual elder, while the
best person to translate it could be a younger bilin-
gual speaker. Someone else again may be the best
choice for performing transcription work. What-
ever the workflow and degree of collaboration,
there is always the need to manage files and cre-
ate secondary materials, a data management prob-
Figure 1: Phrase-aligned bilingual audio
lem. The problem is amplified by the usual prob-
lems that attend linguistic fieldwork: limited hu-
man resources, limited communication, and lim-
ited bandwidth.
The problem is not to collect large quantities of
primary audio in the field using mobile devices (de
Vries et al., 2014). Rather, the problem is to en-
sure the long-term interpretability of the collected
recordings. At the most fundamental level, we
want to know what words were spoken, and what
they meant. Recordings made in the wild suf-
fer from the expected range of problems: far-field
recording, significant ambient noise, audience par-
ticipation, and so forth. We address these prob-
lems via the ?respeaking? task (Woodbury, 2003).
Recordings made in an endangered language may
not be interpretable once the language falls out of
use. We address this problem via the ?oral trans-
lation? task. The result is relatively clean source
audio recordings with phrase-aligned translations
(see Figure 1). NLP methods are applicable to
such data (Dredze et al., 2010), and we can hope
that ultimately, researchers working on archived
bilingual audio sources will be able to automati-
cally extract word-glossed interlinear text.
We describe Aikuma, an open source Android
app that supports recording along with respeaking
Figure 2: Adding a time-aligned translation
1
and oral translation, while capturing basic meta-
data. Aikuma supports local networking so that
a set of mobile phones can be synchronized, and
anyone can listen to and annotate the recordings
made by others. Key functionality is provided
via a text-less interface (Figure 2). Aikuma in-
troduces social media and networked collabora-
tion to village-based fieldwork, all on low-cost de-
vices, and this is a boon for scaling up the quan-
tity of documentary material that can be collected
and processed. Field trials in Papua New Guinea,
Brazil, and Nepal have demonstrated the effective-
ness of the approach (Bird et al., 2014).
2 Thought Experiment: The Future
Philologist
A typical language documentation project is
resource-bound. So much documentation could be
collected, yet the required human resources to pro-
cess it all adequately are often not available. For
instance, some have argued that it is not effective
to collect large quantities of primary recordings
because there is not the time to transcribe it.
1
Estimates differ about the pace of language loss.
Yet it is uncontroversial that ? for hundreds of lan-
guages ? only the oldest living speakers are well-
versed in traditional folklore. While a given lan-
guage may survive for several more decades, the
opportunity to document significant genres may
pass much sooner. Ideally, a large quantity of these
nearly-extinct genres would be recorded and given
sufficient further treatment in the form of respeak-
ings and oral translations, in order to have archival
value. Accordingly, we would like to determine
what documentary materials would be of greatest
practical value to the linguist working in the fu-
ture, possibly ten to a hundred or more years in
future. Given the interest of classical philology in
ancient languages, we think of this researcher as
the ?future philologist.?
Our starting point is texts, as the least processed
item of the so-called ?Boasian trilogy.? A substan-
tial text corpus can serve as the basis for the prepa-
ration of grammars and dictionaries even once a
language is extinct, as we know from the cases of
the extinct languages of the Ancient Near East.
1
E.g. Paul Newman?s 2013 seminar The Law of Un-
intended Consequences: How the Endangered Languages
Movement Undermines Field Linguistics as a Scientific
Enterprise, https://www.youtube.com/watch?v=
xziE08ozQok
Our primary resource is the native speaker com-
munity, both those living in the ancestral home-
land and the members of the diaspora. How
can we engage these communities in the tasks
of recording, respeaking, and oral interpretation,
in order to generate the substantial quantity of
archival documentation?
Respeaking involves listening to an original
recording and repeating what was heard carefully
and slowly, in a quiet recording environment It
gives archival value to recordings that were made
?in the wild? on low-quality devices, with back-
ground noise, and by people having no training in
linguistics. It provides much clearer audio content,
facilitating transcription. Bettinson (2013) has
shown that human transcribers, without knowl-
edge of the language under study, can generally
produce phonetic transcriptions from such record-
ings that are close enough to enable someone who
knows the language to understand what was said,
and which can be used as the basis for phonetic
analysis. This means we can postpone the tran-
scription task ? by years or even decades ? un-
til such time as the required linguistic expertise is
available to work with archived recordings.
By interpretation, we mean listening to a
recording and producing a spoken translation of
what was heard. Translation into another language
obviates the need for the usual resource-intensive
approaches to linguistic analysis that require syn-
tactic treebanks along with semantic annotations,
at the cost of a future decipherment effort (Xia and
Lewis, 2007; Abney and Bird, 2010).
3 Design Principles
Several considerations informed the design of
Aikuma. First, to facilitate use by monolingual
speakers, the primary recording functions need to
be text free.
Second, to facilitate collaboration and guard
against loss of phones, it needs to be possible
to continuously synchronise files between phones.
Once any information has been captured on a
phone, it is synchronized to the other phones on
the local network. All content from any phone is
available from any phone, and thus only a single
phone needs to make it back from village-based
work. After a recording is made, it needs to be
possible to listen to it on the other phones on the
local network. This makes it easy for people to
annotate each other?s recordings. This also en-
2
ables users to experience the dissemination of their
recordings, and to understand that a private activ-
ity of recording a narrative is tantamount to public
speaking. This is useful for establishing informed
consent in communities who have no previous ex-
perience of the Internet or digital archiving.
Third, to facilitate trouble-shooting and future
digital archaeology, the file format of phones
needs to be transparent. We have devised an
easily-interpretable directory hierarchy for record-
ings and users, which permits direct manipulation
of recordings. For instance, all the metadata and
recordings that involve a particular speaker could
be extracted from the hierarchy with a single file-
name pattern.
4 Aikuma
Thanks to proliferating smartphones, it is now rel-
atively easy and cheap for untrained people to col-
lect and share many sorts of recordings, for their
own benefit and also for the benefit of language
preservation efforts. These include oral histories,
oral literature, public speaking, and discussion of
popular culture. With inexpensive equipment and
minimal training, a few dozen motivated people
can create a hundred hours of recorded speech (ap-
prox 1M words) in a few weeks. However, adding
transcription and translation by a trained linguist
introduces a bottleneck: most languages will be
gone before linguists will get to them.
Aikuma puts this work in the hands of language
speakers. It collects recordings, respeakings, and
interpretations, and organizes them for later syn-
chronization with the cloud and archival storage.
People with limited formal education and no prior
experience using smartphones can readily use the
app to record their stories, or listen to other peo-
ple?s stories to respeak or interpret them. Literate
users can supply written transcriptions and trans-
lations. Items can be rated by the linguist and
language workers and highly rated items are dis-
played more prominently, and this may be used to
influence the documentary workflow. Recordings
are stored alongside a wealth of metadata, includ-
ing language, GPS coordinates, speaker, and off-
sets on time-aligned translations and comments.
4.1 Listing and saving recordings
When the app is first started, it shows a list of
available recordings, indicating whether they are
respeakings or translations (Figure 3(a)). These
recordings could have been made on this phone, or
synced to this phone from another, or downloaded
from an archive. The recording functionality is
accessed by pressing the red circle, and when the
user is finished, s/he is prompted to add metadata
to identify the person or people who were recorded
(Figure 3(b)) and the language(s) of the recording
(Figure 3(c)).
(a) Main list (b) Adding speaker metadata (c) Adding language metadata
Figure 3: Screens for listing and saving recordings
3
4.2 Playback and commentary
When a recording is selected, the user sees a dis-
play for the individual recording, with its name,
date, duration, and images of the participants,
cf. Figure 4.
Figure 4: Recording playback screen
The availability of commentaries is indicated by
user images beneath the timeline. Once an orig-
inal recording has commentaries, their locations
are displayed within the playback slider. Playback
interleaves the original recording with the spoken
commentary, cf Figure 5.
Figure 5: Commentary playback screen
4.3 Gesture vs voice activation
Aikuma provides two ways to control any record-
ing activity, using gesture or voice activation. In
the gesture-activated mode, playback is started,
paused, or stopped using on-screen buttons. For
commentary, the user presses and holds the play
button to listen to the source, and presses and holds
the record button to supply a commentary, cf Fig-
ure 2. Activity is suspended when neither button
is being pressed.
In the voice-activated mode, the user puts the
phone to his or her ear and playback begins au-
tomatically. Playback is paused when the user
lifts the phone away from the ear. When the user
speaks, playback stops and the speech is recorded
and aligned with the source recording.
4.4 File storage
The app supports importing of external audio files,
so that existing recordings can be put through the
respeaking and oral translation processes. Stor-
age uses a hierarchical file structure and plain text
metadata formats which can be easily accessed di-
rectly using command-line tools. Files are shared
using FTP. Transcripts are stored using the plain
text NIST HUB-4 transcription format and can be
exported in Elan format.
4.5 Transcription
Aikuma incorporates a webserver and clients can
connect using the phone?s WiFi, Bluetooth, or
USB interfaces. The app provides a browser-based
transcription tool that displays the waveform for
a recording along with the spoken annotations.
Users listen to the source recording along with any
available respeakings and oral translations, and
then segment the audio and enter his or her own
written transcription and translation. These are
saved to the phone?s storage and displayed on the
phone during audio playback.
5 Deployment
We have tested Aikuma in Papua New Guinea,
Brazil, and Nepal (Bird et al., 2014). We taught
members of remote indigenous communities to
record narratives and orally interpret them into a
language of wider communication. We collected
approximately 10 hours of audio, equivalent to
100k words. We found that the networking capa-
bility facilitated the contribution of multiple mem-
bers of the community who have a variety of lin-
guistic aptitudes. We demonstrated that the plat-
form is an effective way to engage remote indige-
nous speech communities in the task of building
phrase-aligned bilingual speech corpora. To sup-
port large scale deployment, we are adding sup-
port for workflow management, plus interfaces to
the Internet Archive and to SoundCloud for long
term preservation and social interaction.
Acknowledgments
We gratefully acknowledge support from the Aus-
tralian Research Council, the National Science
Foundation, and the Swiss National Science Foun-
dation. We are also grateful to Isaac McAlister,
Katie Gelbart, and Lauren Gawne for field-testing
work. Aikuma development is hosted on GitHub.
4
References
Steven Abney and Steven Bird. 2010. The Human
Language Project: building a universal corpus of the
world?s languages. In Proceedings of the 48th Meet-
ing of the Association for Computational Linguis-
tics, pages 88?97. Association for Computational
Linguistics.
Mat Bettinson. 2013. The effect of respeaking on tran-
scription accuracy. Honours Thesis, Dept of Lin-
guistics, University of Melbourne.
Steven Bird, Isaac McAlister, Katie Gelbart, and Lau-
ren Gawne. 2014. Collecting bilingual audio in re-
mote indigenous villages. under review.
Nic de Vries, Marelie Davel, Jaco Badenhorst, Willem
Basson, Febe de Wet, Etienne Barnard, and Alta
de Waal. 2014. A smartphone-based ASR data col-
lection tool for under-resourced languages. Speech
Communication, 56:119?131.
Mark Dredze, Aren Jansen, Glen Coppersmith, and
Ken Church. 2010. NLP on spoken documents
without ASR. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 460?470. Association for Com-
putational Linguistics.
Florian R. Hanke and Steven Bird. 2013. Large-
scale text collection for unwritten languages. In Pro-
ceedings of the 6th International Joint Conference
on Natural Language Processing, pages 1134?1138.
Asian Federation of Natural Language Processing.
Nikolaus P. Himmelmann. 1998. Documentary and
descriptive linguistics. Linguistics, 36:161?195.
Anthony C. Woodbury. 2003. Defining documentary
linguistics. In Peter Austin, editor, Language Docu-
mentation and Description, volume 1, pages 35?51.
London: SOAS.
Fei Xia and William D. Lewis. 2007. Multilingual
structural projection across interlinearized text. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics, pages
452?459. Association for Computational Linguis-
tics.
5
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 93?103,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Transliteration of Arabizi into Arabic Orthography: Developing a 
Parallel Annotated Arabizi-Arabic Script SMS/Chat Corpus 
 
Ann Bies, Zhiyi Song, Mohamed Maamouri, Stephen Grimes, Haejoong Lee,  
Jonathan Wright, Stephanie Strassel, Nizar Habash?, Ramy Eskander?, Owen Rambow? 
Linguistic Data Consortium, University of Pennsylvania 
{bies,zhiyi,maamouri,sgrimes,haejoong, 
jdwright,strassel}@ldc.upenn.edu 
?Computer Science Department, New York University Abu Dhabi 
?
nizar.habash@nyu.edu 
?Center for Computational Learning Systems, Columbia University 
?
{reskander,rambow}@ccls.columbia.edu 
 
  
 
Abstract 
This paper describes the process of creating a 
novel resource, a parallel Arabizi-Arabic 
script corpus of SMS/Chat data.  The lan-
guage used in social media expresses many 
differences from other written genres: its vo-
cabulary is informal with intentional devia-
tions from standard orthography such as re-
peated letters for emphasis; typos and non-
standard abbreviations are common; and non-
linguistic content is written out, such as 
laughter, sound representations, and emoti-
cons.  This situation is exacerbated in the 
case of Arabic social media for two reasons.  
First, Arabic dialects, commonly used in so-
cial media, are quite different from Modern 
Standard Arabic phonologically, morphologi-
cally and lexically, and most importantly, 
they lack standard orthographies. Second, 
Arabic speakers in social media as well as 
discussion forums, SMS messaging and 
online chat often use a non-standard romani-
zation called Arabizi.  In the context of natu-
ral language processing of social media Ara-
bic, transliterating from Arabizi of various 
dialects to Arabic script is a necessary step, 
since many of the existing state-of-the-art re-
sources for Arabic dialect processing expect 
Arabic script input.  The corpus described in 
this paper is expected to support Arabic NLP 
by providing this resource. 
1 Introduction 
The language used in social media expresses 
many differences from other written genres: its 
vocabulary is informal with intentional devia-
tions from standard orthography such as repeated 
letters for emphasis; typos and non-standard ab-
breviations are common; and non-linguistic con-
tent is written out, such as laughter, sound repre-
sentations, and emoticons. 
This situation is exacerbated in the case of Ar-
abic social media for two reasons.  First, Arabic 
dialects, commonly used in social media, are 
quite different from Modern Standard Arabic 
(MSA) phonologically, morphologically and lex-
ically, and most importantly, they lack standard 
orthographies (Maamouri et.al. 2014). Second, 
Arabic speakers in social media as well as dis-
cussion forums, Short Messaging System (SMS) 
text messaging and online chat often use a non-
standard romanization called ?Arabizi? (Dar-
wish, 2013).  Social media communication in 
Arabic takes place using a variety of orthogra-
phies and writing systems, including Arabic 
script, Arabizi, and a mixture of the two.  Alt-
hough not all social media communication uses 
Arabizi, the use of Arabizi is prevalent enough to 
pose a challenge for Arabic NLP research. 
In the context of natural language processing 
of social media Arabic, transliterating from 
Arabizi of various dialects to Arabic script is a 
necessary step, since many of the existing state-
of-the-art resources for Arabic dialect processing 
and annotation expect Arabic script input (e.g., 
Salloum and Habash, 2011; Habash et al. 2012c; 
Pasha et al., 2014). 
To our knowledge, there are no naturally oc-
curring parallel texts of Arabizi and Arabic 
script.  In this paper, we describe the process of 
creating such a novel resource at the Linguistic 
Data Consortium (LDC).  We believe this corpus 
will be essential for developing robust tools for 
converting Arabizi into Arabic script. 
93
The rest of this paper describes the collection 
of Egyptian SMS and Chat data and the creation 
of a parallel text corpus of Arabizi and Arabic 
script for the DARPA BOLT program.1  After 
reviewing the history and features in Arabizi 
(Section 2) and related work on Arabizi (Section 
3), in Section 4, we describe our approach to col-
lecting the Egyptian SMS and Chat data and the 
annotation and transliteration methodology of the 
Arabizi SMS and Chat into Arabic script, while 
in Section 5, we discuss the annotation results, 
along with issues and challenges we encountered 
in annotation. 
2 Arabizi and Egyptian Arabic Dialect 
2.1 What is Arabizi? 
Arabizi is a non-standard romanization of Arabic 
script that is widely adopted for communication 
over the Internet (World Wide Web, email) or 
for sending messages (instant messaging and 
mobile phone text messaging) when the actual 
Arabic script alphabet is either unavailable for 
technical reasons or otherwise more difficult to 
use.  The use of Arabizi is attributed to different 
reasons, from lack of good input methods on 
some mobile devices to writers? unfamiliarity 
with Arabic keyboard.  In some cases, writing in 
Arabizi makes it easier to code switch to English 
or French, which is something educated Arabic 
speakers often do.  Arabizi is used by speakers of 
a variety of Arabic dialects. 
Because of the informal nature of this system, 
there is no single ?correct? encoding, so some 
character usage overlaps.  Most of the encoding 
in the system makes use of the Latin character 
(as used in English and French) that best approx-
imates phonetically the Arabic letter that one 
wants to express (for example, either b or p cor-
responds to ?).  This may sometimes vary due to 
regional variations in the pronunciation of the 
Arabic letter (e.g., j is used to represent ? in the 
Levantine dialect, while in Egyptian dialect g is 
used) or due to differences in the most common 
non-Arabic second language (e.g., sh corre-
sponds to ? in the previously English dominated 
Middle East Arab countries, while ch shows a 
predominantly French influence as found in 
North Africa and Lebanon).  Those letters that do 
not have a close phonetic approximate in the Lat-
in script are often expressed using numerals or 
other characters, so that the numeral graphically 
                                                 
1 http://www.darpa.mil/Our_Work/I2O/Programs/Broad_Op 
erational_Language_Translation_%28BOLT%29.aspx 
approximates the Arabic letter that one wants to 
express (e.g., the numeral 3 represents ? because 
it looks like a mirror reflection of the letter). 
Due to the use of Latin characters and also 
frequent code switching in social media Arabizi, 
it can be difficult to distinguish between Arabic 
words written in Arabizi and entirely unrelated 
foreign language words (Darwish 2013).  For 
example, mesh can be the English word, or 
Arabizi for ?? ?not?.  However, in context these 
cases can be clearly labeled as either Arabic or a 
foreign word.  An additional complication is that 
many words of foreign origin have become Ara-
bic words (?borrowings?).  Examples include 
banadoora ?????? ?tomato? and mobile ?????? 
?mobile phone?.  It is a well-known practical and 
theoretical problem to distinguish borrowings 
(foreign words that have become part of a lan-
guage and are incorporated fully into the mor-
phological and syntactic system of the host lan-
guage) from actual code switching (a bilingual 
writer switches entirely to a different language, 
even if for only a single word).  Code switching 
is easy to identify if we find an extended passage 
in the foreign language which respects that lan-
guage?s syntax and morphology, such as Bas eh 
ra2yak I have the mask.  The problem arises 
when single foreign words appear without Arabic 
morphological marking: it is unclear if the writer 
switched to the foreign language for one word or 
whether he or she simply is using an Arabic 
word of foreign origin.  In the case of banadoora 
?????? ?tomato?, there is little doubt that this has 
become a fully Arabic word and the writer is not 
code switching into Italian; this is also signaled 
by the fact that a likely Arabizi spelling (such as 
banadoora) is not in fact the Italian orthography 
(pomodoro).  However, the case is less clear cut 
with mobile ?????? ?mobile phone?: even if it is a 
borrowing (clearly much more recent than bana-
doora ?????? ?tomato?), a writer will likely spell 
the word with the English orthography as mobile 
rather than write, say, mubail.  More research is 
needed on this issue.  However, because of the 
difficulty of establishing the difference between 
code switching and borrowing, we do not attempt 
to make this distinction in this annotation 
scheme. 
2.2 Egyptian Arabic Dialect 
Arabizi is used to write in multiple dialects of 
Arabic, and differences between the dialects 
themselves have an effect on the spellings cho-
sen by individual writers using Arabizi.  Because 
Egyptian Arabic is the dialect of the corpus cre-
94
ated for this project, we will briefly discuss some 
of the most relevant features of Egyptian Arabic 
with respect to Arabizi transliteration.  For a 
more extended discussion of the differences be-
tween MSA and Egyptian Arabic, see Habash et 
al. (2012a) and Maamouri et al. (2014). 
Phonologically, Egyptian Arabic is character-
ized by the following features, compared with 
MSA: 
(a) The loss of the interdentals /?/ and /?/ 
which are replaced by /d/ or /z/ and /t/ or /s/ 
respectively, thus giving those two original 
consonants a heavier load. Examples in-
clude  ??? /zakar/ ?to mention?, ???  /daba?/ 
?to slaughter?,  ???  /talg/ ?ice?,  ???  /taman/ 
?price?, and  ???  /sibit/ ?to stay in place, 
become immobile?. 
(b) The exclusion of /q/ and /?/ from the conso-
nantal system, being replaced by the /?/ and 
/g/, e.g., ???  /?u?n/ ?cotton?, and  ???  
/gamal/ ?camel?. 
At the level of morphology and syntax, the 
structures of Egyptian Arabic closely resemble 
the overall structures of MSA with relatively mi-
nor differences to speak of.  Finally, the Egyptian 
Arabic lexicon shows some significant elements 
of semantic differentiation. 
The most important morphological difference 
between Egyptian Arabic and MSA is in the use 
of some Egyptian clitics and affixes that do not 
exist in MSA.  For instance, Egyptian Arabic has 
the future proclitics h+ and ?+ as opposed to the 
standard equivalent s+. 
Lexically, there are lexical differences be-
tween Egyptian Arabic and MSA where no ety-
mological connection or no cognate spelling is 
available.  For example, the Egyptian Arabic ??  
/bu??/ ?look? is ???? /?unZur/ in MSA. 
3 Related Work 
Arabizi-Arabic Script Transliteration  Previ-
ous efforts on automatic transliterations from 
Arabizi to Arabic script include work by Chalabi 
and Gerges (2012), Darwish (2013) and Al-
Badrashiny et al. (2014).  All of these approaches 
rely on a model for character-to-character map-
ping that is used to generate a lattice of multiple 
alternative words which are then selected among 
using a language model.  The training data used 
by Darwish (2013) is publicly available but it is 
quite limited (2,200 word pairs).  The work we 
are describing here can help substantially im-
prove the quality of such system.  We use the 
system of Al-Badrashiny et al. (2014) in this pa-
per as part of the automatic transliteration step 
because they target the same conventional or-
thography of dialectal Arabic (CODA) (Habash 
et al., 2012a, 2012b), which we also target.  
There are several commercial products that con-
vert Arabizi to Arabic script, namely: Microsoft 
Maren, 2  Google Ta3reeb, 3  Basis Arabic chat 
translator4 and Yamli.5  Since these products are 
for commercial purposes, there is little infor-
mation available about their approaches, and 
whatever resources they use are not publicly 
available for research purposes.  Furthermore, as 
Al-Badrashiny et al. (2014) point out, Maren, 
Ta3reeb and Yamli are primarily intended as in-
put method support, not full text transliteration.  
As a result, their users? goal is to produce Arabic 
script text not Arabizi text, which affects the 
form of the romanization they utilize as an in-
termediate step.  The differences between such 
?functional romanization? and real Arabizi in-
clude that the users of these systems will use less 
or no code switching to English, and may em-
ploy character sequences that help them arrive at 
the target Arabic script form faster, which other-
wise they would not write if they were targeting 
Arabizi (Al-Badrashiny et al., 2014). 
Name Transliteration  There has been some 
work on machine transliteration by Knight and 
Graehl (1997).  Al-Onaizan and Knight (2002) 
introduced an approach for machine translitera-
tion of Arabic names. Freeman et al. (2006) also 
introduced a system for name matching between 
English and Arabic.  Although the general goal 
of transliterating from one script to another is 
shared between these efforts and ours, we are 
considering a more general form of the problem 
in that we do not restrict ourselves to names. 
Code Switching  There is some work on code 
switching between Modern Standard Arabic 
(MSA) and dialectal Arabic (DA).  Zaidan and 
Callison-Burch (2011) were interested in this 
problem at the inter-sentence level.  They 
crawled a large dataset of MSA-DA news com-
mentaries, and used Amazon Mechanical Turk to 
annotate the dataset at the sentence level.  
Elfardy et al. (2013) presented a system, AIDA, 
that tags each word in a sentence as either DA or 
MSA based on the context.  Lui et al. (2014) 
proposed a system for language identification in 
                                                 
2 http://www.getmaren.com 
3 http://www.google.com/ta3reeb 
4 http://www.basistech.com/arabic-chat-translator-
transforms-social-media-analysis/ 
5 http://www.yamli.com/ 
95
multilingual documents using a generative mix-
ture model that is based on supervised topic 
modeling algorithms.  Darwish (2013) and Voss 
et al. (2014) deal with exactly the problem of 
classifying tokens in Arabizi as Arabic or not.  
More specifically, Voss et al. (2014) deal with 
Moroccan Arabic, and with both French and 
English, meaning they do a three-way classifica-
tion.  Darwish (2013)'s data is more focused on 
Egyptian and Levantine Arabic and code switch-
ing with English. 
Processing Social Media Text  Finally, while 
English NLP for social media has attracted con-
siderable attention recently (Clark and Araki, 
2011; Gimpel et al., 2011; Gouws et al., 2011; 
Ritter et al., 2011; Derczynski et al., 2013), there 
has not been much work on Arabic yet.  Darwish 
et al. (2012) discuss NLP problems in retrieving 
Arabic microblogs (tweets).  They discuss many 
of the same issues we do, notably the problems 
arising from the use of dialectal Arabic such as 
the lack of a standard orthography.  Eskander et 
al. (2013) described a method for normalizing 
spontaneous orthography into CODA. 
4 Corpus Creation 
This work was prepared as part of the DARPA 
Broad Operational Language Translation 
(BOLT) program which aims at developing tech-
nology that enables English speakers to retrieve 
and understand information from informal for-
eign language sources including chat, text mes-
saging and spoken conversations. LDC collects 
and annotates informal linguistic data of English, 
Chinese and Arabic, with Egyptian Arabic being 
the representative of the Arabic language family.  
 
 
Egyptian Arabic has the advantage over all other 
dialects of Arabic of being the language of the 
largest linguistic community in the Arab region, 
and also of having a rich level of internet com-
munication.  
4.1 SMS and Chat Collection 
In BOLT Phase 2, LDC collected large volumes 
of naturally occurring informal text (SMS) and 
chat messages from individual users in English, 
Chinese and Egyptian Arabic (Song et al., 2014).  
Altogether we recruited 46 Egyptian Arabic par-
ticipants, and of those 26 contributed data.  To 
protect privacy, participation was completely 
anonymous, and demographic information was 
not collected.  Participants completed a brief lan-
guage test to verify that they were native Egyp-
tian Arabic speakers.  On average, each partici-
pant contributed 48K words.  The Egyptian Ara-
bic SMS and Chat collection consisted of 2,140 
conversations in a total of 475K words after 
manual auditing by native speakers of Egyptian 
Arabic to exclude inappropriate messages and 
messages that were not Egyptian Arabic.  96% of 
the collection came from the personal SMS or 
Chat archives of participants, while 4% was col-
lected through LDC?s platform, which paired 
participants and captured their live text messag-
ing (Song et al., 2014).  A subset of the collec-
tion was then partitioned into training and eval 
datasets.   
Table 1 shows the distribution of Arabic script 
vs. Arabizi in the training dataset.  The conversa-
tions that contain Arabizi were then further anno-
tated and transliterated to create the Arabizi-
Arabic script parallel corpus, which consists of 
 
 
 Total Arabic 
script only 
Arabizi 
only 
Mix of Arabizi and Arabic script 
Arabizi Arabic script 
Conversations 1,503 233 987 283 
Messages 101,292 18,757 74,820 3,237 4,478 
Sentence units 94,010 17,448 69,639 3,017 3,906 
Words 408,485 80,785 293,900 10,244 23,556 
Table 1. Arabic SMS and Chat Training Dataset 
 
1270 conversations. 6   All conversations in the 
training dataset were also translated into English 
to provide Arabic-English parallel training data. 
                                                 
6 In order to form single, coherent units (Sentence units) of 
an appropriate size for downstream annotation tasks using 
this data, messages that were split mid-sentence (often mid-
Not surprisingly, most Egyptian conversations 
in our collection contain at least some Arabizi; 
                                                                          
word) due to SMS messaging character limits were rejoined, 
and very long messages (especially common in chat) were 
split into two or more units, usually no longer than 3-4 sen-
tences. 
96
only 15% of conversations are entirely written in 
Arabic script, while 66% are entirely Arabizi.  
The remaining 19% contain a mixture of the two 
at the conversation level.  Most of the mixed 
conversations were mixed in the sense that one 
side of the conversation was in Arabizi and the 
other side was in Arabic script, or in the sense 
that at least one of the sides switched between 
the two forms in mid-conversation.  Only rarely 
are individual messages in mixed scripts.  The 
annotation for this project was performed on the 
Arabizi tokens only.  Arabic script tokens were 
not touched and were kept in their original 
forms.  
The use of Arabizi is predominant in the SMS 
and Chat Egyptian collection, in addition to the 
presence of other typical cross-linguistic text ef-
fects in social media data.  For example, the use 
of emoticons and emoji is frequent.  We also ob-
served the frequent use of written out representa-
tions of speech effects, including representations 
of laughter (e.g., hahaha), filled pauses (e.g., 
um), and other sounds (e.g., hmmm).  When these 
representations are written in Arabizi, many of 
them are indistinguishable from the same repre-
sentations in English SMS data.  Neologisms are 
also frequently part of SMS/Chat in Egyptian  
 
Arabic, as they are in other languages.  English 
words use Arabic morphology or determiners, as 
in el anniversary ?the anniversary?.  Sometimes 
English words are spelled in a way that is closer 
phonetically to the way an Egyptian speaker 
would pronounce them, for example lozar for 
?loser?, or beace for ?peace?. 
The adoption of Arabizi for SMS and online 
chat may also go some way to explaining the 
high frequency of code mixing in the Egyptian 
Arabic collection.  While the auditing process 
eliminated messages that were entirely in a non-
target language, many of the acceptable messag-
es contain a mixture of Egyptian Arabic and 
English. 
4.2 Annotation Methodology 
All of the Arabizi conversations, including the 
conversations containing mixtures of Arabizi and 
Arabic script were then annotated and translit-
erated: 
1. Annotation on the Arabizi source text to 
flag certain features 
2. Correction and normalization of the trans-
literation according to CODA conventions 
 
 
 
Figure 1. Arabizi Annotation and Transliteration Tool 
 
The annotators were presented with the source 
conversations in their original Arabizi form as 
well as the transliteration output from an auto-
matic Arabization system, and used a web-based 
tool developed by LDC (see Figure 1) to perform 
the two annotation tasks, which allowed annota-
tors perform both annotation and transliteration 
token by token, sentence by sentence and review 
the corrected transliteration in full context.  The 
GUI shows the full conversation in both the orig-
inal Arabizi and the resulting Arabic script trans-
literation for each sentence.  Annotators must 
97
annotate each sentence in order, and the annota-
tion is displayed in three columns.  The first col-
umn shows the annotation of flag features on the 
source tokens, the second column is the working 
panel where annotators correct the automatic 
transliteration and retokenize, and the third col-
umn displays the final corrected and retokenized 
result. 
Annotation was performed according to anno-
tation guidelines developed at the Linguistic Da-
ta Consortium specifically for this task (LDC, 
2014). 
4.3 Automatic Transliteration 
To speed up the annotation process, we utilized 
an automatic Arabizi-to-Arabic script translitera-
tion system (Al-Badrashiny et al., 2014) which 
was developed using a small vocabulary of 2,200 
words from Darwish (2013) and an additional 
6,300 Arabic-English proper name pairs (Buck-
walter, 2004).  The system has an accuracy of 
69.4%.  We estimate that using this still allowed 
us to cut down the amount of time needed to type 
in the Arabic script version of the Arabizi by 
two-thirds.  This system did not identify Foreign 
words or Names and transliterated all of the 
words.  In one quarter of the errors, the provided 
answer was plausible but not CODA-compliant 
(Al-Badrashiny et al., 2014). 
4.4 Annotation on Arabizi Source Text to 
Flag Features 
This annotation was performed only on sentences 
containing Arabizi words, with the goal of tag-
ging any words in the source Arabizi sentences 
that would be kept the same in the output of an 
English translation with the following flags: 
 
? Punctuation (not including emoticons) 
o Eh ?!//Punct  
o Ma32ula ?!//Punct 
o Ebsty ?//Punct  
 
? Sound effects, such as laughs (?haha? or 
variations), filled pauses, and other sounds 
(?mmmm? or ?shh? or ?um? etc.) 
o hahhhahhah//Sound akeed 3arfa :p da 
enty t3rafy ablia :pp 
o Hahahahaahha//Sound Tb ana ta7t fel 
ahwaa 
o Wala Ana haha//Sound 
o Mmmm//Sound okay 
 
? Foreign language words and numbers.  All 
cases of code switching and all cases of bor-
rowings which are rendered in Arabizi us-
ing standard English orthography are 
marked as ?Foreign?. 
o ana kont mt25er fe t2demm l pro-
jects//Foreign 
o oltilik okay//Foreign ya Babyy//Foreign 
balashhabal!!!! 
o zakrty ll sat//Foreign 
o Bat3at el whatsapp//Foreign 
o La la la merci//Foreign gedan bs la2 
o We 9//Foreign galaeeb dandash lel ban-
at 
 
? Names, mainly person names 
o Youmna//Name 7atigi?? 
 
4.5 Correction and Normalization of the 
Transliteration According to CODA 
Conventions 
The goal of this task was to correct all spelling in 
the Arabic script transliteration to CODA stand-
ards (Habash et al., 2012a, 2012b).  This meant 
that annotators were required to confirm both (1) 
that the word was transliterated into Arabic script 
correctly and also (2) that the transliterated word 
conformed to CODA standards.  The automatic 
transliteration was provided to the annotators, 
and manually corrected by annotators as needed. 
Correcting spelling to a single standard (CO-
DA), however, necessarily included some degree 
of normalization of the orthography, as the anno-
tators had to correct from a variety of dialect 
spellings to a single CODA-compliant spelling 
for each word.  Because the goal was to reach a 
consistent representation of each word, ortho-
graphic normalization was almost the inevitable 
effect of correcting the automatic transliteration.  
This consistent representation will allow down-
stream annotation tasks to take better advantage 
of the SMS/Chat data.  For example, more con-
sistent spelling of Egyptian Arabic words will 
lead to better coverage from the CALIMA mor-
phological analyzer and therefore improve the 
manual annotation task for morphological anno-
tation, as in Maamouri et al. (2014). 
 
Modern Standard Arabic (MSA) cognates and 
Egyptian Arabic sound changes 
Annotators were instructed to use MSA or-
thography if the word was a cognate of an MSA 
98
root, including for those consonants that have 
undergone sound changes in Egyptian Arabic.7 
? use mqfwl ?????  and not ma>fwl ?????  for 
?locked? 
? use HAfZ ???? and not HAfz ???? for the 
name (a proper noun)  
 
Long vowels 
Annotators were instructed to reinstate miss-
ing long vowels, even when they were written as 
short vowels in the Arabizi source, and to correct 
long vowels if they were included incorrectly. 
? use sAEap ???? and not saEap  ???  for 
?hour? 
? use qAlt   ????  and not qlt ??? for ?(she) 
said? 
 
Consonantal ambiguities 
Many consonants are ambiguous when written 
in Arabizi, and many of the same consonants are 
also difficult for the automatic transliteration 
script.  Annotators were instructed to correct any 
errors of this type.   
? S vs. s/ ?  vs. ? 
o use SAyg ????  and not  sAyg  ????  for 
?jeweler? 
? D vs. Z/ ?  vs. ? 
o use DAbT ????  and not  ZAbT ???? for 
?officer? 
o use Zlmp  ????  and not Dlmp  ????  for 
?darkness? 
? Dotted ya vs. Alif Maqsura/ ? vs. ?.  Alt-
hough the dotted ya/ ? and Alif Maqsura/ ? 
are often used interchangeably in Egyptian 
Arabic writing conventions, it was neces-
sary to make the distinction between the 
two for this task. 
o use Ely ???  and not ElY  ???  for ?Ali? 
(the proper name)  
? Taa marbouta.  In Arabizi and so also in the 
Arabic script transliteration, the taa mar-
bouta/ ? may be written for both nominal fi-
nal -h/ ? and verbal final -t/ ?, but for dif-
ferent reasons. 
o mdrsp Ely  ???  ?????  ?Ali?s school? 
o mdrsth  ??????  ?his school? 
 
Morphological ambiguities 
Spelling variation and informal usage can 
combine to create morphological ambiguities as 
well.  For example, the third person masculine 
                                                 
7 Both Arabic script and the Buckwalter transliteration 
(http://www.qamus.org/transliteration.htm) are shown for 
the transliterated examples in this paper. 
singular pronoun and the third person plural ver-
bal suffix can be ambiguous in informal texts.  
For example: 
? use byHbwA bED  ???  ?????? and not byHbh 
bED  ???  ?????  for ?(They) loved each oth-
er? 
? use byEmlwA  ???????  and not byEmlh  ??????  
for ?(They) did? or ?(They) worked? 
In addition, because final -h is sometimes re-
placed in speech by final /-uw/, it was occasion-
ally necessary to correct cases of overuse of the 
third person plural verbal suffix (-wA) to the 
pronoun -h as well. 
 
Merging and splitting tokens written with in-
correct word boundaries 
Annotators were instructed to correct any 
word that was incorrectly segmented.  The anno-
tation tool allowed both the merging and splitting 
of tokens. 
Clitics were corrected to be attached when 
necessary according to (MSA) standard writing 
conventions.  These include single letter proclit-
ics (both verbal and nominal) and the negation 
suffix -$, as well as pronominal clitics such as 
possessive pronouns and direct object pronouns.  
For example, 
? use fAlbyt  ??????    and not  
fAl  byt  ???  ??? or  flbyt  ?????   for ?in the 
house? 
? use EAlsTH ?????? and not  
EAl sTH ??? ??? or ElsTH ????? for ?on the 
roof? 
The conjunction w- / -? is always attached to 
its following word. 
? use wkAn  ????  and not w kAn  ??? ?    for 
?and was? 
? use wrAHt  ????? and not w  rAHt ????  ?  
for ?and (she) left? 
Words that were incorrectly segmented in the 
Arabizi source were also merged.  For example, 
? use msHwrp ?????? and not  
ms Hwrp ???? ??  for ?bewitched 
(fem.sing.)? 
? use $ErhA ????? and not $Er hA  ?? ???   for 
?her hair? 
Particles that are not attached in standard 
MSA written forms were corrected as necessary 
by the splitting function of the tool.  For exam-
ple,  
? use yA Emry  ???? ?? and not yAEmry  
??????  for ?Hey, dear!? 
? use lA trwH  ????  ? and not lAtrwH  ?????  
for ?Do not go? 
99
 
Abbreviations in Arabizi 
Three abbreviations in Arabizi received spe-
cial treatment: msa, isa, 7ma.  These three abbre-
viations only were expanded out to their full 
form using Arabic words in the corrected Arabic 
script transliteration. 
? msa: use mA $A' All~h  ? ???  ?? for ?As 
God wills? 
? isa: use <n $A' All~h  ? ???  ?? for ?God 
willing? 
? 7ma: use AlHmd ll~h for     ??? ??  ?Thank 
God, Praised be the Lord? 
All other Arabic abbreviations were not ex-
panded, and were transliterated simply letter for 
letter.  When the abbreviation was in English or 
another foreign language, it was kept as is in the 
transliteration, using both consonants and semi-
vowels to represent it. 
? use Awkyh  ????  for ?OK? (note that this is 
an abbreviation in English, but not in Egyp-
tian Arabic) 
 
Correcting Arabic typos 
Annotators were instructed to correct typos in 
the transliterated Arabic words, including typos 
in proper names.  However, typos and non-
standard spellings in the transliteration of a for-
eign words were kept as is and not corrected. 
? Ramafan  ?????  should be corrected to 
rmDAn  ?????  for ?Ramadan? 
? babyy  ????  since it is the English word ?ba-
by? it should not be corrected 
 
Flagged tokens in the correction task 
Tokens flagged during task 1 as Sound and 
Foreign were transliterated into Arabic script but 
were not corrected during task 2.  Note that even 
when a whole phrase or sentence appeared in 
English, the transliteration was not corrected. 
? ks  ??  for ?kiss? 
? Dd yA hAf fAn  ??? ???  ??  ??  for ?did you 
have fun? 
The transliteration of proper names was cor-
rected in the same way as all other words. 
Emoticons and emoji were replaced in the 
transliteration with #.  Emoticons refer to a set of 
numbers or letters or punctuation marks used to 
express feelings or mood.  Emoji refers to a spe-
cial set of images used in messages.  Both Emot-
icons and Emoji are frequent in SMS/Chat data. 
5 Discussion 
Annotation and transliteration were performed 
on all sentence units that contain Arabizi.  Sen-
tence units that contain only Arabic script were 
ignored and untouched during annotation.  In 
total, we reviewed 1270 conversations, among 
which over 42.6K sentence units (more than 
300K words) were deemed to be containing 
Arabizi and hence annotated and transliterated. 
The corpus files are in xml format.  All con-
versations have six layers: source, annotation on 
the source Arabizi tokens, automatic translitera-
tion via 3ARRIB, manual correction of the au-
tomatic transliteration, re-tokenized corrected 
transliteration, and human translation.  See Ap-
pendix A for examples of the file format. 
Each conversation was annotated by one anno-
tator, with 10 percent of the data being reviewed 
by a second annotator as a QC procedure.  Twen-
ty six conversations (roughly 3400 words) were 
also annotated dually by blind assignment to 
gauge inter-annotator agreement. 
As we noted earlier, code switching is fre-
quent in the SMS and Chat Arabizi data.  There 
were about 23K words flagged as foreign words.  
Written out speech effects in this type of data are 
also prevalent, and 6610 tokens were flagged as 
Sounds (laughter, filled pause, etc.).  Annotators 
most often agreed with each other in the detec-
tion and flagging of tokens as Foreign, Name, 
Sound or Punctuation, with over 98% agreement 
for all flags. 
The transliteration annotation was more diffi-
cult than the flagging annotation, because apply-
ing CODA requires linguistic knowledge of Ara-
bic.  Annotators went through several rounds of 
training and practice and only those who passed 
a test were allowed to work on the task.  In an 
analysis of inter-annotator agreement in the dual-
ly annotated files, the overall agreement between 
the two annotators was 86.4%.  We analyzed all 
the disagreements and classified them in four 
high level categories: 
? CODA  60% of the disagreements were related 
to CODA decisions that did not carefully follow 
the guidelines.  Two-fifths of these cases were 
related to Alif/Ya spelling (mostly Alif Hamza-
tion, rules of hamza support) and about one-fifth 
involved the spelling of common dialectal words.  
An additional one-third were due to non-CODA 
root, pattern or affix spelling.  Only one-tenth of 
the cases were because of split or merge deci-
sions.  These issues suggest that additional train-
ing may be needed.  Additionally, since some of 
100
the CODA errors may be easy to detect and cor-
rect using available tools for morphological 
analysis of Egyptian Arabic (such as the CALI-
MA-ARZ analyzer), we will consider integrating 
such support in the annotation interface in the 
future.  
? Task  In 23% of the overall disagreements, the 
annotators did not follow the task guidelines for 
handling punctuation, sounds, emoticons, names 
or foreign words.  Examples include disagree-
ment on whether a question mark should be split 
or kept attached, or whether a non-Arabic word 
should be corrected or not.  Many of these cases 
can also be caught as part of the interface; we 
will consider the necessary extensions in the fu-
ture. 
? Ambiguity  In 12% of the cases, the annota-
tors? disagreement reflected a different reading 
of the Arabizi resulting in a different lemma or 
inflectional feature.  These differences are una-
voidable and reflect the natural ambiguity in the 
task. 
? Typos  Finally, in less than 5% of the cases, 
the disagreement was a result of a typographical 
error unrelated to any of the above issues.  
Among the cases that were easy to adjudicate, 
one of the two annotators was correct 60% more 
than the other.  This is consistent with the obser-
vation that more training may be needed to fill in 
some of the knowledge gaps or increase the an-
notator?s attention to detail. 
6 Conclusion 
This is the first Arabizi-Arabic script parallel 
corpus that supports research on transliteration 
from Arabizi to Arabic script.  We expect to 
make this corpus available through the Linguistic 
Data Consortium in the near future. 
This work focuses on the novel challenges of 
developing a corpus like this, and points out the 
close interaction between the orthographic form 
of written informal genres of Arabic and the spe-
cific features of individual Arabic dialects.  The 
use of Arabizi and the use of Egyptian Arabic in 
this corpus come together to present a host of 
spelling ambiguities and multiplied forms that 
were resolved in this corpus by the use of CODA 
for Egyptian Arabic.  Developing a similar cor-
pus and transliteration for other Arabic dialects 
would be a rich area for future work. 
We believe this corpus will be essential for 
NLP work on Arabic dialects and informal gen-
res.  In fact, this corpus has recently been used in 
development by Eskander et al. (2014). 
Acknowledgements 
This material is based upon work supported by 
the Defense Advanced Research Projects Agency 
(DARPA) under Contract No. HR0011-11-C-
0145. The content does not necessarily reflect the 
position or the policy of the Government, and no 
official endorsement should be inferred. 
Nizar Habash performed most of his contribu-
tion to this paper while he was at the Center for 
Computational Learning Systems at Columbia 
University. 
References 
Mohamed Al-Badrashiny, Ramy Eskander, Nizar Ha-
bash, and Owen Rambow. 2014. Automatic Trans-
literation of Romanized Dialectal Arabic. In Pro-
ceedings of the Conference on Computational Nat-
ural Language Learning (CONLL), Baltimore, 
Maryland, 2014. 
Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number 
LDC2004L02, ISBN 1-58563-324-0.  
Achraf Chalabi and Hany Gerges. 2012. Romanized 
Arabic Transliteration. In Proceedings of the Sec- 
ond Workshop on Advances in Text Input Methods 
(WTIM 2012).  
Eleanor Clark and Kenji Araki. 2011. Text normaliza-
tion in social media: Progress, problems and ap- 
plications for a pre-processing system of casual 
English. Procedia - Social and Behavioral Scienc-
es, 27(0):2 ? 11. 
Kareem Darwish, Walid Magdy, and Ahmed Mourad. 
2012. Language processing for arabic microblog 
re- trieval. In Proceedings of the 21st ACM Inter-
national Conference on Information and 
Knowledge Management, CIKM ?12, pages 2427?
2430, New York, NY, USA. ACM.  
Kareem Darwish. 2013. Arabizi Detection and Con- 
version to Arabic. CoRR, arXiv:1306.6755 [cs.CL]. 
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina 
Bontcheva. 2013. Twitter part-of-speech tagging 
for all: Overcoming sparse and noisy data. In Pro-
ceedings of the International Conference Recent 
Advances in Natural Language Processing RANLP 
2013, pages 198?206, Hissar, Bulgaria, September. 
INCOMA Ltd. Shoumen, Bulgaria.  
Heba Elfardy, Mohamed Al-Badrashiny, and Mona 
Diab. 2013. Code Switch Point Detection in Ara-
bic. In Proceedings of the 18th International Con-
ference on Application of Natural Language to In-
formation Systems (NLDB2013), MediaCity, UK, 
June.  
Ramy Eskander, Mohamed Al-Badrashiny, Nizar Ha-
bash and Owen Rambow. 2014. Foreign Words 
101
and the Automatic Processing of Arabic Social 
Media Text Written in Roman Script. In Arabic 
Natural Language Processing Workshop, EMNLP, 
Doha, Qatar. 
Ramy Eskander, Nizar Habash, Owen Rambow, and 
Nadi Tomeh. 2013. Processing Spontaneous Or- 
thography. In Proceedings of the 2013 Conference 
of the North American Chapter of the Association 
for Computational Linguistics: Human Language 
Technologies (NAACL-HLT), Atlanta, GA.  
Andrew T. Freeman, Sherri L. Condon and Christo-
pher M. Ackerman. 2006. Cross Linguistic Name 
Matching in English and Arabic: A ?One to Many 
Mapping? Extension of the Levenshtein Edit Dis-
tance Algorithm. In Proceedings of HLT-NAACL, 
New York, NY. 
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, 
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Mi-
chael Heilman, Dani Yogatama, Jeffrey Flanigan, 
and Noah A. Smith. 2011. Part-of-speech tagging 
for twitter: Annotation, features, and experiments. 
In Proceedings of ACL-HLT ?11.  
Stephan Gouws, Donald Metzler, Congxing Cai, and 
Eduard Hovy. 2011. Contextual bearing on linguis-
tic variation in social media. In Proceedings of the 
Workshop on Languages in Social Media, LSM 
?11, pages 20?29, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics. 
Nizar Habash, Mona Diab, and Owen Rambow 
(2012a).Conventional Orthography for Dialectal 
Arabic: Principles and Guidelines ? Egyptian Ara-
bic. Technical Report CCLS-12-02, Columbia 
University Center for Computational Learning Sys-
tems.  
Nizar Habash, Mona Diab, and Owen Rabmow. 
2012b. Conventional Orthography for Dialectal 
Arabic. In Proceedings of the Language Resources 
and Evaluation Conference (LREC), Istanbul.  
Nizar Habash, Ramy Eskander, and Abdelati Haw-
wari. 2012c. A Morphological Analyzer for Egyp-
tian Arabic. In Proceedings of the Twelfth Meeting 
of the Special Interest Group on Computational 
Morphology and Phonology, pages 1?9, Montr?al, 
Canada.  
Kevin Knight and Jonathan Graehl. 1997. Machine 
Transliteration. In Proceedings of the Conference 
of the Association for Computational Linguistics 
(ACL). 
Linguistic Data Consortium. 2014. BOLT Program: 
Romanized Arabic (Arabizi) to Arabic Translitera-
tion and Normalization Guidelines, Version 3.1. 
Linguistic Data Consortium, April 21, 2014.  
Marco Lui, Jey Han Lau, and Timothy Baldwin. 
2014. Automatic detection and language identifica-
tion of multilingual documents. In Proceedings of 
the Language Resources and Evaluation Confer-
ence (LREC), Reykjavik, Iceland.  
Mohamed Maamouri, Ann Bies, Seth Kulick, Michael 
Ciul, Nizar Habash and Ramy Eskander. 2014. De-
veloping a dialectal Egyptian Arabic Treebank: 
Impact of Morphology and Syntax on Annotation 
and Tool Development. In Proceedings of the Lan-
guage Resources and Evaluation Conference 
(LREC), Reykjavik, Iceland. 
Yaser Al-Onaizan and Kevin Knight. 2002. Machine 
Transliteration of Names in Arabic Text. In Pro-
ceedings of ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab, 
Ahmed El Kholy, Ramy Eskander, Nizar Habash, 
Manoj Pooleery, Owen Rambow, and Ryan M. 
Roth. 2014. MADAMIRA: A Fast, Comprehensive 
Tool for Morphological Analysis and Disambigua-
tion of Arabic. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC), Rey-
kjavik, Iceland.  
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference 
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11. 
Wael Salloum and Nizar Habash. 2011. Dialectal to 
Standard Arabic Paraphrasing to Improve Arabic- 
English Statistical Machine Translation. In Pro- 
ceedings of the First Workshop on Algorithms and 
Resources for Modelling of Dialects and Language 
Varieties, pages 10?21, Edinburgh, Scotland.  
Zhiyi Song, Stephanie Strassel, Haejoong Lee, Kevin 
Walker, Jonathan Wright, Jennifer Garland, Dana 
Fore, Brian Gainor, Preston Cabe, Thomas Thom-
as, Brendan Callahan, Ann Sawyer. Collecting 
Natural SMS and Chat Conversations in Multiple 
Languages: The BOLT Phase 2 Corpus. In Pro-
ceedings of the Language Resources and Evalua-
tion Conference (LREC) 2014, Reykjavik, Iceland. 
Clare Voss, Stephen Tratz, Jamal Laoudi, and Dou- 
glas Briesch. 2014. Finding romanized Arabic dia-
lect in code-mixed tweets. In Proceedings of the 
Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), Reykjavik, 
Iceland. 
Omar F Zaidan and Chris Callison-Burch. 2011. The 
arabic online commentary dataset: an annotated da-
taset of informal arabic with high dialectal content. 
In Proceedings of ACL, pages 37?41. 
102
Appendix A: File Format Examples 
 
 
 
Example 1: 
 
<su id="s1582"> 
  <source>marwan ? ana walahi knt gaya today :/</source> 
   <annotated_arabizi> 
        <token id="t0" tag="name">marwan</token> 
       <token id="t1" tag="punctuation">?</token> 
       <token id="t2">ana</token> 
      <token id="t3">walahi</token> 
   <token id="t4">knt</token> 
        <token id="t5">gaya</token> 
       <token id="t6" tag="foreign">today</token> 
        <token id="t7">:/</token> 
     </annotated_arabizi> 
    <auto_transliteration> :/ ???? ???? ??? ?? ???  ?????? </auto_transliteration> 
<corrected_transliteration> # ????  ???? ??? ?? ???  ?????? </corrected_transliteration> 
<retokenized_transliteration> # ???? ???? ??? ?? ???  ?????? </retokenized_transliteration> 
     <translation lang="eng">Marwan? I swear I was coming today :/</translation> 
     <messages> 
<message id="m2377" time="2013-10-01 22:03:34 UTC" participant="139360">marwan ? ana 
walahi knt gaya today :/</message> 
     </messages> 
  </su> 
 
Example 2: 
 
<su id="s3"> 
<source>W sha3rak ma2sersh:D haha</source> 
<annotated_arabizi> 
<token id="t0">W</token> 
<token id="t1">sha3rak</token> 
<token id="t2">ma2sersh:D</token> 
<token id="t3" tag="sound">haha</token> 
</annotated_arabizi> 
<auto_transliteration> ?? # [-]????? ???? [+]? </auto_transliteration> 
<corrected_transliteration> ?? #[-]????[-]?? ???? [+]? </corrected_transliteration> 
<retokenized_transliteration> ?? # ???? ?? ????? </retokenized_transliteration> 
<translation lang="eng">And your hair did not become short? :D Haha</translation> 
<messages> 
<message id="m0004" medium="IM" time="2012-12-22 15:36:31 UTC" participant="138112">W 
sha3rak ma2sersh:D haha</message> 
</messages> 
</su> 
 
 
 
103
