Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 27?34,
Prague, June 2007. c?2007 Association for Computational Linguistics
Lemmatization of Polish Person Names
Jakub Piskorski
European Commission
Joint Research Centre
Via Fermi 1
21020 Ispra, Italy
Jakub.Piskorski@jrc.it
Marcin Sydow
Polish-Japanese Institute
of Information Technology
Koszykowa 86
02-008 Warsaw, Poland
msyd@pjwstk.edu.pl
Anna Kups?c?
Universit? Paris3/LLF, PAS ICS
Case Postale 7031
2, place Jussieu
75251 Paris Cedex 05
akupsc@univ-paris3.fr
Abstract
The paper presents two techniques for
lemmatization of Polish person names. First,
we apply a rule-based approach which re-
lies on linguistic information and heuris-
tics. Then, we investigate an alterna-
tive knowledge-poor method which employs
string distance measures. We provide an
evaluation of the adopted techniques using
a set of newspaper texts.
1 Introduction
Proper names constitute a significant part of natural
language texts (estimated to about 10% in newspa-
per articles) and are important for NLP applications,
such as Information Extraction, which rely on au-
tomatic text understanding.1 In particular, corefer-
ence resolution (e.g., identifying several name vari-
ants as referring to the same entity) plays a crucial
role in such systems. Although automatic recogni-
tion of proper names in English, French and other
major languages has been in the research focus for
over a decade now, cf. (Bikel et al, 1997), (Borth-
wick, 1999), (Li et al, 2003), only a few efforts have
been reported for Slavic languages, cf. (Cunning-
ham et al, 2003) (Russian and Bulgarian), (Pisko-
rski, 2005) (Polish). Rich inflection and a more re-
laxed word order make recognition of proper names
in Slavic more difficult than for other languages.
Moreover, inflection of proper names is usually
1The research presented in this paper was partially founded
by the Ministry of Education and Science (Poland), grant num-
ber 3T11C00727.
quite different from common nouns, which compli-
cates the lemmatization process necessary for cor-
rect coreference resolution. In this paper, we focus
on lemmatization of Polish person names, the most
idiosyncratic class of proper names in this language.
First, we report results of a rule-based symbolic ap-
proach. We apply different heuristics, mostly based
on the internal (morphological and syntactic) struc-
ture of proper names but also on the surrounding
context. Sometimes, however, the required infor-
mation is not available, even if the entire docu-
ment is considered, and lemmatization cannot be
performed. Therefore, we experimented with var-
ious knowledge-poor methods, namely string dis-
tance metrics, in order to test their usefulness for
lemmatization of Polish person names as an alterna-
tive technique, especially for cases where document-
level heuristics are insufficient.
Lemmatization of proper names in Slavic has not
attracted much attention so far but some work has
been done for Slovene: (Erjavec et al, 2004) present
a machine-learning approach to lemmatization of
unknown single-token words, whereas (Pouliquen et
al., 2005) report on a shallow approach to find base
forms.
The organization of the paper is as follows. First,
we present a description of phenomena which make
lemmatization of Polish person names a difficult
task. Next, a rule-based approach and its evaluation
are presented. Then, various string distance metrics
are introduced, followed by the results of experi-
ments on newspaper texts. The final section presents
conclusions and perspectives for future work.
27
case male name female name
nom Kazimierz Polak Kazimiera Polak
gen Kazimierza Polaka Kazimiery Polak
dat Kazimierzowi Polakowi Kazimierze Polak
acc Kazimierza Polaka Kazimiere? Polak
ins Kazimierzem Polakiem Kazimiera? Polak
loc Kazimierzu Polaku Kazimierze Polak
voc Kazimierzu Polaku Kazimiero Polak
Table 1: Declension of Polish male vs. female names
2 Declension Patterns of Polish Person
Names
Polish is a West Slavic language with rich nomi-
nal inflection: nouns and adjectives are inflected for
case, number and gender. There are 7 cases, 2 num-
bers and traditionally 3 genders are distinguished:
masculine, feminine and neuter. Just like common
nouns, Polish person names undergo declension but
their inflectional patterns are more complicated. A
typical Polish name consists of a first name and a
last name; unlike in Russian or Bulgarian, there are
no patronymics. Additionally, titles (e.g., dr ?Phd?,
inz?. ?engineer?, prof. ?professor?) or honorific forms
(pan ?Mr.? or pani ?Mrs./Miss?) are often used. In
general, both the first and the last name can be in-
flected, e.g., Jan Kowalski (nominative) vs. Jana
Kowalskiego (genitive/accusative). If the surname
is also a regular word form, things get more compli-
cated. Whether the last name can be inflected in such
cases depends on several factors, e.g., on the gen-
der of the first name, a category (part-of-speech) and
gender of the (common) word used as a surname.
For instance, if the surname is a masculine noun, it
is inflected only if the first name is also masculine.
This is illustrated in Table 1 with declension of the
male name Kazimierz Polak ?Casimir Pole? and its
variant with the female first name Kazimiera.
If the surname is an adjective (e.g., Niski ?Short?),
it is inflected (according to the adjectival paradigm)
and agrees in gender with the first name, i.e., male
and female last name forms are different (e.g., Niski
?Short? (masc.) vs. Niska ?Short? (fem.)). The de-
clension of foreign surnames may strongly depend
on their origin, and in particular on the pronuncia-
tion. For example, the name Wilde is pronounced
differently in English and German, which impacts
its declension in Polish. If it?s of English origin, a
nominal declension is applied, i.e., Wilde?a (gen.),
case sg pl sg pl
nom go?a?b go?e?bie Go?a?b Go?a?bowie
gen go?e?bia go?e?bi Go?a?ba Go?a?b?w
dat go?e?biowi go?e?biom Go?a?bowi Go?a?bom
acc go?e?bia go?e?bie Go?a?ba Go?a?b?w
ins go?e?biem go?e?biami Go?a?bem Go?a?bami
loc go?e?bia go?e?bie Go?a?biu Go?a?bach
voc go?e?biu go?e?bie Go?a?b Go?a?bowie
Table 2: Common noun vs. person name inflection
whereas if it comes from German, an adjective-like
declension is adopted: Wildego (gen.).
Declension of surnames which are also common
nouns can be different from the declension of com-
mon nouns.2 In Table 2, we present a comparison
of the common noun go?a?b ?dove? in singular and
plural with the corresponding forms used for the
surname. A comprehensive overview of this rather
intriguing declension paradigm of Polish names is
given in (Grzenia, 1998).
Finally, first name forms present problems as
well. Foreign masculine first names, whose pro-
nounced version ends in a consonant or whose writ-
ten version ends in -a, -o, -y or -i do in general
get inflected (e.g., Jacques (nom.) vs. Jacques?a
(gen./acc.)), whereas names whose pronounced ver-
sion ends in a vowel and are stressed on the last syl-
lable (e.g., Fran?ois) usually do not change form.
For female first names created from a male first
name, e.g., J?zef (masc.) vs. J?zefa (fem.), there is
a frequent homonymy between the nominative form
of the female name and the genitive/accusative form
of the corresponding male form, e.g., J?zefa is no-
minative of J?zefa (fem.) and genitive/accusative of
J?zef (masc.).
3 Rule-Based Approach to Person Name
Lemmatization
3.1 Experiment
Our rule-based approach to person name lemmatiza-
tion exploits existing resources (a dictionary of first
names and contextual triggers) and relies on con-
textual information (heuristics). It has been imple-
mented using SProUT, a shallow processing plat-
form, integrated with a Polish morphological anal-
2The declension of such surnames depends on the local tra-
dition and sometimes can be identical with the pattern used for
common nouns.
28
yser (Piskorski et al, 2004). For first names, all in-
flected forms of the most frequent Polish first names
are stored in a database so a simple gazetteer look-up
associates names with the corresponding base form.
We also used a list of ca 30 000 foreign first names
(nominative forms). For last names, we applied sev-
eral heuristic rules in order to recognize and produce
their base forms. First, we identify most common
types of Polish surnames, e.g., capitalized words
ending in -skiego, -skim, -skiemu or -icza, -iczem, -
iczu (typical last name suffixes), and convert them to
the corresponding base forms (i.e., words ending in
-ski and -icz, respectively). In this way, a significant
number of names can be lemmatized in a brute-force
manner.
For all remaining surnames, more sophisticated
rules have to be applied. As discussed in sec. 2,
these rules have to take into account several pieces
of information such as part-of-speech and gender
of the (common) word which serves as a surname,
but also gender of the first name. The major prob-
lem we encountered while applying these rules is
that the information necessary to trigger the appro-
priate rule is often missing. For example, in sen-
tence (1), inferring gender of the surname/first name
could involve a subcategorization frame for the verb
powiadomic? ?inform?, which requires an accusative
NP argument. In this way we might possibly predict
that the base form of Putina is Putin, as -a is the typi-
cal accusative ending of masculine names. Since the
subcategorization lexicon is not available, such in-
stances are either not covered or different heuristics
are employed for guessing the base form.
(1) Powiadomiono
informed
wczoraj
yesterday
wieczorem
evening
V. Putina
V. Putinacc
o
about
ataku.
attack
?Yesterday evening they informed V. Putin about the at-
tack.?
Additionally, grammar rules may produce vari-
ants of recognized full person names. For exam-
ple, for the full name CEO dr Jan Kowalski the fol-
lowing variants can be produced: Kowalski, CEO
Kowalski, dr Kowalski, etc. As the grammar rules
always return the longest match, a shorter form may
not be recognized. The produced variants are there-
fore used in the second pass through the text in order
to identify ?incomplete? forms. As no morphological
generation is involved, only base forms can be iden-
tified in this way. The system evaluation indicates
that 23.8% of the recognized names were identified
by this partial coreference resolution mechanism.
An analysis of incorrectly recognized named en-
tities (NEs) revealed that major problems concerned
(a) classical ambiguities, such as a proper name
vs. a common word, and (b) person vs. organiza-
tion name, caused by a specific word order and a
structural ambiguity of phrases containing NEs. Let
us consider the following examples to illustrate the
problems.
(2) Dane
Datanom
Federalnego
federalgen
Urze?du
officegen
Statystycznego
statisticalgen
?Data of the federal office for statistics?
(3) prezes
presidentnom
Della
Dellgen
?president of Dell?
(4) kanclerz
chancellornom
Austriak?w
Austriansgen
?chancellor of the Austrians?
(5) ... powiedzia?
said
prezes
presidentnom
sp??ki
companygen
Kruk
Kruknom
?. . . said the president of Kruk company / Kruk, the pre-
sident of the company?
The text fragment Dane Federalnego in (2) is rec-
ognized by the grammar as a person name since
Dane is a gazetteer entry for a foreign (English) first
name. Consequently, Federalnego Urze?du Statys-
tycznego could not be recognized as an organization
name. Potentially, heuristics solving such NE over-
lapping collisions could improve the precision. Sim-
ilar techniques have been applied to other languages.
In (3) and (4) the names Della ?of Dell? and Austri-
ak?w ?of Austrians? were erroneously recognized as
surnames. The rule matching a token representing
a title followed by a capitalized word, adopted for
English person names, is less reliable for Polish due
to declension of proper names and lack of prepo-
sitions in genitive constructions. One solution to
this problem would involve matchingDella and Aus-
triak?w with their base forms (Dell and Austriacy,
resp.), which might appear in the immediate con-
text. In this way, the name type could be validated.
However, a corpus inspection revealed that quite fre-
quently no base form appears in the same document.
The last example, (5), illustrates another problem,
which is even harder to solve. The phrase prezes
29
sp??ki Kruk is structurally ambiguous, i.e., it can
be bracketed as [prezes [sp??ki Kruk]] or [[prezes
sp??ki] Kruk]. Consequently, the name Kruk might
either refer to a company name (?. . . said the pre-
sident of the Kruk company?) or to a person name
(?. . . said Kruk, the president of the company?). In-
ferring the proper interpretation might not be possi-
ble even if we consider the subcategorization frame
of the verb powiedziec? ?to say?.
3.2 Evaluation
For evaluation of recognition and lemmatization of
person names, a set of 30 articles on various top-
ics (politics, finance, sports, culture and science) has
been randomly chosen from Rzeczpospolita (Weiss,
2007), a leading Polish newspaper. The total num-
ber of person name occurrences in this document set
amounts to 858. Evaluation of recognition?s preci-
sion and recall yielded 88.6% and 82.6%, respec-
tively. Precision of lemmatization of first names
and surnames achieved 92.2% and 75.6%, respec-
tively. For 12.4% of the recognized person names
more than one output structure was returned. For in-
stance, in case of the person name Marka Belki, the
first name Marka is interpreted by the gazetteer ei-
ther as an accusative form of the male name Marek
or as a nominative form of a foreign female name
Marka. In fact, 10% of the Polish first-name forms
in our gazetteer are ambiguous with respect to gen-
der. As for the last name Belki, it is a genitive form
of the common Polish noun belka ?beam?, so the
base form can be obtained directly. Nevertheless,
as inflection of proper names differs from that of
common nouns, various combinations of the regular
noun Belka and the special proper name form Belki
are possible, which increases ambiguity of the iden-
tified form. All possible lemmatizations are as fol-
lows:
(6) Marek Belka (masc.),
Marka Belka (fem.),
Marek Belki (masc.),
Marka Belki (fem.)
A good heuristics to reduce such ambiguous
lemmatizations is to prioritize rules which refer to
morphological information over those which rely
solely on orthography and/or token types.
4 Application of String Distance Metrics
for Lemmatization
Since knowledge-based lemmatization of Polish
NEs is extremely hard, we also explored a possibil-
ity of using string distance metrics for matching in-
flected person names with their base forms (and their
variants) in a collection of document, rather than
within a single document. The rest of this section de-
scribes our experiments in using different string dis-
tance metrics for this task, inspired by the work pre-
sented in (Cohen et al, 2003) and (Christen, 2006).
The problem can be formally defined as follows.
Let A, B and C be three sets of strings over some
alphabet ?, with B ? C. Further, let f : A ? B
be a function representing a mapping of inflected
forms (A) into their corresponding base forms (B).
Given,A andC (the search space), the task is to con-
struct an approximation of f , namely f? : A ? C.
If f?(a) = f(a) for a ? A, we say that f? returns
the correct answer for a; otherwise, f? is said to re-
turn an incorrect answer. For another task, a multi-
result experiment, we construct an approximation
f? : A ? ?C , where f? returns the correct answer
for a if f(a) ? f?(a).
4.1 String distance metrics
In our experiments, we have explored mainly
character-level string metrics3 applied by the
database community for record linkage.
Our point of departure is the well-known Lev-
enshtein edit distance metric specified as the min-
imum number of character-level operations (inser-
tion, deletion or substitution) required for trans-
forming one string into another (Levenshtein, 1965)
and bag distance metric (Bartolini et al, 2002)
which is a time-efficient approximation of the Lev-
enshtein metric. Next, we have tested the Smith-
Waterman (Smith and Waterman, 1981) metric,
which is an extension of Levenshtein metric and al-
low a variable cost adjustment to edit operations and
an alphabet mapping to costs.
Another group of string metrics we explored is
based on a comparison of character-level n-grams in
two strings. The q-gram metric (Ukkonen, 1992) is
3Distance (similarity) metrics map a pair of strings s and t
to a real number r, where a smaller (larger) value of r indicates
greater (lower) similarity.
30
computed by counting the number of q-grams con-
tained in both strings. An extension to q-grams is
to add positional information, and to match only
common q-grams that occur at a specified distance
from each other (positional q-grams) (Gravano et
al., 2001). Finally, the skip-gram metric (Keskustalo
et al, 2003) is based on the idea that in addition
to forming bigrams of adjacent characters, bigrams
that skip characters are considered as well. Gram
classes are defined that specify what kind of skip-
grams are created, e.g. {0, 1} class means that regu-
lar bigrams (0 characters skipped) and bigrams that
skip one character are formed. We have explored
{0, 1}, {0, 2} and {0, 1, 2} gram classes.
Taking into account the Polish declension
paradigm, we also added a basic metric based on the
longest common prefix, calculated as follows:
CP ?(s, t) = ((|lcp(s, t)|+ ?)
?/(|s| ? |t|),
where lcp(s, t) denotes the longest common prefix
for s and t. The symbol ? is a parameter for favoring
certain suffix pairs in s (t). We have experimented
with two variants: CP ?? with ? = 0 and CP ?? ,
where ? is set to 1 if s ends in: o, y, a?, e?, and t ends
in an a, or 0 otherwise. The latter setting results
from empirical study of the data and the declension
paradigm.
For coping with multi-token strings, we tested
a similar metric called longest common substrings
(LCS) (Christen, 2006), which recursively finds and
removes the longest common substring in the two
strings compared, up to a specified minimum length.
Its value is calculated as the ratio of the sum of all
found longest common substrings to the length of
the longer string. We extended LCS by additional
weighting the lengths of the longest common sub-
strings. The main idea is to penalize the longest
common substrings which do not match the begin-
ning of a token in at least one of the compared
strings. In such cases, the weight for lcs(s, t) (the
longest common substring for s and t) is computed
as follows. Let ? denote the maximum number of
non-whitespace characters which precede the first
occurrence of lcs(s, t) in s or t. Then, lcs(s, t) is
assigned the weight:
wlcs(s,t) =
|lcs(s, t)|+ ??max(?, p)
|lcs(s, t)|+ ?
where p has been experimentally set to 4. We refer
to the ?weighted? variant of LCS as WLCS.
Good results for name-matching tasks (Cohen et
al., 2003) have been reported using the Jaro metric
and its variant, the Jaro-Winkler (JW ) metric (Win-
kler, 1999). These metrics are based on the num-
ber and order of common characters in two com-
pared strings. We have extended the Jaro-Winkler
metric to improve the comparison of multi-token
strings. We call this modification JWM and it can
be briefly characterized as follows. Let J(s, t) de-
note the value of the Jaro metric for s and t. Then,
let s = s? . . . sK and t = t? . . . tL, where si (ti) rep-
resent i-th token of s and t respectively, and assume,
without loss of generality, L ? K. JWM(s, t) is
defined as:
JWM(s, t) = J(s, t)+? ?boostp(s, t)?(??J(s, t))
where ? denotes the common prefix adjustment fac-
tor and boostp is calculated as follows:
boostp(s, t) =
?
L
?
?
i=?
L?? min(|lcp(si, ti)|, p)+
min(|lcp(sL, tL..tK)|, p)
L
The main idea behind JWM is to boost the Jaro
similarity for strings with the highest number of
agreeing initial characters in the corresponding to-
kens in the compared strings.
Finally, for multi-token strings, we tested a recur-
sive matching pattern, known also as Monge-Elkan
distance (Monge and Elkan, 1996). The intuition be-
hind this measure is the assumption that a token in
s (strings are treated as sequences of tokens) corre-
sponds to a token in t which has the highest num-
ber of agreeing characters. The similarity between
s and t is the mean of these maximum scores. Two
further metrics for multi-token strings were investi-
gated, namely Sorted-Tokens and Permuted-Tokens.
The first one is computed in two steps: (a) first, to-
kens forming a full string are sorted alphabetically,
and then (b) an arbitrary metric is applied to com-
pute the similarity for the ?sorted? strings. The latter
compares all possible permutations of tokens form-
ing the full strings and returns the calculated maxi-
mal similarity value.
A detailed description of string metrics used here
is given in (Christen, 2006) and in (Piskorski et al,
2007).
31
4.2 Test Data
For the experiments on coreference of person names,
we used two resources: (a) a lexicon of the most
frequent Polish first names (PL-F(IRST)-NAMES)
consisting of pairs of an inflected form and the cor-
responding base form, and (b) an analogous lexicon
of inflected full person names (first name + surname)
(PL-FULL-NAMES).4 The latter resource was cre-
ated semi-automatically as follows. We have auto-
matically extracted a list of 22485 full person-name
candidates from a corpus of 15724 on-line news ar-
ticles from Rzeczpospolita by using PL-F-NAMES
lexicon and an additional list of 30000 uninflected
foreign first names. Subsequently, we have ran-
domly selected a subset of about 1900 entries (in-
flected forms) from this list.
In basic experiments, we simply used the base
forms as the search space. Moreover, we produced
variants of PL-F-NAMES and PL-FULL-NAMES
by adding to the search space base forms of for-
eign first names and a complete list of full names ex-
tracted from the Rzeczpospolita corpus, respectively.
Table 3 gives an overview of our test datasets.
Dataset #inflected #base search space
PL-F-NAMES 5941 1457 1457
PL-F-NAMES-2 5941 1457 25490
PL-FULL-NAMES 1900 1219 1219
PL-FULL-NAMES-2 1900 1219 2351
PL-FULL-NAMES-3 1900 1219 20000
Table 3: Dataset used for the experiments
4.3 Evaluation Metrics
Since for a given string more than one answer can be
returned, we measured the accuracy in three ways.
First, we calculated the accuracy on the assumption
that a multi-result answer is incorrect and we defined
all-answer accuracy (AA) measure which penalizes
multi-result answers. Second, we measured the ac-
curacy of single-result answers (single-result accu-
racy (SR)) disregarding the multi-result answers.
Finally, we used a weaker measure which treats a
multi-result answer as correct if one of the results in
the answer set is correct (relaxed-all-answer accu-
racy (RAA)).
4Inflected forms which are identical to their corresponding
base form were excluded from the experiments since finding an
answer for such cases is straightforward.
Let s denote the number of strings for which a sin-
gle result (base form) was returned. Analogously,
m is the number of strings for which more than
one result was returned. Let sc and mc denote, re-
spectively, the number of correct single-result an-
swers returned and the number of multi-result an-
swers containing at least one correct result. The ac-
curacy metrics are computed as: AA = sc/(s+m),
SR = sc/s, and RAA = (sc +mc)/(s+m).
4.4 Experiments
We started our experiments with the PL-F-NAME
dataset and applied all but the multi-token strings
distance metrics. The results of the accuracy eval-
uation are given in Table 4. The first three columns
give the accuracy figures, whereas the column la-
beled AV gives an average number of results re-
turned in the answer set.
Metrics AA SR RAA AV
Bag Distance 0.476 0.841 0.876 3.02
Levenshtein 0.708 0.971 0.976 2.08
Smith-Waterman 0.625 0.763 0.786 3.47
Jaro 0.775 0.820 0.826 2.06
Jaro-Winkler 0.820 0.831 0.831 2.03
q-grams 0.714 0.974 0.981 2.09
pos q-grams 0.721 0.976 0.982 2.09
skip grams 0.873 0.935 0.936 2.14
LCS 0.696 0.971 0.977 12.69
WLCS 0.731 0.983 0.986 2.97
CP ?? 0.829 0.843 0.844 2.11
CP ?? 0.947 0.956 0.955 2.18
Table 4: Results for PL-F-NAMES
Interestingly, the simple linguistically-aware
common prefix-based measure turned out to work
best in the AA category, which is the most relevant
one, whereas WLCS metrics is the most accurate in
case of single-result answers and the RAA category.
Thus, a combination of the two seems to be a rea-
sonable solution to further improve the performance
(i.e., if WLCS provides a single answer, return this
answer, otherwise return the answer ofCP ??). Next,
the time-efficient skip grams metrics performed sur-
prisingly well in the AA category. This result was
achieved with {0, 2} gram classes. Recall that about
10% of the inflected first name forms in Polish are
ambiguous, as they are either a male or a female per-
son name, see sec. 2.
Clearly, the AA accuracy figures in the experi-
ment run on the PL-F-NAME-2 (with a large search
space) was significantly worse. However, the SR
32
accuracy for some of the metrics is still acceptable.
The top ranking metrics with respect to SR and AA
accuracy are given in Table 5. Metrics which return
more than 5 answers on average were excluded from
this list. Also in the case of PL-F-NAME-2 the com-
bination of WLCS and CP ?? seems to be the best
choice.
Metrics SR AA
WLCS 0.893 0.469
CP ?? 0.879 0.855
pos 2-grams 0.876 0.426
skip grams 0.822 0.567
2-grams 0.810 0.398
LCS 0.768 0.340
CP ?? 0.668 0.600
JW 0.620 0.560
Table 5: Top results for PL-F-NAMES-2
Finally, we have made experiments for full per-
son names, each represented as two tokens. It is
important to note that the order of the first name
and the surname in some of the entities in our test
datasets is swapped. This inaccuracy is introduced
by full names where the surname may also function
as a first name. Nevertheless, the results of the ex-
periment on PL-FULL-NAMES given in Table 6 are
nearly optimal. JWM , WLCS, LCS, skip grams
and Smith-Waterman were among the ?best? metrics.
Internal Metrics AA SR RAA AV
Bag Distance 0.891 0.966 0.966 3.13
Smith-Waterman 0,965 0,980 0,975 3,5
Levenshtein 0.951 0.978 0.970 4.59
Jaro 0.957 0.970 0.964 3.54
JW 0.952 0.964 0.958 3.74
JWM 0.962 0.974 0.968 3.74
2-grams 0.957 0.988 0.987 3.915
pos 3-grams 0.941 0.974 0.966 4.32
skip-grams 0.973 0.991 0.990 5.14
LCS 0.971 0.992 0.990 5.7
WLCS 0.975 0.993 0.992 6.29
Table 6: Results for PL-FULL-NAMES
The Monge-Elkan, Sorted-Tokens and Permuted-
Tokens scored in general only slightly better than the
basic metrics. The best results oscillating around
0.97, 0.99, and 0.99 for the three accuracy metrics
were obtained using LCS, WLCS, JWM and CP ?
metrics as internal metrics. The highest score was
achieved by applying Sorted-Tokens with JWM with
0.976 in AA accuracy.
Further, in order to get a better picture, we have
compared the performance of the aforementioned
?recursive? metrics on PL-FULL-NAMES-2, which
has a larger search space. The most significant re-
sults for the AA accuracy are given in Table 7. The
JWM metric seems to be the best choice as an in-
ternal metric, whereas WLCS, CP ?? and Jaro per-
form slightly worse.
Internal M. Monge-Elkan Sorted-Tokens Permuted-Tokens
Bag Distance 0.868 0.745 0.745
Jaro 0.974 0.961 0.968
JWM 0.976 0.976 0.975
SmithWaterman 0.902 0.972 0.967
3-grams 0.848 0.930 0.911
pos 3-grams 0.855 0.928 0.913
skip-grams 0.951 0.967 0.961
LCS 0.941 0.960 0.951
WLCS 0.962 0.967 0.967
CP ?? 0.969 n.a. n.a.
CP ?? 0.974 n.a. n.a.
Table 7: AA accuracy for PL-FULL-NAMES-2
In our last experiment we selected the ?best?
metrics so far and tested them against PL-FULL-
NAMES-3 (largest search space). The top results for
non-recursive metrics are given in Table 8. JWM
and WLCS turned out to achieve the best scores.
Metrics AA SR RAA AV
Levenshtein 0.791 0.896 0.897 2.20
Smith-Waterman 0.869 0.892 0.889 2.35
JW 0.791 0.807 0.802 2.11
JWM 0.892 0.900 0.901 2.11
skip-grams 0.852 0.906 0.912 2.04
LCS 0.827 0.925 0.930 2.48
WLCS 0.876 0.955 0.958 2.47
Table 8: Results for PL-FULL-NAMES-3
The top scores achieved for the recursive metrics
on PL-FULL-NAMES-3 were somewhat better. In
particular, Monge-Elkan performed best with CP ??
(0.937 AA and 0.946 SR) and slightly worse re-
sults were obtained with JWM. Sorted-Tokens scored
best in AA and SR accuracy with JWM (0.904) and
WLCS (0.949), respectively. Finally, for Permuted-
Tokens the identical setting yielded the best results,
namely 0.912 and 0.948, respectively.
5 Conclusions and Perspectives
For Slavic languages, rich and idiosyncratic inflec-
tion of proper names presents a serious problem for
lemmatization. In this paper we investigated two
different techniques for finding base forms of per-
son names in Polish. The first one employs heuris-
33
tics and linguistic knowledge. This method does
not provide optimal results at the moment as nec-
essary tools and linguistic resources, e.g., a morpho-
logical generator or a subcategorization lexicon, are
still underdeveloped for Polish. Moreover, contex-
tual heuristics do not always find a solution as the
required information might not be present in a sin-
gle document. Therefore, we considered string dis-
tance metrics as an alternative approach. The results
of applying various measures indicate that for first
names, simple common prefix (CP?) metric obtains
the best results for all-answer accuracy, whereas
the weighted longest common substrings (WLCS)
measure provides the best score for the single-result
accuracy. Hence, a combination of these two metrics
seems the most appropriate knowledge-poor tech-
nique for lemmatizing Polish first names. As for full
names, our two modifications (WLCS and JWM )
of standard distance metrics and CP? obtain good re-
sults as internal metrics for recursive measures and
as stand-alone measures.
Although the results are encouraging, the pre-
sented work should not be considered a final solu-
tion. We plan to experiment with the best scoring
metrics (e.g., for AA and SR) in order to find opti-
mal figures. Additionally, we consider combining
the two techniques. For example, string distance
metrics can be used for validation of names found
in the context. We also envisage applying the same
methods to other types of proper names as well as to
lemmatization of specialized terminology.
References
I. Bartolini, P. Ciacca, and M. Patella. 2002. String matching
with metric trees using an approximate distance. In Proceed-
ings of SPIRE, LNCS 2476, Lisbon, Portugal.
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel. 1997.
Nymble: A High-performance Learning Name-finder. In
Proceedings of ANLP-1997, Washington DC, USA.
A. Borthwick. 1999. AMaximumEntropy Approach to Named
Entity Recognition. PhD Thesis, Department of Computer
Science, New York University.
P. Christen. 2006. A Comparison of Personal Name Matching:
Techniques and Practical Issues. Technical report, TR-CS-
06-02, Computer Science Laboratory, The Australian Na-
tional University, Canberra, Australia.
W. Cohen, P. Ravikumar, and S. Fienberg. 2003. A compar-
ison of string metrics for matching names and records. In
Proceedings of the KDD2003.
H. Cunningham, E. Paskaleva, K. Bontcheva, and G. Angelova.
2003. Information extraction for Slavonic languages. In
Proceedings of the Workshop IESL, Borovets, Bulgaria.
T. Erjavec and S. D?eroski. 2004. Machine Learning of
Morphosyntactic Structure: Lemmatising Unknown Slovene
Words. In Journal of Applied Artificial Intelligence, 18(1),
pages 17-40.
L. Gravano, P. Ipeirotis, H. Jagadish, S. Koudas, N. Muthukrish-
nan, L. Pietarinen, and D. Srivastava. 2001. Using q-grams
in a DBMS for Approximate String Processing. IEEE Data
Engineering Bulletin, 24(4):28?34.
J. Grzenia. 1998. S?ownik nazw w?asnych ? ortografia,
wymowa, s?owotw?rstwo i odmiana. PWN.
H. Keskustalo, A. Pirkola, K. Visala, E. Leppanen, and
K. Jarvelin. 2003. Non-adjacent digrams improve matching
of cross-lingual spelling variants. In Proceedings of SPIRE,
LNCS 22857, Manaus, Brazil, pages 252?265.
V. Levenshtein. 1965. Binary Codes for Correcting Deletions,
Insertions, and Reversals. Doklady Akademii Nauk SSSR,
163(4):845?848.
W. Li, R. Yangarber, and R. Grishman. 2003. Bootstrapping
Learning of Semantic Classes from Positive and Negative
Examples. In Proceedings of the ICML-2003 Workshop on
The Continuum from Labeled to Unlabeled Data.
A. Monge and C. Elkan. 1996. The Field Matching Problem:
Algorithms and Applications. In Proceedings of Knowledge
Discovery and Data Mining 1996, pages 267?270.
J. Piskorski, P. Homola, M. Marciniak, A. Mykowiecka,
A. Przepi?rkowski, and M. Wolin?ski. 2004. Information
Extraction for Polish Using the Sprout Platform. Proceed-
ings of ISMIS 2004, Zakopane.
J. Piskorski. 2005. Named-entity Recognition for Polish with
SProUT. In Proceedings of IMTCI 2004, LNCS Vol 3490,
Warsaw, Poland.
J. Piskorski and M. Sydow. 2007. Usability of String Distance
Metrics for Name Matching Tasks in Polish. In progress.
B. Pouliquen, R. Steinberger, C. Ignat, I. Temnikova, A. Widi-
ger, W. Zaghouani and J. ?i?ka. 2005. Multilingual person
name recognition and transliteration. CORELA - Cognition,
Repr?sentation, Langage. Num?ros sp?ciaux, Le traitement
lexicographique des noms propres, ISSN 1638-5748.
T. Smith and M. Waterman. 1981. Identification of Common
Molecular Subsequences. Journal of Molecular Biology,
147:195?197.
E. Ukkonen. 1992. Approximate String Matching with q-
grams and Maximal Matches. Theoretical Computer Sci-
ence, 92(1):191?211.
D. Weiss. 2007. Korpus Rzeczpospolitej. Web document:
http://www.cs.put.poznan.pl/dweiss/rzeczpospolita
W. Winkler. 1999. The state of record linkage and current re-
search problems. Technical report, U.S. Bureau of the Cen-
sus, Washington, DC.
34
Towards Light Semantic Processing for Question Answering
Benjamin Van Durme?, Yifen Huang?, Anna Kups?c??+, Eric Nyberg?
?Language Technologies Institute, Carnegie Mellon University
+Polish Academy of Sciences
{vandurme,hyifen,aniak,ehn}@cs.cmu.edu
Abstract
The paper1 presents a lightweight knowledge-
based reasoning framework for the JAVELIN
open-domain Question Answering (QA) sys-
tem. We propose a constrained representation
of text meaning, along with a flexible unifica-
tion strategy that matches questions with re-
trieved passages based on semantic similarities
and weighted relations between words.
1 Introduction
Modern Question Answering (QA) systems aim at pro-
viding answers to natural language questions in an open-
domain context. This task is usually achieved by com-
bining information retrieval (IR) with information extrac-
tion (IE) techniques, modified to be applicable to unre-
stricted texts. Although semantics-poor techniques, such
as surface pattern matching (Soubbotin, 2002; Ravichan-
dran and Hovy, 2002) or statistical methods (Ittycheriah
et al, 2002), have been successful in answering fac-
toid questions, more complex tasks require a consider-
ation of text meaning. This requirement has motivated
work on QA systems to incorporate knowledge process-
ing components such as semantic representation, ontolo-
gies, reasoning and inference engines, e.g., (Moldovan et
al., 2003), (Hovy et al, 2002), (Chu-Carroll et al, 2003).
Since world knowledge databases for open-domain tasks
are unavailable, alternative approaches for meaning rep-
resentation must be adopted. In this paper, we present
our preliminary approach to semantics-based answer de-
tection in the JAVELIN QA system (Nyberg et al, 2003).
In contrast to other QA systems, we are trying to realize
a formal model for a lightweight semantics-based open-
domain question answering. We propose a constrained
semantic representation as well as an explicit unification
1The authors appear in alphabetical order.
framework based on semantic similarities and weighted
relations between words. We obtain a lightweight roboust
mechanism to match questions with answer candidates.
The organization of the paper is as follows: Section 2
briefly presents system components; Section 3 discusses
syntactic processing strategies; Sections 4 and 5 describe
our preliminary semantic representation and the unifica-
tion framework which assigns confidence values to an-
swer candidates. The final section contains a summary
and future plans.
2 System Components
The JAVELIN system consists of four basic components:
a question analysis module, a retrieval engine, a passage
analysis module (supporting both statistical and NLP
techniques), and an answer selection module. JAVELIN
also includes a planner module, which supports feedback
loops and finer control over specific components (Nyberg
et al, 2003). In this paper we are concerned with the two
components which support linguistic analysis: the ques-
tion analysis and passage understanding modules (Ques-
tion Analyzer and Information Extractor, respectively).
The relevant aspects of syntactic processing in both mod-
ules are presented in Section 3, whereas the semantic rep-
resentation is introduced in Section 4.
3 Parsing
The system employs two different parsing techniques:
a chart parser with hand-written grammars for ques-
tion analysis, and a lexicalized, broad coverage skipping
parser for passage analysis. For question analysis, pars-
ing serves two goals: to identify the finest answer focus
(Moldovan et al, 2000; Hermjakob, 2001), and to pro-
duce a grammatical analysis (f-structure) for questions.
Due to the lack of publicly available parsers which have
suitable coverage of question forms, we have manually
developed a set of grammars to achieve these goals. On
the other hand, the limited coverage and ambiguity in
these grammars made adopting the same approach for
passage analysis inefficient. In effect, we use two dis-
tinct parsers which provide two syntactic representations,
including grammatical functions. These syntactic struc-
tures are then transformed into a common semantic rep-
resentation discussed in Section 4.
( (Brill-pos VBN)
(adjunct (
(object (
(Brill-pos WRB)
(atype temporal)
(cat n)
(ortho When)
(q-focus +)
(q-token +)
(root when)
(tokens 1)))
(time +)))
(cat v)
(finite +)
(form finite)
(modified +)
(ortho founded)
(passive +)
(punctuation (
(Brill-pos ".")
(cat punct)
(ortho ?)
(root ?)
(tokens 6)))
(qa (
(gap (
(atype temporal)
(path (*MULT* adjunct
object))))
(qtype entity)))
(root found)
(subject (
(BBN-name person)
(Brill-pos NNP)
(cat n)
(definite +)
(gen-pn +)
(human +)
(number sg)
(ortho "Wendy?s")
(person third)
(proper-noun +)
(root wendy)
(tokens 3)))
(tense past)
(tokens 5))
Figure 1: When was Wendy?s founded: KANTOO f-
structure
3.1 Questions
The question analysis consists of two steps: lexical pro-
cessing and syntactic parsing. For the lexical process-
ing step, we have integrated several external resources:
the Brill part-of-speech tagger (Brill, 1995), BBN Identi-
Finder (BBN, 2000) (to tag named entities such as proper
names, time expressions, numbers, etc.), WordNet (Fell-
baum, 1998) (for semantic categorization), and the KAN-
TOO Lexifier (Nyberg and Mitamura, 2000) (to access a
syntactic lexicon for verb valence information).
The hand-written grammars employed in the project
are based on the Lexical Functional Grammar (LFG) for-
malism (Bresnan, 1982), and are used with the KANTOO
parser (Nyberg and Mitamura, 2000). The parser out-
puts a functional structure (f-structure) which specifies
the grammatical functions of question components, e.g.,
subject, object, adjunct, etc. As illustrated in Fig. 1, the
resulting f-structure provides a deep, detailed syntactic
analysis of the question.
3.2 Passages
Passages selected by the retrieval engine are processed
by the Link Grammar parser (Grinberg et al, 1995). The
parser uses a lexicalized grammar which specifies links,
i.e., grammatical functions, and provides a constituent
structure as output. The parser covers a wide range of
syntactic constructions and is robust: it can skip over un-
recognized fragments of text, and is able to handle un-
known words.
An example of the passage analysis produced by the
Link Parser is presented in Fig. 2. Links are treated as
predicates which relate various arguments. For exam-
ple, O in Fig. 2 indicates that Wendy?s is an object of the
verb founded. In parallel to the Link parser, passages are
tagged with the BBN IdentiFinder (BBN, 2000), in or-
der to group together multi-word proper names such as
R. David Thomas.
4 Semantic Representation
At the core of our linguistic analysis is the semantic rep-
resentation, which bridges the distinct representations of
the functional structure obtained for questions and pas-
sages. Although our semantic representation is quite sim-
ple, it aims at providing the means of understanding and
processing broad-coverage linguistic data. The represen-
tation uses the following main constructs:2
? formula is a conjunction of literals and represents
the meaning of the entire sentence (or question);
? literal is a predicate relation over two terms; in par-
ticular, we distinguish two types of literals: extrin-
sic literal, a literal which relates a label to a label,
and intrinsic literal, a literal which relates a label to
a word;
2The use of terminology common in the field of formal logic
is aimed at providing an intuitive understanding to the reader,
but is not meant to give the impression that our work is built on
a firm logic-theoretic framework.
+------------------------Xp------------------------+
| +-------MVp-------+ |
+--------Wd-------+ +------O------+ | |
| +-G-+---G--+---S---+ +--YS-+ +-IN+ |
| | | | | | | | | |
LEFT-WALL R. David Thomas founded.v Wendy ?s.p in 1969 .
Constituent tree:
(S (NP R. David Thomas)
(VP founded
(NP (NP Wendy ?s))
(PP in
(NP 1969)))
.)
Figure 2: R. David Thomas founded Wendy?s in 1969.: Link Grammar parser output
? predicate is used to capture relations between
terms;
? term is either a label, a variable which refers to a
specific entity or an event, or a word, which is either
a single word (e.g., John) or a sequence of words
separated by whitespace (e.g., for proper names such
as John Smith).
The BNF syntax corresponding to this representation
is given in (1).
(1) <formula> := <literal>+
<literal> := <pred>(<term>,<term>)
<term> := <label>|<word>
<word> := |[a-nA-Z0-9\s]+|
<label> := [a-z]+[0-9]+
<pred> := [A-Z_-]+
With the exception of the unary ANS predicate which
indicates the sought answer, all predicates are binary re-
lations (see examples in Fig. 3). Currently, most pred-
icate names are based on grammatical functions (e.g.,
SUBJECT, OBJECT, DET) which link events and entities
with their arguments. Unlike in (Moldovan et al, 2003),
names of predicates belong to a fixed vocabulary, which
provides a more sound basis for a formal interpretation.
Names of labels and terms are restricted only by the syn-
tax in (1). Examples of semantic representations for the
question When was Wendy?s founded? and the passage R.
David Thomas founded Wendy?s in 1969. are shown in
Fig. 4.
Note that our semantic representation reflects the
?canonical? structure of an active sentence. This design
decision was made in order to eliminate structural differ-
ences between semantically equivalent structures. Hence,
at the semantic level, all passive sentences correspond to
their equivalents in the active form. Semantic representa-
tion of questions is not always derived directly from the
f-structure. For some types of questions, e.g., definition
When was Wendy?s R. David Thomas founded
founded? Wendy?s in 1969.
ROOT(x6,|Wendy?s|) ROOT(x6,|Wendy?s|)
ROOT(x2,|found|) ROOT(x2,|ound|)
ADJUNCT(x2,x1) ADJUNCT(x2,x1)
OBJECT(x2,x6) OBJECT(x2,x6)
SUBJECT(x2,x7) SUBJECT(x2,x7)
ROOT(x7,|R. David Thomas|)
TYPE(x2,|event|) TYPE(x2,|event|)
TENSE(x2,|past|)
ROOT(x1,x9) ROOT(x1,|1969|)
TYPE(x1,|time|) TYPE(x1,|time|)
ANS(x9)
Figure 4: An example of question and passage semantic
representation
questions such as What is the definition of hazmat?, spe-
cialized (dedicated) grammars are used, which allows us
to more easily arrive at an appropriate representation of
meaning. Also, in the preliminary implementation of the
unification algorithm (see Section 5), we have adopted
some simplifying assumptions, and we do not incorpo-
rate sets in the current representation.
The present formalism can quite successfully handle
questions (or sentences) which refer to specific events or
relations. However, it is more difficult to represent ques-
tions like What is the relationship between Jesse Ventura
and Target Stores?, which seek a relation between enti-
ties or a common event they participated in. In the next
section, we discuss the unification scheme which allows
us to select answer candidates based on the proposed rep-
resentation.
5 Fuzzy Unification
A unification algorithm is required to match question rep-
resentations with the representations of extracted pas-
sages which might contain answers. Using a precursor
predicate example comments
ROOT ROOT(x13,|John|) the root form of entity/event x13
OBJECT OBJECT(x2,x3) x3 is the object of verb
or preposition x2
SUBJECT SUBJECT(x2,x3) x3 is the subject of verb x2
DET DET(x2,x1) x1 is a determiner/quantifier of x2
TYPE TYPE(x3,|event|) x3 is of the type event
TENSE TENSE(x1,|present|) x1 is a verb in present tense
EQUIV EQUIV(x1,x3) semantic equivalence:
apposition: ?John, a student of CMU?
equality operator in copular sentences:
?John is a student of CMU?
ATTRIBUTE ATTRIBUTE(x1,x3) x3 is an adjective modifier of x1:
adjective-noun: ?stupid John?
copular constructions: ?John is stupid?
PREDICATE PREDICATE(x2,x3) copular constructions: ?Y is x3?
ROOT(x2,|be|) SUBJECT(x2,Y)
PREDICATE(x2,x3)
POSSESSOR POSSESSOR(x2,x4) x4 is the possessor of x2
?x4?s x2? or ?x2 of x4?
AND AND(x3,x1) ?John and Mary laughed.?
AND(x3,x2) ROOT(x1,|John|) ROOT(x2,|Mary|)
ROOT(x4,|laugh|) TYPE(x4,|event|)
AND(x3,x1)
AND(x3,x2)
SUBJECT(x4,x3)
ANS ANS(x1) only for questions: x1 indicates the answer
Figure 3: Examples of predicates
to the representation presented above, we constructed
an initial prototype using a traditional theorem prover
(Kalman, 2001). Answer extraction was performed by at-
tempting a unification between logical forms of the ques-
tion and retrieved passages. Early tests showed that a uni-
fication strategy based on a strict boolean logic was not
as flexible as we desired, given the lack of traditional do-
main constraints that one normally possesses when con-
sidering this type of approach. Unless a retrieved pas-
sage exactly matched the question, as in Fig. 4, the sys-
tem would fail due to lack of information. For instance,
knowing that Benjamin killed Jefferson. would not an-
swer the question Who murdered Jefferson?, using a strict
unification strategy.
This has led to more recent experimentation with prob-
abilistic models that perform what we informally refer to
as fuzzy unification.3 The basic idea of our unification
strategy is to treat relationships between question terms
as a set of weighted constraints. The confidence score
assigned to each extracted answer candidate is related to
the number of constraints the retrieved passage satisfies,
along with a measure of similarity between the relevant
terms.
5.1 Definitions
In this section, we present definitions which are necessary
for discussion of the similarity measure employed by our
fuzzy unification framework.
Given a user query Q, where Q is a formula, we re-
trieve a set of passages P. Our task to is find the best
passage Pbest ? P from which an answer candidate can
be extracted. An answer candidate exists within a pas-
sage P if the result of a fuzzy unification between Q and
P results in the single term of ANS(x0) being ground in a
term from P .
(2) Pbest = argmaxP? Psim(Q,P )
The restriction that an answer candidate must be found
within a passage P must be made explicit, as our no-
tion of fuzzy unification is such that a passage can unify
against a query with a non-zero level of confidence even
if one or more constraints from the query are left unsat-
isfied. Since the final goal is to find and return the best
possible answer, we are not concerned with those pas-
sages which seem highly related yet do not offer answer
candidates.
In Section 4, we introduced extrinsic literals where
predicates serve as relations over two labels. Extrinsic lit-
erals can be thought of as relations defined over distinct
3Fuzzy unification in a formal setting generally refers to a
unification framework that is employed in the realm of fuzzy
logics. Our current representation is of an ad-hoc nature, but
our usage of this term does foreshadow future progression to-
wards a representation scheme dependent on such a formal,
non-boolean model.
entities in our formula. For example, SUBJECT(x1, x2)
is an extrinsic literal, while ROOT(x1, |Benjamin|) is not.
The latter has been defined as an intrinsic literal in Sec-
tion 4 and it relates a label and a word.
This terminology is motivated by the intuitive distinc-
tion between intrinsic and extrinsic properties of an entity
in the world. We use this distinction as a simplifying as-
sumption in our measurements of similarity, which we
will now explain in more detail.
5.2 Similarity Measure
Given a set of extrinsic literals PE and QE from a pas-
sage and the question, respectively, we measure the sim-
ilarity between QE and a given ordering of PE as the
geometric mean of the similarity between each pair of
extrinsic literals from the sets QE and PE .
Let O be the set of all possible orderings of PE , O
an element of O, QEj literal j of QE , and Oj literal j of
ordering O. Then:
(3) sim(Q,P )= sim(QE , PE)
= maxO? O(
?n
j=0 sim(QEj , Oj))
1
n
The similarity of two extrinsic literals, lE and lE? , is
computed by the square root of the similarity scores of
each pair of labels, multiplied by the weight of the given
literal, dependent on the equivilance of the predicates
p, p? of the respective literals lE , lE? . If the predicates are
not equivilant, we rely on the engineers tactic of assign-
ing an epsilon value of similarity, where  is lower than
any possible similarity score4. Note that the similarity
score is still dependent on the weight of the literal, mean-
ing that failing to satisfy a heavier constraint imposes a
greater penalty than if we fail to satisfy a constraint of
lesser importance.
Let tj and t?j be the respective j-th term of lE , lE
?
.
Then:
(4) sim(lE , lE?) = weight(lE)?
{(sim(t0,t?0)?sim(t1,t?1))
1
2 ,p=p?
,otherwise
The weight of a literal is meant to capture the relative
importance of a particular constraint in a query. In stan-
dard boolean unification the importance of a literal is uni-
form, as any local failure dooms the entire attempt.5 In a
non-boolean framework the importance of one literal vs.
another becomes an issue. As an example, given a ques-
tion concerning a murder we might be more interested in
the suspect?s name than in the fact that he was tall. This
4The use of a constant value of  is ad hoc, and we are in-
vestigating more principled methods for assigning this penalty.
5That is to say, classic unification is usually an all or nothing
affair.
idea is similar to that commonly seen in information re-
trieval systems which place higher relative importance on
terms in a query that are judged a priori to posses higher
information value. While our prototype currently sets all
literals with a weight of 1.0, we are investigating methods
to train these weights to be specific to question type.
Per our definition, all terms within an extrinsic literal
will be labels. Thus, in equation (10), t0 is a label, as is
t1, and so on. Given a pair of labels, b and b?, we let I, I ?
be the respective sets of intrinsic literals from the formula
containing b, b? such that for all intrinsic literals lI ? I ,
the first term of lI is b, and likewise for b?, I ?.
Much like similarity between two formulae, the sim-
ilarity between two labels relies on finding the maximal
score over all possible orderings of a set of literals.
Now let O be the set of all possible orderings of I ?, O
an element of O, Ij the j-th literal of I , and Oj the j-th
literal of O. Then:
(5) sim(b, b?) = maxO? O(
?n
j=0 sim(Ij , Oj))
1
n
We measure the similarity between a pair of intrinsic
literals as the similarity between the two words multi-
plied by the weight of the first literal, dependent on the
predicates p, p? of the respective literals being equivilant.
(6)
sim(lI , lI
?
) = weight(lI) ?
{sim(t1,t?1),p=p?
,otherwise
The similarity between two words is currently measured
using a WordNet distance metric, applying weights intro-
duced in (Moldovan et al, 2003). We will soon be inte-
grating metrics which rely on other dimensions of simi-
larity.
5.3 Example
We now walk through a simple example in order to
present the current framework used to measure the level
of constraint satisfaction (confidence score) achieved by
a given passage. While a complete traversal of even a
small passage would exceed the space available here, we
will present a single instance of each type of usage of the
sim() function.
If we limit our focus to only a few key relationships,
we get the following analysis of a given question and pas-
sage.
(7) Who killed Jefferson?
ANS(x0), ROOT(x1,x0), ROOT(x2,|kill|),
ROOT(x3,|Jefferson|), TYPE(x2,|event|),
TYPE(x1,|person|), TYPE(x3,|person|), SUB-
JECT(x2,x1), OBJECT(x2,x3)
(8) Benjamin murdered Jefferson.
ROOT(y1,|Benjamin|), ROOT(y2,|murder|),
ROOT(y3,|Jefferson|), TYPE(y2,|event|),
TYPE(y1,|person|), TYPE(y3,|person|), SUB-
JECT(y2,y1), OBJECT(y2,y3)
Computing the similarity between two formulae,
(loosely referred to here by their original text), gives the
following:
(9) sim[|Who killed Jefferson?|,
|Benjamin murdered Jefferson.|] =
(sim[ SUBJECT(x2,x1), SUBJECT(y2,y1)]?
sim[ OBJECT(x2,x3), OBJECT(y2,y3)]) 12
The similarity between the given extrinsic literals shar-
ing the predicate SUBJECT:
(10) sim[SUBJECT(x2,x1), SUBJECT(y2,y1)] =
(sim[x2, y2] ? sim[x1, y1]) 12 ?
weight[SUBJECT(x2,x1)]
In order to find the result of this extrinsic similarity
evaluation, we need to determine the similarity between
the paired terms, (x1,y1) and (x2,y2). The similarity be-
tween x1 and y1 is measured as:
(11) sim[x2, y2] =
(sim[ROOT(x2,|kill|), ROOT(y2,|murder|)]?
sim[TYPE(x2,|event|), TYPE(y2,|event|)]) 12
The result of this function depends on the combined
similarity of the intrinsic literals that relate the given
terms to values. The similarity between one of these in-
trinsic literal pairs is measured by:
(12) sim[ROOT(x2,|kill|), ROOT(y2,|murder|)] =
sim[|kill|, |murder|]?weight[ROOT(x2,|kill|)]
Finally, the similarity between a pair of words is com-
puted as:
(13) sim[|kill|, |murder|] = 0.8
As stated earlier, our similarity metrics at the word
level are currently based on recent work on WordNet dis-
tance functions. We are actively developing methods to
complement this approach.
6 Summary and Future Work
The paper presents a lightweight semantic processing
technique for open-domain question answering. We pro-
pose a uniform semantic representation for questions and
passages, derived from their functional structure. We also
describe the unification framework which allows for flex-
ible matching of query terms with retrieved passages.
One characteristics of the current representation is that
it is built from grammatical functions and does not uti-
lize a canonical set of semantic roles and concepts. Our
overall approach in JAVELIN was to start with the sim-
plest form of meaning-based matching that could extend
simple keyword-based approaches. Since it was possi-
ble to extract grammatical functions from unrestricted
text fairly quickly (using KANTOO for questions and the
Link Grammar parser for answer passages), this frame-
work provides a reasonable first step. We intend to extend
our representation and unification algorithm by incorpo-
rating the Lexical Conceptual Structure Database (Dorr,
2001), which will allow us to use semantic roles instead
of grammatical relations as predicates in the represen-
tation. We also plan to enrich the representation with
temporal expressions, incorporating the ideas presented
in (Han, 2003).
Another limitation of the current implementation is
the limited scope of the similarity function. At present,
the similarity function is based on relationships found
in WordNet, and only relates words which belong to the
same syntactic category. We plan to extend our similar-
ity measure by using name lists, gazetteers and statistical
cooccurrence in text corpora. A complete approach to
word similarity will also require a suitable algorithm for
reference resolution. Unrestricted text makes heavy use
of various forms of co-reference, such as anaphora, def-
inite description, etc. We intend to adapt the anaphora
resolution algorithms used in KANTOO for this purpose,
but a general solution to resolving definite reference (e.g.,
the use of ?the organization? to refer to ?Microsoft?) is a
topic for ongoing research.
Acknowledgments
Research presented in this paper has been supported in
part by an ARDA grant under Phase I of the AQUAINT
program. The authors wish to thank all members of the
JAVELIN project for their support in preparing the work
presented in this paper. We are also grateful to two anony-
mous reviewers, Laurie Hiyakumoto and Kathryn Baker
for their comments and suggestions for improving this
paper. Needless to say, all remaining errors and omis-
sions are entirely our responsibility.
References
BBN Technologies, 2000. IdentiFinder User Manual.
Joan Bresnan, editor. 1982. The Mental Representation
of Grammatical Relations. MIT Press Series on Cog-
nitive Theory and Mental Representation. The MIT
Press, Cambridge, MA.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case study
in part-of-speech tagging. Computational Linguistics,
21:543?565.
Jenifer Chu-Carroll, John Prager, Christopher Welty,
Krzysztof Czuba, and David Ferrucci. 2003. A multi-
strategy and multi-source approach to question an-
swering. In TREC 2002 Proceedings.
Bonnie J. Dorr. 2001. LCS Database Docu-
mentation. HTML Manual. available from
http://www.umiacs.umd.edu/?bonnie/LCS Data-
base Documentation.html.
Christiane Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
Dennis Grinberg, John Lafferty, and Daniel Sleator.
1995. A robust parsing algorithm for link grammars.
In Proceedings of the Fourth International Workshop
on Parsing Technologies, Prague, September.
Benjamin Han. 2003. Text temporal analysis. Research
status summary. Draft of January 2003.
Ulf Hermjakob. 2001. Parsing and question classifica-
tion for question answering. In Proceedings of the
Workshop on Open-Domain Question Answering at
ACL-2001.
Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin. 2002.
The use of external knowledge in factoid qa. In Pro-
ceedings of the TREC-10 Conference.
Abraham Ittycheriah and Salim Roukos. 2003. IBM?s
statistical question answering system ? TREC-11. In
TREC 2002 Proceedings.
Abraham Ittycheriah, Martin Franz, and Salim Roukos.
2002. IBM?s statistical question answering system ?
TREC-10. In TREC 2001 Proceedings.
John A. Kalman. 2001. Automated Reasoning with OT-
TER. Rinton Press.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and Vasile
Rus. 2000. The structure and performance of an open-
domain question answering system. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL-2000).
Dan Moldovan, Sanda Harabagiu, Roxana Girju, Paul
Morarescu, Finley Lacatusu, Adrian Novischi, Adriana
Badulescu, and Orest Bolohan. 2003. LCC tools for
question answering. In TREC 2002 Proceedings.
Eric Nyberg and Teruko Mitamura. 2000. The KAN-
TOO machine translation environment. In Proceed-
ings of AMTA 2000.
Eric Nyberg, Teruko Mitamura, Jaime Carbonell, Jaime
Callan, Kevyn Collins-Thompson, Krzysztof Czuba,
Michael Duggan, Laurie Hiyakumoto, Ng Hu, Yifen
Huang, Jeongwoo Ko, Lucian V. Lita, Stephen
Murtagh, Vasco Pedro, and David Svoboda. 2003. The
JAVELIN question answering system at TREC 2002.
In TREC 2002 Proceedings.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a Question Answering system.
In Proceedings of the ACL Conference.
Martin M. Soubbotin. 2002. Patterns of potential answer
expressions as clues to the right answer. In TREC 2001
Proceedings.
