Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 940?946,
Prague, June 2007. c?2007 Association for Computational Linguistics
Probabilistic Parsing Action Models for Multi-lingual Dependency 
Parsing 
Xiangyu Duan 
Institute of Automation, Chi-
nese Academy of Sciences 
xyduan@nlpr.ia.ac.cn 
Jun Zhao 
Institute of Automation, Chi-
nese Academy of Sciences 
jzhao@nlpr.ia.ac.cn 
Bo Xu 
Institute of Automation, Chi-
nese Academy of Sciences 
xubo@hitic.ia.ac.cn 
 
 
Abstract 
Deterministic dependency parsers use pars-
ing actions to construct dependencies. 
These parsers do not compute the probabil-
ity of the whole dependency tree. They 
only determine parsing actions stepwisely 
by a trained classifier. To globally model 
parsing actions of all steps that are taken on 
the input sentence, we propose two kinds 
of probabilistic parsing action models that 
can compute the probability of the whole 
dependency tree. The tree with the maxi-
mal probability is outputted. The experi-
ments are carried on 10 languages, and the 
results show that our probabilistic parsing 
action models outperform the original de-
terministic dependency parser. 
1 Introduction 
The target of CoNLL 2007 shared task (Nivre et al, 
2007) is to parse texts in multiple languages by 
using a single dependency parser that has the ca-
pacity to learn from treebank data. Among parsers 
participating in CoNLL 2006 shared task 
(Buchholz et al, 2006), deterministic dependency 
parser shows great efficiency in time and compa-
rable performances for multi-lingual dependency 
parsing (Nivre et al, 2006). Deterministic parser 
regards parsing as a sequence of parsing actions 
that are taken step by step on the input sentence. 
Parsing actions construct dependency relations be-
tween words. 
Deterministic dependency parser does not score 
the entire dependency tree as most of state-of-the-
art parsers. They only stepwisely choose the most 
probable parsing action. In this paper, to globally 
model parsing actions of all steps that are taken on 
the input sentence, we propose two kinds of prob-
abilistic parsing action models that can compute 
the entire dependency tree?s probability. Experi-
ments are evaluated on diverse data set of 10 lan-
guages provided by CoNLL 2007 shared-task 
(Nivre et al, 2007). Results show that our prob-
abilistic parsing action models outperform the 
original deterministic dependency parser. We also 
present a general error analysis across a wide set of 
languages plus a detailed error analysis of Chinese. 
Next we briefly introduce the original determi-
nistic dependency parsing algorithm that is a basic 
component of our models. 
2 Introduction of Deterministic Depend-
ency Parsing 
There are mainly two representative deterministic 
dependency parsing algorithms proposed respec-
tively by Nivre (2003), Yamada and Matsumoto 
(2003). Here we briefly introduce Yamada and 
Matsumoto?s algorithm, which is adopted by our 
models, to illustrate deterministic dependency 
parsing. The other representative method of Nivre 
also parses sentences in a similar deterministic 
manner except different data structure and parsing 
actions. 
Yamada?s method originally focuses on unla-
beled dependency parsing. Three kinds of parsing 
actions are applied to construct the dependency 
between two focus words. The two focus words are 
the current sub tree?s root and the succeeding (right) 
sub tree?s root given the current parsing state. 
Every parsing step results in a new parsing state, 
which includes all elements of the current partially 
built tree. Features are extracted about these two 
focus words. In the training phase, features and the 
corresponding parsing action compose the training
940
 
 
 
 
 
 
 
 
 
 
 
 
 
He provides confirming evidence RIGHT
He
provides confirming evidence
SHIFT
LEFT
RIGHTconfirming 
He 
provides evidence provides evidence 
He confirming 
provides 
He evidence 
confirming 
Figure 1. The example of the parsing process of Yamada and Matsumoto?s method. The input sentence 
is ?He provides confirming evidence.? 
 
data. In the testing phase, the classifier determines 
which parsing action should be taken based on the 
features. The parsing algorithm ends when there is 
no further dependency relation can be made on the 
whole sentence. The details of the three parsing 
actions are as follows: 
LEFT: it constructs the dependency that the 
right focus word depends on the left focus word. 
RIGHT: it constructs the dependency that the 
left focus word depends on the right focus word. 
SHIFT: it does not construct dependency, just 
moves the parsing focus. That is, the new left focus 
word is the previous right focus word, whose suc-
ceeding sub tree?s root is the new right focus word. 
The illustration of these three actions and the 
parsing process is presented in figure 1. Note that 
the focus words are shown as bold black box. 
We extend the set of parsing actions to do la-
beled dependency parsing. LEFT and RIGHT are 
concatenated by dependency labels, while SHIFT 
remains the same. For example in figure 1, the 
original action sequence ?RIGHT -> SHIFT -> 
RIGHT -> LEFT? becomes ?RIGHT-SBJ -> 
SHIFT -> RIGHT-NMOD -> LEFT-OBJ?. 
3 Probabilistic Parsing Action Models 
Deterministic dependency parsing algorithms are 
greedy. They choose the most probable parsing 
action at every parsing step given the current pars-
ing state, and do not score the entire dependency 
tree. To compute the probability of whole depend-
ency tree, we propose two kinds of probabilistic 
models that are defined on parsing actins: parsing 
action chain model (PACM) and parsing action 
phrase model (PAPM). 
3.1 Parsing Action Chain Model (PACM) 
The parsing process can be viewed as a Markov 
Chain. At every parsing step, there are several can-
didate parsing actions. The objective of this model 
is to find the most probable sequence of parsing 
actions by taking the Markov assumption. As 
shown in figure 1, the action sequence ?RIGHT-
SBJ -> SHIFT -> RIGHT-NMOD -> LEFT-
OBJ? constructs the right dependency tree of the 
example sentence. Choosing this action sequence 
among all candidate sequences is the objective of 
this model.  
Firstly, we should define the probability of the 
dependency tree conditioned on the input sentence. 
)1(),...|()|(
...1
10?
=
?=
ni
ii SdddPSTP  
Where T denotes the dependency tree, S denotes 
the original input sentence,  denotes the parsing 
action at time step i. We add an artificial parsing 
action  as initial action. 
id
0d
We introduce a variable  to denote the 
resulting parsing state when the action  is taken 
on .  is the original input sen-
tence. 
id
context
id
1?idcontext 0dcontext
Suppose  are taken sequentially on the 
input sentence S, and result in a sequence of pars-
ing states , then P(T|S) de-
fined in equation (1) becomes as below: 
ndd ...0
ndd
contextcontext ...
0
941
)4()|(
)3()|(
)2(),...,|(
...1
...1
...1
1
1
10
?
?
?
=
=
=
?
?
?
=
?
ni
di
ni
dd
ni
ddd
i
ii
ii
contextdP
contextcontextP
contextcontextcontextP
 
Formula (3) comes from formula (2) by obeying 
the Markov assumption. Note that formula (4) is 
about the classifier of parsing actions. It denotes 
the probability of the parsing action given the 
parsing state . If we train a classifier 
that can predict with probability output, then we 
can compute P(T|S) by computing the product of 
the probabilities of parsing actions. The classifier 
we use throughout this paper is SVM (Vapnik, 
1995). We adopt Libsvm (Chang and Lin, 2005), 
which can train multi-class classifier and support 
training and predicting with probability output 
(Chang and Lin, 2005). 
id
1?idcontext
For this model, the objective is to choose the 
parsing action sequence that constructs the de-
pendency tree with the maximal probability. 
)5()|(max)|(max
...1
... 11
?
= ?
=
ni
didd in
contextdPSTP  
Because this model chooses the most probable 
sequence, not the most probable parsing action at 
only one step, it avoids the greedy property of the 
original deterministic parsers. 
We use beam search for the decoding of this 
model. We use m to denote the beam size. Then 
beam search is carried out as follows. At every 
parsing step, all parsing states are ordered (or par-
tially m ordered) according to their probabilities. 
Probability of a parsing state is determined by 
multiplying the probabilities of actions that gener-
ate that state. Then we choose m best parsing 
states for this step, and next parsing step only con-
sider these m best parsing states. Parsing termi-
nates when the first entire dependency tree is con-
structed. To obtain a list of n-best parses, we sim-
ply continue parsing until either n trees are found, 
or no further parsing can be fulfilled. 
3.2 Parsing Action Phrase Model (PAPM) 
In the Parsing Action Chain Model (PACM), ac-
tions are competing at every parsing step. Only m 
best parsing states resulted by the corresponding 
actions are kept for every step. But for the parsing 
problem, it is reasonable that actions are competing 
for which phrase should be built. For dependency 
syntax, one phrase consists of the head word and 
all its children. Based on this motivation, we pro-
pose Parsing Action Phrase Model (PAPM), which 
divides parsing actions into two classes: construct-
ing action and shifting action. 
If a phrase is built after an action is performed, 
the action is called constructing action. In original 
Yamada?s algorithm, constructing actions are 
LEFT and RIGHT. For example, if LEFT is taken, 
it indicates that the right focus word has found all 
its children and becomes the head of this new 
phrase. Note that one word with no children can 
also be viewed as a phrase if its dependency on 
other word is constructed. In the extended set of 
parsing actions for labeled parsing, compound ac-
tions, which consist of LEFT and RIGHT con-
catenated by dependency labels, are constructing 
actions. 
If no phrase is built after an action is performed, 
the action is called shifting action. Such action is 
SHIFT. 
We denote  as constructing action and  as 
shifting action. j indexes the time step. Then we 
introduce a new concept: parsing action phrase. 
We use  to denote the ith parsing action phrase. 
It can be expanded as . That is, 
parsing action phrase  is a sequence of parsing 
actions that constructs the next syntactic phrase. 
ja jb
iA
jjkji abbA 1... ???
iA
For example, consider the parsing process in 
figure 1,  is ?RIGHT-SBJ?,  is ?SHIFT, 
RIGHT-NMOD?,  is ?LEFT-OBJ?. Note that 
 consists of a constructing action,  consists 
of a shifting action and a constructing action,  
consists of a constructing action. 
1A 2A
3A
1A 2A
3A
The indexes are different for both sides of the 
expansion ,  is the ith parsing 
action phrase corresponding to both constructing 
action  at time step j and all its preceding shift-
ing actions. Note that on the right side of the ex-
pansion, only one constructing action is allowed 
and is always at the last position, while shifting 
action can occur several times or does not occur at 
all. It is parsing action phrases, i.e. sequences of 
parsing actions, that are competing for which next 
phrase should be built. 
jjkji abbA 1... ??? iA
ja
942
The probability of the dependency tree given the 
input sentence is redefined as: 
)|())|((
)|(
)|...(
)|(
)|(
)...|(
)6(),...|()|(
1
1
1
1
1
11
...2
1
...1
...1
1
...1
...1
...1
...1
11
??
?
?
?
?
?
?
?=
=
=
?
=
=
?
?
?
?
?
?
?
=
+?
=
?
=
??
=
=
=
=
?
jtj
i
i
i
ii
ii
bj
kt
btj
ni
Akj
ni
Ajjkj
ni
Ai
ni
AA
ni
AAA
ni
ii
contextaPcontextbP
contextbP
contextabbP
contextAP
contextcontextP
contextcontextcontextP
SAAAPSTP
 
Where k represents the number of steps that shift-
ing action can be taken.  is the parsing 
state resulting from a sequence of actions 
 taken on . 
iA
context
jjkj abb 1... ?? 1?iAcontext
The objective in this model is to find the most 
probable sequence of parsing action phrases. 
)7()|(max)|(max
...1
... 11
?
= ?
=
ni
AiAA in
contextAPSTP  
Similar with parsing action chain model 
(PACM), we use beam search for the decoding of 
parsing action phrase model (PAPM). The differ-
ence is that PAPM do not keep m best parsing 
states at every parsing step. Instead, PAPM keep m 
best states which are corresponding to m best cur-
rent parsing action phrases (several steps of 
SHIFT and the last step of a constructing action). 
4 Experiments and Results 
Experiments are carried on 10 languages provided 
by CoNLL 2007 shared-task organizers (Nivre et 
al., 2007). Among these languages, Chinese (Chen 
et al, 2003), Catalan (Mart? et al, 2007) and Eng-
lish (Johansson and Nugues, 2007) have low per-
centage of non-projective relations, which are 
0.0%, 0.1% and 0.3% respectively. Except these 
three languages, we use software of projectiviza-
tion/deprojectivization provided by Nivre and 
Nilsson (2005) for other languages. Because our 
algorithm only deals with projective parsing, we 
should projectivize training data at first to prepare 
for the following training of our algorithm. During 
testing, deprojectivization is applied to the output 
of the parser. 
Considering the classifier of Libsvm (Chang and 
Lin, 2005), the features are extracted from the fol-
lowing fields of the data representation: FORM, 
LEMMA, CPOSTAG, POSTAG, FEATS and DE-
PREL. We split values of FEATS field into its 
atomic components. We only use available features 
of DEPREL field during deterministic parsing. We 
use similar feature context window as used in Ya-
mada?s algorithm (Yamada and Matsumoto, 2003). 
In detail, the size of feature context window is six, 
which consists of left two sub trees, two focus 
words related sub trees and right two sub trees. 
This feature template is used for all 10 languages. 
4.1 Results of PACM and Yamada?s Method 
After submitting the testing results of Parsing Ac-
tion Chain Model (PACM), we also perform origi-
nal deterministic parsing proposed by Yamada and 
Matsumoto (2003). The total results are shown in 
table 1. The experimental results are mainly evalu-
ated by labeled attachment score (LAS), unlabeled 
attachment score (UAS) and labeled accuracy (LA). 
Table 1 shows that Parsing Action Chain Model 
(PACM) outperform original Yamada?s parsing 
method for all languages. The LAS improvements 
range from 0.60 percentage points to 1.71 percent-
age points. Note that the original Yamada?s 
method still gives testing results above the official 
reported average performance of all languages. 
 Ara Bas Cat Chi Cze Eng Gre Hun Ita Tur 
YamLAS  69.31 69.67 83.26 81.88 74.63 84.81 72.75 76.24 80.08 73.94
YamUAS  78.93 75.86 88.53 86.17 80.11 85.83 79.45 79.97 83.69 79.79
YamLA  81.13 75.71 88.36 84.56 82.10 89.71 82.58 88.37 86.93 80.81
PACMLAS  69.91 71.26 84.95 82.58 75.34 85.83 74.29 77.06 80.75 75.03
PACMUAS  79.04 77.57 89.71 86.88 80.82 86.97 80.77 80.66 84.20 81.03
PACMLA  81.40 77.35 89.55 85.35 83.17 90.57 83.87 88.92 87.32 81.17
Table 1. The performances of Yamada?s method (Yam) and Parsing Action Chain Model (PACM). 
 
943
4.2 Results of PAPM 
Not all languages have only one root node of a 
sentence. Since Parsing Action Phrase Model 
(PAPM) only builds dependencies, and shifting 
action is not the ending action of a parsing action 
phrase, PAPM always ends with one root word. 
This property makes PAPM only suitable for 
Catalan, Chinese, English and Hungarian, which 
are unary root languages. PAPM result of Catalan 
was not submitted before deadline due to the    
shortage of time and computing resources. We 
report Catalan?s PAPM result together with that of 
other three languages in table 2.  
 
 Cat Chi Eng Hun 
PAPMLAS  87.26 82.64 86.69 76.89 
PAPMUAS  92.07 86.94 87.87 80.53 
PAPMLA  91.89 85.41 92.04 89.73 
Table 2. The performance of Parsing Action 
Phrase Model (PAPM) for Catalan, Chinese, Eng-
lish and Hungarian. 
 
Compared with the results of PACM shown in 
table 1, the performance of PAPM differs among 
different languages. Catalan and English show 
that PAPM improves 2.31% and 0.86% respec-
tively over PACM, while the improvement of Chi-
nese is marginal, and there is a little decrease of 
Hungarian. Hungarian has relatively high percent-
age of non-projective relations. If phrase consists 
of head word and its non-projective children, the 
constructing actions that are main actions in 
PAPM will be very difficult to be learned because 
some non-projective children together with their 
heads have no chance to be simultaneously as fo-
cus words. Although projectivization is also per-
formed for Hungarian, the built-in non-projective 
property still has negative influence on the per-
formance. 
5 Error Analysis 
In the following we provide a general error analy-
sis across a wide set of languages plus a detailed 
analysis of Chinese. 
5.1 General Error Analysis 
One of the main difficulties in dependency parsing 
is the determination of long distance dependencies. 
Although all kinds of evaluation scores differ 
dramatically among different languages, 69.91% 
to 85.83% regarding LAS, there are some general 
observations reflecting the difficulty of long dis-
tance dependency parsing. We study this difficulty 
from two aspects about our full submission of 
PACM: precision of dependencies of different arc 
lengths and precision of root nodes. 
For arcs of length 1, all languages give high 
performances with lowest 91.62% of Czech 
(B?hmova et al, 2003) to highest 96.8% of Cata-
lan (Mart? et al, 2007). As arcs lengths grow 
longer, various degradations are caused. For Cata-
lan, score of arc length 2 is similar with that of arc 
length 1, but there are dramatic degradations for 
longer arc lengths, from 94.94% of arc length 2 to 
85.22% of length 3-6. For English (Johansson and 
Nugues, 2007) and Italian (Montemagni et al, 
2003), there are graceful degradation for arcs of 
length 1,2 and 3-6, with 96-91-85 of English and 
95-85-75 of Italian. For other languages, long arcs 
also give remarkable degradations that pull down 
the performance. 
Precision of root nodes also reflects the per-
formance of long arc dependencies because the 
arc between the root and its children are often 
long arcs. In fact, it is the precision of roots and 
arcs longer than 7 that mainly pull down the over-
all performance. Yamada?s method is a bottom-up 
parsing algorithm that builds short distance de-
pendencies at first. The difficulty of building long 
arc dependencies may partially be resulted from 
the errors of short distance dependencies. The de-
terministic manner causes error propagation, and 
it indirectly indicates that the errors of roots are 
the final results of error propagation of short dis-
tance dependencies. But there is an exception oc-
curred in Chinese. The root precision is 90.48%, 
only below the precision of arcs of length 1. This 
phenomenon exists because the sentences in Chi-
nese data set (Chen et al, 2003) are in fact clauses 
with average length of 5.9 rather than entire sen-
tences. The root words are heads of clauses. 
Both Parsing Action Chain Model (PACM) and 
Parsing Action Phrase Model (PAPM) avoid 
greedy property of original Yamada?s method. It 
can be expected that there will be a precision im-
provement of long distance dependencies over 
original Yamada?s method. For PACM, the results 
of Basque (Aduriz et al, 2003), Catalan (Mart? et 
al., 2007), Chinese (Chen et al, 2003), English 
(Johansson and Nugues, 2007) and Greek (Pro-
944
kopidis et al, 2005) show that the root precision 
improvement over Yamada?s method is more con-
spicuous than that of other long distance depend-
encies. The largest improvement of roots precision 
is 10.7% of Greek. While for Arabic (Hajic et al, 
2004), Czech (B?hmova et al, 2003), Hungarian 
(Csendes et al, 2005), Italian (Montemagni et al, 
2003) and Turkish (Oflazer et al, 2003), the im-
provement of root precision is small, but depend-
encies of arcs longer than 1 give better scores. For 
PAPM, good performances of Catalan and English 
also give significant improvements of root preci-
sion over PACM. For Catalan, the root precision 
improvement is from 63.86% to 95.21%; for Eng-
lish, the root precision improvement is from 
62.03% to 89.25%. 
5.2 Error Analysis of Chinese 
There are mainly two sources of errors regarding 
LAS in Chinese dependency parsing. 
One is from conjunction words (C) that have a 
relatively high percentage of wrong heads (about 
20%), and therefore 19% wrong dependency la-
bels. In Chinese, conjunction words often con-
catenate clauses. Long distance dependencies be-
tween clauses are bridged by conjunction words. 
It is difficult for conjunction words to find their 
heads. 
The other source of errors comes from auxiliary 
words (DE) and preposition words (P). Unlike 
conjunction words, auxiliary words and preposi-
tion words have high performance of finding right 
head, but label accuracy (LA) decrease signifi-
cantly. The reason may lie in the large depend-
ency label set consisting of 57 kinds of depend-
ency labels in Chinese. Moreover, auxiliary words 
(DE) and preposition words (P) have more possi-
ble dependency labels than other coarse POS have. 
This introduces ambiguity for parsers. 
Most common POS including noun and verb 
contribute much to the overall performance of 
83% Labeled Attachment Scores (LAS). Adverbs 
obtain top score while adjectives give the worst. 
6 Conclusion 
We propose two kinds of probabilistic models 
defined on parsing actions to compute the prob-
ability of entire sentence. Compared with original 
Yamada and Matsumoto?s deterministic depend-
ency method which stepwisely chooses most 
probable parsing action, the two probabilistic 
models improve the performance regarding all 10 
languages in CoNLL 2007 shared task. Through 
the study of parsing results, we find that long dis-
tance dependencies are hard to be determined for 
all 10 languages. Further analysis about this diffi-
culty is needed to guide the research direction. 
Feature exploration is also necessary to provide 
more informative features for hard problems. 
Ackowledgements 
This work was supported by Hi-tech Research and 
Development Program of China under grant No. 
2006AA01Z144, the Natural Sciences Foundation 
of China under grant No. 60673042, and the Natu-
ral Science Foundation of Beijing under grant No. 
4052027, 4073043. 
References 
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 
2006. CoNLL-X shared task on multilingual de-
pendency parsing. SIGNLL. 
Chih-Chung Chang and Chih-Jen Lin. 2005. LIBSVM: 
A library for support vector machines. 
J. Nivre. 2003. An efficient algorithm for projective 
dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies 
(IWPT). 
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proc. of ACL-2005, pages 99?
106. 
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, S. Marinov. 
2006. Labeled Pseudo-Projective Dependency 
Parsing with Support Vector Machines. In Proc. of 
the Tenth Conference on Computational Natural 
Language Learning (CoNLL). 
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 
shared task on dependency parsing. In Proc. of the 
Joint Conf. on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning (EMNLP-CoNLL). 
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In 
Proceedings of the 8th International Workshop on 
Parsing Technologies (IWPT). 
V. Vapnik. 1995. The Nature of StatisticalLearning 
Theory. Springer. 
945
A. Abeill?, editor. 2003. Treebanks: Building and 
Using Parsed Corpora. Kluwer.  
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. 
Diaz de Ilarraza, A. Garmendia and M. Oronoz. 
2003. Construction of a Basque Dependency Tree-
bank. In Proc. of the 2nd Workshop on Treebanks 
and Linguistic Theories (TLT), pages 201?204. 
A. B?hmov?, J. Hajic, E. Hajicov? and B. Hladk?. 
2003. The PDT: a 3-level annotation scenario. In 
Abeill? (2003), chapter 7, 103?127. 
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. 
Huang and Z. Gao. 2003. Sinica Treebank: Design 
Criteria, Representational Issues and Implementa-
tion. In Abeill? (2003), chapter 13, pages 231?248. 
D. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor. 
2005. The Szeged Treebank. Springer.  
J. Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E. 
Beska. 2004. Prague Arabic Dependency Treebank: 
Development in Data and Tools. In Proc. of the 
NEMLAR Intern. Conf. on Arabic Language Re-
sources and Tools, pages 110?117. 
R. Johansson and P. Nugues. 2007. Extended 
constituent-to-dependency conversion for English. 
In Proc. of the 16th Nordic Conference on 
Computational Linguistics (NODALIDA).  
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. Computational Linguistics, 
19(2):313?330. 
M. A. Mart?, M. Taul?, L. M?rquez and M. Bertran. 
2007. CESS-ECE: A Multilingual and Multilevel 
Annotated Corpus. Available for download from: 
http://www.lsi.upc.edu/~mbertran/cess-ece/. 
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, 
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M. 
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D. 
Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R. 
Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeill? (2003), chapter 11, 
pages 189?210.  
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 
shared task on dependency parsing. In Proc. of the 
CoNLL 2007 Shared Task. Joint Conf. on Empirical 
Methods in Natural Language Processing and 
Computational Natural Language Learning 
(EMNLP-CoNLL). 
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. 
T?r. 2003. Building a Turkish treebank. In Abeill? 
(2003), chapter 15, pages 261?277.  
P. Prokopidis, E. Desypri, M. Koutsombogera, H. 
Papageorgiou, and S. Piperidis. 2005. Theoretical 
and practical issues in the construction of a Greek 
depen- dency treebank. In Proc. of the 4th 
Workshop on Treebanks and Linguistic Theories 
(TLT), pages 149?160. 
946
Coling 2010: Poster Volume, pages 1444?1452,
Beijing, August 2010
Machine Transliteration: Leveraging on Third Languages 
Min Zhang          Xiangyu Duan           Vladimir Pervouchine         Haizhou Li 
Institute for Infocomm Research, A-STAR 
{mzhang, xduan, vpervouchine, hli}@i2r.a-star.edu.sg 
  
Abstract 
This paper presents two pivot strategies 
for statistical machine transliteration, 
namely system-based pivot strategy 
and model-based pivot strategy. Given 
two independent source-pivot and pi-
vot-target name pair corpora, the mod-
el-based strategy learns a direct source-
target transliteration model while the 
system-based strategy learns a source-
pivot model and a pivot-target model, 
respectively. Experimental results on 
benchmark data show that the system-
based pivot strategy is effective in re-
ducing the high resource requirement 
of training corpus for low-density lan-
guage pairs while the model-based pi-
vot strategy performs worse than the 
system-based one. 
1 Introduction 
Many technical terms and proper names, such 
as personal, location and organization names, 
are translated from one language into another 
language with approximate phonetic equiva-
lents. This phonetic translation using computer 
is referred to as machine transliteration. With 
the rapid growth of the Internet data and the 
dramatic changes in the user demographics 
especially among the non-English speaking 
parts of the world, machine transliteration play 
a crucial role in  most multilingual NLP, MT 
and CLIR applications (Hermjakob et al, 
2008; Mandl and Womser-Hacker, 2004). This 
is because proper names account for the major-
ity of OOV issues and translation lexicons 
(even derived from large parallel corpora) 
usually fail to provide good coverage over di-
verse, dynamically increasing names across 
languages.  
Much research effort has been done to ad-
dress the transliteration issue in the research 
community (Knight and Graehl, 1998; Wan 
and Verspoor, 1998; Kang and Choi, 2000; 
Meng et al, 2001; Al-Onaizan and Knight, 
2002; Gao et al, 2004; Klementiev and Roth, 
2006; Sproat, 2006; Zelenko and Aone, 2006; 
Li et al, 2004, 2009a, 2009b; Sherif and Kon-
drak, 2007; Bertoldi et al, 2008; Goldwasser 
and Roth, 2008). These previous work can be 
categorized into three classes, i.e., grapheme-
based, phoneme-based and hybrid methods. 
Grapheme-based method (Li et al, 2004) 
treats transliteration as a direct orthographic 
mapping process and only uses orthography-
related features while phoneme-based method 
(Knight and Graehl, 1998) treats transliteration 
as a phonetic mapping issue, converting source 
grapheme to source phoneme followed by a 
mapping from source phoneme to target pho-
neme/grapheme. Hybrid method in machine 
transliteration refers to the combination of sev-
eral different models or decoders via re-
ranking their outputs. The report of the first 
machine transliteration shared task (Li et al, 
2009a, 2009b) provides benchmarking data in 
diverse language pairs and systemically sum-
marizes and compares different transliteration 
methods and systems using the benchmarking 
data. 
Although promising results have been re-
ported, one of major issues is that the state-of-
the-art machine transliteration approaches rely 
heavily on significant source-target parallel 
name pair corpus to learn transliteration model. 
However, such corpora are not always availa-
1444
ble and the amounts of the current available 
corpora, even for language pairs with English 
involved, are far from enough for training, let-
ting alone many low-density language pairs. 
Indeed, transliteration corpora for most lan-
guage pairs without English involved are un-
available and usually rather expensive to ma-
nually construct. However, to our knowledge, 
almost no previous work touches this issue. 
To address the above issue, this paper 
presents two pivot language-based translitera-
tion strategies for low-density language pairs. 
The first one is system-based strategy (Khapra 
et al, 2010), which learns a source-pivot mod-
el from source-pivot data and a pivot-target 
model from pivot-target data, respectively. In 
decoding, it first transliterates a source name to 
N-best pivot names and then transliterates each 
pivot names to target names which are finally 
re-ranked using the combined two individual 
model scores. The second one is model-based 
strategy. It learns a direct source-target transli-
teration model from two independent1 source-
pivot and pivot-target name pair corpora, and 
then does direct source-target transliteration. 
We verify the proposed methods using the 
benchmarking data released by the 
NEWS20092 (Li et al, 2009a, 2009b). Expe-
riential results show that without relying on 
any source-target parallel data the system-
based pivot strategy performs quite well while 
the model-based strategy is less effective in 
capturing the phonetic equivalent information. 
The remainder of the paper is organized as 
follows. Section 2 introduces the baseline me-
thod. Section 3 discusses the two pivot lan-
guage-based transliteration strategies. Experi-
mental results are reported at section 4. Final-
ly, we conclude the paper in section 5. 
2 The Transliteration Model 
Our study is targeted to be language-
independent so that it can be applied to 
different language pairs without any adaptation 
effort. To achieve this goal, we use joint 
source-channel model (JSCM, also named as 
                                                 
1 Here ?independent? means the source-pivot and 
pivot-target data are not derived from the same 
English name source. 
2  http://www.acl-ijcnlp-2009.org/workshops/NEWS 
2009/pages/sharedtask.html 
n-gram transliteration model) (Li et la., 2004) 
under grapheme-based framework as our 
transliteration model due to its state-of-the-art 
performance by only using orthographical 
information (Li et al, 2009a). In addition, 
unlike other feature-based methods, such as 
CRFs (Lafferty et al, 2001), MaxEnt (Berger 
et al, 1996) or SVM (Vapnik, 1995), the 
JSCM model directly computes model 
probabilities using maximum likelihood 
estimation (Dempster et al, 1977). This 
property facilitates the implementation of the 
model-based strategy.  
JSCM directly models how both source and 
target names can be generated simultaneously.  
Given a source name S and a target name T, it 
estimates the joint probability of S and T as 
follows: 
 
                                 
                              
           
                         
         
                                
    
 
   
 
                                   
    
 
   
 
 
where    and    is an aligned transliteration 
unit3 pair, and n is the n-gram order.  
In implementation, we compare different 
unsupervised transliteration alignment me-
thods, including Giza++ (Och and Ney, 2003), 
the JSCM-based EM algorithm (Li et al, 
2004), the edit distance-based EM algorithm 
(Pervouchine et al, 2009) and Oh et al?s 
alignment tool (Oh et al, 2009). Based on the 
aligned transliteration corpus, we simply learn 
the transliteration model using maximum like-
lihood estimation (Dempster et al, 1977) and 
decode the transliteration result    
              using stack decoder 
(Schwartz and Chow, 1990). 
                                                 
3 Transliteration unit is language dependent. It can 
be a Chinese character, a sub-string of English 
words, a Korean Hangual or a Japanese Kanji or 
several Japanese Katakanas.  
1445
3 Pivot Transliteration Strategies 
3.1 System-based Strategy  
The system-based strategy is first proposed by  
Khapra et al (2010). They worked on system-
based strategy together with CRF and did ex-
tensively empirical studies on In-
dic/Slavic/Semetic languages and English. 
Given a source name S, a target name T and 
let Z(S, ?) be the n-best transliterations of S in 
one or more pivot language ? 4, the system-
based transliteration strategy under JSCM can 
be formalized as follows: 
 
                          
       
 
          
 
 
  
                                
 
 
 
In the above formula, we assume that there is 
only one pivot language used in the derivation 
from the first line to the second line. Under the 
pivot transliteration framework, we can further 
simplify the above formula by assuming that   
is independent of    when given  . The as-
sumption holds because the parallel name cor-
pus between S and T is not available under the 
pivot transliteration framework. The n-best 
transliterations in pivot language are expected 
to be able to carry enough information of the 
source name S for translating S to target name 
T. Then, we have: 
                     
 
 
 
                
             
    
          
 
 
Obviously we can train the two JSCMs of 
       and        using the two parallel cor-
pora of        and      , and train the lan-
guage model      using the monolingual cor-
pus of   . Following the nature of JSCM, Eq. 
                                                 
4 There can be multiple pivot languages used in the 
two strategies. However, without loss of generality, 
we only use one pivot language to facilitate our 
discussion. It is very easy to extend one pivot lan-
guage to multiple ones by considering all the pivot 
transliterations in all pivot languages.  
(1) directly models how the source name S and 
pivot name   and how the pivot name   and 
the target name   are generated simultaneous-
ly. Since   is considered twice in        and 
      , the duplicated impact of   is removed 
by dividing the model by     . 
Given the model as described at Eq. (1), the 
decoder can be formulized as: 
                 
 
       
           
 
  
             
    
 
      
 
If we consider multiple pivot languages, the 
modeling and decoding process are: 
       
    
                       
         
 
       
 
 
       
       
 
   
                       
         
       
  
 
3.2 Model-based Strategy 
Rather than combining the transitive translite-
ration results at system level, the model-based 
strategy aims to learn a direct model       by 
combining the two individual models of 
       and       , which are learned from 
the two parallel corpora of       and      , 
respectively. Now let us use bigram as an ex-
ample to illustrate how to learn the translitera-
tion model                   
 
   
         using the model-based strategy. 
 
                       
 
                  
           
        
where,  
 
                     
                           
                            
       
 
                             
       
                    
1446
The same as the system-based strategy, we 
can further simplify the above formula by as-
suming that   is independent of    when given 
 . Indeed,                            cannot 
be estimated directly from training corpus. 
Then we have:  
                   
                             
       
                    
                    
       
                    
                          
       
                    
                                        
where                   ,                    
and            can be directly learned from 
training corpus.              for Eq (3) can 
also be estimatedas follows.  
 
             
                     
      
 
 
In summary, eq. (1) formulizes the system-
based strategy and eq. (3), (4) and (5) formul-
ize the model-based strategy, where we can 
find that they share the same nature of generat-
ing source, pivot and target names simulta-
neously. The difference is that the model-based 
strategy operates at fine-grained transliteration 
unit level. 
3.3 Comparison with Previous Work  
Almost all previous work on machine translite-
ration focuses on direct transliteration or trans-
literation system combination. There is only 
one recent work (Khapra et al, 2010) touching 
this issue. They work on system-based strategy 
together with CRF. Compared with their work, 
this paper gives more formal definitions and 
derivations of system-based strategy from 
modeling and decoding viewpoints based on 
the JSCM model.  
The pivot-based strategies at both system 
and model levels have been explored in ma-
chine translation. Bertoldi et al (2008) studies 
two pivot approaches for phrase-based statis-
tical machine translation. One is at system lev-
el and one is to re-construct source-target data 
and alignments through pivot data. Cohn and 
Lapata (2007) explores how to utilize multilin-
gual parallel data (rather than pivot data) to 
improve translation performance. Wu and 
Wang (2007, 2009) extensively studies the 
model-level pivot approach and also explores 
how to leverage on rule-based translation re-
sults in pivot language to improve translation 
performance. Utiyama and Isahara (2007) 
compares different pivot approaches for 
phrase-based statistical machine translation. 
All of the previous work on machine transla-
tion works on phrase-based statistical machine 
translation. Therefore, their translation model 
is to calculate phrase-based conditional proba-
bilities at unigram level (        ) while our 
transliteration model is to calculate joint trans-
literation unit-based conditional probabilities 
at bigram level (                   ). 
4 Experimental Results 
4.1 Experimental Settings 
We use the NEWS 2009 benchmark data as 
our experimental data (Li et al, 2009). The 
NEWS 2009 data includes 8 language pairs, 
where we select English to Chinese/Japanese 
/Korean data (E-C/J/K) and based on which we 
further construct Chinese to Japanese/Korean 
and Japanese to Korean for our data.  
 
Language Pair Training Dev Test 
English-Chinese 31,961  2896 2896 
English-Japanese 23,225 1492 1489 
English-Korean 4,785 987 989 
Chinese-Japanese 12,417 75 77 
Chinese-Korean 2,148 32 31 
Japanese-Korean 6,035 65 69 
 
Table 1. Statistics on the data set 
 
Table 1 reports the statistics of all the expe-
rimental data. To have a more accurate evalua-
tion, the test sets have been cleaned up to make 
sure that there is no overlapping between any 
test set with any training set. In addition, the 
three E-C/J/K data are generated independently 
so that there is very small percentage of over-
1447
lapping between them. This can ensures the 
evaluation of the pivot study fair and accurate.  
We compare different alignment algorithms 
on the DEV set. Finally we use Pervouchine et 
al. (2009)?s alignment algorithm for Chinese-
English/Japanese/Korean and Oh et al 
(2009)?s alignment algorithm for English-
Korean and Li et al (2004)?s alignment algo-
rithm for English-Japanese and Japanese-
Korean. Given the aligned corpora, we directly 
learn each individual JSCM model (i.e., n-
gram transliteration model) using SRILM tool-
kits (Stolcke, 2002). We also use SRILM tool-
kits to do decoding. For the system-based 
strategy, we output top-20 pivot transliteration 
results.  
For the evaluation matrix, we mainly use 
top-1 accuracy (ACC) (Li et al, 2009a) to 
measure transliteration performance. For refer-
ence purpose, we also report the performance 
using all the other evaluation matrixes used in 
NEWS 2009 benchmarking (Li et al, 2009a), 
including F-score, MRR, MAP_ref, MAP_10 
and MAP_sys. It is reported that F-score has 
less correlation with other matrixes (Li et al, 
2009a). 
4.2 Experimental Results 
4.2.1 Results of Direct Transliteration 
Table 2 reports the performance of direct trans-
literation. The first three experiments (line 1-3) 
are part of the NEWS 2009 share tasks and the 
others are our additional experiments for our 
pivot studies.  
Comparison of the first three experimental 
results and the results reported at NEWS 2009 
shows that we achieve comparable perfor-
mance with their best-reported systems at the 
same conditions of using single system and 
orthographic features only. This indicates that 
our baseline represents the state-of-the-art per-
formance. In addition, we find that the back-
transliteration (line 4-6) consistently performs 
worse than its corresponding forward-
transliteration (line 1-3). This observation is 
consistent with what reported at previous work 
(Li et al, 2004; Zhang et al, 2004). The main 
reason is because English has much more 
transliteration units than foreign C/J/K lan-
guages. This makes the transliteration from 
English to C/J/K a many-to-few mapping issue 
and back-transliteration a few-to-many map-
ping issue. Therefore back-transliteration has 
more ambiguities and thus is more difficult. 
Overall, the lower six experiments (line 7-
12) shows worse performance than the upper 
six experiments which has English involved. 
This is mainly due to the less available training 
data for the language pairs without English 
involved. This observation motivates our study 
using pivot language for machine translitera-
tion. 
4.2.2 Results of System-based Strategy 
Table 3 reports three empirical studies of sys-
tem-based strategies: Japanese to Chinese 
through English, Chinese to Japanese through 
English and Chinese to Korean through Eng-
lish. Considering the fact that those language 
pairs with English involved have the most 
training data, we select English as pivot lan-
guage in the system-based study. Table 3 
clearly shows that:  
? The system-based pivot strategy is very 
effective, achieving significant perfor-
mance improvement over the direct 
transliteration by 0.09, 0.07 and 0.03 
point of ACC in the three language pairs, 
respectively; 
? Different from other pipeline methodol-
ogies, the system-based pivot strategy 
does not suffer heavily from the error 
propagation issue. Its ACC is significant-
ly better than the product of the ACCs of 
the two individual systems; 
? The combination of pivot system and di-
rect system slightly improves overall 
ACC. 
We then conduct more experiments to figure 
out the reasons. Our further statistics and anal-
ysis show the following reasons for the above 
observations: 
The pivot approach is able to use source-
pivot and pivot-target data whose amount is 
much more than that of the available direct 
source-target data.  
? The nature of transliteration is phonetic 
translation. Therefore a little bit variation 
in orthography may not hurt or even help 
to improve transliteration performance in 
some cases as long as the orthographical 
variations keep the phonetic equivalent 
1448
Language Pairs ACC F-Score MRR MAP_ref MAP_10 MAP_sys 
English  Chinese 0.678867 0.871497 0.771563 0.678867 0.252382 0.252382 
English  Japanese 0.482203 0.831983 0.594235 0.471766 0.201510 0.201510 
English  Korean 0.439838 0.722365 0.543039 0.439585 0.171621 0.171621 
Chinese  English 0.395250 0.867702 0.518292 0.372403 0.222787 0.222787 
Japanese  English 0.334839 0.838212 0.450984 0.319277 0.168032 0.168032 
Korean  English 0.088505 0.494205 0.109249 0.088759 0.034380 0.034380 
Chinese  Japanese 0.385965 0.769245 0.473851 0.348319 0.159948 0.159948 
Japanese  Chinese 0.402597 0.714193 0.491595 0.402597 0.165581 0.165581 
Chinese  Korean 0.290323 0.571587 0.341129 0.290323 0.178652 0.178652 
Korean  Chinese 0.129032 0.280645 0.156042 0.129032 0.048163 0.048163 
Japanese  Korean 0.313433 0.678240 0.422862 0.313433 0.208310 0.208310 
Korean  Japanese 0.089286 0.321617 0.143948 0.091270 0.049992 0.049992 
 
Table 2. Performance of direct transliterations 
 
 
Language Pairs ACC   F-Score MRR MAP_ref MAP_10 MAP_sys 
Jap Eng Chi (Pivot) 0.493506 0.750711 0.617440 0.493506 0.195151 0.195151 
Jap Eng Chi (Pivot)  
+ Jap  Chi (Direct) 
0.506494 0.753958 0.622851 0.506494 0.196017 0.196017 
Jap  Chi (Direct) 0.402597 0.714193 0.491595 0.402597 0.165581 0.165581 
Jap  Eng (Direct) 0.334839 0.838212 0.450984 0.319277 0.168032 0.168032 
Eng  Chi (Direct) 0.678867 0.871497 0.771563 0.678867 0.252382 0.252382 
Chi Eng Jap (Pivot) 0.456140 0.777494 0.536591 0.414961 0.183222 0.183222 
Chi Eng Jap (Pivot) 
 + Chi  Jap (Direct) 
0.491228 0.801443 0.563297 0.450049 0.191742 0.191742 
Chi  Jap (Direct) 0.385965 0.769245 0.473851 0.348319 0.159948 0.159948 
Chi  Eng (Direct) 0.395250 0.867702 0.518292 0.372403 0.222787 0.222787 
Eng  Jap (Direct) 0.482203 0.831983 0.594235 0.471766 0.201510 0.201510 
Chi Eng Kor (Pivot) 0.322581 0.628146 0.432642 0.322581 0.175822 0.175822 
Chi Eng Kor (Pivot)   
+ Chi  Kor (Direct) 
0.331631 0.632967 0.439143 0.334222 0.176543 0.176543 
Chi  Kor (Direct) 0.290323 0.571587 0.341129 0.290323 0.178652 0.178652 
Chi  Eng (Direct) 0.395250 0.867702 0.518292 0.372403 0.222787 0.222787 
Eng  Kor (Direct) 0.439838 0.722365 0.543039 0.439585 0.171621 0.171621 
 
Table 3. Performance comparison of system-based strategy on Jap (Japanese) to Chi (Chinese) and 
Chi (Chinese) to Jap (Japanese)/Kor (Korean) through Eng (English) as pivot language, 
where ??(Pivot) + ?(Direct)? means that for the same language pair we merge and re-
rank the pivot transliteration and direct  transliteration results 
 
information. Indeed, given one source 
English names, there are usually more 
than one correct transliteration references 
in Japanese/Korean. This case also hap-
pens to English to Chinese although not 
so heavy as in English to Japa-
nese/Korean. 
 
1449
Language Pairs ACC   F-Score MRR MAP_ref MAP_10 MAP_sys 
Chi Eng Jap  
(Model-based Pivot: O) 
0.087719 0.538454 0.117446 0.085770 0.040645 0.040645 
Chi Eng Jap  
(Model-based Pivot: R) 
0.210526 0.746497 0.381210 0.201267 0.156106 0.156106 
Chi Eng Jap  
(System-based Pivot) 
0.456140 0.777494 0.536591 0.414961 0.183222 0.183222 
Chi  Jap  (Direct) 0.385965 0.769245 0.473851 0.348319 0.159948 0.159948 
Jap Chi Eng  
(Model-based Pivot) 
0.148504 0.724623 0.224253 0.141791 0.088966 0.088966 
Jap Chi Eng 
(System-based Pivot) 
0.201581 0.741627 0.266507 0.191926 0.098024 0.134730 
Jap  Eng (Direct) 0.334839 0.838212 0.450984 0.319277 0.168032 0.168032 
Eng Jap Kor  
(Model-based Pivot) 
0.206269 0.547732 0.300641 0.206269 0.145882 0.145882 
Eng Jap Kor 
(System-based Pivot) 
0.315470 0.629640 0.404769 0.315723 0.167587 0.225892 
Eng  Kor (Direct) 0.439838 0.722365 0.543039 0.439585 0.171621 0.171621 
 
Table 4. Performance of Model-based Pivot Transliteration Strategy 
 
? The N-best accuracy of machine transli-
teration (of both to and from English) is 
very high5. It means that in most cases 
the correct transliteration in pivot lan-
guage can be found in the top-20 results 
and the other 19 results hold the similar 
pronunciations with the correct one, 
which can serve as alternative ?quasi-
correct? inputs to the second stage trans-
literations and thus largely improve the 
overall accuracy.  
 
The above analysis holds when using Eng-
lish as pivot language. Now let us see the case 
of using non-English as pivot language. Table 
4 reports two system-based strategies using 
Chinese and Japanese as pivot languages, 
                                                 
5  Both our studies and previous work (Li et al, 
2004; Zhang et al, 2004) shows that the top-20 
accuracy from English to J/K is more than 0.85 and 
more than 0.95 in English-Chinese case. The top-20 
accuracy is a little worse from C/J/K to English, but 
still more than 0.7. 
where we can find that the performance of two 
system-based strategies is worse than that of 
the direct transliterations. The main reason is 
because that the direct transliteration utilizes 
much more training data than the pivot ap-
proach. However, the good thing is that the 
system-based pivot strategy using non-English 
as pivot language still does not suffer from 
error propagation issue. Its ACC is significant-
ly better than the product of the ACCs of the 
two individual systems. 
4.2.3 Results of Model-based Strategy 
Table 4 reports the performance of model-
based strategy. It clearly shows that the model-
based strategy is less effective and performs 
much worse than both the system-based strate-
gy and direct transliteration.  
While the model-based strategy works well 
at phrase-based statistical machine translation 
(Wu and Wang, 2007, 2009), it does not work 
at machine transliteration. To investigate the 
reasons, we conduct many additional experi-
ments and do statistics on the model and 
1450
aligned training data6. From this in-depth anal-
ysis, we find that main reason is due to the fact 
that the model-based strategy introduces too 
many entries (ambiguities) to the final transli-
teration model. For example, in the 
Jap Chi Eng experiment, the unigram and 
bigram entries of the transliteration model ob-
tained by the model-based strategy are 45 and 
6.6 times larger than that of the transliteration 
model trained directly from parallel data.  This 
is not surprising. Given a transliteration unit in 
pivot language, it can generate     source-
to-target transliteration unit mappings (unigram 
entry of the model), where  is the number of 
the source units that can be mapped to the pi-
vot unit and   is the number of the target units 
that can be mapped from the pivot unit. 
Besides the ambiguities introduced by the 
large amount of entries in the model, another 
reason that leads to the worse performance of 
model-based strategy is the size inconsistence 
of transliteration unit of pivot language. As 
shown at Table 4, we conduct three experi-
ments. In the first experiment (Chi Eng Jap), 
we use English as pivot language. We find that 
the English transliteration unit size in 
Chi Eng model is much larger than that in 
Eng Jap model. This is because from phonetic 
viewpoint, in Chi Eng model, the English unit 
is at syllable level (corresponding one Chinese 
character) while in Eng Jap model, the English 
unit is at sub-syllable level (consonant or vowel 
or syllable, corresponding one Japanese Kata-
kana). This is the reason why we conduct two 
model-based experiments for Chi Eng Jap. 
One is based on the original alignments (Mod-
el-based Pivot: O) and one is based on the re-
constructed alignments 7  (Model-based Pivot: 
R). Experimental results clearly show that the 
reconstruction improves performance signifi-
cantly. In the second and third experiments 
(Jap Chi Eng, Eng Jap Kor), we use Chi-
nese and Japanese as pivot languages. Therefore 
we do not need to re-construct transliteration 
                                                 
6 However, due to space limitation, we are not al-
lowed to report the details of those experiments.  
7Based on the English transliteration units obtained 
from Chi Eng, we reconstruct the English transli-
teration units and alignments in Eng Jap by merg-
ing the adjacent units of both English and Japanese 
to syllable level. 
units and alignments. However, the perfor-
mance is still very poor. This is due to the first 
reason of the large amount of ambiguities. 
The above two reasons (ambiguities and 
transliteration unit inconsistence) are mixed 
together, leading to the worse performance of 
the model-based strategy. We believe that the 
fundamental reason is because the pivot transli-
teration unit is too small to be able to convey 
enough phonetic information of source lan-
guage to target language and thus generates too 
many alignments and ambiguities. 
5 Conclusions 
A big challenge to statistical-based machine 
transliteration is the lack of the training data, 
esp. to those language pairs without English 
involved. To address this issue, inspired by the 
research in the SMT research community, we 
study two pivot transliteration methods. One is 
at system level while another one is at model 
level. We conduct extensive experiments using 
NEW 2009 benchmarking data. Experimental 
results show that system-based method is very 
effective in capturing the phonetic information 
of source language. It not only avoids success-
fully the error propagation issue, but also fur-
ther boosts the transliteration performance by 
generating more alternative pivot results as the 
inputs of the second stage. In contrast, the 
model-based method in its current form fails to 
convey enough phonetic information from 
source language to target language.  
For the future work, we plan to study how to 
improve the model-based strategy by pruning 
out the so-called ?bad? transliteration unit 
pairs and re-sampling the so-called ?good? unit 
pairs for better model parameters. In addition, 
we also would like to explore other pivot-
based transliteration methods, such as con-
structing source-target training data through 
pivot languages. 
References 
Yaser Al-Onaizan and Kevin Knight. 2002. Trans-
lating named entities using monolingual and bi-
lingual resources. ACL-02 
Adam L. Berger, Stephen A. Della Pietra and 
Vincent J. Della Pietra. 1996. A Maximum En-
tropy Approach to Natural Language Processing. 
Computational Linguistics. 22(1):39?71 
1451
N. Bertoldi, M. Barbaiant, M. Federico and R. Cat-
toni. 2008. Phrase-based Statistical Machine 
Translation with Pivot Languages. IWSLT-08 
Trevor Cohn and Mirella Lapata. 2007. Machine 
Translation by Triangulation: Making Effective 
Use of Multi-Parallel Corpora. ACL-07 
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. IJCNLP-08 
Wei Gao, Kam-Fai Wong and Wai Lam. 2004. 
Phoneme-based Transliteration of Foreign 
Names for OOV Problems. IJCLNP-04  
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. EMNLP-08 
A.P. Dempster, N.M. Laird, D.B.Rubin.1977. Max-
imum likelihood from incomplete data via the 
EM algorithm, J. Roy. Stat. Soc., Ser. B. Vol. 39 
Ulf Hermjakob, K. Knight and Hal Daum e?. 2008. 
Name translation in statistical machine transla-
tion: Learning when to transliterate. ACL-08 
John Lafferty, Fernando Pereira, Andrew McCal-
lum. 2001. Conditional random fields: Probabil-
istic models for segmenting and labeling se-
quence data. ICML-01  
B.J. Kang and Key-Sun Choi. 2000. Automatic 
Transliteration and Back-transliteration by De-
cision Tree Learning. LREC-00 
Mitesh Khapra, Kumaran A and Pushpak Bhatta-
charyya. 2010. Everybody loves a rich cousin: 
An empirical study of transliteration through 
bridge languages. NAACL-HLT-10 
Alexandre Klementiev and Dan Roth. 2006. Weakly 
supervised named entity transliteration and dis-
covery from multilingual comparable corpora. 
COLING-ACL-06 
K. Knight and J. Graehl. 1998. Machine Translite-
ration, Computational Linguistics, Vol 24, No. 4 
P. Koehn, F. J. Och and D. Marcu. 2003. Statistical 
phrase-based translation. HLT-NAACL-03 
J. Lafferty, A. McCallum and F. Pereira. 2001. 
Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. 
ICML-01 
Haizhou Li, A Kumaran, Vladimir Pervouchine and 
Min Zhang. 2009a. Report of NEWS 2009 Ma-
chine Transliteration Shared Task. IJCNLP-
ACL-09 Workshop: NEWS-09 
Haizhou Li, A Kumaran, Min Zhang and Vladimir 
Pervouchine. 2009b. Whitepaper of NEWS 2009 
Machine Transliteration Shared Task. IJCNLP-
ACL-09 Workshop: NEWS-09 
Haizhou Li, Ming Zhang and Jian Su. 2004. A Joint 
Source-Channel Model for Machine Translitera-
tion. ACL-04 
Thomas Mandl and Christa Womser-Hacker. 2004. 
How do Named Entities Contribute to Retrieval 
Effectiveness? CLEF-04 
Helen M. Meng, Wai-Kit Lo, Berlin Chen and Ka-
ren Tang. 2001. Generate Phonetic Cognates to 
Handle Name Entities in English-Chinese cross-
language spoken document retrieval. ASRU-01 
Jong-Hoon Oh, Kiyotaka Uchimoto, and k. Torisa-
wa. 2009. Machine Transliteration with Target-
Language Grapheme and Phoneme: Multi-
Engine Transliteration Approach. NEWS 2009 
Franz Josef Och and Hermann Ney. 2003. A Syste-
matic Comparison of Various Statistical Align-
ment Models. Computational Linguistics 29(1) 
V. Pervouchine, H. Li and B. Lin. 2009. Translite-
ration Alignment. ACL-IJCNLP-09 
R. Schwartz and Y. L. Chow. 1990. The N-best 
algorithm: An efficient and exact procedure for 
finding the N most likely sentence hypothesis, 
ICASSP-90 
Tarek Sherif and Grzegorz Kondrak. 2007. Sub-
string-based transliteration. ACL-07 
Richard Sproat, Tao Tao and ChengXiang Zhai. 
2006. Named entity transliteration with compa-
rable corpora. COLING-ACL-06 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. ICSLP-02 
Masao Utiyama and Hitoshi Isahara. 2007. A Com-
parison of Pivot Methods for Phrase-based Sta-
tistical Machine Translation. NAACL-HLT-07 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer 
Stephen Wan and Cornelia Maria Verspoor. 1998. 
Automatic English-Chinese name transliteration 
for development of multilingual resources. COL-
ING-ACL-98 
Hua Wu and Haifeng Wang. 2007. Pivot Language 
Approach for Phrase-based Statistical Machine 
Translation. ACL-07 
Hua Wu and Haifeng Wang. 2009. Revisiting Pivot 
Language Approach for Machine Translation. 
ACL-09 
Dmitry Zelenko and Chinatsu Aone. 2006. Discri-
minative methods for transliteration. EMNLP-06 
Min Zhang, Haizhou Li and Jian Su. 2004. Direct 
Orthographical Mapping for machine translite-
ration. COLING-04 
1452
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1865?1874, Dublin, Ireland, August 23-29 2014.
Synchronous Constituent Context Model for Inducing Bilingual
Synchronous Structures
Xiangyu Duan Min Zhang
?
Qiaoming Zhu
School of Computer Science & Technology, Soochow University
{xiangyuduan;minzhang;qmzhu}@suda.edu.cn
Abstract
Traditional Statistical Machine Translation (SMT) systems heuristically extract synchronous
structures from word alignments, while synchronous grammar induction provides better so-
lutions that can discard heuristic method and directly obtain statistically sound bilingual syn-
chronous structures. This paper proposes Synchronous Constituent Context Model (SCCM) for
synchronous grammar induction. The SCCM is different to all previous synchronous grammar
induction systems in that the SCCM does not use the Context Free Grammars to model the bilin-
gual parallel corpus, but models bilingual constituents and contexts directly. The experiments
show that valuable synchronous structures can be found by the SCCM, and the end-to-end ma-
chine translation experiment shows that the SCCM improves the quality of SMT results.
1 Introduction
Traditional Statistical Machine Translation (SMT) learns translation model from bilingual corpus that
is sentence aligned. No large-scale hand aligned structures inside the parallel sentences are usually
available to the SMT community, while the aligned structures are essential for training the translation
model. Thus, various unsupervised methods had been explored to automatically obtain aligned structures
inside the parallel sentences. Currently, the dominant method is a two step pipeline that obtains word
alignments by unsupervised learning (Brown et al., 1993) at the first step, then obtains aligned structures
at the second step by heuristically extracting all bilingual structures that are consistent with the word
alignments.
The second step in this two step pipeline is problematic due to its obtained aligned structures, whose
counts are heuristically collected and violate valid translation derivations, while most SMT decoders
perform translation via valid translation derivations. This problem leads to researches on synchronous
grammar induction that discards the heuristic method and the two separate steps pipeline.
Synchronous grammar induction aims to directly obtain aligned structures by using one statistically
sound model. The aligned structures in synchronous grammar induction are hierarchical/syntax level
(Cohn and Blunsom, 2009) synchronous structures, which can be modeled by Synchronous Context Free
Grammars (SCFGs) (Cohn and Blunsom, 2009; Levenberg et al., 2012; Xiao et al., 2012; Xiao and
Xiong, 2013) or a kind of SCFGs variant - Inversion Transduction Grammars (ITGs) (Neubig et al.,
2011; Cohn and Haffari, 2013). Both SCFGs and ITGs are studied in recent years by using generative or
discriminative modeling.
This paper departs from using the above two traditional CFGs-based grammars, and proposes Syn-
chronous Constituent Context Model (SCCM) which models synchronous constituents and contexts
directly so that bilingual translational equivalences can be directly modeled. The proposed SCCM is
inspired by researches on monolingual grammar induction, whose experience is valuable to the syn-
chronous grammar induction community due to its standard evaluation on released monolingual tree-
banks, while no hand annotated bilingual synchronous treebank is available for evaluating synchronous
?
Corresponding Author
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1865
grammar induction. According to the evaluation results, the state-of-the-art monolingual grammar induc-
tion was achieved by Bayesian modeling of the Constituent Context Model (CCM) (Duan et al., 2013;
Klein and Manning, 2002), while traditional CFGs based monolingual grammar induction methods per-
form well below the CCM.
In view of the significant achievements of the CCM in monolingual grammar induction, we propose
the SCCM to apply the CCM to the bilingual case. The tremendous possible constituents and contexts
incurred in this bilingual case put a challenge for the SCCM to model such kind of sparse variables. We
further propose a non-parametric Bayesian Modeling of the SCCM to cope with the sparse variables.
Experiments on Chinese-English machine translation show that meaningful synchronous phrases can be
detected by our SCCM, and the performance of the end-to-end SMT is significantly improved.
The rest of the paper is structured as follows: we propose the SCCM in Section 2. The non-parametric
Bayesian modeling of the SCCM is presented in Section 3, followed by the presentation of posterior
inference for the Bayesian SCCM. Then experiments and results are presented. Conclusion are presented
in the final section.
2 Synchronous Constituent Context Model (SCCM)
We propose the SCCM to model synchronous structures explicitly. Unlike Synchronous Context Free
Grammars (SCFGs) which are defined on latent production rules of parallel corpus, the SCCM deals
with both synchronous tree spans (syn spans) and non-synchronous spans (non-syn spans). All spans
are represented by two kinds of strings: bilingual constituents and bilingual contexts. The SCCM is a
generative model defined over such representations.
2.1 Bilingual Constituents and Contexts
By extending the concept of constituents and contexts introduced in (Klein and Manning, 2002), we
define bilingual constituents and contexts as follows. Bilingual constituents are pairs of contiguous
surface strings of sentence spans (bilingual subsequences), bilingual contexts are tokens preceding and
following the bilingual constituents. In the SCCM, each bi-span in a sentence pair, either a syn span or
a non-syn span, is represented by a bilingual constituent and a bilingual context.
Fig. 1 gives an illustration of the bilingual constituents and contexts. In Fig. 1-(a), a latent syn-
chronous tree over the example sentence pair is illustrated. With the word alignments shown in the
sentence pair, the latent tree over the target sentence ?e
1
e
2
e
3
? can be inferred. For the ease of presen-
tation, the latent target side tree is neglected in Fig. 1-(a).
Given the synchronous tree, two sets of bilingual constituents and contexts can be extracted as shown
in the two tables of Fig. 1. One is about syn spans, the other is about non-syn spans. 3 appearing in
the contexts denotes a sentence boundary. nil appearing in the constituents of the non-tree spans denotes
an empty span, which is actually a space between two terminals (or between a terminal and 3).
2.2 Generative Model
The SCCM computes the joint probability of a sentence pair S and its synchronous tree T as below:
P (S, T ) = P (S|T )P (T ) = P (S|T )P (B)P (T |B) (1)
= P (S|T )P (B)
?
0?i?j?m
0?p?q?n
P (?
ij,pq
|B
ij,pq
)P (?
ij,pq
|B
ij,pq
)
where B denotes a synchronous bracketing skeleton, in which no words are populated. Fig. 1-(b) shows
the skeleton of Fig. 1-(a). The skeleton B is considered being filled by the synchronous tree T , and
P (T |B) is decomposed into conditional probabilities of bilingual constituents ? and contexts ? condi-
tioning on B
ij,pq
, a Boolean variable indicating whether the under-consideration bi-span <i, j><p, q>
is a syn span or not. In particular, ?
ij,pq
denotes the bilingual constituent spanning from i to j on source
side sentence, and spanning from p to q on target side sentence. ?
ij,pq
denotes the context of ?
ij,pq
.
1866
  
 
 
 
 
 
 
f1 f2 f3 0 1 2 3 
e1 e2 e3 0 1 2 3 
syn span <0,1><2,3> <1,2> <1,2> <2,3><0,1> <0,2><1,3> <0,3><0,3> 
constituent (f1)(e3) (f2)(e2) (f3)(e1) (f1 f2)(e2 e3) (f1 f2 f3)(e1 e2 e3) 
context (-f2)(e2-) (f 1-f3)(e1- e3) (f2-)(- e2) (-f3)(e1-) (-)(-) 
 
non-syn span <1,3><0,1> <1,1><1,2>       ? 
constituent (f2 f3)(e1) (nil)(e2)         ? 
context (f1-)(-e2) (f 1-f2)(e1- e3)      ? 
 
 
 
 
 
 
 
 
 
 
 
 
(a) (b)
Figure 1: Illustration of bilingual constituents and contexts over a sentence pair which consists of a
source side sentence ?f
1
f
2
f
3
? and a target side sentence ?e
1
e
2
e
3
?. In (a), the bottom numbers around
each word are indexes for denoting spans. A synchronous tree is illustrated in (a), based on which two
sets of bilingual constituents and contexts are extracted as shown in the two tables below the tree. Take a
syn span <1,2><1,2> for example, the source side span <1,2> is ?f
2
? and the target side span <1,2>
is ?e
2
?. They constitutes a bilingual constituent ?(f
2
)(e
2
)?, whose context is ?(f
1
-f
3
)(e
1
-e
3
)? that is
preceding and following the bilingual constituent. Figure (b) shows the skeleton of figure (a).
B
ij,pq
is defined as below:
B
ij,pq
=
{
1 if bispan < i, j >< p, q > is a syn span
0 otherwise
In the SCCM, skeletons Bs are restricted to be binary branching and are distributed uniformly. Fur-
thermore, since T and S are consistent, P (S|T ) is always equal to 1 in Eq. (1). Therefore, we can infer
(with the expansion of the continued multiplication operator of Eq. (1) ):
P (S, T ) ?
?
<i,j><p,q>?T
(P (?
ij,pq
|B
ij,pq
= 1)P (?
ij,pq
|B
ij,pq
= 1)) (2)
?
<i,j><p,q> ??T
(P (?
ij,pq
|B
ij,pq
= 0)P (?
ij,pq
|B
ij,pq
= 0))
where <i, j><p, q> ? T indicates that bi-span <i, j><p, q> is a syn span contained in T ,
<i, j><p, q> ?? T indicates otherwise case. Formula (2) is the basis for Bayesian modeling of the
SCCM and the posterior inference that are proposed in the following sections.
3 Bayesian Modeling for the SCCM
For the SCCM, the posterior of a synchronous tree T given the observation of a sentence pair S is:
P (T |S) ? P (S, T ). As shown in formula (2), it turns out that the posterior P (T |S) depends on the four
kinds of distributions:
P (?
ij,pq
|B
ij,pq
= 1) P (?
ij,pq
|B
ij,pq
= 1)
P (?
ij,pq
|B
ij,pq
= 0) P (?
ij,pq
|B
ij,pq
= 0)
1867
We propose to define two kinds of Bayesian priors over the constituents related variables ?
ij,pq
|B
ij,pq
and the contexts related variables ?
ij,pq
|B
ij,pq
respectively. Since constituents exhibits richer appear-
ances than contexts, the proposed Bayesian prior over ?
ij,pq
|B
ij,pq
is more complicate than that over
?
ij,pq
|B
ij,pq
.
Specifically, one of the non-parametric Bayesian priors, the Pitman-Yor-Process (PYP) prior, is defined
on ?
ij,pq
|B
ij,pq
. The PYP prior can produce the power-law distribution (Goldwater et al., 2009) that is
commonly observed in natural languages, and can flexibly model distributions on layer structures due to
its defined distribution on distribution hierarchy. The PYP prior had been successfully applied on many
NLP tasks such as language modeling (YeeWhye, 2006), word segmentation (Johnson et al., 2007b;
Goldwater et al., 2011), dependency grammar induction (Cohen et al., 2008; Cohn et al., 2010), grammar
refinement (Liang et al., 2007; Finkel et al., 2007) and Tree-Substitution Grammar induction (Cohn et
al., 2010). We use the PYP to model the constituents? layered structure by using the PYP?s distribution
hierarchy. On ?
ij,pq
|B
ij,pq
, we use the Dirichlet distribution for its simplicity because contexts appear in
much fewer kinds of surface strings than those of constituents.
3.1 The PYP Prior over Bilingual Constituents
Constituents consist of both words and POS tags. Though in much monolingual grammar induction
works, only POS tag sequences were used as the observed constituents for their significant hints of
phrases (Klein and Manning, 2002; Cohn et al., 2010), our work needs considering raw words as obser-
vation data too because word alignments encode the important translation correspondence and contribute
to synchronous bi-spans. But it causes severe data sparse problem due to the quite large number of unique
constituents consisting of both words and POS tags. Besides, constituents can be extremely long which
intensify the data sparse problem. So, solely using the surface strings of constituents is impractical.
In this section, we propose a hierarchical representation of constituents to overcome the data sparse
problem and use the PYP prior on this kind of representation. From top to bottom, the hierarchical rep-
resentation encodes the information of a bilingual constituent from fine-grained level to coarse-grained
levels. The probability of a fine-grained level can be backed-off to the probabilities of coarse-grained
levels.
The first (top) level of the hierarchical representation is the bilingual constituent itself. The second
level is composed of two sequences: one is word sequence, the other is POS tags sequence. The third
level mainly decomposes the second level into boundaries and middle words/POSs. Since the target of
inducing synchronous structures in this paper is to induce the latent phrasal equivalences of a parallel
sentence, boundaries of bilingual constituents play the key role of identifying phrasal equivalences. The
third level is the function to make use of boundaries. Fig. 2 gives an illustration of the hierarchical
representation.
w1p1 w2p2 w3p3 w4p4
w1 w2 w3 w4 p1 p2 p3 p4
w1 w2 w4 w3 p1 p2 p4 p3
Figure 2: Illustration of the hierarchical representation of a bilingual constituent ?w
1
p
1
w
2
p
2
w
3
p
3
w
4
p
4
?. Here w and p denote word and POS respectively, and the suffixes denote positions. Note that
both w and p are composite, w denotes a source side word and a target side word, and p denotes the
POS case. The second level decomposes the first level into a word sequence and a POS sequence, and
the third level decomposes further into boundaries and middle words/POSs. The boundary width in this
figure is two for left side boundary and one for right side boundary.
1868
The PYP prior encodes distribution on distribution. Recursively using the PYP prior can create a
distribution hierarchy, which is appropriate for modeling the distribution over the hierarchical repre-
sentations of constituents. Smoothing is fulfilled through backing-off fine-grained level distributions to
coarse-grained level distributions.
3.1.1 The PYP Hierarchy
We define the PYP hierarchy over the hierarchical representation of bilingual constituents in a top-down
manner. For the topmost (first) level:
?
ij,pq
|B
ij,pq
= b ? G
first
b
G
first
b
? PY P (d
first
b
, ?
first
b
, P
word?pos
(.|B
ij,pq
= b))
The PYP has three parameters: (d
first
b
, ?
first
b
, P
word?pos
). P
word?pos
(.|B
ij,pq
= b) is a base
distribution over infinite space of bilingual constituents conditioned on span type b, which provides
the back-off probability of P (?
ij,pq
|B
ij,pq
= b). The remaining parameters d
first
b
and ?
first
b
control the
strength of the base distribution.
The back-off probability P
word?pos
(?
ij,pq
= x|B
ij,pq
= b) is defined as below:
P
word?pos
(?
ij,pq
= x|B
ij,pq
= b)) = P
word
(Rw(x)|b)? P
pos
(Rp(x)|b)
where Rw(x) is the function returning a word sequence of a bilingual constituent x, Rp(x) returning
the correspondent POS sequence. This is the second level of the hierarchical representation of bilingual
constituents as illustrated in Fig. 2. Further, Rw(x) and Rp(x) are decomposed into the third level of
the hierarchy. Taking Rw(x) for example:
P
word
(Rw(x)|B
ij,pq
= b)) = P
word?bound
(Rwb(x)|b)?
1
|W |
|Rw(x)|?|Rwb(x)|
where Rwb is a function returning a word sequence?s boundary representation, |W | is the vocabulary
size, |Rw(x)| ? |Rwb(x)| is the number of the words in Rw(x) excluding those in the boundary rep-
resentation. The above equation models the generation of a word sequence with surface string Rw(x)
(given b) by first generating its boundary representation Rwb(x), then generating its middle words from
a uniform distribution over the vocabulary. P
pos
(Rp(x)|B
ij,pq
= b)) is defined similarly.
We put the Dirichlet prior over P
word?bound
(Rwb(x)|b):
Rwb(x)|b ? Discrete(G
Rwb
b
)
G
Rwb
b
? Dirichlet(?
b
)
For P
pos?bound
(Rpb(x)|b), similar definition to P
word?bound
(Rwb(x)|b) is applied.
3.2 The Dirichlet Prior over Bilingual Contexts
The Dirichlet prior is defined as below:
?
ij,pq
|B
ij,pq
= b ? Discrete(G
Dir
b
)
G
Dir
b
? Dirichlet(?
b
)
A context ?
ij,pq
(given the specific span type b) is drawn i.i.d according to a multinomial parameter
G
Dir
b
, which is drawn from the Dirichlet distribution with a real value parameter ?
b
.
1869
4 MCMC Sampling for Inferring the Latent Synchronous Trees
We approximate the distribution over latent synchronous trees by sampling them from the posterior
P (T |S), where T is a latent synchronous tree of a sentence pair S. As presented in the beginning of
section 3, the posterior depends on P (?
ij,pq
|B
ij,pq
= b) and P (?
ij,pq
|B
ij,pq
= b), on which we put
the PYP prior and the Dirichlet prior respectively. Because of integrating out all Gs in all of the priors,
interdependency between samples of ?
ij,pq
|B
ij,pq
= b or ?
ij,pq
|B
ij,pq
= b is introduced, resulting in
simultaneously obtaining multiple samples impractical. On the other hand, blocked sampling, which ob-
tains sentence-level samples simultaneously (Blunsom and Cohn, 2010; Cohn et al., 2010; Johnson et al.,
2007a) is attractive for the fast mixing speed and the easy application of standard dynamic programming
algorithms.
4.1 Metropolis-Hastings (MH) Sampler
We apply a MH sampler similar to (Johnson et al., 2007a) to overcome the difficulty of obtaining multi-
ple samples simultaneously from posterior. The MH sampler is a MCMC technique that draws samples
from a true distribution by first drawing samples simultaneously from a proposal distribution, and then
correcting the samples to the true distribution by using an accept/reject test. In practical, the proposal
distribution is designed to facilitate the use of blocked sampling that applies standard dynamic program-
ming, and the resulting samples are corrected by the accept/reject test to the true distribution.
In our case, the proposal distribution is theMaximum-a-Posteriori (MAP) estimate of P (?
i,j
|B
i,j
= b)
and P (?
i,j
|B
i,j
= b), and the blocked sampling of T applies a dynamic programming algorithm that is
based on the inside chart derived from a transformation of Eq. (1):
P (S, T ) = K(S)
?
<i,j><p,q>?T
?(ij, pq)
where ?(ij, pq) =
P (?
ij,pq
|B
ij,pq
= 1)P (?
ij,pq
|B
ij,pq
= 1)
P (?
ij,pq
|B
ij,pq
= 0)P (?
ij,pq
|B
ij,pq
= 0)
K(S) is a constant given S. The inside chart I can be constructed recursively as below:
I
ij,pq
=
?
?
?
?
?
?
?
?
?
?
?
?(ij, pq) if j ? i ? 1 and q ? p ? 1
?(ij, pq)
?
i?u?j
p?v?q
(I
iu,pv
I
uj,vq
+ I
iu,vq
I
uj,pv
) otherwise
Based on this inside chart, a synchronous tree can be top-down sampled (Johnson et al., 2007a), then
is accepted or rejected by the MH-test to correct to the true distribution.
5 Experiments
The experiments were conducted on both a pilot word alignment task and an end-to-end Chinese-to-
English machine translation task to test the quality of the learned synchronous structures by the SCCM.
The bi-side monolingual gold bracketings contained in Penn treebanks were not used for evaluating the
quality of the learned synchronous structures because of great syntactic divergence between source tree
and target tree, which results in that gold monolingual syntactic trees on both sides are asynchronous
(large number of tree nodes can not be aligned). The synchronous grammar induction community as-
sumes the existence of synchronous grammar for MT, and do not evaluate synchronous grammar induc-
tion on monolingual gold treebanks because of their asynchronous property. The synchronous grammar
induction community is not the same with the multilingual grammar induction community, which targets
at inducing bi-side monolingual syntactic trees. Due to the same reason, our synchronous bracketing
induction method was not evaluated on bi-side monolingual bracketing trees which are asynchronous.
1870
5.1 Sampler Configuration
Our sampler was initialised with trees through a random split process. Firstly, we used GIZA++ mod-
el 4 to get source-to-target and target-to-source word alignments, and used grow-diag-final-and (gdfa)
heuristic to extract reliable word alignments for each sentence pair. Secondly, we randomly split each
sentence pair in a top-down manner, and make sure that each split is consistent with the GIZA++ gdfa
word alignments. For example, given a sentence pair of m source words and n target words, we random-
ly choose a split point at each side and the alignment type (straight alignment or inverted alignment),
then recursively build bi-spans further on each new split. Finally, a synchronous binary tree is built at
the end of this process
1
. Note that all splits must be consistent with the GIZA++ gdfa word alignments.
When a piece of word alignments (such as non-ITG alignment structure) do not permit binary split, we
keep this structure unsplitted and continue split only on its sub-structures that are ITG derivable.
Our sampler ran 200 iterations for all data. After each sampling iteration, we resample all the hyper-
parameters using slice-sampling, with the following priors: d ? Beta(1, 1), ? ? Gamma(10, 0.1).
The time complexity of our inference algorithm is O(n
6
), which is not practical in applications. We
reduce the time complexity by only considering bi-spans that do not violate GIZA++ intersection word
alignments (intersection of source-to-target and target-to-source word alignments) (Cohn and Haffari,
2013).
5.2 Word Alignment Task
5.2.1 Experimental Setting
Since there are no annotated synchronous treebanks, we evaluate the SCCM indirectly by evaluating its
output word alignments on a gold standard English Chinese parallel tree bank with hand aligned word
alignments referred as HIT corpus
2
. The HIT corpus, which was collected from English learning text
books in China as well as example sentences in dictionaries, was originally designed for annotating
bilingual tree node alignments. The annotation strictly reserves the semantic equivalence of the aligned
sub-tree pair. The byproduct of this corpus is the hand aligned word alignments, which was utilized
to evaluate word alignment performance
3
. The word segmentation, tokenization and parse-tree in the
corpus were manually constructed or checked. The statistics of the HIT corpus are shown in table 1.
Table 1: Corpus statistics of the HIT corpus.
ch en
sent 16131
word 210k 209k
avg. len. 13.06 13.0
5.2.2 Results
We adopt the commonly used metric: the alignment error rate (AER) to evaluate our proposed align-
ments (a) against hand-annotated alignments, which are marked with sure (s) and possible (p) align-
ments. The AER is given by (the lower the better):
AER(a, s, p) = 1?
|a ? s|+ |a ? p|
|a|+ |s|
In the HIT corpus, only sure alignments were annotated, possible alignments were bypassed because
of the strict annotation standard of semantic equivalence.
The word alignments evaluation results are reported in Table 2. The baseline was GIZA++ model
4 in both directions with symmetrization by the grow-diag-final-and heuristic (Koehn et al., 2003). A
1
The initialization with different random split bi-trees results in marginal variance of performances.
2
HIT corpus is designed and constructed by HIT-MITLAB. http://mitlab.hit.edu.cn/index.php/resources.html
3
We did not use annotated tree node alignments for synchronous structure evaluation because the coverage of tree nodes
that can be aligned is quite low. The reason of low coverage is that Chinese and English exhibit great syntax divergences from
monolingual treebank point of view.
1871
released induction system - PIALIGN (Neubig et al., 2011)
4
was also experimented to compare with our
proposed induction system - SCCM.
PIALIGN is a model that generalizes adaptor grammars for machine translation (MT), while our mod-
el is to generalize CCM for MT. Adaptor grammars has been successfully applied on shallow unsuper-
vised tasks such as morphlogical/word analysis, while CCM has obtained state-of-the-art performance
on the more complex unsupervised task - inducing syntactic trees. In view of CCM?s successful mono-
lingual application, we generalize it to bilingual case. In depth comparison: our SCCM deals with both
consituents and distituents, and contexts of them, while PIALIGN only deals with constituents. Fur-
thermore, SCCM does not model non-terminal rewriting rules, while PIALIGN model those rules which
can rewrite a non-terminal into a complete subtree as adaptor grammars does. In addition, PIALIGN
adopts a beam search algorithm of (Saers et al., 2009). Through setting small beam size, PIALIGN?s
time complexity is almost O(n
3
). But as critisized by (Cohn and Haffari, 2013), their heuristic beam
search algorithm does not meet either of the Markov Chain Monte Carlo (MCMC) criteria of ergodic-
ity or detailed balance. Our method adopts MCMC sampling (Johnson et al., 2007a) which meets the
MCMC criteria.
We can see that the two induction systems perform significantly better than GIZA++, and our proposed
SCCM performs better than PIALIGN. Manual evaluation for the quality of the phrase pairs generated
from word alignments is also reported in Table 2. We considered the top-100 high frequency phrase pairs
that are beyond word level and less than six words on both sides, and report the proportion of reasonably
well phrase pairs through manual check. We found that more good phrase pairs can be derived from the
SCCM?s word alignments than from others.
Table 2: Quality of word alignments and their generated phrase pairs.
AER good phrase pairs proportion
GIZA++ 0.322 0.493
PIALIGN 0.263 0.531
SCCM 0.255 0.534
5.3 Machine Translation Task
5.3.1 Experimental Setting
A released tourism-related domain machine translation data was used in our experiment. It consists of a
parallel corpus extracted from the Basic Travel Expression Corpus (BTEC), which had been used in
evaluation campaigns of the yearly International Workshop on Spoken Language Translation (IWSLT).
Table 3 lists statistics of the corpus used in the experiment.
Table 3: Statistics of the corpus used by IWSLT
ch en
sent 23k
word 190k 213k
avg. len. 8.3 9.2
We used CSTAR03 as development set, used IWSLT04 and IWSLT05 official test set for test. A
4-gram language model with modified Kneser-Ney smoothing was trained on English side of parallel
corpus. We use minimum error rate training (Och, 2003) with nbest list size 100 to optimize the fea-
ture weights for maximum development BLEU. Experimental results were evaluated by case-insensitive
BLEU-4 (Papineni et al., 2001). Closest reference sentence length was used for brevity penalty.
5.3.2 Results
Following (Levenberg et al., 2012; Neubig et al., 2011; Cohn and Haffari, 2013), we evaluate our model
by using the SCCM?s output word alignments to construct a phrase table. As a baseline, we train a
phrase-based model using the moses toolkit
5
based on the word alignments obtained using GIZA++
4
http://www.phontron.com/pialign/
5
http://www.statmt.org/moses
1872
model 4 in both directions and symmetrized using the grow-diag-final-and heuristic (Koehn et al., 2003).
For comparison with CFG-based induction systems, word alignments generated by the PIALIGN were
also used to train a phrase-based model.
In the end-to-end MT evaluation, we used the standard set of features: relative-frequency and lexical
translation model probabilities in both directions; distance-based distortion model; language model and
word count. The evaluation results are reported in table 4. Word alignments derived by the two induction
systems can be more helpful to obtain better translations than GIZA++ derived word alignments. The
SCCM, while departing from traditional CFG-based methods, achieves comparable translation perfor-
mance to the PIALIGN.
Table 4: BLEU on both the development set: CSTAR03, and the two test sets: IWSLT04 and IWSLT05.
CSTAR03 IWSLT04 IWSLT05
GIZA++ 0.4304 0.4190 0.4866
PIALIGN 0.4661 0.4556 0.5248
SCCM 0.4560 0.4469 0.5193
6 Conclusion
A new model for synchronous structure induction is proposed in this paper. Different to all the previous
works that are based on Context Free Grammars, our proposed SCCM deals with bilingual constituents
and contexts explicitely so that bilingual translational equivalences can be directly modeled. A non-
parametric Bayesian modeling of the SCCM is applied to cope with the sparse representations of bilin-
gual constituents and contexts. Both intrinsic evaluation on word alignments and extrinsic evaluation on
end-to-end machine translation were conducted. The intrinsic evaluation show that the highest quality
word alignments were obtained by our proposed SCCM. Such statistically sound word alignments of
the SCCM were used in the extrinsic evaluation on machine translation, showing that significantly better
translations were achieved than those obtained by using the word alignments of GIZA++, the widely
used word aligner in the two-step pipeline.
Acknowledgments
This work was supported by the National Natural Science Foundation of China under grant No.
61273319, and grant No. 61373095. Thanks for the helpful advices of anonymous reviewers.
References
Phil Blunsom and Trevor Cohn. 2010. Unsupervised induction of tree substitution grammars for dependency
parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages
1204?1213. Association for Computational Linguistics.
Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263?311.
Shay B Cohen, Kevin Gimpel, and Noah A Smith. 2008. Logistic normal priors for unsupervised probabilistic
grammar induction. In Proceedings of the Advances in Neural Information Processing Systems.
Trevor Cohn and Phil Blunsom. 2009. A bayesian model of syntax-directed tree to string grammar induction. In
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume
1, pages 352?361. Association for Computational Linguistics.
Trevor Cohn and Gholamreza Haffari. 2013. An infinite hierarchical bayesian model of phrasal translation. In
Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. Journal of
Machine Learning Research, 11:3053?3096.
Xiangyu Duan, Zhang Min, and Chen Wenliang. 2013. Smoothing for bracketing induction. In Proceedings of
23rd International Joint Conference on Artificial Intelligence. AAAI Press/International Joint Conferences on
Artificial Intelligence.
1873
Jenny Rose Finkel, Trond Grenager, and Christopher D Manning. 2007. The infinite tree. In ANNUAL MEETING-
ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 272.
Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. 2011. Producing power-law distributions and damping
word frequencies with two-stage language models. Journal of Machine Learning Research, 12:2335?2382.
Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007a. Bayesian inference for pcfgs via markov chain
monte carlo. In Proceedings of Human Language Technologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139?146.
Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007b. Adaptor grammars: A framework for specify-
ing compositional nonparametric bayesian models. Proceedings of Advances in neural information processing
systems, 19:641.
Dan Klein and Christopher Manning. 2002. A generative constituent-context model for improved grammar induc-
tion. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 128?135.
Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 48?54. Association for Computational Linguistics.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012. A bayesian model for learning scfgs with discontiguous
rules. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and
computational natural language learning, pages 223?232. Association for Computational Linguistics.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein. 2007. The infinite PCFG using hierarchical Dirichlet process-
es. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning
(EMNLP/CoNLL).
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised
model for joint phrase alignment and extraction. In Proceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language Technologies-Volume 1, pages 632?641. Association for
Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160?167. Association for
Computational Linguistics.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009. Learning stochastic bracketing inversion transduction gram-
mars with a cubic time biparsing algorithm. In Proceedings of the 11th International Conference on Parsing
Technologies, pages 29?32. Association for Computational Linguistics.
Xinyan Xiao and Deyi Xiong. 2013. Max-margin synchronous grammar induction for machine translation. In
EMNLP.
Xinyan Xiao, Deyi Xiong, Yang Liu, Qun Liu, and Shouxun Lin. 2012. Unsupervised discriminative induction of
synchronous grammar for machine translation. In COLING, pages 2883?2898.
Teh YeeWhye. 2006. A bayesian interpretation of interpolated kneser-ney. In Technical Report TRA2/06. School
of Computing, National University of Singapore.
1874
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 148?156,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Pseudo-word for Phrase-based Machine Translation 
 
 
Xiangyu Duan Min Zhang Haizhou Li 
Institute for Infocomm Research, A-STAR, Singapore 
{Xduan, mzhang, hli}@i2r.a-star.edu.sg 
 
  
 
Abstract 
 
The pipeline of most Phrase-Based Statistical 
Machine Translation (PB-SMT) systems starts 
from automatically word aligned parallel cor-
pus. But word appears to be too fine-grained 
in some cases such as non-compositional 
phrasal equivalences, where no clear word 
alignments exist. Using words as inputs to PB-
SMT pipeline has inborn deficiency. This pa-
per proposes pseudo-word as a new start point 
for PB-SMT pipeline. Pseudo-word is a kind 
of basic multi-word expression that character-
izes minimal sequence of consecutive words in 
sense of translation. By casting pseudo-word 
searching problem into a parsing framework, 
we search for pseudo-words in a monolingual 
way and a bilingual synchronous way. Ex-
periments show that pseudo-word significantly 
outperforms word for PB-SMT model in both 
travel translation domain and news translation 
domain. 
1 Introduction 
The pipeline of most Phrase-Based Statistical 
Machine Translation (PB-SMT) systems starts 
from automatically word aligned parallel corpus 
generated from word-based models (Brown et al, 
1993), proceeds with step of induction of phrase 
table (Koehn et al, 2003) or synchronous gram-
mar (Chiang, 2007) and with model weights tun-
ing step. Words are taken as inputs to PB-SMT at 
the very beginning of the pipeline. But there is a 
deficiency in such manner that word is too fine-
grained in some cases such as non-compositional 
phrasal equivalences, where clear word align-
ments do not exist. For example in Chinese-to-
English translation, ??? and ?would like to? 
constitute a 1-to-n phrasal equivalence, ??? 
?? and ?how much is it? constitute a m-to-n 
phrasal equivalence. No clear word alignments 
are there in such phrasal equivalences. Moreover, 
should basic translational unit be word or coarse-
grained multi-word is an open problem for opti-
mizing SMT models. 
Some researchers have explored coarse-
grained translational unit for machine translation. 
Marcu and Wong (2002) attempted to directly 
learn phrasal alignments instead of word align-
ments. But computational complexity is prohibi-
tively high for the exponentially large number of 
decompositions of a sentence pair into phrase 
pairs. Cherry and Lin (2007) and Zhang et al 
(2008) used synchronous ITG (Wu, 1997) and 
constraints to find non-compositional phrasal 
equivalences, but they suffered from intractable 
estimation problem. Blunsom et al (2008; 2009) 
induced phrasal synchronous grammar, which 
aimed at finding hierarchical phrasal equiva-
lences. 
Another direction of questioning word as basic 
translational unit is to directly question word 
segmentation on languages where word bounda-
ries are not orthographically marked. In Chinese-
to-English translation task where Chinese word 
boundaries are not marked, Xu et al (2004) used 
word aligner to build a Chinese dictionary to re-
segment Chinese sentence. Xu et al (2008) used 
a Bayesian semi-supervised method that com-
bines Chinese word segmentation model and 
Chinese-to-English translation model to derive a 
Chinese segmentation suitable for machine trans-
lation. There are also researches focusing on the 
impact of various segmentation tools on machine 
translation (Ma et al 2007; Chang et al 2008; 
Zhang et al 2008). Since there are many 1-to-n 
phrasal equivalences in Chinese-to-English trans-
lation (Ma and Way. 2009), only focusing on 
Chinese word as basic translational unit is not 
adequate to model 1-to-n translations. Ma and 
Way (2009) tackle this problem by using word 
aligner to bootstrap bilingual segmentation suit-
able for machine translation. Lambert and 
Banchs (2005) detect bilingual multi-word ex-
148
pressions by monotonically segmenting a given 
Spanish-English sentence pair into bilingual 
units, where word aligner is also used. 
IBM model 3, 4, 5 (Brown et al, 1993) and 
Deng and Byrne (2005) are another kind of re-
lated works that allow 1-to-n alignments, but 
they rarely questioned if such alignments exist in 
word units level, that is, they rarely questioned 
word as basic translational unit. Moreover, m-to-
n alignments were not modeled. 
This paper focuses on determining the basic 
translational units on both language sides without 
using word aligner before feeding them into PB-
SMT pipeline. We call such basic translational 
unit as pseudo-word to differentiate with word. 
Pseudo-word is a kind of multi-word expression 
(includes both unary word and multi-word). 
Pseudo-word searching problem is the same to 
decomposition of a given sentence into pseudo-
words. We assume that such decomposition is in 
the Gibbs distribution. We use a measurement, 
which characterizes pseudo-word as minimal 
sequence of consecutive words in sense of trans-
lation, as potential function in Gibbs distribution. 
Note that the number of decomposition of one 
sentence into pseudo-words grows exponentially 
with sentence length. By fitting decomposition 
problem into parsing framework, we can find 
optimal pseudo-word sequence in polynomial 
time. Then we feed pseudo-words into PB-SMT 
pipeline, and find that pseudo-words as basic 
translational units improve translation perform-
ance over words as basic translational units. Fur-
ther experiments of removing the power of 
higher order language model and longer max 
phrase length, which are inherent in pseudo-
words, show that pseudo-words still improve 
translational performance significantly over 
unary words. 
This paper is structured as follows: In section 
2, we define the task of searching for pseudo-
words and its solution. We present experimental 
results and analyses of using pseudo-words in 
PB-SMT model in section 3. The conclusion is 
presented at section 4. 
2 Searching for Pseudo-words 
Pseudo-word searching problem is equal to de-
composition of a given sentence into pseudo-
words. We assume that the distribution of such 
decomposition is in the form of Gibbs distribu-
tion as below: 
)exp(
1
)|( ?= ySigXYP
where X denotes the sentence, Y denotes a de-
composition of X. Sig function acts as potential 
function on each multi-word yk, and ZX acts as 
partition function. Note that the number of yk is 
not fixed given X because X can be decomposed 
into various number of multi-words. 
Given X, ZX is fixed, so searching for optimal 
decomposition is as below: 
?==
k
y
YY kK
SigARGMAXXYPARGMAXY
1
)|(?   (2) 
where Y1K denotes K multi-word units from de-
composition of X. A multi-word sequence with 
maximal sum of Sig function values is the search 
target ? pseudo-word sequence. From (2) we 
can see that Sig function is vital for pseudo-word 
searching. In this paper Sig function calculates 
sequence significance which is proposed to char-
acterize pseudo-word as minimal sequence of 
consecutive words in sense of translation. The 
detail of sequence significance is described in the 
following section. 
2.1 Sequence Significance 
Two kinds of definitions of sequence signifi-
cance are proposed. One is monolingual se-
quence significance. X and Y are monolingual 
sentence and monolingual multi-words respec-
tively in this monolingual scenario. The other is 
bilingual sequence significance. X and Y are sen-
tence pair and multi-word pairs respectively in 
this bilingual scenario. 
2.1.1 Monolingual Sequence Significance 
Given a sentence w1, ?, wn, where wi denotes 
unary word, monolingual sequence significance 
is defined as: 
1,1
,
,
+?
=
ji
ji
ji Freq
Freq
Sig   (3) 
where Freqi, j (i?j) represents frequency of word 
sequence wi, ?, wj in the corpus, Sigi, j  repre-
sents monolingual sequence significance of a 
word sequence wi, ?, wj. We also denote word 
sequence wi, ?, wj as span[i, j], whole sentence 
as span[1, n]. Each span is also a multi-word ex-
pression. 
Monolingual sequence significance of span[i, j] 
is proportional to span[i, j]?s frequency, while is 
inversely proportion to frequency of expanded 
span (span[i-1, j+1]). Such definition character-
izes minimal sequence of consecutive words 
which we are looking for. Our target is to find 
pseudo-word sequence which has maximal sum 
of spans? significances: kX kZ
  (1) 
149
k (4) ? == Kk spanspanK K SigARGMAXpw 11 1
where pw denotes pseudo-word, K is equal to or 
less than sentence?s length. spank is the kth span 
of K spans span1K. Equation (4) is the rewrite of 
equation (2) in monolingual scenario. Searching 
for pseudo-words pw1K is the same to finding 
optimal segmentation of a sentence into K seg-
ments span1K (K is a variable too). Details of 
searching algorithm are described in section 
2.2.1. 
We firstly search for monolingual pseudo-
words on source and target side individually. 
Then we apply word alignment techniques to 
build pseudo-word alignments. We argue that 
word alignment techniques will work fine if non-
existent word alignments in such as non-
compositional phrasal equivalences have been 
filtered by pseudo-words. 
2.1.2 Bilingual Sequence Significance 
Bilingual sequence significance is proposed to 
characterize pseudo-word pairs. Co-occurrence 
of sequences on both language sides is used to 
define bilingual sequence significance. Given a 
bilingual sequence pair: span-pair[is, js, it, jt] 
(source side span[is, js] and target side span[it, jt]), 
bilingual sequence significance is defined as be-
low: 
1
k
,1,1,1
,,,
,,,
+?+?
=
ttss
ttss
ttss
jiji
jiji
jiji Freq
Freq
Sig   (5) 
where Freq denotes the frequency of a span-pair. 
Bilingual sequence significance is an extension 
of monolingual sequence significance. Its value 
is proportional to frequency of span-pair[is, js, it, 
jt], while is inversely proportional to frequency 
of expanded span-pair[is-1, js+1, it-1, jt+1]. 
Pseudo-word pairs of one sentence pair are such 
pairs that maximize the sum of span-pairs? bilin-
gual sequence significances: 
? = ??= Kk pairspanpairspanK K SigARGMAXpwp 11 1  (6) 
pwp represents pseudo-word pair. Equation (6) is 
the rewrite of equation (2) in bilingual scenario. 
Searching for pseudo-word pairs pwp1K is equal 
to bilingual segmentation of a sentence pair into 
optimal span-pair1K. Details of searching algo-
rithm are presented in section 2.2.2. 
2.2 Algorithms of Searching for Pseudo-
words 
Pseudo-word searching problem is equal to de-
composition of a sentence into pseudo-words. 
But the number of possible decompositions of 
the sentence grows exponentially with the sen-
tence length in both monolingual scenario and 
bilingual scenario. By casting such decomposi-
tion problem into parsing framework, we can 
find pseudo-word sequence in polynomial time. 
According to the two scenarios, searching for 
pseudo-words can be performed in a monolin-
gual way and a synchronous way. Details of the 
two kinds of searching algorithms are described 
in the following two sections. 
2.2.1 Algorithm of Searching for Monolin-
gual Pseudo-words (SMP) 
Searching for monolingual pseudo-words is 
based on the computation of monolingual se-
quence significance. Figure 1 presents the search 
algorithm. It is performed in a way similar to 
CKY (Cocke-Kasami-Younger) parser. 
 
Initialization: Wi, i = Sigi, i; 
Wi, j = 0,  (i?j); 
1:  for d = 2 ? n do 
2:      for all i, j s.t. j-i=d-1 do 
3:          for k = i ? j ? 1 do 
4:              v = Wi, k + Wk+1, j
5:              if v > Wi, j then 
6:                  Wi, j = v; 
7:          u = Sigi, j
8:          if u > Wi, j then 
9:              Wi, j = u; 
Figure 1. Algorithm of searching for monolingual 
pseudo-words (SMP). 
 
In this algorithm, Wi, j records maximal sum of 
monolingual sequence significances of sub spans 
of span[i, j]. During initialization, Wi, i is initial-
ized as Sigi,i (note that this sequence is word wi 
only). For all spans that have more than one 
word (i?j), Wi, j is initialized as zero. 
In the main algorithm, d represents span?s 
length, ranging from 2 to n, i represents start po-
sition of a span, j represents end position of a 
span, k represents decomposition position of 
span[i,j]. For span[i, j], Wi, j is updated if higher 
sum of monolingual sequence significances is 
found. 
The algorithm is performed in a bottom-up 
way. Small span?s computation is first. After 
maximal sum of significances is found in small 
spans, big span?s computation, which uses small 
spans? maximal sum, is continued. Maximal sum 
of significances for whole sentence (W1,n, n is 
sentence?s length)  is guaranteed in this way, and 
optimal decomposition is obtained correspond-
ingly. 
150
The method of fitting the decomposition prob-
lem into CKY parsing framework is located at 
steps 7-9. After steps 3-6, all possible decompo-
sitions of span[i, j] are explored and Wi, j of op-
timal decomposition of span[i, j] is recorded. 
Then monolingual sequence significance Sigi,j of 
span[i, j] is computed at step 7, and it is com-
pared to Wi, j at step 8. Update of Wi, j is taken at 
step 9 if Sigi,j is bigger than Wi, j, which indicates 
that span[i, j] is non-decomposable. Thus 
whether span[i, j] should be non-decomposable 
or not is decided through steps 7-9. 
2.2.2 Algorithm of Synchronous Searching 
for Pseudo-words (SSP) 
Synchronous searching for pseudo-words utilizes 
bilingual sequence significance. Figure 2 pre-
sents the search algorithm. It is similar to ITG 
(Wu, 1997), except that it has no production 
rules and non-terminal nodes of a synchronous 
grammar. What it cares about is the span-pairs 
that maximize the sum of bilingual sequence sig-
nificances. 
 
Initialization:  if is = js or it = jt then 
ttssttss
ttss
jijijiji SigW ,,,,,, = ; 
                       else 
0,,, =jijiW ; 
1:  for ds = 2 ? ns, dt = 2 ? nt do 
2:      for all  is, js, it, jt s.t. js-is=ds-1 and jt-it=dt-1 do
3:             for ks = is ? js ? 1, kt = it ? jt ? 1 do 
4:                    v = max{ ,
ttssttss jkjkkiki
WW ,1,,1,,, +++
ttsst
tjiji ,,,
tj,,,
tj,,,
jiji ,,,
tss kijkjkki
WW ,,,1,1,, ++ + } 
5:                    if v > W  then 
tss
6:                           W = v; 
tss iji
7:              u =  
ttss jiji
Sig ,,,
8:              if u > W  then 
tss iji
9:                    W = u; 
ttss
Figure 2. Algorithm of Synchronous Searching for 
Pseudo-words(SSP). 
 
In the algorithm, records maximal 
sum of bilingual sequence significances of sub 
span-pairs of span-pair[i
ttss jiji
W ,,,
s, js, it, jt]. For 1-to-m 
span-pairs, Ws are initialized as bilingual se-
quence significances of such span-pairs. For 
other span-pairs, Ws are initialized as zero. 
In the main algorithm, ds/dt denotes the length 
of a span on source/target side, ranging from 2 to 
ns/nt (source/target sentence?s length). is/it is the 
start position of a span-pair on source/target side, 
js/jt is the end position of a span-pair on 
source/target side, ks/kt is the decomposition po-
sition of a span-pair[is, js, it, jt] on source/target 
side. 
Update steps in Figure 2 are similar to that of 
Figure 1, except that the update is about span-
pairs, not monolingual spans. Reversed and non-
reversed alignments inside a span-pair are com-
pared at step 4. For span-pair[is, js, it, jt], 
 is updated at step 6 if higher sum of 
bilingual sequence significances is found. 
ttss jiji
W ,,,
Fitting the bilingually searching for pseudo-
words into ITG framework is located at steps 7-9. 
Steps 3-6 have explored all possible decomposi-
tions of span-pair[is, js, it, jt] and have recorded 
maximal 
ttss
 of these decompositions. Then 
bilingual sequence significance of span-pair[i
jijiW ,,,
s, js, 
it, jt] is computed at step 7. It is compared to 
ttss
 at step 8. Update is taken at step 9 if 
bilingual sequence significance of span-pair[i
jijiW ,,,
s, js, 
it, jt] is bigger than 
ttss
, which indicates that 
span-pair[i
jijiW ,,,
s, js, it, jt] is non-decomposable. 
Whether the span-pair[is, js, it, jt] should be non-
decomposable  or not is decided through steps 7-
9. 
In addition to the initialization step, all span-
pairs? bilingual sequence significances are com-
puted. Maximal sum of bilingual sequence sig-
nificances for one sentence pair is guaranteed 
through this bottom-up way, and the optimal de-
composition of the sentence pair is obtained cor-
respondingly. 
z Algorithm of Excluded Synchronous 
Searching for Pseudo-words (ESSP) 
The algorithm of SSP in Figure 2 explores all 
span-pairs, but it neglects NULL alignments, 
where words and ?empty? word are aligned. In 
fact, SSP requires that all parts of a sentence pair 
should be aligned. This requirement is too strong 
because NULL alignments are very common in 
many language pairs. In SSP, words that should 
be aligned to ?empty? word are programmed to 
be aligned to real words. 
Unlike most word alignment methods (Och 
and Ney, 2003) that add ?empty? word to ac-
count for NULL alignment entries, we propose a 
method to naturally exclude such NULL align-
ments. We call this method as Excluded Syn-
chronous Searching for Pseudo-words (ESSP). 
The main difference between ESSP and SSP is 
in steps 3-6 in Figure 3. We illustrate Figure 3?s 
span-pair configuration in Figure 4. 
151
 
Initialization:  if is = js or it = jt then 
ttssttss jijijiji ,,,,,,
,,, jijiW
SigW = ; 
                       else 
0=
ttss
; 
1:  for ds = 2 ? ns, dt = 2 ? nt do 
2:        for all  is, js, it, jt s.t. js-is=ds-1 and jt-it=dt-1 do 
3:              for ks1=is+1 ? js, ks2=ks1-1 ? js-1 
kt1=it+1 ? jt, kt2=kt1-1 ? jt-1 do 
4:                    v = max{W ,
ttssttss jkjkkiki
W ,1,,11,,1, 2211 ++?? +
1,,,1,1, 122 ?++ + ttsstt kijkjk W
tt j,,,
tj,,,
Sig
tt ji ,,,
ttss jiji ,,,
1, 1?ss kiW }
5:                    if v > W  then 
ss iji
6:                           W = v; 
tss iji
7:               u =  
ttss jiji ,,,
8:               if u > W  then 
ss ji
9:                    W = u; 
Figure 3. Algorithm of Excluded Synchronous 
Searching for Pseudo-words (ESSP). 
 
The solid boxes in Figure 4 represent excluded 
parts of span-pair[is, js, it, jt] in ESSP. Note that, 
in SSP, there is no excluded part, that is, ks1=ks2 
and kt1=kt2. 
We can see that in Figure 4, each monolingual 
span is configured into three parts, for example: 
span[is, ks1-1], span[ks1, ks2] and span[ks2+1, js] 
on source language side. ks1 and ks2 are two new 
variables gliding between is and js, span[ks1, ks2] 
is source side excluded part of span-pair[is, js, it, 
jt]. Bilingual sequence significance is computed 
only on pairs of blank boxes, solid boxes are ex-
cluded in this computation to represent NULL 
alignment cases. 
 
 
Figure 4. Illustration of excluded configuration. 
 
Note that, in Figure 4, solid box on either lan-
guage side can be void (i.e., length is zero) if 
there is no NULL alignment on its side. If all 
solid boxes are shrunk into void, algorithm of 
ESSP is the same to SSP. 
Generally, span length of NULL alignment is 
not very long, so we can set a length threshold 
for NULL alignments, eg. ks2-ks1?EL, where EL 
denotes Excluded Length threshold. Computa-
tional complexity of the ESSP remains the same 
to SSP?s complexity O(ns3.nt3), except multiply a 
constant EL2. 
There is one kind of NULL alignments that 
ESSP can not consider. Since we limit excluded 
parts in the middle of a span-pair, the algorithm 
will end without considering boundary parts of a 
sentence pair as NULL alignments. 
3 Experiments and Results 
In our experiments, pseudo-words are fed into 
PB-SMT pipeline. The pipeline uses GIZA++ 
model 4 (Brown et al, 1993; Och and Ney, 2003) 
for pseudo-word alignment, uses Moses (Koehn 
et al, 2007) as phrase-based decoder, uses the 
SRI Language Modeling Toolkit to train lan-
guage model with modified Kneser-Ney smooth-
ing (Kneser and Ney 1995; Chen and Goodman 
1998). Note that MERT (Och, 2003) is still on 
original words of target language. In our experi-
ments, pseudo-word length is limited to no more 
than six unary words on both sides of the lan-
guage pair. 
We conduct experiments on Chinese-to-
English machine translation. Two data sets are 
adopted, one is small corpus of IWSLT-2008 
BTEC task of spoken language translation in 
travel domain (Paul, 2008), the other is large 
corpus in news domain, which consists Hong 
Kong News (LDC2004T08), Sinorama Magazine 
(LDC2005T10), FBIS (LDC2003E14), Xinhua 
(LDC2002E18), Chinese News Translation 
(LDC2005T06), Chinese Treebank 
(LDC2003E07), Multiple Translation Chinese 
(LDC2004T07). Table 1 lists statistics of the 
corpus used in these experiments. 
is ks1 ks2 js
it kt1 kt2 jt
is ks1 ks2 js
it kt1 kt2 jt
a) non-reversed 
b) reversed 
 
small large  
Ch ? En Ch ? En 
Sent. 23k 1,239k 
word 190k 213k 31.7m 35.5m
ASL 8.3 9.2 25.6 28.6 
Table 1. Statistics of corpora, ?Ch? denotes Chinese, 
?En? denotes English, ?Sent.? row is the number of 
sentence pairs, ?word? row is the number of words, 
?ASL? denotes average sentence length. 
 
152
For small corpus, we use CSTAR03 as devel-
opment set, use IWSLT08 official test set for test. 
A 5-gram language model is trained on English 
side of parallel corpus. For large corpus, we use 
NIST02 as development set, use NIST03 as test 
set. Xinhua portion of the English Gigaword3 
corpus is used together with English side of large 
corpus to train a 4-gram language model. 
Experimental results are evaluated by case-
insensitive BLEU-4 (Papineni et al, 2001). 
Closest reference sentence length is used for 
brevity penalty. Additionally, NIST score (Dod-
dington, 2002) and METEOR (Banerjee and La-
vie, 2005) are also used to check the consistency 
of experimental results. Statistical significance in 
BLEU score differences was tested by paired 
bootstrap re-sampling (Koehn, 2004). 
3.1 Baseline Performance 
Our baseline system feeds word into PB-SMT 
pipeline. We use GIZA++ model 4 for word 
alignment, use Moses for phrase-based decoding. 
The setting of language model order for each 
corpus is not changed. Baseline performances on 
test sets of small corpus and large corpus are re-
ported in table 2. 
 
 small Large 
BLEU 0.4029 0.3146 
NIST 7.0419 8.8462 
METEOR 0.5785 0.5335 
Table 2. Baseline performances on test sets of small 
corpus and large corpus. 
3.2 Pseudo-word Unpacking 
Because pseudo-word is a kind of multi-word 
expression, it has inborn advantage of higher 
language model order and longer max phrase 
length over unary word. To see if such inborn 
advantage is the main contribution to the per-
formance or not, we unpack pseudo-word into 
words after GIZA++ aligning. Aligned pseudo-
words are unpacked into m?n word alignments. 
PB-SMT pipeline is executed thereafter. The ad-
vantage of longer max phrase length is removed 
during phrase extraction, and the advantage of 
higher order of language model is also removed 
during decoding since we use language model 
trained on unary words. Performances of pseudo-
word unpacking are reported in section 3.3.1 and 
3.4.1. Ma and Way (2009) used the unpacking 
after phrase extraction, then re-estimated phrase 
translation probability and lexical reordering 
model. The advantage of longer max phrase 
length is still used in their method. 
3.3 Pseudo-word Performances on Small 
Corpus 
Table 3 presents performances of SMP, SSP, 
ESSP on small data set. pwchpwen denotes that 
pseudo-words are on both language side of train-
ing data, and they are input strings during devel-
opment and testing, and translations are also 
pseudo-words, which will be converted to words 
as final output. wchpwen/pwchwen denotes that 
pseudo-words are adopted only on Eng-
lish/Chinese side of the data set. 
We can see from table 3 that, ESSP attains the 
best performance, while SSP attains the worst 
performance. This shows that excluding NULL 
alignments in synchronous searching for pseudo-
words is effective. SSP puts overly strong align-
ment constraints on parallel corpus, which im-
pacts performance dramatically. ESSP is superior 
to SMP indicating that bilingually motivated 
searching for pseudo-words is more effective. 
Both SMP and ESSP outperform baseline consis-
tently in BLEU, NIST and METEOR. 
There is a common phenomenon among SMP, 
SSP and ESSP. wchpwen always performs better 
than the other two cases. It seems that Chinese 
word prefers to have English pseudo-word 
equivalence which has more than or equal to one 
word. pwchpwen in ESSP performs similar to the 
baseline, which reflects that our direct pseudo-
word pairs do not work very well with GIZA++ 
alignments. Such disagreement is weakened by 
using pseudo-words on only one language side 
(wchpwen or pwchwen), while the advantage of 
pseudo-words is still leveraged in the alignments. 
Best ESSP (wchpwen) is significantly better 
than baseline (p<0.01) in BLEU score, best SMP 
(wchpwen) is significantly better than baseline 
(p<0.05) in BLEU score. This indicates that 
pseudo-words, through either monolingual 
searching or synchronous searching, are more 
effective than words as to being basic transla-
tional units. 
Figure 5 illustrates examples of pseudo-words 
of one Chinese-to-English sentence pair. Gold 
standard word alignments are shown at the bot-
tom of figure 5. We can see that ?front desk? is 
recognized as one pseudo-word in ESSP. Be-
cause SMP performs monolingually, it can not 
consider ???? and ?front desk? simultaneously. 
SMP only detects frequent monolingual multi-
words as pseudo-words. SSP has a strong con-
straint that all parts of a sentence pair should be 
aligned, so source sentence and target sentence 
have same length after merging words into 
153
 Table 3. Performance of using pseudo-words on small data. 
 
pseudo-words. We can see that too many pseudo-
words are detected by SSP. 
 
 
Figure 5. Outputs of the three algorithms ESSP, 
SMP and SSP on one sentence pair and gold standard 
word alignments. Words in one pseudo-word are con-
catenated by ?_?. 
 
3.3.1 Pseudo-word Unpacking Perform-
ances on Small Corpus 
We test pseudo-word unpacking in ESSP. Table 
4 presents its performances on small corpus. 
 
unpackingESSP 
pwchpwen wchpwen pwchwen
baseline
BLEU 0.4097 0.4182 0.4031 0.4029
NIST 7.5547 7.2893 7.2670 7.0419
METEOR 0.5951 0.5874 0.5846 0.5785
Table 4. Performances of pseudo-word unpacking on 
small corpus. 
 
We can see that pseudo-word unpacking sig-
nificantly outperforms baseline. wchpwen is sig-
nificantly better than baseline (p<0.04) in BLEU 
score. Unpacked pseudo-word performs com-
paratively with pseudo-word without unpacking. 
There is no statistical difference between them. It 
shows that the improvement derives from 
pseudo-word itself as basic translational unit, 
does not rely very much on higher language 
model order or longer max phrase length setting. 
3.4 Pseudo-word Performances on Large 
Corpus 
Table 5 lists the performance of using pseudo-
words on large corpus. We apply SMP on this 
task. ESSP is not applied because of its high 
computational complexity. Table 5 shows that all 
three configurations (pwchpwen, wchpwen, pwchwen) 
of SMP outperform the baseline. If we go back to 
the definition of sequence significance, we can 
see that it is a data-driven definition that utilizes 
corpus frequencies. Corpus scale has an influ-
ence on computation of sequence significance in 
long sentences which appear frequently in news 
domain. SMP benefits from large corpus, and 
wchpwen is significantly better than baseline 
(p<0.01). Similar to performances on small cor-
pus, wchpwen always performs better than the 
other two cases, which indicates that Chinese 
word prefers to have English pseudo-word 
equivalence which has more than or equal to one 
word. 
 
SMP  
pwchpwen wchpwen pwchwen
baseline
BLEU 0.3185 0.3230 0.3166 0.3146
NIST 8.9216 9.0447 8.9210 8.8462
METEOR 0.5402 0.5489 0.5435 0.5335
Table 5. Performance of using pseudo-words on large 
corpus. 
3.4.1 Pseudo-word Unpacking Perform-
ances on Large Corpus 
Table 6 presents pseudo-word unpacking per-
formances on large corpus. All three configura-
tions improve performance over baseline after 
pseudo-word unpacking. pwchpwen attains the 
best BLEU among the three configurations, and 
is significantly better than baseline (p<0.03). 
wchpwen is also significantly better than baseline 
(p<0.04). By comparing table 6 with table 5, we 
can see that unpacked pseudo-word performs 
comparatively with pseudo-word without un-
packing. There is no statistical difference be-
SMP SSP ESSP  
pwchpwen wchpwen pwchwen pwchpwen wchpwen pwchwen pwchpwen wchpwen pwchwen
baseline
BLEU 0.3996 0.4155 0.4024 0.3184 0.3661 0.3552 0.3998 0.4229 0.4147 0.4029
NIST 7.4711 7.6452 7.6186 6.4099 6.9284 6.8012 7.1665 7.4373 7.4235 7.0419
METEOR 0.5900 0.6008 0.6000 0.5255 0.5569 0.5454 0.5739 0.5963 0.5891 0.5785
??  ?  ??  ?  ?  ??  ? 
The guy at the front desk is pretty rude . 
??  ?  ??  ?  ?  ??  ? 
The guy_at the front_desk is pretty_rude . 
??  ?  ??  ?  ?  ??  ? 
The guy at the front_desk is pretty rude . 
ESSP
??   ?    ??    ?    ?     ??    ? 
 
 
The guy at the front desk is pretty rude  .
Gold standard word alignments 
SMP
SSP 
154
tween them. It shows that the improvement de-
rives from pseudo-word itself as basic transla-
tional unit, does not rely very much on higher 
language model order or longer max phrase 
length setting. In fact, slight improvement in 
pwchpwen and pwchwen is seen after pseudo-word 
unpacking, which indicates that higher language 
model order and longer max phrase length im-
pact the performance in these two configurations. 
 
UnpackingSMP 
pwchpwen wchpwen pwchwen
Baseline
BLEU 0.3219 0.3192 0.3187 0.3146 
NIST 8.9458 8.9325 8.9801 8.8462 
METEOR 0.5429 0.5424 0.5411 0.5335 
Table 6. Performance of pseudo-word unpacking on 
large corpus. 
3.5 Comparison to English Chunking 
English chunking is experimented to compare 
with pseudo-word. We use FlexCRFs (Xuan-
Hieu Phan et al, 2005) to get English chunks. 
Since there is no standard Chinese chunking data 
and code, only English chunking is executed. 
The experimental results show that English 
chunking performs far below baseline, usually 8 
absolute BLEU points below. It shows that sim-
ple chunks are not suitable for being basic trans-
lational units. 
4 Conclusion 
We have presented pseudo-word as a novel ma-
chine translational unit for phrase-based machine 
translation. It is proposed to replace too fine-
grained word as basic translational unit. Pseudo-
word is a kind of basic multi-word expression 
that characterizes minimal sequence of consecu-
tive words in sense of translation. By casting 
pseudo-word searching problem into a parsing 
framework, we search for pseudo-words in poly-
nomial time. Experimental results of Chinese-to-
English translation task show that, in phrase-
based machine translation model, pseudo-word 
performs significantly better than word in both 
spoken language translation domain and news 
domain. Removing the power of higher order 
language model and longer max phrase length, 
which are inherent in pseudo-words, shows that 
pseudo-words still improve translational per-
formance significantly over unary words. 
References  
S. Banerjee, and A. Lavie. 2005. METEOR: An 
automatic metric for MT evaluation with im-
proved correlation with human judgments. In 
Proceedings of the ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization (ACL?05). 65?72. 
P. Blunsom, T. Cohn, C. Dyer, M. Osborne. 2009. A 
Gibbs Sampler for Phrasal Synchronous 
Grammar Induction. In Proceedings of ACL-
IJCNLP, Singapore. 
P. Blunsom, T. Cohn, M. Osborne. 2008. Bayesian 
synchronous grammar induction. In Proceed-
ings of NIPS 21, Vancouver, Canada. 
P. Brown, S. Della Pietra, V. Della Pietra, and R. 
Mercer. 1993. The mathematics of machine 
translation: Parameter estimation. Computa-
tional Linguistics, 19:263?312. 
P.-C. Chang, M. Galley, and C. D. Manning. 2008. 
Optimizing Chinese word segmentation for 
machine translation performance. In Proceed-
ings of the 3rd Workshop on Statistical Machine 
Translation (SMT?08). 224?232. 
Chen, Stanley F. and Joshua Goodman. 1998. An 
empirical study of smoothing techniques for 
language modeling. Technical Report TR-10-98, 
Harvard University Center for Research in Com-
puting Technology. 
C. Cherry, D. Lin. 2007. Inversion transduction 
grammar for joint phrasal translation model-
ing. In Proc. of the HLTNAACL Workshop on 
Syntax and Structure in Statistical Translation 
(SSST 2007), Rochester, USA. 
D. Chiang. 2007. Hierarchical phrase-based 
translation.Computational Linguistics, 33(2):201?
228. 
Y. Deng and W. Byrne. 2005. HMM word and 
phrase alignment for statistical machine trans-
lation. In Proc. of HLT-EMNLP, pages 169?176. 
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram cooc-
currence statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nology (HLT?02). 138?145. 
Kneser, Reinhard and Hermann Ney. 1995. Improved 
backing-off for M-gram language modeling. In 
Proceedings of the IEEE International Conference 
on Acoustics, Speech, and Signal Processing, 
pages 181?184, Detroit, MI. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan,W. Shen, C. 
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, 
E. Herbst. 2007. Moses: Open source toolkit for 
statistical machine translation. In Proc. of the 
155
45th Annual Meeting of the ACL (ACL-2007), 
Prague. 
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical 
phrasebased translation. In Proc. of the 3rd In-
ternational conference on Human Language Tech-
nology Research and 4th Annual Meeting of the 
NAACL (HLT-NAACL 2003), 81?88, Edmonton, 
Canada. 
P. Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proceed-
ings of EMNLP. 
P. Lambert and R. Banchs. 2005. Data Inferred 
Multi-word Expressions for Statistical Ma-
chine Translation. In Proceedings of MT Summit 
X. 
Y. Ma, N. Stroppa, and A. Way. 2007. Bootstrap-
ping word alignment via word packing. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL?07). 
304?311. 
Y. Ma, and A. Way. 2009. Bilingually Motivated 
Word Segmentation for Statistical Machine 
Translation. In ACM Transactions on Asian Lan-
guage Information Processing, 8(2). 
D. Marcu,W.Wong. 2002. A phrase-based, joint 
probability model for statistical machine 
translation. In Proc. of the 2002 Conference on 
Empirical Methods in Natural Language Process-
ing (EMNLP-2002), 133?139, Philadelphia. Asso-
ciation for Computational Linguistics. 
F. J. Och. 2003. Minimum error rate training in 
statistical machine translation. In Proc. of ACL, 
pages 160?167. 
F. J. Och and H. Ney. 2003. A systematic compari-
son of various statistical alignment models. 
Computational Linguistics, 29(1):19?51. 
Xuan-Hieu Phan, Le-Minh Nguyen, and Cam-Tu 
Nguyen. 2005. FlexCRFs: Flexible Conditional 
Random Field Toolkit, http://flexcrfs.sourceforge. 
net 
K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu: 
a method for automatic evaluation of machine 
translation, 2001. 
M. Paul, 2008. Overview of the IWSLT 2008 
evaluation campaign. In Proc. of Internationa 
Workshop on Spoken Language Translation, 20-21 
October 2008. 
A. Stolcke. (2002). SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of 
ICSLP, Denver, Colorado. 
D. Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel 
corpora. Computational Linguistics, 23(3):377?
403. 
J. Xu, Zens., and H. Ney. 2004. Do we need Chi-
nese word segmentation for statistical ma-
chine translation? In Proceedings of the ACL 
Workshop on Chinese Language Processing 
SIGHAN?04). 122?128. 
J. Xu, J. Gao, K. Toutanova, and H. Ney. 2008. 
Bayesian semi-supervised chinese word seg-
mentation for statistical machine translation. 
In Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING?08). 
1017?1024. 
H. Zhang, C. Quirk, R. C. Moore, D. Gildea. 2008. 
Bayesian learning of non-compositional 
phrases with synchronous parsing. In Proc. of 
the 46th Annual Conference of the Association for 
Computational Linguistics: Human Language 
Technologies (ACL-08:HLT), 97?105, Columbus, 
Ohio. 
R. Zhang, K. Yasuda, and E. Sumita. 2008. Improved 
statistical machine translation by multiple 
Chinese word segmentation. In Proceedings of 
the 3rd Workshop on Statistical Machine Transla-
tion (SMT?08). 216?223. 
 
156
