Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 28?36,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Millstream Systems ? a Formal Model for
Linking Language Modules by Interfaces
Suna Bensch
Department of Computing Science,
Umea? University, Umea?, Sweden
suna@cs.umu.se
Frank Drewes
Department of Computing Science,
Umea? University, Umea?, Sweden
drewes@cs.umu.se
Abstract
We introduce Millstream systems, a for-
mal model consisting of modules and an
interface, where the modules formalise
different aspects of language, and the in-
terface links these aspects with each other.
1 Credits
This work is partially supported by the project
Tree Automata in Computational Language Tech-
nology within the Sweden ? South Africa Re-
search Links Programme. A preliminary but more
detailed version of this article is available as a
technical report (Bensch and Drewes, 2009).
2 Introduction
Modern linguistic theories (Sadock, 1991; Jack-
endoff, 2002) promote the view that different as-
pects of language, such as phonology, morphol-
ogy, syntax, and semantics should be viewed as
autonomous modules that work simultaneously
and are linked with each other by interfaces that
describe their interaction and interdependency.
Formalisms in modern computational linguistics
which establish interfaces between different as-
pects of language are the Combinatory Categorical
Grammar (CCG), the Functional Generative De-
scription (FGD), the Head-Driven Phrase Struc-
ture Grammar (HPSG), the Lexical Functional
Grammar (LFG), and the Extensible Dependency
Grammar (XDG).1 Here, we propose Millstream
systems, an approach from a formal language the-
oretic point of view which is based on the same
ideas as XDG, but uses tree-generating modules
of arbitrary kinds.
Let us explain in slightly more detail what a
Millstream system looks like. A Millstream sys-
tem contains any number of tree generators, called
1See, e.g., (Dalrymple, 2001; Sgall et al, 1986; Pollard
and Sag, 1994; Steedman, 2000; Debusmann, 2006; Debus-
mann and Smolka, 2006).
its modules. Such a tree generator is any device
that specifies a tree language. For example, a tree
generator may be a context-free grammar, tree ad-
joining grammar, a finite-state tree automaton, a
dependency grammar, a corpus, human input, etc.
Even within a single Millstream system, the mod-
ules need not be of the same kind, since they are
treated as ?black boxes?. The Millstream system
links the trees generated by the modules by an in-
terface consisting of logical formulas.
Suppose that a Millstream system has k mod-
ules. Then the interface consists of interface rules
in the form of logical expressions that establish
links between the (nodes of the) trees t1, . . . , tk
that are generated by the individual modules.
Thus, a valid combination of trees is not just any
collection of trees t1, . . . , tk generated by the k
modules. It also includes, between these struc-
tures, interconnecting links that represent their
relationships and that must follow the rules ex-
pressed by the interface. Grammaticality, in terms
of a Millstream system, means that the individ-
ual structures must be valid (i.e., generated by the
modules) and are linked in such a way that all in-
terface rules are logically satisfied. A Millstream
system can thus be considered to perform indepen-
dent concurrent derivations of autonomous mod-
ules, enriched by an interface that establishes links
between the outputs of the modules, thus con-
straining the acceptable configurations.
Millstream systems may, for example, be of in-
terest for natural language understanding and nat-
ural language generation. Simply put, the task
of natural language understanding is to construct
a suitable semantic representation of a sentence
that has been heard (phonology) and parsed (syn-
tax). Within the framework of Millstream sys-
tems this corresponds to the problem where we are
given a syntactic tree (and possibly a phonologi-
cal tree if such a module is involved) and the goal
is to construct an appropriate semantic tree. Con-
28
versely, natural language generation can be seen as
the problem to construct an appropriate syntactic
(and/or phonological) tree from a given semantic
tree. In abstract terms, the situations just described
are identical. We refer to the problem as the
completion problem. While the current paper is
mainly devoted to the introduction and motivation
of Millstream systems, in (Bensch et al, 2010) the
completion problem is investigated for so-called
regular MSO Millstream systems, i.e. systems in
which the modules are regular tree grammars (or,
equivalently, finite tree automata) and the interface
conditions are expressed in monadic second-order
(MSO) logic. In Section 7, the results obtained so
far are briefly summarised.
Now, let us roughly compare Millstream sys-
tems with XDG. Conceptually, the k modules of a
Millstream system correspond to the k dimensions
of an XDG. In an XDG, a configuration consists
of dependency structures t1, . . . , tk. The interface
of a Millstream system corresponds to the princi-
ples of the XDG. The latter are logical formulas
that express conditions that the collection of de-
pendency structures must fulfill.
The major difference between the two for-
malisms lies in the fact that XDG inherently builds
upon dependency structures, whereas the modules
of a Millstream system are arbitrary tree genera-
tors. In XDG, each of t1, . . . , tk is a dependency
analysis of the sentence considered. In particu-
lar, they share the yield and the set of nodes (as
the nodes of a dependency tree correspond to the
words in the sentence analysed, and its yield is that
sentence). Millstream systems do not make simi-
lar assumptions, which means that they may give
rise to new questions and possibilities:
? The purpose of a Millstream system is not
necessarily the analysis of sentences. For ex-
ample, a Millstream system with two mod-
ules could translate one language into an-
other. For this, tree grammars representing
the source and target languages could be used
as modules, with an interface expressing that
t2 is a correct translation of t1. This sce-
nario makes no sense in the context of XDG,
because the sentences represented by t1 and
t2 differ. Many similar applications of Mill-
stream system may the thought of, for exam-
ple correction or simplification of sentences.
? As the modules may be arbitrary devices
specifying tree languages, they contribute
their own generative power and theoretical
properties to the whole (in contrast to XDG,
which does not have such a separation). This
makes it possible to apply known results from
tree language theory, and to study the inter-
play between different kinds of modules and
interface logics.
? The fact that the individual modules of a
Millstream system may belong to different
classes of tree generators could be linguis-
tically valuable. For example, a Millstream
system combining a dependency grammar
module with a regular tree grammar module,
could be able to formalise aspects of a given
natural language that cannot be formalised by
using only one of these formalisms.
? For Millstream systems whose modules are
generative grammar formalisms (such as reg-
ular tree grammars, tree-adjoining grammars
and context-free tree grammars), it will be in-
teresting to study conditions under which the
Millstream system as a whole becomes gen-
erative, in the sense that well-formed config-
urations can be constructed in a step-by-step
manner based on the derivation relations of
the individual modules.
Let us finally mention another, somewhat sub-
tle difference between XDG and Millstream sys-
tems. In XDG, the interfaces are dimensions
on their own. For example, an XDG captur-
ing the English syntax and semantics would have
three dimensions, namely syntax, semantics, and
the syntax-semantics interface. An analysis of a
sentence would thus consist of three dependency
trees, where the third one represents the relation
between the other two. In contrast, a correspond-
ing Millstream system would only have two mod-
ules. The interface between them is considered
to be conceptually different and establishes direct
links between the trees that are generated by the
two modules. One of our tasks (which is, however,
outside the scope of this contribution) is a study of
the formal relation between XDG and Millstream
systems, to achieve a proper understanding of their
similarities and differences.
The rest of the paper is organised as follows.
In the next section, we discuss an example illus-
trating the linguistic notions and ideas that Mill-
stream systems attempt to provide a formal basis
for. After some mathematical preliminaries, which
29
are collected in Section 4, the formal definition of
Millstream systems is presented in Section 5. Sec-
tion 6 contains examples and remarks related to
Formal Language Theory. Finally, Section 7 dis-
cusses preliminary results and future work.
3 Linguistical Background
In this section, we discuss an example, roughly
following (Jackendoff, 2002), that illustrates the
linguistic ideas that have motivated our approach.
Figure 1 shows the phonological, syntactical and
semantical structure, depicted as trees (a), (b) and
(c), respectively of the sentence Mary likes Peter.
Trees are defined formally in the next section, for
the time being we assume the reader to be familiar
with the general notion of a tree as used in linguis-
tics and computer science.
s
w1
mE@ri
w2
w3
laIk
cl4
s
w5
pit@r
Morphophonology
Segmental structure
(a)
S
NP1 VP
V2
V3 infl4
NP5
(b) s
pres situation4
likestate3
maryagent1 peterpatient5
(c)
Figure 1: Phonological, syntactical and semantical
structure of Mary likes Peter.
The segmental structure in the phonological
tree (a) is the basic pronunciation of the sentence
Mary likes Peter, where each symbol represents
a speech sound. This string of speech sound sym-
bols is structured into phonological words by mor-
phophonolgy. The morphophonological structure
in our example consists of the three full phono-
logical words mE@ri, laIk, pit@r and of the clitic s.
The clitic is attached to the adjacent phonologi-
cal word, thus forming a larger phonological con-
stituent. The syntactical tree (b) depicts the syn-
tactical constituents. The sentence S is divided
into a noun phrase NP and a verb phrase VP.
The verb phrase VP is divided into an inflected
verb V and a noun phrase NP. The inflected verb
consists of its uninflected form and its inflection,
which refers, in our example, to the grammatical
features present tense and third person singular.
The semantical tree (c) depicts the semantical con-
stituents. In our example, like is a function of type
state and takes two arguments, namely mary and
peter which are of type agent and patient .
The structure of Mary likes Peter is not just the
sum of its phonological, syntactical and semanti-
cal structures. It also includes the relationships be-
tween certain constituents in these tree structures.
To illustrate these relationships we use indices in
Figure 1. The sole role of the indices here is to
express the linguistic relationships among coin-
dexed constituents. The indices do not occur in the
formalisation, where they are replaced by logical
links relating the nodes that, in the figure, carry the
same indices.2 The morphophonological wordw1,
for example, is linked with the noun phrase NP1
in the syntactical tree and with the conceptual con-
stituent maryagent1 in the semantical tree. This il-
lustrates that w1, NP1, and maryagent1 are the cor-
responding morphophonological, syntactical and
semantical representations of Mary, respectively.
But there are also correspondences that concern
only the phonological and syntactical trees, ex-
cluding the semantical tree. For example, the in-
flected word V2 in the syntactical structure corre-
sponds to the phonological word w2, but has no
link to the semantical structure whatsoever.
4 Preliminaries
The set of non-negative integers is denoted by N,
and N+ = N \ {0}. For k ? N, we let [k] =
{1, . . . , k}. For a set S, the set of all nonempty
finite sequences (or strings) over S is denoted by
S+; if the empty sequence  is included, we write
S?. As usual, A1?? ? ??Ak denotes the Cartesian
product of sets A1, . . . , Ak. The transitive and re-
flexive closure of a binary relation? ? A?A on
a set A is denoted by ??. A ranked alphabet is
a finite set ? of pairs (f, k), where f is a symbol
and k ? N is its rank. We denote (f, k) by f (k), or
simply by f if k is understood or of lesser impor-
tance. Further, we let ?(k) = {f (n) ? ? | n = k}.
We define trees over ? in one of the standard ways,
by identifying the nodes of a tree t with sequences
of natural numbers. Intuitively, such a sequence
2The reader is referred to (Bensch and Drewes, 2009) for
the proper formalisation of the example in terms of a Mill-
stream system.
30
shows that path from the root of the tree to the
node in question. In particular, the root is the
empty sequence .
Formally, the set T? of trees over ? consists of
all mappings t : V (t) ? ? (called trees) with the
following properties:
? The set V (t) of nodes of t is a finite and non-
empty prefix-closed subset of N?+. Thus, for
every node vi ? V (t) (where i ? N+), its
parent v is in V (t) as well.
? For every node v ? V (t), if t(v) = f (k), then
{i ? N | vi ? V (t)} = [k]. In other words,
the children of v are v1, . . . , vk.
Let t ? T? be a tree. The root of t is the node
. For every node v ? V (t), the subtree of t
rooted at v is denoted by t/v. It is defined by
V (t/v) = {v? ? N? | vv? ? V (t)} and, for all
v? ? V (t/v), (t/v)(v?) = t(vv?). We shall de-
note a tree t as f [t1, . . . , tk] if t() = f (k) and
t/i = ti for i ? [k]. In the special case where
k = 0 (i.e., V (t) = {}), the brackets may be
omitted, thus denoting t as f . For a set S of trees,
the set of all trees of the form f [t1, . . . , tk] such
that f (k) ? ? and t1, . . . , tk ? S is denoted by
?(S). For a tuple T ? Tk?, we let V (T ) denote
the set {(i, v) | i ? [k] and v ? V (ti)}. Thus,
V (T ) is the disjoint union of the sets V (ti). Fur-
thermore, we let V (T, i) denote the ith component
of this disjoint union, i.e., V (T, i) = {i} ? V (ti)
for all i ? [k]. A tree language is a subset of T?,
for a ranked alphabet ?, and a ?-tree generator (or
simply tree generator) is any sort of formal device
G that determines a tree language L(G) ? T?. A
typical sort of tree generator, which we will use in
our examples, is the regular tree grammar.
Definition 1 (regular tree grammar). A regular
tree grammar is a tuple G = (N,?, R, S) con-
sisting of disjoint ranked alphabets N and ? of
nonterminals and terminals, where N = N (0), a
finite set R of rules A ? r, where A ? N and
r ? T??N , and an initial nonterminal S ? N .
Given trees t, t? ? T??N , there is a derivation
step t ? t? if t? is obtained from t by replacing
a single occurrence of a nonterminal A with r,
where A ? r is a rule in R. The regular tree
language generated by G is
L(G) = {t ? T? | S
?
? t}.
It is well known that a string language L is
context-free if and only if there is a regular tree
language L?, such that L = yield(L?). Here,
yield(L?) = {yield(t) | t ? L?} denotes the set of
all yields of trees in L?, the yield yield(t) of a tree
t being the string obtained by reading its leaves
from left to right.
5 Millstream Systems
Throughout the rest of this paper, let ? denote any
type of predicate logic that allows us to make use
of n-ary predicates symbols. We indicate the ar-
ity of predicate symbols in the same way as the
rank of symbols in ranked alphabets, i.e., by writ-
ing P (n) if P is a predicate symbol of arity n. The
set of all well-formed formulas in ? without free
variables (i.e., the set of sentences of ?) is denoted
by F?. If S is a set, we say that a predicate symbol
P (n) is S-typed if it comes with an associated type
(s1, . . . , sn) ? Sn. We write P : s1 ? ? ? ? ? sn to
specify the type of P . Recall that an n-ary predi-
cate ? on D is a function ? : Dn ? {true, false}.
Alternatively, ? can be viewed as a subset of Dn,
namely the set of all (d1, . . . , dn) ? Dn such that
?(d1, . . . , dn) = true. We use these views in-
terchangeably, selecting whichever is more conve-
nient. Given a (finite) set P of predicate symbols,
a logical structure ?D; (?P )P?P? consists of a set
D called the domain and, for each P (n) ? P , a
predicate ?P ? Dn. If an existing structure Z
is enriched with additional predicates (?P )P?P ?
(where P?P ? = ?), we denote the resulting struc-
ture by ?Z; (?P )P?P ??. In this paper, we will only
consider structures with finite domains. To repre-
sent (tuples of) trees as logical structures, consider
a ranked alphabet ?, and let r be the maximum
rank of symbols in ?. A tuple T = (t1, . . . , tk) ?
Tk? will be represented by the structure
|T | = ?V (T ); (Vi)i?[k], (labg)g??, (?i)i?[r]?
consisting of the domain V (T ) and the predicates
V (1)i (i ? [k]), lab(1)g (g ? ?) and ?(2)i (i ? [r]).
The predicates are given as follows:
? For every i ? [k], Vi = V (T, i). Thus, Vi(d)
expresses that d is a node in ti (or, to be pre-
cise, that d represents a node of ti in the dis-
joint union V (T )).
? For every g ? ?, labg = {(i, v) ? V (T ) |
i ? [k] and ti(v) = g}. Thus, labg(d) ex-
presses that the label of d is g.
? For every j ? [r], ?j = {((i, v), (i, vj)) |
i ? [k] and v, vj ? V (ti)}. Thus, ?j(d, d?)
31
expresses that d? is the jth child of d in one
of the trees t1, . . . , tk. In the following, we
write d ?j d? instead of ?j(d, d?).
Note that, in the definition of |T |, we have
blurred the distinction between predicate symbols
and their interpretation as predicates, because this
interpretation is fixed. Especially in intuitive ex-
planations, we shall sometimes also identify the
logical structure |T | with the tuple T it represents.
To define Millstream systems, we start by for-
malising our notion of interfaces. The idea is that
a tuple T = (t1, . . . , tk) of trees, represented by
the structure |T |, is augmented with additional in-
terface links that are subject to logical conditions.
An interface may contain finitely many different
kinds of interface links. Formally, the collection
of all interface links of a given kind is viewed as
a logical predicate. The names of the predicates
are called interface symbols. Each interface sym-
bol is given a type that indicates which trees it is
intended to link with each other.
For example, if we want to make use of ternary
links called TIE, each linking a node of t1 with
a node of t3 and a node of t4, we use the in-
terface symbol TIE : 1 ? 3 ? 4. This interface
symbol would then be interpreted as a predicate
?TIE ? V (T, 1)? V (T, 3)? V (T, 4). Each triple
in ?TIE would thus be an interface link of type TIE
that links a node in V (t1) with a node in V (t3) and
a node in V (t4).
Definition 2 (interface). Let ? be a ranked al-
phabet. An interface on Tk? (k ? N) is a pair
INT = (I,?), such that
? I is a finite set of [k]-typed predicate symbols
called interface symbols, and
? ? is a finite set of formulas in F? that may,
in addition to the fixed vocabulary of ?, con-
tain the predicate symbols in I and those of
the structures |T | (where T ? Tk?). These
formulas are called interface conditions.
A configuration (w.r.t. INT ) is a structure C =
?|T |; (?I)I?I?, such that
? T ? Tk?,
? ?I ? V (T, i1)? ? ? ? ? V (T, il) for every in-
terface symbol I : i1 ? ? ? ? ? il in I, and
? C satisfies the interface conditions in ? (if
each symbol I ? I is interpreted as ?I ).
Note that several interfaces can always be com-
bined into one by just taking the union of their sets
of interface symbols and interface conditions.
Definition 3 (Millstream system). Let ? be a
ranked alphabet and k ? N. A Millstream sys-
tem (MS, for short) is a system of the form MS =
(M1, . . . ,Mk; INT ) consisting of ?-tree genera-
tors M1, . . . ,Mk, called the modules of MS , and
an interface INT on Tk?. The language L(MS )
generated by MS is the set of all configurations
?|T |; (?I)I?I? such that T ? L(M1) ? ? ? ? ?
L(Mk).
Sometimes we consider only some of the trees
in these tuples. For this, if MS is as above and
1 ? i1 < ? ? ? < il ? k, we define the notation
LMi1?????Mil (MS ) = {(ti1 , . . . , til) |
?|(t1, . . . , tk); (?I)I?I |? ? L(MS )}.
The reader should note that, intentionally, Mill-
stream systems are not a priori ?generative?. Even
less so, they are ?derivational? by nature. This is
because there is no predefined notion of derivation
that allows us to create configurations by means
of a stepwise (though typically nondeterministic)
procedure. In fact, there cannot be one, unless we
make specific assumptions regarding the way in
which the modules work, but also regarding the
logic ? and the form of the interface conditions
that may be used. Similarly, as mentioned in the
introduction, there is no predefined order of im-
portance or priority among the modules.
6 Examples and Remarks Related to
Formal Language Theory
The purpose of this section is to indicate, by
means of examples and easy observations, that
Millstream systems are not only linguistically well
motivated, but also worth studying from the point
of view of computer science, most notably regard-
ing their algorithmic and language-theoretic prop-
erties. While this kind of study is beyond the scope
of the current article, part of our future research on
Millstream systems will be devoted to such ques-
tions.
Example 1. Let ? be ordinary first-order logic
with equality, and consider the Millstream system
MS over ? = {?(2), a(0), b(0), c(0), d(0)} which
consists of two identical modules M1 = M2 that
simply generate T? (e.g., using the regular tree
grammar with the single nonterminal S and the
32
rules3 S ? ?[S, S] | a | b | c | d) and a sin-
gle interface symbol BIJ : 1 ? 2 with the interface
conditions
?x : lab{a,b,c,d}(x)?
?y : BIJ(x, y) ? BIJ(y, x),
?x, y, z : (BIJ(x, y) ? BIJ(x, z)?
BIJ(y, x) ? BIJ(z, x))? y = z,
?x, y : BIJ(x, y)?
?
z?{a,b,c,d}
(labz(x) ? labz(y)).
The first interface condition expresses that all
and only the leaves of both trees are linked. The
second expresses that no leaf is linked with two or
more leaves. In effect, this amounts to saying that
BIJ is a bijection between the leaves of the two
trees. The third interface condition expresses that
this bijection is label preserving. Altogether, this
amounts to saying that the yields of the two trees
are permutations of each other; see Figure 2.
?
?
b ?
c d
?
a c
?
d ?
?
c b
?
c a
bij
bij
bij
bij
bij
Figure 2: An element of L(MS ) in Example 1.
Now, let us replace the modules by slightly
more interesting ones. For a string w over {A,B,
a, b, c, d}, let w denote any tree over {?(2), A(0),
B(0), a(0), b(0), c(0), d(0)} with yield(w) = w.
(For example, we may choose w to be the left
comb whose leaf symbols are given by w.) Let the
Millstream system MS ? be defined as MS , but us-
ing the modules M ?1 = ({A,B,C,D},?, R1, A)
and M ?2 = ({A,B},?, R2, A) with the following
rules:
R?1 = {A? aA | aB, B ? bB | bC,
C ? cC | cD, D ? dD | d},
R?2 = {A? acA | acB, B ? bdB | bd}.
Thus, M ?1 and M ?2 are the ?standard? grammars
(written as regular tree grammars) that generate
the regular languages {akblcmdn | k, l,m, n ?
3As usual, A ? r | r? stands for A ? r, A ? r?.
1} and {(ac)m(bd)n | m,n ? 1}. The inter-
face makes sure that LM ?1?M ?2(MS ?) contains only
those pairs of trees t1, t2 in which yield(t1) is a
permutation of yield(t2). As a consequence, it
follows that yield(LM ?1(MS )) = {ambncmdn |
m,n ? 1}.
The next example discusses how top-down tree
transductions can be implemented as Millstream
systems.
Example 2 (top-down tree transduction). Recall
that a tree transduction is a binary relation ? ?
T? ? T?? , where ? and ?? are ranked alpha-
bets. The set of trees that a tree t ? T? is trans-
formed into is given by ?(t) = {t? ? T?? |
(t, t?) ? ?}. Obviously, every Millstream system
of the form MS = (M1,M2; INT ) defines a tree
transduction, namely LM1?M2(MS ). Let us con-
sider a very simple instance of a deterministic top-
down tree transduction ? (see, e.g., (Ge?cseg and
Steinby, 1997; Fu?lo?p and Vogler, 1998; Comon et
al., 2007) for definitions and references regarding
top-down tree transductions), where ? = ?? =
{f (2), g(2), a(0)}. We transform a tree t ? T? into
the tree obtained from t by interchanging the sub-
trees of all top-most fs (i.e., of all nodes that are
labelled with f and do not have an ancestor that
is labelled with f as well) and turning the f at
hand into a g. To accomplish this, a top-down
tree transducer would use two states, say SWAP
and COPY, to traverse the input tree from the top
down, starting in state SWAP. Whenever an f is
reached in this state, its subtrees are interchanged
and the traversal continues in parallel on each of
the subtrees in state COPY. The only purpose of
this state is to copy the input to the output without
changing it. Formally, this would be expressed by
the following term rewrite rules, viewing the states
as symbols of rank 1:
SWAP[f [x1, x2]] ? g[COPY[x2], COPY[x1]],
COPY[f [x1, x2]] ? f [COPY[x1], COPY[x2]],
SWAP[g[x1, x2]] ? g[SWAP[x1], SWAP[x2]],
COPY[g[x1, x2]] ? g[COPY[x1], COPY[x2]],
SWAP[a] ? a,
COPY[a] ? a.
(We hope that these rules are intuitive enough to
be understood even by readers who are unfamiliar
with top-down tree transducers, as giving the for-
mal definition of top-down tree transducers would
be out of the scope of this article.) We mimic
the behaviour of the top-down tree transducer us-
33
ing a Millstream system with interface symbols
SWAP : 1 ? 2 and COPY : 1 ? 2. Since the mod-
ules simply generate T?, they are not explicitly
discussed. The idea behind the interface is that an
interface link labelled q ? {SWAP, COPY} links a
node v in the input tree with a node v? in the output
tree if the simulated computation of the tree trans-
ducer reaches v in state q, resulting in node v? in
the output tree. First, we specify that the initial
state is SWAP, which simply means that the roots
of the two trees are linked by a SWAP link:
?x, y : root1(x) ? root2(y)? SWAP(x, y),
where root i is defined as root i(x) ? Vi(x) ?
@y : y ?1 x. It expresses that x is the root of
tree i. The next interface condition corresponds to
the first rule of the simulated top-down tree trans-
ducer:
?x, y, x1, x2 : SWAP(x, y)?labf (x)? x ?1 x1?
x ?2 x2 ? labg(y)??y1, y2 : y ?1 y1?y ?2 y2?
COPY(x1, y2) ? COPY(x2, y1).
In a similar way, the remaining rules are turned
into interface conditions, e.g.,
?x, y, x1, x2 : COPY(x, y)?labf (x)? x ?1 x1?
x ?2 x2 ? labf (y)??y1, y2 : y ?1 y1?y ?2 y2?
COPY(x1, y1) ? COPY(x2, y2).
The reader should easily be able to figure out
the remaining interface conditions required.
One of the elements of L(MS ) is shown in Fig-
ure 3. It should not be difficult to see that, indeed,
LM1?M2(MS ) = ? .
g
f
a f
a a
a
g
g
f
a a
a
a
swap
swap
copy
copy
copy copy
swap
Figure 3: An element of L(MS ) in Example 2.
Extending the previous example, one can eas-
ily see that all top-down and bottom-up tree trans-
ductions can be turned into Millstream systems
in a way similar to the construction above. A
similar remark holds for many other types of tree
transductions known from the literature. Most no-
tably, monadic second-order definable tree trans-
ductions (Engelfriet and Maneth, 1999; Engelfriet
and Hoogeboom, 2001; Engelfriet and Maneth,
2003) can be expressed as Millstream systems.
Since the mentioned types of tree transductions
are well studied, and much is known about their
algorithmic properties, future research on Mill-
stream systems should investigate the relation-
ship between different types of tree transductions
and Millstream systems in detail. In particular,
it should be tried to formulate requirements on
the interface conditions that can be used to ob-
tain characterisations of various classes of tree
transductions. We note here that results of this
type would not only be interesting from a purely
mathematical point of view, since tree transduc-
ers have turned out to be a valuable tool in, for
example, machine translation (Knight and Graehl,
2005; May and Knight, 2006; Graehl et al, 2008).
7 Preliminary Results and Future Work
Millstream systems, as introduced in this article,
are formal devices that allow to model situations
in which several tree-generating modules are inter-
connected by logical interfaces. In a forthcoming
paper (Bensch et al, 2010), we investigate the the-
oretical properties of regular MSO Millstream sys-
tems, i.e., Millstream systems in which the mod-
ules are regular tree grammars and the logic used
is monadic second-order logic. In particular, we
study the so-called completion problem. Given
a Millstream system with k modules and l ? k
known trees ti1 , . . . , til (1 ? i1 < ? ? ? < il ? k),
the task is to find a completion, i.e., a configura-
tion whose ij th tree is tij for all j ? [l]. Thus, if
viewed as a pure decision problem, the completion
problem corresponds to the membership problem
for LMi1?????Mil (MS ). To be useful in applica-
tions, algorithms solving the completion problem
should, of course, be required to explicitly con-
struct a completion rather than just answering yes.
Let us briefly summarize the results of (Bensch
et al, 2010).
1. In general, the completion problem is unde-
cidable for k ? l ? 2 even in the case where
only the use of first-order logic is permitted.
This can be shown by reducing Post?s corre-
spondence problem (PCP) to the emptiness
problem for a regular FO Millstream system
with k = 2. The Millstream system con-
structed is somewhat similar to the one in Ex-
ample 1, as it establishes bijective correspon-
dences between the nodes of two trees (that
34
represent the two parts of a solution to a PCP
instance).
2. If there are no direct links between unknown
trees (i.e., |{j1, . . . , jm} \ {i1, . . . , il}| ? 1
for each interface symbol I : j1 ? ? ? ? ? jm),
then the completion problem is solvable for
all regular MSO Millstream systems.
3. Applying some well-known results, the com-
pletion problem is solvable for all regular
MSO Millstream systems for which L(MS )
is of bounded tree width. Thus, it is of inter-
est to establish conditions that guarantee the
configurations in L(MS ) to be of bounded
tree width. Two such conditions, are given
in (Bensch et al, 2010). Roughly speaking,
they require that the links respect the struc-
ture of the trees. Let us informally describe
one of them, called nestedness. Say that a
link I ?(u1, . . . , um) is directly below a link
I(v1, . . . , vl) if there are i, j such that uj is
a descendant of vi and none of the nodes in
between carries a link. Now, fix a constant
h. A configuration is nested if the roots are
linked with each other and the following hold
for every link ? = I(v1, . . . , vl):
(a) There are at most h links I ?(u1, . . . , um)
directly below ?.
(b) Each of the nodes uj in (a) is a descen-
dant of one of the nodes vi.
As mentioned above, L(MS ) is of bounded
tree width if its configurations are nested
(with respect to the same constant h).
Nestedness, and also the second sufficient con-
dition for bounded tree width studied in (Bensch
et al, 2010) restrict the configurations themselves.
While such conditions may be appropriate in many
practical cases (where one knows what the config-
urations look like), future research should also at-
tempt to find out whether it is possible to put some
easily testable requirements on the interface con-
ditions in order to force the configurations to be
of bounded tree width. Note that, since the prop-
erty of being of tree width at most d is expressible
in monadic second-order logic, one can always ar-
tificially force the configurations of a given MSO
Millstream system to be of bounded tree width, but
this is not very useful as it would simply exclude
those configurations whose tree width is greater
than the desired constant d, thus changing the se-
mantics of the given Millstream system in a usu-
ally undesired manner.
Future work should also investigate properties
that make it possible to obtain or complete config-
urations in a generative way. For example, for reg-
ular MSO Millstream systems with interface con-
ditions of a suitable type, it should be possible to
generate the configurations in L(MS ) by generat-
ing the k trees in a parallel top-down manner, at
the same time establishing the interface links. Re-
sults of this kind could also be used for solving
the completion problem in an efficient manner. In
general, it is clear that efficiency must be an im-
portant aspect of future theoretical investigations
into Millstream systems.
In addition to theoretical results, a good imple-
mentation of Millstream systems is needed in or-
der to make it possible to implement nontrivial ex-
amples. While this work should, to the extent pos-
sible, be application independent, it will also be
necessary to seriously attempt to formalise and im-
plement linguistic theories as Millstream systems.
This includes exploring various such theories with
respect to their appropriateness.
To gain further insight into the usefulness and
limitations of Millstream systems for Computa-
tional Linguistics, future work should elaborate if
and how it is possible to translate formalisms such
as HPSG, LFG, CCG, FDG and XDG into Mill-
stream systems.
Acknowledgments
We thank Dot and Danie van der Walt for pro-
viding us with a calm and relaxed atmosphere at
Millstream Guest House in Stellenbosch (South
Africa), where the first ideas around Millstream
systems were born in April 2009. Scientifically,
we would like to thank Henrik Bjo?rklund, Stephen
J. Hegner, and Brink van der Merwe for discus-
sions and constructive input. Furthermore, we
would like to thank one of the referees for valu-
able comments.
References
Suna Bensch and Frank Drewes. 2009. Mill-
stream systems. Report UMINF 09.21,
Umea? University. Available at http:
//www8.cs.umu.se/research/uminf/
index.cgi?year=2009&number=21.
35
Suna Bensch, Henrik Bjo?rklund, and Frank Drewes.
2010. Algorithmic properties of Millstream sys-
tems. In Sheng Yu, editor, Proc. 14th Intl. Conf.
on Developments in Language Theory (DLT 2010),
Lecture Notes in Computer Science. To appear.
Hubert Comon, Max Dauchet, Re?mi Gilleron, Flo-
rent Jacquemard, Christof Lo?ding, Denis Lugiez,
Sophie Tison, and Marc Tommasi. 2007. Tree
Automata Techniques and Applications. Internet
publication available at http://tata.gforge.
inria.fr. Release October 2007.
Mary Dalrymple. 2001. Lexical Functional Gram-
mar, volume 34 of Syntax and Semantics. Academic
Press.
Ralph Debusmann and Gert Smolka. 2006. Multi-
dimensional dependency grammar as multigraph de-
scription. In Proceedings of FLAIRS Conference,
pages 740?745.
Ralph Debusmann. 2006. Extensible Dependency
Grammar: A Modular Grammar Formalism Based
On Multigraph Description. Ph.D. thesis, Univer-
sita?t des Saarlandes. Available at http://www.
ps.uni-sb.de/
?
rade/papers/diss.pdf.
Joost Engelfriet and Henrik Jan Hoogeboom. 2001.
MSO definable string transductions and two-way
finite-state transducers. ACM Transactions on Com-
putational Logic, 2:216?254.
Joost Engelfriet and Sebastian Maneth. 1999. Macro
tree transducers, attribute grammars, and MSO de-
finable tree translations. Information and Computa-
tion, 154:34?91.
Joost Engelfriet and Sebastian Maneth. 2003. Macro
tree translations of linear size increase are MSO de-
finable. SIAM Journal on Computing, 32:950?1006.
Zolta?n Fu?lo?p and Heiko Vogler. 1998. Syntax-Directed
Semantics: Formal Models Based on Tree Transduc-
ers. Springer.
Ferenc Ge?cseg and Magnus Steinby. 1997. Tree lan-
guages. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages. Vol. 3: Beyond
Words, chapter 1, pages 1?68. Springer.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427.
Ray Jackendoff. 2002. Foundations of Language:
Brain, Meaning, Grammar, Evolution. Oxford Uni-
versity Press, Oxford.
Kevin Knight and Jonathan Graehl. 2005. An
overview of probabilistic tree transducers for natural
language processing. In Alexander F. Gelbukh, edi-
tor, Proc. 6th Intl. Conf. on Computational Linguis-
tics and Intelligent Text Processing (CICLing 2005),
volume 3406 of Lecture Notes in Computer Science,
pages 1?24. Springer.
Jonathan May and Kevin Knight. 2006. Tiburon:
A weighted tree automata toolkit. In Oscar H.
Ibarra and Hsu-Chun Yen, editors, Proc. 11th Intl.
Conf. on Implementation and Application of Au-
tomata (CIAA 2006), volume 4094 of Lecture Notes
in Computer Science, pages 102?113. Springer.
Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase
Structure Grammar. Chicago University Press.
Jerrold Sadock. 1991. Autolexical Syntax - A Theory
of Parallel Grammatical Representations. The Uni-
versity of Chicago Press, Chicago & London.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The meaning of the sentence in its semantic and
pragmatic aspects. Reidel, Dordrecht.
Mark Steedman. 2000. The Syntactic Process (Lan-
guage, Speech, and Communication). MIT Press.
36
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 21?29,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
On the Parameterized Complexity of
Linear Context-Free Rewriting Systems
Martin Berglund
Umea? University, Sweden
mbe@cs.umu.se
Henrik Bjo?rklund
Umea? University, Sweden
henrikb@cs.umu.se
Frank Drewes
Umea? University, Sweden
drewes@cs.umu.se
Abstract
We study the complexity of uniform mem-
bership for Linear Context-Free Rewriting
Systems, i.e., the problem where we are
given a string w and a grammar G and are
asked whether w ? L(G). In particular,
we use parameterized complexity theory
to investigate how the complexity depends
on various parameters. While we focus
primarily on rank and fan-out, derivation
length is also considered.
1 Introduction
Linear Context-Free Rewriting Systems (LCFRS)
were introduced by Vijay-Shanker et al (1987)
with the purpose of capturing the syntax of nat-
ural language.1 It is one of several suggested ways
of capturing Joshi?s concept of mildly context-
sensitive languages (Joshi, 1985). As such, it
strengthens the expressive power of context-free
grammars, while avoiding the full computational
complexity of context-sensitive grammars.
One of the defining features of mildly context-
sensitive languages is that they should be decid-
able in polynomial time. This is indeed true for ev-
ery language that can be generated by an LCFRS.
Unlike the case for context-free grammars, how-
ever, the universal or uniform membership prob-
lem for LCFRSs, where both the grammar and
the string in question are considered as input, is
known to be PSPACE-complete (Kaji et al, 1992),
making a polynomial time solution very improba-
ble.
The best known algorithms for the problem
have a running time of O(|G| ? |w|f ?(r+1)), where
G is the grammar, w is the string, f is the fan-out
and r is the rank of the grammar (Seki et al, 1991;
Burden and Ljunglo?f, 2005; Boullier, 2004). (For
1Seki et al (1991) independently suggested the nearly
identical Multiple Context-Free Grammars.
a definition of fan-out and rank, see Section 2.)
Unlike the rank of a context-free grammar, the fan-
out and rank of an LCFRS cannot in general be re-
duced to some fixed constant. Increasing the fan-
out always gives more generative power, as does
increasing the rank while keeping the fan-out fixed
(Satta, 1998). The rank can be reduced to two, but
at the price of an exponential increase in the fan-
out.
Research into algorithms for LCFRS parsing
that are efficient enough for practical use is quite
active. For example, algorithms for restricted
cases are being studied, e.g., by Go?mez-Rodr??guez
et al (2010), as well as rank reduction, primarily
in special cases, where the fan-out is not affected;
see, e.g., Sagot and Satta (2010).
This article is a first step towards a finer com-
putational complexity analysis of the membership
problem for LCFRSs. Specifically it asks the
question ?could there exist an algorithm for the
uniform LCFRS membership problem whose run-
ning time is a fixed polynomial in |w| times an ar-
bitrary function in f and r?? By employing pa-
rameterized complexity theory, we show that such
an algorithm is very unlikely to be found. Fix-
ing the rank of the grammar to one, the mem-
bership problem, parameterized by the fan-out, is
W[SAT]-hard. Fixing the fan-out to two and tak-
ing the rank as the parameter, the problem is W[1]-
hard. Finally, if the fan-out, rank, and derivation
length are included in the parameter, the problem
is W[1]-complete. These results help guide future
work, suggesting other types of parameters and
grammar restrictions that may yield more favor-
able complexity results.
2 Preliminaries
For n ? N, we write [n] for {1, . . . , n} and [n]0
for {0} ? [n]. Given an alphabet ? we write ??
for all strings over ? and ?+ for all non-empty
strings. The empty string is denoted by ?.
21
2.1 Linear context-free rewriting systems
Let ? be an alphabet, x1, . . . , xn variables, and
w1, . . . , wk strings over ? such that
w1 ? ? ?wk = ?0 ? xpi(1) ? ?1 ? ? ?xpi(n) ? ?n
for some permutation pi and some strings
?0, . . . , ?n ? ??. Then define f as a function
over tuples of strings such that
f((x1, . . .), . . . , (. . . , xn)) = (w1, . . . , wk).
A function is linear regular if and only if it
can be described in this way. For example
f((x1), (x2)) = (a, bx2x1c) is linear regular, and
f((aaa), (bc)) = (a, bbcaaac).
Definition 2.1. A Linear Context-Free Rewriting
System is a tupleG = (N,?, F,R, S), whereN is
an alphabet of nonterminals, where each A ? N
has an associated fan-out #(A); S ? N is the
initial nonterminal with #(S) = 1; ? is an al-
phabet of terminals; F is a set of linear regu-
lar functions; and R is a set of rules of the form
A ? g(B1, . . . , Bn), where A,B1, . . . , Bn ? N
and g is a function in F of type
(??)#(B1) ? ? ? ? ? (??)#(Bn) ? (??)#(A).
For rules A ? g(), where g has arity 0 and
g() = (?1, . . . , ?#(A)), we often simply write
A? (?1, . . . , ?#(A)).
The rank of a rule is the number of nontermi-
nals on the right-hand side. The rank of G is the
maximal rank of any rule in R. The fan-out of G
is the maximal fan-out of any nonterminal in N .
The language generated by a nonterminal A is a
set of n-tuples, where n = #(A).
Definition 2.2. Let G = (N,?, F,R, S) be
a linear context-free rewriting system. Let
LA ? (??)#(A) denote the tuples that a nontermi-
nal A ? N can generate. This is the smallest set
such that if A ? f(B1, . . . , Bn) is in R then, for
all bi ? LBi where i ? [n], f(b1, . . . , bn) ? LA.
The language of G is L(G) = LS .
For i ? N, we write i-LCFRS for the class of
all LCFRSs of rank at most i and LCFRS(i) for
the class of all LCFRSs of fan-out at most i. We
also write i-LCFRS(j) for i-LCFRS?LCFRS(j).
2.2 Parameterized complexity theory
We only reproduce the most central definitions of
parameterized complexity theory. For a more thor-
ough treatment, we refer the reader to (Downey
and Fellows, 1999; Flum and Grohe, 2006).
A parameterized problem is a language
L ? ?? ? N, where ? is a finite alphabet. The
second component is called the parameter. An al-
gorithm for L is fixed-parameter tractable if there
is a computable function f and a polynomial p
such that for every (x, k) ? ???N, the algorithm
decides in time f(k) ? p(|x|) whether (x, k) ? L.
The problem of deciding L is fixed-parameter
tractable if there is such an algorithm. If so, L
belongs to the class FPT.
A parameterized problem L ? ?? ? N
is fpt-reducible to another parameterized prob-
lem K ? ?? ? N if there is a mapping
R : ?? ? N? ?? ? N such that
1. for all (x, k) ? ?? ? N, we have (x, k) ? L
if and only if R(x, k) ? K,
2. there is a computable function f and a poly-
nomial p such that R(x, k) can be computed
in time f(k) ? p(|x|), and
3. there is a computable function g such that for
every (x, k) ? ?? ? N, if R(x, k) = (y, k?),
then k? ? g(k).
Note that several parameters may be combined
into one by taking their maximum (or sum).
The most commonly used hierarchy of parame-
terized complexity classes is the following.
FPT ?W[1] ?W[2] ? ? ? ? ?
?W[SAT] ?W[P] ? XP
The classes W[1],. . . ,W[P] are defined using cir-
cuits or, alternatively, logic. None of the inclu-
sions is known to be strict, except that FPT is a
strict subclass of XP. It is widely believed, how-
ever, that each of them is strict. The class XP is
the class of all parameterized problems for which
there is a computable function f such that every
instance (x, k) can be decided in time |x|f(k).
2.3 Problems of interest
We know from Kaji et al (1992) that the universal
membership problem for 1-LCFRSs is PSPACE-
complete. Satta (1992) has further shown that
LCFRS(2)-MEMBERSHIP is NP-hard.
We study the following decision problems,
where the symbol P is used to indicate what the
parameter is:
? P-LCFRS(j)-MEMBERSHIP, where j ? N
is the membership problem for LCFRS(j),
parameterized by the rank.
22
? i-LCFRS(P)-MEMBERSHIP, where i ? N
is the membership problem for i-LCFRS, pa-
rameterized by the fan-out.
? P-LCFRS(P)-MEMBERSHIP is the mem-
bership problem for LCFRS parameterized
by the rank and the fan-out.
? SHORT P-LCFRS(P)-DERIVATION is the
membership problem for LCFRS parameter-
ized by the rank, the fan-out, and the deriva-
tion length.
Since there are algorithms that solve the member-
ship problem for LCFRSs with rank r and fan-out
t and string w in time |w|(r+1)t (see, e.g., (Seki
et al, 1991; Burden and Ljunglo?f, 2005; Boul-
lier, 2004)), we can immediately conclude that
P-LCFRS(P)-MEMBERSHIP, as well as every
other parameterized membership problem men-
tioned above, belongs to XP.
3 Fixed rank grammars
The following theorem establishes a lower bound
for 1-LCFRSs parameterized by the fan-out.
Theorem 3.1. 1-LCFRS(P)-MEMBERSHIP is
W[SAT]-hard.
The proof of Theorem 3.1 is by reduction from
WEIGHTED MONOTONE SATISFIABILITY. Be-
fore we get into the actual proof, we discuss some
properties of this problem.
Definition 3.1. A monotone Boolean formula is a
Boolean formula that contains only conjunctions,
disjunctions, and variables. In particular, there are
no negations. An instance of WEIGHTED MONO-
TONE SATISFIABILITY is a pair (?, k), where ?
is a monotone Boolean formula and k ? N. The
question is whether ? has a satisfying assignment
of weight k, i.e., an assignment that sets exactly
k of the variables that occur in ? to true. The pa-
rameter is k. WEIGHTED MONOTONE SATISFIA-
BILITY is W[SAT]-complete (Abrahamson et al,
1993; Abrahamson et al, 1995; Downey and Fel-
lows, 1999).
We can view a monotone Boolean formula ?
as an unranked tree, where the root node corre-
sponds to the top level clause and the leaves corre-
spond to bottom level clauses, i.e., variable occur-
rences. The set pos(?) of positions of ? is defined
as usual, consisting of strings of natural numbers
that indicate how to navigate to the clauses in a
tree representation of ?. We denote each subclause
of ? by Cs, where s ? pos(?) is its position. Thus
? = (((x1 ? (x2 ? x3)) ? x3 ? (x3 ? x4))?
?x2 ? ((x1 ? (x2 ? x4)) ? (x1 ? x3)))
C?
C1
C11
C111
C112
C1121
C1122C12
C13
C131
C132
C2
C3
C31
C311
C312
C3121
C3122
C32
C321
C322
Figure 1: A formula ? and its tree representa-
tion. Conjunctive clauses are round and disjunc-
tive rectangular. For example, C111 is the leftmost
occurrence of x1 and C13 the clause (x3 ? x4).
C? denotes the whole of ?, while, e.g., Cijl is the
lth clause of the jth clause of the ith clause of ?.
See Figure 1 for an example. We use C for the set
of all clauses of ? and C?, C?, and CVar for the
sets of conjunctive, disjunctive, and bottom level
clauses, respectively. For all c ? CVar let Var(c)
denote the variable in c, and let Var(?) denote the
set of all variables in ?.
Given a monotone Boolean formula ? and a
variable assignment ? : Var(?) ? B, we de-
fine a verification tour for ? and ?. Such a tour
moves through the tree representation of ?, start-
ing at the root node, and verifies that ? satis-
fies ?. To this end, we first define the function
Next : pos(?) ? pos(?) ? {True} as follows.
For the root clause let Next(?) = True . For all
si ? pos(?), where s ? N? and i ? N, if Cs ? C?
and s(i + 1) ? pos(?) let Next(si) = s(i + 1),
otherwise let Next(si) = Next(s).
A verification tour over ?, given a variable as-
signment ? is constructed by the following proce-
dure. Set the initial position p = ?, then
? If Cp ? C? set p ? p1 (i.e., go to the first
subclause).
? If Cp ? C? set p ? pi for any i (i.e. non-
deterministically pick a subclause).
? IfCp ? CVar verify that ?(Var(Cp)) = true .
If so, set p ? Next(p) and repeat. Other-
wise, the verification tour fails.
A verification tour succeeds if it reaches True .
23
The following lemma can be proved by straight-
forward induction on the structure of ?.
Lemma 3.2. If a verification tour for ? and vari-
able assignment ? succeeds, then ? satisfies ?.
We are now ready to prove Theorem 3.1.
Proof of Theorem 3.1. Let (?, k) be an instance of
WEIGHTED MONOTONE SATISFIABILITY. Let
{x1, . . . , xn} be the variables that appear in ?. In
particular, n is the number of distinct variables.
Let m be the number of bottom level clauses.
Intuitively, the LCFRS we will construct will
guess a weight k variable assignment ? and then
simulate a verification tour for ? and ?.
Basically, we will use one nonterminal per
clause and use the structure of the grammar to sim-
ulate a verification tour. In order to verify that the
necessary bottom level clauses can all be satisfied
through the same k true variables, we will use the
input string to be parsed. The string w will con-
sist of bracketed sequences of m copies of each of
the n variables, i.e., w = [xm1 ] ? ? ? [x
m
n ]. To un-
derstand the construction of the grammar, please
keep in mind that the only derivations that matter
are those generating this particular input string.
The grammar will guess which k variables
should be set to true and disregard the other vari-
ables. Technically, this is done by first letting a
nonterminal F generate a tuple of k + 1 strings
s0, . . . , sk such that each si consists of zero or
more of the bracketed sequences of variables to
be disregarded. The rest of the grammar generates
exactly k bracketed sequences that will be inter-
leaved with s0, . . . , sk. During the generation of
these k bracketed sequences it is nondeterministi-
cally verified that the corresponding truth assign-
ment satisfies ?.
We use the following set of nonterminals:
{S, F} ? {Cs | s ? pos(?) ? {True}}
For S, there is only one rule: S ? fS(F ). The
function fS places brackets around the k vari-
ables that are guessed to be true, represented by
the strings t1, . . . , tk, and interleaves them with
the remaining variables, represented by the strings
s0, . . . , sk:
fS(s0, . . . , sk, t1, . . . , tk) = (s0[t1]s1 ? ? ? [tk]sk)
The nonterminal F has rules F ? fF,i,j(F ) for
all i ? [n] and j ? [k]0. These rules produce the
bracketed sequences of copies of the variables xi
to be disregarded, as can be seen from the corre-
sponding function:
fF,i,j(s0, . . . , sk, t1, . . . , tk) =
(s0, . . . , sj [x
m
i ], . . . , sk, t1, . . . , tk)
Moreover, there is a single rule
F ? fF (C?)
with
fF (t1, . . . , tk) = (?, . . . , ?, t1, . . . , tk)
The rules for the nonterminals that represent
clauses differ according to the type of the clause,
i.e., if the nonterminal represents a conjunctive
clause, a disjunctive clause, or a variable. For each
conjunctive clause Cs there is exactly one rule,
representing a move to its first subclause. Here,
fid is the identity function.
Cs ? fid (Cs1)
For every disjunctive clause Cs and every i such
that Csi is a subclause of Cs there is one rule.
Cs ? fid (Csi)
For every bottom level clause, i.e., Cs ? CVar ,
every i ? [k] and every j ? [m] there is one rule.
Cs ? fs,i,j(CNext(s))
Intuitively, such a rule corresponds to producing j
copies of the variable of clause Cs in component i
of the tuple and moving on to the next clause that
should be visited in a verification tour. This can be
seen from the corresponding function.
fs,i,j(t1, . . . , tk) = (t1, . . . ,Var(Cs)
jti, . . . , tk)
The reason that the function produces j copies of
the variable, rather than just one, is that it is un-
known beforehand how many times a bottom level
clause that represents that particular variable will
be visited. Thus the number of copies to be pro-
duced has to be guessed nondeterministically in
order to make sure that a total of m copies of each
variable set to true are eventually produced.
If there is a weight k satisfying assignment,
there will also be a verification tour that even-
tually reaches True when Next is called (by
Lemma 3.2). The single rule forCTrue simply pro-
duces a k-tuple of empty strings.
24
The reduction is polynomial and the fan-out of
the resulting grammar is 2k+1. Thus it is an FPT-
reduction. It remains to argue that the grammar
can produce w if and only if ? has a satisfying
assignment of weight k.
We first note that whatever tuple is derived from
F , the first k + 1 entries in the tuple consist of
bracketed sequences of the form [xml ]. If the
grammar can produce w, it follows that the tuple
(t1, . . . , tk) produced from C? must be such that
each ti equalsm copies of the same variable name.
Any successful derivation of a string by the
grammar corresponds to a verification tour of ?
and the variable assignment that sets the variables
that appear in (t1, . . . , tk) to true and all other vari-
ables to false. Thus ? has a satisfying assignment
of weight k.
For the other direction, assume that ? has a sat-
isfying assignment of weight k. Then the grammar
can guess this assignment and a corresponding
successful verification tour, thus producingw.
Note that Theorem 3.1 can easily be strength-
ened to grammars with a binary terminal alphabet.
It is enough to represent each variable name by
a bitstring of length dlog2(m)e in the above re-
duction. We also note that Theorem 3.1 immedi-
ately implies that P-LCFRS(P)-MEMBERSHIP
is W[SAT]-hard.
4 Fixed fan-out grammars
We next turn to the case where the fan-out is fixed
to two, while the rank is treated as a parameter.
Theorem 4.1. P-LCFRS(2)-MEMBERSHIP is
W[1]-hard.
Proof. We reduce from k-CLIQUE, the problem of
deciding whether a given graph has a clique of size
k, with k as the parameter. This problem is known
to be W[1]-complete (Flum and Grohe, 2006). Let
G = (V,E) be an undirected graph. We assume,
without loss of generality, that V = {1, . . . , n}
and that an edge connecting nodes i, j ? V is rep-
resented as the ordered pair (i, j) such that i < j,
i.e., E ? {(i, j) ? V ? V | i < j}. To find out
whether G has a clique of size k we construct an
instance of the membership problem for LCFRSs.
The input alphabet is ? = {0, 1}. Construct the
input string as
w = 0n10n10n1 ? ? ? 10n? ?? ?
(3k + 2)(k ? 1)/2 ones
.
The nonterminals are N = {A,E,C, S}, with S
being the initial nonterminal. The rules are the fol-
lowing.
{A? 0i | i ? {1, . . . , n}}.
{E ? 0n?i10n?j | (i, j) ? E}.
{C ? (0i, 0n?i10i) | i ? {1, . . . , n}}.
Handling S is a bit more complex. Let
? = k(k?1)/2, the number of edges in a k-clique.
Then the unique rule for S is:
S ? f(E, . . . , E
? ?? ?
?
, C, . . . , C
? ?? ?
2?
, A, . . . , A
? ?? ?
2k
).
Now we need to define f . Consider the following
application of f .
f(e1, . . . , e?, (c1, c?1), . . . , (c?, c??),
(d1, d?1), . . . , (d?, d??), a1, . . . , a2k).
The application above evaluates to the string
c1e1d11c2e2d21 ? ? ?
? ? ? 1c?e?d?1a1?1a21a3?2a41s1a2k?1?ka2k.
The substrings ?1 through ?k are left to be de-
fined, and will contain all the c? and d? arguments
in a careful configuration derived from the struc-
ture of a clique. Let (pi1, pi?1), . . . , (pi?, pi
?
?) be the
lexicographically sorted sequence of edges in a
k-clique with nodes numbered 1 through k. For
example, (pi1, pi?1) = (1, 2), (pi2, pi
?
2) = (1, 3),
(pik, pi?k) = (2, 3), and (pi?, pi
?
?) = (k ? 1, k).
Then, for each l, find the longest subsequences
i1, . . . , ip and j1, . . . , jq of 1, . . . , ? for which
pii1 = ? ? ? = piip = l and pi
?
j1 = ? ? ? = pijq = l, and
let ?l = c?i1 ? ? ? c?ip d?j1 ? ? ? d?jq .
This construction is simpler than it may at first
appear. Basically, the clique is found by generat-
ing k(k? 1)/2 copies of E, each of which will be
placed so that it has no choice but to generate an
edge in a k-clique. Looking at the first part of the
string, each 1cleldl1 must generate a string of the
form 10n10n1: el will generate some 0n?i10n?j ,
were (i, j) is an edge in G, which forces cl to gen-
erate 0i and dl to generate 0j . The trick is that cl
and dl yield the first string in a pair generated by
an instance of C. The other string in the pair de-
scribes the same number as the first, but in such
a way that it can be carefully placed in the lat-
ter part of the derivation string, thus forcing other
instances of the C nonterminal to pick the same
25
node (number of zeros) to generate. These are
then placed in such a way that the edges picked by
the instances of E all belong to the same clique.
For example, for k = 3 the result of f will
be c1e1d11c2e2d21c3e3d31a1c1c2a21c3d11d2d3,
where the latter part ensures that c1 and c2 have
to pick the same node (lowest-numbered node in
the clique), as do c3 and d1, and d2 and d3.
5 Short derivations
In this section, we consider the length of deriva-
tions as an additional parameter. As usual, the
length of a derivation is the number of derivation
steps it consists of. (In a derivation of an LCFRS
(N,?, F,R, S), this is the same as the number of
applications of functions in F .)
Let G = (N,?, F,R, S) be an LCFRS in the
following. Consider the following problem:
Definition 5.1. An instance of the SHORT P-
LCFRS(P) DERIVATION problem consists of a
LCFRS G, some w ? ?? and a constant d ? N.
The question asked is: can w be derived by G in
at most d steps? The parameter is k = d + r + f
where r is the maximum rank and f the maximum
fanout.
Lemma 5.1. SHORT P-LCFRS(P) DERIVA-
TION is W[1]-hard.
Proof. The W[1]-hardness of the problem follows
immediately from the reduction in the proof of
Theorem 4.1, since k-Clique is reduced to an in-
stance of LCFRS membership with O(k2) deriva-
tion steps, rank O(k2), and fixed fan-out.
We next demonstrate that SHORT P-
LCFRS(P) DERIVATION is in W[1] (and is
therefore W[1]-complete) by reducing to SHORT
CONTEXT-SENSITIVE DERIVATION, shown to
be W[1]-complete by Downey et al (1994).
Let H = (NH ,?H , RH , SH) be an arbitrary
context-sensitive grammar in the following. A
context-sensitive grammar has nonterminals,
terminals and a starting nonterminal just like a
LCFRS, but the rules are of the form ? ? ? for
strings ?, ? ? (?H ?NH)? where 0 < |?| ? |?|.
A derivation starts with the string SH . A string
w ? ? ? w? can be turned into w ? ? ? w? in one
derivation step if (?, ?) ? RH .
Definition 5.2. An instance of the SHORT
CONTEXT-SENSITIVE DERIVATION problem
consists of a context-sensitive grammar H , a
string w ? ??H , and a constant dH ? N. The
question is: can w be derived by H in at most dH
steps? The parameter is dH .
We are now ready to prove membership in W[1]
by a FPT-reduction from (G,w, d) to (H,w, dH).
Lemma 5.2. The SHORT P-LCFRS(P)
DERIVATION problem is in W[1].
Proof. We can restrict ourselves to the case where
no nonterminal appears twice in a right-hand side
of any rule in G. This is because, e.g., a rule
of the form A ? f(B,B) can be turned into
A ? f(B,B?), using a fresh copy B? of B that
has the same rules asB (except for having the left-
hand side B? rather than B). Note that this modifi-
cation does not affect the parameter, and increases
the size of the grammar only polynomially.
The complete reduction is somewhat lengthy,
but the core intuition is very simple. The string
is kept the same, and a context-sensitive gram-
mar H is constructed such that L(H) = L(G).
H simulates G by maintaining a string serializa-
tion of the current ?configuration? of G, walking
through the whole string rewriting the appropriate
non-terminal for every rule application in G. A
configuration of G can be viewed in this way,
aa ? b ? ? ? b ? ba
A B A
where the derivation has, so far, generated some
terminal symbols (the lower-case letters), two in-
stances of the non-terminal A and one instance of
B. The configuration keeps track of where the
symbols generated by the non-terminals should
go in the string, so #(A) = 2, #(B) = 1,
and if (c, d) ? LA and e ? LB this derivation
can generate the final string aacbeddbcba. These
intermediary configurations are in H serialized
into strings of nonterminals, with a ?nonterminal
marker? symbol in each position where a non-
terminal is referred to (i.e., H generates a symbol
stating ?the ith string generated by instance j of
the nonterminal A goes here?). H then operates
like a Turing machine. A special nonterminal, the
rewriting head, picks a rule from G to apply, and
walks through the string replacing the nonterminal
markers that are affected by that rule. This proce-
dure is then repeated d times.
We start by illustrating the principles of the re-
duction by an example. Consider the grammar
26
Pr1,1?2XS,1,1 =? XA,1,2XA,2,2Pr1,1?2 =? XA,1,2XA,2,2R =? XA,1,2RXA,2,2 =?
RXA,1,2XA,2,2 =? Pr2,2?3XA,1,2XA,2,2
?
=? XA,1,3XB,1,3XA,2,3B2,3R
?
=?
Pr2,3?4XA,1,3XB,1,3XA,2,3XB,2,3
?
=? XA,1,4XB,1,4XB,1,3XA,2,3XB,2,4XB,2,3R
?
=?
Pr6,3?1XA,1,4XB,1,4XB,1,3XA,2,3XB,2,4XB,2,3
?
=? Pr3,4?1XA,1,4XB,1,4bXA,2,3XB,2,4b
?
=?
Pr5,4?1aXB,1,4baXB,2,4b
?
=? aabaabR
?
=? aabaab
Figure 2: A derivation in the context-sensitive grammar constructed to simulate an LCFRS. All steps in
the application of the first rule, r1 = S ? f(A), are given, the rest is abbreviated.
G = ({S,A,B}, {a, b}, F,R, S) where F is
{f(x, y) = xy, ha() = (a, a), hb() = (b, b),
g((x, y), (x?, y?)) = (xx?, yy?)},
and R contains the following
r1 = S ? f(A) r2 = A? g(A,B)
r3 = A? ha() r4 = A? hb()
r5 = B ? ha() r6 = B ? hb()
Notice that L(G) = {ww | w ? {a, b}+}. We
now describe how H is constructed by the reduc-
tion, after which the more general description fol-
lows. A derivation in G starts with the nontermi-
nal S and must then apply r1. H is constructed
to start with the string Pr1,1?2XS,1,1 (all these
symbols are nonterminals, H has the same termi-
nal alphabet as G). The symbols  and  mark
the beginning and end of the string. The nonter-
minal XS,1,1 is a ?nonterminal marker? and de-
notes the location where the first string generated
by instance 1 of the nonterminal S is to be placed.
Since #(S) = 1 the first string is the only string
generated from S. The last subscript, the instance
number, is there to differentiate markers belong-
ing to different instances of the same nonterminal.
The rewriting head non-deterministically picks an
instance number for a round of rewriting (single
rule application) from a pool sufficiently large to
differentiate between the maximal number of non-
terminals (since the rank of G is at most k, no
more than k2 nonterminals can be generated in k
rule applications). Pr1,1?2 is the ?rewriting head?,
the anchor for rule applications. The subscripts on
P determines that it will apply the rule r1, rewrit-
ing nonterminal markers corresponding to the left
hand side nonterminal of r1 which have instance
number 1. Applying the rule may create new non-
terminal markers, all of which get the instance
number 2, also determined by the subscript.
That is, the rules for Pr1,i?j in H will
be Pr1,i?jXS,1,i ? XA,1,jXA,2,jPr1,i?j , for
i, j ? [2k2], and Pr1,i?jx ? xPr1,i?j for all
other x 6= . Pr5,i?jXB,1,i ? aPr5,i?j is an-
other example of a rule corresponding to rule r5
of G. When a rewriting head hits  it is replaced
by a nonterminal R which reverses through the
string (with rules of the form xR ? Rx for all
x 6= ), after which a new rewriting head is non-
deterministically picked using one of the rules in
{R ? Pr,i?j | r ? R, i, j ? [2k2]}, after
which the string is rewritten once more. Finally,
there are rules  ? ?,  ? ? and R ? ?, to
remove all nonterminals once rewriting has termi-
nated. A derivation is demonstrated in Figure 2.
By induction on the length of derivations, one
can show that L(H) = L(G). Now we need to
modify the construction slightly to ensure that H
can simulate d steps of G in dH steps.
Limiting steps in G. Construct a SHORT
P-LCFRS(P) DERIVATION instance (G?, w, d)
from (G,w, d) whereG? is such that it cannot per-
form more than d derivation steps. Let
N ? = {Ai | A ? N, i ? [d]},
and let
Ai ? f(Bj1 , Cj2 , . . .) ? R
?
for all A ? f(B,C, . . .) ? R, i ? [d] and
j1 + j2 + ? ? ? = i? 1. Then G? = (N ?,?, R?, S1).
This reduction is somewhat heavy-handed, but is
in FPT since it leaves k unchanged and each rule
is replaced by less than kk rules (since d and the
rank of the grammar are part of the parameter k).
Deferring terminals. A problem in completing
the reduction from (G,w, d) to (H,w, dH) is that
the number of terminal symbolsG generates is not
in its parameter k. For example, G may contain
a rule like A ? a ? ? ? a, for an arbitrary num-
ber of as. Applying this rule may make the in-
termediary string H is operating on too long for
it to complete rewriting in dH steps. This can
27
easily be fixed by a polynomial-time rewriting of
H . For any rule w ? w? in H such that w? con-
tains at least one terminal, replace every maximal
substring ? ? ?? by a new nonterminal T?, a
?terminal place-holder?.The rewriting head P and
reversal nonterminal R just walk over the place-
holders without changing them. Now add the rule
T? ? ? for each T?. For example, where a
rewriting head inH might have replacedXA,1,1 by
abcXB,1,1baXB,2,1cc it will now instead replace it
by TabcXB,1,1TbaXB,2,1Tcc, and can defer replac-
ing the place-holder nonterminals until the end.
Completing the reduction. Now we are ready
to put all the pieces together. Given the SHORT
P-LCFRS(P) DERIVATION instance (G,w, d),
apply the limiting steps reduction to construct
(G?, w, d?). Apply the rewriting construction to
G to get the context-sensitive grammar H . Now
L(H) equals the language G can generate in d
steps. Apply the deferring terminals construction
to H to get H ?. All that remains is to calcu-
late dH , the number of steps that H ? may take.
For an FPT-reduction this number may only de-
pend on the parameter k of (G?, w, d?). Picking
dH = k5+103 is sufficient. Each rule inG? gener-
ates less than k nonterminals (since the maximum
rank is at most k), each of which will generate
at most k markers in the derivation in H ? (since
the fanout is at most k). The rule may in addi-
tion generate (k + 1)k terminal place-holders (the
k2 nonterminal markers and string ends separating
maximal terminal substrings). After k rule appli-
cations, without replacing terminal placeholders,
the intermediary string in a derivation in H is less
than k(k2+(k+1)k)+3 symbols long. Simulating
a rule application in H ? entails walking the string
twice (forward and then reversing), and k rules are
applied, giving 2k(k(k2+(k+1)k)+3) steps. An-
other k(k+ 1) + 3 steps at the end replace the ter-
minal place-holders and remove markers and the
rewriting head. Adding things up we arrive at a
polynomial of degree 4 that can be rounded up to
k5 + 103.
Theorem 5.3. SHORT P-LCFRS(P) DERIVA-
TION is W[1]-hard.
Proof. This combines Lemmas 5.1 and 5.2.
The result of Theorem 5.3 also trivially applies
to another natural choice of parameters, the depth
of acyclic LCFRS, since they can naturally only
take a limited number of derivation steps.
Definition 5.3. A LCFRS is acyclic of depth d if d
is the smallest integer such that there is a function
? : N ? [d] such that for all A? f(B1, . . . , Bn)
in R and i ? [n] it holds that ?(A) < ?(Bi).
Corollary. The membership problem for acyclic
LCFRS where the rank, fan-out, and depth are
taken as the parameter is W[1]-complete.
6 Discussion
We have shown that the 1-LCFRS(P)-
MEMBERSHIP problem is W[SAT]-hard, but
we have no upper bound, except for the trivial
XP membership. A conjecture of Pietrzak (2003)
may help explain the difficulty of finding such
an upper bound. It states that any parameterized
problem that has a property that Pietrzak calls
additive is either in FPT or not in W[P]. Basically,
additivity says that any number of instances,
sharing a parameter value, can in polynomial time
be combined into one big instance, with the same
parameter. While 1-LCFRS(P)-MEMBERSHIP
is not additive, it has subproblems that are. This
means that if Pietrzak?s conjecture is true (and
FPT 6= W[P]), then 1-LCFRS(P)-MEMBERSHIP
cannot belong to W[P].
While our results are mostly intractability re-
sults, we see them as a first step towards a more
finely grained understanding of the complexity of
LCFRS parsing. Ruling out simple parameteri-
zation by fan-out or rank as a road towards effi-
cient algorithms lets us focus on other possibili-
ties. Many possible parameterizations remain un-
explored. In particular, we conjecture that param-
eterizing by string length yields FPT membership.
In the search for features that can be used in algo-
rithm development, it may also be useful to inves-
tigate other formalisms, such as e.g., hypergraph
replacement and tree-walking transducers.
Acknowledgments
We acknowledge the support of the Swedish Re-
search Council grant 621-2011-6080.
References
K. A. Abrahamson, R. G. Downey, and M. R. Fellows.
1993. Fixed-parameter intractability II (Extended
abstract). In Proceedings of the 10th Annual Sym-
posium on Theoretical Aspects of Computer Science
(STACS?93), pages 374?385.
28
K. A. Abrahamson, R. G. Downey, and M. R. Fellows.
1995. Fixed-parameter tractability and complete-
ness IV: On completeness for W[P] and PSPACE-
analogues. Annals of Pure and Applied Logic,
73:235?276.
P. Boullier. 2004. Range concatenation grammars.
In New Developments in Parsing Technology, pages
269?289. Kluwer Academic Publishers.
H. Burden and P. Ljunglo?f. 2005. Parsing linear
context-free rewriting systems. In Proceedings of
9th International Workshop on Parsing Technolo-
gies.
R. G. Downey and M. R. Fellows. 1999. Parameter-
ized Complexity. Springer-Verlag.
R. G. Downey, M. R. Fellows, B. M. Kapron, M. T.
Hallett, and H. T. Wareham. 1994. The parameter-
ized complexity of some problems in logic and lin-
guistics. In Logical Foundations of Computer Sci-
ence, pages 89?100. Springer.
J. Flum and M. Grohe. 2006. Parameterized Complex-
ity Theory. Springer-Verlag.
C. Go?mez-Rodr??guez, M. Kuhlmann, and G. Satta.
2010. Efficient parsing of well-nested linear
context-free rewriting systems. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the ACL, pages 276?
284.
A. Joshi. 1985. Tree adjoining grammars: How much
context-sensitivity is required to provide reasonable
structural descriptions. In Natural Language Pars-
ing, pages 206?250. Cambridge University Press.
Y. Kaji, R. Nakanisi, H. Seki, and T. Kasami. 1992.
The universal recognition problem for multiple
context-free grammars and for linear context-free
rewriting systems. IEICE Transactions on Informa-
tion and Systems, E75-D(1):78?88.
K. Pietrzak. 2003. A conjecture on the parameterized
hierarchy. Notes on a talk given at Dagstuhl Seminar
03311. Unpublished manuscript.
B. Sagot and G. Satta. 2010. Optimal rank reduction
for linear context-free rewriting systems with fan-
out two. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?10), pages 525?533.
G. Satta. 1992. Recognition of linear context-free
rewriting systems. In Proceedings of the 30th An-
nual Meeting of the Association for Computational
Linguistics (ACL?92), pages 89?95.
G. Satta. 1998. Trading independent for synchronized
parallelism in finite copying parallel rewriting sys-
tems. J. Computer and System Sciences, 56(1):27?
45.
H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991.
On multiple context-free grammars. Theoretical
Computer Science, 88(2):191?229.
K. Vijay-Shanker, D. J. Weir, and A. K. Joshi. 1987.
Characterizing structural descriptions produced by
various grammatical formalisms. In Proceedings of
the 25th Meeting of the Association for Computa-
tional Linguists (ACL?87), pages 104?111.
29
