Extracting Important Sentences with Support Vector Machines
Tsutomu HIRAO and Hideki ISOZAKI and Eisaku MAEDA
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan
{hirao,isozaki,maeda}@cslab.kecl.ntt.co.jp
Yuji MATSUMOTO
Graduate School of Information and Science, Nara Institute of Science and Technology
8516-9, Takayama, Ikoma, Nara 630-0101 Japan
matsu@is.aist-nara.ac.jp
Abstract
Extracting sentences that contain important in-
formation from a document is a form of text
summarization. The technique is the key to the
automatic generation of summaries similar to
those written by humans. To achieve such ex-
traction, it is important to be able to integrate
heterogeneous pieces of information. One ap-
proach, parameter tuning by machine learning,
has been attracting a lot of attention. This pa-
per proposes a method of sentence extraction
based on Support Vector Machines (SVMs). To
confirm the method?s performance, we conduct
experiments that compare our method to three
existing methods. Results on the Text Summa-
rization Challenge (TSC) corpus show that our
method offers the highest accuracy. Moreover,
we clarify the different features effective for ex-
tracting different document genres.
1 Introduction
Extracting important sentences means extract-
ing from a document only those sentences that
have important information. Since some sen-
tences are lost, the result may lack coherence,
but important sentence extraction is one of
the basic technologies for generating summaries
that are useful for humans to browse. There-
fore, this technique plays an important role in
automatic text summarization.
Many researchers have been studied impor-
tant sentence extraction since the late 1950?s
(Luhn, 1958). Conventional methods focus on
sentence features and define significance scores.
The features include key words, sentence posi-
tion, and certain linguistic clues. Edmundson
(1969) and Nobata et al (2001) have proposed
scoring functions to integrate heterogeneous fea-
tures. However, we can not tune the parameter
values by hand when the number of features is
large.
When a large quantity of training data is
available, tuning can be effectively realized by
machine learning. In recent years, machine
learning has attracted attention in the field of
automatic text summarization. Aone et al
(1998) and Kupiec et al (1995) employed
Bayesian classifiers, Mani et al (1998), Nomoto
et al (1997), Lin (1999), and Okumura et
al. (1999) used decision tree learning. How-
ever, most machine learning methods overfit the
training data when many features are given.
Therefore, we need to select features carefully.
Support Vector Machines (SVMs) (Vapnik,
1995) is robust even when the number of
features is large. Therefore, SVMs have
shown good performance for text categoriza-
tion (Joachims, 1998), chunking (Kudo and
Matsumoto, 2001), and dependency structure
analysis (Kudo and Matsumoto, 2000).
In this paper, we present an important sen-
tence extraction technique based on SVMs. We
verified the technique against the Text Summa-
rization Challenge (TSC) (Fukushima and Oku-
mura, 2001) corpus.
2 Important Sentence Extraction
based on Support Vector Machines
2.1 Support Vector Machines (SVMs)
SVM is a supervised learning algorithm for 2-
class problems.
Training data is given by
(x
1
, y
1
), ? ? ? , (xu, yu), xj ? Rn, yj ? {+1,?1}.
Here, x
j
is a feature vector of the j-th
sample; y
j
is its class label, positive(+1) or
negative(?1). SVM separates positive and neg-
ative examples by a hyperplane defined by
w ? x + b = 0, w ? Rn, b ? R, (1)
Positive
Negative
margin
wx + b = 0
wx + b = 1
wx + b = -1
Support Vector
Figure 1: Support Vector Machines.
where ??? represents the inner product.
In general, such a hyperplane is not unique.
Figure 1 shows a linearly separable case. The
SVM determines the optimal hyperplane by
maximizing the margin. A margin is the dis-
tance between negative examples and positive
examples.
Since training data is not necessarily linearly
separable, slack variables (?
j
) are introduced for
all x
j
. These ?
j
incur misclassification error,
and should satisfy the following inequalities:
w ? xj + b ? 1? ?j
w ? xj + b ? ?1 + ?j . (2)
Under these constraints, the following objective
function is to be minimized.
1
2
||w||2 + C
u
?
j=1
?j . (3)
The first term in (3) corresponds to the size
of the margin and the second term represents
misclassification.
By solving a quadratic programming prob-
lem, the decision function f(x) = sgn(g(x)) can
be derived where
g(x) =
( 
?
i=1
?iyixi ? x+ b
)
. (4)
The decision function depends on only sup-
port vectors (x
i
). Training examples, except
for support vectors, have no influence on the
decision function.
Non-linear decision surfaces can be realized
by replacing the inner product of (4) with a ker-
nel function K(x ? x
i
) :
g(x) =
( 
?
i=1
?iyiK(xi,x) + b
)
. (5)
In this paper, we use polynomial kernel func-
tions that have been very effective when applied
to other tasks, such as natural language pro-
cessing (Joachims, 1998; Kudo and Matsumoto,
2001; Kudo and Matsumoto, 2000):
K(x,y) = (x ? y + 1)d. (6)
2.2 Sentence Ranking by using Support
Vector Machines
Important sentence extraction can be regarded
as a two-class problem: important or unimpor-
tant. However, the proportion of important sen-
tences in training data will differ from that in
the test data. The number of important sen-
tences in a document is determined by a sum-
marization rate that is given at run-time. A
simple solution for this problem is to rank sen-
tences in a document. We use g(x) the distance
from the hyperplane to x to rank the sentences.
2.3 Features
We define the boolean features discussed below
that are associated with sentence S
i
by taking
past studies into account (Zechner, 1996; No-
bata et al, 2001; Hirao et al, 2001; Nomoto
and Matsumoto, 1997).
We use 410 boolean variables for each S
i
.
Where x = (x[1], ? ? ? , x[410]). A real-valued fea-
ture normalized between 0 and 1 is represented
by 10 boolean variables. Each variable corre-
sponds to an internal [i/10,(i + 1)/10) where
i = 0 to 9. For example, Posd = 0.75 is rep-
resented by ?0000000100? because 0.75 belongs
to [7/10,8/10).
Position of sentences
We define three feature functions for the posi-
tion of S
i
. First, Lead is a boolean that corre-
sponds to the output of the lead-based method
described below1 . Second, Posd is S
i
?s position
in a document. Third, Posp is S
i
?s position in a
paragraph. The first sentence obtains the high-
est score, the last obtains the lowest score:
1 When a sentence appears in the first N of document,
we assign 1 to the sentence. An N was given for each
document by TSC committee.
Posd(Si) = 1?BD(Si)/|D(Si)|
Posp(Si) = 1?BP (Si)/|P (Si)|.
Here, |D(S
i
)| is the number of characters in
the document D(S
i
) that contains S
i
; BD(S
i
)
is the number of characters before S
i
in D(S
i
);
|P (S
i
)| is the number of characters of the para-
graph P (S
i
) that contains S
i
, and BP (S
i
) is
the number of characters before S
i
in the para-
graph.
Length of sentences
We define a feature function that addresses the
length of sentence as
Len(Si) = |Si|/ max
S
z
?D(S
i
)
|Sz|.
Here, |S
i
| is the number of characters of sen-
tence S
i
, and max
S
z
?D
|S
z
| is the maximum
number of characters in a sentence that belongs
to D(S
i
).
In addition, the length of a previous sentence
Len
?1(Si) = Len(Si?1) and the length of a next
sentence Len+1(Si) = Len(Si+1) are also fea-
tures of sentence S
i
.
Weight of sentences
We defined the feature function that weights
sentences based on frequency-based word
weighting as
Wf (Si) =
?
t
tf(t, Si) ? w(t, D(Si)).
Here, Wf(S
i
) is the summention of weighting
w(t, D(S
i
)) of words that appear in a sentence.
tf(t, S
i
) is term frequency of t in S
i
. We used
only nouns. In addition, we define word weight
w(t, D(S
i
)) based on a specific field (Hara et al,
1997):
w(t, D(Si)) = ?
(
1
T
T
?
z=1
?z
Vz
)
+?
(
tf(t, D(Si))
?
t? tf(t
?, D(Si))
)
.
Here, T is the number of sentence in a docu-
ment, and V
z
is the number of words in sentence
S
z
? D(S
i
) (repetitions are ignored). Also, ?
z
is
a boolean value: that is 1 when t appears inS
z
.
The first term of the equation above is the
weighting of a word in a specific field. The sec-
ond term is the occurrence probability of word
t.
We set parameters ? and ? as 0.8, 0.2, re-
spectively. The weight of a previous sentence
Wf
?1(Si)=Wf (Si?1), and the weight of a next
sentence Wf +1(Si)=Wf (Si+1) are also features
of sentence S
i
.
Density of key words
We define the feature function Den(S
i
) that
represents density of key words in a sentence
by using Hanning Window function (f
H
(k, m)):
Den(Si) = maxm
m+Win/2
?
k=m?Win/2
fH(k,m) ? a(k, Si),
where f
H
(k, m) is given by
f
H
(k,m) =
{
1
2
(
1 + cos2? k?m
W in
)
(|k ?m| ? Win/2)
0 (|k ?m| > Win/2).
The key words (KW ) are the top 30% of
words in a document according to w(t, D(S
i
)).
Also, m is the center position of the window,
Win = |S
i
|/2. In addition, a(k, S
i
) is defined as
follows:
a(k, S
i
) =
?
?
?
?
?
w(t,D) Where a word t (? KW ) begins
at k
0 k is not the beginning position
of a word in KW.
Named Entities
x[r]=1 (1?r?8) indicates that a certain Named
Entity class appears in S
i
. The number of
Named Entity classes is 8 (Sekine and Eriguchi,
2000), e.g., PERSON, LOCATION. We use
Isozaki?s NE recognizer (Isozaki, 2001).
Conjunctions
x[r]=1 (9?r?61) if and only if a certain con-
junction is used in the sentence. The number of
conjunctions is 53.
Functional words
x[r]=1 (62?r?234) if and only if a certain func-
tional word such as ga, ha, and ta is used in
the sentence. The number of functional words
is 173.
Part of speech
x[r]=1 (235?r?300) if and only if a certain part
of speech such as ?Noun-jiritsu? and ?Verb-
jiritsu? is used in the sentence. The number
of part of speech is 66.
Semantical depth of nouns
x[r]=1 (301?r?311) if and only if S
i
contains
a noun at a certain semantical depth according
to a Japanese lexicon, Goi-Taikei (Ikehara et al,
1997). The number of depth levels is 11. For
instance, Semdep=2 means that a noun in S
i
belongs to the second depth level.
Document genre
x[r]=1 (312?r?315) if and only if the docu-
ment belongs to a certain genre. The genre is
explicitly written in the header of each docu-
ment. The number of genres is four: General,
National, Editorial, and Commentary.
Symbols
x[r]=1 (r=316) if and only if sentence includes
a certain symbol (for example: ?,&,
).
Conversation
x[r]=1 (r=317) if and only if S
i
includes a con-
versation style expression.
Assertive expressions
x[r]=1 (r=318) if and only if S
i
includes an as-
sertive expression.
3 Experimental settings
3.1 Corpus
We used the data set of TSC (Fukushima and
Okumura, 2001) summarization collection for
our evaluation. TSC was established as a sub-
task of NTCIR-2 (NII-NACSIS Test Collection
for IR Systems). The corpus consists of 180
Japanese documents2 from the Mainichi News-
papers of 1994, 1995, and 1998. In each doc-
ument, important sentences were manually ex-
tracted at summarization rates of 10%, 30%,
and 50%. Note that the summarization rates
depend on the number of sentences in a doc-
ument not the number of characters. Table 1
shows the statistics.
3.2 Evaluated methods
We compared four methods: decision tree learn-
ing, boosting, lead, and SVM. At each summa-
rization rate, we trained classifiers and classified
test documents.
Decision tree learning method
We used C4.5 (Quinlan, 1993) for our experi-
ments with the default settings. We used the
2 Each document is presented in SGML style with sen-
tence and paragraph separators attached.
features described in section 2. Sentences were
ranked according to their certainty factors given
by C4.5.
Boosting method
We used C5.0, which applies boosting to deci-
sion tree learning. The number of rounds was
set to 10. Sentences were ranked according to
their certainty factors given by C5.0.
Lead-based method
The first N sentences of a document were se-
lected. N was determined according to the sum-
marization rates.
SVM method
This is our method as outlined in section 2. We
used the second-order polynomial kernel, and
set C (in equation (3)) as 0.0001. We used
TinySVM3 .
3.3 Measures for evaluation
In the TSC corpus, the number of sentences to
be extracted was explicitly given by the TSC
committee. When we extract sentences accord-
ing to that number, Precision, Recall, and F-
measure become the same value. We call this
value Accuracy. Accuracy is defined as follows:
Accuracy = b/a ? 100,
where a is the specified number of important
sentences, and b is the number of true impor-
tant sentences that were contained in system?s
output.
4 Results
Table 2 shows the results of five-fold cross vali-
dation by using all 180 documents.
For all summarization rates and all genres,
SVM achieved the highest accuracy, the lead-
based method the lowest. Let the null hypoth-
esis be ?There are no differences among the
scores of the four methods?. We tested this null
hypothesis at a significance level of 1% by using
Tukey?s method. Although the SVM?s perfor-
mance was best, the differences were not sta-
tistically significant at 10%. At 30% and 50%,
SVM performed better than the other methods
with a statistical significance.
3 http://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM/
Table 1: Details of data sets.
General National Editorial Commentary
# of documents 16 76 41 47
# of sentences 342 1721 1362 1096
# of important sentences (10%) 34 172 143 112
# of important sentences (30%) 103 523 414 330
# of important sentences (50%) 174 899 693 555
Table 2: Evaluation results of cross validation.
Summarization rate 10%
Genre SVM C4.5 C5.0 Lead
General 55.7 55.2 52.4 47.9
Editorial 34.2 33.6 27.9 31.6
National 61.4 52.0 56.3 51.8
Commentary 28.7 27.4 21.4 15.9
Average 46.2 41.4 40.4 37.4
Summarization rate 30%
Genre SVM C4.5 C5.0 Lead
General 51.0 45.7 50.4 50.5
Editorial 47.8 41.6 43.3 36.7
National 55.9 44.1 49.3 54.3
Commentary 48.7 39.4 40.1 32.4
Average 51.6 42.4 45.7 44.2
Summarization rate 50%
Genre SVM C4.5 C5.0 Lead
General 65.2 63.0 60.2 60.4
Editorial 60.6 54.1 54.6 51.0
National 63.3 58.7 58.7 61.5
Commentary 65.7 59.6 60.6 50.4
Average 63.5 58.2 58.4 56.1
5 Discussion
Table 2 shows that Editorial and Commentary
are more difficult than the other genres. We
can consider two reasons for the poor scores of
Editorial and Commentary:
? These genres have no feature useful for dis-
crimination.
? Non-standard features are useful in these
genres.
Accordingly, we conduct an experiment to
clarify genre dependency4 .
4 We did not use General because the number of doc-
uments in this genre was insufficient.
1 Extract 36 documents at random from
genre i for training.
2 Extract 4 documents at random from genre
j for test.
3 Repeat this 10 times for all combinations
of (i, j).
Table 3 shows that the result implies that
non-standard features are useful in Editorial
and Commentary documents.
Now, we examine effective features in each
genre. Since we used the second order polyno-
mial kernel, we can expand g(x) as follows:
g(x) = b+

?
i=1
wi + 2

?
i=1
wi
u
?
k=1
xi[k]x[k] +

?
i=1
wi
u
?
h=1
u
?
k=1
xi[h]xi[k]x[h]x[k ], (7)
where ) is the number of support vectors, and
w
i
equals ?
i
y
i
.
We can rewrite it as follows when all vectors
are boolean:
g(x) = W
0
+
u
?
k=1
W
1
[k]x[k] +
u?1
?
h=1
u
?
k=h+1
W
2
[k, h]x[h]x[k] (8)
where
W0 = b +
?

i=1 wi,W1[k] = 3
?

i=1 wixi[k], and
W2[h, k] = 2
?

i=1 wixi[h]xi[k].
Therefore, W1[k] indicates the significance of
an individual feature and W2[h, k] indicates the
significance of a feature pair. When |W1[k]| or
|W2[h, k]| was large, the feature or the feature
pair had a strong influence on the optimal hy-
perplane.
Table 3: Evaluation results for three genres.
Training \ Test
National Editorial Commentary
10% 30% 50% 10% 30% 50% 10% 30% 50%
National 63.4 57.6 65.5 32.8 39.4 53.6 24.0 39.5 60.8
Editorial 49.3 46.8 58.4 33.9 49.1 64.4 24.9 43.6 62.1
Commentary 37.4 43.3 61.1 18.4 41.8 57.8 30.6 49.6 67.0
Table 4: Effective features and their pairs
Summarization rate 10%
National Editorial Commentary
Lead ? ga 0.9?Posd?1.0 ? 0.7?Wf<0.8 0.9?P osd?1.0 ? Semdep=2
0.9?Posd?1.0 ? ga NE ? de 0.5?Len
+1
<0.6 ? Noun-hijiritsu
Lead ? ta 0.9?Posd?1.0 ? de 0.0?P osp<0.1 ? 0.5?Wf
+1
<0.6
0.9?Posd?1.0 ? ta Lead ? 0.7?Wf<0.8 0.8?P osd<0.9 ? Particle
Summarization rate 30%
National Editorial Commentary
Lead ? Semdep=6 0.0?Posp<0.1 ? ga Aux verb ? Semdep=2
0.9?Posd?1.0 ? Semdep=6 0.9?Posd?1.0 ? NE Verb-jiritsu ? Semdep=2
Lead ? ga Lead ? NE Semdep=2
0.9?Posd?1.0 0.0?P osd<0.1 0.0?Posp<0.1 ? 0.5?Den<0.6
Summarization rate 50%
National Editorial Commentary
Lead 0.0?Posp<0.1 ? Semdep=6 0.0?Posp<0.1 ? Particle
Lead ? ha 0.0?Posp<0.1 ? ga 0.2?P osd<0.3
Lead ? Verb-jiritsu 0.0?Posp<0.1 0.4?Len<0.5
Lead ? ta 0.0?P osd<0.1 0.0?Posp<0.1
Table 4 shows some of the effective features
that had large weights W1[k], W2[h, k] for each
genre.
Effective features common to three genres at
three rates were sentence positions. Since Na-
tional has a typical newspaper style, the begin-
ning of the document was important. More-
over, ?ga? and ?ta? were important. These
functional words are used when a new event is
introduced.
In Editorial and Commentary, the end of a
paragraph and that of a document were impor-
tant. The reason for this result is that subtopic
or main topic conclusions are common in those
positions. This implies that National has a dif-
ferent text structure from Editorial and Com-
mentary.
Moreover, in Editorial, ?de? and sentence
weight was important. In Commentary, seman-
tically shallow words, sentence weight and the
length of a next sentence were important.
In short, we confirmed that the feature(s) ef-
fective for discriminating a genre differ with the
genre.
6 Conclusion
This paper presented a SVM-based important
sentence extraction technique. Comparisons
were made using the lead-based method, deci-
sion tree learning method, and boosting method
with the summarization rates of 10%, 30%,
and 50%. The experimental results show that
the SVM-based method outperforms the other
methods at all summarization rates. Moreover,
we clarified the effective features for three gen-
res, and showed that the important features
vary with the genre.
In our future work, we would like to apply our
method to trainable Question Answering Sys-
tem SAIQA-II developed in our group.
Acknowledgement
We would like to thank all the members of the
Knowledge Processing Research Group for valu-
able comments and discussions.
References
C. Aone, M. Okurowski, and J. Gorlinsky. 1998.
Trainable Scalable Summarization Using Ro-
bust NLP and Machine Learning. Proc. of the
17th COLING and 36th ACL, pages 62?66.
H. Edmundson. 1969. New methods in
automatic abstracting. Journal of ACM,
16(2):246?285.
T. Fukushima and M. Okumura. 2001. Text
Summarization Challenge Text summariza-
tion evaluation in Japan. Proc. of the
NAACL2001 Workshop on Automatic sum-
marization, pages 51?59.
M. Hara, H. Nakajima, and T. Kitani. 1997.
Keyword Extraction Using a Text Format
and Word Importance in a Specific Filed (in
Japanese). Transactions of Information Pro-
cessing Society of Japan, 38(2):299?309.
T. Hirao, M. Hatayama, S. Yamada, and
K. Takeuchi. 2001. Text Summarization
based on Hanning Window and Dependency
structure analysis. Proc. of the 2nd NTCIR
Workshop, pages 349?354.
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo,
H. Nakaiwa, K. Ogura, Y. Ooyama, and
Y. Hayashi. 1997. Goi-Taikei ? A Japanese
Lexicon (in Japanese). Iwanami Shoten.
H. Isozaki. 2001. Japanese Named Entity
Recognition based on Simple Rule Generator
and Decision Tree Learning. Proc. of the 39th
ACL, pages 306?313.
T. Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with
Many Relevant Features. Proc. of ECML,
pages 137?142.
T. Kudo and Y. Matsumoto. 2000. Japane De-
pendency Structure Analysis Based on Su-
port Vector Machines. Proc. of EMNLP and
VLC, pages 18?25.
T. Kudo and Y. Matsumoto. 2001. Chunking
with Support Vector Machine. Proc. of the
2nd NAACL, pages 192?199.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A
Trainable Document Summarizer. Proc. of
the 18th ACM-SIGIR, pages 68?73.
Chin-Yew Lin. 1999. Training a Selection Func-
tion for Extraction. Proc. of the 18th ACM-
CIKM, pages 55?62.
H. Luhn. 1958. The Automatic Creation of Lit-
erature Abstracts. IBM Journal of Research
and Development, 2(2):159?165.
I. Mani and E. Bloedorn. 1998. Machine Learn-
ing of Generic and User-Focused Summariza-
tion. Proc. of the 15th AAAI, pages 821?826.
C. Nobata, S. Sekine, M. Murata, K. Uchimoto,
M. Utiyama, and H. Isahara. 2001. Sentence
Extraction System Assembling Multiple Ev-
idence. Proc. of the 2nd NTCIR Workshop,
pages 319?324.
T. Nomoto and Y. Matsumoto. 1997. The Reli-
ability of Human Coding and Effects on Auto-
matic Abstracting (in Japanese). The Special
Interest Group Notes of IPSJ (NL-120-11),
pages 71?76.
M. Okumura, Y. Haraguchi, and H. Mochizuki.
1999. Some Observations on Automatic
Text Summarization Based on Decision Tree
Learning (in Japanese). Proc. of the 59th Na-
tional Convention of IPSJ (5N-2), pages 393?
394.
J. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
S. Sekine and Y. Eriguchi. 2000. Japanese
Named Entity Extraction Evaluation - Anal-
ysis of Results -. Proc. of the 18th COLING,
pages 1106?1110.
V. Vapnik. 1995. The Nature of Statistical
Learning Theory. New York.
K. Zechner. 1996. Fast Generation of Abstracts
from General Domain Text Corpora by Ex-
tracting Relevant Sentences. Proc. of the 16th
COLING, pages 986?989.
A Deterministic Word Dependency Analyzer
Enhanced With Preference Learning
Hideki Isozaki and Hideto Kazawa and Tsutomu Hirao
NTT Communication Science Laboratories
NTT Corporation
2-4 Hikaridai, Seikacho, Sourakugun, Kyoto, 619-0237 Japan
{isozaki,kazawa,hirao}@cslab.kecl.ntt.co.jp
Abstract
Word dependency is important in parsing tech-
nology. Some applications such as Informa-
tion Extraction from biological documents ben-
efit from word dependency analysis even with-
out phrase labels. Therefore, we expect an ac-
curate dependency analyzer trainable without
using phrase labels is useful. Although such
an English word dependency analyzer was pro-
posed by Yamada and Matsumoto, its accu-
racy is lower than state-of-the-art phrase struc-
ture parsers because of the lack of top-down in-
formation given by phrase labels. This paper
shows that the dependency analyzer can be im-
proved by introducing a Root-Node Finder and
a Prepositional-Phrase Attachment Resolver.
Experimental results show that these modules
based on Preference Learning give better scores
than Collins? Model 3 parser for these subprob-
lems. We expect this method is also applicable
to phrase structure parsers.
1 Introduction
1.1 Dependency Analysis
Word dependency is important in parsing technol-
ogy. Figure 1 shows a word dependency tree. Eis-
ner (1996) proposed probabilistic models of depen-
dency parsing. Collins (1999) used dependency
analysis for phrase structure parsing. It is also stud-
ied by other researchers (Sleator and Temperley,
1991; Hockenmaier and Steedman, 2002). How-
ever, statistical dependency analysis of English sen-
tences without phrase labels is not studied very
much while phrase structure parsing is intensively
studied. Recent studies show that Information Ex-
traction (IE) and Question Answering (QA) benefit
from word dependency analysis without phrase la-
bels. (Suzuki et al, 2003; Sudo et al, 2003)
Recently, Yamada and Matsumoto (2003) pro-
posed a trainable English word dependency ana-
lyzer based on Support Vector Machines (SVM).
They did not use phrase labels by considering an-
notation of documents in expert domains. SVM
(Vapnik, 1995) has shown good performance in dif-
He
a
girl
a
telescope
with
saw
He saw a girl with a telescope.
Figure 1: A word dependency tree
ferent tasks of Natural Language Processing (Kudo
and Matsumoto, 2001; Isozaki and Kazawa, 2002).
Most machine learning methods do not work well
when the number of given features (dimensionality)
is large, but SVM is relatively robust. In Natural
Language Processing, we use tens of thousands of
words as features. Therefore, SVM often gives good
performance.
However, the accuracy of Yamada?s analyzer is
lower than state-of-the-art phrase structure parsers
such as Charniak?s Maximum-Entropy-Inspired
Parser (MEIP) (Charniak, 2000) and Collins? Model
3 parser. One reason is the lack of top-down infor-
mation that is available in phrase structure parsers.
In this paper, we show that the accuracy of the
word dependency parser can be improved by adding
a base-NP chunker, a Root-Node Finder, and a
Prepositional Phrase (PP) Attachment Resolver. We
introduce the base-NP chunker because base NPs
are important components of a sentence and can be
easily annotated. Since most words are contained
in a base NP or are adjacent to a base NP, we ex-
pect that the introduction of base NPs will improve
accuracy.
We introduce the Root-Node Finder because Ya-
mada?s root accuracy is not very good. Each sen-
tence has a root node (word) that does not modify
any other words and is modified by all other words
directly or indirectly. Here, the root accuracy is de-
fined as follows.
Root Accuracy (RA) =
#correct root nodes / #sentences (= 2,416)
We think that the root node is also useful for depen-
dency analysis because it gives global information
to each word in the sentence.
Root node finding can be solved by various ma-
chine learning methods. If we use classifiers, how-
ever, two or more words in a sentence can be classi-
fied as root nodes, and sometimes none of the words
in a sentence is classified as a root node. Practically,
this problem is solved by getting a kind of confi-
dence measure from the classifier. As for SVM,
f(x) defined below is used as a confidence measure.
However, f(x) is not necessarily a good confidence
measure.
Therefore, we use Preference Learning proposed
by Herbrich et al (1998) and extended by Joachims
(2002). In this framework, a learning system is
trained with samples such as ?A is preferable to
B? and ?C is preferable to D.? Then, the system
generalizes the preference relation, and determines
whether ?X is preferable to Y? for unseen X and
Y. This framework seems better than SVM to select
best things.
On the other hand, it is well known that attach-
ment ambiguity of PP is a major problem in parsing.
Therefore, we introduce a PP-Attachment Resolver.
The next sentence has two interpretations.
He saw a girl with a telescope.
1) The preposition ?with? modifies ?saw.? That is, he
has the telescope. 2) ?With? modifies ?girl.? That is,
she has the telescope.
Suppose 1) is the correct interpretation. Then,
?with modifies saw? is preferred to ?with mod-
ifies girl.? Therefore, we can use Preference
Learning again.
Theoretically, it is possible to build a new De-
pendency Analyzer by fully exploiting Preference
Learning, but we do not because its training takes
too long.
1.2 SVM and Preference Learning
Preference Learning is a simple modification of
SVM. Each training example for SVM is a pair
(yi, xi), where xi is a vector, yi = +1 means that
xi is a positive example, and yi = ?1 means that xi
is a negative example. SVM classifies a given test
vector x by using a decision function
f(x) = wf ? ?(x) + b =
?`
i
yi?iK(x, xi) + b,
where {?i} and b are constants and ` is the number
of training examples. K(xi, xj) = ?(xi) ? ?(xj) is
a predefined kernel function. ?(x) is a function that
maps a vector x into a higher dimensional space.
Training of SVM corresponds to the follow-
ing quadratic maximization (Cristianini and Shawe-
Taylor, 2000)
W (?) =
?`
i=1
?i ?
1
2
?`
i,j=1
?i?jyiyjK(xi, xj),
where 0 ? ?i ? C and
?`
i=1 ?iyi = 0. C is a soft
margin parameter that penalizes misclassification.
On the other hand, each training example
for Preference Learning is given by a triplet
(yi, xi.1, xi.2), where xi.1 and xi.2 are vectors. We
use xi.? to represent the pair (xi.1, xi.2). yi = +1
means that xi.1 is preferable to xi.2. We can regard
their difference ?(xi.1) ? ?(xi.2) as a positive ex-
ample and ?(xi.2) ? ?(xi.1) as a negative example.
Symmetrically, yi = ?1 means that xi.2 is prefer-
able to xi.1.
Preference of a vector x is given by
g(x) = wg??(x) =
?`
i
yi?i(K(xi.1, x)?K(xi.2, x)).
If g(x) > g(x?) holds, x is preferable to x?. Since
Preference Learning uses the difference ?(xi.1) ?
?(xi.2) instead of SVM?s ?(xi), it corresponds to
the following maximization.
W (?) =
?`
i=1
?i ?
1
2
?`
i,j=1
?i?jyiyjK(xi.?, xj.?)
where 0 ? ?i ? C and K(xi.?, xj.?) =
K(xi.1, xj.1) ? K(xi.1, xj.2) ? K(xi.2, xj.1) +
K(xi.2, xj.2). The above linear constraint
?`
i=1 ?iyi = 0 for SVM is not applied to
Preference Learning because SVM requires this
constraint for the optimal b, but there is no b in g(x).
Although SVMlight (Joachims, 1999) provides an
implementation of Preference Learning, we use our
own implementation because the current SVMlight
implementation does not support non-linear kernels
and our implementation is more efficient.
Herbrich?s Support Vector Ordinal Regression
(Herbrich et al, 2000) is based on Preference Learn-
ing, but it solves an ordered multiclass problem.
Preference Learning does not assume any classes.
2 Methodology
Instead of building a word dependency corpus from
scratch, we use the standard data set for comparison.
Dependency Analyzer? PP-Attachment Resolver?
Root-Node Finder?
Base NP Chunker?
(POS Tagger)
?
= SVM, ? = Preference Learning
Figure 2: Module layers in the system
That is, we use Penn Treebank?s Wall Street Journal
data (Marcus et al, 1993). Sections 02 through 21
are used as training data (about 40,000 sentences)
and section 23 is used as test data (2,416 sentences).
We converted them to word dependency data by us-
ing Collins? head rules (Collins, 1999).
The proposed method uses the following proce-
dures.
? A base NP chunker: We implemented an
SVM-based base NP chunker, which is a sim-
plified version of Kudo?s method (Kudo and
Matsumoto, 2001). We use the ?one vs. all
others? backward parsing method based on an
?IOB2? chunking scheme. By the chunking,
each word is tagged as
? B: Beginning of a base NP,
? I: Other elements of a base NP.
? O: Otherwise.
Please see Kudo?s paper for more details.
? A Root-Node Finder (RNF): We will describe
this later.
? A Dependency Analyzer: It works just like Ya-
mada?s Dependency Analyzer.
? A PP-Attatchment Resolver (PPAR): This re-
solver improves the dependency accuracy of
prepositions whose part-of-speech tags are IN
or TO.
The above procedures require a part-of-speech
tagger. Here, we extract part-of-speech tags from
the Collins parser?s output (Collins, 1997) for sec-
tion 23 instead of reinventing a tagger. According
to the document, it is the output of Ratnaparkhi?s
tagger (Ratnaparkhi, 1996). Figure 2 shows the ar-
chitecture of the system. PPAR?s output is used to
rewrite the output of the Dependency Analyzer.
2.1 Finding root nodes
When we use SVM, we regard root-node finding as
a classification task: Root nodes are positive exam-
ples and other words are negative examples.
For this classification, each word wi in a tagged
sentence T = (w1/p1, . . . , wi/pi, . . . , wN/pN ) is
characterized by a set of features. Since the given
POS tags are sometimes too specific, we introduce
a rough part-of-speech qi defined as follows.
? q = N if p = NN, NNP, NNS,
NNPS, PRP, PRP$, POS.
? q = V if p = VBD, VB, VBZ, VBP,
VBN.
? q = J if p = JJ, JJR, JJS.
Then, each word is characterized by the following
features, and is encoded by a set of boolean vari-
ables.
? The word itself wi, its POS tags pi and
qi, and its base NP tag bi = B, I,O.
We introduce boolean variables such as
current word is John and cur-
rent rough POS is J for each of these
features.
? Previous word wi?1 and its tags, pi?1, qi?1,
and bi?1.
? Next word wi+1 and its tags, pi+1, qi+1, and
bi+1.
? The set of left words {w0, . . . , wi?1}, and
their tags, {p0, . . . , pi?1}, {q0, . . . , qi?1}, and
{b0, . . . , bi?1}. We use boolean variables such
as one of the left words is Mary.
? The set of right words {wi+1, . . . , wN},
and their POS tags, {pi+1, . . . , pN} and
{qi+1, . . . , qN}.
? Whether the word is the first word or not.
We also add the following boolean features to get
more contextual information.
? Existence of verbs or auxiliary verbs (MD) in
the sentence.
? The number of words between wi
and the nearest left comma. We
use boolean variables such as near-
est left comma is two words away.
? The number of words between wi and the near-
est right comma.
Now, we can encode training data by using these
boolean features. Each sentence is converted to the
set of pairs {(yi, xi)} where yi is +1 when xi cor-
responds to the root node and yi is ?1 otherwise.
For Preference Learning, we make the set of triplets
{(yi, xi.1, xi.2)}, where yi is always +1, xi.1 corre-
sponds to the root node, and xi.2 corresponds to a
non-root word in the same sentence. Such a triplet
means that xi.1 is preferable to xi.2 as a root node.
2.2 Dependency analysis
Our Dependency Analyzer is similar to Ya-
mada?s analyzer (Yamada and Matsumoto,
2003). While scanning a tagged sentence
T = (w1/p1, . . . , wn/pn) backward from the
end of the sentence, each word wi is classified into
three categories: Left, Right, and Shift.1
? Right: Right means that wi directly modifies
the right word wi+1 and that no word in T
modifies wi. If wi is classified as Right, the
analyzer removes wi from T and wi is regis-
tered as a left child of wi+1.
? Left: Left means that wi directly modifies the
left word wi?1 and that no word in T modifies
wi. If wi is classified as Left, the analyzer re-
moves wi from T and wi is registered as a right
child of wi?1.
? Shift: Shift means that wi is not next to its
modificand or is modified by another word in
T . If wi is classified as Shift, the analyzer
does nothing for wi and moves to the left word
wi?1.
This process is repeated until T is reduced to a
single word (= root node). Since this is a three-class
problem, we use ?one vs. rest? method. First, we
train an SVM classifier for each class. Then, for
each word in T , we compare their values: fLeft(x),
fRight(x), and fShift(x). If fLeft(x) is the largest,
the word is classified as Left.
However, Yamada?s algorithm stops when all
words in T are classified as Shift, even when T has
two or more words. In such cases, the analyzer can-
not generate complete dependency trees.
Here, we resolve this problem by reclassifying a
word in T as Left or Right. This word is selected in
terms of the differences between SVM outputs:
? ?Left(x) = fShift(x) ? fLeft(x),
? ?Right(x) = fShift(x) ? fRight(x).
These values are non-negative because fShift(x)
was selected. For instance, ?Left(x) ' 0 means that
fLeft(x) is almost equal to fShift(x). If ?Left(xk)
gives the smallest value of these differences, the
word corresponding to xk is reclassified as Left. If
1Yamada used a two-word window, but we use a one-word
window for simplicity.
?Right(xk) gives the smallest value, the word cor-
responding to xk is reclassified as Right. Then, we
can resume the analysis.
We use the following basic features for each word
in a sentence.
? The word itself wi and its tags pi, qi, and bi,
? Whether wi is on the left of the root node or on
the right (or at the root node). The root node is
determined by the Root-Node Finder.
? Whether wi is inside a quotation.
? Whether wi is inside a pair of parentheses.
? wi?s left children {wi1, . . . , wik}, which
were removed by the Dependency Analyzer
beforehand because they were classified as
?Right.? We use boolean variables such as
one of the left child is Mary.
Symmetrically, wi?s right children
{wi1, . . . , wik} are also used.
However, the above features cover only near-
sighted information. If wi is next to a very long
base NP or a sequence of base NPs, wi cannot get
information beyond the NPs. Therefore, we add the
following features.
? Li, Ri: Li is available when wi immediately
follows a base NP sequence. Li is the word be-
fore the sequence. That is, the sentence looks
like:
. . . Li ? a base NP ? wi . . .
Ri is defined symmetrically.
The following features of neigbors are also used
as wi?s features.
? Left words wi?3, . . . , wi?1 and their basic fea-
tures.
? Right words wi+1, . . . , wi+3 and their basic
features.
? The analyzer?s outputs (Left/Right/Shift) for
wi+1, . . . , wi+3. (This analyzer runs backward
from the end of T .)
If we train SVM by using the whole data at once,
training will take too long. Therefore, we split
the data into six groups: nouns, verbs, adjectives,
prepositions, punctuations, and others.
2.3 PP attachment
Since we do not have phrase labels, we use all
prepositions (except root nodes) as training data.
We use the following features for resolving PP at-
tachment.
? The preposition itself: wi.
? Candidate modificand wj and its POS tag.
? Left words (wi?2, wi?1) and their POS tags.
? Right words (wi+1, wi+2) and their POS tags.
? Previous preposition.
? Ending word of the following base NP and its
POS tag (if any).
? i ? j, i.e., Number of the words between wi
and wj .
? Number of commas between wi and wj .
? Number of verbs between wi and wj .
? Number of prepositions between wi and wj .
? Number of base NPs between wi and wj .
? Number of conjunctions (CCs) between wi and
wj .
? Difference of quotation depths between wi and
wj . If wi is not inside of a quotation, its quo-
tation depth is zero. If wj is in a quotation, its
quotation depth is one. Hence, their difference
is one.
? Difference of parenthesis depths between wi
and wj .
For each preposition, we make the set of triplets
{(yi, xi,1, xi,2)}, where yi is always +1, xi,1 corre-
sponds to the correct word that is modified by the
preposition, and xi,2 corresponds to other words in
the sentence.
3 Results
3.1 Root-Node Finder
For the Root-Node Finder, we used a quadratic ker-
nel K(xi, xj) = (xi ? xj + 1)2 because it was better
than the linear kernel in preliminary experiments.
When we used the ?correct? POS tags given in the
Penn Treebank, and the ?correct? base NP tags given
by a tool provided by CoNLL 2000 shared task2,
RNF?s accuracy was 96.5% for section 23. When
we used Collins? POS tags and base NP tags based
on the POS tags, the accuracy slightly degraded to
95.7%. According to Yamada?s paper (Yamada and
2http://cnts.uia.ac.be/conll200/chunking/
Matsumoto, 2003), this root accuracy is better than
Charniak?s MEIP and Collins? Model 3 parser.
We also conducted an experiment to judge the ef-
fectiveness of the base NP chunker. Here, we used
only the first 10,000 sentences (about 1/4) of the
training data. When we used all features described
above and the POS tags given in Penn Treebank,
the root accuracy was 95.4%. When we removed
the base NP information (bi, Li, Ri), it dropped
to 94.9%. Therefore, the base NP information im-
proves RNF?s performance.
Figure 3 compares SVM and Preference Learn-
ing in terms of the root accuracy. We used the
first 10,000 sentences for training again. Accord-
ing to this graph, Preference Learning is better than
SVM, but the difference is small. (They are bet-
ter than Maximum Entropy Modeling3 that yielded
RA=91.5% for the same data.) C does not affect the
scores very much unless C is too small. In this ex-
periment, we used Penn?s ?correct? POS tags. When
we used Collins? POS tags, the scores dropped by
about one point.
3.2 Dependency Analyzer and PPAR
As for the dependency learning, we used the same
quadratic kernel again because the quadratic kernel
gives the best results according to Yamada?s experi-
ments. The soft margin parameter C is 1 following
Yamada?s experiment. We conducted an experiment
to judge the effectiveness of the Root-Node Finder.
We follow Yamada?s definition of accuracy that ex-
cludes punctuation marks.
Dependency Accuracy (DA) =
#correct parents / #words (= 49,892)
Complete Rate (CR) =
#completely parsed sentences / #sentences
According to Table 1, DA is only slightly improved,
but CR is more improved.
3http://www2.crl.go.jp/jt/a132/members/mutiyama/
software.html
SVM
Preference Learning
Accuracy (%)
C0.10.030.010.0030.0010.00030.0001
90
91
92
93
94
95
96 ?????
?
?
??????
?
Figure 3: Comparison of SVM and Preference
Learning in terms of Root Accuracy (Trained with
10,000 sentences)
DA RA CR
without RNF 89.4% 91.9% 34.7%
with RNF 89.6% 95.7% 35.7%
The
Dependency Analyzer was trained with 10,000
sentences. RNF was trained with all of the training data.
DA: Dependency Accuracy, RA: Root Acc., CR:
Complete Rate
Table 1: Effectiveness of the Root-Node Finder
Accuracy (%)
C
Preference Learning
SVM
0.1
0.03
0.01
0.003
0.001
0.0003
0.000170
72
74
76
78
80
82
?
??
?
?
?
?
?
?
?
?
?
????
?
?
?????
?
?
Figure 4: Comparison of SVM and Preference
Learning in terms of Dependency Accuracy of
prepositions (Trained with 5,000 sentences)
Figure 4 compares SVM and Preference Learning
in terms of the Dependency Accuracy of preposi-
tions. SVM?s performance is unstable for this task,
and Preference Learning outperforms SVM. (We
could not get scores of Maximum Entropy Model-
ing because of memory shortage.)
Table 2 shows the improvement given by PPAR.
Since training of PPAR takes a very long time, we
used only the first 35,000 sentences of the train-
ing data. We also calculated the Dependency Accu-
racy of Collins? Model 3 parser?s output for section
23. According to this table, PPAR is better than the
Model 3 parser.
Now, we use PPAR?s output for each preposition
instead of the dependency parser?s output unless the
modification makes the dependency tree into a non-
tree graph. Table 3 compares the proposed method
with other methods in terms of accuracy. This data
except ?Proposed? was cited from Yamada?s paper.
IN TO average
Collins Model 3 84.6% 87.3% 85.1%
Dependency Analyzer 83.4% 86.1% 83.8%
PPAR 85.3% 87.7% 85.7%
PPAR was trained with 35,000 sentences. The number
of IN words is 5,950 and that of TO is 1,240.
Table 2: PP-Attachment Resolver
DA RA CR
with MEIP 92.1% 95.2% 45.2%
phrase info. Collins Model3 91.5% 95.2% 43.3%
without Yamada 90.3% 91.6% 38.4%
phrase info. Proposed 91.2% 95.7% 40.7%
Table 3: Comparison with related work
According to this table, the proposed method is
close to the phrase structure parsers except Com-
plete Rate. Without PPAR, DA dropped to 90.9%
and CR dropped to 39.7%.
4 Discussion
We used Preference Learning to improve the SVM-
based Dependency Analyzer for root-node finding
and PP-attachment resolution. Preference Learn-
ing gave better scores than Collins? Model 3 parser
for these subproblems. Therefore, we expect that
our method is also applicable to phrase structure
parsers. It seems that root-node finding is relatively
easy and SVM worked well. However, PP attach-
ment is more difficult and SVM?s behavior was un-
stable whereas Preference Learning was more ro-
bust. We want to fully exploit Preference Learn-
ing for dependency analysis and parsing, but train-
ing takes too long. (Empirically, it takes O(`2) or
more.) Further study is needed to reduce the compu-
tational complexity. (Since we used Isozaki?s meth-
ods (Isozaki and Kazawa, 2002), the run-time com-
plexity is not a problem.)
Kudo and Matsumoto (2002) proposed an SVM-
based Dependency Analyzer for Japanese sen-
tences. Japanese word dependency is simpler be-
cause no word modifies a left word. Collins and
Duffy (2002) improved Collins? Model 2 parser
by reranking possible parse trees. Shen and Joshi
(2003) also used the preference kernel K(xi.?, xj.?)
for reranking. They compare parse trees, but our
system compares words.
5 Conclusions
Dependency analysis is useful and annotation of
word dependency seems easier than annotation of
phrase labels. However, lack of phrase labels makes
dependency analysis more difficult than phrase
structure parsing. In this paper, we improved a de-
terministic dependency analyzer by adding a Root-
Node Finder and a PP-Attachment Resolver. Pref-
erence Learning gave better scores than Collins?
Model 3 parser for these subproblems, and the per-
formance of the improved system is close to state-
of-the-art phrase structure parsers. It turned out
that SVM was unstable for PP attachment resolu-
tion whereas Preference Learning was not. We ex-
pect this method is also applicable to phrase struc-
ture parsers.
References
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 132?139.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels
over discrete structures, and the voted percep-
tron. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics
(ACL), pages 263?270.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics, pages 16?23.
Michael Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, Univ. of Pennsylvania.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines. Cam-
bridge University Press.
Jason M. Eisner. 1996. Three new probabilistic
models for dependency parsing: An exploration.
In Proceedings of the International Conference
on Computational Linguistics, pages 340?345.
Ralf Herbrich, Thore Graepel, Peter Bollmann-
Sdorra, and Klaus Obermayer. 1998. Learning
preference relations for information retrieval. In
Proceedings of ICML-98 Workshop on Text Cate-
gorization and Machine Learning, pages 80?84.
Ralf Herbrich, Thore Graepel, and Klaus Ober-
mayer, 2000. Large Margin Rank Boundaries for
Ordinal Regression, chapter 7, pages 115?132.
MIT Press.
Julia Hockenmaier and Mark Steedman. 2002.
Generative models for statistical parsing with
combinatory categorial grammar. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 335?342.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recog-
nition. In Proceedings of COLING-2002, pages
390?396.
Thorsten Joachims. 1999. Making large-scale
support vector machine learning practical. In
B. Scho?lkopf, C. J. C. Burges, and A. J. Smola,
editors, Advances in Kernel Methods, chapter 16,
pages 170?184. MIT Press.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the ACM Conference on Knowledge Discovery
and Data Mining.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
NAACL-2001, pages 192?199.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking.
In Proceedings of CoNLL, pages 63?69.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1993. Building a large annotated
corpus of english: the penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Adwait Ratnaparkhi. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing.
Libin Shen and Aravind K. Joshi. 2003. An SVM
based voting algorithm with application to parse
reranking. In Proceedings of the Seventh Confer-
ence on Natural Language Learning, pages 9?16.
Daniel Sleator and Davy Temperley. 1991. Parsing
English with a Link grammar. Technical Report
CMU-CS-91-196, Carnegie Mellon University.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern represen-
tation model for automatic IE pattern acquisi-
tion. In Proceedings of the Annual Meeting of the
Association for Cimputational Linguistics, pages
224?231.
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and
Eisaku Maeda. 2003. Hierarchical direct acyclic
graph kernel: Methods for structured natural lan-
guage data. In Proceedings of ACL-2003, pages
32?39.
Vladimir N. Vapnik. 1995. The Nature of Statisti-
cal Learning Theory. Springer.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Sta-
tistical dependency analysis. In Proceedings of
the International Workshop on Parsing Technolo-
gies, pages 195?206.
Dependency-based Sentence Alignment for Multiple Document
Summarization
Tsutomu HIRAO and Jun SUZUKI and Hideki ISOZAKI and Eisaku MAEDA
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
 
hirao,jun,isozaki,maeda  @cslab.kecl.ntt.co.jp
Abstract
In this paper, we describe a method of automatic
sentence alignment for building extracts from ab-
stracts in automatic summarization research. Our
method is based on two steps. First, we introduce
the ?dependency tree path? (DTP). Next, we calcu-
late the similarity between DTPs based on the ESK
(Extended String Subsequence Kernel), which con-
siders sequential patterns. By using these proce-
dures, we can derive one-to-many or many-to-one
correspondences among sentences. Experiments us-
ing different similarity measures show that DTP
consistently improves the alignment accuracy and
that ESK gives the best performance.
1 Introduction
Many researchers who study automatic summariza-
tion want to create systems that generate abstracts
of documents rather than extracts. We can gener-
ate an abstract by utilizing various methods, such
as sentence compaction, sentence combination, and
paraphrasing. In order to implement and evalu-
ate these techniques, we need large-scale corpora
in which the original sentences are aligned with
summary sentences. These corpora are useful for
training and evaluating sentence extraction systems.
However, it is costly to create these corpora.
Figure 1 shows an example of summary sentences
and original sentences from TSC-2 (Text Summa-
rization Challenge 2) multiple document summa-
rization data (Okumura et al, 2003). From this ex-
ample, we can see many-to-many correspondences.
For instance, summary sentence (A) consists of a
part of source sentence (A). Summary sentence (B)
consists of parts of source sentences (A), (B), and
(C). It is clear that the correspondence among the
sentences is very complex. Therefore, robust and
accurate alignment is essential.
In order to achieve such alignment, we need not
only syntactic information but also semantic infor-
mation. Therefore, we combine two methods. First,
we introduce the ?dependency tree path? (DTP) for
Source(A): 
	Corpus and Evaluation Measures for Multiple Document Summarization
with Multiple Sources
Tsutomu HIRAO
NTT Communication Science Laboratories
hirao@cslab.kecl.ntt.co.jp
Takahiro FUKUSIMA
Otemon Gakuin University
fukusima@res.otemon.ac.jp
Manabu OKUMURA
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Chikashi NOBATA
Communication Research Laboratories
nova@crl.go.jp
Hidetsugu NANBA
Hiroshima City University
nanba@its.hiroshima-cu.ac.jp
Abstract
In this paper, we introduce a large-scale test collec-
tion for multiple document summarization, the Text
Summarization Challenge 3 (TSC3) corpus. We
detail the corpus construction and evaluation mea-
sures. The significant feature of the corpus is that it
annotates not only the important sentences in a doc-
ument set, but also those among them that have the
same content. Moreover, we define new evaluation
metrics taking redundancy into account and discuss
the effectiveness of redundancy minimization.
1 Introduction
It has been said that we have too much informa-
tion on our hands, forcing us to read through a great
number of documents and extract relevant informa-
tion from them. With a view to coping with this situ-
ation, research on automatic text summarization has
attracted a lot of attention recently and there have
been many studies in this field. There is a particular
need to establish methods for the automatic sum-
marization of multiple documents rather than single
documents.
There have been several evaluation workshops
on text summarization. In 1998, TIPSTER SUM-
MAC (Mani et al, 2002) took place and the Doc-
ument Understanding Conference (DUC)1 has been
held annually since 2001. DUC has included multi-
ple document summarization among its tasks since
the first conference. The Text Summarization Chal-
lenge (TSC)2 has been held once in one and a half
years as part of the NTCIR (NII-NACSIS Test Col-
lection for IR Systems) project since 2001. Multiple
document summarization was included for the first
time as one of the tasks at TSC2 (in 2002) (Okumura
et al, 2003). Multiple document summarization is
now a central issue for text summarization research.
1http://duc.nist.gov
2http://www.lr.pi.titech.ac.jp/tsc
In this paper, we detail the corpus construction
and evaluation measures used at the Text Summa-
rization Challenge 3 (TSC3 hereafter), where multi-
ple document summarization is the main issue. We
also report the results of a preliminary experiment
on simple multiple document summarization sys-
tems.
2 TSC3 Corpus
2.1 Guidelines for Corpus Construction
Multiple document summarization from multiple
sources, i.e., several newspapers concerned with the
same topic but with different publishers, is more dif-
ficult than single document summarization since it
must deal with more text (in terms of numbers of
characters and sentences). Moreover, it is peculiar
to multiple document summarization that the sum-
marization system must decide how much redun-
dant information should be deleted3.
In a single document, there will be few sentences
with the same content. In contrast, in multiple doc-
uments with multiple sources, there will be many
sentences that convey the same content with differ-
ent words and phrases, or even identical sentences.
Thus, a text summarization system needs to recog-
nize such redundant sentences and reduce the redun-
dancy in the output summary.
However, we have no way of measuring the ef-
fectiveness of such redundancy in the corpora for
DUC and TSC2. Key data in TSC2 was given as
abstracts (free summaries) whose number of char-
acters was less than a fixed number and, thus, it
is difficult to use for repeated or automatic evalu-
ation, and for the extraction of important sentences.
Moreover, in DUC, where most of the key data were
abstracts whose number of words was less than a
3It is true that we need other important techniques such as
those for maintaining the consistency of words and phrases that
refer to the same object, and for making the results more read-
able; however, they are not included here.
fixed number, the situation was the same as TSC2.
At DUC 2002, extracts (important sentences) were
used, and this allowed us to evaluate sentence ex-
traction. However, it is not possible to measure the
effectiveness of redundant sentences reduction since
the corpus was not annotated to show sentence with
same content. In addition, this is the same even if
we use the SummBank corpus (Radev et al, 2003).
In any case, because many of the current summa-
rization systems for multiple documents are based
on sentence extraction, we believe these corpora to
be unsuitable as sets of documents for evaluation.
On this basis, in TSC3, we assumed that the pro-
cess of multiple document summarization consists
of the following three steps, and we produce a cor-
pus for the evaluation of the system at each of the
three steps4.
Step 1 Extract important sentences from a given set
of documents
Step 2 Minimize redundant sentences from the re-
sult of Step 1
Step 3 Rewrite the result of Step 2 to reduce the
size of the summary to the specified number of
characters or less.
We have annotated not only the important sen-
tences in the document set, but also those among
them that have the same content. These are the cor-
pora for steps 1 and 2. We have prepared human-
produced free summaries (abstracts) for step 3.
In TSC3, since we have key data (a set of cor-
rect important sentences) for steps 1 and 2, we con-
ducted automatic evaluation using a scoring pro-
gram. We adopted an intrinsic evaluation by human
judges for step 3, which is currently under evalu-
ation. We provide details of the extracts prepared
for steps 1 and 2 and their evaluation measures in
the following sections. We do not report the overall
evaluation results for TSC3.
2.2 Data Preparation for Sentence Extraction
We begin with guidelines for annotating important
sentences (extracts). We think that there are two
kinds of extract.
1. A set of sentences that human annotators
judge as being important in a document set
(Fukusima and Okumura, 2001; Zechner,
1996; Paice, 1990).
4This is based on general ideas of a summarization system
and is not intended to impose any conditions on a summariza-
tion system.
Mainichi articles
Yomiuri articles
abstract
(a)
(b)
(c)
(d)
Doc. x
Doc. y
Figure 1: An example of an abstract and its sources.
2. A set of sentences that are suitable as a source
for producing an abstract, i.e., a set of sen-
tences in the original documents that corre-
spond to the sentences in the abstracts(Kupiec
et al, 1995; Teufel and Moens, 1997; Marcu,
1999; Jing and McKeown, 1999).
When we consider how summaries are produced,
it seems more natural to identify important seg-
ments in the document set and then produce sum-
maries by combining and rephrasing such informa-
tion than to select important sentences and revise
them as summaries. Therefore, we believe that sec-
ond type of extract is superior and thus we prepared
the extracts in that way.
However, as stated in the previous section, with
multiple document summarization, there may be
more than one sentence with the same content, and
thus we may have more than one set of sentences
in the original document that corresponds to a given
sentence in the abstract; that is to say, there may be
more than one key datum for a given sentence in the
abstract5.
we have two sets of sentences that correspond to
sentence   in the abstract.
(1)  of document  , or
(2) a combination of  and  of document 	
This means that  alone is able to produce   , and
  can also be produced by combining   and   (Fig-
ure 1).
We marked all the sentences in the original doc-
uments that were suitable sources for producing the
sentences of the abstract, and this made it possible
for us to determine whether or not a summariza-
tion system deleted redundant sentences correctly
at Step 2. If the system outputs the sentences in
the original documents that are annotated as cor-
responding to the same sentence in the abstract, it
5We use ?set of sentences? since we often find that more
than one sentence corresponds to a sentence in the abstract.
Table 1: Important Sentence Data.
Sentence ID of Abstract Set of Corresponding Sentences
1 
 

2 
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 145?152, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Kernel-based Approach for Automatic Evaluation of Natural Language
Generation Technologies: Application to Automatic Summarization
Tsutomu Hirao
NTT Communication Science Labs.
NTT Corp.
hirao@cslab.kecl.ntt.co.jp
Manabu Okumura
Precision and Intelligence Labs.
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Hideki Isozaki
NTT Communication Science Labs.
NTT Corp.
isozaki@cslab.kecl.ntt.co.jp
Abstract
In order to promote the study of auto-
matic summarization and translation, we
need an accurate automatic evaluation
method that is close to human evalua-
tion. In this paper, we present an eval-
uation method that is based on convolu-
tion kernels that measure the similarities
between texts considering their substruc-
tures. We conducted an experiment us-
ing automatic summarization evaluation
data developed for Text Summarization
Challenge 3 (TSC-3). A comparison with
conventional techniques shows that our
method correlates more closely with hu-
man evaluations and is more robust.
1 Introduction
Automatic summarization, machine translation, and
paraphrasing have attracted much attention recently.
These tasks include text-to-text language genera-
tion. Evaluation workshops are held in the U.S.
and Japan, e.g., the Document Understanding Con-
ference (DUC)1, NIST Machine Translation Evalu-
ation2 as part of the TIDES project, the Text Sum-
marization Challenge (TSC)3 of the NTCIR project,
and the International Workshop on Spoken Lan-
guage Translation (IWSLT)4.
These evaluation workshops employ human eval-
uations, which are essential in terms of achieving
1http://duc.nist.gov
2http://www.nist.gov/speech/tests/mt/
3http://www.lr.titech.ac.jp/tsc
4http://www.slt.atr.co.jp/IWSLT2004
high quality evaluations results. However, human
evaluations require a huge effort and the cost is con-
siderable. Moreover, we cannot automatically eval-
uate a new system even if we use the corpora built
for these workshops, and we cannot conduct re-
evaluation experiments.
To cope with this situation, there is a particular
need to establish a high quality automatic evalua-
tion method. Once this is done, we can expect great
progress to be made on natural language generation.
In this paper, we propose a novel automatic
evaluation method for natural language generation
technologies. Our method is based on the Ex-
tended String Subsequence Kernel (ESK) (Hirao
et al, 2004b) which is a kind of convolution ker-
nel (Collins and Duffy, 2001). ESK allows us to
calculate the similarities between a pair of texts tak-
ing account of word sequences, their word sense se-
quences and their combinations.
We conducted an experimental evaluation using
automatic summarization evaluation data developed
for TSC-3 (Hirao et al, 2004a). The results of the
comparison with ROUGE-N (Lin and Hovy, 2003;
Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b;
Lin and Och, 2004) and ROUGE-L (Lin, 2004a;
Lin, 2004b) show that our method correlates more
closely with human evaluations and is more robust.
2 Related Work
Automatic evaluation methods for automatic sum-
marization and machine translation are grouped into
two classes. One is the longest common subse-
quence (LCS) based approach (Hori et al, 2003;
Lin, 2004a; Lin, 2004b; Lin and Och, 2004). The
other is the N-gram based approach (Papineni et al,
145
Table 1: Components of vectors corresponding to S1 and S2. Bold subsequences are common to S1 and S2.
 
subsequence S1 S2   subsequence S1 S2   subsequence S1 S2
Becoming 1 1 Becoming-is 



astronaut-DREAM 0 

DREAM 1 1 Becoming-my  astronaut-ambition 0 

SPACEMAN 1 1 SPACEMAN-DREAM 

astronaut-is 0 1
a 1 0 SPACEMAN-ambition 0 

astronaut-my 0 
ambition 0 1 SPACEMAN-dream   0 cosmonaut-DREAM   0
1
an 0 1 SPACEMAN-great 

0 cosmonaut-dream   0
astronaut 0 1 SPACEMAN-is 1 1 cosmonaut-great 

0
cosmonaut 1 0 SPACEMAN-my  cosmonaut-is 1 0
dream 1 0 a-DREAM 

0 cosmonaut-my  0
great 1 0 a-SPACEMAN 1 0 great-DREAM 1 0
is 1 1 2 a-cosmonaut 1 0 2 great-dream 1 0
my 1 1 a-dream 

0 is-DREAM 


Becoming-DREAM 

a-great   0 is-ambition 0 
Becoming-SPACEMAN  a-is  0 is-dream 

0
Becoming-a 1 0 a-my 

0 is-great  0
Becoming-ambition 0 

an-DREAM 0   is-my 1 1
2 Becoming-an 0 1 an-SPACEMAN 0 1 my-DREAM  1
Becoming-astronaut 0  an-ambition 0   my-ambition 0 1
Becoming-cosmonaut  0 an-astronaut 0 1 my-dream  0
Becoming-dream  0 an-is 0  my-great 1 0
Becoming-great 

0 an-my 0 

2002; Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b;
Soricut and Brill, 2004).
Hori et. al (2003) proposed an automatic eval-
uation method for speech summarization based on
word recognition accuracy. They reported that their
method is superior to BLEU (Papineni et al, 2002)
in terms of the correlation between human assess-
ment and automatic evaluation. Lin (2004a; 2004b)
and Lin and Och (2004) proposed an LCS-based au-
tomatic evaluation measure called ROUGE-L. They
applied ROUGE-L to the evaluation of summariza-
tion and machine translation. The results showed
that the LCS-based measure is comparable to N-
gram-based automatic evaluation methods. How-
ever, these methods tend to be strongly influenced
by word order.
Various N-gram-based methods have been pro-
posed since BLEU, which is now widely used for the
evaluation of machine translation. Lin et al (2003)
proposed a recall-oriented measure, ROUGE-N,
whereas BLEU is precision-oriented. They reported
that ROUGE-N performed well as regards automatic
summarization. In particular, ROUGE-1, i.e., uni-
gram matching, provides the best correlation with
human evaluation. Soricut et. al (2004) proposed
a unified measure. They integrated a precision-
oriented measure with a recall-oriented measure by
using an extension of the harmonic mean formula. It
performs well in evaluations of machine translation,
automatic summarization, and question answering.
However, N-gram based methods have a critical
problem; they cannot consider co-occurrences with
gaps, although the LCS-based method can deal with
them. Therefore, Lin and Och (2004) introduced
skip-bigram statistics for the evaluation of machine
translation. However, they did not consider longer
skip-n-grams such as skip-trigrams. Moreover, their
method does not distinguish between bigrams and
skip-bigrams.
3 Kernel-based Automatic Evaluation
The above N-gram-based methods correlated
closely with human evaluations. However, we
think some skip-n-grams (n 	
 ) are useful. In this
paper, we employ the Extended String Subsequence
Kernel (ESK), which considers both n-grams and
skip-n-grams. In addition, the ESK allows us to add
word senses to each word. The use of word senses
enables flexible matching even when paraphrasing
is used.
The ESK is a kind of convolution kernel (Collins
and Duffy, 2001). Convolution kernels have recently
attracted attention as a novel similarity measure in
natural language processing.
3.1 ESK
The ESK is an extension of the String Subsequence
Kernel (SSK) (Lodhi et al, 2002) and the Word Se-
quence Kernel (WSK) (Cancedda et al, 2003).
The ESK receives two node sequences as inputs
146
and maps each of them into a high-dimensional vec-
tor space. The kernel?s value is simply the inner
product of the two vectors in the vector space. In
order to discount long-skip-n-grams, the decay pa-
rameter  is introduced.
We explain the computation of the ESK?s value
whose inputs are the sentences (S1 and S2) shown
below. In the example, word senses are shown in
braces.
S1 Becoming a cosmonaut:  SPACEMAN  is my great
dream:  DREAM 
S2 Becoming an astronaut:  SPACEMAN  is my ambi-
tion:  DREAM 
In this case, ?cosmonaut? and ?astronaut? share
the same sense  SPACEMAN  and ?ambition? and
?dream? also share the same sense  DREAM  . We
can use WordNet for English and Goitaikei (Ikehara
et al, 1997) for Japanese.
Table 1 shows the subsequences derived from S1
and S2 and its weights. Note that the subsequence
length is two or less. From the table, there are fif-
teen subsequences5 that are common to S1 and S2.
Therefore, ffHierarchical Directed Acyclic Graph Kernel:
Methods for Structured Natural Language Data
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and Eisaku Maeda
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
  jun, hirao, sasaki, maeda  @cslab.kecl.ntt.co.jp
Abstract
This paper proposes the ?Hierarchical Di-
rected Acyclic Graph (HDAG) Kernel? for
structured natural language data. The
HDAG Kernel directly accepts several lev-
els of both chunks and their relations,
and then efficiently computes the weighed
sum of the number of common attribute
sequences of the HDAGs. We applied the
proposed method to question classifica-
tion and sentence alignment tasks to eval-
uate its performance as a similarity mea-
sure and a kernel function. The results
of the experiments demonstrate that the
HDAG Kernel is superior to other kernel
functions and baseline methods.
1 Introduction
As it has become easy to get structured corpora such
as annotated texts, many researchers have applied
statistical and machine learning techniques to NLP
tasks, thus the accuracies of basic NLP tools, such
as POS taggers, NP chunkers, named entities tag-
gers and dependency analyzers, have been improved
to the point that they can realize practical applica-
tions in NLP.
The motivation of this paper is to identify and
use richer information within texts that will improve
the performance of NLP applications; this is in con-
trast to using feature vectors constructed by a bag-
of-words (Salton et al, 1975).
We now are focusing on the methods that use nu-
merical feature vectors to represent the features of
natural language data. In this case, since the orig-
inal natural language data is symbolic, researchers
convert the symbolic data into numeric data. This
process, feature extraction, is ad-hoc in nature and
differs with each NLP task; there has been no neat
formulation for generating feature vectors from the
semantic and grammatical structures inside texts.
Kernel methods (Vapnik, 1995; Cristianini and
Shawe-Taylor, 2000) suitable for NLP have recently
been devised. Convolution Kernels (Haussler, 1999)
demonstrate how to build kernels over discrete struc-
tures such as strings, trees, and graphs. One of the
most remarkable properties of this kernel method-
ology is that it retains the original representation
of objects and algorithms manipulate the objects
simply by computing kernel functions from the in-
ner products between pairs of objects. This means
that we do not have to map texts to the feature
vectors by explicitly representing them, as long as
an efficient calculation for the inner products be-
tween a pair of texts is defined. The kernel method
is widely adopted in Machine Learning methods,
such as the Support Vector Machine (SVM) (Vap-
nik, 1995). In addition, kernel function 
	
has been described as a similarity function that
satisfies certain properties (Cristianini and Shawe-
Taylor, 2000). The similarity measure between texts
is one of the most important factors for some tasks in
the application areas of NLP such as Machine Trans-
lation, Text Categorization, Information Retrieval,
and Question Answering.
This paper proposes the Hierarchical Directed
Acyclic Graph (HDAG) Kernel. It can handle sev-
eral of the structures found within texts and can cal-
culate the similarity with regard to these structures
at practical cost and time. The HDAG Kernel can be
widely applied to learning, clustering and similarity
measures in NLP tasks.
The following sections define the HDAG Kernel
and introduce an algorithm that implements it. The
results of applying the HDAG Kernel to the tasks
of question classification and sentence alignment are
then discussed.
2 Convolution Kernels
Convolution Kernels were proposed as a concept of
kernels for a discrete structure. This framework de-
fines a kernel function between input objects by ap-
plying convolution ?sub-kernels? that are the kernels
for the decompositions (parts) of the objects.
Let  be a positive integer and 


be nonempty, separable metric spaces. This paper
focuses on the special case that 




are
countable sets. We start with ffProceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826?833,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Syntax-Free Approach to Japanese Sentence Compression
Tsutomu HIRAO, Jun SUZUKI and Hideki ISOZAKI
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan
{hirao,jun,isozaki}@cslab.kecl.ntt.co.jp
Abstract
Conventional sentence compression meth-
ods employ a syntactic parser to compress
a sentence without changing its mean-
ing. However, the reference compres-
sions made by humans do not always re-
tain the syntactic structures of the original
sentences. Moreover, for the goal of on-
demand sentence compression, the time
spent in the parsing stage is not negligi-
ble. As an alternative to syntactic pars-
ing, we propose a novel term weighting
technique based on the positional infor-
mation within the original sentence and
a novel language model that combines
statistics from the original sentence and a
general corpus. Experiments that involve
both human subjective evaluations and au-
tomatic evaluations show that our method
outperforms Hori?s method, a state-of-the-
art conventional technique. Because our
method does not use a syntactic parser, it
is 4.3 times faster than Hori?s method.
1 Introduction
In order to compress a sentence while retaining
its original meaning, the subject-predicate rela-
tionship of the original sentence should be pre-
served after compression. In accordance with this
idea, conventional sentence compression methods
employ syntactic parsers. English sentences are
usually analyzed by a full parser to make parse
trees, and the trees are then trimmed (Knight and
Marcu, 2002; Turner and Charniak, 2005; Unno
et al, 2006). For Japanese, dependency trees are
trimmed instead of full parse trees (Takeuchi and
Matsumoto, 2001; Oguro et al, 2002; Nomoto,
2008)1 This parsing approach is reasonable be-
cause the compressed output is grammatical if the
1Hereafter, we refer these compression processes as ?tree
trimming.?
input is grammatical, but it offers only moderate
compression rates.
An alternative to the tree trimming approach
is the sequence-oriented approach (McDonald,
2006; Nomoto, 2007; Clarke and Lapata, 2006;
Hori and Furui, 2003). It treats a sentence as a se-
quence of words and structural information, such
as a syntactic or dependency tree, is encoded in
the sequence as features. Their methods have the
potential to drop arbitrary words from the original
sentence without considering the boundary deter-
mined by the tree structures. However, they still
rely on syntactic information derived from fully
parsed syntactic or dependency trees.
We found that humans usually ignored the syn-
tactic structures when compressing sentences. For
example, in many cases, they compressed the sen-
tence by dropping intermediate nodes of the syn-
tactic tree derived from the source sentence. We
believe that making compression strongly depen-
dent on syntax is not appropriate for reproducing
reference compressions. Moreover, on-demand
sentence compression is made problematic by the
time spent in the parsing stage.
This paper proposes a syntax-free sequence-
oriented sentence compression method. To main-
tain the subject-predicate relationship in the com-
pressed sentence and retain fluency without us-
ing syntactic parsers, we propose two novel fea-
tures: intra-sentence positional term weighting
(IPTW) and the patched language model (PLM).
IPTW is defined by the term?s positional informa-
tion in the original sentence. PLM is a form of
summarization-oriented fluency statistics derived
from the original sentence and the general lan-
guage model. The weight parameters for these
features are optimized within the Minimum Clas-
sification Error (MCE) (Juang and Katagiri, 1992)
learning framework.
Experiments that utilize both human subjective
and automatic evaluations show that our method is
826
????? ?
?? ? ? ? ??
?? ?? ?
Source Sentence
??? ?
????? ????
??????? ?
Chunk 1
Chunk 2 Chunk 3
Chunk 4
Chunk 5
Chunk 6
Chunk 7
Compressed Sentence
Chunk7 = a part of Chunk6 + parts of Chunk4
????? ?? ?
suitei shi ta
haiten nitsuite fukutake ga
edamonbubun no
kouhyou shi te nai
center shiken de
??? ?
????? ????
Chunk 1
Chunk 2 Chunk 3
suitei shi ta
haiten nitsuite fukutake ga
center shiken
edamon no
edamon no
center shiken
Compression
compound noun
i
Figure 1: An example of the dependency relation between an original sentence and its compressed
variant.
superior to conventional sequence-oriented meth-
ods that employ syntactic parsers while being
about 4.3 times faster.
2 Analysis of reference compressions
Syntactic information does not always yield im-
proved compression performance because humans
usually ignore the syntactic structures when they
compress sentences. Figure 1 shows an exam-
ple. English translation of the source sentence is
?Fukutake Publishing Co., Ltd. presumed prefer-
ential treatment with regard to its assessed scores
for a part of the questions for a series of Center
Examinations.? and its compression is ?Fukutake
presumed preferential scores for questions for a
series of Center Examinations.?
In the figure, each box indicates a syntactic
chunk, bunsetsu. The solid arrows indicate de-
pendency relations between words2. We observe
that the dependency relations are changed by com-
pression; humans create compound nouns using
the components derived from different portions of
the original sentence without regard to syntactic
constraints. ?Chunk 7? in the compressed sen-
tence was constructed by dropping both content
and functional words and joining other content
words contained in ?Chunk 4? and ?Chunk 6? of
2Generally, a dependency relation is defined between bun-
setsu. Therefore, in order to identify word dependencies, we
followed Kudo?s rule (Kudo and Matsumoto, 2004)
the original sentence. ?Chunk 5? is dropped com-
pletely. This compression cannot be achieved by
tree trimming.
According to an investigation in our corpus of
manually compressed Japanese sentences, which
we used in the experimental evaluation, 98.7% of
them contain at least one segment that does not
retain the original tree structure. Human usually
compress sentences by dropping the intermediate
nodes in the dependency tree. However, the re-
sulting compressions retain both adequacy and flu-
ency. This statistic supports the view that sentence
compression that strongly depends on syntax is
not useful in reproducing reference compressions.
We need a sentence compression method that can
drop intermediate nodes in the syntactic tree ag-
gressively beyond the tree-scoped boundary.
In addition, sentence compression methods that
strongly depend on syntactic parsers have two
problems: ?parse error? and ?decoding speed.?
44% of sentences output by a state-of-the-art
Japanese dependency parser contain at least one
error (Kudo and Matsumoto, 2005). Even more, it
is well known that if we parse a sentence whose
source is different from the training data of the
parser, the performance could be much worse.
This critically degrades the overall performance
of sentence compression. Moreover, summariza-
tion systems often have to process megabytes of
documents. Parsers are still slow and users of on-
827
demand summarization systems are not prepared
to wait for parsing to finish.
3 A Syntax Free Sequence-oriented
Sentence Compression Method
As an alternative to syntactic parsing, we pro-
pose two novel features, intra-sentence positional
term weighting (IPTW) and the patched language
model (PLM) for our syntax-free sentence com-
pressor.
3.1 Sentence Compression as a
Combinatorial Optimization Problem
Suppose that a compression system reads sen-
tence x= x1 , x2, . . . , xj , . . . , xN , where xj
is the j-th word in the input sentence. The
system then outputs the compressed sentence y
=y1, y2, . . . , yi, . . . , yM , where yi is the i-
th word in the output sentence. Here, yi ?
{x1, . . . , xN}. We assume y0=x0=<s> (BOS)
and yM+1=xN+1=</s> (EOS). We define func-
tion I(?), which maps word yi to the index of
the word in the original sentence. For example,
if source sentence is x = x1, x2, . . . , x5 and its
compressed variant is y = x1, x3, x4, I(y1) = 1,
I(y2) = 3, I(y3) = 4.
We define a significance score f(x, y,?) for
compressed sentence y based on Hori?s method
(Hori and Furui, 2003). ? = {?g, ?h} is a pa-
rameter vector.
f(x, y;?) =
M+1
?
i=1
{g(x, I(yi);?g) +
h(x, I(yi), I(yi?1);?h)} (1)
The first term of equation (1) (g(?)) is the impor-
tance of each word in the output sentence, and the
second term (h(?)) is the the linguistic likelihood
between adjacent words in the output sentence.
The best subsequence y?=argmax
y
f(x, y;?) is
identified by dynamic programming (DP) (Hori
and Furui, 2003).
3.2 Features
We use IPTW to define the significance score
g(x, I(yi);?g). Moreover, we use PLM to define
the linguistic likelihood h(x, I(yi+1), I(yi);?h).
3.2.1 Intra-sentence Positional Term
Weighting (IPTW)
IDF is a global term weighting scheme in that it
measures the significance score of a word in a
text corpus, which could be extremely large. By
contrast, this paper proposes another type of term
weighting; it measures the positional significance
score of a word within its sentence. Here, we as-
sume the following hypothesis:
? The ?significance? of a word depends on its
position within its sentence.
In Japanese, the main subject of a sentence
usually appears at the beginning of the sentence
(BOS) and the main verb phrase almost always
appears at the end of the sentence (EOS). These
words or phrases are usually more important than
the other words in the sentence. In order to
add this knowledge to the scoring function, term
weight is modeled by the following Gaussian mix-
ture.
N(psn(x, I(yi));?g) =
m1
1
?
2??1
exp
(
?
1
2
(
psn(x, I(yi)) ? ?1
?1
)2
)
+
m2
1
?
2??2
exp
(
?
1
2
(
psn(x, I(yi)) ? ?2
?2
)2
)
(2)
Here, ?g = {?k, ?k, mk}k=1,2. psn(x, I(yi))
returns the relative position of yi in the original
sentence x which is defined as follows:
psn(x, I(yi)) =
start(x, I(yi))
length(x)
(3)
?length(x)? denotes the number of characters in
the source sentence and ?start(x, I(yi))? denotes
the accumulated run of characters from BOS to
(x, I(yi)). In equation (2), ?k,?k indicates the
mean and the standard deviation for the normal
distribution, respectively. mk is a mixture param-
eter.
We use the distribution (2) in defining
g(x, I(yi);?g) as follows:
g(x, I(yi);?g) =
?
?
?
?
?
?
?
IDF(x, I(yi)) ? N(psn(x, I(yi);?g)
if pos(x,I(yi)) = noun, verb, adjective
Constant ? N(psn(x, I(yi);?g)
otherwise
(4)
828
Here, pos(x, I(yi)) denotes the part-of-speech tag
for yi. ?g is optimized by using the MCE learning
framework.
3.2.2 Patched Language Model
Many studies on sentence compression employ the
n-gram language model to evaluate the linguistic
likelihood of a compressed sentence. However,
this model is usually computed by using a huge
volume of text data that contains both short and
long sentences. N-gram distribution of short sen-
tences may different from that of long sentences.
Therefore, the n-gram probability sometimes dis-
agrees with our intuition in terms of sentence com-
pression. Moreover, we cannot obtain a huge
corpus consisting solely of compressed sentences.
Even if we collect headlines as a kind of com-
pressed sentence from newspaper articles, corpus
size is still too small. Therefore, we propose
the following novel linguistic likelihood based on
statistics derived from the original sentences and a
huge corpus:
PLM(x, I(yj), I(yj?1)) =
?
?
?
1 if I(yj) = I(yj?1) + 1
?PLM Bigram(x, I(yj), I(yj?1))
otherwise
(5)
PLM stands for Patched Language Model.
Here, 0 ? ?PLM ? 1, Bigram(?) indicates word
bigram probability. The first line of equation (5)
agrees with Jing?s observation on sentence align-
ment tasks (Jing and McKeown, 1999); that is,
most (or almost all) bigrams in a compressed sen-
tence appear in the original sentence as they are.
3.2.3 POS bigram
Since POS bigrams are useful for rejecting un-
grammatical sentences, we adopt them as follows:
Ppos(x, I(yi+1)|I(yi)) =
P (pos(x, I(yi+1))|pos(x, I(yi))). (6)
Finally, the linguistic likelihood between adja-
cent words within y is defined as follows:
h(x, I(yi+1), I(yi);?h) =
PLM(x, I(yi+1), I(yi)) +
?(pos(x,I(y
i+1
))|pos(x,I(y
i
)))Ppos(x, I(yi+1)|I(yi))
3.3 Parameter Optimization
We can regard sentence compression as a two class
problem: we give a word in the original sentence
class label +1 (the word is used in the compressed
output) or ?1 (the word is not used). In order to
consider the interdependence of words, we employ
the Minimum Classification Error (MCE) learning
framework (Juang and Katagiri, 1992), which was
proposed for learning the goodness of a sequence.
xt denotes the t-th original sentence in the training
data set T . y?t denotes the reference compression
that is made by humans and y?t is a compressed
sentence output by a system.
When using the MCE framework, the misclas-
sification measure is defined as the difference be-
tween the score of the reference sentence and that
of the best non-reference output and we optimize
the parameters by minimizing the measure.
d(y, x;?) = {
|T |
?
t=1
f(xt, y?t ;?)
? max
y?
t
6=y?
t
f(xt, y?t;?)} (7)
It is impossible to minimize equation (7) because
we cannot derive the gradient of the function.
Therefore, we employ the following sigmoid func-
tion to smooth this measure.
L(d(x, y;?)) =
|T |
?
t=1
1
1 + exp(?c ? d(xt, yt;?))
(8)
Here, c is a constant parameter. To minimize equa-
tion (8), we use the following equation.
?L=
?L
?d
(
?d
??1
,
?d
??2
, . . .
)
=0 (9)
Here, ?L?d is given by:
?L
?d
=
c
1 + exp (?c ? d)
(
1 ?
1
1 + exp (?c ? d)
)
(10)
Finally, the parameters are optimized by using
the iterative form. For example, ?w is optimized
as follows:
?w(new) = ?w(old) ? 
?L
??w(old)
(11)
829
Our parameter optimization procedure can be
replaced by another one such as MIRA (McDon-
ald et al, 2005) or CRFs (Lafferty et al, 2001).
The reason why we employed MCE is that it is
very easy to implement.
4 Experimental Evaluation
4.1 Corpus and Evaluation Measures
We randomly selected 1,000 lead sentences (a lead
sentence is the first sentence of an article exclud-
ing the headline.) whose length (number of words)
was greater than 30 words from the Mainichi
Newspaper from 1994 to 2002. There were five
different ideal compressions (reference compres-
sions produced by human) for each sentence; all
had a 0.6 compression rate. The average length of
the input sentences was about 42 words and that of
the reference compressions was about 24 words.
For MCE learning, we selected the reference
compression that maximize the BLEU score (Pap-
ineni et al, 2002) (= argmaxr?RBLEU(r, R\r))
from the set of reference compressions and used it
as correct data for training. Note that r is a ref-
erence compression and R is the set of reference
compressions.
We employed both automatic evaluation and hu-
man subjective evaluation. For automatic evalua-
tion, we employed BLEU (Papineni et al, 2002)
by following (Unno et al, 2006). We utilized 5-
fold cross validation, i.e., we broke the whole data
set into five blocks and used four of them for train-
ing and the remainder for testing and repeated the
evaluation on the test data five times changing the
test block each time.
We also employed human subjective evaluation,
i.e., we presented the compressed sentences to six
human subjects and asked them to evaluate the
sentence for fluency and importance on a scale 1
(worst) to 5 (best). For each source sentence, the
order in which the compressed sentences were pre-
sented was random.
4.2 Comparison of Sentence Compression
Methods
In order to investigate the effectiveness of the pro-
posed features, we compared our method against
Hori?s model (Hori and Furui, 2003), which is
a state-of-the-art Japanese sentence compressor
based on the sequence-oriented approach.
Table 1 shows the feature set used in our exper-
iment. Note that ?Hori?? indicates the earlier ver-
Table 1: Configuration setup
Label g() h()
Proposed IPTW PLM + POS
w/o PLM IPTW Bigram+POS
w/o IPTW IDF PLM+POS
Hori? IDF Trigram
Proposed+Dep IPTW PLM + POS +Dep
w/o PLM+Dep IPTW Bigram+POS+Dep
w/o IPTW+Dep IDF PLM+POS+Dep
Hori IDF Trigram+Dep
Table 2: Results: automatic evaluation
Label BLEU
Proposed .679
w/o PLM .617
w/o IPTW .635
Hori? .493
Proposed+Dep .632
w/o PLM+Dep .669
w/o IPTW+Dep .656
Hori .600
sion of Hori?s method which does not require the
dependency parser. For example, label ?w/o IPTW
+ Dep? employs IDF term weighting as function
g(?) and word bigram, part-of-speech bigram and
dependency probability between words as func-
tion h(?) in equation (1).
To obtain the word dependency probability, we
use Kudo?s relative-CaboCha (Kudo and Mat-
sumoto, 2005). We developed the n-gram lan-
guage model from a 9 year set of Mainichi News-
paper articles. We optimized the parameters by
using the MCE learning framework.
5 Results and Discussion
5.1 Results: automatic evaluation
Table 2 shows the evaluation results yielded by
BLUE at the compression rate of 0.60.
Without introducing dependency probability,
both IPTW and PLM worked well. Our method
achieved the highest BLEU score. Compared to
?Proposed?, ?w/o IPTW? offers significantly worse
performance. The results support the view that our
hypothesis, namely that the significance score of
a word depends on its position within a sentence,
is effective for sentence compression. Figure 2
shows an example of Gaussian mixture with pre-
830
00.05
0.1
0.15
0.2
0 N/4 N/2 3N/4 N
x1, x2, ,xj, ,xN<S> </S>
x
Figure 2: An example of Gaussian mixture with
predicted parameters
dicted parameters. From the figure, we can see
that the positional weights for words have peaks
at BOS and EOS. This is because, in many cases,
the subject appears at the beginning of Japanese
sentences and the predicate at the end.
Replacing PLM with the bigram language
model (w/o PLM) degrades the performance sig-
nificantly. This result shows that the n-gram lan-
guage model is improper for sentence compres-
sion because the n-gram probability is computed
by using a corpus that includes both short and long
sentences. Most bigrams in a compressed sentence
followed those in the source sentence.
The dependency probability is very helpful pro-
vided either IPTW or PLM is employed. For ex-
ample, ?w/o PLM + Dep? achieved the second
highest BLEU score. The difference of the score
between ?Proposed? and ?w/o PLM + Dep? is only
0.01 but there were significant differences as de-
termined by Wilcoxon signed rank test. Compared
to ?Hori??, ?Hori? achieved a significantly higher
BLEU score.
The introduction of both IPTW and PLM makes
the use of dependency probability unnecessary. In
fact, the score of ?Proposed + Dep? is not good.
We believe that this is due to overfitting. PLM
is similar to dependency probability in that both
features emphasize word pairs that occurred as
bigrams in the source sentence. Therefore, by
introducing dependency probability, the informa-
tion within the feature vector is not increased even
though the number of features is increased.
Table 3: Results: human subjective evaluations
Label Fluency Importance
Proposed 4.05 (?0.846) 3.33 (?0.854)
w/o PLM + Dep 3.91 (?0.759) 3.24 (?0.753)
Hori? 3.09 (?0.899) 2.34 (?0.696)
Hori 3.28 (?0.924) 2.64 (?0.819)
Human 4.86 (?0.268) 4.66 (?0.317)
5.2 Results: human subjective evaluation
We used human subjective evaluations to compare
our method to human compression, ?w/o PLM +
Dep? which achieved the second highest perfor-
mance in the automatic evaluation, ?Hori?? and
?Hori?. We randomly selected 100 sentences from
the test corpus and evaluated their compressed
variants in terms of ?fluency? and ?importance.?
Table 3 shows the results, mean score of all
judgements as well as the standard deviation.
The results indicate that human compression
achieved the best score in both fluency and impor-
tance. Human compression significantly outper-
formed other compression methods. This results
supports the idea that humans can easily compress
sentences with the compression rate of 0.6. Of
the automatic methods, our method achieved the
best score in both fluency and importance while
?Hori?? was the worst performer. Our method sig-
nificantly outperformed both ?Hori? and ?Hori??
on both metrics. Moreover, our method outper-
formed ?w/o PLM + Dep? again. However, the
differences in the scores are not significant. We
believe that this is due to a lack of data. If we use
more data for the significant test, significant dif-
ferences will be found. Although our method does
not employ any explicit syntactic information, its
fluency and importance are extremely good. This
confirms the effectiveness of the new features of
IPTW and PLM.
5.3 Comparison of decoding speed
We compare the decoding speed of our method
against that of Hori?s method.
We measured the decoding time for all 1,000
test sentences on a standard Linux Box (CPU:
Intel c? CoreTM 2 Extreme QX9650 (3.00GHz),
Memory: 8G Bytes). The results were as follows:
Proposed: 22.14 seconds
(45.2 sentences / sec),
831
Hori: 95.34 seconds
(10.5 sentences / sec).
Our method was about 4.3 times faster than
Hori?s method due to the latter?s use of depen-
dency parser. This speed advantage is significant
when on-demand sentence compression is needed.
6 Related work
Conventional sentence compression methods em-
ploy the tree trimming approach to compress a
sentence without changing its meaning. For in-
stance, most English sentence compression meth-
ods make full parse trees and trim them by ap-
plying the generative model (Knight and Marcu,
2002; Turner and Charniak, 2005), discrimina-
tive model (Knight and Marcu, 2002; Unno et
al., 2006). For Japanese sentences, instead of us-
ing full parse trees, existing sentence compression
methods trim dependency trees by the discrim-
inative model (Takeuchi and Matsumoto, 2001;
Nomoto, 2008) through the use of simple lin-
ear combined features (Oguro et al, 2002). The
tree trimming approach guarantees that the com-
pressed sentence is grammatical if the source sen-
tence does not trigger parsing error. However, as
we mentioned in Section 2, the tree trimming ap-
proach is not suitable for Japanese sentence com-
pression because in many cases it cannot repro-
duce human-produced compressions.
As an alternative to these tree trimming
approaches, sequence-oriented approaches have
been proposed (McDonald, 2006; Nomoto, 2007;
Hori and Furui, 2003; Clarke and Lapata, 2006).
Nomoto (2007) and McDonald (2006) employed
the random field based approach. Hori et al
(2003) and Clarke et al (2006) employed the lin-
ear model with simple combined features. They
simply regard a sentence as a word sequence and
structural information, such as full parse tree or
dependency trees, are encoded in the sequence as
features. The advantage of these methods over the
tree trimming approach is that they have the poten-
tial to drop arbitrary words from the original sen-
tence without the need to consider the boundaries
determined by the tree structures. This approach is
more suitable for Japanese compression than tree
trimming. However, they still rely on syntactic
information derived from full parsed trees or de-
pendency trees. Moreover, their use of syntactic
parsers seriously degrades the decoding speed.
7 Conclusions
We proposed a syntax free sequence-oriented
Japanese sentence compression method with two
novel features: IPTW and PLM. Our method
needs only a POS tagger. It is significantly supe-
rior to the methods that employ syntactic parsers.
An experiment on a Japanese news corpus re-
vealed the effectiveness of the new features. Al-
though the proposed method does not employ any
explicit syntactic information, it outperformed,
with statistical significance, Hori?s method a state-
of-the-art Japanese sentence compression method
based on the sequence-oriented approach.
The contributions of this paper are as follows:
? We revealed that in compressing Japanese
sentences, humans usually ignore syntactic
structures; they drop intermediate nodes of
the dependency tree and drop words within
bunsetsu,
? As an alternative to the syntactic parser, we
proposed two novel features, Intra-sentence
positional term weighting (IPTW) and the
Patched language model (PLM), and showed
their effectiveness by conducting automatic
and human evaluations,
? We showed that our method is about 4.3 times
faster than Hori?s method which employs a
dependency parser.
References
J. Clarke and M. Lapata. 2006. Models for sentence
compression: A comparison across domains, train-
ing requirements and evaluation measures. In Proc.
of the 21st COLING and 44th ACL, pages 377?384.
C. Hori and S. Furui. 2003. A new approach to auto-
matic speech summarization. IEEE trans. on Multi-
media, 5(3):368?378.
H. Jing and K. McKeown. 1999. The Decomposition
of Human-Written Summary Sentences. In Proc. of
the 22nd SIGIR, pages 129?136.
B. H. Juang and S. Katagiri. 1992. Discriminative
Learning for Minimum Error Classification. IEEE
Trans. on Signal Processing, 40(12):3043?3053.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction. Artificial Intelligence,
139(1):91?107.
832
T. Kudo and Y. Matsumoto. 2004. A Boosting Algo-
rithm for Classification of Semi-Structured Text. In
Proc. of the EMNLP, pages 301?308.
T. Kudo and Y. Matsumoto. 2005. Japanese De-
pendency Parsing Using Relative Preference of De-
pendency (in japanese). IPSJ Journal, 46(4):1082?
1092.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
the 18th ICML, pages 282?289.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line Large Margrin Training of Dependency Parser.
In Proc. of the 43rd ACL, pages 91?98.
R. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proc. of
the 11th EACL, pages 297?304.
T. Nomoto. 2007. Discriminative sentence compres-
sion with conditional random fields. Information
Processing and Management, 43(6):1571?1587.
T. Nomoto. 2008. A generic sentence trimmer with
crfs. In Proc. of the ACL-08: HLT, pages 299?307.
R. Oguro, H. Sekiya, Y. Morooka, K. Takagi, and
K. Ozeki. 2002. Evaluation of a japanese sentence
compression method based on phrase significance
and inter-phrase dependency. In Proc. of the TSD
2002, pages 27?32.
K. Papineni, S. Roukos, T. Ward, and W-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistic (ACL),
pages 311?318.
K. Takeuchi and Y. Matsumoto. 2001. Acquisition
of sentence reduction rules for improving quality of
text summaries. In Proc. of the 6th NLPRS, pages
447?452.
J. Turner and E. Charniak. 2005. Supervised and un-
supervised learning for sentence compression. In
Proc. of the 43rd ACL, pages 290?297.
Y. Unno, T. Ninomiya, Y. Miyao, and J. Tsujii. 2006.
Trimming cfg parse trees for sentence compression
using machine learning approach. In Proc. of the
21st COLING and 44th ACL, pages 850?857.
833
Japanese Zero Pronoun Resolution based on
Ranking Rules and Machine Learning
Hideki Isozaki and Tsutomu Hirao
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
2-4 Hikaridai, Seika-cho, Souraku-gun, Kyoto, Japan, 619-0237
(isozaki,hirao)@cslab.kecl.ntt.co.jp
Abstract
Anaphora resolution is one of the most
important research topics in Natural Lan-
guage Processing. In English, overt pro-
nouns such as she and definite noun
phrases such as the company are anaphors
that refer to preceding entities (an-
tecedents). In Japanese, anaphors are of-
ten omitted, and these omissions are called
zero pronouns. There are two major ap-
proaches to zero pronoun resolution: the
heuristic approach and the machine learn-
ing approach. Since we have to take var-
ious factors into consideration, it is diffi-
cult to find a good combination of heuris-
tic rules. Therefore, the machine learn-
ing approach is attractive, but it requires
a large amount of training data. In this
paper, we propose a method that com-
bines ranking rules and machine learning.
The ranking rules are simple and effective,
while machine learning can take more fac-
tors into account. From the results of our
experiments, this combination gives better
performance than either of the two previ-
ous approaches.
1 Introduction
Anaphora resolution is an important research topic
in Natural Language Processing. For instance,
machine translation systems should identify an-
tecedents of anaphors (such as he or she) in the
source language to achieve better translation quality
in the target language.
We are now studying open-domain question an-
swering systems1, and we expect QA systems to
benefit from anaphora resolution. Typical QA sys-
tems try to answer a user?s question by finding rel-
evant phrases from large corpora. When a correct
answer phrase is far from the keywords given in
the question, the systems will not succeed in find-
ing the answer. If the system can correctly resolve
anaphors, it will find keywords or answers repre-
sented by anaphors, and the chances of finding the
answer will increase. From this motivation, we are
developing our system toward the ability to resolve
anaphors in full-text newspaper articles.
In Japanese, anaphors are often omitted and these
omissions are called zero pronouns. Since they do
not give any hints (e.g., number or gender) about an-
tecedents, automatic zero pronoun resolution is dif-
ficult. In this paper, we focus on resolving the zero
pronoun, which is shortened for simplicity to ?zero.?
Most studies on Japanese zero pronoun resolution
have not tried to resolve zeros in full-text newspa-
per articles. They have discussed simple sentenses
(Kameyama, 1986; Walker et al, 1994; Yamura-
Takei et al, 2002), dialogues (Yamamoto et al,
1997), stereotypical lead sentences of newspaper ar-
ticles (Nakaiwa and Ikehara, 1993), intrasentential
resolution (Nakaiwa and Ikehara, 1996; Ehara and
Kim, 1996) or organization names in newspaper ar-
ticles (Aone and Bennett, 1995).
There are two approaches to the problem: the
heuristic approach and the machine learning ap-
1http://trec.nist.gov/data/qa.html
proach. The Centering Theory (Grosz et al, 1995)
is important in the heuristic approach. Walker
et al (1994) proposed forward center ranking for
Japanese. Kameyama (1986) emphasized the im-
portance of a property-sharing constraint. Okumura
and Tamura (1996) experimented on the roles of
conjunctive postpositions in complex sentences.
However, these improvements are not sufficient
for resolving zeros accurately. Murata and Na-
gao (1997) proposed complicated heuristic rules that
take various features of antecedents and anaphors
into account. We have to take even more factors into
account, but it is difficult to maintain such heuris-
tic rules. Therefore, recent studies employ machine
learning approaches. However, it is also difficult to
prepare a sufficient number of annotated corpora.
In this paper, we propose a method that com-
bines these two approaches. Heuristic ranking rules
give a general preference, while a machine learn-
ing method excludes inappropriate antecedent can-
didates. From the results of our experiments, the
proposed method shows better performance than ei-
ther of the two approaches alone.
Before giving a description of our methodology,
we briefly introduce the grammar of the Japanese
language here. A Japanese sentence is a sequence
of bunsetsus:
  
	
. A bunsetsu is a se-
quence of content words (e.g., nouns, adjectives,
and verbs) followed by zero or more functional
words (e.g., particles and auxiliary verbs):  







. A bunsetsu modifies one of
the following bunsetsus. A particle (joshi) marks the
grammatical case of the noun phrase immediately
before it. For example, ga is nominative (subject),
wo is accusative (object), ni is dative (object2), and
wa marks a topic.
Tomu ga
Tom=subj
/ Bobu ni
Bob=object2
/ hon wo
book=object
/ okutta.
sent
(Tom sent a book to Bob.)
Bunsetsu dependency is represented by a list of
bunsetsu pairs (modifier, modified). For instance,

ffEvaluation Measures Considering Sentence Concatenation for
Automatic Summarization by Sentence or Word Extraction
Chiori Hori, Tsutomu Hirao and Hideki Isozaki
NTT Communication Science Laboratories
{chiori, hirao, isozaki}@cslab.kecl.ntt.co.jp
Abstract
Automatic summaries of text generated through
sentence or word extraction has been evaluated by
comparing them with manual summaries generated
by humans by using numerical evaluation measures
based on precision or accuracy. Although sentence
extraction has previously been evaluated based only
on precision of a single sentence, sentence concate-
nations in the summaries should be evaluated as
well. We have evaluated the appropriateness of sen-
tence concatenations in summaries by using eval-
uation measures used for evaluating word concate-
nations in summaries through word extraction. We
determined that measures considering sentence con-
catenation much better reflect the human judgment
rather than those based only on the precision of a
single sentence.
1 Introduction
Summarization Target and Approach
The amount of text is explosively increasing day by
day, and it is becoming very difficult to manage in-
formation by reading all the text. To manage infor-
mation easily and find target information quickly,
we need technologies for summarizing text. Al-
though research into text summarization started in
the 1950?s, it is still largely in the research phase
(Mani and Maybury, 1999). Several projects on
text summarization have been carried out. 1 In
these project, text summarization has so far focused
on summarizing single documents through sentence
extraction. Recently, summarizing multiple docu-
ments with the same topic has been made a tar-
get. The major approach to extracting sentences that
have significant information is statistical, i.e., su-
pervised learning from parallel corpora consisting
of original texts and their summarization (Kupiec et
1SUMMAC in the Tipster project by DARPA (http://www-
nlpir.nist.gov/related projects/tipster summac) and DUC in
the TIDES project (http://duc.nist.gov/) in the U.S. TSC
(http://research.nii.ac.jp/ntcir/) in the NTCIR by NII (The Na-
tional Institute of Informatica) in Japan.
al., 1995) (Aone et al, 1998) (Mani and Bloedorn,
1998).
Several summarization techniques for multime-
dia including image, speech, and text have been re-
searched. Manually transcribed newswire speech
(TDT data) and meeting speech (Zechner, 2003)
have been set as summarization targets. The need
to automatically generate summaries from speech
has led to research on summarizing transcription re-
sults obtained by automatic speech recognition in-
stead of manually transcribed speech (Hori and Fu-
rui, 2000a). This summarization approach is word
extraction (sentence compaction) that attempts to
extract significant information, exclude acoustically
and linguistically unreliable words, and maintain
the meanings of the original speech.
The summarization approaches that have been
mainly researched so far are extracting sentences
or words from original text or transcribed speech.
There has also been research on generating an ?ab-
stract? like the much higher level summarization
composed freely by human experts (Jing, 2002).
This approach includes not only extracting sen-
tences but also combining sentences to generate new
sentences, replacing words, reconstructing syntactic
structure, and so on.
Evaluation Measures for Summarization
Metrics that can be used to accurately evaluate
the various appropriateness to summarization are
needed.The simplest and probably the ideal way of
evaluating automatic summarization is to have hu-
man subjects read the summaries and evaluate them
in terms of the appropriateness of summarization.
However, this type of evaluation is too expensive
for comparing the efficiencies of many different ap-
proaches precisely and repeatedly. We thus need au-
tomatic evaluation metrics to numerically validate
the efficiency of various approaches repeatedly and
consistently.
Automatic summaries can be evaluated by com-
paring them with manual summaries generated by
humans. The similarities between the targets and
the automatically processed results provide metrics
indicating the extent to which the task was accom-
plished. The similarity that can better reflect sub-
jective judgments is a better metric.
To create correct answers for automatic sum-
marization, humans generate manual summaries
through sentence or word extraction. However,
references consisting of manual summaries vary
among humans. The problems in validating auto-
matic summaries by comparing them with various
references are as follows:
? correct answers for automatic results cannot be
unified because of subjective variation,
? the coverage of correct answers in the collected
manual summaries is unknown, and
? the reliability of references in the collected
manual summaries is not always guaranteed.
When the similarity between automatic results
and references is used for the evaluation metrics,
the similarity determination function counts over-
lapping of each component or sequence of com-
ponents in the automatic results. If concatenations
between components in a summary had no mean-
ing, the overlap of a single component between the
automatic results and the references can represent
the extent of summarization. However, concatena-
tions between sentences or words have meanings,
so some concatenations of sentences or words in the
automatic summaries sometimes generate meanings
different from the original. The evaluation metrics
for summarization should thus consider each con-
catenation between components in the automatic re-
sults.
To evaluate sentence automatically generated
with taking consideration word concatenation into
by using references varied among humans, vari-
ous metrics using n-gram precision and word ac-
curacy have been proposed: word string preci-
sion (Hori and Furui, 2000b) for summarization
through word extraction, ROUGE (Lin and Hovy,
2003) for abstracts, and BLEU (Papineni et al,
2002) for machine translation. Evaluation metrics
based on word accuracy, summarization accuracy
(SumACCY), using a word network made by merg-
ing manual summaries has been proposed (Hori and
Furui, 2001). In addition, to solve the problems for
the coverage of correct answers and the reliability
of manual summaries as correct answers, weighted
summarization accuracy (WSumACCY) in which
SumACCY is weighted by the majority of the hu-
mans? selections, has been proposed (Hori and Fu-
rui, 2003a).
In contrast, summarization through sentence ex-
traction has been evaluated using only single sen-
tence precision. Sentence extraction should also be
evaluated using measures that take into account sen-
tence concatenations, the coverage of correct an-
swers, and the reliability of manual summaries.
This paper presents evaluation results of auto-
matic summarization through sentence or word ex-
traction using the above mentioned metrics based on
n-gram precision and sentence/word accuracy and
examines how well these measures reflect the judg-
ments of humans as well.
2 Evaluation Metrics for Extraction
In summarization through sentence or word extrac-
tion under a specific summarization ratio, the order
of the sentences or words and the length of the sum-
maries are restricted by the original documents or
sentences. Metrics based on the accuracy of the
components in the summary is a straight-forward
approach to measuring similarities between the tar-
get and automatic summaries.
2.1 Accuracy
In the field of speech recognition, automatic recog-
nition results are compared with manual transcrip-
tion results. The conventional metric for speech
recognition is recognition accuracy calculated based
on word accuracy:
ACCY
= Len ? (Sub + Ins + Del)Len ? 100[%], (1)
where Sub, Ins, Del, and Len are the numbers
of substitutions, insertions, deletions, and words in
the manual transcription, respectively. Although
word accuracy cannot be used to directly evaluate
the meanings of sentences, higher accuracy indi-
cates that more of the original information has been
preserved. Since the meaning of the original doc-
uments is generated by combining sentences, this
metric can be applied to the evaluation for sentence
extraction. Sentence accuracy defined by eq. (1)
with words replaced by sentences represents how
much the automatic result is similar to the answer
and how well it preserves the original meaning.
Accuracy is the simplest and most efficient metric
when the target for the automatic summaries can be
set as only one answer. However, there are usually
multiple targets for each automatic summary due to
the variation in manual summarization among hu-
mans. Therefore, it is not easy to use accuracy to
evaluate automatic summaries. Subjective variation
results into two problems:
? how to consider all possible correct answers in
the manual summaries, and
? how to measure the similarity between the
evaluation sentence and multiple manual sum-
maries.
If we could collect all possible manual sum-
maries, the one most similar to the automatic re-
sult could be chosen as the correct answer and used
for the evaluation. The sentence or word accuracy
compared with the most similar manual summary is
denoted as NrstACCY. However, in real situations,
the number of manual summaries that could be col-
lected is limited. The coverage of correct answers in
the collected manual summaries is unknown. When
the coverage is low, the summaries are compared
with inappropriate targets, and the NrstACCY ob-
tained by such comparison does not provide an effi-
cient measure.
2.2 N-gram Precision
One way to cope with the coverage problem is to
use local matching of components or component
strings with all the manual summaries instead of
using a measure comparing a word sequence as a
whole sentence, such as NrstACCY. The similar-
ity can be measured by counting the precision, i.e.,
the number of sentence or word n-gram overlapping
between the automatic result and all the references.
Even if there are multiple targets for an automatic
summary, the precision of components in each orig-
inal can be used to evaluate the similarity between
the automatic result and the multiple references.
Precision is an efficient way of evaluating the sim-
ilarity of component occurrence between automatic
results and targets with a different order of compo-
nents and different lengths.
In the evaluation of summarization through ex-
traction, a component occurring in a different loca-
tion in the original is considered to be a different
component even if it is the same component as one
in the result. When an answer for the automatic re-
sult can be unified and the lengths of the automatic
result and its answer are the same, accuracy counts
insertion errors and deletion errors and thus has both
the precision and recall characteristics.
Since meanings are basically conveyed by word
strings rather than single words, word string preci-
sion (Hori and Furui, 2000b) can be used to evalu-
ate linguistic precision and the maintenance of the
original meanings of an utterance. In this method,
word strings of various lengths, that is n-grams, are
used as components for measuring precision. The
extraction ratio, pn, of each word string consist-
ing of n words in a summarized sentence, V =
v1, v2, . . . , vM , is given by
pn =
M
?
m=n
?(vm?n+1, . . . , vm?1, vm)
M ? n + 1 , (2)
where
?(un) =
{ 1 if un ? Un
0 if un /? Un , (3)
un: each word string consisting of n words
Un: a set of word strings consisting of n words
in all manual summarizations.
When n is 1, pn corresponds to the precision of
each word, and when n is the same length as a
summarized sentence (n = M ), pn indicates the
precision of the summarized sentence itself.
2.3 Summarization Accuracy: SumACCY
Summarization accuracy (SumACCY) was pro-
posed to cope with the problem of correct answer
coverage and various references among humans
(Hori and Furui, 2001). To cover all possible correct
answers for summarization using a limited number
of manual summaries, all the manual summaries
are merged into a word network. In this evaluation
method, the word sequence in the network closest to
the evaluation word sequence is considered to be the
target answer. The word accuracy of the automatic
result is calculated in comparison with the target an-
swer extracted from the network.
Since summarization is processed by extracting
words from an original; the words cannot be re-
placed by other words, and the order of words can-
not be changed. Multiple manual summaries can
be combined into a network that represents the vari-
ations. Each set of words that could be extracted
from the network consists of words and word strings
occurring at least once in all the manual summaries.
The network made by the manual summaries can
be considered to represent all possible variations of
correct summaries.
SUB The beautiful cherry blossoms in Japan bloom in spring
A The cherry blossoms in Japan
B cherry blossoms in Japan bloom
C beautiful cherry bloom in spring
D beautiful cherry blossoms in spring
E The beautiful cherry blossoms bloom
Table 1: Example of manual summarization by sen-
tence compaction
<s> </s> The beautiful cherry blossoms inJapan bloomin spring
Figure 1: Word network made by merging manual
summaries
The sentence ?The beautiful cherry blossoms in
Japan bloom in spring.? is assumed to be manually
summarized as shown in Table 1. In this example,
five words are extracted from the nine words. There-
fore, the summarization ratio is 56%. The variations
of manual summaries are merged into a word net-
work, as shown in Fig. 1. We use <s> and </s>
as the beginning and ending symbols of a sentence.
Although ?Cherry blossoms bloom in spring? is not
among the manual answers in Table 1, this sentence,
which could be extracted from the network, is con-
sidered a correct answer.
When references consisting of manual sum-
maries cannot cover all possible answers and lack
the appropriate answer for an automatic summary,
SumACCY calculated using such a network is bet-
ter than NrstACCY for evaluating the automatic re-
sult. This evaluation method gives a penalty for
each word concatenation in the automatic results
that is excluded in the network, so it can be used
to evaluate the sentence-level appropriateness more
precisely than matching each word in all the refer-
ences.
2.4 Weighted SumACCY: WSumACCY
In SumACCY, all possible sets of words extracted
from the network of manually summarized sen-
tences are equally used as target answers. How-
ever, the set of words containing word strings se-
lected by many humans would presumably be better
and give more reliable answers. To obtain reliability
that reflects the majority of selections by humans,
the summarization accuracy is weighted by a pos-
terior probability based on the manual summariza-
tion network. The reliability of a sentence extracted
from the network is defined as the product of the
ratios of the number of subjects who selected each
word to the total number of subjects. The weighted
summarization accuracy is given by
WSumACCY
= P? (v1 . . . vM |R) ? SumACCY
P? (v?1 . . . v?M? |R)
, (4)
where P? (v1 . . . vM |R) is the reliability score of a
set of words v1 . . . vM in the manual summariza-
tion network, R, and M represents the total num-
ber of words in the target answer. The set of words
v?1 . . . v?M? represents the word sequence that maxi-
mizes the reliability score, P? (?|R), given by
P? (v1 . . . vM |R)
=
( M
?
m=2
C(vm?1, vm|R)
HR
)
1
M?1
, (5)
where vm is the m-th word in the sentence ex-
tracted from the network as the target answer, and
C(x, y|R) indicates the number of subjects who se-
lected the word connection of x and y. Here, ?word
connection? means an arc in the manual summariza-
tion network. HR is the number of subjects.
2.5 Evaluation Experiments
Newspaper articles and broadcast news speech were
automatically summarized through sentence extrac-
tion and word extraction respectively under the
given summarization ratio, which is the ratio of the
numbers of sentences or words in the summary to
that in the original.
The automatic summarization results were sub-
jectively evaluated by ten human subjects. The sub-
jects read these summaries and rated each one from
1 (incorrect) to 5 (perfect). The automatic sum-
maries were also evaluated by using the numerical
metrics SumACCY, WSumACCY, NrstACCY,
and n-gram precision (1 ? n ? 5) in compari-
son with reference summaries generated by humans.
The precisions of 1-gram, . . ., 5-gram are denoted
PREC1, . . ., PREC5. The numerical evaluation re-
sults were averaged over the number of automatic
summaries.
Note that the subjects who judged the automatic
summaries did not include anyone who generated
the references. To examine the similarity of the hu-
man judgments and that of the manual summaries,
the kappa statistics, ?, was calculated using eq. (A-
1) in the Appendix.
Finally, to examine how much the evaluation
measures reflected the human judgment, the correla-
tion coefficients between the human judgments and
the numerical evaluation results were calculated.
Sentence extraction
Sixty articles in Japanese newspaper published in
94, 95, and 98 were automatically summarized with
a 30% summarization ratio. Half the articles were
general news report (NEWS), and other half were
columns (EDIT).
The automatic summarization was performed us-
ing a Support Vector Machine (SVM) (Hirao et al,
2003), random extraction (RDM), the lead method
(LEAD) extracting sentences from the head of ar-
ticles. In comparison with these automatic sum-
maries, manual summaries (TSC) was also evalu-
ated.
These 4 types of summaries, SVM, RDM, LEAD,
and TSC were read and rated 1 to 5 by 10 humans.
The summaries were evaluated in terms of extrac-
tion of significance information (SIG), coherence
of sentences (COH), maintenance of original mean-
ings (SEM), and appropriateness of summary as a
whole (WHOLE).
To numerically evaluate the results using the ob-
jective metrics, 20 other human subjects gener-
ated manual summaries through sentence extrac-
tion. These manual summaries were set as the target
set for the automatic summaries.
Word extraction
Japanese TV news broadcasts aired in 1996 were
automatically recognized and summarized sentence
by sentence (Hori and Furui, 2003b). They con-
sisted of 50 utterances by a female announcer. The
out-of-vocabulary (OOV) rate for the 20k word vo-
cabulary was 2.5%, and the test-set perplexity was
54.5. Fifty utterances with word recognition accu-
racy above 90%, which was the average rate over the
50 utterances, were selected and used for the evalu-
ation. The summarization ratio was set to 40%.
Nine automatic summaries with various summa-
rization accuracies from 40% to 70% and a manual
summary (SUB) were selected as a test set. These
ten summaries for each utterance were judged in
terms of the appropriateness of the summary as a
whole (WHOLE).
To numerically evaluate the results using the ob-
jective metrics, 25 humans generated manual sum-
maries through word extraction. These manual
summaries were set as a target set for the automatic
summaries, and merged into a network. Note that a
set of 24 manual summaries made by other subjects
was used as the target for SUB.
2.6 Evaluation Results
Figures 2 and 3 show the correlation coefficients
between the judgments of the subjects and the nu-
merical evaluation results for EDIT and NEWS.
They show that the measures based on accuracy
much better reflected human judgments than those
of the n-gram precisions for evaluating SIG and
WHOLE for both EDIT and NEWS. On the other
hand, PREC2 better reflected the human judgments
for evaluating COH and SEM. These results show
that measures taking into account sentence concate-
nations better reflected human judgments than sin-
gle component precision. The precisions of longer
0
0.2
0.4
0.6
0.8
1.0
SIG COH SEM WHOLE
C
or
re
la
tio
n 
co
ef
fic
ie
nt
Evaluation point
SumACCY
WSumACCY
  NrstACCY
PREC1
PREC2
PREC3
PREC4
PREC5
Figure 2: Correlation coefficients between human
judgment and numerical evaluation results for EDIT
0
0.2
0.4
0.6
0.8
1.0
SIG COH SEM WHOLE
C
or
re
la
tio
n 
co
ef
fic
ie
nt
Evaluation point
SumACCY
WSumACCY
  NrstACCY
PREC1
PREC2
PREC3
PREC4
PREC5
Figure 3: Correlation coefficients between hu-
man judgment and numerical evaluation results for
NEWS
sentence strings (PREC3 to PREC5) didn?t reflect
the human judgments for all the conditions. These
results show that meanings of the original article can
maintain by the concatenations of only a few sen-
tences in summarization through sentence extrac-
tion.
Table 2 lists the kappa statistics for the manual
summaries and the human judgments for EDIT and
NEWS. The manual results varied among humans
DATA SUMMARIES ?
EDIT manual summaries 0.35
NEWS manual summaries 0.39
Table 2: Kappa statistics for manual summaries and
human judgments for sentence extraction.
and the similarity among humans was low. The
kappa statistics for NEWS is slightly higher than
that for EDIT. The difference of similarities among
manual summaries is due to the difference in struc-
tures of information in each article. Although the
articles in EDIT had a discourse structure, NEWS
had isolated and stereotyped information scattered
throughout the articles.
While the human judgments for NEWS were sim-
ilar, those for EDIT varied.The difficulty in evaluat-
ing COH and SEM in EDIT is due to the variation
in both manual summaries and human judgment.
Figure 4 shows the correlation coefficients be-
tween the judgments of the subjects and the numer-
ical evaluation results for summaries of broadcast
news speech through word extraction. Table 3 lists
0
0.2
0.4
0.6
0.8
1
WHOLE
C
or
re
la
tio
n 
co
ef
fic
ie
nt
s
Evaluation point
PREC1
PREC2
PREC3
PREC4
PREC5
SumACCY
WSumACC
NrstACCY  
Figure 4: Correlation coefficients between human
judgment and numerical evaluation results for sum-
maries through word extraction
the kappa statistics for the manual summaries and
the human judgments for summaries through word
extraction. In word extraction, the human judg-
DATA SUMMARIES ?
Broadcast news manual summaries 0.47
Table 3: Kappa statistics for manual summaries and
human judgments for word extraction
ments and the manual summaries were very similar
among the subjects.
As shown in figure 4, WSumACCY yielded the
best correlation to the human judgments. This
means that the correctness as a sentence and the
weight (that is how many subjects support the ex-
tracted phrases in summarized sentences) are im-
portant in summarization through word extraction.
In comparison with the results of sentence extrac-
tion in Figures 2 and 3, PREC1 effectively reflected
the human judgments for word extraction. Since in
the manual summarized sentences through word ex-
traction under the low summarization ratio, the sen-
tences were summarized based on significance word
extraction rather than syntactic structure mainte-
nance to generate grammatically correct sentences.
3 Conclusion
We have presented the results of evaluating
the appropriateness of the sentence concatena-
tions in summaries generated using SumACCY,
WSumACCY, NrstACCY and n-gram precision.
We found that the measures taking into account sen-
tence concatenation much better reflected the judg-
ments of humans than did the single sentence pre-
cision, so the concatenation of sentences in sum-
maries should be evaluated.
Although the human judgments and the man-
ual summaries for word extraction did not vary
much among the subjects, those for sentence extrac-
tion for single article summarization greatly varied
among the subjects. As a result, it is very difficult to
set correct answers for single article summarization
through sentence extraction.
Future works involves experiments to examine
the efficiency of each numerical measures in re-
sponse to the coverage of correct answers.
4 Acknowledgments
We thank NHK (Japan Broadcasting Corporation)
for providing the broadcast news database. We
also thank Prof. Sadaoki Furui at Tokyo Institute
of Technology for providing the summaries of the
broadcast news speech.
References
C. Aone, M. Okurowski, and J. Gorlinsky. 1998.
Trainable scalable summarization using robust
NLP and machine learning. In Proceedings ACL,
pages 62?66.
J. Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
T. Hirao, K. Takeuchi, H. Isozaki, Y. Sasaki, and
E. Maeda. 2003. SVM-based multi-document
summarization integrating sentence extraction
with bunsetsu elimination. IEICE Trans. Inf. &
Syst., E86-D(9):1702?1709.
C. Hori and S. Furui. 2000a. Automatic speech
summarization based on word significance and
linguistic likelihood. In Proceedings ICASSP,
volume 3, pages 1579?1582.
C. Hori and S. Furui. 2000b. Improvements in
automatic speech summarization and evaluation
methods. In Proceedings ICSLP, volume 4,
pages 326?329.
C. Hori and S. Furui. 2001. Advances in auto-
matic speech summarization. In Proceedings Eu-
rospeech, volume 3, pages 1771?1774.
C. Hori and S. Furui. 2003a. Evaluation methods
for automatic speech summarization. In Proceed-
ings Eurospeech, pages 2825?2828.
C. Hori and S. Furui. 2003b. A new approach to
automatic speech summarization. IEEE Transac-
tions on Multimedia, 3:368?378.
H. Jing. 2002. Using hidden markov modeling to
decompose human-written summaries. Compu-
tational Linguistics, 28(4):527?543.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. In Proceedings of
the 18th ACM-SIGIR, pages 68?73.
Chin-Yew Lin and E. H. Hovy. 2003. Auto-
matic evaluation of summaries using n-gram
co-occurrence statistics. In Proceedings HLT-
NAACL.
I. Mani and E. Bloedorn. 1998. Machine learning
of general and user-focused summarization. In
Proceedings of the 15th National Conference on
Artificial Intelligence, pages 821?826.
I. Mani and M. Maybury. 1999. Advances in Auto-
matic Text Summarization. The MIT Press.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings ACL.
K. Takeuchi and Y. Matsumoto. 2001. Relation be-
tween text structure and linguistic clues: An in-
vestigation on text structure of newspaper arti-
cles. Mathematical Linguistics, 22(8).
K. Zechner. 2003. Automatic summarization of
open-domain multiparty dialogues in diverse
genres. Computational Linguistics, 28(4):447?
485.
Appendix
? is given by
? = P (A) ? P (E)1 ? P (E) , (A-1)
where P (A) and P (E) are the probabilities of hu-
man agreement and chance agreement, respectively,
so ? is adjusted by the possibility of chance agree-
ment. This measure was used to assess agreement of
human selections for discourse segmentation (Car-
letta, 1996).
In this study, kappa was calculated using a table
of objects and categories (Takeuchi and Matsumoto,
2001). P (A) was calculated using
P (A) = 1N
N
?
i=1
Si, (A-2)
where N is the number of trials to select one class
among all classes, and Si is the probability that two
humans at least agree at the i-th selection:
Si =
m
?
j=1
nij C2
kC2
, (A-3)
where k and m are the number of subjects and
classes, respectively. When the task is sentence or
word extraction, the number of classes is two, i.e.,
extract/not extract. The numerator of eq. (A-3)
shows the sum of the combinations that two humans
at least agree for each class; nij is the number of hu-
mans who select the j-th class at the i-th selection.
P (E) is the probability of chance agreement by
at least two humans:
P (E) =
m
?
j=1
pj2, (A-4)
where pj is the probability of selecting the j-th class
given by
Pj =
N
?
i=1
nij
Nk , (A-5)
where the total number of humans who select the j-
th class for each trial is divided by the total number
of trials performed by all humans.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1648?1659, Dublin, Ireland, August 23-29 2014.
Learning to Generate Coherent Summary
with Discriminative Hidden Semi-Markov Model
Hitoshi Nishikawa
1
, Kazuho Arita
1
, Katsumi Tanaka
1
,
Tsutomu Hirao
2
, Toshiro Makino
1
and Yoshihiro Matsuo
1
Nippon Telegraph and Telephone Corporation
1
1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847 Japan
2
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{
nishikawa.hitoshi, arita.kazuho, tanaka.katsumi
hirao.tsutomu, makino.toshiro, matsuo.yoshihiro
}
@lab.ntt.co.jp
Abstract
In this paper we introduce a novel single-document summarization method based on a hidden
semi-Markov model. This model can naturally model single-document summarization as the
optimization problem of selecting the best sequence from among the sentences in the input doc-
ument under the given objective function and knapsack constraint. This advantage makes it
possible for sentence selection to take the coherence of the summary into account. In addition
our model can also incorporate sentence compression into the summarization process. To demon-
strate the effectiveness of our method, we conduct an experimental evaluation with a large-scale
corpus consisting of 12,748 pairs of a document and its reference. The results show that our
method significantly outperforms the competitive baselines in terms of ROUGE evaluation, and
the linguistic quality of summaries is also improved. Our method successfully mimicked the
reference summaries, about 20 percent of the summaries generated by our method were com-
pletely identical to their references. Moreover, we show that large-scale training samples are
quite effective for training a summarizer.
1 Introduction
Single-document summarization is attracting much more attention as a key technology in providing
better information access in a commercial context. The Financial Times and CNN have been providing
summaries of articles in their websites to attract users, and Summly, which has been acquired by Yahoo!,
provided the service of automatically summarizing articles on the Internet. Given the cost of manual
summarization, we can greatly improve the information access of Internet users by creating an automatic
summarizer that can approach the summarization quality of humans.
To mimic manually-written summaries, one important aspect is coherence (Nenkova and McKeown,
2011). Although coherence has been studied widely in a field of multi-document summarization (Kara-
manis et al., 2004; Barzilay and Lapata, 2005; Nishikawa et al., 2010; Christensen et al., 2013), it has not
been studied enough in the context of single-document summarization. In this paper, we revisit the prob-
lem of coherence and employ it to produce both informative and linguistically high-quality summaries.
To obtain such summaries, we introduce a novel summarization method based on a hidden semi-
Markov model. The method has the properties of both the popular single-document summarization
model, the knapsack problem, which packs the sentences into the given length and the hidden Markov
model, which takes summary coherence into account by determining sentence context when selecting
sentences. By leveraging this, we can build a summarizer that naturally achieves coherence.
We state the novelty and contributions of this paper as follows:
? We regard single-document summarization as a combinatorial optimization problem modeled by a
hidden semi-Markov model and propose an efficient decoding algorithm for the problem.
? We introduce various features related to coherence in a combinatorial formulation. We extend a
hidden semi-Markov model to achieve discrimination, so our method can take advantage of many
features for predicting coherence.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1648
? We show that our large-scale corpus greatly improves the performance of summarization.
This paper is organized as follows. In Section 2, we describe related work. In Section 3, we detail
our proposed model. We also explain how the parameters in our model are optimized and how sentences
are compressed. In Section 4, we explain how variants of the original sentences are generated. In
Section 5, we explain the decoding algorithm for our method. In Section 6, we explain the settings of
our experiments, our corpus, and compared methods. In Section 7, we show results of the experiments
conducted to evaluate our method. In Section 8, we conclude this paper.
2 Related Work
2.1 Single-Document Summarization
Basically, single-document summarization can be done through sentence selection (Nenkova and McK-
eown, 2011) . The document to be summarized is decomposed into a set of sentences and then the
summarizer selects a subset of the sentences as a summary.
McDonald (2007) pointed out that single-document summarization can be formulated as a well-known
combinatorial optimization problem, the knapsack problem. Given a set of sentences together with their
lengths and values, the summarizer packs them into a summary so that the total value is as large as possi-
ble but the total length is less than or equal to a given maximum summary length. Interestingly, a hidden
semi-Markov model (Yu, 2010) can be regarded as a natural extension of the knapsack problem, we take
advantage of this property for single-document summarization. We elaborate the relation between the
knapsack problem and the hidden semi-Markov model in Section 3.
To generate coherent summaries in single-document summarization, there are two types of ap-
proaches
1
: tree-based approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) and
sequence-based approaches (Barzilay and Lee, 2004; Shen et al., 2007). The former rely on the tree
representation of a document based on the Rhetorical Structure Theory (RST) (Mann and Thompson,
1988). Basically, the former approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) trim
the tree representation of a document by making use of nucleus-satellite relations among sentences. The
advantage of RST-based approaches is that they can take advantage of global information about the doc-
uments. However, a drawback is that they depend heavily on the RST parser that is used. Performance
is remarkably sensitive to the accuracy of RST parsing, and hence we have to build a good RST parser.
Instead of making use of the global structure of the document, the sequence-based methods rely on and
take advantage of the local coherence of sentences. As one advantage over the tree-based approaches,
the sequence-based approaches do not require tools as RST parsers, and hence they are more robust. For
this reason, this paper focuses on sequence-based approaches.
The previous works most closely related to our method are those proposed by Barzilay and Lee (2004)
and Shen et al. (2007). Barzilay and Lee built a hidden Markov model to capture the content structure of
documents and used it to identify the important sentences. Shen et al. (2007) extended the HMM-based
approach to make it discriminative by making use of conditional random fields (Lafferty et al., 2001).
Conditional random fields can incorporate various features to identify the importance of a sentence and
they showed its effectiveness. A shortcoming of these approaches is that their model only classifies sen-
tences into two classes, it cannot take account of output length directly. This deficiency is problematic
because in practical usage the maximum length of a summary is specified by the user; hence, the sum-
marizer should be able to control output length. In contrast to their method, our approach naturally takes
the maximum summary length into account when summarizing a document.
2.2 Coherence
In the context of multi-document summarization, coherence has been studied widely. In multi-document
summarization, sentences are selected from different documents, and hence some way of ordering the
sentences is required. Sentence ordering (Barzilay et al., 2002; Althaus et al., 2004; Karamanis et al.,
1
As an interesting related work, Clarke and Lapata (2007) compresses documents by making use of Centering Theory
(Grosz et al., 1995). However, in their approach, the desired length of an output summary could not be specified and hence they
said their method was compression rather than summarization.
1649
Figure 1: An example of the hidden semi-Markov model. The system observes a sequence consisting
of 10 symbols o
1
...o
10
over time t
1
...t
10
and transitions between states s
1
...s
3
. Unlike the basic hidden
Markov model, states can persist for a non-unit length. In this figure, state s
2
and state s
3
persist for
non-unit lengths. Hence, the system traverses only 6 states despite observing 10 symbols.
2004; Okazaki et al., 2004) is a task to order extracted sentences and is closely related to coherence
(Lapata, 2003; Barzilay and Lapata, 2005; Nenkova et al., 2010; Pitler et al., 2010; Louis and Nenkova,
2012). Many effective features have been found out to capture coherence and we utilize these features.
Some work proposed a model that could jointly taking the content of the summary and its coherence
into account (Nishikawa et al., 2010; Christensen et al., 2013). Since extracted sentences in multi-
document summarization must be ordered, a task that is NP-hard, they relied on integer linear program-
ming (Nishikawa et al., 2010) or a local search strategy (Christensen et al., 2013). The former can locate
the optimal solution at a heavy computation cost, while the latter runs quickly but there is no guarantee
of locating the optimal solution. In contrast to their trade-off, our proposed algorithm, based on dynamic
programming, can locate the optimal solution quickly because the single-document summarization can
skip the ordering operation by reproducing the original order of the input sentences.
In this paper, we show that coherence also takes an important role in single-document summarization.
We model the coherence between adjacent sentences in the summary by leveraging the hidden semi-
Markov model, which can naturally capture the coherence between sentences.
3 Summarization with Hidden Semi-Markov Model
We first introduce the knapsack problem, which can naturally model single-document summarization.
Next, we explain the hidden semi-Markov model and show its relationship to the knapsack problem.
Then, we elaborate our summarization method.
3.1 Knapsack Problem
The knapsack problem is a type of combinatorial optimization problem (Korte and Vygen, 2008). Given
a set of elements, each of which has a score and size, the problem is formulated as the task of finding
the best subset in terms of maximizing the sum of their scores under the size limitation. As mentioned
above, single-document summarization can be regarded as an instance of the knapsack problem. The
best combination of input sentences can be found by calculating the value of each sentence and packing
them into a summary through the dynamic programming knapsack algorithm.
3.2 Hidden Semi-Markov Model
The hidden semi-Markov model (HSMM) is an extension of the hidden Markov model (HMM) (Yu,
2010). In the popular hidden Markov model, each state persists for only one unit length. For example,
if a system observes 10 discrete symbols, it outputs 10 hidden states. In the HSMM, each state can
persist for some unit lengths through the concept of duration. For example, if a system observes 10
discrete symbols and each state persists for two unit lengths, i.e., their duration is 2, the system outputs
5 hidden states. We show an example in Figure 1. The system observes a sequence consisting of 10
symbols o
1
...o
10
over time t
1
...t
10
and transitions between states s
1
...s
3
. Unlike the basic HMM, states
can persist for a non-unit length. In this figure, state s
2
and state s
3
persist for a non-unit length. Hence,
the system traverses 6 states even though it observes 10 symbols. This property has been utilized for
1650
sequential tagging, such as named entity recognition (Sarawagi and Cohen, 2004), scene text recognition
(Weinman et al., 2008) and phonetic recognition (Kim et al., 2011).
The hidden semi-Markov model is closely related to the knapsack problem. The length, K, of the
observed symbols can be regarded as a knapsack constraint. We can consider that the system tries to pack
the states of the model into the observed sequence of symbols by transitioning over the states under the
knapsack constraint so as to maximize the likelihood. Therefore, the hidden semi-Markov can naturally
be used for single-document summarization. Suppose that the document to be summarized consists of
10 sentences and the length of each of them is measured by the number of words. In this case, the system
transitions over 10 states corresponding to the 10 sentences until it cannot select any further sentence due
to the given length requirement. Since each state persists for the length of the corresponding sentence,
the remaining length decreases every time the system transitions to a new state.
While an HMM is basically a generative model, Collins (2002) extended it to create a discriminative
model. An HSMM can also be extended to become discriminative model (Sarawagi and Cohen, 2004).
Our discriminative HSMM learns through the application of max-margin training.
3.3 Formulation
We consider there are n input sentences s
1
, s
2
, ..., s
n
. These sentences have lengths ?
1
, ?
2
, ..., ?
n
and
weights w
1
, w
2
, ..., w
n
. We assume that a sentence that has a high weight should be present in the output
summary. We also consider each sentence, s
i
, has m
i
variants s
i,1
, s
i,2
, ..., s
i,m
, each produced by some
sort of sentence compression or paraphrase module. These variants also have lengths ?
i,1
, ?
i,2
, ..., ?
i,m
i
and weights w
i,1
, w
i,2
, ..., w
i,m
i
. For simplicity, we hereinafter note the original sentences s
1
, s
2
, ..., s
n
as s
1,0
, s
2,0
, ..., s
n,0
. Hence we have original sentence s
i,0
and variants s
i,1
, s
i,2
, ..., s
i,m
. Let s
0,0
and
s
n+1,0
be special symbols indicating the beginning of a document and the end of a document, respec-
tively. We define coherence c
g,h,i,j
as the coherence between sentence s
g,h
and sentence s
i,j
. An output
summary is described as a sequence of input sentences, g. LetG be the entire set of sequences that can be
constructed from the input sentences, i.e., g ? G. Finally, let K be the maximum length of the summary
desired. With these notations, our proposed method can be formulated as the following optimization
problem:
g
?
= argmax
g?G
?
s
i,j
?sent(g)
w
i,j
+
?
(s
g,h
,s
i,j
)?adj(g)
c
g,h,i,j
(1)
s.t.
?
s
i,j
?sent(g)
?
i,j
? K, (2)
where sent(g) and adj(g) indicate a set of sentences in g and a set of adjacent sentences in g, respec-
tively. That is, our model tries to find the best sequence of sentences under the knapsack constraint so as
to maximize the sum of weights and sentence coherence. In contrast to the common knapsack problem
which cannot take the variants and sentence coherence into account, our method, based on the hidden
semi-Markov model, does so naturally.
3.4 Parameter Optimization
Here we elaborate how parameters in the model are optimized to achieve the desired summaries. The
goal is to determine the value of w
i,j
for all i, j and c
g,h,i,j
for all g, h, i, j. We define w
i,j
and c
g,h,i,j
as
follows:
w
i,j
= w
w
? f
w
(s
i,j
) (3)
c
g,h,i,j
= w
c
? f
c
(s
g,h
, s
i,j
), (4)
where f
w
and f
c
are d
w
-dimensional and d
c
-dimensional feature vectors for sentences and sentence pairs,
respectively, andw
w
andw
c
are d
w
-dimensional and d
c
-dimensional parameter vectors for sentences and
sentence pairs, respectively. The goal of optimization is to determine the values of both vector w
w
and
1651
wc
, given feature function f
w
and f
c
. For simplicity, let s be a summary, let f = ?f
w
, f
c
? be a (d
w
+ d
c
)-
dimensional feature function for the whole summary and let w = ?w
w
,w
c
? be a (d
w
+ d
c
)-dimensional
weight vector. The value that the objective function outputs for summary s is w ? f(s).
To optimize the parameter, we employ the Passive-Aggressive algorithm (Crammer, 2006), a widely-
used structured learning method. Since the algorithm offers online learning, it can learn the parameter
quickly and is easy to implement. To learn the parameter so that the output summary is optimized to
the evaluation criteria popular in document summarization research, ROUGE (Lin, 2004), we introduce
ROUGE as the loss function. The parameter is estimated by solving the following formula iteratively
2
:
w
new
= argmin
w
1
2
||w ? w
old
||
2
(5)
s.t. w ? f(r) ? w ? f(s) ? loss(s; r),
where w
new
is the parameter vector after update, w
old
is the parameter vector before update, r is a
reference summary, and loss is the loss function. We define loss as 1 ? ROUGE(s; r). Among the
variants of ROUGE, we used ROUGE-1 for the loss function.
3.4.1 Sentence Feature
The features introduced in this section are used to calculate the weights of sentences, w
i,j
.
Term Frequency: Term frequency is a classic feature in document summarization (Luhn, 1958). We
calculate the total number of times each content word occurs in the document and then, for each sentence,
sum the totals of the content words that appear in the sentence as the value of this feature.
Word: We also use the words and parts-of-speech as features.
Named Entity: Named entities such as a name of person or organization are important. We use named
entities and classes as features.
Length: The length of a sentence may indicate the information value of its content. We use the length of
a sentence, measured by character number, as a feature.
Position: The position of a sentence is a classically important feature. We use the position of a sentence,
the relative position of a sentence, whether the sentence is the first in the document and whether the
sentence is the first in a paragraph, the position of the paragraph in which the sentence is, as features.
3.4.2 Coherence Feature
The features introduced in this section are used to calculate sentence coherence, c
g,h,i,h
.
Lexical Transition: Lapata (2003) showed that the structure of the document can be captured by word-
pairs consisting of words of two adjacent sentences. We use this feature for capturing the links between
two sentences
3
. We build a set of word pairs where one occurs in a precedent sentence and the other
occurs in a succeeding one, and use the elements of the set as a feature.
Lexical Cohesion: Pitler et al. (2010) showed that the similarity of two sentences is one of the strongest
features for predicting coherence. We reproduce this feature for generating coherent summaries. We
calculate cosine similarity between two sentences and use its value as a feature.
Entity Grid: Previous studies showed that Entity Grid (Barzilay and Lapata, 2005) is a strong feature
for predicting coherence (Pitler et al., 2010). We also employ this feature for summarization. While the
entity vector made from the entity grid was originally defined for whole documents, we build the entity
vector for each pair of two sentences because our model is based on the Markovian assumption, and
hence the coherence score is defined between two sentences.
2
As we explain later in Section 5, computation complexity of our algorithm is pseudo-polynomial, and hence the best
solution of our model can be located quickly. This is also advantageous in the learning phase because to learn parameters using
structured learning, the learner has to generate a summary to calculate the loss. Since our algorithm can quickly find the best
solution and generate a summary, it can also contribute to shortening the time required for learning.
3
It is expected that this feature will also contribute to sentence selection. Barzilay and Elhadad (1997) showed that a closely
related word-pair was a good indicator for sentence selection. This feature captures this property by learning.
1652
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 18000
 0  5  10  15  20  25  30
Th
e n
um
be
r o
f s
en
ten
ces
Levenshtein distance
Figure 2: Distribution of Levenshtein distance in the
aligned sentences. Among the 36,413 sentences in
the references, 16,643 were identical (Levenshtein
distance is 0) to the aligned sentences in the input
documents.
 0.6
 0.61
 0.62
 0.63
 0.64
 0.65
 0.66
 0.67
 0.68
 0.69
 0.7
 0  2000  4000  6000  8000  10000
RO
UG
E-
2
The number of training samples
Figure 3: Learning curve of HSMM.
4 Generating Sentence Variants
Since our model can take the variants of an original sentence in the input document as in the multi-
candidate reduction framework (Zajic et al., 2007), we incorporate sentence compression.
We generate a few variants of each original sentence by trimming the dependency tree of the sentence;
this simple operation is sufficient for reproducing reference summaries. By aligning sentences in a refer-
ence summary with those in the corresponding input document
4
, we found that human summaries were
quite conservative. Among the 36,413 sentences in the references, 16,643 were identical to the aligned
sentences in the input documents. Furthermore, most remaining sentences were virtually identical to the
original sentences; revisions were minor, and can be reproduced by simple operations. Few sentences
exhibited paraphrasing or more sophisticated operations. We plot the distribution of Levenshtein distance
in the aligned sentences in Figure 2. According to this observation, we produce the following types of
variants by sentence compression:
1. Removing information in parentheses. Some sentences contain parentheses containing additional
information for readers. The first type of variant deletes text in parentheses.
2. Shortening sentences by trimming their dependency trees. Basically this method follows the sen-
tence trimmer proposed by Nomoto (2008). While using his method, we keep the predicate and its
obligatory arguments in the sentences to keep the sentences grammatical. If a predicate is trimmed,
its obligatory arguments are also trimmed and vice versa. Since there are an exponential number
of subtrees in one tree, we use only n-best subtrees by ranking them according to n-gram language
likelihood and dependency-based language likelihood. We used the dependency parser proposed by
Imamura et al (Imamura et al., 2007) to acquire the dependency tree.
5 Decoding with Dynamic Programming
To solve Equation 1 under the constraints of Equation 2, we use dynamic programming. Algorithm
1 shows the pseudo code of the decoding algorithm. Line 1 to Line 7 initializes the variables used in
the algorithm. Vector x = ?x
0
, ..., x
n+1
? stores which sentence and which variants are included in the
output summary. If x
3
= 2, s
3,2
is included in the summary. V , P and S are two-dimensional arrays,
each of which is used as a dynamic programming table. They store the process of dynamic programming.
4
Alignment proceeds in two steps: first, we calculate the Levenshtein distance between sentences in the document and its
reference, and then we align sentences so as to minimize the distance between them.
1653
Algorithm 1 Decoding Algorithm: Filling Table
1: x = ?x
0
, ..., x
n+1
?
2: for i = 0 to n + 1 do
3: x
i
= ?1
4: V [0][i]? ?1
5: P [0][i]? ?1
6: S[0][i]? 0
7: V [0][0] = 0
8: for k = 1 to K do
9: for i = 1 to n do
10: V [k][i]? V [k ? 1][i]
11: P [k][i]? P [k ? 1][i]
12: S[k][i]? S[k ? 1][i]
13: for v = 0 to m
i
do
14: if ?
i,v
? k then
15: for h = 0 to i? 1 do
16: u = V [k ? ?
i,v
][h]
17: if u ?= ?1 ? S[k ? ?
i,v
][h] + w
i,v
+ c
h,u,i,v
? S[k][i] then
18: V [k][i]? v
19: P [k][i]? h
20: S[k][i]? S[k ? ?
i,v
][h] + w
i,v
+ c
h,u,i,v
21: V [K + 1][n + 1]? 0
22: P [K + 1][n + 1]? 0
23: S[K + 1][n + 1]? 0
24: for h = 1 to n do
25: u = V [K][h]
26: if S[K][h] + c
h,u,n+1,0
? S[K + 1][n + 1] then
27: P [K + 1][n + 1]? h
28: S[K + 1][n + 1]? S[K][h] + c
h,u,n+1,0
Document Reference
Avg. # of characters 476.2 142.0
Avg. # of words 298.6 88.3
Avg. # of sentences 9.7 2.9
Table 1: The statistics of our corpus.
V [k][i] stores which variants are used at time k, i. If V [k][i] = 0, original sentence s
i,0
is selected at
time k, i. If V [k][i] = ?1, no sentence is selected at time k, i. P [k][i] stores a pointer to the sentence
connected to the front of the current sentence. S[k][i] stores the value of the objective function at time
k, i. Line 8 to Line 36 locates the best sequence of sentences based on the following recurrence formula:
S[k][i] =
{
max
h=0...i?1,v=0...m
S[k ? ?
i,v
][h] + w
i,v
+ c
h,V [k??
i,v
][h],i,v
(A)
S[k ? 1][i] (B),
(6)
where case A is: ?
i,v
? k ? S[k ? 1][i] ? S[k ? ?
i,v
][h] + w
i,v
+ c
h,V [k??
i,v
][h],i,v
and case B is:
otherwise. This recurrence formula means that at time k, i the best variant to be selected as can be
determined at time k ? ?
i,v
, h. Hence, for all k ? 1...K and i ? 1...n, the algorithm finds the best
sequence of sentences at time k, i. After Algorithm 1 locates the best sequence of sentences by filling
the tables, the best sequence can be restored by backtracing along the pointers stored in P . Finally, the
algorithm outputs x, which stores which sentences and variants are used in the best sequence. Since
this algorithm is based on a dynamic programming knapsack algorithm (Korte and Vygen, 2008), it runs
in pseudo-polynomial time. This is a significant advantage over the methods that rely on integer linear
programming solvers due to their substantial computation cost.
6 Experiments
6.1 Data
We prepared 12,748 pairs of Japanese newspaper articles and their manually-written reference sum-
maries. This is one of the largest corpus available for single-document summarization research. The
length of all references is within 150 characters. All references in the corpus were written by a specialist
staff in a Japanese newspaper company and the company sold these summaries for commercial purposes.
1654
We list the statistics of our corpus in Table 1. As shown, the task is to summarize the document in about
a third of its original length in terms of the number of words.
6.2 Evaluation Criteria
ROUGE; ROUGE is an automatic evaluation method for automatic summarization proposed by Lin
(2004). We used ROUGE-1 and ROUGE-2 to evaluate the summaries. Since our document-reference
pairs are written in Japanese, we segmented the sentences into words using the Japanese morphological
analyzer developed by Fuchi and Takagi (1998). When calculating the ROUGE score, we used only
content words (i.e. nouns, verbs and adjectives) and so excluded function words as stop words.
Linguistic Quality: To evaluate the linguistic quality of the summaries generated by our method, we
performed a manual evaluation according to quality questions proposed by the National Institute of
Standards and Technology (NIST) (2007)
5
. We randomly sampled 100 summaries from the outputs of
each method described below and asked 7 subjects to evaluate the summaries according to the questions.
All subjects were Japanese native and none were among the authors. Since the quality questions by
NIST (2007) were designed for multi-document summarization, we used 3 of the 5 NIST questions for
single-document summarization: grammaticality, referential clarity, and structure/coherence. We also
asked the subjects to evaluate overall summary quality.
6.3 Compared Methods
We compared the following 8 methods.
Random: Random method selects sentences in the input document randomly.
Lead: Lead method is a classic baseline in single-document summarization. It only extracts the words
from the beginning of the document until the extracted words reach the given length. We simply extracted
150 characters from the beginning of each document.
Knapsack: The knapsack problem can be used as a single-document summarization model (McDonald,
2007). In this baseline, the weight of each sentence was calculated based on the average probabilities
of the words in the sentence (Nenkova and Vanderwende, 2005). Then, a summary was generated by
solving the knapsack problem.
Knapsack with Supervision: Instead of the average word probabilities used in the above baseline, we
used only sentence features f
w
to weigh a sentence.
Conditional Random Fields: Conditional random fields can be used to weigh sentences (Shen et al.,
2007). Since CRFs required binary labels in learning, we aligned sentences in an input document with
the sentences in its reference as explained in Section 4. We used the probabilities of sentences from
CRFs as the weights of the knapsack problem.
Hidden Semi-Markov Model: This is our proposed method without variants of the original sentences.
It selected sentences only from the set of original sentences.
Hidden Semi-Markov Model with Compression: This is our proposed method with variants of the
original sentences. It selected from among the variants and the original ones.
Human: In the linguistic quality evaluation, we added references to the summaries generated by the
above methods to show the upper bound.
When learning, we did 10-fold cross validation. In the experiments, statistical significance was
checked by Wilcoxon signed-rank test (Wilcoxon, 1945). To counteract the problem of multiple com-
parisons, we used the Holm-Bonferroni method (Holm, 1979) to adjust the significance level, ?.
7 Results and Discussion
We show the results of our experiment in Table 2 and Table 3. In this section, first we discuss the results
of the ROUGE evaluation, and then we discuss the results of the linguistic quality evaluation.
In the ROUGE evaluation, all the compared methods except for RANDOM showed good performance.
This is because, as shown in Section 4, many references consisted of sentences identical to the original
5
Some recent studies have tried to predict the readability of the text automatically (Pitler et al., 2010).
1655
Method R-1 R-2 Idt.
RANDOM 0.417 0.291 1.2%
LEAD 0.779
C,S,U,R
0.727
C,S,U,R
4.4%
KP 0.704
R
0.611
R
9.3%
KP(S) 0.729
U,R
0.647
U,R
10.4%
CRFs 0.741
U,R
0.675
S,U,R
11.3%
HSMM 0.769
C,S,U,R
0.703
C,S,U,R
15.2%
HSMM(C) 0.785
C,S,U,R
0.722
C,S,U,R
20.4%
Table 2: Results of the ROUGE evaluation.
?R-1? and ?R-2? correspond to ROUGE-1 and
ROUGE-2, respectively. The values in the col-
umn of ?Idt.? are the percentage of summaries
completely-identical to the corresponding refer-
ences. In the table,
C,S,U,L,R
indicate statisti-
cal significance against CRFs, KP(S), KP, LEAD,
RANDOM, respectively.
Method Gram. Ref. S./C. Overall
LEAD 1.9 3.9 2.5 2.1
KP 4.1
L
3.7 3.4 3.5
KP(S) 4.2
L
3.6 3.5 3.6
L
CRFs 4.1
L
3.9 3.7
L
3.6
L
HSMM 4.3
L
4.0 4.1
L
4.0
L
HSMM(C) 4.0
L
3.9 4.0
L
3.9
L
HUMAN 4.7
L
4.5 4.7
L
4.8
L
Table 3: Results of the linguistic quality evalua-
tion. The values ranged from 1 (very poor) to 5
(very good) (National Institute of Standards and
Technology, 2007). We show statistical signifi-
cance with the same notations as Table 2.
ones, and hence the references can be reproduced if important sentences are identified. Since the com-
pression rate in our corpus was relatively light, it made important information easy to identify. Among
the compared methods, both LEAD and our proposed method, HSMM(C), achieved the best result. There
was no significant difference between LEAD and HSMM(C). This surprising performance of LEAD was
due to the ROUGE evaluation. The words in the document leads were likely to be important, and LEAD
drew on this property. However, as we mentioned later, it sacrificed the linguistic quality to achieve the
high ROUGE score. Furthermore, it failed to yield summaries identical to the reference. In contrast to
LEAD, almost 20% of the summaries generated by HSMM(C) were identical to the references. This
shows that our method successfully mimicked human assessments. HSMM followed the best models.
There was a statistically significant difference between HSMM(C) and HSMM. Since some sentences,
especially the first sentence in the document, were long and the first sentence was particularly impor-
tant to summarize the document, sentence compression yielded a significant improvement. As shown
in Table 2, employing compression greatly improved the percentage of identical summaries. HSMM
significantly outperformed all of the baseline extractive methods except LEAD. While CRFs can take
advantage of all features used in HSMM, CRFs cannot take the evaluation measure such as ROUGE and
the knapsack constraint into account in learning. HSMM also significantly outperformed KP(S). This
difference is particularly important, and shows the usefulness of features related to coherence. While
KP(S) used only features about sentences, HSMM successfully mimicked the references as it drew on
the features related to coherence.
We show the learning curve of HSMM in Figure 3. We fixed 2,748 pairs for testing, and learned
parameters from 100, 250, 500, 1,000, 2,500, 5,000, 7,500 and 10,000 pairs. The curve in the figure
clearly shows the effectiveness of our large-scale corpus in learning. It seems that the curve does not
saturate and hence HSMM performance can be improved by more training samples. As in the results
recently shown by Filippova (2013), this result implies that large-scale data is important in the field
of document summarization as in other fields of computational linguistics. Past studies in document
summarization relied on relatively small datasets consisting of a few dozen or at most a few hundred
pairs of a document and its reference in learning. In contrast to the past studies, there are over 10,000
pairs in our dataset and the results show its effectiveness.
Second, we discuss the result of the linguistic quality evaluation. Unlike the ROUGE evaluation,
HSMM achieved the best result. As previous studies have pointed out (Nenkova and McKeown, 2011),
sentence compression commonly tends to degrade the linguistic quality of a summary while improving
its content. As shown in Table 3, the grammaticality of HSMM(C) is lower than that of HSMM, but the
1656
difference is not significant. Although we could not observe any significant difference between HSMM
and other extractive baselines, our proposals, HSMM and HSMM(C), yielded the best result in terms
of structure/coherence. By making use of the features related to coherence, we successfully improved
summary quality. In contrast to the surprising performance of LEAD in the ROUGE evaluation, in the
linguistic quality evaluation, LEAD yielded the worst performance. Since LEAD had to cut the sentences
when it reached the given length, it create ungrammatical fragments.
Finally, we touch on the balance between the quality of content and linguistic quality. Comparing
Table 2 to 3, we can see the correlation between the quality of content and linguistic quality. This re-
sult is reasonable because we can extract much more information from grammatical and well-organized
sentences. Although we optimized the parameter to maximize the ROUGE score, it also yielded im-
provements in linguistic quality. This is because the manually-generated reference summaries are ba-
sically grammatical and well-organized and the parameter is learnt to mimic them. However, there is
an inherent trade-off between the quality of content and linguistic quality. For example, under stricter
length limitations, instead of cohesive devices such as conjunctions, which can improve the coherence of
sentences, content words would be preferred for summary inclusion to augment information. Balancing
them to maximize reader satisfaction is an interesting problem.
8 Conclusions
In this paper we presented a novel single-document summarization method based on the hidden semi-
Markov model, which is a natural extension of the knapsack problem. Our model naturally takes account
of sentence context when identifying important sentences. This property is particularly important to
ensure the coherence of output summaries and to produce informative and linguistically high-quality
summaries. We also proposed an algorithm based on dynamic programming so the best solution can be
located quickly. Experiments on a very large-scale single-document summarization corpus showed that
our proposed method significantly outperforms competitive baselines.
As future work, we plan to tackle on the summarization task where higher compression is demanded.
To generate shorter summaries, we plan to employ more sophisticated approaches, such as paraphrasing.
Acknowledgement
The corpus used in this paper is owned by The Mainichi Newspapers Co., Ltd. and is leased to Nippon
Telegraph and Telephone Corporation. We sincerely appreciate their consideration. We also appreciate
the insightful comments from reviewers. Their comments greatly improved the quality of this paper.
References
Ernst Althaus, Nikiforos Karamanis, and Alexander Koller. 2004. Computing locally coherent discourses. In
Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL), pages 399?406.
Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the
Intelligent Scalable Text Summarization Workshop (ISTS), pages 10?17.
Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: an entity-based approach. In Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics (ACL), pages 141?148.
Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to
generation and summarization. In HLT-NAACL 2004: Main Proceedings, pages 113?120.
Regina Barzilay, Noemie Elhadad, and Kathleen R. McKeown. 2002. Inferring strategies for sentence ordering in
multidocument news summarization. Journal of Artificial Intelligence Research, 17:35?55.
Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2013. Towards coherent multi-document
summarization. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 1163?1173.
James Clarke and Mirella Lapata. 2007. Modelling compression with discourse constraints. In Proceedings of
the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 1?11.
1657
Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1?8.
Koby Crammer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research,
7(Mar):551?585.
Hal Daume, III and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 449?456.
Katja Filippova. 2013. Overcoming the lack of parallel data in sentence compression. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1481?1491.
Takeshi Fuchi and Shinichiro Takagi. 1998. Japanese morphological analyzer using word co-occurrence: Jtag.
In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th
International Conference on Computational Linguistics (ACL-COLING), pages 409?413.
Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi. 1995. Centering: a framework for modeling the local
coherence of discourse. Computational Linguistics, 21(2):203?225.
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino, Norihito Yasuda, and Masaaki Nagata. 2013. Single-
document summarization as a tree knapsack problem. In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 1515?1520.
Sture Holm. 1979. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics,
6(2):65?70.
Kenji Imamura, Genichiro Kikui, and Norihito Yasuda. 2007. Japanese dependency parsing using sequential label-
ing for semi-spoken language. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 225?228.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish, and Jon Oberlander. 2004. Evaluating centering-based
metrics of coherence. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics
(ACL), pages 391?398.
Sungwoong Kim, Sungrack Yun, and Chang D. Yoo. 2011. Large margin discriminative semi-markov model
for phonetic recognition. IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING,
7(19):1999?2012.
Bernhard Korte and Jens Vygen. 2008. Combinatorial Optimization. Springer-Verlag, third edition.
John Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning (ICML), pages 282?289.
Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the
41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 545?552.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of ACL Workshop
Text Summarization Branches Out, pages 74?81.
Annie Louis and Ani Nenkova. 2012. A coherence model based on syntactic patterns. In Proceedings of the 2012
Conference on Empirical Methods on Natural Language Processing and Computational Natural Language
Learning (EMNLP-CoNLL).
Hans P. Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research and Development,
22(2):159?165.
William C. Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 1997. From discourse structure to text summaries. In Proceedings of ACL/EACL 1997 Summariza-
tion Workshop, pages 82?88.
Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings
of the 29th European Conference on Information Retrieval (ECIR), pages 557?564.
1658
National Institute of Standards and Technology. 2007. The linguistic quality questions. http://www-nlpir.
nist.gov/projects/duc/duc2007/quality-questions.txt.
Ani Nenkova and Kathleen McKeown. 2011. Automatic Summarization. Now Publishers.
Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Technical report,
MSR-TR-2005-101.
Ani Nenkova, Jieun Chae, Annie Louis, and Emily Pitler. 2010. Structural features for predicting the linguistic
quality of text: Applications to machine translation, automatic summarization and human-authored text. In
Emiel Krahmer and Theunem Mariet, editors, Empirical Methods in Natural Language Generation: Data-
oriented Methods and Empirical Evaluation, pages 222?241. Springer.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo, and Genichiro Kikui. 2010. Opinion summarization
with integer linear programming formulation for sentence extraction and ordering. In Coling 2010: Posters,
pages 910?918.
Tadashi Nomoto. 2008. A generic sentence trimmer with crfs. In Proceedings of the 46th Annual Conference of
the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 299?307.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka. 2004. Improving chronological sentence ordering by
precedence relation. In Proceedings of the 20th International Conference on Computational Linguistics (Col-
ing), pages 750?756.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010. Automatic evaluation of linguistic quality in multi-document
summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics
(ACL), pages 544?554.
Sunita Sarawagi and William W. Cohen. 2004. Semi-markov conditional random fields for information extraction.
In Advances in Neural Information Processing Systems 17, pages 1185?1192.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Document summarization using conditional
random fields. In Proceedings of the 20th international joint conference on Artifical intelligence (IJCAI), pages
2862?2867.
Jerod J. Weinman, Erik Learned-Miller, and Allen Hanson. 2008. A discriminative semi-markov model for robust
scene text recognition. In Proceedings of the 19th International Conference on Pattern Recognition (ICPR),
pages 1?5.
Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80?83.
Shun-Zheng Yu. 2010. Hidden semi-markov models. Artificial Intelligence, 174(2):215?243.
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Schwartz Richard. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization tasks. Information Processing and Management, 43:1549?
1570.
1659
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944?952,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation of Translation Quality for Distant Language Pairs
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, Hajime Tsukada
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan
{isozaki,hirao,kevinduh,sudoh,tsukada}@cslab.kecl.ntt.co.jp
Abstract
Automatic evaluation of Machine Translation
(MT) quality is essential to developing high-
quality MT systems. Various evaluation met-
rics have been proposed, and BLEU is now
used as the de facto standard metric. How-
ever, when we consider translation between
distant language pairs such as Japanese and
English, most popular metrics (e.g., BLEU,
NIST, PER, and TER) do not work well. It
is well known that Japanese and English have
completely different word orders, and special
care must be paid to word order in transla-
tion. Otherwise, translations with wrong word
order often lead to misunderstanding and in-
comprehensibility. For instance, SMT-based
Japanese-to-English translators tend to trans-
late ?A because B? as ?B because A.? Thus,
word order is the most important problem
for distant language translation. However,
conventional evaluation metrics do not sig-
nificantly penalize such word order mistakes.
Therefore, locally optimizing these metrics
leads to inadequate translations. In this pa-
per, we propose an automatic evaluation met-
ric based on rank correlation coefficients mod-
ified with precision. Our meta-evaluation of
the NTCIR-7 PATMT JE task data shows that
this metric outperforms conventional metrics.
1 Introduction
Automatic evaluation of machine translation (MT)
quality is essential to developing high-quality ma-
chine translation systems because human evaluation
is time consuming, expensive, and irreproducible. If
we have a perfect automatic evaluation metric, we
can tune our translation system for the metric.
BLEU (Papineni et al, 2002b; Papineni et al,
2002a) showed high correlation with human judg-
ments and is still used as the de facto standard au-
tomatic evaluation metric. However, Callison-Burch
et al (2006) argued that the MT community is overly
reliant on BLEU by showing examples of poor per-
formance. For Japanese-to-English (JE) translation,
Echizen-ya et al (2009) showed that the popular
BLEU and NIST do not work well by using the sys-
tem outputs of the NTCIR-7 PATMT (patent transla-
tion) JE task (Fujii et al, 2008). On the other hand,
ROUGE-L (Lin and Hovy, 2003), Word Error Rate
(WER), and IMPACT (Echizen-ya and Araki, 2007)
worked better.
In these studies, Pearson?s correlation coefficient
and Spearman?s rank correlation ? with human eval-
uation scores are used to measure how closely an
automatic evaluation method correlates with human
evaluation. This evaluation of automatic evaluation
methods is called meta-evaluation. In human eval-
uation, people judge the adequacy and the fluency of
each translation.
Denoual and Lepage (2005) pointed out that
BLEU assumes word boundaries, which is ambigu-
ous in Japanese and Chinese. Here, we assume
the word boundaries given by ChaSen, one of the
standard morphological analyzers (http://chasen-
legacy.sourceforge.jp/) following Fujii et al
(2008)
In JE translation, most Statistical Machine Trans-
lation (SMT) systems translate the Japanese sen-
tence
(J0) kare wa sono hon wo yonda node
sekaishi ni kyoumi ga atta
which means
944
(R0) he was interested in world
history because he read the book
into an English sentence such as
(H0) he read the book because he was
interested in world history
in which the cause and the effect are swapped. Why
does this happen? The former half of (J0) means ?He
read the book,? and the latter half means ?(he) was
interested in world history.? The middle word
?node? between them corresponds to ?because.?
Therefore, SMT systems output sentences like (H0).
On the other hand, Rule-based Machine Translation
(RBMT) systems correctly give (R0).
In order to find (R0), SMT systems have to search
a very large space because we cannot restrict its
search space with a small distortion limit. Most
SMT systems thus fail to find (R0).
Consequently, the global word order is essential
for translation between distant language pairs, and
wrong word order can easily lead to misunderstand-
ing or incomprehensibility. Perhaps, some readers
do not understand why we emphasize word order
from this example alone. A few more examples
will clarify what happens when SMT is applied to
Japanese-to-English translation. Even the most fa-
mous SMT service available on the web failed to
translate the following very simple sentence at the
time of writing this paper.
Japanese: meari wa jon wo koroshita.
Reference: Mary killed John.
SMT output: John killed Mary.
Since it cannot translate such a simple sentence, it
obviously cannot translate more complex sentences
correctly.
Japanese: bobu ga katta hon wo jon wa yonda.
Reference: John read a book that Bob bought.
SMT output: Bob read the book John bought.
Another example is:
Japanese: bobu wa meari ni yubiwa wo kau
tameni, jon no mise ni itta.
Reference: Bob went to John?s store to buy a
ring for Mary.
SMT output: Bob Mary to buy the ring, John
went to the store.
In this way, this SMT service usually gives incom-
prehensible or misleading translations, and thus peo-
ple prefer RBMT services. Other SMT systems also
tend to make similar word order mistakes, and spe-
cial care should be paid to the translation between
distant language pairs such as Japanese and English.
Even Japanese people cannot solve this word or-
der problem easily: It is well known that Japanese
people are not good at speaking English.
From this point of view, conventional automatic
evaluation metrics of translation quality disregard
word order mistakes too much. Single-reference
BLEU is defined by a geometrical mean of n-gram
precisions pn and is modified by Brevity Penalty
(BP) min(1, exp(1? r/h)), where r is the length of
the reference and h is the length of the hypothesis.
BLEU = BP? (p1p2p3p4)
1/4.
Its range is [0, 1]. The BLEU score of (H0) with ref-
erence (R0) is 1.0?(11/11?9/10?6/9?4/8)1/4 =
0.740. Therefore, BLEU gives a very good score to
this inadequate translation because it checks only n-
grams and does not regard global word order.
Since (R0) and (H0) look similar in terms of flu-
ency, adequacy is more important than fluency in
the translation between distant language pairs.
Similarly, other popular scores such as NIST,
PER, and TER (Snover et al, 2006) also give
relatively good scores to this translation. NIST
also considers only local word orders (n-grams).
PER (Position-Independent Word Error Rate) was
designed to disregard word order completely.
TER (Snover et al, 2006) was designed to allow
phrase movements without large penalties. There-
fore, these standard metrics are not optimal for eval-
uating translation between distant language pairs.
In this paper, we propose an alternative automatic
evaluation metric appropriate for distant language
pairs. Our method is based on rank correlation co-
efficients. We use them to compare the word ranks
in the reference with those in the hypothesis.
There are two popular rank correlation coeffi-
cients: Spearman?s ? and Kendall?s ? (Kendall,
1975). In Isozaki et al (2010), we used Kendall?s ?
to measure the effectiveness of our Head Finaliza-
tion rule as a preprocessor for English-to-Japanese
translation, but we measured the quality of transla-
tion by using conventional metrics.
945
It is not clear how well ? works as an automatic
evaluation metric of translation quality. Moreover,
Spearman?s ? might work better than Kendall?s ? .
As we discuss later, ? considers only the direction
of the rank change, whereas ? considers the distance
of the change.
The first objective of this paper is to examine
which is the better metric for distant language pairs.
The second objective is to find improvements of
these rank correlation-metrics.
Spearman?s ? is based on Pearson?s correlation
coefficients. Suppose we have two lists of numbers
x = [0.1, 0.4, 0.2, 0.6],
y = [0.9, 0.6, 0.2, 0.7].
To obtain Pearson?s coefficients between x and y,
we use the raw values in these lists. If we substitute
their ranks for their raw values, we get
x? = [1, 3, 2, 4] and y? = [4, 2, 1, 3].
Then, Spearman?s ? between x and y is given by
Pearson?s coefficients between x? and y?. This ?
can be rewritten as follows when there is no tie:
? = 1?
?
i d
2
i
n+1C3
.
Here, di indicates the difference in the ranks of the
i-th element. Rank distances are squared in this
formula. Because of this square, we expect that ?
decreases drastically when there is an element that
significantly changes in rank. But we are also afraid
that ? may be too severe for alternative good trans-
lations.
Since Pearson?s correlation metric assumes lin-
earity, nonlinear monotonic functions can change
its score. On the other hand, Spearman?s ? and
Kendall?s ? uses ranks instead of raw evaluation
scores, and simple application of monotonic func-
tions cannot change them (use of other operations
such as averaging sentence scores can change them).
2 Methodology
2.1 Word alignment for rank correlations
We have to determine word ranks to obtain rank cor-
relation coefficients. Suppose we have:
(R1) John hit Bob yesterday
(H1) Bob hit John yesterday
The 1st word ?John? in R1 becomes the 3rd word
in H1. The 2nd word ?hit? in R1 becomes the 2nd
word in H1. The 3rd word ?Bob? in R1 becomes the
1st word in H1. The 4th word ?yesterday? in R1 be-
comes the 4th word in H1. Thus, we get H1?s word
order list [3, 2, 1, 4]. The number of all pairs of in-
tegers in this list is 4C2 = 6. It has three increasing
pairs: (3,4), (2,4), and (1,4). Since Kendall?s ? is
given by:
? = 2?
the number of increasing pairs
the number of all pairs
? 1,
H1?s ? is 2? 3/6? 1 = 0.0.
In this case, we can obtain Spearman?s ? as fol-
lows: ?John? moved by d1 = 2 words, ?hit? moved
by d2 = 0 words, ?Bob? moved by d3 = 2 words,
and ?yesterday? moved by d4 = 0 words. Therefore,
H1?s ? is 1? (22 + 02 + 22 + 02)/5C3 = 0.2.
Thus, ? considers only the direction of the move-
ment, whereas ? considers the distance of the move-
ment. Both ? and ? have the same range [?1, 1]. The
main objective of this paper is to clarify which rank
correlation is closer to human evaluation scores.
We have to consider the limitation of the rank cor-
relation metrics. They are defined only when there
is one-to-one correspondence. However, a refer-
ence sentence and a hypothesis sentence may have
different numbers of words. They may have two or
more occurrences of the same word in one sentence.
Sometimes, a word in the reference does not appear
in the hypothesis, or a word in the hypothesis does
not appear in the reference. Therefore, we cannot
calculate ? and ? following the above definitions in
general.
Here, we determine the correspondence of words
between hypotheses and references as follows. First,
we find one-to-one corresponding words. That is,
we find words that appear in both sentences and only
once in each sentence. Suppose we have:
(R2) the boy read the book
(H2) the book was read by the boy
By removing non-aligned words by one-to-one cor-
respondence, we get:
946
(R3) boy read book
(H3) book read boy
Thus, we lost ?the.? We relax this one-to-one cor-
respondence constraint by using one-to-one corre-
sponding bigrams. (R2) and (H2) share ?the boy?
and ?the book,? and we can align these instances of
?the? correctly.
(R4) the1 boy2 read3 the4 book5
(H4) the4 book5 read3 the1 boy2
Now, we have five aligned words, and H4?s word
order is represented by [4, 5, 3, 1, 2].
In returning to H0 and R0, we find that each of
these sentences has eleven words. Almost all words
are aligned by one-to-one correspondence but ?he?
is not aligned because it appears twice in each sen-
tence. By considering one-to-one corresponding bi-
grams (?he was? and ?he read?), ?he? is aligned as
follows.
(R5) he1 was2 interested3 in4 world5
history6 because7 he8 read9 the10
book11
(H5) he8 read9 the10 book11 because7
he1 was2 interested3 in4 world5
history6
H5?s word order is [8, 9, 10, 11, 7, 1, 2, 3, 4, 5, 6].
The number of increasing pairs is: 4C2 = 6 pairs in
[8, 9, 10, 11] and 6C2 = 15 pairs in [1, 2, 3, 4, 5,
6]. Then we obtain ? = 2 ? (6 + 15)/11C2 ? 1 =
?0.236. On the other hand,
?
i d
2
i = 5
2 ? 6 + 22 +
72 ? 4 = 350, and we obtain ? = 1 ? 350/12C3 =
?0.591.
Therefore, both Spearman?s ? and Kendall?s ?
give very bad scores to the misleading translation
H0. This fact implies they are much better metrics
than BLEU, which gave a good score to it. ? is much
lower than ? as we expected.
In general, we can use higher-order n-grams for
this alignment, but here we use only unigrams and
bigrams for simplicity. This algnment algorithm is
given in Figure 1. Since some hypothesis words do
not have corresponding reference words, the output
integer list worder is sometimes shorter than the
evaluated sentence. Therefore, we should not use
worder[i] ? i as di directly. We have to renumber
the list by rank as we did in Section 1.
Read a hypothesis sentence h = h1h2 . . . hm
and its reference sentence r = r1r2 . . . rn.
Initialize worder with an empty list.
For each word hi in h:
? If hi appears only once each in h and r, append j
s.t. rj = hi to worder.
? Otherwise, if the bigram hihi+1 appears only once
each in h and r, append j s.t. rjrj+1 = hihi+1 to
worder.
? Otherwise, if the bigram hi?1hi appears only once
each in h and r, append j s.t. rj?1rj = hi?1hi to
worder.
Return worder.
Figure 1: Word alignment algorithm for rank correlation
2.2 Word order metrics and meta-evaluation
metrics
These rank correlation metrics sometimes have neg-
ative values. In order to make them just like other
automatic evaluation metrics, we normalize them as
follows.
? Normalized Kendall?s ? : NKT = (? + 1)/2.
? Normalized Spearman?s ?: NSR = (?+ 1)/2.
Accordingly, NKT is 0.382 and NSR is 0.205.
These metrics are defined only when the number
of aligned words is two or more. We define both
NKT and NSR as zero when the number is one or
less. Consequently, these normalized metrics have
the same range [0, 1].
In order to avoid confusion, we use these abbre-
viations (NKT and NSR) when we use rank corre-
lations as word order metrics, because these cor-
relation metrics are also used in the machine trans-
lation community for meta-evaluation. For meta-
evaluation, we use Spearman?s ? and Pearson?s cor-
relation coefficient and call them ?Spearman? and
?Pearson,? respectively.
2.3 Overestimation problem
Since we measure the rank correlation of only cor-
responding words, these metrics will overestimate
the correlation. For instance, a hypothesis sentence
might have only two corresponding words among
947
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
BP (brevity penalty)
no
rm
al
iz
ed
av
er
ag
e
ad
eq
ua
cy
? ?
?
?
? ?
?
?
?
?
?
?
?
??
?
?
?
?? ??
?
?
?
??
?
?
?
?
?
?
?
?
?
??
? ?
?
?
?
??
?
?
?
??
?
? ?
?
??
?
?
?
?
? ? ?
? ? ? ??
?
?
?
? ?
?
?
?
?? ??
?
? ?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
?
?
?
?
?
?
?
??
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
? ?
?
?
?
?
?
? ?
?
?
?
?
?
?
?
?
?
?
? ?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
? ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
P (precision)
no
rm
al
iz
ed
av
er
ag
e
ad
eq
ua
cy
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?? ?
?
?
?
?
?
?
?
??
?
? ?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
??
?
???
?
??? ?
?
?
? ?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
? ?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
??
?
?
?
?
??
?
?
?
?
?
?
?
? ?
? ?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
??
?
?
?
?? ?
?
???
?
?
??
?
??
?
?
?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?? ?
?
?
?
?
?
??
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
? ?
?
?
?
?
? ??
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
? ?
??
?
? ?
??
?
?
?
?
?
Figure 2: Scatter plots of normalized average adequacy with brevity penalty (left) and precision (right).
(Each ? corresponds to one sentence generated by one MT system)
dozens of words. In this case, these two words
determine the score of the whole sentence. If the
two words appear in their order in the reference,
the whole sentence obtains the best score, NSR =
NKT = 1.0, in spite of the fact that only two words
matched.
Solving this overestimation problem is the second
objective of this paper. BLEU uses ?Brevity Penalty
(BP)? (Section 1) to reduce the scores of too-short
sentences. We can combine the above word order
metrics with BP, e.g., NKT? BP and NSR? BP.
However, we cannot very much expect from this
solution because BP scores do not correlate with
human judgments well. The left graph of Figure
2 shows a scatter plot of BP and ?normalized av-
erage adequacy.? This graph has 15 (systems) ?
100 (sentences) dots. Each dot (?) corresponds to
one sentence from one translation system.
In the NTCIR-7 data, three human judges gave
five-point scores (1, 2, 3, 4, 5) for ?adequacy? and
?fluency? of each translated sentence. Although
each system translated 1,381 sentences, only 100
sentences were evaluated by the judges.
For each translated sentence, we averaged three
judges? adequacy scores and normalized this aver-
age x by (x?1)/4. This is our ?normalized average
adequacy,? and the dots appears only at multiples of
1/3? 1/4.
This graph shows that BP has very little correla-
tion with adequacy, and we cannot expect BP to im-
prove the meta-evaluation performance very much.
Perhaps, BP?s poor performance was caused by the
fact that most MT systems output almost the same
number of words, and if the number exceeds the
length of the reference, BP=1.0 holds.
Therefore, we have to consider other modifiers
for this overestimation problem. We can use other
common metrics such as precision, recall, and F-
measure to reduce the overestimation of NSR and
NKT.
? Precision: P = c/|h|, where c is the number of
corresponding words and |h| is the number of
words in the hypothesis sentence h.
? Recall: R = c/r, where |r| is the number of
words in the reference sentence r.
? F-measure: F? = (1 + ?2)PR/(?2P + R),
where ? is a parameter.
In (R2)&(H2)?s case, precision is 5/7 = 0.714 and
recall is 5/5 = 1.000.
Which metric should we use? Our preliminary
experiments with NTCIR-7 data showed that preci-
sion correlated best with adequacy among these
three metrics (P , R, and F?=1). In addition, BLEU
is essentially made for precision. Therefore, preci-
sion seems the most promising modifier.
The right graph of Figure 2 shows a scatter plot
of precision and normalized average adequacy. The
graph shows that precision has more correlation with
adequacy than BP. We can observe that sentences
with very small P values usually obtain very low
adequacy scores but those with mediocre P values
often obtain good adequacy scores.
948
If we multiply P directly by NSR or NKT, those
sentences with mediocre P values will lose too
much of their scores. The use of
?
x will miti-
gate this problem. Since
?
P is closer to 1.0 than
P itself, multiplication of
?
P instead of P itself
will save these sentences. If we apply
?
x twice
(
??
P = 4
?
P ), it will further save them. There-
fore, we expect?
?
P and? 4
?
P to work better than
?P . Now, we propose two new metrics:
NSRP? and NKTP?,
where ? is a parameter (0 ? ? ? 1).
3 Experiments
3.1 Meta-evaluation with NTCIR-7 data
In order to compare automatic translation evalua-
tion methods, we use submissions to the NTCIR-7
Patent Translation (PATMT) task (Fujii et al, 2008).
Fourteen MT systems participated in the Japanese-
English intrinsic evaluation. There were two Rule-
Based MT (RMBT) systems and one Example-
based MT (EBMT) system. All other systems were
Statistical MT (SMT) systems. The task organiz-
ers provided a baseline SMT system. These 15 sys-
tems translated 1,381 Japanese sentences into En-
glish. The organizers evaluated these translations by
using BLEU and human judgments. In the human
judgements, three experts independently evaluated
100 selected sentences in terms of ?adequacy? and
?fluency.?
For automatic evaluation, we used a single refer-
ence sentence for each of these 100 manually evalu-
ated sentences. Echizen-ya et al (2009) used multi-
reference data, but their data is not publicly available
yet.
For this meta-evaluation, we measured the
corpus-level correlation between the human evalua-
tion scores and the automatic evaluation scores. We
simply averaged scores of 100 sentences for the pro-
posed metrics. For existing metrics such as BLEU,
we followed their definitions for corpus-level eval-
uation instead of simple averages of sentence-level
scores. We used default settings for conventional
metrics, but we tuned GTM (Melamed et al, 2007)
with -e option. This option controls preferences
on longer word runs. We also used the para-
phrase database TERp (http://www.umiacs.umd.
edu/?snover/terp) for METEOR (Banerjee and
Lavie, 2005).
3.2 Meta-evaluation with WMT-07 data
We developed our metric mainly for automatic eval-
uation of translation quality for distant language
pairs such as Japanese-English, but we also want
to know how well the metric works for similar lan-
guage pairs. Therefore, we also use the WMT-
07 data (Callison-Burch et al, 2007) that covers
only European language pairs. Callison-Burch et al
(2007) tried different human evaluation methods and
showed detailed evaluation scores. The Europarl test
set has 2,000 sentences, and The News Commentary
test set has 2,007 sentences.
This data has different language pairs: Spanish,
French, German ? English. We exclude Czech-
English because there were so few systems (See the
footnote of p. 146 in their paper).
4 Results
4.1 Meta-evaluation with NTCIR-7 data
Table 1 shows the main results of this paper. The
left part has corpus-level meta-evaluation with ade-
quacy. Error metrics, WER, PER, and TER, have
negative correlation coefficients, but we did not
show their minus signs here.
Both NSR-based metrics and NKT-based metrics
perform better than conventional metrics for this NT-
CIR PATMT JE translation data. As we expected,
?BP and ?P (1/1) performed badly. Spearman of
BP itself is zero.
NKT performed slightly better than NSR. Per-
haps, NSR penalized alternative good translations
too much. However, one of the NSR-based metrics,
NSRP 1/4, gave the best Spearman score of 0.947,
and the difference between NSRP? and NKTP?
was small. Modification with P led to this improve-
ment.
NKT gave the best Pearson score of 0.922. How-
ever, Pearson measures linearity and we can change
its score through a nonlinear monotonic function
without changing Spearman very much. For in-
stance, (NSRP 1/4)1.5 also has Spearman of 0.947
but its Pearson is 0.931, which is better than NKT?s
0.922. Thus, we think Spearman is a better meta-
evaluation metric than Pearson.
949
Table 1: NTCIR-7 Meta-evaluation: correlation with hu-
man judgments (Spm = Spearman, Prs = Pearson)
human judge Adequacy Fluency
eval\ meta-eval Spm Prs Spm Prs
P 0.615 0.704 0.672 0.876
R 0.436 0.669 0.461 0.854
F?=1 0.525 0.692 0.543 0.871
BP 0.000 0.515 -0.007 0.742
NSR 0.904 0.906 0.869 0.910
NSRP 1/8 0.937 0.905 0.890 0.934
NSRP 1/4 0.947 0.900 0.901 0.944
NSRP 1/2 0.937 0.890 0.926 0.949
NSRP 1/1 0.883 0.872 0.883 0.939
NSR ? BP 0.851 0.874 0.769 0.910
NKT 0.940 0.922 0.887 0.931
NKTP 1/8 0.940 0.913 0.908 0.944
NKTP 1/4 0.940 0.904 0.908 0.949
NKTP 1/2 0.929 0.890 0.897 0.949
NKTP 1/1 0.897 0.869 0.879 0.936
NKT ? BP 0.829 0.878 0.726 0.918
ROUGE-L 0.903 0.874 0.889 0.932
ROUGE-S(4) 0.593 0.757 0.640 0.869
IMPACT 0.797 0.813 0.751 0.932
WER 0.894 0.822 0.836 0.926
TER 0.854 0.806 0.372 0.856
PER 0.375 0.642 0.393 0.842
METEOR(TERp) 0.490 0.708 0.508 0.878
GTM(-e 12) 0.618 0.723 0.601 0.850
NIST 0.343 0.661 0.372 0.856
BLEU 0.515 0.653 0.500 0.795
The right part of Table 1 shows correlation with
fluency, but adequacy is more important, because
our motivation is to provide a metric that is useful to
reduce incomprehensible or misunderstanding out-
puts of MT systems. Again, the correlation-based
metrics gave better scores than conventional metrics,
and BP performed badly. NSR-based metrics proved
to be as good as NKT-based metrics.
Meta-evaluation scores of the de facto standard
BLEU is much lower than those of other metrics.
Echizen-ya et al (2009) reported that IMPACT per-
formed very well for sentence-level evaluation of
NTCIR-7 PATMT JE data. This corpus-level result
also shows that IMPACT works better than BLEU,
but ROUGE-L, WER, and our methods give better
scores than IMPACT.
Table 2: WMT-07 meta-evaluation: Each source lan-
guage has two columns: the left one is News Corpus and
the right one is Europarl.
Spearman?s ? with human ?rank?
source French Spanish German
NSR 0.775 0.837 0.523 0.766 0.700 0.593
NSRP 1/8 0.821 0.857 0.786 0.595 0.400 0.685
NSRP 1/4 0.821 0.857 0.786 0.455 0.400 0.714
NSRP 1/2 0.821 0.857 0.786 0.347 0.400 0.714
NKT 0.845 0.857 0.607 0.838 0.700 0.630
NKTP 1/8 0.793 0.857 0.786 0.595 0.400 0.714
NKTP 1/4 0.793 0.857 0.786 0.524 0.400 0.714
NKTP 1/2 0.793 0.857 0.786 0.347 0.400 0.714
BLEU 0.786 0.679 0.750 0.595 0.400 0.821
WER 0.607 0.857 0.750 0.429 0.000 0.500
ROUGEL 0.893 0.739 0.786 0.707 0.700 0.857
ROUGES 0.883 0.679 0.786 0.690 0.400 0.929
4.2 Meta-evaluation with WMT-07 data
Callison-Burch et al (2007) have performed differ-
ent human evaluation methods for different language
pairs and different corpora. Their Table 5 shows
inter-annotator agreements for the human evaluation
methods. According to their table, the ?sentence
ranking? (or ?rank?) method obtained better agree-
ment than ?adequacy.? Therefore, we show Spear-
man?s ? for ?rank.? We used the scores given in
their Tables 9, 10, and 11. (The ?constituent? meth-
ods obtained the best inter-annotator agreement, but
these methods focus on local translation quality and
have nothing to do with global word order, which we
are discussing here.)
Table 2 shows that our metrics designed for
distant language pairs are comparable to conven-
tional methods even for similar language pairs, but
ROUGE-L and ROUGE-S performed better than
ours for French News Corpus and German Europarl.
BLEU scores in this table agree with those in Table
17 of Callison-Burch et al (2007) within rounding
errors.
After some experiments, we noticed that the use
ofR instead of P often gives better scores for WMT-
07, but it degrades NTCIR-7 scores. We can extend
our metric by F? , weighted harmonic mean of P and
R, or any other interpolation, but the introduction
of new parameters into our metric makes it difficult
950
to control. Improvement without new parameters is
beyond the scope of this paper.
5 Discussion
It has come to our attention that Birch et al (2010)
has independently proposed an automatic evaluation
method based on Kendall?s ? . First, they started
with Kendall?s ? distance, which can be written as
?1?NKT? in our terminology, and then subtracted
it from one. Thus, their metric is nothing but NKT.
Then, they proposed application of the square root
to get better Pearson by improving ?the sensitivity
to small reorderings.? Since they used ?Kendall?s ??
and ?Kendall?s ? distance? interchangeably, it is not
clear what they mean by ?
?
Kendall?s ? ,? but per-
haps they mean 1 ?
?
1?NKT because
?
NKT is
more insensitive to small reorderings. Table 3 shows
the performance of these metrics for NTCIR-7 data.
Pearson?s correlation coefficient with adequacy was
improved by 1 ?
?
1? NKT, but other scores were
degraded in this experiment.
The difference between our method and Birch et
al. (2010)?s method comes from the fact that we
used Japanese-English translation data and Spear-
man?s correlation for meta-evaluation, whereas they
used Chinese-English translation data and only Pear-
son?s correlation for meta-evaluation. Chinese word
order is different from English, but Chinese is a
Subject-Verb-Object (SVO) language and thus is
much closer to English word order than Japanese,
a typical SOV language.
We preferred NSR because it penalizes global
word order mistakes much more than does NKT, and
as discussed above, global word order mistakes of-
ten lead to incomprehensibility and misunderstand-
ing.
On the other hand, they also tried Hamming dis-
tance, and summarized their experiments as follows:
However, the Hamming distance seems to
be more informative than Kendall?s tau for
small amounts of reordering.
This sentence and the introduction of the square root
to NKT imply that Chinese word order is close to
that of English, and they have to measure subtle
word order mistakes.
Table 3: NTCIR-7 meta-evaluation: Effects of square
root (b(x) = 1?
?
1? x)
NKT
?
NKT b(NKT)
Spearman w/ adequacy 0.940 0.940 0.922
Pearson w/ adequacy 0.922 0.817 0.941
Spearman w/ fluency 0.887 0.865 0.858
Pearson w/ fluency 0.931 0.917 0.833
In spite of these differences, the two groups inde-
pendently recognized the usefulness of rank correla-
tions for automatic evaluation of translation quality
for distant language pairs.
In their WMT-2010 paper (Birch and Osborne,
2010), they multiplied NKT with the brevity penalty
and interpolated it with BLEU for the WMT-2010
shared task. This fact implies that incomprehensible
or misleading word order mistakes are rare in trans-
lation among European languages.
6 Conclusions
When Statistical Machine Translation is applied to
distant language pairs such as Japanese and English,
word order becomes an important problem. SMT
systems often fail to find an appropriate translation
because of a large search space. Therefore, they
often output misleading or incomprehensible sen-
tences such as ?A because B? vs. ?B because A.? To
penalize such inadequate translations, we presented
an automatic evaluation method based on rank corre-
lation. There were two questions for this approach.
First, which correlation coefficient should we use:
Spearman?s ? or Kendall?s ?? Second, how should
we solve the overestimation problem caused by the
nature of one-to-one correspondence?
We answered these questions through our exper-
iments using the NTCIR-7 PATMT JE translation
data. For the first question, ? was slightly better
than ?, but ? was improved by precision. For the
second question, it turned out that BLEU?s Brevity
Penalty was counter-productive. A precision-based
penalty gave a better solution. With this precision-
based penalty, both ? and ? worked well and they
outperformed conventional methods for NTCIR-7
data. For similar language pairs, our method was
comparable to conventional evaluation methods. Fu-
951
ture work includes extension of the method so that it
can outperform conventional methods even for sim-
ilar language pairs.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgements. In Proc. of ACL
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and Summarization, pages 65?72.
Alexandra Birch and Miles Osborne. 2010. LRscore for
evaluating lexical and reordering quality in MT. In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 327?
332.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating reorder-
ing. Machine Translation, 24(1):15?26.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluatiing the role of Bleu in ma-
chine translation research. In Proc. of the Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 249?256.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Chrstof Monz, and Josh Schroeder. 2007.
(Meta-)Evaluation of machine translation. In Proc. of
the Workshop on Machine Translation (WMT), pages
136?158.
Etienne Denoual and Yves Lepage. 2005. BLEU in char-
acters: towards automatic MT evaluation in languages
without word delimiters. In Companion Volume to the
Proceedings of the Second International Joint Confer-
ence on Natural Language Processing, pages 81?86.
Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic
evaluation of machine translation based on recursive
acquisition of an intuitive common parts continuum.
In Proceedings of MT Summit XII Workshop on Patent
Translation, pages 151?158.
Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimohata,
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, and Noriko Kando. 2009. Meta-
evaluation of automatic evaluation methods for ma-
chine translation using patent translation data in ntcir-
7. In Proceedings of the 3rd Workshop on Patent
Translation, pages 9?16.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Work-
ing Notes of the NTCIR Workshop Meeting (NTCIR),
pages 389?400.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A simple re-
ordering rule for SOV languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 250?257.
Maurice G. Kendall. 1975. Rank Correlation Methods.
Charles Griffin.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proc. of the North American Chapter of the
Association of Computational Linguistics (NAACL),
pages 71?78.
Dan Melamed, Ryan Green, and Joseph P. Turian. 2007.
Precision and recall of machine translation. In Proc.
of NAACL-HLT, pages 61?63.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002a. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish Results. In
Proc. of the International Conference on Human Lan-
guage Technology Research (HLT), pages 132?136.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002b. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
952
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1515?1520,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Single-Document Summarization as a Tree Knapsack Problem
Tsutomu Hirao? Yasuhisa Yoshida? Masaaki Nishino? Norihito Yasuda? Masaaki Nagata?
?NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{hirao.tsutomu,yoshida.y,nishino.masaaki,
nagata.masaaki}@lab.ntt.co.jp
? Japan Science and Technology Agency
North 14 West 9, Sapporo, Hokkaido, 060-0814, Japan
yasuda@erato.ist.hokudai.ac.jp
Abstract
Recent studies on extractive text summariza-
tion formulate it as a combinatorial optimiza-
tion problem such as a Knapsack Problem, a
Maximum Coverage Problem or a Budgeted
Median Problem. These methods successfully
improved summarization quality, but they did
not consider the rhetorical relations between
the textual units of a source document. Thus,
summaries generated by these methods may
lack logical coherence. This paper proposes a
single document summarization method based
on the trimming of a discourse tree. This is
a two-fold process. First, we propose rules
for transforming a rhetorical structure theory-
based discourse tree into a dependency-based
discourse tree, which allows us to take a tree-
trimming approach to summarization. Sec-
ond, we formulate the problem of trimming
a dependency-based discourse tree as a Tree
Knapsack Problem, then solve it with integer
linear programming (ILP). Evaluation results
showed that our method improved ROUGE
scores.
1 Introduction
State-of-the-art extractive text summarization meth-
ods regard a document (or a document set) as a set
of textual units (e.g. sentences, clauses, phrases)
and formulate summarization as a combinatorial op-
timization problem, i.e. selecting a subset of the set
of textual units that maximizes an objective with-
out violating a length constraint. For example, Mc-
Donald (2007) formulated text summarization as a
Knapsack Problem, where he selects a set of textual
units that maximize the sum of significance scores
of each unit. Filatova et al (2004) proposed a
summarization method based on a Maximum Cov-
erage Problem, in which they select a set of textual
units that maximizes the weighted sum of the con-
ceptual units (e.g. unigrams) contained in the set.
Although, their greedy solution is only an approxi-
mation, Takamura et al (2009a) extended it to ob-
tain the exact solution. More recently, Takamura et
al. (2009b) regarded summarization as a Budgeted
Median Problem and obtain exact solutions with in-
teger linear programming.
These methods successfully improved ROUGE
(Lin, 2004) scores, but they still have one critical
shortcoming. Since these methods are based on sub-
set selection, the summaries they generate cannot
preserve the rhetorical structure of the textual units
of a source document. Thus, the resulting summary
may lack coherence and may not include significant
textual units from a source document.
One powerful and potential way to overcome the
problem is to include discourse tree constraints in
the summarization procedure. Marcu (1998) re-
garded a document as a Rhetorical Structure The-
ory (RST) (William Charles, Mann and Sandra An-
near, Thompson, 1988)-based discourse tree (RST-
DT) and selected textual units according to a prefer-
ence ranking derived from the tree structure to make
a summary. Daume? et al (2002) proposed a docu-
ment compression method that directly models the
probability of a summary given an RST-DT by us-
ing a noisy-channel model. These methods generate
well-organized summaries, however, since they do
not formulate summarizations as combinatorial op-
1515
Root?
N? S?
N? S? N? S?
S? N? N? S? S? N? S? N?
N? N?
N? S?
Elaboration?
Elaboration?
Background?
Elaboration?
Elaboration?
Contrast? Contrast?
Evidence?
Example?
Concession? Antithesis?
Figure 1: Example RST-DT from (Marcu, 1998).
timization problems, the optimality of the generated
summaries is not guaranteed.
In this paper, we propose a single document sum-
marization method based on the trimming of a dis-
course tree based on the Tree Knapsack Problem. If
a discourse tree explicitly represents parent-child re-
lationships between textual units, we can apply the
well-known tree-trimming approach to a discourse
tree and reap the benefit of combinatorial optimiza-
tion methods. In other words, to apply the tree-
trimming approach, we need a tree whose all nodes
represent textual units. Unfortunately, the RST-DT
does not allow it, because textual units in the RST-
DT are located only on leaf nodes and parent-child
relationship between textual units are represented
implicitly at higher positions in a tree. Therefore, we
first propose rules that transform an RST-DT into a
dependency-based discourse tree (DEP-DT) that ex-
plicitly defines the parent-child relationships. Sec-
ond, we treat it as a rooted subtree selection, in other
words, a Tree Knapsack Problem and formulate the
problem as an ILP.
2 From RST-DT to DEP-DT
2.1 RST-DT
According to RST, a document is represented as an
RST-DT whose terminal nodes correspond to ele-
mentary discourse units (EDU)s1 and whose non-
terminal nodes indicate the role of the contiguous
1EDUs roughly correspond to clauses.
Root?
N? S?
N? S? N? S?
S? N? N? S? S? N? S? N?
N? N?
N? S?
0	
1	
2	
4	 6	 7	
5	
3	
8	
10	
9	
11	
12	
13	
14	 15	 18	
16	
17	
Figure 2: Heads of non-terminal nodes.
EDUs namely, ?nucleus (N)? or ?satellite (S)?. A nu-
cleus is more important than a satellite in terms of
the writer?s purpose. That is, a satellite is a child of
a nucleus in the RST-DT. Some discourse relations
such as ?Elaboration?, ?Contrast? and ?Evidence? be-
tween a nucleus and a satellite or two nuclei are de-
fined. Figure 1 shows an example of an RST-DT.
2.2 DEP-DT
An RST-DT is not suitable for tree trimming because
it does not always explicitly define parent-child re-
lationships between textual units. For example, if
we consider how to trim the RST-DT in Figure 1,
when we drop e8, we have to drop e7 because of the
parent-child relationship defined between e7 and e8,
i.e. e7 is a satellite (child) of the nucleus (parent)
e8. On the other hand, we cannot judge whether we
have to drop e9 or e10 because the parent-child rela-
tionships are not explicitly defined between e8 and
e9, e8 and e10. This view motivates us to produce a
discourse tree that explicitly defines parent-child re-
lationships and whose root node represents the most
important EDU in a source document. If we can ob-
tain such a tree, it is easy to formulate summariza-
tion as a Tree Knapsack Problem.
To construct a discourse tree that represents
the parent-child relationships between EDUs, we
propose rules for transforming an RST-DT to a
dependency-based discourse tree (DEP-DT). The
procedure is defined as follows:
1. For each non-terminal node excluding the par-
1516
Depth=1
Depth=2
Depth=3
Depth=4
Figure 3: The DEP-DT obtained from the RST-DT in Fig-
ure 1.
ent of an EDU in the RST-DT, we define a
?head?. Here, a ?head? of a non-terminal node
is the leftmost descendant EDUwhose parent is
N. In Figure 2, ?H? indicates the ?head? of each
node.
2. For each EDU whose parent is N, we pick the
nearest S with a ?head? from the EDU?s ances-
tors and we add the EDU to the DEP-DT as a
child of the head of the S?s parent. If there is no
nearest S, the EDU is the root of the DEP-DT.
For example, in Figure 2, the nearest S to e3
that has a head is node 5 and the head of node
5?s parent is e2. Thus, e3 is a child of e2.
3. For each EDU whose parent is S, we pick the
nearest non-terminal with a ?head? from the an-
cestors and we add the EDU to the DEP-DT as
a child of the head of the non-terminal node.
For example, the nearest non-terminal node of
e9 that has a head is node 16 and the head of
node 16 is e10. Thus, e9 is a child of e10.
Figure 3 shows the DEP-DT obtained from the
RST-DT in Figure 1. The DEP-DT expresses the
parent-child relationship between the EDUs. There-
fore, we have to drop e7, e9 and e10 when we drop
e8. Note that, by applying the rules, discourse rela-
tions defined between non-terminals of an RST-DT
are eliminated. However, we believe that these re-
lations are no needed for the summarization that we
are attempting to realize.
3 Tree Knapsack Model for
Single-Document Summarization
3.1 Formalization
We denote T as a set of all possible rooted subtrees
obtained from a DEP-DT. F (t) is the significance
score for a rooted subtree t ? T and L is the maxi-
mum number of words allowed in a summary. The
optimal subtree t? is defined as follows:
t? = argmax
t?T
F (t) (1)
s.t. Length(t) ? L. (2)
Here, we define F (t) as
F (t) =
?
e?E(t)
W(e)
Depth(e)
. (3)
E(t) is the set of EDUs contained in t, Depth(e)
is the depth of an EDU e within the DEP-DT. For
example, Depth(e2) = 1, Depth(e6) = 4 for the
DEP-DT of Figure 3. W(e) is defined as follows:
W(e) =
?
w?W (e)
tf(w,D). (4)
W (e) is the set of words contained in e and
tf(w,D) is the term frequency of word w in a docu-
ment D.
3.2 ILP Formulation
We formulate the optimization problem in the pre-
vious section as a Tree Knapsack Problem, which is
a kind of Precedence-Constrained Knapsack Prob-
lem (Samphaiboon and Yamada, 2000) and we can
obtain the optimal rooted subtree by solving the fol-
lowing ILP problem2:
maximize
x
N
?
i=1
W(ei)
Depth(ei)
xi (5)
s.t.
N
?
i=1
?ixi ? L (6)
?i : xparent(i) ? xi (7)
?i : xi ? {0, 1}, (8)
2A similar approach has been applied to sentence compres-
sion (Filippova and Strube, 2008).
1517
ROUGE-1 ROUGE-2
F R F R
TKP(G) .310H,K,L .321G,H,K,L .108 .112H
TKP(H) .281H .284H .092 .093
Marcu(G) .291H .272H .101 .093
Marcu(H) .236 .219 .073 .068
MCP .279 .295H .073 .077
KP .251 .266H .071 .075
LEAD .255 .240 .092 .086
Table 1: ROUGE scores of the RST discourse treebank
dataset. In the table, G,H,K,L indicate a method sta-
tistically significant against Marcu(G), Marcu(H), KP,
LEAD, respectively.
where x is an N -dimensional binary vector that
represents the summary, i.e. xi=1 denotes that the i-
th EDU is included in the summary. N is the number
of EDUs in a document, ?i is the length (the number
of words) of the i-th EDU, and parent(i) indicates
the ID of the parent of the i-th EDU in the DEP-DT.
Constraint (6) ensures that the length of a summary
is less than limit L. Constraint (7) ensures that a
summary is a rooted subtree of the DEP-DT. Thus,
xparent(i) is always 1 when the i-th EDU is included
in the summary.
In general, the Tree Knapsack Problem is NP-
hard, but fortunately we can obtain the optimal solu-
tion in a feasible time by using ILP solvers for doc-
uments of practical tree size. In addition, bottom-
up DP (Lukes, 1974) and depth-first DP algorithms
(Cho and Shaw, 1997) are known to find the optimal
solution efficiently.
4 Experimental Evaluation
4.1 Settings
We conducted an experimental evaluation on the test
collection for single document summarization eval-
uation contained in the RST Discourse Treebank
(RST-DTB)(Carlson et al, 2001) distributed by the
Linguistic Data Consortium (LDC)3. The RST-DTB
Corpus includes 385 Wall Street Journal articles
with RST annotation, and 30 of these documents
also have one human-made reference summary. The
average length of the reference summaries corre-
sponds to about 10 % of the words in the source
3http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2002T07
document.
We compared our method (TKP) with Marcu?s
method (Marcu) (Marcu, 1998), a simple knapsack
model (KP), a maximum coverage model (MCP)
and a lead method (LEAD). MCP is known to be a
state-of-the-art method for multiple document sum-
marization and we believe that MCP also performs
well in terms of single document summarization.
LEAD is also a widely used summarizer that simply
takes the first K textual units of the document. Al-
though this is a simple heuristic rule, it is known as
a state-of-the-art summarizer (Nenkova and McKe-
own, 2011).
For our method, we examined two types of
DEP-DT. One was obtained from the gold RST-
DT. The other was obtained from the RST-DT pro-
duced by a state-of-the-art RST parser, HILDA (du-
Verle and Prendinger, 2009; Hernault et al, 2010).
For Marcu?s method, we examined both the gold
RST-DT and HILDA?s RST-DT. We re-implemented
HILDA and re-trained it on the RST-DT Corpus ex-
cluding the 30 documents used in the evaluation.
The F-score of the parser was around 0.5. For KP,
we exclude constraint (7) from the ILP formulation
of TKP and set the depth of all EDUs in equations
(3) and (5) at 1. For MCP, we use tf (equation (4))
as the word weight.
We evaluated the summarization systems with
ROUGE version 1.5.5 4. Performance metrics were
the recall (R) and F-score (F) of ROUGE-1,2.
4.2 Results and Discussion
Table 1 shows the evaluation results. In the ta-
ble, TKP(G) and TKP(H) denote methods with the
DEP-DT obtained from the gold RST-DT and from
HILDA, respectively. Marcu(G) and Marcu(H) de-
note Marcu?s method described in (Marcu, 1998)
with gold RST-DT and with HILDA, respectively.
We performed a multiple comparison test for the dif-
ferences among ROUGE scores, we calculated the p-
values between systems with the Wilcoxon signed-
rank test (Wilcoxon, 1945) and used the False Dis-
covery Rate (FDR) (Benjamini and Hochberg, 1995)
to calculate adjusted p-values, in order to limit false
positive rate to 5%.
From the table, TKP(G) and Marcu(G) achieved
4Options used: -n 2 -s -m -x
1518
Reference:
The Fuji apple may one day replace the Red Delicious as the number one U.S. apple. Since the Red Delicious has been
over-planted and prices have dropped to new lows, the apple industry seems ready for change. Along with growers, supermarkets
are also trying different varieties of apples. Although the Fuji is smaller and not as perfectly shaped as the Red Delicious, it is
much sweeter, less mealy and has a longer shelf life.
TKP(G):
We?ll still have mom and apple pie. A Japanese apple called the Fuji. Some fruit visionaries say the Fuji could someday tumble
the Red Delicious from the top of America?s apple heap. It has a long shelf life. Now, even more radical changes seem afoot. The
Delicious hegemony won?t end anytime soon. New apple trees grow slowly. But the apple industry is ripe for change. There?s a
Fuji apple cult.
Marcu(G):
We?ll still have mom and apple pie. On second thought, make that just mom. The Fuji could someday tumble the Red Delicious
from the top of America?s apple heap. Now, even more radical changes seem afoot. The Delicious hegemony won?t end anytime
soon. More than twice as many Red Delicious apples are grown as the Golden variety, America?s No. 2 apple. But the apple
industry is ripe for change.
MCP:
Called the Fuji. It has a long shelf life. New apple trees grow slowly. Its roots are patriotic. I?m going to have to get another job
this year. Scowls. They still buy apples mainly for big, red good looks. Japanese researchers have bred dozens of strains of Fujis.
Mr. Auvil, the Washington grower, says. Stores sell in summer. The ? big boss ? at a supermarket chain even rejected his Red
Delicious recently. Many growers employ.
LEAD:
Soichiro Honda?s picture now hangs with Henry Ford?s in the U.S. Automotive Hall of Fame, and the game-show ? Jeopardy ? is
soon to be Sony-owned. But no matter how much Japan gets under our skin, we?ll still have mom and apple pie. On second
thought, make that just mom. A Japanese apple called the Fuji is cropping up in orchards the way Hondas did on U.S. roads.
Figure 4: Summaries obtained from wsj 1128.
better results than MCP, KP and LEAD, although
some of the comparisons are not significant. In par-
ticular, TKP(G) achieved the highest ROUGE scores
on all measures. On ROUGE-1 Recall, TKP(G) sig-
nificantly outperformed Marcu(G), Marcu(H), KP
and LEAD. These results support the effectiveness
of our method that utilizes the discourse structure.
Comparing TKP(H) with Marcu(H), the former
achieved higher scores with statistical significance
on ROUGE-1. In addition, Marcu(H) was outper-
formed by MCP, KP and LEAD. The results confirm
the effectiveness of our summarization model and
trimming proposal for DEP-DT. Moreover, the dif-
ference between TKP(G) and TKP(H) was smaller
than that between Marcu(G) and Marcu(H). This
implies that our method is more robust against dis-
course parser error than Marcu?s method.
Figure 4 shows the example summaries gener-
ated by TKP(G), Marcu(G), MCP and LEAD, re-
spectively for an article, wsj 1128. Since TKP(G)
and Marcu(G) utilize a discourse tree, the summary
generated by TKP(G) is similar to that generated by
Marcu(G) but it is different from those generated by
MCP and LEAD.
5 Conclusion
This paper proposed rules for transforming an RST-
DT to a DEP-DT to obtain the parent-child relation-
ships between EDUs. We treated a single document
summarization method as a Tree Knapsack Problem,
i.e. the summarizer selects the best rooted subtree
from a DEP-DT. To demonstrate the effectiveness of
our method, we conducted an experimental evalua-
tion using 30 documents selected from the RST Dis-
course Treebank Corpus. The results showed that
our method achieved the highest ROUGE-1,2 scores.
References
Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the false discovery rate: A practical and powerful
approach to multiple testing. Journal of the Royal Sta-
tistical Society, Series B (Methodological), 57(1):289?
300.
Lynn Carlson, Daniel Marcu, andMary Ellen Okurowski.
2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proc. of
the SIGDIAL01, pages 1?10.
Geon Cho and Dong X Shaw. 1997. A depth-first
dynamic programming algorithm for the tree knap-
1519
sack problem. INFORMS Journal on Computing,
9(4):431?438.
Hal Daume? III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proc. of the 40th
ACL, pages 449?456.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine clas-
sification. In Proc. of the Joint Conference of the 47th
ACL and 4th IJCNLP, pages 665?673.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence extraction. In Proc. of the 20th COLING,
pages 397?403.
Katja Filippova and Michael Strube. 2008. Dependency
tree based sentence compression. In Proc. of the 5th
International Natural Language Generation Confer-
ence (INLG), pages 25?32.
Hugo Hernault, Helmut Prendinger, David A duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A discourse
parser using support vector machine classification. Di-
alogue and Discourse, 1(3):1?33.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proc. of Workshop on
Text Summarization Branches Out, pages 74?81.
J. A. Lukes. 1974. Efficient algorithm for the partition-
ing of trees. IBM Journal of Research and Develop-
ment, 18(3):217?224.
Daniel Marcu. 1998. Improving summarization through
rhetorical parsing tuning. In Proc. of the 6th Workshop
on Very Large Corpora, pages 206?215.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Proc.
of the 29th ECIR, pages 557?564.
Ani Nenkova and Kathaleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in In-
formation Retrieval, 5(2-3):103?233.
Natthawut Samphaiboon and Takeo Yamada. 2000.
Heuristic and exact algorithms for the precedence-
constrained knapsack problem. Journal of Optimiza-
tion Theory and Applications, 105(3):659?676.
Hiroya Takamura and Manabu Okumura. 2009a. Text
summarization model based on maximum coverage
problem and its variant. In Proc. of the 12th EACL,
pages 781?789.
Hiroya Takamura and Manabu Okumura. 2009b. Text
summarization model based on the budgeted median
problem. In Proceedings of the 18th CIKM.
Frank Wilcoxon. 1945. Individual comparisons by rank-
ing methods. Biometrics Bulletin, 1(6):80?83.
William Charles, Mann and Sandra Annear, Thompson.
1988. Rhetorical Structure Theory: Toward a func-
tional theory of text organization. Text, 8(3):243?281.
1520
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1834?1839,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Dependency-based Discourse Parser for Single-Document Summarization
Yasuhisa Yoshida, Jun Suzuki, Tsutomu Hirao, and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{yoshida.y,suzuki.jun,hirao.tsutomu,nagata.masaaki}@lab.ntt.co.jp
Abstract
The current state-of-the-art single-
document summarization method gen-
erates a summary by solving a Tree
Knapsack Problem (TKP), which is the
problem of finding the optimal rooted sub-
tree of the dependency-based discourse
tree (DEP-DT) of a document. We can
obtain a gold DEP-DT by transforming a
gold Rhetorical Structure Theory-based
discourse tree (RST-DT). However, there
is still a large difference between the
ROUGE scores of a system with a gold
DEP-DT and a system with a DEP-DT
obtained from an automatically parsed
RST-DT. To improve the ROUGE score,
we propose a novel discourse parser
that directly generates the DEP-DT. The
evaluation results showed that the TKP
with our parser outperformed that with
the state-of-the-art RST-DT parser, and
achieved almost equivalent ROUGE
scores to the TKP with the gold DEP-DT.
1 Introduction
Discourse structures of documents are believed
to be highly beneficial for generating informa-
tive and coherent summaries. Several discourse-
based summarization methods have been devel-
oped, such as (Marcu, 1998; Daum?e III and
Marcu, 2002; Hirao et al., 2013; Kikuchi et al.,
2014). Moreover, the current best ROUGE score
for the summarization benchmark data of the RST-
discourse Treebank (Carlson et al., 2002) has been
provided by (Hirao et al., 2013), whose method
also utilizes discourse trees. Thus, the discourse-
based summarization approach is one promising
way to obtain high-quality summaries.
One possible weakness of discourse-based sum-
marization techniques is that they rely greatly on
the accuracy of the discourse parser they use.
For example, the above discourse-based summa-
rization methods utilize discourse trees based on
the Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988) for their discourse information.
Unfortunately, the current state-of-the-art RST
parser, as described in (Hernault et al., 2010),
is insufficient as an off-the-shelf discourse parser.
In fact, there is empirical evidence that the qual-
ity (i.e., ROUGE score) of summaries from auto-
parsed discourse trees is significantly degraded
compared with those generated from gold dis-
course trees (Marcu, 1998; Hirao et al., 2013).
From this background, the goal of this paper
is to develop an appropriate discourse parser for
discourse-based summarization. We first focus on
one of the best discourse-based single document
summarization methods as proposed in (Hirao et
al., 2013). Their method formulates a single doc-
ument summarization problem as a Tree Knap-
sack Problem (TKP) over a dependency-based dis-
course tree (DEP-DT). In their method, DEP-DTs
are automatically transformed from (auto-parsed)
RST-discourse trees (RST-DTs) by heuristic rules.
Instead, we develop a DEP-DT parser, that di-
rectly provides DEP-DTs for their state-of-the-art
discourse-based summarization method. We show
that summaries generated by our parser improve
the ROUGE scores to almost the same level as
those generated by gold DEP-DTs. We also inves-
tigate the way in which the parsing accuracy helps
to improve the ROUGE scores.
2 Single-Document Summarization as a
Tree Knapsack Problem
Hirao et al. (2013) formulated single-document
summarization as a TKP that is run on the DEP-
DT. They obtained a summary by trimming the
DEP-DT, i.e. the summary is a rooted subtree of
the DEP-DT.
Suppose that we have N EDUs in a document,
1834
Root!
N! S!
N! S! N! S!
S! N! N! S! S! N! S! N!
N! N!
N! S!
Elaboration!
Background!
Elaboration!
Elaboration!
Contrast! Contrast!
Evidence!
Example!
Concession! Antithesis!
(a)
Background Elabora.on
Elabora.on
Elabora.on Elabora.on
Concession Example
Evidence
An.thesis
(b)
Background Elabora.on Elabora.on
Elabora.on Elabora.on Concession Example
Evidence An.thesis
(c)
Figure 1: Examples of RST-DT and DEP-DT. e
1
, ? ? ? , e
10
are EDUs. (a) Example of an RST-DT from
(Marcu, 1998). n
1
, ? ? ? , n
19
are the non-terminal nodes. (b) Example of the DEP-DT obtained from the
incorrect RST-DT that is made by swapping the Nucleus-Satellite relationship of the node n
2
and the
node n
3
. (c) The correct DEP-DT obtained from the RST-DT in (a).
and the i-th EDU e
i
has l
i
words. L is the maxi-
mum number of words allowed in a summary. In
the TKP, if we select e
i
, we need to select its par-
ent EDU in the DEP-DT. We denote parent(i) as
the index of the parent of e
i
in the DEP-DT. x is
an N -dimensional binary vector that represents a
summary, i.e. x
i
= 1 denotes that e
i
is included in
the summary. The TKP is defined as the following
ILP problem:
maximize
x
?
N
i=1
F (e
i
)x
i
s.t.
?
N
i=1
l
i
x
i
? L
?i : x
parent(i)
? x
i
?i : x
i
? {0, 1},
where F (e
i
) is the score of e
i
. We define F (e
i
) as
follows:
F (e
i
) =
?
w?W (e
i
)
tf(w,D)
Depth(e
i
)
,
where W (e
i
) is the set of words contained in e
i
.
tf(w,D) is the term frequency of word w in a doc-
ument D. Depth(e
i
) is the depth of e
i
in the DEP-
DT.
3 Tree Knapsack Problem with
Dependency-based Discourse Parser
3.1 Motivation
In (Hirao et al., 2013), they automatically ob-
tain the DEP-DT by transforming from the parsed
RST-DT. We simply followed their method for ob-
taining the DEP-DTs
1
. The transformation algo-
rithm can be found in detail in (Hirao et al., 2013).
Figure 1(a) shows an example of the RST-DT. Ac-
cording to RST, a document is represented as a tree
whose terminal nodes correspond to elementary
discourse units (EDUs) and whose non-terminal
nodes indicate the role of the contiguous EDUs,
namely, ?nucleus (N)? or ?satellite (S)?. Since a nu-
cleus is more important than a satellite in terms of
the writer?s purpose, a satellite is always a child of
a nucleus in the RST-DT. Some discourse relations
between a nucleus and a satellite or two nuclei are
defined.
Since the TKP of (Hirao et al., 2013) employs
a DEP-DT obtained from an automatically parsed
RST-DT, their method strongly relies on the ac-
curacy of the RST parser. For example, in Fig-
ure 1(a), if the RST-DT parser incorrectly sets
the node n
2
as Satellite and the node n
3
as Nu-
cleus, we obtain an incorrect DEP-DT in Figure
1(b) because the transformation algorithm uses
the Nucleus-Satellite relationships in the RST-DT.
The dependency relationships in Figure 1(b) are
quite different from that of the correct DEP-DT in
Figure 1(c). In this example, the parser failed to
determine the most salient EDU e
2
, that is the root
EDU of the gold DEP-DT. Thus, the summary ex-
tracted from this DEP-DT will have a low ROUGE
score.
The results motivated us to design a new dis-
course parser fully trained on the DEP-DTs and
1
Li et al. also defined a similar transformation algorithm
(Li et al., 2014). In this paper, we follow the transformation
algorithm defined in (Hirao et al., 2013).
1835
Discourse)Dependency)Parser
Document Summary
Discourse)Dependency)Parser
Tree)Knapsack)Problem
Parser)Training)Phase
Summariza;on)Phase
e2#e1# e3# e8# e10#e4# e5# e7# e9#e6#
Elabora3on# Elabora3on#Elabora3on# Elabora3on# An3thesis#Example#Evidence# Concession#
Background# e2#e1# e3# e8# e10#e4# e5# e7# e9#e6#
Elabora3on# Elabora3on#Elabora3on# Elabora3on# An3thesis#Example#Evidence# Concession#
Background# e2#e1# e3# e8# e10#e4# e5# e7# e9#e6#
Elabora3on# Elabora3on#Elabora3on# Elabora3on# An3thesis#Example#Evidence# Concession#
Background#
DEP=DTs
e2#e1# e3# e8# e10#e4# e5# e7# e9#e6#
Elabora3on# Elabora3on#Elabora3on# Elabora3on# An3thesis#Example#Evidence# Concession#
Background#DEP=DT
Root$N$ S$N$ S$ N$ S$S$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$Root$N$ S$N$ S$ N$ S$S$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$Root$N$ S$N$ S$ N$ SS$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$
RST=DTs
Transforma;on)Algorithm
(a)
e2#e1# e3# e8# e10#e4# e5# e7# e9#e6#
Elabora3on# Elabora3on#Elabora3on# Elabora3on# An3thesis#Example#Evidence# Concession#
Background#
RST$Parser
Document Root$N$ S$N$ S$ N$ S$S$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$ SummaryRST$Parser
Transforma3on$Algorithm Tree$Knapsack$Problem
Parser$Training$Phase
Summariza3on$Phase
Root$N$ S$N$ S$ N$ S$S$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$Root$N$ S$N$ S$ N$ S$S$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$Root$N$ S$N$ S$ N$ SS$ N$ N$ S$ S$ N$ S$ N$N$ N$N$ S$e1$ e2$ e3$ e4$ e5$ e6$ e7$ e8$ e9$ e10$
Elabora7on$Background$Elabora7on$Elabora7on$Contrast$Evidence$
Example$Concession$ An7thesis$
RST>DTs
RST>DT DEP>DT
(b)
Figure 2: (a) Overview of our proposed method. In the parser training phase, the parser is trained on
the DEP-DTs, and in the summarization phase, the document is directly parsed into the DEP-DT. (b)
Overview of (Hirao et al., 2013). In the parser training phase, the parser is trained on RST-DTs, and
in the summarization phase, the document is parsed into the RST-DT, and then transformed into the
DEP-DT.
that could directly generate the DEP-DT. Figure
2(a) shows an overview of the TKP combined with
our DEP-DT parser. In the parser training phase,
we transform RST-DTs into DEP-DTs, and di-
rectly train our parser with the DEP-DTs. In the
summarization phase, our method parses a raw
document directly into a DEP-DT, and generates
a summary with the TKP.
3.2 Description of Discourse Dependency
Parser
Our parser is based on the first-order Maximum
Spanning Tree (MST) algorithm (McDonald et al.,
2005b). Our parser extracts the features from the
EDU e
i
and the EDU e
j
. We use almost the fea-
tures as those shown in (Hernault et al., 2010).
Lexical N-gram features use the beginning (or
end) lexical N-grams (N ? {1, 2, 3}) in e
i
and
e
j
. We also include POS tags for the beginning
(or end) lexical N-grams (N ? {1, 2, 3}) in e
i
and
e
j
. Organizational features include the distance
between e
i
and e
j
. They also include the num-
ber of tokens, and features for identifying whether
or not e
i
and e
j
belong to the same sentence (or
paragraph). Soricut et al. (2003) introduced dom-
inance set features. They include syntactic labels
and the lexical heads of head and attachment nodes
along with their dominance relationship. We can-
not use the strong compositionality features and
rhetorical structure features described in (Her-
nault et al., 2010) because we have to know the
subtree structures in advance when using these
features.
To train the parser, we choose the Margin In-
fused Relaxed Algorithm (MIRA) (McDonald et
al., 2005a; Crammer et al., 2006). We denote
s(w,y) = w
T
f
y
as a score function given a
weight vector w and a DEP-DT y. L(y,y
?
) is
a loss function, and we define it as the number of
EDUs that have an incorrect parent EDU in a pre-
dicted DEP-DT y
?
= arg max
y
s(w,y). Then, we
solve the following optimization problem:
min
w
||w ?w
(t)
||
s.t. s(w,y) ? s(w,y
?
) ? L(y,y
?
),
(1)
where w
(t)
is a weight vector in the t-th iteration.
3.3 Redesign of Loss Function for Tree
Knapsack Problem
When we make a summary by solving a TKP, we
do not necessarily need a DEP-DT where all of the
parent-child relationships are correct. This is be-
cause we rarely select the EDUs around the leaves
in the DEP-DT. On the other hand, the parent-
child relationships around the root EDU in the
DEP-DT are important because we often select the
EDUs around the root EDU. Incorporating these
intuitions enables us to develop a DEP-DT parser
optimized for the TKP. To incorporate this infor-
mation, we define the following loss function:
L
Depth
(y,y
?
) =
?
(i,r,j)?y
[1 ? I(y
?
, i, j)]
Depth(e
i
)
, (2)
where I(y
?
, i, j) is an indicator function that
equals 1 if EDU e
j
is the parent of EDU e
i
in the
1836
DEP-DT y
?
and 0 otherwise. In Section 4, we re-
port results with the original loss function L(?, ?)
and with the modified loss function L
Depth
(?, ?).
4 Experimental Evaluation
4.1 Corpus
We used the RST-DT corpus (Carlson et al., 2002)
for our experimental evaluations. The corpus con-
sists of 385 Wall Street Journal articles with RST
annotation, and 30 of these documents also have
one human-made reference summary. We used
these 30 documents as the test documents for the
summarization evaluation, and used the remaining
355 RST annotated documents as the training data
for the parser. Note that we did not use the 30 test
documents for the summarization evaluation when
we trained the parser.
4.2 Summarization Evaluation
We compared the following three systems that dif-
fer in the way they obtain the DEP-DT.
TKP-GOLD Used a DEP-DT converted from a
gold RST-DT.
TKP-DIS-DEP Used a DEP-DT automatically
parsed by our discourse dependency-based
parser (DIS-DEP). Figure 2(a) shows an
overview of this system.
TKP-DIS-DEP-LOSS Used a DEP-DT automat-
ically parsed by our discourse dependency-
based parser (DIS-DEP). Figure 2(a) shows
an overview of this system. It is trained with
the loss function defined in equation (2).
TKP-HILDA Used a DEP-DT obtained by trans-
forming a RST-DT parsed by HILDA, a state-
of-the-art RST-DT parser (Hernault et al.,
2010). Figure 2(b) shows an overview of this
system.
Hirao et al. (2013) proved that TKP-HILDA
outperformed other methods including Marcu?s
method (Marcu, 1998), a simple knapsack model,
a maximum coverage model and LEAD method
that simply takes the first L tokens (L = summary
length). Thus, we only employed TKP-HILDA as
our baseline.
We follow the evaluation conditions described
in (Hirao et al., 2013). The number of tokens in
each summary is determined by the number in the
ROUGE-1 ROUGE-2
TKP-GOLD 0.321 0.112
TKP-DIS-DEP 0.319 0.109
TKP-DIS-DEP-LOSS 0.323 0.121
TKP-HILDA 0.284 0.093
Table 1: ROUGE Recall scores
human-annotated reference summary. The aver-
age length of the reference summaries corresponds
to about 10% of the words in the source document.
This is also the commonly used evaluation con-
dition for single-document summarization evalu-
ation on the RST-DT corpus. We employed the
recall of ROUGE-1, 2 as the evaluation measures.
Table 1 shows ROUGE scores on the RST-DT
corpus. We can see TKP-DIS-DEP and TKP-
DIS-DEP-LOSS outperformed TKP-HILDA, and
achieved almost the same ROUGE scores as TKP-
GOLD. Wilcoxon?s signed rank test in terms
of ROUGE rejected the null hypothesis, ?there
is a difference between TKP-HILDA and TKP-
DIS-DEP (or TKP-DIS-DEP-LOSS)? (Wilcoxon,
1945). This would be because test documents are
relatively small.
We analyzed the differences between the pro-
posed systems (TKP-DIS-DEP and TKP-DIS-
DEP-LOSS) and TKP-HILDA. First, we evaluated
the overlaps between the EDUs in summaries gen-
erated by the system and the EDUs in summaries
generated by TKP-GOLD. To see the overlaps, we
calculated the average F-value using Recall and
Precision defined as follows: Recall = |S
s
?
S
g
|/|S
g
|, Precision = |S
s
? S
g
|/|S
s
|, where S
s
is a set of EDUs in a summary generated by a sys-
tem, and S
g
a set of EDUs in a summary generated
by TKP-GOLD. The first line in Table 2 shows the
results. TKP-DIS-DEP and TKP-DIS-DEP-LOSS
outperformed TKP-HILDA as regards the aver-
age F-values. The result revealed that TKP-DIS-
DEP and TKP-DIS-DEP-LOSS have more EDUs
in common with TKP-GOLD than TKP-HILDA.
This result is evidence that TKP-DIS-DEP and
TKP-DIS-DEP-LOSS outperformed TKP-HILDA
in terms of ROUGE score.
Second, we evaluated the root accuracy (RA),
the rate at which a parser can find the root of DEP-
DTs. Since the root of a gold DEP-DT is the most
salient EDU in a document, it should be included
in the summary. The second line in Table 2 shows
that our methods succeeded in extracting the root
1837
TKP-DIS-DEP TKP-DIS-DEP-LOSS TKP-HILDA
Avg F-value 0.532
?
0.532
?
0.415
RA 0.933
?
0.933
?
0.733
Avg DAS 0.847
?
0.843
?
0.596
?: significantly better than TKP-HILDA (p < .05)
Table 2: Average F-value, Root Accuracy (RA), and average Dependency Accuracy in Summary (DAS).
Wilcoxon?s signed rank test in terms of average F-value, RA and DAS accepted the null hypothesis.
TKP-GOLD:
Elcotel Inc. expects fiscal second-quarter earnings to trail 1988 results. Elcotel, a telecommunications company, had net
income of $272,000, or five cents a share, in its year-earlier second quarter. The lower results, Mr. Pierce said. Elcotel will
also benefit from moving into other areas. Elcotel has also developed an automatic call processor. Automatic call processors
will provide that system for virtually any telephone, Mr. Pierce said, not just phones.
TKP-DIS-DEP, TKP-DIS-DEP-LOSS:
Elcotel Inc. expects fiscal second-quarter earnings to trail 1988 results. Elcotel, a telecommunications company, had net
income of $272,000, or five cents a share, in its year-earlier second quarter. George Pierce, chairman and chief executive officer,
said in an interview. Although Mr. Pierce expects that line of business to strengthen in the next year. Elcotel will also benefit
from moving into other areas. Elcotel has also developed an automatic call processor.
TKP-HILDA:
Elcotel Inc. expects fiscal second-quarter earnings to trail 1988 results. That several new products will lead to a ?much
stronger? performance in its second half. George Pierce, chairman and chief executive officer, said in an interview. Mr.
Pierce said Elcotel should realize a minimum of $10 of recurring net earnings for each machine each month. Elcotel has also
developed an automatic call processor. Automatic call processors will provide that system for virtually any telephone.
Figure 3: Summaries of wsj 2317. The sentences shown in bold-face are the root EDUs in each DEP-DT
of the summary.
of DEP-DT with high accuracy.
Third, to evaluate the coherency of the gener-
ated summaries, we compared the average Depen-
dency Accuracy in Summary (DAS), which is de-
fined as follows:
DAS(S) =
1
|S|
?
e?S
?(e),
?(e) =
{
1 (if parent(e) ? S)
0 (otherwise),
where S is a set of EDUs contained in the sum-
mary and parent(e) returns the parent EDU of e
in the gold DEP-DT. DAS(S) measures the rate of
the correct parent-child relationships in S. When
DAS equals 1, the summary is a rooted subtree of
the gold DEP-DT. The third line in Table 2 shows
the results. The results demonstrate that the sum-
maries generated by TKP-DIS-DEP or TKP-DIS-
DEP-LOSS tend to preserve the upper level depen-
dency relationships between the EDUs within the
gold DEP-DT.
Figure 3 shows summaries of wsj 2317 gener-
ated by the three systems. The EDUs correspond-
ing to the root of the DEP-DT are used in each
system shown in boldface. We can see that the
root EDU in the gold DEP-DT is found in the
summaries generated by TKP-DIS-DEP and TKP-
DIS-DEP-LOSS, but not in the summary gener-
ated by TKP-HILDA.
5 Conclusion
In this paper, we proposed a novel dependency-
based discourse parser for single-document sum-
marization. The parser enables us to obtain the
DEP-DT without transforming the RST-DT. The
evaluation results showed that the TKP with our
parser outperformed that with the state-of-the-art
RST-DT parser, and achieved almost equivalent
ROUGE scores to the TKP with the gold DEP-DT.
References
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2002. Rst discourse treebank,
ldc2002t07.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research, 7:551?585.
1838
Hal Daum?e III and Daniel Marcu. 2002. A noisy-
channel model for document compression. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 449
? 456, Philadelphia, PA, July 6 ? 12.
Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka,
et al. 2010. Hilda: a discourse parser using support
vector machine classification. Dialogue and Dis-
course, 1(3).
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,
Norihito Yasuda, and Masaaki Nagata. 2013.
Single-document summarization as a tree knapsack
problem. In Proceedings of the 2013 Conference on
EMNLP, pages 1515?1520.
Yuta Kikuchi, Tsutomu Hirao, Hiroya Takamura, Man-
abu Okumura, and Masaaki Nagata. 2014. Single
document summarization based on nested tree struc-
ture. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 315?320, Baltimore,
Maryland, June. Association for Computational Lin-
guistics.
Sujian Li, Liang Wang, Ziqiang Cao, and Wenjie Li.
2014. Text-level discourse dependency parsing. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 25?35, Baltimore, Maryland,
June. Association for Computational Linguistics.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Daniel Marcu. 1998. Improving summarization
through rhetorical parsing tuning. In Proc. of The
6th Workshop on VLC, pages 206?215.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?05, pages 91?98, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics.
Frank Wilcoxon. 1945. Individual Comparisons by
Ranking Methods. Biometrics Bulletin, 1(6):80?83,
December.
1839
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 349?353,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Sentence Compression with Semantic Role Constraints
Katsumasa Yoshikawa
Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
IBM Research-Tokyo, IBM Japan, Ltd.
katsumasay@gmail.com
Ryu Iida
Department of Computer Science,
Tokyo Institute of Technology, Japan
ryu-i@cl.cs.titech.ac.jp
Tsutomu Hirao
NTT Communication Science Laboratories,
NTT Corporation, Japan
hirao.tsutomu@lab.ntt.co.jp
Manabu Okumura
Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
oku@lr.pi.titech.ac.jp
Abstract
For sentence compression, we propose new se-
mantic constraints to directly capture the relations
between a predicate and its arguments, whereas
the existing approaches have focused on relatively
shallow linguistic properties, such as lexical and
syntactic information. These constraints are based
on semantic roles and superior to the constraints
of syntactic dependencies. Our empirical eval-
uation on the Written News Compression Cor-
pus (Clarke and Lapata, 2008) demonstrates that
our system achieves results comparable to other
state-of-the-art techniques.
1 Introduction
Recent work in document summarization do not
only extract sentences but also compress sentences.
Sentence compression enables summarizers to re-
duce the redundancy in sentences and generate in-
formative summaries beyond the extractive summa-
rization systems (Knight and Marcu, 2002). Con-
ventional approaches to sentence compression ex-
ploit various linguistic properties based on lexical
information and syntactic dependencies (McDonald,
2006; Clarke and Lapata, 2008; Cohn and Lapata,
2008; Galanis and Androutsopoulos, 2010).
In contrast, our approach utilizes another property
based on semantic roles (SRs) which improves weak-
nesses of syntactic dependencies. Syntactic depen-
dencies are not sufficient to compress some complex
sentences with coordination, with passive voice, and
with an auxiliary verb. Figure 1 shows an example
with a coordination structure. 1
1This example is from Written News Compression Cor-
pus (http://jamesclarke.net/research/resources).
Figure 1: Semantic Role vs. Dependency Relation
In this example, a SR labeler annotated thatHarari
is an A0 argument of left and an A1 argument of
became. Harari is syntactically dependent on left ?
SBJ(left-2, Harari-1). However, Harari is not depen-
dent on became and we are hence unable to utilize a
dependency relation between Harari and became di-
rectly. SRs allow us to model the relations between
a predicate and its arguments in a direct fashion.
SR constraints are also advantageous in that we
can compress sentences with semantic information.
In Figure 1, became has three arguments, Harari as
A1, businessman as A2, and shortly afterward as
AM-TMP. As shown in this example, shortly after-
word can be omitted (shaded boxes). In general,
modifier arguments like AM-TMP or AM-LOC are
more likely to be reduced than complement cases
like A0-A4. We can implement such properties by
SR constraints.
Liu and Gildea (2010) suggests that SR features
contribute to generating more readable sentence in
machine translation. We expect that SR features also
help our system to improve readability in sentence
compression and summarization.
2 Why are Semantic Roles Useful for Com-
pressing Sentences?
Before describing our system, we show the statis-
tics in terms of predicates, arguments and their rela-
349
Label In Compression / Total Ratio
A0 1454 / 1607 0.905
A1 1916 / 2208 0.868
A2 427 / 490 0.871
AM-TMP 261 / 488 0.535
AM-LOC 134 / 214 0.626
AM-ADV 115 / 213 0.544
AM-DIS 8 / 85 0.094
Table 1: Statistics of Arguments in Compression
tions in the Written News Compression (WNC) Cor-
pus. It has 82 documents (1,629 sentences). We di-
vided them into three: 55 documents are used for
training (1106 sentences); 10 for development (184
sentences); 17 for testing (339 sentences).
Our investigation was held in training data. There
are 3137 verbal predicates and 7852 unique argu-
ments. We performed SR labeling by LTH (Johans-
son and Nugues, 2008), an SR labeler for CoNLL-
2008 shared task. Based on the SR labels annotated
by LTH, we investigated that, for all predicates in
compression, how many their arguments were also
in. Table 1 shows the survival ratio of main argu-
ments in compression. Labels A0, A1, and A2 are
complement case roles and over 85% of them survive
with their predicates. On the other hand, for modifier
arguments (AM-X), survival ratios are down to lower
than 65%. Our SR constraints implement the differ-
ence of survival ratios by SR labels. Note that de-
pendency labels SBJ and OBJ generally correspond
to SR labels A0 and A1, respectively. But their total
numbers are 777 / 919 (SBJ) and 918 / 1211 (OBJ)
and much fewer than A0 and A1 labels. Thus, SR la-
bels can connect much more arguments to their pred-
icates.
3 Approach
This section describes our new approach to sen-
tence compression. In order to introduce rich syn-
tactic and semantic constraints to a sentence com-
pression model, we employ Markov Logic (Richard-
son and Domingos, 2006). Since Markov Logic sup-
ports both soft and hard constraints, we can imple-
ment our SR constraints in simple and direct fash-
ion. Moreover, implementations of learning and
inference methods are already provided in existing
Markov Logic interpreters such as Alchemy 2 and
Markov thebeast. 3 Thus, we can focus our effort
2http://alchemy.cs.washington.edu/
3http://code.google.com/p/thebeast/
on building a set of formulae called Markov Logic
Network (MLN). So, in this section, we describe our
proposed MLN in detail.
3.1 Proposed Markov Logic Network
First, let us define our MLN predicates. We sum-
marize the MLN predicates in Table 2. We have only
one hidden MLN predicate, inComp(i) which mod-
els the decision we need to make: whether a token i
is in compression or not. The other MLN predicates
are called observed which provide features. With our
MLN predicates defined, we can now go on to in-
corporate our intuition about the task using weighted
first-order logic formulae. We define SR constraints
and the other formulae in Sections 3.1.1 and 3.1.2,
respectively.
3.1.1 Semantic Role Constraints
Semantic role labeling generally includes the three
subtasks: predicate identification; argument role la-
beling; sense disambiguation. Our model exploits
the results of predicate identification and argument
role labeling. 4 pred(i) and role(i, j, r) indicate the
results of predicate identification and role labeling,
respectively.
First, the formula describing a local property of a
predicate is
pred(i) ? inComp(i) (1)
which denotes that, if token i is a predicate then i is
in compression. A formula with exact one hidden
predicate is called local formula.
A predicate is not always in compression. The for-
mula reducing some predicates is
pred(i) ? height(i,+n) ? ?inComp(i) (2)
which implies that a predicate i is not in compression
with n height in a dependency tree. Note the + nota-
tion indicates that the MLN contains one instance of
the rule, with a separate weight, for each assignment
of the variables with a plus sign.
As mentioned earlier, our SR constraints model
the difference of the survival rate of role labels in
compression. Such SR constraints are encoded as:
role(i, j, +r) ? inComp(i) ? inComp( j) (3)
role(i, j,+r) ? ?inComp(i) ? ?inComp( j) (4)
which represent that, if a predicate i is (not) in com-
pression, then its argument j is (not) also in with
4Sense information is too sparse because the size of the
WNC Corpus is not big enough.
350
predicate definition
inComp(i) Token i is in compression
pred(i) Token i is a predicate
role(i, j, r) Token i has an argument j with role r
word(i, w) Token i has word w
pos(i, p) Token i has Pos tag p
dep(i, j, d) Token i is dependent on token j with
dependency label d
path(i, j, l) Tokens i and j has syntactic path l
height(i, n) Token i has height n in dependency tree
Table 2: MLN Predicates
role r. These formulae are called global formulae
because they have more than two hidden MLN pred-
icates. With global formulae, our model makes two
decisions at a time. When considering the example
in Figure 1, Formula (3) will be grounded as:
role(9, 1, A0) ? inComp(9) ? inComp(1) (5)
role(9, 7, AM-TMP) ? inComp(9) ? inComp(7). (6)
In fact, Formula (5) gains a higher weight than For-
mula (6) by learning on training data. As a re-
sult, our system gives ?1-Harari? more chance to
survive in compression. We also add some exten-
sions of Formula (3) combined with dep(i, j, +d) and
path(i, j, +l) which enhance SR constraints. Note, all
our SR constraints are ?predicate-driven? (only ?
not ? as in Formula (13)). Because an argument is
usually related to multiple predicates, it is difficult to
model ?argument-driven? formula.
3.1.2 Lexical and Syntactic Features
For lexical and syntactic features, we mainly refer
to the previous work (McDonald, 2006; Clarke and
Lapata, 2008). The first two formulae in this sec-
tion capture the relation of the tokens with their lexi-
cal and syntactic properties. The formula describing
such a local property of a word form is
word(i,+w) ? inComp(i) (7)
which implies that a token i is in compression with a
weight that depends on the word form.
For part-of-speech (POS), we add unigram and bi-
gram features with the formulae,
pos(i, +p) ? inComp(i) (8)
pos(i, +p1) ? pos(i + 1,+p2) ? inComp(i). (9)
POS features are often more reasonable than word
form features to combine with the other properties.
The formula,
pos(i, +p) ? height(i, +n) ? inComp(i). (10)
is a combination of POS features and a height in a
dependency tree.
The next formula combines POS bigram features
with dependency relations.
pos(i,+p1) ? pos( j, +p2) ?
dep(i, j,+d) ? inComp(i). (11)
Moreover, our model includes the following
global formulae,
dep(i, j,+d) ? inComp(i) ? inComp( j) (12)
dep(i, j,+d) ? inComp(i) ? inComp( j) (13)
which enforce the consistencies between head and
modifier tokens. Formula (12) represents that if
we include a head token in compression then its
modifier must also be included. Formula (13) en-
sures that head and modifier words must be simul-
taneously kept in compression or dropped. Though
Clarke and Lapata (2008) implemented these depen-
dency constraints by ILP, we implement them by
soft constraints of MLN. Note that Formula (12) ex-
presses the same properties as Formula (3) replacing
dep(i, j, +d) by role(i, j,+r).
4 Experiment and Result
4.1 Experimental Setup
Our experimental setting follows previous
work (Clarke and Lapata, 2008). As stated in
Section 2, we employed the WNC Corpus. For
preprocessing, we performed POS tagging by
stanford-tagger. 5 and dependency parsing by
MST-parser (McDonald et al, 2005). In addition,
LTH 6 was exploited to perform both dependency
parsing and SR labeling. We implemented our
model by Markov Thebeast with Gurobi optimizer. 7
Our evaluation consists of two types of automatic
evaluations. The first evaluation is dependency based
evaluation same as Riezler et al (2003). We per-
formed dependency parsing on gold data and system
outputs by RASP. 8 Then we calculated precision, re-
call, and F1 for the set of label(head, modi f ier).
In order to demonstrate how well our SR con-
straints keep correct predicate-argument structures
in compression, we propose SRL based evalua-
tion. We performed SR labeling on gold data
5http://nlp.stanford.edu/software/tagger.shtml
6http://nlp.cs.lth.se/software/semantic_
parsing:_propbank_nombank_frames
7http://www.gurobi.com/
8http://www.informatics.susx.ac.uk/research/
groups/nlp/rasp/
351
Original [A0 They] [pred say] [A1 the refugees will enhance productivity and economic growth].
MLN with SRL [A0 They] [pred say] [A1 the refugees will enhance growth].
Gold Standard [A1? the refugees will enhance productivity and growth].
Original [A0 A ?16.1m dam] [AM?MOD will] [pred hold] back [A1 a 2.6-mile-long artificial lake to be
known as the Roadford Reservoir].
MLN with SRL [A0 A dam] will [pred hold] back [A1 a artificial lake to be known as the Roadford Reservoir].
Gold Standard [A0 A ?16.1m dam] [AM?MOD will] [pred hold back [A1 a 2.6-mile-long Roadford Reservoir].
Table 4: Analysis of Errors
Model CompR F1-Dep F1-SRL
McDonald 73.6% 38.4% 49.9%
MLN w/o SRL 68.3% 51.3% 57.2%
MLN with SRL 73.1% 58.9% 64.1%
Gold Standard 73.3% ? ?
Table 3: Results of Sentence Compression
and system outputs by LTH. Then we calculated
precision, recall, and F1 value for the set of
role(predicate, argument).
The training time of our MLN model are approx-
imately 8 minutes on all training data, with 3.1GHz
Intel Core i3 CPU and 4G memory. While the pre-
diction can be done within 20 seconds on the test
data.
4.2 Results
Table 3 shows the results of our compression
models by compression rate (CompR), dependency-
based F1 (F1-Dep), and SRL-based F1 (F1-SRL). In
our experiment, we have three models. McDonald
is a re-implementation of McDonald (2006). Clarke
and Lapata (2008) also re-implemented McDonald?s
model with an ILP solver and experimented it on the
WNC Corpus. 9 MLN with SRL and MLN w/o
SRL are our Markov Logic models with and with-
out SR Constraints, respectively.
Note our three models have no constraint for the
length of compression. Therefore, we think the com-
pression rate of the better system should get closer to
that of human compression. In comparison between
MLNmodels and McDonald, the former models out-
perform the latter model on both F1-Dep and F1-
SRL. Because MLN models have global constraints
and can generate syntactically correct sentences.
Our concern is how a model with SR constraints
is superior to a model without them. MLN with
SRL outperforms MLN without SRL with a 7.6
points margin (F1-Dep). The compression rate of
MLN with SRL goes up to 73.1% and gets close
9Clarke?s re-implementation got 60.1% for CompR and
36.0%pt for F1-Dep
to that of gold standard. SRL-based evaluation also
shows that SR constraints actually help extract cor-
rect predicate-argument structures. These results are
promising to improve readability.
It is difficult to directly compare our results with
those of state-of-the-art systems (Cohn and Lapata,
2009; Clarke and Lapata, 2010; Galanis and An-
droutsopoulos, 2010) since they have different test-
ing sets and the results with different compression
rates. However, though our MLN model with SR
constraints utilizes no large-scale data, it is the only
model which achieves close on 60% in F1-Dep.
4.3 Error Analysis
Table 4 indicates two critical examples which our
SR constraints failed to compress correctly. For the
first example, our model leaves an argument with its
predicate because our SR constraints are ?predicate-
driven?. In addition, ?say? is the main verb in this
sentence and hard to be deleted due to the syntactic
significance.
The second example in Table 4 requires to iden-
tify a coreference relation between artificial lake and
Roadford Reservour. We consider that discourse
constraints (Clarke and Lapata, 2010) help our model
handle these cases. Discourse and coreference infor-
mation enable our model to select important argu-
ments and their predicates.
5 Conclusion
In this paper, we proposed new semantic con-
straints for sentence compression. Our model with
global constraints of semantic roles selected correct
predicate-argument structures and successfully im-
proved performance of sentence compression.
As future work, we will compare our model with
the other state-of-the-art systems. We will also inves-
tigate the correlation between readability and SRL-
based score by manual evaluations. Furthermore, we
would like to combine discourse constraints with SR
constraints.
352
References
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31(1):399?429.
James Clarke and Mirella Lapata. 2010. Discourse con-
straints for document compression. Computational
Linguistics, 36(3):411?441.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
the 22nd International Conference on Computational
Linguistics-Volume 1, pages 137?144. Association for
Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 885?893, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning, pages 183?187. Association for
Computational Linguistics.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelligence,
139(1):91?107.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 716?724, Beijing, China,
August. Coling 2010 Organizing Committee.
RyanMcDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajic?. 2005. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proceedings of the
conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, HLT
?05, pages 523?530, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL, pages 297?304.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
118?125. Association for Computational Linguistics.
353
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 212?216,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Latent Semantic Matching: Application to Cross-language Text
Categorization without Alignment Information
Tsutomu Hirao and Tomoharu Iwata and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{hirao.tsutomu,iwata.tomoharu,nagata.masaaki}@lab.ntt.co.jp
Abstract
Unsupervised object matching (UOM) is
a promising approach to cross-language
natural language processing such as bilin-
gual lexicon acquisition, parallel corpus
construction, and cross-language text cat-
egorization, because it does not require
labor-intensive linguistic resources. How-
ever, UOM only finds one-to-one corre-
spondences from data sets with the same
number of instances in source and target
domains, and this prevents us from ap-
plying UOM to real-world cross-language
natural language processing tasks. To al-
leviate these limitations, we proposes la-
tent semantic matching, which embeds
objects in both source and target lan-
guage domains into a shared latent topic
space. We demonstrate the effectiveness
of our method on cross-language text cat-
egorization. The results show that our
method outperforms conventional unsu-
pervised object matching methods.
1 Introduction
Unsupervised object matching is a method for
finding one-to-one correspondences between ob-
jects across different domains without knowledge
about the relation between the domains. Kernel-
ized sorting (Novi et al, 2010) and canonical cor-
relation analysis based methods (Haghighi et al,
2008; Tripathi et al, 2010) are two such exam-
ples of unsupervised object matching, which have
been shown to be quite useful for cross-language
natural language processing (NLP) tasks. One of
the most important properties of the unsupervised
object matching is that it does not require any lin-
guistic resources which connects between the lan-
guages. This distinguishes it from other cross-
language NLP methods such as machine transla-
tion based and projection based approaches (Du-
mais et al, 1996; Gliozzo and Strapparava, 2005;
Platt et al, 2010), which we need bilingual dictio-
naries or parallel sentences.
When we apply unsupervised object matching
methods to cross-language NLP tasks, there are
two critical problems. The first is that they only
find one-to-one matching. The second is they re-
quire the same size of source- and target-data. For
example, the correct translation of a word is not
always unique. French words ?maison?, ?appart-
ment? and ?domicile? can be regarded as transla-
tion of an English word ?home?. In addition, En-
glish vocabulary size is not equal to that of French.
These discussions motivate us to introduce a
shared space in which both source and target do-
main objects will reside. If we can obtain such
a shared space, we can match objects within the
space, because we can use standard distance met-
rics on this space. This will also enable us to use
various kinds of non-strict matching. For exam-
ple, k-nearest objects in the source domain will be
retrieved for a query object in the target domain.
In this paper, we propose a simple but effective
method to find the shared space by assuming that
two languages have common latent topics, which
we call latent semantic matching. With latent se-
mantic matching, we first find latent topics in two
domains independently. Then, the topics in two
domains are aligned by kernelized sorting, and ob-
jects are embedded in a shared latent topic space.
Latent topic representations are successfully used
in a wide range of NLP tasks, such as information
retrieval and text classification, because they rep-
resent intrinsic information of documents (Deer-
wester et al, 1990). By matching latent topics,
we can find relation between source and target do-
mains, and additionally we can handle different
numbers of objects in two domains.
We compared latent semantic matching with
conventional unsupervised object matching meth-
212
ods on the task of cross-language text categoriza-
tion, i.e. classifying target side unlabeled docu-
ments by label information obtained from source
side documents. The results show that, with more
source side documents, our method achieved the
highest classification accuracy.
2 Related work
Many cross-language text processing methods
have been proposed that require correspondences
between source and target languages. For exam-
ple, (Dumais et al, 1996) proposed cross-lingual
latent semantic indexing, and (Platt et al, 2010)
employed oriented principle component analysis
and canonical correlation analysis (CCA). They
concatenate the document pairs (source document
and its translation) obtained from a document-
level parallel corpus. They then apply multi-
variate analysis to acquire the translingual projec-
tion. There are extensions of latent Dirichlet alo-
cation (LDA) (Blei et al, 2003) for cross-language
analysis, such as multilingual topic models (Boyd-
Graber and Blei, 2009), joint LDA (Jagadeesh
and Daume III, 2010) and multilingual LDA (Xi-
aochuan et al, 2011). They require a bilingual dic-
tionary or document-level parallel corpora.
Unsupervised object matching methods have
been proposed recently (Novi et al, 2010;
Haghighi et al, 2008; Yamada and Sugiyama,
2011). These methods are promising in terms of
language portability because they do not require
external language resources. (Novi et al, 2010)
proposed kernelized sorting (KS); it finds one-to-
one correspondences between objects in different
domains by permuting a set to maximize the de-
pendence between two sets. Here, the Hilbert-
Schmidt independence criterion is used for mea-
suring dependence. (Djuric et al, 2012) proposed
convex kernelized sorting as an extension of KS.
(Yamada and Sugiyama, 2011) proposed least-
squares object matching which maximizes the
squared-loss mutual information between matched
pairs. (Haghighi et al, 2008) proposed another
framework, matching CCA (MCCA), based on a
probabilistic interpretation of CCA (Bach and Jor-
dan, 2005). MCCA simultaneously finds latent
variables that represent correspondences and la-
tent features so that the latent features of corre-
sponding examples exhibit the maximum correla-
tion. However, these unsupervised object match-
ing methods have limitations. They require that
the source and target domains have the same data
size, and they find one-to-one correspondences.
There are critical weaknesses of these methods
when we attempt to apply them to real world
cross-language NLP applications.
3 Latent Semantic Matching
We propose latent semantic matching to find a
shared latent space by assuming that two lan-
guages have common latent topics. Our method
consists of following four steps: (1) for both
source and target domains, we map the documents
to a K-dimensional latent topic space indepen-
dently, (2) we find the one-to-one correspondences
between topics across source and target domains
by unsupervised object matching, (3) we permute
topics of the target side according to the corre-
spondences, while fixing the topics of the source
side, and (4) finally, we map documents in the
source and target domains to a shared latent space
by using permuted and fixed topics.
3.1 Topic Extraction as Dimension Reduction
Suppose that we have N documents in the source
domain. sn=(sni)Ii=1 is the nth document rep-
resented as a multi-dimensional column vector in
the domain, i.e. each document is represented as
a bag-of-words vector. Here, each element of the
vectors indicates the TF?IDF score of the corre-
sponding word in the document. I is the size of the
feature set, i.e., the vocabulary size in the source
domain. Also, we have M documents in the tar-
get domain. tm=(tmj)Jj=1 is the mth document
represented as a multi-dimensional vector. J is
the vocabulary size in the target domain. Thus,
the data set in the source domain is represented by
an I ? N matrix, S=(s1, ? ? ? , sN ), the data set
in the target is represented by a J ? M matrix,
T=(t1, ? ? ? , tM ).
We factorize these matrices using nonnegative
matrix factorization (Lee and Seung, 2000) to find
topics as follows:
S ?WSHS , (1)
T ?WTHT . (2)
WS is an I?K matrix that represents a set of top-
ics, i.e. each column vector denotes word weights
for each topic. HS is a K ? N matrix that de-
notes a set of latent semantic representations of
documents in the source domain, i.e. each row
213
?????? WS HS? * = 0 0 1 00 1 0 00 0 0 11 0 0 0
ment. I is the size of feature set, i.e., the size of vocabulary inthe source domain. Also, we have M documents in a targetdomain. tm = (tmj)Jj=1 is the m-th document representedas a multi-dimensional vector. J is the size of vocabulary inthe target domain. Thus, the data set in the source domain isrepresented as the I ?N matrix, S, the data set in the targetis represented as the J ?M matrix, T .Here, we assume that these matrices are approximated asthe product of low rank matrices as follows:
S ? WSHS , (1)T ? WTHT (2)WS is I?K matrix, which represents a set of topic propor-tions in the source domain, i.e., each column vector denotestopic proportion. HS is K ? N matrix, which denotes a setof documents in the K-dimensional latent space which cor-responds to the source domain, i.e., each row vector denotesthe document in the latent space. The k(1 ? k ? K)-th basisin the latent space corresponds to the k-th topic proportion.WT is I ? K matrix, which represents a set of topic pro-portions in the target domain. HT is K ? N matrix, whichdenotes a set of documents in the latent topic space with di-mentionaly K. K is less than I , J . In this paper, we employNon-negative Matrix Factorization (NMF) [Lee and Seung,2000] to factorize the original matrices.According to the factorization of the original matrices, wecan map the documents in the source and target domain tolatent topic space with dimentionaly K, independently.3.2 Finding Optimal Topic Alignments byUnsupervised Object MatchingTo connect the different latent space, the basis of the spacehave to be aligned each other. That is, topic proportion ex-tracted from the source language must be aligned that fromthe target language. This is reasonable consideration becausewe can assume the same latent concept for both language.For example, a topic proportion obtained from English docu-ments can be aligned a topic proportion obtained from Frenchdocuments. For all k and k?, k-th column vector in WS arealigned k?-th column vector in WT .However, we can not measure similarity between the topicproportions because we do not have any language resourcessuch as dictionary. Therefore, we utilize unsupervised ob-ject matching method to find one-to-one correspondences be-tween topic proportions. In this paper, we employ KernelizedSorting (KS) [Novi et al, 2010]. Of cource, we can replaceKS to another unsupervised object matching sush as MCCA[Haghighi et al, 2008], LSOM [Yamada and Sugiyama,2011].KS finds the best one-to-one matching by followings:
pi? = argmaxpi??K tr(G?SpiTG?Tpi),s.t. pi1K = 1K and piT1K = 1K . (3)pi is K?K matrix which represents one-to-one correspon-dence between topic proportion, i.e., piij = 1 indicates i-thtopic proportion in the source language corresponds to j-th
one of the target language. ? indicates set of all possibleK ? K matrices which store one-to-one corresponrence. GdenotesK?K kernel matrix obtained from topic proportion,Gij = K(WTi,:,W:,j), and G? is the centerd matrix of G. K(, )is a kernel function. 1K is K-dimensional column vector ofall ones. pi? is obtained by iterative procedure. According topi?, we can permutate the basis of the latent space obtainedfrom source language. See fig hoge.
S ? WSHS . (4)On the other hand, we can directly fomulate objective func-tion of unsupervised mapping. If the topic proportions arealigned each other, the correlation matrix (or gram matrix)obtained from source language is proportional to one fromtarget language:
||GS ? ?GT ||2 = 0. (5)? denotes the hyperparameter for tuning the socore range be-tween two gram matrices.By minimize the error of the matrix factorization (equa-tion (1),(2)) and the difference between correlation matrices(equation (6)), the objective function is defined as follow:
E = ?S ?WSHS?2+ ?T ?WTHT ?2+ ?||GS ? ?GT ||2. (6)
? is cost parameter between first, second argu-ment and third argument. The optimal parameters(WS,WT ,HS,HT ) are obtained by minimizing theobjective function. To mimimize the objective, gradient de-scend can be used. but However that is not convex function,we only obtained local optimal. Thefore, we employed abovetwo step procedure??????This objective function is not convex. That meanswe can only obtain local optimal parameters. By min-imizing equation (6), we can obtain a set of parameter(WS,WT ,HS,HT ) for unsupervised mapping. we couldbe employed gradient based algorithm but, as the first step,we employ former two step optimization procedure.3.3 Cross-lingual Text Categorization viaUnsupervised Mappingm-th document in the target domain (tm) is mapped to thesource domain as follows,
s(tm) = HT$:,mWS . (7)Here, HT :,m denotes the m-th column vector of HT , s(tm)is I dimentional vector.When each document in the source domain has a classlabel yn, we can train a classifier on the training data set{sn, yn}Nn=1. Therefore, the class label of the mapped docu-ment in the target domain s(tm) is assigned by the classifier.In the later experiments, we employ k(= 10)-NN as a classi-fier.WT HTMI NJ KKKTST
Figure 1: Topic alignments.
vector denotes an embedding of a document in the
K-dimensional latent space. Similarly, WT is an
I ?K matrix that represents a set of topics in the
target domain, and HT is a K ? M matrix that
denotes a set of latent semantic representations of
target documents. K is less than I and J .
By factorizing the original matrices, we can in-
dependently map the documents in the source and
target domains to the latent topic spaces whose di-
mensionality is K.
3.2 Finding Optimal Topic Alignments by
Unsupervised Object Matching
To connect the different latent spaces, topics ex-
tracted from the source language must be aligned
to one from the target language. This is reasonable
because we can assume that both languages share
the same latent concept.
However, we cannot quantify the similarity be-
tween the topics because we do not have any ex-
ternal language resources such as a dictionary.
Therefore, we utilize unsupervised object match-
ing method to find one-to-one correspondences
between topics. In this paper, we employ kernel-
ized sorting (KS) (Novi et al, 2010). KS finds the
best one-to-one matching as follows:
pi? = argmax
pi??K
tr(GSpi?GTpi),
s.t. pi1K=1K and pi?1K=1K . (3)
Here, pi is a K?K matrix that represents the one-
to-one correspondence between topics, i.e. piij=1
indicates that the ith topic in the source language
corresponds to the jth one of the target language.
Overall Average
KS 0.252 ? 0.112
CKS 0.249 ? 0.033
LSOM 0.278 ? 0.086
LSM(300) 0.298 ? 0.077
LSM(600) 0.359 ? 0.062
Table 1: Average accuracy over all language pairs
?K indicates the set of all possible matrices stor-
ing one-to-one correspondences. G denotes the
K ? K kernel matrix obtained from topic pro-
portion, Gij=K(W?i,: ,W:,j), and G is the centered
matrix of G. K(, ) is a kernel function. 1K is a
K-dimensional column vector of all ones. pi? is
obtained by iterative procedure.
According to pi?, we obtain permuted matrices,
WT=WTpi? and HT=pi??HT , and the product
of permuted matrices is the same with that of un-
permuted matrices as follows:
T ?WTHT=WTHT . (4)
Fig. 1 shows the topic alignment procedure.
Since documents from both domains are repre-
sented in a shared latent space, we can directly cal-
culate the similarity between the nth document in
the source domain and the mth document in the
target domain based on HT :,m (mth column vec-
tor of HT ) and HS:,n (nth column vector of HS).
4 Cross-language Text Categorization
via Latent Semantic Matching
Cross-language text categorization is the task of
exploiting labeled documents in the source lan-
guage (e.g. English) to classify documents in
the target language (e.g. French). Suppose we
have training data set {sn, yn}Nn=1 in the source
language domain. yn ? Y is the class label
for the nth document. We can train a classifier
in the K-dimensional latent space with data set
{H?S:,n, yn}Nn=1. H?S:,n is the projected vector of
sn. Also, the mth document in the target language
domain tm is projected into the latent space as
H?T :,m. Here, the documents in both domains are
projected into the same size latent space and the
basis vectors of the spaces are aligned. Therefore,
we can classify a document in the target domain
tm by a classifier trained with {H?S:,n, yn}Nn=1.
214
Books
English Hack, Parent, tale, subversion, Interesting, centre, Paper, T., prejudice, Murphy
German Lydia, Sebastian, Seelenbrecher, Patient, Fitzek, Patrick, Fiktion, Patientenakte, Realitt, Klinik
Electronics
English SD800, Angle, Digital, Optical, Silver, understnad, camra, 7.1MP, P3N, 10MP
German *****, 550D, 600D, Objektiv, Canon, ablichten, Body, Werkzeug, Kamera, einliet
Kitchen
English Briel, Electra-Craft, Chamonix, machine, Due, crema, supervisor, technician, espresso, tamp
German ESGE, Prierkopf, Zauberstab, Gummikupplung, Suppe/Sauce, Braun , Bolognese, prieren, Testsieger, Topf
Music
English Amy, Poison, Doherty, Schottin, Mid, Prince, Song, ausdrucksstark , Tempo, knocking
German Norah, mini, ?Little, ?Rome, ?Come, Gardot, Lana, listenings , dreamlike, digipak
Watch
English watch, indicate, timex, HRM, month, icon, Timex, datum, troubleshooting, reasonable
German Orient, Diver, Lnette, Leuchtpunkt, Zahlenringes, Handgelenksdurchmesser, Stoppsekunde, Uhrforum,
Konsumbereiche, Schwingungen/Std
Table 2: Examples of aligned latent topics
5 Experimental Evaluation
5.1 Experimental Settings
We compared our method, latent semantic match-
ing (LSM), with three unsupervised object match-
ing methods: Kernelized Sorting (KS), Convex
Kernelized Sorting (CKS), Least-Squares Object
Matching (LSOM). We set the number of the la-
tent topics K to 100 and employed the k-nearest
neighbor method (k=10) as the classifier.
For, KS, CKS and LSOM, we find the one-
to-one correspondence between documents in the
source language and documents in the target lan-
guage. Then, we assign class labels of the target
documents according to the correspondence.
In order to build a corpus with various lan-
guage pairs for evaluation, we crawled product
reviews from Amazon U.S., German, France and
Japan with five categories: ?Books?, ?Electronics?,
?Music?, ?Kitchen?, ?Watch?. The corpus is nei-
ther sentence level parallel nor comparable. For
each category, we randomly select 60 documents
as the test data (M=300) for all methods and 60
documents as the training data (N=300) for KS,
CKS, LSOM and LSM(300). We also compared
latent semantic matching with 120 training docu-
ments for each category (N=600), and called this
method LSM(600). Note that since KS, CKS and
LSOM require that the data sizes are the same for
source and target domains, they cannot use train-
ing data more than test data. To avoid local opti-
mum solutions of NMF, we executed our methods
with 100 different initialization values and chose
the solution that achieved the best objective func-
tion of KS.
5.2 Results and Discussion
Table 1 shows average accuracies with standard
division over all language pairs. From the table,
classification accuracy of all methods significantly
outperformed random classifier (accuracy=0.2).
The results showed the effectiveness of both un-
supervised object matching and latent semantic
matching. When comparing LSM(300) with KS,
CKS and LSOM, LSM(300) obtained better re-
sults than these unsupervised object matching
methods. The result supports the effectiveness of
the latent topic matching. Moreover, LSM(600)
achieved the highest accuracy. There are large dif-
ferences between LSM(600) and the others. This
result implies not only the effectiveness of the la-
tent topic matching but also increasing the number
of source side documents (labeled training data)
contributes to improving classification accuracy.
This is natural in terms of supervised learning but
only our method can deal with source side docu-
ments that are larger in number.
Table 2 shows examples of latent topics in
English and German extracted and aligned by
LSM(600). We can see that some author names,
words related to camera, and cooking equipment
appear in ?Books?, ?Electronics? and ?Kitchen?
topics, respectively. Similarity, there are some
artists? names in ?Music? and watch brands in
?Watch?.
215
6 Conclusion
As an extension of unsupervised object matching,
this paper proposed latent semantic matching that
considers the shared latent space between two lan-
guage domains. To generate such a space, top-
ics of the target space are permuted by exploit-
ing unsupervised object matching. We can mea-
sure distances between objects by standard met-
rics, which enable us retrieving k-nearest objects
in the source domain for a query object in the tar-
get domain. This is a significant advantage over
conventional unsupervised object matching meth-
ods. We used Amazon review corpus to demon-
strate the effectiveness of our method on cross-
language text categorization. The results showed
that our method outperformed conventional object
matching methods with the same number of train-
ing samples. Moreover, our method achieved even
higher performance by utilizing more documents
in the source domain.
Acknowledgements
The authors would like to thank Nemanja Djuric
for providing code for Convex Kernelized Sorting
and the three anonymous reviewers for thoughtful
suggestions.
References
Francis Bach and Michael Jordan. 2005. A probabilis-
tic interpretation of canonical correlation analysis.
Technical report, Department of Statistics, Univer-
sity of California, Berkeley.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. JMLR, 3(Jan.):993?
1022.
Jordan Boyd-Graber and David Blei. 2009. Multilin-
gual topic model for unaligned text. In Proc. of the
25th UAI, pages 75?82.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Nemanja Djuric, Mihajlo Grbovic, and Slobodan
Vucetic. 2012. Convex kernelized sorting. In Proc.
of the 26th AAAI, pages 893?899.
Susan Dumais, Lanauer Thomas, and Michael Littman.
1996. Automatic cross-linguistic information re-
trieval using latent semantic indexing. In Proc.
of the Workshop on Cross-Linguistic Information
Retieval in SIGIR, pages 16?23.
Alfio Gliozzo and Carlo Strapparava. 2005. Cross lan-
guage text categorization by acquiring multilingual
domain models from comparable corpora. In Proc.
of the ACL Workshop on Building and Using Paral-
lel Texts, pages 9?16.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. of ACL-08:
HLT, pages 771?779.
Jagarlamudi Jagadeesh and Hal Daume III. 2010. Ex-
tracting multilingual topics from unaligned corpora.
In Proc of the 32nd ECIR, pages 444?456.
Daniel Lee and Sebastian Seung. 2000. Algorithm
for non-negative matrix factorization. In Advances
in Neural Information Processing Systems 13, pages
556?562.
Quadrianto Novi, Smola Alexander, Song Le, and
Tuytelaars Tinne. 2010. Kernelized sorting. IEEE
Trans. on Pattern Analysis and Machine Intelli-
gence, 32(10):1809?1821.
Jhon Platt, Kristina Toutanova, andWen-tau Yih. 2010.
Translingual document representation from discrim-
inative projections. In Proc. of the 2010 Conference
on EMNLP, pages 251?261.
Abhishek Tripathi, Arto Klami, and Sami Virpioja.
2010. Bilingual sentence matching using kernel
CCA. In Proc. of the 2010 IEEE International
Workshop on MLSP, pages 130?135.
Ni Xiaochuan, Sun Lian-Tao, Hu Jian, and Chen
Zheng. 2011. Cross lingual text classification by
mining multilingual topics from wikipedia. In Proc.
of the 4th WSDM, pages 375?384.
Makoto Yamada and Masashi Sugiyama. 2011. Cross-
domain object matching with model selection. In
Proc. of the 14th AISTATS, pages 807?815.
216
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 315?320,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Single Document Summarization based on Nested Tree Structure
Yuta Kikuchi
?
Tsutomu Hirao
?
Hiroya Takamura
?
?
Tokyo Institute of technology
4295, Nagatsuta, Midori-ku, Yokohama, 226-8503, Japan
{kikuchi,takamura,oku}@lr.pi.titech.ac.jp
?
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{hirao.tsutomu,nagata.masaaki}@lab.ntt.co.jp
Manabu Okumura
?
Masaaki Nagata
?
Abstract
Many methods of text summarization
combining sentence selection and sen-
tence compression have recently been pro-
posed. Although the dependency between
words has been used in most of these
methods, the dependency between sen-
tences, i.e., rhetorical structures, has not
been exploited in such joint methods. We
used both dependency between words and
dependency between sentences by con-
structing a nested tree, in which nodes
in the document tree representing depen-
dency between sentences were replaced by
a sentence tree representing dependency
between words. We formulated a sum-
marization task as a combinatorial opti-
mization problem, in which the nested
tree was trimmed without losing impor-
tant content in the source document. The
results from an empirical evaluation re-
vealed that our method based on the trim-
ming of the nested tree significantly im-
proved the summarization of texts.
1 Introduction
Extractive summarization is one well-known ap-
proach to text summarization and extractive meth-
ods represent a document (or a set of documents)
as a set of some textual units (e.g., sentences,
clauses, and words) and select their subset as a
summary. Formulating extractive summarization
as a combinational optimization problem greatly
improves the quality of summarization (McDon-
ald, 2007; Filatova and Hatzivassiloglou, 2004;
Takamura and Okumura, 2009). There has re-
cently been increasing attention focused on ap-
proaches that jointly optimize sentence extraction
and sentence compression (Tomita et al, 2009;
Qian and Liu, 2013; Morita et al, 2013; Gillick
and Favre, 2009; Almeida and Martins, 2013;
Berg-Kirkpatrick et al, 2011). We can only ex-
tract important content by trimming redundant
parts from sentences.
However, as these methods did not include the
discourse structures of documents, the generated
summaries lacked coherence. It is important for
generated summaries to have a discourse struc-
ture that is similar to that of the source docu-
ment. Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988) is one way of introduc-
ing the discourse structure of a document to a
summarization task (Marcu, 1998; Daum?e III and
Marcu, 2002; Hirao et al, 2013). Hirao et al
recently transformed RST trees into dependency
trees and used them for single document summa-
rization (Hirao et al, 2013). They formulated the
summarization problem as a tree knapsack prob-
lem with constraints represented by the depen-
dency trees.
We propose a method of summarizing a single
document that utilizes dependency between sen-
tences obtained from rhetorical structures and de-
pendency between words obtained from a depen-
dency parser. We have explained our method with
an example in Figure 1. First, we represent a doc-
ument as a nested tree, which is composed of two
types of tree structures: a document tree and a
sentence tree. The document tree is a tree that has
sentences as nodes and head modifier relationships
between sentences obtained by RST as edges. The
sentence tree is a tree that has words as nodes
and head modifier relationships between words
obtained by the dependency parser as edges. We
can build the nested tree by regarding each node of
the document tree as a sentence tree. Finally, we
formulate the problem of single document sum-
marization as that of combinatorial optimization,
which is based on the trimming of the nested tree.
315
John  was  running  on  a  track  in  the  park.
He  looks very tired. Mike  said  he  is  trainning  for  a  race.
The  race  is  held  on  next  month.
?
  Source document                                   
John was running on a track in the park.
He looks very tired.
Mike said he is training for a race.
The race is held on next month.
  Summary                                              
John was running on a track.
he is training for a race. *
The race is held on next month.
EDU?? ???? ??? ????
0
2
4
6
8
10
12
14
16
EDU
selsection
sentence subtree
selection
sentence
selection
reference 
summary
Nu
m
be
r o
f  
se
lec
ted
 se
nt
en
ce
s 
fro
m
 so
ur
ce
 do
cu
m
en
t
John  was  running  on  a  track  in  the  park.
He  looks very tired. Mike  said  he  is  training  for  a  race.
The  race  is  held  next  month.
?
  Source document                                   
John was running on a track in the park.
He looks very tired.
Mike said he is training for a race.
The race is held next month.
  Summary                                              
John was running on a track.
he is training for a race. *
The race is held next month.
Figure 1: Overview of our method. The source document is represented as a nested tree. Our method
simultaneously selects a rooted document subtree and sentence subtree from each node.
Our method jointly utilizes relations between sen-
tences and relations between words, and extracts
a rooted document subtree from a document tree
whose nodes are arbitrary subtrees of the sentence
tree.
Elementary Discourse Units (EDUs) in RST are
defined as the minimal building blocks of dis-
course. EDUs roughly correspond to clauses.
Most methods of summarization based on RST use
EDUs as extraction textual units. We converted
the rhetorical relations between EDUs to the re-
lations between sentences to build the nested tree
structure. We could thus take into account both
relations between sentences and relations between
words.
2 Related work
Extracting a subtree from the dependency tree of
words is one approach to sentence compression
(Tomita et al, 2009; Qian and Liu, 2013; Morita
et al, 2013; Gillick and Favre, 2009). However,
these studies have only extracted rooted subtrees
from sentences. We allowed our model to extract
a subtree that did not include the root word (See
the sentence with an asterisk ? in Figure 1). The
method of Filippova and Strube (2008) allows the
model to extract non-rooted subtrees in sentence
compression tasks that compress a single sentence
with a given compression ratio. However, it is not
trivial to apply their method to text summariza-
tion because no compression ratio is given to sen-
tences. None of these methods use the discourse
structures of documents.
Daum?e III and Marcu (2002) proposed a noisy-
channel model that used RST. Although their
method generated a well-organized summary, no
optimality of information coverage was guaran-
teed and their method could not accept large texts
because of the high computational cost. In addi-
- The scare over Alar, a growth regulator
- that makes apples redder and crunchier
- but may be carcinogenic,
- made consumers shy away from the Delicious,
- though they were less affected than the McIntosh.
Figure 2: Example of one sentence. Each line cor-
responds to one EDU.
tion, their method required large sets of data to cal-
culate the accurate probability. There have been
some studies that have used discourse structures
locally to optimize the order of selected sentences
(Nishikawa et al, 2010; Christensen et al, 2013).
3 Generating summary from nested tree
3.1 Building Nested Tree with RST
A document in RST is segmented into EDUs and
adjacent EDUs are linked with rhetorical relations
to build an RST-Discourse Tree (RST-DT) that has
a hierarchical structure of the relations. There are
78 types of rhetorical relations between two spans,
and each span has one of two aspects of a nu-
cleus and a satellite. The nucleus is more salient
to the discourse structure, while the other span, the
satellite, represents supporting information. RST-
DT is a tree whose terminal nodes correspond
to EDUs and whose nonterminal nodes indicate
the relations. Hirao et al converted RST-DTs
into dependency-based discourse trees (DEP-DTs)
whose nodes corresponded to EDUs and whose
edges corresponded to the head modifier relation-
ships of EDUs. See Hirao et al for details (Hirao
et al, 2013).
Our model requires sentence-level dependency.
Fortunately we can simply convert DEP-DTs to
obtain dependency trees between sentences. We
specifically merge EDUs that belong to the same
sentence. Each sentence has only one root EDU
that is the parent of all the other EDUs in the sen-
tence. Each root EDU in a sentence has the parent
316
max.
n
?
i
m
i
?
j
w
ij
z
ij
s.t.
?
n
i
?
m
i
j
z
ij
? L; (1)
x
parent(i)
? x
i
; ?i (2)
z
parent(i,j)
? z
ij
+ r
ij
? 0; ?i, j (3)
x
i
? z
ij
; ?i, j (4)
?
m
i
j
z
ij
? min(?, len(i))x
i
; ?i (5)
?
m
i
j
r
ij
= x
i
; ?i (6)
?
j /?R
c
(i)
r
ij
= 0; ?i (7)
r
ij
? z
ij
; ?i, j (8)
r
ij
+ z
parent(i,j)
? 1; ?i, j (9)
r
iroot(i)
= z
iroot(i)
; ?i (10)
?
j?sub(i)
z
ij
? x
i
; ?i (11)
?
j?obj(i)
z
ij
? x
i
; ?i (12)
Figure 3: ILP formulation (x
i
, z
ij
, r
ij
? {0, 1})
EDU in another sentence. Hence, we can deter-
mine the parent-child relations between sentences.
As a result, we obtain a tree that represents the
parent-child relations of sentences, and we can use
it as a document tree. After the document tree is
obtained, we use a dependency parser to obtain the
syntactic dependency trees of sentences. Finally,
we obtain a nested tree.
3.2 ILP formulation
Our method generates a summary by trimming a
nested tree. In particular, we extract a rooted docu-
ment subtree from the document tree, and sentence
subtrees from sentence trees in the document tree.
We formulate our problem of optimization in this
section as that of integer linear programming. Our
model is shown in Figure 3.
Let us denote by w
ij
the term weight of word
ij (word j in sentence i). x
i
is a variable that
is one if sentence i is selected as part of a sum-
mary, and z
ij
is a variable that is one if word ij
is selected as part of a summary. According to the
objective function, the score for the resulting sum-
mary is the sum of the term weights w
ij
that are
included in the summary. We denote by r
ij
the
variable that is one if word ij is selected as a root
of an extracting sentence subtree. Constraint (1)
guarantees that the summary length will be less
than or equal to limit L. Constraints (2) and (3)
are tree constraints for a document tree and sen-
tence trees. r
ij
in Constraint (3) allows the system
to extract non-rooted sentence subtrees, as we pre-
viously mentioned. Function parent(i) returns the
parent of sentence i and function parent(i, j) re-
turns the parent of word ij. Constraint (4) guaran-
tees that words are only selected from a selected
sentence. Constraint (5) guarantees that each se-
lected sentence subtree has at least ? words. Func-
tion len(i) returns the number of words in sentence
i. Constraints (6)-(10) allow the model to extract
subtrees that have an arbitrary root node. Con-
straint (6) guarantees that there is only one root
per selected sentence. We can set the candidate
for the root node of the subtree by using constraint
(7). The R
c
(i) returns a set of the nodes that are
the candidates of the root nodes in sentence i. It
returned the parser?s root node and the verb nodes
in this study. Constraint (8) maintains consistency
between z
ij
and r
ij
. Constraint (9) prevents the
system from selecting the parent node of the root
node. Constraint (10) guarantees that the parser?s
root node will only be selected when the system
extracts a rooted sentence subtree. The root(i) re-
turns the word index of the parser?s root. Con-
straints (11) and (12) guarantee that the selected
sentence subtree has at least one subject and one
object if it has any. The sub(i) and obj(i) return
the word indices whose dependency tag is ?SUB?
and ?OBJ?.
3.3 Additional constraint for grammaticality
We added two types of constraints to our model
to extract a grammatical sentence subtree from a
dependency tree:
z
ik
= z
il
, (13)
?
k?s(i,j)
z
ik
= |s(i, j)|x
i
. (14)
Equation (13) means that words z
ik
and z
il
have
to be selected together, i.e., a word whose depen-
dency tag is PMOD or VC and its parent word, a
negation and its parent word, a word whose de-
pendency tag is SUB or OBJ and its parent verb,
a comparative (JJR) or superlative (JJS) adjective
and its parent word, an article (a/the) and its par-
ent word, and the word ?to? and its parent word.
Equation (14) means that the sequence of words
has to be selected together, i.e., a proper noun se-
quence whose POS tag is PRP$, WP%, or POS
and a possessive word and its parent word and the
words between them. The s(i, j) returns the set of
word indices that are selected together with word
ij.
317
Table 1: ROUGE score of each model. Note that
the top two rows are both our proposals.
ROUGE-1
Sentence subtree 0.354
Rooted sentence subtree 0.352
Sentence selection 0.254
EDU selection (Hirao et al, 2013) 0.321
LEAD
EDU
0.240
LEAD
snt
0.157
4 Experiment
4.1 Experimental Settings
We experimentally evaluated the test collection for
single document summarization contained in the
RST Discourse Treebank (RST-DTB) (Carlson et
al., 2001) distributed by the Linguistic Data Con-
sortium (LDC)
1
. The RST-DTB Corpus includes
385 Wall Street Journal articles with RST anno-
tations, and 30 of these documents also have one
manually prepared reference summary. We set the
length constraint, L, as the number of words in
each reference summary. The average length of
the reference summaries corresponded to approxi-
mately 10% of the length of the source document.
This dataset was first used by Marcu et al for
evaluating a text summarization system (Marcu,
1998). We used ROUGE (Lin, 2004) as an eval-
uation criterion.
We compared our method (sentence subtree)
with that of EDU selection (Hirao et al, 2013).
We examined two other methods, i.e., rooted sen-
tence subtree and sentence selection. These two
are different from our method in the way that they
select a sentence subtree. Rooted sentence subtree
only selects rooted sentence subtrees
2
. Sentence
selection does not trim sentence trees. It simply
selects full sentences from a document tree
3
. We
built all document trees from the RST-DTs that
were annotated in the corpus.
We set the term weight, w
ij
, for our model as:
w
ij
=
log(1 + tf
ij
)
depth(i)
2
, (15)
where tf
ij
is the term frequency of word ij in a
document and depth(i) is the depth of sentence
1
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2002T07
2
We achieved this by making R
c
(i) only return the
parser?s root node in Figure 7.
3
We achieved this by setting ? to a very large number.
i within the sentence-level DEP-DT that we de-
scribed in Section 3.1. For Constraint (5), we set
? to eight.
4.2 Results and Discussion
4.2.1 Comparing ROUGE scores
We have summarized the Recall-Oriented Under-
study for Gisting Evaluation (ROUGE) scores for
each method in Table 1. The score for sentence
selection is low (0.254). However, introducing
sentence compression to the system greatly im-
proved the ROUGE score (0.354). The score is
also higher than that with EDU selection, which
is a state-of-the-art method. We applied a multi-
ple test by using Holm?s method and found that
our method significantly outperformed EDU se-
lection and sentence selection. The difference be-
tween the sentence subtree and the rooted sentence
subtree methods was fairly small. We therefore
qualitatively analyzed some actual examples that
will be discussed in Section 4.2.2. We also exam-
ined the ROUGE scores of two LEAD
4
methods
with different textual units: EDUs (LEAD
EDU
)
and sentences (LEAD
SNT
). Although LEAD
works well and often obtains high ROUGE scores
for news articles, the scores for LEAD
EDU
and
LEAD
SNT
were very low.
4.2.2 Qualitative Evaluation of Sentence
Subtree Selection
This subsection compares the methods of subtree
selection and rooted subtree selection. Figure 4
has two example sentences for which both meth-
ods selected a subtree as part of a summary. The
{?} indicates the parser?s root word. The [?] indi-
cates the word that the system selected as the root
of the subtree. Subtree selection selected a root in
both examples that differed from the parser?s root.
As we can see, subtree selection only selected im-
portant subtrees that did not include the parser?s
root, e.g., purpose-clauses and that-clauses. This
capability is very effective because we have to
contain important content in summaries within
given length limits, especially when the compres-
sion ratio is high (i.e., the method has to gener-
ate much shorter summaries than the source docu-
ments).
4
LEADmethods simply take the firstK textual units from
a source document until the summary length reaches L.
318
Original sentence : John Kriz, a Moody?s vice president, {said} Boston Safe Deposit?s performance has been
hurt this year by a mismatch in the maturities of its assets and liabilities.
Rooted subtree selection : John Kriz a Moody?s vice president [{said}] Boston Safe Deposit?s performance has been
hurt this year
Subtree selection : Boston Safe Deposit?s performance has [been] hurt this year
Original sentence : Recent surveys by Leo J. Shapiro & Associates, a market research firm in Chicago,
{suggest} that Sears is having a tough time attracting shoppers because it hasn?t yet done
enough to improve service or its selection of merchandise.
Rooted subtree selection : surveys [{suggest}] that Sears is having a time
Subtree selection : Sears [is] having a tough time attracting shoppers
Figure 4: Example sentences and subtrees selected by each method.
Table 2: Average number of words that individual
extracted textual units contained.
Subtree Sentence EDU
15.29 18.96 9.98
4.2.3 Fragmentation of Information
Many studies that have utilized RST have simply
adopted EDUs as textual units (Mann and Thomp-
son, 1988; Daum?e III and Marcu, 2002; Hirao et
al., 2013; Knight and Marcu, 2000). While EDUs
are textual units for RST, they are too fine grained
as textual units for methods of extractive summa-
rization. Therefore, the models have tended to se-
lect small fragments from many sentences to max-
imize objective functions and have led to frag-
mented summaries being generated. Figure 2 has
an example of EDUs. A fragmented summary
is generated when small fragments are selected
from many sentences. Hence, the number of sen-
tences in the source document included in the re-
sulting summary can be an indicator to measure
the fragmentation of information. We counted
the number of sentences in the source document
that each method used to generate a summary
5
.
The average for our method was 4.73 and its me-
dian was four sentences. In contrast, methods
of EDU selection had an average of 5.77 and a
median of five sentences. This meant that our
method generated a summary with a significantly
smaller number of sentences
6
. In other words, our
method relaxed fragmentation without decreasing
the ROUGE score. There are boxplots of the num-
bers of selected sentences in Figure 5. Table 2 lists
the number of words in each textual unit extracted
by each method. It indicates that EDUs are shorter
than the other textual units. Hence, the number of
sentences tends to be large.
5
Note that the number for the EDU method is not equal to
selected textual units because a sentence in the source docu-
ment may contain multiple EDUs.
6
We used the Wilcoxon signed-rank test (p < 0.05).
John  was  running  on  a  track  in  the  park.
He  looks very tired. Mike  said  he  is  trainning  for  a  race.
The  race  is  held  on  next  month.
  Source document                                   John was running on a track in the park.He looks very tired.Mike said he is training for a race.The race is held on next month.
  Summary                                              John was running on a track.he is training for a race.The race is held on next month.
EDU?? ???? ??? ????0
2
4
6
8
10
12
14
16
EDUselsection sentence subtreeselection sentenceselection reference summary
Num
ber o
f  sel
ected
 sent
ence
s 
from
 sour
ce do
cume
nt
Figure 5: Number of sentences that each method
selected.
5 Conclusion
We proposed a method of summarizing a sin-
gle document that included relations between sen-
tences and relations between words. We built a
nested tree and formulated the problem of summa-
rization as that of integer linear programming. Our
method significantly improved the ROUGE score
with significantly fewer sentences than the method
of EDU selection. The results suggest that our
method relaxed the fragmentation of information.
We also discussed the effectiveness of sentence
subtree selection that did not restrict rooted sub-
trees. Although ROUGE scores are widely used
as evaluation metrics for text summarization sys-
tems, they cannot take into consideration linguis-
tic qualities such as human readability. Hence, we
plan to conduct evaluations with people
7
.
We only used the rhetorical structures between
sentences in this study. However, there were also
rhetorical structures between EDUs inside individ-
ual sentences. Hence, utilizing these for sentence
compression has been left for future work. In addi-
tion, we used rhetorical structures that were man-
ually annotated. There have been related studies
on building RST parsers (duVerle and Prendinger,
2009; Hernault et al, 2010) and by using such
parsers, we should be able to apply our model to
other corpora or to multi-document settings.
7
For example, the quality question metric from the Docu-
ment Understanding Conference (DUC).
319
References
Miguel Almeida and Andre Martins. 2013. Fast
and robust compressive summarization with dual de-
composition and multi-task learning. In ACL, pages
196?206, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
ACL, pages 481?490, Portland, Oregon, USA, June.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In SIGDIAL, pages 1?10.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In NAACL:HLT, pages
1163?1173.
Hal Daum?e III and Daniel Marcu. 2002. A noisy-
channel model for document compression. ACL,
pages 449?456.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine
classification. In IJCNLP, pages 665?673.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In COLING.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In INLG,
pages 25?32.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In ILP, pages 10?18.
Hugo Hernault, Helmut Prendinger, David duVerle,
and Mitsuru Ishizuka. 2010. Hilda: A discourse
parser using support vector machine classification.
Dialogue & Discourse, 1(3):1?30.
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,
Norihito Yasuda, and Masaaki Nagata. 2013.
Single-document summarization as a tree knapsack
problem. In EMNLP, pages 1515?1520.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In National Conference on Artificial Intelli-
gence (AAAI), pages 703?710.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out, pages 74?81.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, pages 243?281.
Daniel Marcu. 1998. Improving summarization
through rhetorical parsing tuning. In In Proc. of the
6th Workshop on Very Large Corpora, pages 206?
215.
Ryan T. McDonald. 2007. A study of global infer-
ence algorithms in multi-document summarization.
In ECIR, pages 557?564.
Hajime Morita, Ryohei Sasano, Hiroya Takamura, and
Manabu Okumura. 2013. Subtree extractive sum-
marization via submodular maximization. In ACL,
pages 1023?1032.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-
suo, and Genichiro Kikui. 2010. Opinion summa-
rization with integer linear programming formula-
tion for sentence extraction and ordering. In COL-
ING, pages 910?918.
Xian Qian and Yang Liu. 2013. Fast joint compres-
sion and summarization via graph cuts. In EMNLP,
pages 1492?1502.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on the budgeted median
problem. In CIKM, pages 1589?1592.
Kohei Tomita, Hiroya Takamura, and Manabu Oku-
mura. 2009. A new approach of extractive sum-
marization combining sentence selection and com-
pression. IPSJ SIG Notes, pages 13?20.
320
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 418?427,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Divide and Translate: Improving Long Distance Reordering in Statistical
Machine Translation
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Tsutomu Hirao, Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan
sudoh@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a novel method
for long distance, clause-level reordering
in statistical machine translation (SMT).
The proposed method separately translates
clauses in the source sentence and recon-
structs the target sentence using the clause
translations with non-terminals. The non-
terminals are placeholders of embedded
clauses, by which we reduce complicated
clause-level reordering into simple word-
level reordering. Its translation model
is trained using a bilingual corpus with
clause-level alignment, which can be au-
tomatically annotated by our alignment
algorithm with a syntactic parser in the
source language. We achieved signifi-
cant improvements of 1.4% in BLEU and
1.3% in TER by using Moses, and 2.2%
in BLEU and 3.5% in TER by using
our hierarchical phrase-based SMT, for
the English-to-Japanese translation of re-
search paper abstracts in the medical do-
main.
1 Introduction
One of the common problems of statistical ma-
chine translation (SMT) is to overcome the differ-
ences in word order between the source and target
languages. This reordering problem is especially
serious for language pairs with very different word
orders, such as English-Japanese. Many previous
studies on SMT have addressed the problem by
incorporating probabilistic models into SMT re-
ordering. This approach faces the very large com-
putational cost of searching over many possibili-
ties, especially for long sentences. In practice the
search can be made tractable by limiting its re-
ordering distance, but this also renders long dis-
tance movements impossible. Some recent stud-
ies avoid the problem by reordering source words
prior to decoding. This approach faces difficul-
ties when the input phrases are long and require
significant word reordering, mainly because their
reordering model is not very accurate.
In this paper, we propose a novel method for
translating long sentences that is different from
the above approaches. Problematic long sentences
often include embedded clauses1 such as rela-
tive clauses. Such an embedded (subordinate)
clause can usually be translated almost indepen-
dently of words outside the clause. From this
viewpoint, we propose a divide-and-conquer ap-
proach: we aim to translate the clauses sepa-
rately and reconstruct the target sentence using the
clause translations. We first segment a source sen-
tence into clauses using a syntactic parser. The
clauses can include non-terminals as placeholders
for nested clauses. Then we translate the clauses
with a standard SMT method, in which the non-
terminals are reordered as words. Finally we re-
construct the target sentence by replacing the non-
terminals with their corresponding clause transla-
tions. With this method, clause-level reordering is
reduced to word-level reordering and can be dealt
with efficiently. The models for clause translation
are trained using a bilingual corpus with clause-
level alignment. We also present an automatic
clause alignment algorithm that can be applied to
sentence-aligned bilingual corpora.
In our experiment on the English-to-Japanese
translation of multi-clause sentences, the proposed
method improved the translation performance by
1.4% in BLEU and 1.3% in TER by using Moses,
and by 2.2% in BLEU and 3.5% in TER by using
our hierarchical phrase-based SMT.
The main contribution of this paper is two-fold:
1Although various definitions of a clause can be
considered, this paper follows the definition of ?S?
(sentence) in Enju. It basically follows the Penn Tree-
bank II scheme but also includes SINV, SQ, SBAR. See
http://www-tsujii.is.s.u-tokyo.ac.jp/enju/enju-manual/enju-
output-spec.html#correspondence for details.
418
1. We introduce the idea of explicit separa-
tion of in-clause and outside-clause reorder-
ing and reduction of outside-clause reorder-
ing into common word-level reordering.
2. We propose an automatic clause alignment
algorithm, by which our approach can be
used without manual clause-level alignment.
This paper is organized as follows. The next
section reviews related studies on reordering. Sec-
tion 3 describes the proposed method in detail.
Section 4 presents and discusses our experimen-
tal results. Finally, we conclude this paper with
our thoughts on future studies.
2 Related Work
Reordering in SMT can be roughly classified into
two approaches, namely a search in SMT decod-
ing and preprocessing.
The former approach is a straightforward way
that models reordering in noisy channel transla-
tion, and has been studied from the early period
of SMT research. Distance-based reordering is a
typical approach used in many previous studies re-
lated to word-based SMT (Brown et al, 1993) and
phrase-based SMT (Koehn et al, 2003). Along
with the advances in phrase-based SMT, lexical-
ized reordering with a block orientation model was
proposed (Tillmann, 2004; Koehn et al, 2005).
This kind of reordering is suitable and commonly
used in phrase-based SMT. On the other hand,
a syntax-based SMT naturally includes reorder-
ing in its translation model. A lot of research
work undertaken in this decade has used syntac-
tic parsing for linguistically-motivated translation.
(Yamada and Knight, 2001; Graehl and Knight,
2004; Galley et al, 2004; Liu et al, 2006). Wu
(1997) and Chiang (2007) focus on formal struc-
tures that can be extracted from parallel corpora,
instead of a syntactic parser trained using tree-
banks. These syntactic approaches can theoret-
ically model reordering over an arbitrary length,
however, long distance reordering still faces the
difficulty of searching over an extremely large
search space.
The preprocessing approach employs deter-
ministic reordering so that the following trans-
lation process requires only short distance re-
ordering (or even a monotone). Several previ-
ous studies have proposed syntax-driven reorder-
ing based on source-side parse trees. Xia and
McCord (2004) extracted reordering rules auto-
matically from bilingual corpora for English-to-
French translation; Collins et al (2005) used
linguistically-motivated clause restructuring rules
for German-to-English translation; Li et al (2007)
modeled reordering on parse tree nodes by us-
ing a maximum entropy model with surface and
syntactic features for Chinese-to-English trans-
lation; Katz-Brown and Collins (2008) applied
a very simple reverse ordering to Japanese-to-
English translation, which reversed the word order
in Japanese segments separated by a few simple
cues; Xu et al (2009) utilized a dependency parser
with several hand-labeled precedence rules for re-
ordering English to subject-object-verb order like
Korean and Japanese. Tromble and Eisner (2009)
proposed another reordering approach based on a
linear ordering problem over source words with-
out a linguistically syntactic structure. These pre-
processing methods reorder source words close
to the target-side order by employing language-
dependent rules or statistical reordering models
based on automatic word alignment. Although
the use of language-dependent rules is a natural
and promising way of bridging gaps between lan-
guages with large syntactic differences, the rules
are usually unsuitable for other language groups.
On the other hand, statistical methods can be ap-
plied to any language pairs. However, it is very
difficult to reorder all source words so that they are
monotonic with the target words. This is because
automatic word alignment is not usually reliable
owing to data sparseness and the weak modeling
of many-to-many word alignments. Since such
a reordering is not complete or may even harm
word ordering consistency in the source language,
these previous methods further applied reordering
in their decoding. Li et al (2007) used N-best
reordering hypotheses to overcome the reordering
ambiguity.
Our approach is different from those of previous
studies that aim to perform both short and long dis-
tance reordering at the same time. The proposed
method distinguishes the reordering of embedded
clauses from others and efficiently accomplishes it
by using a divide-and-conquer framework. The re-
maining (relatively short distance) reordering can
be realized in decoding and preprocessing by the
methods described above. The proposed frame-
work itself does not depend on a certain language
pair. It is based on the assumption that a source
419
language clause is translated to the corresponding
target language clause as a continuous segment.
The only language-dependent resource we need is
a syntactic parser of the source language. Note
that clause translation in the proposed method is a
standardMT problem and therefore any reordering
method can be employed for further improvement.
This work is inspired by syntax-based meth-
ods with respect to the use of non-terminals. Our
method can be seen as a variant of tree-to-string
translation that focuses only on the clause struc-
ture in parse trees and independently translates the
clauses. Although previous syntax-based methods
can theoretically model this kind of derivation, it
is practically difficult to decode long multi-clause
sentences as described above.
Our approach is also related to sentence sim-
plification and is intended to obtain simple and
short source sentences for better translation. Kim
and Ehara (1994) proposed a rule-based method
for splitting long Japanese sentences for Japanese-
to-English translation; Furuse et al (1998) used
a syntactic structure to split ill-formed inputs in
speech translation. Their splitting approach splits
a sentence sequentially to obtain short segments,
and does not undertake their reordering.
Another related field is clause identification
(Tjong et al, 2001). The proposed method is not
limited to a specific clause identification method
and any method can be employed, if their clause
definition matches the proposed method where
clauses are independently translated.
3 Proposed Method
The proposed method consists of the following
steps illustrated in Figure 1.
During training:
1) clause segmentation of source sentences with
a syntactic parser (section 3.1)
2) alignment of target words with source clauses
to develop a clause-level aligned corpus (section
3.2)
3) training the clause translation models using
the corpus (section 3.3)
During testing:
1) clause translation with the clause translation
models (section 3.4)
2) sentence reconstruction based on non-
terminals (section 3.5)
Bilingual
Corpus
(Training)
source
target
parse & clause
segmentation
parse &
clause
segmen-
tation
Source Sentences
(clause-segmented)
Word Alignment
Model
Target Word Bigram
Language Model
LM training
word
alignment
Bilingual Corpus
(clause-aligned)
automatic clause alignment
Clause
Translation Models
(Phrase Table, N-gram LMs, ...)
training from scratch
Bilingual
Corpus
(Development)
(clause-segmented)
MERT
Test Sentence
Sentence
Translation
clause
clause
clause
clause
translation
clause
translation
clause
translation
sentence reconstruction
based on non-terminals
translation
Original (sentence-aligned)
corpus can also be used
Figure 1: Overview of proposed method.
3.1 Clause Segmentation of Source Sentences
Clauses in source sentences are identified by a
syntactic parser. Figure 2 shows a parse tree for
the example sentence below. The example sen-
tence has a relative clause modifying the noun
book. Figure 3 shows the word alignment of this
example.
English: John lost the book that was borrowed
last week from Mary.
Japanese: john wa (topic marker) senshu (last
week) mary kara (from) kari (borrow) ta
(past tense marker) hon (book) o (direct ob-
ject marker) nakushi (lose) ta (past tense
marker) .
We segment the source sentence at the clause level
and the example is rewritten with two clauses as
follows.
? John lost the book s0 .
? that was borrowed last week from Mary
s0 is a non-terminal symbol the serves as a place-
holder of the relative clause. We allow an arbitrary
420
SS
John
lost
the
book
that
was
borrowed
from Mary
last week
Figure 2: Parse tree for example English sentence.
Node labels are omitted except S.
John
lo
st
the
book
that
w
as
borrow
ed
from
M
ary
last
w
eek
john
wa
ta
nakushi
o
hon
ta
kari
kara
mary
senshu
Figure 3: Word alignment for example bilingual
sentence.
number of non-terminals in each clause2. A nested
clause structure can be represented in the same
manner using such non-terminals recursively.
3.2 Alignment of Target Words with Source
Clauses
To translate source clauses with non-terminal sym-
bols, we need models trained using a clause-level
aligned bilingual corpus. A clause-level aligned
corpus is defined as a set of parallel, bilingual
clause pairs including non-terminals that represent
embedded clauses.
We assume that a sentence-aligned bilingual
corpus is available and consider the alignment of
target words with source clauses. We can manu-
ally align these Japanese words with the English
clauses as follows.
? john wa s0 hon o nakushi ta .
2In practice not so many clauses are embedded in a single
sentence but we found some examples with nine embedded
clauses for coordination in our corpora.
John lost the book s0 .
? senshu mary kara kari ta
that was borrowed last week from Mary
Since the cost of manual clause alignment is
high especially for a large-scale corpus, a natu-
ral question to ask is whether this resource can be
obtained from a sentence-aligned bilingual corpus
automatically with no human input. To answer
this, we now describe a simple method for deal-
ing with clause alignment data from scratch, us-
ing only the word alignment and language model
probabilities inferred from bilingual and monolin-
gual corpora.
Our method is based on the idea that automatic
clause alignment can be viewed as a classification
problem: for an English sentence with N words (e
= (e1, e2, . . . , eN )) andK clauses (e?1,e?2,. . . ,e?K),
and its Japanese translation with M words (f
= (f1, f2, . . . , fM )), the goal is to classify each
Japanese word into one of {1, . . . ,K} classes. In-
tuitively, the probability that a Japanese word fm
is assigned to class k ? {1, . . . ,K} depends on
two factors:
1. The probability of translating fm into the En-
glish words of clause k (i.e.
?
e?e?k p(e|fm)).
We expect fm to be assigned to a clause
where this value is high.
2. The language model probability
(i.e. p(fm|fm?1)). If this value is high,
we expect fm and fm?1 to be assigned to the
same clause.
We implement this intuition using a graph-
based method. For each English-Japanese sen-
tence pair, we construct a graph with K clause
nodes (representing English clauses) and M word
nodes (representing Japanese words). The edge
weights between word and clause nodes are de-
fined as the sum of lexical translation probabilities
?
e?e?k p(e|fm). The edge weights between words
are defined as the bigram probability p(fm|fm?1).
Each clause node is labeled with a class ID k ?
{1, . . . ,K}. We then propagate these K labels
along the graph to label the M word nodes. Fig-
ure 4 shows the graph for the example sentence.
Many label propagation algorithms are avail-
able. The important thing is to use an algo-
rithm that encourages node pairs with strong edge
weights to receive the same label. We use the label
propagation algorithm of (Zhu et al, 2003). If we
421
John  lost  the  book  that  was  borrowed ...
clause(1) clause(2)
John Mary fromlast weektopicmarker
p(John |           )
+ p(lost |           )
+ ...
p(that |        )
+ p(was |        )
+ ...
p(     |         ) p(         |            ) p(        |         )p(            |     )
john kara
karajohn
john wa senshu mary kara
wa  john senshu  wa mary  senshu kara mary
Figure 4: Graph-based representation of the ex-
ample sentence. We propagate the clause labels to
the Japanese word nodes on this graph to form the
clause alignments.
assume the labels are binary, the following objec-
tive is minimized:
argmin
l?RK+M
?
i,j
wij(li ? lj)2 (1)
where wij is the edge weight between nodes i
and j (1 ? i ? K + M , 1 ? j ? K +
M ), and l (li ? {0, 1}) is a vector of labels
on the nodes. The first K elements of l, lc =
(l1, l2, ..., lK)T , are constant because the clause
nodes are pre-labeled. The remaining M ele-
ments, lf = (lK+1, lK+2, ..., lK+M )T , are un-
known and to be determined. Here, we consider
the decomposition of the weight matrixW = [wij ]
into four blocks after the K-th row and column as
follows:
W =
[
W cc W cf
W fc W ff
]
(2)
The solution of eqn. (1), namely lf , is given by the
following equation:
lf = (Dff ?W ff )?1W fc lc (3)
where D is the diagonal matrix with di =
?
j wij
and is decomposed similarly to W . Each element
of lf is in the interval (0, 1) and can be regarded
as the label propagation probability. A detailed ex-
planation of this solution can be found in Section 2
of (Zhu et al, 2003). For our multi-label problem
with K labels, we slightly modified the algorithm
by expanding the vector l to an (M + K) ? K
binary matrix L = [ l1 l2 ... lK ].
After the optimization, we can normalize Lf
to obtain the clause alignment scores t(lm =
k|fm) between each Japanese word fm and En-
glish clause k. Theoretically, we can simply out-
put the clause id k? for each fm by finding k? =
argmaxk t(lm = k|fm). In practice, this may
sometimes lead to Japanese clauses that have too
many gaps, so we employ a two-stage procedure
to extract clauses that are more contiguous.
First, we segment the Japanese sentence into K
clauses based on a dynamic programming algo-
rithm proposed by Malioutov and Barzilay (2006).
We define an M ? M similarity matrix S = [sij ]
with sij = exp(?||li?lj ||) where li is (K + i)-th
row vector in the label matrix L. sij represents
the similarity between the i-th and j-th Japanese
words with respect to their clause alignment score
distributions; if the score distributions are sim-
ilar then sij is large. The details of this algo-
rithm can be found in (Malioutov and Barzilay,
2006). The clause segmentation gives us contigu-
ous Japanese clauses f?1, f?2, ..., f?K , thus min-
imizing inter-segment similarity and maximizing
intra-segment similarity. Second, we determine
the clause labels of the segmented clauses, based
on clause alignment scores T = [Tkk? ] for English
and automatically-segmented Japanese clauses:
Tkk? =
?
fm?f? k?
t(lm = k|fm) (4)
where f?k? is the j?-th Japanese clause. In descend-
ing order of the clause alignment score, we greed-
ily determine the clause label 3.
3.3 Training Clause Translation Models
We train clause translation models using the
clause-level aligned corpus. In addition we can
also include the original sentence-aligned corpus.
We emphasize that we can use standard techniques
for heuristically extracted phrase tables, word n-
gram language models, and so on.
3.4 Clause Translation
By using the source language parser, a multi-
clause source sentence is reduced to a set of
clauses. We translate these clauses with a common
SMT method using the clause translation models.
Here we present another English example I
bought the magazine which Tom recommended
yesterday. This sentence is segmented into clauses
as follows.
3Although a full search is available when the number of
clauses is small, we employ a greedy search in this paper.
422
? I bought the magazine s0 .
? which Tom recommended yersterday
These clauses are translated into Japanese:
? watashi (I) wa (topic marker) s0
zasshi (magazine) o (direct object marker)
kat (buy) ta (past tense marker).
? tom ga (subject marker) kino (yesterday)
susume (recommend) ta (past tense marker)
3.5 Sentence Reconstruction
We reconstruct the target sentence from the clause
translations, based on non-terminals. Starting
from the clause translation of the top clause, we re-
cursively replace non-terminal symbols with their
corresponding clause translations. Here, if a non-
terminal is eventually deleted in SMT decoding,
we simply concatenate the translation behind its
parent clause.
Using the example above, we replace the non-
terminal symbol s0 with the second clause and
obtain the Japanese sentence:
watashi wa tom ga kino susume ta zasshi o kat ta .
4 Experiment
We conducted the following experiments on the
English-to-Japanese translation of research paper
abstracts in the medical domain. Such techni-
cal documents are logically and formally writ-
ten, and sentences are often so long and syntac-
tically complex that their translation needs long
distance reordering. We believe that the medical
domain is suitable as regards evaluating the pro-
posed method.
4.1 Resources
Our bilingual resources were taken from the med-
ical domain. The parallel corpus consisted of
research paper abstracts in English taken from
PubMed4 and the corresponding Japanese transla-
tions.
The training portion consisted of 25,500 sen-
tences (no-clause-seg.; original sentences with-
out clause segmentation). 4,132 English sen-
tences in the corpus were composed of multi-
ple clauses and were separated at the clause level
4http://www.ncbi.nlm.nih.gov/pubmed/
by the procedure in section 3.1. As the syntac-
tic parser, we used the Enju5 (Miyao and Tsu-
jii, 2008) English HPSG parser. For these train-
ing sentences, we automatically aligned Japanese
words with each English clause as described in
section 3.2 and developed a clause-level aligned
corpus, called auto-aligned corpus. We prepared
manually-aligned (oracle) clauses for reference,
called oracle-aligned clauses. The clause align-
ment error rate of the auto-aligned corpus was
14% (number of wrong clause assignments di-
vided by total number of words). The develop-
ment and test portions each consisted of 1,032
multi-clause sentences. because this paper focuses
only on multi-clause sentences. Their English-
side was segmented into clauses in the same man-
ner as the training sentences, and the development
sentences had oracle clause alignment for MERT.
We also used the Life Science Dictionary6 for
training. We extracted 100,606 unique English
entries from the dictionary including entries with
multiple translation options, which we expanded
to one-to-one entries, and finally we obtained
155,692 entries.
English-side tokenization was obtained using
Enju, and we applied a simple preprocessing that
removed articles (a, an, the) and normalized plu-
ral forms to singular ones. Japanese-side tokeniza-
tion was obtained using MeCab7 with ComeJisyo8
(dictionary for Japanese medical document tok-
enization). Our resource statistics are summarized
in Table 1.
4.2 Model and Decoder
We used two decoders in the experiments,
Moses9 (Koehn et al, 2007) and our in-
house hierarchical phrase-based SMT (almost
equivalent to Hiero (Chiang, 2007)). Moses
used a phrase table with a maximum phrase
length of 7, a lexicalized reordering model with
msd-bidirectional-fe, and a distortion
limit of 1210. Our hierarchical phrase-based SMT
used a phrase table with a maximum rule length of
7 and a window size (Hiero?s ?) of 12 11. Both
5http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html
6http://lsd.pharm.kyoto-u.ac.jp/en/index.html
7http://mecab.sourceforge.net/
8http://sourceforge.jp/projects/comedic/ (in Japanese)
9http://www.statmt.org/moses/
10Unlimited distortion was also tested but the results were
worse.
11A larger window size could not be used due to its mem-
ory requirements.
423
Table 1: Data statistics on training, development,
and test sets. All development and test sentences
are multi-clause sentences.
Training
Corpus Type #words #sentences
Parallel E 690,536
(no-clause-seg.) J 942,913
25,550
Parallel E 135,698
(auto-aligned) J 183,043
4,132
(oracle-aligned) J 183,147
(10,766 clauses)
E 263,175 155.692Dictionary
J 291,455 (entries)
Development
Corpus Type #words #sentences
Parallel E 34,417 1,032
(oracle-aligned) J 46,480 (2,683 clauses)
Test
Corpus Type #words #sentences
Parallel E 34,433 1,032
(clause-seg.) J 45,975 (2,737 clauses)
decoders employed two language models: a word
5-gram language model from the Japanese sen-
tences in the parallel corpus and a word 4-gram
language model from the Japanese entries in the
dictionary. The feature weights were optimized
for BLEU (Papineni et al, 2002) by MERT, using
the development sentences.
4.3 Compared Methods
We compared four different training and test con-
ditions with respect to the use of clauses in training
and testing. The development (i.e., MERT) condi-
tions followed the test conditions. Two additional
conditions with oracle clause alignment were also
tested for reference.
Table 2 lists the compared methods. First,
the proposed method (proposed) used the auto-
aligned corpus in training and clause segmen-
tation in testing. Second, the baseline method
(baseline) did not use clause segmentation in ei-
ther training or testing. Using this standard base-
line method, we focused on the advantages of the
divide-and-conquer translation itself. Third, we
tested the same translation models as used with
the proposed method for test sentences without
clause segmentation, (comp.(1)). Although this
comparison method cannot employ the proposed
clause-level reordering, it was expected to be bet-
ter than the baseline method because its transla-
tion model can be trained more precisely using the
finely aligned clause-level corpus. Finally, the sec-
ond comparison method (comp.(2)) translated seg-
mented clauses with the baseline (without clause
segmentation) model, as if each of them was a sin-
gle sentence. Its translation of each clause was
expected to be better than that of the baseline be-
cause of the efficient search over shortened inputs,
while its reordering of clauses (non-terminals) was
unreliable due to the lack of clause information
in training. Its sentence reconstruction based on
non-terminals was the same as with the proposed
method. Although non-terminals in the second
comparison method were out-of-vocabulary words
and may be deleted in decoding, all of them sur-
vived and we could reconstruct sentences from
translated clauses throughout the experiments. In
addition, two other conditions were tested: us-
ing oracle-aligned clauses in training: the pro-
posed method trained using oracle-aligned (ora-
cle) clauses and the first comparison method using
oracle-aligned (oracle-comp.) clauses.
4.4 Results
Table 3 shows the results in BLEU, Transla-
tion Edit Rate (TER) (Snover et al, 2006),
and Position-independent Word-error Rate (PER)
(Och et al, 2001), obtained with Moses and our
hierarchical phrase-based SMT, respectively. Bold
face results indicate the best scores obtained with
the compared methods (excluding oracles).
The proposed method consistently outper-
formed the baseline. The BLEU improve-
ments with the proposed method over the base-
line and comparison methods were statistically
significant according to the bootstrap sampling
test (p < 0.05, 1,000 samples) (Zhang et al,
2004). With Moses, the improvement when us-
ing the proposed method was 1.4% (33.19% to
34.60%) in BLEU and 1.3% (57.83% to 56.50%)
in TER, with a slight improvement in PER
(35.84% to 35.61%). We observed: oracle ?
proposed ? comp.(1) ? baseline ? comp.(2)
by the Bonferroni method, where the symbol
A ? B means ?A?s improvement over B is
statistically significant.? With the hierarchical
phrase-based SMT, the improvement was 2.2%
(32.39% to 34.55%) in BLEU, 3.5% (58.36% to
54.87%) in TER, and 1.5% in PER (36.42% to
34.79%). We observed: oracle ? proposed ?
424
Table 2: Compared methods.
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. proposed comp.(2) oracle
no-clause-seg. comp.(1) baseline oracle-comp.
{comp.(1), comp.(2)} ? baseline by the Bon-
ferroni method. The oracle results were better than
these obtained with the proposed method but the
differences were not very large.
4.5 Discussion
We think the advantage of the proposed method
arises from three possibilities: 1) better translation
model training using the fine-aligned corpus, 2) an
efficient decoder search over shortened inputs, and
3) an effective clause-level reordering model real-
ized by using non-terminals.
First, the results of the first comparison method
(comp.(1)) indicate an advantage of the transla-
tion models trained using the auto-aligned corpus.
The training of the translation models, namely
word alignment and phrase extraction, is difficult
for long sentences due to their large ambiguity.
This result suggests that the use of clause-level
alignment provides fine-grained word alignments
and precise translation models. We can also ex-
pect that the model of the proposed method will
work better for the translation of single-clause sen-
tences.
Second, the average and median lengths (in-
cluding non-terminals) of the clause-seg. test set
were 13.2 and 10 words, respectively. They were
much smaller than those of no-clause-seg. at 33.4
and 30 words and are expected to help realize
an efficient SMT search. Another observation is
the relationship between the number of clauses
and translation performance, as shown in Fig-
ure 5. The proposed method achieved a greater im-
provement in sentences with a greater number of
clauses. This suggests that our divide-and-conquer
approach works effectively for multi-clause sen-
tences. Here, the results of the second comparison
method (comp.(2)) with Moses were worse than
the baseline results, while there was an improve-
ment with our hierarchical phrase-based SMT.
This probably arose from the difference between
the decoders when translating out-of-vocabulary
words. The non-terminals were handled as out-of-
vocabulary words under the comp.(2) condition.
52
54
56
58
60
62
64
66
2 4 53
TE
R
 (%
)
The number of clauses
baseline
proposed
comp.(2)
Figure 5: Relationship between TER and number
of clauses for proposed, baseline, and comp.(2)
when using our hierarchical phrase-based SMT.
Moses generated erroneous translations around
such non-terminals that can be identified at a
glance, while our hierarchical phrase-based SMT
generated relatively good translations. This may
be a decoder-dependent issue and is not an essen-
tial problem.
Third, the results obtained with the proposed
method reveal an advantage in reordering in ad-
dition to the previous two advantages. The differ-
ence between the PERs with the proposed method
and the baseline with Moses was small (0.2%)
in spite of the large differences in BLEU and
TER (about 1.5%). This suggests that the pro-
posed method is better in word ordering and im-
plies our method is also effective in reordering.
With the hierarchical phrase-based SMT, the pro-
posed method showed a large improvement from
the baseline and comparison methods, especially
in TER which was better than the best Moses
configuration (proposed). This suggests that the
decoding of long sentences with long-distance
reordering is not easy even for the hierarchical
phrase-based SMT due to its limited window size,
while the hierarchical framework itself can natu-
rally model a long-distance reordering. If we try to
find a derivation with such long-distance reorder-
ing, we will probably be faced with an intractable
search space and computation time. Therefore,
we can conclude that the proposed divide-and-
425
Table 3: Experimental results obtained with Moses and our hierarchical phrase-based SMT, in BLEU,
TER, and PER.
Moses : BLEU (%) / TER (%) / PER (%)
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. 34.60 / 56.50 / 35.61 32.14 / 58.78 / 36.08 35.31 / 55.12 / 34.42
no-clause-seg. 34.22 / 56.90 / 35.20 33.19 / 57.83 / 35.84 34.24 / 56.67 / 35.03
Hierarchical : BLEU (%) / TER (%) / PER (%)
P
P
P
P
P
P
P
P
Test
Training w/ auto-aligned w/o aligned w/ oracle-aligned
clause-seg. 34.55 / 54.87 / 34.79 33.03 / 56.70 / 36.03 35.08 / 54.22 / 34.77
no-clause-seg. 33.41 / 57.02 / 35.86 32.39 / 58.36 / 36.42 33.83 / 56.26 / 34.96
conquer approach provides more practical long-
distance reordering at the clause level.
We also analyzed the difference between auto-
matic and manual clause alignment. Since auto-
aligned corpus had many obvious alignment er-
rors, we suspected these noisy clauses hurt the
clause translation model. However, they were not
serious in terms of final translation performance.
So we can conclude that our proposed divide-and-
conquer approach is promising for long sentence
translation. Although we aimed to see whether we
could bootstrap using existing bilingual corpora in
this paper, we imagine better clause alignment can
be obtained with some supervised classifiers.
One problem with the divide-and-conquer ap-
proach is that its independently-translated clauses
potentially cause disfluencies in final sentence
translations, mainly due to wrong inflections. A
promising solution is to optimize a whole sentence
translation by integrating search of each clause
translation but this may require a much larger
search space for decoding. More simply, we may
be able to approximate it using n-best clause trans-
lations. This problem should be addressed for fur-
ther improvement in future studies.
5 Conclusion
In this paper we proposed a clause-based divide-
and-conquer approach for SMT that can re-
duce complicated clause-level reordering to sim-
ple word-level reordering. The proposed method
separately translates clauses with non-terminals by
using a well-known SMT method and reconstructs
a sentence based on the non-terminals, to reorder
long clauses. The clause translation models are
trained using a bilingual corpus with clause-level
alignment, which can be obtained with an un-
supervised graph-based method using sentence-
aligned corpora. The proposed method improves
the translation of long, multi-clause sentences and
is especially effective for language pairs with
large word order differences, such as English-to-
Japanese.
This paper focused only on clauses as segments
for division. However, other long segments such
as prepositional phrases are similarly difficult to
reorder correctly. The divide-and-conquer ap-
proach itself can be applied to long phrases, and
it is worth pursuing such an extension. As another
future direction, we must develop a more sophis-
ticated method for automatic clause alignment if
we are to use the proposed method for various lan-
guage pairs and domains.
Acknowledgments
We thank the U. S. National Library of Medicine
for the use of PubMed abstracts and Prof. Shuji
Kaneko of Kyoto University for the use of Life
Science Dictionary. We also thank the anonymous
reviewers for their valuable comments.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL, pages 531?540.
426
Osamu Furuse, Setsuo Yamada, and Kazuhide Ya-
mamoto. 1998. Splitting long or ill-formed in-
put for robust spoken-language translation. In Proc.
COLING-ACL, pages 421?427.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. NAACL, pages 273?280.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proc. HLT-NAACL, pages 105?
112.
Jason Katz-Brown and Michael Collins. 2008. Syntac-
tic reordering in preprocessing for Japanese-English
translation: MIT system description for NTCIR-7
patent translation task. In Proc. NTCIR-7, pages
409?414.
Yeun-Bae Kim and Terumasa Ehara. 1994. A method
for partitioning of long Japanese sentences with sub-
ject resolution in J/E machine translation. In Proc.
International Conference on Computer Processing
of Oriental Languages, pages 467?473.
Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL, pages 263?270.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proc. IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ACL Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proc. ACL, pages 720?727.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-String alignment template for statistical machine
translation. In Proc. Coling-ACL, pages 609?616.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Proc.
Coling-ACL, pages 25?32.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for statis-
tical machine translation. In Proc. the ACL Work-
shop on Data-Driven Methods in Machine Transla-
tion, pages 55?62.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL,
pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Proc.
HLT-NAACL, pages 101?104.
Erik F. Tjong, Kim Sang, and Herve? De?jean. 2001. In-
troduction to the CoNLL-2001 shared task: Clause
identification. In Proc. CoNLL, pages 53?57.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proc.
EMNLP, pages 1007?1016.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. COLING, pages 508?514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for Subject-Object-Verb languages. In Proc.
HLT-NAACL, pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. ACL,
pages 523?530.
Ying Zhang, Stephan Vogel, and Alex Weibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proc. LREC, pages 2051?2054.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proc. ICML, pages
912?919.
427
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 287?292,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Dependency-based Automatic Enumeration of Semantically Equivalent
Word Orders for Evaluating Japanese Translations
Hideki Isozaki, Natsume Kouchi
Okayama Prefectural University
111 Kuboki, Soja-shi, Okayama, 719-1197, Japan
isozaki@cse.oka-pu.ac.jp
Tsutomu Hirao
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho, Sorakugun, Kyoto, 619-0237, Japan
hirao.tsutomu@lab.ntt.co.jp
Abstract
Scrambling is acceptable reordering of
verb arguments in languages such as
Japanese and German. In automatic eval-
uation of translation quality, BLEU is
the de facto standard method, but BLEU
has only very weak correlation with hu-
man judgements in case of Japanese-to-
English/English-to-Japanese translations.
Therefore, alternative methods, IMPACT
and RIBES, were proposed and they have
shown much stronger correlation than
BLEU. Now, RIBES is widely used in
recent papers on Japanese-related transla-
tions. RIBES compares word order of MT
output with manually translated reference
sentences but it does not regard scram-
bling at all. In this paper, we present a
method to enumerate scrambled sentences
from dependency trees of reference sen-
tences. Our experiments based on NTCIR
Patent MT data show that the method im-
proves sentence-level correlation between
RIBES and human-judged adequacy.
1 Introduction
Statistical Machine Translation has grown with an
automatic evaluation method BLEU (Papineni et
al., 2002). BLEU measures local word order by n-
grams and does not care about global word order.
In JE/EJ translations, this insensitivity degrades
BLEU?s correlation with human judgements.
Therefore, alternative automatic evaluation
methods are proposed. Echizen-ya and Araki
(2007) proposed IMPACT. Isozaki et al. (2010)
presented the idea of RIBES. Hirao et al. (2011)
named this method ?RIBES? (Rank-based Intu-
itive Bilingual Evaluation Score). This version of
RIBES was defined as follows:
RIBES = NKT? P
?
Table 1: Meta-evaluation of NTCIR-7 JE task data
(Spearman?s ?, System-level correlation)
BLEU METEOR ROUGE-L IMPACT RIBES
0.515 0.490 0.903 0.826 0.947
where NKT (Normalized Kendall?s ? ) is defined
by (? + 1)/2. This NKT is used for measur-
ing word order similarity between a reference sen-
tence and an MT output sentence. Thus, RIBES
penalizes difference of global word order. P is
precision of unigrams. RIBES is defined for each
test sentence and averaged RIBES is used for eval-
uating the entire test corpus.
Table 1 is a table in an IWSLT-2012 invited
talk (http://hltc.cs.ust.hk/iwslt/slides/
Isozaki2012 slides.pdf). METEOR was pro-
posed by Banerjee and Lavie (2005). ROUGE-L
was proposed by Lin and Och (2004). According
to this table, RIBES with ? = 0.2 has a very
strong correlation (Spearman?s ? = 0.947) with
human-judged adequacy. For each sentence,
we use the average of adequacy scores of three
judges. Here, we call this average ?Adequacy?.
We focus on Adequacy because current SMT
systems tend to output inadequate sentences.
Note that only single reference translations are
available for this task although use of multiple
references is common for BLEU.
RIBES is publicly available from http://
www.kecl.ntt.co.jp/icl/lirg/ribes/ and
was used as a standard quality measure in recent
NTCIR PatentMT tasks (Goto et al., 2011; Goto
et al., 2013). Table 2 shows the result of meta-
evaluation at NICTR-9/10 PatentMT. The table
shows that RIBES is more reliable than BLEU
and NIST.
Current RIBES has the following improve-
ments.
? BLEU?s Brevity Penalty (BP) was introduced
287
Table 2: Meta-evaluation at NTCIR-9/10
PatentMT (Spearman?s ?, Goto et al. 2011, 2013)
BLEU NIST RIBES
NTCIR-9 JE ?0.042 ?0.114 0.632
NTCIR-9 EJ ?0.029 ?0.074 0.716
NTCIR-10 JE 0.31 0.36 0.88
NTCIR-10 EJ 0.36 0.22 0.79
in order to penalize too short sentences.
RIBES = NKT? P
?
? BP
?
where ? = 0.25 and ? = 0.10. BLEU uses
BP for the entire test corpus, but RIBES uses
it for each sentence.
? The word alignment algorithm in the original
RIBES used only bigrams for disambiguation
when the same word appears twice or more
in one sentence. This restriction is now re-
moved, and longer n-grams are used to get a
better alignment.
RIBES is widely used in recent Annual Mee-
ings of the (Japanese) Association for NLP. In-
ternational conference papers on Japanese-related
translations also use RIBES. (Wu et al., 2012;
Neubig et al., 2012; Goto et al., 2012; Hayashi
et al., 2013). Dan et al. (2012) uses RIBES for
Chinese-to-Japanese translation.
However, we have to take ?scrambling? into
account when we think of Japanese word order.
Scrambling is also observed in other languages
such as German. Current RIBES does not regard
this fact.
2 Methodology
For instance, a Japanese sentence S1
jon ga sushi-ya de o-sushi wo tabe-ta .
(John ate sushi at a sushi restaurant.)
has the following acceptable word orders.
1. jon ga sushi-ya de o-sushi wo tabe-ta .
2. jon ga o-sushi wo sushi-ya de tabe-ta .
3. sushi-ya de jon ga o-sushi wo tabe-ta .
4. sushi-ya de o-sushi wo jon ga tabe-ta .
5. o-sushi wo jon ga sushi-ya de tabe-ta .
6. o-sushi wo sushi-ya de jon ga tabe-ta .
The boldface short words ?ga?, ?de?, and
?wo?, are case markers (?Kaku joshi? in
Japanese).
tabe-ta
sushi-ya dejon ga
o-sushi wo
Figure 1: Dependency Tree of S1
? ?ga? is a nominative case marker that means
the noun phrase before it is the subject of a
following verb/adjective.
? ?de? is a locative case marker that means the
noun phrase before it is the location of a fol-
lowing verb/adjective.
? ?wo? is an accusative case marker that means
the noun phrase before it is the direct object
of a following verb.
The term ?scrambling? stands for these accept-
able permutations. These case markers explicitly
show grammatical cases and reordering of them
does not hurt interpretation of these sentences. Al-
most all other permutations of words are not ac-
ceptable (?).
? jon ga de sushi-ya o-sushi tabe-ta wo .
? jon de sushi-ya ga o-sushi wo tabe-ta .
? jon tabe-ta ga o-sushi wo sushi-ya de .
? sushi-ya ga jon tabe-ta de o-sushi wo .
Most readers unfamiliar with Japanese will not
understand which word order is acceptable.
2.1 Scrambling as Post-Order Traversal of
Depenedncy Trees
Here, we describe this ?scrambling? from the
viewpoint of Computer Science. Figure 1 shows
S1?s dependency tree. Each box indicates a ?bun-
setsu? or a grammatical chunk of words. Each ar-
row starts from a modifier (dependent) to its head.
The root of S1 is ?tabe-ta? (ate). This verb
has three modifiers:
? ?jon ga? (John is its subject)
? ?sushi-ya de? (A sushi restaurant is its location)
? ?o-sushi wo? (Sushi is its object)
It is well known that Japanese is a typical head-
final language. In order to generate a head-final
word order from this dependency tree, we should
output tree nodes in post-order. That is, we have
to output all children of a node N before the node
N itself.
288
mi-ta
ato ni kabuki wo
tabe-ta
sushi-ya dejon ga
o-sushi wo
Figure 2: Dependency Tree of S2
All of the above acceptable word orders follows
this post-order. Even in post-order traverse, prece-
dence among children is not determined and this
fact leads to different permutations of children. In
the above example, the root ?tabe-ta? has three
children, and its permutation is 3! = 6.
2.2 Simple Case Marker Constraint
Figure 2 shows the dependency tree of a more
complicated sentence S2:
jon ga sushi-ya de o-sushi wo tabe-ta
ato ni kabuki wo mi-ta .
(John watched kabuki after eating sushi at a shushi
restaurant)
Kabuki is a traditional Japanese drama performed
in a theatre. In this case, the root ?mi-ta?
(watched) has two children: ?ato ni? (after it)
and ?kabuki wo? (kabuki is its object).
? ?ni? is a dative/locative case marker that
means the noun phrase before it is an indi-
rect object or a location/time of a following
verb/adjective.
In this case, we obtain 3!?2! = 12 permutations:
1. *S1P* ato ni kabuki wo mi-ta .
2. kabuki wo *S1P* ato ni mi-ta .
Here, *S1P* is any of the above 3! permutations
of S1. If we use S1?s 3 as *S1P* in S2?s 1, we get
sushi-ya de jon ga o-sushi wo tabe-ta
ato ni kabuki wo mi-ta .
However, we cannot accept all of these permu-
tations equally. For instance,
kabuki wo o-sushi wo sushi-ya de
jon ga tabe-ta ato ni mi-ta .
is comprehensible but strange. This strangness
comes from the two objective markers ?wo? be-
fore the first verb ?tabe-ta.? Which did John
eat, kabuki or sushi? Semantically, we cannot
eat kabuki (drama), and we can understand this
sentence. But syntactic ambiguity causes this
strangeness. Without semantic knowledge about
kabuki and sushi, we cannot disambiguate this
case.
For readers/listeners, we should avoid such
syntactically ambiguous sentences. Modifiers
(here, ?kabuki wo?) of a verb (here, ?mi-ta?,
watched) should not be placed before another verb
(here, ?tabe-ta?, ate).
In Japanese, verbs and adjectives are used sim-
ilarly. In general, adjectives are not modified by
?wo? case markers. Therefore, we can place ?wo?
case markers before adjectives. In the following
sentences, ?atarashii? (new) is an adjective
and placing ?inu wo? (A dog is the direct object)
before ?atarashii? does not make the sentence
ambiguous.
? atarashii ie ni inu wo ture te itta .
((Someone) took the dog to the new house.)
? inu wo atarashii ie ni ture te itta .
This idea leads to the following Simple Case
Marker Constraint:
Definition 1 (Simple Case Marker Constraint)
If a reordered sentence has a case marker phrase
of a verb that precedes another verb before the
verb, the sentence is rejected. ?wo? case markers
can precede adjectives before the verb.
This is a primitive heuristic constraint and there
must be better ways to make it more flexible.
If we use Nihongo Goi Taikei (Ikehara et al.,
1997), we will be able to implement such a flex-
isble constraint. For example, some verbs such
as ?sai-ta? (bloomed) are never modified by
?wo? case marker phrases. Therefore, the follow-
ing sentence is not ambiguous at all although the
wo phrase precedes ?sai-ta?.
? hana ga sai-ta ato ni sono ki wo mi-ta.
((Someone) saw the tree after it bloomed.)
? sono ki wo hana ga sai-ta ato ni mi-ta.
2.3 Evaluation with scrambled sentences
As we mentioned before, RIBES measures global
word order similarity between machine-translated
sentences and reference sentences. It does not re-
gard scrambling at all. When the target language
allows scrambling just like Japanese, RIBES
should consider scrambling.
Once we have a correct dependency tree of the
reference sentence, we enumerate scrambled sen-
tences by reordering children of each node. The
289
number of the reordered sentences depend on the
structure of the dependency tree.
Current RIBES code (RIBES-1.02.4) assumes
that every sentence has a fixed number of refer-
ences, but here the number of automatically gen-
erated reference sentences depends on the depen-
dency structure of the original reference sentence.
Therefore, we modified the code for variable num-
bers of reference sentences. RIBES-1.02.4 simply
uses the maximum value of the scores for different
reference sentences, and we followed it.
Here, we compare the following four methods.
? single: We use only single reference transla-
tions provided by the NTCIR organizers.
? postOrder: We generate all permutations of
the given reference sentence generated by
post-order traversals of its dependency tree.
This can be achieved by the following two
steps. First, we enumerate all permutations
of child nodes at each node. Then, we com-
bine these permutations. This is implemented
by cartesian products of the permutation sets.
? caseMarkers: We reorder only ?case marker
(kaku joshi) phrases?. Here, a ?case marker
phrase? is post-order traversal of a subtree
rooted at a case marker bunsetsu. For in-
stance, the root of the following sentence S3
has a non-case marker child ?kaburi ,?
(wear) between case marker children, ?jon
ga? and ?zubon wo? (Trousers are the ob-
ject). Figure 3 shows its dependency tree.
jon ga shiroi boushi wo kaburi ,
kuroi zubon wo hai te iru.
(John wears a white hat and wears black trousers.)
This is implemented by removing non-case
marker nodes from the set of child nodes
to be reordered in the above ?postOrder?
method. For simplicity, we do not reorder
other markers such as the topic marker ?wa?
here. This is future work.
? proposed: We reorder only contiguous case
marker children of a node, and we accept sen-
tences that satisfy the aforementioned Sim-
ple Case Marker Constraint. S3?s root node
has two case marker children, but they are
not contiguous. Therefore, we do not reorder
them. We expect that the constraint inhibit
generation of incomprehensible or mislead-
ing sentences.
hai te iru.
kaburi ,
jon ga
zubon wo
boushi wo
shiroi
kuroi
Figure 3: Dependency Tree of S3
Table 3: Distribution of the number of generated
permutations
#permutations 1 2 4 6 8 12 16 24 >24
single 100 0 0 0 0 0 0 0 0
proposed 70 20 7 3 0 0 0 0 0
caseMarkers 64 23 4 6 2 2 0 2 0
postOrder 1 17 9 11 4 12 1 12 33
3 Results
We applied the above four methods to the ref-
erence sentences of human-judged 100 sentences
of NTCIR-7 Patent MT EJ task. (Fujii et al.,
2008) We applied CaboCha (Kudo and Mat-
sumoto, 2002) to the reference sentences, and
manually corrected the dependency trees because
Japanese dependency parsers are not satisfactory
in terms of sentence accuracy (Tamura et al.,
2007).
To support this manual correction, CaboCha?s
XML output was automatically converted
to dependency tree pictures by using
cabochatrees package for L
A
T
E
X. http://
softcream.oka-pu.ac.jp/wp/wp-content/
uploads/cabochatrees.pdf. Then, it is easy
to find mistakes of the dependency trees. In
addition, CaboCha?s dependency accuracy is very
high (89?90%) (Kudo and Matsumoto, 2002).
Therefore, it took only one day to fix dependency
trees of one hundred reference sentences.
Table 3 shows distribution of the number of
word orders generated by the above methods. Pos-
tOrder sometimes generates tens of thousands of
permutations.
Figure 4 shows a sentence-level scatter plot
between Adequacy and RIBES for the baseline
Moses system. Each ? indicates a sentence.
Arrows indicate significant improvements of
RIBES scores by the proposed method. For in-
stance, the?mark at (5.0, 0.53) corresponds to an
MT output:
290
Adequacy0 1 2 3 4 5
RIBES
0
0.2
0.4
0.6
0.8
1
Average of RIBES: 0.706? 0.719
Pearson?s r: 0.607? 0.663
Spearman?s ?: 0.607? 0.670
Figure 4: Scatter plot between Adequacy and
RIBES for 100 human-judged sentences in the
output of NTCIR-7?s baseline Moses system and
the effects of the proposed method
indekkusu kohna wo zu 25 ni shimesu .
which is a Japanese translation of ?FIG.25 shows
the index corner.? The reference sentence for this
sentence is
zu 25 ni indekkusu kohna wo shimeshi
te iru .
In this case, RIBES is 0.53, but all of the three
judges evaluated this as 5 of 5-point scale. That
is, RIBES disagrees with human judges. The pro-
posed method reorders this reference sentence as
follows:
indekkusu kohna wo zu 25 ni shimeshi
te iru .
This is very close to the above MT output and
RIBES is 0.884 for this automatically reordered
reference sentence. This shows that automatic re-
ordering reduces the gap between single-reference
RIBES and Adequacy.
Although RIBES strongly correlates with ade-
quacy at the system level (Table 1), it has only
mediocre correlation with adequacy at the sen-
tence level: Spearman?s ? is 0.607 for the baseline
Moses system. The ?proposed? method improves
it to 0.670.
We can draw similar scatter plots for each sys-
tem. Table 4 summarises such improvement of
correlations. And this is the main result of this
Table 4: Improvement of sentence-level correla-
tion between Adequacy and RIBES for human-
judged NTCIR-7 EJ systems (MAIN RESULT)
Pearson?s r Spearman?s ?
single? proposed single? proposed
tsbmt 0.466 ? 0.472 0.439 ? 0.452
Moses 0.607 ? 0.663 0.607 ? 0.670
NTT 0.709 ? 0.735 0.692 ? 0.727
NICT-ATR 0.620 ? 0.631 0.582 ? 0.608
kuro 0.555 ? 0.608 0.515 ? 0.550
Table 5: Increase of averaged RIBES scores
Adeq. RIBES
system single proposed caseMarkers postOrder
tsbmt 3.527 0.715 0.7188 0.719 0.7569
moses 2.897 0.706 0.7192 0.722 0.781
NTT 2.740 0.671 0.683 0.686 0.7565
NICT-ATR 2.587 0.655 0.664 0.670 0.749
kuro 2.420 0.629 0.638 0.647 0.752
paper. The ?proposed? method consistently im-
proves sentence-level correlation between Ade-
quacy and RIBES.
Table 5 shows increase of averaged RIBES, but
this increase is not always an improvement. We
expected that ?PostOrder? generates not only ac-
ceptable sentences but also incomprehensible or
misleading sentences. This must be harmful to the
automatic evaluation by RIBES. Accoding to this
table, PostOrder gave higher RIBES scores to all
systems and correlation between RIBES and Ade-
quacy is lost as expected.
The ranking by RIBES-1.02.4 with ?single?
reference sentences completely agrees with Ad-
equacy, but the weakest constraint, ?postOrder?,
disagrees. Spearman?s ? of the two ranks is 0.800
but Pearson?s r is as low as 0.256. It generates too
many incomprehensible/misleading word orders,
and they also raise RIBES scores of bad transla-
tions. On the other hand, ?proposed? and ?case-
Markers? agree with Adequacy except the ranks
of tsbmt and the baseline Moses.
4 Concluding Remarks
RIBES is now widely used in Japanese-related
translation evaluation. But RIBES sometimes pe-
nalizes good sentences because it does not re-
gard scrambling. Once we have correct depen-
dency trees of reference sentences, we can auto-
matically enumerate semantically equivalent word
291
orders. Less constrained reordering tend to gener-
ate syntactically ambiguous sentences. They be-
come incomprehensible or misleading sentences.
In order to avoid them, we introduced Simple
Case Marker Constraint and restricted permuta-
tions to contiguous case marker children of verbs/
adjectives. Then, sentence-level correlation coef-
ficients were improved.
The proposed enumeration method is also ap-
plicable to other automatic evaluation methods
such as BLEU, IMPACT, and ROUGE-L, but we
have to modify their codes for variable numbers of
multi-reference sentences. We will examine them
in the full paper.
We hope our method is also useful for other lan-
guages that have scrambling.
Acknowledgement
This research was supported by NTT Communi-
cation Science Laboratories.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with im-
proved correlation with human judgements. In Proc.
of ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for MT and Summarization, pages
65?72.
Han Dan, Katsuhito Sudoh, Xianchao Wu, Kevin Duh,
Hajime Tsukada, and Masaaki Nagata. 2012. Head
finalization reordering for Chinese-to-Japanese ma-
chine translation. In Proceedings of SSST-6, Sixth
Workshop on Syntax, Semantics and Structure in
Statistical Translation, pages 57?66.
Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic
evaluation of machine translation based on recursive
acquisition of an intuitive common parts continuum.
In MT Summit XI, pages 151?158.
Atsushi Fujii, Masao Uchimura, Mikio Yamamoto, and
Takehito Usturo. 2008. Overview of the patent
machine translation task at the NTCIR-7 workshop.
In Working Notes of the NTCIR Workshop Meeting
(NTCIR).
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Working Notes of the NTCIR Workshop Meeting
(NTCIR).
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for japanese-english statis-
tical machine translation. In Proc. of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 311?316.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K. Tsou. 2013. Overview of the patent
machine translation task at the NTCIR-10 work-
shop. In Working Notes of the NTCIR Workshop
Meeting (NTCIR).
Katsuhiko Hayashi, Katsuhito Sudoh, Hajime Tsukada,
Jun Suzuki, and Masaaki Nagata. 2013. Shift-
reduce word reordering for machine translation.
In Proc. of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1382?1386.
Tsutomu Hirao, Hideki Isozaki, Kevin Duh, Katsuhito
Sudoh, Hajime Tsukada, and Masaaki Nagao. 2011.
RIBES: An automatic evaluation method of trans-
lation based on rank correlation (in Japanese). In
Proc. of the Annual Meeting of the Association for
Natural Language Processing, pages 1115?1118.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997.
Goi-Taikei ? A Japanese Lexicon (in Japanese).
Iwanami Shoten.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Kat-
suhito Sudoh, Hajime Tsukada, and Masaaki Na-
gata. 2010. Automatic evaluation of translation
quality for distant language pairs. In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 944?952.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of the Conference on Computational Natural
Language Learning (CoNLL).
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of translation quality using longest com-
mon subsequences and skip-bigram statistics. In
Proc. of the Annual Meeting of the Association of
Computational Linguistics (ACL), pages 605?612.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proc. of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 843?853.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 311?318.
Akihiro Tamura, Hiroya Takamura, and Manabu Oku-
mura. 2007. Japanese dependency analysis using
the ancestor-descendant relation. In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 600?609.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsu-
jii. 2012. Akamon: An open source toolkit for
tree/forest-based statistical machine translation. In
Proc. of the Annual Meeting of the Association of
Computational Linguistics (ACL), pages 127?132.
292
