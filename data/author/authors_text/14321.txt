Coling 2010: Poster Volume, pages 1131?1139,
Beijing, August 2010
Using Clustering to Improve Retrieval Evaluation without 
Relevance Judgments 
Zhiwei Shi 
Institute of Computing Technology 
Chinese Academy of Science 
shizhiwei@ict.ac.cn
Peng Li 
Institute of Computing Technology 
Chinese Academy of Science 
lipeng01@ict.ac.cn
Bin Wang 
Institute of Computing Technology 
Chinese Academy of Science 
wangbin@ict.ac.cn
Abstract
Retrieval evaluation without relevance 
judgments is a hard but also very mean-
ingful work. In this paper, we use clus-
tering technique to improve the per-
formance of judgment free retrieval 
evaluation. By using one system to rep-
resent all the systems that are similar to 
it, we can largely reduce the negative ef-
fect of similar retrieval results in Re-
trieval evaluation. Experimental results 
demonstrated that our method outper-
formed all the previous judgment free 
evaluation methods significantly. Its 
overall average performance outper-
formed the best previous result by 
20.5%. Besides, our work is a general 
framework that can be applied to any 
other judgment free evaluation method 
for performance improvement. 
1 Introduction 
Generally, to compare the effectiveness of in-
formation retrieval systems, we need to prepare 
a test collection composed of a set of documents, 
a set of query topics, and a set of relevance 
judgments indicating which documents are rele-
vant to which topics. Among these requirements, 
relevance judgment is the most human resource 
exhausting and time consuming part. It even 
becomes infeasible when the test collection is 
extremely large. To address this problem, the 
TREC conferences used a pooling technology 
(Voorhees and Harman, 1999), where the top n
(e.g., n=100) documents retrieved by each par-
ticipating system are collected into a pool and 
then only the documents in the pool are judged 
for system comparison. Zobel (1998) has shown 
that this pooling method leads to reliable results 
in term of determining the effectiveness of re-
trieval systems and their relative rankings. Yet, 
the relevance determination process is still very 
resource intensive especially when the test col-
lection reaches or exceeds terabyte, or much 
more queries are included. More seriously, 
when we change to a new document collection, 
we have to redo the entire evaluation process.  
There are two possible solutions to the prob-
lem above, evaluation with incomplete rele-
vance judgments and evaluation without rele-
vance judgments. The former is well studied.  
Many well designed ranking methods with in-
complete judgments were carried out. Two of 
them, Minimal Test Collection (MTC) method 
(Carterette et al, 2006) and Statistical evalua-
tion (statMAP) method (Aslam et al, 2006), 
even got practical application in the Million 
Query (1MQ) track in TREC 2007 (Allan et al,
2007), and achieved satisfactory evaluation per-
formance. The latter is comparatively less stud-
ied. Only a few papers concentrate on the issue 
of evaluating retrieval systems without rele-
vance judgments. In Section 2 of this paper, we 
will briefly review some representative methods. 
We will see what they are and how they work.  
1131
In this paper, we focus our effort on the re-
trieval evaluation without relevance judgments. 
Although ?blind? evaluation is really a hard 
problem and its evaluation performance is far 
less than that of methods with incomplete 
judgments, it is undeniable that non-judgment 
evaluation has its own advantages. In some 
cases, relevance judgments are non-attainable. 
For example, when researchers compare their 
novel retrieval algorithms to existing methods, 
or search for optimal parameters of their algo-
rithms, or conduct data fusion in a dynamic en-
vironment, relevance judgment usually seems 
impossible. Besides, to construct a good evalua-
tion method without relevance judgments, re-
searchers need to mine the retrieval results thor-
oughly, and try to find laws that indicate the 
correlation between the effectiveness of a sys-
tem and features of its retrieval result. These 
laws are not only useful for ?blind? evaluation 
methods but also valuable for evaluation meth-
ods with incomplete judgments. 
One of the useful laws for ?blind? evaluation 
methods is Authority Effect (Spoerri, 2005). Yet 
it always ruined by multiple similar results. 
In this work, we use clustering technique to 
solve this problem. By selecting one system to 
represent all the systems that are similar to it, 
we can largely reduce the negative effect of 
similar retrieval results. Details of this method 
will be presented Section 3. Experimental re-
sults, which are reported in Section 4, also veri-
fied that our idea is feasible and effective. Our 
method outperformed all the previous judgment 
free evaluation methods on every test bed.  The 
overall average performance outperformed the 
best previous result by 20.5%. Finally, we con-
clude our work in Section 5. 
2 Related Work 
In 2001, Soboroff et al (2001) firstly proposed 
the concept of evaluating retrieval systems in 
the absence of relevance judgments. They 
generated a set of pseudo-relevance judgments 
by randomly selecting and declaring some 
documents from the pool of top 100 documents 
as relevant. This set of pseudo-relevance 
judgments (instead of a set of human relevance 
judgments) was then used to determine the 
effectiveness of the retrieval systems. Four 
versions of this random pseudo-relevance 
method were designed and tested on data from 
the ad hoc track in TREC 3, 5, 6, 7 and 8. They 
were simple random pseudo-relevance method, 
the variant with duplicate documents, the 
variant with Shallow pools and the variant with 
Exact-fraction sampling. All their resulting 
system assessments and rankings were well 
correlated with actual TREC rankings, and the 
variant with duplicate documents in pools got 
the best performance, with an average Kendall?s 
tau value 0.50 over the data of TREC 3, 5, 6, 7 
and 8. 
Soboroff et al?s idea came from two results 
in retrieval evaluation. One is that incomplete 
judgments do not harm evaluation results 
greatly. Zobel?s (1998) research had showed 
that the results obtained using pooling technol-
ogy were quite reliable given a pool depth of 
100. He also found that even though the pool 
depth was limited to 10, the relative perform-
ance among systems changed little, although 
actual precision scores did change for some sys-
tems. The other is that partially incorrect rele-
vance judgments do not harm evaluation results 
greatly. Voorhees (1998) ascertained that de-
spite a low average overlap between assessment 
sets, and wide variation in overlap among par-
ticular topics, the relative rankings of systems 
remained largely unchanged across the different 
sets of relevance judgments. These two points 
are bases of Soboroff et al?s random pseudo-
relevance method, and give explanation to the 
result that their rankings were positively related 
to that of the actual TRECs. As a matter of fact, 
the two points are bases of all the retrieval 
evaluation methods without or with incomplete 
relevance judgments. 
Aslam and Savell (2003) devised a method to 
measure the relative retrieval effectiveness of 
systems through system similarity computation. 
In their work, the similarity between two re-
trieval systems was the ratio of the number of 
documents in their intersection and union. Each 
system was scored by the average similarity 
between it and all other systems. This measure-
ment produced results that were highly corre-
lated with the random pseudo-relevance method. 
Aslam and Savell hypothesized that this was 
caused by ?tyranny of the masses? effect, and 
these two related methods were assessing the 
systems based on ?popularity? instead of ?per-
formance?. The analysis by Spoerri (2005) sug-
1132
gested that the ?popularity? effect was caused by 
considering all the runs submitted by a retrieval 
system, instead of only selecting one run per 
system. Our later experimental results will show 
that this point of view is partially correct. The 
?popularity? effect could not be avoided com-
pletely by only selecting one run per system. 
This is indeed a hard problem for all the evalua-
tion methods without relevance judgments. 
Wu and Crestani (2003) developed multiple 
?reference count? based methods to rank re-
trieval systems. They made the distinction be-
tween an ?original? document and its duplicates 
in all other lists, called the ?reference? docu-
ments, when computing a document?s score. A 
system?s score is the (weighted) sum of the 
scores of its ?original? documents. Several ver-
sions of reference count method were carried 
out and tested. The basic method (Basic) scored 
each ?original? document by the number of its 
?reference? documents. The first variant (V1) 
assigned different weights to ?reference? docu-
ments based on their ranking positions. The 
second variant (V2) assigned different weights 
to the ?original? document based on its ranking 
position. The third variant (V3) assigned differ-
ent weights to both the ?original? documents and 
the ?reference? documents based on their rank-
ing positions. The fourth variant (V4) was simi-
lar to V3, except that it normalized the weights 
to ?reference? documents. Wu and Crestani?s 
method output similar evaluation performance 
to that of the random pseudo-relevance method. 
Their work also showed that the similarity be-
tween the multiple runs submitted by the same 
retrieval system affected the ranking process. If 
only one run was selected for any of the partici-
pant system for any query, for 3-9 systems, V3 
outperformed random pseudo-relevance method 
by 45.6%; for 10-15 systems, random pseudo-
relevance method outperformed V3 by 6.5%. 
Nuray and Can (2006) introduced a method 
to rank retrieval systems automatically using 
data fusion. Their method consists of two parts. 
One is selecting systems for data fusion, and the 
other is selecting documents as pseudo relevant 
documents as the fusion result. In the former 
part, they hypothesized that systems returning 
documents different from the majority could 
provide better discrimination among the docu-
ments and systems. In return, this could lead to 
a more accurate pseudo relevant documents and 
more accurate rankings. To find proper systems, 
they introduced the ?bias? concept for system 
selection. In their work, bias was 1 minus the 
similarity between a system and the majority, 
where the similarity is a normalized dot product 
of two vectors. In the latter part, Nuray and Can 
tested three criterions, namely Rank position, 
Borda count and Condorcet. Experimental re-
sults on data from TREC 3, 5, 6 and 7 showed 
that bias plus Condorcet got the best evaluation 
results and it outperformed the reference count 
method and random pseudo relevance method 
greatly. 
More recently, Spoerri (2007) proposed a 
method using the structure of overlap between 
search results to rank retrieval systems. This 
method provides us a new view on how to rank 
retrieval systems without relevance judgments. 
He used local statistics of retrieval results as 
indicators of relative effectiveness of retrieval 
systems. Concretely, if there are N systems to be 
ranked, N groups are constructed randomly with 
the constraint that each group contains five sys-
tems and each system will appear in five groups; 
then the percentages of a system?s documents 
not found by other systems (Single%) as well as 
the difference between the percentages of docu-
ments found by a single system and all five sys-
tems (Single%-AllFive%) are calculated as in-
dicators of relative effectiveness respectively. 
Spoerri found that these two local statistics were 
highly and negatively correlated with the mean 
average precision and precision at 1000 scores 
of the systems. By utilizing the two statistics to 
rank systems from subsets of TREC 3, 6, 7 and 
8, Spoerri obtained appealing evaluation results. 
The overlap structure of the top 50 documents 
were sufficient to rank retrieval systems and 
produced the best results, which outperformed 
previous attempts to rank retrieval systems 
without relevance judgments significantly. 
So far, we have reviewed 5 representatives of 
non-judgment evaluation methods. All these 
methods faced the same serious problem: simi-
lar runs harmed the effectiveness of ranking 
process. Different methods handled this prob-
lem differently. Aslam and Savell (2003) called 
this the ?tyranny of the masses? and provided no 
solution. Wu and Crestani (2003) addressed this 
problem by selecting only one run for any of the 
participant system for any query. Nuray and 
Can (2006) selected systems that were less simi-
1133
lar to the majority for data fusion. Spoerri (2007) 
performed his method on a selected subset of all 
the systems. All these treatments led to evalua-
tion performance improvement. Yet we will say 
it could be improved more. In the next section, 
we will present a new solution to this problem. 
Its performance is examined in Section 4. 
3 Using Clustering to Improve Re-
trieval Evaluation without Relevance 
Judgments
3.1 Problem
As we reviewed in Section 2, previous research 
had shown that incomplete relevance judgments 
and partially incorrect relevance judgments do 
not harm retrieval evaluation greatly. This is 
why pooling technique can lead to reliable 
retrieval evaluation results. It is also the 
theoretical foundation of evaluation without 
relevance judgments.
Besides, non-judgments methods armed with 
more laws inside retrieval results. These laws 
indicate the correlation between retrieval effec-
tiveness of a system and features in its retrieval 
results. One of the most important laws used in 
non-judgments evaluation is Authority Effect 
(Spoerri, 2005): document, which is retrieval by 
more systems, is more likely being relevant. 
Unfortunately, similar retrieval results ruined 
this law. Aslam and Savell (2003) called this the 
?tyranny of the masses?. So, how to alleviate the 
negative effect of similar retrieval results is a 
big issue in non-judgments evaluation.  
3.2 Solution
Generally, our solution to the ?tyranny of the 
masses? is removing similar systems by cluster-
ing. The whole process is as follows: 
Firstly, all systems to be evaluated are clus-
tered into several subsets. 
Secondly, for each subset, one system is se-
lected as a representative. 
Thirdly, all the information used for system 
evaluation comes from these representatives. 
Finally, score every system according to the 
information collected in the previous step.
This is the general framework of our method-
ology. Notice that, in the third step, only se-
lected systems contribute to the information 
required for system evaluation. So we can elimi-
nate the negative effect caused by similar re-
trieval results. 
This solution can be applied to any method of 
retrieval evaluation without relevance judg-
ments. To illustrate how to apply it to a retrieval 
evaluation method, we will describe using clus-
tering to improve Average System Similarity, 
which is proposed by Aslam and Savell (2003), 
in detail as an example. 
3.3 Average System Similarity Based on 
Clustering
In Aslam and Savell?s (2003) method, each sys-
tem is evaluated based on a criterion named Av-
erage System Similarity. The average system 
similarity of a given system S0 is calculated ac-
cording to formula (1). 
?
z
 
0
),(
1
1
)(AvgSysSim
0
0
SS
SSSysSim
n
S
(1)
where n is the number of systems to be evalu-
ated, and similarity between two systems S and 
S0, SysSim(S, S0), is calculated based on for-
mula (2). 
21
21
21 RetRet
RetRet
),(SysSim
?
?
 SS (2)
where Reti indicates the set of documents re-
turned by System i (i = 1, 2). 
When applying clustering technique to the 
system similarity method, we need to define an 
equivalence relation first. 
Definition 1 (System Equivalence): Suppose 
that all systems are clustered into m clusters 
namely C1, C2, ?, Cm. Two systems S1 and S2
are equivalent if and only if there exists k (1 ? k
? m) so that S1?Ck and S2?Ck.
kk CSCSmkk
iff
SS
??dd
 
21
21
,,1,
(3)
Given the definition of System Equivalence, 
we get the average system similarity based on 
clustering as follows: 
?
z
 
0
),(
1
1
)(AvgSysSim
0
0
SR
SRSysSim
m
S
(4)
where m is the number of clusters and R is the 
representative system of a cluster. 
1134
Replacing formula (1) with formula (4), we 
get the retrieval evaluation method Average 
System Similarity Based on Clustering, shortly 
ASSBC.
There are two important issues for ASSBC 
that need to be addressed. Issue 1: How to select 
representative system from a cluster? Issue 2: 
How to decide the number of clusters we need? 
Before we address Issue 1, we introduce an-
other definition, Cluster Similarity. 
Definition 2 (Cluster Similarity): for any 
given two clusters C1 and C2, with their respec-
tive representative systems S1 and S2, the cluster 
similarity between C1 and C2 is the system simi-
larity between S1 and S2.
),(SysSim),(ClusterSim 2121 SSCC  (5)
Now we come to selecting representative sys-
tems for clusters. Here, we utilize a hierarchical 
bottom up clustering technique. The entire clus-
tering process is as follows. 
Initially, each system forms a cluster.
Loop Until the number of clusters is m 
Two most similar clusters merge, and 
one of their representatives with higher 
average system similarity survives as 
the representative of the new cluster. 
End Loop. 
In the initial step, since every cluster contains 
only one system, the representative system is 
unquestionable. Within each loop, two represen-
tative systems of the old clusters are candidates 
of the new cluster, and the one with higher score, 
which means higher retrieval performance, be-
comes the representative of the new cluster. 
For Issue 2, technically, how to decide the 
number of clusters is always a problem for clus-
tering. Yet, we do not have to rush in the deci-
sion. Let us examine the evaluation perform-
ance on different values of m first. 
4 Experiments 
In this section, we will illustrate the evaluation 
performance of Average System Similarity 
Based on Clustering vs. different values of m.
Before we come to the experimental results, we 
would like to make some details clear first. 
4.1 Some Clarification 
4.1.1 Dataset
We perform our experiments on the ad hoc tasks 
of TREC-3, -5, -6 and -7. Most existing works 
on retrieval evaluation without judgments are 
tested on these tasks. To make a direct compari-
son with these work mentioned in Section 2 
later, we also choose these tasks as our test bed. 
4.1.2 Performance Measurement 
One of the measures of retrieval effectiveness 
used by TREC is mean non-interpolated average 
precision (MAP). Since average precision is 
based on much more information than other ef-
fectiveness measures such as R-precision or 
P(10) and known to be a more powerful and 
more stable effectiveness measure (Buckley and 
Voorhees, 2000), we utilize MAP as the effec-
tive measurement of retrieval systems in our 
experiments. 
The correlation of the ranking with our pro-
posed methods, as well as other methods, to the 
TREC official rankings is measured using the 
Spearman?s rank correlation coefficient. One 
reason is that it suits better for evaluating corre-
lation between ratio sequences, e.g. MAP, than 
Kendall?s tau. The other reason is that we can 
directly compare our results with those of pre-
vious attempts reviewed in Section 2, since 
most of them provided Spearman?s rank correla-
tion coefficient results. 
4.1.3 Substitute for Number of Clusters 
TREC Runs 
3 40 
5 61 
6 74 
7 103 
Table 1. Number of TREC runs 
As we know, the number of systems (runs) var-
ies in different TREC dataset (see Table 1 for 
details). Instead of examining the evaluation 
performance variation when absolute number of 
clusters m changes, we illustrate the evaluation 
performance vs. the percentage of m. Actually, 
for the sake of convenience, we will plot the 
correlation of our method to the TREC official 
rankings vs. the percentage of systems removed 
from the representative group in the following 
subsection.
1135
4.2 Experimental results 
Figure 1-4 show the plots of the correlation 
of our method to the TREC official rankings vs. 
the percentage of systems removed from the 
representative group on TREC-3, -5, -6 and -7 
respectively. The percentage of systems re-
moved goes from 0 to 85%, where 0 means no 
system removed and represents the original Av-
erage System Similarity method, and 85% is an 
up bound in our experiments. The horizontal 
line indicates the original performance. The 
tagged number on the curve says when the per-
formance curve reaches its peak and the peak 
value.
