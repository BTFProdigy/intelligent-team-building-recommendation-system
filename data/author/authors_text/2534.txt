Multi-level Similar Segment Matching Algorithm 
for Translation Memories and Example-Based Machine Translation 
Emmanuel PLANAS 
Cyber Solutions Laboratories 
2-4, Hikaridai Seika-cho Soraku-gun 
Kyoto, 619-0237 Japan 
planas @ soy.kecl.ntt.co.jp 
Osamu FURUSE 
Cyber Solutions Laboratories 
2-4, Hikaridai Seika-cho Soraku-gun 
Kyoto, 619-0237 Japan 
furuse@soy.kecl.ntt.co.jp 
Abstract 
We propose a dynamic programming 
algorithm for calculaing the similarity 
between two segmeuts of words of the same 
language. The similarity is considered as a 
vector whose coordinates refer to the levels 
of analysis of the segments. This algorithm 
is extremely efficient for retrieving the best 
example in Translation Memory systems. 
The calculus being constructive, it also gives 
the correspondences between the words of 
the two segments. This allows the extension 
of Translation Memory systems towards 
Example-based Machiue Translation. 
\]Introduction 
\[n Translation Memory (TM) or Example-Based 
lVlachine Translation (EBMT) systems, one of 
lhe decisive tasks is to retrieve from the database, 
the example that best approaches the input 
sentence. In Planas (1999) we proposed a two- 
step retriewd procedure, where a rapid and rough 
index-based search gives a short list of example 
candidates, and a refined matching selects the 
best candidates from this list. This procedure 
drastically improves the reusability rate of 
selected examples to 97% at worst, for our 
English-Japanese TM prototype; with the 
classical TM strategy, this rate would constantly 
decline with the number of non matched words. 
It also allows a better ecall rate when searching 
for very similar examples. 
We describe here the Multi-level Similar 
Seglnent Matching (MSSM) algorithm on which 
is based the second step of the above retrieval 
procedure. This algorithm does not only give the 
distance between the input and the example 
source segmeuts, but also indicates which words 
would inatch together. It uses F different levels 
of data (surface words, lemlnas, parts of speech 
(POS), etc.) in a combined and uniform way. 
The computation of the worst case requires 
F*m*(n-m+2) operations, where m and n are 
respectively the lengths of the input and the 
candidate (m<=n). This leads to a linear 
behavior when m and n have similar lengths, 
which is often the case for TM segmentsL 
Furthermore, because this algorithm gives the 
exact matching links (along with the level o1' 
match) between all of the words of the input and 
the candidate sentence, it prepares the transfer 
stage of an evolution of TM that we call Shallow 
Translation. This involves substituting in the 
corresponding translated candidate (stored in the 
melnory), the translation of the substituted 
words, provided that the input and the candidate 
are "similar enough". 
1 Matching Principle 
1.1 The TELA Structure 
The purpose of this algorithm is to match two 
segments of words: input i and candidate C. 
These can each be any sequence of words: 
phrases, sentences, or paragraphs, for example. 
Let us consider input I of length m, not as a 
single segment of surface words, but rather as a 
group of F parallel layered segments r \] 0 <:t<:v) 
each bearing m tokens. Such a structure is shown 
in Figure 1, and we call it a TELA structure z. On 
each layer f, the i-th token corresponds to one of 
the paradigms of the i-th word of input i. in our 
implementation, we use a shallow analyzer that 
gives three paradigms (F=3) for each surface 
I We use this algorithm on a sorted list of ah'cady 
similar sentences, retrieved with the help of an index. 
2 Tile idea o1' this structure is ah'eady in Lafourcade's 
LEAF (1993), and is explained in Planas (1998). 
621 
C 
C 1 
C 2 
C 3 PN 
CI C2 C3 
noun 
C4 
verb 
C 5 C6 C 7 
NqzF 
 a4ty 
PN adv 
stayed 
stay 
verb 
I ii I2 
I ~ gony stayed 
I 2 Sony stay 
13 PN verb 
Figure 1: Example of matching TELA structures 
word of the segments: the surface word itself 
(f=l), its lemma (f=2), and its POS tag (f:3). 
Because we do not need a syntactic analyzer, the 
time required for this analysis is not an handicap, 
moreover such parsers are available for lnany 
languages. Let C be a candidate segment of 
length n, for matching input I of length m 
(n>=m). The basic problem involves matching 
f the elements of the set (C i)f<:~:. ~ . .. to those of 
(I~)f<:~: j<=,n- Only three layers are shown in the 
following examples but other types of layers, 
like semantics, or even non linguistic 
information like layout features can be 
considered, as in Planas (1998). Our algorithm is 
written for the general case (F layers). 
1.2 Edit Distance based Similarity 
We consider a match from C to 1 as an edit 
distance process. This edition uses a sequence of 
basic edit operations between the words of the 
segments, like in Wagner & Fisher (1974) who 
used four basic operations: deletion, insertion, 
strict and equal substitution between the letters 
of a word. This approach as also been followed 
by Gale & Church (1993) for their alignment 
algorithm, with six operations. Here, we only 
consider deletions and equalities (i.e. equal 
substitutions): F+I basic operations in totaP. 
One equality corresponds toeach of the F layers, 
and a deletion affects all layers at once. In 
Figure 1, the items in bold match each other, and 
the strikethrough ones have to be deleted. The 
edition of C into I involves five deletions 
("Nikkei", "journal", "reported", that", "really"), 
one equality at layer 1 ("stayed"), two at layer 2 
3 Lepage (1998) also uses deletions and one level of 
equality lbr calculating his "pseudo-distance", for 
getting the similarity between two strings. 
Cs C9 
stm  Monaay 
strong Monday 
adj UOUll 
Matclfing zone 
I3 I4 
strong 
adj 
x ues ay 
uoun 
("stay", "strong"), and four at layer 3 ("PN", 
"verb", "adj", "noun"). At the Word level, the 
similarity between the two segments is 
considered to be the relative number of words of 
the input segment hat are matched by some 
word of the candidate segmeut in the matching 
zone (from "NTT" to "Monday" in our 
example): 1/4 in Figure 1. The same similarity 
can be considered at different levels. Here, the 
lemma similarity is 2/4, and the POS similarity 
is 4/4. We consider the total similarity as a 
vector involving all layer equalities, plus 
deletions: G(C, 1) = (1/4, 2/4, 4/4, 1-1/4, 1-5/9) 
The fourth coordinate counts the complementary 
proportion of deletions in the "matching zone" of 
the candidate C. The last coordinate counts the 
same proportion, relatively to the whole 
candidate. We take the complement to 1 because, 
the more deletions them am, the smaller the 
similarity becomes. 
When different Ci candidates are possible for 
matching I, the greatest (~(Cio, 1), according to 
common the partial order on vectors, determines 
the best candidate Cio. 
1.3 Matching Strategy 
1.3.1 Basics 
We try to match each word C a of candidate C, to 
a word Ij of input I. Ci inatches lj if one of the 
paradigms of C~ equals one of the paradigms of Ij 
at the same level f, i.e. if Cfi and I\], are equal. 
When a failure to match two words with their 
paradigms Cfi to i~ occurs at a given level f, we 
try to match the words at the next upper level 
f+l: Cmi and Ir+~j. When all of the possible 
layers of the two words have been tried without 
success, we try to match the next word C m to the 
same Ij. If Ci does not match any word of I at any 
622 
level, we consider that it has to be deleted. All 
words of I have to be matched by some word of 
C: no insertion is allowed (see section 1.3.4). 
1'.3.2 Lazy match 
With TM tools, if some useful candidates are 
found, they usually utilize words silnilar to the 
input words because translation memories arc 
applied within very similar documents, most of 
tile time between ancient and newer versions of 
a same document. When tile priority is rapidity 
(rather than non-ambiguity), we can consider 
that a lnatch is reached as soon as a word of C 
and a word of I match at a certain layer f. It is 
not necessary to look at upper levels, for they 
should lnatch because of tile expected similarity 
between tile input and tile candidate. Tile 
previous example illustrates this. As upper levels 
are not tested, this allows a gain in tile number 
of iterations ot' the algorithm. Experiments (see 
Planas (1999)) have confirmed this to be a 
correct strategy for TM. That's why, we consider 
from now on dealing with such a lazy match. 
1.3.3 Exhaustive match 
In the most general case, ambiguity problems 
prevent us fl'om employing the lazy strategy, and 
a correct inatch requires that whenever two items 
CJ~ and I f. match at a certain level f, they should 
J 
match at upper levels. Here is an example: 
221 Sony stay c-~4ed 
~2 2 Sony stay ended M~mday 
;23 PN ne+m ~?erb noun 
( ~ Sony ,~vyed sU~ngef T44esday 
\]\[ Sony stay strong Tuesday 
I PN ~?erb adj" noun 
Figure 2: Lemma ambiguity 
h3 C2, the lemma "stay" of surface word "stay" 
matches tile lemma "stay" of surl'ace word 
"stayed" of I, but they do not match at the POS 
level (noun and verb). Tile algorithm should go 
to this level to find that there is no match. Once 
again, however, because this algorithm has been 
built for TM systems, such alnbiguities hardly 
occur .  
1.3.4 Insertion 
If some items in I are not matched by any iteln 
of C, the match involves an insertion. 
Case of Translation Memories 
If tile candidate sentences are to be used by a 
hmnan translator, s/he will be able to insert the 
missing word at the right place. Accordingly, a 
match with insertion can be used for pure TM. 
Case of Shallow Translation (EBMT) 
in the EBMT system we are targeting, we plan 
to use tile matching sub-string of C for 
adaptation to I without syntactic rules. 
Accordingly, we consider that we do not know 
where to insert the non matching item: in this 
case, we force tile algorithm to stop if an 
insertion is needed for matching C and I. From 
now on, we will follow this position. 
1.3.5 "1)'ace 
We want the output of the algorithm as a list of 
triplets (Cri I f, op)~< i<-. called a "trace", where cri 
J - _ 
corresponds to P; through the "op" operation. We 
note op="f" an cquality at level f, and op="0" a 
deletion. For Example 1, the trace should be: 
(100) (200) (300) (400) (513)  (600) (72  
I) (832) (943) 
2 Adapting Wagner & Fischer, and 
Sellers algorithms 
2.1 Algorithm Principle 
The Wagner & Fischer (W&F) dynamic 
programming algorithm in Figure 3 gives tile 
edit distance between C and I: 
For j=0 to m 
d\[j, 0\]-i//initiating the cohmms 
For i=l to n 
dlO, i\]=i//initiating the rows 
For i= 1 to n 
Forj=l to m 
If(l\[j\]=C\[i\]) {d=d\[i-l, j-1\]}//equality 
Else {d=d\[i-1, j-1\]+l} //subst. 
d\[j,i\]=min(d\[i-1, j\]+l, d\[i, j-1\]+l, d) 
End For 
End For 
Print d\[n, m\] 
Figure 3: The Wagner & Fisher algorithm 
Tile distance is obtained in m*n operations, by 
building an \[re+l, n+l\] array (see Figure 6). Ill 
addition, W&F (1974) proposed a backtracking 
procedure, shown in Figure 4, that scans back 
this array to give a "trace" of the match between 
623 
C and 1 (i.e. it prints the position of the matching 
words), in (m+n) operations. The trace is then 
obtained in (mn+m+n) operations in total. This 
algorithm was previously used in Planas (1998) 
at each layer of a TELA structure to give a trace 
by layer. The data fi'om the traces of the 
different layers were combined afterwards for 
the purposes of TM and EBMT. However, this 
procedure is not optimal for at least two reasons. 
First, the layers are compared in an independent 
way, leading to a waste of time in the case of 
TM, because the lazy match phenomenon is not 
used. Second, the combination of the results was 
processed after the algorithm, and this required a
supplementary process. One can imagine that 
processing the whole data in the flow of the 
instructions of the algorithm is more efficient. 
i= i0; j = m; 
while (i > 0) and (j > 0) 
//del// i f (d \ [ i , j \ ]=d\ [ i - l , j \ ]+ l ){ i= i -1}  
//ins// else if(d\[i,j\]= d\[i, j- l\]+ 1) {j =j -1} 
else//equality orsubstitution 
print (i, j) 
i= i -1 ; j= j - I  
end if 
end while 
Figure 4: W&F backtracking algorithm 
2.2 Two operation based mininf izat ion 
If we look back at the W&F algorithm, shown in 
Figure 3, the part in bold represents the code 
involved in the calculus of the next local 
distance d\[i, j\]. It testes which of the four basic 
edit operations (deletion, insertion, equal or 
strict substitution) gives the lowest partial 
distance. Nevertheless, we have shown in 
section 1.3.4 that only deletions and equalities 
I i 111 112 113 
First press the 
Cli 0 inf inf inf 
C1~ First 0 0 inf inf 
C12 press 0 1 0 inf 
C1.~ only 0 2 1 inf 
C14 the 0 3 2 1 
Cl.s red 0 4 3 2 
C~6 button 0 5 4 3 
do interest us. We therefore reduce the test in the 
algorithm to that shown in Figure 5. Furthermore, 
we initiate the columns of the array with infinite 
values (huge values in practice) to show that 
initial insertions are not possible, and the rows to 
"0", to count the deletions relatively to iuput I. 
See Sellers (1980) for a due explanation. 
If(I\[j\]=C\[i\]) {d=d\[i-l, j-I \]}//equal: no cost 
Else {d=inf}//big integer, in theory h~finite 
d\[j,i\] = rain (d\[i-1, j\]+l, d)//deletion or equal ? 
Figure 5: Matching with deletions and equalities 
An example of the successive scores calculated 
with this algorithm are shown in Figure 6. The 
total distance (equal to 1) between C and I 
appears in the lowest right cell. 
The fact that only two operations are used 
eradicates the ambiguity that appears in 
selecting the next cell in the W&F algorithm 
backtracking procedure with four operations. In 
our algorithm, either there is an equality (cost 0), 
or a deletion (cost 1). The possibility of having 
the same cost 1 for insertions, deletions, or strict 
substitutions has been eliminated. 
2.3 In t roduc ing  one equal i ty per  level 
As mentioned previously, we need to match 
items at different layers. We introduce here two 
new points to deal with this: 
? In order to keep the score for each equality 
deletion, d\[i,j\] is a vector instead of a 
number: d\[i,j\]=\[scorel ..... scorer, score\]. 
? In this vector, score~ through scor% store 
the number of equalities for each layer f, 
and score records the number of deletions, 
as in W&F (underlined in the arrays). 
114 
red 
inf 
inf 
inf 
inf 
inf 
115 
button 
inf 
inf 
inf 
inf 
inf 
D\[i-1, j- l\] 
1 inf 
2 W 
Figure 6: Successive scores produced by the adapted W&F algorithm 
D\[i-1, j\] 
deletion 
Ci-gs 
D\[i, j\] 
624 
0 
CI 
G 
G 
c~ 
G 
G 
C7 
G 
lti 
i / 
12 
C l C 2 CS/l s 
word lem POS 
Sony Sony PN 
say 
that 
leported 
that 
0 Ii L Is 
word Sony stays strong 
lem Sony stay strong 
POS PN verb adj 
9000 oooN 9oo,,j ooo,,j 
9000 100_0 300imf O00inf 
9000 100_1 101_0 000inl" 
9000 1002 101_1 000inf 
9000 0010 1012 000inf 
90OO 001_1 IH1-0 000inl_" 
9000 001_2 311_1 021_0 
9000 001_0 3112 021_1 
9000 001-1 3113 021_2 
verb 
coati 
NTT NTT PN 
stayed stay Verb 
strong 
Tuesday 
stronger 
Tuesday 
morning morning 
Adj 
PN 
nouu 
I4 
Tuesday 
Tuesday 
PN 
900,,j 
300in__f 
300inf 
300inf 
300in__f 
300inf 
300in__ f 
1210 
1.211 
Figure 7: Introducing a vector of deletion and layer equalities cores 
Figure 7 shows an example of diflbrent score 
vectors involved in a match. To calculate the 
successive d\[id\], we use tile algorithm of Figure 
5 adapted for F levels in Figure 8. 
If(Ir\[j\]=cf\[i\]) 
d~=\[d' \[i- l ,j- 1 \] .. . . .  dV\[i - l ,j-l 1+ 1 ,d--\[i- 1j-  l \] \] 
Else 
d~=\[0 ..... 0,inf.I 
End \]1" 
dd=\[d' \[i-1 ,j\] . . . . .  df\[i - I d\] . . . .  dV\[i - 1 d \], d--\[i-1 ,j \]+1\] 
d\[j,i\] = max (d~, d,i) //equali O, or deletion 
Figure 8: Adapting the algorithm to F levels 
We first try to get the maxinmm nmnber of 
equalities and then tile mininmm of deletions. 
Each tilne we find a new match in the first 
colunm, we start a new path (see I ~ matching 
with C I, C 4 and C 7 in Figure 7). It' one of the 
vectors of the last column of tile array is such 
that: SUMk=r<=v (scorer) = In ,  there is a matching 
substring of C in which there is a matching word 
for each of the words of I: this constitutes a
solution. In our example, cell (7, 4), with score 
121__0 shows that there is a sub chain of the 
candidate that matches tile input with 1, 2, and 1 
matches at the word, lemma, and POS levels and 
0 deletions. Cell (8, 4) indicates a similar naatch, 
but with 1 deletion Cmorning"). Tile best path 
then ends at cell (7,4). Starting from this cell, we 
can retrieve tile full solution using the W&F 
backtrack algorithm adapted to F levels. 
This approach allows us to choose as compact a
string as possible. When there are several 
possible paths, like in Figure 9, the algorithm is 
able to choose the best matching sub-string. If 
we are looking for a similarity involving first 
11 
C 2 C-71 s 
lem POS 
Sony PN 
say verb 
strong adj 
Tuesday PN 
and zonj 
NTT PN 
stay Verb 
strong Adj 
Tuesday PN 
morning noun 
Cri C l 
0 word 
C~ Sony 
C2 stayed 
C3 stronger 
C4 Tuesday 
C s and 
C6 NTT 
C7 stayed 
Cs stronger 
C9 Tuesday 
CIo morning 
0 l j I2 I3 
word Sony stays strong 
lem Sony stay strong 
POS PN verb adj 
oooo ooo#_vf 9oo~ ooo~ 
0000 1000 300inl__" O00inf 
0000 1001 110-0 000in__f 
0000 100_2 1101 120_0 
0000 100_Q 102 1201 
0000 1001 110._3 1202 
0000 001_0 1104 1203 
0000 0011 Dll-0 1204 
0000 0012 311-1 021_0 
0000 001_0 3112 021_1 
0000 001_1 3113 0212 
Figure 9: Selecting the best concurrent sub segment 
\[,1 
Tuesday 
Tuesday 
PN 
)00/~r 
)00in__f 
)00int" 
)00in_f 
Z200 
Z20! 
2202 
2203 
2204 
1210 
121! 
625 
surface word matches, then lemmas and parts of 
speech, then cell (4,4) of score 2200 will be 
chosen. This strategy can be adapted to 
particular needs: it suffices to change the order 
of the scores in the vectors. 
3 Optimizing 
3.1 Triangularization of the array 
In this algorithm, for each Ij, there must be at 
least one possible matching C~. Hence, in a valid 
path, there are at least m matches. As a match 
between C~ and Ij occurs when "stepping across a 
diagonal", the (m-l) first diagonals (from the 
lower left corner of the array) can not give birth 
to a valid path. Therefore, we do not calculate 
d\[i,j\] across these small diagonals. 
Symmetrically, the small diagonals after the last 
full one (in the upper right corner) cannot give 
birth to a valid path. We then also eliminate 
these (m-l) last diagonals. This gives a reduced 
matrix as shown in the new example in Figure 
10. The computed cells are then situated in a 
parallelogram of dimensions (n-m+l) and m. 
The results is: only m(n-m+l) cells have to be 
computed. Instead of initiating the first row 0 to 
"inf", we initiate the cells of the diagonal just 
before the last full top diagonal (between cell 
(0,1) and cell (3,4)in Figure 10) to "000inf" to 
be sure that no insertion is possible. 
3.2. Complexity 
The worst time complexity of this algorithm is 
F-proportional to the number of cells in the 
computed array, which is ln*(n-m+l). With the 
"lazy" strategy, all F levels are often not visited. 
As the number of cells computed by the W&F 
algorithm is m'n, our algorithm is always more 
rapid. The backtracking algorithm takes m+n 
operations in the W&F algorithm, as well as in 
our algorithm, leading to m(n-m+2)+n 
operations in the MSSM algorithm, and 
m(n+l)+n operations in the W&F algorithm. 
The general complexity is then sub-quadratic. 
When the lengths of both segments to be 
compared are similar (like it often happens in 
TMs), the complexity tends towards linearity. 
The two graphics in Figure 11 show two 
interesting particular cases (ln=n and m running 
from 1 to n=10), comparing W&F and our 
algorithm. For strings of similar lengths, the 
longer they are, the more the MSSM algorithm 
becomes interesting. When n is fixed, the 
MSSM algorithm is more interesting for extreme 
values of the length of I: small and similar to n. 
Conclusions 
The first contribution of this algorithm is to 
provide TM and EBMT systems with a precise 
and quick way to compare segments of words 
with a similarity vector. This leads to an ahnost 
complete radication of noise for the matter of 
retrieving similar sentences in TM systems (97% 
"reusability" in our prototype). The second is to 
offer an unambiguous word to word matching 
through the "trace". This last point opens the 
way to the Shallow Translation paradigm. 
ca 
0 
C, 
C2 
C3 
C4 
Cs 
C6 
C7 
Cs 
i / 
F 
C 1 C 2 C3/I 3 
woM !era POS 
~ony 
reported 
that 
NTT 
stayed 
stronger 
I'uesday 
morning 
Sony 
~ay 
Lhat 
NTT 
~tay 
~trong 
?uesday 
morning 
PN 
verb 
conj 
PN 
Verb 
Adj 
P noun 
n o u n 
0 11 12 13 14 
word Sony stays strong Tuesday 
lem Sony stay strong Tuesday 
POS PN verb adj PN 
0000 900i~ 
0000 1000 O00'mf 
0000 1001 101_0 900/nf 
0000 1002 1011 300inf 000~ 
0000 I)01_0 1012 300inf" 000inf. 
0000 9011 011_0 )00in_f 000inf" 
0000 0111 D210 000int" 
0000 3211 121_0 
0000 1211 
Figure 10: Eliminating left and Hght small diagonals 
626 
140 
120 
100 
o 80 
60 
40 
20 
0 
Comparison W&F / MSSM 
1 2 3 4 5 6 7 8 9 10 
n=m 
+W&F 
--t1~-- MSSM 
-'--r~---W&F + Bk 
I : MSSM + Bk 
t40 
120 
100 
80 
60 
40 
20 
0 
Comparison W&F / MSSM 
~J*7 * 
2 3 4 5 6 7 8 9 10 
m (n=l O) 
-~ I ' - -  W& F 
- I~- - -  MSSM 
- /~- -  W&F 4. Bk 
: MSSM 4. Bk 
Figure 11: Comparing the Wagner & Fisher and 
MSSM algorithms 
For more information about the use of this 
algorithm, please refer to Planas (1999). These 
two contributions bring in the main difference 
with relative research 4 concentrating on 
similarity only, represented by a sole integer. 
The TELA structure, that allows the parallel use 
of different layers of analysis (linguistic 
paradigms, but possibly non linguistic 
information) is essential to this work because it 
provides the algorithm with the supplementary 
information classical systems lack. 
The fact that the shallow parser (lemmas, POS) 
is ambiguous or not does not affect significantly 
the performance of the algorithln. If the same 
parser is used for both example and input 
segments, parallel errors compensate each other. 
Of course, these errors do have an influence for 
EBMT: the non ambiguity is then a must. 
A first evaluation of the MSSM speed gives 0.5 
to 2 milliseconds for comparing only s two 
randomly chosen English or Japanese sentences 
over 3 levels (word, lemmas, POS). The 
4 Cranias et al (1997), Thompson & Brew (1994), or 
in a more specific way, Lcpage (1998) 
5 Without he shallow analysis 
implementation has been done with a DELL 
Optiplex GX 1 233 Mhz, Window NT, Java 1 18. 
This algorithm can be improved in different 
ways. For speed, we can introduce a similarity 
threshold so as not to evaluate the last cells of 
the columns of the computed array as soon as the 
threshold is overtaken. For adaptability, being 
able to deal with a different number of tokens 
according to each layer will allow us to deal 
nicely with compound words. 
In short, if the basis of this matching algorithm 
is the W&F algorithm, other algorithms can be 
adapted similarly to deal with multi-level data. 
Acknowledgements 
Thanks to Takayuki Adachi, Francis Bond, 
Timothy Balwin, and Christian Boitet for their 
useful remarks and fruitful discussions. 
References 
Cranias, L., Papageorgiou, H., & Pipcridis, S. (1997) 
Example retrieval .fron~ a 7)zmslation Memory. 
Natural Language Engineering 3(4), Cambridge 
University Press, pp. 255-277. 
Gale, W.A. & Church, K.W. (1993) A program .for 
Aligning Sentences in Bilingttal Corpora. Compu- 
lational Linguistics, ACL, Vol. 19, No. 1. 
Lafourcade M. (1993) LEAF, ou comment garder 
l'Originalitd de l'ambiguitd. Aclualitd Scienlifiquc - 
Troisi~mes Journdes Scientifiques Traduclique-TA- 
TAO, Montrdal, Canada, AUPELF-UREF, Vol. 1/1, 
pp. 165-185. 
Lepage Y. (1998) Solving amtlogies on words: cm 
algorithm. Coling-ACL'98, Vol. I, pp. 728-734. 
Sellers, P.H. (1980) The theory and computation of 
evolutionmy distances: pauenl recognition. Jour- 
nal o1' Algorithms, Vol. 127, pp. 359-373. 
Thompson Henry S. & Brew Chris (1996) Automatic 
Evaluation of Computer Generated text: Final 
Report on the TextEval Project. Human 
Communication Research Center, University of 
Edinburg. 
Wagner, A. R. & Fischer M. (1974) 7he String-to- 
String Con'ection Problem. Journal of the ACM, 
Vol. 21, #1, pp. 168-173. 
Planas, E. (1998) TELA: Structures and Algorithms 
for Memory-Based Machine 7)'anslation. Ph.D. 
thesis, University Joseph Fourier, Grenoble. 
Planas, E. & Furuse O. (1999) Fom~alizing 
Translation Memories. Machine Translation 
St, remit VII, Singapore, pp. 331-339 
627 
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 39?46,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Searching for Sentences Expressing Opinions 
by using Declaratively Subjective Clues 
 
 
 Nobuaki Hiroshima, Setsuo Yamada, Osamu Furuse and Ryoji Kataoka 
NTT Cyber Solutions Laboratories, NTT Corporation 
1-1 Hikari-no-oka Yokosuka-Shi Kanagawa, 239-0847 Japan 
hiroshima.nobuaki@lab.ntt.co.jp 
 
  
 
Abstract 
This paper presents a method for search-
ing the web for sentences expressing 
opinions. To retrieve an appropriate 
number of opinions that users may want 
to read, declaratively subjective clues are 
used to judge whether a sentence ex-
presses an opinion. We collected declara-
tively subjective clues in opinion-
expressing sentences from Japanese web 
pages retrieved with opinion search que-
ries. These clues were expanded with the 
semantic categories of the words in the 
sentences and were used as feature pa-
rameters in a Support Vector Machine to 
classify the sentences. Our experimental 
results using retrieved web pages on 
various topics showed that the opinion 
expressing sentences identified by the 
proposed method are congruent with sen-
tences judged by humans to express 
opinions. 
1 Introduction 
Readers have an increasing number of opportu-
nities to read opinions (personal ideas or beliefs), 
feelings (mental states), and sentiments (positive 
or negative judgments) that have been written or 
posted on web pages such as review sites, per-
sonal web sites, blogs, and BBSes. Such subjec-
tive information on the web can often be a useful 
basis for finding out what people think about a 
particular topic or making a decision. 
A number of studies on automatically extract-
ing and analyzing product reviews or reputations 
on the web have been conducted (Dave et al, 
2003; Morinaga et al, 2002; Nasukawa and Yi, 
2003; Tateishi et al, 2004; Kobayashi et al, 
2004). These studies focus on using sentiment 
analysis to extract positive or negative informa-
tion about a particular product. Different kinds 
of subjective information, such as neutral opin-
ions, requests, and judgments, which are not ex-
plicitly associated with positive/negative as-
sessments, have not often been considered in 
previous work. Although sentiments provide 
useful information, opinion-expressing sentences 
like ?In my opinion this product should be 
priced around $15,? which do not express ex-
plicitly positive or negative judgments (unlike 
sentiments) can also be informative for a user 
who wants to know others? opinions about a 
product. When a user wants to collect opinions 
about an event, project, or social phenomenon, 
requests and judgments can be useful as well as 
sentiments. With open-domain topics, sentences 
expressing sentiments should not be searched 
exclusively; other kinds of opinion expressing 
sentences should be searched as well. 
The goal of our research is to achieve a web 
search engine that locates opinion-expressing 
sentences about open-domain topics on products, 
persons, events, projects, and social phenomena. 
Sentence-level subjectivity/objectivity classifica-
tion in some of the previous research (Riloff and 
Wiebe, 2003; Wiebe and Riloff, 2005) can iden-
tify subjective statements that include specula-
tion in addition to positive/negative evaluations. 
In these efforts, the subjectivity/objectivity of a 
current sentence is judged based on the existence 
of subjective/objective clues in both the sentence 
itself and the neighboring sentences. The subjec-
tive clues, some adjective, some noun, and some 
verb phrases, as well as other collocations, are 
learned from corpora (Wiebe, 2000; Wiebe et al, 
2001). Some of the clues express subjective 
meaning unrestricted to positive/negative meas-
urements. The sentence-level subjectivity ap-
39
proach suggests a way of searching for opinion 
expressing sentences in the open domain.  
The problem of applying sentence-level sub-
jectivity classification to opinion-expressing sen-
tence searches is the likelihood of collecting too 
many sentences for a user to read. According to 
the work of Wiebe et al (2001), 70% of sen-
tences in opinion-expressing articles like editori-
als and 44% of sentences in non-opinion ex-
pressing articles like news reports were judged 
to be subjective. In analyzing opinions (Cardie 
et al, 2003; Wilson et al, 2004), judging docu-
ment-level subjectivity (Pang et al, 2002; Tur-
ney, 2002), and answering opinion questions 
(Cardie et al, 2003; Yu and Hatzivassiloglou, 
2003), the output of a sentence-level subjectivity 
classification can be used without modification. 
However, in searching opinion-expressing sen-
tences, it is necessary to designate criteria for 
opinion-expressing sentences that limit the num-
ber of retrieved sentences so that a user can sur-
vey them without difficulty. While it is difficult 
to formally define an opinion, it is possible to 
practically tailor the definition of an opinion to 
the purpose of the application (Kim and Hovy, 
2004).  
This study introduces the notion of declara-
tively subjective clues as a criterion for judging 
whether a sentence expresses an opinion and 
proposes a method for finding opinion-
expressing sentences that uses these clues. De-
claratively subjective clues such as the subjec-
tive predicate part of the main clause and subjec-
tive sentential adverb phrases suggest that the 
writer is the source of the opinion. We hypothe-
size that a user of such an ?opinion-expressing 
sentence? search wants to read the writer?s opin-
ions and that explicitly stated opinions are pre-
ferred over quoted or implicational opinions. We 
suppose that writer?s ideas or beliefs are explic-
itly declared in a sentence with declaratively 
subjective clues whereas sentences without de-
claratively subjective clues mainly describe 
things. The number of sentences with declara-
tively subjective clues is estimated to be less 
than the number of subjective sentences defined 
in the previous work. We expect that the opinion 
expressing sentences identified with our method 
will be appropriate from the both qualitative and 
quantitative viewpoints. 
Section 2 describes declaratively subjective 
clues and explains how we collected them from 
opinion-expressing sentences on Japanese web 
pages retrieved with opinion search queries. Sec-
tion 3 explains our strategy for searching opin-
ion-expressing sentences by using declaratively 
subjective clues. Section 4 evaluates the pro-
posed method and shows how the opinion-
expressing sentences found by the proposed 
method are congruent with the sentences judged 
by humans to be opinions. 
2 Declaratively Subjective Clues 
Declaratively subjective clues are a basic crite-
rion for judging whether a sentence expresses an 
opinion. We extracted the declaratively subjec-
tive clues from Japanese sentences that evalua-
tors judged to be opinions. 
2.1 Opinion-expressing Sentence Judgment 
We regard a sentence to be ?opinion expressing? 
if it explicitly declares the writer?s idea or belief 
at a sentence level. We define as a ?declaratively 
subjective clue?, the part of a sentence that con-
tributes to explicitly conveying the writer?s idea 
or belief in the opinion-expressing sentence. For 
example, "I am glad" in the sentence "I am glad 
to see you" can convey the writer?s pleasure to a 
reader, so we regard the sentence as an ?opinion-
expressing sentence? and ?I am glad? as a ?de-
claratively subjective clue.? Another example of 
a declaratively subjective clue is the exclamation 
mark in the sentence "We got a contract!" It con-
veys the writer?s emotion about the event to a 
reader. 
If a sentence only describes something ab-
stract or concrete even though it has word-level 
or phrase-level subjective parts, we do not con-
sider it to be opinion expressing. On the other 
hand, some word-level or phrase-level subjective 
parts can be declaratively subjective clues de-
pending on where they occur in the sentence. 
Consider the following two sentences. 
 
(1) This house is beautiful. 
(2) We purchased a beautiful house. 
 
Both (1) and (2) contain the word-level subjec-
tive part "beautiful". Our criterion would lead us 
to say that sentence (1) is an opinion, because 
"beautiful" is placed in the predicate part and (1) 
is considered to declare the writer?s evaluation 
of the house to a reader. This is why ?beautiful? 
in (1) is eligible as a declaratively subjective 
clue. On the other hand, sentence (2) is not 
judged to contain an opinion, because "beauti-
ful" is placed in the noun phrase, i.e., the object 
of the verb ?purchase,? and (2) is considered to 
report the event of the house purchase rather ob-
40
jectively to a reader. Sentence (2) partially con-
tains subjective information about the beauty of 
the house; however this information is unlikely 
to be what a writer wants to emphasize. Thus, 
"beautiful" in (2) does not work as a declara-
tively subjective clue. 
These two sentences illustrate the fact that the 
presence of a subjective word (?beautiful?) does 
not unconditionally assure that the sentence ex-
presses an opinion. Additionally, these examples 
do suggest that sentences containing an opinion 
can be judged depending on where such word-
level or phrase-level subjective parts as evalua-
tive adjectives are placed in the predicate part. 
Some word-level or phrase-level subjective 
parts such as subjective sentential adverbs can be 
declaratively subjective clues depending on 
where they occur in the sentence. In sentence (3), 
?amazingly? expresses the writer?s feeling about 
the event. Sentence (3) is judged to contain an 
opinion because there is a subjective sentential 
adverb in its main clause. 
 
(3) Amazingly, few people came to my party. 
 
The existence of some idiomatic collocations 
in the main clause also affects our judgment as 
to what constitutes an opinion-expressing sen-
tence. For example, sentence (4) can be judged 
as expressing an opinion because it includes ?my 
wish is?. 
 
(4) My wish is to go abroad. 
 
Thus, depending on the type of declaratively 
subjective clue, it is necessary to consider where 
the expression is placed in the sentence to judge 
whether the sentence is an opinion. 
 
2.2 Clue Expression Collection 
We collected declaratively subjective clues in 
opinion-expressing sentences from Japanese web 
pages. Figure 1 illustrates the flow of collection 
of eligible expressions. 
 
type query?s topic 
Product cell phone, car, beer, cosmetic 
Entertainment sports, movie, game, animation 
Facility  museum, zoo, hotel, shop 
Politics diplomacy, election 
Phenomena diction, social behavior 
Event firework, festival 
Culture artwork, book, music 
Organization company 
Food cuisine, noodle, ice cream 
Creature bird 
Table 1: Topic Examples 
 
First, we retrieved Japanese web pages from 
forty queries covering a wide range of topics 
such as products, entertainment, facilities, and 
phenomena, as shown in Table 1. We used que-
ries on various topics because we wanted to ac-
quire declaratively subjective clues for open-
domain opinion web searches. Most of the que-
ries contain proper nouns. These queries corre-
spond to possible situations in which a user 
wants to retrieve opinions from web pages about 
a particular topic, such as ?Cell phone X,? ?Y 
museum,? and ?Football coach Z?s ability?, 
where X, Y, and Z are proper nouns. 
Next, opinion-expressing sentences were ex-
tracted from the top twenty retrieved web pages 
in each query, 800 pages in total. There were 
75,575 sentences in these pages.  
Figure 1: Flow of Clue Expression Collection 
41
 Three evaluators judged whether each sen-
tence contained an opinion or not. The 13,363 
sentences judged to do so by all three evaluators 
were very likely to be opinion expressing. The 
number of sentences which three evaluators 
agreed on as non-opinion expressing was 
42,346.1 Out of the 13,363 opinion expressing 
sentences, 8,425 were then used to extract de-
claratively subjective clues and learn positive 
examples in a Support Vector Machine (SVM), 
and 4,938 were used to assess the performance 
of opinion expressing sentence search (Section 
4). Out of the 42,346 non-opinion sentences, 
26,340 were used to learn negative examples, 
and 16,006 were used to assess, keeping the 
number ratio of the positive and negative exam-
ple sentences in learning and assessing. 
One analyst extracted declaratively subjective 
clues from 8,425 of the 13,363 opinion-
expressing sentences, and another analyst 
checked the result. The number of declaratively 
                                                 
1 Note that not all of these opinion-expressing sentences 
retrieved were closely related to the query because some of 
the pages described miscellaneous topics.  
subjective clues obtained was 2,936. These clues 
were classified into fourteen types as shown in 
Table 2, where the underlined expressions in 
example sentences are extracted as declaratively 
subjective clues. The example sentences in Table 
2 are Japanese opinion-expressing sentences and 
their English translations. Although some Eng-
lish counterparts of Japanese clue expressions 
might not be cogent because of the characteristic 
difference between Japanese and English, the 
clue types are likely to be language-independent. 
We can see that various types of expressions 
compose opinion-expressing sentences. 
As mentioned in Section 2.1, it is important to 
check where a declaratively subjective clue ap-
pears in the sentence in order to apply our crite-
rion of whether the sentence is an opinion or not. 
The clues in the types other than (b), (c) and (l) 
usually appear in the predicate part of a main 
clause.  
The declaratively subjective clues in Japanese 
examples are placed in the rear parts of sen-
tences except in types (b), (c) and (l). This re-
flects the heuristic rule that Japanese predicate 
 type example sentence (English translation of Japanese sentence) 
(a) Thought Kono hon wa kare no dato omou. 
(I think this book is his.) 
(b) Declarative adverb Tabun rainen yooroppa ni iku. 
(I will possibly go to Europe next year.) 
(c) Interjection Waa, suteki. 
(Oh, wonderful.) 
(d) Intensifier Karera wa totemo jouzu ni asonda. 
(They played extremely well) 
(e) Impression Kono yougo wa yayakoshii. 
(This terminology is confusing.) 
(f) Emotion Oai dekite ureshii desu. 
(I am glad to see you.) 
(g) Positive/negative judgment Anata no oodio kiki wa sugoi. 
(Your audio system is terrific.) 
(h) Modality about propositional attitude Sono eiga wo miru beki da. 
(You should go to the movie.) 
(i) Value judgment Kono bun wa imi fumei da. 
(This sentence makes no sense.) 
(j) Utterance-specific sentence form Towa ittemo,ima wa tada no yume dakedo. 
(Though, it's literally just a dream now.) 
(k) Symbol Keiyaku wo tottazo! 
(We got a contract!)
(l) Idiomatic collocation Ii nikui. 
(It's hard to say.) 
(m) Uncertainty Ohiru ni nani wo tabeyou kanaa. 
(I am wondering what I should eat for lunch.) 
(n) Imperative Saizen wo tukushi nasai. 
(Give it your best.) 
Table 2: Clue Types
42
parts are in principle placed in the rear part of a 
sentence. 
 
3 Opinion-Sentence Extraction 
In this section, we explain the method of classi-
fying each sentence by using declaratively sub-
jective clues. 
The simplest method for automatically judging 
whether a sentence is an opinion is a rule-based 
one that extracts sentences that include declara-
tively subjective clues. However, as mentioned 
in Section 2, the existence of declaratively sub-
jective clues does not assure that the sentence 
expresses an opinion. It is a daunting task to 
write rules that describe how each declaratively 
subjective clue should appear in an opinion-
expressing sentence. A more serious problem is 
that an insufficient collection of declaratively 
subjective clues will lead to poor extraction per-
formance. 
For that reason, we adopted a learning method 
that binarily classifies sentences by using de-
claratively subjective clues and their positions in 
sentences as feature parameters of an SVM. 
With this method, a consistent framework of 
classification can be maintained even if we add 
new declaratively subjective clues, and it is pos-
sible that we can extract the opinion-expressing 
sentences which have unknown declaratively 
subjective clues. 
3.1 Augmentation by Semantic Categories 
Before we can use declaratively subjective clues 
as feature parameters, we must address two is-
sues: 
? Cost of building a corpus:  It is costly 
to provide a sufficient amount of tagged 
corpus of opinion-expressing-sentence la-
bels to ensure that learning achieves a 
high-performance extraction capability. 
? Coverage of words co-occurring with 
declaratively subjective clues:  Many of 
the declaratively subjective clue expres-
sions have co-occurring words in the 
opinion-expressing sentence. Consider the 
following two sentences. 
(5) The sky is high. 
(6) The quality of this product is high. 
 
Both (5) and (6) contain the word "high" 
in the predicate part. Sentence (5) is con-
sidered to be less of an opinion than (6) 
because an evaluator might judge (5) to be 
the objective truth, while all evaluators are 
likely to judge (6) to be an opinion. The 
adjective "high" in the predicate part can 
be validated as a declaratively subjective 
clue depending on co-occurring words. 
However, it is not realistic to provide all 
possible co-occurring words with each 
declaratively subjective clue expression. 
Semantic categories can be of help in dealing 
with the above two issues. Declaratively subjec-
tive clue expressions can be augmented by se-
mantic categories of the words in the expressions. 
An augmentation involving both declaratively 
subjective clues and co-occurrences will increase 
feature parameters. In our implementation, we 
adopted the semantic categories proposed by 
Ikehara et al (1997). Utilization of semantic 
categories has another effect: it improves the 
extraction performance. Consider the following 
two sentence patterns: 
 
(7) X is beautiful. 
(8) X is pretty. 
 
The words "beautiful" and "pretty" are adjec-
tives in the common semantic category, "appear-
ance", and the degree of declarative subjectivity 
of these sentences is almost the same regardless 
of what X is. Therefore, even if "beautiful" is 
learned as a declaratively subjective clue but 
"pretty" is not, the semantic category "appear-
ance" that the learned word "beautiful" belongs 
to, enables (8) to be judged opinion expressing 
as well as (7). 
3.2 Feature Parameters to Learn 
We implemented our opinion-sentence extrac-
tion method by using a Support Vector Machine 
(SVM) because an SVM can efficiently learn the 
model for classifying sentences into opinion-
expressing and non-opinion expressing, based on 
the combinations of multiple feature parameters. 
The following are the crucial feature parameters 
of our method. 
? 2,936 declaratively subjective clues 
? 2,715 semantic categories that words in 
a sentence can fall into 
If the sentence has a declaratively subjective 
clue of type (b), (c) or (l) in Table 2, the feature 
parameter about the clue is assigned a value of 1; 
if not, it is assigned 0. If the sentence has de-
claratively subjective clues belonging to types 
43
other than (b), (c) or (l) in the predicate part, the 
feature parameter about the clue is assigned 1; if 
not, it is assigned 0. 
The feature parameters for the semantic cate-
gory are used to compensate for the insufficient 
amount of declaratively subjective clues pro-
vided and to consider co-occurring words with 
clue expressions in the opinion-expressing sen-
tences, as mentioned in Section 3.1. 
The following are additional feature parame-
ters. 
? 150 frequent words 
? 13 parts of speech 
Each feature parameter is assigned a value of 1 if 
the sentence has any of the frequent words or 
parts of speech. We added these feature parame-
ters based on the hypotheses that some frequent 
words in Japanese have the function of changing 
the degree of declarative subjectivity, and that 
the existence of such parts of speech as adjec-
tives and adverbs possibly influences the de-
clarative subjectivity. The effectiveness of these 
additional feature parameters was confirmed in 
our preliminary experiment. 
4 Experiments 
We conducted three experiments to assess the 
validity of the proposed method: comparison 
with baseline methods, effectiveness of position 
information in SVM feature parameters, and ef-
fectiveness of SVM feature parameters such as 
declaratively subjective clues and semantic cate-
gories. 
All experiments were performed using the 
Japanese sentences described in Section 2.1.  We 
used 8,425 opinion expressing sentences, which 
were used to collect declaratively subjective 
clues as a training set, and used 4,938 opinion-
expressing sentences as a test set. We also used 
26,340 non-opinion sentences as a training set 
and used 16,006 non-opinion sentences as a test 
set. The test set was divided into ten equal sub-
sets. The experiments were evaluated with the 
following measures following the variable 
scheme in Table 3: 
ba
a
Pop +=   ca
a
Rop +=  
opop
opop
op RP
RP
F +=
2
 
dc
d
P opno +=_  db
d
R opno +=_  
opnoopno
opnoopno
opno RP
RP
F
__
__
_
2
+=  
 
dcba
da
A +++
+=  
 
We evaluated ten subsets with the above 
measures and took the average of these results. 
4.1  Comparison with Baseline Methods 
We first performed an experiment comparing 
two baseline methods with our proposed method. 
We prepared a baseline method that regards a 
sentence as an opinion if it contains a number of 
declaratively subjective clues that exceeds a cer-
tain threshold. The best threshold was set 
through trial and error at five occurrences. We 
also prepared another baseline method that 
learns a model and classifies a sentence using 
only features about a bag of words. 
The experimental results are shown in Table 4. 
It can be seen that our method performs better 
than the two baseline methods. Though the dif-
ference between our method?s results and those 
of the bag-of-words method seems rather small, 
the superiority of the proposed method cannot be 
rejected at the significance level of 5% in t-test. 
Answer 
System 
Opinion No opinion 
Opinion a b 
No opinion c d 
Opinion No opinion 
Method 
Precision Recall F-measure Precision Recall F-measure 
Accuracy 
Occurrences of DS clues 
(baseline 1) 
66.4% 35.3% 46.0% 82.6% 94.5% 88.1% 80.5% 
Bag of words 
(baseline 2) 
80.9% 64.2% 71.6% 89.6% 95.3% 92.4% 88.0% 
Proposed 78.6% 70.8% 74.4% 91.3% 94.0% 92.6% 88.6% 
Table 4: Results for comparison with baseline methods 
Table 3: Number of sentences in a test set 
44
 4.2 Feature Parameters with Position In-
formation 
We inspected the effect of position information 
of 2,936 declaratively subjective clues based on 
the heuristic rule that a Japanese predicate part 
almost always appears in the last ten words in a 
sentence. Instead of more precisely identifying 
predicate position from parsing information, we 
employed this heuristic rule as a feature parame-
ter in the SVM learner for practical reasons. 
Table 5 lists the experimental results. "All 
words" indicates that all feature parameters are 
permitted at any position in the sentence. "Last 
10 words" indicates that all feature parameters 
are permitted only if they occur within the last 
ten words in the sentence.  
We can see that feature parameters with posi-
tion information perform better than those with-
out position information in all evaluations. This 
result confirms our claim that the position of the 
feature parameters is important for judging 
whether a sentence is an opinion or not. 
However, the difference did not indicate supe-
riority between the two results at the significance 
level of 5%. In the ?last 10 word? experiment, 
we restricted the position of 422 declaratively 
subjective clues like (b), (c) and (l) in Table 2, 
which appear in any position of a sentence, to 
the same conditions as with the other types of 
2,514 declaratively subjective clues. The fact 
that the equal position restriction on all declara-
tively subjective clues slightly improved per-
formance suggests there will be significant im-
provement in performance from assigning the 
individual position condition to each declara-
tively subjective clue. 
4.3 Effect of Feature Parameters 
The third experiment was designed to ascertain 
the effects of declaratively subjective clues and 
semantic categories. The declaratively subjective 
clues and semantic categories were employed as 
feature parameters for the SVM learner. The ef-
fect of each particular feature parameter can be 
seen by using it without the other feature pa-
rameter, because the feature parameters are in-
dependent of each other. 
The experimental results are shown in Table 6. 
The first row shows trials using only frequent 
words and parts of speech as feature parameters. 
"Y" in the first and second columns indicates 
exclusive use of declaratively subjective clues 
and semantic categories as the feature parame-
ters, respectively. For instance, we can deter-
mine the effect of declaratively subjective clues 
by comparing the first row with the second row. 
The results show the effects of declaratively 
subjective clues and semantic categories. The 
results of the first row show that the method us-
ing only frequent words and parts of speech as 
the feature parameters cannot precisely classify 
subjective sentences. Additionally, the last row 
of the results clearly shows that using both de-
claratively subjective clues and semantic catego-
ries as the feature parameters is the most effec-
tive. The difference between the last row of the 
results and the other rows cannot be rejected 
even at the significance level of 5%. 
Feature sets Opinion No opinion 
DS 
clues 
Semantic 
categories 
Precision Recall F-
measure
Precision Recall F-
measure 
Accuracy 
  71.4% 53.2% 60.9% 87.7% 94.1% 90.8% 85.2% 
Y  79.9% 64.3% 71.2% 89.6% 95.0% 92.2% 87.8% 
 Y 76.1% 68.9% 72.2% 90.7% 93.3% 92.0% 87.5% 
Y Y 78.6% 70.8% 74.4% 91.3% 94.0% 92.6% 88.6% 
Opinion No opinion Position 
Precision Recall F-measure Precision Recall F-measure 
Accuracy 
All words 76.8% 70.6% 73.5% 91.2% 93.4% 92.3% 88.0% 
Last 10 words 78.6% 70.8% 74.4% 91.3% 94.0% 92.6% 88.6% 
Table 5: Results for feature parameters with position information 
Table 6: Results for effect of feature parameters 
45
5 Conclusion and Future Work 
 We proposed a method of extracting sentences 
classified by an SVM as opinion-expressing that 
uses feature sets of declaratively subjective clues 
collected from opinion-expressing sentences in 
Japanese web pages and semantic categories of 
words obtained from a Japanese lexicon. The 
first experiment showed that our method per-
formed better than baseline methods. The second 
experiment suggested that our method performed 
better when extraction of features was limited to 
the predicate part of a sentence rather than al-
lowed anywhere in the sentence. The last ex-
periment showed that using both declaratively 
subjective clues and semantic categories as fea-
ture parameters yielded better results than using 
either clues or categories exclusively. 
Our future work will attempt to develop an 
open-domain opinion web search engine. To 
succeed, we first need to augment the proposed 
opinion-sentence extraction method by incorpo-
rating the query relevancy mechanism. Accord-
ingly, a user will be able to retrieve opinion-
expressing sentences relevant to the query. Sec-
ond, we need to classify extracted sentences in 
terms of emotion, sentiment, requirement, and 
suggestion so that a user can retrieve relevant 
opinions on demand. Finally, we need to sum-
marize the extracted sentences so that the user 
can quickly learn what the writer wanted to say.  
References 
Claire Cardie, Janyce Wiebe, Theresa Wilson, and 
Diane J. Litman. 2003. Combining Low-Level and 
Summary Representations of Opinions. for Multi-
Perspective Question Answering. Working Notes - 
New Directions in Question Answering (AAAI 
Spring Symposium Series) . 
Kushal Dave, Steve Lawrence, and David M. Pennock. 
2003. Mining the peanut gallery: Opinion extraction 
and semantic classification of product reviews. Pro-
ceedings of the 12th International World Wide Web 
Conference, 519-528. 
Satoru Ikehara, Masahiro Miyazaki, Akio Yokoo, Sato-
shi Shirai, Hiromi Nakaiwa, Kentaro Ogura, Yoshi-
fumi Ooyama, and Yoshihiko Hayashi. 1997. Ni-
hongo Goi Taikei ? A Japanese Lexicon. Iwanami 
Shoten. 5 volumes. (In Japanese). 
Soo-Min Kim and Eduard Hovy. 2004. Determining the 
Sentiment of Opinions. Proceedings of the. COLING-
04. 
Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, 
Kenji Tateishi, and Toshikazu Fukushima. 2004. Col-
lecting Evaluative Expressions for Opinion Extrac-
tion. Proceedings of the First International Joint Con-
ference on Natural Language Processing (IJCNLP-
04), 584-589. 
Satoshi Morinaga, Kenji Yamanishi, and Kenji Tateishi. 
2002. Mining Product Reputations on the Web. Pro-
ceedings of the eighth ACM SIGKDD International 
Conference on Knowledge Discovery and Data Min-
ing (KDD 2002).  
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment Classification using 
Machine Learning Techniques. Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), 76-86. 
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment 
Analysis: Capturing Favorability Using Natural 
Language Processing. Proceedings of the 2nd Inter-
national Conference on Knowledge Capture(K-CAP 
2003). 
Ellen Riloff and Janyce Wiebe. 2003. Learning Extrac-
tion Patterns for Subjective Expressions. Proceedings 
of the Conference on Empirical Methods in Natural 
Language Processing (EMNLP-03), 105-112. 
Kenji Tateishi, Yoshihide Ishiguro, and Toshikazu Fu-
kushima, 2004. A Reputation Search Engine that 
Collects People?s Opinions by Information Extrac-
tion Technology, IPSJ Transactions Vol. 45 
No.SIG07, 115-123. 
Peter Turney. 2002. Thumbs Up or Thumbs Down? Se-
mantic Orientation Applied to Unsupervised Classifi-
cation of Reviews. Proceedings of the 40th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-2002), 417-424. 
Janyce Wiebe. 2000. Learning Subjective Adjectives 
from Corpora. Proceedings of the 17th National Con-
ference on Artificial Intelligence (AAAI -2000).  
Janyce Wiebe, Theresa Wilson, and Matthew Bell. 2001. 
Identifying Collocations for Recognizing Opinions. 
Proceedings of ACL/EACL 2001 Workshop on Col-
location.  
Janyce Wiebe and Ellen Riloff. 2005. Creating Subjec-
tive and Objective Sentence Classifiers from Unanno-
tated Texts. Proceedings of Sixth International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLing-2005), 486-497.  
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa, 2004. 
Just how mad are you? Finding strong and weak 
opinion clauses. Proceeding of the AAAI Spring 
Symposium on Exploring Attitude and Affect in 
Text: Theories and Applications. 
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards Answering Opinion Questions: Separating 
Facts from Opinions and Identifying the Polarity of 
Opinion Sentences. Proceedings of the Conference 
on Empirical Methods in Natural Language Process-
ing (EMNLP-2003). 
 
46
