Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 11?20,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Graph Alignment for Semi-Supervised Semantic Role Labeling
Hagen F?urstenau
Dept. of Computational Linguistics
Saarland University
Saarbr?ucken, Germany
hagenf@coli.uni-saarland.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
Unknown lexical items present a major
obstacle to the development of broad-
coverage semantic role labeling systems.
We address this problem with a semi-
supervised learning approach which ac-
quires training instances for unseen verbs
from an unlabeled corpus. Our method re-
lies on the hypothesis that unknown lexical
items will be structurally and semantically
similar to known items for which annota-
tions are available. Accordingly, we rep-
resent known and unknown sentences as
graphs, formalize the search for the most
similar verb as a graph alignment prob-
lem and solve the optimization using inte-
ger linear programming. Experimental re-
sults show that role labeling performance
for unknown lexical items improves with
training data produced automatically by
our method.
1 Introduction
Semantic role labeling, the task of automatically
identifying the semantic roles conveyed by sen-
tential constituents, has recently attracted much at-
tention in the literature. The ability to express the
relations between predicates and their arguments
while abstracting over surface syntactic configu-
rations holds promise for many applications that
require broad coverage semantic processing. Ex-
amples include information extraction (Surdeanu
et al, 2003), question answering (Narayanan
and Harabagiu, 2004), machine translation (Boas,
2005), and summarization (Melli et al, 2005).
Much progress in the area of semantic role la-
beling is due to the creation of resources like
FrameNet (Fillmore et al, 2003), which document
the surface realization of semantic roles in real
world corpora. Such data is paramount for de-
veloping semantic role labelers which are usually
based on supervised learning techniques and thus
require training on role-annotated data. Examples
of the training instances provided in FrameNet are
given below:
(1) a. If [you]
Agent
[carelessly]
Manner
chance going back there, you
deserve what you get.
b. Only [one winner]
Buyer
purchased
[the paintings]
Goods
c. [Rachel]
Agent
injured [her
friend]
Victim
[by closing the car
door on his left hand]
Means
.
Each verb in the example sentences evokes a frame
which is situation-specific. For instance, chance
evokes the Daring frame, purchased the Com-
merce buy frame, and injured the Cause harm
frame. In addition, frames are associated with
semantic roles corresponding to salient entities
present in the situation evoked by the predicate.
The semantic roles for the frame Daring are Agent
and Manner, whereas for Commerce buy these are
Buyer and Goods. A system trained on large
amounts of such hand-annotated sentences typi-
cally learns to identify the boundaries of the argu-
ments of the verb predicate (argument identifica-
tion) and label themwith semantic roles (argument
classification).
A variety of methods have been developed for
semantic role labeling with reasonably good per-
formance (F
1
measures in the low 80s on standard
test collections for English; we refer the interested
reader to the proceedings of the SemEval-2007
shared task (Baker et al, 2007) for an overview
of the state-of-the-art). Unfortunately, the reliance
on training data, which is both difficult and highly
expensive to produce, presents a major obstacle
to the widespread application of semantic role la-
beling across different languages and text gen-
res. The English FrameNet (version 1.3) is not
11
a small resource ? it contains 502 frames cov-
ering 5,866 lexical entries and 135,000 annotated
sentences. Nevertheless, by virtue of being un-
der development it is incomplete. Lexical items
(i.e., predicates evoking existing frames) are miss-
ing as well as frames and annotated sentences
(their number varies greatly across lexical items).
Considering how the performance of supervised
systems degrades on out-of-domain data (Baker
et al, 2007), not to mention unseen events, semi-
supervised or unsupervised methods seem to offer
the primary near-term hope for broad coverage se-
mantic role labeling.
In this work, we develop a semi-supervised
method for enhancing FrameNet with additional
annotations which could then be used for clas-
sifier training. We assume that an initial set of
labeled examples is available. Then, faced with
an unknown predicate, i.e., a predicate that does
not evoke any frame according to the FrameNet
database, we must decide (a) which frames it be-
longs to and (b) how to automatically annotate
example sentences containing the predicate. We
solve both problems jointly, using a graph align-
ment algorithm. Specifically, we view the task
of inferring annotations for new verbs as an in-
stance of a structural matching problem and fol-
low a graph-based formulation for pairwise global
network alignment (Klau, 2009). Labeled and un-
labeled sentences are represented as dependency-
graphs; we formulate the search for an optimal
alignment as an integer linear program where dif-
ferent graph alignments are scored using a func-
tion based on semantic and structural similarity.
We evaluate our algorithm in two ways. We assess
how accurate it is in predicting the frame for an
unknown verb and also evaluate whether the an-
notations we produce are useful for semantic role
labeling.
In the following section we provide an overview
of related work. Next, we describe our graph-
alignment model in more detail (Section 3) and
present the resources and evaluation methodology
used in our experiments (Section 4). We conclude
the paper by presenting and discussing our results.
2 Related Work
Much previous work has focused on creating
FrameNet-style annotations for languages other
than English. A common strategy is to exploit
parallel corpora and transfer annotations from
English sentences onto their translations (Pad?o
and Lapata, 2006; Johansson and Nugues, 2006).
Other work attempts to automatically augment the
English FrameNet in a monolingual setting either
by extending its coverage or by creating additional
training data.
There has been growing interest recently in
determining the frame membership for unknown
predicates. This is a challenging task, FrameNet
currently lists 502 frames with example sentences
which are simply too many (potentially related)
classes to consider for a hypothetical system.
Moreover, predicates may have to be assigned to
multiple frames, on account of lexical ambiguity.
Previous work has mainly used WordNet (Fell-
baum, 1998) to extend FrameNet. For example,
Burchardt et al (2005) apply a word sense dis-
ambiguation system to annotate predicates with
a WordNet sense and hyponyms of these predi-
cates are then assumed to evoke the same frame.
Johansson and Nugues (2007) treat this problem
as an instance of supervised classification. Using
a feature representation based also on WordNet,
they learn a classifier for each frame which decides
whether an unseen word belongs to the frame or
not. Pennacchiotti et al (2008) create ?distribu-
tional profiles? for frames. Each frame is repre-
sented as a vector, the (weighted) centroid of the
vectors representing the meaning of the predicates
it evokes. Unknown predicates are then assigned
to the most similar frame. They also propose a
WordNet-based model that computes the similar-
ity between the synsets representing an unknown
predicate and those activated by the predicates of
a frame.
All the approaches described above are type-
based. They place more emphasis on extending
the lexicon rather than the annotations that come
with it. In our earlier work (F?urstenau and Lapata,
2009) we acquire new training instances, by pro-
jecting annotations from existing FrameNet sen-
tences to new unseen ones. The proposed method
is token-based, however, it only produces annota-
tions for known verbs, i.e., verbs that FrameNet
lists as evoking a given frame.
In this paper we generalize the proposals of
Pennacchiotti et al (2008) and F?urstenau and Lap-
ata (2009) in a unified framework. We create train-
ing data for semantic role labeling of unknown
predicates by projection of annotations from la-
beled onto unlabeled data. This projection is con-
12
ceptualized as a graph alignment problem where
we seek to find a globally optimal alignment sub-
ject to semantic and structural constraints. Instead
of predicting the same frame for each occurence of
an unknown predicate, we consider a set of candi-
date frames and allow projection from any labeled
predicate that can evoke one of these frames. This
allows us to make instance-based decisions and
thus account for predicate ambiguity.
3 Graph Alignment Method
Our approach acquires annotations for an un-
known frame evoking verb by selecting sen-
tences featuring this verb from a large unlabeled
corpus (the expansion corpus). The choice is
based upon a measure of similarity between the
predicate-argument structure of the unknown verb
and those of similar verbs in a manually labeled
corpus (the seed corpus). We formulate the prob-
lem of finding the most similar verbs as the search
for an optimal graph alignment (we represent
labeled and unlabeled sentences as dependency
graphs). Conveniently, this allows us to create la-
beled training instances for the unknown verb by
projecting role labels from the most similar seed
instance. The annotations can be subsequently
used for training a semantic role labeler.
Given an unknown verb, the first step is to nar-
row down the number of frames it could poten-
tially evoke. FrameNet provides definitions for
more than 500 frames, of which we entertain only
a small number. This is done using a method sim-
ilar to Pennacchiotti et al (2008). Each frame
is represented in a semantic space as the cen-
troid of the vectors of all its known frame evoking
verbs. For an unknown verb we then consider as
frame candidates the k closest frames according to
a measure of distributional similarity (which we
compute between the unknown verb?s vector and
the frame centroid vector). We provide details of
the semantic space we used in our experiments in
Section 4.
Next, we compare each sentence featuring the
unknown verb in question to labeled sentences fea-
turing known verbs which according to FrameNet
evoke any of the k candidate frames. If sufficiently
similar seeds exist, the unlabeled sentence is anno-
tated by projecting role labels from the most sim-
ilar one. The similarity score of this best match is
recorded as a measure of the quality (or reliability)
of the new instance. After carrying out this pro-
Body movement
FEE
??
~
~
~
~
~
~
~
~
~
Agent

_
e
i
k
m
p
r
u
y



Body part






~
|
}

and
SUBJ
xxq
q
q
q
q
q
q
q
q
q
CONJ

CONJ
''
O
O
O
O
O
O
O
O
O
O
O
O
O
Herkimer
MOD

blink
DOBJ

nod
MOD

Old
eye
DET

wisely
his
Figure 1: Annotated dependency graph for the
sentenceOld Herkimer blinked his eye and nodded
wisely. The alignment domain is indicated in bold
face. Labels in italics denote frame roles, whereas
grammatical roles are rendered in small capitals.
The verb blink evokes the frame Body Movement.
cedure for all sentences in the expansion corpus
featuring an unknown verb, we collect the highest
scoring new instances and add them back to our
seed corpus as new training items. In the follow-
ing we discuss in more detail how the similarity of
predicate-argument structures is assessed.
3.1 Alignment Scoring
Let s be a semantically labeled dependency graph
in which node n
FEE
represents the frame evoking
verb. Here, we use the term ?labeled? to indi-
cate that the graph contains semantic role labels
in addition to grammatical role labels (e.g., sub-
ject or object). Let g be an unlabeled graph
and n
target
a verbal node in it. The ?unlabeled?
graph contains grammatical roles but no semantic
roles. We wish to find an alignment between the
predicate-argument structures of n
FEE
and n
target
,
respectively. Such an alignment takes the form of
a function ? from a set M of nodes of s (the align-
ment domain) to a set N of nodes of g (the align-
ment range). These two sets represent the rele-
vant predicate-argument structures within the two
graphs; nodes that are not members of these sets
are excluded from any further computations.
If there were no mismatches between (frame)
semantic arguments and syntactic arguments, we
would expect all roles in s to be instantiated by
syntactic dependents in n
FEE
. This is usually the
case but not always. We cannot therefore sim-
13
ply define M as the set of direct dependents of
the predicate, but also have to consider complex
paths between n
FEE
and role bearing nodes. An
example is given in Figure 1, where the role Agent
is filled by a node which is not dominated by the
frame evoking verb blink ; instead, it is connected
to blink by the complex path (CONJ
?1
, SUBJ). For
a given seed s we build a list of all such complex
paths and also include all nodes of s connected
to n
FEE
by one of these paths. We thus define the
alignment domain M as:
1. the predicate node n
FEE
2. all direct dependents of n
FEE
, except auxil-
iaries
3. all nodes on complex paths originating
in n
FEE
4. single direct dependents of any preposition or
conjunction node which is in (2) or end-point
of a complex path covered in (3)
The last rule ensures that the semantic heads
of prepositional phrases and conjunctions are in-
cluded in the alignment domain.
The alignment range N is defined in a similar
way. However, we cannot extract complex paths
from the unlabeled graph g, as it does not con-
tain semantic role information. Therefore, we use
the same list of complex paths extracted from s.
Note that this introduces an unavoidable asymme-
try into our similarity computation.
An alignment is a function ? : M ? N?{?}
which is injective for all values except ?,
i.e., ?(n
1
) = ?(n
2
) 6= ? ? n
1
= n
2
. We score the
similarity of two subgraphs expressed by an align-
ment function ? by the following term:
?
n?M
?(n)6=?
sem(n,?(n))+? ?
?
(n
1
,n
2
)?E(M)
(?(n
1
),?(n
2
))?E(N)
syn
(
r
n
1
n
2
,r
?(n
1
)
?(n
2
)
)
(2)
Here, sem represents a semantic similarity mea-
sure between graph nodes and syn a syntactic sim-
ilarity measure between the grammatical role la-
bels of graph edges. E(M) and E(N) are the sets
of all graph edges between nodes of M and nodes
of N, respectively, and r
n
1
n
2
denotes the grammati-
cal relation between nodes n
1
and n
2
.
Equation (2) expresses the similarity between
two predicate-argument structures in terms of the
sum of semantic similarity scores of aligned graph
nodes and the sum of syntactic similarity scores of
aligned graph edges. The relative weight of these
two sums is determined by the parameter ?. Fig-
ure 2 shows an example of an alignment between
two dependency graphs. Here, the aligned node
pairs thud and thump, back and rest, against and
against, as well as wall and front contribute se-
mantic similarity scores, while the three edge pairs
SUBJ and SUBJ, IOBJ and IOBJ, as well as DOBJ
and DOBJ contribute syntactic similarity scores.
We normalize the resulting score so that it al-
ways falls within the interval [0,1]. To take into
account unaligned nodes in both the alignment do-
main and the alignment range, we divide Equa-
tion (2) by:
?
|M| ? |N|+?
?
|E(M)| ? |E(N)| (3)
A trivial alignment of a seed with itself where all
semantic and syntactic scores are 1 will thus re-
ceive a score of:
|M| ?1+? ? |E(M)| ?1
?
|M|
2
+?
?
E(M)
2
= 1 (4)
which is the largest possible similarity score. The
lowest possible score is obviously 0, assuming that
the semantic and syntactic scores cannot be nega-
tive.
Considerable latitude is available in selecting
the semantic and syntactic similarity measures.
With regard to semantic similarity, WordNet is a
prime contender and indeed has been previously
used to acquire new predicates in FrameNet (Pen-
nacchiotti et al, 2008; Burchardt et al, 2005; Jo-
hansson and Nugues, 2007). Syntactic similarity
may be operationalized in many ways, for exam-
ple by taking account a hierarchy of grammatical
relations (Keenan and Comrie, 1977). Our experi-
ments employed relatively simple instantiations of
these measures. We did not make use of Word-
Net, as we were interested in exploring the set-
ting where WordNet is not available or has limited
coverage. Therefore, we approximate the seman-
tic similarity between two nodes via distributional
similarity. We present the details of the semantic
space model we used in Section 4.
If n and n
?
are both nouns, verbs or adjectives,
we set:
sem(n,n
?
) := cos(~v
n
,~v
n
?
) (5)
where ~v
n
and ~v
n
?
are the vectors representing the
lemmas of n and n
?
respectively. If n and n
?
14
Impact
FEE
OO


Impactor

_
i
w
	





Impactee

_
V
J
9
.
(
$
!

thud
((
SUBJ
zzv
v
v
v
v
v
v
v
v
v
IOBJ
%%
K
K
K
K
K
K
K
K
K
K
thump
SUBJ
{{v
v
v
v
v
v
v
v
v
IOBJ
%%
K
K
K
K
K
K
K
K
K
back
DET

''
against
DOBJ

66
rest
DET

IOBJ
$$
H
H
H
H
H
H
H
H
H
H
against
DOBJ

his wall
DET

77
the of
DOBJ

front
DET

IOBJ
$$
I
I
I
I
I
I
I
I
I
I
the
body
DET

the of
DOBJ

his
cage
DET

the
Figure 2: The dotted arrows show aligned nodes in the graphs for the two sentences His back thudded
against the wall. and The rest of his body thumped against the front of the cage. (Graph edges are also
aligned to each other.) The alignment domain and alignment range are indicated in bold face. The verb
thud evokes the frame Impact.
are identical prepositions or conjunctions we set
sem(n,n
?
) := 1. In all other cases sem(n,n
?
) := 0.
As far as syntactic similarity is concerned, we
chose the simplest metric possible and set:
syn
(
r,r
?
)
:=
{
1 if r = r
?
0 otherwise
(6)
3.2 Alignment Search
The problem of finding the best alignment ac-
cording to the scoring function presented in Equa-
tion (2) can be formulated as an integer linear pro-
gram. Let the binary variables x
ik
indicate whether
node n
i
of graph s is aligned to node n
k
of graph g.
Since it is not only nodes but also graph edges
that must be aligned we further introduce binary
variables y
i jkl
, where y
i jkl
= 1 indicates that the
edge between nodes n
i
and n
j
of graph s is aligned
to the edge between nodes n
k
and n
l
of graph g.
This follows a general formulation of the graph
alignment problem based on maximum structural
matching (Klau, 2009). In order for the x
ik
and
y
i jkl
variables to represent a valid alignment, the
following constraints must hold:
1. Each node of s is aligned to at most one node
of g:
?
k
x
ik
? 1
2. Each node of g is aligned to at most one node
of s:
?
i
x
ik
? 1
3. Two edges may only be aligned if their
adjacent nodes are aligned: y
i jkl
? x
ik
and
y
i jkl
? x
jl
The scoring function then becomes:
?
i,k
sem(n
i
,n
k
)x
ik
+? ?
?
i, j,k,l
syn
(
r
n
i
n
j
,r
n
k
n
l
)
y
i jkl
(7)
We solve this optimization problem with a ver-
sion of the branch-and-bound algorithm (Land
and Doig, 1960). In general, this graph align-
ment problem is NP-hard (Klau, 2009) and usually
solved approximately following a procedure simi-
lar to beam search. However, the special structure
of constraints 1 to 3, originating from the required
injectivity of the alignment function, allows us to
solve the optimization exactly. Our implementa-
tion of the branch-and-bound algorithm does not
generally run in polynomial time, however, we
found that in practice we could efficiently com-
pute optimal alignments in almost all cases (less
than 0.1% of alignment pairs in our data could not
be solved in reasonable time). This relatively be-
nign behavior depends crucially on the fact that
we do not have to consider alignments between
15
full graphs, and the number of nodes in the aligned
subgraphs is limited.
4 Experimental Design
In this section we present our experimental set-up
for assessing the performance of our method. We
give details on the data sets we used, describe the
baselines we adopted for comparison with our ap-
proach, and explain how our system output was
evaluated.
Data Our experiments used annotated sentences
from FrameNet as a seed corpus. These were
augmented with automatically labeled sentences
from the BNC which we used as our expan-
sion corpus. FrameNet sentences were parsed
with RASP (Briscoe et al, 2006). In addi-
tion to phrase structure trees, RASP delivers a
dependency-based representation of the sentence
which we used in our experiments. FrameNet role
annotations were mapped onto those dependency
graph nodes that corresponded most closely to the
annotated substring (see F?urstenau (2008) for a de-
tailed description of the mapping algorithm). BNC
sentences were also parsed with RASP (Andersen
et al, 2008).
We randomly split the FrameNet corpus
1
into 80% training set, 10% test set, and 10% de-
velopment set. Next, all frame evoking verbs in
the training set were ordered by their number of
occurrence and split into two groups, seen and un-
seen. Every other verb from the ordered list was
considered unseen. This quasi-random split covers
a broad range of predicates with a varying number
of annotations. Accordingly, the FrameNet sen-
tences in the training and test sets were divided
into the sets train seen, train unseen, test seen,
and test unseen. As we explain below, this was
necessary for evaluation purposes.
The train seen dataset consisted of 24,220 sen-
tences, with 1,238 distinct frame evoking verbs,
whereas train unseen contained 24,315 sentences
with the same number of frame evoking verbs.
Analogously, test seen had 2,990 sentences and
817 unique frame evoking verbs; the number
of sentences in test unseen was 3,064 (with
847 unique frame evoking verbs).
Model Parameters The alignment model pre-
sented in Section 3 crucially relies on the similar-
1
Here, we consider only FrameNet example sentences
featuring verbal predicates.
ity function that scores potential alignments (see
Equation (2)). This function has a free parameter,
the weight ? for determining the relative contri-
bution of semantic and syntactic similarity. We
tuned ? using leave-one-out cross-validation on
the development set. For each annotated sentence
in this set we found its most similar other sentence
and determined the best alignment between the
two dependency graphs representing them. Since
the true annotations for each sentence were avail-
able, it was possible to evaluate the accuracy of our
method for any ? value. We did this by compar-
ing the true annotation of a sentence to the anno-
tation its nearest neighbor would have induced by
projection. Following this procedure, we obtained
best results with ? = 0.2.
The semantic similarity measure relies on a se-
mantic space model which we built on a lemma-
tized version of the BNC. Our implementation fol-
lowed closely the model presented in F?urstenau
and Lapata (2009) as it was used in a similar
task and obtained good results. Specifically, we
used a context window of five words on either
side of the target word, and 2,000 vector dimen-
sions. These were the common context words in
the BNC. Their values were set to the ratio of the
probability of the context word given the target
word to the probability of the context word over-
all. Semantic similarity was measured using the
cosine of the angle between the vectors represent-
ing any two words. The same semantic space was
used to create the distributional profile of a frame
(which is the centroid of the vectors of its verbs).
For each unknown verb, we consider the k most
similar frame candidates (again similarity is mea-
sured via cosine). Our experiments explored dif-
ferent values of k ranging from 1 to 10.
Evaluation Our evaluation assessed the perfor-
mance of a semantic frame and role labeler with
and without the annotations produced by our
method. The labeler followed closely the im-
plementation described in Johansson and Nugues
(2008). We extracted features from dependency
parses corresponding to those routinely used in
the semantic role labeling literature (see Baker
et al (2007) for an overview). SVM classifiers
were trained
2
with the LIBLINEAR library (Fan
et al, 2008) and learned to predict the frame
name, role spans, and role labels. We followed
2
The regularization parameterC was set to 0.1.
16
Figure 3: Frame labeling accuracy on high,
medium and low frequency verbs, before and af-
ter applying our expansion method; the labeler de-
cides among k = 1, . . . ,10 candidate frames.
the one-versus-one strategy for multi-class classi-
fication (Friedman, 1996).
Specifically, the labeler was trained on the
train seen data set without any access to training
instances representative of the ?unknown? verbs in
test unseen. We then trained the labeler on a larger
set containing train seen and new training exam-
ples obtained with our method. To do this, we used
train seen as the seed corpus and the BNC as the
expansion corpus. For each ?unknown? verb in
train unseen we obtained BNC sentences with an-
notations projected from their most similar seeds.
The quality of these sentences as training instances
varies depending on their similarity to the seed.
In our experiments we added to the training set
the 20 highest scoring BNC sentences per verb
(adding less or more instances led to worse per-
formance).
The average number of frames which can be
evoked by a verb token in the set test unseen
was 1.96. About half of them (1,522 instances)
can evoke only one frame, 22% can evoke two
frames, and 14 instances can evoke up to 11 differ-
ent frames. Finally, there are 120 instances (4%)
in test unseen for which the correct frame is not
annotated on any sentence in train seen.
Figure 4: Role labeling F
1
for high, medium, and
low frequency verbs (roles of mislabeled frames
are counted as wrong); the labeler decides among
k = 1, . . . ,10 candidate frames.
5 Results
We first examine how well our method performs
at frame labeling. We partitioned the frame evok-
ing verbs in our data set into three bands (High,
Medium, and Low) based on an equal division
of the range of their occurrence frequency in the
BNC. As frequency is strongly correlated with
polysemy, the division allows us to assess how
well our method is performing at different degrees
of ambiguity. Figure 3 summarizes our results for
High, Medium, and Low frequency verbs. The
number of verbs in each band are 282, 282, and
283, respectively. We compare the frame accuracy
of a labeler trained solely on the annotations avail-
able in FrameNet (Without expansion) against a
labeler that also uses annotations created with our
method (After expansion). Both classifiers were
employed in a setting where they had to decide
among k candidate frames. These were the k most
similar frames to the unknown verb in question.
We also show the accuracy of a simple baseline
labeler, which randomly chooses one of the k can-
didate frames.
The graphs in Figure 3 show that for verbs in the
Medium and Low frequency bands, both classi-
fiers (with and without expansion) outperform the
baseline of randomly choosing among k candidate
frames. Interestingly, rather than defaulting to the
most similar frame (k = 1), we observe that ac-
17
Figure 5: Hybrid frame labeling accuracy (k = 1
for High frequency verbs).
curacy improves when frame selection is viewed
as a classification task. The classifier trained on
the expanded training set consistently outperforms
the one trained on the original training set. While
this is also true for the verbs in the High frequency
band, labeling accuracy peaks at k = 1 and does
not improve when more candidate frames are con-
sidered. This is presumably due to the skewed
sense distributions of high frequency verbs, and
defaulting to the most likely sense achieves rela-
tively good performance.
Next, we evaluated our method on role label-
ing, again by comparing the performance of our
role labeler on the expanded and original train-
ing set. Since role and frame labeling are inter-
dependent, we count all predicted roles of an in-
correctly predicted frame as wrong. This unavoid-
ably results in low role labeling scores, but allows
us to directly compare performance across differ-
ent settings (e.g., different number of candidate
frames, with or without expansion). Figure 4 re-
ports labeled F
1
for verbs in the High, Medium
and Low frequency bands. The results are simi-
lar to those obtained for frame labeling; the role
labeler trained on the the expanded training set
consistently outperforms the labeler trained on the
unexpanded one. (There is no obvious baseline
for role labeling, which is a complex task involv-
ing the prediction of frame labels, identification of
the role bearing elements, and assignment of role
labels.) Again, for High frequency verbs simply
defaulting to k = 1 performs best.
Taken together, our results on frame and role
labeling indicate that our method is not very effec-
tive for High frequency verbs (which in practice
should be still annotated manually). We there-
Figure 6: Hybrid role labeling F
1
(k = 1 for High
frequency verbs).
fore also experimented with a hybrid approach
that lets the classifier choose among k candi-
dates for Medium and Low frequency verbs and
defaults to the most similar candidate for High
frequency verbs. Results for this approach are
shown in Figures 5 and 6. All differences be-
tween the expanded and the unexpanded classi-
fier when choosing between the same k > 1 can-
didates are significant according to McNemar?s
test (p < .05). The best frame labeling accu-
racy (26.3%) is achieved by the expanded classi-
fier when deciding among k = 6 candidate frames.
This is significantly better (p < .01) than the best
performance of the unexpanded classifier (25.0%),
which is achieved at k = 2. Role labeling results
follow a similar pattern. The best expanded classi-
fier (F
1
=14.9% at k = 6) outperforms the best un-
expanded one (F
1
=14.1% at k = 2). The difference
in performance as significant at p < 0.05, using
stratified shuffling (Noreen, 1989).
6 Conclusions
This paper presents a novel semi-supervised ap-
proach for reducing the annotation effort involved
in creating resources for semantic role labeling.
Our method acquires training instances for un-
known verbs (i.e., verbs that are not evoked by
existing FrameNet frames) from an unlabeled cor-
pus. A key assumption underlying our work is
that verbs with similar meanings will have sim-
ilar argument structures. Our task then amounts
to finding the seen instances that resemble the un-
seen instances most, and projecting their annota-
tions. We represent this task as a graph alignment
problem, and formalize the search for an optimal
alignment as an integer linear program under an
18
objective function that takes semantic and struc-
tural similarity into account.
Experimental results show that our method im-
proves frame and role labeling accuracy, espe-
cially for Medium and Low frequency verbs. The
overall frame labeling accuracy may seem low.
There are at least two reasons for this. Firstly, the
unknown verb might have a frame for which no
manual annotation exists. And secondly, many er-
rors are due to near-misses, i.e., we assign the un-
known verb a wrong frame which is nevertheless
very similar to the right one. In this case, accuracy
will not give us any credit.
An obvious direction for future work concerns
improving our scoring function. Pennacchiotti
et al (2008) show that WordNet-based similarity
measures outperform their simpler distributional
alternatives. An interesting question is whether the
incorporation of WordNet-based similarity would
lead to similar improvements in our case. Also
note that currently our method assigns unknown
lexical items to existing frames. A better alterna-
tive would be to decide first whether the unknown
item can be classified at all (because it evokes a
known frame) or whether it represents a genuinely
novel frame for which manual annotation must be
provided.
Acknowledgments The authors acknowledge
the support of DFG (IRTG 715) and EPSRC (grant
GR/T04540/01). We are grateful to Richard Jo-
hansson for his help with the re-implementation of
his semantic role labeler. Special thanks to Man-
fred Pinkal for valuable feedback on this work.
References
?istein E. Andersen, Julien Nioche, Ted Briscoe,
and John Carroll. 2008. The BNC Parsed with
RASP4UIMA. In Proceedings of the 6th Interna-
tional Language Resources and Evaluation Confer-
ence, pages 865?869, Marrakech, Morocco.
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 19: Frame Semantic
Structure Extraction. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 99?104, Prague, Czech Republic.
Hans C. Boas. 2005. Semantic frames as interlingual
representations for multilingual lexical databases.
International Journal of Lexicography, 18(4):445?
478.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The Second Release of the RASP System. In Pro-
ceedings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, pages 77?80, Sydney, Australia.
Aljoscha Burchardt, Katrin Erk, and Anette Frank.
2005. A WordNet Detour to FrameNet. In Proceed-
ings of the GLDV 200Workshop GermaNet II, Bonn,
Germany.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A
Library for Large Linear Classification. Journal of
Machine Learning Research, 9:1871?1874.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Jerome H. Friedman. 1996. Another approach to poly-
chotomous classification. Technical report, Depart-
ment of Statistics, Stanford University.
Hagen F?urstenau and Mirella Lapata. 2009. Semi-
supervised semantic role labeling. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 220?228, Athens, Greece.
Hagen F?urstenau. 2008. Enriching frame semantic re-
sources with dependency graphs. In Proceedings of
the 6th Language Resources and Evaluation Confer-
ence, pages 1478?1484, Marrakech, Morocco.
Richard Johansson and Pierre Nugues. 2006. A
FrameNet-based semantic role labeler for Swedish.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 436?443, Syd-
ney, Australia.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Richard
Johansson and Pierre Nugues, editors, FRAME
2007: Building Frame Semantics Resources for
Scandinavian and Baltic Languages, pages 27?30,
Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role la-
beling. In Proceedings of the 22nd International
Conference on Computational Linguistics, pages
393?400, Manchester, UK.
E. Keenan and B. Comrie. 1977. Noun phrase acces-
sibility and universal grammar. Linguistic Inquiry,
8:62?100.
Gunnar W. Klau. 2009. A new graph-based method
for pairwise global network alignment. BMC Bioin-
formatics, 10 (Suppl 1).
A.H. Land and A.G. Doig. 1960. An automatic
method for solving discrete programming problems.
Econometrica, 28:497?520.
19
Gabor Melli, Yang Wang, Yurdong Liu, Mehdi M.
Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,
and Fred Popowich. 2005. Description of
SQUASH, the SFU question answering summary
handler for the duc-2005 summarization task. In
Proceedings of the HLT/EMNLP Document Under-
standing Workshop, Vancouver, Canada.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 693?701, Geneva,
Switzerland.
E. Noreen. 1989. Computer-intensive Methods for
Testing Hypotheses: An Introduction. John Wiley
and Sons Inc.
Sebastian Pad?o and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for seman-
tic projection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 1161?1168, Sydney,
Australia.
Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of FrameNet lexical units. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 457?465, Honolulu,
Hawaii.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8?15, Sap-
poro, Japan.
20
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 220?228,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Semi-Supervised Semantic Role Labeling
Hagen Fu?rstenau
Dept. of Computational Linguistics
Saarland University
Saarbru?cken, Germany
hagenf@coli.uni-saarland.de
Mirella Lapata
School of Informatics
University of Edinburgh
Edinburgh, UK
mlap@inf.ed.ac.uk
Abstract
Large scale annotated corpora are pre-
requisite to developing high-performance
semantic role labeling systems. Unfor-
tunately, such corpora are expensive to
produce, limited in size, and may not be
representative. Our work aims to reduce
the annotation effort involved in creat-
ing resources for semantic role labeling
via semi-supervised learning. Our algo-
rithm augments a small number of man-
ually labeled instances with unlabeled ex-
amples whose roles are inferred automat-
ically via annotation projection. We for-
mulate the projection task as a generaliza-
tion of the linear assignment problem. We
seek to find a role assignment in the un-
labeled data such that the argument sim-
ilarity between the labeled and unlabeled
instances is maximized. Experimental re-
sults on semantic role labeling show that
the automatic annotations produced by our
method improve performance over using
hand-labeled instances alone.
1 Introduction
Recent years have seen a growing interest in the
task of automatically identifying and labeling the
semantic roles conveyed by sentential constituents
(Gildea and Jurafsky, 2002). This is partly due to
its relevance for applications ranging from infor-
mation extraction (Surdeanu et al, 2003; Mos-
chitti et al, 2003) to question answering (Shen and
Lapata, 2007), paraphrase identification (Pado? and
Erk, 2005), and the modeling of textual entailment
relations (Tatu and Moldovan, 2005). Resources
like FrameNet (Fillmore et al, 2003) and Prop-
Bank (Palmer et al, 2005) have also facilitated the
development of semantic role labeling methods by
providing high-quality annotations for use in train-
ing. Semantic role labelers are commonly devel-
oped using a supervised learning paradigm1 where
a classifier learns to predict role labels based on
features extracted from annotated training data.
Examples of the annotations provided in
FrameNet are given in (1). Here, the meaning of
predicates (usually verbs, nouns, or adjectives) is
conveyed by frames, schematic representations of
situations. Semantic roles (or frame elements) are
defined for each frame and correspond to salient
entities present in the situation evoked by the pred-
icate (or frame evoking element). Predicates with
similar semantics instantiate the same frame and
are attested with the same roles. In our exam-
ple, the frameCause harm has three core semantic
roles, Agent, Victim, and Body part and can be in-
stantiated with verbs such as punch, crush, slap,
and injure. The frame may also be attested with
non-core (peripheral) roles that are more generic
and often shared across frames (see the roles De-
gree, Reason, and Means, in (1c) and (1d)).
(1) a. [Lee]Agent punched [John]Victim
[in the eye]Body part.
b. [A falling rock]Cause crushed [my
ankle]Body part.
c. [She]Agent slapped [him]Victim
[hard]Degree [for his change of
mood]Reason.
d. [Rachel]Agent injured [her
friend]Victim [by closing the car
door on his left hand]Means.
The English FrameNet (version 1.3) contains
502 frames covering 5,866 lexical entries. It also
comes with a set of manually annotated exam-
ple sentences, taken mostly from the British Na-
tional Corpus. These annotations are often used
1The approaches are too numerous to list; we refer the
interested reader to the proceedings of the SemEval-2007
shared task (Baker et al, 2007) for an overview of the state-
of-the-art.
220
as training data for semantic role labeling sys-
tems. However, the applicability of these sys-
tems is limited to those words for which labeled
data exists, and their accuracy is strongly corre-
lated with the amount of labeled data available.
Despite the substantial annotation effort involved
in the creation of FrameNet (spanning approxi-
mately twelve years), the number of annotated in-
stances varies greatly across lexical items. For in-
stance, FrameNet contains annotations for 2,113
verbs; of these 12.3% have five or less annotated
examples. The average number of annotations per
verb is 29.2. Labeled data is thus scarce for indi-
vidual predicates within FrameNet?s target domain
and would presumably be even scarcer across do-
mains. The problem is more severe for languages
other than English, where training data on the
scale of FrameNet is virtually non-existent. Al-
though FrameNets are being constructed for Ger-
man, Spanish, and Japanese, these resources are
substantially smaller than their English counter-
part and of limited value for modeling purposes.
One simple solution, albeit expensive and time-
consuming, is to manually create more annota-
tions. A better alternative may be to begin with
an initial small set of labeled examples and aug-
ment it with unlabeled data sufficiently similar to
the original labeled set. Suppose we have man-
ual annotations for sentence (1a). We shall try and
find in an unlabeled corpus other sentences that
are both structurally and semantically similar. For
instance, we may think that Bill will punch me in
the face and I punched her hard in the head re-
semble our initial sentence and are thus good ex-
amples to add to our database. Now, in order to
use these new sentences as training data we must
somehow infer their semantic roles. We can prob-
ably guess that constituents in the same syntactic
position must have the same semantic role, espe-
cially if they refer to the same concept (e.g., ?body
parts?) and thus label in the face and in the head
with the role Body part. Analogously, Bill and
I would be labeled as Agent and me and her as
Victim.
In this paper we formalize the method sketched
above in order to expand a small number of
FrameNet-style semantic role annotations with
large amounts of unlabeled data. We adopt a learn-
ing strategy where annotations are projected from
labeled onto unlabeled instances via maximizing
a similarity function measuring syntactic and se-
mantic compatibility. We formalize the annotation
projection problem as a generalization of the linear
assignment problem and solve it efficiently using
the simplex algorithm. We evaluate our algorithm
by comparing the performance of a semantic role
labeler trained on the annotations produced by our
method and on a smaller dataset consisting solely
of hand-labeled instances. Results in several ex-
perimental settings show that the automatic anno-
tations, despite being noisy, bring significant per-
formance improvements.
2 Related Work
The lack of annotated data presents an obstacle
to developing many natural language applications,
especially when these are not in English. It is
therefore not surprising that previous efforts to re-
duce the need for semantic role annotation have
focused primarily on non-English languages.
Annotation projection is a popular framework
for transferring frame semantic annotations from
one language to another by exploiting the transla-
tional and structural equivalences present in par-
allel corpora. The idea here is to leverage the ex-
isting English FrameNet and rely on word or con-
stituent alignments to automatically create an an-
notated corpus in a new language. Pado? and Lap-
ata (2006) transfer semantic role annotations from
English onto German and Johansson and Nugues
(2006) from English onto Swedish. A different
strategy is presented in Fung and Chen (2004),
where English FrameNet entries are mapped to
concepts listed in HowNet, an on-line ontology
for Chinese, without consulting a parallel corpus.
Then, Chinese sentences with predicates instan-
tiating these concepts are found in a monolin-
gual corpus and their arguments are labeled with
FrameNet roles.
Other work attempts to alleviate the data re-
quirements for semantic role labeling either by re-
lying on unsupervised learning or by extending ex-
isting resources through the use of unlabeled data.
Swier and Stevenson (2004) present an unsuper-
vised method for labeling the arguments of verbs
with their semantic roles. Given a verb instance,
their method first selects a frame from VerbNet, a
semantic role resource akin to FrameNet and Prop-
Bank, and labels each argument slot with sets of
possible roles. The algorithm proceeds iteratively
by first making initial unambiguous role assign-
ments, and then successively updating a probabil-
221
ity model on which future assignments are based.
Being unsupervised, their approach requires no
manual effort other than creating the frame dic-
tionary. Unfortunately, existing resources do not
have exhaustive coverage and a large number of
verbs may be assigned no semantic role informa-
tion since they are not in the dictionary in the
first place. Pennacchiotti et al (2008) address
precisely this problem by augmenting FrameNet
with new lexical units if they are similar to an ex-
isting frame (their notion of similarity combines
distributional and WordNet-based measures). In
a similar vein, Gordon and Swanson (2007) at-
tempt to increase the coverage of PropBank. Their
approach leverages existing annotations to handle
novel verbs. Rather than annotating new sentences
that contain novel verbs, they find syntactically
similar verbs and use their annotations as surro-
gate training data.
Our own work aims to reduce but not entirely
eliminate the annotation effort involved in creating
training data for semantic role labeling. We thus
assume that a small number of manual annotations
is initially available. Our algorithm augments
these with unlabeled examples whose roles are in-
ferred automatically. We apply our method in a
monolingual setting, and thus do not project an-
notations between languages but within the same
language. In contrast to Pennacchiotti et al (2008)
and Gordon and Swanson (2007), we do not aim
to handle novel verbs, although this would be a
natural extension of our method. Given a verb
and a few labeled instances exemplifying its roles,
we wish to find more instances of the same verb
in an unlabeled corpus so as to improve the per-
formance of a hypothetical semantic role labeler
without having to annotate more data manually.
Although the use of semi-supervised learning is
widespread in many natural language tasks, rang-
ing from parsing to word sense disambiguation, its
application to FrameNet-style semantic role label-
ing is, to our knowledge, novel.
3 Semi-Supervised Learning Method
Our method assumes that we have access to a
small seed corpus that has been manually anno-
tated. This represents a relatively typical situation
where some annotation has taken place but not on
a scale that is sufficient for high-performance su-
pervised learning. For each sentence in the seed
corpus we select a number of similar sentences
Fluidic motion
FEE
~~}
}
}
}
}
}
}
}
}
}
Fluid
		
h
k
n
q
t
y
}







Path
||





~
z
feel
SUBJ
uukkk
kk
kk
kk
kk
kk
kk
k
AUX
{{vv
v
v
v
v
v
v
v
DOBJ

XCOMP
%%K
KK
KK
KK
KK
K
we can course
SUBJ
yyss
ss
ss
ss
ss
IOBJ

MOD
((P
PP
PP
PP
PP
PP
P
blood
DET

through
DOBJ

again
the vein
DET

our
Figure 1: Labeled dependency graph with seman-
tic role annotations for the frame evoking ele-
ment (FEE) course in the sentence We can feel the
blood coursing through our veins again. The frame
is Fluidic motion, and its roles are Fluid and Path.
Directed edges (without dashes) represent depen-
dency relations between words, edge labels denote
types of grammatical relations (e.g., SUBJ, AUX).
from an unlabeled expansion corpus. These are
automatically annotated by projecting relevant se-
mantic role information from the labeled sentence.
The similarity between two sentences is opera-
tionalized by measuring whether their arguments
have a similar structure and whether they express
related meanings. The seed corpus is then en-
larged with the k most similar unlabeled sentences
to form the expanded corpus. In what follows we
describe in more detail how we measure similarity
and project annotations.
3.1 Extracting Predicate-Argument
Structures
Our method operates over labeled dependency
graphs. We show an example in Figure 1 for
the sentence We can feel the blood coursing
through our veins again. We represent verbs
(i.e., frame evoking elements) in the seed and
unlabeled corpora by their predicate-argument
structure. Specifically, we record the direct de-
pendents of the predicate course (e.g., blood
or again in Figure 1) and their grammatical
roles (e.g., SUBJ, MOD). Prepositional nodes
are collapsed, i.e., we record the preposition?s
object and a composite grammatical role (like
IOBJ THROUGH, where IOBJ stands for ?preposi-
tional object? and THROUGH for the preposition
itself). In addition to direct dependents, we also
222
Lemma GramRole SemRole
blood SUBJ Fluid
vein IOBJ THROUGH Path
again MOD ?
Table 1: Predicate-argument structure for the verb
course in Figure 1.
consider nodes coordinated with the predicate as
arguments. Finally, for each argument node we
record the semantic roles it carries, if any. All sur-
face word forms are lemmatized. An example of
the argument structure information we obtain for
the predicate course (see Figure 1) is shown in Ta-
ble 1.
We obtain information about grammatical roles
from the output of RASP (Briscoe et al, 2006),
a broad-coverage dependency parser. However,
there is nothing inherent in our method that re-
stricts us to this particular parser. Any other
parser with broadly similar dependency output
could serve our purposes.
3.2 Measuring Similarity
For each frame evoking verb in the seed corpus our
method creates a labeled predicate-argument re-
presentation. It also extracts all sentences from the
unlabeled corpus containing the same verb. Not
all of these sentences will be suitable instances
for adding to our training data. For example, the
same verb may evoke a different frame with dif-
ferent roles and argument structure. We therefore
must select sentences which resemble the seed an-
notations. Our hypothesis is that verbs appearing
in similar syntactic and semantic contexts will be-
have similarly in the way they relate to their argu-
ments.
Estimating the similarity between two predi-
cate argument structures amounts to finding the
highest-scoring alignment between them. More
formally, given a labeled predicate-argument
structure pl with m arguments and an unla-
beled predicate-argument structure pu with n ar-
guments, we consider (and score) all possible
alignments between these arguments. A (partial)
alignment can be viewed as an injective function
? : M? ? {1, . . . , n} where M? ? {1, . . . ,m}.
In other words, an argument i of pl is aligned to
argument ?(i) of pu if i ? M?. Note that this al-
lows for unaligned arguments on both sides.
We score each alignment ? using a similarity
function sim(?) defined as:
?
i?M?
(
A ? syn(gli, g
u
?(i)) + sem(w
l
i, w
u
?(i))?B
)
where syn(gli, g
u
?(i)) denotes the syntactic similar-
ity between grammatical roles gli and g
u
?(i) and
sem(wli, w
u
?(i)) the semantic similarity between
head words wli and w
u
?(i).
Our goal is to find an alignment such
that the similarity function is maximized:
?? := argmax
?
sim(?). This optimization
problem is a generalized version of the linear
assignment problem (Dantzig, 1963). It can be
straightforwardly expressed as a linear program-
ming problem by associating each alignment ?
with a set of binary indicator variables xij :
xij :=
{
1 if i ? M? ? ?(i) = j
0 otherwise
The similarity objective function then becomes:
m?
i=1
n?
j=1
(
A ? syn(gli, g
u
j ) + sem(w
l
i, w
u
j )?B
)
xij
subject to the following constraints ensuring that ?
is an injective function on some M?:
n?
j=1
xij ? 1 for all i = 1, . . . ,m
m?
i=1
xij ? 1 for all j = 1, . . . , n
Figure 2 graphically illustrates the alignment
projection problem. Here, we wish to project
semantic role information from the seed blood
coursing through our veins again onto the un-
labeled sentence Adrenalin was still coursing
through her veins. The predicate course has three
arguments in the labeled sentence and four in the
unlabeled sentence (represented as rectangles in
the figure). There are 73 possible alignments in
this example. In general, for any m and n argu-
ments, where m ? n, the number of alignments
is
?m
k=0
m!n!
(m?k)!(n?k)!k! . Each alignment is scored
by taking the sum of the similarity scores of the in-
dividual alignment pairs (e.g., between blood and
be, vein and still ). In this example, the highest
scoring alignment is between blood and adrenalin,
vein and vein, and again and still, whereas be is
223
left unaligned (see the non-dotted edges in Fig-
ure 2). Note that only vein and blood carry seman-
tic roles (i.e., Fluid and Path) which are projected
onto adrenalin and vein, respectively.
Finding the best alignment crucially depends
on estimating the syntactic and semantic similar-
ity between arguments. We define the syntactic
measure on the grammatical relations produced
by RASP. Specifically, we set syn(gli, g
u
?(i)) to 1
if the relations are identical, to a ? 1 if the rela-
tions are of the same type but different subtype2
and to 0 otherwise. To avoid systematic errors,
syntactic similarity is also set to 0 if the predicates
differ in voice. We measure the semantic similar-
ity sem(wli, w
u
?(i)) with a semantic space model.
The meaning of each word is represented by a vec-
tor of its co-occurrences with neighboring words.
The cosine of the angle of the vectors represent-
ingwl andwu quantifies their similarity (Section 4
describes the specific model we used in our exper-
iments in more detail).
The parameter A counterbalances the impor-
tance of syntactic and semantic information, while
the parameter B can be interpreted as the lowest
similarity value for which an alignment between
two arguments is possible. An optimal align-
ment ?? cannot link arguments i0 of pl and j0
of pu, if A ? syn(gli0 , g
u
j0) + sem(w
l
i0 , w
u
j0) < B
(i.e., either i0 /? M?? or ??(i0) 6= j0). This
is because for an alignment ? with ?(i0) = j0
we can construct a better alignment ?0, which is
identical to ? on all i 6= i0, but leaves i0 un-
aligned (i.e., i0 /? M?0). By eliminating a neg-
ative term from the scoring function, it follows
that sim(?0) > sim(?). Therefore, an alignment ?
satisfying ?(i0) = j0 cannot be optimal and con-
versely the optimal alignment ?? can never link
two arguments with each other if the sum of their
weighted syntactic and semantic similarity scores
is below B.
3.3 Projecting Annotations
Once we obtain the best alignment ?? between pl
and pu, we can simply transfer the role of each
role-bearing argument i of pl to the aligned argu-
ment ??(i) of pu, resulting in a labeling of pu.
To increase the accuracy of our method we dis-
card projections if they fail to transfer all roles
of the labeled to the unlabeled dependency graph.
2This concerns fine-grained distinctions made by the
parser, e.g., the underlying grammatical roles in passive con-
structions.
Fluid //___ bloodSUBJ
//
!!


adrenalin
SUBJ
Path //___ veinIOBJ THROUGH
==
//
!!

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 be
AUX
again
MOD
FF
==
//
!!
still
MOD
vein
IOBJ THROUGH
Figure 2: Alignments between the argument
structures representing the clauses blood coursing
through our veins again and Adrenalin was still
coursing through her veins; non-dotted lines illus-
trate the highest scoring alignment.
This can either be the case if pl does not cover all
roles annotated on the graph (i.e., there are role-
bearing nodes which we do not recognize as argu-
ments of the frame evoking verb) or if there are
unaligned role-bearing arguments (i.e., i /? M??
for a role-bearing argument i of pl).
The remaining projections form our expan-
sion corpus. For each seed instance we select
the k most similar neighbors to add to our training
data. The parameter k controls the trade-off be-
tween annotation confidence and expansion size.
4 Experimental Setup
In this section we discuss our experimental setup
for assessing the usefulness of the method pre-
sented above. We give details on our training pro-
cedure and parameter estimation, describe the se-
mantic labeler we used in our experiments and ex-
plain how its output was evaluated.
Corpora Our seed corpus was taken from
FrameNet. The latter contains approximately
2,000 verb entries out of which we randomly se-
lected a sample of 100. We next extracted all an-
notated sentences for each of these verbs. These
sentences formed our gold standard corpus, 20%
of which was reserved as test data. We used
the remaining 80% as seeds for training purposes.
We generated seed corpora of various sizes by
randomly reducing the number of annotation in-
stances per verb to a maximum of n. An addi-
tional (non-overlapping) random sample of 100
verbs was used as development set for tuning the
parameters for our method. We gathered unla-
beled sentences from the BNC.
224
The seed and unlabeled corpora were parsed
with RASP (Briscoe et al, 2006). The FrameNet
annotations in the seed corpus were converted
into dependency graphs (see Figure 1) using the
method described in Fu?rstenau (2008). Briefly,
the method works by matching nodes in the de-
pendency graph with role bearing substrings in
FrameNet. It first finds the node in the graph
which most closely matches the frame evoking
element in FrameNet. Next, individual graph
nodes are compared against labeled substrings in
FrameNet to transfer all roles onto their closest
matching graph nodes.
Parameter Estimation The similarity function
described in Section 3.2 has three free parameters.
These are the weight A which determines the rel-
ative importance of syntactic and semantic infor-
mation, the parameter B which determines when
two arguments cannot be aligned and the syntactic
score a for almost identical grammatical roles. We
optimized these parameters on the development
set using Powell?s direction set method (Brent,
1973) with F1 as our loss function. The optimal
values for A, B and a were 1.76, 0.41 and 0.67,
respectively.
Our similarity function is further parametrized
in using a semantic space model to compute the
similarity between two words. Considerable lat-
itude is allowed in specifying the parameters of
vector-based models. These involve the defi-
nition of the linguistic context over which co-
occurrences are collected, the number of com-
ponents used (e.g., the k most frequent words
in a corpus), and their values (e.g., as raw co-
occurrence frequencies or ratios of probabilities).
We created a vector-based model from a lem-
matized version of the BNC. Following previ-
ous work (Bullinaria and Levy, 2007), we opti-
mized the parameters of our model on a word-
based semantic similarity task. The task involves
examining the degree of linear relationship be-
tween the human judgments for two individual
words and vector-based similarity values. We ex-
perimented with a variety of dimensions (ranging
from 50 to 500,000), vector component definitions
(e.g., pointwise mutual information or log likeli-
hood ratio) and similarity measures (e.g., cosine or
confusion probability). We used WordSim353, a
benchmark dataset (Finkelstein et al, 2002), con-
sisting of relatedness judgments (on a scale of 0
to 10) for 353 word pairs.
We obtained best results with a model using a
context window of five words on either side of the
target word, the cosine measure, and 2,000 vec-
tor dimensions. The latter were the most com-
mon context words (excluding a stop list of func-
tion words). Their values were set to the ratio of
the probability of the context word given the tar-
get word to the probability of the context word
overall. This configuration gave high correlations
with the WordSim353 similarity judgments using
the cosine measure.
Solving the Linear Program A variety of algo-
rithms have been developed for solving the linear
assignment problem efficiently. In our study, we
used the simplex algorithm (Dantzig, 1963). We
generate and solve an LP of every unlabeled sen-
tence we wish to annotate.
Semantic role labeler We evaluated our method
on a semantic role labeling task. Specifically, we
compared the performance of a generic seman-
tic role labeler trained on the seed corpus and
a larger corpus expanded with annotations pro-
duced by our method. Our semantic role labeler
followed closely the implementation of Johans-
son and Nugues (2008). We extracted features
from dependency parses corresponding to those
routinely used in the semantic role labeling liter-
ature (see Baker et al (2007) for an overview).
SVM classifiers were trained to identify the argu-
ments and label them with appropriate roles. For
the latter we performed multi-class classification
following the one-versus-one method3 (Friedman,
1996). For the experiments reported in this paper
we used the LIBLINEAR library (Fan et al, 2008).
The misclassification penalty C was set to 0.1.
To evaluate against the test set, we linearized
the resulting dependency graphs in order to obtain
labeled role bracketings like those in example (1)
and measured labeled precision, labeled recall and
labeled F1. (Since our focus is on role labeling and
not frame prediction, we let our role labeler make
use of gold standard frame annotations, i.e., label-
ing of frame evoking elements with frame names.)
5 Results
The evaluation of our method was motivated by
three questions: (1) How do different training set
sizes affect semantic role labeling performance?
3Given n classes the one-versus-one method builds
n(n? 1)/2 classifiers.
225
TrainSet Size Prec (%) Rec (%) F1 (%)
0-NN 849 35.5 42.0 38.5
1-NN 1205 36.4 43.3 39.5
2-NN 1549 38.1 44.1 40.9?
3-NN 1883 37.9 43.7 40.6?
4-NN 2204 38.0 43.9 40.7?
5-NN 2514 37.4 43.9 40.4?
self train 1609 34.0 41.0 37.1
Table 2: Semantic role labeling performance using
different amounts of training data; the seeds are
expanded with their k nearest neighbors; ?: F1 is
significantly different from 0-NN (p < 0.05).
Training size varies depending on the number of
unlabeled sentences added to the seed corpus. The
quality of these sentences also varies depending
on their similarity to the seed sentences. So,
we would like to assess whether there is a trade-
off between annotation quality and training size.
(2) How does the size of the seed corpus influence
role labeling performance? Here, we are interested
to find out what is the least amount of manual
annotation possible for our method to have some
positive impact. (3) And finally, what are the an-
notation savings our method brings?
Table 2 shows the performance of our semantic
role labeler when trained on corpora of different
sizes. The seed corpus was reduced to at most 10
instances per verb. Each row in the table corre-
sponds to adding the k nearest neighbors of these
instances to the training data. When trained solely
on the seed corpus the semantic role labeler yields
a (labeled) F1 of 38.5%, (labeled) recall is 42.0%
and (labeled) precision is 35.5% (see row 0-NN
in the table). All subsequent expansions yield
improved precision and recall. In all cases ex-
cept k = 1 the improvement is statistically signif-
icant (p < 0.05). We performed significance test-
ing onF1 using stratified shuffling (Noreen, 1989),
an instance of assumption-free approximative ran-
domization testing. As can be seen, the optimal
trade-off between the size of the training corpus
and annotation quality is reached with two nearest
neighbors. This corresponds roughly to doubling
the number of training instances. (Due to the re-
strictions mentioned in Section 3.3 a 2-NN expan-
sion does not triple the number of instances.)
We also compared our results against a self-
training procedure (see last row in Table 2). Here,
we randomly selected unlabeled sentences corre-
sponding in number to a 2-NN expansion, labeled
them with our role labeler, added them to the train-
ing set, and retrained. Self-training resulted in per-
formance inferior to the baseline of adding no un-
labeled data at all (see the first row in Table 2).
Performance decreased even more with the addi-
tion of more self-labeled instances. These results
indicate that the similarity function is crucial to the
success of our method.
An example of the annotations our method pro-
duces is given below. Sentence (2a) is the seed.
Sentences (2b)?(2e) are its most similar neighbors.
The sentences are presented in decreasing order of
similarity.
(2) a. [He]Theme stared and came
[slowly]Manner [towards me]Goal.
b. [He]Theme had heard the shooting
and come [rapidly]Manner [back to-
wards the house]Goal.
c. Without answering, [she]Theme left
the room and came [slowly]Manner
[down the stairs]Goal.
d. [Then]Manner [he]Theme won?t come
[to Salisbury]Goal.
e. Does [he]Theme always come round
[in the morning]Goal [then]Manner?
As we can see, sentences (2b) and (2c) accu-
rately identify the semantic roles of the verb come
evoking the frame Arriving. In (2b) He is la-
beled as Theme, rapidly as Manner, and towards
the house as Goal. Analogously, in (2c) she is
the Theme, slowly is Manner and down the stairs
is Goal. The quality of the annotations decreases
with less similar instances. In (2d) then is marked
erroneously as Manner, whereas in (2e) only the
Theme role is identified correctly.
To answer our second question, we varied the
size of the training corpus by varying the num-
ber of seeds per verb. For these experiments we
fixed k = 2. Table 3 shows the performance of the
semantic role labeler when the seed corpus has one
annotation per verb, five annotations per verb, and
so on. (The results for 10 annotations are repeated
from Table 2). With 1, 5 or 10 instances per verb
our method significantly improves labeling perfor-
mance. We observe improvements in F1 of 1.5%,
2.1%, and 2.4% respectively when adding the 2
most similar neighbors to these training corpora.
Our method also improves F1 when a 20 seeds
226
TrainSet Size Prec (%) Rec (%) F1 (%)
? 1 seed 95 24.9 31.3 27.7
+ 2-NN 170 26.4 32.6 29.2?
? 5 seeds 450 29.7 38.4 33.5
+ 2-NN 844 31.8 40.4 35.6?
? 10 seeds 849 35.5 42.0 38.5
+ 2-NN 1549 38.1 44.1 40.9?
? 20 seeds 1414 38.7 46.1 42.1
+ 2-NN 2600 40.5 46.7 43.4
all seeds 2323 38.3 47.0 42.2
+ 2-NN 4387 39.5 46.7 42.8
Table 3: Semantic role labeling performance us-
ing different numbers of seed instances per verb in
the training corpus; the seeds are expanded with
their k = 2 nearest neighbors; ?: F1 is signifi-
cantly different from seed corpus (p < 0.05).
corpus or all available seeds are used, however the
difference is not statistically significant.
The results in Table 3 also allow us to draw
some conclusions regarding the relative quality
of manual and automatic annotation. Expand-
ing a seed corpus with 10 instances per verb im-
proves F1 from 38.5% to 40.9%. We can com-
pare this to the labeler?s performance when trained
solely on the 20 seeds corpus (without any ex-
pansion). The latter has approximately the same
size as the expanded 10 seeds corpus. Interest-
ingly, F1 on this exclusively hand-annotated cor-
pus is only 1.2% better than on the expanded cor-
pus. So, using our expansion method on a 10 seeds
corpus performs almost as well as using twice as
many manual annotations. Even in the case of the
5 seeds corpus, where there is limited informa-
tion for our method to expand from, we achieve
an improvement from 33.5% to 35.6%, compared
to 38.5% for manual annotation of about the same
number of instances. In sum, while additional
manual annotation is naturally more effective for
improving the quality of the training data, we can
achieve substantial proportions of these improve-
ments by automatic expansion alone. This is a
promising result suggesting that it is possible to
reduce annotation costs without drastically sacri-
ficing quality.
6 Conclusions
This paper presents a novel method for reducing
the annotation effort involved in creating resources
for semantic role labeling. Our strategy is to ex-
pand a manually annotated corpus by projecting
semantic role information from labeled onto un-
labeled instances. We formulate the projection
problem as an instance of the linear assignment
problem. We seek to find role assignments that
maximize the similarity between labeled and un-
labeled instances. Similarity is measured in terms
of structural and semantic compatibility between
argument structures.
Our method improves semantic role labeling
performance in several experimental conditions. It
is especially effective when a small number of an-
notations is available for each verb. This is typi-
cally the case when creating frame semantic cor-
pora for new languages or new domains. Our ex-
periments show that expanding such corpora with
our method can yield almost the same relative im-
provement as using exclusively manual annota-
tion.
In the future we plan to extend our method
in order to handle novel verbs that are not at-
tested in the seed corpus. Another direction con-
cerns the systematic modeling of diathesis alter-
nations (Levin, 1993). These are currently only
captured implicitly by our method (when the se-
mantic similarity overrides syntactic dissimilar-
ity). Ideally, we would like to be able to system-
atically identify changes in the realization of the
argument structure of a given predicate. Although
our study focused solely on FrameNet annotations,
we believe it can be adapted to related annotation
schemes, such as PropBank. An interesting ques-
tion is whether the improvements obtained by our
method carry over to other role labeling frame-
works.
Acknowledgments The authors acknowledge
the support of DFG (IRTG 715) and EPSRC
(grant GR/T04540/01). We are grateful to
Richard Johansson for his help with the re-
implementation of his semantic role labeler.
References
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 19: Frame Semantic
Structure Extraction. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 99?104, Prague, Czech Republic.
R. P. Brent. 1973. Algorithms for Minimization with-
out Derivatives. Prentice-Hall, Englewood Cliffs,
NJ.
227
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The Second Release of the RASP System. In Pro-
ceedings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, pages 77?80, Sydney, Australia.
J. A. Bullinaria and J. P. Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
George B. Dantzig. 1963. Linear Programming and
Extensions. Princeton University Press, Princeton,
NJ, USA.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classification. Journal of Machine Learning
Research, 9:1871?1874.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Jerome H. Friedman. 1996. Another approach to poly-
chotomous classification. Technical report, Depart-
ment of Statistics, Stanford University.
Pascale Fung and Benfeng Chen. 2004. BiFrameNet:
Bilingual frame semantics resources construction
by cross-lingual induction. In Proceedings of the
20th International Conference on Computational
Linguistics, pages 931?935, Geneva, Switzerland.
Hagen Fu?rstenau. 2008. Enriching frame semantic re-
sources with dependency graphs. In Proceedings of
the 6th Language Resources and Evaluation Confer-
ence, Marrakech, Morocco.
Daniel Gildea and Dan Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguis-
tics, 28:3:245?288.
Andrew Gordon and Reid Swanson. 2007. General-
izing semantic role annotations across syntactically
similar verbs. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 192?199, Prague, Czech Republic.
Richard Johansson and Pierre Nugues. 2006. A
FrameNet-based semantic role labeler for Swedish.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 436?443, Syd-
ney, Australia.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role la-
beling. In Proceedings of the 22nd International
Conference on Computational Linguistics, pages
393?400, Manchester, UK.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Alessandro Moschitti, Paul Morarescu, and Sanda
Harabagiu. 2003. Open-domain information extrac-
tion via automatic semantic labeling. In Proceed-
ings of FLAIRS 2003, pages 397?401, St. Augustine,
FL.
E. Noreen. 1989. Computer-intensive Methods for
Testing Hypotheses: An Introduction. John Wiley
and Sons Inc.
Sebastian Pado? and Katrin Erk. 2005. To cause
or not to cause: Cross-lingual semantic matching
for paraphrase modelling. In Proceedings of the
EUROLAN Workshop on Cross-Linguistic Knowl-
edge Induction, pages 23?30, Cluj-Napoca, Roma-
nia.
Sebastian Pado? and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for seman-
tic projection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 1161?1168, Sydney,
Australia.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 31(1):71?
106.
Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of FrameNet lexical units. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 457?465, Honolulu,
Hawaii.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceed-
ings of the joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Computational Natural Language Learning, pages
12?21, Prague, Czech Republic.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8?15, Sap-
poro, Japan.
Robert S. Swier and Suzanne Stevenson. 2004. Un-
supervised semantic role labelling. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 95?102. Bacelona,
Spain.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Pro-
ceedings of the joint Human Language Technology
Conference and Conference on Empirical Methods
in Natural Language Processing, pages 371?378,
Vancouver, BC.
228
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 782?792,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Robust Disambiguation of Named Entities in Text
Johannes Hoffart1, Mohamed Amir Yosef1, Ilaria Bordino2, Hagen Fu?rstenau3,
Manfred Pinkal3, Marc Spaniol1, Bilyana Taneva1, Stefan Thater3, Gerhard Weikum1
1 Max Planck Institute for Informatics, Saarbru?cken, Germany
2 Yahoo! Research Lab, Barcelona, Spain
3 Saarland University, Saarbru?cken, Germany
{jhoffart,mamir,mspaniol,btaneva,weikum}@mpi-inf.mpg.de
bordino@yahoo-inc.com {hagenf,pinkal,stth}@coli.uni-sb.de
Abstract
Disambiguating named entities in natural-
language text maps mentions of ambiguous
names onto canonical entities like people or
places, registered in a knowledge base such as
DBpedia or YAGO. This paper presents a ro-
bust method for collective disambiguation, by
harnessing context from knowledge bases and
using a new form of coherence graph. It unifies
prior approaches into a comprehensive frame-
work that combines three measures: the prior
probability of an entity being mentioned, the
similarity between the contexts of a mention
and a candidate entity, as well as the coherence
among candidate entities for all mentions to-
gether. The method builds a weighted graph of
mentions and candidate entities, and computes
a dense subgraph that approximates the best
joint mention-entity mapping. Experiments
show that the new method significantly outper-
forms prior methods in terms of accuracy, with
robust behavior across a variety of inputs.
1 Introduction
1.1 Motivation
Web pages, news articles, blog postings, and other
Internet data contain mentions of named entities such
as people, places, organizations, etc. Names are often
ambiguous: the same name can have many different
meanings. For example, given a text like ?They per-
formed Kashmir, written by Page and Plant. Page
played unusual chords on his Gibson.?, how can we
tell that ?Kashmir? denotes a song by Led Zeppelin
and not the Himalaya region (and that Page refers
to guitarist Jimmy Page and not to Google founder
Larry Page, and that Gibson is a guitar model rather
than the actor Mel Gibson)?
Establishing these mappings between the mentions
and the actual entities is the problem of named-entity
disambiguation (NED).
If the possible meanings of a name are known up-
front - e.g., by using comprehensive gazetteers such
as GeoNames (www.geonames.org) or knowledge
bases such as DBpedia (Auer07), Freebase (www.
freebase.com), or YAGO (Suchanek07), which
have harvested Wikipedia redirects and disambigua-
tion pages - then the simplest heuristics for name res-
olution is to choose the most prominent entity for a
given name. This could be the entity with the longest
Wikipedia article or the largest number of incoming
links in Wikipedia; or the place with the most inhab-
itants (for cities) or largest area, etc. Alternatively,
one could choose the entity that uses the mention
most frequently as a hyperlink anchor text. For the
example sentence given above, all these techniques
would incorrectly map the mention ?Kashmir? to the
Himalaya region. We refer to this suite of methods
as a popularity-based (mention-entity) prior.
Key to improving the above approaches is to con-
sider the context of the mention to be mapped, and
compare it - by some similarity measure - to contex-
tual information about the potential target entities.
For the example sentence, the mention ?Kashmir?
has context words like ?performed? and ?chords? so
that we can compare a bag-of-words model against
characteristic words in the Wikipedia articles of the
different candidate entities (by measures such as co-
sine similarity, weighted Jaccard distance, KL diver-
gence, etc.). The candidate entity with the highest
similarity is chosen. Alternatively, labeled training
data can be harnessed to learn a multi-way classifier,
and additional features like entire phrases, part-of-
speech tags, dependency-parsing paths, or nearby
782
hyperlinks can be leveraged as well. These methods
work well for sufficiently long and relatively clean
input texts such as predicting the link target of a Wi-
kipedia anchor text (Milne08). However, for short or
more demanding inputs like news, blogs, or arbitrary
Web pages, relying solely on context similarity can-
not achieve near-human quality. Similarity measures
based on syntactically-informed distributional mod-
els require minimal context only. They have been
developed for common nouns and verbs (Thater10),
but not applied to named entities.
The key to further improvements is to jointly con-
sider multiple mentions in an input and aim for a col-
lective assignment onto entities (Kulkarni09). This
approach should consider the coherence of the re-
sulting entities, in the sense of semantic relatedness,
and it should combine such measures with the con-
text similarity scores of each mention-entity pair. In
our example, one should treat ?Page?, ?Plant? and
?Gibson? also as named-entity mentions and aim to
disambiguate them together with ?Kashmir?.
Collective disambiguation works very well when a
text contains mentions of a sufficiently large number
of entities within a thematically homogeneous con-
text. If the text is very short or is about multiple, un-
related or weakly related topics, collective mapping
tends to produce errors by directing some mentions
towards entities that fit into a single coherent topic
but do not capture the given text. For example, a text
about a football game between ?Manchester? and
?Barcelona? that takes place in ?Madrid? may end up
mapping either all three of these mentions onto foot-
ball clubs (i.e., Manchester United, FC Barcelona,
Real Madrid) or all three of them onto cities. The
conclusion here is that none of the prior methods
for named-entity disambiguation is robust enough to
cope with such difficult inputs.
1.2 Contribution
Our approach leverages recently developed knowl-
edge bases like YAGO as an entity catalog and a
rich source of entity types and semantic relationships
among entities. These are factored into new measures
for the similarity and coherence parts of collectively
disambiguating all mentions in an input text. For
similarity, we also explore an approach that lever-
ages co-occurrence information obtained from large,
syntactically parsed corpora (Thater10).
We cast the joint mapping into the following graph
problem: mentions from the input text and candidate
entities define the node set, and we consider weighted
edges between mentions and entities, capturing con-
text similarities, and weighted edges among entities,
capturing coherence. The goal on this combined
graph is to identify a dense subgraph that contains
exactly one mention-entity edge for each mention,
yielding the most likely disambiguation. Such graph
problems are NP-hard, as they generalize the well-
studied Steiner-tree problem. We develop a greedy
algorithm that provides high-quality approximations,
and is customized to the properties of our mention-
entity graph model.
In addition to improving the above assets for the
overall disambiguation task, our approach gains in
robustness by using components selectively in a self-
adapting manner. To this end, we have devised the
following multi-stage procedure.
? For each mention, we compute popularity priors
and context similarities for all entity candidates
as input for our tests.
? We use a threshold test on the prior to decide
whether popularity should be used (for mentions
with a very high prior) or disregarded (for men-
tions with several reasonable candidates).
? When both the entity priors and the context simi-
larities are reasonably similar in distribution for
all the entity candidates, we keep the best candi-
date and remove all others, fixing this mention
before running the coherence graph algorithm.
We then run the coherence graph algorithm on all
the mentions and their remaining entity candidates.
This way, we restrict the coherence graph algorithm
to the critical mentions, in situations where the goal
of coherence may be misleading or would entail high
risk of degradation.
The paper makes the following novel contribu-
tions: 1) a framework for combining popularity pri-
ors, similarity measures, and coherence into a robust
disambiguation method; 2) new measures for defin-
ing mention-entity similarity; 3) a new algorithm
for computing dense subgraphs in a mention-entity
graph, which produces high-quality mention-entity
mappings; 4) an empirical evaluation on a demand-
ing corpus (based on additional annotations for the
dataset of the CoNLL 2003 NER task), with signifi-
783
cant improvements over state-of-the-art opponents.
2 State of the Art
Recognizing named entities (NER tagging) in natural-
language text has been extensively addressed in NLP
research. The output is labeled noun phrases. How-
ever, these are not yet canonical entities, explicitly
and uniquely denoted in a knowledge repository.
Approaches that use Wikipedia for explicit disam-
biguation date back to (Bunescu06) and have been
further pursued by (Cucerzan07; Han09; Milne08;
Nguyen08; Mihalcea07). (Bunescu06) defined a sim-
ilarity measure that compared the context of a men-
tion to the Wikipedia categories of an entity candi-
date. (Cucerzan07; Milne08; Nguyen08) extended
this framework by using richer features for the simi-
larity comparison. (Milne08) additionally introduced
a supervised classifier for mapping mentions to en-
tities, with learned feature weights rather than using
the similarity function directly. (Milne08) introduced
a notion of semantic relatedness between a mention?s
candidate entities and the unambiguous mentions in
the textual context. The relatedness values are de-
rived from the overlap of incoming links in Wikipedia
articles. (Han09) considered another feature: the re-
latedness of common noun phrases in a mention?s
context, matched against Wikipedia article names.
While these features point towards semantic coher-
ence, the approaches are still limited to mapping each
mention separately. Nonetheless, this line of feature-
rich similarity-driven methods achieved very good
results in experiments, especially for the task of pre-
dicting Wikipedia link targets for a given href anchor
text. On broader input classes such as news articles
(called ?wikification in the wild? in (Milne08)), the
precision was reported to be about 75 percent.
The first work with an explicit collective-learning
model for joint mapping of all mentions has been
(Kulkarni09). This method starts with a supervised
learner for a similarity prior, and models the pair-
wise coherence of entity candidates for two different
mentions as a probabilistic factor graph with all pairs
as factors. The MAP (maximum a posteriori) es-
timator for the joint probability distribution of all
mappings is shown to be an NP-hard optimization
problem, so that (Kulkarni09) resorts to approxima-
tions and heuristics like relaxing an integer linear
program (ILP) into an LP with subsequent round-
ing or hill-climbing techniques. The experiments in
(Kulkarni09) show that this method is superior to the
best prior approaches, most notably (Milne08). How-
ever, even approximate solving of the optimization
model has high computational costs.
Coreference resolution is the task of mapping
mentions like pronouns or short phrases to a pre-
ceding, more explicit, mention. Recently, interest
has arisen in cross-document coreference resolution
(Mayfield09), which comes closer to NED, but does
not aim at mapping names onto entities in a knowl-
edge base. Word sense disambiguation (McCarthy09;
Navigli09) is the more general task of mapping con-
tent words to a predefined inventory of word senses.
While the NED problem is similar, it faces the chal-
lenges that the ambiguity of entity names tends to be
much higher (e.g., mentions of common lastnames
or firstname-only).
Projects on automatically building knowledge
bases (Doan08) from natural-language text include
KnowItAll (Banko07), YAGO and its tool SOFIE
(Suchanek09; Nakashole11), StatSnowball (Zhu09),
ReadTheWeb (Carlson10), and the factor-graph work
by (Wick09). Only SOFIE maps names onto canon-
ical entities; the other projects produce output with
ambiguous names. SOFIE folds the NED into its
MaxSat-based reasoning for fact extraction. This ap-
proach is computationally expensive and not intended
for online disambiguation of entire texts.
3 Framework
Mentions and Ambiguity: We consider an input
text (Web page, news article, blog posting, etc.) with
mentions (i.e., surface forms) of named entities (peo-
ple, music bands, songs, universities, etc.) and aim
to map them to their proper entries in a knowledge
base, thus giving a disambiguated meaning to entity
mentions in the text. We first identify noun phrases
that potentially denote named entities. We use the
Stanford NER Tagger (Finkel05) to discover these
and segment the text accordingly.
Entity Candidates: For possible entities (with
unique canonical names) that a mention could denote,
we harness existing knowledge bases like DBpedia
or YAGO. For each entity they provide a set of short
names (e.g., ?Apple? for Apple Inc. and para-
784
phrases (e.g., ?Big Apple? for New York City).
In YAGO, these are available by the means relation,
which in turn is harvested from Wikipedia disam-
biguation pages, redirects, and links.
Popularity Prior for Entities: Prominence or
popularity of entities can be seen as a probabilistic
prior for mapping a name to an entity. The most com-
mon way of estimating this are the Wikipedia-based
frequencies of particular names in link anchor texts
referring to specific entities, or number of inlinks.
Context Similarity of Mentions and Entities:
The key for mapping mentions onto entities are the
contexts on both sides of the mapping. We consider
two different approaches. First, for each mention,
we construct a context from all words in the entire
input text. This way, we can represent a mention
as a set of (weighted) words or phrases that it co-
occurs with. Second, we alternatively consider simi-
larity scores based on syntactically-parsed contexts,
based on (Thater10). On the entity side of the map-
ping, we associate each entity with characteristic
keyphrases or salient words, precomputed from Wi-
kipedia articles and similar sources. For example,
Larry Page would have keyphrases like ?Stan-
ford?, ?search engine?, etc., whereas Jimmy Page
may have keyphrases ?Gibson guitar?, ?hard rock?,
etc. Now we can define and compute similarity mea-
sures between a mention and an entity candidate,
e.g., the weighted word overlap, the KL divergence,
n-gram-based measures, etc. In addition, we may
use syntactic contextualization techniques, based on
dependency trees, that suggest phrases that are typi-
cally used with the same verb that appears with the
mention in the input text (Thater10).
Coherence among Entities: On the entity side,
each entity has a context in the underlying knowl-
edge base(s): other entities that are connected via
semantic relationships (e.g., memberOf) or have the
same semantic type (e.g., rock musician). An
asset that knowledge bases like DBpedia and YAGO
provide us with is the same-as cross-referencing to
Wikipedia. This way, we can quantify the coherence
between two entities by the number of incoming links
that their Wikipedia articles share. When we consider
candidate entities for different mentions, we can now
define and compute a notion of coherence among the
corresponding entities, e.g., by the overlap among
their related entities or some form of type distance.
Coherence is a key asset because most texts deal with
a single or a few semantically related topics such as
rock music or Internet technology or global warming,
but not everything together.
Overall Objective Function: To aim for the best
disambiguation mappings, our framework combines
prior, similarity, and coherence measures into a
combined objective function: for each mention mi,
i = 1..k, select entity candidates eji , one per men-
tion, such that
? ?
?
i=1..k
prior(mi, eji)+
? ?
?
i=1..k
sim(cxt(mi), cxt(eji))+
? ? coh(ej1 ? cnd(m1) . . . ejk ? cnd(mk)) = max!
where ? + ? + ? = 1, cnd(mi) is the set of pos-
sible meanings of mi, cxt( ) denotes the context of
mentions and entities, respectively, and coh( ) is the
coherence function for a set of entities.
Section 4 gives details on each of these three com-
ponents. For robustness, our solution selectively en-
ables or disables the three components, based on tests
on the mentions of the input text; see Section 5.
4 Features and Measures
4.1 Popularity Prior
As mentioned above, our framework supports multi-
ple forms of popularity-based priors, but we found a
model based on Wikipedia link anchors to be most
effective: For each surface form that constitutes an
anchor text, we count how often it refers to a partic-
ular entity. For each name, these counts provide us
with an estimate for a probability distribution over
candidate entities. For example, ?Kashmir? refers to
Kashmir (the region) in 90.91% of all occurrences
and in 5.45% to Kashmir (Song).
4.2 Mention-Entity Similarity
Keyphrase-based Similarity: On the mention side,
we use all tokens in the document (except stopwords
and the mention itself) as context. We experimented
with a distance discount to discount the weight of
tokens that are further away, but this did not improve
the results for our test data.
On the entity side, the knowledge base knows au-
thoritative sources for each entity, for example, the
785
corresponding Wikipedia article or an organizational
or individual homepage. These are the inputs for
an offline data-mining step to determine character-
istic keyphrases for each entity and their statistical
weights. We describe this only for Wikipedia as in-
put corpus, the approach extends to other inputs. As
keyphrase candidates for an entity we consider its
corresponding Wikipedia article?s link anchors texts,
including category names, citation titles, and external
references. We extended this further by considering
also the titles of articles linking to the entity?s article.
All these phrases form the keyphrase set of an entity:
KP (e).
For each word w that occurs in a keyphrase, we
compute a specificity weight with regard to the given
entity: the MI (mutual information) between the en-
tity e and the keyword w, calculating the joint proba-
bilities for MI as follows:
p(e, w) =
??w ?
(
KP (e) ??e??INe KP (e?)
)??
N
reflecting if w is contained in the keyphrase set of e
or any of the keyphrase sets of an entity linking to e,
IN(e), with N denoting the total number of entities.
The joint probabilities for the cases p(e, w?), p(e?, w),
p(e?, w?) are calculated accordingly.
Keyphrases may occur only partially in an input
text. For example, the phrase ?Grammy Award win-
ner? associated with entity Jimmy Page may oc-
cur only in the form ?Grammy winner? near some
mention ?Page?. Therefore, our algorithm for the
similarity of mention m with regard to entity e com-
putes partial matches of e?s keyphrases in the text.
This is done by matching individual words and re-
warding their proximity in an appropriate score. To
this end we compute, for each keyphrase, the shortest
window of words that contains a maximal number
of words of the keyphrase. We refer to this window
as the phrase?s cover (cf. (Taneva11)). For example,
matching the text ?winner of many prizes including
the Grammy? results in a cover length of 7 for the
keyphrase ?Grammy award winner?. By this ratio-
nale, the score of partially matching phrase q in a text
is set to:
score(q) = z
(?
w?cover weight(w)?
w?q weight(w)
)2
where z = # matching wordslength of cover(q) andweight(w) is eitherthe MI weight (defined above) or the collection-wide
IDF weight of the keyphrase word w. Note that the
second factor is squared, so that there is a superlinear
reduction of the score for each word that is missing
in the cover.
For the similarity of a mention m to candidate
entity e, this score is aggregated over all keyphrases
of e and all their partial matches in the text, leading
to the similarity score
simscore(m, e) =
?
q?KP (e)
score(q)
Syntax-based Similarity: In addition to surface
features of words and phrases, we leverage informa-
tion about the immediate syntactic context in which
an entity mention occurs. For example, in the sen-
tence ?Page played unusual chords?, we can extract
the fact that the mention ?Page? is the subject of the
verb ?play?. Using a large text corpus for training,
we collect statistics about what kinds of entities tend
to occur as subjects of ?play?, and then rank the can-
didate entities according to their compatibility with
the verb.
Specifically, we employ the framework of
(Thater10), which allows us to derive vector represen-
tations of words in syntactic contexts (such as being
the subject of a particular verb). We do not directly
apply this model to derive contextualized representa-
tions of entity mentions, as information about specific
proper names is very sparse in corpora like GigaWord
or Wikipedia. Instead, we consider a set of substi-
tutes for each possible entity e, which we take as its
context cxt(e). For this, we use the WordNet synsets
associated with the entity?s YAGO types and all their
hypernyms. For each substitute, we compute a stan-
dard distributional vector and a contextualized vector
according to (Thater10). Syntax-based similarity be-
tween cxt(e) and the context cxt(m) of the mention
is then defined as the sum of the scalar-product simi-
larity between these two vectors for each substitute.
This results in high similarity if the syntactic contex-
tualization only leads to small changes of the vectors,
reflecting the compatibility of the entity?s substitutes.
In our example, we compute a vector for ?gui-
tarist? as subject of ?play?, and another one for ?en-
trepreneur? in the same context. The former is more
786
compatible with the given context than the latter, lead-
ing to higher similarity for the entity Jimmy Page.
4.3 Entity-Entity Coherence
As all entities of interest are registered in a knowl-
edge base (like YAGO), we can utilize the semantic
type system, which is usually a DAG of classes. The
simples measure is the distance between two entities
in terms of type and subclassOf edges.
The knowledge bases also provide same-as cross-
referencing to Wikipedia, amd we quantify the coher-
ence between two entities by the number of incom-
ing links that their Wikipedia articles share. This
approach has been refined by Milne and Witten
(Milne08), taking into account the total number N of
entities in the (Wikipedia) collection:
mw coh(e1, e2) =
1? log (max(|INe1 |, |INe2 |))? log(|INe1 ? INe2 |)log(|N |)? log (min(|INe1 |, |INe2 |))
if > 0 and else set to 0.
5 Graph Model and Algorithms
5.1 Mention-Entity Graph
From the popularity, similarity, and coherence mea-
sures discussed in Section 4, we construct a weighted,
undirected graph with mentions and candidate enti-
ties as nodes. As shown in the example of Figure 1,
the graph has two kinds of edges:
? A mention-entity edge is weighted with a similar-
ity measure or a combination of popularity and
similarity measure. Our experiments will use a
linear combination with coefficients learned from
withheld training data.
? An entity-entity edge is weighted based on
Wikipedia-link overlap, or type distance, or some
combination along these lines.
Our experiments will focus on anchor-based pop-
ularity, keyphrase-based and/or syntactic similarity,
and link-based coherence (mw coh). The mention-
entity graph is dense on the entities side and often has
hundreds or thousands of nodes, as the YAGO knowl-
edge base offers many candidate entities for common
mentions (e.g., country names that could also denote
sports teams, common lastnames, firstnames, etc.).
5.2 Graph Algorithm
Given a mention-entity graph, our goal is to com-
pute a dense subgraph that would ideally contain all
mention nodes and exactly one mention-entity edge
for each mention, thus disambiguating all mentions.
We face two main challenges here. The first is how
to specify a notion of density that is best suited for
capturing the coherence of the resulting entity nodes.
The seemingly most natural approach would be to
measure the density of a subgraph in terms of its total
edge weight. Unfortunately, this will not work ro-
bustly for the disambiguation problem. The solution
could be dominated by a few entity nodes with very
high weights of incident edges, so the approach could
work for prominent targets, but it would not achieve
high accuracy also for the long tail of less prominent
and more sparsely connected entities. We need to
capture the weak links in the collective entity set of
the desired subgraph. For this purpose, we define
the weighted degree of a node in the graph to be the
total weight of its incident edges. We then define the
density of a subgraph to be equal to the minimum
weighted degree among its nodes. Our goal is to
compute a subgraph with maximum density, while
observing constraints on the subgraph structure.
The second critical challenge that we need to face
is the computational complexity. Dense-subgraph
problems are almost inevitably NP-hard as they gen-
eralize the Steiner-tree problem. Hence, exact algo-
rithms on large input graphs are infeasible.
To address this problem, we adopt and extend an
approximation algorithm of (Sozio10) for the prob-
lem of finding strongly interconnected, size-limited
groups in social networks. The algorithm starts from
the full mention-entity graph and iteratively removes
the entity node with the smallest weighted degree.
Among the subgraphs obtained in the various steps,
the one maximizing the minimum weighted degree
will be returned as output. To guarantee that we
arrive at a coherent mention-entity mapping for all
mentions, we enforce each mention node to remain
connected to at least one entity. However, this con-
straint may lead to very suboptimal results.
For this reason, we apply a pre-processing phase to
prune the entities that are only remotely related to the
mention nodes. For each entity node, we compute the
distance from the set of all mention nodes in terms
787
They performed 
Kashmir,  
written by  
Page    
and Plant.   
Page played  
unusual chords  
on his Gibson. 
?? Led Zeppelin 
?? Hard rock 
?? Electric guitar 
?? Session guitarist 
?? Led Zeppelin 
?? Gibson 
?? Jimmy Page 
signature model 
?? Hard rock 
Kashmir (song) 
Kashmir (region) 
Larry Page 
Jimmy Page 
Page, Arizona 
Robert Plant 
Gibson Les Paul 
Gibson, Missouri 
Figure 1: Mention-Entity Graph Example
of the sum of the corresponding squared shortest-
path distances. We then restrict the input graph to
the entity nodes that are closest to the mentions. An
experimentally determined good choice for the size
of this set is five times the number of the mention
nodes. Then the iterative greedy method is run on
this smaller subgraph. Algorithm 1 summarizes this
procedure, where an entity is taboo if it is the
last candidate for a mention it is connected to.
Algorithm 1: Graph Disambiguation Algorithm
Input: weighted graph of mentions and entities
Output: result graph with one edge per mention
begin
pre?processing phase;
foreach entity do
calculate distance to all mentions;
keep the closest (5? mentions count)
entities, drop the others;
main loop;
while graph has non-taboo entity do
determine non-taboo entity node
with lowest weighted degree, remove it
and all its incident edges;
if minimum weighted degree increased
then
set solution to current graph;
post?processing phase;
process solution by local search or full
enumeration for best configuration;
The output of the main loop would often be close
to the desired result, but may still have more than one
mention-entity edge for one or more mentions. At
this point, however, the subgraph is small enough to
consider an exhaustive enumeration and assessment
of all possible solutions. This is one of the options
that we have implemented as post-processing step.
Alternatively, we can perform a faster local-search
algorithm. Candidate entities are randomly selected
with probabilities proportional to their weighted de-
grees. This step is repeated for a prespecified number
of iterations, and the best configuration with the high-
est total edge-weight is used as final solution.
5.3 Robustness Tests
The graph algorithm generally performs well. How-
ever, it may be misled in specific situations, namely,
if the input text is very short, or if it is thematically
heterogeneous. To overcome these problems, we in-
troduce two robustness tests for individual mentions
and, depending on the tests? outcomes, use only a
subset of our framework?s features and techniques.
Prior test: Our first test ensures that the popularity
prior does not unduly dominate the outcome if the
true entities are dominated by false alternatives. We
check, for each mention, whether the popularity prior
for the most likely candidate entity is above some
threshold ?, e. g. above 90% probability. If this is not
the case, then the prior is completely disregarded for
computing the mention-entity edge weights. Other-
wise, the prior is combined with the context-based
similarity computation to determine edge weights.
788
We never rely solely on the prior.
Coherence test: As a test for whether the coher-
ence part of our framework makes sense or not,
we compare the popularity prior and the similarity-
only measure, on a per-mention basis. For each
mention, we compute the L1 distance between the
popularity-based vector of candidate probabilities
and the similarity-only-based vector of candidate
probabilities:
?
i=1..k
|prior(m, ei)? simscore(m, ei)|
This difference is always between 0 and 2. If it ex-
ceeds a specified threshold ? (e.g., 1), the disagree-
ment between popularity and similarity-only indi-
cates that there is a situation that coherence may be
able to fix. If, on the other hand, there is hardly any
disagreement, using coherence as an additional as-
pect would be risky for thematically heterogeneous
texts and should better be disabled. In that case, we
choose an entity for the mention at hand, using the
combination of prior and similarity. Only the win-
ning entity is included in the mention-entity graph, all
other candidates are omitted for the graph algorithm.
The robustness tests and the resulting adaptation of
our method are fully automated.
6 Experiments
6.1 Setup
System: All described methods are implemented in
a prototype system called AIDA (Accurate Online
Disambiguation of Named Entities). We use the Stan-
ford NER tagger (Finkel05) to identify mentions in
input texts, the YAGO2 knowledge base (Hoffart11)
as a repository of entities, and the English Wikipe-
dia edition (as of 2010-08-17) as a source of mining
keyphrases and various forms of weights. The graph
algorithm makes use of Webgraph (Boldi04).
Datasets: There is no established benchmark for
NED. The best prior work (Kulkarni09)) compiled
its own hand-annotated dataset, sampled from online
news. Unfortunately, this data set is fairly small (102
short news articles, about 3,500 proper noun men-
tions). Moreover, its entity annotations refer to an old
version of Wikipedia. To avoid unfair comparisons,
we created our own dataset based on CoNLL 2003
articles 1,393
mentions (total) 34,956
mentions with no entity 7,136
words per article (avg.) 216
mentions per article (avg.) 25
distinct mentions per article (avg.) 17
mentions with candidate in KB (avg.) 21
entities per mention (avg) 73
initial annotator disagreement (%) 21.1
Table 1: CoNLL Dataset Properties
data, extensively used in prior work on NER tagging
(Sang03).
This consists of proper noun annotations for 1393
Reuters newswire articles. We hand-annotated all
these proper nouns with corresponding entities in
YAGO2. Each mention was disambiguated by two
students and resolved by us in case of conflict. This
data set is referred to as CoNLL in the following
and fully available at http://www.mpi-inf.mpg.
de/yago-naga/aida/. Table 1 summarizes prop-
erties of the dataset.
Methods under comparison: Our framework in-
cludes many variants of prior methods from the lit-
erature. We report experimental results for some of
them. AIDA?s parameters were tuned by line-search
on 216 withheld development documents. We found
the following to work best:
? threshold for prior test: ? = 0.9
? weights for popularity, similarity, coherence:
? = 0.43, ? = 0.47, ? = 0.10
? initial number of entites in graph: 5 ? #mentions
? threshold for coherence test: ? = 0.9
We checked the sensitivity of the hyper-parameter
settings and found the influence of variations to be
small, e. g. when varying ? within the range [0.5,1.3],
the changes in precision@1.0 are within 1%.
The baseline for our experiments is the collective-
inference method of (Kulkarni09), which outper-
forms simpler methods (such as (Milne08)). We
refer to this method as Kul CI. Since program code
for this method is not available, we re-implemented
it using the LP solver CPLEX for the optimization
problem with subsequent rounding, as described in
(Kulkarni09). In addition, we compare against (our
re-implementation of) the method of (Cucerzan07),
789
Our Methods Competitors
sim-k prior
sim-k
prior
sim-s
sim-k
sim-s
r-prior
sim-k
r-prior
sim-k
coh
r-prior
sim-k
r-coh
prior Cuc Kul s Kul sp Kul CI
Macro P@1.0 76.53 75.75 71.43 76.40 80.71 80.73 81.91 71.24 43.74 58.06 76.74 76.74
Micro P@1.0 76.09 70.72 66.09 76.13 79.57 81.77 81.82 65.84 51.03 63.42 72.31 72.87
MAP 66.98 83.99 85.97 67.00 85.91 89.05 87.31 86.63 40.06 63.90 86.50 85.44
Table 2: Experimental results on CoNLL (all values in %)
referred to as Cuc. For all methods, weights for
combining components were obtained by training
a SVM classifier on 946 withheld CoNLL training
documents.
Performance measures: The key measures in our
evaluation are precision and recall. We consider
the precision-recall curve, as there is an inherent
trade-off between the two measures. Precision is the
fraction of mention-entity assignments that match
the ground-truth assignment. Recall is the fraction
of the ground-truth assignments that our method(s)
could compute. Both measures can aggregate over of
all mentions (across all texts) or over all input texts
(each with several mentions). The former is called
micro-averaging, the latter macro-averaging.
As we use a knowledge base with millions of enti-
ties, we decided to neglect the situation that a mention
may refer to an unknown entity not registered in the
knowledge base. We consider only mention-entity
pairs where the ground-truth gives a known entity,
and thus ignore roughly 20% of the mentions without
known entity in the ground-truth. This simplifies the
calculation of aggregated precision-recall measures
like (interpolated) MAP (mean average precision):
MAP = 1m
?
i=1..m
precision@ im
where precision@ im is the precision at a specificrecall level. This measure is equivalent to the area
under the precision-recall curve.
For constructing the precision-recall curve, we sort
the mention-entity pairs in descending order of con-
fidence, so that x% recall refers to the x% with the
highest confidence. We use each method?s mention-
entity similarity for the confidence values.
6.2 Results
The results of AIDA vs. the collective-inference
method of (Kulkarni09) and the entity disambigua-
tion method of (Cucerzan07) on 229 test documents
are shown in Table 21. The table includes variants
of our framework, with different choices for the sim-
ilarity and coherence computations. The shorthand
notation for the combinations in the table is as fol-
lows: prior: popularity prior; r-prior: popularity
prior with robustness test; sim-k: keyphrase based
similarity measure; sim-s: syntax-based similarity;
coh: graph coherence; r-coh: graph coherence with
robustness test.
The shorthand names for competitors are: Cuc:
(Cucerzan07) similarity measure; Kul s: (Kulka-
rni09) similarity measure only; Kul sp: Kul s com-
bined with plus popularity prior; Kul CI: Kul sp com-
bined with coherence. All coherence methods use
the Milne-Witten inlink overlap measure mw coh.
The most important measure is macro/micro preci-
son@1.0, which corresponds to the overall correct-
ness of the methods for all mentions that are assigned
to an entity in the ground-truth data. Our sim-k pre-
cision is already very good. Combining it with the
syntax-based similarity improves micro-averaged pre-
cision@1.0, but the macro-averaged results are a bit
worse. Thus, the more advanced configurations of
AIDA did not use syntax-based similarity. Uncondi-
tionally combining prior and sim-k degrades the qual-
ity, but including the prior robustness test (r-prior
sim-k) improves the results significantly. The preci-
sion for our best method, the prior- and coherence-
tested Keyphrase-based mention-entity similarity (r-
prior sim-k r-coh), significantly outperforms all com-
petitors (with a p-value of a paired t-test< 0.01). Our
macro-averaged precision@1.0 is 81.91%, whereas
Kul CI only achieves 76.74%. Even r-prior sim-
k, without any coherence, significantly outperforms
12 of the 231documents in the original test set could not be
processed by Kul CI due to memory limitations. All results are
given for the subset, for the sake of comparability. Results for
the complete set are available on our website.
790
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.7
0.8
0.9
r-prior sim-k r-coh
r-prior sim-k
Kul CI Kul sp
prior
recall
pre
cis
ion
Figure 2: Experimental results on CoNLL: precision-recall curves
Kul CI (with coherence) with a p-value of < 0.01.
In micro-average precision@1.0, the differences are
even higher, showing that we perform better through-
out all documents.
The macro-averaged precision-recall curves in Fig-
ure 2 show that the best AIDA method performs
particularly well in the tail of high recall values. The
MAP underlines the robustness of our best methods.
The high MAP for the prior method is because
we rank by mention-entity edge weight; for prior
this is simply the prior probability. As the prior is
most probably correct for mentions with a very high
prior for their most popular entity (by definition), the
initial ranking of the prior is very good, but drops
more sharply. We believe that the main difficulty in
named entity disambiguation lies exactly in the ?long
tail? of not-so-prominent entities.
We also tried the (Milne08) web service on a sub-
set of our test collection, but this was obviously
geared for Wikipedia linkage and performed poorly.
6.3 Discussion
Our keyphrase-based similarity measure performs
better than the Kul s measure, which is a combina-
tion of 4 different entity contexts (abstract tokens,
full text tokens, inlink anchor tokens, inlink anchor
tokens + surrounding tokens), 3 similarity measures
(Jaccard, dot product, and tf.idf cosine similarity),
and the popularity prior. Adding the prior to our
similarity measure by linear combination degrades
the performance. We found that our measure already
captures a notion of popularity because popular enti-
ties have more keyphrases and can thus accumulate
a higher total score. The popularity should only be
used when one entitiy has a very high probability, and
introducing the robustness test for the prior achieved
this, improving on both our similarity and Kul sp.
Unconditionally adding the notion of coherence
among entities improves the micro-average precision,
but not the macro-average. Investigating potential
problems, we found that the coherence can be led
astray when parts of the document form a coherent
cluster of entities, and other entities are then forced
to be coherent to this cluster. To overcome this is-
sue, we introduced the coherence robustness test,
and the results with r-coh show that it makes sense
to fix an entity for a mention when the prior and
similarity are in reasonable agreement. Adding this
coherence test leads to a signigicant (p-value < 0.05)
improvement over the non-coherence based measures
in both micro- and macro-average precision. Our ex-
periments showed that when adding this coherence
test, around 23 of the mentions are solved using localsimilarity only and are assigned an entity before run-
ning the graph algorithm. In summary, we observed
that the AIDA configuration with r-prior, keyphrase-
based sim-k, and r-coh significantly outperformed all
competitors.
7 Conclusions and Future Work
The AIDA system provides an integrated NED
method using popularity, similarity, and graph-based
coherence, and includes robustness tests for self-
adaptive behavior. AIDA performed significantly bet-
ter than state-of-the-art baselines. The system is fully
implemented and accessible online (http://www.
mpi-inf.mpg.de/yago-naga/aida/). Our fu-
ture work will consider additional semantic proper-
ties between entities (types, memberOf/partOf, etc.)
for further enhancing the coherence algorithm.
Acknowledgements
This work has been partially supported by the German Sci-
ence Foundation (DFG) through the Cluster of Excellence
on ?Multimodal Computing and Interaction? and the Eu-
ropean Union through the 7th Framework IST Integrated
Project ?LivingKnowledge? (no. 231126). We also thank
Mauro Sozio for the discussion on the graph algorithm.
791
References
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, Zachary G. Ives: DB-
pedia: A Nucleus for a Web of Open Data. ISWC 2007
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, Oren Etzioni: Open Information
Extraction from the Web. IJCAI 2007
Paolo Boldi and Sebastiano Vigna. The WebGraph frame-
work I: Compression techniques. WWW 2004, soft-
ware at http://webgraph.dsi.unimi.it/
Razvan C. Bunescu, Marius Pasca: Using Encyclopedic
Knowledge for Named entity Disambiguation. EACL
2006
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., Tom M. Mitchell.
Toward an Architecture for Never-Ending Language
Learning. AAAI 2010
Silviu Cucerzan: Large-Scale Named Entity Disambigua-
tion Based on Wikipedia Data. EMNLP-CoNLL 2007
AnHai Doan, Luis Gravano, Raghu Ramakrishnan, Shiv-
akumar Vaithyanathan. (Eds.). Special issue on infor-
mation extraction. SIGMOD Record, 37(4), 2008.
Jenny Rose Finkel, Trond Grenager, Christopher Man-
ning: Incorporating Non-local Information into Infor-
mation Extraction Systems by Gibbs Sampling. ACL
2005, software at http://nlp.stanford.edu/
software/CRF-NER.shtml
Xianpei Han, Jun Zhao: Named entity disambiguation
by leveraging wikipedia semantic knowledge. CIKM
2009.
Johannes Hoffart, Fabian Suchanek, Klaus Berberich, Ed-
win Lewis-Kelham, Gerard de Melo, Gerhard Weikum:
YAGO2: Exploring and Querying World Knowledge in
Time, Space, Context, and Many Languages. Demo Pa-
per, WWW 2011, data at http://www.mpi-inf.
mpg.de/yago-naga/yago/
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
Soumen Chakrabarti: Collective annotation of Wikipe-
dia entities in web text. KDD 2009
James Mayfield et al: Corss-Document Coreference Res-
olution: A Key Technology for Learning by Reading.
AAAI Spring Symposium on Learning by Reading and
Learning to Read, 2009.
Diane McCarthy. Word Sense Disambiguation: An
Overview. Language and Linguistics Compass 3(2):
537-558, Wiley, 2009
Rada Mihalcea, Andras Csomai: Wikify!: Linking Docu-
ments to Encyclopedic Knowledge. CIKM 2007
David N. Milne, Ian H. Witten: Learning to Link with
Wikipedia. CIKM 2008
Ndapandula Nakashole, Martin Theobald, Gerhard
Weikum: Scalable Knowledge Harvesting with High
Precision and High Recall. WSDM 2011
Roberto Navigli: Word sense disambiguation: A survey.
ACM Comput. Surv., 41(2), 2009
Hien T. Nguyen, Tru H. Cao: Named Entity Disambigua-
tion on an Ontology Enriched by Wikipedia. RIVF
2008
Erik F. Tjong Kim Sang, Fien De Meulder: Introduction to
the CoNLL-2003 Shared Task: Language-Independent
Named Entity Recognition. CoNLL 2003
Mauro Sozio, Aristides Gionis: The Community-search
Problem and How to Plan a Successful Cocktail Party.
KDD 2010
Fabian M. Suchanek, Gjergji Kasneci, Gerhard Weikum:
YAGO: a Core of Semantic Knowledge. WWW 2007
Fabian Suchanek, Mauro Sozio, Gerhard Weikum: SOFIE:
a Self-Organizing Framework for Information Extrac-
tion. WWW 2009
Bilyana Taneva, Mouna Kacimi, and Gerhard Weikum:
Finding Images of Rare and Ambiguous Entities. Tech-
nical Report MPI-I-2011-5-002, Max Planck Institute
for Informatics, 2011.
Stefan Thater, Hagen Fu?rstenau, Manfred Pinkal. Contex-
tualizing Semantic Representations using Syntactically
Enriched Vector Models. ACL 2010
Michael L. Wick, Aron Culotta, Khashayar Rohani-
manesh, Andrew McCallum: An Entity Based Model
for Coreference Resolution. SDM 2009: 365-376
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, Ji-Rong
Wen: StatSnowball: a Statistical Approach to Extract-
ing Entity Relationships. WWW 2009
792
Semi-Supervised Semantic Role Labeling
via Structural Alignment
Hagen Fu?rstenau?
Columbia University
Mirella Lapata??
University of Edinburgh
Large-scale annotated corpora are a prerequisite to developing high-performance semantic role
labeling systems. Unfortunately, such corpora are expensive to produce, limited in size, and
may not be representative. Our work aims to reduce the annotation effort involved in creating
resources for semantic role labeling via semi-supervised learning. The key idea of our approach
is to find novel instances for classifier training based on their similarity to manually labeled seed
instances. The underlying assumption is that sentences that are similar in their lexical material
and syntactic structure are likely to share a frame semantic analysis. We formalize the detection of
similar sentences and the projection of role annotations as a graph alignment problem, which we
solve exactly using integer linear programming. Experimental results on semantic role labeling
show that the automatic annotations produced by our method improve performance over using
hand-labeled instances alone.
1. Introduction
Recent years have seen growing interest in the shallow semantic analysis of natural
language text. The term is most commonly used to refer to the automatic identification
and labeling of the semantic roles conveyed by sentential constituents (Gildea and
Jurafsky 2002). Semantic roles themselves have a long-standing tradition in linguistic
theory, dating back to the seminal work of Fillmore (1968). They describe the relations
that hold between a predicate and its arguments, abstracting over surface syntactic
configurations. Consider the following example sentences:
(1) a. The burglar broke the window with a hammer.
b. A hammer broke the window.
c. The window broke.
? Center for Computational Learning Systems, Columbia University, 475 Riverside Drive, Suite 850,
New York, NY 10115, USA. E-mail: hagen@ccls.columbia.edu.
(The work reported in this paper was carried out while the author was at Saarland University, Germany.)
?? School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh, EH8 9AB, UK.
E-mail: mlap@inf.ed.ac.uk.
Submission received: 30 August 2010; revised submission received: 29 April 2011; accepted for publication:
14 June 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 1
Here, the phrase the window occupies different syntactic positions?it is the object of
break in sentences (1a) and (1b), and the subject in (1c)?and yet bears the same semantic
role denoting the affected physical object of the breaking event. Analogously, hammer is
the instrument of break both when attested with a prepositional phrase in (1a) and as
a subject in (1b). The examples represent diathesis alternations1 (Levin 1993), namely,
regular variations in the syntactic expressions of semantic roles, and their computational
treatment is one of the main challenges faced by automatic semantic role labelers.
Several theories of semantic roles have been proposed in the literature, differing
primarily in the number and type of roles they postulate. These range from Fillmore?s
(1968) small set of universal roles (e.g., Agentive, Instrumental, Dative) to individual
roles for each predicate (Palmer, Gildea, and Kingsbury 2005). Frame semantic theory
(Fillmore, Johnson, and Petruck 2003) occupies the middle ground by postulating
situations (or frames) that can be evoked by different predicates. In this case, roles
are not specific to predicates but to frames, and therefore ought to generalize among
semantically related predicates. As an example, consider the sentences in Example (2):
(2) a. [Lee]Agent [punched]CAUSE HARM [John]Victim [in the eye]Body part.
b. [A falling rock]Cause [crushed]CAUSE HARM [my ankle]Body part.
c. [She]Agent [slapped]CAUSE HARM [him]Victim [hard]Degree [for his
change of mood]Reason.
d. [Rachel]Agent [injured]CAUSE HARM [her friend]Victim [by closing
the car door on his left hand]Means.
Here, the verbs punch, crush, slap, and injure are all frame evoking elements (FEEs),
that is, they evoke the CAUSE HARM frame, which in turn exhibits the frame-specific
(or ?core?) roles Agent, Victim, Body part, and Cause, and the more general (?non-core?)
roles Degree, Reason, and Means. A frame may be evoked by different lexical items,
which may in turn inhabit several frames. For instance, the verb crush may also evoke
the GRINDING frame, and slap the IMPACT frame.
The creation of resources that document the realization of semantic roles in
example sentences such as FrameNet (Fillmore, Johnson, and Petruck 2003) and
PropBank (Palmer, Gildea, and Kingsbury 2005) has greatly facilitated the develop-
ment of learning algorithms capable of automatically analyzing the role semantic struc-
ture of input sentences. Moreover, the shallow semantic analysis produced by existing
systems has been shown to benefit a wide spectrum of applications ranging from
information extraction (Surdeanu et al 2003) and question answering (Shen and Lapata
2007), to machine translation (Wu and Fung 2009) and summarization (Melli et al 2005).
Most semantic role labeling (SRL) systems to date conceptualize the task as
a supervised learning problem and rely on role-annotated data for model training.
Supervised methods deliver reasonably good performance2 (F1 measures in the low
80s on standard test collections for English); however, the reliance on labeled training
data, which is both difficult and highly expensive to produce, presents a major obstacle
to the widespread application of semantic role labeling across different languages and
text genres. And although nowadays corpora with semantic role annotations exist in
1 Sentences (1a) and (1b) illustrate the instrument subject alternation and sentences (1a) and (1c) illustrate
the causative/inchoative alternation.
2 We refer the interested reader to the reports on the SemEval-2007 shared task (Baker, Ellsworth, and Erk
2007) for an overview of the state of the art.
136
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
other languages (e.g., German, Spanish, Catalan, Chinese, Korean), they tend to be
smaller than their English equivalents and of limited value for modeling purposes.
It is also important to note that the performance of supervised systems degrades
considerably (by 10%) on out-of-domain data even within English, a language for which
two major annotated corpora are available (Pradhan, Ward, and Martin 2008). And this
is without taking unseen events into account, which unavoidably affect coverage. The
latter is especially an issue for FrameNet (version 1.3) which is still under development,
despite being a relatively large resource?it contains almost 140,000 annotated sentences
for a total of 502 frames, which are evoked by over 5,000 different lexical units. Coverage
issues involve not only lexical units but also missing frames and incompletely exempli-
fied semantic roles.
In this article, we attempt to alleviate some of these problems by using semi-
supervised methods that make use of a small number of manually labeled training
instances and a large number of unlabeled instances. Whereas manually labeled data are
expensive to create, unlabeled data are often readily available in large quantities. Our
approach aims to improve the performance of a supervised SRL system by enlarging
its training set with automatically inferred annotations of unlabeled sentences. The key
idea of our approach is to find novel instances for classifier training based on their simi-
larity to manually labeled seed instances. The underlying assumption is that sentences
that are similar in their lexical material and syntactic structure are likely to share a frame
semantic analysis. The annotation of an unlabeled sentence can therefore be inferred from
a sufficiently similar labeled sentence. For example, given the labeled sentence (3) and
the unlabeled sentence (4), we wish to recognize that they are lexically and structurally
similar; and infer that thumped also evokes the IMPACT frame, whereas the rest of his body
and against the front of the cage represent the Impactor and Impactee roles, respectively.
(3) [His back]Impactor [thudded]IMPACT [against the wall]Impactee.
(4) The rest of his body thumped against the front of the cage.
We formalize the detection of similar sentences and the projection of role annota-
tions in graph-theoretic terms by conceptualizing the similarity between labeled and
unlabeled sentences as a graph alignment problem. Specifically, we represent sentences
as dependency graphs and seek an optimal (structural) alignment between them. Given
this alignment, we then project annotations from the labeled onto the unlabeled sen-
tence. Graphs are scored using a function based on lexical and syntactic similarity which
allows us to identify alternations like those presented in Example (1) and more generally
to obtain training instances with novel structure and lexical material. We obtain the best
scoring graph alignment using integer linear programming, a general-purpose exact
optimization framework. Importantly, our approach is not tied to a particular SRL
system. We obtain additional annotations irrespective of the architecture or implemen-
tation details of the supervised role labeler that uses them. This renders our approach
portable across learning paradigms, languages, and domains.
After discussing related work (Section 2), we describe the details of our semi-
supervised method (Section 3) and then move on to evaluate its performance (Section 4).
We conduct two sets of experiments using data from the FrameNet corpus: In Section 5,
we apply our method to increase the training data for known predicates, that is, words
for which some seed annotations already exist. In Section 6, we focus on the comple-
mentary task of creating training instances for unknown predicates, that is, words that
do not occur in the FrameNet corpus at all. Section 7 concludes the article.
137
Computational Linguistics Volume 38, Number 1
2. Related Work
The lack of annotated data presents an obstacle to developing many natural language
applications, especially for resource-poor languages. It is therefore not surprising that
previous efforts to reduce the need for semantic role annotation have focused primarily
on languages other than English.
Annotation projection is a popular framework for transferring semantic role anno-
tations from one language to another while exploiting the translational and structural
equivalences present in parallel corpora. The idea here is to leverage the existing En-
glish FrameNet and rely on word or constituent alignments to automatically create an
annotated corpus in a new language. Pado? and Lapata (2009) transfer semantic role
annotations from English onto German and Johansson and Nugues (2006) from English
onto Swedish. A different strategy is presented in Fung and Chen (2004), where English
FrameNet entries are mapped to concepts listed in HowNet, an on-line ontology for
Chinese, without consulting a parallel corpus. Then, Chinese sentences with predicates
instantiating these concepts are found in a monolingual corpus and their arguments are
labeled with FrameNet roles.
Other work attempts to alleviate the data requirements for semantic role labeling
within the same language either by increasing the coverage of existing resources or by
inducing role annotations from unlabeled data. Swier and Stevenson (2004) propose
a method for bootstrapping a semantic role labeler. Given a verb instance, they first
select a frame from VerbNet, a semantic role resource akin to FrameNet and PropBank,
and label each argument slot with sets of possible roles. Their algorithm then proceeds
iteratively by first making initial unambiguous role assignments, and then successively
updating a probability model on which future assignments are based. Gordon and
Swanson (2007) attempt to increase the coverage of PropBank. Their approach leverages
existing annotations to handle novel verbs. Rather than annotating new sentences that
contain novel verbs, they find syntactically similar verbs and use their annotations as
surrogate training data.
Much recent work has focused on increasing the coverage of FrameNet, either
by generalizing semantic roles across different frames or by determining the frame
membership of unknown predicates. Matsubayashi, Okazaki, and Tsujii (2009) propose
to exploit the relations between semantic roles in an attempt to overcome the scarcity
of frame-specific role annotations. They propose several ways of grouping roles into
classes based on the FrameNet role hierarchy, human-understandable descriptors of
roles, selectional restrictions, and a FrameNet to VerbNet role mapping. They show that
transforming this information into feature functions and incorporating it into super-
vised learning improves role classification considerably.
The task of relating known frames to unknown predicates is addressed primarily by
resorting to WordNet (Fellbaum 1998). For example, Burchardt, Erk, and Frank (2005)
apply a word sense disambiguation system to annotate predicates with a WordNet sense
and hyponyms of these predicates are then assumed to evoke the same frame. Johansson
and Nugues (2007b) treat this problem as an instance of supervised classification. Using
a feature representation based also on WordNet, they learn a classifier for each frame,
which decides whether an unseen word belongs to the frame or not. Pennacchiotti
et al (2008) create ?distributional profiles? for frames. The meaning of each frame is
represented by a vector, which is the (weighted) centroid of the vectors representing
the predicates that can evoke it. Unknown predicates are then assigned to the most
similar frame. They also propose a WordNet-based model that computes the similarity
between the synsets representing an unknown predicate and those activated by the
138
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
predicates of a frame (see Section 6 for details). Das et al (2010) represent a departure
from the WordNet-based approaches in their use of a latent variable model to allow for
the disambiguation of unknown predicates.
Unsupervised approaches to SRL have been few and far between. Abend, Reichart,
and Rappoport (2009) propose an algorithm that identifies the arguments of predicates
by relying only on part-of-speech annotations, without, however, assigning their se-
mantic roles. In contrast, Grenager and Manning (2006) focus on role induction which
they formalize as probabilistic inference in a Bayesian network. Their model defines
a joint probability distribution over a verb, its semantic roles, and possible syntactic
realizations. More recently, Lang and Lapata (2010) formulate the role induction prob-
lem as one of detecting alternations and finding a canonical syntactic form for them.
Their model extends the logistic classifier with hidden variables and is trained on parsed
output which is used as a noisy target for learning.
Our own work aims to reduce but not entirely eliminate the annotation effort
involved in semantic role labeling. We thus assume that a small number of manual an-
notations is initially available. Our algorithm augments these with unlabeled examples
whose roles are inferred automatically. We apply our method in a monolingual setting,
and thus do not project annotations between languages but within the same language.
Importantly, we acquire new training instances for both known and unknown pred-
icates. Previous proposals extend FrameNet with novel predicates without inducing
annotations that exemplify their usage. We represent labeled and unlabeled instances
as graphs, and seek to find a globally optimal alignment between their nodes, subject to
semantic and structural constraints. Finding similar labeled and unlabeled sentences is
reminiscent of paraphrase identification (Qiu, Kan, and Chua 2006; Wan et al 2006; Das
and Smith 2009; Chang et al 2010), the task of determining whether one sentence is a
paraphrase of another. The sentences we identify are not strictly speaking paraphrases
(even if the two predicates are similar their arguments often are not); however, the
idea of modeling the correspondence structure (or alignment) between parts of the
two sentences is also present in the paraphrase identification work (Das and Smith
2009; Chang et al 2010). Besides machine translation (Matusov, Zens, and Ney 2004;
Taskar, Lacoste-Julien, and Klein 2005), methods based on graph alignments have been
previously employed for the recognition of semantic entailments (Haghighi, Ng, and
Manning 2005; de Marneffe et al 2007), where an optimization problem similar to
ours is solved using approximate techniques (our method is exact) and an alignment
scoring function is learned from annotated data (our scoring function does not require
extensive supervision). On a related note, de Salvo Braz et al (2005) model entail-
ments via a subsumption algorithm that operates over concept graphs representing
a source S and target T sentence and uses integer linear programming to prove that
S  T.
3. Method
In this section we describe the general idea behind our semi-supervised algorithm and
then move on to present our specific implementation. Given a set L of sentences labeled
with FrameNet frames and roles (the seed corpus) and a (much larger) set U of unla-
beled sentences (the expansion corpus), we wish to automatically create a set X ? U
of novel annotated instances. Algorithm 1 describes our approach, which consists of
two parts. In the labeling stage, annotations are proposed for every unlabeled sentence
(lines 1?20), and in the selection stage, instances with high quality annotations are
chosen to make up the final new corpus (lines 21?26).
139
Computational Linguistics Volume 38, Number 1
In the labeling stage, (almost) every unlabeled sentence u ? U receives an annota-
tion via projection from the seed l? ? L most similar to it. In theory, this means that each
unlabeled sentence u is compared with each labeled seed l. In practice, however, we
reduce the number of comparisons by requiring that u and l have identical or at least
similar FEEs. This process will yield many sentences for every seed with annotations
of varying quality. In default of a better way of distilling high-quality annotations, we
use similarity as our criterion in the selection stage. From the annotations originating
from a particular seed, we therefore collect the k instances with the highest similarity
values. Our selection procedure is guided by the seeds available rather than the corpus
from which unlabeled sentences are extracted. This is intended, as the seeds can be
used to create a balanced training set or one that exemplifies difficult or rare training
instances.
In the remainder of this section, we present the labeling stage of our algorithm in
more detail. Section 3.1 formally introduces the notion of semantically labeled depen-
dency graphs and defines the subgraphs M and N representing relevant predicate?
argument structures. Section 3.2 formalizes alignments as mappings between graph
nodes and defines our similarity score as a function on alignments between labeled
and unlabeled dependency graphs. Section 3.3 formulates an integer linear program
140
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
(ILP) for finding optimal alignments, and Section 3.4 presents an efficient algorithm
for solving this ILP. Finally, Section 3.5 describes how annotations are projected from
labeled onto unlabeled graphs.
3.1 Semantically Labeled Dependency Graphs
Seed sentences labeled with role-semantic annotations are represented by dependency
graphs. The latter capture grammatical relations between words via directed edges
from syntactic heads to their dependents (e.g., from a verb to its subject or from a
noun to a modifying adjective). Edges can be labeled to indicate the type of head?
dependent relationship (e.g., subject, object, modifier). In our case, dependency graphs
are further augmented with FrameNet annotations corresponding to the FEE and its
semantic roles.
A dependency graph of the sentence Old Herkimer blinked his eye and nodded wisely
is shown in Figure 1. Nodes are indicated by rectangles and dependencies by edges
(arrows). Solid arrows represent syntactic dependencies (e.g., subject, object), and
dashed arrows correspond to FrameNet annotations. Here, blink evokes the frame
Body movement, Herkimer bears the role Agent, and eye the role Body part.
Unfortunately, FrameNet annotations have not been created with dependency
graphs in mind. FEEs and roles are marked as substrings and contain limited syntac-
tic information, distinguishing only the grammatical functions ?external argument,?
?object,? and ?dependent? for the arguments of verbal FEEs. To obtain dependency
graphs with semantic annotations like the one shown in Figure 1, we parse the sentences
in the seed corpus with a dependency parser and compare the FrameNet annotations
(substrings) to the nodes of the dependency graph. For the FEE, we simply look for a
graph node that coincides with the word marked by FrameNet. Analogously, we map
Figure 1
Dependency graph with semantic annotations for the sentence Old Herkimer blinked his eye and
nodded wisely (taken from the FrameNet corpus). Nodes in the alignment domain are indicated
by double frames. Labels in italics denote frame roles, and grammatical roles are rendered in
small capitals. Annotations are only shown for the predicate blink, which evokes the frame
Body Movement.
141
Computational Linguistics Volume 38, Number 1
role annotations onto the graph by finding a node with a yield equal to the marked
substring, that is, a node that (together with its dominated nodes) represents the words
of the role. Our experiments make use of the dependency graphs produced by RASP
(Briscoe, Carroll, and Watson 2006), although there is nothing inherent in our approach
that assumes this specific parser. Any other dependency parser with broadly similar
output could be used instead.
Searching for nodes representing the FEE and its semantic roles may in some cases
yield no match. There are two reasons for this?parser errors and role annotations vio-
lating syntactic structure. We address this problem heuristically: If no perfect match is
found, the closest match is determined based on the number of mismatching characters
in the string. We thus compute a mismatch score for the FEE and each role. To make
allowances for parser errors, we compute these scores for the n-best parses produced
by the dependency parser and retain the dependency graph with the lowest mismatch.
This mapping procedure is more thoroughly discussed in Fu?rstenau (2008).
Each sentence in the seed corpus contains annotations for a predicate and its se-
mantic roles. A complex sentence (with many subordinate clauses) will be represented
by a large dependency graph, with only a small subgraph corresponding to these
annotations. Our method for computing alignments between graphs only considers
subgraphs with nodes belonging to the predicate-argument structure in question. This
allows us to compare graphs in a computationally efficient manner as many irrelevant
alignments are discarded, although admittedly the entire graph may provide useful
contextual clues to the labeling problem.
We are now ready to define the alignment domain M of a labeled dependency
graph. Let p be a node (i.e., word) in the graph corresponding to the FEE. If there are
no mismatches between semantic and syntactic arguments, we expect all roles in the
graph to be instantiated by syntactic dependents of p. Although this is often the case, it
does not always hold?for example, because of the way the dependency parser analyzes
raising, control, or coordination structures. We therefore cannot simply define M as
the set of direct dependents of the predicate, but also have to consider complex paths
between p and role-bearing nodes. An example is given in Figure 1, where the role Agent
is filled by a node that is not dominated by the FEE blink; instead, it is connected to blink
by the complex path (CONJ?1, SUBJ). For a given sentence, we build the set of all such
complex paths to any role-bearing node and also include all nodes connected to p by
one of these paths. We thus define the subgraph M to contain:
i. the predicate node p
ii. all direct dependents of p, except auxiliaries
iii. all nodes on complex paths from p to any role-bearing node
iv. single direct dependents of any preposition or conjunction node which is
in (ii) or end-point of a complex path covered in (iii)
In Figure 1 the nodes in the alignment domain are indicated by double frames.
In an unlabeled dependency graph we similarly identify the alignment range as the
subgraph corresponding to the predicate?argument structure of a target predicate. As
we do not have any frame semantic analysis for the unlabeled sentence, however, we
cannot determine a set of complex paths. We could ignore complex paths altogether and
thus introduce a substantial asymmetry into the comparison between a labeled and an
unlabeled sentence, as unlabeled sentences would be assumed to be structurally simpler
142
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
than labeled ones. This assumption will often be wrong and moreover introduce a bias
towards simpler structures for the new annotations. To avoid this, we reuse the set of
complex paths from the labeled sentence. Although this is not ideal either (it makes
the comparison asymmetrically dependent on the annotation of the labeled sentence)
it allows us to compare labeled and unlabeled sentences on a more equal footing. We
therefore define the alignment range N in exact analogy to the alignment domain M, the
only exception being that complex paths to role-bearing nodes are determined by the
labeled partner in the comparison.
3.2 Scoring Graph Alignments
We conceptualize the similarity between subgraphs representing predicate?argument
structures as an alignment problem. Specifically, we seek to find an optimal alignment
between the alignment domain M of a labeled graph and the alignment range N of
an unlabeled sentence. Alignments are scored using a similarity measure that takes
syntactic and lexical information into account.
We formalize the alignment between M and N as a partial injective function from
M to N, that is, a function ? : M ? N ? {} where ?(x) = ?(x?) =  implies x = x?.
Here,  denotes a special empty value. We say that x ? M is aligned to x? ? N by ?, iff
?(x) = x?. Correspondingly, a node x ? M with ?(x) =  or a node x? ? N that is not the
image of any x ? M is called unaligned. Figure 2 shows an example of an alignment
Figure 2
The dotted arrows show aligned nodes in the graphs for the two sentences His back thudded
against the wall and The rest of his body thumped against the front of the cage (graph edges are also
aligned to each other). The nodes in the alignment domain and alignment range are indicated by
double frames.
143
Computational Linguistics Volume 38, Number 1
between a labeled and an unlabeled dependency graph for the predicates thud and
thump.
Each alignment ? between M and N receives a score, the weighted sum of the lexical
similarity between nodes (lex) and syntactic similarity between edges (syn):
score(?) := 1
C
?
?
?
?
?
x?M
?(x)=
lex (x,?(x)) + ? ?
?
(x1,x2 )?E(M)
(?(x1),?(x2 ))?E(N)
syn
(
rx1x2 , r
?(x1)
?(x2)
)
?
?
?
?
(1)
Here, E(M) and E(N) denote the sets of graph edges between the nodes of M and N,
respectively, while rx1x2 is the label of the edge (x1, x2), that is, the grammatical relation
between these two nodes.
Equation (1) introduces a normalizing factor C whose purpose is to render similarity
scores of different pairs of sentences comparable. Without normalization, it would be
easier to achieve high similarity to a complex predicate?argument structure than a
simpler one, which is counter-intuitive. This can be seen from the fact that the self-
similarity of a sentence (i.e., the similarity of a sentence to itself) depends on the number
of nodes in M. Assuming that the maximal value for lex and syn is 1 for identical
words and grammatical relations, self-similarity is then |M|+ ?|E(M)| and constitutes
an upper bound for the similarity between any two sentences. We could use this term
to normalize the similarity score. However, this would only account for unaligned or
badly aligned nodes and edges in the labeled sentence while ignoring the unlabeled
partner. To obtain a symmetric normalization factor we therefore define:
C :=
?
|M| ? |N|+ ?
?
|E(M)| ? |E(N)| (2)
C is now symmetric in the two sentences and when introduced in equation (1) leads to
self-similarities of 1:
score(?self) =
1
?
|M|2 + ?
?
E(M)2
(|M| ? 1 + ? ? |E(M)| ? 1) = 1 (3)
Notice that our formulation uses the same score for finding whether there exists
an alignment and for evaluating its quality. Consequently, our algorithm will attempt
to construct an alignment even if there is none, that is, in cases where the similarity
between labeled and unlabeled sentences is low. Our approach is to filter out erroneous
alignments by considering only the k nearest neighbors of each seed. Alternatively, we
could first establish valid alignments and then score them; we leave this to future work.
The employed score is the weighted combination of lexical and syntactic similarity. In
our experiments we use cosine similarity in a vector space model of co-occurrence statis-
tics for lex and define syn as a binary function reflecting the identity of grammatical
relations (see Section 4 for details). Other measures based on WordNet (e.g., Budanitsky
and Hirst 2001) or finer grammatical distinctions are also possible.
3.3 ILP Formulation
We define the similarity of two predicate?argument structures as the maximum score
of any alignment ? between them. Intuitively, the alignment score corresponds to the
amount of changes required to transform one graph into the other. High scores indicate
144
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
high similarity and thus minimal changes. We do not need to formalize such changes,
although it would be possible to describe them in terms of substitutions, deletions,
and insertions. For our purposes, the alignment scores themselves can be used to
indicate whether two graphs are substantially similar to warrant projection of the frame
semantic annotations. We do this by finding an optimal alignment, that is, an alignment
with the highest score as defined in Equation (1).
To solve this optimization problem efficiently, we recast it as an integer linear pro-
gram (ILP). The ILP modeling framework has been recently applied to a wide range of
natural language processing tasks, demonstrating improvements over more traditional
optimization methods. Examples include reluctant paraphrasing (Dras 1999), relation
extraction (Roth and tau Yih 2004), semantic role labeling (Punyakanok et al 2004),
concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006), de-
pendency parsing (Riedel and Clarke 2006), sentence compression (Clarke and Lapata
2008), and coreference resolution (Denis and Baldridge 2007). Importantly, the ILP
approach3 delivers a globally optimal solution by searching over the entire alignment
space without employing heuristics or approximations (see de Marneffe et al [2007]
and Haghighi, Ng, and Manning [2005]). Furthermore, an ILP-based formulation seems
well-suited to our problem because the domain of the optimization, namely, the set of
partial injective functions from M to N, is discrete. We define arbitrary linear orders on
the sets M and N, writing M = {n1, . . . , nm} and N = {n?1, . . . , n?n} and then introduce
binary indicator variables xij to represent an alignment ?:
xij :=
{
1 if ?(ni) = n?j
0 else
(4)
Each alignment ? thus corresponds to a distinct configuration of xij values. In order
to ensure that the latter describe a partial injective function, we enforce the following
constraints:
1. ?j :
?
1?i?m xij ? 1 (Each node in N is aligned to at most one node in M.)
2. ?i :
?
1?j?n xij ? 1 (Each node in M is aligned to at most one node in N.)
We can now write Equation (1) in terms of the variables xij (which capture exactly the
same information as the function ?):
score(x) = 1
C
?
?
?
?
?
1?i?m
1?j?n
lex
(
ni, n
?
j
)
xij + ? ?
?
1?i,k?m
1?j,l?n
syn
(
rnink , r
n?j
n?l
)
xijxkl
?
?
?
?
(5)
Note that Equations (1) and (5) are summations of the same terms.4 However,
Equation (5) is not linear in the variables xij as it contains products of the form xijxkl.
3 It is outside the scope of this article to provide an introduction to ILP. We refer the interested reader to
Winston and Venkataramanan (2003) and Vanderbei (2001) for comprehensive overviews.
4 For convenience, we define rn1n2 =  if there is no relation between n1 and n2, and assume that syn is 0 if
either of its arguments is .
145
Computational Linguistics Volume 38, Number 1
This can be remedied through the introduction of another set of binary variables yijkl
subject to additional constraints ensuring that yijkl = xijxkl:
3. ?i,j,k,l : yijkl ? xij
4. ?i,j,k,l : yijkl ? xkl
5. ?i,j,k,l : yijkl ? xij + xkl ? 1
We also want to make sure that the FEE of the labeled sentence is aligned to the
target predicate of the unlabeled sentence. We express this with the following con-
straint, assuming that the FEE and the target predicate are represented by n1 and n?1,
respectively:
6. x11 = 1
We therefore have to solve an ILP in the mn + m2n2 variables xij and yijkl, subject to
m + n + 3m2n2 + 1 constraints (see constraints (1)?(6)), with the objective function:
score(x, y) = 1
C
?
?
?
?
?
1?i?m
1?j?n
lex
(
ni, n
?
j
)
xij + ? ?
?
1?i,k?m
1?j,l?n
syn
(
rnink , r
n?j
n?l
)
yijkl
?
?
?
?
(6)
Exact optimization for the general ILP problem is NP-hard (Cormen, Leiserson, and
Rivest 1992). ILPs with a totally unimodular constraint matrix5 are solvable efficiently,
using polynomial time algorithms. In this special case, it can be shown that the optimal
solution to the linear program is integral. Unfortunately, our ILP falls outside this class
due to the relatively complex structure of our constraints. This can be easily seen when
considering the three constraints x11 + x12 + ? ? ?+ x1m ? 1, ?x11 + y1112 ? 0 and ?x12 +
y1112 ? 0. The coefficients of the three variables x11, x12, and y1112 in these constraints
make up the matrix
?
?
1 1 0
?1 0 1
0 ?1 1
?
?
The determinant of this matrix is 2 and therefore the complete coefficient matrix of the
ILP has a quadratic submatrix with a determinant that is not 0 or ?1, which means
that it is not totally unimodular. Indeed, it has been shown that the structural matching
problem is NP-hard (Klau 2009).
3.4 Solving the ILP
There are various techniques for finding the optimal solution of an ILP, such as ap-
proximation with error bounds (Klau 2009) or application of the branch-and-bound
5 A matrix A is totally unimodular if every square sub-matrix of A has its determinant equal to 0, +1, or ?1.
146
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
algorithm (Land and Doig 1960). The latter allows for solving an ILP exactly and signif-
icantly faster than by naive enumeration. It does this by relaxing the integer constraints
and solving the resulting LP problem, known as the LP relaxation. If the solution of
the LP relaxation is integral, then it is the optimal solution. Otherwise, the resulting
solution provides an upper bound on the solution for the ILP. The algorithm proceeds
by creating two new sub-problems based on the non-integer solution for one variable
at a time. These are solved and the process repeats until the optimal integer solution
is found. Our alignment problem has only binary variables and is thus an instance of
a ?pure? 0?1 ILP. For such problems, implicit enumeration can be used to simplify
both the braching and bounding components of the branch-and-bound process and
to determine efficiently when a node is infeasible. This is achieved by systematically
evaluating all possible solutions, without, however, explicitly solving a potentially large
number of LPs derived from the relaxation.
To obtain a solution for the ILP in Section 3.3, we could have used any solver that
implements the standard branch-and-bound algorithm. To speed up computation time,
we have instead modified the branch-and-bound algorithm so as to take into account
the special structure of our graph alignment problem. Our own algorithm follows the
principles of branch-and-bound but avoids explicit representation of the variables yijkl,
performs early checks of the constraints on the variables xij on branching, and takes
into account some of the constraints on the variables yijkl for the estimation of lower
and therefore more efficient bounds. In the following, we first describe our modified
algorithm and then assess its runtime in comparison to a publicly available solver.
Algorithm 2 shows how to find an optimal alignment ?? with score s? in pseu-
docode. ?0 and ?1 denote partial solutions, while completions are built in ?. syn? is the
maximum possible value of syn, that is, syn? = 1 for a binary measure. We initialize ??
with the trivial solution which aligns n1 to n?1 and leaves all other nodes unaligned.
6
This gives a score of lex(n1, n?1). To find better solutions we start with an initial partial
alignment ?0, which contains only the mapping n1 ? n?1 and leaves the alignments of
all other n ? M unspecified. (Note that this is different from the complete alignment
?? which specifies those nodes as unaligned: n ? .) As in the general branch-and-
bound algorithm, the space of all alignments is searched recursively by branching on the
alignment decision for each remaining node. A branch is left as soon as an upper bound
on the achievable score indicates that the current best solution cannot be improved
within this branch.
Given a partial alignment ?0 (the initial or any subsequent one) defined on some
subset of M, we estimate a suitable bound by extending ?0 to a complete function ? on
all nodes in M: Each of the remaining nodes is aligned to its partner in N maximizing lex.
If no positive value can be found for lex, the node is defined as unaligned. We then
define the bound s as the score of ?0 together with the lexical scores of the newly created
alignments and a hypothetical syntactic score which assumes that each of the newly
considered edges is aligned perfectly, that is, with the maximum value syn? attainable
by syn. (This is a lower bound than the one a naive application of the branch-and-bound
algorithm would compute.)
Of course, ? need not fulfill the constraints of the ILP and s need not be an attainable
score. It is, however, an upper bound for the score of any valid alignment. If it is not
6 In the description of the algorithm, we use the more intuitive notation ni ? n?j to indicate that ni is
aligned to n?j . Note, however, that this could be equivalently formulated in terms of the ILP variables
(i.e., xij = 1), and our algorithm still broadly follows the branch-and-bound procedure for ILPs.
147
Computational Linguistics Volume 38, Number 1
greater than the current best score s?, we leave the current branch. Otherwise, we check
if ? is a valid alignment with score s, that is, if it satisfies the constraints of the ILP
and s is its score (which means that the assumptions of perfect syntactic scores were
justified). If this is the case, we have a new current optimum and do not need to follow
the current branch any more either. If, however, the bound s is greater than the current
optimum s?, but ? violates some constraints or does not achieve a score of s because it
contains imperfect syntactic alignments, we have to branch on the decision of how to
extend ?0 by an additional alignment link. We consider the next node with unspecified
alignment and recursively apply the algorithm to extensions of ?0. Each extension ?1
aligns this node to a partner in N that has thus far been left unaligned. (This simple
148
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
check of constraint (1), which extends the general branch-and-bound algorithm, avoids
recursion into branches that cannot contain any valid solutions.) The partial score s1
corresponding to ?1 is computed by taking into account the consequences of the new
alignment to the lexical and syntactic scores.
We found this algorithm to be very effective in solving the ILPs arising in our
experiments. While its worst case performance is still exponential in the number of
aligned nodes and edges, it almost always finds the optimum within a relatively small
number of iterations of the outer loop (line 4 in Figure 2). This is also due to the fact that
the alignment domain and range are typically not very large. In a realistic application of
our method, 70% of the ILPs were solvable with less than 100 iterations, 93% with less
than 1,000 iterations, 98.6% with less than 10,000 iterations, and 99.95% with less than
1,000,000 iterations. As the remaining 0.05% of the ILPs may still take an inordinate
amount of time, we abort the search at this point. In this case, it is highly likely that the
alignment domain and range are large and any resulting alignment would be overly
specific and thus not very useful. Aborting at 1,000,000 iterations is also preferable to a
time-out based on processing time, as it makes the result deterministic and independent
of the specific implementation and hardware. All expansion sets in the experiments
described in Sections 5 and 6 were computable within hours on modern hardware and
under moderate parallelization, which is trivial to implement over the instances of the
unlabeled corpus.
Because our branch-and-bound algorithm performs exact optimization, it could be
replaced by any other exact solution algorithm, without affecting our results. To assess
its runtime performance further, we compared it to the publicly available lp solve7
solver which can handle integer variables via the branch-and-bound algorithm. We
sampled 100 alignment problems for each problem size (measured in number of nodes
in the alignment domain) and determined the average runtime of our algorithm and
lp solve. (The latter was run with the option -time, which excludes CPU time spent
on input parsing). Figure 3 shows how the average time required to solve an ILP
varies with the problem size. As can be seen, our algorithm is about one order of
magnitude more efficient than the implementation of the general-purpose branch-and-
bound algorithm.
3.5 Annotation Projection
Given a labeled graph l, an unlabeled graph u, and an optimal alignment ? between
them, it is relatively straightforward to project frame and role information from one to
the other. As described in Section 3.1, frame names are associated with the nodes of their
FEEs and role names with the nodes of their role filler heads. By definition, all of these
nodes are in the alignment range M. It is therefore natural to label ?(x) ? N with the
role carried by x for each role-bearing node x ? M. The only complicating factor is that
we have allowed unaligned nodes, that is, nodes with ?(x) = . Although this is useful
for ignoring irrelevant nodes in M, we must decide how to treat these when they are
role-bearing (note that FEEs are always aligned by constraint (6), so frame names can
always be projected).
A possible solution would be to only project roles on nodes x with ?(x) = , so
that roles associated with unaligned nodes do not show up in the inferred annotation.
Unfortunately, allowing such partial projections introduces a systematic bias in favor
7 Version 5.5, available at http://lpsolve.sourceforge.net/.
149
Computational Linguistics Volume 38, Number 1
Figure 3
Average time required to solve an ILP as a function of the size of the alignment domain.
of simpler structures. When these new instances are used as a training set for a role
labeler, they will bias the classifier towards under-annotating roles and thus decrease
performance. We therefore do not want to allow partial projections and demand that
?(x) =  for all role-bearing nodes x.
We could incorporate this additional constraint into the ILP by finding a (lower scor-
ing) solution that satisfies it. However, there is no theoretical justification for favoring a
lower ranking alignment over the optimal one only because of projection requirements.
If lexical and structural measures tell us that a certain alignment is best, we should
not dismiss this information, but rather take the contradiction between the optimal
alignment and the frame semantic (non-)projectability to indicate that l is not suitable
for inferring a labeling of u. There are several possible reasons for this, ranging from
idiosyncratic annotations to parser or pre-processing errors. We therefore do not discard
the optimal alignment in favor of a lower scoring one, but rather dismiss the seed l as a
source of information for inferring a labeling on u. This reflects our precision-oriented
approach: If u does not find a better partner among the other seeds, it will be discarded
as unsuitable for the expansion set.
4. Experimental Set-up
In this section, we describe the data and supervised semantic role labeler used in our
experiments and explain how the free parameters of our method were instantiated. We
then move on to present two experiments that evaluate our semi-supervised method.
4.1 Data
In our experiments, we use various subsets of the English FrameNet corpus (version 1.3;
Fillmore, Johnson, and Petruck 2003) as seed sets for our semi-supervised method and
as test sets in our evaluation. We only consider sentences with verbal FEEs (60,666 in
total). Furthermore, we always assume that an oracle identifies the verbal predicate, so
recognition of the FEE is not part of our evaluation. Unlabeled sentences for expansion
150
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Table 1
Features used by the frame classifier. Example values for the annotated graph in Figure 1 are
given in parentheses.
Feature Type Description and example value
target lemma atomic lemma of the target node (blink)
frames set frames that can be evoked by the target verb
({BODY MOVEMENT})
voice binary voice of the target node (active)
parent word set lemma of the parents of the target node ({and})
parent POS set part of speech of the parents of the target node ({CC})
rel to parent set grammatical relations between the target node and its
parents ({CONJ})
parent has obj binary whether any parents have an outgoing ?object?
relation (no)
dsubcat atomic subcategorization frame, the multi-set of all outgoing
relations of the target node (DOBJ)
child word set set lemma of the children of the target node ({eye})
child dep set set outgoing relations of the target node ({DOBJ})
child word dep set set pair (lemma, relation) for the children of the target
node ({(eye, DOBJ)})
were taken from the British National Corpus (BNC), excluding sentences with manual
annotations in FrameNet. The BNC is considerably larger compared with FrameNet,
approximately by a factor of 100. Dependency graphs were produced with RASP
(Briscoe, Carroll, and Watson 2006). Frame semantic annotations for labeled sentences
were merged with their dependency-based representations as described in Section 3.1.
Sentences for which this was not possible (mismatch score greater than 0) were excluded
from the seed set, but retained in the test sets to allow for unbiased evaluation. For unla-
beled BNC sentences, we used an existing RASP-parsed version of the BNC (Andersen
et al 2008).
4.2 Supervised SRL System
A natural way of evaluating the proposed semi-supervised method is by comparing
two instantiations of a supervised SRL system, one that is trained solely on FrameNet
annotations and one that also uses the additional training instances produced by our
algorithm. We will henceforth use the term unexpanded to refer to the corpus (and sys-
tem trained on it) that contains only human-annotated instances, and accordingly, the
term expanded to describe the corpus (and system) resulting from the application of our
method or any other semi-supervised approach that obtains training instances automat-
ically. As our approach is based on dependency graphs, we employed a dependency-
based SRL system for evaluation.8
We thus implemented a supervised SRL system based on the features proposed
by Johansson and Nugues (2007a). Many of these features have been found useful in
a number of previous SRL systems, and can be traced back to the seminal work of
Gildea and Jurafsky (2002). Our own implementation uses the features listed in Tables 1
and 2 for frame labeling and role labeling, respectively. Atomic features are converted
8 Semantic role labelers that take advantage of dependency information perform comparably to those that
rely on phrase structure trees (Johansson 2008).
151
Computational Linguistics Volume 38, Number 1
Table 2
Features used by the role classifiers. Example values for the Body part role of the annotated
graph in Figure 1 are given in parentheses.
Feature Type Description and example value
target lemma atomic lemma of the FEE (blink)
target POS atomic part of speech of the FEE (VVD)
roles set roles that can feature in the given frame ({Agent, Body part,
Addressee, ...})
voice binary voice of the FEE (active)
parent word set lemma of the parents of the FEE ({and})
parent POS set part of speech of the parents of the FEE ({CC})
rel to parent set grammatical relation between the FEE and its parents ({CONJ})
parent has obj binary whether any parents have an outgoing ?object? relation (no)
dsubcat atomic subcategorization frame, multi-set of all outgoing relations
of the FEE (DOBJ)
child dep set set outgoing relations of the FEE ({DOBJ})
arg word atomic lemma of the argument (eye)
arg POS atomic part of speech of the argument (NN1)
position atomic position of the argument (before, on, or after) in the sentence,
relative to the FEE (after)
left word atomic lemma of the word to the left of the argument in the sentence (his)
left POS atomic part of speech of the word to the left of the argument in the sentence
(APP$)
right word atomic lemma of the word to the right of the argument in the sentence (and)
right POS atomic part of speech of the word to the right of the argument in the
sentence (CC)
path atomic path of grammatical relations between FEE and argument (DOBJ)
function set relations between argument and its heads ({DOBJ})
into binary features of the SVM by 1-of-k coding, and for set features each possible set
element is represented by its own binary feature. (Features pertaining to parent nodes
are set features as we do not require our dependency graphs to be trees and a node
can therefore have more than one parent.) We followed a classical pipeline architecture,
first predicting a frame name for a given lexical unit, then identifying role-bearing
dependency graph nodes, and finally labeling these nodes with specific roles. All three
classification stages were implemented as support vector machines, using LIBLINEAR
(Fan et al 2008). The frame classifier is trained on instances of all available predicates,
while individual role classifiers are trained for each frame. The one-vs-one strategy
(Friedman 1996) was employed for multi-classification.
We evaluate the performance of the SRL system on a test set in terms of frame accu-
racy and role labeling F1. The former is simply the relative number of correctly identified
frame names. The latter is based on the familiar measure of labeled F1 (the harmonic
mean of labeled precision and recall). When a frame is labeled incorrectly, however, we
assume that its roles are also misclassified. This is in agreement with the notion of frame-
specific roles. Moreover, it allows us to compare the performance of different classifiers,
which would not be possible if we evaluated role labeling performance on changing test
sets, such as the set of only those sentences with correct frame predictions.
The misclassification penalty C for the SVM was optimized on a small training set
consisting of five annotated sentences per predicate randomly sampled from FrameNet.
We varied C for the frame classification, role recognition, and role classification SVMs
152
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
between 0.01 and 10.0 and measured F1 on a test set consisting of 10% of FrameNet (see
Section 5.1). For frame and role classification, we did not observe significant changes
in F1 and therefore maintained the default of C = 1.0. For role recognition, we obtained
best performance with C = 0.1 (F1 was 38.78% compared to 38.04% with the default
C = 1), which we subsequently used for all our experiments. All other SVM parameters
were left at their default values.
4.3 Lexical and Syntactic Similarity
Our definition of the lexical similarity measure, lex, uses a vector space model of word
co-occurrence which we created from a lemmatized version of the BNC. Specifically, we
created a semantic space with a context window of five words on either side of the target
word and the most common 2,000 context words as vector dimensions. Their values
were set to the ratio of the probability of the context word given the target word to the
probability of the context word overall. Previous work shows that this configuration is
optimal for measuring word similarity (Mitchell and Lapata 2010; Bullinaria and Levy
2007). In our specific setting, lex is then simply the cosine of the angle between the
vectors representing any two words.9
For the syntactic measure syn we chose the simplest definition possible: syn(r, r?) is 1
if r and r? denote the same grammatical relation (r = r?), and 0 otherwise. We also con-
sidered more sophisticated definitions based on different degrees of similarity between
grammatical relations, but were not able to find parameters performing consistently
better than this simple approach.
A crucial parameter in the formulation of our similarity score (see Equation (1)) is
the relative weight ? of syntactic compared to lexical similarity. Intuitively, both types
of information should be taken into account, as favoring one over the other may yield
sentences with either similar structure or similar words, but entirely different meaning.
This suggests that ? should be neither very small nor very large and will ultimately also
depend on the specific measures used for lex and syn.
We optimized ? on a development set using F1 score as the objective function.
Specifically, we used a random sample of 20% of the FrameNet instances as seed
corpus and expanded it with instances from the BNC using different values for ?. For
each seed sentence, the most similar neighbor was selected (i.e., k = 1). We evaluated
performance of the role labeler enhanced with automatic annotations on a test set
consisting of another random 10% of the FrameNet instances. (These development and
test sets were not used in any of the subsequent experiments.) The parameter ? ranges
between 0 (using only lexical information) and ? (using only syntactic information).
We therefore performed a grid search on a logarithmic scale, varying log? between ?3
and 3 with steps of size 0.2. We also computed performance in the extreme cases of
log? = ??.
Figure 4 shows the results of the tuning procedure. With the exception of ? = ??
(i.e., ignoring syntactic information) all expansions of the seed corpus lead to better
role labelers in terms of F1. Furthermore, extreme values of ? are clearly not as good as
values that take both types of information into account. The optimal value according
to this tuning experiment is log? = ?0.6. Finer tuning of the parameter will most
9 Experiments with off-the-shelf WordNet-based similarity measures did not yield performance superior to
the cosine measure (see Fu?rstenau [2011] for details).
153
Computational Linguistics Volume 38, Number 1
Figure 4
Performance of our method on the development set for different values of the ? parameter. The
baseline is the performance of a semantic role labeler trained on the seed set.
likely not yield improvements, as the differences in F1 are already relatively small.
We therefore set ? = e?0.6 ? 0.55 for all further experiments. This means that lex is
weighted approximately twice as strongly as syn.
5. Experiment 1: Known Verbs
In this section, we describe a first set of experiments with the aim of automatically
creating novel annotation instances for SRL training. We assume that a small number
of manually labeled instances are available and apply our method to obtain more
annotations for the FEEs attested in the seed corpus. The FEE of the labeled sentence and
the target verb of the unlabeled sentence are presumed identical. However, we waive
this restriction in Experiment 2, where we acquire annotations for unknown FEEs, that
is, predicates for which no manual annotations are available.
5.1 Method
We applied our expansion method to seed corpora of different sizes. A random sample
of 60% of the FrameNet instances was used as training set and 10% as test set (the
remaining 30% were used as development set for tuning the ? parameter). The training
set was reduced in size by randomly choosing between 1 and 10 annotated instances
per FEE. These reduced sets are our seed corpora. We first trained the supervised SRL
system on each of these seed corpora. Next, we used our expansion method to add the
k nearest neighbors of each seed instance to the training corpus, with k ranging from 1
to 6, and retrained the SRL classifiers.
We also compared our approach to self-training by selecting k sentences from the
unlabeled corpus, labeling them with the baseline classifier trained on the unexpanded
corpus (instead of applying our projection method), and then adding these to the
training corpus and retraining the classifier. Specifically, we employed three variants of
self-training. Firstly, unlabeled sentences were selected for each seed sentence randomly,
the only constraint being that both sentences feature the same FEE.
154
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Secondly, new instances were chosen according to a sentence similarity measure
shown to be highly competitive on a paraphrase recognition task (Achananuparp, Hu,
and Shen 2008). We used the measure proposed in Malik, Subramaniam, and Kaushik
(2007), which is a simpler variant of a sentence similarity measure originally described
in Mihalcea, Corley, and Strapparava (2006). Given two sentences or more generally text
segments Ti and Tj, their similarity is determined as follows:
sim(Ti, Tj) =
?
w?Ti
maxSim(w, Tj) +
?
w?Tj
maxSim(w, Ti)
|Ti|+ |Tj|
(7)
where maxSim(w, Tj) is the maximum similarity score between the word w in Ti and any
word in Tj with the same part of speech (i.e., noun, verb, adjective). A large number of
measures have been proposed in the literature for identifying word-to-word similarities
using corpus-based information, a taxonomy such as WordNet (Fellbaum 1998) or a
combination of both (see Budanitsky and Hirst [2001] for an overview). Here, we use
cosine similarity and the vector space model defined in Section 4.3.
Our third variant of self-training identified new instances according to our own
measure (see Section 4.3), which incorporates both lexical and syntactic similarity. The
different self-training settings allow us to assess the extent to which the success of
our method depends simply on the increase of the training data, the definition of the
sentence similarity measure, the alignment algorithm for annotation projection, or their
combination.
5.2 Results
Our results are summarized in Figure 5 (and documented exhaustively in the Ap-
pendix). Here, we only consider role labeling performance, that is, we use gold-standard
Figure 5
Role labeling F1 obtained by expanding seed corpora of different sizes: The dotted lines show
performance of unexpanded classifiers trained on two to six seed instances per verb. Each
solid line starts from such a baseline at k = 0 and for k > 0 shows the performance obtained
by adding the k nearest neighbors of each seed to the respective baseline corpus.
155
Computational Linguistics Volume 38, Number 1
frames of the test set and evaluate the role recognition and classification stages of the
classifiers. (Frame labeling accuracy will be evaluated in the following section.) The
dotted lines show the performance of unexpanded classifiers trained on two to six seed
instances per verb. The solid lines show the performance of our expanded classifiers
when the k nearest neighbors (of each seed instance) are added to the training set. So, to
give a concrete example, the unexpanded classifier trained on a corpus with two seeds
per verb yields an F1 of 35.94%. When the single nearest neighbors are added, F1 in-
creases to 36.63%, when the two nearest neighbors are added, F1 increases to 37.00%,
and so on.
As can be seen in Figure 5, most expansions lead to improved SRL performance. All
improvements for 1 ? k ? 5 are statistically significant (at p < 0.05 and p < 0.001) as
determined by stratified shuffling (Noreen 1989; see the Appendix for details). The only
exception is k = 5 for two seeds per FEE. We obtain largest improvements when k ranges
between 2 and 4, with a decline in performance for higher values of k. This illustrates the
trade-off between acquiring many novel annotations and inevitably introducing noise.
For progressively less similar neighbors, the positive effect of the former is out-weighted
by the detrimental effect of the latter.
It is also interesting to observe that automatically generated instances often have
a positive effect on role labeling performance similar to, or even larger than, manually
labeled instances. For example, the corpus with two seeds per FEE, expanded by two,
three or four nearest neighbors, leads to better performance than the corpus with three
manually labeled seeds; and an expanded version of the five seeds/FEE corpus closes
60% of the gap to the six seeds/FEE corpus. Generally, the positive effect of our expan-
sion method is largest for corpora with only a few seed instances per FEE. The results in
Figure 5 may seem low, especially with respect to the state of the art (see the discussion
in Section 1). Bear in mind, however, that the semantic role labeler is trained on a small
fraction of the available annotated data. This fits well with its intended application to
minimize annotation effort when creating resources for new languages or adapting to
new domains.
Figure 6 shows the results of self-training. Dotted lines again denote the perfor-
mance of unexpanded classifiers trained on seed corpora of different sizes (ranging
from two to five seeds per verb). The solid lines show the performance of these clas-
sifiers expanded with k neighbors. Figures 6(a)?6(c) correspond to different methods
for selecting the k-best sentences to add to the seed corpus (i.e., randomly, according
to the similarity function presented in Malik, Subramaniam, and Kaushik (2007), and
our own similarity measure that takes both syntactic and semantic information into
account). In all cases we observe that self-training cannot improve upon the baseline
classifier. Randomly selecting new sentences yields the lowest F1 scores, followed by
Malik, Subramaniam, and Kaushik and our own measure. Figure 6(d) compares the
three self-training methods in the five seeds per verb setting. These results indicate that
the ability to improve labeling performance is not merely due to selecting sentences
similar to the seeds. In other words, the graph alignment algorithm is worth the added
work as the projection of annotations contributes to achieving better SRL results.
To gain a better understanding of the quality of the annotations inferred by our
system, we further analyzed a small sample. Specifically, we randomly selected 100 seed
instances from the FrameNet corpus, and used 59,566 instances as the unlabeled ex-
pansion corpus, treating their gold standard annotations as unseen (the remaining
1,000 instances were held out as a separate test set, as discussed subsequently). Seed
and expansion corpora were thus proportionately similar to those used in our main
experiments (where seed instances in the range of [2,092?16,595] were complemented
156
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Figure 6
Role labeling F1 with self-training; dotted lines show the performance of unexpanded classifiers
trained on two to five seed instances per verb. Each solid line starts from such a baseline at k = 0
and for k > 0 shows the performance obtained by adding k sentences with the same FEE to the
respective baseline corpus.
with approximately 6 million unlabeled BNC sentences). For each of the 100 seeds,
we projected annotations to their nearest neighbors according to our algorithm, and
compared their quality to the held-out gold standard. Figure 7 reports labeled F1 for
the sets of d-th neighbors. Unlike the neighbors used in our previous experiments, these
are mutually exclusive. In other words, the set for d = 1 includes only the first most
similar neighbors, for d = 2 the second most similar neighbors, and so on. As expected,
we observe decreasing quality for more distant neighbors, falling from 44.24% for d = 1
to 20.53% for d = 12.
Next, we examined how the quality of the novel annotations impacts the semi-
supervised learning task when these are used as additional training data. As in our
previous experiments, we trained the system on the 100 seed sentences alone to obtain
an ?unexpanded? baseline and on several ?expanded? versions containing the seeds
and one of the d = 1, . . . , 12 sets. The resulting role labeling systems were evaluated
on the 1,000 held-out test sentences mentioned previously. As shown in Figure 7,
performance increases for intermediate values of d and then progressively decreases
for larger values. The performance of the expanded classifiers corresponds closely to
the quality of the projected annotations (or lack thereof). We observe substantial gains
157
Computational Linguistics Volume 38, Number 1
for the sets d = 1, . . . , 6 compared to the baseline role labeler. The latter achieves an F1
of 9.06% which increases to 12.82% for d = 1 neighbors, to 11.61% for d = 2 neighbors,
and so on. In general, improvements in semantic role labeling occur when the projected
annotations maintain an F1 quality in the range of [40?30%]. When F1 drops below 30%,
improvements are relatively small and finally disappear.
We also manually inspected the projected annotations in the set of first neighbors
(i.e., d = 1). Of these, 33.3% matched the gold standard exactly, 55.5% received the right
frame but showed one or more role labeling errors, and 11.1% were labeled with an
incorrect frame. We further analyzed sentences with incorrect roles and found that
for 22.5% of them this was caused by parser errors, whereas another 42.5% could not
have received a correct annotation in the first place by any alignment, because there was
no node in the dependency graph whose yield exactly corresponded to the annotated
substring of the gold standard. This was again due to parser errors or to FrameNet
specific idiosyncrasies (e.g., the fact that roles may span more than one constituent).
For 35.0% of these sentences, the incorrect roles were genuine failures of our projection
algorithm. Some of these failures are due to subtle role distinctions (e.g., Partner1
and Partners for the frame FORMING RELATIONSHIPS), whereas others require detailed
linguistic knowledge which the parser does not capture either by mistake or by design.
For example, seed sentences without overtly realized subjects (such as imperatives) can
lead to incomplete annotations, missing on the Agent role.
In total, we found that parser errors contributed to 45.8% of the erroneous annota-
tions. The remaining errors range from minor problems, which could be fixed by more
careful preprocessing or more linguistically aware features in the similarity function,
to subtle distinctions in the FrameNet annotation, which are not easily addressed by
computational methods. As parsing errors are the main source of projection errors, one
would expect improvements in semantic role labeling with more accurate parsers. We
leave a detailed comparison of dependency parsers and their influence on our method
to future work, however. Moreover, our results demonstrate that some mileage can be
Figure 7
Evaluation of annotations projected onto the d-th neighbors of 100 randomly chosen seed
sentences. The quality of the novel annotations is evaluated directly against held-out
gold-standard data, and indirectly when these are used as training data for an SRL system.
In both cases, we measure labeled F1.
158
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
gained from annotation projection in spite of parser noise. In fact, comparison with
self-training indicates that annotation projection is a major contributor to performance
improvements.
6. Experiment 2: Unknown Verbs
In this section, we describe a second set of experiments, where our method is ap-
plied to acquire novel instances for unknown FEEs, that is, predicates for which no
manually labeled instances are available. Unknown predicates present a major obstacle
to existing supervised SRL systems. Labeling performance on such predicates is typi-
cally poor due to the lack of specific training material for learning (Baker, Ellsworth,
and Erk 2007).
6.1 Method
To simulate frame and role labeling for unknown FEEs, we divided the set of verbal
FEEs in FrameNet into two sets, namely, ?known? and ?unknown.? All annotations of
verbs marked as ?unknown? made up the test set, and the annotations for the ?known?
verbs were the seed corpus (in both cases excluding the 30% of FrameNet used as
development set). To get a balanced division, we sorted all verbal predicates by their
number of annotated sentences and marked every fifth verb in the resulting list (i.e., 20%
of the verbs) as ?unknown,? the rest as ?known.? We used our expansion algorithm to
automatically produce labeled instances for unknown verbs, selecting the most similar
neighbor of each seed sentence (k = 1). Then we trained the SRL system on both the
seeds and the new annotations and tested it on the held-out instances of the ?unknown?
verbs.
6.2 Frame Candidates
So far we have made the simplifying assumption (see Experiment 1, Section 5) that the
FEE of the labeled sentence and the target verb of the unlabeled sentence are identical.
This assumption is not strictly necessary in our framework; however, it reduces com-
putational effort and ensures precision that is higher than would be expected when
comparing arbitrary pairs of verbs. When acquiring novel instances for unseen FEEs, it
is no longer possible to consider identical verbs. The vast majority of seeds, however,
will be inappropriate for a given unlabeled sentence, because their predicates relate to
different situations. So, in order to maintain high precision, and to make expansions
computationally feasible, we must first identify the seeds that might be relevant for a
sentence featuring an unknown predicate. In the following, we propose two methods
for determining frame candidates for an unknown verb, one using vector-based simi-
larity and one that takes WordNet information into account. As we shall see, WordNet-
based similarity yields significantly better results, but its application is restricted to
languages or domains with similar resources.
6.2.1 Vector-based Method. To associate unknown FEEs with known frames, Pennacchiotti
et al (2008) make use of a simple co-occurrence-based semantic space similar to the one
we used to define the lexical measure lex. They represent each FEE v by a vector v and
159
Computational Linguistics Volume 38, Number 1
then compute a vector representation f for a frame f as the weighted centroid of the
vectors of all words evoking it:
f =
?
v?f
wvfv (8)
The weight wvf is operationalized as the relative frequency of v among the FEEs evok-
ing f , counted over the corpus used in building the vector space. The (cosine) similarity
between the unknown target v0 and each frame vector f produces an ordering of frames,
the n-best of which are considered frame candidates.
simV(v0, f ) = cos
(
v0,f
)
(9)
6.2.2 WordNet-based Method. In addition to the vector-based approach, Pennacchiotti
et al (2008) propose a method that is based on WordNet (Fellbaum 1998) and treats
nouns, verbs, and adjectives differently. Given a frame and an unknown verb v0, they
count the number of FEEs that are co-hyponyms of v0 in WordNet. If the number of
co-hyponyms exceeds a threshold ?,10 then the frame is considered a candidate for v0.
In our experiments, we found this method to perform poorly. This suggests that
the improvements reported in Pennacchiotti et al (2008) are due to their more refined
treatment of nouns, which are not considered in our set-up. We thus follow the basic
idea of measuring relatedness between an unknown verb v0 and the set of lexical
units of a frame, and propose a measure based on counts of synonyms, hypernyms,
hyponyms, and co-hyponyms in WordNet. We define:
simW (v0, f ) =
?
v?F
r(v0, v) (10)
where r(v, v?) is 1 if v and v? are synonyms, 12 if one is a hypernym of the other,
1
4 if
they are co-hyponyms, and 0 otherwise. These numbers were chosen heuristically to
represent different degrees of relatedness in WordNet. Relations more distant than co-
hyponymy did not improve performance, as the verb hierarchy in WordNet is shallow.
It therefore seems unlikely that much could be gained by refining the measure r, for
example, by incorporating traditional WordNet similarity measures (e.g., Budanitsky
and Hirst 2001).
6.2.3 Method Comparison. To evaluate which of the methods just described performs best,
we used a leave-one-out procedure over the FrameNet predicates marked as ?known?
in our experimental set-up. Specifically, we set aside one predicate at a time and use
all remaining predicates to predict its frame candidates. The resulting candidates are
then compared to the true frames evoked by the predicate. (We do not consider ?un-
known? predicates here as these are reserved for evaluating the expansion method as a
whole.) For the vector-based method we also explore an unweighted variant, setting all
wvf = 1.
Evaluation results are summarized in Figure 8, which shows the proportion of
predicates for which at least one frame candidate is among the true evokable frames
10 Set to ? = 2 according to personal communication.
160
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Figure 8
Frame labeling accuracy out of n frame candidates; open circles indicate vector-based similarity;
black circles indicate WordNet-based similarity.
(when considering up to 10 best candidates).11 As can be seen, performance increases
by a large margin when unweighted centroids are considered instead of weighted ones.
Apparently, the stabilizing effect of the centroid computation, which allows common
meaning aspects of the predicates to reinforce each other and reduces the effect of spuri-
ous word senses, is more pronounced when all predicates are weighted equally. Figure 8
also shows that a WordNet-based approach that takes into account various kinds of
semantic relations is superior to vector-based methods and to Pennachiotti et al?s (2008)
original proposal based only on co-hyponyms. All subsequent experiments will identify
frame candidates using our WordNet-based definition (Equation (10)).
6.3 Results
Evaluation results of our approach on unknown verbs are summarized in Figure 9.
Frame labeling accuracy is shown in Figure 9(a) and role labeling performance in
Figure 9(b).
As far as frame labeling accuracy is concerned, we compare a semantic role labeler
trained on additional annotations produced by our method against a baseline classifier
trained on known verbs only. Both expanded and unexpanded classifiers choose frames
from the same sets of candidates, which is also the set of frames that the expansion
algorithm is considering. We could have let the unexpanded classifier select among the
entire set of FrameNet frames (more than 500 in total). This would perform poorly,
however, and our evaluation would conflate the effect of additional training material
with the effect of restricting the set of possible frame predictions to likely candidates.
11 Note that although our evaluation is similar to Pennacchiotti et al (2008) the numbers are not strictly
comparable due to differences in the test sets, as well as the fact that they consider FEEs across parts of
speech (not only verbs) and omit infrequent predicates.
161
Computational Linguistics Volume 38, Number 1
Figure 9
Frame labeling accuracy (a) and role labeling performance (b); comparison between unexpanded
and expanded classifiers and random baseline; frame candidates selected based on WordNet.
We also show the accuracy of a simple baseline labeler, which chooses one of the k
candidate frames at random.
As illustrated in Figure 9(a), both expanded and unexpanded classifiers outperform
the random baseline by a wide margin. This indicates that the SRL system is indeed able
to generalize to unknown predicates, even without specific training data. The expanded
classifier is in turn consistently better than the unexpanded one for all numbers of
frame candidates (x axis). The case where only one frame candidate (k = 1) is considered
deserves a special mention. Here, a predicate is assigned the frame most similar to it,
irrespectively of its sentential context. In other words, all instances of the predicate
are assigned the same frame, without any attempt at disambiguation. In this case,
both expanded and unexpanded classifiers obtain the same performance. Although
the unexpanded classifier does not improve over and above this type-based frame
labeling approach, however, the expanded classifier yields significantly better results
for two candidates (p < 0.01 with McNemar?s test). This means that the additional
162
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
training material enables the classifier to successfully favor lower scoring candidates
over higher-scoring ones based on sentential context.
Figure 9(b) shows our results for the role labeling task. We again compare ex-
panded and unexpanded classifiers. Note that there is no obvious random baseline
for the complex task of predicting role spans and their labels, however. Again, we
observe that the expanded classifier outperforms the unexpanded one, save the arti-
ficial case of one candidate where it yields slightly lower results. In this configuration,
our expansion framework cannot account for FEEs that are polysemous by selecting
among different frames, and as a result role labeling performance is compromised.
For two candidates the expanded classifier yields significantly better results than this
token-based approach (p < 0.05 with stratified shuffling). For three, four, and five can-
didates, performance is also numerically better, but the results do not reach statistical
significance. This shows that the expanded classifier is not only able to correctly select
lower scoring frame candidates for unknown verbs, but also to accurately label their
roles. The overall scale of our F1 scores might seem low. This is due to both the
difficulty of the task of predicting fine-grained sense distinctions for verbs without
specific training data, and the comprehensive evaluation measure, which takes into
account all three stages of the SRL system: frame labeling, role recognition, and role
classification.
Incidentally, we should point out that similar tendencies are observed when using
vector-based similarity for identifying the frame candidates. Although overall classi-
fier performance is worse, results are qualitatively similar: The expanded classifiers
outperform the unexpanded ones, and obtain best frame accuracy and labeled F1
with two candidates. Performance also significantly improves compared to selecting a
frame randomly or defaulting to the first candidate (we summarize these results in the
Appendix).
7. Conclusions
We have presented a novel semi-supervised approach for reducing the annotation effort
involved in creating resources for semantic role labeling. Our method automatically
produces training instances from an unlabeled corpus. The key idea is to project an-
notations from labeled sentences onto similar unlabeled ones. We formalize the projec-
tion task as a graph alignment problem. Specifically, we optimize alignments between
dependency graphs under an objective function that takes both lexical and structural
similarity into account. The optimization problem is solved exactly by an integer linear
program.
Experimental results show that the additional training instances produced by our
method significantly improve role labeling performance of a supervised SRL system on
predicates for which only a few or no manually labeled training instances are available.
In the latter case, we first determine suitable frame candidates, improving over similar
methods proposed in the literature. Comparison with a self-training approach shows
that the improvements attained with our method are not merely a side effect of addi-
tional training data. Rather, by identifying sentences that are structurally and lexically
similar to the labeled seeds we are able to acquire qualitatively novel annotations.
Our experiments make use of relatively simple similarity measures, which could be
improved in future work. Incorporating a notion of selectional preferences would allow
for finer-grained distinctions in computing argument similarities. Analogously, our
definition of syntactic similarity could be refined by considering grammar formalisms
163
Computational Linguistics Volume 38, Number 1
with richer syntactic categories such as Combinatory Categorial Grammar (Steedman
2000).
Possible extensions to the work presented in this article are many and varied.
For example, we could combine our approach with cross-lingual annotation projection
(Johansson and Nugues 2006; Pado? and Lapata 2009). For languages without any role
semantic resources, initial annotations could be obtained by cross-lingual projection and
then extended with our semi-supervised method. Another application of our frame-
work would be in domain adaptation, where a supervised model is trained on a seed
corpus, and then unlabeled data from a target domain is used to select new instances
and thus train a new semantic role labeler for the given domain. As our algorithm
produces novel annotated sentences, it could also be used to reduce annotation effort
by offering automatically labeled sentences to humans to inspect and correct. The
experiments presented here are limited to verbal categories and focus solely on English.
In the future, we would like to examine whether our approach generalizes to other
syntactic categories such as nouns, adjectives, and prepositions. An obvious extension
also involves experiments with other languages. Experiments on the SALSA corpus
(Burchardt et al 2006) show that similar improvements can be obtained for German
(Fu?rstenau 2011).
Finally, the general formulation of our expansion framework allows its application
to other tasks. Deschacht and Moens (2009) adapt our approach to augment subsets
of the PropBank corpus and observe improvements over a supervised system for a
small seed corpus. They also show that defining the lexical similarity measure in terms
of Jensen?Shannon divergence instead of cosine similarity can additionally improve
performance. Another possibility would be to employ our framework for the acquisition
of paraphrases, for example, by extending the multiple-sequence alignment approach
of Barzilay and Lee (2003) with our notion of graph alignments. Finally, it would be
interesting to investigate how to reduce the dependency on full syntactic analyses, for
example, by employing shallow parsers or chunkers.
Appendix: Detailed Experimental Results
In this appendix, we give complete results for the expansion experiments discussed in
Sections 5 and 6. Asterisks in the tables indicate levels of significance. For simplicity, we
only present two levels of significance, p < 0.05 with a single asterisk (*) and p < 0.001
with two asterisks (**). Significance tests for exact match and frame labeling accuracy
were performed using McNemar?s test. We used stratified shuffling Noreen (1989) to
examine whether differences in labeled F1 were significant.12
Experiments on Known Predicates. The following table shows the performance of ex-
panded classifiers when [1?6] automatically generated nearest neighbors (NN) are
added to seed corpora containing [1?10] manually labeled sentences per verb. We report
precision (Prec), recall (Rec), their harmonic mean (F1), and exact match (ExMatch; the
proportion of sentences that receive entirely correct frame and role annotations). Some
of these results were visualized in Figure 5.
12 We used the sigf tool (Pado? 2006).
164
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Training set Size Prec (%) Rec (%) F1 (%) ExMatch (%)
1 seed/verb 2, 092 40.74 23.69 29.96 6.38
+ 1-NN 3, 297 40.52 24.23 30.33 6.81 *
+ 2-NN 4, 481 40.29 24.99 30.85 * 6.97 *
+ 3-NN 5, 649 39.52 25.02 30.64 * 7.35 **
+ 4-NN 6, 803 39.52 25.39 30.92 * 7.30 **
+ 5-NN 7, 947 39.04 25.34 30.73 * 7.12 *
+ 6-NN 9, 076 38.40 25.16 30.40 6.89
2 seeds/verb 4, 105 45.22 29.81 35.94 9.40
+ 1-NN 6, 500 45.09 30.84 36.63 * 10.19 **
+ 2-NN 8, 850 44.82 31.50 37.00 ** 10.32 **
+ 3-NN 11, 157 44.65 31.85 37.18 ** 10.32 **
+ 4-NN 13, 423 43.99 31.94 37.01 ** 10.15 *
+ 5-NN 15, 652 42.64 31.23 36.05 9.73
+ 6-NN 17, 846 42.57 31.36 36.11 9.63
3 seeds/verb 6, 021 45.03 31.29 36.92 9.81
+ 1-NN 9, 492 44.78 32.45 37.63 * 10.35 *
+ 2-NN 12, 874 44.15 32.69 37.57 * 10.37 *
+ 3-NN 16, 179 43.90 33.00 37.68 * 10.68 *
+ 4-NN 19, 424 43.60 33.36 37.80 * 10.35
+ 5-NN 22, 609 43.15 33.26 37.56 * 10.50 *
+ 6-NN 25, 734 42.72 33.17 37.34 10.45 *
4 seeds/verb 7, 823 44.42 32.21 37.35 9.48
+ 1-NN 12, 321 44.45 33.31 38.09 * 10.20 **
+ 2-NN 16, 688 44.26 34.13 38.54 ** 10.40 **
+ 3-NN 20, 944 43.71 34.20 38.37 ** 10.72 **
+ 4-NN 25, 098 43.37 34.35 38.34 ** 10.57 **
+ 5-NN 29, 166 43.25 34.45 38.35 * 10.67 **
+ 6-NN 33, 142 42.48 34.24 37.92 10.40 *
5 seeds/verb 9, 515 45.45 33.81 38.78 10.35
+ 1-NN 15, 026 45.47 34.90 39.49 * 10.95 *
+ 2-NN 20, 363 45.03 35.39 39.63 * 11.42 **
+ 3-NN 25, 533 44.56 35.51 39.53 * 11.56 **
+ 4-NN 30, 576 44.44 35.78 39.64 * 11.70 **
+ 5-NN 35, 494 44.22 35.94 39.65 * 11.72 **
+ 6-NN 40, 286 43.74 35.83 39.39 * 11.49 **
6 seeds/verb 11, 105 46.50 35.44 40.22 10.95
+ 1-NN 17, 553 46.05 36.11 40.48 11.56 *
+ 2-NN 23, 779 45.71 36.67 40.70 12.07 **
+ 3-NN 29, 787 45.16 36.83 40.57 11.92 **
+ 4-NN 35, 623 44.82 36.92 40.49 11.80 *
+ 5-NN 41, 310 44.60 36.91 40.40 12.13 **
+ 6-NN 46, 851 44.07 36.86 40.14 12.02 **
8 seeds/verb 13, 999 47.60 37.29 41.82 12.25
+ 1-NN 22, 115 47.08 37.71 41.88 12.48
+ 2-NN 29, 907 46.45 38.01 41.81 12.64
+ 3-NN 37, 400 46.01 38.11 41.69 12.69
+ 4-NN 44, 656 45.55 38.12 41.51 12.78
+ 5-NN 51, 705 45.53 38.38 41.65 13.22 *
+ 6-NN 58, 562 45.00 38.24 41.34 13.34 **
10 seeds/verb 16, 595 48.97 39.02 43.43 13.73
+ 1-NN 26, 180 48.24 39.55 43.47 14.01
+ 2-NN 35, 336 47.11 39.32 42.86 13.80
+ 3-NN 44, 113 46.69 39.45 42.77 13.85
+ 4-NN 52, 602 46.18 39.31 42.47 13.63
+ 5-NN 60, 827 46.22 39.76 42.75 13.68
+ 6-NN 68, 791 45.69 39.58 42.42 13.95
165
Computational Linguistics Volume 38, Number 1
Experiments on Unknown Predicates. In the following, we show the performance of un-
expanded and expanded classifiers when selecting among [1?5] frame candidates gen-
erated by the WordNet-based method. We report frame labeling accuracy, role labeling
performance, and exact match scores. Asterisks indicate that the expanded classifier is
significantly better than an unexpanded classifier choosing among the same number
of candidates. For frame labeling accuracy, we additionally provide the results of the
random baseline and an upper bound, which always chooses the correct frame if it is
among the candidates. Some of these results were shown in Figure 9.
Frame labeling accuracy (%)
Candidates Random Unexpanded Expanded Upper bound
1 45.50 45.50 45.50 45.50
2 29.61 41.24 46.89 ** 59.23
3 22.20 36.02 44.82 ** 66.60
4 17.31 28.75 44.75 ** 69.23
5 14.45 26.56 43.58 ** 72.25
Unexpanded (%) Expanded (%)
Candidates Prec Rec F1 ExMatch Prec Rec F1 ExMatch
1 24.77 18.94 21.47 6.54 23.61 18.72 20.88 6.56
2 22.52 17.63 19.78 5.87 24.60 20.05 22.09 ** 7.02 **
3 19.52 15.20 17.09 5.04 24.23 19.79 21.79 ** 7.24 **
4 16.18 12.31 13.98 4.02 24.59 20.09 22.11 ** 7.26 **
5 14.78 11.27 12.78 3.77 24.12 19.70 21.69 ** 7.44 **
For two candidates, the expanded classifier also performs significantly better than the
best unexpanded classifier (i.e., the one given only one candidate) in terms of frame
labeling accuracy, F1, and exact match (p < 0.05). In terms of exact match, it also
performs significantly better for three candidates (p < 0.05), four candidates (p < 0.05),
and five candidates (p < 0.001).
Vector-based frame candidates. The following graphs show the performance of the un-
expanded and expanded classifiers when frame candidates are selected by the vector-
based method. The expanded classifiers significantly outperform the unexpanded ones
in terms of frame labeling accuracy and role labeling F1 for [2?5] candidates (p < 0.001).
For two candidates, frame labeling accuracy and role labeling F1 also significantly
improve compared with the type-based approach of always choosing the first candidate
(p < 0.001). For three candidates performance is significantly better only in terms of
frame labeling accuracy (p < 0.05) but not F1.
166
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
a.
10
15
20
25
30
35
40
1 2 3 4 5
Accuracy
frame candidates
unexpanded classifier
 




expanded classifier






random baseline





b.
10
11
12
13
14
15
16
17
18
19
1 2 3 4 5
F1
frame candidates
unexpanded classifier





 expanded classifier


 


Acknowledgments
We are grateful to the anonymous referees,
whose feedback helped to substantially
improve this article. Special thanks are due
to Richard Johansson for his help with
the re-implementation of his semantic role
labeler and Manfred Pinkal for insightful
comments and suggestions. We acknowledge
the support of EPSRC (Lapata; grant
GR/T04540/01) and DFG (Fu?rstenau;
IRTG 715 and project PI 154/9-3).
References
Abend, Omri, Roi Reichart, and Ari
Rappoport. 2009. Unsupervised argument
identification for semantic role labeling.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural
Language Processing of the AFNLP,
pages 28?36, Singapore.
Achananuparp, Palakorn, Xiaohua Hu, and
Xiajiong Shen. 2008. The evaluation of
167
Computational Linguistics Volume 38, Number 1
sentence similarity measures.
In Proceedings of the 10th International
Conference on Data Warehousing and
Knowledge Discovery, pages 305?316,
Turin.
Andersen, ?istein E., Julien Nioche,
Ted Briscoe, and John Carroll. 2008.
The BNC parsed with RASP4UIMA.
In Proceedings of LREC, pages 865?869,
Marrakech.
Baker, Collin F., Michael Ellsworth, and
Katrin Erk. 2007. SemEval-2007 Task 19:
Frame Semantic structure extraction.
In Proceedings of the 4th International
Workshop on Semantic Evaluations,
pages 99?104, Prague.
Barzilay, Regina and Mirella Lapata. 2006.
Aggregation via set partitioning for
natural language generation. In Proceedings
of the Human Language Technology
Conference of the NAACL, pages 359?366,
New York, NY.
Barzilay, Regina and Lillian Lee. 2003.
Learning to paraphrase: An unsupervised
approach using multiple-sequence
alignment. In Proceedings of the 2003 Human
Language Technology Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 16?23,
Edmonton.
Briscoe, Ted, John Carroll, and Rebecca
Watson. 2006. The second release
of the RASP system. In Proceedings
of the COLING/ACL 2006 Interactive
Presentation Sessions, pages 77?80,
Sydney.
Budanitsky, Alexander and Graeme Hirst.
2001. Semantic distance in WordNet:
An experimental, application-oriented
evaluation of five measures. In Proceedings
of the ACL Workshop on WordNet and
other Lexical Resources, pages 29?34,
Pittsburgh, PA.
Bullinaria, John A. and Joseph P. Levy.
2007. Extracting semantic representations
from word co-occurrence statistics:
A computational study. Behavior
Research Methods, 39:510?526.
Burchardt, Aljoscha, Katrin Erk, and
Anette Frank. 2005. A WordNet detour
to FrameNet. In Proceedings of the GLDV
GermaNet II Workshop, pages 408?421,
Bonn.
Burchardt, Aljoscha, Katrin Erk, Anette
Frank, Andrea Kowalski, Sebastian Pado?,
and Manfred Pinkal. 2006. The SALSA
corpus: A German corpus resource for
lexical semantics. In Proceedings of LREC,
pages 969?974, Genoa.
Chang, Ming-Wei, Dan Goldwasser,
Dan Roth, and Vivek Srikumar. 2010.
Discriminative learning over constrained
latent representations. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 429?437, Los Angeles, CA.
Clarke, James and Mirella Lapata. 2008.
Global inference for sentence compression:
An integer linear programming approach.
Journal of Artificial Intelligence Research,
31:273?381.
Cormen, Thomas H., Charles E. Leiserson,
and Ronald L. Rivest. 1992. Introduction
to Algorithms. The MIT Press,
Cambridge, MA.
Das, Dipanjan, Nathan Schneider,
Desai Chen, and Noah A. Smith. 2010.
Probabilistic Frame-Semantic parsing.
In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 948?956,
Los Angeles, CA.
Das, Dipanjan and Noah A. Smith. 2009.
Paraphrase identification as probabilistic
quasi-synchronous recognition. In
Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on
Natural Language Processing of the AFNLP,
pages 468?476, Singapore.
de Marneffe, Marie-Catherine, Trond
Grenager, Bill MacCartney, Daniel Cer,
Daniel Ramage, Chloe? Kiddon, and
Christopher D. Manning. 2007. Aligning
semantic graphs for textual inference
and machine reading. In AAAI Spring
Symposium at Stanford.
de Salvo Braz, Rodrigo, Roxana Girju,
Vasin Punyakanok, Dan Roth, and Mark
Sammons. 2005. An inference model for
semantic entailment in natural language.
In Proceedings of the 20th National Conference
on Artificial Intelligence, pages 1043?1049,
Pittsburgh, PA.
Denis, Pascal and Jason Baldridge. 2007.
Joint determination of anaphoricity and
coreference resolution using integer
programming. In Human Language
Technologies 2007: The Conference of the
North American Chapter of the Association
for Computational Linguistics; Proceedings
of the Main Conference, pages 236?243,
Rochester, NY.
Deschacht, Koen and Marie-Francine Moens.
2009. Semi-supervised semantic role
labeling using the Latent Words Language
168
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Model. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language
Processing, pages 21?29, Singapore.
Dras, Mark. 1999. Tree Adjoining Grammar
and the Reluctant Paraphrasing of Text.
Ph.D. thesis, Macquarie University,
Sydney.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui
Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008. LIBLINEAR: A library for large linear
classification. Journal of Machine Learning
Research, 9:1871?1874.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Database. MIT Press,
Cambridge, MA.
Fillmore, Charles J. 1968. The case for case.
In Emmon Bach and Robert T. Harms,
editors, Universals in Linguistic Theory.
Holt, Rinehart & Winston, New York,
NY, pages 1?88.
Fillmore, Charles J., Christopher R. Johnson,
and Miriam R. L. Petruck. 2003.
Background to FrameNet. International
Journal of Lexicography, 16:235?250.
Friedman, Jerome H. 1996. Another
approach to polychotomous classification.
Technical report, Department of Statistics,
Stanford University.
Fung, Pascale and Benfeng Chen. 2004.
BiFrameNet: Bilingual Frame Semantics
resource construction by cross-lingual
induction. In Proceedings of COLING 2004,
pages 931?937, Geneva.
Fu?rstenau, Hagen. 2008. Enriching frame
semantic resources with dependency
graphs. In Proceedings of LREC,
pages 1478?1484, Marrakech.
Fu?rstenau, Hagen. 2011. Semi-supervised
Semantic Role Labeling via Graph
Alignment, volume 32 of Saarbru?cken
Dissertations in Computational Linguistics
and Language Technology. German Research
Center for Artificial Intelligence and
Saarland University, Saarbru?cken,
Germany.
Gildea, Daniel and Dan Jurafsky. 2002.
Automatic labeling of semantic
roles. Computational Linguistics,
28(3):245?288.
Gordon, Andrew and Reid Swanson. 2007.
Generalizing semantic role annotations
across syntactically similar verbs.
In Proceedings of the 45th Annual Meeting
of the Association of Computational
Linguistics, pages 192?199, Prague.
Grenager, Trond and Christopher D.
Manning. 2006. Unsupervised discovery of
a statistical verb lexicon. In Proceedings of
the 2006 Conference on Empirical Methods in
Natural Language Processing, pages 1?8,
Sydney.
Haghighi, Aria D., Andrew Y. Ng, and
Christopher D. Manning. 2005. Robust
textual inference via graph matching.
In Proceedings of Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing, pages 387?394, Vancouver.
Johansson, Richard. 2008. Dependency-based
Semantic Analysis of Natural-language Text.
Ph.D. thesis, Department of Computer
Science, Lund University, Sweden.
Johansson, Richard and Pierre Nugues.
2006. A FrameNet-based semantic role
labeler for Swedish. In Proceedings of the
COLING/ACL 2006 Main Conference Poster
Sessions, pages 436?443, Sydney.
Johansson, Richard and Pierre Nugues.
2007a. Syntactic representations
considered for frame-semantic analysis.
In Proceedings of the Sixth International
Workshop on Treebanks and Linguistic
Theories (TLT 2007), Bergen.
Johansson, Richard and Pierre Nugues.
2007b. Using WordNet to extend
FrameNet coverage. In Proceedings of the
NODALIDA-2007 Workshop FRAME
2007: Building Frame Semantics Resources
for Scandinavian and Baltic Languages,
pages 27?30, Tartu.
Klau, Gunnar W. 2009. A new graph-based
method for pairwise global network
alignment. BMC Bioinformatics,
10 (Suppl 1):S59.
Land, Ailsa H. and Alison G. Doig. 1960.
An automatic method for solving discrete
programming problems. Econometrica,
28(3):497?520.
Lang, Joel and Mirella Lapata. 2010.
Unsupervised induction of semantic
roles. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 939?947,
Los Angeles, CA.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press.
Malik, Rahul, L. Venkata Subramaniam,
and Saroj Kaushik. 2007. Automatically
selecting answer templates to respond
to customer emails. In Proceedings of the
20th International Joint Conference on
Artificial Intelligence, pages 1659?1664,
Hyderabad.
Marciniak, Tomasz and Michael Strube.
2005. Beyond the pipeline: Discrete
optimization in NLP. In Proceedings of
169
Computational Linguistics Volume 38, Number 1
the 9th Conference on Computational Natural
Language Learning (CoNLL-2005),
pages 136?143, Ann Arbor, MI.
Matsubayashi, Yuichiroh, Naoaki Okazaki,
and Jun?ichi Tsujii. 2009. A comparative
study on generalization of semantic
roles in FrameNet. In Proceedings of
the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language
Processing of the AFNLP, pages 19?27,
Singapore.
Matusov, Evgeny, Richard Zens, and
Hermann Ney. 2004. Symmetric word
alignments for statistical matching
translation. In Proceedings of COLING
2004, pages 219?225, Geneva.
Melli, Gabor, Yang Wang, Yudong Liu,
Mehdi M. Kashani, Zhongmin Shi,
Baohua Gu, Anoop Sarkar, and Fred
Popowich. 2005. Description of SQUASH,
the SFU question answering summary
handler for the DUC-2005 summarization
task. In Proceedings of the HLT-EMNLP
Document Understanding Workshop,
Vancouver.
Mihalcea, Rada, Courtney Corley, and
Carlo Strapparava. 2006. Corpus-based
and knowledge-based measures of
text semantic similarity. In Proceedings
of the 21st National Conference on
Artificial Intelligence, pages 775?780,
Boston, MA.
Mitchell, Jeff and Mirella Lapata. 2010.
Composition in distributional models
of semantics. Cognitive Science,
(34):1388?1429.
Noreen, Eric. 1989. Computer-intensive
Methods for Testing Hypotheses: An
Introduction. New York, Wiley.
Pado?, Sebastian, 2006. User?s guide to
sigf: Significance testing by approximate
randomization. Available at:
www.nlpado.de/?sebastian/
software/sigf.shtml.
Pado?, Sebastian and Mirella Lapata.
2009. Cross-lingual annotation
projection for semantic roles. Journal
of Artificial Intelligence Research,
36:307?340.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Pennacchiotti, Marco, Diego De Cao, Roberto
Basili, Danilo Croce, and Michael Roth.
2008. Automatic induction of FrameNet
lexical units. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing, pages 457?465,
Honolulu, HI.
Pradhan, Sameer S., Wayne Ward, and
James H. Martin. 2008. Towards robust
semantic role labeling. Computational
Linguistics, 34(2):289?310.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih,
and Dav Zimak. 2004. Semantic role
labeling via integer linear programming
inference. In Proceedings of COLING 2004,
pages 1346?1352, Geneva.
Qiu, Long, Min-Yen Kan, and Tat-Seng
Chua. 2006. Paraphrase recognition via
dissimilarity significance classification.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 18?26, Sydney.
Riedel, Sebastian and James Clarke.
2006. Incremental integer linear
programming for non-projective
dependency parsing. In Proceedings
of the 2006 Conference on Empirical
Methods in Natural Language Processing,
pages 129?137, Sydney.
Roth, Dan and Wen tau Yih. 2004. A linear
programming formulation for global
inference in natural language tasks.
In Proceedings of the 8th Conference on
Computational Natural Language Learning,
pages 1?8, Boston, MA.
Shen, Dan and Mirella Lapata. 2007. Using
semantic roles to improve question
answering. In Proceedings of the 2007
Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 12?21, Prague.
Steedman, Mark. 2000. The Syntactic Process.
The MIT Press, Cambridge, MA.
Surdeanu, Mihai, Sanda Harabagiu,
John Williams, and Paul Aarseth. 2003.
Using predicate?argument structures for
information extraction. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, pages 8?15,
Sapporo.
Swier, Robert S. and Suzanne Stevenson.
2004. Unsupervised semantic role
labelling. In Proceedings of the 2004
Conference on Empirical Methods in Natural
Language Processing, pages 95?102,
Bacelona.
Taskar, Ben, Simon Lacoste-Julien, and
Dan Klein. 2005. A discriminative
matching approach to word alignment.
In Proceedings of Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing, pages 73?80, Vancouver.
170
Fu?rstenau and Lapata Semi-Supervised SRL via Structural Alignment
Vanderbei, Robert J. 2001. Linear
Programming: Foundations and Extensions.
Berlin, Springer.
Wan, Stephen, Mark Dras, Robert Dale,
and Ce?cile Paris. 2006. Using
dependency-based features to take
the ?para-farce? out of paraphrase.
In Proceedings of the 2006 Australasian
Language Technology Workshop,
pages 131?138, Sydney.
Winston, Wayne L. and Munirpallam
Venkataramanan. 2003. Introduction to
Mathematical Programming: Applications
and Algorithms (4th edition). Pacific Grove,
CA, Duxbury Press.
Wu, Dekai and Pascale Fung. 2009. Semantic
roles for SMT: A hybrid two-pass model.
In Proceedings of HLT-NAACL, Companion
Volume: Short Papers, pages 13?16,
Boulder, CO.
171
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948?957,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Contextualizing Semantic Representations
Using Syntactically Enriched Vector Models
Stefan Thater and Hagen F?rstenau and Manfred Pinkal
Department of Computational Linguistics
Saarland University
{stth, hagenf, pinkal}@coli.uni-saarland.de
Abstract
We present a syntactically enriched vec-
tor model that supports the computation
of contextualized semantic representations
in a quasi compositional fashion. It em-
ploys a systematic combination of first- and
second-order context vectors. We apply
our model to two different tasks and show
that (i) it substantially outperforms previ-
ous work on a paraphrase ranking task, and
(ii) achieves promising results on a word-
sense similarity task; to our knowledge, it is
the first time that an unsupervised method
has been applied to this task.
1 Introduction
In the logical paradigm of natural-language seman-
tics originating from Montague (1973), semantic
structure, composition and entailment have been
modelled to an impressive degree of detail and
formal consistency. These approaches, however,
lack coverage and robustness, and their impact
on realistic natural-language applications is lim-
ited: The logical framework suffers from over-
specificity, and is inappropriate to model the per-
vasive vagueness, ambivalence, and uncertainty
of natural-language semantics. Also, the hand-
crafting of resources covering the huge amounts
of content which are required for deep semantic
processing is highly inefficient and expensive.
Co-occurrence-based semantic vector models of-
fer an attractive alternative. In the standard ap-
proach, word meaning is represented by feature
vectors, with large sets of context words as dimen-
sions, and their co-occurrence frequencies as val-
ues. Semantic similarity information can be ac-
quired using unsupervised methods at virtually no
cost, and the information gained is soft and gradual.
Many NLP tasks have been modelled successfully
using vector-based models. Examples include in-
formation retrieval (Manning et al, 2008), word-
sense discrimination (Sch?tze, 1998) and disam-
biguation (McCarthy and Carroll, 2003), to name
but a few.
Standard vector-space models have serious lim-
itations, however: While semantic information is
typically encoded in phrases and sentences, distri-
butional semantics, in sharp contrast to logic-based
semantics, does not offer any natural concept of
compositionality that would allow the semantics
of a complex expression to be computed from the
meaning of its parts. A different, but related prob-
lem is caused by word-sense ambiguity and con-
textual variation of usage. Frequency counts of
context words for a given target word provide in-
variant representations averaging over all different
usages of the target word. There is no obvious way
to distinguish the different senses of e.g. acquire
in different contexts, such as acquire knowledge or
acquire shares.
Several approaches for word-sense disambigua-
tion in the framework of distributional semantics
have been proposed in the literature (Sch?tze, 1998;
McCarthy and Carroll, 2003). In contrast to these
approaches, we present a method to model the mu-
tual contextualization of words in a phrase in a com-
positional way, guided by syntactic structure. To
some extent, our method resembles the approaches
proposed by Mitchell and Lapata (2008) and Erk
and Pad? (2008). We go one step further, however,
in that we employ syntactically enriched vector
models as the basic meaning representations, as-
suming a vector space spanned by combinations
of dependency relations and words (Lin, 1998).
This allows us to model the semantic interaction
between the meaning of a head word and its de-
pendent at the micro-level of relation-specific co-
occurrence frequencies. It turns out that the benefit
to precision is considerable.
Using syntactically enriched vector models
raises problems of different kinds: First, the use
948
of syntax increases dimensionality and thus may
cause data sparseness (Pad? and Lapata, 2007).
Second, the vectors of two syntactically related
words, e.g., a target verb acquire and its direct ob-
ject knowledge, typically have different syntactic
environments, which implies that their vector repre-
sentations encode complementary information and
there is no direct way of combining the information
encoded in the respective vectors.
To solve these problems, we build upon pre-
vious work (Thater et al, 2009) and propose to
use syntactic second-order vector representations.
Second-order vector representations in a bag-of-
words setting were first used by Sch?tze (1998);
in a syntactic setting, they also feature in Dligach
and Palmer (2008). For the problem at hand, the
use of second-order vectors alleviates the sparse-
ness problem, and enables the definition of vector
space transformations that make the distributional
information attached to words in different syntactic
positions compatible. Thus, it allows vectors for
a predicate and its arguments to be combined in a
compositional way.
We conduct two experiments to assess the suit-
ability of our method. Our first experiment is car-
ried out on the SemEval 2007 lexical substitution
task dataset (McCarthy and Navigli, 2007). It will
show that our method significantly outperforms
other unsupervised methods that have been pro-
posed in the literature to rank words with respect
to their semantic similarity in a given linguistic
context. In a second experiment, we apply our
model to the ?word sense similarity task? recently
proposed by Erk and McCarthy (2009), which is
a refined variant of a word-sense disambiguation
task. The results show a substantial positive effect.
Plan of the paper. We will first review related
work in Section 2, before presenting our model in
Section 3. In Sections 4 and 5 we evaluate our
model on the two different tasks. Section 6 con-
cludes.
2 Related Work
Several approaches to contextualize vector repre-
sentations of word meaning have been proposed.
One common approach is to represent the mean-
ing of a word a in context b simply as the sum, or
centroid of a and b (Landauer and Dumais, 1997).
Kintsch (2001) considers a variant of this simple
model. By using vector representations of a predi-
cate p and an argument a, Kintsch identifies words
that are similar to p and a, and takes the centroid
of these words? vectors to be the representation of
the complex expression p(a).
Mitchell and Lapata (2008), henceforth M&L,
propose a general framework in which meaning rep-
resentations for complex expressions are computed
compositionally by combining the vector represen-
tations of the individual words of the complex ex-
pression. They focus on the assessment of different
operations combining the vectors of the subexpres-
sions. An important finding is that component-wise
multiplication outperforms the more common addi-
tion method. Although their composition method
is guided by syntactic structure, the actual instanti-
ations of M&L?s framework are insensitive to syn-
tactic relations and word-order, assigning identical
representation to dog bites man and man bites dog
(see Erk and Pad? (2008) for a discussion). Also,
they use syntax-free bag-of-words-based vectors as
basic representations of word meaning.
Erk and Pad? (2008), henceforth E&P, represent
the meaning of a word w through a collection of
vectors instead of a single vector: They assume
selectional preferences and inverse selectional pref-
erences to be constitutive parts of the meaning in
addition to the meaning proper. The interpretation
of a word p in context a is a combination of p?s
meaning with the (inverse) selectional preference
of a. Thus, a verb meaning does not combine di-
rectly with the meaning of its object noun, as on
the M&L account, but with the centroid of the vec-
tors of the verbs to which the noun can stand in an
object relation. Clearly, their approach is sensitive
to syntactic structure. Their evaluation shows that
their model outperforms the one proposed by M&L
on a lexical substitution task (see Section 4). The
basic vectors, however, are constructed in a word
space similar to the one of the M&L approach.
In Thater et al (2009), henceforth TDP, we took
up the basic idea from E&P of exploiting selec-
tional preference information for contextualization.
Instead of using collections of different vectors,
we incorporated syntactic information by assuming
a richer internal structure of the vector represen-
tations. In a small case study, moderate improve-
ments over E&P on a lexical substitution task could
be shown. In the present paper, we formulate a
general model of syntactically informed contextu-
alization and show how to apply it to a number a
of representative lexical substitution tasks. Eval-
uation shows significant improvements over TDP
949
acquire
VB
purchase
VB
gain
VB
share
NN
knowlege
NN
obj, 5 obj, 3 obj, 6 obj, 7
skill
NN
buy-back
NN
conj, 2 nn, 1
Figure 1: Co-occurrence graph of a small sample
corpus of dependency trees.
and E&P.
3 The model
In this section, we present our method of contex-
tualizing semantic vector representations. We first
give an overview of the main ideas, which is fol-
lowed by a technical description of first-order and
second-order vectors (Section 3.2) and the contex-
tualization operation (Section 3.3).
3.1 Overview
Our model employs vector representations for
words and expressions containing syntax-specific
first and second order co-occurrences information.
The basis for the construction of both kinds of
vector representations are co-occurrence graphs.
Figure 1 shows the co-occurrence graph of a small
sample corpus of dependency trees: Words are
represented as nodes in the graph, possible depen-
dency relations between them are drawn as labeled
edges, with weights corresponding to the observed
frequencies. From this graph, we can directly read
off the first-order vector for every word w: the vec-
tor?s dimensions correspond to pairs (r,w?) of a
grammatical relation and a neighboring word, and
are assigned the frequency count of (w,r,w?).
The noun knowledge, for instance, would be rep-
resented by the following vector:
?5(OBJ?1,gain),2(CONJ?1,skill),3(OBJ?1,acquire), . . .?
This vector talks about the possible dependency
heads of knowledge and thus can be seen as the
(inverse) selectional preference of knowledge (see
Erk and Pad? (2008)).
As soon as we want to compute a meaning rep-
resentation for a phrase like acquire knowledge
from the verb acquire together with its direct ob-
ject knowledge, we are facing the problem that
verbs have different syntactic neighbors than nouns,
hence their first-order vectors are not easily com-
parable. To solve this problem we additionally
introduce another kind of vectors capturing infor-
mations about all words that can be reached with
two steps in the co-occurrence graph. Such a path
is characterized by two dependency relations and
two words, i.e., a quadruple (r,w?,r?,w??), whose
weight is the product of the weights of the two
edges used in the path. To avoid overly sparse vec-
tors we generalize over the ?middle word? w? and
build our second-order vectors on the dimensions
corresponding to triples (r,r?,w??) of two depen-
dency relations and one word at the end of the two-
step path. For instance, the second-order vector for
acquire is
?15(OBJ,OBJ?1,gain),
6(OBJ,CONJ?1,skill),
6(OBJ,OBJ?1,buy-back),
42(OBJ,OBJ?1,purchase), . . .?
In this simple example, the values are the prod-
ucts of the edge weights on each of the paths. The
method of computation is detailed in Section 3.2.
Note that second order vectors in particular con-
tain paths of the form (r,r?1,w?), relating a verb
w to other verbs w? which are possible substitution
candidates.
With first- and second-order vectors we can
now model the interaction of semantic informa-
tion within complex expressions. Given a pair
of words in a particular grammatical relation like
acquire knowledge, we contextualize the second-
order vector of acquire with the first-order vec-
tor of knowledge. We let the first-order vector
with its selectional preference information act as a
kind of weighting filter on the second-order vector,
and thus refine the meaning representation of the
verb. The actual operation we will use is point-
wise multiplication, which turned out to be the
best-performing one for our purpose. Interestingly,
Mitchell and Lapata (2008) came to the same result
in a different setting.
In our example, we obtain a new second-order
vector for acquire in the context of knowledge:
?75(OBJ,OBJ?1,gain),
12(OBJ,CONJ?1,skill),
0(OBJ,OBJ?1,buy-back),
0(OBJ,OBJ?1,purchase), . . .?
Note that all dimensions that are not ?licensed? by
the argument knowledge are filtered out as they are
multiplied with 0. Also, contextualisation of ac-
quire with the argument share instead of knowledge
950
would have led to a very different vector, which
reflects the fact that the two argument nouns induce
different readings of the inherently ambiguous ac-
quire.
3.2 First and second-order vectors
Assuming a set W of words and a set R of depen-
dency relation labels, we consider a Euclidean vec-
tor space V1 spanned by the set of orthonormal
basis vectors {~er,w? | r ? R,w? ?W}, i.e., a vector
space whose dimensions correspond to pairs of a re-
lation and a word. Recall that any vector of V1 can
be represented as a finite sum of the form ?ai~er,w?
with appropriate scalar factors ai. In this vector
space we define the first-order vector [w] of a word
w as follows:
[w] = ?
r?R
w??W
?(w,r,w?) ?~er,w?
where ? is a function that assigns the dependency
triple (w,r,w?) a corresponding weight. In the sim-
plest case, ? would denote the frequency in a cor-
pus of dependency trees of w occurring together
with w? in relation r. In the experiments reported be-
low, we use pointwise mutual information (Church
and Hanks, 1990) instead as it proved superior to
raw frequency counts:
pmi(w,r,w?) = log
p(w,w? | r)
p(w | r)p(w? | r)
We further consider a similarly defined vec-
tor space V2, spanned by an orthonormal basis
{~er,r?,w? | r,r? ? R,w? ?W}. Its dimensions there-
fore correspond to triples of two relations and a
word. Evidently this is a higher dimensional space
than V1, which therefore can be embedded into
V2 by the ?lifting maps? Lr : V1 ?? V2 defined by
Lr(~er?,w?) :=~er,r?,w? (and by linear extension there-
fore on all vectors of V1). Using these lifting maps
we define the second-order vector [[w]] of a word w
as
[[w]] = ?
r?R
w??W
?(w,r,w?) ?Lr
(
[w?]
)
Substituting the definitions of Lr and [w?], this
yields
[[w]] = ?
r,r??R
w???W
(
?
w??W
?(w,r,w?)?(w?,r?,w??)
)
~er,r?,w??
which shows the generalization over w? in form of
the inner sum.
For example, if w is a verb, r = OBJ and r? =
OBJ?1 (i.e., the inverse object relation), then the
coefficients of ~er,r?,w?? in [[w]] would characterize
the distribution of verbs w?? which share objects
with w.
3.3 Composition
Both first and second-order vectors are defined for
lexical expressions only. In order to represent the
meaning of complex expressions we need to com-
bine the vectors for grammatically related words
in a given sentence. Given two words w and w? in
relation r we contextualize the second-order vector
of w with the r-lifted first-order vector of w?:
[[wr:w? ]] = [[w]]?Lr([w
?])
Here ? may denote any operator on V2. The ob-
jective is to incorporate (inverse) selectional pref-
erence information from the context (r,w?) in such
a way as to identify the correct word sense of w.
This suggests that the dimensions of [[w]] should
be filtered so that only those compatible with the
context remain. A more flexible approach than
simple filtering, however, is to re-weight those di-
mensions with context information. This can be
expressed by pointwise vector multiplication (in
terms of the given basis of V2). We therefore take
? to be pointwise multiplication.
To contextualize (the vector of) a word w with
multiple words w1, . . . ,wn and corresponding rela-
tions r1, . . . ,rn, we compute the sum of the results
of the pairwise contextualizations of the target vec-
tor with the vectors of the respective dependents:
[[wr1:w1,...,rn:wn ]] =
n
?
k=1
[[wrk:wk ]]
4 Experiments: Ranking Paraphrases
In this section, we evaluate our model on a para-
phrase ranking task. We consider sentences with
an occurrence of some target word w and a list of
paraphrase candidates w1, . . . ,wk such that each of
the wi is a paraphrase of w for some sense of w.
The task is to decide for each of the paraphrase
candidates wi how appropriate it is as a paraphrase
of w in the given context. For instance, buy, pur-
chase and obtain are all paraphrases of acquire, in
the sense that they can be substituted for acquire in
some contexts, but purchase and buy are not para-
phrases of acquire in the first sentence of Table 1.
951
Sentence Paraphrases
Teacher education students will acquire the knowl-
edge and skills required to [. . . ]
gain 4; amass 1; receive 1; obtain 1
Ontario Inc. will [. . . ] acquire the remaining IXOS
shares [. . . ]
buy 3; purchase 1; gain 1; get 1; procure 2; obtain 1
Table 1: Two examples from the lexical substitution task data set
4.1 Resources
We use a vector model based on dependency trees
obtained from parsing the English Gigaword corpus
(LDC2003T05). The corpus consists of news from
several newswire services, and contains over four
million documents. We parse the corpus using the
Stanford parser1 (de Marneffe et al, 2006) and a
non-lexicalized parser model, and extract over 1.4
billion dependency triples for about 3.9 million
words (lemmas) from the parsed corpus.
To evaluate the performance of our model, we
use various subsets of the SemEval 2007 lexical
substitution task (McCarthy and Navigli, 2007)
dataset. The complete dataset contains 10 instances
for each of 200 target words?nouns, verbs, adjec-
tives and adverbs?in different sentential contexts.
Systems that participated in the task had to generate
paraphrases for every instance, and were evaluated
against a gold standard containing up to 10 possible
paraphrases for each of the individual instances.
There are two natural subtasks in generating
paraphrases: identifying paraphrase candidates and
ranking them according to the context. We follow
E&P and evaluate it only on the second subtask:
we extract paraphrase candidates from the gold
standard by pooling all annotated gold-standard
paraphrases for all instances of a verb in all con-
texts, and use our model to rank these paraphrase
candidates in specific contexts. Table 1 shows two
instances of the target verb acquire together with
its paraphrases in the gold standard as an example.
The paraphrases are attached with weights, which
correspond to the number of times they have been
given by different annotators.
4.2 Evaluation metrics
To evaluate the performance of our method we use
generalized average precision (Kishida, 2005), a
1We use version 1.6 of the parser. We modify the depen-
dency trees by ?folding? prepositions into the edge labels to
make the relation between a head word and the head noun of
a prepositional phrase explicit.
variant of average precision.
Average precision (Buckley and Voorhees, 2000)
is a measure commonly used to evaluate systems
that return ranked lists of results. Generalized aver-
age precision (GAP) additionally rewards the cor-
rect order of positive cases w.r.t. their gold standard
weight. We define average precision first:
AP =
?ni=1xi pi
R
pi =
?ik=1xk
i
where xi is a binary variable indicating whether
the ith item as ranked by the model is in the gold
standard or not, R is the size of the gold standard,
and n is the number of paraphrase candidates to
be ranked. If we take xi to be the gold standard
weight of the ith item or zero if it is not in the
gold standard, we can define generalized average
precision as follows:
GAP =
?ni=1 I(xi) pi
?Ri=1 I(yi)yi
where I(xi) = 1 if xi is larger than zero, zero oth-
erwise, and yi is the average weight of the ideal
ranked list y1, . . . ,yi of gold standard paraphrases.
As a second scoring method, we use precision
out of ten (P10). The measure is less discriminative
than GAP. We use it because we want to compare
our model with E&P. P10 measures the percentage
of gold-standard paraphrases in the top-ten list of
paraphrases as ranked by the system, and can be
defined as follows (McCarthy and Navigli, 2007):
P10 =
?s?M?G f (s)
?s?G f (s)
,
where M is the list of 10 paraphrase candidates top-
ranked by the model, G is the corresponding anno-
tated gold-standard data, and f (s) is the weight of
the individual paraphrases.
4.3 Experiment 1: Verb paraphrases
In our first experiment, we consider verb para-
phrases using the same controlled subset of the
952
lexical substitution task data that had been used by
TDP in an earlier study. We compare our model
to various baselines and the models of TDP and
E&P, and show that our new model substantially
outperforms previous work.
Dataset. The dataset is identical to the one used
by TDP and has been constructed in the same way
as the dataset used by E&P: it contains those gold-
standard instances of verbs that have?according
to the analyses produced by the MiniPar parser
(Lin, 1993)?an overtly realized subject and object.
Gold-standard paraphrases that do not occur in the
parsed British National Corpus are removed.2 In
total, the dataset contains 162 instances for 34 dif-
ferent verbs. On average, target verbs have 20.5
substitution candidates; for individual instances of
a target verb, an average of 3.9 of the substitution
candidates are annotated as correct paraphrases.
Below, we will refer to this dataset as ?LST/SO.?
Experimental procedure. To compute the vec-
tor space, we consider only a subset of the complete
set of dependency triples extracted from the parsed
Gigaword corpus. We experimented with various
strategies, and found that models which consider
all dependency triples exceeding certain pmi- and
frequency thresholds perform best.
Since the dataset is rather small, we use a four-
fold cross-validation method for parameter tuning:
We divide the dataset into four subsets, test vari-
ous parameter settings on one subset and use the
parameters that perform best (in terms of GAP) to
evaluate the model on the three other subsets. We
consider the following parameters: pmi-thresholds
for the dependency triples used in the computa-
tion of the first- and second-order vectors, and
frequency thresholds. The parameters differ only
slightly between the four subsets, and the general
tendency is that good results are obtained if a low
pmi-threshold (? 2) is applied to filter dependency
triples used in the computation of the second-order
vectors, and a relatively high pmi-threshold (? 4)
to filter dependency triples in the computation of
the first-order vectors. Good performing frequency
thresholds are 10 or 15. The threshold values for
context vectors are slightly different: a medium
pmi-threshold between 2 and 4 and a low frequency
threshold of 3.
To rank paraphrases in context, we compute con-
textualized vectors for the verb in the input sen-
2Both TDP and E&P use the British National Corpus.
tence, i.e., a second order vector for the verb that
is contextually constrained by the first order vec-
tors of all its arguments, and compare them to the
unconstrained (second-order) vectors of each para-
phrase candidate, using cosine similarity.3 For the
first sentence in Table 1, for example, we compute
[[acquireSUBJ:student,OBJ:knowledge]] and compare it to
[[gain]], [[amass]], [[buy]], [[purchase]] and so on.
Baselines. We evaluate our model against a ran-
dom baseline and two variants of our model: One
variant (?2nd order uncontexualized?) simply uses
contextually unconstrained second-order vectors
to rank paraphrase candidates. Comparing the full
model to this variant will show how effective our
method of contextualizing vectors is. The sec-
ond variant (?1st order contextualized?) represents
verbs in context by their first order vectors that
specify how often the verb co-occurs with its argu-
ments in the parsed Gigaword corpus. We compare
our model to this baseline to demonstrate the bene-
fit of (contextualized) second-order vectors. As for
the full model, we use pmi values rather than raw
frequency counts as co-occurrence statistics.
Results. For the LST/SO dataset, the generalized
average precision, averaged over all instances in the
dataset, is 45.94%, and the average P10 is 73.11%.
Table 2 compares our model to the random base-
line, the two variants of our model, and previous
work. As can be seen, our model improves about
8% in terms of GAP and almost 7% in terms of
P10 upon the two variants of our model, which in
turn perform 10% above the random baseline. We
conclude that both the use of second-order vectors,
as well as the method used to contextualize them,
are very effective for the task under consideration.
The table also compares our model to the model
of TDP and two different instantiations of E&P?s
model. The results for these three models are cited
from Thater et al (2009). We can observe that
our model improves about 9% in terms of GAP
and about 7% in terms of P10 upon previous work.
Note that the results for the E&P models are based
3Note that the context information is the same for both
words. With our choice of pointwise multiplication for the
composition operator ? we have (~v1?~w) ?~v2 =~v1 ? (~v2?~w).
Therefore the choice of which word is contextualized does not
strongly influence their cosine similarity, and contextualizing
both should not add any useful information. On the contrary
we found that it even lowers performance. Although this
could be repaired by appropriately modifying the operator ?,
for this experiment we stick with the easier solution of only
contextualizing one of the words.
953
Model GAP P10
Random baseline 26.03 54.25
E&P (add, object) 29.93 66.20
E&P (min, subject & object) 32.22 64.86
TDP 36.54 63.32
1st order contextualized 36.09 59.35
2nd order uncontextualized 37.65 66.32
Full model 45.94 73.11
Table 2: Results of Experiment 1
on a reimplementation of E&P?s original model?
the P10-scores reported by Erk and Pad? (2009)
range between 60.2 and 62.3, over a slightly lower
random baseline.
According to a paired t-test the differences are
statistically significant at p < 0.01.
Performance on the complete dataset. To find
out how our model performs on less controlled
datasets, we extracted all instances from the lexical
substitution task dataset with a verb target, exclud-
ing only instances which could not be parsed by
the Stanford parser, or in which the target was mis-
tagged as a non-verb by the parser. The resulting
dataset contains 496 instances. As for the LST/SO
dataset, we ignore all gold-standard paraphrases
that do not occur in the parsed (Gigaword) corpus.
If we use the best-performing parameters from
the first experiment, we obtain a GAP score of
45.17% and a P10-score of 75.43%, compared to
random baselines of 27.42% (GAP) and 58.83%
(P10). The performance on this larger dataset is
thus almost the same compared to our results for
the more controlled dataset. We take this as evi-
dence that our model is quite robust w.r.t. different
realizations of a verb?s subcategorization frame.
4.4 Experiment 2: Non-verb paraphrases
We now apply our model to parts of speech (POS)
other than verbs. The main difference between
verbs on the one hand, and nouns, adjectives, and
adverbs on the other hand, is that verbs typically
come with a rich context?subject, object, and so
on?while non-verbs often have either no depen-
dents at all or only closed class dependents such as
determiners which provide only limited contextual
informations, if any at all. While we can apply the
same method as before also to non-verbs, we might
expect it to work less well due to limited contextual
POS Instances M1 M2 Baseline
Noun 535 46.38 42.54 30.01
Adj 508 39.41 43.21 28.32
Adv 284 48.19 51.43 37.25
Table 3: GAP-scores for non-verb paraphrases us-
ing two different methods.
information.
We therefore propose an alternative method to
rank non-verb paraphrases: We take the second-
order vector of the target?s head and contextually
constrain it by the first order vector of the target.
For instance, if we want to rank the paraphrase
candidates hint and star for the noun lead in the
sentence
(1) Meet for coffee early, swap leads and get per-
mission to contact if possible.
we compute [[swapOBJ:lead]] and compare it to the
lifted first-order vectors of all paraphrase candi-
dates, LOBJ([hint]) and LOBJ([star]), using cosine
similarity.
To evaluate the performance of the two methods,
we extract all instances from the lexical substitution
task dataset with a nominal, adjectival, or adverbial
target, excluding instances with incorrect parse or
no parse at all. As before, we ignore gold-standard
paraphrases that do not occur in the parsed Giga-
word corpus.
The results are shown in Table 3, where ?M1?
refers to the method we used before on verbs, and
?M2? refers to the alternative method described
above. As one can see, M1 achieves better results
than M2 if applied to nouns, while M2 is better
than M1 if applied to adjectives and adverbs. The
second result is unsurprising, as adjectives and ad-
verbs often have no dependents at all.
We can observe that the performance of our
model is similarly strong on non-verbs. GAP scores
on nouns (using M1) and adverbs are even higher
than those on verbs. We take these results to show
that our model can be successfully applied to all
open word classes.
5 Experiment: Ranking Word Senses
In this section, we apply our model to a different
word sense ranking task: Given a word w in context,
the task is to decide to what extent the different
954
WordNet (Fellbaum, 1998) senses of w apply to
this occurrence of w.
Dataset. We use the dataset provided by Erk and
McCarthy (2009). The dataset contains ordinal
judgments of the applicability of WordNet senses
on a 5 point scale, ranging from completely differ-
ent to identical for eight different lemmas in 50
different sentential contexts. In this experiment,
we concentrate on the three verbs in the dataset:
ask, add and win.
Experimental procedure. Similar to Pennac-
chiotti et al (2008), we represent different word
senses by the words in the corresponding synsets.
For each word sense, we compute the centroid of
the second-order vectors of its synset members.
Since synsets tend to be small (they even may con-
tain only the target word itself), we additionally
add the centroid of the sense?s hypernyms, scaled
down by the factor 10 (chosen as a rough heuristic
without any attempt at optimization).
We apply the same method as in Section 4.3:
For each instance in the dataset, we compute the
second-order vector of the target verb, contextually
constrain it by the first-order vectors of the verb?s
arguments, and compare the resulting vector to
the vectors that represent the different WordNet
senses of the verb. The WordNet senses are then
ranked according to the cosine similarity between
their sense vector and the contextually constrained
target verb vector.
To compare the predicted ranking to the gold-
standard ranking, we use Spearman?s ? , a standard
method to compare ranked lists to each other. We
compute ? between the similarity scores averaged
over all three annotators and our model?s predic-
tions. Based on agreement between human judges,
Erk and McCarthy (2009) estimate an upper bound
? of 0.544 for the dataset.
Results. Table 4 shows the results of our exper-
iment. The first column shows the correlation of
our model?s predictions with the human judgments
from the gold-standard, averaged over all instances.
All correlations are significant (p< 0.001) as tested
by approximate randomization (Noreen, 1989).
The second column shows the results of a
frequency-informed baseline, which predicts the
ranking based on the order of the senses in Word-
Net. This (weakly supervised) baseline outper-
forms our unsupervised model for two of the three
verbs. As a final step, we explored the effect of
Word Present paper WN-Freq Combined
ask 0.344 0.369 0.431
add 0.256 0.164 0.270
win 0.236 0.343 0.381
average 0.279 0.291 0.361
Table 4: Correlation of model predictions and hu-
man judgments
combining our rankings with those of the frequency
baseline, by simply computing the average ranks
of those two models. The results are shown in the
third column. Performance is significantly higher
than for both the original model and the frequency-
informed baseline. This shows that our model cap-
tures an additional kind of information, and thus
can be used to improve the frequency-based model.
6 Conclusion
We have presented a novel method for adapting
the vector representations of words according to
their context. In contrast to earlier approaches, our
model incorporates detailed syntactic information.
We solved the problems of data sparseness and
incompatibility of dimensions which are inherent in
this approach by modeling contextualization as an
interplay between first- and second-order vectors.
Evaluating on the SemEval 2007 lexical substitu-
tion task dataset, our model performs substantially
better than all earlier approaches, exceeding the
state of the art by around 9% in terms of general-
ized average precision and around 7% in terms of
precision out of ten. Also, our system is the first un-
supervised method that has been applied to Erk and
McCarthy?s (2009) graded word sense assignment
task, showing a substantial positive correlation with
the gold standard. We further showed that a weakly
supervised heuristic, making use of WordNet sense
ranks, can be significantly improved by incorporat-
ing information from our system.
We studied the effect that context has on target
words in a series of experiments, which vary the
target word and keep the context constant. A natu-
ral objective for further research is the influence of
varying contexts on the meaning of target expres-
sions. This extension might also shed light on the
status of the modelled semantic process, which we
have been referring to in this paper as ?contextu-
alization?. This process can be considered one of
955
mutual disambiguation, which is basically the view
of E&P. Alternatively, one can conceptualize it as
semantic composition: in particular, the head of a
phrase incorporates semantic information from its
dependents, and the final result may to some extent
reflect the meaning of the whole phrase.
Another direction for further study will be the
generalization of our model to larger syntactic con-
texts, including more than only the direct neighbors
in the dependency graph, ultimately incorporating
context information from the whole sentence in a
recursive fashion.
Acknowledgments. We would like to thank Ed-
uard Hovy and Georgiana Dinu for inspiring discus-
sions and helpful comments. This work was sup-
ported by the Cluster of Excellence ?Multimodal
Computing and Interaction?, funded by the Ger-
man Excellence Initiative, and the project SALSA,
funded by DFG (German Science Foundation).
References
Chris Buckley and Ellen M. Voorhees. 2000. Evaluat-
ing evaluation measure stability. In Proceedings of
the 23rd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 33?40, Athens, Greece.
Kenneth W. Church and Patrick Hanks. 1990. Word
association, mutual information and lexicography.
Computational Linguistics, 16(1):22?29.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the fifth international conference on
Language Resources and Evaluation (LREC 2006),
pages 449?454, Genoa, Italy.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
Proceedings of ACL-08: HLT, Short Papers, pages
29?32, Columbus, OH, USA.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 440?449, Singapore.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Honolulu,
HI, USA.
Katrin Erk and Sebastian Pad?. 2009. Paraphrase as-
sessment in structured vector space: Exploring pa-
rameters and datasets. In Proc. of the Workshop
on Geometrical Models of Natural Language Seman-
tics, Athens, Greece.
Christiane Fellbaum, editor. 1998. Wordnet: An Elec-
tronic Lexical Database. Bradford Book.
Walter Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
Kazuaki Kishida. 2005. Property of average precision
and its generalization: An examination of evaluation
indicator for information retrieval experiments. NII
Technical Report.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Dekang Lin. 1993. Principle-based parsing without
overgeneration. In Proceedings of the 31st Annual
Meeting of the Association for Computational Lin-
guistics, pages 112?120, Columbus, OH, USA.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics, Volume 2, pages 768?774.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639?654.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task. In
Proc. of SemEval, Prague, Czech Republic.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244, Columbus, OH,
USA.
Richard Montague. 1973. The proper treatment of
quantification in ordinary English. In Jaakko Hin-
tikka, Julius Moravcsik, and Patrick Suppes, editors,
Approaches to Natural Language, pages 221?242.
Dordrecht.
Eric W. Noreen. 1989. Computer-intensive Methods
for Testing Hypotheses: An Introduction. John Wi-
ley and Sons Inc.
Sebastian Pad? and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of framenet lexical units. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 457?465, Hon-
olulu, HI, USA.
956
Hinrich Sch?tze. 1998. Automatic word sense discrim-
ination. Computational Linguistics, 24(1):97?124.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of the 2009 Workshop on Applied Textual Infer-
ence, pages 44?47, Singapore.
957
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 180?188,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Induction of a Syntax-Semantics Lexicon
Using Iterative Refinement
Hagen Fu?rstenau
CCLS
Columbia University
New York, NY, USA
hagen@ccls.columbia.edu
Owen Rambow
CCLS
Columbia University
New York, NY, USA
rambow@ccls.columbia.edu
Abstract
We present a method for learning syntax-
semantics mappings for verbs from unanno-
tated corpora. We learn linkings, i.e., map-
pings from the syntactic arguments and ad-
juncts of a verb to its semantic roles. By learn-
ing such linkings, we do not need to model in-
dividual semantic roles independently of one
another, and we can exploit the relation be-
tween different mappings for the same verb,
or between mappings for different verbs. We
present an evaluation on a standard test set for
semantic role labeling.
1 Introduction
A verb can have several ways of mapping its seman-
tic arguments to syntax (?diathesis alternations?):
(1) a. We increased the response rate with SHK.
b. SHK increased the response rate.
c. The response rate increased.
The subject of increase can be the agent (1a), the in-
strument (1b), or the theme (what is being increased)
(1c). Other verbs that show this pattern include
break or melt.
Much theoretical and lexicographic (descriptive)
work has been devoted to determining how verbs
map their lexical predicate-argument structure to
syntactic arguments (Burzio, 1986; Levin, 1993).
The last decades have seen a surge in activity on
the computational front, spurred in part by efforts to
annotate large corpora for lexical semantics (Baker
et al, 1998; Palmer et al, 2005). Initially, we have
seen computational efforts devoted to finding classes
of verbs that share similar syntax-semantics map-
pings from annotated and unannotated corpora (La-
pata and Brew, 1999; Merlo and Stevenson, 2001).
More recently, there has been an explosion of inter-
est in semantic role labeling (with too many recent
publications to cite).
In this paper, we explore learning syntax-
semantics mappings for verbs from unannotated cor-
pora. We are specifically interested in learning link-
ings. A linking is a mapping for one verb from its
syntactic arguments and adjuncts to all of its se-
mantic roles, so that individual semantic roles are
not modeled independently of one another and so
that we can exploit the relation between different
mappings for the same verb (as in (1) above), or
between mappings for different verbs. We there-
fore follow Grenager and Manning (2006) in treat-
ing linkings as first-class objects; however, we dif-
fer from their work in two important respects. First,
we use semantic clustering of head words of argu-
ments in an approach that resembles topic modeling,
rather than directly modeling the subcategorization
of verbs with a distribution over words. Second and
most importantly, we do not make any assumptions
about the linkings, as do Grenager and Manning
(2006). They list a small set of rules from which
they derive all linkings possible in their model; in
contrast, we are able to learn any linking observed
in the data. Therefore, our approach is language-
independent. Grenager and Manning (2006) claim
that their rules represent ?a weak form of Universal
Grammar?, but their rules lack such common linking
operations as the addition of an accusative reflex-
ive for the unaccusative (Romance) or case mark-
ing (many languages), and they include a specific
(English) preposition. We have no objection to us-
ing linguistic knowledge, but we do not feel that we
have the empirical basis as of now to provide a set
of Universal Grammar rules relevant for our task.
180
A complete syntax-semantics lexicon describes
how lexemes syntactically realize their semantic ar-
guments, and provides selectional preferences on
these dependents. Though rich lexical resources ex-
ist (such as the PropBank rolesets, the FrameNet lex-
icon, or VerbNet, which relates and extends these
sources), none of them is complete, not even for En-
glish, on which most of the efforts have focused.
However, if a complete syntax-semantics lexicon
did exist, it would be an extremely useful resource:
the task of shallow semantic parsing (semantic ar-
gument detection and semantic role labeling) could
be reduced to determining the best analysis accord-
ing to this lexicon. In fact, the learning model we
present in this paper is itself a semantic role labeling
model, since we can simply apply it to the data we
want to label semantically.
This paper is a step towards the unsupervised in-
duction of a complete syntax-semantics lexicon. We
present a unified procedure for associating verbs
with linkings and for associating the discovered se-
mantic roles with selectional preferences. As input,
we assume a syntactic representation scheme and a
parser which can produce syntactic representations
of unseen sentences in the chosen scheme reason-
ably well, as well as unlabeled text. We do not as-
sume a specific theory of lexical semantics, nor a
specific set of semantic roles. We induce a set of
linkings, which are mappings from semantic role
symbols to syntactic functions. We also induce a
lexicon, which associates a verb lemma with a dis-
tribution over the linkings, and which associates the
sematic role symbols with verb-specific selectional
preferences (which are distributions over distribu-
tions of words). We evaluate on the task of semantic
role labeling using PropBank (Palmer et al, 2005)
as a gold standard.
We focus on semantic arguments, as they are de-
fined specifically for each verb and thus have verb-
specific mappings to syntactic arguments, which
may further be subject to diathesis alternations. In
contrast, semantic adjuncts (modifiers) apply (in
principle) to all verbs, and do not participate in
diathesis alternations. For this reason, the Prop-
Bank lexicon includes arguments but not adjuncts
in its framesets. The method we present in this pa-
per is designed to find verb-specific arguments, and
we therefore take the results on semantic arguments
(Argn) as our primary result. On these, we achieve a
20% F-measure error reduction over a high syntac-
tic baseline (which maps each syntactic relation to a
single semantic argument).
2 Related Work
As mentioned above, our approach is most similar
to that of Grenager and Manning (2006). However,
since their model uses hand-crafted rules, they are
able to predict and evaluate against actual PropBank
role labels, whereas our approach has to be evaluated
in terms of clustering quality.
The problem of unsupervised semantic role la-
beling has recently attracted some attention (Lang
and Lapata, 2011a; Lang and Lapata, 2011b; Titov
and Klementiev, 2012). While the present paper
shares the general aim of inducing semantic role
clusters in an unsupervised way, it differs in treat-
ing syntax-semantics linkings explicitly and model-
ing predicate-specific distributions over them.
Abend et al (2009) address the problem of un-
supervised argument recognition, which we do not
address in the present paper. For the purpose of
building a complete unsupervised semantic parser,
a method such as theirs would be complementary to
our work.
3 Model
In this section, we decribe a model that generates
arguments for a given predicate instance. Specifi-
cally, this generative model describes the probability
of a given set of argument head words and associated
syntactic functions in terms of underlying semantic
roles, which are modelled as latent variables. The
semantic role labeling task is therefore framed as the
induction of these latent variables from the observed
data, which we assume to be preprocessed by a syn-
tactic parser.
The basic idea of our approach is to explicitly
model linkings between the syntactic realizations
and the underlying semantic roles of the arguments
in a predicate-argument structure. Since our model
of argument classification is completely unsuper-
vised, we cannot assign familiar semantic role labels
like Agent or Instrument, but rather aim at inducing
role clusters, i.e., clusters of argument instances that
share a semantic role. For example, each of the three
181
instances of response rate in (1) should be assigned
to the same cluster. We assume a fixed maximum
number R of semantic roles per predicate and for-
mulate argument classification as the task of assign-
ing each argument in a predicate-argument struc-
ture to one of the numbered roles 1, . . . , R. Such
an assignment can therefore be represented by an
R-tuple, where each role position is either filled
by one of the arguments or empty (denoted as ).
We represent each argument by its head word and
its syntactic function, i.e., the path of syntactic de-
pendency relations leading to it from the predicate.
In our example (1a), a possible assignment of ar-
guments to semantic roles could therefore be rep-
resented by a head word tuple (we, rate, ,SHK)
and a corresponding tuple of syntactic functions
(nsubj, dobj, , prep with), where for the sake of the
example we have chosen R = 4 and the third se-
mantic role slot is empty. Note that this ordered
R-tuple thus represents a semantic labeling of the
unordered set of arguments, which our model takes
as input. While in the case of a single predicate-
argument structure the assignment of arguments to
arbitrary semantic role numbers does not provide
additional information, its value lies in the con-
sistent assignment of arguments to specific roles
across instances of the same predicate. For exam-
ple, to be consistent with the assignment above, (1b)
would have to be represented by (, rate, ,SHK)
and (, dobj, , nsubj).
To formulate a generative model of argument tu-
ples, we separately consider the tuple of argument
head words and the tuple of syntactic functions. The
following two subsections will address each of these
in turn.
3.1 Selectional Preferences
The probability of an argument in a certain semantic
role depends strongly on the selectional preferences
of the predicate with respect to this role. In the con-
text of our model, we therefore need to describe the
probability P (wr|p, r) of an argument head wordwr
depending on the predicate p and the role r. Instead
of directly modeling predicate- and role-specific dis-
tributions over head words, however, we model se-
lectional preferences as distributions ?p,r(c) over se-
mantic word classes c = 1, . . . , C (with C being a
fixed model parameter), each of which is in turn as-
sociated with a distribution ?c(wr) over the vocab-
ulary. They are thus similar to topics in semantic
topic models. An advantage of this approach is that
semantic word classes can be shared among different
predicates, which facilitates their inference. Techni-
cally, the introduction of semantic word classes can
be seen as a factorization of the probability of the
argument head P (wr|p, r) =
?C
c=1 ?p,r(c)?c(wr).
3.2 Linkings
Another important factor for the assignment of ar-
guments to semantic roles are their syntactic func-
tions. While in the preceding subsection we consid-
ered selectional preferences for each semantic role
separately (assuming their independence), the inter-
dependence between syntactic functions is crucial
and cannot be ignored: The assignment of an ar-
gument does not depend solely on its own syntactic
function, but on the whole subcategorization frame
of the predicate-argument structure. We therefore
have to model the probability of the whole tuple
y = (y1, . . . , yR) of syntactic functions.
We assume that for each predicate there is a rela-
tively small number of ways in which it realizes its
arguments syntactically, i.e., in which semantic roles
are linked to syntactic functions. These may corre-
spond to alternations like those shown in (1). Instead
of directly modeling the predicate-specific probabil-
ity P (y|p), we consider predicate-specific distribu-
tions ?p(l) over linkings l = (x1, . . . , xR). Such a
linking then gives rise to the tuple y = (y1, . . . , yR)
by way of probability distributions P (yr|xr) =
?xr(yr). This allows us to keep the number of possi-
ble linkings l per predicate relatively small (by set-
ting ?p(l) = 0 for most l), and generate a wide vari-
ety of syntactic function tuples y from them.
3.3 Structure of the Model
Figure 1 presents our linking model. For each
predicate-argument structure in the corpus, it con-
tains observable variables for the predicate p and the
unordered set s of arguments, and further shows la-
tent variables for the linking l and (for each role r)
the semantic word class c, the head word w, and the
syntactic function y.
The distributions ?p,r(c) and ?c(w) are drawn
from Dirichlet priors with symmetric parameters ?
and ?, respectively. In the case of the linking dis-
182
wR N
p
l
c
y
s
? ?
? ?
Figure 1: Representation of our linking model as a
Bayesian network. The nodes p and s are observed for
each of the N predicate-argument structures in the cor-
pus. The latent variables c, w, l, and y are inferred from
the data along with their distributions ?, ?, ?, and ?.
tribution ?p(l), we are faced with an exponentially
large space of possible linkings (considering a set
G of syntactic functions, there are (|G| + 1)R pos-
sible linkings). This is both computationally prob-
lematic and counter-intuitive. We therefore maintain
a global list L of permissible linkings and enforce
?p(l) = 0 for all l /? L. On the set L we then draw
?p(l) from a Dirichlet prior with symmetric param-
eter ?. In Section 3.5, we will describe how the link-
ing list L is iteratively induced from the data.
We introduced the distribution ?x to allow for in-
cidental changes when generating the tuple of syn-
tactic functions out of the linking. If this pro-
cess were allowed to arbitrarily change any syntactic
function in the linking, the linkings would be too un-
constrained and not reflect the syntactic functions in
the corpus. We therefore parameterize ?x in such
a way that the only allowed modifications are the
addition or removal of syntactic functions from the
linking, but no change from one syntactic function
to another. We attain this by parameterizing ?x as
follows:
?x(y) =
?
?????
?????
? if x = y = 
1??
|G| if x =  and y ? G
1? ?x if x ? G and y = 
?x if x = y ? G
0 else
Here, G again denotes the set of all syntactic func-
tions. The parameter ? is drawn from a uniform
prior on the interval [0.0, 1.0] and the |G| parame-
ters ?x for x ? G have uniform priors on [0.5, 1.0].
This has the effect that no syntactic function can
change into another, that a syntactic function is
never more probable to disappear than to stay, and
that all syntactic functions are added with the same
probability. This last property will be important for
the iterative refinement process described in Sec-
tion 3.5.
3.4 Training
In this subsection, we describe how we train the
model described so far, assuming that we are given
a fixed linking list L. The following subsection will
address the problem of infering this list. In Sec-
tion 3.6, we will then describe how we apply the
trained model to infer semantic role assignments for
given predicate-argument structures.
To train the linking model, we apply a Gibbs sam-
pling procedure to the latent variables shown in Fig-
ure 1. In each sampling iteration, we first sample
the values of the latent variables of each predicate-
argument structure based on the current distribu-
tions, and then the latent distributions based on
counts obtained over the corpus. For each predicate-
argument structure, we begin with a blocked sam-
pling step, simultaneously drawing values for w and
y, while summing out c. This gives us
P (w, y|p, l, s) ?
R?
r=1
?xr(yr)
C?
c=1
?p,r(c)?c(wr)
where we have omitted the factor P (s|w, y), which
is uniform as long as we assume that w and y in-
deed represent permutations of the argument set s.
To sample efficiently from this distribution, we pre-
compute the inner sum (as a tensor contraction or,
equivalently, R matrix multiplications). We then
enumerate all permutations of the argument set and
compute their probabilities, defaulting to an approx-
imative beam search procedure in cases where the
space of permutations is too large.
Next, the linking l is sampled according to
P (l|p, y) ? P (l|p)P (y|l) = ?p(l)
R?
r=1
?xr(yr)
Since the space L of possible linkings is small, com-
pletely enumerating the values of this distribution is
183
not a problem.
After sampling the latent variables w, y, and l for
each corpus instance, we go on to apply Gibbs sam-
pling to the latent distributions. For example, for ?p
we obtain
P (?p|p
1, l1, . . . , pN , lN ) ? P (?p)
N?
i=1
P (li|pi)
? Dir(?)(?p) ?
?
l?L
[?p(l)]
np(l) = Dir(~np + ?)(?p)
Here np(l) is the number of corpus instances with
predicate p and latent linking l, and ~np is the vector
of these counts for a fixed p, indexed by l. Hence,
?p is drawn from the Dirichlet distribution parame-
terized by this vector, smoothed in each component
by ?.
In the same way, the sampling distributions for
?p,r and ?c are determined as Dir(~np,r + ?) and
Dir(~nc + ?), where each ~np,r is a vector of counts1
indexed by word classes c and each ~nc is a vector
of counts indexed by head words wr. Similarly,
we draw the parameter ? in the parameterization
of ?x from Beta
(
n(, ) + 1,
?
x?G n(, x) + 1
)
and approximate ?x by drawing ?x from
Beta (n(x, x) + 1, n(x, ) + 1) and redrawing
it uniformly from [0.5, 1.0], if it is smaller than 0.5.
In this context, n(x, y) refers to the number of times
the syntactic relation x is turned into y, counted
over all corpus instances and semantic roles.
To test for convergence of the sampling process,
we monitor the log-likelihood of the data. For each
predicate-argument structure with predicate pi and
argument set si, we have
P (pi, si) ?
?
l
P (l|pi)P (si|l) ? P (si|li)
=
?
w,y
P (w, y, si|li) =
?
w,y?si
P (w, y|li) =: Li
The approximation is rather crude (replacing an ex-
pected value by a single sample from P (l|pi)), but
we expect the errors to mostly cancel out over the
instances of the corpus. The last sum ranges over all
pairs (w, y) that represent permutations of the argu-
ment set s, and this can be computed as a by-product
1Since we do not sample c, we use pseudo-counts based on
P (cr|p, r, wr) for each instance.
of the sampling process of w and y. We then com-
pute L := log
?N
i=1 Li =
?N
i=1 logLi, and termi-
nate the sampling process if L does not increase by
more than 0.1% over 5 iterations.
3.5 Iterative Refinement of Possible Linkings
In Section 3.3, we have addressed the problem of
the exponentially large space of possible linkings by
introducing a subset L ? GR from which linkings
may be drawn. We now need to clarify how this sub-
set is determined. In contrast to Grenager and Man-
ning (2006), we do not want to use any linguistic
intuitions or manual rules to specify this subset, but
rather automatically infer it from the data, so that the
model stays agnostic to the language and paradigm
of semantic roles. We therefore adopt a strategy of
iterative refinement.
We start with a very small set that only contains
the trivial linking (, . . . , ) and one linking for each
of the R most frequent syntactic functions, placing
the most frequent one in the first slot, the second one
in the second slot etc. We then run Gibbs sampling.
When it has converged in terms of log-likelihood,
we add some new linkings to L. These new link-
ings are inferred by inspecting the action of the step
from l to y in the generative model. Here, a syntac-
tic function may be added to or deleted from a link-
ing. If a particular syntactic function is frequently
added to some linking, then a corresponding linking,
i.e., one featuring this syntactic function and thus not
requiring such a modification, seems to be missing
from the set L. We therefore count for each link-
ing l how often it is either reduced by the deletion of
any syntactic function or expanded by the addition
of a syntactic function. We then rank these modifi-
cations in descending order and for each of them de-
termine the semantic role slot in which the modifica-
tion (deletion or addition) occured most frequently.
By applying the modification to this slot, each of the
linkings gives rise to a new one. We add the first a of
those, skipping new linkings if they are duplicates of
those we already have in the linking set. We iterate
this procedure, alternating between Gibbs sampling
to convergence and the addition of a new linkings.
3.6 Inference
To predict semantic roles for a given predicate and
argument set, we maximize P (l, w, y|p, s). If the
184
space of permutations is too large for exhaustive
enumeration, we apply a similar beam search pro-
cedure as the one employed in training to approxi-
mately maximize P (w, y|p, s, l) for each value of l.
For efficiency, we do not marginalize over l. This
has the potential of reducing prediction quality, as
we do not predict the most likely role assignment,
but rather the most likely combination of role assign-
ment and latent linking.
In all experiments we averaged over 10 consec-
utive samples of the latent distributions, at the end
of the sampling process (i.e., when convergence has
been reached).
4 Experimental Setup
We train and evaluate our linking model on the data
set produced for the CoNLL-08 Shared Task on
Joint Parsing of Syntactic and Semantic Dependen-
cies (Surdeanu et al, 2008), which is based on the
PropBank corpus (Palmer et al, 2005). This data
set includes part-of-speech tags, lemmatized tokens,
and syntactic dependencies, which have been con-
verted from the manual syntactic annotation of the
underlying Penn Treebank (Marcus et al, 1993).
4.1 Data Set
As input to our model, we decided not to use the syn-
tactic representation in the CoNLL-08 data set, but
instead to rely on Stanford Dependencies (de Marn-
effe et al, 2006), which seem to facilitate seman-
tic analysis. We thus used the Stanford Parser2 to
convert the underlying phrase structure trees of the
Penn Tree Bank into Stanford Dependencies. In the
resulting dependency analyses, the syntactic head
word of a semantic role may differ from the syntactic
head according to the provided syntax. We therefore
mapped the semantic role annotation onto the Stan-
ford Dependency trees by identifying the tree node
that covers the same set of tokens as the one marked
in the CoNLL-08 data set.
The focus of the present work is on the linking
behavior and classification of semantic arguments
and not their identification. The latter is a substan-
tially different task, and likely to be best addressed
by other approaches, such as that of (Abend et al,
2version 1.6.8, available at http://nlp.stanford.
edu/software/lex-parser.shtml
2009). We therefore use gold standard information
of the CoNLL-08 data set for identifying argument
sets as input to our model. The task of our model is
then to classify these arguments into semantic roles.
We train our model on a corpus consisting of the
training and the test part of the CoNLL-08 data set,
which is permissible since as a unsupervised system
our model does not make any use of the annotated
argument labels for training. We test the model per-
formance against the gold argument classification on
the test part. For development purposes (both de-
signing the model and tuning the parameters as de-
scribed in Section 4.4), we train on the training and
development part and test on the development part.
4.2 Evaluation Measures
As explained above, our model does not predict spe-
cific role labels, such as those annotated in Prop-
Bank, but rather aims at clustering like argument
instances together. Since the (numbered) labels of
these clusters are arbitrary, we cannot evaluate the
predictions of our model against the PropBank gold
annotation directly. We follow Lang and Lapata
(2011b) in measuring the quality of our clustering
in terms of cluster purity and collocation instead.
Cluster purity is a measure of the degree to which
the predicted clusters meet the goal of containing
only instances with the same gold standard class la-
bel. Given predicted clusters C1, . . . , CnC and gold
clusters G1, . . . , GnG over a set of n argument in-
stances, it is defined as
Pu =
1
n
nC?
i=1
max
j=1,...,nG
|Ci ?Gj |
Similarly, cluster collocation measures how well the
clustering meets the goal of clustering all gold in-
stances with the same label into a single predicted
cluster, formally:
Co =
1
n
nG?
j=1
max
i=1,...,nC
|Ci ?Gj |
We determine purity and collocation separately for
each predicate type and then compute their micro-
average, i.e., weighting each score by the number of
argument instances of this precidate. Just as preci-
sion and recall, purity and collocation stand in trade-
off. In the next section, we therefore report their
F1 score, i.e., their harmonic mean 2?Pu?CoPu+Co .
185
4.3 Syntactic Baseline
We compare the performance of our model with a
simple syntactic baseline that assumes that semantic
roles are identical with syntactic functions. We fol-
low Lang and Lapata (2011b) in clustering argument
instances of each predicate by their syntactic func-
tions. We do not restrict the number of clusters per
predicate. In contrast, Lang and Lapata (2011b) re-
strict the number of clusters to 21, which is the num-
ber of clusters their system generates. We found that
this reduces the baseline by 0.1% F1-score (Argn on
the development set, c.f. Table 1). If we reduce the
number of clusters in the baseline to the number of
clusters in our system (7), the baseline is reduced by
another 0.8% F1-score. These lower baselines are
due to lower purity values. In general, we find that a
smaller number of clusters results in lower F1 mea-
sure for the baseline; the reported baseline therefore
is the strictest possible.
4.4 Parameters and Tuning
For all experiments, we fixed the number of seman-
tic roles at R = 7. This is the maximum size of the
argument set over all instances of the data set and
thus the lower limit for R. If R was set to a higher
value, the model would be able to account for the
possibility of a larger number of roles, out of which
never more than 7 are expressed simultaneously. We
leave such investigation to future work. We set the
symmetric parameters for the Dirichlet distributions
to ? = 1.0, ? = 0.1, and ? = 1.0. This corresponds
to uninformative uniform priors for ?p,r and ?p, and
a prior encouraging a sparse lexical distribution ?c,
similar as in topic models such as LDA (Blei et al,
2003).
The number C of word classes, the number a of
additional linkings in each refinement of the linking
set L, and the number k of refinement steps were
tuned on the development set. We first fixed a = 10
and trained models for C = 10, 20, . . . , 100, per-
forming 50 refinement steps. The best F1 score was
obtained withC = 10 after k = 20 refinements (i.e.,
with 200 linkings). Next, we fixed these two param-
eters and trained models for a = 5, 10, 15, 20, 25.
Here, we confirmed an optimal value of a = 10.
5 Results
In this section, we give quantitative results, compar-
ing our system to the syntactic baseline in terms of
cluster purity and collocation, and a qualitative dis-
cussion of some phenomena observed in the perfor-
mance of the model.
5.1 Quantitative Results
Table 1 shows the results of applying our models to
the CoNLL-08 test with the parameter values tuned
in Section 4.4. For comparison, we also show re-
sults on the development set. The table is divided
into three parts, one only considering semantic ar-
guments (Argn), one considering adjuncts (ArgM),
and one aggregating results over both kinds of Prop-
Bank roles (Arg*). It can be seen that our model
consistently outperforms the syntactic baseline in
terms of collocation (by 10% on Argn, 3% on ArgM,
and 8.2% overall). In terms of purity, however, it
falls short of the baseline. As mentioned above,
there is a trade-off between purity and collocation.
Compared to our model, which we run with a total
of 7 semantic role slots, the baseline predicts a large
number of small argument clusters for each predi-
cate, whereas our model tends to group arguments
together based on selectional preferences.
In terms of F1 score, our model outperforms the
baseline by 3.6% on Argn, which translates into a
relative error reduction by 20%. On adjuncts, on
the other hand, our model falls short of the base-
line by almost 10% F1 score. This indicates that
our approach based on explicit representations of
linkings is most suited to the classification of argu-
ments rather than adjuncts, which do not participate
in diathesis alternations and do therefore not profit
as much from our explicit induction of linkings.
5.2 Qualitative Observations
Among the verbs with at least 10 test instances, in-
clude shows the largest gain in F1 score over the
baseline. In the test corpus, we find an interesting
pair of sentences for this predicate:
(2) a. Mr. Herscu proceeded to launch an ambi-
tious, but ill-fated, $1 billion acquisition
binge that included Bonwit Teller and B.
Altman & Co. [...]
186
Argn ArgM Arg*
Test Set Pu Co F1 Pu Co F1 Pu Co F1
Syntactic Baseline 90.6 75.4 82.3 87.0 73.3 79.6 88.0 74.9 80.9
Linking Model 86.4 85.4 85.9 64.4 76.3 69.8 74.5 83.1 78.6
Development Set Pu Co F1 Pu Co F1 Pu Co F1
Syntactic Baseline 91.5 73.9 81.8 88.7 78.6 83.3 89.2 75.1 81.5
Linking Model 85.6 84.4 85.0 67.7 79.9 73.3 75.2 83.2 79.0
Table 1: Purity (Pu), collocation (Co), and F1 scores of our model and the syntactic baseline in percent. Performance
on arguments (Argn), adjuncts (ArgM), and overall results (Arg*) are shown separately.
b. Not included in the bid are Bonwit Teller or
B. Altman & Co. [...]
The first of these two sentences is generated from the
linking (nsubj, dobj, , , , , -rcmod), which does
not need to be modified in any way to account for the
subject that (coreferent with the head of the pred-
icate in the modifying relative clause, binge) and
the direct object Teller (head of the phrase Bonwit
Teller and B. Altman & Co.). These are assigned
to the first and second role slots, respectively. The
second sentence, on the other hand, is generated out
of the linking (prep in, nsubjpass, , , , , ). Here,
the passive subject Teller is assigned to the second
role slot (which we may interpret as the Includee),
while the first semantic role (the Includer) is labeled
on bid, which is realized in a prepositional phrase
headed by the preposition in. Note that this alter-
nation is not the general passive alternation though,
which would have led to Teller is not included by the
bid. Instead, the model learned a specific alternation
pattern for the predicate include.
But even where a specific linking has not been
learned, the model can often still infer a correct la-
beling by virtue of its selectional preference com-
ponent. In our corpus, the predicate give occurs
mostly with a direct and an indirect object as in
CNN recently gave most employees raises of as
much as 15%. The model therefore learns a link-
ing (nsubj, dobj, , , , , iobj), but fails to learn that
the Beneficient role can also be expressed with the
preposition to as in
(3) [...] only 25% give $2,500 or more to charity
each year.
However, when applying our model to this sentence,
it nonetheless assigns charity to the last role slot (the
same one previously occupied by the indirect ob-
ject). This is due to the fact that charity is a good
fit for the selectional preference of this role slot of
the predicate give.
6 Conclusions
We have presented a novel generative model of
predicate-argument structures that incorporates se-
lectional preferences of argument heads and explic-
itly describes linkings between semantic roles and
syntactic functions. The model iteratively induces
a lexicon of possible linkings from unlabeled data.
The trained model can be used to cluster given ar-
gument instances according to their semantic roles,
outperforming a competitive syntactic baseline.
The approach is independent of any particular lan-
guage or paradigm of semantic roles. However, in
its present form the model assumes that each predi-
cate has its own set of semantic roles. In formalisms
such as Frame Semantics (Baker et al, 1998), se-
mantic roles generalize across semantically similar
predicates belonging to the same frame. A natural
extension of our approach would therefore consist in
modeling predicate groups that share semantic roles
and selectional preferences.
Acknowledgments. This work was supported by the In-
telligence Advanced Research Projects Activity (IARPA) via
Department of Interior National Business Center (DoI/NBC)
contract number D11PC20153. The U.S. Government is autho-
rized to reproduce and distribute reprints for Governmental pur-
poses notwithstanding any copyright annotation thereon. Dis-
claimer: The views and conclusions contained herein are those
of the authors and should not be interpreted as necessarily rep-
resenting the official policies or endorsements, either expressed
or implied, of IARPA, DoI/NBC, or the U.S. Government.
187
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 28?36, Singapore.
Collin F. Baker, J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet project. In 36th Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics (COLING-ACL?98), pages 86?90, Montre?al.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Luigi Burzio. 1986. Italian Syntax: A Government-
Binding Approach. Reidel, Dordrecht.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC 2006.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 1?8,
Sydney, Australia.
Joel Lang and Mirella Lapata. 2011a. Unsupervised se-
mantic role induction via split-merge clustering. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1117?1126, Portland, Ore-
gon, USA.
Joel Lang and Mirella Lapata. 2011b. Unsupervised se-
mantic role induction with graph partitioning. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1320?
1331, Edinburgh, Scotland, UK.
Maria Lapata and Chris Brew. 1999. Using subcatego-
rization to resolve verb class ambiguity. In In Proceed-
ings of Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 266?-274, College Park, MD.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press.
Mitchell M. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19.2:313?330, June.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
verb classification based on statistical distributions of
argument structure. Computational Linguistics, 27(3).
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England.
Ivan Titov and Alexandre Klementiev. 2012. A bayesian
approach to unsupervised semantic role induction. In
Proceedings of the Conference of the European Chap-
ter of the Association for Computational Linguistics,
Avignon, France, April.
188
