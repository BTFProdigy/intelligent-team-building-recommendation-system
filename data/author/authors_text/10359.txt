Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 117?126,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Adding Redundant Features for CRFs-based Sentence Sentiment 
Classification 
 
 
Jun Zhao, Kang Liu, Gen Wang 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China 
{jzhao, kliu, gwang}@nlpr.ia.ac.cn 
 
 
 
 
 
 
Abstract 
In this paper, we present a novel method 
based on CRFs in response to the two special 
characteristics of ?contextual dependency? 
and ?label redundancy? in sentence sentiment 
classification. We try to capture the contextual 
constraints on sentence sentiment using CRFs. 
Through introducing redundant labels into the 
original sentimental label set and organizing 
all labels into a hierarchy, our method can add 
redundant features into training for capturing 
the label redundancy.  The experimental 
results prove that our method outperforms the 
traditional methods like NB, SVM, MaxEnt 
and standard chain CRFs. In comparison with 
the cascaded model, our method can 
effectively alleviate the error propagation 
among different layers and obtain better 
performance in each layer. 
1 Introduction* 
There are a lot of subjective texts in the web, such 
as product reviews, movie reviews, news, 
editorials and blogs, etc. Extracting these 
subjective texts and analyzing their orientations 
play significant roles in many applications such as 
electronic commercial, etc. One of the most 
important tasks in this field is sentiment 
                                                           
* Contact: Jun ZHAO, jzhao@nlpr.ia.ac.cn 
classification, which can be performed in several 
levels: word level, sentence level, passage level, 
etc. This paper focuses on sentence level sentiment 
classification. 
Commonly, sentiment classification contains 
three layers of sub-tasks. From upper to lower, (1) 
Subjective/Objective classification: the subjective 
texts are extracted from the corpus teeming with 
both subjective and objective texts. (2) Polarity 
classification: a subjective text is classified into 
?positive? or ?negative? according to the 
sentimental expressions in the text. (3) Sentimental 
strength rating: a subjective text is classified into 
several grades which reflect the polarity degree of 
?positive? or ?negative?. It is a special multi-class 
classification problem, where the classes are 
ordered. In machine learning, this kind of problem 
is also regarded as an ordinal regression problem 
(Wei Wu et al 2005). In this paper, we mainly 
focus on this problem in sentiment classification. 
Sentiment classification in sentence level has its 
special characteristics compared with traditional 
text classification tasks. Firstly, the sentiment of 
each sentence in a discourse is not independent to 
each other. In other words, the sentiment of each 
sentence is related to those of other adjacent 
sentences in the same discourse. The sentiment of 
a sentence may vary in different contexts. If we 
detach a sentence from the context, its sentiment 
may not be inferred correctly. Secondly, there is 
redundancy among the sentiment classes, 
117
especially in sentimental strength classes. For 
example: 
?I love the scenario of ?No country for old man? 
very much!!? 
?This movie sounds good.? 
The first sentence is labeled as ?highly praised? 
class and the second one is labeled as ?something 
good? class. Both the sentences express positive 
sentiment for the movie, but the former expresses 
stronger emotion than the latter. We can see that 
both ?highly praised? and ?something good? 
belong to an implicit class ?positive?, which can be 
regarded as the relation between them.  If we add 
these implicit classes in the label set, the sentiment 
classes will form a hierarchical structure. For 
example, ?positive? can be regarded as the parent 
class of ?highly praised? and ?something good?, 
?subjective? can be regarded as the parent class of 
?positive? and ?negative?. This implicit 
hierarchical structure among labels should not be 
neglected because it may be beneficial for 
improving the accuracy of sentiment classification. 
In the paper, we call this characteristic of 
sentiment classification as ?label redundancy?. 
Unfortunately, in our knowledge most of the 
current research treats sentiment classification as a 
traditional multi-classification task or an ordinal 
regression task, which regard the sentimental 
classes being independent to each other and each 
sentence is also independent to the adjacent 
sentences in the context. In other words, they 
neglect the contextual information and the 
redundancy among sentiment classes. 
In order to consider the contextual information in 
the process of the sentence sentiment classification, 
some research defines contextual features and 
some uses special graph-based formulation, like 
(Bo Pang, et al 2005). In order to consider the 
label redundancy, one potential solution is to use a 
cascaded framework which can combine 
subjective/objective classification, polarity 
classification and sentimental strength 
classification together, where the classification 
results of the preceding step will be the input of the 
subsequent one. However, the subsequent 
classification cannot provide constraint and 
correction to the results of the preceding step, 
which will lead to the accumulation and 
propagation of the classification errors. As a result, 
the performance of sentiment analysis of sentences 
is often not satisfactory.  
This paper focuses on the above two special 
characteristics of the sentiment classification 
problem in the sentence level. To the first 
characteristic, we regard the sentiment 
classification as a sequence labeling problem and 
use conditional random field (CRFs) model to 
capture the relation between two adjacent 
sentences in the context. To the second 
characteristic, we propose a novel method based on 
a CRF model, in which the original task is mapped 
to a classification on a hierarchical structure, which 
is formed by the original label set and some 
additional implicit labels. In the hierarchical 
classification framework, the relations between the 
labels can be represented as the additional features 
in classification. Because these features are related 
to the original labels but unobserved, we name 
them as ?redundant features? in this paper. They 
can be used to capture the redundant and 
hierarchical relation between different sentiment 
classes. In this way, not only the performance of 
sentimental strength rating is improved, the 
accuracies of subjective/objective classification 
and polarity classification are also improved 
compared with the traditional sentiment 
classification method. And in comparison with the 
cascaded method, the proposed approach can 
effectively alleviate error propagation. The 
experimental results on movie reviews prove the 
validity of our method. 
2 Capturing Contextual Influence for 
Sentiment Classification 
For capturing the influence of the contexts to the 
sentiment of a sentence, we treat original sentiment 
classification as a sequence labeling problem. We 
regard the sentiments of all the sentences 
throughout a paragraph as a sequential flow of 
sentiments, and we model it using a conditional 
model. In this paper, we choose Conditional 
Random Fields (CRFs) (Lafferty et al 2001) 
because it has better performance than other 
sequence labeling tools in most NLP applications.  
CRFs are undirected graphical models used to 
calculate the conditional probability of a set of 
labels given a set of input variables. We cite the 
definitions of CRFs in (Lafferty et al 2001). It 
defines the conditional probability proportional to 
the product of potential functions on cliques of the 
graph, 
118
exp ( , )( | )
( )
F Y XP Y X
Z X?
? ?
=      (1) 
where X is a set of input random variables and Y is 
a set of random labels. ( , )F Y X is an arbitrary 
feature function over its arguments, ? is a learned 
weight for each feature function and  
( ) exp( ( , ))
y
XZ F Y X?= ?? . 
The training of CRFs is based on Maximum 
Likelihood Principle (Fei Sha et al 2003). The log 
likelihood function is 
[ ]( ) ( , ) log ( )k k kkL F Y X Z X?? ?= ? ??  
Therefore, Limited-memory BFGS (L-BFGS) 
algorithm is used to find this nonlinear 
optimization parameters.  
3 Label Redundancy in Sentiment 
Classification 
In this section, we explain the ?label redundancy? 
in sentiment classification mentioned in the first 
section. We will analyze the effect of the label 
redundancy on the performance of sentiment 
classification from the experimental view.  
We conduct the experiments of polarity 
classification and sentimental strength rating on the 
corpus which will be introduced in section 5 later. 
The class set is also illustrated in that section. 
Polarity classification is a three-class classification 
process, and sentimental strength rating is a five-
class classification process. We use first 200 
reviews as the training set which contains 6,079 
sentences, and other 49 reviews, totally 1,531 
sentences, are used as the testing set. Both the 
three-class classification and the five-class 
classification use standard CRFs model with the 
same feature set. The results are shown in Table 1, 
2 and 3, where ?Answer? denotes the results given 
by human, ?Results? denotes the results given by 
?CRFs model ?Correct? denotes the number of 
correct samples which is labeled by CRFs model. 
We use precision, recall and F1 value as the 
evaluation metrics.  
Table 1 gives the result of sentimental strength 
rating. Table 2 shows the polarity classification 
results extracted from the results of sentimental 
strength rating in Table 1. The extraction process is 
as follows. In the sentimental strength rating 
results, we combine the sentences with ?PP? class 
and the sentences with ?P? class into ?Pos? class, 
and the sentences with ?NN? class and the 
sentences with ?N? class into ?Neg? class. So the 
results of five-class classification are transformed 
into the results of three-class classification. Table 3 
is the results of performing polarity classification 
in the data set by CRFs directly. 
 
Label Answer Results Correct Precision Recall F1 
PP 51 67 5 0.0746 0.0980 0.0847 
P 166 177 32 0.1808 0.1928 0.1866 
Neu 1190 1118 968 0.8658 0.81.34 0.8388 
N 105 140 25 0.1786 0.2381 0.2041 
NN 19 29 1 0.0345 0.0526 0.0417 
Total 1531 1531 1031 0.67.34 0.6734 0.6734 
Table 1. Result of Sentimental Strength Rating 
Label Answer Results Correct Precision Recall F1 
Pos 217 244 79 0.3238 0.3641 0.3427 
Neu 1190 1118 968 0.8658 0.8134 0.8388 
Neg 124 169 41 0.2426 0.3306 0.2799 
Total 1531 1531 1088 0.7106 0.7106 0.7106 
Table 2.  Result of Polarity Classification Extracted from Table 1. 
Label Answer Results Correct Precision Recall F1 
Pos 217 300 108 0.3600 0.4977 0.4178 
Neu 1190 1101 971 0.8819 0.8160 0.8477 
Neg 124 130 40 0.3077 0.3226 0.3150 
Total 1531 1531 1119 0.7309 0.7309 0.7309 
Table 3. Result of Polarity Classification 
119
From the results we can find the following 
phenomena.  
(1) The corpus is severely unbalanced, the 
objective sentences take the absolute majority in 
the corpus, which leads to the poor accuracy for 
classifying subjective sentences. The experiment in 
Table 1 puts polarity classification and sentimental 
strength rating under a unique CRFs model, 
without considering the redundancy and 
hierarchical structure between different classes. As 
a result, the features for polarity classification will 
usually cover the features for sentimental strength 
rating. These reasons can explain why there is only 
one sample labeled as ?NN? correctly and only 5 
samples labeled as ?PP? correctly. 
(2) Comparing Table 2 with 3, we can find that, 
the F1 value of the polarity classification results 
extracted from sentimental strength rating results is 
lower than that of directly conducting polarity 
classification. That is because the redundancy 
between sentimental strength labels makes the 
classifier confused to determine the polarity of the 
sentence. Therefore, we should deal with the 
sentiment analysis in a hierarchical frame which 
can consider the redundancy between the different 
classes and make full use of the subjective and 
polarity information implicitly contained in 
sentimental strength classes. 
4 Capturing Label Redundancy for CRFs 
via Adding Redundant Features 
As mentioned above, it?s important for a classifier 
to consider the redundancy between different 
labels. However, from the standard CRFs 
described in formula (1), we can see that the 
training of CRFs only maximizes the probabilities 
of the observed labels Y  in the training corpus. 
Actually, the redundant relation between sentiment 
labels is unobserved. The standard CRFs still treats 
each class as an isolated item so that its 
performance is not satisfied.  
In this section, we propose a novel method for 
sentiment classification, which can capture the 
redundant relation between sentiment labels 
through adding redundant features. In the 
following, we firstly show how to add these 
redundant features, then illustrate the 
characteristics of this method. After that, for the 
sentiment analysis task, the process of feature 
generation will be presented. 
4.1 Adding Redundant Features for CRFs 
Adding redundant features has two steps. Firstly, 
an implicit redundant label set is designed, which 
can form a multi-layer hierarchical structure 
together with the original labels. Secondly, in the 
hierarchical classification framework, the implicit 
labels, which reflect the relations between the 
original labels, can be used as redundant features 
in the training process. We will use the following 
example to illustrate the first step for sentimental 
strength rating task.  
For the task of sentimental strength rating, the 
original label set is {?PP (highly praised)?, ?P 
(something good)?, ?Neu (objective description)?, 
?N (something that needs improvement)? and ?NN 
(strong aversion)?}. In order to introduce 
redundant labels, the 5-class classification task is 
decomposed into the following three layers shown 
in Figure 1. The label set in the first layer is 
{?subjective?, ?objective?}, The label set in the 
second layer is for polarity classification 
{?positive?, ?objective?, ?negative?}, and the label 
set in the third layer is the original set.  Actually, 
the labels in the first and second layers are 
unobserved redundant labels, which will not be 
reflected in the final classification result obviously.  
 
Figure 1. The hierarchical structure of 
 sentimental labels 
In the second step, with these redundant labels, 
some implicit features can be generated for CRFs. 
So the standard CRFs can be rewritten as follows. 
The first layer 
The third layer 
The second layer 
Sentiment Analysis 
Subjective Objective 
Positive Negative 
P PP N NN 
Objective 
Objective 
120
11
exp( ( , ) )( | )
( )
exp( ( , ) )
exp( ( , ) )
T
m
j j j
j
m
j j j
T j
F X TP T X
Z X
F X Y
F X Y
?
?
?
=
=
?
=
?
=
?
?
? ?
        (2) 
where 1 2( ), ,... ...,j mT Y Y Y Y= , and jY denotes the 
label sequence in the jth layer. ( , )j jF X Y denotes 
the arbitrary feature function in the jth layer. 
From the formula (2), we can see that the 
original label set is rewritten as 
1 2( ), ,... ...,j mT Y Y Y Y= , which contains implicit 
labels in the hierarchical structure shown in Figure 
1. The difference between our method and the 
standard chain CRFs is that we make some implicit 
redundant features to be active when training. The 
original feature function ( , )F Y X is replaced by 
1
( , )
m
j j
j
F X Y
=
? . We use an example to illustrate the 
process of feature generation. When a sentence 
including the word ?good? is labeled as ?PP?, our 
model not only generate the state feature (good, 
?PP?), but also two implicit redundant state feature 
(good, ?positive?) and (good, ?subjective?). 
Through adding larger-granularity labels ?positive? 
and ?negative? into the model, our method can 
increase the probability of ?positive? and decrease 
the probability of ?negative?. Furthermore, ?P? and 
?PP? will share the probability gain of ?positive?, 
therefore the probability of ?P? will be larger than 
that of ?N?. For the transition feature, the same 
strategy is used. Therefore the complexity of its 
training procedure is ( )
m
j
j
O M N F l? ? ??  where M 
is the number of the training samples, N is the 
average sentence length, jF  is the average number 
of activated features in the jth layer, l  is the 
number of the original labels and m is the number 
of the layers. For the complexity of the decoding 
procedure, our method has ( )
m
j
j
O N F l? ?? . 
It?s worth noting that, (1) transition features are 
extracted in each layer separately rather than 
across different layers. For example, feature (good, 
?subjective?, ?positive?) will never be extracted 
because ?subjective? and ?positive? are from 
different layers; (2) if one sentence is labeled as 
?Neu?, no implicit redundant features will be 
generated.  
4.2 The Characteristics of Our Method 
Our method allows that the label sets are 
dependent and redundant. As a result, it can 
improve the performance of not only the classifier 
for the original sentimental strength rating task, but 
also the classifiers for other tasks in the 
hierarchical frame, i.e. polarity classification and 
subjective/objective classification. This kind of 
dependency and redundancy can lead to two 
characteristics of the proposed method for 
sentiment classification compared with traditional 
methods, such as the cascaded method. 
(1) Error-correction: Two dependent tasks in the 
neighboring layers can correct the errors of each 
other relying on the inconsistent redundant 
information. For example, if in the first layer, the 
features activated by ?objective? get larger scores 
than the features activated by ?subjective?, and in 
the second layer the features activated by 
?positive? get larger scores than the features 
activated by ?objective?, then inconsistency 
emerges. At this time, our method can globally 
select the label with maximum probability. This 
characteristic can make up the deficiency of the 
cascaded method which may induce error 
propagation. 
(2) Differentiating the ordinal relation among 
sentiment labels: Our method organizes the ordinal 
sentiment labels into a hierarchy through 
introducing redundant labels into standard chain 
CRFs, in this way the degree of classification 
errors can be controlled. In the different layers of 
sentiment analysis task, the granularities of 
classification are different. Therefore, when an 
observation cannot be correctly labeled on a 
smaller-granularity label set, our method will use 
the larger-granularity labels in the upper layer to 
control the final classification labels.  
4.3 Feature Selection in Different Layers 
For feature selection, our method selects different 
features for each layer in the hierarchical frame. 
In the top layer of the frame shown in Figure 1, 
for subjective/objective classification task, we use 
121
not only adjectives and the verbs which contain 
subjective information (e.g., ?believe?, ?think?) as 
the features, but also the topic words. The topic 
words are defined as the nouns or noun phases 
which frequently appear in the corpus. We believe 
that some topic words contain subjective 
information. 
In the middle and bottom layers, we not only use 
the features in the first layer, but also some special 
features as follows.  
(1) The prior orientation scores of the sentiment 
words: Firstly, a sentiment lexicon is generated by 
extending the synonymies and antonyms in 
WordNet2 from a positive and negative seed list. 
Then, the positive score and the negative score of a 
sentiment word are individually accumulated and 
weighted according to the polarity of its 
synonymies and antonyms. At last we scale the 
normalized distance of the two scores into 5 levels, 
which will be the prior orientation of the word. 
When there is a negative word, like {not, no, can?t, 
merely, never, ?}, occurring nearby the feature 
word in the range of 3 words size window, the 
orientation of this word will be reversed and ?NO? 
will be added in front of the original feature word 
for creating a new feature word.  
(2) Sentence transition features: We consider two 
types of sentence transition features. The first type 
is the conjunctions and the adverbs occurring in the 
beginning of this sentence. These conjunctions and 
adverbs are included in a word list which is 
manually selected, like {and, or, but, though, 
however, generally, contrarily, ?}. The second 
type of the sentence transition feature is the 
position of the sentence in one review. The reason 
lies in that: the reviewers often follow some 
writing patterns, for example some reviewers 
prefer to concede an opposite factor before 
expressing his/her real sentiment. Therefore, we 
divide a review into five parts, and assign each 
sentence with the serial number of the part which 
the sentence belongs to. 
5 Experiments 
5.1 Data and Baselines 
In order to evaluate the performance of our method, 
we conducted experiments on a sentence level 
                                                           
2 http://wordnet.princeton.edu/ 
annotation corpus obtained from Purdue University, 
which is also used in (Mao and Lebanon 07). This 
corpus contains 249 movie reviews and 7,610 
sentences totally, which is randomly selected from 
the Cornell sentence polarity dataset v1.0. Each 
sentence was hand-labeled with one of five classes: 
PP (highly praised), P (something good), Neu 
(objective description), N (something that needs 
improvement) and NN (strong aversion), which 
contained the orientation polarity of each sentence. 
Based on the 5-class manually labeled results 
mentioned above, we also assigned each sentence 
with one of three classes: Pos (positive polarity), 
Neu (objective description), Neg (negative 
polarity). Data statistics for the corpus are given in 
Table 4. 
Pos Neu Neg 
Label 
PP P Neu N NN 
Total 
5 classes 383 860 5508 694 165 7610 
3 classes 1243 5508 859 7610 
Table 4. Data Statistics for Movies Reviews 
Corpus 
There is a problem in the dataset that more than 
70% of the sentences are labeled as ?Neu? and 
labels are seriously unbalanced. As a result, the 
?Neu? label is over-emphasized. For this problem, 
Mao and Lebanon (2007) made a balanced data set 
(equal number sentences for different labels) which 
is sampled in the original corpus. Since randomly 
sampling sentences from the original corpus will 
break the intrinsic relationship between two 
adjacent sentences in the context, we don?t create 
balanced label data set. 
For the evaluation of our method, we choose 
accuracy as the evaluation metrics and some 
classical methods as the baselines. They are Na?ve 
Bayes (NB), Support Vector Machine (SVM), 
Maximum Entropy (MaxEnt) (Kamal Nigam et al 
1999) and standard chain CRFs (Fei et al 2003). 
We also regard cascaded-CRFs as our baseline for 
comparing our method with the cascaded-based 
method. For NB, we use Laplace smoothing 
method. For SVM, we use the LibSVM3  with a 
linear kernel function4. For MaxEnt, we use the 
implementation in the toolkit Mallet5. For CRFs, 
                                                           
3 http://www.csie.ntu.tw/~cjlin/libsvm 
4 http://svmlight.joachims.org/ 
5 http://mallet.cs.umass.edu/index.php/Main_Page 
122
Label NB SVM MaxEnt Standard CRF Cascaded CRF Our Method 
PP 0.1745 0.2219 0.2055 0.2027 0.2575 0.2167 
P 0.2049 0.2877 0.2353 0.2536 0.2881 0.3784 
Neu 0.8083 0.8685 0.8161 0.8273 0.8554 0.8269 
N 0.2636 0.3014 0.2558 0.2981 0.3092 0.4204 
NN 0.0976 0.1162 0.1148 0.1379 0.1510 0.2967 
Total 0.6442 0.6786 0.6652 0.6856 0.7153 0.7521 
Table 5. The accuracy of Sentimental Strength Rating 
Label NB SVM MaxEnt Standard CRF Cascaded-CRF Our Method 
Pos 0.4218 0.4743 0.4599 0.4405 0.5122 0.6008 
Neu 0.8147 0.8375 0.8424 0.8260 0.8545 0.8269 
Neg 0.3217 0.3632 0.2739 0.3991 0.4067 0.5481 
Total 0.7054 0.7322 0.7318 0.7327 0.7694 0.7855 
Table 6?The Results of Polarity Classification 
Label NB SVM MaxEnt Standard CRF Our Method 
Subjective 0.4743 0.5847 0.4872 0.5594 0.6764 
Objective 0.8170 0.8248 0.8212 0.8312 0.8269 
Total 0.7238 0.7536 0.7518 0.7561 0.8018 
Table 7. The accuracy of Subjective/Objective Classification 
 
we use the implementation in Flex-CRFs6. We set 
the iteration number to 120 in the training process 
of the method based on CRFs. In the cascaded 
model we set 3 layers for sentimental strength 
rating, where the first layer is subjective/objective 
classification, the second layer is polarity 
classification and the last layer is sentimental 
strength classification. The upper layer passes the 
results as the input to the next layer. 
5.2 Sentimental Strength Rating 
In the first experiment, we evaluate the 
performance of our method for sentimental 
strength rating. Experimental results for each 
method are given in Table 5. We not only give the 
overall accuracy of each method, but also the 
performance for each sentimental strength label. 
All baselines use the same feature space mentioned 
in section 4.3, which combine all the features in 
the three layers together, except cascaded CRFs 
and our method. In cascaded-CRFs and our method, 
we use different features in different layers 
mentioned in section 4.3. These results were 
gathered using 5-fold cross validation with one 
fold for testing and the other 4 folds for training.  
From the results, we can obtain the following 
conclusions. (1) The three versions of CRFs 
perform consistently better than Na?ve Bayes, 
                                                           
6 http://flexcrfs.sourceforge.net 
SVM and MaxEnt methods. We think that is 
because CRFs model considers the contextual 
influence of each sentence. (2) Comparing the 
performance of cascaded CRFs with that of 
standard sequence CRFs, we can see that not only 
the overall accuracy but also the accuracy for each 
sentimental strength label are improved, where the 
overall accuracy is increased by 3%. It proves that 
taking the hierarchical relationship between labels 
into account is very essential for sentiment 
classification. The reason is that: the cascaded 
model performs sentimental strength rating in three 
hierarchical layers, while standard chain CRFs 
model treats each label as an independent 
individual. So the performance of the cascaded 
model is superior to the standard chain CRFs. (3) 
The experimental results also show that our 
method performs better than the Cascaded CRFs. 
The classification accuracy is improved from 
71.53% to 75.21%. We think that is because our 
method adds the label redundancy among the 
sentimental strength labels into consideration 
through adding redundant features into the feature 
sets, and the three subtasks in the cascaded model 
are merged into a unified model. So the output 
result is a global optimal result. In this way, the 
problem of error propagation in the cascaded frame 
can be alleviated. 
123
5.3 Sentiment Polarity Classification 
In the second experiment, we evaluate the 
performance of our method for sentiment polarity 
classification. Our method is based on a 
hierarchical frame, which can perform different 
tasks in different layers at the same time.  For 
example, it can determine the polarity of sentences 
when sentimental strength rating is performed. 
Here, the polarity classification results of our 
method are extracted from the results of the 
sentimental strength rating mentioned above. In the 
sentimental strength rating results, we combine the 
sentences with PP label and the sentences with P 
label into one set, and the sentences with NN label 
and the sentences with N label into one set. So the 
results of 5-class classification are transformed into 
the results of 3-class classification. Other methods 
like NB, SVM, MaxEnt, standard chain CRFs 
perform 3-class classification directly, and their 
label sets in the training corpus is {Pos, Neu, Neg}. 
The parameter setting is the same as sentimental 
strength rating. For the cascaded-CRFs method, we 
firstly perform subjective/objective classification, 
and then determine the polarity of the sentences 
based on the subjective sentences. The 
experimental results are given in Table 6. 
From the experimental results, we can obtain the 
following conclusion for sentiment polarity 
classification, which is similar to the conclusion 
for sentimental strength rating mentioned in 
section 5.2. That is both our model and the 
cascaded model can get better performance than 
other traditional methods, such as NB, SVM, 
MaxEnt, etc. But the performance of the cascaded 
CRFs (76.94%) is lower than that of our method 
(78.55%). This indicates that because our method 
exploits the label redundancy in the different layers, 
it can increase the accuracies of both polarity 
classification and sentimental strength rating at the 
same time compared with other methods. 
5.4 Subjective/Objective Classification 
In the last experiment, we test our method for 
subjective/objective classification. The 
subjective/objective label of the data is extracted 
from its original label like section 5.3. As the same 
as the experiment for polarity classification, all 
baselines perform subjective/objective 
classification directly. It?s no need to perform the 
cascaded-based method because it?s a 2-class task. 
The results of our method are extracted from the 
results of the sentimental strength rating too. The 
results are shown in Table 7. From it, we can 
obtain the similar conclusion, i.e. our method 
outperforms other methods and has the 80.18% 
classification accuracy. Our method, which 
introduces redundant features into training, can 
increase the accuracies of all tasks in the different 
layers at the same time compared with other 
baselines. It proves that considering label 
redundancy are effective for promoting the 
performance of a sentimental classifier. 
6 Related Works 
Recently, many researchers have devoted into the 
problem of the sentiment classification. Most of 
researchers focus on how to extract useful textual 
features (lexical, syntactic, punctuation, etc.) for 
determining the semantic orientation of the 
sentences using machine learning algorithm (Bo et 
al. 2002; Kim and Hovy, 2004; Bo et al 2005, Hu 
et al 2004; Alina et al2008; Alistair et al2006). 
But fewer researchers deal with this problem using 
CRFs model.  
For identifying the subjective sentences, there 
are several research, like (Wiebe et al 2005). For 
polarity classification on sentence level, (Kim and 
Hovy, 2004) judged the sentiment by classifying a 
pseudo document composed of synonyms of 
indicators in one sentence. (Pang and Lee, 04) 
proposed a semi-supervised machine learning 
method based on subjectivity detection and 
minimum-cut in graph.  
Cascaded models for sentiment classification 
were studied by (Pang and Lee, 2005). Their work 
mainly used the cascaded frame for determining 
the orientation of a document and the sentences. In 
that work, an initial model is used to determine the 
orientation of each sentence firstly, then the top 
subjective sentences are input into a document -
level model to determine the document?s 
orientation.  
The CRFs has previously been used for 
sentiment classification. Those methods based on 
CRFs are related to our work. (Mao et al 2007) 
used a sequential CRFs regression model to 
measure the polarity of a sentence in order to 
determine the sentiment flow of the authors in 
reviews. However, this method must manually 
124
select a word set for constraints, where each 
selected word achieved the highest correlation with 
the sentiment. The performance of isotonic CRFs 
is strongly related to the selected word set. 
(McDonald et al2007; Ivan et al2008) proposed a 
structured model based on CRFs for jointly 
classifying the sentiment of text at varying levels 
of granularity. They put the sentence level and 
document level sentiment analysis in an integrated 
model and employ the orientation of the document 
to influence the decision of sentence?s orientation. 
Both the above two methods didn?t consider the 
redundant and hierarchical relation between 
sentimental strength labels. So their methods 
cannot get better results for the problem mentioned 
in this paper. 
Another solution to this problem is to use a joint 
multi-layer model, such as dynamic CRFs, multi-
layer CRFs, etc. Such kind of models can treat the 
three sub-tasks in sentiment classification as a 
multi-task problem and can use a multi-layer or 
hierarchical undirected graphic to model the 
sentiment of sentences. The main difference 
between our method and theirs is that we consider 
the problem from the feature representation view. 
Our method expands the feature set according to 
the number of layers in the hierarchical frame. So 
the complexity of its decoding procedure is lower 
than theirs, for example the complexity of the 
multi-layer CRFs is ( )j
j
lO N F? ?? when 
decoding and our method only has ( )j
j
FO N l? ?? , 
where N is the average sentence length, jF  is the 
average number of activated features in the jth layer, 
l  is the number of the original labels. 
7 Conclusion and Future Work 
In the paper, we propose a novel method for 
sentiment classification based on CRFs in response 
to the two special characteristics of ?contextual 
dependency? and ?label redundancy? in sentence 
sentiment classification.  We try to capture the 
contextual constraints on the sentence sentiment 
using CRFs. For capturing the label redundancy 
among sentiment classes, we generate a 
hierarchical framework through introducing 
redundant labels, under which redundant features 
can be introduced. The experimental results prove 
that our method outperforms the traditional 
methods (like NB, SVM, ME and standard chain 
CRFs). In comparison with cascaded CRFs, our 
method can effectively alleviate error propagation 
among different layers and obtain better 
performance in each layer.  
For our future work, we will explore other 
hierarchical models for sentimental strength rating 
because the experiments presented in this paper 
prove this hierarchical frame is effective for 
ordinal regression. We would expand the idea in 
this paper  into other models, such as Semi-CRFs 
and Hierarchical-CRFs. 
Acknowledgments 
The work is supported by the National Natural 
Science Foundation of China under Grants no. 
60673042, the Natural Science Foundation of 
Beijing under Grants no. 4073043 and the National 
High Technology Development 863 Program of 
China under Grants no. 2006AA01Z144. 
References 
Alina A. and Sabine B. 2008. When Specialists and 
Generalists Work Together: Overcoming Domain 
Dependence in Sentiment Tagging. In Proc. of ACL-
08 
Alistair Kennedy and Diana Inkpen. 2006. Sentiment 
Classification of Movie Reviews Using Contextual 
Valence Shifters. Computational Intelligence, 22(2), 
pages 110-125 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
machine learning techniques. In Proceedings of 
EMNLP 2002, pp.79-86. 
Bo Pang and Lillian Lee. 2004. A sentimental education: 
Sentiment analysis using subjectivity summarization 
based on minimum cuts. In Proceedings of ACL 2004, 
pp.271-278. 
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting 
class relationships for sentiment categorization with 
respect to rating scales. In Proceedings of ACL 2005, 
pp.115-124.  
Ivan Titov and Ryan McDonald. 2008. A Joint Model o 
f Text and Aspect Ratings of Sentiment 
Summarization. In Proceedings of ACL-08, pages 
308-316 
125
Janyce Webie, Theresa Wilson and Claire Cardie. 2005. 
Annotating expressions of opinions and emotions in 
lauguage. Language Resources and Evaluation 2005 
Fei Sha and Fernando Pereira, 2003 Shallow Parsing 
with Conditional Random Fields, In Proceedings 
ofHLT-NAACL 2003, Edmonton, Canada, pp. 213-
220. 
Kim, S and Edward H. Hovy. 2004. Determining the 
Sentiment of Opinions. In Proceedings of COLING-
04. 
Kamal Nigam, John Lafferty and Andrew McCallum. 
1999. Using Maximum Entropy for Text 
Classification. In Proceedings of IJCAI Workshop on 
Machine Learning for Information Filtering, pages 
61-67. 
J Lafferty, A McCallum, F Pereira. 2001. Conditional 
random fields: Probabilistic models for segmenting 
and labeling sequence data. In Proceedings of  
ICML-01,  pages 282.289.  
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie review 
mining and summarization. In Proceedings of the 
15th ACM international conference on Information 
and knowledge management (CIKM), pages 43-50. 
M. Hu and B. Liu. 2004a. Mining and summarizing 
customer reviews. In Proceedings of the 2004 ACM 
SIGKDD international conference on Knowledge 
discovery and data mining, pages 168-177. 
Ryan McDonald, Kerry Hannan and Tyler Neylon et al 
Structured Models for Fine-to-Coarse Sentiment 
Analysis. In Proceedings of ACL 2007, pp. 432-439. 
Wei Wu, Zoubin Ghahraman, 2005. Gaussian 
Processes for Oridinal Regression. The Journal of 
Machine learning Research, 2005 
Y. Mao and G. Lebanon, 2007. Isotonic Conditional 
Random Fields and Local Sentiment Flow. Advances 
in Neural Information Processing Systems 19, 2007 
126
Proceedings of ACL-08: HLT, pages 541?549,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
 
Chinese-English Backward Transliteration Assisted with Mining Mono-lingual Web Pages    Fan Yang, Jun Zhao, Bo Zou, Kang Liu, Feifan Liu  National Laboratory of Pattern Recognition   Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China {fyang,jzhao,bzou,kliu,ffliu}@nlpr.ia.ac.cn     
Abstract 
In this paper, we present a novel backward transliteration approach which can further as-sist the existing statistical model by mining monolingual web resources. Firstly, we em-ploy the syllable-based search to revise the transliteration candidates from the statistical model. By mapping all of them into existing words, we can filter or correct some pseudo candidates and improve the overall recall. Secondly, an AdaBoost model is used to re-rank the revised candidates based on the in-formation extracted from monolingual web pages. To get a better precision during the re-ranking process, a variety of web-based in-formation is exploited to adjust the ranking score, so that some candidates which are less possible to be transliteration names will be as-signed with lower ranks. The experimental re-sults show that the proposed framework can significantly outperform the baseline translit-eration system in both precision and recall. 1 Introduction* The task of Name Entity (NE) translation is to translate a name entity from source language to target language, which plays an important role in machine translation and cross-language informa-tion retrieval (CLIR). Transliteration is a subtask in NE translation, which translates NEs based on the phonetic similarity. In NE translation, most person names are transliterated, and some parts of location names or organization names also need to be trans-literated. Transliteration has two directions: for-ward transliteration which transforms an original name into target language, and backward translit-eration which recovers a name back to its original expression. For instance, the original English per-                                                           *Contact: Jun ZHAO, jzhao@nlpr.ia.ac.cn.  
son name ?Clinton? can be forward transliterated to its Chinese expression ??/ke ?/lin?/dun? and the backward transliteration is the inverse process-ing. In this paper, we focus on backward translit-eration from Chinese to English. Many previous researches have tried to build a transliteration model using statistical approach [Knight and Graehl, 1998; Lin and Chen, 2002; Virga and Khudanpur, 2003; Gao, 2004]. There are two main challenges in statistical backward trans-literation: First, statistical transliteration approach selects the most probable translations based on the knowledge learned from the training data. This approach, however, does not work well when there are multiple standards [Gao, 2004]. Second, back-ward transliteration is more challenging than for-ward transliteration as it is required to disambiguate the noises introduced in the forward transliteration and estimate the original name as close as possible [Lin and Chen, 2002]. One of the most important causes in introducing noises is that: some silent syllables in original names have been missing when they are transliterated to target lan-guage. For example, when ?Campbell? is translit-erated into ??/kan?/bei?/er?, the ?p? is missing.  In order to make up the disadvantages of statisti-cal approach, some researchers have been seeking for the assistance of web resource. [Wang et al, 2004; Cheng et al, 2004; Nagata et al, 2001; Zhang et al 2005] used bilingual web pages to ex-tract translation pairs. Other efforts have been made to combine a statistical transliteration model with web mining [Al-Onaizan and Knight, 2002; Long Jiang et al 2007]. Most of these methods need bilingual resources. However, those kinds of resources are not readily available in many cases. Moreover, to search for bilingual pages, we have to depend on the performance of search engines. We can?t get Chinese-English bilingual pages when the input is a Chinese query. Therefore, the existing 
541
 
assistance approaches using web-mining to assist transliteration are not suitable for Chinese to Eng-lish backward transliteration. Thus in this paper, we mainly focus on the fol-lowing two problems to be solved in transliteration. Problem I: Some silent syllables are missing in English-Chinese forward transliteration. How to recover them effectively and efficiently in back-ward transliteration is still an open problem. Problem II: Statistical transliteration always chooses the translations based on probabilities. However, in some cases, the correct translation may have lower probability. Therefore, more stud-ies are needed on combination with other tech-niques as supplements. Aiming at these two problems, we propose a method which mines monolingual web resources to assist backward transliteration. The main ideas are as follows. We assume that for every Chinese en-tity name which needs to be backward transliter-ated to an English original name, the correct transliteration exists somewhere in the web. What we need to do is to find out the answers based on the clues given by statistical transliteration results. Different from the traditional methods which ex-tract transliteration pairs from bilingual pages, we only use monolingual web resources. Our method has two advantages. Firstly, there are much more monolingual web resources available to be used. Secondly, our method can revise the transliteration candidates to the existing words before the subse-quent re-ranking process, so that we can better mine the correct transliteration from the Web. Concretely, there are two phases involved in our approach. In the first phase, we split the result of transliteration into syllables, and then a syllable-based searching processing can be employed to revise the result in a word list generated from web pages, with an expectation of higher recall of trans-literation. In the second phase, we use a revised word as a search query to get its contexts and hit information, which are integrated into the AdaBoost classifier to determine whether the word is a transliteration name or not with a confidence score. This phase can readjust the candidate?s score to a more reasonable point so that precision of transliteration can be improved. Table 1 illustrates how to transliterate the Chinese name ??/a?/jia?/xi? back to ?Agassi?.  Chinese name Transliteration results Revised Candidate Re-rank Results 
??? a  jia xi Agassi 
aggasi agahi agacy agasie ? 
agasi agathi agathe agassi ? 
agassi agasi agache agga ? Table 1. An example of transliteration flow The experimental results show that our approach improves the recall from 41.73% to 59.28% in open test when returning the top-100 results, and the top-5 precision is improved from 19.69% to 52.19%. The remainder of the paper is structured as fol-lows. Section 2 presents the framework of our sys-tem. We discuss the details of our statistical transliteration model in Section 3. In Section 4, we introduce the approach of revising and re-ranking the results of transliteration. The experiments are reported in Section 5. The last section gives the conclusion and the prediction of future work. 2 System Framework  Our system has three main modules. 
  Figure 1. System framework 1) Statistical transliteration: This module re-ceives a Chinese Pinyin sequence as its input, and output the N-best results as the transliteration can-didates.  2) Candidate transliteration revision through syllable-based searching: In the module, a transliteration candidate is transformed into a syllable query. We use a syllable-based searching strategy to select the revised candidate from a huge word list. Each word in the list is indexed by sylla-bles, and the similarity between the word and the query is calculated. The most similar words are returned as the revision results. This module guar-
Monolingual web pages 
Words list 
Chinese name 
Statistical model 
Transliteration candidates Syllable-based search 
Revised candidates Re-ranking phase 
Final results Search engine 
542
 
antees the transliteration candidates are all existing words. 3) Revised candidate re-ranking in web pages: In the module, we search the revised candi-dates to get their contexts and hit information which we can use to score the probability of being a transliteration name. This phase doesn?t generate new candidates, but re-rank the revised candidate set to improve the performance in top-5. Under this framework, we can solve the two problems of statistical model mentioned above.  (1) The silent syllables will be given lower weights in syllable-based search, so the missing syllables will be recovered through selecting the most similar existing words which can contain some silent syllables.  (2) The query expansion technology can recall more potential transliteration candidates by ex-panding syllables to their ?synonymies?. So the mistakes introduced when selecting syllables in statistical transliteration will be corrected through giving suitable weights to synonymies.  Through the revision phase, the results of statis-tical model which may have illegal spelling will be mapped to its most similar existing words. That can improve the recall. In re-ranking phase, the revised candidate set will be re-ranked to put the right answer on the top using hybrid information got from web resources. So the precision of trans-literation will be improved. 3 Statistical Transliteration Model We use syllables as translation units to build a sta-tistical Chinese-English backward transliteration model in our system. 3.1 Traditional Statistical Translation Model [P. Brown et al, 1993] proposed an IBM source-channel model for statistical machine translation (SMT). When the channel output f= f1,f2 ?. fn ob-served, we use formula (1) to seek for the original sentence e=e1,e2 ?. en with the most likely poste-riori. 
' argmax ( | ) argmax ( | ) ( )
e e
e P e f P f e P e= =
      (1) The translation model ( | )P f e  is estimated from a paired corpus of foreign-language sentences and their English translations. The language model ( )P e  is trained from English texts. 
3.2 Our Transliteration Model The alignment method is the base of statistical transliteration model. There are mainly two kinds of alignment methods: phoneme-based alignment [Knight and Graehl, 1998; Virga and Khudanpur, 2003] and grapheme-based alignment [Long Jiang, 2007]. In our system, we adopt the syllable-based alignment from Chinese pinyin to English syllables, where the syllabication rules mentioned in [Long Jiang et al, 2007] are used. For example, Chinese name ??/xi ?/er ?/dun? and its backward transliteration ?Hilton? can be aligned as follows. ?Hilton? is split into syllable sequence as ?hi/l/ton?, and the alignment pairs are ?xi-hi?, ?er-l?, ?dun-ton?.  Based on the above alignment method, we can get our statistical Chinese-English backward trans-literation model as, 
argmax ( | ) ( )
E
E p PY ES p ES=
            (2) Where, PY is a Chinese Pinyin sequence, ES is a English syllables sequence, ( | )p PY ES  is the probability of translating ES into PY, ( )p E S  is the generative probability of a English syllable lan-guage model. 3.3 The Difference between Backward Trans-literation and Traditional Translation Chinese-English backward transliteration has some differences from traditional translation. 1) We don?t need to adjust the order of sylla-bles when transliteration.  2) The language model in backward translitera-tion describes the relationship of syllables in words. It can?t work as well as the language model de-scribing the word relationship in sentences. We think that the crucial problem in backward transliteration is selecting the right syllables at every step. It?s very hard to obtain the exact an-swer only based on the statistical transliteration model. We will try to improve the statistical model performance with the assistance of mining web resources. 4 Mining Monolingual Web Pages to As-sist Backward Transliteration  In order to get assistance from monolingual Web resource to improve statistical transliteration, our 
543
 
method contains two main phases: ?revision? and ?re-ranking?. In the revision phase, transliteration candidates are revised using syllable-based search in the word list, which are generated by collecting the existing words in web pages. Because the proc-ess of named entity recognition may lose some NEs, we will reserve all the words in web corpus without any filtering. The revision process can im-prove the recall through correcting some mistakes in the transliteration results of statistical model. In the re-ranking phase, we search every revised candidate on English pages, score them according to their contexts and hit information so that the right answer will be given a higher rank.  4.1 Using Syllable-based Retrieval to Revise Transliteration Candidates In this section, we will propose two methods re-spectively for the two problems of statistical model mentioned in section 1.  4.1.1  Syllable-based retrieval model When we search a transliteration candidate tci in the word list, we firstly split it into syllables {es1,es2,?..esn}. Then this syllable sequence is used as a query for syllable-based searching.  We define some notions here. ? Term set T={t1,t2?.tk} is an orderly set of all syllables which can be viewed as terms.  ? Pinyin set P={py1,py2?.pyk} is an orderly set of all Pinyin.  ? An input word can be represented by a vec-tor of syllables {es1,es2,?..esn}.  We calculate the similarity between a translitera-tion result and each word in the list to select the most similar words as the revised candidates. The {es1,es2,?..,esn} will be transformed into a vector Vquery={t1,t2?.tk} where ti represents the ith term in T. The value of ti is equal to 0 if the ith term doesn?t appear in query. In the same way, the word in list can also be transformed into vector represen-tation. So the similarity can be calculated as the inner product between these two vectors.  We don?t use tf and idf conceptions as traditional information retrieval (IR) to calculate the terms? weight. We use the weight of ti to express the ex-pectation probability of ith term having pronuncia-tion. If the term has a lower probability of having pronunciation, its weight is low. So when we searching, the missing silent syllables in the results 
of statistical transliteration model can be recovered because such syllables have little impact on simi-larity measurement. The formula we used is as fol-lows. 
( , )
/
query word
word py
V V
Sim query word
L L
?
=
            (3) 
The numerator is the inner product of two vec-tors. The denominator is the length of word Lword divided by the length of Chinese pinyin sequence Lpy. In this formula, the more syllables in one word, the higher score of inner production it may get, but the word will get a loss for its longer length. The word which has the shortest length and the highest syllable hitting ratio will be the best. Another difference from traditional IR is how to deal with the order of the words in a query. Ac-cording to transliteration, the similarity must be calculated under the limitation of keeping order, which can?t be satisfied by current methods. We use the algorithm like calculating the edit distance between two words. The syllables are viewed as the units which construct a word. The edit distance calculation finds the best matching with the least operation cost to change one word to another word by using deletion/addition/insertion operations on syllables. But the complexity will be too high to afford if we calculate the edit distance between a query and each word in the list. So, we just calcu-late the edit distance for the words which get high score without the order limitation. This trade off method can save much time but still keep perform-ance. 4.1.2  Mining the Equivalent through Syllable Expansion In most collections, the same concept may be re-ferred to using different words. This issue, known as synonymy, has an impact on the recall of most information retrieval systems. In this section, we try to use the expansion technology to solve prob-lem II. There are three kinds of expansions to be explained below.  Syllable expansion based on phonetic similar-ity: The syllables which correspond to the same Chinese pinyin can be viewed as synonymies. For example, the English syllables ?din? and ?tin? can be aligned to the same Chinese pinyin ?ding?. Given a Chinese pinyin sequence {py1,py2,?..pyn} as the input of transliteration model, for every pyi, there are a set of syllables 
544
 
{es1, es2 ?.. esk} which can be selected as its translation. The statistical model will select the most probable one, while others containing the right answer are discarded. To solve this problem, we expand the query to take the synonymies of terms into consideration. We create an expansion set for each Chinese pinyin. A syllable esi will be selected into the expansion set of pyj based on the alignment probability P(esi|pyj) which can be ex-tracted from the training corpus. The phonetic similarity expansion is based on the input Chinese Pinyin sequence, so it?s same for all candidates. Syllable expansion based on syllable similar-ity: If two syllables have similar alignment prob-ability with every pinyin, we can view these two syllables as synonymy. Therefore, if a syllable is in the query, its synonymies should be contained too. For example, ?fea? and ?fe? can replace each other. To calculate the similarity, we first obtain the alignment probability P(pyj|esk) of every syllable. Then the distance between any two syllables will be calculated using formula (4). 
1
1
( , ) ( | ) ( | )
N
j k i j i k
i
Sim es es P py es P py es
N
=
=
?
(4) This formula is used to evaluate the similarity of two syllables in alignment. The expansion set of the ith syllable can be generated by selecting the most similar N syllables. This kind of expansion is conducted upon the output of statistical translitera-tion model. Syllable expansion based on syllable edit dis-tance: The disadvantage of last two expansions is that they are entirely dependent on the training set. In other word, if some syllables haven?t appeared in the training corpus, they will not be expanded. To solve the problem, we use the method of expan-sion based on edit distance. We use edit distance to measure the similarity between two syllables, one is in training set and the other is absent. Because the edit distance expansion is not very relevant to pronunciation, we will give this expansion method a low weight in combination. It works when new syllables arise.  Combine the above three strategies: We will combine the three kinds of expansion method to-gether. We use the linear interpolation to integrate them. The formulas are follows.   
(1 )
pre sy ed
S S S S? ? ?= ? + +                (5) 
(1 )
pre py ed
S S S S? ? ?= ? + +                (6) 
where Spre is the score of exact matching, Ssy is the score of expansion based on syllables similarity and Spy based on phonetic similarity. We will ad-just these parameters to get the best performance. The experimental results and analysis will be re-ported in section 5.3. 4.2 Re-Ranking the Revised Candidates Set using the Monolingual Web Resource In the first phase, we have generated the revised candidate set {rc1,rc2,?,rcn} from the word list us-ing the transliteration results as clues. The objec-tive is to improve the overall recall. In the second phase, we try to improve the precision, i.e. we wish to re-rank the candidate set so that the correct an-swer will be put in a higher rank. [Al-Onaizan et al, 2002] has proposed some methods to re-score the transliteration candidates. The limitation of their approach is that some can-didates are propbale not existing words, with which we will not get any information from web. So it can only re-rank the transliteration results to improve the precision of top-5. In our work, we can improve the recall of transliteration through the revising process before re-ranking. In this section, we employ the AdaBoost frame-work which integrates several kinds of features to re-rank the revised candidate set. The function of the AdaBoost classifier is to calculate the probabil-ity of the candidate being a NE. Then we can re-rank the revised candidate set based on the score. The features used in our system are as follows. NE or not: Using rci as query to search for monolingual English Web Pages, we can get the context set {Ti1, Ti2??Tin} of rci. Then for every Tik, we use the named entity recognition (NER) software to determine whether rci is a NE or not. If rci is recognized as a NE in some Tik, rci will get a score. If rci can?t be recognized as NE in any con-texts, it will be pruned. The hit of the revised candidate: We can get the hit information of rci from search engine. It is used to evaluate the importance of rci. Unlike [Al-Onaizan et al, 2002], in which the hit can be used to eliminate the translation results which contain illegal spelling, we just use hit number as a feature. The limitation of compound NEs: When trans-literating a compound NE, we always split them into several parts, and then combine their translit-eration results together. But in this circumstance, 
545
 
every part can add a limitation in the selection of the whole NE. For example: ??/xi?/la?/li ?  ?/ke?/lin?/dun? is a compound name. ??/xi?/la?/li? can be transliterate to ?Hilary? or ?Hilaly? and ??/ke?/lin?/dun? can be transliterate to ?Clinton? or ?Klinton?. But the combination of ?Hilary?Clinton? will be selected for it is the most common combination. So the hit of combination query will be extracted as a feature in classifier. Hint words around the NE: We can take some hint words around the NE into the query, in order to add some limitations to filter out noisy words. For example: ??? (president)? can be used as hint word for ???? (Clinton)?. To find the hint words, we first search the Chinese name in Chi-nese web pages. The frequent words can be ex-tracted as hint words and they will be translated to English using a bilingual dictionary. These hint words are combined with the revised candidates to search English web pages. So, the hit of the query will be extracted as feature. The formula of AdaBoost is as follow. 
1
( ) ( ( ))
T
t t
t
H x sign h x?
=
=
?
                 (7) 
Where 
t
?  is the weight for the ith weak classifier 
( )
t
h x . 
t
?  can be calculated based on the precision of its corresponding classifier. 5 Experiments We carry out experiments to investigate how much the revision process and the re-ranking process can improve the performance compared with the base-line of statistical transliteration model. We will also evaluate to which extents we can solve the two problems mentioned in section 1 with the as-sistance of Web resources. 5.1 Experimental data The training corpus for statistical transliteration model comes from the corpus of Chinese <-> Eng-lish Name Entity Lists v 1.0 (LDC2005T34). It contains 565,935 transliteration pairs. Ruling out those pairs which are not suitable for the research on Chinese-English backward transliteration, such as Chinese-Japanese, we select a training set which contains 14,443 pairs of Chinese-European & American person names. In the training set, 1,344 
pairs are selected randomly as the close test data. 1,294 pairs out of training set are selected as the open test data. To set up the word list, a 2GB-sized collection of web pages is used. Since 7.42% of the names in the test data don?t appear in the list, we use Google to get the web page containing the ab-sent names and add these pages into the collection. The word list contains 672,533 words. 5.2 Revision phase vs. statistical approach Using the results generated from statistical model as baseline, we evaluate the revision module in recall first. The statistical transliteration model works in the following 4 steps: 1) Chinese name are transformed into pinyin representation and the English names are split into syllables. 2) The GIZA++1 tool is invoked to align pinyin to sylla-bles, and the alignment probabilities ( | )P py es are obtained. 3) Those frequent sequences of syllables are combined as phrases. For example, ?be/r/g???berg?, ?s/ky???sky?. 4) Camel 2  de-coder is executed to generate 100-best candidates for every name. We compare the statistical transliteration results with the revised results in Table 2. From Table 2 we can find that the recall of top-100 after revision is improved by 13.26% in close test set and 17.55% in open test set. It proves that the revision module is effective for correcting the mistakes made in statistical transliteration model. Transliteration results Revised results  close open close open Top1 33.64% 9.41% 27.15% 11.04% Top5 40.37% 13.38% 42.83% 19.69% Top10 47.79% 17.56% 56.98% 26.52% Top20 61.88% 25.44% 71.05% 37.81% Top50 66.49% 36.19% 82.16% 46.22% Top100 72.52% 41.73% 85.78% 59.28% Table 2. Statistical model vs. Revision module To show the effects of the revision on the two above-mentioned problems in which the statistical model does not solve well: the losing of silent syl-lables and the selection bias problem, we make a statistics of the improvements with a measurement of ?correction time?. For a Chinese word whose correct transliteration appears in top-100 candidates only if it has been 
                                                           1 http://www.fjoch.com/GIZA++.html 2 http://www.nlp.org.cn 
546
 
revised, we count the ?correction time?. For exam-ple, when ?Argahi? is revised to ?Agassi? the cor-rection time is ?1? for Problem II and ?1? for Problem I, because in ?hi?? ?si? the syllable is expanded, and in ?si? ??ssi? an ?s? is added.   Close test Open test Problem I 0.6931 0.7853 Problem II 0.9264 1.1672 Table 3. Average time of correction This measurement reflects the efficiency of the revision of search strategy, in contrast to those spelling correction techniques in which several operations of ?add? and ?expand? are inevitable. It has proved that the more an average correction time is, the more efficient our strategy is.  
?
???
???
???
???
???
???
???
???
???
?
? ? ? ? ? ? ? ? ?
??????? ?????????  Figure 2. Length influence in recall comparison The recall of the statistical model relies on the length of English name in some degree. It is more difficult to obtain an absolutely correct answer for longer names, because they may contain more si-lent and confused syllables. However, through the revision phase, this tendency can be effectively alleviated. In Figure 2, we make a comparison be-tween the results of the statistical model and the revision module with the changing of syllable?s length in open test. The curves demonstrate that the revision indeed prevents the decrease of recall for longer names. 5.3 Parameter setting in the revision phase We will show the experimental results when set-ting different parameters for query expansion. In the expansion based on phonetic similarity, for every Chinese pinyin, we select at most 20 sylla-bles to create an expansion set. We set 0.1? =  in formula (5). The results are shown in the columns labeled ?exp1? in Table 4. From the results we can conclude that, we get the best performance when 
0.4? = . That means the performance is best when the weight of exact 
matching is a little larger than the weight of fuzzy matching. We can also see that, higher weight of exact matching will lead to low recall, while higher weight of fuzzy matching will bring noise in. The expansion method based on syllable similar-ity is also evaluated. For every syllable, we select at most 15 syllables to create the expansion set. We set 0.1? = . The results are shown in the columns labeled ?exp2? in Table 4. From the results we can conclude that, we get the best performance when 0.5? = . It means that we can?t put emphasis on any matching methods. Comparison with the expansion based on phonetic similarity, the performance is poorer. It means that the expansion based on phonetic similarity is more suitable for revising transliteration candidates. 5.4 Revision phase vs. re-ranking phase After the phase of revising transliteration candi-dates, we re-rank the revised candidate set with the assistance of monolingual web resources. In this section, we will show the improvement in preci-sion after re-ranking. We have selected four kinds of features to inte-grate in the AdaBoost framework. To determine whether the candidate is NE or not in its context, we use the software tool Lingpipe3. The queries are sent to google, so that we can get the hit of queries and the top-10 snippets will be extracted as context. The comparison of revision results and re-ranking results is shown as follows. Revised results Re-ranked results  close open close open Top1 27.15% 11.04% 58.08% 38.63% Top5 42.83% 19.69% 76.35% 52.19% Top10 56.98% 26.52% 83..92% 54.33% Top20 71.05% 37.81% 83.92% 57.61% Top50 82.16% 46.22% 83.92% 57.61% Top100 85.78% 59.28% 85.78% 59.28% Table 5. Revision results vs. Re-ranking results From these results we can conclude that, after re-ranking phase, the noisy words will get a lower 
                                                           3 http://www.alias-i.com/lingpipe/ 
547
 
0.2? =  0.3? =   0.4? =  0.5? =  0.6? =  0.7? =  0.8? =   exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 exp1 exp2 Top1 13.46 13.32 13.79 13.61 11.04 12.70 11.65 10.93 10.83 11.25 9.62 10.63 8.73 10.18 Top5 21.58 19.59 23.27 20.17 19.69 18.28 21.07 17.25 22.05 16.84 17.90 16.26 17.38 15.34 Top10 27.39 22.71 28.41 24.73 26.52 22.93 26.83 21.81 27.26 20.39 24.38 21.20 25.42 18.20 Top20 35.23 34.88 35.94 29.49 37.81 31.57 38.59 33.04 36.52 31.72 35.25 29.75 34.65 27.62 Top50 43.91 40.63 43.75 40.85 46.22 41.46 48.72 42.79 45.48 40.49 41.57 39.94 42.81 38.07 Top100 53.76 48.47 54.38 52.04 59.28 53.15 57.36 53.46 55.19 51.83 55.63 49.52 53.41 47.15 Table 4.  Parameters Experiment rank. Through the revision module, we get both higher recall and higher precision than statistical transliteration model when at most 5 results are returned. We also use the average rank and average recip-rocal rank (ARR) [Voorhees and Tice, 2000] to evaluate the improvement. ARR is calculated as       
1
1 1
( )
M
i
ARR
M R i
=
=
?
                             (8) where ( )R i  is the rank of the answer of ith test word. M is the size of test set. The higher of ARR, the better the performance is. The results are shown as Table 6. Statistical  model Revision  module Re-rank  Module  close open close open close open Average rank 37.63 70.94 24.52 58.09 16.71 43.87 ARR 0.3815 0.1206 0.3783 0.1648 0.6519 0.4492 Table 6. ARR and AR evaluation The ARR after revision phase is lower than the statistical model. Because the goal of revision module is to improve the recall as possible as we can, some noisy words will be introduced in. The noisy words will be pruned in re-ranking module. That is why we get the highest ARR value at last. So we can conclude that the revision module im-proves recall and re-ranking module improves pre-cision, which help us get a better performance than pure statistical transliteration model 6 Conclusion In this paper, we present a new approach which can revise the results generated from statistical transliteration model with the assistance of mono-lingual web resource. Through the revision process, the recall of transliteration results has been im-proved from 72.52% to 85.78% in the close test set and from 41.73% to 59.28% in open test set, re-spectively. We improve the precision in re-ranking phase, the top-5 precision can be improved to 76.35% in close test and 52.19% in open test. The 
promising results show that our approach works pretty well in the task of backward transliteration. In the future, we will try to improve the similar-ity measurement in the revision phase. And we also wish to develop a new approach using the transliteration candidates to search for their right answer more directly and effectively. Acknowledgments The work is supported by the National High Tech-nology Development 863 Program of China under Grants no. 2006AA01Z144, the National Natural Science Foundation of China under Grants No. 60673042, the Natural Science Foundation of Bei-jing under Grants no. 4073043. References  Yaser Al-Onaizan and Kevin Knight. 2002. Translating named entities using monolingual and bilingual re-sources. In Proc.of ACL-02.  Kevin Knight and Jonathan Graehl. 1998. Machine Transliteration. Computational Linguistics 24(4). Wei-Hao Lin and Hsin-His Chen. 2002 Backward Ma-chine Transliteration by Learning Phonetic Similarity. In Proc. Of the 6th CoNLL Donghui Feng, Yajuan Lv, and Ming Zhou. 2004. A New Approach for English-Chinese Named Entity Alignment. In Proc. of EMNLP-2004. Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng Niu, 2007. Named Entity Translation with Web Min-ing and Transliteration. In Proc. of IJCAI-2007. Wei Gao. 2004. Phoneme-based Statistical Translitera-tion of Foreign Name for OOV Problem. A thesis of Master. The Chinese University of Hong Kong. Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining translations of OOV terms from the web through cross-lingual query expansion. SIGIR 2005. Pu-Jen Cheng, Wen-Hsiang Lu, Jer-Wen Teng, and Lee-Feng Chien. 2004 Creating Multilingual Transla-tion Lexicons with Regional Variations Using Web Corpora. In Proc. of ACL-04 Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 2001. Using the Web as a Bilingual Dictionary. In Proc. of ACL 2001 Workshop on Data-driven Methods in Machine Translation. 
548
 
Paola Virga and Sanjeev Khudanpur. 2003. Translitera-tion of proper names in cross-lingual information re-trieval. In Proc. of the ACL workshop on Multi-lingual Named Entity Recognition. Jenq-Haur Wang, Jei-Wen Teng, Pu-Jen Cheng, Wen-Hsiang Lu, Lee-Feng Chien. 2004. Translating un-known cross-lingual queries in digital libraries using a web-based approach. In Proc. of JCDL 2004. E.M.Voorhees and D.M.Tice. 2000. The trec-8 question answering track report. In Eighth Text Retrieval Con-ference (TREC-8) 
549
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 387?395,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Chinese-English Organization Name Translation System Using 
Heuristic Web Mining and Asymmetric Alignment 
 
 
Fan Yang, Jun Zhao, Kang Liu 
National Laboratory of Pattern Recognition  
 Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China 
{fyang,jzhao,kliu}@nlpr.ia.ac.cn 
  
  
 
Abstract 
In this paper, we propose a novel system for 
translating organization names from Chinese 
to English with the assistance of web 
resources. Firstly, we adopt a chunking-
based segmentation method to improve the 
segmentation of Chinese organization names 
which is plagued by the OOV problem. 
Then a heuristic query construction method 
is employed to construct an efficient query 
which can be used to search the bilingual 
Web pages containing translation 
equivalents. Finally, we align the Chinese 
organization name with English sentences 
using the asymmetric alignment method to 
find the best English fragment as the 
translation equivalent. The experimental 
results show that the proposed method 
outperforms the baseline statistical machine 
translation system by 30.42%. 
1 Introduction 
The task of Named Entity (NE) translation is to 
translate a named entity from the source language 
to the target language, which plays an important 
role in machine translation and cross-language 
information retrieval (CLIR). The organization 
name (ON) translation is the most difficult 
subtask in NE translation. The structure of ON is 
complex and usually nested, including person 
name, location name and sub-ON etc. For 
example, the organization name ???????
????? (Beijing Nokia Communication 
Ltd.)? contains a company name (???/Nokia) 
and a location name (??/Beijing). Therefore, 
the translation of organization names should 
combine transliteration and translation together.  
Many previous researchers have tried to solve 
ON translation problem by building a statistical 
model or with the assistance of web resources. 
The performance of ON translation using web 
knowledge is determined by the solution of the 
following two problems:  
 The efficiency of web page searching: how 
can we find the web pages which contain the 
translation equivalent when the amount of the 
returned web pages is limited? 
 The reliability of the extraction method: how 
reliably can we extract the translation equivalent 
from the web pages that we obtained in the 
searching phase?  
For solving these two problems, we propose a 
Chinese-English organization name translation 
system using heuristic web mining and 
asymmetric alignment, which has three 
innovations.  
1) Chunking-based segmentation: A Chinese 
ON is a character sequences, we need to segment 
it before translation. But the OOV words always 
make the ON segmentation much more difficult. 
We adopt a new two-phase method here. First, 
the Chinese ON is chunked and each chunk is 
classified into four types. Then, different types of 
chunks are segmented separately using different 
strategies. Through chunking the Chinese ON 
first, the OOVs can be partitioned into one chunk 
which will not be segmented in the next phase. In 
this way, the performance of segmentation is 
improved.  
2) Heuristic Query construction: We need to 
obtain the bilingual web pages that contain both 
the input Chinese ON and its translation 
equivalent. But in most cases, if we just send the 
Chinese ON to the search engine, we will always 
get the Chinese monolingual web pages which 
don?t contain any English word sequences, let 
alone the English translation equivalent. So we 
propose a heuristic query construction method to 
generate an efficient bilingual query. Some 
words in the Chinese ON are selected and their 
translations are added into the query. These 
English words will act as clues for searching 
387
bilingual web pages. The selection of the Chinese 
words to be translated will take into 
consideration both the translation confidence of 
the words and the information contents that they 
contain for the whole ON.  
3) Asymmetric alignment: When we extract the 
translation equivalent from the web pages, the 
traditional method should recognize the named 
entities in the target language sentence first, and 
then the extracted NEs will be aligned with the 
source ON. However, the named entity 
recognition (NER) will always introduce some 
mistakes. In order to avoid NER mistakes, we 
propose an asymmetric alignment method which 
align the Chinese ON with an English sentence 
directly and then extract the English fragment 
with the largest alignment score as the equivalent. 
The asymmetric alignment method can avoid the 
influence of improper results of NER and 
generate an explicit matching between the source 
and the target phrases which can guarantee the 
precision of alignment.  
In order to illustrate the above ideas clearly,  
we give an example of translating the Chinese 
ON ??????????? (China Huarong 
Asset Management Corporation)?.  
Step1: We first chunk the ON, where ?LC?, 
?NC?, ?MC? and ?KC? are the four types of 
chunks defined in Section 4.2. 
??(China)/LC  ??(Huarong)/NC  ????
(asset management)/MC  ??(corporation)/KC 
Step2: We segment the ON based on the 
chunking results.  
??(china)  ??(Huarong)  ??(asset)     
??(management)  ??(corporation) 
If we do not chunk the ON first, the OOV 
word ???(Huarong)? may be segmented as ??   
??. This result will certainly lead to translation 
errors. 
Step 3: Query construction:  
We select the words ???? and ???? to 
translate and a bilingual query is constructed as: 
? ? ? ? ? ? ? ? ? ? ? ? + asset + 
management 
If we don?t add some English words into the 
query, we may not obtain the web pages which 
contain the English phrase ?China Huarong Asset 
Management Corporation?. In that case, we can 
not extract the translation equivalent. 
Step 4: Asymmetric Alignment: We extract a 
sentence ??President of China Huarong Asset 
Management Corporation?? from the returned 
snippets. Then the best fragment of the sentence 
?China Huarong Asset Management 
Corporation? will be extracted as the translation 
equivalent. We don?t need to implement English 
NER process which may make mistakes. 
The remainder of the paper is structured as 
follows. Section 2 reviews the related works. In 
Section 3, we present the framework of our 
system. We discuss the details of the ON 
chunking in Section 4. In Section 5, we introduce 
the approach of heuristic query construction. In 
section 6, we will analyze the asymmetric 
alignment method. The experiments are reported 
in Section 7. The last section gives the 
conclusion and future work. 
2 Related Work 
In the past few years, researchers have proposed 
many approaches for organization translation. 
There are three main types of methods. The first 
type of methods translates ONs by building a 
statistical translation model. The model can be 
built on the granularity of word [Stalls et al, 
1998], phrase [Min Zhang et al, 2005] or 
structure [Yufeng Chen et al, 2007]. The second 
type of methods finds the translation equivalent 
based on the results of alignment from the source 
ON to the target ON [Huang et al, 2003; Feng et 
al., 2004; Lee et al, 2006]. The ONs are 
extracted from two corpora. The corpora can be 
parallel corpora [Moore et al, 2003] or content-
aligned corpora [Kumano et al, 2004]. The third 
type of methods introduces the web resources 
into ON translation. [Al-Onaizan et al, 2002] 
uses the web knowledge to assist NE translation 
and [Huang et al, 2004; Zhang et al, 2005; Chen 
et al, 2006] extracts the translation equivalents 
from web pages directly.  
The above three types of methods have their 
advantages and shortcomings. The statistical 
translation model can give an output for any 
input. But the performance is not good enough on 
complex ONs. The method of extracting 
translation equivalents from bilingual corpora 
can obtain high-quality translation equivalents. 
But the quantity of the results depends heavily on 
the amount and coverage of the corpora. So this 
kind of method is fit for building a reliable ON 
dictionary. In the third type of method, with the 
assistance of web pages, the task of ON 
translation can be viewed as a two-stage process. 
Firstly, the web pages that may contain the target 
translation are found through a search engine. 
Then the translation equivalent will be extracted 
from the web pages based on the alignment score 
with the original ON. This method will not 
388
depend on the quantity and quality of the corpora 
and can be used for translating complex ONs. 
3 The Framework of Our System 
The Framework of our ON translation system 
shown in Figure 1 has four modules.  
 
Figure 1. System framework 
1) Chunking-based ON Segmentation Module: 
The input of this module is a Chinese ON. The 
Chunking model will partition the ON into 
chunks, and label each chunk using one of four 
classes. Then, different segmentation strategies 
will be executed for different types of chunks. 
2) Statistical Organization Translation Module: 
The input of the module is a word set in which 
the words are selected from the Chinese ON. The 
module will output the translation of these words.  
3) Web Retrieval Module: When input a 
Chinese ON, this module generates a query 
which contains both the ON and some words? 
translation output from the translation module. 
Then we can obtain the snippets that may contain 
the translation of the ON from the search engine. 
The English sentences will be extracted from 
these snippets.  
4) NE Alignment Module: In this module, the 
asymmetric alignment method is employed to 
align the Chinese ON with these English 
sentences obtained in Web retrieval module. The 
best part of the English sentences will be 
extracted as the translation equivalent. 
4 The Chunking-based Segmentation 
for Chinese ONs  
In this section, we will illustrate a chunking-
based Chinese ON segmentation method, which 
can efficiently deal with the ONs containing 
OOVs. 
4.1 The Problems in ON Segmentation 
The performance of the statistical ON translation 
model is dependent on the precision of the 
Chinese ON segmentation to some extent. When 
Chinese words are aligned with English words, 
the mistakes made in Chinese segmentation may 
result in wrong alignment results. We also need 
correct segmentation results when decoding. But 
Chinese ONs usually contain some OOVs that 
are hard to segment, especially the ONs 
containing names of people or brand names. To 
solve this problem, we try to chunk Chinese ONs 
firstly and the OOVs will be partitioned into one 
chunk. Then the segmentation will be executed 
for every chunk except the chunks containing 
OOVs. 
4.2 Four Types of Chunks  
We define the following four types of chunks for 
Chinese ONs: 
 Location Chunk (LC): LC contains the 
location information of an ON. 
 Name Chunk (NC): NC contains the name   
or brand information of an ON. In most 
cases, Name chunks should be 
transliterated. 
 Modification Chunk (MC): MC contains 
the modification information of an ON. 
 Key word Chunk (KC): KC contains the 
type information of an ON. 
The following is an example of an ON 
containing these four types of chunks. 
??(Beijing)/LC ? ? ? (Peregrine)/NC
????(investment consulting)/MC  ????
(co.)/KC  
In the above example, the OOV ????
(Peregrine)? is partitioned into name chunk. Then 
the name chunk will not be segmented.  
4.3 The CRFs Model for Chunking 
Considered as a discriminative probabilistic 
model for sequence joint labeling and with the 
advantage of flexible feature fusion ability, 
Conditional Random Fields (CRFs) [J.Lafferty et 
al., 2001] is believed to be one of the best 
probabilistic models for sequence labeling tasks. 
So the CRFs model is employed for chunking. 
We select 6 types of features which are proved 
to be efficient for chunking through experiments. 
The templates of features are shown in Table 1,  
389
Description Features 
current/previous/success 
character C0?C-1?C1 
whether the characters is 
a word 
W(C
-2C-1C0)?W(C0C1C2)?
W(C
-1C0C1) 
whether the characters is 
a location name 
L(C
-2C-1C0)?L(C0C1C2)?    
L(C
-1C0C1) 
whether the characters is 
an ON suffix 
SK(C
-2C-1C0)?SK(C0C1C2)? 
SK(C
-1C0C1) 
whether the characters is 
a location suffix 
SL(C
-2C-1C0)?SL(C0C1C2)?
SL(C
-1C0C1) 
relative position in the 
sentence 
POS(C0) 
Table 1. Features used in CRFs model 
where Ci denotes a Chinese character, i denotes 
the position relative to the current character. We 
also use bigram and unigram features but only 
show trigram templates in Table 1. 
5 Heuristic Query Construction 
In order to use the web information to assist 
Chinese-English ON translation, we must firstly 
retrieve the bilingual web pages effectively. So 
we should develop a method to construct 
efficient queries which are used to obtain web 
pages through the search engine. 
5.1 The Limitation of Monolingual Query 
We expect to find the web pages where the 
Chinese ON and its translation equivalent co-
occur. If we just use a Chinese ON as the query, 
we will always obtain the monolingual web 
pages only containing the Chinese ON. In order 
to solve the problem, some words in the Chinese 
ON can be translated into English, and the 
English words will be added into the query as the 
clues to search the bilingual web pages. 
5.2 The Strategy of Query Construction  
We use the metric of precision here to evaluate 
the possibility in which the translation equivalent 
is contained in the snippets returned by the search 
engine. That means, on the condition that we 
obtain a fixed number of snippets, the more the 
snippets which contain the translation equivalent 
are obtained, the higher the precision is. There 
are two factors to be considered. The first is how 
efficient the added English words can improve 
the precision. The second is how to avoid adding 
wrong translations which may bring down the 
precision. The first factor means that we should 
select the most informative words in the Chinese 
ON. The second factor means that we should 
consider the confidence of the SMT model at the 
same time. For example: 
??/LC  ??/NC 
?
?? /MC ????/KC 
(Tianjin   Honda     motor           co. ltd.) 
There are three strategies of constructing 
queries as follows: 
Q1.?????????????  Honda 
Q2.?????????????  Ltd. 
Q3.???????????? ? Motor 
Tianjin 
In the first strategy, we translate the word ??
?(Honda)? which is the most informative word 
in the ON. But its translation confidence is very 
low, which means that the statistical model gives 
wrong results usually. The mistakes in translation 
will mislead the search engine. In the second 
strategy, we translate the word which has the 
largest translation confidence. Unfortunately the 
word is so common that it can?t give any help in 
filtering out useless web pages. In the third 
strategy, the words which have sufficient 
translation confidence and information content 
are selected.  
5.3 Heuristically Selecting the Words to be 
Translated 
The mutual information is used to evaluate the 
importance of the words in a Chinese ON. We 
calculate the mutual information on the 
granularity of words in formula 1 and chunks in 
formula 2. The integration of the two kinds of 
mutual information is in formula 3. 
y Y
p ( x ,y )( , ) = lo g
p ( x ) p ( y )M I W x Y ??
     (1) 
Y
p ( y , c )( , ) = lo g
p ( y ) p ( c )y
M I C c Y
?
?       (2) 
( , )= ( , )+(1- ) ( , )xIC x Y MIW x Y MIC c Y? ?     (3) 
Here, MIW(x,Y) denotes the mutual 
information of word x with ON Y. That is the 
summation of the mutual information of x with 
every word in Y. MIC(c,Y) is similar. cx denotes 
the label of the chunk containing x. 
We should also consider the risk of obtaining 
wrong translation results. We can see that the 
name chunk usually has the largest mutual 
information. However, the name chunk always 
needs to be transliterated, and transliteration is 
often more difficult than translation by lexicon. 
So we set a threshold Tc for translation 
confidence. We only select the words whose 
translation confidences are higher than Tc, with 
their mutual information from high to low. 
390
6 Asymmetric Alignment Method for 
Equivalent Extraction 
After we have obtained the web pages with the 
assistant of search engine, we extract the 
equivalent candidates from the bilingual web 
pages. So we first extract the pure English 
sentences and then an asymmetric alignment 
method is executed to find the best fragment of 
the English sentences as the equivalent candidate. 
6.1 Traditional Alignment Method 
To find the translation candidates, the traditional 
method has three main steps.  
1) The NEs in the source and the target 
language sentences are extracted separately. The 
NE collections are Sne and Tne. 
2) For each NE in Sne, calculate the alignment 
probability with every NE in Tne. 
3) For each NE in Sne, the NE in Tne which has 
the highest alignment probability will be selected 
as its translation equivalent. 
This method has two main shortcomings: 
1) Traditional alignment method needs the 
NER process in both sides, but the NER process 
may often bring in some mistakes. 
2) Traditional alignment method evaluates the 
alignment probability coarsely. In other words, 
we don?t know exactly which target word(s) 
should be aligned to for the source word. A 
coarse alignment method may have negative 
effect on translation equivalent extraction.                                                                                                                                                    
6.2 The Asymmetric Alignment Method 
To solve the above two problems, we propose an 
asymmetric alignment method. The alignment 
method is so called ?asymmetric? for that it 
aligns a phrase with a sentence, in other words, 
the alignment is conducted between two objects 
with different granularities. The NER process is 
not necessary for that we align the Chinese ON 
with English sentences directly.  
[Wai Lam et al, 2007] proposed a method 
which uses the KM algorithm to find the optimal 
explicit matching between a Chinese ON and a 
given English ON. KM algorithm [Kuhn, 1955] 
is a traditional graphic algorithm for finding the 
maximum matching in bipartite weighted graph. 
In this paper, the KM algorithm is extended to be 
an asymmetric alignment method. So we can 
obtain an explicit matching between a Chinese 
ON and a fragment of English sentence. 
A Chinese NE CO={CW1, CW2, ?, CWn} is a 
sequence of Chinese words CWi and the English 
sentence ES={EW1, EW2, ?, EWm} is a sequence 
of English words EWi. Our goal is to find a 
fragment EWi,i+n={EWi, ?, EWi+n} in ES, which 
has the highest alignment score with CO. 
Through executing the extended KM algorithm, 
we can obtain an explicit matching L. For any 
CWi, we can get its corresponding English word 
EWj, written as L(CWi)=EWj and vice versa. We 
find the optimal matching L between two phrases, 
and calculate the alignment score based on L. An 
example of the asymmetric alignment will be 
given in Fig2. 
 
Fig2. An example of asymmetric alignment 
In Fig2, the Chinese ON ???????? is 
aligned to an English sentence ?? the 
Agriculture Bank of China is the four??. The 
stop words in parentheses are deleted for they 
have no meaning in Chinese. In step 1, the 
English fragment contained in the square 
brackets is aligned with the Chinese ON. We can 
obtain an explicit matching L1, shown by arrows, 
and an alignment score. In step 2, the square 
brackets move right by one word, we can obtain a 
new matching L2 and its corresponding alignment 
score, and so on. When we have calculated every 
consequent fragment in English sentence, we can 
find the best fragment ?the Agriculture Bank of 
China? according to the alignment score as the 
translation equivalent.  
The algorithm is shown in Fig3. Where, m is 
the number of words in an English sentence and 
n is the number of words in a Chinese ON. KM 
algorithm will generate an equivalent sub-graph 
by setting a value to each vertex. The edge whose 
weight is equal to the summation of the values of 
its two vertexes will be added into the sub-graph. 
Then the Hungary algorithm will be executed in 
the equivalent sub-graph to find the optimal 
matching. We find the optimal matching between 
CW1,n and EW1,n first. Then we move the window 
right and find the optimal matching between 
CW1,n and EW2,n+1. The process will continue 
until the window arrives at the right most of the 
? [(The) Agriculture Bank (of) China] (is) (the) four 
??    ??      ?? 
 (The) Agriculture [Bank (of) China] (is) (the) four]? 
??    ??      ?? 
Step 1: 
Step 2: 
391
English sentence. When the window moves right, 
we only need to find a new matching for the new 
added English vertex EWend and the Chinese 
vertex Cdrop which has been matched with EWstart 
in the last step. In the Hungary algorithm, the 
matching is added through finding an augmenting 
path. So we only need to find one augmenting 
path each time. The time complexity of finding 
an augmenting path is O(n3). So the whole 
complexity of asymmetric alignment is O(m*n3). 
Algorithm: Asymmetric Alignment Algorithm 
Input: A segmented Chinese ON CO and an 
English sentence ES. 
Output: an English fragment EWk,k+n 
1. Let start=1, end=n, L0=null 
2. Using KM algorithm to find the optimal 
matching between two phrases CW1,n and 
EWstart,end based on the previous matching Lstart-
1. We obtain a matching Lstart and calculate the 
alignment score Sstart based on Lstart. 
3. CWdrop = L(EWstart)  L(CWdrop)=null. 
4. If (end==m) go to 7, else start=start+1, 
end=end+1. 
5. Calculate the feasible vertex labeling for the 
vertexes CWdrop and EWend 
6. Go to 2. 
7. The fragment EWk,k+n-1 which has the highest 
alignment score will be returned. 
Fig3. The asymmetric alignment algorithm 
6.3 Obtain the Translation Equivalent 
For each English sentence, we can obtain a 
fragment ESi,i+n which has the highest alignment 
score. We will also take into consideration the 
frequency information of the fragment and its 
distance away from the Chinese ON. We use 
formula (4) to obtain a final score for each 
translation candidate ETi and select the largest 
one as translation result.  
( )= + log( +1)+ log(1 / +1)i i i iS ET SA C D? ? ?  (4) 
Where Ci denotes the frequency of ETi, and Di 
denotes the nearest distance between ETi and the 
Chinese ON. 
7 Experiments 
We carried out experiments to investigate the 
performance improvement of ON translation 
under the assistance of web knowledge.  
7.1 Experimental Data 
Our experiment data are extracted from 
LDC2005T34. There are two corpora, 
ldc_propernames_org_ce_v1.beta (Indus_corpus 
for short) and ldc_propernames_indu 
stry_ce_v1.beta (Org_corpus for short). Some 
pre-process will be executed to filter out some 
noisy translation pairs. For example, the 
translation pairs involving other languages such 
as Japanese and Korean will be filtered out. 
There are 65,835 translation pairs that we used as 
the training corpus and the chunk labels are 
added manually. 
We randomly select 250 translation pairs from  
the Org_corpus and 253 translation pairs from 
the Indus_corpus. Altogether, there are 503 
translation pairs as the testing set. 
7.2 The Effect of Chunking-based 
Segmentation upon ON Translation  
In order to evaluate the influence of segmentation 
results upon the statistical ON translation system, 
we compare the results of two translation models. 
One model uses chunking-based segmentation 
results as input, while the other uses traditional 
segmentation results. 
To train the CRFs-chunking model, we 
randomly selected 59,200 pairs of equivalent 
translations from Indus_corpus and org_corpus. 
We tested the performance on the set which 
contains 6,635 Chinese ONs and the results are 
shown as Table-2. 
For constructing a statistical ON translation 
model, we use GIZA++1 to align the Chinese NEs 
and the English NEs in the training set. Then the 
phrase-based machine translation system 
MOSES2 is adopted to translate the 503 Chinese 
NEs in testing set into English. 
 Precision Recall F-measure 
LC 0.8083 0.7973 0.8028 
NC 0.8962 0.8747 0.8853 
MC 0.9104 0.9073 0.9088 
KC 0.9844 0.9821 0.9833 
All 0.9437 0.9372 0.9404 
Table 2. The test results of CRFs-chunking model 
We have two metrics to evaluate the 
translation results. The first metric L1 is used to 
evaluate whether the translation result is exactly 
the same as the answer. The second metric L2 is 
used to evaluate whether the translation result 
contains almost the same words as the answer, 
                                                          
1
 http://www.fjoch.com/GIZA++.html 
2
 http://www.statmt.org/moses/ 
392
without considering the order of words. The 
results are shown in Table-3: 
 chunking-based 
segmentation  
traditional 
segmentation 
L1 21.47% 18.29% 
L2 40.76% 36.78% 
Table 3. Comparison of segmentation influence 
From the above experimental data, we can see 
that the chunking-based segmentation improves 
L1 precision from 18.29% to 21.47% and L2 
precision from 36.78% to 40.76% in comparison 
with the traditional segmentation method. 
Because the segmentation results will be used in 
alignment, the errors will affect the computation 
of alignment probability. The chunking based 
segmentation can generate better segmentation 
results; therefore better alignment probabilities 
can be obtained.  
7.3 The Efficiency of Query Construction 
The heuristic query construction method aims to 
improve the efficiency of Web searching. The 
performance of searching for translation 
equivalents mostly depends on how to construct 
the query. To test its validity, we design four 
kinds of queries and evaluate their ability using 
the metric of average precision in formula 5 and 
macro average precision (MAP) in formula 6, 
1
1P r
N
i
i i
HA vera g e ec is io n
N S
=
= ?             (5) 
where Hi is the count of snippets that contain at 
least one equivalent for the ith query. And Si is 
the total number of snippets we got for the ith 
query, 
1= 1
1
( )
1 j
i
HN
j j
i
M A P
R iN H
=
= ??               (6) 
where R(i) is the order of snippet where the ith 
equivalent occurs. We construct four kinds of 
queries for the 503 Chinese ONs in testing set as 
follows: 
Q1: only the Chinese ON.  
Q2: the Chinese ON and the results of the 
statistical translation model.  
Q3: the Chinese ON and some parts? 
translation selected by the heuristic query 
construction method.  
Q4: the Chinese ON and its correct English 
translation equivalent.  
We obtain at most 100 snippets from Google 
for every query. Sometimes there are not enough 
snippets as we expect. We set ? in formula 4 at 
0.7?and the threshold of translation confidence 
at 0.05. The results are shown as Table 4.  
 Average 
precision 
MAP 
Q1 0.031 0.0527 
Q2 0.187 0.2061 
Q3 0.265 0.3129 
Q4 1.000 1.0000 
Table 4. Comparison of four types query 
Here we can see that, the result of Q4 is the 
upper bound of the performance, and the Q1 is 
the lower bound of the performance. We 
concentrate on the comparison between Q2 and 
Q3. Q2 contains the translations of every word in 
a Chinese ON, while Q3 just contains the 
translations of the words we select using the 
heuristic method. Q2 may give more information 
to search engine about which web pages we 
expect to obtain, but it also brings in translation 
mistakes that may mislead the search engine. The 
results show that Q3 is better than Q2, which 
proves that a careful clue selection is needed. 
7.4 The Effect of Asymmetric Alignment 
Algorithm 
The asymmetric alignment method can avoid the 
mistakes made in the NER process and give an 
explicit alignment matching. We will compare 
the asymmetric alignment algorithm with the 
traditional alignment method on performance. 
We adopt two methods to align the Chinese NE 
with the English sentences. The first method has 
two phases, the English ONs are extracted from 
English sentences firstly, and then the English 
ONs are aligned with the Chinese ON. Lastly, the 
English ON with the highest alignment score will 
be selected as the translation equivalent. We use 
the software Lingpipe3 to recognize NEs in the 
English sentences. The alignment probability can 
be calculated as formula 7: 
( , ) ( | )i j
i j
Score C E p e c= ??       (7) 
The second method is our asymmetric 
alignment algorithm. Our method is different 
from the one in [Wai Lam et al, 2007] which 
segmented a Chinese ON using an English ON as 
suggestion. We segment the Chinese ON using 
the chunking-based segmentation method. The 
English sentences extracted from snippets will be 
preprocessed. Some stop words will be deleted, 
such as ?the?, ?of?, ?on? etc. To execute the 
extended KM algorithm for finding the best 
alignment matching, we must assure that the 
vertex number in each side of the bipartite is the 
                                                          
3
 http://www.alias-i.com/lingpipe/ 
393
same. So we will execute a phrase combination 
process before alignment, which combines some 
frequently occurring consequent English words 
into single vertex, such as ?limited company? etc. 
The combination is based on the phrase pair table 
which is generated from phrase-based SMT 
system. The results are shown in Table 5: 
 Asymmetric 
Alignment 
Traditional 
method 
Statistical 
model 
Top1 48.71% 36.18% 18.29% 
Top5 53.68% 46.12% -- 
Table 5. Comparison the precision of alignment 
method 
From the results (column 1 and column 2) we 
can see that, the Asymmetric alignment method 
outperforms the traditional alignment method. 
Our method can overcome the mistakes 
introduced in the NER process. On the other 
hand, in our asymmetric alignment method, there 
are two main reasons which may result in 
mistakes, one is that the correct equivalent 
doesn?t occur in the snippet; the other is that 
some English ONs can?t be aligned to the 
Chinese ON word by word.  
7.5 Comparison between Statistical ON 
Translation Model and Our Method 
Compared with the statistical ON translation 
model, we can see that the performance is 
improved from 18.29% to 48.71% (the bold data 
shown in column 1 and column 3 of Table 5) by 
using our Chinese-English ON translation system. 
Transforming the translation problem into the 
problem of searching for the correct translation 
equivalent in web pages has three advantages. 
First, word order determination is difficult in 
statistical machine translation (SMT), while 
search engines are insensitive to this problem. 
Second, SMT often loses some function word 
such as ?the?, ?a?, ?of?, etc, while our method 
can avoid this problem because such words are 
stop words in search engines. Third, SMT often 
makes mistakes in the selection of synonyms. 
This problem can be solved by the fuzzy 
matching of search engines. In summary, web 
assistant method makes Chinese ON translation 
easier than traditional SMT method.  
8 Conclusion 
In this paper, we present a new approach which 
translates the Chinese ON into English with the 
assistance of web resources. We first adopt the 
chunking-based segmentation method to improve 
the ON segmentation. Then a heuristic query 
construction method is employed to construct a 
query which can search translation equivalent 
more efficiently. At last, the asymmetric 
alignment method aligns the Chinese ON with 
English sentences directly. The performance of 
ON translation is improved from 18.29% to 
48.71%. It proves that our system can work well 
on the Chinese-English ON translation task. In 
the future, we will try to apply this method in 
mining the NE translation equivalents from 
monolingual web pages. In addition, the 
asymmetric alignment algorithm also has some 
space to be improved. 
Acknowledgement 
The work is supported by the National High 
Technology Development 863 Program of China 
under Grants no. 2006AA01Z144, and the 
National Natural Science Foundation of China 
under Grants no. 60673042 and 60875041. 
 
 
References  
Yaser Al-Onaizan and Kevin Knight. 2002. 
Translating named entities using monolingual and 
bilingual resources. In Proc of ACL-2002.  
Yufeng Chen, Chenqing Zong. 2007. A Structure-
Based Model for Chinese Organization Name 
Translation. In Proc. of ACM Transactions on 
Asian Language Information Processing (TALIP) 
Donghui Feng, Yajuan Lv, Ming Zhou. 2004. A new 
approach for English-Chinese named entity 
alignment. In Proc. of  EMNLP 2004. 
Fei Huang, Stephan Vogal. 2002. Improved named 
entity translation and bilingual named entity 
extraction. In Proc. of the 4th IEEE International 
Conference on Multimodal Interface. 
Fei Huang, Stephan Vogal, Alex Waibel. 2003. 
Automatic extraction of named entity translingual 
equivalence based on multi-feature cost 
minimization. In Proc. of the 2003 Annual 
Conference of the ACL, Workshop on Multilingual 
and Mixed-language Named Entity Recognition 
Masaaki Nagata, Teruka Saito, and Kenji Suzuki. 
2001. Using the Web as a Bilingual Dictionary. In 
Proc. of ACL 2001 Workshop on Data-driven 
Methods in Machine Translation. 
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Proc. of 
ACL 2005. 
Conrad Chen, Hsin-His Chen. 2006. A High-Accurate 
Chinese-English NE Backward Translation System 
Combining Both Lexical Information and Web 
Statistics. In Proc. of ACL 2006. 
394
Wai Lam,  Shing-Kit Chan. 2007. Named Entity 
Translation Matching and Learning: With 
Application for Mining Unseen Translations. In 
Proc. of ACM Transactions on Information 
Systems.  
Chun-Jen Lee, Jason S. Chang, Jyh-Shing R. Jang. 
2006. Alignment of bilingual named entities in 
parallel corpora using statistical models and 
multiple knowledge sources. In Proc. of ACM 
Transactions on Asian Language Information 
Processing (TALIP). 
Kuhn, H. 1955. The Hungarian method for the 
assignment problem. Naval Rese. Logist. Quart 
2,83-97. 
Min Zhang., Haizhou Li, Su Jian, Hendra Setiawan. 
2005. A phrase-based context-dependent joint 
probability model for named entity translation. In 
Proc. of the 2nd International Joint Conference on 
Natural Language Processing(IJCNLP)  
Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining 
translations of OOV terms from the web through 
cross-lingual query expansion. In Proc. of the 28th 
ACM SIGIR. 
Bonnie Glover Stalls and Kevin Knight. 1998. 
Translating names and technical terms in Arabic 
text. In Proc. of the COLING/ACL Workshop on 
Computational Approaches to Semitic Language. 
J. Lafferty, A. McCallum, and F. Pereira. 2001.  
Conditional random fields: Probabilistic models for 
segmenting and labeling sequence data. In Proc. 
ICML-2001. 
Tadashi Kumano, Hideki Kashioka, Hideki Tanaka 
and Takahiro Fukusima. 2004. Acquiring bilingual 
named entity translations from content-aligned 
corpora. In Proc. IJCNLP-04. 
Robert C. Moore. 2003. Learning translation of 
named-entity phrases from parallel corpora. In Proc. 
of 10th conference of the European chapter of ACL.  
 
 
 
 
395
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 677?687, Dublin, Ireland, August 23-29 2014.
Joint Opinion Relation Detection Using One-Class Deep Neural Network
Liheng Xu, Kang Liu and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{lhxu, kliu, jzhao}@nlpr.ia.ac.cn
Abstract
Detecting opinion relation is a crucial step for fine-gained opinion summarization. A valid opin-
ion relation has three requirements: a correct opinion word, a correct opinion target and the
linking relation between them. Previous works prone to only verifying two of these requirements
for opinion extraction, while leave the other requirement unverified. This could inevitably intro-
duce noise terms. To tackle this problem, this paper proposes a joint approach, where all three
requirements are simultaneously verified by a deep neural network in a classification scenario.
Some seeds are provided as positive labeled data for the classifier. However, negative labeled
data are hard to acquire for this task. We consequently introduce one-class classification problem
and develop a One-Class Deep Neural Network. Experimental results show that the proposed
joint approach significantly outperforms state-of-the-art weakly supervised methods.
1 Introduction
Opinion summarization aims to extract and summarize customers? opinions from reviews on products or
services (Hu and Liu, 2004; Cardie et al., 2004). With the rapid expansion of e-commerce, the number of
online reviews is growing at a high speed, which makes it impractical for customers to read throughout
large amounts of reviews to choose better products. Therefore, it is imperative to automatically gener-
ate opinion summarization to help customers make more informed purchase decisions, where detecting
opinion relation is a crucial step for opinion summarization.
Before going further, we first introduce some notions. An opinion relation, is a triple o = (s, t, r),
where three factors are involved: s is an opinion word which refers to those words indicating sentiment
polarities; t is an opinion target, which can be any entity or aspect of an entity about which an opinion has
been expressed; r refers to the linking relation between s and t. As in Example 1, s={clear}, t={sceen},
and there is a linking relation between the two words because clear is used to modify screen.
Example 1. This mp3 has a clear screen.
For a valid opinion relation, there are three requirements corresponding to the three factors: (i) the
opinion word indicates sentiment polarity; (ii) the opinion target is related to current domain; (iii) the
opinion word modifies the opinion target. Previous weakly supervised methods often expand a seed set
and identify opinion relation either by co-occurrence statistics (Hu and Liu, 2004; Hai et al., 2012) or
syntactic dependencies (Popescu and Etzioni, 2005; Qiu et al., 2009) following the assumption below.
Assumption 1. Terms that are likely to have linking relation with the seed terms are believed to be
opinion words or opinion targets.
For example, if one has an opinion word seed clear (which satisfies requirement i), and one finds that
it modifies the word screen in Example 1 (which satisfies requirement iii). Then one infers that screen
is an opinion target according to Assumption 1 (whether screen is correct is not checked). However, in
Example 2(a), we can see that good is an opinion word and it modifies thing, but thing is not related to
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
677
mp3 domain. If one follows Assumption 1, thing will be mistaken as an opinion target. Similarly, in
Example 2(b), if one uses mp3 to extract another as an opinion word, he may get an objective word.
Example 2. (a) This mp3 has many good things. (b) Just another mp3 I bought.
The reason for the errors above is that Assumption 1 only verifies two requirements for an opinion
relation. Unfortunately, this issue occurs frequently in online reviews. As a result, previous methods
often suffer from these noise terms. To produce more precise opinion summary, it is argued that we shall
follow a more restricted assumption as follows.
Assumption 2. The three requirements: the opinion word, the opinion target and the linking relation
between them, shall be all verified during opinion relation detection.
To make accordance with Assumption 2, this paper proposes a novel joint opinion relation detection
method, where opinion words, opinion targets and linking relations are simultaneously considered in a
classification scenario. Following previous works, we provide a small set of seeds (i.e. opinion words
or targets) for supervision, which are regarded as positive labeled examples for classification. However,
negative labeled examples (i.e. noise terms) are hard to acquire, because we do not know which term
is not an opinion word or target. This leads to One-Class Classification (OCC) problem (Moya et al.,
1993). The key to OCC is semantic similarity measuring between terms, and Deep Neural Network
(DNN) with word embeddings is a powerful tool for handling this problem. We consequently inte-
grate DNN into a OCC classifier and develop a One-Class Deep Neural Network (OCDNN). Concretely,
opinion words/targets/relations are first represented by embedding vectors and then jointly classified.
Experimental results show that the proposed joint method which follows Assumption 2 significantly
outperforms state-of-the-art weakly supervised methods which are based on Assumption 1.
2 Related Work
In opinion relation detection task, previous works often used co-occurrence statistics or syntax informa-
tion to identify opinion relations. For co-occurrence statistical methods, Hu and Liu (2004) proposed a
pioneer research for opinion summarization based on association rules. Popescu and Etzioni (2005) de-
fined some syntactic patterns and used Pointwise Mutual Information (PMI) to extract product features.
Hai et al. (2012) proposed an opinion feature mining method which employed Likelihood Ratio Tests
(LRT) (Dunning, 1993) as the co-occurrence statistical measure. For syntax-based approaches, Riloff
and Wiebe (2003) performed syntactic pattern learning while extracting subjective expressions. Zhuang
et al. (2006) used various syntactic templates from an annotated movie corpus and applied them to su-
pervised movie feature extraction. Kobayashi et al. (2007) identified opinion relations by searching for
useful syntactic contextual clues. Qiu et al. (2009) proposed a bootstrapping framework called Double
Propagation which introduced eight heuristic syntactic rules to detect opinion relations.
However, none of the above methods could verify opinion words/targets/relations simultaneously dur-
ing opinion relation detection. To perform joint extraction, various models had been proposed, most of
which employed classification or sequence labeling models, such as HMM (Jin and Ho, 2009), SVM
(Wu et al., 2009) and CRFs (Breck et al., 2007; Jakob and Gurevych, 2010; Li et al., 2010). Besides, op-
timal models such as Integer Linear Programming (ILP) were also employed to perform joint inference
for opinion extraction (Choi et al., 2006; Yang and Cardie, 2013).
Joint methods had been shown to achieve better performance than pipeline approaches. Nevertheless,
most existing joint models rely on full supervision, which have the difficulty of obtaining annotated
training data in practical applications. Also, supervised models that are trained on one domain often fail
to give satisfactory results when shifted to another domain. Our method does not require annotated data.
3 The Proposed Method
To detect opinion relations, previous methods often leverage some seed terms, such as opinion word
seeds (Hu and Liu, 2004; Baccianella et al., 2010) and opinion target seeds (Jijkoun et al., 2010; Hai
et al., 2012). These seeds can be used as positive labeled examples to train a classifier. However, it is
hard to get negative labeled examples for this task. Because opinion words or targets are often domain
678
dependent and words that do not bear any sentiment polarity in one domain may be used to express
opinion in another domain. It is also very hard to specify in what case there is no linking relation
between two words.
To deal with this problem, we employ one-class classification, and develop a One-Class Deep Neural
Network (OCDNN) for opinion relation detection. The architecture of OCDNN is shown in Figure 1,
which consists of two levels. The lower level learns feature representations unsupervisedly for opinion
words/targets/relations, where the left component uses word embedding learning to represent opinion
words/targets, and the right component maps linking relations to embedding vectors by a recursive au-
toencoder. Then the upper level uses the learnt features to perform one-class classification.
 
Figure 1: The architecture of OCDNN.
 
Figure 2: An example of recursive autoencoder.
3.1 Opinion Seed Generation
To obtain training data for OCDNN, we shall first get some seed terms as follows.
Opinion Word Seeds. We manually pick 186 domain independent opinion words from SentiWordNet
(Baccianella et al., 2010) as the opinion word seed set SS.
Opinion Target Seeds. Likelihood Ratio Tests (LRT) (Dunning, 1993) used in (Hai et al., 2012) is
employed to generate opinion target seeds. LRT aims to measure how greatly two terms T
i
and T
j
are
associated with each other by sentence-level corpus statistics which is defined as follows,
LRT = 2[logL(p
1
, k
1
, n
1
) + logL(p
2
, k
2
, n
2
)? logL(p, k
1
, n
1
)? logL(p, k
2
, n
2
)] (1)
where k
1
= tf(T
i
, T
j
), k
2
= tf(T
i
,
?
T
j
), k
3
= tf(
?
T
i
, T
j
), k
4
= tf(
?
T
i
,
?
T
j
), tf(?) denotes term frequency;
L(p, k, n) = p
k
(1 ? p)
n?k
, n
1
= k
1
+ k
3
, n
2
= k
2
+ k
4
, p
1
= k
1
/n
1
, p
2
= k
2
/n
2
and p =
(k
1
+ k
2
)/(n
1
+ n
2
). We measure LRT between a domain name (e.g. mp3, hotel, etc.) and all opinion
target candidates. Then N terms with highest LRT scores are added into the opinion target seed set TS.
Linking Relation Seeds. Linking relation can be naturally captured by syntactic dependency, because
it directly models the modification relation between opinion word and opinion target. We employ an
automatic syntactic opinion pattern learning method called Sentiment Graph Walking (Xu et al., 2013)
and get 12 opinion patterns with highest confidence as the linking relation seed set RS.
After seed generation, every opinion relation s
o
= (s
s
, s
t
, s
r
) in review corpus that satisfies s
s
? SS,
s
t
? TS and s
r
? RS is taken as a positive labeled training instance.
3.2 Opinion Relation Candidate Generation
The opinion term candidate set is denoted by C = {SC, TC}, where SC/TC represents opinion
word/target candidate. Following previous works (Hu and Liu, 2004; Popescu and Etzioni, 2005; Qiu
et al., 2009), we take adjectives or verbs as opinion word candidates, and take nouns or noun phrases as
opinion target candidates. A statistic-based method in Zhu et al. (2009) is used to detect noun phrases.
An opinion relation candidate is denoted by c
o
= (c
s
, c
t
, c
r
), where c
s
? SC, c
t
? TC, and c
r
is a
potential linking relation. To get c
r
, we first get dependency tree of a sentence using Stanford Parser (de
679
Marneffe et al., 2006). Then, the shortest dependency path between a c
s
and a c
t
is taken as a c
r
. To
avoid introducing too many noise candidates, we constrain that there are at most four terms in a c
r
.
3.3 Word Representation by Word Embedding Learning
Word embedding, a.k.a word representation, is a mathematical object associated with each word, which
is often used in a vector form, where each dimension?s value corresponds to a feature and might even
have a semantic or grammatical interpretation (Turian et al., 2010). By word embedding learning, words
are embedded into a hyperspace, where two words that are more semantically similar to each other are
located closer. This characteristic is precisely what we want, because the key to one-class classification
is semantic similarity measuring (illustrated in Section 3.5).
For word representation, we use a matrix LT ? R
n?|V
w
|
, where i-th column represents the embedding
vector for term t
i
, n is the size of embedding vector and V
w
is the vocabulary of LT . Therefore, we
can denote t
i
by a binary vector b
i
? R
|V
w
|
and get its embedding vector by x
i
= LTb
i
. The training
criterion for word embeddings is,
?
? = argmin
?
?
c?C
?
v?V
w
max{0, 1? s
?
(c) + s
?
(v)} (2)
where ? is the parameters of neural network used for training. See Collobert et al. (2011) for the detailed
implementation.
3.4 Linking Relation Representation by Using Recursive Autoencoder
The goal of this section is to represent the linking relation between an opinion word and an opinion target
by a n-element vector as we do during word representation. Specifically, we combine embedding vectors
of words in a linking relation by a recursive autoencoder (Socher et al., 2011) according to syntactic
dependency structure. In this way, linking relations are no longer limited to the initial seeds during
classification, because linking relations that are similar to the seed relations will have similar vector
representations.
Figure 2 shows a linking relation representation process by an example: too loud to listen to the player.
First, we get its dependency path between the opinion word c
s
:loud and the opinion target c
t
:player.
Then c
s
and c
t
are replaced by wildcards [SC] and [TC] because they are not concerned in the linking
relation. The dash line box in Figure 2 shows a standard autoencoder, which is a three-layer neural
network, where the number of nodes in input layer is equal to that of output layer. It takes two n-element
vectors as input and compresses semantics of the two vectors into one n-element vector in hidden layer
by,
y = f(W
(dep)
[x
1
;x
2
] + b), W
(dep)
=
1
2
[I
1
; I
2
; I
b
] +  (3)
where [x
1
;x
2
] is the concatenation of the two input vectors and f is the sigmoid function; W
(dep)
is a
parameter matrix that is chosen according to the dependency relation between x
1
and x
2
(In the case of
y
1
, W
(dep)
= W
(xcomp)
), which is initialized by I
i
, where I
i
is a n ? n unit matrix, I
b
is a n-element
null vector, and  is sampled from a uniform distribution U [?0.001, 0.001] (Socher et al., 2013). Then
W
(dep)
are updated during training. The training criterion of autoencoder is to minimize Euclidean
distance between the original input and its output,
E
rae
= ||[x
1
;x
2
]? [x
?
1
;x
?
2
]||
2
(4)
where [x
?
1
;x
?
2
] = W
(out)
y and W
(out)
is initialized by W
(dep)
T
.
We always start the combination process from [SC] and it is repeated along the dependency path. For
example, the result vector y
1
of the first combination is used as the input vector when computing y
2
.
Finally, the linking relation is represented by a n-element vector (the green vector in Figure 2).
680
3.5 One-Class Classification for Opinion Relation Detection
We represent an opinion relation candidate c
o
= (c
s
, c
t
, c
r
) by a vector v
o
= [v
s
; v
t
; v
r
], which is
a concatenation of the opinion word embedding v
s
, the opinion target embedding v
t
and the linking
relation embedding v
r
. Then v
o
is feed to the upper level autoencoder in Figure 1.
To perform one-class classification, the number of nodes in the hidden layer of the upper level autoen-
coder is constrained to be smaller than that of the input layer. By using such a ?bottleneck? network
structure, characteristics of the input are first compressed into the hidden layer and then reconstructed
by the output layer (Japkowicz et al., 1995). Concretely, characteristics of positive labeled opinion rela-
tions are first compressed into the hidden layer, and then the autoencoder should be able to adequately
reconstruct positive instances in the output layer, but should fail to reconstruct negative instances which
present different characteristics from positive instances. Therefore, the detection of opinion relation is
equivalent to assessing how well a candidate is reconstructed by the autoencoder. As the input vector
v
o
consists of representations for opinion words/targets/relations, characteristics of the three factors are
jointly compressed by one hidden layer. Either false opinion word/target/relation will lead to failure of
reconstruction. Consequently, our approach follows Assumption 2.
For opinion relation detection, candidates with reconstruction error scores that are smaller than a
threshold ? are classified as positive. Determining the exact value of ? is very difficult. Inspired by other
one-class approaches (Liu et al., 2002; Manevitz and Yousef, 2007), we introduce some negative opinion
terms to help to estimate ?.
1
Although negative instances are hard to acquire, Xu et al. (2013) show that
a set of general nouns (such as thing, one, etc., we denote them by GN ) seldom appear to be opinion
targets. One the other hand, we create a 50-opinion-word validation set SV from SentiWordNet.
To estimate ?, we first introduce a positive proportion (pp) score,
pp(t) = tf
+
(t)/tf(t), t ? PE, PE = {c
o
|E
r
(c
o
) < ?} (5)
where PE denotes the opinion relations that are classified as positive, E
r
(?) is the reconstruction error
of OCDNN and tf
+
(?) is the frequency of term in PE. Then an error function E
?
is minimized, which
balances between the proportion of non-target terms (GN ) in PE (which shall be as small as possible)
and the proportion of opinion words in validation set (SV ) in PE (which shall be as large as possible).
E
?
=
?
t?GN?PE
[pp(t)? 0]
2
+
?
s?SV ?PE
[pp(s)? 1]
2
(6)
3.6 Opinion Target Expansion
We apply bootstrapping to iteratively expand opinion target seeds. It is because the vocabulary of seed
set is limited, which cannot fully represent the distribution of opinion targets. So we expand opinion
target seeds in a self-training manner to alleviate this issue. After training OCDNN, all opinion relation
candidates are classified, and opinion targets are ranked in descent order by,
s(t) = log tf(t)? pp(t). (7)
Then, top M candidates are added into the target seed set TS for the next training iteration.
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets. Three real world datasets are selected for evaluation. The first one is called Customer Review
Dataset (CRD)
2
which contains reviews on five products (denoted by D1 to D5). The second is a bench-
mark dataset (Wang et al., 2011) on MP3 and Hotel
3
. The last one is crawled from www.amazon.com,
which involves Mattress and Phone. Two annotating criteria are applied.
1
This is not in contradiction with OCC problem, because these negative examples are NOT used during training.
2
http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html
3
http://timan.cs.uiuc.edu/downloads.html
681
Annotation 1 is used to evaluate opinion words/targets extraction. Firstly, 10,000 sentences are ran-
domly selected from reviews and all possible terms are extracted along with their contexts. Then, anno-
tators are required to judge whether each term is an opinion word or an opinion target.
Annotation 2 is used to evaluate intra-sentence opinion relation detection. Annotators are required to
carefully read through each sentence and find out every opinion relation, which consists of an opinion
word, an opinion target, as well as the linking relation between them. The annotation is very labor-
intensive, so only 5,000 sentences are annotated for MP3 and Hotel.
Two annotators were required to annotate following the criteria above. When conflicts happened, a
third annotator would make the final judgment. Note that Annotation 1 and Annotation 2 were annotated
by two different groups. Detailed information of the annotated datasets are shown in Table 1. Further-
more, the kappa values between Annotation 1 and Annotation 2 are 0.88 for opinion words and 0.84 for
opinion targets, showing highly substantial agreement.
Domain #OW #OT Kappa OW Kappa OT
Hotel 434 1,015 0.72 0.67
MP3 559 1,158 0.69 0.65
Mattress 366 523 0.67 0.62
Phone 391 862 0.68 0.64
(a) Annotation 1
Domain #LR #OW #OT Kappa LR
Hotel 2,196 317 735 0.62
MP3 2,328 342 791 0.61
(b) Annotation 2
Table 1: The detailed information of Annotations. OW/OT/LR stands for opinion words/opinion tar-
gets/linking relations. The Kappa-values are calculated by using exact matching metric for Annotation 1
and overlap matching metric for Annotation 2.
Evaluation Metrics. We perform evaluation in terms of Precision(P), Recall(R) and F-measure(F)
according to exact and overlap matching metrics (Wiebe et al., 2005). The exact metric is used to
evaluate opinion word/target extraction, which requires exact string match. And the overlap metric is
used to evaluate opinion relation detection, where an extracted opinion relation is regarded as correct
when both the opinion word and the opinion target in it overlap with the gold standard.
4
Evaluation Settings. Four state-of-the-art weakly supervised approaches are selected as competi-
tors. Two are co-occurrence statistical methods and two are syntax-based methods, all of which follow
Assumption 1.
AdjRule extracts opinion words/targets by using adjacency rules (Hu and Liu, 2004).
LRTBOOT is a bootstrapping algorithm which employs Likelihood Ratio Tests (Dunning, 1993) as
the co-occurrence statistical measure (Hai et al., 2012).
DP denotes the Double Propagation algorithm (Qiu et al., 2009).
DP-HITS is an enhanced version of DP proposed by Zhang et al. (2010), which ranks terms by
s(t) = log tf(t)? importance(t) (8)
where importance(t) is estimated by the HITS algorithm (Kleinberg, 1999).
OCDNN is the proposed method. The target seed size N = 40, the opinion targets expanded in each
iteration M = 20, and the max bootstrapping iteration number is X = 10. The representation learning
in lower level of OCDNN is trained on the whole corpus, while the test data are the same for all settings.
All results of OCDNN are taken by average performance over five runs with randomized parameters.
4.2 OCDNN vs. the State-of-the-art
We compare OCDNN with state-of-the-art methods for opinion words/targets extraction. In OCDNN,
Eq. 7 is used to rank opinion words/targets. The results on CRD and the four domains are shown in
Table 2 and Table 3. DP-HITS does not extract opinion words so their results for opinion words are not
taken into account.
4
Determining the exact boundaries of opinion terms is hard even for human (Wiebe et al., 2005), so we use this relaxation.
682
Opinion Targets
Method
D1 D2 D3 D4 D5
Avg.
P R F P R F P R F P R F P R F F
AdjRule 0.75 0.82 0.78 0.71 0.79 0.75 0.72 0.76 0.74 0.69 0.82 0.75 0.74 0.80 0.77 0.76
DP 0.87 0.81 0.84 0.90 0.81 0.85 0.90 0.86 0.88 0.81 0.84 0.82 0.92 0.86 0.89 0.86
DP-HITS 0.83 0.84 0.83 0.86 0.85 0.85 0.86 0.88 0.87 0.80 0.85 0.82 0.86 0.86 0.86 0.85
LRTBOOT 0.77 0.87 0.82 0.74 0.90 0.81 0.79 0.89 0.84 0.72 0.88 0.79 0.74 0.88 0.80 0.81
OCDNN 0.83 0.82 0.82 0.86 0.85 0.85 0.86 0.87 0.86 0.78 0.84 0.81 0.89 0.85 0.87 0.84
Opinion Words
AdjRule 0.57 0.75 0.65 0.51 0.76 0.61 0.57 0.73 0.64 0.54 0.62 0.58 0.62 0.67 0.64 0.62
DP 0.64 0.73 0.68 0.57 0.79 0.66 0.65 0.70 0.67 0.61 0.65 0.63 0.70 0.68 0.69 0.67
LRTBOOT 0.60 0.79 0.68 0.52 0.82 0.64 0.60 0.76 0.67 0.56 0.70 0.62 0.66 0.71 0.68 0.66
OCDNN 0.64 0.77 0.70 0.63 0.79 0.70 0.66 0.73 0.69 0.68 0.70 0.69 0.70 0.69 0.69 0.70
Table 2: Results of opinion terms extraction on Customer Review Dataset.
Opinion Targets
Method
MP3 Hotel Mattress Phone
Avg.
P R F P R F P R F P R F F
AdjRule 0.53 0.55 0.54 0.55 0.57 0.56 0.50 0.60 0.55 0.52 0.51 0.51 0.54
DP 0.66 0.57 0.61 0.66 0.60 0.63 0.55 0.60 0.57 0.60 0.53 0.56 0.59
DP-HITS 0.65 0.62 0.63 0.64 0.66 0.65 0.55 0.67 0.60 0.62 0.64 0.63 0.63
LRTBOOT 0.60 0.77 0.67 0.59 0.78 0.67 0.55 0.78 0.65 0.57 0.76 0.65 0.66
OCDNN 0.70 0.68 0.69 0.71 0.70 0.70 0.63 0.69 0.66 0.69 0.68 0.68 0.68
Opinion Words
AdjRule 0.48 0.65 0.55 0.51 0.68 0.58 0.51 0.68 0.58 0.48 0.61 0.54 0.56
DP 0.58 0.62 0.60 0.60 0.66 0.63 0.54 0.68 0.60 0.55 0.59 0.57 0.60
LRTBOOT 0.52 0.69 0.59 0.54 0.74 0.62 0.51 0.73 0.60 0.50 0.68 0.58 0.60
OCDNN 0.68 0.65 0.66 0.70 0.68 0.69 0.59 0.70 0.64 0.63 0.59 0.61 0.65
Table 3: Results of opinion terms extraction on the four domains.
From Table 2, we can see that our method outperforms co-occurrence-based methods AdjRule and
LRTBOOT, but achieves comparable or a little worse results than syntax-based methods DP and DP-
HITS. This is because CRD is quite small, which only contains several hundred sentences for each prod-
uct review set. In this case, methods based on careful-designed syntax rules have superiority over those
based on statistics (Liu et al., 2013). For results on larger datasets shown in Table 3, our method out-
performs all of the competitors. Comparing OCDNN with DP-HITS, the two approaches use similar
term ranking metrics (Eq. 7 and Eq. 8), but OCDNN significantly outperforms DP-HITS. Therefore, the
positive proportion score estimated by OCDNN is more effective than the importance score in DP-HITS.
Comparing OCDNN with LRTBOOT, we find that LRTBOOT achieves better recall but lower precision.
This is because LRTBOOT follows Assumption 1 during bootstrapping, which suffers a lot from error
propagation, while our joint classification approach effectively alleviates this issue. We will discuss the
impact of error propagation in detail later.
4.3 Assumption 1 vs. Assumption 2
This section evaluates intra-sentence opinion relation detection, which is more useful for practical appli-
cations. It also reflects the impacts of Assumption 1 and Assumption 2. The results are shown in Table
4 and Table 5, where OCDNN significantly outperforms all competitors. The average improvement of
F-measure over the best competitor is 6% on CRD and 9% on Hotel and MP3.
As Assumption 1 only verifies two of the requirements in an opinion relation, it would inevitably
introduce noise terms during extraction. For syntax-based method DP, it extracts many false opinion
relations such as good thing and nice one (where thing and one are false opinion targets) or objective
expressions like another mp3 and every mp3 (which contain false opinion words another and every). For
co-occurrence statistical methods AdjRule and LRTBOOT, it is very hard to deal with ambiguous linking
relations. For example, in phrase this mp3 is very good except the size, co-occurrence statistical methods
could hardly tell which opinion target does good modify (mp3 or size). Our method follows Assumption
683
Method
D1 D2 D3 D4 D5
Avg.
P R F P R F P R F P R F P R F F
AdjRule 0.51 0.66 0.58 0.53 0.63 0.58 0.50 0.61 0.55 0.48 0.60 0.53 0.50 0.61 0.55 0.56
DP 0.66 0.63 0.64 0.68 0.60 0.64 0.69 0.62 0.65 0.66 0.57 0.61 0.67 0.60 0.63 0.64
LRTBOOT 0.53 0.70 0.60 0.57 0.72 0.64 0.55 0.69 0.61 0.52 0.70 0.60 0.55 0.68 0.61 0.61
OCDNN 0.76 0.66 0.71 0.74 0.67 0.70 0.77 0.67 0.72 0.70 0.65 0.67 0.77 0.66 0.71 0.70
Table 4: Results of opinion relation detection on Customer Review Dataset.
Method
MP3 Hotel
Avg.
P R F P R F F
AdjRule 0.49 0.55 0.52 0.45 0.53 0.49 0.50
DP 0.63 0.51 0.56 0.59 0.50 0.54 0.55
LRTBOOT 0.54 0.63 0.58 0.50 0.60 0.55 0.56
OCDNN 0.73 0.60 0.66 0.70 0.59 0.64 0.65
Table 5: Results of opinion relation detection on the two domains.
2, which verifies all three requirements for opinion word/target/relation in an opinion relation, so the
above errors are greatly reduced. Therefore, Assumption 2 is more reasonable than Assumption 1.
4.4 The Effect of Joint Classification
We evaluate the three bootstrapping methods (DP, LRTBOOT and OCDNN) for opinion target expansion.
The precision of each iteration is shown in Figure 3. We can see that DP and LRTBOOT gradually suffer
from error propagation and the precision drops quickly along with the number of iteration increases. For
OCDNN, although error propagation is inevitable, the precision curve retains at a high level. Therefore,
the joint approach produces more precise results.
For more detailed analysis, we give a variation of the proposed method named 3NN, which uses
3 individual autoencoders to classify opinion words/targets/relations separately. An opinion relation
candidate is classified as positive only when the three factors are all classified as positive. Then opinion
relations are ranked by the sum of reconstruction scores of the three factors. In the results of opinion
relation detection, when the recall is fixed at 0.6, the precisions of 3NN are 0.67 for MP3 and 0.65
for Hotel, while the precisions of OCDNN are 0.73 for MP3 and 0.70 for Hotel. Therefore, OCDNN
achieves much better performance than 3NN.
An example may explain the reason of why 3NN gets worse performance. In our experiment on Hotel,
a false opinion relation happy day is misclassified as positive by 3NN. It is because the word day has
a small reconstruction score in 3NN. At the same time, happy is a correct opinion word, so the whole
expression happy day also has a small reconstruction score and then be misclassified. In contrast, the
reconstruction score of happy day from OCDNN is quite large so the phrase is dropped. The reason
is that the joint approach captures the semantic of a whole phrase rather than its single components.
Therefore, it is more reasonable.
 
1 2 3 4 5 6 7 8 9 10
.5
.6
.7
.8
.9
1.0
OCDNN
DP
LRTBOOT
(a) MP3
 
1 2 3 4 5 6 7 8 9 10
.5
.6
.7
.8
.9
1.0
OCDNN
DP
LRTBOOT
(b) Hotel
 
1 2 3 4 5 6 7 8 9 10
.5
.6
.7
.8
.9
1.0
OCDNN
DP
LRTBOOT
(c) Mattress
 
1 2 3 4 5 6 7 8 9 10
.5
.6
.7
.8
.9
1.0
OCDNN
DP
LRTBOOT
(d) Phone
Figure 3: Precision (y-axis) of opinion target seed expansion at each bootstrapping iteration (x-axis).
684
5 Conclusion and Future Work
This paper proposes One-Class Deep Neural Network for joint opinion relation detection in one-class
classification scenario, where opinion words/targets/relations are simultaneously verified during classifi-
cation. Experimental results show the proposed method significantly outperforms state-of-the-art weakly
supervised methods that only verify two factors in an opinion relation.
In future work, we plan to adapt our method and make it be capable of capturing implicit opinion
relations.
Acknowledgement
This work was sponsored by the National Natural Science Foundation of China (No. 61202329 and No.
61333018) and CCF-Tencent Open Research Fund.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical re-
source for sentiment analysis and opinion mining. Seventh conference on International Language Resources
and Evaluation, pages 2200?2204.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In Proceedings
of the 20th international joint conference on Artifical intelligence, IJCAI?07, pages 2683?2688, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Claire Cardie, Janyce Wiebe, Theresa Wilson, and Diane Litman. 2004. Low-level annotations and summary
representations of opinions for multi-perspective question answering.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recognition.
In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ?06,
pages 431?439, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493?2537,
November.
Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the IEEE / ACL 2006 Workshop on Spoken Language
Technology. The Stanford Natural Language Processing Group.
Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Comput. Linguist., 19(1):61?
74, March.
Zhen Hai, Kuiyu Chang, and Gao Cong. 2012. One seed to find them all: mining opinion features via association.
In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM
?12, pages 255?264, New York, NY, USA. ACM.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge discovery and data mining, pages 168?177.
Niklas Jakob and Iryna Gurevych. 2010. Extracting opinion targets in a single- and cross-domain setting with
conditional random fields. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 1035?1045, Stroudsburg, PA, USA. Association for Computational Linguistics.
Nathalie Japkowicz, Catherine Myers, and Mark Gluck. 1995. A novelty detection approach to classification. In
Proceedings of the 14th international joint conference on Artificial intelligence - Volume 1, IJCAI?95, pages
518?523, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Valentin Jijkoun, Maarten de Rijke, and Wouter Weerkamp. 2010. Generating focused topic-specific sentiment
lexicons. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL
?10, pages 585?594, Stroudsburg, PA, USA. Association for Computational Linguistics.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized hmm-based learning framework for web opinion mining. In
Proceedings of the 26th Annual International Conference on Machine Learning, ICML ?09, pages 465?472.
685
Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. J. ACM, 46(5):604?632, September.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1065?1074,
Prague, Czech Republic, June. Association for Computational Linguistics.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010. Structure-aware
review mining and summarization. In Proceedings of the 23rd International Conference on Computational
Linguistics, COLING ?10, pages 653?661, Stroudsburg, PA, USA. Association for Computational Linguistics.
Bing Liu, Wee Sun Lee, Philip S. Yu, and Xiaoli Li. 2002. Partially supervised classification of text documents.
In Proceedings of the Nineteenth International Conference on Machine Learning, ICML ?02, pages 387?394,
San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Kang Liu, Liheng Xu, and Jun Zhao. 2013. Syntactic patterns versus word alignment: Extracting opinion tar-
gets from online reviews. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1754?1763, August.
Larry Manevitz and Malik Yousef. 2007. One-class document classification via neural networks. Neurocomput-
ing, 70(7C9):1466?1481.
Mary M. Moya, Mark W. Koch, and Larry D. Hostetler. 1993. One-class classifier networks for target recognition
applications. In Proceedings world congress on neural networks, pages 797?801.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceed-
ings of the conference on Human Language Technology and Empirical Methods in Natural Language Process-
ing, HLT ?05, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI?09, pages
1199?1204.
Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceedings
of the 2003 conference on Empirical methods in natural language processing, EMNLP ?03, pages 105?112,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-
supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing, EMNLP ?11, pages 151?161, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional
vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 455?465, Sofia, Bulgaria, August. Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 384?394, Stroudsburg, PA, USA. Association for Computational Linguistics.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword
supervision. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 618?626.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-3):165?210.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009. Phrase dependency parsing for opinion mining.
In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 -
Volume 3, EMNLP ?09, pages 1533?1541, Stroudsburg, PA, USA. Association for Computational Linguistics.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion targets
in a two-stage framework. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1764?1773, Sofia, Bulgaria, August. Association for Computational
Linguistics.
686
Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proceedings of
the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
1640?1649, Sofia, Bulgaria, August. Association for Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O?Brien-Strain. 2010. Extracting and ranking product features
in opinion documents. In Proceedings of the 23rd International Conference on Computational Linguistics:
Posters, COLING ?10, pages 1462?1470, Stroudsburg, PA, USA. Association for Computational Linguistics.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie review mining and summarization. In Proceedings of the
15th ACM international conference on Information and knowledge management, CIKM ?06, pages 43?50, New
York, NY, USA. ACM.
687
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2107?2116, Dublin, Ireland, August 23-29 2014.
Exploring Fine-grained Entity Type Constraints for Distantly Supervised
Relation Extraction
Yang Liu Kang Liu Liheng Xu Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
Zhongguancun East Road #95, Beijing 100190, China
{yang.liu, kliu, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Distantly supervised relation extraction, which can automatically generate training data by align-
ing facts in the existing knowledge bases to text, has gained much attention. Previous work used
conjunction features with coarse entity types consisting of only four types to train their model-
s. Entity types are important indicators for a specific relation, for example, if the types of two
entities are ?PERSON? and ?FILM? respectively, then there is more likely a ?DirectorOf? rela-
tion between the two entities. However, the coarse entity types are not sufficient to capture the
constraints of a relation between entities. In this paper, we propose a novel method to explore
fine-grained entity type constraints, and we study a series of methods to integrate the constraints
with the relation extracting model. Experimental results show that our methods achieve bet-
ter precision/recall curves in sentential extraction with smoother curves in aggregated extraction
which mean more stable models.
1 Introduction
Relation Extraction is the task of extracting semantic relations between a pair of entities from sentences
containing them. It can potentially benefit many applications, such as knowledge base construction,
question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc. Tra-
ditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need to
manually label training data, which is expensive and limits the ability to scale up. Due to the shortcom-
ing of supervised approaches mentioned above, recently, a more promising approach named distantly
supervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has be-
come popular. Instead of manual labeling, it automatically generates training data by aligning facts in
existing knowledge bases to text.
However, the paradigm of distant supervision also causes new problems of noisy training data both in
positive training instances and negative training instances. To overcome the false positive problem caused
by the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Sur-
deanu et al., 2012) proposed multi-instance models to model noisy positive training data, where they
assumed that at least one sentence in those containing an entity pair is truly positive. Takamatsu et al.
(Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would fail
when there was only one sentence containing both entities. They proposed a method to learn and filter
noisy pattern features from training instances to overcome the false positive problem. Researchers (Xu
et al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negative
training data caused by the incomplete knowledge base. Xu el al. (Xu et al., 2013) used the pseudo-
relevance feedback method trying to find out the false negative instances and add them into positive
training instances. Zhang et al. (Zhang et al., 2013) employed some rules to select negative training in-
stances carefully, hoping not to include the false negative instances. And Ritter et al. (Ritter and Etzioni,
2013) used hidden variables to model the missing data in databases based on a graphical model. The
training data generation process for all the above work is under the framework of (Mintz et al., 2009),
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2107
one important step of which is to recognize entity mentions from text and assign them entity types which
are used to compose features for training the model. The entity types they used are very coarse only con-
sisting of four categories (PERSON, ORGANIZATION, LOCATION, NONE). We argue that the coarse
entity types are not sufficient to indicate relations.
A specific relation constrains the entity types of its two entities. For instance, the SingerOf relation
limits the entity type of its first entity as PERSON or more fine-grained ARTIST, and the entity type of
its second entity as ART or more fine-grained MUSIC. Therefore, when extracting a relation instance,
the entity types of its two entities are important indicators for a specific relation. Previous work used
conjunction features (Details in Section 3.3) by combining the coarse entity types of entity mentions
with its contextual lexical and syntactic features. However, the conjunction features may fail to dis-
tinguish the relations. For example, the following two sentences contain two relation instances, one is
DirectorOf(Ang Lee, Life of Pi), and the other is AuthorOf(George R.R. Martin, A Song of Ice and Fire).
1. Ang Lee?s Life of Pi surprised many by scoring a leading four Oscars on Sunday night...
2. Westeros is the premiere fansite for George R.R. Martin?s A Song of Ice and Fire.
Only using the above conjunction features, we cannot tell the difference between the two entity pairs,
and are probable to incorrectly classify them as the same relation. By contrast, if we can assign each
entity with fine-grained entity types, for example, Ang Lee as the entity type ARTIST and George R.R.
Martin as AUTHOR, we may succeed in classifying the two entity pairs correctly.
To achieve the goal mentioned above, there are mainly three challenges: (1) how to define the fine-
grained type set; (2) how to assign the types to entity mentions; (3) how to integrate the fine-grained
entity type constraints with the relation extracting model. To address these challenges, in this paper,
we propose a novel approach to explore the fine-grained entity type constraints for distantly supervised
relation extraction. First, we use the types defined in (Ling and Weld, 2012) stemmed from Freebase
1
as the fine-grained entity type set (introduced in Section 3.1). Second, we leverage Web knowledge
to train a fine-grained entity type classifier and predict entity types for each entity mention. Third, we
study several methods to integrate the type constraints with an existing system MULTIR, a multi-instance
multi-label model in (Hoffmann et al., 2011), to train the extractor.
In summary, the contribution of this paper can be concluded as follows.
(a) We explore the effect of fine-grained entity type constraints on distantly supervised relation extrac-
tion. A novel method is proposed to leverage Web knowledge to automatically train a fine-grained
entity type classifier, which is used to predict the fine-grained types of each entity mention.
(b) We study a series of methods for integrating the fine-grained entity type constraints with the extract-
ing model and compare their performance with different parameter settings.
(c) We conduct experiments to demonstrate the effects of the newly exploited fine-grained entity type
constraints. It shows that our method achieves a much better precision/recall curves over the base-
line system in sentential extraction, and improves the performance with a smoother precision/recall
curve in aggregated extraction, which means a more stable model.
2 Distant Supervision for Relation Extraction
We define a relation instance (or a fact), which means a binary relation, as r(e
1
, e
2
). r is the relation, and
e
1
and e
2
mean the two entities in the relation instance, for example, BornIn(Y ao Ming, Shanghai).
Distant supervision supplies a method to automatically generate training data. In this part, we will
introduce the general steps in distant supervision for relation extraction. First, we define the notations
we use. ? denotes sentences comprising the corpus, E denotes entity mentions in the corpus which are
consecutive words with the same named entity tags assigned by an NER system, ? denotes the facts (or
relation instances) in the existing knowledge base. R denotes the relations in ?.
1
http://www.freebase.com/
2108
 Figure 1: Fine-grained entity type set.
Figure 2: Framework of fine-grained entity type classifier.
To generate training data, we align pairs of entity mentions in the same sentence with ?. The aligned
entity mentions E
train
and their sentences ?
train
along with R
train
are used as training data. Features
are extracted from them to train the relation extracting model.
To predict the unknown data for extracting new relation instances, we input pairs of entity mentions
E
predict
and the sentences containing them ?
preidct
into the trained extracting model for extracting new
relation instances.
3 Fine-grained Entity Type Constraints
Entity mentions in sentences are considered consecutive words with the same entity types (Section 2).
The entity types are part of the lexical and syntactic features(Mintz et al., 2009), and the feature setting
is followed by other related work. Their entity types are assigned by an NER system and consist of
four categories (PERSON, ORGANIZATION, LOCATION, NONE). The types of entity mentions in
a relation are important indicators for the very type of relation. However, the coarse (only four types)
entity types may not capture sufficient constraints to distinguish a relation. In this section, we explore
fine-grained entity type constraints and study different methods to integrate them with the extracting
model.
This section first introduces the fine-grained entity type set(Section 3.1), and then describes our method
which leverages Web knowledge to train the fine grained entity type classifier and assign entity mentions
with the fine-grained entity types (Section 3.2). At last, we illustrate methods to integrate fine-grained
entity type constraints with the relation extracting model.
2109
Entity pair [Hank Ratner], [Cablevision]
Sentence
Cablevision?s $600 million offer came in the form of a letter to Peter S.Kalikow,
chairman of the M.T.A., from the Garden?s vice chairman, Hank Ratner.
Conjunction Reverse Left NE1 Middle NE2 Right
Feature examples
False PER ORG
False Hank[NMOD] PER [NMOD]chairman ... offer[SBJ] ORG
True B -1 ORG POS $ ... NN NN, PER .B 1
Table 1: Examples of conjunction features.
3.1 Fine-grained Entity Types
Figure 1 is the type set we use. It was introduced in (Ling and Weld, 2012) and was derived from
Freebase types. The bold types in each small box of Figure 1 are upper-class types for others in that
small box. For example, /actor is a lower-class type of /person which is denoted as /person/actor.
And /person and /person/actor coexist in the type set.
3.2 Fine-grained Entity Type Classifier
In this section, we describe our method that leverages Web knowledge to train a fine-grained entity type
classifier and predict entity types of each entity mention. Its architecture is shown in Figure 2.
3.2.1 Training
The training data are obtained from Wikipedia. Because the defined fine-grained types are tailored based
on Freebase types, we can find the mappings between the two type sets, for example, /person/doctor
maps to two Freebase types /medicine/physician and /medicine/surgeon. And Freebase WEX
2
supplies a mapping between Freebase types to Wikipedia articles. As a result, we can map Wikipedia
articles to defined fine-grained types.
Based on the mappings, we obtain Wikipedia articles for each type as training data and negative
training examples are sampled from articles not contained in the mappings. We preprocess the articles
by: stop words filtering, stemming, and term frequency filtering and use a maxent model to train the
classifier.
3.2.2 Predicting
To predict types of each entity mention, we first use search engines to expand entity mentions. Specif-
ically, each entity mention is used as a query sent to the search engine
3
. Titles and descriptions of top
k returned snippets are selected (We keep the top 20 in the experiments). The obtained text are pre-
processed with the same method as training examples. Then we use the trained fine-grained entity type
classifier to predict the types of each entity mention.
After predicting, we obtain a ranked list of types for each entity mention, which are ranked by the
predicting scores.
3.3 Integrating Fine-grained Entity Type Constraints into the Extracting Model
This section introduces our methods to integrate the fine-grained entity type constraints with the ex-
tracting model. First of all, we briefly review the features used in previous models which derived from
(Mintz et al., 2009) and (Riedel et al., 2010). Their features mainly comprise two types: lexical features
(POS tags, words and entity types) and syntactic features (dependency parsing tags, words and entity
types). Each feature is a conjunction with several parts: entity types of two entity mentions, the left
context window of the first entity mention, the right context window of the second entity mention and
the part between them (the window contains none or one or two words ). Table 1 shows an example of
the conjunction features.
2
http://wiki.freebase.com/wiki/WEX
3
We use Bing search API. http://datamarket.azure.com/dataset/bing/search
2110
To integrate the exploited fine-grained entity type constraints with the extracting model, we proposed
three methods (substitution, augment and selection) to make the type constraints take effects.
3.3.1 Substitution Method
In this method, we substitute coarse entity types of the features with the entity mentions? fine-grained
types, and use the new features to train the model. Instead of substituting directly, an entity mention
is first represented by its fine-grained types and the upper-class of the fine-grained type, for example,
/person/politician derives two types /person and /person/politician itself. The reason is that the
extracting model can benefit from the related types like the upper-class types. And then we use the
obtained entity types to substitute the old coarse entity types as new features greedily, which mean-
s that all the possible combinations of types between the entity pair are considered. For example,
?Barack Obama? has the fine-grained type /person/politician and his birth place ?Hawaii? has
the type /location/island, then there are 4 combinations between the two entities, they are (/person,
/location), (/person, /location/island), (/person/politician, /location) and (/person/politician,
/location).
3.3.2 Augment Method
In this method, we generate new features by substituting the coarse entity types with predicted fine-
grained types, and expand the old features with new features. Different from the substitution method, we
do not add the upper-class types, for that we think the coarse types in old features have the same effect.
In this method, we use the fine-grained constraints as a complementary.
3.3.3 Selection Method
The selection method is similar to the augment method. The difference is that we do not expand all
old features with new features. We select some of them to expand. The reason is that some of the
conjunction features are of high-precision themselves, it can clearly indicate the relations with its left,
middle and right parts, even without the entity types (informative ones). If we expand these features,
it may cause more noisy features. So we expect to only expand the ones that lack of the indicating
abilities (non-informative ones). In this paper, we employ a simple method to distinguish between the
informative ones and non-informative ones by the length of the features, which means that the longer is
more informative than the shorter. In our experiments, the length threshold is set as 20.
In the predicting phase (Section 3.2), we obtain a ranked type list for each entity mention. The top list
types are considered in our methods. Experiments in Section 4.3 are conducted on top k {k ? 1, 2, 3}
type/types in the obtained ranked list. And they are combined with a greedy method similar to that in the
substitution method explained above.
4 Experiments
4.1 Settings
We use the same data sets as (Riedel et al., 2010) and (Hoffmann et al., 2011), where NYTimes sentences
in the years 2005-2006 are used as training corpus ?
train
for distant supervision and sentences in 2007
are used as testing corpus ?
predict
. The data was first tagged with an NER system (Finkel et al., 2005)
and consecutive words with the same tag are extracted as entity mentions. And then, entity mentions
E
train
in training corpus are aligned to facts ? in Freebase as training examples to train the models.
We integrate our fine-grained entity type constraint with MULTIR, an existing multi-instance multi-
label extracting model in (Hoffmann et al., 2011). Following their setttings, we conduct experiments on
aggregated extraction and sentential extraction to show the effect of fine-grained entity type constraints.
? Aggregated extraction: Aggregated extraction is corpus-level extraction. When given an entity
pair, it predicts its relation types based on the whole corpus. After extraction, the precision and
recall are computed by comparing the results with facts in Freebase. The evaluation underestimates
the accuracy because there may be correct facts in the extracted results but not existing in Freebase,
these facts are labeled as incorrect by mistake here. Because aggregated extraction is an automatic
evaluation, it is used to tune parameters like held-out evaluation in (Mintz et al., 2009).
2111
(a) PR curves of the substitution method (b) PR curves of the augment method
(c) PR curves of the selection method (d) Comparison with other methods
Figure 3: Precision-recall (PR) curves of the aggregated extraction.
? Sentential extraction: Sentential extraction predicts an entity pair only based on a specified sen-
tence containing the pair of entities. We use manually labeled data in (Hoffmann et al., 2011) as
benchmark. The data consist of 1,000 sentences and are sampled from the results their system out-
puts and sentences aligned with facts in Freebase. As they stated in their paper, these results provide
a good approximation to the true precision but can overestimate the actual recall.
4.2 Experimental Results
In aggregated extraction, we first evaluate the three type-constraint integration methods (substitution,
augment and selection) with the top k {k ? 1, 2, 3} type/types (Section 3.3). And then, we compare the
best parameter setting methods with previous work. In sentential extraction, we compare methods tuned
in aggregated extraction with MULTIR.
4.2.1 Aggregated Extraction
Figure 3 shows the precision-recall (PR) curves of the aggregated extraction. In it, Sub topk {k ?
1, 2, 3} means using the substitution method (Section 3.3) with top k fine-grained entities types re-
turned by the type classifier in Section 3.2. Correspondingly, Aug topk is for the augment method
and Select topk is for the selection method.
Figure 3(a) shows that Sub top3 outperforms the other two settings of k in the substitution method,
it seems that more fine-grained types produce better curves. In Figure 3(b), Aug top1 and Aug top2
achieve similar performances. However, when adding one more type with k = 3, we obtain a lower
curve, which contradicts the trend showed in the curves of the substitution method (Figure 3(a)). Fig-
ure 3(c) shows the PR curves of three selection methods, Select top1 has a better performance at the
beginning. Then Select top2 exceeds it a bit consistently.
In Figure 3(d), we demonstrate the comparison of best tuned methods above with previous work.
They are Sub top3, Aug top1 and Select top2. From Figure 3(d), it shows that, among the three of
our methods, Aug top1 achieves better precisions along the PR curves, and Select top2 reaches the best
2112
Figure 4: Comparison with MULTIR
recall at the highest recall point. Comparing to other methods, the PR curve of Aug top1 reaches a higher
recall with 29.3% at the highest recall point than MULTIR (24.5%). Select top2 achieves 29.3% at the
highest recall point, best among all methods. And by integrating the fine-grained entity type constraints,
they improve the PR curve of MULTIR with a more smoother curve without most of the depressions seen
in MULTIR. As stated in (Hoffmann et al., 2011), the smoother curve indicated a more stable model.
4.2.2 Sentential Extraction
Figure 4 shows the precision-recall (PR) curves of the sentential extraction. In the evaluation, we com-
pare the three best integration methods tuned in aggregated extraction with original MULTIR. Among our
three method, Aug top1 outperforms in precision and achieves a better curve in general among the three
methods, however, Select top2 gains a better recall at the end. Sub top3 has the worst recall. In gen-
eral, our methods have much better precisions than MULTIR. Aug top1 and Select top2 achieve better
curves than MULTIR. Since the evaluation of sentential extraction is a good approximation of precision,
it implies that the proposed methods are effective.
4.2.3 Analysis
On one hand, among the three proposed integration methods, generally, the augment method and selec-
tion method get better performance. The reason is that substitution method uses predicted fine-grained
entity types to replace the old coarse features in the conjunction features completely, and the conjunction
features are sensitive to entity types for different entity types indicate different conjunction features, as
a result, if we can not promise a good accuracy in the type classification which is hard to achieve in
classifying hundreds of fine-grained types, the performance will be badly influenced. Different from the
substitution method, augment method and selection method keep the old features with coarse features,
they use the features with fine-grained entity type constraints as extra information to help the extraction
and achieve better results.
On the other hand, comparing to other methods, by integration the exploited fine-grained entity type
constraints, our methods achieve improvements in both aggregated and sentential extraction. It proves
that the fine-grained entity type constraints we exploit are effective, and our proposed integration meth-
ods succeed in integrating the constraints into the extracting model. Our augment method outperforms
MULTIR in precision along the PR curves in sentential extraction and improve it performance with a
more smoother PR curve in aggregated extraction, which indicates a more stable model. Moreover, the
method gets a better recall. And our selection method consistently outperforms MULTIR in sentential
2113
k=1 k=2 k=3
Recall@k 0.596 0.740 0.806
Table 2: Evaluation of the fine-grained type classifier.
extraction. In aggregated extraction, it also achieves a smoother curve and an impressive promotion at
the highest recall point. Since the evaluation of aggregated extraction only considers the facts existing
in Freebase which may incorrectly label the right extracting results and underestimate the true precision,
and based on its better performance of precision in sentential extraction, we consider it is a more promis-
ing method. This paper only employs very naive method to select the non-informative features by its
length (Section 3.3.3), a more effective selecting method may lead further improvements.
4.3 Performance of Entity Type Classifier
We evaluate the performance of the fine-grained entity type classifier (Section 3.2). In section 3.2, we
sample the training examples from a collection of Wikipeida articles mapped with the fine-grained types.
To generate test entity mentions, we first remove the sampled training articles from the collection, and
then sample the articles from it, where the titles of sampled articles are used as the test entity mentions
(we sample 12,000 test entity mentions) and their mapped fine-grained types are used as benchmark.
After that, the predicting method in Section 3.2.2 is used to expand mentions and predict the types of
each test entity mention. After predicting, we obtain a ranked list of types for each test entity mention.
To evaluate, we define a notation of Hit@k, which equals 1 if the true type of an entity mention is
hit in the top k predicted types, otherwise equals 0. And then we evaluate it by the Recall@k defined
bellow.
Recall@k =
?
12000
i=1
Hit@k
i
12000
(1)
In equation (1), i means the ith test entity mention. Table 2 shows the results for the top 3 predicted
types.
5 Related Work
Distant supervision (also known as weak supervision or self supervision) is used to a broad class of meth-
ods in information extraction which aims to automatically generate labeled data by aligning with data
in knowledge bases. It is introduced by Craven and Kumlien (Craven et al., 1999) who used the Yeast
Protein Database to generate labeled data and trained a naive-Bayes extractor. Bellare and McCallum
(Bellare and McCallum, 2007) used BibTex records as the source of distant supervision. The KYLIN
system in (Wu and Weld, 2007) used article titles and infoboxes of Wikipedia to label sentences and
trained a CRF extractor aiming to generate infoboxes automatically. The Open IE systems TEXTRUN-
NER (Yates et al., 2007) and WOE (Wu and Weld, 2010) trained their extractors with the automatic
labeled data from Penn Treebank and Wikipedia infoboxes respectively.
Mintz (Mintz et al., 2009) first introduced their work that performed distant supervision for relation
extraction. It used Freebase as the knowledge base to align sentences in Wikipedia as training data and
trained a logistic regression classifier to extract relations between entities.Distant supervision supplied a
method to generate training data automatically, however it also bring the problem of noisy labeling. After
their work, a variety of methods focused to solve this problem. Riedel (Riedel et al., 2010) proposed a
multi-instance model to model the false positive noise in training data with the assumption that at least
one of the labeled sentences truly expressed their relation. After their work, Hoffmann (Hoffmann et
al., 2011) and Surdeanu (Surdeanu et al., 2012) tried to not only model the noisy training data, but also
overcame the problem of multi-label where two entities may exist more than one relation, they proposed
graphic models as kinds of multi-instance multi-label learning methods and made improvements over
previous work. The at-least-one assumption would fail when encountering entity pairs with only one
aligned sentence. Takamatsu (Takamatsu et al., 2012) employed an alternative approach without the
mentioned assumptions. Their work predicted negative patterns using a generative model and remove
labeled data containing negative patterns to reducing noise in labeled data.
2114
Besides the problem of false positive training examples caused by distant supervision. There were a
bunch of researches trying to solve the problem of false negative training examples caused by incomplete
knowledge bases. Zhang (Zhang et al., 2013) made heuristic rules to filter the false negative training
examples. And Xu (Xu et al., 2013) tried to overcom this problem by pseudo-relevance feedback. Min
(Min et al., 2013) improved MIML in (Surdeanu et al., 2012) by adding a new layer in their 3-layer
graphic model to model the incomplete knowledge base. Ritter (Ritter and Etzioni, 2013) employed
similar intuition with (Xu et al., 2013) that they thought rear entities missing in the database would
be often mentioned in the text. They proposed a latent-variable approach to model it and showed its
improvement over aggregate and sentential extraction.
6 Conclusion
In this paper, we propose a novel approach to explore the fine-grained entity type constraints for distantly
supervised relation extraction. We leverage Web knowledge to automatically train a fine-grained entity
type classifier and predict entity types of each entity mention. And we study a series of methods to inte-
grate the type constraints with a relation extraction model. At last, thorough experiments are conducted.
The experimental results imply our methods are effective with better precision/recall curves in senten-
tial extraction and smoother precision/recall curves in aggregated extraction, which indicate more stable
models.
In the future we hope to explore more details of integration methods that integrates fine-grained entity
type constraints with relation extraction models, especially the selection integration method. We consider
that a more effective method to distinguish between the informative and non-informative features will
lead more improvements.
Acknowledgements
This work was sponsored by the National Basic Research Program of China (No. 2014CB340503) and
the National Natural Science Foundation of China (No. 61202329). This work was supported in part by
Noahs Ark Lab of Huawei Tech. Co. Ltd.
References
Kedar Bellare and Andrew McCallum. 2007. Learning extractors from unlabeled text using relevant databases. In
Sixth International Workshop on Information Integration on the Web.
Mark Craven, Johan Kumlien, et al. 1999. Constructing biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular
Biology, pages 77?86. Heidelberg, Germany.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 363?370. Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge-based
weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 541?
550.
Xiao Ling and DS Weld. 2012. Fine-Grained Entity Recognition. In AAAI.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation
extraction with an incomplete knowledge base. In Proceedings of NAACL-HLT, pages 777?782.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages
1003?1011. Association for Computational Linguistics.
2115
Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In
Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 41?47. Associa-
tion for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without
labeled text. In Machine Learning and Knowledge Discovery in Databases, pages 148?163. Springer.
Alan Ritter and Oren Etzioni. 2013. Modeling Missing Data in Distant Supervision for Information Extraction.
Transactions of the Association for Computational Linguistics, 1:367?378.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 455?465. Association for
Computational Linguistics.
Idan Szpektor, Hristo Tanev, Ido Dagan, Bonaventura Coppola, et al. 2005. Scaling Web-based aquisition of
entailment relations. Ph.D. thesis, Tel Aviv University.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for
relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 721?729. Association for Computational Linguistics.
Fei Wu and Daniel S Weld. 2007. Autonomously semantifying wikipedia. In Proceedings of the sixteenth ACM
conference on Conference on information and knowledge management, pages 41?50. ACM.
Fei Wu and Daniel S Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th An-
nual Meeting of the Association for Computational Linguistics, pages 118?127. Association for Computational
Linguistics.
W Xu, RH Le Zhao, and R Grishman. 2013. Filling Knowledge Base Gaps for Distant Supervision of Relation
Extraction. Proceedings of Association for Computational Linguistics.
Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland.
2007. Textrunner: open information extraction on the web. In Proceedings of Human Language Technolo-
gies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics:
Demonstrations, pages 25?26. Association for Computational Linguistics.
Xingxing Zhang, jianwen Zhang, Junyu Zeng, Jun Yan, Zheng Chen, and Zuifang Sui. 2013. Towards Accurate
Distant Supervision for Relational Facts Extraction. In Proceedings of Association for Computational Linguis-
tics.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 427?434.
Association for Computational Linguistics.
GuoDong Zhou, Min Zhang, Dong Hong Ji, and Qiaoming Zhu. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In Proceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language Learning.
2116
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2335?2344, Dublin, Ireland, August 23-29 2014.
Relation Classification via Convolutional Deep Neural Network
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{djzeng,kliu,swlai,gyzhou,jzhao}@nlpr.ia.ac.cn
Abstract
The state-of-the-art methods used for relation classification are primarily based on statistical ma-
chine learning, and their performance strongly depends on the quality of the extracted features.
The extracted features are often derived from the output of pre-existing natural language process-
ing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders
the performance of these systems. In this paper, we exploit a convolutional deep neural network
(DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as
input without complicated pre-processing. First, the word tokens are transformed to vectors by
looking up word embeddings
1
. Then, lexical level features are extracted according to the given
nouns. Meanwhile, sentence level features are learned using a convolutional approach. These
two level features are concatenated to form the final extracted feature vector. Finally, the fea-
tures are fed into a softmax classifier to predict the relationship between two marked nouns. The
experimental results demonstrate that our approach significantly outperforms the state-of-the-art
methods.
1 Introduction
The task of relation classification is to predict semantic relations between pairs of nominals and can
be defined as follows: given a sentence S with the annotated pairs of nominals e
1
and e
2
, we aim
to identify the relations between e
1
and e
2
(Hendrickx et al., 2010). There is considerable interest in
automatic relation classification, both as an end in itself and as an intermediate step in a variety of NLP
applications.
The most representative methods for relation classification use supervised paradigm; such methods
have been shown to be effective and yield relatively high performance (Zelenko et al., 2003; Bunescu
and Mooney, 2005; Zhou et al., 2005; Mintz et al., 2009). Supervised approaches are further divided
into feature-based methods and kernel-based methods. Feature-based methods use a set of features that
are selected after performing textual analysis. They convert these features into symbolic IDs, which are
then transformed into a vector using a paradigm that is similar to the bag-of-words model
2
. Conversely,
kernel-based methods require pre-processed input data in the form of parse trees (such as dependency
parse trees). These approaches are effective because they leverage a large body of linguistic knowledge.
However, the extracted features or elaborately designed kernels are often derived from the output of pre-
existing NLP systems, which leads to the propagation of the errors in the existing tools and hinders the
performance of such systems (Bach and Badaskar, 2007). It is attractive to consider extracting features
that are as independent from existing NLP tools as possible.
To identify the relations between pairs of nominals, it is necessary to a skillfully combine lexical and
sentence level clues from diverse syntactic and semantic structures in a sentence. For example, in the
sentence ?The [fire]
e
1
inside WTC was caused by exploding [fuel]
e
2
?, to identify that fire and fuel are in a
This work is licenced under a Creative Commons Attribution 4.0 International License.Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
A word embedding is a distributed representation for a word. For example, Collobert et al. (2011) use a 50-dimensional
vector to represent a word.
2
http://en.wikipedia.org/wiki/Bag-of-words model
2335
Cause-Effect relationship, we usually leverage the marked nouns and the meanings of the entire sentence.
In this paper, we exploit a convolutional DNN to extract lexical and sentence level features for relation
classification. Our method takes all of the word tokens as input without complicated pre-processing,
such as Part-of-Speech (POS) tagging and syntactic parsing. First, all the word tokens are transformed
into vectors by looking up word embeddings. Then, lexical level features are extracted according to the
given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two
level features are concatenated to form the final extracted feature vector. Finally, the features are feed
into a softmax classifier to predict the relationship between two marked nouns.
The idea of extracting features for NLP using convolutional DNN was previously explored by Col-
lobert et al. (2011), in the context of POS tagging, chunking (CHUNK), Named Entity Recogni-
tion (NER) and Semantic Role Labeling (SRL). Our work shares similar intuition with that of Collobert
et al. (2011). In (Collobert et al., 2011), all of the tasks are considered as the sequential labeling prob-
lems in which each word in the input sentence is given a tag. However, our task, ?relation classification?,
can be considered a multi-class classification problem, which results in a different objective function.
Moreover, relation classification is defined as assigning relation labels to pairs of words. It is thus nec-
essary to specify which pairs of words to which we expect to assign relation labels. For that purpose, the
position features (PF) are exploited to encode the relative distances to the target noun pairs. To the best
of our knowledge, this work is the first example of using a convolutional DNN for relation classification.
The contributions of this paper can be summarized as follows.
? We explore the feasibility of performing relation classification without complicated NLP pre-
processing. A convolutional DNN is employed to extract lexical and sentence level features.
? To specify pairs of words to which relation labels should be assigned, position features are proposed
to encode the relative distances to the target noun pairs in the convolutional DNN.
? We conduct experiments using the SemEval-2010 Task 8 dataset. The experimental results demon-
strate that the proposed position features are critical for relation classification. The extracted lexical
and sentence level features are effective for relation classification. Our approach outperforms the
state-of-the-art methods.
2 Related Work
Relation classification is one of the most important topics in NLP. Many approaches have been explored
for relation classification, including unsupervised relation discovery and supervised classification. Re-
searchers have proposed various features to identify the relations between nominals using different meth-
ods.
In the unsupervised paradigms, contextual features are used. Distributional hypothesis theory (Harris,
1954) indicates that words that occur in the same context tend to have similar meanings. Accordingly, it is
assumed that the pairs of nominals that occur in similar contexts tend to have similar relations. Hasegawa
et al. (2004) adopted a hierarchical clustering method to cluster the contexts of nominals and simply
selected the most frequent words in the contexts to represent the relation between the nominals. Chen
et al. (2005) proposed a novel unsupervised method based on model order selection and discriminative
label identification to address this problem.
In the supervised paradigm, relation classification is considered a multi-classification problem, and re-
searchers concentrate on extracting more complex features. Generally, these methods can be categorized
into two types: feature-based and kernel-based. In feature-based methods, a diverse set of strategies
have been exploited to convert the classification clues (such as sequences and parse trees) into feature
vectors (Kambhatla, 2004; Suchanek et al., 2006). Feature-based methods suffer from the problem
of selecting a suitable feature set when converting the structured representation into feature vectors.
Kernel-based methods provide a natural alternative to exploit rich representations of the input classifica-
tion clues, such as syntactic parse trees. Kernel-based methods allow the use of a large set of features
without explicitly extracting the features. Various kernels, such as the convolution tree kernel (Qian et
2336
WordRepresentation
FeatureExtraction
Output W3x
Figure 1: Architecture of the neural network used
for relation classification.
WindowProcessing
max over timesConvolution
tanh W2x
W1
WF
PF
Sentence levelFeatures
Figure 2: The framework used for extracting sen-
tence level features.
al., 2008), subsequence kernel (Mooney and Bunescu, 2005) and dependency tree kernel (Bunescu and
Mooney, 2005), have been proposed to solve the relation classification problem. However, the methods
mentioned above suffer from a lack of sufficient labeled data for training. Mintz et al. (2009) proposed
distant supervision (DS) to address this problem. The DS method selects sentences that match the facts
in a knowledge base as positive examples. The DS algorithm sometimes faces the problem of wrong
labels, which results in noisy labeled data. To address the shortcoming of DS, Riedel et al. (2010) and
Hoffmann et al. (2011) cast the relaxed DS assumption as multi-instance learning. Furthermore, Taka-
matsu et al. (2012) noted that the relaxed DS assumption would fail and proposed a novel generative
model to model the heuristic labeling process in order to reduce the wrong labels.
The supervised method has been demonstrated to be effective for relation detection and yields rela-
tively high performance. However, the performance of this method strongly depends on the quality of the
designed features. With the recent revival of interest in DNN, many researchers have concentrated on us-
ing Deep Learning to learn features. In NLP, such methods are primarily based on learning a distributed
representation for each word, which is also called a word embeddings (Turian et al., 2010). Socher et al.
(2012) present a novel recursive neural network (RNN) for relation classification that learns vectors in
the syntactic tree path that connects two nominals to determine their semantic relationship. Hashimoto
et al. (2013) also use an RNN for relation classification; their method allows for the explicit weighting
of important phrases for the target task. As mentioned in Section 1, it is difficult to design high quality
features using the existing NLP tools. In this paper, we propose a convolutional DNN to extract lexical
and sentence level features for relation classification; our method effectively alleviates the shortcomings
of traditional features.
3 Methodology
3.1 The Neural Network Architecture
Figure 1 describes the architecture of the neural network that we use for relation classification. The
network takes an input sentence and discovers multiple levels of feature extraction, where higher levels
represent more abstract aspects of the inputs. It primarily includes the following three components: Word
Representation, Feature Extraction and Output. The system does not need any complicated syntactic or
semantic preprocessing, and the input of the system is a sentence with two marked nouns. Then, the
word tokens are transformed into vectors by looking up word embeddings. In succession, the lexical and
sentence level features are respectively extracted and then directly concatenated to form the final feature
vector. Finally, to compute the confidence of each relation, the feature vector is fed into a softmax
classifier. The output of the classifier is a vector, the dimension of which is equal to the number of
predefined relation types. The value of each dimension is the confidence score of the corresponding
relation.
2337
Features Remark
L1 Noun 1
L2 Noun 2
L3 Left and right tokens of noun 1
L4 Left and right tokens of noun 2
L5 WordNet hypernyms of nouns
Table 1: Lexical level features.
3.2 Word Representation
In the word representation component, each input word token is transformed into a vector by looking
up word embeddings. Collobert et al. (2011) reported that word embeddings learned from significant
amounts of unlabeled data are far more satisfactory than the randomly initialized embeddings. In relation
classification, we should first concentrate on learning discriminative word embeddings, which carry more
syntactic and semantic information, using significant amounts of unlabeled data. Unfortunately, it usually
takes a long time to train the word embeddings
3
. However, there are many trained word embeddings that
are freely available (Turian et al., 2010). A comparison of the available word embeddings is beyond
the scope of this paper. Our experiments directly utilize the trained embeddings provided by Turian et
al.(2010).
3.3 Lexical Level Features
Lexical level features serve as important cues for deciding relations. The traditional lexical level features
primarily include the nouns themselves, the types of the pairs of nominals and word sequences between
the entities, the quality of which strongly depends on the results of existing NLP tools. Alternatively,
this paper uses generic word embeddings as the source of base features. We select the word embeddings
of marked nouns and the context tokens. Moreover, the WordNet hypernyms
4
are adopted as MVRNN
(Socher et al., 2012). All of these features are concatenated into our lexical level features vector l. Table
1 presents the selected word embeddings that are related to the marked nouns in the sentence.
3.4 Sentence Level Features
As mentioned in section 3.2, all of the tokens are represented as word vectors, which have been demon-
strated to correlate well with human judgments of word similarity. Despite their success, single word
vector models are severely limited because they do not capture long distance features and semantic com-
positionality, the important quality of natural language that allows humans to understand the meanings
of a longer expression. In this section, we propose a max-pooled convolutional neural network to offer
sentence level representation and automatically extract sentence level features. Figure 2 shows the frame-
work for sentence level feature extraction. In the Window Processing component, each token is further
represented as Word Features (WF) and Position Features (PF) (see section 3.4.1 and 3.4.2). Then, the
vector goes through a convolutional component. Finally, we obtain the sentence level features through a
non-linear transformation.
3.4.1 Word Features
Distributional hypothesis theory (Harris, 1954) indicates that words that occur in the same context tend
to have similar meanings. To capture this characteristic, the WF combines a word?s vector representation
and the vector representations of the words in its context. Assume that we have the following sequence
of words.
S : [People]
0
have
1
been
2
moving
3
back
4
into
5
[downtown]
6
The marked nouns are associated with a label y that defines the relation type that the marked pair contains.
Each word is also associated with an index into the word embeddings. All of the word tokens of the
sentence S are then represented as a list of vectors (x
0
,x
1
, ? ? ? ,x
6
), where x
i
corresponds to the word
3
Collobert et al. (2011) proposed a pairwise ranking approach to train the word embeddings, and the total training time for
an English corpus (Wikipedia) was approximately four weeks.
4
http://sourceforge.net/projects/supersensetag/
2338
embedding of the i-th word in the sentence. To use a context size of w, we combine the size w windows
of vectors into a richer feature. For example, when we take w = 3, the WF of the third word ?moving?
in the sentence S is expressed as [x
2
,x
3
,x
4
]. Similarly, considering the whole sentence, the WF can be
represented as follows:
{[x
s
,x
0
,x
1
], [x
0
,x
1
,x
2
], ? ? ? , [x
5
,x
6
,x
e
]}
5
3.4.2 Position Features
Relation classification is a very complex task. Traditionally, structure features (e.g., the shortest depen-
dency path between nominals) are used to solve this problem (Bunescu and Mooney, 2005). Apparently,
it is not possible to capture such structure information only through WF. It is necessary to specify which
input tokens are the target nouns in the sentence. For this purpose, PF are proposed for relation classi-
fication. In this paper, the PF is the combination of the relative distances of the current word to w
1
and
w
2
. For example, the relative distances of ?moving? in sentence S to ?people? and ?downtown? are 3
and -3, respectively. In our method, the relative distances also are mapped to a vector of dimension d
e
(a
hyperparameter); this vector is randomly initialized. Then, we obtain the distance vectors d
1
and d
2
with
respect to the relative distances of the current word to w
1
and w
2
, and PF = [d
1
,d
2
]. Combining the WF
and PF, the word is represented as [WF,PF]
T
, which is subsequently fed into the convolution component
of the algorithm.
3.4.3 Convolution
We will see that the word representation approach can capture contextual information through combina-
tions of vectors in a window. However, it only produces local features around each word of the sentence.
In relation classification, an input sentence that is marked with target nouns only corresponds to a re-
lation type rather than predicting label for each word. Thus, it might be necessary to utilize all of the
local features and predict a relation globally. When using neural network, the convolution approach is a
natural method to merge all of the features. Similar to Collobert et al. (2011), we first process the output
of Window Processing using a linear transformation.
Z = W
1
X (1)
X ? R
n
0
?t
is the output of the Window Processing task, where n
0
= w? n, n (a hyperparameter) is the
dimension of feature vector, and t is the token number of the input sentence. W
1
? R
n
1
?n
0
, where n
1
(a
hyperparameter) is the size of hidden layer 1, is the linear transformation matrix. We can see that the
features share the same weights across all times, which greatly reduces the number of free parameters to
learn. After the linear transformation is applied, the output Z ? R
n
1
?t
is dependent on t. To determine
the most useful feature in the each dimension of the feature vectors, we perform a max operation over
time on Z.
m
i
= maxZ(i, ?) 0 ? i ? n
1
(2)
where Z(i, ?) denote the i-th row of matrix Z. Finally, we obtain the feature vector m =
{m
1
,m
2
, ? ? ? ,m
n
1
}, the dimension of which is no longer related to the sentence length.
3.4.4 Sentence Level Feature Vector
To learn more complex features, we designed a non-linear layer and selected hyperbolic tanh as the
activation function. One useful property of tanh is that its derivative can be expressed in terms of the
function value itself:
d
dx
tanhx = 1? tanh
2
x (3)
It has the advantage of making it easy to compute the gradient in the backpropagation training procedure.
Formally, the non-linear transformation can be written as
g = tanh(W
2
m) (4)
5
x
s
and x
e
are special word embeddings that correspond to the beginning and end of the sentence, respectively.
2339
W2
? R
n
2
?n
1
is the linear transformation matrix, where n
2
(a hyperparameter) is the size of hidden
layer 2. Compared with m ? R
n
1
?1
, g ? R
n
2
?1
can be considered higher level features (sentence level
features).
3.5 Output
The automatically learned lexical and sentence level features mentioned above are concatenated into a
single vector f = [l, g]. To compute the confidence of each relation, the feature vector f ? R
n
3
?1
(n
3
equals n
2
plus the dimension of the lexical level features) is fed into a softmax classifier.
o = W
3
f (5)
W
3
? R
n
4
?n
3
is the transformation matrix and o ? R
n
4
?1
is the final output of the network, where n
4
is equal to the number of possible relation types for the relation classification system. Each output can
be then interpreted as the confidence score of the corresponding relation. This score can be interpreted
as a conditional probability by applying a softmax operation (see Section 3.6).
3.6 Backpropagation Training
The DNN based relation classification method proposed here could be stated as a quintuple ? =
(X,N,W
1
,W
2
,W
3
)
6
. In this paper, each input sentence is considered independently. Given an in-
put example s, the network with parameter ? outputs the vector o, where the i-th component o
i
contains
the score for relation i. To obtain the conditional probability p(i|x, ?), we apply a softmax operation over
all relation types:
p(i|x, ?) =
e
o
i
n
4
?
k=1
e
o
k
(6)
Given all our (suppose T ) training examples (x
(i)
; y
(i)
), we can then write down the log likelihood of the
parameters as follows:
J (?) =
T
?
i=1
log p(y
(i)
|x
(i)
, ?) (7)
To compute the network parameter ?, we maximize the log likelihood J(?) using a simple optimization
technique called stochastic gradient descent (SGD). N,W
1
,W
2
and W
3
are randomly initialized and
X is initialized using the word embeddings. Because the parameters are in different layers of the neural
network, we implement the backpropagation algorithm: the differentiation chain rule is applied through
the network until the word embedding layer is reached by iteratively selecting an example (x, y) and
applying the following update rule.
? ? ? + ?
? log p(y|x, ?)
??
(8)
4 Dataset and Evaluation Metrics
To evaluate the performance of our proposed method, we use the SemEval-2010 Task 8 dataset (Hen-
drickx et al., 2010). The dataset is freely available
7
and contains 10,717 annotated examples, including
8,000 training instances and 2,717 test instances. There are 9 relationships (with two directions) and
an undirected Other class. The following are examples of the included relationships: Cause-Effect,
Component-Whole and Entity-Origin. In the official evaluation framework, directionality is taken into
account. A pair is counted as correct if the order of the words in the relationship is correct. For example,
both of the following instances S
1
and S
2
have the relationship Component-Whole.
S
1
: The [haft]
e
1
of the [axe]
e
2
is make ? ? ? ? Component-Whole(e
1
,e
2
)
S
2
: This [machine]
e
1
has two [units]
e
2
? ? ? ? Component-Whole(e
2
,e
1
)
6
N represents the word embeddings of WordNet hypernyms.
7
http://docs.google.com/View?id=dfvxd49s 36c28v9pmw
2340
?# Window size
1 2 3 4 5 6 7
F1
72
74
76
78
80
82
# Hidden layer 1
0 100 200 300 400 500 600
F1
72
74
76
78
80
82
# Hidden layer 2
0 100 200 300 400 500 600
F1
72
74
76
78
80
82
Figure 3: Effect of hyperparameters.
However, these two instances cannot be classified into the same category because Component-
Whole(e
1
,e
2
) and Component-Whole(e
2
,e
1
) are different relationships. Furthermore, the official rank-
ing of the participating systems is based on the macro-averaged F1-scores for the nine proper relations
(excluding Other). To compare our results with those obtained in previous studies, we adopt the macro-
averaged F1-score and also account for directionality into account in our following experiments
8
.
5 Experiments
In this section, we conduct three sets of experiments. The first is to test several variants via cross-
validation to gain some understanding of how the choice of hyperparameters impacts upon the perfor-
mance. In the second set of experiments, we make comparison of the performance among the convolu-
tional DNN learned features and various traditional features. The goal of the third set of experiments is
to evaluate the effectiveness of each extracted feature.
5.1 Parameter Settings
In this section, we experimentally study the effects of the three parameters in our proposed method:
the window size in the convolutional component w, the number of hidden layer 1, and the number of
hidden layer 2. Because there is no official development dataset, we tuned the hyperparameters by trying
different architectures via 5-fold cross-validation.
In Figure 3, we respectively vary the number of hyper parameters w, n
1
and n
2
and compute the F1.
We can see that it does not improve the performance when the window size is greater than 3. Moreover,
because the size of our training dataset is limited, the network is prone to overfitting, especially when
using large hidden layers. From Figure 3, we can see that the parameters have a limited impact on the
results when increasing the numbers of both hidden layers 1 and 2. Because the distance dimension has
little effect on the result (this is not illustrated in Figure 3), we heuristically choose d
e
= 5. Finally,
the word dimension and learning rate are the same as in Collobert et al. (2011). Table 2 reports all the
hyperparameters used in the following experiments.
Hyperparameter Window size Word dim. Distance dim. Hidden layer 1 Hidden layer 2 Learning rate
Value w = 3 n = 50 d
e
= 5 n
1
= 200 n
2
= 100 ? = 0.01
Table 2: Hyperparameters used in our experiments.
5.2 Results of Comparison Experiments
To obtain the final performance of our automatically learned features, we select seven approaches as com-
petitors to be compared with our method in Table 3. The first five competitors are described in Hendrickx
et al. (2010), all of which use traditional features and employ SVM or MaxEnt as the classifier. These
systems design a series of features and take advantage of a variety of resources (WordNet, ProBank,
and FrameNet, for example). RNN represents recursive neural networks for relation classification, as
8
The corpus contains a Perl-based automatic evaluation tool.
2341
Classifier Feature Sets F1
SVM POS, stemming, syntactic patterns 60.1
SVM word pair, words in between 72.5
SVM POS, stemming, syntactic patterns, WordNet 74.8
MaxEnt POS, morphological, noun compound, thesauri, Google n-grams, WordNet 77.6
SVM POS, prefixes, morphological, WordNet, dependency parse, Levin classed, ProBank,
FrameNet, NomLex-Plus, Google n-gram, paraphrases, TextRunner
82.2
RNN - 74.8
POS, NER, WordNet 77.6
MVRNN - 79.1
POS, NER, WordNet 82.4
Proposed word pair, words around word pair, WordNet 82.7
Table 3: Classifier, their feature sets and the F1-score for relation classification.
proposed by Socher et al. (2012). This method learns vectors in the syntactic tree path that connect two
nominals to determine their semantic relationship. The MVRNN model builds a single compositional
semantics for the minimal constituent, including both nominals as RNN (Socher et al., 2012). It is almost
certainly too much to expect a single fixed transformation to be able to capture the meaning combination
effects of all natural language operators. Thus, MVRNN assigns a matrix to every word and modifies the
meanings of other words instead of only considering word embeddings in the recursive procedure.
Table 3 illustrates the macro-averaged F1 measure results for these competing methods along with the
resources, features and classifier used by each method. Based on these results, we make the following
observations:
(1) Richer feature sets lead to better performance when using traditional features. This improvement
can be explained by the need for semantic generalization from training to test data. The quality of
traditional features relies on human ingenuity and prior NLP knowledge. It is almost impossible to
manually choose the best feature sets.
(2) RNN and MVRNN contain feature learning procedures; thus, they depend on the syntactic tree used
in the recursive procedures. Errors in syntactic parsing inhibit the ability of these methods to learn
high quality features. RNN cannot achieve a higher performance than the best method that uses
traditional features, even when POS, NER and WordNet are added to the training dataset. Compared
with RNN, the MVRNN model can capture the meaning combination effectively and achieve a higher
performance.
(3) Our method achieves the best performance among all of the compared methods. We also perform
a t-test (p 6 0.05), which indicates that our method significantly outperforms all of the compared
methods.
5.3 The Effect of Learned Features
Feature Sets F1
Lexical L1 34.7
+L2 53.1
+L3 59.4
+L4 65.9
+L5 73.3
Sentence WF 69.7
+PF 78.9
Combination all 82.7
Table 4: Score obtained for various sets of features on for the test set. The bottom portion of the table
shows the best combination of lexical and sentence level features.
In our method, the network extract lexical and sentence level features. The lexical level features pri-
marily contain five sets of features (L1 to L5). We performed ablation tests on the five sets of features
from the lexical part of Table 4 to determine which type of features contributed the most. The results are
2342
presented in Table 4, from which we can observe that our learned lexical level features are effective for
relation classification. The F1-score is improved remarkably when new features are added. Similarly, we
perform experiment on the sentence level features. The system achieves approximately 9.2% improve-
ments when adding PF. When all of the lexical and sentence level features are combined, we achieve the
best result.
6 Conclusion
In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence
level features for relation classification. In the network, position features (PF) are successfully proposed
to specify the pairs of nominals to which we expect to assign relation labels. The system obtains a
significant improvement when PF are added. The automatically learned features yield excellent results
and can replace the elaborately designed features that are based on the outputs of existing NLP tools.
Acknowledgments
This work was sponsored by the National Basic Research Program of China (No. 2014CB340503) and
the National Natural Science Foundation of China (No. 61272332, 61333018, 61202329, 61303180).
This work was supported in part by Noah?s Ark Lab of Huawei Tech. Co. Ltd. We thank the anonymous
reviewers for their insightful comments.
References
Nguyen Bach and Sameer Badaskar. 2007. A review of relation extraction. Literature review for Language and
Statistics II.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In
Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language
Processing, pages 724?731.
Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zhengyu Niu. 2005. Unsupervised feature selection for relation
extraction. In Proceedings of the International Joint Conference on Natural Language Processing, pages 262?
267.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
Zellig Harris. 1954. Distributional structure. Word, 10(23):146?162.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman. 2004. Discovering relations among named entities from
large corpora. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages
415?422.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama. 2013. Simple customization
of recursive neural networks for semantic relation classification. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing, pages 1372?1376.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid
?
O. S?eaghdha, Sebastian Pad?o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification
of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic
Evaluation, SemEval ?10, pages 33?38.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based
weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies - Volume 1, pages 541?
550.
Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for
extracting relations. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics
on Interactive poster and demonstration sessions.
2343
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP: Volume 2, pages 1003?1011.
Raymond J Mooney and Razvan C Bunescu. 2005. Subsequence kernels for relation extraction. In Advances in
neural information processing systems, pages 171?178.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu, and Peide Qian. 2008. Exploiting constituent depen-
dencies for tree kernel-based semantic relation extraction. In Proceedings of the 22nd International Conference
on Computational Linguistics, pages 697?704.
Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without
labeled text. In Proceedings of the 2010 European conference on Machine learning and knowledge discovery
in databases: Part III, pages 148?163.
Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 1201?1211.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard Weikum. 2006. Combining linguistic and statistical analysis
to extract relations from web documents. In Proceedings of the 12th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 712?717.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for
relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
Long Papers - Volume 1, pages 721?729.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394.
Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. The
Journal of Machine Learning Research, 3:1083?1106.
GuoDong Zhou, Su Jian, Zhang Jie, and Zhang Min. 2005. Exploring various knowledge in relation extraction.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 427?434.
2344
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1346?1356, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Opinion Target Extraction Using Word-Based Translation Model 
 
Kang Liu, Liheng Xu, Jun Zhao 
 
National Laboratory of Pattern Recognition,  
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China 
{kliu, lhxu, jzhao}@nlpr.ia.ac.cn  
 
 
Abstract 
This paper proposes a novel approach to 
extract opinion targets based on word-
based translation model (WTM). At first, 
we apply WTM in a monolingual scenario 
to mine the associations between opinion 
targets and opinion words. Then, a graph-
based algorithm is exploited to extract 
opinion targets, where candidate opinion 
relevance estimated from the mined 
associations, is incorporated with candidate 
importance to generate a global measure. 
By using WTM, our method can capture 
opinion relations more precisely, especially 
for long-span relations. In particular, 
compared with previous syntax-based 
methods, our method can effectively avoid 
noises from parsing errors when dealing 
with informal texts in large Web corpora. 
By using graph-based algorithm, opinion 
targets are extracted in a global process, 
which can effectively alleviate the problem 
of error propagation in traditional 
bootstrap-based methods, such as Double 
Propagation. The experimental results on 
three real world datasets in different sizes 
and languages show that our approach is 
more effective and robust than state-of-art 
methods. 
1 Introduction 
With the rapid development of e-commerce, most 
customers express their opinions on various kinds 
of entities, such as products and services. These 
reviews not only provide customers with useful 
information for reference, but also are valuable for 
merchants to get the feedback from customers and 
enhance the qualities of their products or services. 
Therefore, mining opinions from these vast 
amounts of reviews becomes urgent, and has 
attracted a lot of attentions from many researchers.  
In opinion mining, one fundamental problem is 
opinion target extraction. This task is to extract 
items which opinions are expressed on. In reviews, 
opinion targets are usually nouns/noun phrases. 
For example, in the sentence of ?The phone has a 
colorful and even amazing screen?, ?screen? is an 
opinion target. In online product reviews, opinion 
targets often are products or product features, so 
this task is also named as product feature 
extraction in previous work (Hu et al 2004; Ding 
et al 2008; Liu et al 2005; Popescu et al 2005; 
Wu et al 2005; Su et al 2008).  
To extract opinion targets, many studies 
regarded opinion words as strong indicators (Hu et 
al., 2004; Popescu et al 2005; Liu et al 2005; 
Qiu et al 2011; Zhang et al 2010), which is 
based on the observation that opinion words are 
usually located around opinion targets, and there 
are associations between them. Therefore, most 
pervious methods iteratively extracted opinion 
targets depending upon the associations between 
opinion words and opinion targets (Qiu et al 2011; 
Zhang et al 2010). For example, ?colorful? and 
?amazing? is usually used to modify ?screen? in 
reviews about cell phone, so there are strong 
associations between them. If ?colorful? and 
?amazing? had been known to be opinion words, 
?screen? is likely to be an opinion target in this 
domain. In addition, the extracted opinion targets 
can be used to expand more opinion words 
according to their associations. It?s a mutual 
reinforcement procedure. 
Therefore, mining associations between opinion 
targets and opinion words is a key for opinion 
1346
target extraction (Wu et al 2009). To this end, 
most previous methods (Hu et al 2004; Ding et al 
2004; Wang et al 2008), named as adjacent 
methods, employed the adjacent rule, where an 
opinion target was regarded to have opinion 
relations with the surrounding opinion words in a 
given window. However, because of the limitation 
of window size, opinion relations cannot be 
captured precisely, especially for long-span 
relations, which would hurt estimating associations 
between opinion targets and opinion words. To 
resolve this problem, several studies exploited 
syntactic information such as dependency trees 
(Popescu et al 2005; Qiu et al 2009; Qiu et al 
2011; Wu et al 2009; Zhang et al 2010). If the 
syntactic relation between an opinion word and an 
opinion target satisfied a designed pattern, then 
there was an opinion relation between them. 
Experiments consistently reported that syntax-
based methods could yield better performance than 
adjacent methods for small or medium corpora 
(Zhang et al 2010). The performance of syntax-
based methods heavily depends on the parsing 
performance. However, online reviews are often 
informal texts (including grammar mistakes, typos, 
improper punctuations etc.). As a result, parsing 
may generate many mistakes. Thus, for large 
corpora from Web including a great deal of 
informal texts, these syntax-based methods may 
suffer from parsing errors and introduce many 
noises. Furthermore, this problem maybe more 
serious on non-English language reviews, such as 
Chinese reviews, because that the performances of 
parsing on these languages are often worse than 
that on English. 
To overcome the weakness of the two kinds of 
methods mentioned above, we propose a novel 
unsupervised approach to extract opinion targets 
by using word-based translation model (WTM). 
We formulate identifying opinion relations 
between opinion targets and opinion words as a 
word alignment task. We argue that an opinion 
target can find its corresponding modifier through 
monolingual word alignment. For example in 
Figure 1, the opinion words ?colorful? and 
?amazing? are aligned with the target ?screen? 
through word alignment. To this end, we use WTM 
to perform monolingual word alignment for mining 
associations between opinion targets and opinion 
words. In this process, several factors, such as 
word co-occurrence frequencies, word positions 
etc., can be considered globally. Compared with 
adjacent methods, WTM doesn?t identify opinion 
relations between words in a given window, so 
long-span relations can be effectively captured 
(Liu et al 2009). Compared with syntax-based 
methods, without using parsing, WTM can 
effectively avoid errors from parsing informal texts. 
So it will be more robust. In addition, by using 
WTM, our method can capture the ?one-to-many? 
or ?many-to-one? relations (?one-to-many? means 
that, in a sentence one opinion word modifies 
several opinion targets, and ?many-to-one? means 
several opinion words modify one opinion target). 
Thus, it?s reasonable to expect that WTM is likely 
to yield better performance than traditional 
methods for mining associations between opinion 
targets and opinion words.  
Based on the mined associations, we extract 
opinion targets in a ranking framework. All 
nouns/noun phrases are regarded as opinion target 
candidates. Then a graph-based algorithm is 
exploited to assign confidences to each candidate, 
in which candidate opinion relevance and 
importance are incorporated to generate a global 
measure. At last, the candidates with higher ranks 
are extracted as opinion targets. Compared with 
most traditional methods (Hu et al2004; Liu et al 
2005; Qiu et al 2011), we don?t extract opinion 
targets iteratively based on the bootstrapping 
strategy, such as Double Propagation (Qiu et al 
2011), instead all candidates are dynamically 
ranked in a global process. Therefore, error 
propagation can be effectively avoided and the 
performance can be improved.  
 
 Figure 1: Word-based translation model for 
opinion relation identification 
The main contributions of this paper are as 
follows. 
1) We formulate the opinion relation 
identification between opinion targets and 
opinion words as a word alignment task. To 
our best knowledge, none of previous methods 
deal with this task using monolingual word 
alignment model (in Section 3.1). 
Translation 
The phone has a colorful and even amazing screen 
The phone has a colorful and even amazing screen 
1347
2) We propose a graph-based algorithm for 
opinion target extraction in which candidate 
opinion relevance and importance are 
incorporated into a unified graph to estimate 
candidate confidence. Then the candidates 
with higher confidence scores are extracted as 
opinion targets (in Section 3.2). 
3) We have performed experiments on three 
datasets in different sizes and languages. The 
experimental results show that our approach 
can achieve performance improvement over 
the traditional methods. (in Section 4). 
The rest of the paper is organized as follows. In 
the next section, we will review related work in 
brief. Section 3 describes our approach in detail. 
Then experimental results will be given in Section 
4. At the same time, we will give some analysis 
about the results. Finally, we give the conclusion 
and the future work. 
2 Related Work 
Many studies have focused on the task of opinion 
target extraction, such as (Hu et al 2004; Ding et 
al., 2008; Liu et al 2006; Popescu et al 2005; 
Wu et al 2005; Wang et al 2008; Li et al 2010; 
Su et al 2008; Li et al 2006). In general, the 
existing approaches can be divided into two main 
categories: supervised and unsupervised methods. 
In supervised approaches, the opinion target 
extraction task was usually regarded as a sequence 
labeling task (Jin et al2009; Li et al2010; Wu et 
al., 2009; Ma et al2010; Zhang et al 2009). Jin et 
al. (2009) proposed a lexicalized HMM model to 
perform opinion mining. Li et al(2010) proposed 
a Skip-Tree CRF model for opinion target 
extraction. Their methods exploited three 
structures including linear-chain structure, 
syntactic structure, and conjunction structure. In 
addition, Wu et al(2009) utilized a SVM classifier 
to identify relations between opinion targets and 
opinion expressions by leveraging phrase 
dependency parsing. The main limitation of these 
supervised methods is that labeling training data 
for each domain is impracticable because of the 
diversity of the review domains.  
In unsupervised methods, most approaches 
regarded opinion words as the important indicators 
for opinion targets (Hu et al 2004; Popsecu et al 
2005; Wang et al 2008; Qiu et al 2011; Zhang et 
al., 2010). The basic idea was that reviewers often 
use the same opinion words when they comment 
on the similar opinion targets. The extraction 
procedure was often a bootstrapping process which 
extracted opinion words and opinion targets 
iteratively, depending upon their associations. 
Popsecu et al(2005) used syntactic patterns to 
extract opinion target candidates. After that they 
computed the point-wise mutual information (PMI) 
score between a candidate and a product category 
to refine the extracted results. Hu et al(2004) 
exploited an association rule mining algorithm and 
frequency information to extract frequent explicit 
product features. The adjective nearest to the 
frequent explicit feature was extracted as an 
opinion word. Then the extracted opinion words 
were used to extract infrequent opinion targets. 
Wang et al(2008) adopted the similar idea, but 
their method needed a few seeds to weakly 
supervise the extraction process. Qiu et al(2009, 
2011) proposed a Double Propagation method to 
expand a domain sentiment lexicon and an opinion 
target set iteratively. They exploited direct 
dependency relations between words to extract 
opinion targets and opinion words iteratively. The 
main limitation of Qiu?s method is that the patterns 
based on dependency parsing tree may introduce 
many noises for the large corpora (Zhang et al 
2010). Meanwhile, Double Propagation is a 
bootstrapping strategy which is a greedy process 
and has the problem of error propagation. Zhang et 
al. (2010) extended Qiu?s method. Besides the 
patterns used in Qiu?s method, they adopted some 
other patterns, such as phrase patterns, sentence 
patterns and ?no? pattern, to increase recall. In 
addition they used the HITS (Klernberg et al 1999) 
algorithm to compute the feature relevance scores, 
which were simply multiplied by the log of feature 
frequencies to rank the extracted opinion targets. In 
this way, the precision of result can be improved.  
3 Opinion Target Extraction Using 
Word-Based Translation Model 
3.1 Method Framework 
As mentioned in the first section, our approach for 
opinion target extraction is composed of the 
following two main components:  
1) Mining associations between opinion targets 
and opinion words: Given a collection of 
reviews, we adopt a word-based translation 
1348
model to identify potential opinion relations in 
all sentences, and then the associations 
between opinion targets and opinion words are 
estimated.  
2) Candidate confidence estimation: Based on 
these associations, we exploit a graph-based 
algorithm to compute the confidence of each 
opinion target candidate. Then the candidates 
with higher confidence scores are extracted as 
opinion targets.  
3.2 Mining associations between opinion 
targets and opinion words using Word-
based Translation Model 
This component is to identify potential opinion 
relations in sentences and estimate associations 
between opinion targets and opinion words. We 
assume opinion targets and opinion words 
respectively to be nouns/noun phrases and 
adjectives, which have been widely adopted in 
previous work (Hu et al 2004; Ding et al 2008; 
Wang et al 2008; Qiu et al 2011). Thus, our aim 
is to find potential opinion relations between 
nouns/noun phrases and adjectives in sentences, 
and calculate the associations between them. As 
mentioned in the first section, we formulate 
opinion relation identification as a word alignment 
task. We employ the word-based translation model 
(Brown et al1993) to perform monolingual word 
alignment, which has been widely used in many 
tasks, such as collocation extraction (Liu et al 
2009), question retrieval (Zhou et al 2011) and so 
on. In our method, every sentence is replicated to 
generate a parallel corpus, and we apply the 
bilingual word alignment algorithm to the 
monolingual scenario to align a noun/noun phase 
with its modifier. 
Given a sentence with n words 
1 2{ , ,..., }nS w w w? , the word alignment 
{( , ) | [1, ]}iA i a i n? ? can be obtained by 
maximizing the word alignment probability of the 
sentence as follows. 
?=arg max ( | )
A
A P A S
                   (1) 
where ( , )ii a  means that a noun/noun phrase at 
positioni  is aligned with an adjective at position ia . 
If we directly use this alignment model to our task, 
a noun/noun phrase may align with the irrelevant 
words other than adjectives, like prepositions or 
conjunctions and so on. Thus, in the alignment 
procedure, we introduce some constrains: 1) 
nouns/noun phrases (adjectives) must be aligned 
with adjectives (nouns/noun phrases) or null words; 
2) other words can only align with themselves. 
Totally, we employ the following 3 WTMs (IBM 
1~3) to identify opinion relations. 
1
1
( | ) ( | )j
n
IBM j a
j
P A S t w w?
?
??
 
2
1
( | ) ( | ) ( | , )j
n
IBM j a j
j
P A S t w w d j a n?
?
??
 
3
1 1
( | ) ( | ) ( | ) ( | , )j
n n
IBM i i j a j
i j
P A S n w t w w d j a n??
? ?
?? ?
(2) 
There are three main factors: ( | )jj at w w
, 
( | , )jd j a n
and ( | )i in w? , which respectively 
models different information.  
1) ( | )jj at w w
models the co-occurrence 
information of two words in corpora. If an 
adjective co-occurs with a noun/noun phrase 
frequently in the reviews, this adjective has high 
association with this noun/noun phrase. For 
example, in reviews of cell phone, ?big? often co-
occurs with ?phone?s size?, so ?big? has high 
association with ?phone?s size?. 
2) ( | , )jd j a l
 models word position information, 
which describes the probability of a word in 
position 
ja aligned with a word in position j .  
3) ( | )i in w? models the fertility of words, which 
describe the ability of a word for ?one-to-many? 
alignment. 
i? denotes the number of words that are 
aligned with 
iw . For example, ?Iphone4 has 
amazing screen and software?. In this sentence, 
?amazing? is used to modify two words: ?screen? 
and ?software?. So? equals to 2 for ?amazing?.  
Therefore, in Eq. (2), 
1( | )IBMP A S?  only models 
word co-occurrence information. 
2 ( | )IBMP A S?  
additionally employs word position information. 
Besides these two information, 
3( | )IBMP A S?  
considers the ability of a word for ?one-to-many? 
alignment. In the following experiments section, 
we will discuss the performance difference among 
these models in detail. Moreover, these models 
1349
may capture ?one-to-many? or ?many-to-one? 
opinion relations (mentioned in the first section). 
In our knowledge, it isn?t specifically considered 
by previous methods including adjacent methods 
and syntax-based methods. Meanwhile ? the 
alignment results may contain empty-word 
alignments, which means a noun/noun phrase has 
no modifier or an adjective modify nothing in the 
sentence. 
After gathering all word pairs from the review 
sentences, we can estimate the translation 
probabilities between nouns/noun phrases and 
adjectives as follows. 
( , )( | ) ( )
N A
N A
A
Count w wp w w Count w?
           (3) 
where ( | )N Ap w w means the translation 
probabilities from adjectives to nouns/noun 
phrases. Similarly, we can obtain translation 
probability ( | )A Np w w . Therefore, similar to (Liu 
et al2009), the association between a noun/noun 
phrase and an adjective is estimated as follows. 
1| |
( , )
( ( ) (1 ) ( ))
N A
N NA A
Association w w
t p w w t p w w ?? ? ?
    (4) 
where t is the harmonic factor to combine these 
two translation probabilities. In this paper, we set 
0.5t ? . For demonstration, we give some 
examples in Table 1. We can see that our method 
using WTM can successfully capture associations 
between opinion targets and opinion words. 
 battery life sound software 
wonderful 0.000 0.042 0.000 
poor 0.032 0.000 0.026 
long 0.025 0.000 0.000 
Table 1: Examples of associations between opinion 
targets and opinion words. 
3.3 Candidate Confidence Estimation 
In this component, we compute the confidence of 
each opinion target candidate and rank them. The 
candidates with higher confidence are regarded as 
the opinion targets. We argue that the confidence 
of a candidate is determined by two factors: 1) 
Opinion Relevance; 2) Candidate Importance. 
Opinion Relevance reflects the degree that a 
candidate is associated to opinion words. If an 
adjective has higher confidence to be an opinion 
word, the noun/noun phrase it modifies will have 
higher confidence to be an opinion target. 
Similarly, if a noun/noun phrase has higher 
confidence to be an opinion target, the adjective 
which modifies it will be highly possible to be an 
opinion word. It?s an iterative reinforcement 
process, which indicates that existing graph-based 
algorithms are applicable.  
Candidate Importance reflects the salience of a 
candidate in the corpus. We assign an importance 
score to an opinion target candidate f according to 
its -tf idf score, which is further normalized by the 
sum of -tf idf scores of all candidates. 
- ( )( )
- ( )
c
tf idf cImportance c
tf idf c
??
              (5) 
where c represents a candidate, tf is the term 
frequency in the dataset, and df is computed by 
using the Google n-gram corpus1. 
To model these two factors, a bipartite graph is 
constructed, the vertices of which include all 
nouns/noun phrases and adjectives. As shown in 
Figure 2, the white vertices represent nouns/noun 
phrases and the gray vertices represent adjectives. 
An edge between a noun/noun phrase and an 
adjective represents that there is an opinion 
relation between them. The weight on the edges 
represents the association between them, which are 
estimated by using WTM, as shown in Eq. (4).  
To estimate the confidence of each candidate on 
this bipartite graph, we exploit a graph-based 
algorithm, where we use C to represent candidate 
confidence vector, a 1n? vector. We set the 
candidate initial confidence with candidate 
importance score, i.e. 0C S? , where S is the 
candidate initial confidence vector and each item 
in S is computed using Eq. (5). 
 
 
Figure 2: Bipartite graph for modeling relations 
between opinion targets and opinion words 
                                                          
1 http://books.google.com/ngrams/datasets 
..... 
..... 
Opinion Word Candidates (adjectives) 
Opinion Target Candidates (nouns/noun phrases) 
1350
Then we compute the candidate confidence by 
using the following iterative formula. 
1t T tC M M C? ? ? ?                  (6) 
where tC is the candidate confidence vector at 
time t , and 1tC ?  is the candidate confidence 
vector at time 1t ? . M is an opinion relevance 
matrix, a m n? matrix, where ,i jM is the 
associated weight between a noun/noun phrase 
i and an adjective j . 
To consider the candidate importance scores, we 
introduce a reallocate condition: combining the 
candidate opinion relevance with the candidate 
importance at each step. Thus we can get the final 
recursive form of the candidate confidence as 
follows. 
1 (1 )t T tC M M C S? ?? ? ? ? ? ? ? ?       (7) 
where [0,1]?? is the proportion of candidate 
importance in the candidate confidence. When 
1? ? , the candidate confidence is completely 
determined by the candidate importance; and when 
0? ? , the candidate confidence is determined by 
the candidate opinion relevance. We will discuss 
its effect in the section of experiments.  
To solve Eq. (7), we rewrite it as the following 
form. 
1( (1 ) )TC I M M S? ? ?? ? ? ? ? ? ?        (8) 
where I is an identity matrix. To handle the 
inverse of the matrix, we expand the Eq. (8) as a 
power series as following. 
[ ]kC I B B S?? ? ? ? ? ?              (9) 
where (1 ) TB M M?? ? ? ?and [0, )k? ? is an 
approximate factor. In experiments, we set 
100k ? . Using this equation, we estimate 
confidences for opinion target candidates. The 
candidates with higher confidence scores than the 
threshold will be extracted as the opinion targets.  
4 Experiments 
4.1 Datasets and Evaluation Metrics 
In our experiments, we select three real world 
datasets to evaluate our approach. The first dataset 
is COAE2008 dataset22, which contains Chinese 
reviews of four different products. The detailed 
                                                          
2 http://ir-china.org.cn/coae2008.html 
information can be seen in Table 2. Moreover, to 
evaluate our method comprehensively, we collect a 
larger collection named by Large, which includes 
three corpora from three different domains and 
different languages. The detailed statistical 
information of this dataset is also shown in Table 2. 
Restaurant is crawled from the Chinese Web site: 
www.dianping.com. The Hotel and MP3 3  were 
used in (Wang et al 2011), which are respectively 
clawed from www.tripadvisor.com and 
www.amazon.com. For each collection, we 
perform random sampling to generate testing 
dataset, which include 6,000 sentences for each 
domain. Then the opinion targets in Large were 
manually annotated as the gold standard for 
evaluations. Three annotators are involved in the 
annotation process as follows. First, every 
noun/noun phrase and its contexts in review 
sentences are extracted. Then two annotators were 
required to judge whether every noun/noun phrase 
is opinion target or not. If a conflict happens, a 
third annotator will make judgment for finial 
results. The inter-agreement was 0.72. In total, we 
respectively obtain 1,112, 1,241 and 1,850 opinion 
targets in Hotel, MP3 and Restaurant. The third 
dataset is Customer Review Datasets 4  (English 
reviews of five products), which was also used in 
(Hu et al 2004; Qiu et al 2011). They have 
labeled opinion targets. The detailed information 
can be found in (Hu et al 2004).  
 
Domain Language #Sentence #Reviews 
Camera Chinese 2075 137 
Car Chinese 4783 157 
Laptop Chinese 1034 56 
Phone Chinese 2644 123 
(a) COAE2008 dataset2 
Domain Language #Sentence #Reviews 
Hotel English 1,855,351 185,829 
MP3 English 289,931 30,837 
Restaurant Chinese 1,683,129 395,124 
(b) Large 
Table 2: Experimental Data Sets, # denotes the size 
of the reviews/sentences 
In experiments, each review is segmented into 
sentences according to punctuations. Then 
sentences are tokenized and the part-of-speech of 
                                                          
3 http://sifaka.cs.uiuc.edu/~wang296/Data/index.html 
4 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
1351
Methods 
Camera Car Laptop Phone 
P R F P R F P R F P R F 
Hu 0.63 0.65 0.64 0.62 0.58 0.60 0.51 0.67 0.58 0.69 0.60 0.64 
DP 0.71 0.70 0.70 0.72 0.65 0.68 0.58 0.69 0.63 0.78 0.66 0.72 
Zhang 0.71 0.78 0.74 0.69 0.68 0.68 0.57 0.80 0.67 0.80 0.71 0.75 
Ours 0.75 0.81 0.78 0.71 0.71 0.71 0.61 0.85 0.71 0.83 0.74 0.78 
Table 3: Experiments on COAE2008 dataset2 
Methods 
Hotel MP3 Restaurant 
P R F P R F P R F 
Hu 0.60  0.65  0.62  0.61  0.68  0.64  0.64  0.69  0.66  
DP 0.67  0.69  0.68  0.69  0.70  0.69  0.74  0.72  0.73  
Zhang 0.67  0.76  0.71  0.67  0.77  0.72  0.75  0.79  0.77  
Ours 0.71  0.80  0.75  0.70  0.82  0.76  0.80  0.84  0.82  
Table 4: Experiments on Large 
Methods 
D1 D2 D3 D4 D5 
P R F P R F P R F P R F P R F 
Hu 0.75  0.82  0.78  0.71  0.79  0.75  0.72  0.76  0.74  0.69  0.82  0.75  0.74  0.80  0.77  
DP 0.87  0.81  0.84  0.90  0.81  0.85  0.90  0.86  0.88  0.81  0.84  0.82  0.92  0.86  0.89  
Zhang 0.83  0.84  0.83  0.86  0.85  0.85  0.86  0.88  0.87  0.80  0.85  0.83  0.86  0.86  0.86  
Ours 0.84  0.85  0.84  0.87  0.85  0.86  0.88  0.89  0.88  0.81  0.85  0.83  0.89  0.87  0.88  
Table 5: Experiments on Customer Review Dataset 
each word is assigned. Stanford NLP tool5 is used 
to perform POS-tagging and dependency parsing. 
The method in (Zhu et al 2009) is used to identify 
noun phrases. We select precision, recall and F-
measure as the evaluation metrics. We also 
perform a significant test, i.e., a t-test with a 
default significant level of 0.05. 
4.2 Our Methods vs. State-of-art Methods 
To prove the effectiveness of our method, we 
select the following state-of-art unsupervised 
methods as baselines for comparison. 
1) Hu is the method described in (Hu et al 2004), 
which extracted opinion targets by using adjacent 
rule.  
2) DP is the method described in (Qiu et al 2011), 
which used Double Propagation algorithm to 
extract opinion targets depending on syntactic 
relations between words.  
3) Zhang is the method described in (Zhang et al 
2010), which is an extension of DP. They extracted 
opinion targets candidates using syntactic patterns 
and other specific patterns. Then HITS (Kleinberg 
1999) algorithm combined with candidate 
frequency is employed to rank the results for 
opinion target extraction.  
Hu is selected to represent adjacent methods for 
opinion target extraction. And DP and Zhang are 
                                                          
5 http://nlp.stanford.edu/software/tagger.shtml 
selected to represent syntax-based methods. The 
parameter settings in these three baselines are the 
same as the original papers. In special, for DP and 
Zhang, we used the same patterns for different 
language reviews. The overall performance results 
are shown in Table 3, 4 and 5, respectively, where 
?P? denotes precision, ?R? denotes recall and ?F? 
denotes F-measure. Ours denotes full model of our 
method, in which we use IBM-3 model for 
identifying opinion relations between words. 
Moreover, we set
max 2? ? in Eq. (2) and 0.3? ? in 
Eq. (7). From results, we can make the following 
observations. 
1) Ours achieves performance improvement over 
other methods. This indicates that our method 
based on word-based translation model is 
effective for opinion target extraction.  
2) The graph-based methods (Ours and Zhang) 
outperform the methods using Double 
Propagation (DP). Similar observations have 
been made by Zhang et al(2010). The reason 
is that graph-based methods extract opinion 
targets in a global framework and they can 
effectively avoid the error propagation made 
by traditional methods based on Double 
Propagation. Moreover, Ours outperforms 
Zhang. We believe the reason is that Ours 
consider the opinion relevance and the 
candidate importance in a unified graph-based 
framework. By contrast, Zhang only simply 
1352
plus opinion relevance with frequency to 
determine the candidate confidence. 
3) In Table 4, the improvement made by Ours on 
Restaurant (Chinese reviews) is larger than 
that on Hotel and MP3 (English reviews). The 
same phenomenon can be found when we 
compare the improvement made by Ours in 
Table 3 (Chinese reviews) with that in Table 5 
(English reviews). We believe that reason is 
that syntactic patterns used in DP and Zhang 
were exploited based on English grammar, 
which may not be suitable to Chinese language. 
Moreover, another reason is that the 
performance of parsing on Chinese texts is not 
better than that on English texts, which will 
hurt the performance of syntax-based methods 
(DP and Zhang).  
4) Compared the results in Table 3 with the 
results in Table 4, we can observe that Ours 
obtains larger improvements with the increase 
of the data size. This indicates that our method 
is more effective for opinion target extraction 
than state-of-art methods, especially for large 
corpora. When the data size increase, the 
methods based on syntactic patterns will 
introduce more noises due to the parsing errors 
on informal texts. On the other side, Ours uses 
WTM other than parsing to identify opinion 
relations between words, and the noises made 
by inaccurate parsing can be avoided. Thus, 
Ours can outperform baselines. 
5) In Table 5, Ours makes comparable results 
with baselines in Customer Review Datasets, 
although there is a little loss in precision in 
some domains. We believe the reason is that 
the size of Customer Review Datasets is too 
small. As a result, WTM may suffer from data 
sparseness for association estimation. 
Nevertheless, the average recall is improved. 
An Example In Table 6, we show top 10 opinion 
targets extracted by Hu, DP, Zhang and Ours in 
MP3 of Large. In Hu and DP, since they didn?t 
rank the results, their results are ranked according 
to frequency in this experiment. The errors are 
marked in bold face. From these examples, we can 
see Ours extracts more correct opinion targets than 
others. In special, Ours outperforms Zhang. It 
indicates the effectiveness of our graph-based 
method for candidate confidence estimation. 
Moreover, Ours considers candidate importance 
besides opinion relevance, so some specific 
opinion targets are ranked to the fore, such as 
?voice recorder?, ?fm radio? and ?lcd screen?.  
4.3 Effect of Word-based Translation Model 
In this subsection, we aim to prove the 
effectiveness of our WTM for estimating 
associations between opinion targets and opinion 
words. For comparison, we select two baselines for 
comparison, named as Adjacent and Syntax. These 
baselines respectively use adjacent rule (Hu et al
2004; Wang et al 2008) and syntactic patterns 
(Qiu et al 2009) to identify opinion relations in 
sentences. Then the same method (Eq.3 and Eq.4) 
is used to estimate associations between opinion 
targets and opinion words. At last the same graph-
based method (in Section 3.3) is used to extract 
opinion targets. Due to the limitation of the space, 
the experimental results only on COAE2008 
dataset2 and Large are shown in Figure 3. 
 
 
Figure 3: Experimental comparison among 
different relation identification methods 
 
Hu quality, thing, drive, feature, battery, sound, 
time, music, price 
DP quality, battery, software, device, screen, file, 
thing, feature, battery life 
Zhang quality, size, battery life, hour, version, function, 
upgrade, number, music 
Ours quality, battery life, voice recorder, video, fm 
radio, battery, file system, screen, lcd screen 
Table 6: Top 10 opinion targets extracted by 
different methods. 
In Figure 3, we observe that Ours using WTM 
makes significant improvements compared with 
1353
two baselines, both on precision and recall. It 
indicates that WTM is effective for identifying 
opinion relations, which makes the estimation of 
the associations be more precise. 
4.4 Effect of Our Graph-based Method 
In this subsection, we aim to prove the 
effectiveness of our graph-based method for 
opinion target extraction. We design two baselines, 
named as WTM_DP and WTM_HITS. Both 
WTM_DP and WTM_HITS use WTM to mine 
associations between opinion targets and opinion 
words. Then, WTM_DP uses Double Propagation 
adapted in (Wang et al2008; Qiu et al2009) to 
extract opinion targets, which only consider the 
candidate opinion relevance. WTM_HITS uses a 
graph-based method of Zhang et al(2010) to 
extract opinion targets, which consider both 
candidate opinion relevance and frequency. Figure 
4 gives the experimental results on COAE2008 
dataset2 and Large. In Figure 4, we can observe 
that our graph-based algorithm outperforms not 
only the method based on Double Propagation, but 
also the previous graph-based approach.  
 
 
Figure 4: Experimental Comparison between 
different ranking algorithms 
4.5 Parameter Influences 
4.5.1 Effect of Different WTMs 
In section 3, we use three different WTMs in Eq. 
(2) to identify opinion relations. In this subsection, 
we make comparison among them. Experimental 
results on COAE2008 dataset2 and Large are 
shown in Figure 5. Ours_1, Ours_2 and Ours_3 
respectively denote our method using different 
WTMs (IBM 1~3). From the results in Figure 5, 
we observe that Ours_2 outperforms Ours_1, 
which indicates that word position is useful for 
identifying opinion relations. Furthermore, Ours_3 
outperforms other models, which indicates that 
considering the fertility of a word can produce 
better performance. 
4.5.2 Effect of ?  
In our method, when we employ Eq. (7) to assign 
confidence score to each candidate, 
[0,1]?? decides the proportion of candidate 
importance in our method. Due to the limitation of 
space, we only show the F-measure of Ours on 
COAE2008 dataset2 and Large when varying ? in 
Figure 6.  
In Figure 6, curves increase firstly, and decrease 
with the increase of ? . The best performance is 
obtained when ? is around 0.3. It indicates that 
candidate importance and candidate opinion 
relevance are both important for candidate 
confidence estimation. The performance of opinion 
target extraction benefits from their combination. 
 
 
 
Figure 5. Experimental results by using different 
word-based translation model. 
 
 
Figure 6. Experimental results when varying ?  
1354
5 Conclusions and Future Work 
This paper proposes a novel graph-based approach 
to extract opinion targets using WTM. Compared 
with previous adjacent methods and syntax-based 
methods, by using WTM, our method can capture 
opinion relations more precisely and therefore be 
more effective for opinion target extraction, 
especially for large informal Web corpora.  
In future work, we plan to use other word 
alignment methods, such as discriminative model 
(Liu et al 2010) for this task. Meanwhile, we will 
add some syntactic information into WTM to 
constrain the word alignment process, in order to 
identify opinion relations between words more 
precisely. Moreover, we believe that there are 
some verbs or nouns can be opinion words and 
they may be helpful for opinion target extraction. 
And we think that it?s useful to add some prior 
knowledge of opinion words (sentiment lexicon) in 
our model for estimating candidate opinion 
relevance. 
Acknowledgements 
The work is supported by the National Natural 
Science Foundation of China (Grant No. 
61070106), the National Basic Research Program 
of China (Grant No. 2012CB316300), Tsinghua 
National Laboratory for Information Science and 
Technology (TNList) Cross-discipline Foundation 
and the Opening Project of Beijing Key Laboratory 
of Internet Culture and Digital Dissemination 
Research (Grant No. 5026035403). We thank the 
anonymous reviewers for their insightful 
comments. 
 
References  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311.  
Xiaowen Ding, Bing Liu and Philip S. Yu. 2008. A 
Holistic Lexicon-Based Approach to Opinion Mining. 
In Proceedings of WSDM 2008. 
Xiaowen Ding and Bing Liu. 2010. Resolving Object 
and Attribute Reference in Opinion Mining. In 
Proceedings of COLING 2010. 
Mingqin Hu and Bing Liu. 2004. Mining and 
Summarizing Customer Reviews. In Proceedings of 
KDD 2004 
Minqing Hu and Bing Liu. 2004. Mining Opinion 
Features in Customer Reviews. In Proceedings of 
AAAI-2004, San Jose, USA, July 2004. 
Wei Jin and Huang Hay Ho. A Novel Lexicalized 
HMM-based Learning Framework for Web Opinion 
Mining. In Proceedings of ICML 2009. 
Jon Klernberg. 1999. Authoritative Sources in 
Hyperlinked Environment. Journal of the ACM 46(5): 
604-632 
Zhuang Li, Feng Jing, Xiao-yan Zhu. 2006. Movie 
Review Mining and Summarization. In Proceedings 
of CIKM 2006 
Fangtao Li, Chao Han, Minlie Huang and Xiaoyan Zhu. 
2010. Structure-Aware Review Mining and 
Summarization. In Proceedings of COLING 2010. 
Zhichao Li, Min Zhang, Shaoping Ma, Bo Zhou, Yu 
Sun. Automatic Extraction for Product Feature 
Words from Comments on the Web. In Proceedings 
of AIRS 2009.  
Bing Liu, Hu Mingqing and Cheng Junsheng. 2005. 
Opinion Observer: Analyzing and Comparing 
Opinions on the Web. In Proceedings of WWW 2005 
Bing Liu. 2006. Web Data Mining: Exploring 
Hyperlinks, contents and usage data. Springer, 2006 
Bing Liu. 2010. Sentiment analysis and subjectivity. 
Handbook of Natural Language Processing, second 
edition, 2010. 
Yang Liu, Qun Liu, and Shouxun Lin. 2010. 
Discriminative word alignment by linear modeling. 
Computational Linguistics, 36(3):303?339. 
Zhanyi Liu, Haifeng Wang, Hua Wu and Sheng Li. 
2009. Collocation Extraction Using Monolingual 
Word Alignment Model. In Proceedings of EMNLP 
2009.  
Tengfei Ma and Xiaojun Wan. 2010. Opinion Target 
Extraction in Chinese News Comments. In 
Proceedings of COLING 2010. 
Popescu, Ana-Maria and Oren, Etzioni. 2005. 
Extracting produt fedatures and opinions from 
reviews. In Proceedings of EMNLP 2005 
Guang Qiu, Bing Liu., Jiajun Bu and Chun Che. 2009. 
Expanding Domain Sentiment Lexicon through 
Double Popagation. In Proceedings of IJCAI 2009 
Guang Qiu, Bing Liu, Jiajun Bu and Chun Chen. 2011. 
Opinion Word Expansion and Target Extraction 
1355
through Double Propagation. Computational 
Linguistics, March 2011, Vol. 37, No. 1: 9.27 
Qi Su, Xinying Xu., Honglei Guo, Zhili Guo, Xian Wu, 
Xiaoxun Zhang, Bin Swen and Zhong Su. 2008. 
Hidden Sentiment Association in Chinese Web 
Opinion Mining. In Proceedings of WWW 2008 
Bo Wang, Houfeng Wang. Bootstrapping both Product 
Features and Opinion Words from Chinese Customer 
Reviews with Cross-Inducing. In Proceedings of 
IJCNLP 2008. 
Hongning Wang, Yue Lu and Chengxiang Zhai. 2011. 
Latent Aspect Rating Analysis without Aspect 
Keyword Supervision. In Proceedings of KDD 2011. 
Yuanbin Wu, Qi Zhang, Xuangjing Huang and Lide 
Wu, 2009, Phrase Dependency Parsing For Opinion 
Mining, In Proceedings of EMNLP 2009 
Lei Zhang, Bing Liu, Suk Hwan Lim and Eamonn 
O?Brien-Strain. 2010. Extracting and Ranking 
Product Features in Opinion Documents. In 
Proceedings of COLING 2010. 
Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara, 
Joseph Johnson, Xuanjing Huang. 2009. Mining 
Product Reviews Based on Shallow Dependency 
Parsing, In Proceedings of SIGIR 2009.  
Guangyou Zhou, Li Cai, Jun Zhao and Kang Liu. 2011. 
Phrase-based Translation Model for Question 
Retrieval in Community Question Answer Archives. 
In Proceedings of ACL 2011. 
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou and 
Muhua Zhu. 2009. Multi-aspect Opinion Polling 
from Textual Reviews. In Proceedings of CIKM 
2009. 
1356
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1092?1103,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Question Answering over Linked Data Using First-order Logic
?
Shizhu He, Kang Liu, Yuanzhe Zhang, Liheng Xu and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{shizhu.he, kliu, yzzhang, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Question Answering over Linked Data
(QALD) aims to evaluate a question an-
swering system over structured data, the
key objective of which is to translate
questions posed using natural language
into structured queries. This technique
can help common users to directly ac-
cess open-structured knowledge on the
Web and, accordingly, has attracted much
attention. To this end, we propose a
novel method using first-order logic. We
formulate the knowledge for resolving
the ambiguities in the main three steps
of QALD (phrase detection, phrase-to-
semantic-item mapping and semantic item
grouping) as first-order logic clauses in a
Markov Logic Network. All clauses can
then produce interacted effects in a unified
framework and can jointly resolve all am-
biguities. Moreover, our method adopts a
pattern-learning strategy for semantic item
grouping. In this way, our method can
cover more text expressions and answer
more questions than previous methods us-
ing manually designed patterns. The ex-
perimental results using open benchmarks
demonstrate the effectiveness of the pro-
posed method.
1 Introduction
With the rapid development of the Web of Data,
many RDF datasets have been published as Linked
Data (Bizer et al., 2009), such as DBpedia (Auer
et al., 2007), Freebase (Bollacker et al., 2008)
and YAGO (Suchanek et al., 2007). The grow-
ing amount of Linked Data contains a wealth of
knowledge, including entities, classes and rela-
tions. Moreover, these linked data usually have
?
Shizhu He and Kang Liu have equal contribution to this
work.
complex structures and are highly heterogeneous.
As a result, there are gaps for users regarding ac-
cess. Although a few experts can write queries us-
ing structured languages (such as SPARQL) based
on their needs, this skill cannot be easily utilized
by common users (Christina and Freitas, 2014).
Thus, providing user-friendly, simple interfaces
to access these linked data becomes increasingly
more urgent.
Because of this, question answering over linked
data (QALD) (Walter et al., 2012) has recently
received much interest, and most studies on this
topic have focused on translating natural lan-
guage questions into structured queries (Freitas
and Curry, 2014; Yahya et al., 2012; Unger et al.,
2012; Shekarpour et al., 2013; Yahya et al., 2013;
Bao et al., 2014; Zou et al., 2014). For example,
with respect to the question
?Which software has been developed by organi-
zations founded in California, USA??,
the aim is to automatically convert this utterance
into an SPARQL query that contains the follow-
ing subject-property-object (SPO) triple format:
??url rdf:type dbo:Software, ?url dbo:developer ?x1,
?x1 rdf:type dbo:Company, ?x1 dbo:foundationPlace
dbr:California?
1
.
To fulfill this objective, existing systems (Lopez
et al., 2006; Unger et al., 2012; Yahya et al., 2012;
Zou et al., 2014) usually adopt a pipeline frame-
work that contains four major steps: 1) decompos-
ing the question and detecting phrases, 2) map-
ping the detected phrases into semantic items of
Linked Data, 3) grouping the mapped semantic
items into semantic triples, and 4) generating the
correct SPARQL query.
However, completing these four steps and con-
structing such a structured query is not easy. The
first three steps mentioned above are subject to the
1
The prefixes in semantic items indicate the source of
their vocabularies.
1092
problem of ambiguity, which is the major chal-
lenge in QALD. Using the question mentioned
above as an example, we can choose Califor-
nia or California, USA when detecting phrases,
the phrase California can be mapped to the en-
tity California State or California Film, and the class
Software (mapped from the phrase software) can
be matched with the first argument of the rela-
tion producer or developer (these two relations can
be mapped from the phrase developed). Previ-
ous methods (Lopez et al., 2006; Lehmann et
al., 2012; Freitas and Curry, 2014) have usu-
ally performed disambiguation at each step only,
and the subsequent step was performed based on
the disambiguation results in the previous step(s).
However, we argue that the three steps men-
tioned above have mutual effects. In the previ-
ous example, the phrase founded in (verb) can
be mapped to the entities (Founding of Rome and
Founder (company)), classes (Company and Depart-
ment) or relations (foundedBy and foundationPlace).
If we know that the phrase California can refer
to the entity California State, and which can be the
second argument of the relation foundationPlace,
together with a verb phrase being more likely
to be mapped to Relation, we should map the
phrase founded in to foundationPlace in this ques-
tion. Thus, we aim to determine if joint disam-
biguation is better than individual disambigua-
tion. (Question One)
In addition, previous systems usually employed
manually designed patterns to extract predicate-
argument structures that are used to guide the dis-
ambiguation process in the three steps mentioned
above (Yahya et al., 2012; Unger et al., 2012; Zou
et al., 2014). For example, (Yahya et al., 2012)
used only three dependency patterns to group the
mapped semantic items into semantic triples. Nev-
ertheless, these three manually designed patterns
miss many cases because of the diversity of the
question expressions. We gathered statistics on
144 questions and found that the macro-average
F1 and micro-average F1 of the three patterns
2
used in (Yahya et al., 2012) are only 62.8 and
66.2%, respectively. Furthermore, these specially
designed patterns may not be valid with variations
in domains or languages. Therefore, another im-
portant question arises: can we automatically
learn rules or patterns to achieve the same ob-
2
They are 1) verbs and their arguments, 2) adjectives and
their arguments and 3) propositionally modified tokens and
objects of prepositions.
jective? (Question Two)
Focusing on the two problems mentioned
above, this paper proposes a novel algorithm based
on a learning framework, Markov Logic Networks
(MLNs) (Richardson and Domingos, 2006), to
learn a joint model for constructing structured
queries from natural language utterances. MLN
is a statistical relational learning framework that
combines first-order logic and Markov networks.
The appealing property of MLN is that it is read-
ily interpretable by humans and that it is a natural
framework for performing joint learning. We for-
mulate the knowledge for resolving the ambigui-
ties in the main three steps of QALD (phrase de-
tection, phrase-to-semantic-item mapping and se-
mantic item grouping) as first-order logic clauses
in an MLN. In the framework of MLN, all clauses
will produce interacted effects that jointly resolve
all problems into a unified process. In this way,
the result in each step can be globally optimized.
Moreover, in contrast to previous methods, we
adopt a learning strategy to automatically learn
the patterns for semantic item grouping. We de-
sign several meta patterns as opposed to the spe-
cific patterns. In addition, these meta patterns are
formulated as the first-order logic formulas in the
MLN. The specific patterns can be generated by
these meta patterns based on the training data. The
model will learn the weights of each clause to de-
termine the most effective patterns for semantic
triple construction. In this way, with little effort,
our approach can cover more semantic expressions
and answer more questions than previous meth-
ods, which depend on manually designed patterns.
We evaluate the proposed method using several
benchmarks (QALD-1, QALD-3, QALD-4). The
experimental results demonstrate the advantage of
the joint disambiguation process mentioned above.
They also prove that our approach, employing
MLN to automatically learn the patterns of seman-
tic triple grouping, is effective. Our system can
answer more questions and obtain better perfor-
mance than the traditional methods based on man-
ually designed heuristic rules.
2 Background
2.1 Linked Data Sources
Linked Data consist of many relational data,
which are usually inter-linked as subject-property-
object (SPO) triple statements (such as using the
owl:sameAs relation). In this paper, we mainly use
1093
Subject(Arg1) Relation(Property) Object(Arg2) 
ProgrammingLanguage subClassOf Software 
Java_(programming_language) type Software 
Java_(programming_language) developer Oracle_Corporation 
Oracle_Corporation foundationPlace California_(State) 
foundationPlace domain Organisation 
California_(State) label ?California? 
California_(1977_film) label ?California? 
Oracle_Corporation numEmployees 118119(xsd:integer) 
 
Figure 1: Sample knowledge base facts.
DBpedia
3
and some classes from Yago
4
. These
knowledge bases (KBs) are composed of many on-
tological and instance statements, and all state-
ments are expressed by SPO triple facts. Figure
1 shows some triple fact samples from DBpedia.
Each fact is composed of three semantic items. A
semantic item can be an entity (California (State),
Oracle Corporation, etc.), a class (Software, Organ-
isation, etc.) or a relation (called a property
or predicate in some occasions). Some entities
are literals including strings, numbers and dates
(118119(xsd:integer), etc.). Relations contain stan-
dard Semantic Web relations (subClassOf, type, do-
main and label) and ontological relations (developer,
foundationPlace and numEmployees).
2.2 Task Statement
Given a knowledge base (KB), our objective is to
translate a natural language question q
NL
into a
formal language query q
FL
that targets the seman-
tic vocabularies given by the KB, and the query
q
FL
should capture the user information needs ex-
pressed by q
NL
.
Following (Yahya et al., 2012), we focus on the
factoid questions, and the answers to such ques-
tions are an entity or a set of entities. We ignore
the questions that need the aggregation
5
(max/min,
etc.) and negation operations. That is, we generate
queries that consist of a plentiful number of triple
patterns, which are multiple conjunctions of SPO
search conditions.
3 Framework
Figure 2 shows the entire framework of our system
for translating a question into a formal SPARQL
query. The first three steps address the input ques-
tion through 1) Phrase Detection (detecting pos-
sible phrases), 2) Phrase Mapping (mapping all
3
http://dbpedia.org/
4
http://www.mpi-inf.mpg.de/yago-naga/yago/
5
We can address the count query questions, which will
be explained in Section 3.
phrase candidates to the corresponding seman-
tic items), and 3) Feature Extraction (extracting
the linguistic features and semantic item features
from the question and the Linked Data, respec-
tively). As a result, a space of candidates is con-
structed, including possible phrases, mapped se-
mantic items and the possible argument match re-
lations among them. Next, the fourth step (In-
ference) formulates the joint disambiguation as a
generalized inference task. We employ rich fea-
tures and constraints (including hard and soft con-
straints) to infer a joint decision through an MLN.
Finally, with the inference results, we can con-
struct a semantic item query graph and generate
an executable SPARQL query. In the following
subsection, we demonstrate each step in detail.
1) Phrase detection. In this step, we detect
phrases (sequences of tokens) that probably indi-
cate semantic items in the KB. We do not use a
named entity recognizer (NER) because of its low
coverage. We perform testing on two commonly
used question corpora, QALD-3 and free917
6
, us-
ing the Stanford NER tool
7
. The results demon-
strate that only 51.5 and 23.8% of the NEs are
correctly recognized, respectively. To avoid miss-
ing useful phrases, we retain all n-grams as phrase
candidates, and then use some rules to filter them.
The rules include the following: the span length
must be less than 4 (accepting that all contiguous
tokens are capitalizations), the POS tag of the start
token must be jj, nn, rb and vb, all contiguous
capitalization tokens must not be split, etc. For
instance, software, developed by, organizations,
founded in and California are detected in the ex-
ample of the first section.
2) Phrase mapping. After the phrases are de-
tected, each phrase can be mapped to the corre-
sponding semantic item in KB (entity, class and
relation). For example, software is mapped to
dbo:Software, dbo:developer, etc., and California is
mapped to dbr:California, dbr:California (wine), etc.
For different types of semantic items, we use dif-
ferent techniques. For mapping phrases to en-
tities, considering that the entities in DBpedia
and Wikipedia are consistent, we employ anchor,
redirection and disambiguation information from
Wikipedia. For mapping phrases to classes, con-
sidering that classes have lexical variation, espe-
cially synonyms, e.g., dbo:Film can be mapped
6
http://www.cis.temple.edu/?yates/open-sem-
parsing/index.html
7
http://nlp.stanford.edu/software/CRF-NER.shtml
1094
Which software has been developed by
organizations founded in California, USA? software, developed, developed by, organizations,
founded, founded in, California, USA
software
developed by
...
...
...
California
phraseIndex
phrasePosTag
resourceType
priorMatchScore
hasMeanWord
phraseDepTag
hasRelatedness
...
isTypeCompatible
hasPhrase hasResource
hasRelation
Figure 2: Framework of our system.
from film, movie and show, we compute the simi-
larity between the phrase and the class in the KB
with the word2vec tool
8
. The word2vec tool com-
putes fixed-length vector representations of words
with a recurrent-neural-network based language
model (Mikolov et al., 2010). The similarity scor-
ing methods are introduced in Section 4.2. Then,
the top-N most similar classes for each phrase are
returned. For mapping phrases to relations, we
employ the resources from PATTY (Nakashole et
al., 2012) and ReVerb (Fader et al., 2011). Specif-
ically, we first compute the associations between
the ontological relations in DBpedia and the re-
lation patterns in PATTY and ReVerb through in-
stance alignments as in (Berant et al., 2013). Next,
if a detected phrase is matched to some relation
pattern, the corresponding ontological relations in
DBpedia will be returned as a candidate. This step
only generates candidates for every possible map-
ping, and the decision of the best selection will be
performed in the next step.
3) Feature extraction and joint inference.
There exist ambiguities in phrase detection and in
mapping phrases to semantic items. This step fo-
cuses on addressing these ambiguities and deter-
8
https://code.google.com/p/word2vec/
mining the argument match relations among the
mapped semantic items. This is the core compo-
nent of our system, and it performs disambigua-
tion in a unified manner. First, feature extraction
is performed to prepare a rich number of features
from the input question and from the KB. Next,
the disambiguation is performed in a joint fashion
with a Markov Logic Network. Detailed informa-
tion will be presented in Section 4.
4) Semantic item query graph construction.
Based on the inference results, we construct a
query graph. The vertices contain the following:
the detected phrase, the token span indexes of
the phrases, the mapped semantic items and their
types. The edge indicates the argument match re-
lation between two semantic items. For example,
we use 1 2 to indicate that the first argument of
an item matches the second argument of another
item
9
. The right bottom in Figure 2 shows an ex-
ample of this.
5) Query generation. The SPARQL queries
require the grouped triples of semantic items.
Thus, in this step, we convert a query graph
into multiple joined semantic triples. Three in-
terconnected semantic items, whereby it must
9
The other marks will be introduced in Section 4.2.
1095
be ensured that the middle item is a rela-
tion, are converted into a semantic triple (mul-
tiple joined facts containing variables). For
example, the query graph Vdbo:Book[Class] 1 2
??
dbo:author[Relation] 1 1
??
dbr:Danielle Steel[Entity]W is
converted into ??x rdf:type dbo:Book, dbr:Danielle
dbo:author ?x?, and Vdbo:populationTotal[Relation]
1 2
??
dbo:capital[Relation] 1 1
??
dbr:Australia[Entity]W
10
is
converted into ??x1 dbo:populationTotal ?answer, ?x1
dbo:capital dbr:Australia?. If the query graph only
contains one vertex that indicates a class ClassURI,
we generate ??x rdf:type ClassURI?. If the query
graph only contains two connected vertexes, we
append a variable to bind the missing match argu-
ment of the semantic item.
The final SPARQL query is constructed by join-
ing the semantic item triples based on the cor-
responding SPARQL template. We divide the
questions into three types: Yes/No, Normal and
Number. Yes/No questions use the ASK WHERE
template. Normal questions use the SELECT ?url
WHERE template. Number questions first use the
normal question template, and if they cannot ob-
tain a correct answer (a valid numeric value), we
use the SELECT COUNT(?url) WHERE template to
generate a query again. For instance, we construct
the SPARQL query SELECT(?url) WHERE{ ?url
rdf:type dbo:Software. ?url dbo:developer ?x1. ?x1 rdf:type
dbo:Company. ?x1 dbo:foundationPlace dbr:California.}
for this example.
4 Joint Disambiguation with MLN
In this section, we present our method for ques-
tion answering over linked data using a Markov
Logic Network (MLN). In the following subsec-
tions, we first briefly describe the MLN. Then, we
present the predicates and the first-order logic for-
mulas used in the model.
4.1 Markov Logic Networks
Markov logic networks combine Markov networks
with first-order logic in a probabilistic framework
(Richardson and Domingos, 2006). An MLNM
consists of several weighted formulas {(?
i
, w
i
)}
i
,
where ?
i
is a first order formula and w
i
is the
penalty (the formula?s weight). In contrast to
the first-order logic, whereby a formula repre-
sents a hard constraint, these logic formulas are
relaxed and can be violated with penalties in the
10
This corresponds to the question ?How many people live
in the capital of Australia??
MLN. Each formula ?
i
consists of a set of first-
order predicates, logical connectors and variables.
These weighted formulas define a probability dis-
tribution over a possible world. Let y denote a pos-
sible world. Then p(y) is defined as follows:
p(y) =
1
Z
exp
?
?
?
(?
i
,w
i
)?M
w
i
?
c?C
n
?
i
f
?
i
c
(y)
?
?
,
where each c is a binding of the free variables in
?
i
to constants; f
?
i
c
is a binary feature function
that returns 1 if the ground formula that we ob-
tain through replacing the free variables in ?
i
with
the constants in c under the given possible world
y is true and is 0 otherwise; and C
n
?
i
is the set of
all possible bindings for the free variables in ?
i
.
Z is a normalized constant. The Markov network
corresponds to this distribution, where nodes rep-
resent ground atoms and factors represent ground
formulas.
4.2 Predicates
In the MLN, we design several predicates to re-
solve the ambiguities in phrase detection, map-
ping phrases to semantic items and semantic item
grouping. Specifically, we design a hidden pred-
icate hasPhrase(i) to indicate that the i-th candi-
date phrase has been chosen. The predicate hasRe-
source(i,j) indicates that the i-th phrase is mapped
to the j-th semantic item. The predicate hasRe-
lation(j,k,rr) indicates that the j-th semantic item
and the k-th semantic item should be grouped to-
gether with the argument-match-type rr. Note that
we define four argument match types between two
semantic items: 1 1, 1 2, 2 1 and 2 2. Here, the
argument match type t s denotes that the t-th argu-
ment of the first semantic item corresponds to the
s-th argument of the second semantic item
11
. The
detailed illustration is shown in Table 1.
Type Example Question
1 1 dbo:height 1 1 dbr:Michael Jordan How tall is Michael Jor-
dan?
1 2 dbo:River 1 2 dbo:crosses Which river does the
Brooklyn Bridge cross?
2 1 dbo:creator 2 1 dbr:Walt Disney Which television shows
were created by Walt
Disney?
2 2 dbo:birthPlace 2 2 dbo:capital Which actors were born in
the capital of American?
Table 1: Examples of the argument match types.
11
The 2-nd argument is corresponding to the object argu-
ment of the relation, and the 1-st argument is corresponding
with the subject argument of the relation and the entity (in-
cluding the class) itself.
1096
Describing the attributes of phrases and relation between two phrases
phraseIndex(p, i, j) The start and end position of phrase p in question.
phrasePosTag(p, pt) The POS tag of the head word in phrase p.
phraseDepTag(p, q, dt) The dependency path tags between phrase p and q.
phraseDepOne (p, q) If there is only one tag in the dependency path, the predicate is true.
hasMeanWord (p, q) If there is any one meaning word in the dependency path of two phrases, the predicate is true.
Describing the attributes of semantic item and the mappings between phrases and semantic items
resourceType(r, rt) The type of semantic item r. Types of semantic items include Entity, Class and Relation
priorMatchScore(p, r, s) The prior score of phrase p mapping to semantic item r.
Describing the attributes of relation between two semantic items in a knowledge base
hasRelatedness(p, q, s) The semantic coherence of semantic items.
isTypeCompatible(p, q, rr) If the semantic items p are type-compatible with the semantic items q, the predicate is true.
hasQueryResult(s, p, o, rr1, rr2) If the triple pattern consisting of semantic items s, p, o and argument-match-types rr1 and rr2 have query
results, the predicate is true.
Table 2: Descriptions of observed predicates.
Moreover, we define a set of observed predi-
cates to describe the properties of phrases, seman-
tic items, relations between phrases and relations
between semantic items. The observed predicates
and descriptions are shown in Table 2.
Previous methods usually designed some
heuristic patterns to group semantic items, which
usually employed a human-designed syntactic
path between two phrases to determine their re-
lations. In contrast, we collect all the tokens in
the dependency path between two phrases as pos-
sible patterns. The predicates phraseDepTag and
hasMeanWord are designed to indicate the possi-
ble patterns. Note that if these tokens only contain
POS tags dt|in|wdt|to|cc|ex|pos|wp or stop words,
the value of the predicate hasMeanWord is false;
otherwise, it is true. In this way, our system is ex-
pected to cover more question expressions. More-
over, the SPARQL endpoint is used to verify the
type compatibility of two semantic items and if
one triple pattern can obtain query results.
The predicate hasRelatedness needs to compute
the coherence score between two semantic items.
Following (Yahya et al., 2012), we use the Jaccard
coefficient (Jaccard, 1908) based on the inlinks be-
tween two semantic items.
The predicate priorMatchScore assigns a prior
score when mapping a phrase to a semantic item.
We use different methods to compute this score
according to different semantic item types. For
entities, we use a normalized score based on the
frequencies of a phrase referring to an entity.
For classes and relations, we use different meth-
ods. We first define the following three similar-
ity metrics: a) s
1
: The Levenshtein distance score
(Navarro, 2001) between the labels of the seman-
tic item and the phrase; b) s
2
: The word embed-
ding (Mikolov et al., 2010) score, which measures
the similarity between two phrases and is the max-
imum cosine value of the words? word embed-
dings between two phrases; and c) s
3
: the instance
overlap score, which is computed using the Jac-
card coefficient of the instance overlap. All scores
are normalized to produce a comparable scores
in the interval of (0, 1). The final prior scores
for mapping phrases to classes and relations are
?s
1
+ (1? ?)s
2
and ?s
1
+ ?s
2
+ (1? ?? ?)s
3
,
respectively. The parameters are set to empirical
values
12
.
4.3 Formulas
According to these predicates, we design several
first-order logic formulas for joint disambiguation.
As mentioned in the first section, these formulas
represent the meta patterns. The concrete pat-
terns can be generated through these meta pat-
terns with training data. Specifically, we use two
types of formulas for the joint decisions: Boolean
and Weighted formulas. Boolean formulas are
hard constraints, which must be satisfied by all
of the ground atoms in the final inference results.
Weighted formulas are soft constraints, which can
be violated with some penalties.
4.3.1 Boolean Formulas (Hard Constraints)
Table 3 lists the Boolean formulas used in this
work. The ? ? notation in the formulas indicates
an arbitrary constant. The ?|f |? notation expresses
the number of true grounded atoms in the formula
f . These formulas express the following con-
straints:
hf1: If a phrase is chosen, then it must have a
mapped semantic item;
hf2: If a semantic item is chosen, then its mapped
phrase must be chosen;
hf3: A phrase can be mapped to at most one se-
mantic item;
hf4: If the phrase is not chosen, then its mapped
12
Set ? to 0.6 for Class and set ? and ? to 0.3 and 0.3 for
Relation, respectively.
1097
hf1 hasPhrase(p)? hasResource(p, )
hf2 hasResource(p, )? hasPhrase(p)
hf3 |hasResource(p, )| ? 1
hf4 !hasPhrase(p)?!hasResource(p, r)
hf5 hasResource( , r)? hasRelation(r, , ) ? hasRelation( , r, )
hf6 |hasRelation(r1, r2, )| ? 1
hf7 hasRelation(r1, r2, )? hasResource( , r1) ? hasResource( , r2)
hf8 phraseIndex(p1, s1, e1) ? phraseIndex(p2, s2, e2) ? overlap(s1, e1, s2, e2) ? hasPhrase(p1)?!hasPhrase(p2)
hf9 resourceType(r, ?Entity?)?!hasRelation(r, , ?2 1?) ? !hasRelation(r, , ?2 2?)
hf10 resourceType(r, ?Entity?)?!hasRelation( , r, ?2 1?) ? !hasRelation(r, , ?2 2?)
hf11 resourceType(r, ?Class?)?!hasRelation(r, , ?2 1?) ? !hasRelation(r, , ?2 2?)
hf12 resourceType(r, ?Class?)?!hasRelation( , r, ?2 1?) ? !hasRelation(r, , ?2 2?)
hf13 !isTypeCompatible(r1, r2, rr)?!hasRelation(r1, r2, rr)
Table 3: Descriptions of Boolean formulas.
sf1 priorMatchScore(p, r, s)? hasPhrase(p)
sf2 priorMatchScore(p, r, s)? hasResource(p)
sf3 phrasePosTag(p, pt+) ? resourceType(r, rt+)? hasResource(p, r)
sf4 phraseDepTag(p1, p2, dp+) ? hasResource(p1, r1) ? hasResource(p2, r2)? hasRelation(r1, r2, rr+)
sf5 phraseDepTag(p1, p2, dp+) ? hasResource(p1, r1) ? hasResource(p2, r2)?!hasMeanWord(p1, p2) ?
hasRelation(r1, r2, rr+)
sf6 phraseDepTag(p1, p2, dp+) ? hasResource(p1, r1) ? hasResource(p2, r2) ? phraseDepOne(p1, p2) ?
hasRelation(r1, r2, rr+)
sf7 hasRelatedness(r1, r2, s) ? hasResource( , r1) ? hasResource( , r2)? hasRelation(r1, r2, )
sf8 hasQueryResult(r1, r2, r3, rr1, rr2)? hasRelation(r1, r2, rr1) ? hasRelation(r2, r3, rr2)
Table 4: Descriptions of weighted formulas.
semantic item should not be chosen;
hf5: If a semantic item is chosen, then it should
have at least one argument match relation with
other semantic items;
hf6: Two semantic items have at most one argu-
ment match relation;
hf7: If an argument match relation for two seman-
tic items is chosen, then they must be chosen;
hf8: Each of two chosen phrases must not overlap;
hf9, hf10, hf11, hf12: The semantic item with
type Entity and Class should not have a second ar-
gument that matches with others;
hf13: The chosen argument match relation for two
sematic items must be type compatible.
4.3.2 Weighted Formulas (Soft Constraints)
Table 4 lists the weighted formulas used in this
work. The ?+? notation in the formulas indicates
that each constant of the logic variable should be
weighted separately. Those formulas express the
following properties in joint decisions:
sf1, sf2: The larger the score of the phrase map-
ping to a semantic item, the more likely the cor-
responding phrase and semantic item should been
chosen;
sf3: There are some associations between the POS
tags of phase and the types of mapped semantic
items;
sf4, sf5, sf6: There are some associations be-
tween the dependency tags in the dependency pat-
tern path of two phases and the types of argument
match relations of two mapped semantic items;
sh7: The larger the relatedness of two seman-
tic items, the more likely they have an argument
match relation;
sf8: If the triple pattern has query results, these se-
mantic items should have corresponding argument
match relations.
5 Experiments
5.1 Dataset & Evaluation Metrics
We use the following three collections of questions
from the QALD
13
task for question answering
over linked data: QALD-1, QALD-3 and QALD-
4. The generated SPARQL queries are evaluated
on Linked Data from DBpedia and YAGO using
a Virtuoso engine
14
. A typical example question
from the QALD benchmark is ?Which books writ-
ten by Kerouac were published by Viking Press??.
As mentioned in Section 2.2, our system is not de-
signed to answer questions that contain numbers,
date comparisons and aggregation operations such
as group by or order by. Therefore, we remove
these types of questions and retain 110 questions
from the QALD-4 training set for generating the
specific formulas and for training their weights in
MLN. We test our system using 37, 75 and 26
questions from the training set of QALD-1
15
, and
the testing set of QALD-3 and QALD-4 respec-
tively. We use #T, #Q and #A to indicate the total
13
www.sc.cit-ec.uni-bielefeld.de/qald/
14
https://github.com/openlink/virtuoso-opensource
15
We use the training set because we try to make a fair
comparison with (Yahya et al., 2012).
1098
number of questions in the testing set, the num-
ber of questions we could address and the number
of questions answered correct, respectively. We
select Precision (P =
#A
#Q
), Recall (R =
#A
#T
),
and F1-score (F1 =
2?P ?R
P+R
) as the evaluation met-
rics. To assess the effectiveness of the disambigua-
tion process in the MLN, we computed the overall
quality measures by precision and recall with the
manually obtained results.
5.2 Experimental Configurations
The Stanford dependency parser (De Marneffe et
al., 2006) is used for extracting features from the
dependency parse trees. We use the toolkit the-
beast
16
to learn the weights of the formulas and
to perform the MAP inference. The inference al-
gorithm uses a cutting plane approach. In addi-
tion, for the parameter learning, we set all ini-
tial weights to zero and use an online learning
algorithm with MIRA update rules to update the
weights of the formulas. The number of iterations
for the training and testing are set to 10 and 200,
respectively.
5.3 Results and Discussion
5.3.1 The Effect of Joint Learning
To demonstrate the advantages of our joint learn-
ing, we design a pipeline system for compari-
son, which independently performs phrase detec-
tion, phrase mapping, and semantic item grouping
by removing the unrelated formulas in MLN. For
example, the formulas
17
related to the predicates
hasResource and hasRelation are removed when
detecting phrases in questions.
Table 5 shows the results, where Joint de-
notes the proposed method with joint inference
and Pipeline denotes the compared method per-
forming each step independently. We perform a
comparison with the question answering results of
QALD (QA), and comparisons at each of the fol-
lowing steps: PD (phrase detection), PM (phrase
mapping) and MG (mapped semantic items group-
ing). From the results, we observe that our method
answers over half of the questions. Moreover, our
joint model based on MLN can obtain better per-
formance in question answering compared to the
pipeline system. We also observe that Joint ex-
hibits better performance than Pipeline in most
steps, except for MG in QALD-3. We believe this
16
http://code.google.com/p/thebeast
17
including entire formulas, excluding hf8 and sf1
is because the three tasks (phrase detection, phrase
mapping, and semantic item grouping) are con-
nected with each other. Each step can provide use-
ful information for the other two tasks. Therefore,
performing joint inference can effectively improve
the performance. Finally, we observe that the for-
mer task usually produces better results than the
subsequent tasks (phrase detection exhibits a bet-
ter performance than phrase mapping, and phrase
mapping exhibits a better performance than se-
mantic item grouping). The main reason is that
the latter subtask is more complex than the former
task. The decisions of the latter subtask strongly
rely on the former results even though they have
interacted effects.
5.3.2 The Effect of Pattern Learning
Table 6 shows a comparison of our system with
DEANNA (Yahya et al., 2012), which is based
on a joint disambiguation model but which em-
ploys hand-written patterns in its system. Because
DEANNA only reports its results of the QALD-1
dataset, we do not show the results for QALD-3
and QALD-4 for equity. From the results, we can
see that our system solved more questions and ex-
hibited a better performance than did DEANNA.
One of the greatest strengths of our system is that
the learning system can address more questions
than hand-written pattern rules.
System #T #Q #A P R F1
DEANNA (Yahya et al., 2012) 50 27 13 0.48 0.26 0.33
Ours 50 37 20 0.54 0.4 0.46
Table 6: Comparisons with DEANNA using the
QALD-1 test questions.
Compared to the ILP (Integer Linear Program-
ming) used in (Yahya et al., 2012) for joint disam-
biguation, we argue that there are two major dif-
ferences to our method. 1) Our method is a data-
driven approach that can learn effective patterns
or rules for the task. Therefore, it exhibits more
robustness and adaptability for various KBs. 2)
We design several meta rules in MLN as opposed
to specific ones. The specific rules can be gen-
erated by these meta rules based on the training
data. By contrast, the traditional approach using
ILP needs to set specific rules in advance, which
requires more intensive labor than our approach.
To further illustrate the effectiveness of our
pattern-learning strategy, we show the weights of
the learned patterns corresponding to formula sf3
in the MLN, as shown in Table 7. From the table,
1099
Benchmark
PD PM MG QA
P R F1 P R F1 P R F1 #T #Q #A P R F1
QALD-1(Joint) 0.93 0.981 0.955 0.895 0.944 0.919 0.703 0.813 0.754 50 37 20 0.54 0.4 0.46
QALD-1(Pipeine) 0.921 0.972 0.946 0.868 0.917 0.892 0.585 0.859 0.696 50 34 17 0.5 0.34 0.41
QALD-3(Joint) 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 99 75 45 0.6 0.46 0.52
QALD-3(Pipeline) 0.912 0.912 0.912 0.829 0.867 0.848 0.677 0.789 0.729 99 75 42 0.56 0.42 0.48
QALD-4(Joint) 0.947 0.978 0.963 0.937 0.967 0.952 0.776 0.865 0.817 50 26 15 0.58 0.3 0.4
QALD-4(Pipeline) 0.937 0.967 0.952 0.905 0.935 0.920 0.683 0.827 0.748 50 24 13 0.54 0.26 0.35
Table 5: The performance of joint learning on three benchmark datasets.
we can see that nn
18
is more likely mapped to En-
tity
19
than to Class and Relation, and vb is most
likely mapped to Relation. This proves that our
model can learn effective and reasonable patterns
for QALD.
POS tag of Phrase type of mapped Item Weight
nn Entity 2.11
nn Class 0.243
nn Relation 0.335
vb Relation 0.517
wp Class 0.143
wr Class 0.025
Table 7: Sample weights of formulas, correspond-
ing with formula sf3.
5.3.3 Comparison to the state of the art
To illustrate the effectiveness of the proposed
method, we perform comparisons to the state-of-
the-art methods. Table 8 shows the results using
QALD-3 and QALD-4. These systems are the
participants in the QALD evaluation campaigns.
From the results, we can see that our system out-
performs most systems at a competitive perfor-
mance. They further prove the effectiveness of the
proposed method.
Test set System #T #Q #A P R F1
QALD-3
CASIA (He et al.,
2013)
99 52 29 0.56 0.3 0.38
Scalewelis (Joris
and Ferr?e, 2013)
99 70 32 0.46 0.32 0.38
RTV (Cristina et
al., 2013)
99 55 30 0.55 0.3 0.39
Intui2 (Corina,
2013)
99 99 28 0.28 28 0.28
SWIP (Pradel et al.,
2013)
99 21 15 0.71 0.15 0.25
Ours 99 75 45 0.6 0.46 0.52
QALD-4
20
gAnswer 50 25 16 0.64 0.32 0.43
Intui3 50 33 10 0.30 0.2 0.24
ISOFT 50 50 10 0.2 0.2 0.2
RO FII 50 50 6 0.12 0.12 0.12
Ours 50 26 15 0.58 0.3 0.4
Table 8: Comparisons with state-of-the-art sys-
tems using the QALD benchmark.
18
The POS tag of the head word in the phrase
19
The type of semantic item
20
Because the QALD-4 conference does not start un-
til after submission, we have no citation for the state-of-
5.3.4 The Effect of Different Formulas
To determine which formulas are more useful for
QALD, we evaluate the performance of the pro-
posed method with different predicate sets. We
subtract one weighted formula from the original
sets at a time, except retaining the first two for-
mulas sf1 and sf2 for basic inference. Because of
space limitations, only the results using QALD-3
testing set are shown in Table 9.
From the results, we can observe that remov-
ing some formulas can boost the performance on
some single tasks, but employing all formulas can
produce the best performance. This illustrates that
solely resolving the steps in QALD (phrase detec-
tion, phrase mapping, semantic items grouping)
can obtain local results, and that making joint in-
ference is necessary and useful.
6 Related Work
Our proposed method is related to two lines of
work: Question Answering over Knowledge bases
and Markov Logic Networks.
Question answering over knowledge bases
has attracted a substantial amount of interest over
a long period of time. The initial attempts in-
cluded BaseBall (Green Jr et al., 1961) and Lu-
nar (Woods, 1977). However, these systems were
mostly limited to closed domains due to a lack of
knowledge resources. With the rapid development
of structured data, such as DBpedia, Freebase and
Yago, the need for providing user-friendly inter-
face to these data has become increasingly urgent.
Keyword (Elbassuoni and Blanco, 2011) and se-
mantic (Pound et al., 2010) searches are limited
to their ability to specify the relations among the
different keywords.
The open topic progress has also been pushed
by the QALD evaluation campaigns (Walter et al.,
2012). Lopez et al. (2011) gave a comprehensive
survey in this research area. The authors devel-
oped the PowerAqua system (Lopez et al., 2006) to
the-art systems in QALD-4. The results can be found at
http://greententacle.techfak.uni-bielefeld.de/ cunger/qald.
1100
Formulas
PD PM MG Avg
P R F1 P R F1 P R F1 P R F1
All Formulas 0.941 0.941 0.941 0.878 0.918 0.898 0.636 0.798 0.708 0.839 0.901 0.869
-sf3 0.931 0.927 0.929 0.877 0.913 0.895 0.637 0.816 0.715 0.834 0.897 0.864
-sf4 0.926 0.917 0.922 0.852 0.883 0.867 0.63 0.763 0.69 0.824 0.87 0.846
-sf5 0.931 0.927 0.929 0.873 0.908 0.89 0.633 0.816 0.713 0.831 0.895 0.862
-sf6 0.922 0.922 0.922 0.844 0.883 0.863 0.702 0.746 0.723 0.842 0.868 0.855
-sf7 0.931 0.917 0.924 0.881 0.908 0.894 0.621 0.763 0.685 0.833 0.88 0.856
-sf8 0.927 0.927 0.927 0.868 0.908 0.888 0.639 0.807 0.713 0.83 0.893 0.861
Table 9: Performance comparisons of different weighted formulas evaluated using the QALD-3 question
set.
answer questions on large, heterogeneous datasets.
For questions containing quantifiers, comparatives
or superlatives, Unger et al. (2012) translated
NL to FL using several SPARQL templates and
using a set of heuristic rules mapping phrases
to semantic items. The system most similar to
ours is DEANNA (Yahya et al., 2012). However,
DEANNA extracts predicate-argument structures
from the questions using three hand-written pat-
terns. Our system jointly learns these mappings
and extractions completely from scratch.
Recently, the Semantic Parsing (SP) community
targeted this problem from limited domains (Tang
and Mooney, 2001; Liang et al., 2013) to open do-
mains (Cai and Yates, 2013; Berant et al., 2013).
The methods in semantic parsing answer questions
by first converting natural language utterances into
meaningful representations (e.g., the lambda cal-
culus) and subsequently executing the formal log-
ical forms over KBs. Compared to deriving the
complete logical representation, our method aims
to parse a question into a limited logic form with
the semantic item query, which we believe is more
appropriate for answering factoid questions.
Markov Logic Networks have been widely
used in NLP tasks. Huang (2012) applied MLN
to compress sentences by formulating the task as a
word/phrase deletion problem. Fahrni and Strube
(2012) jointly disambiguated and clustered con-
cepts using MLN. MLN has also been used in
coreference resolution (Song et al., 2012). For
the task of identifying subjective text segments
and of extracting their corresponding explanations
from product reviews, Zhang et al. (2013) mod-
eled these segments with MLN. To discover log-
ical knowledge for deep question answering, Liu
(2012) used MLN to resolve the inconsistencies
of multiple knowledge bases.
Meza-Ruiz and Riedel (2009) employed MLN
for Semantic Role Labeling (SRL). They jointly
performed the following tasks for a sentence:
predicate identification, frame disambiguation, ar-
gument identification and argument classification.
The semantic analysis of SRL solely rested on
the lexical level, but our analysis focuses on the
knowledge-base level and aims to obtain an exe-
cutable query and to support natural language in-
ference.
7 Conclusions and Future Work
For the task of QALD, we present a joint learn-
ing framework for phrase detection, phrase map-
ping and semantic item grouping. The novelty of
our method lies in the fact that we perform joint
inference and pattern learning for all subtasks in
QALD using first-order logic. Our experimental
results demonstrate the effectiveness of the pro-
posed method.
In the future, we plan to address the follow-
ing limitations that still exist in the current sys-
tem: a) numerous hand-labeled data are required
for training the MLN, and we could use a la-
tent form of semantic item query graphs (Liang et
al., 2013); b) more robust solutions can be devel-
oped to find the implicit relations in questions; c)
our system can be scaled up to large-scale open-
domain knowledge bases (Fader et al., 2013; Yao
and Van Durme, 2014); and d) the learning system
has the advantage of being easily adapted to new
settings, and we plan to extend it to other domains
and languages (Liang and Potts, 2014).
Acknowledgments
The authors are grateful to the anonymous re-
viewers for their constructive comments. This
work was sponsored by the National Basic Re-
search Program of China (No. 2014CB340503)
and the National Natural Science Foundation of
China (No. 61202329, 61272332), CCF-Tencent
Open Fund. This work was also supported in part
by Noahs Ark Lab of Huawei Tech. Ltm.
1101
References
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
In The semantic web, pages 722?735. Springer.
Junwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.
2014. Knowledge-based question answering as ma-
chine translation. In ACL.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP.
Christian Bizer, Tom Heath, and Tim Berners-Lee.
2009. Linked data-the story so far. International
journal on semantic web and information systems,
5(3):1?22.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In SIGMOD.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In ACL.
Unger Christina and Andr Freitas. 2014. Question an-
swering over linked data: Challenges, approaches,
trends. In ESWC.
Dima Corina. 2013. Intui2: A prototype system
for question answering over linked data. In Work.
Multilingual Question Answering over Linked Data
(QALD-3).
Giannone Cristina, Bellomaria Valentina, and Basili
Roberto. 2013. A hmm-based approach to question
answering against linked data. In Work. Multilin-
gual Question Answering over Linked Data (QALD-
3).
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In LREC.
Shady Elbassuoni and Roi Blanco. 2011. Keyword
search over rdf graphs. In CIKM.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL.
Angela Fahrni and Michael Strube. 2012. Jointly
disambiguating and clustering concepts and entities
with markov logic. In COLING.
Andre Freitas and Edward Curry. 2014. Natural
language queries over heterogeneous linked data
graphs: A distributional-compositional semantics
approach. In IUI.
Bert F Green Jr, Alice K Wolf, Carol Chomsky, and
Kenneth Laughery. 1961. Baseball: an automatic
question-answerer. In Papers presented at the May
9-11, 1961, western joint IRE-AIEE-ACM computer
conference, pages 219?224. ACM.
Shizhu He, Shulin Liu, Yubo Chen, Guangyou Zhou,
Kang Liu, and Jun Zhao. 2013. Casia@qald-3:
A question answering system over linked data. In
Work. Multilingual Question Answering over Linked
Data (QALD-3).
Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.
2012. Using first-order logic to compress sentences.
In AAAI.
Paul. Jaccard. 1908. Nouvelles recherches sur la dis-
tribution florale. Bulletin de la Soci`ete Vaudense des
Sciences Naturelles, 44:223?270.
Guyonvarc?H Joris and S?ebastien Ferr?e. 2013.
Scalewelis: a scalable query-based faceted search
system on top of sparql endpoints. In Work.
Multilingual Question Answering over Linked Data
(QALD-3).
Jens Lehmann, Tim Furche, Giovanni Grasso, Axel-
Cyrille Ngonga Ngomo, Christian Schallhart, An-
drew Sellers, Christina Unger, Lorenz B?uhmann,
Daniel Gerber, Konrad H?offner, et al. 2012. Deqa:
deep web extraction for question answering. In
ISWC.
Percy Liang and Christopher Potts. 2014. Bringing
machine learning and compositional semantics to-
gether. Annual Reviews of Linguistics (to appear).
Percy Liang, Michael I Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389?446.
Zhao Liu, Xipeng Qiu, Ling Cao, and Xuanjing Huang.
2012. Discovering logical knowledge for deep ques-
tion answering. In CIKM.
Vanessa Lopez, Enrico Motta, and Victoria Uren.
2006. Poweraqua: Fishing the semantic web. In
The Semantic Web: research and applications, pages
393?410. Springer.
Vanessa Lopez, Victoria Uren, Marta Sabou, and En-
rico Motta. 2011. Is question answering fit for the
semantic web?: a survey. Semantic Web, 2(2):125?
155.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In NAACL.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: a taxonomy of relational
patterns with semantic types. In EMNLP.
1102
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Comput. Surv., 33(1):31?88.
Jeffrey Pound, Ihab F Ilyas, and Grant Weddell. 2010.
Expressive and flexible access to web-extracted
data: a keyword-based structured query language.
In SIGMOD.
C Pradel, G Peyet, O Haemmerl?e, and N Hernandez.
2013. Swip at qald-3: results, criticisms and les-
son learned (working notes). In Work. Multilingual
Question Answering over Linked Data (QALD-3).
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine learning, 62(1-
2):107?136.
Saeedeh Shekarpour, Axel-Cyrille Ngonga Ngomo,
and S?oren Auer. 2013. Question answering on in-
terlinked data. In WWW.
Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li,
and Houfeng Wang. 2012. Joint learning for coref-
erence resolution with markov logic. In EMNLP.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In WWW.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Proceedings
of the 12th European Conference on Machine Learn-
ing, pages 466?477.
Christina Unger, Lorenz B?uhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-based question
answering over rdf data. In WWW.
Sebastian Walter, Christina Unger, Philipp Cimiano,
and Daniel B?ar. 2012. Evaluation of a layered
approach to question answering over linked data.
In The Semantic Web?ISWC 2012, pages 362?374.
Springer.
William A Woods. 1977. Lunar rocks in natural en-
glish: Explorations in natural language question an-
swering. In Linguistic structures processing, pages
521?569.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for the
web of data. In EMNLP.
Mohamed Yahya, Klaus Berberich, Shady Elbassuoni,
and Gerhard Weikum. 2013. Robust question an-
swering over the web of linked data. In CIKM.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In ACL.
Qi Zhang, Jin Qian, Huan Chen, Jihua Kang, and Xu-
anjing Huang. 2013. Discourse level explanatory
relation extraction from product reviews using first-
order logic. In ACL.
Lei Zou, Ruizhe Huang, Haixun WangZou, Jeffrey Xu
Yu, Wenqiang He, and Dongyan Zhao. 2014. Natu-
ral language question answering over rdf ? a graph
data driven approach. In SIGMOD.
1103
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 653?662,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Phrase-Based Translation Model for Question Retrieval in Community
Question Answer Archives
Guangyou Zhou, Li Cai, Jun Zhao?, and Kang Liu
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,lcai,jzhao,kliu}@nlpr.ia.ac.cn
Abstract
Community-based question answer (Q&A)
has become an important issue due to the pop-
ularity of Q&A archives on the web. This pa-
per is concerned with the problem of ques-
tion retrieval. Question retrieval in Q&A
archives aims to find historical questions that
are semantically equivalent or relevant to the
queried questions. In this paper, we propose
a novel phrase-based translation model for
question retrieval. Compared to the traditional
word-based translation models, the phrase-
based translation model is more effective be-
cause it captures contextual information in
modeling the translation of phrases as a whole,
rather than translating single words in isola-
tion. Experiments conducted on real Q&A
data demonstrate that our proposed phrase-
based translation model significantly outper-
forms the state-of-the-art word-based transla-
tion model.
1 Introduction
Over the past few years, large scale question and
answer (Q&A) archives have become an important
information resource on the Web. These include
the traditional Frequently Asked Questions (FAQ)
archives and the emerging community-based Q&A
services, such as Yahoo! Answers1, Live QnA2, and
Baidu Zhidao3.
?Correspondence author: jzhao@nlpr.ia.ac.cn
1http://answers.yahoo.com/
2http://qna.live.com/
3http://zhidao.baidu.com/
Community-based Q&A services can directly re-
turn answers to the queried questions instead of a
list of relevant documents, thus provide an effective
alternative to the traditional adhoc information re-
trieval. To make full use of the large scale archives
of question-answer pairs, it is critical to have func-
tionality helping users to retrieve historical answers
(Duan et al, 2008). Therefore, it is a meaningful
task to retrieve the questions that are semantically
equivalent or relevant to the queried questions. For
example in Table 1, given questionQ1,Q2 can be re-
turned and their answers will then be used to answer
Q1 because the answer ofQ2 is expected to partially
satisfy the queried question Q1. This is what we
called question retrieval in this paper.
The major challenge for Q&A retrieval, as for
Query:
Q1: How to get rid of stuffy nose?
Expected:
Q2: What is the best way to prevent a cold?
Not Expected:
Q3: How do I air out my stuffy room?
Q4: How do you make a nose bleed stop quicker?
Table 1: An example on question retrieval
most information retrieval models, such as vector
space model (VSM) (Salton et al, 1975), Okapi
model (Robertson et al, 1994), language model
(LM) (Ponte and Croft, 1998), is the lexical gap (or
lexical chasm) between the queried questions and
the historical questions in the archives (Jeon et al,
2005; Xue et al, 2008). For example in Table 1, Q1
and Q2 are two semantically similar questions, but
they have very few words in common. This prob-
653
lem is more serious for Q&A retrieval, since the
question-answer pairs are usually short and there is
little chance of finding the same content expressed
using different wording (Xue et al, 2008). To solve
the lexical gap problem, most researchers regarded
the question retrieval task as a statistical machine
translation problem by using IBM model 1 (Brown
et al, 1993) to learn the word-to-word translation
probabilities (Berger and Lafferty, 1999; Jeon et al,
2005; Xue et al, 2008; Lee et al, 2008; Bernhard
and Gurevych, 2009). Experiments consistently re-
ported that the word-based translation models could
yield better performance than the traditional meth-
ods (e.g., VSM. Okapi and LM). However, all these
existing approaches are considered to be context in-
dependent in that they do not take into account any
contextual information in modeling word translation
probabilities. For example in Table 1, although nei-
ther of the individual word pair (e.g., ?stuffy?/?cold?
and ?nose?/?cold?) might have a high translation
probability, the sequence of words ?stuffy nose? can
be easily translated from a single word ?cold? in Q2
with a relative high translation probability.
In this paper, we argue that it is beneficial to cap-
ture contextual information for question retrieval.
To this end, inspired by the phrase-based statistical
machine translation (SMT) systems (Koehn et al,
2003; Och and Ney, 2004), we propose a phrase-
based translation model (P-Trans) for question re-
trieval, and we assume that question retrieval should
be performed at the phrase level. This model learns
the probability of translating one sequence of words
(e.g., phrase) into another sequence of words, e.g.,
translating a phrase in a historical question into an-
other phrase in a queried question. Compared to the
traditional word-based translation models that ac-
count for translating single words in isolation, the
phrase-based translation model is potentially more
effective because it captures some contextual infor-
mation in modeling the translation of phrases as a
whole. More precise translation can be determined
for phrases than for words. It is thus reasonable to
expect that using such phrase translation probabili-
ties as ranking features is likely to improve the ques-
tion retrieval performance, as we will show in our
experiments.
Unlike the general natural language translation,
the parallel sentences between questions and an-
swers in community-based Q&A have very different
lengths, leaving many words in answers unaligned
to any word in queried questions. Following (Berger
and Lafferty, 1999), we restrict our attention to those
phrase translations consistent with a good word-
level alignment.
Specifically, we make the following contribu-
tions:
? we formulate the question retrieval task as a
phrase-based translation problem by modeling
the contextual information (in Section 3.1).
? we linearly combine the phrase-based transla-
tion model for the question part and answer part
(in Section 3.2).
? we propose a linear ranking model framework
for question retrieval in which different models
are incorporated as features because the phrase-
based translation model cannot be interpolated
with a unigram language model (in Section
3.3).
? finally, we conduct the experiments on
community-based Q&A data for question re-
trieval. The results show that our proposed ap-
proach significantly outperforms the baseline
methods (in Section 4).
The remainder of this paper is organized as fol-
lows. Section 2 introduces the existing state-of-the-
art methods. Section 3 describes our phrase-based
translation model for question retrieval. Section 4
presents the experimental results. In Section 5, we
conclude with ideas for future research.
2 Preliminaries
2.1 Language Model
The unigram language model has been widely used
for question retrieval on community-based Q&A
data (Jeon et al, 2005; Xue et al, 2008; Cao et al,
2010). To avoid zero probability, we use Jelinek-
Mercer smoothing (Zhai and Lafferty, 2001) due to
its good performance and cheap computational cost.
So the ranking function for the query likelihood lan-
guage model with Jelinek-Mercer smoothing can be
654
written as:
Score(q, D) =
?
w?q
(1 ? ?)Pml(w|D) + ?Pml(w|C)
(1)
Pml(w|D) =
#(w,D)
|D|
, Pml(w|C) =
#(w,C)
|C|
(2)
where q is the queried question, D is a document, C
is background collection, ? is smoothing parameter.
#(t,D) is the frequency of term t in D, |D| and |C|
denote the length of D and C respectively.
2.2 Word-Based Translation Model
Previous work (Berger et al, 2000; Jeon et al, 2005;
Xue et al, 2008) consistently reported that the word-
based translation models (Trans) yielded better per-
formance than the traditional methods (VSM, Okapi
and LM) for question retrieval. These models ex-
ploit the word translation probabilities in a language
modeling framework. Following Jeon et al (2005)
and Xue et al (2008), the ranking function can be
written as:
Score(q, D) =
?
w?q
(1??)Ptr(w|D)+?Pml(w|C) (3)
Ptr(w|D) =
?
t?D
P (w|t)Pml(t|D), Pml(t|D) =
#(t,D)
|D|
(4)
where P (w|t) denotes the translation probability
from word t to word w.
2.3 Word-Based Translation Language Model
Xue et al (2008) proposed to linearly mix two dif-
ferent estimations by combining language model
and word-based translation model into a unified
framework, called TransLM. The experiments show
that this model gains better performance than both
the language model and the word-based translation
model. Following Xue et al (2008), this model can
be written as:
Score(q, D) =
?
w?q
(1 ? ?)Pmx(w|D) + ?Pml(w|C)
(5)
Pmx(w|D) = ?
?
t?D
P (w|t)Pml(t|D)+(1??)Pml(w|D)
(6)
D:                      ?  for good cold home remedies ? document
E:                  [for,    good,    cold,    home remedies] segmentation
F:            [for1,    best2,    stuffy nose3,    home remedy4] translation
M:                     (1?3?2?1?3?4?4?2) permutation
q:                     best home remedy for stuffy nose queried question
Figure 1: Example describing the generative procedure
of the phrase-based translation model.
3 Our Approach: Phrase-Based
Translation Model for Question
Retrieval
3.1 Phrase-Based Translation Model
Phrase-based machine translation models (Koehn
et al, 2003; D. Chiang, 2005; Och and Ney,
2004) have shown superior performance compared
to word-based translation models. In this paper,
the goal of phrase-based translation model is to
translate a document4 D into a queried question
q. Rather than translating single words in isola-
tion, the phrase-based model translates one sequence
of words into another sequence of words, thus in-
corporating contextual information. For example,
we might learn that the phrase ?stuffy nose? can be
translated from ?cold? with relative high probabil-
ity, even though neither of the individual word pairs
(e.g., ?stuffy?/?cold? and ?nose?/?cold?) might have
a high word translation probability. Inspired by the
work of (Sun et al, 2010; Gao et al, 2010), we
assume the following generative process: first the
document D is broken into K non-empty word se-
quences t1, . . . , tK , then each t is translated into a
new non-empty word sequence w1, . . . ,wK , and fi-
nally these phrases are permutated and concatenated
to form the queried questions q, where t and w de-
note the phrases or consecutive sequence of words.
To formulate this generative process, let E
denote the segmentation of D into K phrases
t1, . . . , tK , and let F denote the K translation
phrases w1, . . . ,wK ?we refer to these (ti,wi)
pairs as bi-phrases. Finally, letM denote a permuta-
tion of K elements representing the final reordering
step. Figure 1 describes an example of the genera-
tive procedure.
Next let us place a probability distribution over
rewrite pairs. Let B(D,q) denote the set of E,
4In this paper, a document has the same meaning as a histor-
ical question-answer pair in the Q&A archives.
655
F , M triples that translate D into q. Here we as-
sume a uniform probability over segmentations, so
the phrase-based translation model can be formu-
lated as:
P (q|D) ?
?
(E,F,M)?
B(D,q)
P (F |D,E) ? P (M |D,E, F ) (7)
As is common practice in SMT, we use the maxi-
mum approximation to the sum:
P (q|D) ? max
(E,F,M)?
B(D,q)
P (F |D,E) ? P (M |D,E, F ) (8)
Although we have defined a generative model for
translatingD into q, our goal is to calculate the rank-
ing score function over existing q andD, rather than
generating new queried questions. Equation (8) can-
not be used directly for document ranking because
q and D are often of very different lengths, leav-
ing many words in D unaligned to any word in q.
This is the key difference between the community-
based question retrieval and the general natural lan-
guage translation. As pointed out by Berger and Laf-
ferty (1999) and Gao et al (2010), document-query
translation requires a distillation of the document,
while translation of natural language tolerates little
being thrown away.
Thus we attempt to extract the key document
words that form the distillation of the document, and
assume that a queried question is translated only
from the key document words. In this paper, the
key document words are identified via word align-
ment. We introduce the ?hidden alignments? A =
a1 . . . aj . . . aJ , which describe the mapping from a
word position j in queried question to a document
word position i = aj . The different alignment mod-
els we present provide different decompositions of
P (q, A|D). We assume that the position of the key
document words are determined by the Viterbi align-
ment, which can be obtained using IBM model 1 as
follows:
A? = argmax
A
P (q, A|D)
= argmax
A
{
P (J |I)
J
?
j=1
P (wj |taj )
}
=
[
argmax
aj
P (wj |taj )
]J
j=1
(9)
Given A?, when scoring a given Q&A pair, we re-
strict our attention to those E, F , M triples that are
consistent with A?, which we denote as B(D,q, A?).
Here, consistency requires that if two words are
aligned in A?, then they must appear in the same bi-
phrase (ti,wi). Once the word alignment is fixed,
the final permutation is uniquely determined, so we
can safely discard that factor. Thus equation (8) can
be written as:
P (q|D) ? max
(E,F,M)?B(D,q,A?)
P (F |D,E) (10)
For the sole remaining factor P (F |D,E), we
make the assumption that a segmented queried ques-
tion F = w1, . . . ,wK is generated from left to
right by translating each phrase t1, . . . , tK indepen-
dently:
P (F |D,E) =
K
?
k=1
P (wk|tk) (11)
where P (wk|tk) is a phrase translation probability,
the estimation will be described in Section 3.3.
To find the maximum probability assignment ef-
ficiently, we use a dynamic programming approach,
somewhat similar to the monotone decoding algo-
rithm described in (Och, 2002). We define ?j to
be the probability of the most likely sequence of
phrases covering the first j words in a queried ques-
tion, then the probability can be calculated using the
following recursion:
(1) Initialization:
?0 = 1 (12)
(2) Induction:
?j =
?
j?<j,w=wj?+1...wj
{
?j?P (w|tw)
}
(13)
(3) Total:
P (q|D) = ?J (14)
3.2 Phrase-Based Translation Model for
Question Part and Answer Part
In Q&A, a document D is decomposed into (q?, a?),
where q? denotes the question part of the historical
question in the archives and a? denotes the answer
part. Although it has been shown that doing Q&A
retrieval based solely on the answer part does not
perform well (Jeon et al, 2005; Xue et al, 2008),
the answer part should provide additional evidence
about relevance and, therefore, it should be com-
bined with the estimation based on the question part.
656
In this combined model, P (q|q?) and P (q|a?) are cal-
culated with equations (12) to (14). So P (q|D) will
be written as:
P (q|D) = ?1P (q|q?) + ?2P (q|a?) (15)
where ?1 + ?2 = 1.
In equation (15), the relative importance of ques-
tion part and answer part is adjusted through ?1 and
?2. When ?1 = 1, the retrieval model is based
on phrase-based translation model for the question
part. When ?2 = 1, the retrieval model is based on
phrase-based translation model for the answer part.
3.3 Parameter Estimation
3.3.1 Parallel Corpus Collection
In Q&A archives, question-answer pairs can be con-
sidered as a type of parallel corpus, which is used for
estimating the translation probabilities. Unlike the
bilingual machine translation, the questions and an-
swers in a Q&A archive are written in the same lan-
guage, the translation probability can be calculated
through setting either as the source and the other as
the target. In this paper, P (a?|q?) is used to denote
the translation probability with the question as the
source and the answer as the target. P (q?|a?) is used
to denote the opposite configuration.
For a given word or phrase, the related words
or phrases differ when it appears in the ques-
tion or in the answer. Following Xue et
al. (2008), a pooling strategy is adopted. First,
we pool the question-answer pairs used to learn
P (a?|q?) and the answer-question pairs used to
learn P (q?|a?), and then use IBM model 1 (Brown
et al, 1993) to learn the combined translation
probabilities. Suppose we use the collection
{(q?, a?)1, . . . , (q?, a?)m} to learn P (a?|q?) and use the
collection {(a?, q?)1, . . . , (a?, q?)m} to learn P (q?|a?),
then {(q?, a?)1, . . . , (q?, a?)m, (a?, q?)1, . . . , (a?, q?)m} is
used here to learn the combination translation prob-
ability Ppool(wi|tj).
3.3.2 Parallel Corpus Preprocessing
Unlike the bilingual parallel corpus used in SMT,
our parallel corpus is collected from Q&A archives,
which is more noisy. Directly using the IBM model
1 can be problematic, it is possible for translation
model to contain ?unnecessary? translations (Lee et
al., 2008). In this paper, we adopt a variant of Tex-
tRank algorithm (Mihalcea and Tarau, 2004) to iden-
tify and eliminate unimportant words from parallel
corpus, assuming that a word in a question or an-
swer is unimportant if it holds a relatively low sig-
nificance in the parallel corpus.
Following (Lee et al, 2008), the ranking algo-
rithm proceeds as follows. First, all the words in
a given document are added as vertices in a graph
G. Then edges are added between words if the
words co-occur in a fixed-sized window. The num-
ber of co-occurrences becomes the weight of an
edge. When the graph is constructed, the score of
each vertex is initialized as 1, and the PageRank-
based ranking algorithm is run on the graph itera-
tively until convergence. The TextRank score of a
word w in document D at kth iteration is defined as
follows:
Rkw,D = (1? d) + d ?
?
?j:(i,j)?G
ei,j
?
?l:(j,l)?G ej,l
Rk?1w,D
(16)
where d is a damping factor usually set to 0.85, and
ei,j is an edge weight between i and j.
We use average TextRank score as threshold:
words are removed if their scores are lower than the
average score of all words in a document.
3.3.3 Translation Probability Estimation
After preprocessing the parallel corpus, we will cal-
culate P (w|t), following the method commonly
used in SMT (Koehn et al, 2003; Och, 2002) to ex-
tract bi-phrases and estimate their translation proba-
bilities.
First, we learn the word-to-word translation prob-
ability using IBM model 1 (Brown et al, 1993).
Then, we perform Viterbi word alignment according
to equation (9). Finally, the bi-phrases that are con-
sistent with the word alignment are extracted using
the heuristics proposed in (Och, 2002). We set the
maximum phrase length to five in our experiments.
After gathering all such bi-phrases from the train-
ing data, we can estimate conditional relative fre-
quency estimates without smoothing:
P (w|t) = N(t,w)
N(t)
(17)
where N(t,w) is the number of times that t is
aligned to w in training data. These estimates are
657
source stuffy nose internet explorer
1 stuffy nose internet explorer
2 cold ie
3 stuffy internet browser
4 sore throat explorer
5 sneeze browser
Table 2: Phrase translation probability examples. Each
column shows the top 5 target phrases learned from the
word-aligned question-answer pairs.
useful for contextual lexical selection with sufficient
training data, but can be subject to data sparsity is-
sues (Sun et al, 2010; Gao et al, 2010). An alter-
nate translation probability estimate not subject to
data sparsity is the so-called lexical weight estimate
(Koehn et al, 2003). Let P (w|t) be the word-to-
word translation probability, and let A be the word
alignment between w and t. Here, the word align-
ment contains (i, j) pairs, where i ? 1 . . . |w| and
j ? 0 . . . |t|, with 0 indicating a null word. Then we
use the following estimate:
Pt(w|t, A) =
|w|
?
i=1
1
|{j|(j, i) ? A}|
?
?(i,j)?A
P (wi|tj)
(18)
We assume that for each position inw, there is ei-
ther a single alignment to 0, or multiple alignments
to non-zero positions in t. In fact, equation (18)
computes a product of per-word translation scores;
the per-word scores are the averages of all the trans-
lations for the alignment links of that word. The
word translation probabilities are calculated using
IBM 1, which has been widely used for question re-
trieval (Jeon et al, 2005; Xue et al, 2008; Lee et al,
2008; Bernhard and Gurevych, 2009). These word-
based scores of bi-phrases, though not as effective
in contextual selection, are more robust to noise and
sparsity.
A sample of the resulting phrase translation ex-
amples is shown in Table 2, where the top 5 target
phrases are translated from the source phrases ac-
cording to the phrase-based translation model. For
example, the term ?explorer? used alone, most likely
refers to a person who engages in scientific explo-
ration, while the phrase ?internet explorer? has a
very different meaning.
3.4 Ranking Candidate Historical Questions
Unlike the word-based translation models, the
phrase-based translation model cannot be interpo-
lated with a unigram language model. Following
(Sun et al, 2010; Gao et al, 2010), we resort to
a linear ranking framework for question retrieval in
which different models are incorporated as features.
We consider learning a relevance function of the
following general, linear form:
Score(q, D) = ?T ??(q, D) (19)
where the feature vector ?(q, D) is an arbitrary
function that maps (q, D) to a real value, i.e.,
?(q, D) ? R. ? is the corresponding weight vec-
tor, we optimize this parameter for our evaluation
metrics directly using the Powell Search algorithm
(Paul et al, 1992) via cross-validation.
The features used in this paper are as follows:
? Phrase translation features (PT):
?PT (q, D,A) = logP (q|D), where P (q|D)
is computed using equations (12) to (15), and
the phrase translation probability P (w|t) is
estimated using equation (17).
? Inverted Phrase translation features (IPT):
?IPT (D,q, A) = logP (D|q), where P (D|q)
is computed using equations (12) to (15) ex-
cept that we set ?2 = 0 in equation (15), and
the phrase translation probability P (w|t) is es-
timated using equation (17).
? Lexical weight feature (LW):
?LW (q, D,A) = logP (q|D), here P (q|D)
is computed by equations (12) to (15), and the
phrase translation probability is computed as
lexical weight according to equation (18).
? Inverted Lexical weight feature (ILW):
?ILW (D,q, A) = logP (D|q), here P (D|q)
is computed by equations (12) to (15) except
that we set ?2 = 0 in equation (15), and the
phrase translation probability is computed as
lexical weight according to equation (18).
? Phrase alignment features (PA):
?PA(q, D,B) =
?K
2 |ak ? bk?1 ? 1|,
where B is a set of K bi-phrases, ak is the start
position of the phrase in D that was translated
658
into the kth phrase in queried question, and
bk?1 is the end position of the phrase in D
that was translated into the (k ? 1)th phrase in
queried question. The feature, inspired by the
distortion model in SMT (Koehn et al, 2003),
models the degree to which the queried phrases
are reordered. For all possible B, we only
compute the feature value according to the
Viterbi alignment, B? = argmaxB P (q, B|D).
We find B? using the Viterbi algorithm, which is
almost identical to the dynamic programming
recursion of equations (12) to (14), except that
the sum operator in equation (13) is replaced
with the max operator.
? Unaligned word penalty features (UWP):
?UWP (q, D), which is defined as the ratio be-
tween the number of unaligned words and the
total number of words in queried questions.
? Language model features (LM):
?LM (q, D,A) = logPLM (q|D), where
PLM (q|D) is the unigram language model
with Jelinek-Mercer smoothing defined by
equations (1) and (2).
? Word translation features (WT):
?WT (q, D) = logP (q|D), where P (q|D) is
the word-based translation model defined by
equations (3) and (4).
4 Experiments
4.1 Data Set and Evaluation Metrics
We collect the questions from Yahoo! Answers and
use the getByCategory function provided in Yahoo!
Answers API5 to obtain Q&A threads from the Ya-
hoo! site. More specifically, we utilize the resolved
questions under the top-level category at Yahoo!
Answers, namely ?Computers & Internet?. The re-
sulting question repository that we use for question
retrieval contains 518,492 questions. To learn the
translation probabilities, we use about one million
question-answer pairs from another data set.6
In order to create the test set, we randomly se-
lect 300 questions for this category, denoted as
5http://developer.yahoo.com/answers
6The Yahoo! Webscope dataset Yahoo answers com-
prehensive questions and answers version 1.0.2, available at
http://reseach.yahoo.com/Academic Relations.
?CI TST?. To obtain the ground-truth of ques-
tion retrieval, we employ the Vector Space Model
(VSM) (Salton et al, 1975) to retrieve the top 20 re-
sults and obtain manual judgements. The top 20 re-
sults don?t include the queried question itself. Given
a returned result by VSM, an annotator is asked to
label it with ?relevant? or ?irrelevant?. If a returned
result is considered semantically equivalent to the
queried question, the annotator will label it as ?rel-
evant?; otherwise, the annotator will label it as ?ir-
relevant?. Two annotators are involved in the anno-
tation process. If a conflict happens, a third person
will make judgement for the final result. In the pro-
cess of manually judging questions, the annotators
are presented only the questions. Table 3 provides
the statistics on the final test set.
#queries #returned #relevant
CI TST 300 6,000 798
Table 3: Statistics on the Test Data
We evaluate the performance of our approach us-
ing Mean Average Precision (MAP). We perform
a significant test, i.e., a t-test with a default signif-
icant level of 0.05. Following the literature, we set
the parameters ? = 0.2 (Cao et al, 2010) in equa-
tions (1), (3) and (5), and ? = 0.8 (Xue et al, 2008)
in equation (6).
4.2 Question Retrieval Results
We randomly divide the test questions into five
subsets and conduct 5-fold cross-validation experi-
ments. In each trial, we tune the parameters ?1 and
?2 with four of the five subsets and then apply it to
one remaining subset. The experiments reported be-
low are those averaged over the five trials.
Table 4 presents the main retrieval performance.
Row 1 to row 3 are baseline systems, all these meth-
ods use word-based translation models and obtain
the state-of-the-art performance in previous work
(Jeon et al, 2005; Xue et al, 2008). Row 3 is simi-
lar to row 2, the only difference is that TransLM only
considers the question part, while Xue et al (2008)
incorporates the question part and answer part. Row
4 and row 5 are our proposed phrase-based trans-
lation model with maximum phrase length of five.
Row 4 is phrase-based translation model purely
based on question part, this model is equivalent to
659
# Methods Trans Prob MAP
1 Jeon et al (2005) Ppool 0.289
2 TransLM Ppool 0.324
3 Xue et al (2008) Ppool 0.352
4 P-Trans (?1 = 1, l = 5) Ppool 0.366
5 P-Trans (l = 5) Ppool 0.391
Table 4: Comparison with different methods for question
retrieval.
setting ?1 = 1 in equation (15). Row 5 is the phrase-
based combination model which linearly combines
the question part and answer part. As expected,
different parts can play different roles: a phrase to
be translated in queried questions may be translated
from the question part or answer part. All these
methods use pooling strategy to estimate the transla-
tion probabilities. There are some clear trends in the
result of Table 4:
(1) Word-based translation language model
(TransLM) significantly outperforms word-based
translation model of Jeon et al (2005) (row 1 vs. row
2). Similar observations have been made by Xue et
al. (2008).
(2) Incorporating the answer part into the models,
either word-based or phrase-based, can significantly
improve the performance of question retrieval (row
2 vs. row 3; row 4 vs. row 5).
(3) Our proposed phrase-based translation model
(P-Trans) significantly outperforms the state-of-the-
art word-based translation models (row 2 vs. row 4
and row 3 vs. row 5, all these comparisons are sta-
tistically significant at p < 0.05).
4.3 Impact of Phrase Length
Our proposed phrase-based translation model, due to
its capability of capturing contextual information, is
more effective than the state-of-the-art word-based
translation models. It is important to investigate the
impact of the phrase length on the final retrieval per-
formance. Table 5 shows the results, it is seen that
using the longer phrases up to the maximum length
of five can consistently improve the retrieval per-
formance. However, using much longer phrases in
the phrase-based translation model does not seem to
produce significantly better performance (row 8 and
row 9 vs. row 10 are not statistically significant).
# Systems MAP
6 P-Trans (l = 1) 0.352
7 P-Trans (l = 2) 0.373
8 P-Trans (l = 3) 0.386
9 P-Trans (l = 4) 0.390
10 P-Trans (l = 5) 0.391
Table 5: The impact of the phrase length on retrieval per-
formance.
Model # Methods Average MAP
P-Trans (l = 5) 11 Initial 69 0.38012 TextRank 24 0.391
Table 6: Effectiveness of parallel corpus preprocessing.
4.4 Effectiveness of Parallel Corpus
Preprocessing
Question-answer pairs collected from Yahoo! an-
swers are very noisy, it is possible for translation
models to contain ?unnecessary? translations. In this
paper, we attempt to identify and decrease the pro-
portion of unnecessary translations in a translation
model by using TextRank algorithm. This kind of
?unnecessary? translation between words will even-
tually affect the bi-phrase translation.
Table 6 shows the effectiveness of parallel corpus
preprocessing. Row 11 reports the average number
of translations per word and the question retrieval
performance when only stopwords 7 are removed.
When using the TextRank algorithm for parallel cor-
pus preprocessing, the average number of transla-
tions per word is reduced from 69 to 24, but the
performance of question retrieval is significantly im-
proved (row 11 vs. row 12). Similar results have
been made by Lee et al (2008).
4.5 Impact of Pooling Strategy
The correspondence of words or phrases in the
question-answer pair is not as strong as in the bilin-
gual sentence pair, thus noise will be inevitably in-
troduced for both P (a?|q?) and P (q?|a?).
To see how much the pooling strategy benefit the
question retrieval, we introduce two baseline meth-
ods for comparison. The first method (denoted as
P (a?|q?)) is used to denote the translation probabil-
ity with the question as the source and the answer as
7http://truereader.com/manuals/onix/stopwords1.html
660
Model # Trans Prob MAP
P-Trans (l = 5)
13 P (a?|q?) 0.387
14 P (q?|a?) 0.381
15 Ppool 0.391
Table 7: The impact of pooling strategy for question re-
trieval.
the target. The second (denoted as P (a?|q?)) is used
to denote the translation probability with the answer
as the source and the question as the target. Table 7
provides the comparison. From this Table, we see
that the pooling strategy significantly outperforms
the two baseline methods for question retrieval (row
13 and row 14 vs. row 15).
5 Conclusions and Future Work
In this paper, we propose a novel phrase-based trans-
lation model for question retrieval. Compared to
the traditional word-based translation models, the
proposed approach is more effective in that it can
capture contextual information instead of translating
single words in isolation. Experiments conducted
on real Q&A data demonstrate that the phrase-
based translation model significantly outperforms
the state-of-the-art word-based translation models.
There are some ways in which this research could
be continued. First, question structure should be
considered, so it is necessary to combine the pro-
posed approach with other question retrieval meth-
ods (e.g., (Duan et al, 2008; Wang et al, 2009;
Bunescu and Huang, 2010)) to further improve the
performance. Second, we will try to investigate the
use of the proposed approach for other kinds of data
set, such as categorized questions from forum sites
and FAQ sites.
Acknowledgments
This work was supported by the National Natural
Science Foundation of China (No. 60875041 and
No. 61070106). We thank the anonymous reviewers
for their insightful comments. We also thank Maoxi
Li and Jiajun Zhang for suggestion to use the align-
ment toolkits.
References
A. Berger and R. Caruana and D. Cohn and D. Freitag and
V. Mittal. 2000. Bridging the lexical chasm: statistical
approach to answer-finding. In Proceedings of SIGIR,
pages 192-199.
A. Berger and J. Lafferty. 1999. Information retrieval as
statistical translation. In Proceedings of SIGIR, pages
222-229.
D. Bernhard and I. Gurevych. 2009. Combining lexical
semantic resources with question & answer archives
for translation-based answer finding. In Proceedings
of ACL, pages 728-736.
P. F. Brown and V. J. D. Pietra and S. A. D. Pietra and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263-311.
R. Bunescu and Y. Huang. 2010. Learning the relative
usefulness of questions in community QA. In Pro-
ceedings of EMNLP, pages 97-107.
X. Cao and G. Cong and B. Cui and C. S. Jensen. 2010.
A generalized framework of exploring category infor-
mation for question retrieval in community question
answer archives. In Proceedings of WWW.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL.
H. Duan and Y. Cao and C. Y. Lin and Y. Yu. 2008.
Searching questions by identifying questions topics
and question focus. In Proceedings of ACL, pages
156-164.
J. Gao and X. He and J. Nie. 2010. Clickthrough-based
translation models for web search: from word models
to phrase models. In Proceedings of CIKM.
J. Jeon and W. Bruce Croft and J. H. Lee. 2005. Find-
ing similar questions in large question and answer
archives. In Proceedings of CIKM, pages 84-90.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into text. In Proceedings of EMNLP, pages 404-
411.
P. Koehn and F. Och and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL,
pages 48-54.
J. -T. Lee and S. -B. Kim and Y. -I. Song and H. -C. Rim.
2008. Bridging lexical gaps between queries and ques-
tions on large online Q&A collections with compact
translation models. In Proceedings of EMNLP, pages
410-418.
F. Och. 2002. Statistical mahcine translation: from sin-
gle word models to alignment templates. Ph.D thesis,
RWTH Aachen.
F. Och and H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Computa-
tional Linguistics, 30(4):417-449.
661
J. M. Ponte and W. B. Croft. 1998. A language modeling
approach to information retrieval. In Proceedings of
SIGIR.
W. H. Press and S. A. Teukolsky and W. T. Vetterling
and B. P. Flannery. 1992. Numerical Recipes In C.
Cambridge Univ. Press.
S. Robertson and S. Walker and S. Jones and M.
Hancock-Beaulieu and M. Gatford. 1994. Okapi at
trec-3. In Proceedings of TREC, pages 109-126.
G. Salton and A. Wong and C. S. Yang. 1975. A vector
space model for automatic indexing. Communications
of the ACM, 18(11):613-620.
X. Sun and J. Gao and D. Micol and C. Quirk. 2010.
Learning phrase-based spelling error models from
clickthrough data. In Proceedings of ACL.
K. Wang and Z. Ming and T-S. Chua. 2009. A syntactic
tree matching approach to finding similar questions in
community-based qa services. In Proceedings of SI-
GIR, pages 187-194.
X. Xue and J. Jeon and W. B. Croft. 2008. Retrieval
models for question and answer archives. In Proceed-
ings of SIGIR, pages 475-482.
C. Zhai and J. Lafferty. 2001. A study of smooth meth-
ods for language models applied to ad hoc information
retrieval. In Proceedings of SIGIR, pages 334-342.
662
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1556?1565,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Exploiting Web-Derived Selectional Preference to Improve Statistical
Dependency Parsing
Guangyou Zhou, Jun Zhao?, Kang Liu, and Li Cai
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Beijing 100190, China
{gyzhou,jzhao,kliu,lcai}@nlpr.ia.ac.cn
Abstract
In this paper, we present a novel approach
which incorporates the web-derived selec-
tional preferences to improve statistical de-
pendency parsing. Conventional selectional
preference learning methods have usually fo-
cused on word-to-class relations, e.g., a verb
selects as its subject a given nominal class.
This paper extends previous work to word-
to-word selectional preferences by using web-
scale data. Experiments show that web-scale
data improves statistical dependency pars-
ing, particularly for long dependency relation-
ships. There is no data like more data, perfor-
mance improves log-linearly with the number
of parameters (unique N-grams). More impor-
tantly, when operating on new domains, we
show that using web-derived selectional pref-
erences is essential for achieving robust per-
formance.
1 Introduction
Dependency parsing is the task of building depen-
dency links between words in a sentence, which has
recently gained a wide interest in the natural lan-
guage processing community. With the availabil-
ity of large-scale annotated corpora such as Penn
Treebank (Marcus et al, 1993), it is easy to train
a high-performance dependency parser using super-
vised learning methods.
However, current state-of-the-art statistical de-
pendency parsers (McDonald et al, 2005; McDon-
ald and Pereira, 2006; Hall et al, 2006) tend to have
?Correspondence author: jzhao@nlpr.ia.ac.cn
lower accuracies for longer dependencies (McDon-
ald and Nivre, 2007). The length of a dependency
from word wi to word wj is simply equal to |i ? j|.
Longer dependencies typically represent the mod-
ifier of the root or the main verb, internal depen-
dencies of longer NPs or PP-attachment in a sen-
tence. Figure 1 shows the F1 score1 relative to the
dependency length on the development set by using
the graph-based dependency parsers (McDonald et
al., 2005; McDonald and Pereira, 2006). We note
that the parsers provide very good results for adja-
cent dependencies (96.89% for dependency length
=1), while the dependency length increases, the ac-
curacies degrade sharply. These longer dependen-
cies are therefore a major opportunity to improve the
overall performance of dependency parsing. Usu-
ally, these longer dependencies can be parsed de-
pendent on the specific words involved due to the
limited range of features (e.g., a verb and its mod-
ifiers). Lexical statistics are therefore needed for
resolving ambiguous relationships, yet the lexical-
ized statistics are sparse and difficult to estimate di-
rectly. To solve this problem, some information with
different granularity has been investigated. Koo et
al. (2008) proposed a semi-supervised dependency
parsing by introducing lexical intermediaries at a
coarser level than words themselves via a cluster
method. This approach, however, ignores the se-
lectional preference for word-to-word interactions,
such as head-modifier relationship. Extra resources
1Precision represents the percentage of predicted arcs of
length d that are correct, and recall measures the percentage
of gold-standard arcs of length d that are correctly predicted.
F1 = 2? precision ? recall/(precision + recall)
1556
1 5 10 15 20 25 300.7
0.75
0.8
0.85
0.9
0.95
1
Dependency Length
F1 S
core
 (%)
MST1MST2
Figure 1: F score relative to dependency length.
beyond the annotated corpora are needed to capture
the bi-lexical relationship at the word-to-word level.
Our purpose in this paper is to exploit web-
derived selectional preferences to improve the su-
pervised statistical dependency parsing. All of our
lexical statistics are derived from two kinds of web-
scale corpus: one is the web, which is the largest
data set that is available for NLP (Keller and Lap-
ata, 2003). Another is a web-scale N-gram corpus,
which is a N-gram corpus with N-grams of length 1-
5 (Brants and Franz, 2006), we call it Google V1 in
this paper. The idea is very simple: web-scale data
have large coverage for word pair acquisition. By
leveraging some assistant data, the dependency pars-
ing model can directly utilize the additional informa-
tion to capture the word-to-word level relationships.
We address two natural and related questions which
some previous studies leave open:
Question I: Is there a benefit in incorporating
web-derived selectional preference features for sta-
tistical dependency parsing, especially for longer de-
pendencies?
Question II: How well do web-derived selec-
tional preferences perform on new domains?
For Question I, we systematically assess the value
of using web-scale data in state-of-the-art super-
vised dependency parsers. We compare dependency
parsers that include or exclude selectional prefer-
ence features obtained from web-scale corpus. To
the best of our knowledge, none of the existing stud-
ies directly address long dependencies of depen-
dency parsing by using web-scale data.
Most statistical parsers are highly domain depen-
dent. For example, the parsers trained on WSJ text
perform poorly on Brown corpus. Some studies have
investigated domain adaptation for parsers (Mc-
Closky et al, 2006; Daume? III, 2007; McClosky et
al., 2010). These approaches assume that the parsers
know which domain it is used, and that it has ac-
cess to representative data in that domain. How-
ever, in practice, these assumptions are unrealistic
in many real applications, such as when processing
the heterogeneous genre of web texts. In this paper
we incorporate the web-derived selectional prefer-
ence features to design our parsers for robust open-
domain testing.
We conduct the experiments on the English Penn
Treebank (PTB) (Marcus et al, 1993). The results
show that web-derived selectional preference can
improve the statistical dependency parsing, partic-
ularly for long dependency relationships. More im-
portantly, when operating on new domains, the web-
derived selectional preference features show great
potential for achieving robust performance (Section
4.3).
The remainder of this paper is divided as follows.
Section 2 gives a brief introduction of dependency
parsing. Section 3 describes the web-derived selec-
tional preference features. Experimental evaluation
and results are reported in Section 4. Finally, we dis-
cuss related work and draw conclusion in Section 5
and Section 6, respectively.
2 Dependency Parsing
In dependency parsing, we attempt to build head-
modifier (or head-dependent) relations between
words in a sentence. The discriminative parser we
used in this paper is based on the part-factored
model and features of the MSTParser (McDonald et
al., 2005; McDonald and Pereira, 2006; Carreras,
2007). The parsing model can be defined as a con-
ditional distribution p(y|x;w) over each projective
parse tree y for a particular sentence x, parameter-
ized by a vector w. The probability of a parse tree
is
p(y|x;w) = 1
Z(x;w)
exp
{
?
??y
w ??(x, ?)
}
(1)
where Z(x;w) is the partition function and ? are
part-factored feature functions that include head-
1557
modifier parts, sibling parts and grandchild parts.
Given the training set {(xi, yi)}Ni=1, parameter es-
timation for log-linear models generally resolve
around optimization of a regularized conditional
log-likelihood objective w? = argminwL(w)
where
L(w) = ?C
N
?
i=1
logp(yi|xi;w) +
1
2
||w||2 (2)
The parameter C > 0 is a constant dictating the
level of regularization in the model. Since objec-
tive function L(w) is smooth and convex, which is
convenient for standard gradient-based optimization
techniques. In this paper we use the dual exponenti-
ated gradient (EG)2 descent, which is a particularly
effective optimization algorithm for log-linear mod-
els (Collins et al, 2008).
3 Web-Derived Selectional Preference
Features
In this paper, we employ two different feature sets:
a baseline feature set3 which draw upon ?normal?
information source, such as word forms and part-of-
speech (POS) without including the web-derived se-
lectional preference4 features, a feature set conjoins
the baseline features and the web-derived selectional
preference features.
3.1 Web-scale resources
All of our selectional preference features described
in this paper rely on probabilities derived from unla-
beled data. To use the largest amount of data possi-
ble, we exploit web-scale resources. one is web, N-
gram counts are approximated by Google hits. An-
other we use isGoogle V1 (Brants and Franz, 2006).
This N-gram corpus records how often each unique
sequence of words occurs. N-grams appearing 40
2http://groups.csail.mit.edu/nlp/egstra/
3This kind of feature sets are similar to other feature sets in
the literature (McDonald et al, 2005; Carreras, 2007), so we
will not attempt to give a exhaustive description.
4Selectional preference tells us which arguments are plau-
sible for a particular predicate, one way to determine the se-
lectional preference is from co-occurrences of predicates and
arguments in text (Bergsma et al, 2008). In this paper, the
selectional preferences have the same meaning with N-grams,
which model the word-to-word relationships, rather than only
considering the predicates and arguments relationships.
obj
detdet
root
obj
mod
subj
Figure 2: An example of a labeled dependency tree. The
tree contains a special token ?$? which is always the root
of the tree. Each arc is directed from head to modifier and
has a label describing the function of the attachment.
times or more (1 in 25 billion) are kept, and appear
in the n-gram tables. All n-grams with lower counts
are discarded. Co-occurrence probabilities can be
calculated directly from the N-gram counts.
3.2 Web-derived N-gram features
3.2.1 PMI
Previous work on noun compounds bracketing
has used adjacency model (Resnik, 1993) and de-
pendency model (Lauer, 1995) to compute associa-
tion statistics between pairs of words. In this pa-
per we generalize the adjacency and dependency
models by including the pointwise mutual informa-
tion (Church and Hanks, 1900) between all pairs of
words in the dependency tree:
PMI(x, y) = log p(?x y?)
p(?x?)p(?y?)
(3)
where p(?x y?) is the co-occurrence probabilities.
When use the Google V1 corpus, this probabilities
can be calculated directly from the N-gram counts,
while using the Google hits, we send the queries to
the search engine Google5 and all the search queries
are performed as exact matches by using quotation
marks.6
The value of these features is the PMI, if it is de-
fined. If the PMI is undefined, following the work
of (Pitler et al, 2010), we include one of two binary
features:
p(?x y?) = 0 or p(?x?) ? p(?y?) = 0
Besides, we also consider the trigram features be-
5http://www.google.com/
6Google only allows automated querying through the
Google Web API, this involves obtaining a license key, which
then restricts the number of queries to a daily quota of 1000.
However, we obtained a quota of 20,000 queries per day by
sending a request to api-support@google.com for research pur-
poses.
1558
PMI(?hit with?)
xi-word=?hit?, xj-word=?with?, PMI(?hit with?)
xi-word=?hit?, xj-word=?with?, xj-pos=?IN?, PMI(?hit with?)
xi-word=?hit?, xi-pos=?VBD?, xj-word=?with?, PMI(?hit with?)
xi-word=?hit?, b-pos=?ball?, xj-word=?with?, PMI(?hit with?)
xi-word=?hit?, xj-word=?with?, PMI(?hit with?), dir=R, dist=3
. . .
Table 1: An example of the N-gram PMI features and the conjoin features with the baseline.
tween the three words in the dependency tree:
PMI(x, y, z) = log p(?x y z?)
p(?x y?)p(?y z?)
(4)
This kinds of trigram features, for example in MST-
Parser, which can directly capture the sibling and
grandchild features.
We illustrate the PMI features with an example
of dependency parsing tree in Figure 2. In deciding
the dependency between the main verb hit and its ar-
gument headed preposition with, an example of the
N-gram PMI features and the conjoin features with
the baseline are shown in Table 1.
3.2.2 PP-attachment
Propositional phrase (PP) attachment is one of
the hardest problems in English dependency pars-
ing. An English sentence consisting of a subject, a
verb, and a nominal object followed by a preposi-
tional phrase is often ambiguous. Ambiguity resolu-
tion reflects the selectional preference between the
verb and noun with their prepositional phrase. For
example, considering the following two examples:
(1) John hit the ball with the bat.
(2) John hit the ball with the red stripe.
In sentence (1), the preposition with depends on the
main verb hit; but in sentence (2), the prepositional
phrase is a noun attribute and the preposition with
needs to depends on the word ball. To resolve this
kind of ambiguity, there needs to measure the attach-
ment preference. We thus have PP-attachment fea-
tures that determine the PMI association across the
preposition word ?IN?7:
PMIIN (x, z) = log
p(?x IN z?)
p(x)
(5)
7Here, the preposition word ?IN? (e.g., ?with?, ?in?, . . .) is
any token whose part-of-speech is IN
N-gram feature templates
hw, mw, PMI(hw,mw)
hw, ht, mw, PMI(hw,mw)
hw, mw, mt, PMI(hw,mw)
hw, ht, mw, mt, PMI(hw,mw)
. . .
hw, mw, sw
hw, mw, sw, PMI(hw, mw, sw)
hw, mw, gw
hw, mw, gw, PMI(hw, mw, gw)
Table 2: Examples of N-gram feature templates. Each
entry represents a class of indicator for tuples of informa-
tion. For example, ?hw, mw? reprsents a class of indi-
cator features with one feature for each possible combi-
nation of head word and modifier word. Abbreviations:
hw=head word, ht= head POS. st, gt=likewise for sibling
and grandchild.
PMIIN (y, z) = log
p(?y IN z?)
p(y)
(6)
where the word x and y are usually verb and noun,
z is a noun which directly depends on the preposi-
tion word ?IN?. For example in sentence (1), we
would include the features PMIwith(hit, bat) and
PMIwith(ball, bat). If both PMI features exist and
PMIwith(hit, bat) > PMIwith(ball, bat), indicating
to our dependency parsing model that the preposi-
tion word with depends on the verb hit is a good
choice. While in sentence (2), the features include
PMIwith(hit, stripe) and PMIwith(ball, stripe).
3.3 N-gram feature templates
We generate N-gram features by mimicking the
template structure of the original baseline features.
For example, the baseline feature set includes indi-
cators for word-to-word and tag-to-tag interactions
between the head and modifier of a dependency. In
the N-gram feature set, we correspondingly intro-
duce N-gram PMI for word-to-word interactions.
1559
The N-gram feature set for MSTParser is shown
in Table 2. Following McDonald et al (2005),
all features are conjoined with the direction of
attachment as well as the distance between the two
words creating the dependency. In between N-gram
features, we include the form of word trigrams
and PMI of the trigrams. The surrounding word
N-gram features represent the local context of the
selectional preference. Besides, we also present
the second-order feature templates, including the
sibling and grandchild features. These features are
designed to disambiguate cases like coordinating
conjunctions and prepositional attachment. Con-
sider the examples we have shown in section 3.2.2,
for sentence (1), the dependency graph path feature
ball ? with ? bat should have a lower weight
since ball rarely is modified by bat, but is often
seen through them (e.g., a higher weight should be
associated with hit ? with ? bat). In contrast,
for sentence (2), our N-gram features will tell us
that the prepositional phrase is much more likely
to attach to the noun since the dependency graph
path feature ball ? with ? stripe should have a
high weight due to the high strength of selectional
preference between ball and stripe.
Web-derived selectional preference features
based on PMI values are trickier to incorporate
into the dependency parsing model because they
are continuous rather than discrete. Since all the
baseline features used in the literature (McDonald et
al., 2005; Carreras, 2007) take on binary values of 0
or 1, there is a ?mis-match? between the continuous
and binary features. Log-linear dependency parsing
model is sensitive to inappropriately scaled feature.
To solve this problem, we transform the PMI
values into a more amenable form by replacing the
PMI values with their z-score. The z-score of a
PMI value x is x??? , where ? and ? are the mean
and standard deviation of the PMI distribution,
respectively.
4 Experiments
In order to evaluate the effectiveness of our proposed
approach, we conducted dependency parsing exper-
iments in English. The experiments were performed
on the Penn Treebank (PTB) (Marcus et al, 1993),
using a standard set of head-selection rules (Yamada
and Matsumoto, 2003) to convert the phrase struc-
ture syntax of the Treebank into a dependency tree
representation, dependency labels were obtained via
the ?Malt? hard-coded setting.8 We split the Tree-
bank into a training set (Sections 2-21), a devel-
opment set (Section 22), and several test sets (Sec-
tions 0,9 1, 23, and 24). The part-of-speech tags for
the development and test set were automatically as-
signed by the MXPOST tagger10, where the tagger
was trained on the entire training corpus.
Web page hits for word pairs and trigrams are ob-
tained using a simple heuristic query to the search
engine Google.11 Inflected queries are performed
by expanding a bigram or trigram into all its mor-
phological forms. These forms are then submitted as
literal queries, and the resulting hits are summed up.
John Carroll?s suite of morphological tools12 is used
to generate inflected forms of verbs and nouns. All
the search terms are performed as exact matches by
using quotation marks and submitted to the search
engines in lower case.
We measured the performance of the parsers us-
ing the following metrics: unlabeled attachment
score (UAS), labeled attachment score (LAS) and
complete match (CM), which were defined by Hall
et al (2006). All the metrics are calculated as mean
scores per word, and punctuation tokens are consis-
tently excluded.
4.1 Main results
There are some clear trends in the results of Ta-
ble 3. First, performance increases with the order
of the parser: edge-factored model (dep1) has the
lowest performance, adding sibling and grandchild
relationships (dep2) significantly increases perfor-
mance. Similar observations regarding the effect of
model order have also been made by Carreras (2007)
and Koo et al (2008).
Second, note that the parsers incorporating the N-
gram feature sets consistently outperform the mod-
els using the baseline features in all test data sets,
regardless of model order or label usage. Another
8http://w3.msi.vxu.se/ nivre/research/MaltXML.html
9We removed a single 249-word sentence from Section 0 for
computational reasons.
10http://www.inf.ed.ac.uk/resources/nlp/local doc/MXPOST.html
11http://www.google.com/
12http://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html.
1560
Sec dep1 +hits +V1 dep2 +hits +V1 dep1-L +hits-L +V1-L dep2-L +hits-L +V1-L
00 90.39 90.94 90.91 91.56 92.16 92.16 90.11 90.69 90.67 91.94 92.47 92.42
01 91.01 91.60 91.60 92.27 92.89 92.86 90.77 91.39 91.39 91.81 92.38 92.37
23 90.82 91.46 91.39 91.98 92.64 92.59 90.30 90.98 90.92 91.24 91.83 91.77
24 89.53 90.15 90.13 90.81 91.44 91.41 89.42 90.03 90.02 90.30 90.91 90.89
Table 3: Unlabeled accuracies (UAS) and labeled accuracies (LAS) on Section 0, 1, 23, 24. Abbreviation:
dep1/dep2=first-order parser and second-order parser with the baseline features; +hits=N-gram features derived from
the Google hits; +V1=N-gram features derived from the Google V1; suffix-L=labeled parser. Unlabeled parsers are
scored using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
finding is that the N-gram features derived from
Google hits are slightly better than Google V1 due
to the large N-gram coverage, we will discuss later.
As a final note, all the comparisons between the inte-
gration of N-gram features and the baseline features
in Table 3 are mildly significant using the Z-test of
Collins et al (2005) (p < 0.08).
Type Systems UAS CM
D
Yamada and Matsumoto (2003) 90.3 38.7
McDonald et al (2005) 90.9 37.5
McDonald and Pereira (2006) 91.5 42.1
Corston-Oliver et al (2006) 90.9 37.5
Hall et al (2006) 89.4 36.4
Wang et al (2007) 89.2 34.4
Carreras et al (2008) 93.5 -
GoldBerg and Elhadad (2010)? 91.32 40.41
Ours 92.64 46.61
C
Nivre and McDonald (2008)? 92.12 44.37
Martins et al (2008)? 92.87 45.51
Zhang and Clark (2008) 92.1 45.4
S
Koo et al (2008) 93.16 -
Suzuki et al (2009) 93.79 -
Chen et al (2009) 93.16 47.15
Table 4: Comparison of our final results with other best-
performing systems on the whole Section 23. Type
D, C and S denote discriminative, combined and semi-
supervised systems, respectively. ? These papers were
not directly reported the results on this data set, we im-
plemented the experiments in this paper.
To put our results in perspective, we also com-
pare them with other best-performing systems in Ta-
ble 4. To facilitate comparisons with previous work,
we only use Section 23 as the test data. The re-
sults show that our second order model incorpo-
rating the N-gram features (92.64) performs better
than most previously reported discriminative sys-
tems trained on the Treebank. Carreras et al (2008)
reported a very high accuracy using information of
constituent structure of TAG grammar formalism,
while in our system, we do not use such knowl-
edge. When compared to the combined systems, our
system is better than Nivre and McDonald (2008)
and Zhang and Clark (2008), but a slightly worse
than Martins et al (2008). We also compare our
method with the semi-supervised approaches, the
semi-supervised approaches achieved very high ac-
curacies by leveraging on large unlabeled data di-
rectly into the systems for joint learning and decod-
ing, while in our method, we only explore the N-
gram features to further improve supervised depen-
dency parsing performance.
Table 5 shows the details of some other N-gram
sources, where NEWS: created from a large set of
news articles including the Reuters and Gigword
(Graff, 2003) corpora. For a given number of unique
N-gram, using any of these sources does not have
significant difference in Figure 3. Google hits is
the largest N-gram data and shows the best perfor-
mance. The other two are smaller ones, accuracies
increase linearly with the log of the number of types
in the auxiliary data set. Similar observations have
been made by Pitler et al (2010). We see that the
relationship between accuracy and the number of N-
gram is not monotonic for Google V1. The reason
may be that Google V1 does not make detailed pre-
processing, containing many mistakes in the corpus.
Although Google hits is noisier, it has very much
larger coverage of bigrams or trigrams.
Some previous studies also found a log-linear
relationship between unlabeled data (Suzuki and
Isozaki, 2008; Suzuki et al, 2009; Bergsma et al,
2010; Pitler et al, 2010). We have shown that this
trend continues well for dependency parsing by us-
ing web-scale data (NEWS and Google V1).
13Google indexes about more than 8 billion pages and each
contains about 1,000 words on average.
1561
Corpus # of tokens ? # of types
NEWS 3.2B 1 3.7B
Google V1 1,024.9B 40 3.4B
Google hits13 8,000B 100 -
Table 5: N-gram data, with total number of words in the
original corpus (in billions, B). Following (Brants and
Franz, 2006; Pitler et al, 2010), we set the frequency
threshold to filter the data ?, and total number of unique
N-gram (types) remaining in the data.
1e4 1e5 1e6 1e7 1e8 1e991.9
92
92.1
92.2
92.3
92.4
92.5
92.6
92.7
Number of Unique N-grams
UAS
 Sco
re (%
)
NEWSGoogle V1Google hits
Figure 3: There is no data like more data. UAS accu-
racy improves with the number of unique N-grams but
still lower than the Google hits.
4.2 Improvement relative to dependency length
The experiments in (McDonald and Nivre, 2007)
showed a negative impact on the dependency pars-
ing performance from too long dependencies. For
our proposed approach, the improvement relative
to dependency length is shown in Figure 4. From
the Figure, it is seen that our method gives observ-
able better performance when dependency lengths
are larger than 3. The results here show that the
proposed approach improves the dependency pars-
ing performance, particularly for long dependency
relationships.
4.3 Cross-genre testing
In this section, we present the experiments to vali-
date the robustness the web-derived selectional pref-
erences. The intent is to understand how well the
web-derived selectional preferences transfer to other
sources.
The English experiment evaluates the perfor-
mance of our proposed approach when it is trained
1 10 20 300.75
0.8
0.85
0.9
0.95
1
Dependency Length
F1 S
core
 (%)
MST2MST2+N-gram
Figure 4: Dependency length vs. F1 score.
on annotated data from one genre of text (WSJ) and
is used to parse a test set from a different genre: the
biomedical domain related to cancer (PennBioIE.,
2005) with 2,600 parsed sentences. We divided the
data into 500 for training, 100 for development and
others for testing. We created five sets of train-
ing data with 100, 200, 300, 400, and 500 sen-
tences respectively. Figure 5 plots the UAS ac-
curacy as function of training instances. WSJ is
the performance of our second-order dependency
parser trained on section 2-21; WSJ+N-gram is the
performance of our proposed approach trained on
section 2-21; WSJ+BioMed is the performance of
the parser trained on WSJ and biomedical data.
WSJ+BioMed+N-gram is the performance of our
proposed approach trained on WSJ and biomedical
data. The results show that incorporating the web-
scale N-gram features can significantly improve the
dependency parsing performance, and the improve-
ment is much larger than the in-domain testing pre-
sented in Section 4.1, the reason may be that web-
derived N-gram features do not depend directly on
training data and thus work better on new domains.
4.4 Discussion
In this paper, we present a novel method to im-
prove dependency parsing by using web-scale data.
Despite the success, there are still some problems
which should be discussed.
(1) Google hits is less sparse than Google V1
in modeling the word-to-word relationships, but
Google hits are likely to be noisier than Google V1.
It is very appealing to carry out a correlation anal-
1562
100 150 200 250 300 350 400 450 50080
81
82
83
84
85
86
87
88
UAS
 Sco
re (%
)
WSJWSJ+N-gramWSJ+BioMedWSJ+BioMed+N-gram
Figure 5: Adapting a WSJ parser to biomedical text.
WSJ: performance of parser trained only on WSJ;
WSJ+N-gram: performance of our proposed approach
trained only on WSJ; WSJ+BioMed: parser trained on
WSJ and biomedical text; WSJ+BioMed+N-gram: our
approach trained on WSJ and biomedical text.
ysis to determine whether Google hits and Google
V1 are highly correlated. We will leave it for future
research.
(2) Veronis (2005) pointed out that there had been
a debate about reliability of Google hits due to the
inconsistencies of page hits estimates. However, this
estimate is scale-invariant. Assume that when the
number of pages indexed by Google grows, the num-
ber of pages containing a given search term goes to
a fixed fraction. This means that if pages indexed
by Google doubles, then so do the bigrams or tri-
grams frequencies. Therefore, the estimate becomes
stable when the number of indexed pages grows un-
boundedly. Some details are presented in Cilibrasi
and Vitanyi (2007).
5 Related Work
Our approach is to exploit web-derived selectional
preferences to improve the dependency parsing. The
idea of this paper is inspired by the work of Suzuki
et al (2009) and Pitler et al (2010). The former uses
the web-scale data explicitly to create more data for
training the model; while the latter explores the web-
scale N-grams data (Lin et al, 2010) for compound
bracketing disambiguation. Our research, however,
applies the web-scale data (Google hits and Google
V1) to model the word-to-word dependency rela-
tionships rather than compound bracketing disam-
biguation.
Several previous studies have exploited the web-
scale data for word pair acquisition. Keller and
Lapata (2003) evaluated the utility of using web
search engine statistics for unseen bigram. Nakov
and Hearst (2005) demonstrated the effectiveness of
using search engine statistics to improve the noun
compound bracketing. Volk (2001) exploited the
WWWas a corpus to resolve PP attachment ambigu-
ities. Turney (2007) measured the semantic orienta-
tion for sentiment classification using co-occurrence
statistics obtained from the search engines. Bergsma
et al (2010) created robust supervised classifiers
via web-scale N-gram data for adjective ordering,
spelling correction, noun compound bracketing and
verb part-of-speech disambiguation. Our approach,
however, extends these techniques to dependency
parsing, particularly for long dependency relation-
ships, which involves more challenging tasks than
the previous work.
Besides, there are some work exploring the word-
to-word co-occurrence derived from the web-scale
data or a fixed size of corpus (Calvo and Gel-
bukh, 2004; Calvo and Gelbukh, 2006; Yates et al,
2006; Drabek and Zhou, 2000; van Noord, 2007)
for PP attachment ambiguities or shallow parsing.
Johnson and Riezler (2000) incorporated the lex-
ical selectional preference features derived from
British National Corpus (Graff, 2003) into a stochas-
tic unification-based grammar. Abekawa and Oku-
mura (2006) improved Japanese dependency pars-
ing by using the co-occurrence information derived
from the results of automatic dependency parsing of
large-scale corpora. However, we explore the web-
scale data for dependency parsing, the performance
improves log-linearly with the number of parameters
(unique N-grams). To the best of our knowledge,
web-derived selectional preference has not been suc-
cessfully applied to dependency parsing.
6 Conclusion
In this paper, we present a novel method which in-
corporates the web-derived selectional preferences
to improve statistical dependency parsing. The re-
sults show that web-scale data improves the de-
pendency parsing, particularly for long dependency
relationships. There is no data like more data,
performance improves log-linearly with the num-
1563
ber of parameters (unique N-grams). More impor-
tantly, when operating on new domains, the web-
derived selectional preferences show great potential
for achieving robust performance.
Acknowledgments
This work was supported by the National Natural
Science Foundation of China (No. 60875041 and
No. 61070106), and CSIDM project (No. CSIDM-
200805) partially funded by a grant from the Na-
tional Research Foundation (NRF) administered by
the Media Development Authority (MDA) of Singa-
pore. We thank the anonymous reviewers for their
insightful comments.
References
T. Abekawa and M. Okumura. 2006. Japanese depen-
dency parsing using co-occurrence information and a
combination of case elements. In Proceedings of ACL-
COLING.
S. Bergsma, D. Lin, and R. Goebel. 2008. Discriminative
learning of selectional preference from unlabeled text.
In Proceedings of EMNLP, pages 59-68.
S. Bergsma, E. Pitler, and D. Lin. 2010. Creating robust
supervised classifier via web-scale N-gram data. In
Proceedings of ACL.
T. Brants and Alex Franz. 2006. The Google Web 1T
5-gram Corpus Version 1.1. LDC2006T13.
H. Calvo and A. Gelbukh. 2004. Acquiring selec-
tional preferences from untagged text for prepositional
phrase attachment disambiguation. In Proceedings of
VLDB.
H. Calvo and A. Gelbukh. 2006. DILUCT: An open-
source Spanish dependency parser based on rules,
heuristics, and selectional preferences. In Lecture
Notes in Computer Science 3999, pages 164-175.
X. Carreras. 2007. Experiments with a higher-order pro-
jective dependency parser. In Proceedings of EMNLP-
CoNLL, pages 957-961.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In Proceedings of CoNLL.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.
2000. BLLIP 1987-89 WSJ Corpus Release 1, LDC
No. LDC2000T43.Linguistic Data Consortium.
W. Chen, D. Kawahara, K. Uchimoto, and Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of EMNLP,
pages 570-579.
K. W. Church and P. Hanks. 1900. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22-29.
R. L. Cilibrasi and P. M. B. Vitanyi. 2007. The Google
similarity distance. IEEE Transaction on Knowledge
and Data Engineering, 19(3):2007. pages 370-383.
M. Collins, A. Globerson, T. Koo, X. Carreras, and P.
L. Bartlett. 2008. Exponentiated gradient algorithm
for conditional random fields and max-margin markov
networks. Journal of Machine Learning Research,
pages 1775?1822.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of ACL, pages 531-540.
S. Corston-Oliver, A. Aue, Kevin. Duh, and E. Ringger.
2006. Multilingual dependency parsing using bayes
point machines. In Proceedings of NAACL.
H. Daume? III. 2007. Frustrating easy domain adaptation.
In Proceedings of ACL.
E. F. Drabek and Q. Zhou. 2000. Using co-occurrence
statistics as an information source for partial parsing of
Chinese. In Proceedings of Second Chinese Language
Processing Workshop, ACL, pages 22-28.
Y. GoldBerg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Proceedings of NAACL, pages 742-750.
D. Graff. 2003. English Gigaword, LDC2003T05.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discrimina-
tive classifier for deterministic dependency parsing. In
Proceedings of ACL, pages 316-323.
M. Johnson and S. Riezler. 2000. Exploiting auxiliary
distribution in stochastic unification-based garmmars.
In Proceedings of NAACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceedings
of ACL, pages 595-603.
F. Keller and M. Lapata. 2003. Using the web to ob-
tain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459-484.
M. Lapata and F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1), pages 1-30.
M. Lauer. 1995. Corpus statistics meet the noun com-
pound: some empirical results. In Proceedings of
ACL.
D. K. Lin, H. Church, S. Ji, S. Sekine, D. Yarowsky, S.
Bergsma, K. Patil, E. Pitler, E. Lathbury, V Rao, K.
Dalwani, and S. Narsale. 2010. New tools for web-
scale n-grams. In Proceedings of LREC.
M.P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. Computational Linguistics.
1564
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In Proceedings
of EMNLP, pages 157-166.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of ACL.
D. McClosky, E. Charniak, and M. Johnson. 2010. Au-
tomatic Domain Adapatation for Parsing. In Proceed-
ings of NAACL-HLT.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proceedings of EMNLP-CoNLL.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of EACL, pages 81-88.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL, pages 91-98.
P. Nakov and M. Hearst. 2005. Search engine statis-
tics beyond the n-gram: application to noun compound
bracketing. In Proceedings of CoNLL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL, pages 950-958.
G. van Noord. 2007. Using self-trained bilexical pref-
erences to improve disambiguation accuracy. In Pro-
ceedings of IWPT, pages 1-10.
PennBioIE. 2005. Mining the bibliome project, 2005.
http:bioie.ldc.upenn.edu/.
E. Pitler, S. Bergsma, D. Lin, and K. Church. 2010. Us-
ing web-scale N-grams to improve base NP parsing
performance. In Proceedings of COLING, pages 886-
894.
P. Resnik. 1993. Selection and information: a class-
based approach to lexical relationships. Ph.D. thesis,
University of Pennsylvania.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proceed-
ings of EMNLP, pages 551-560.
J. Suzuki and H. Isozaki. 2008. Semi-supervised sequen-
tial labeling and segmentation using giga-word scale
unlabeled data. In Proceedings of ACL, pages 665-
673.
P. D. Turney. 2003. Measuring praise and criticism:
Inference of semantic orientation from association.
ACM Transactions on Information Systems, 21(4).
J. Veronis. 2005. Web: Google adjusts its counts. Jean
Veronis? blog: http://aixtal.blogsplot.com/2005/03/
web-google-adjusts-its-count.html.
M. Volk. 2001. Exploiting the WWW as corpus to re-
solve PP attachment ambiguities. In Proceedings of
the Corpus Linguistics.
Q. I. Wang, D. Lin, and D. Schuurmans. 2007. Simple
training of dependency parsers via structured boosting.
In Proceedings of IJCAI, pages 1756-1762.
Yamada and Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proceedings
of IWPT, pages 195-206.
A. Yates, S. Schoenmackers, and O. Etzioni. 2006. De-
tecting parser errors using web-based semantic filters.
In Proceedings of EMNLP, pages 27-34.
Y. Zhang and S. Clark. 2008. A tale of two parsers: in-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of EMNLP, pages 562-571.
1565
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1754?1763,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Syntactic Patterns versus Word Alignment: Extracting Opinion Targets
from Online Reviews
Kang Liu, Liheng Xu and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
{kliu, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Mining opinion targets is a fundamen-
tal and important task for opinion min-
ing from online reviews. To this end,
there are usually two kinds of methods:
syntax based and alignment based meth-
ods. Syntax based methods usually ex-
ploited syntactic patterns to extract opin-
ion targets, which were however prone to
suffer from parsing errors when dealing
with online informal texts. In contrast,
alignment based methods used word align-
ment model to fulfill this task, which could
avoid parsing errors without using pars-
ing. However, there is no research fo-
cusing on which kind of method is more
better when given a certain amount of re-
views. To fill this gap, this paper empiri-
cally studies how the performance of these
two kinds of methods vary when chang-
ing the size, domain and language of the
corpus. We further combine syntactic pat-
terns with alignment model by using a par-
tially supervised framework and investi-
gate whether this combination is useful or
not. In our experiments, we verify that
our combination is effective on the corpus
with small and medium size.
1 Introduction
With the rapid development of Web 2.0, huge
amount of user reviews are springing up on the
Web. Mining opinions from these reviews be-
come more and more urgent since that customers
expect to obtain fine-grained information of prod-
ucts and manufacturers need to obtain immediate
feedbacks from customers. In opinion mining, ex-
tracting opinion targets is a basic subtask. It is
to extract a list of the objects which users express
their opinions on and can provide the prior infor-
mation of targets for opinion mining. So this task
has attracted many attentions. To extract opin-
ion targets, pervious approaches usually relied on
opinion words which are the words used to ex-
press the opinions (Hu and Liu, 2004a; Popescu
and Etzioni, 2005; Liu et al, 2005; Wang and
Wang, 2008; Qiu et al, 2011; Liu et al, 2012). In-
tuitively, opinion words often appear around and
modify opinion targets, and there are opinion re-
lations and associations between them. If we have
known some words to be opinion words, the words
which those opinion words modify will have high
probability to be opinion targets.
Therefore, identifying the aforementioned opin-
ion relations between words is important for ex-
tracting opinion targets from reviews. To fulfill
this aim, previous methods exploited the words
co-occurrence information to indicate them (Hu
and Liu, 2004a; Hu and Liu, 2004b). Obviously,
these methods cannot obtain precise extraction be-
cause of the diverse expressions by reviewers, like
long-span modified relations between words, etc.
To handle this problem, several methods exploited
syntactic information, where several heuristic pat-
terns based on syntactic parsing were designed
(Popescu and Etzioni, 2005; Qiu et al, 2009; Qiu
et al, 2011). However, the sentences in online
reviews usually have informal writing styles in-
cluding grammar mistakes, typos, improper punc-
tuation etc., which make parsing prone to gener-
ate mistakes. As a result, the syntax-based meth-
ods which heavily depended on the parsing per-
formance would suffer from parsing errors (Zhang
et al, 2010). To improve the extraction perfor-
mance, we can only employ some exquisite high-
precision patterns. But this strategy is likely to
miss many opinion targets and has lower recall
with the increase of corpus size. To resolve these
problems, Liu et al (2012) formulated identifying
opinion relations between words as an monolin-
gual alignment process. A word can find its cor-
responding modifiers by using a word alignment
1754
Figure 1: Mining Opinion Relations between Words using Partially Supervised Alignment Model
model (WAM). Without using syntactic parsing,
the noises from parsing errors can be effectively
avoided. Nevertheless, we notice that the align-
ment model is a statistical model which needs suf-
ficient data to estimate parameters. When the data
is insufficient, it would suffer from data sparseness
and may make the performance decline.
Thus, from the above analysis, we can observe
that the size of the corpus has impacts on these
two kinds of methods, which arises some impor-
tant questions: how can we make selection be-
tween syntax based methods and alignment based
method for opinion target extraction when given
a certain amount of reviews? And which kind of
methods can obtain better extraction performance
with the variation of the size of the dataset? Al-
though (Liu et al, 2012) had proved the effective-
ness of WAM, they mainly performed experiments
on the dataset with medium size. We are still curi-
ous about that when the size of dataset is larger
or smaller, can we obtain the same conclusion?
To our best knowledge, these problems have not
been studied before. Moreover, opinions may be
expressed in different ways with the variation of
the domain and language of the corpus. When the
domain or language of the corpus is changed, what
conclusions can we obtain? To answer these ques-
tions, in this paper, we adopt a unified framework
to extract opinion targets from reviews, in the key
component of which we vary the methods between
syntactic patterns and alignment model. Then we
run the whole framework on the corpus with dif-
ferent size (from #500 to #1, 000, 000), domain
(three domains) and language (Chinese and En-
glish) to empirically assess the performance varia-
tions and discuss which method is more effective.
Furthermore, this paper naturally addresses an-
other question: is it useful for opinion targets ex-
traction when we combine syntactic patterns and
word alignment model into a unified model? To
this end, we employ a partially supervised align-
ment model (PSWAM) like (Gao et al, 2010; Liu
et al, 2013). Based on the exquisitely designed
high-precision syntactic patterns, we can obtain
some precisely modified relations between words
in sentences, which provide a portion of links of
the full alignments. Then, these partial alignment
links can be regarded as the constrains for a stan-
dard unsupervised word alignment model. And
each target candidate would find its modifier un-
der the partial supervision. In this way, the er-
rors generated in standard unsupervised WAM can
be corrected. For example in Figure 1, ?kindly?
and ?courteous? are incorrectly regarded as the
modifiers for ?foods? if the WAM is performed
in an whole unsupervised framework. However,
by using some high-precision syntactic patterns,
we can assert ?courteous? should be aligned to
?services?, and ?delicious? should be aligned to
?foods?. Through combination under partial su-
pervision, we can see ?kindly? and ?courteous?
are correctly linked to ?services?. Thus, it?s rea-
sonable to expect to yield better performance than
traditional methods. As mentioned in (Liu et al,
2013), using PSWAM can not only inherit the
advantages of WAM: effectively avoiding noises
from syntactic parsing errors when dealing with
informal texts, but also can improve the mining
performance by using partial supervision. How-
ever, is this kind of combination always useful for
opinion target extraction? To access this problem,
we also make comparison between PSWAM based
method and the aforementioned methods in the
same corpora with different size, language and do-
main. The experimental results show the combina-
tion by using PSWAM can be effective on dataset
with small and medium size.
1755
2 Related Work
Opinion target extraction isn?t a new task for opin-
ion mining. There are much work focusing on
this task, such as (Hu and Liu, 2004b; Ding et al,
2008; Li et al, 2010; Popescu and Etzioni, 2005;
Wu et al, 2009). Totally, previous studies can be
divided into two main categories: supervised and
unsupervised methods.
In supervised approaches, the opinion target ex-
traction task was usually regarded as a sequence
labeling problem (Jin and Huang, 2009; Li et al,
2010; Ma and Wan, 2010; Wu et al, 2009; Zhang
et al, 2009). It?s not only to extract a lexicon or list
of opinion targets, but also to find out each opin-
ion target mentions in reviews. Thus, the contex-
tual words are usually selected as the features to
indicate opinion targets in sentences. And classi-
cal sequence labeling models are used to train the
extractor, such as CRFs (Li et al, 2010), HMM
(Jin and Huang, 2009) etc.. Jin et al (2009) pro-
posed a lexicalized HMM model to perform opin-
ion mining. Both Li et al (2010) and Ma et al
(2010) used CRFs model to extract opinion tar-
gets in reviews. Specially, Li et al proposed a
Skip-Tree CRF model for opinion target extrac-
tion, which exploited three structures including
linear-chain structure, syntactic structure, and con-
junction structure. However, the main limitation
of these supervised methods is the need of labeled
training data. If the labeled training data is insuf-
ficient, the trained model would have unsatisfied
extraction performance. Labeling sufficient train-
ing data is time and labor consuming. And for dif-
ferent domains, we need label data independently,
which is obviously impracticable.
Thus, many researches focused on unsupervised
methods, which are mainly to extract a list of opin-
ion targets from reviews. Similar to ours, most ap-
proaches regarded opinion words as the indicator
for opinion targets. (Hu and Liu, 2004a) regarded
the nearest adjective to an noun/noun phrase as
its modifier. Then it exploited an association
rule mining algorithm to mine the associations be-
tween them. Finally, the frequent explicit prod-
uct features can be extracted in a bootstrapping
process by further combining item?s frequency in
dataset. Only using nearest neighbor rule to mine
the modifier for each candidate cannot obtain pre-
cise results. Thus, (Popescu and Etzioni, 2005)
used syntax information to extract opinion targets,
which designed some syntactic patterns to capture
the modified relations between words. The experi-
mental results showed that their method had better
performance than (Hu and Liu, 2004a). Moreover,
(Qiu et al, 2011) proposed a Double Propagation
method to expand sentiment words and opinion
targets iteratively, where they also exploited syn-
tactic relations between words. Specially, (Qiu
et al, 2011) didn?t only design syntactic patterns
for capturing modified relations, but also designed
patterns for capturing relations among opinion tar-
gets and relations among opinion words. How-
ever, the main limitation of Qiu?s method is that
the patterns based on dependency parsing tree may
miss many targets for the large corpora. There-
fore, Zhang et al (2010) extended Qiu?s method.
Besides the patterns used in Qiu?s method, they
adopted some other special designed patterns to
increase recall. In addition they used the HITS
(Kleinberg, 1999) algorithm to compute opinion
target confidences to improve the precision. (Liu
et al, 2012) formulated identifying opinion re-
lations between words as an alignment process.
They used a completely unsupervised WAM to
capture opinion relations in sentences. Then the
opinion targets were extracted in a standard ran-
dom walk framework where two factors were con-
sidered: opinion relevance and target importance.
Their experimental results have shown that WAM
was more effective than traditional syntax-based
methods for this task. (Liu et al, 2013) extend
Liu?s method, which is similar to our method and
also used a partially supervised alignment model
to extract opinion targets from reviews. We notice
these two methods ((Liu et al, 2012) and (Liu et
al., 2013)) only performed experiments on the cor-
pora with a medium size. Although both of them
proved that WAM model is better than the meth-
ods based on syntactic patterns, they didn?t dis-
cuss the performance variation when dealing with
the corpora with different sizes, especially when
the size of the corpus is less than 1,000 and more
than 10,000. Based on their conclusions, we still
don?t know which kind of methods should be se-
lected for opinion target extraction when given a
certain amount of reviews.
3 Opinion Target Extraction
Methodology
To extract opinion targets from reviews, we adopt
the framework proposed by (Liu et al, 2012),
which is a graph-based extraction framework and
1756
has two main components as follows.
1) The first component is to capture opinion
relations in sentences and estimate associations
between opinion target candidates and potential
opinion words. In this paper, we assume opinion
targets to be nouns or noun phrases, and opinion
words may be adjectives or verbs, which are usu-
ally adopted by (Hu and Liu, 2004a; Qiu et al,
2011; Wang and Wang, 2008; Liu et al, 2012).
And a potential opinion relation is comprised of
an opinion target candidate and its corresponding
modified word.
2) The second component is to estimate the
confidence of each candidate. The candidates with
higher confidence scores than a threshold will be
extracted as opinion targets. In this procedure, we
formulate the associations between opinion target
candidates and potential opinion words in a bipar-
tite graph. A random walk based algorithm is em-
ployed on this graph to estimate the confidence of
each target candidate.
In this paper, we fix the method in the sec-
ond component and vary the algorithms in the
first component. In the first component, we re-
spectively use syntactic patterns and unsupervised
word alignment model (WAM) to capture opinion
relations. In addition, we employ a partially super-
vised word alignment model (PSWAM) to incor-
porate syntactic information into WAM. In exper-
iments, we run the whole framework on the differ-
ent corpora to discuss which method is more effec-
tive. In the following subsections, we will present
them in detail.
3.1 The First Component: Capturing
Opinion Relations and Estimating
Associations between Words
3.1.1 Syntactic Patterns
To capture opinion relations in sentences by using
syntactic patterns, we employ the manual designed
syntactic patterns proposed by (Qiu et al, 2011).
Similar to Qiu, only the syntactic patterns based
on the direct dependency are employed to guar-
antee the extraction qualities. The direct depen-
dency has two types. The first type indicates that
one word depends on the other word without any
additional words in their dependency path. The
second type denotes that two words both depend
on a third word directly. Specifically, we employ
Minipar1 to parse sentences. To further make syn-
1http://webdocs.cs.ualberta.ca/lindek/minipar.htm
tactic patterns precisely, we only use a few depen-
dency relation labels outputted by Minipar, such
as mod, pnmod, subj, desc etc. To make a clear
explanation, we give out some syntactic pattern
examples in Table 1. In these patterns, OC is a
potential opinion word which is an adjective or a
verb. TC is an opinion target candidate which is
a noun or noun phrase. The item on the arrows
means the dependency relation type. The item in
parenthesis denotes the part-of-speech of the other
word. In these examples, the first three patterns
are based on the first direct dependency type and
the last two patterns are based on the second direct
dependency type.
Pattern#1: <OC> mod????<TC>
Example: This phone has an amazing design
Pattern#2: <TC> obj???<OC>
Example: I like this phone very much
Pattern#3: <OC> pnmod?????<TC>
Example: the buttons easier to use
Pattern#4: <OC> mod????(NN) subj????<TC>
Example: IPhone is a revolutionary smart phone
Pattern#5: <OC> pred????(VBE) subj????<TC>
Example: The quality of LCD is good
Table 1: Some Examples of Used Syntactic Pat-
terns
3.1.2 Unsupervised Word Alignment Model
In this subsection, we present our method for cap-
turing opinion relations using unsupervised word
alignment model. Similar to (Liu et al, 2012),
every sentence in reviews is replicated to gener-
ate a parallel sentence pair, and the word align-
ment algorithm is applied to the monolingual sce-
nario to align a noun/noun phase with its modi-
fiers. We select IBM-3 model (Brown et al, 1993)
as the alignment model. Formally, given a sen-
tence S = {w1, w2, ..., wn}, we have
Pibm3(A|S)
?
N?
i=1
n(?i|wi)
N?
j=1
t(wj |waj )d(j|aj , N)
(1)
where t(wj |waj ) models the co-occurrence infor-
mation of two words in dataset. d(j|aj , n) mod-
els word position information, which describes the
probability of a word in position aj aligned with a
word in position j. And n(?i|wi) describes the
ability of a word for modifying (being modified
by) several words. ?i denotes the number of words
1757
that are aligned with wi. In our experiments, we
set ?i = 2.
Since we only have interests on capturing opin-
ion relations between words, we only pay at-
tentions on the alignments between opinion tar-
get candidates (nouns/noun phrases) and potential
opinion words (adjectives/verbs). If we directly
use the alignment model, a noun (noun phrase)
may align with other unrelated words, like prepo-
sitions or conjunctions and so on. Thus, we set
constrains on the model: 1) Alignment links must
be assigned among nouns/noun phrases, adjec-
tives/verbs and null words. Aligning to null words
means that this word has no modifier or modifies
nothing; 2) Other unrelated words can only align
with themselves.
3.1.3 Combining Syntax-based Method with
Alignment-based Method
In this subsection, we try to combine syntactic in-
formation with word alignment model. As men-
tioned in the first section, we adopt a partially
supervised alignment model to make this com-
bination. Here, the opinion relations obtained
through the high-precision syntactic patterns (Sec-
tion 3.1.1) are regarded as the ground truth and
can only provide a part of full alignments in sen-
tences. They are treated as the constrains for the
word alignment model. Given some partial align-
ment links A? = {(k, ak)|k ? [1, n], ak ? [1, n]},
the optimal word alignment A? = {(i, ai)|i ?
[1, n], ai ? [1, n]} can be obtained as A? =
argmax
A
P (A|S, A?), where (i, ai) means that a
noun (noun phrase) at position i is aligned with
its modifier at position ai.
Since the labeled data provided by syntactic pat-
terns is not a full alignment, we adopt a EM-based
algorithm, named as constrained hill-climbing al-
gorithm(Gao et al, 2010), to estimate the parame-
ters in the model. In the training process, the con-
strained hill-climbing algorithm can ensure that
the final model is marginalized on the partial align-
ment links. Particularly, in the E step, their method
aims to find out the alignments which are consis-
tent to the alignment links provided by syntactic
patterns, where there are main two steps involved.
1) Optimize towards the constraints. This step
aims to generate an initial alignments for align-
ment model (IBM-3 model in our method), which
can be close to the constraints. First, a simple
alignment model (IBM-1, IBM-2, HMM etc.) is
trained. Then, the evidence being inconsistent
to the partial alignment links will be got rid of
by using the move operator operator mi,j which
changes aj = i and the swap operator sj1,j2 which
exchanges aj1 and aj2 . The alignment is updated
iteratively until no additional inconsistent links
can be removed.
2) Towards the optimal alignment under the
constraints. This step aims to optimize towards
the optimal alignment under the constraints which
starts from the aforementioned initial alignments.
Gao et.al. (2010) set the corresponding cost value
of the invalid move or swap operation in M and
S to be negative, where M and S are respec-
tively called Moving Matrix and Swapping Ma-
trix, which record all possible move and swap
costs between two different alignments. In this
way, the invalid operators will never be picked
which can guarantee that the final alignment links
to have high probability to be consistent with the
partial alignment links provided by high-precision
syntactic patterns.
Then in M-step, evidences from the neighbor of
final alignments are collected so that we can pro-
duce the estimation of parameters for the next iter-
ation. In the process, those statistics which come
from inconsistent alignment links aren?t be picked
up. Thus, we have
P (wi|wai , A?)
=
{ ?, otherwise
P (wi|wai) + ?, inconsistent with A?(2)
where ? means that we make soft constraints on
the alignment model. As a result, we expect some
errors generated through high-precision patterns
(Section 3.1.1) may be revised in the alignment
process.
3.2 Estimating Associations between Words
After capturing opinion relations in sentences, we
can obtain a lot of word pairs, each of which is
comprised of an opinion target candidate and its
corresponding modified word. Then the condi-
tional probabilities between potential opinion tar-
get wt and potential opinion word wo can be es-
timated by using maximum likelihood estimation.
Thus, we have P (wt|wo) = Count(wt,wo)Count(wo) , where
Count(?) means the item?s frequency informa-
tion. P (wt|wo) means the conditional probabili-
ties between two words. At the same time, we can
obtain conditional probability P (wo|wt). Then,
1758
similar to (Liu et al, 2012), the association be-
tween an opinion target candidate and its modifier
is estimated as follows. Association(wt, wo) =
(?? P (wt|wo) + (1? ?)? P (wo|wt))?1, where
? is the harmonic factor. We set ? = 0.5 in our
experiments.
3.3 The Second Component: Estimating
Candidate Confidence
In the second component, we adopt a graph-based
algorithm used in (Liu et al, 2012) to compute
the confidence of each opinion target candidate,
and the candidates with higher confidence than the
threshold will be extracted as the opinion targets.
Here, opinion words are regarded as the impor-
tant indicators. We assume that two target candi-
dates are likely to belong to the similar category, if
they are modified by similar opinion words. Thus,
we can propagate the opinion target confidences
through opinion words.
To model the mined associations between
words, a bipartite graph is constructed, which
is defined as a weighted undirected graph G =
(V,E,W ). It contains two kinds of vertex: opin-
ion target candidates and potential opinion words,
respectively denoted as vt ? V and vo ? V .
As shown in Figure 2, the white vertices repre-
sent opinion target candidates and the gray ver-
tices represent potential opinion words. An edge
evt,vo ? E between vertices represents that there is
an opinion relation, and the weight w on the edge
represents the association between two words.
Figure 2: Modeling Opinion Relations between
Words in a Bipartite Graph
To estimate the confidence of each opinion tar-
get candidate, we employ a random walk algo-
rithm on our graph, which iteratively computes
the weighted average of opinion target confidences
from neighboring vertices. Thus we have
Ci+1 = (1? ?)?M ?MT ? Ci + ? ? I (3)
where Ci+1 and Ci respectively represent the
opinion target confidence vector in the (i + 1)th
and ith iteration. M is the matrix of word asso-
ciations, where Mi,j denotes the association be-
tween the opinion target candidate i and the po-
tential opinion word j. And I is defined as the
prior confidence of each candidate for opinion tar-
get. Similar to (Liu et al, 2012), we set each item
in Iv = tf(v)idf(v)?
v tf(v)idf(v)
, where tf(v) is the term fre-
quency of v in the corpus, and df(v) is computed
by using the Google n-gram corpus2. ? ? [0, 1]
represents the impact of candidate prior knowl-
edge on the final estimation results. In experi-
ments, we set ? = 0.4. The algorithm run un-
til convergence which is achieved when the confi-
dence on each node ceases to change in a tolerance
value.
4 Experiments
4.1 Datasets and Evaluation Metrics
In this section, to answer the questions men-
tioned in the first section, we collect a large
collection named as LARGE, which includes re-
views from three different domains and differ-
ent languages. This collection was also used
in (Liu et al, 2012). In the experiments, re-
views are first segmented into sentences accord-
ing to punctuation. The detailed statistical in-
formation of the used collection is shown in Ta-
ble 2, where Restaurant is crawled from the Chi-
nese Web site: www.dianping.com. The Hotel and
MP3 are used in (Wang et al, 2011), which are re-
spectively crawled from www.tripadvisor.com and
www.amazon.com. For each dataset, we perform
random sampling to generate testing set with dif-
ferent sizes, where we use sampled subsets with
#sentences = 5? 102, 103, 5? 103, 104, 5?
104, 105 and 106 sentences respectively. Each
Domain Language Sentence Reviews
Restaurant Chinese 1,683,129 395,124
Hotel English 1,855,351 185,829
MP3 English 289,931 30,837
Table 2: Experimental Dataset
sentence is tokenized, part-of-speech tagged by
using Stanford NLP tool3, and parsed by using
Minipar toolkit. And the method of (Zhu et al,
2009) is used to identify noun phrases.
2http://books.google.com/ngrams/datasets
3http://nlp.stanford.edu/software/tagger.shtml
1759
We select precision and recall as the metrics.
Specifically, to obtain the ground truth, we man-
ually label all opinion targets for each subset. In
this process, three annotators are involved. First,
every noun/noun phrase and its contexts in review
sentences are extracted. Then two annotators were
required to judge whether every noun/noun phrase
is opinion target or not. If a conflict happens, a
third annotator will make judgment for final re-
sults. The average inter-agreements is 0.74. We
also perform a significant test, i.e., a t-test with a
default significant level of 0.05.
4.2 Compared Methods
We select three methods for comparison as fol-
lows.
? Syntax: It uses syntactic patterns mentioned
in Section 3.1.1 in the first component to
capture opinion relations in reviews. Then
the associations between words are estimated
and the graph based algorithm proposed in
the second component (Section 3.3) is per-
formed to extract opinion targets.
? WAM: It is similar to Syntax, where the only
difference is that WAM uses unsupervised
WAM (Section 3.1.2) to capture opinion re-
lations.
? PSWAM is similar to Syntax and WAM,
where the difference is that PSWAM uses the
method mentioned in Section 3.1.3 to capture
opinion relations, which incorporates syntac-
tic information into word alignment model by
using partially supervised framework.
The experimental results on different domains are
respectively shown in Figure 3, 4 and 5.
4.3 Syntax based Methods vs. Alignment
based Methods
Comparing Syntax with WAM and PSWAM, we
can obtain the following observations:
Figure 3: Experimental results on Restaurant
Figure 4: Experimental results on Hotel
Figure 5: Experimental results on MP3
1) When the size of the corpus is small, Syntax
has better precision than alignment based meth-
ods (WAM and PSWAM). We believe the reason
is that the high-precision syntactic patterns em-
ployed in Syntax can effectively capture opinion
relations in a small amount of texts. In contrast,
the methods based on word alignment model may
suffer from data sparseness for parameter estima-
tion, so the precision is lower.
2) However, when the size of the corpus in-
creases, the precision of Syntax decreases, even
worse than alignment based methods. We believe
it?s because more noises were introduced from
parsing errors with the increase of the size of the
corpus , which will have more negative impacts on
extraction results. In contrast, for estimating the
parameters of alignment based methods, the data
is more sufficient, so the precision is better com-
pared with syntax based method.
3) We also observe that recall of Syntax is
worse than other two methods. It?s because the
human expressions of opinions are diverse and the
manual designed syntactic patterns are limited to
capture all opinion relations in sentences, which
may miss an amount of correct opinion targets.
4) It?s interesting that the performance gap be-
tween these three methods is smaller with the in-
crease of the size of the corpus (more than 50,000).
We guess the reason is that when the data is suffi-
cient enough, we can obtain sufficient statistics for
each opinion target. In such situation, the graph-
based ranking algorithm in the second component
will be apt to be affected by the frequency infor-
mation, so the final performance could not be sen-
sitive to the performance of opinion relations iden-
1760
tification in the first component. Thus, in this situ-
ation, we can get conclusion that there is no obvi-
ously difference on performance between syntax-
based approach and alignment-based approach.
5) From the results on dataset with different lan-
guages and different domains, we can obtain the
similar observations. It indicates that choosing ei-
ther syntactic patterns or word alignment model
for extracting opinion targets can take a few con-
sideration on the language and domain of the cor-
pus.
Thus, based on the above observations, we can
draw the following conclusions: making chooses
between different methods is only related to the
size of the corpus. The method based on syn-
tactic patterns is more suitable for small cor-
pus (#sentences < 5 ? 103 shown in our
experiments). And word alignment model is
more suitable for medium corpus (5 ? 103 <
#sentences < 5 ? 104). Moreover, when the
size of the corpus is big enough, the performance
of two kinds of methods tend to become the same
(#sentences ? 105 shown in our experiments).
4.4 Is It Useful Combining Syntactic Patterns
with Word Alignment Model
In this subsection, we try to see whether combin-
ing syntactic information with alignment model by
using PSWAM is effective or not for opinion tar-
get extraction. From the results in Figure 3, 4 and
5, we can see that PSWAM has the similar recall
compared with WAM in all datasets. PSWAM
outperforms WAM on precision in all dataset. But
the precision gap between PSWAM and WAM
decreases when the size of the corpus increases.
When the size is larger than 5 ? 104, the perfor-
mance of these two methods is almost the same.
We guess the reason is that more noises from pars-
ing errors will be introduced by syntactic patterns
with the increase of the size of corpus , which have
negative impacts on alignment performance. At
the same time, as mentioned above, a great deal of
reviews will bring sufficient statistics for estimat-
ing parameters in alignment model, so the roles
of partial supervision from syntactic information
will be covered by frequency information used in
our graph based ranking algorithm.
Compared with State-of-the-art Methods.
However, it?s not say that this combination is
not useful. From the results, we still see that
PSWAM outperforms WAM in all datasets on
precision when size of corpus is smaller than
5 ? 104. To further prove the effectiveness of
our combination, we compare PSWAM with some
state-of-the-art methods, including Hu (Hu and
Liu, 2004a), which extracted frequent opinion tar-
get words based on association mining rules, DP
(Qiu et al, 2011), which extracted opinion tar-
gets through syntactic patterns, and LIU (Liu et
al., 2012), which fulfilled this task by using un-
supervised WAM. The parameter settings in these
baselines are the same as the settings in the orig-
inal papers. Because of the space limitation, we
only show the results on Restaurant and Hotel, as
shown in Figure 6 and 7.
Figure 6: Compared with the State-of-the-art
Methods on Restaurant
Figure 7: Compared with the State-of-the-art
Methods on Hotel
From the experimental results, we can obtain
the following observations. PSWAM outperforms
other methods in most datasets. This indicates
that our method based on PSWAM is effective
for opinion target extraction. Especially compared
PSWAM with LIU, both of which are based on
word alignment model, we can see PSWAM iden-
tifies opinion relations by performing WAM under
partial supervision, which can effectively improve
the precision when dealing with small and medium
corpus. However, these improvements are limited
when the size of the corpus increases, which has
the similar observations obtained above.
The Impact of Syntactic Information on
Word Alignment Model. Although we have
prove the effectiveness of PSWAM in the corpus
with small and medium size, we are still curious
about how the performance varies when we incor-
1761
porate different amount of syntactic information
into WAM. In this experiment, we rank the used
syntactic patterns mentioned in Section 3.1.1 ac-
cording to the quantities of the extracted alignment
links by these patterns. Then, to capture opin-
ion relations, we respectively use top N syntactic
patterns according to frequency mentioned above
to generate partial alignment links for PSWAM in
section 3.1.3. We respectively define N=[1,7]. The
larger is N , the more syntactic information is in-
corporated. Because of the space limitation, only
the average performance of all dataset is shown in
Figure 8.
Figure 8: The Impacts of Different Syntactic In-
formation on Word Alignment Model
In Figure 8, we can observe that the syntactic in-
formation mainly have effect on precision. When
the size of the corpus is small, the opinion rela-
tions mined by high-precision syntactic patterns
are usually correct, so incorporating more syntac-
tic information can improve the precision of word
alignment model more. However, when the size of
the corpus increases, incorporating more syntactic
information has little impact on precision.
5 Conclusions and Future Work
This paper discusses the performance variation of
syntax based methods and alignment based meth-
ods on opinion target extraction task for the dataset
with different sizes, different languages and dif-
ferent domains. Through experimental results, we
can see that choosing which method is not related
with corpus domain and language, but strongly
associated with the size of the corpus . We can
conclude that syntax-based method is likely to be
more effective when the size of the corpus is small,
and alignment-based methods are more useful for
the medium size corpus. We further verify that in-
corporating syntactic information into word align-
ment model by using PSWAM is effective when
dealing with the corpora with small or medium
size. When the size of the corpus is larger and
larger, the performance gap between syntax based,
WAM and PSWAM will decrease.
In future work, we will extract opinion targets
based on not only opinion relations. Other seman-
tic relations, such as the topical associations be-
tween opinion targets (or opinion words) should
also be employed. We believe that considering
multiple semantic associations will help to im-
prove the performance. In this way, how to model
heterogenous relations in a unified model for opin-
ion targets extraction is worthy to be studied.
Acknowledgement
This work was supported by the National Natu-
ral Science Foundation of China (No. 61070106,
No. 61272332 and No. 61202329), the Na-
tional High Technology Development 863 Pro-
gram of China (No. 2012AA011102), the Na-
tional Basic Research Program of China (No.
2012CB316300), Tsinghua National Laboratory
for Information Science and Technology (TNList)
Cross-discipline Foundation and the Opening
Project of Beijing Key Laboratory of Inter-
net Culture and Digital Dissemination Research
(ICDD201201).
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263?
311, June.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the Conference on Web Search and
Web Data Mining (WSDM).
Qin Gao, Nguyen Bach, and Stephan Vogel. 2010. A
semi-supervised word alignment algorithm with par-
tial manual alignments. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 1?10, Uppsala, Sweden,
July. Association for Computational Linguistics.
1762
Mingqin Hu and Bing Liu. 2004a. Mining opinion fea-
tures in customer reviews. In Proceedings of Con-
ference on Artificial Intelligence (AAAI).
Minqing Hu and Bing Liu. 2004b. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Wei Jin and Hay Ho Huang. 2009. A novel lexical-
ized hmm-based learning framework for web opin-
ion mining. In Proceedings of International Confer-
ence on Machine Learning (ICML).
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Yingju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization.
In Chu-Ren Huang and Dan Jurafsky, editors, COL-
ING, pages 653?661. Tsinghua University Press.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In Allan Ellis and Tatsuya Hagino,
editors, WWW, pages 342?351. ACM.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1346?1356, Jeju Island, Korea,
July. Association for Computational Linguistics.
Kang Liu, Liheng Xu, Yang Liu, and Jun Zhao. 2013.
Opinion target extraction using partially supervised
word alignment model.
Tengfei Ma and Xiaojun Wan. 2010. Opinion tar-
get extraction in chinese news comments. In Chu-
Ren Huang and Dan Jurafsky, editors, COLING
(Posters), pages 782?790. Chinese Information Pro-
cessing Society of China.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ?05, pages 339?346,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Che. 2009.
Expanding domain sentiment lexicon through dou-
ble propagation.
Guang Qiu, Bing Liu 0001, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Lin-
guistics, 37(1):9?27.
Bo Wang and Houfeng Wang. 2008. Bootstrapping
both product features and opinion words from chi-
nese customer reviews with cross-inducing.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Chid Apt, Joydeep Ghosh,
and Padhraic Smyth, editors, KDD, pages 618?626.
ACM.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP, pages 1533?1541. ACL.
Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara,
Joseph Johnson, and Xuanjing Huang. 2009. Min-
ing product reviews based on shallow dependency
parsing. In Proceedings of the 32nd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, SIGIR ?09, pages
726?727, New York, NY, USA. ACM.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking
product features in opinion documents. In Chu-
Ren Huang and Dan Jurafsky, editors, COLING
(Posters), pages 1462?1470. Chinese Information
Processing Society of China.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In David Wai-Lok Cheung,
Il-Yeol Song, Wesley W. Chu, Xiaohua Hu, and
Jimmy J. Lin, editors, CIKM, pages 1799?1802.
ACM.
1763
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1764?1773,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mining Opinion Words and Opinion Targets in a Two-Stage Framework
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{lhxu, kliu, swlai, ybchen, jzhao}@nlpr.ia.ac.cn
Abstract
This paper proposes a novel two-stage
method for mining opinion words and
opinion targets. In the first stage, we
propose a Sentiment Graph Walking algo-
rithm, which naturally incorporates syn-
tactic patterns in a Sentiment Graph to ex-
tract opinion word/target candidates. Then
random walking is employed to estimate
confidence of candidates, which improves
extraction accuracy by considering confi-
dence of patterns. In the second stage, we
adopt a self-learning strategy to refine the
results from the first stage, especially for
filtering out high-frequency noise terms
and capturing the long-tail terms, which
are not investigated by previous meth-
ods. The experimental results on three real
world datasets demonstrate the effective-
ness of our approach compared with state-
of-the-art unsupervised methods.
1 Introduction
Opinion mining not only assists users to make in-
formed purchase decisions, but also helps busi-
ness organizations understand and act upon cus-
tomer feedbacks on their products or services in
real-time. Extracting opinion words and opinion
targets are two key tasks in opinion mining. Opin-
ion words refer to those terms indicating positive
or negative sentiment. Opinion targets represent
aspects or attributes of objects toward which opin-
ions are expressed. Mining these terms from re-
views of a specific domain allows a more thorough
understanding of customers? opinions.
Opinion words and opinion targets often co-
occur in reviews and there exist modified relations
(called opinion relation in this paper) between
them. For example, in the sentence ?It has a clear
screen?, ?clear? is an opinion word and ?screen? is
an opinion target, and there is an opinion relation
between the two words. It is natural to identify
such opinion relations through common syntactic
patterns (also called opinion patterns in this pa-
per) between opinion words and targets. For ex-
ample, we can extract ?clear? and ?screen? by us-
ing a syntactic pattern ?Adj-{mod}-Noun?, which
captures the opinion relation between them. Al-
though previous works have shown the effective-
ness of syntactic patterns for this task (Qiu et al,
2009; Zhang et al, 2010), they still have some lim-
itations as follows.
False Opinion Relations: As an example, the
phrase ?everyday at school? can be matched by
a pattern ?Adj-{mod}-(Prep)-{pcomp-n}-Noun?,
but it doesn?t bear any sentiment orientation. We
call such relations that match opinion patterns but
express no opinion false opinion relations. Pre-
vious pattern learning algorithms (Zhuang et al,
2006; Kessler and Nicolov, 2009; Jijkoun et al,
2010) often extract opinion patterns by frequency.
However, some high-frequency syntactic patterns
can have very poor precision (Kessler and Nicolov,
2009).
False Opinion Targets: In another case, the
phrase ?wonderful time? can be matched by
an opinion pattern ?Adj-{mod}-Noun?, which is
widely used in previous works (Popescu and Et-
zioni, 2005; Qiu et al, 2009). As can be seen, this
phrase does express a positive opinion but unfortu-
nately ?time? is not a valid opinion target for most
domains such as MP3. Thus, false opinion targets
are extracted. Due to the lack of ground-truth
knowledge for opinion targets, non-target terms
introduced in this way can be hardly filtered out.
Long-tail Opinion Targets: We further no-
tice that previous works prone to extract opinion
targets with high frequency (Hu and Liu, 2004;
Popescu and Etzioni, 2005; Qiu et al, 2009; Zhu
et al, 2009), and they often have difficulty in iden-
tifying the infrequent or long-tail opinion targets.
1764
To address the problems stated above, this pa-
per proposes a two-stage framework for mining
opinion words and opinion targets. The under-
lying motivation is analogous to the novel idea
?Mine the Easy, Classify the Hard? (Dasgupta and
Ng, 2009). In our first stage, we propose a Senti-
ment Graph Walking algorithm to cope with the
false opinion relation problem, which mines easy
cases of opinion words/targets. We speculate that
it may be helpful to introduce a confidence score
for each pattern. Concretely, we create a Sen-
timent Graph to model opinion relations among
opinion word/target/pattern candidates and apply
random walking to estimate confidence of them.
Thus, confidence of pattern is considered in a uni-
fied process. Patterns that often extract false opin-
ion relations will have low confidence, and terms
introduced by low-confidence patterns will also
have low confidence accordingly. This could po-
tentially improve the extraction accuracy.
In the second stage, we identify the hard cases,
which aims to filter out false opinion targets and
extract long-tail opinion targets. Previous super-
vised methods have been shown to achieve state-
of-the-art results for this task (Wu et al, 2009; Jin
and Ho, 2009; Li et al, 2010). However, the big
challenge for fully supervised method is the lack
of annotated training data. Therefore, we adopt a
self-learning strategy. Specifically, we employ a
semi-supervised classifier to refine the target re-
sults from the first stage, which uses some highly
confident target candidates as the initial labeled
examples. Then opinion words are also refined.
Our main contributions are as follows:
? We propose a Sentiment Graph Walking al-
gorithm to mine opinion words and opinion
targets from reviews, which naturally incor-
porates confidence of syntactic pattern in a
graph to improve extraction performance. To
our best knowledge, the incorporation of pat-
tern confidence in such a Sentiment Graph
has never been studied before for opinion
words/targets mining task (Section 3).
? We adopt a self-learning method for refining
opinion words/targets generated by Sentiment
Graph Walking. Specifically, it can remove
high-frequency noise terms and capture long-
tail opinion targets in corpora (Section 4).
? We perform experiments on three real world
datasets, which demonstrate the effectiveness
of our method compared with state-of-the-art
unsupervised methods (Section 5).
2 Related Work
In opinion words/targets mining task, most unsu-
pervised methods rely on identifying opinion rela-
tions between opinion words and opinion targets.
Hu and Liu (2004) proposed an association mining
technique to extract opinion words/targets. The
simple heuristic rules they used may potentially
introduce many false opinion words/targets. To
identify opinion relations more precisely, subse-
quent research work exploited syntax information.
Popescu and Etzioni (2005) used manually com-
plied syntactic patterns and Pointwise Mutual In-
formation (PMI) to extract opinion words/targets.
Qiu et al (2009) proposed a bootstrapping frame-
work called Double Propagation which intro-
duced eight heuristic syntactic rules. While man-
ually defining syntactic patterns could be time-
consuming and error-prone, we learn syntactic
patterns automatically from data.
There have been extensive works on mining
opinion words and opinion targets by syntac-
tic pattern learning. Riloff and Wiebe (2003)
performed pattern learning through bootstrapping
while extracting subjective expressions. Zhuang
et al (2006) obtained various dependency re-
lationship templates from an annotated movie
corpus and applied them to supervised opinion
words/targets extraction. Kobayashi et al (2007)
adopted a supervised learning technique to search
for useful syntactic patterns as contextual clues.
Our approach is similar to (Wiebe and Riloff,
2005) and (Xu et al, 2013), all of which apply
syntactic pattern learning and adopt self-learning
strategy. However, the task of (Wiebe and Riloff,
2005) was to classify sentiment orientations in
sentence level, while ours needs to extract more
detailed information in term level. In addition,
our method extends (Xu et al, 2013), and we
give a more complete and in-depth analysis on
the aforementioned problems in the first section.
There were also many works employed graph-
based method (Li et al, 2012; Zhang et al, 2010;
Hassan and Radev, 2010; Liu et al, 2012), but
none of previous works considered confidence of
patterns in the graph.
In supervised approaches, various kinds of
models were applied, such as HMM (Jin and Ho,
2009), SVM (Wu et al, 2009) and CRFs (Li et al,
2010). The downside of supervised methods was
the difficulty of obtaining annotated training data
in practical applications. Also, classifiers trained
1765
on one domain often fail to give satisfactory re-
sults when shifted to another domain. Our method
does not rely on annotated training data.
3 The First Stage: Sentiment Graph
Walking Algorithm
In the first stage, we propose a graph-based al-
gorithm called Sentiment Graph Walking to mine
opinion words and opinion targets from reviews.
3.1 Opinion Pattern Learning for Candidates
Generation
For a given sentence, we first obtain its depen-
dency tree. Following (Hu and Liu, 2004; Popescu
and Etzioni, 2005; Qiu et al, 2009), we regard all
adjectives as opinion word candidates (OC) and
all nouns or noun phrases as opinion target can-
didates (TC). A statistic-based method in (Zhu et
al., 2009) is used to detect noun phrases. Then
candidates are replaced by wildcards ?<OC>? or
?<TC>?. Figure 1 gives a dependency tree exam-
ple generated by Minipar (Lin, 1998).
p red s det
m od
gor geous<OC>
is(VBE)
style<TC>
the(Det)
of(P r ep) scr een<TC>pcom p-n
the(Det)
det
Figure 1: The dependency tree of the sentence
?The style of the screen is gorgeous?.
We extract two kinds of opinion patterns: ?OC-
TC? pattern and ?TC-TC? pattern. The ?OC-
TC? pattern is the shortest path between an OC
wildcard and a TC wildcard in dependency tree,
which captures opinion relation between an opin-
ion word candidate and an opinion target can-
didate. Similarly, the ?TC-TC? pattern cap-
tures opinion relation between two opinion tar-
get candidates.1 Words in opinion patterns are
replaced by their POS tags, and we constrain
that there are at most two words other than
wildcards in each pattern. In Figure 1, there
are two opinion patterns marked out by dash
lines: ?<OC>{pred}(VBE){s}<TC>? for the
?OC-TC? type and ?<TC>{mod}(Prep){pcomp-
n}<TC>? for the ?TC-TC? type. After all pat-
1We do not identify the opinion relation ?OC-OC? be-
cause this relation is often unreliable.
terns are generated, we drop those patterns with
frequency lower than a threshold F .
3.2 Sentiment Graph Construction
To model the opinion relations among opinion
words/targets and opinion patterns, a graph named
as Sentiment Graph is constructed, which is a
weighted, directed graph G = (V,E,W ), where
? V = {Voc ? Vtc ? Vp} is the set of vertices in
G, where Voc, Vtc and Vp represent the set of
opinion word candidates, opinion target can-
didates and opinion patterns, respectively.
? E = {Epo?Ept} ? {Vp?Voc}?{Vp?Vtc}
is the weighted, bi-directional edge set in G,
where Epo and Ept are mutually exclusive
sets of edges connecting opinion word/target
vertices to opinion pattern vertices. Note that
there are no edges between Voc and Vtc.
? W : E ? R+ is the weight function which
assigns non-negative weight to each edge.
For each (e : va ? vb) ? E, where
va, vb ? V , the weight function w(va, vb) =
freq(va, vb)/freq(va), where freq(?) is the
frequency of a candidate extracted by opinion
patterns or co-occurrence frequency between
two candidates.
Figure 2 shows an example of Sentiment Graph.
n icelarge
screen display
<OC>{mod}<TC> <OC>{mod}<TC>{con j}<TC>
1
0.8
0.7
0.2
0.3
0.4
0.2
0.33
0.33
0.33
0.6
0.4
0.2 0.2
Figure 2: An example of Sentiment Graph.
3.3 Confidence Estimation by Random
Walking with Restart
We believe that considering confidence of patterns
can potentially improve the extraction accuracy.
Our intuitive idea is: (i) If an opinion word/target
is with higher confidence, the syntactic patterns
containing this term are more likely to be used to
express customers? opinion. (ii) If an opinion pat-
tern has higher confidence, terms extracted by this
pattern are more likely to be correct. It?s a rein-
forcement process.
1766
We use Random Walking with Restart (RWR)
algorithm to implement our idea described above.
Let Moc p denotes the transition matrix from Voc
to Vp, for vo ? Voc, vp ? Vp, Moc p(vo, vp) =
w(vo, vp). Similarly, we have Mtc p, Mp oc,
Mp tc. Let c denotes confidence vector of candi-
dates so ctoc, cttc and ctp are confidence vectors for
opinion word/target/pattern candidates after walk-
ing t steps. Initially c0oc is uniformly distributed
on a few domain-independent opinion word seeds,
then the following formula are updated iteratively
until cttc and ctoc converge:
ct+1p = MToc p ? ctoc +MTtc p ? cttc (1)
ct+1oc = (1? ?)MTp oc ? ctp + ?c0oc (2)
ct+1tc = MTp tc ? ctp (3)
where MT is the transpose of matrix M and ? is
a small probability of teleporting back to the seed
vertices which prevents us from walking too far
away from the seeds. In the experiments below, ?
is set 0.1 empirically.
4 The Second Stage: Refining Extracted
Results Using Self-Learning
At the end of the first stage, we obtain a ranked
list of opinion words and opinion targets, in which
higher ranked terms are more likely to be correct.
Nevertheless, there are still some issues needed to
be addressed:
1) In the target candidate list, some high-
frequency frivolous general nouns such as
?thing? and ?people? are also highly ranked.
This is because there exist many opinion ex-
pressions containing non-target terms such as
?good thing?, ?nice people?, etc. in reviews.
Due to the lack of ground-truth knowledge
for opinion targets, the false opinion target
problem still remains unsolved.
2) In another aspect, long-tail opinion targets
may have low degree in Sentiment Graph.
Hence their confidence will be low although
they may be extracted by some high qual-
ity patterns. Therefore, the first stage is in-
capable of dealing with the long-tail opinion
target problem.
3) Furthermore, the first stage also extracts
some high-frequency false opinion words
such as ?every?, ?many?, etc. Many terms
of this kind are introduced by high-frequency
false opinion targets, for there are large
amounts of phrases like ?every time? and
?many people?. So this issue is a side effect
of the false opinion target problem.
To address these issues, we exploit a self-
learning strategy. For opinion targets, we use a
semi-supervised binary classifier called target re-
fining classifier to refine target candidates. For
opinion words, we use the classified list of opin-
ion targets to further refine the extracted opinion
word candidates.
4.1 Opinion Targets Refinement
There are two keys for opinion target refinement:
(i) How to generate the initial labeled data for tar-
get refining classifier. (ii) How to properly repre-
sent a long-tail opinion target candidate other than
comparing frequency between different targets.
For the first key, it is clearly improper to select
high-confidence targets as positive examples and
choose low-confidence targets as negative exam-
ples2, for there are noise with high confidence and
long-tail targets with low confidence. Fortunately,
a large proportion of general noun noises are the
most frequent words in common texts. Therefore,
we can generate a small domain-independent gen-
eral noun (GN) corpus from large web corpora to
cover some most frequently used general noun ex-
amples. Then labeled examples can be drawn from
the target candidate list and the GN corpus.
For the second key, we utilize opinion words
and opinion patterns with their confidence scores
to represent an opinion target. By this means, a
long-tail opinion target can be determined by its
own contexts, whose weights are learnt from con-
texts of frequent opinion targets. Thus, if a long-
tail opinion target candidate has high contextual
support, it will have higher probability to be found
out in despite of its low frequency.
Creation of General Noun Corpora. 1000
most frequent nouns in Google-1-gram3 were se-
lected as general noun candidates. On the other
hand, we added all nouns in the top three levels of
hyponyms in four WordNet (Miller, 1995) synsets
?object?, ?person?, ?group? and ?measure? into
the GN corpus. Our idea was based on the fact that
a term is more general when it sits in higher level
in the WordNet hierarchy. Then inapplicable can-
didates were discarded and a 3071-word English
2Note that the ?positive? and ?negative? here denote opin-
ion targets and non-target terms respectively and they do not
indicate sentiment polarities.
3http://books.google.com/ngrams.
1767
GN corpus was created. Another Chinese GN cor-
pus with 3493 words was generated in the similar
way from HowNet (Gan and Wong, 2000).
Generation of Labeled Examples. Let T =
{Y+1,Y?1} denotes the initial labeled set, where
N most highly confident target candidates but not
in our GN corpora are regarded as the positive ex-
ample set Y+1, other N terms from GN corpora
which are also top ranked in the target list are se-
lected as the negative example set Y?1. The re-
minder unlabeled candidates are denoted by T ?.
Feature Representation for Classifier. Given
T and T ? in the form of {(xi, yi)}. For a target
candidate ti, xi = (o1, . . . , on, p1, . . . , pm)T rep-
resents its feature vector, where oj is the opinion
word feature and pk is the opinion pattern feature.
The value of feature is defined as follows,
x(oj) = conf(oj)?
?
pk freq(ti, oj , pk)
freq(oj)
(4)
x(pk) = conf(pk)?
?
oj freq(ti, oj , pk)
freq(pk)
(5)
where conf(?) denotes confidence score estimated
by RWR, freq(?) has the same meaning as in Sec-
tion 3.2. Particularly, freq(ti, oj , pk) represents
the frequency of pattern pk extracting opinion tar-
get ti and opinion word oj .
Target Refinement Classifier: We use support
vector machine as the binary classifier. Hence, the
classification problem can be formulated as to find
a hyperplane < w, b > that separates both labeled
set T and unlabeled set T ? with maximum mar-
gin. The optimization goal is to minimize over
(T ,T ?,w, b, ?1, ..., ?n, ??1 , ..., ??k):
1
2 ||w||
2 + C
n?
i=0
?i + C?
k?
j=0
??j
subject to : ?ni=1 : yi[w ? xi + b] ? 1? ?i
?kj=1 : y?j [w ? x?j + b] ? 1? ??j
?ni=1 : ?i > 0
?kj=1 : ??j > 0
where yi, y?j ? {+1,?1}, xi and x?j represent
feature vectors, C and C? are parameters set by
user. This optimization problem can be imple-
mented by a typical Transductive Support Vector
Machine (TSVM) (Joachims, 1999).
4.2 Opinion Words Refinement
We use the classified opinion target results to re-
fine opinion words by the following equation,
s(oj) =
?
ti?T
?
pk
s(ti)conf(pk)freq(ti, oj , pk)
freq(ti)
where T is the opinion target set in which each el-
ement is classified as positive during opinion tar-
get refinement, s(ti) denotes confidence score ex-
ported by the target refining classifier. Particularly,
freq(ti) =
?
oj
?
pk freq(ti, oj , pk). A higher
score of s(oj) means that candidate oj is more
likely to be an opinion word.
5 Experiments
5.1 Datasets and Evaluation Metrics
Datasets: We select three real world datasets to
evaluate our approach. The first one is called
Customer Review Dataset (CRD) (Hu and Liu,
2004) which contains reviews on five different
products (represented by D1 to D5) in English.
The second dataset is pre-annotated and published
in COAE084, where two domains of Chinese re-
views are selected. At last, we employ a bench-
mark dataset in (Wang et al, 2011) and named it
as Large. We manually annotated opinion words
and opinion targets as the gold standard. Three
annotators were involved. Firstly, two annotators
were required to annotate out opinion words and
opinion targets in sentences. When conflicts hap-
pened, the third annotator would make the final
judgment. The average Kappa-values of the two
domains were 0.71 for opinion words and 0.66
for opinion targets. Detailed information of our
datasets is shown in Table 1.
Dataset Domain #Sentences #OW #OT
Large
(English)
Hotel 10,000 434 1,015
MP3 10,000 559 1,158
COAE08(Chinese)
Camera 2,075 351 892
Car 4,783 622 1,179
Table 1: The detailed information of datasets. OW
stands for opinion words and OT stands for targets.
Pre-processing: Firstly, HTML tags are re-
moved from texts. Then Minipar (Lin, 1998)
is used to parse English corpora, and Standford
Parser (Chang et al, 2009) is used for Chinese
4http://ir-china.org.cn/coae2008.html
1768
Methods D1 D2 D3 D4 D5 Avg.P R F P R F P R F P R F P R F F
Hu 0.75 0.82 0.78 0.71 0.79 0.75 0.72 0.76 0.74 0.69 0.82 0.75 0.74 0.80 0.77 0.76
DP 0.87 0.81 0.84 0.90 0.81 0.85 0.90 0.86 0.88 0.81 0.84 0.82 0.92 0.86 0.89 0.86
Zhang 0.83 0.84 0.83 0.86 0.85 0.85 0.86 0.88 0.87 0.80 0.85 0.82 0.86 0.86 0.86 0.85
Ours-Stage1 0.79 0.85 0.82 0.82 0.87 0.84 0.83 0.87 0.85 0.78 0.88 0.83 0.82 0.88 0.85 0.84
Ours-Full 0.86 0.82 0.84 0.88 0.83 0.85 0.89 0.86 0.87 0.83 0.86 0.84 0.89 0.85 0.87 0.86
Table 2: Results of opinion target extraction on the Customer Review Dataset.
Methods D1 D2 D3 D4 D5 Avg.P R F P R F P R F P R F P R F F
Hu 0.57 0.75 0.65 0.51 0.76 0.61 0.57 0.73 0.64 0.54 0.62 0.58 0.62 0.67 0.64 0.62
DP 0.64 0.73 0.68 0.57 0.79 0.66 0.65 0.70 0.67 0.61 0.65 0.63 0.70 0.68 0.69 0.67
Ours-Stage1 0.61 0.75 0.67 0.55 0.80 0.65 0.63 0.75 0.68 0.60 0.69 0.64 0.68 0.70 0.69 0.67
Ours-Full 0.64 0.74 0.69 0.59 0.79 0.68 0.66 0.71 0.68 0.65 0.67 0.66 0.72 0.67 0.69 0.68
Table 3: Results of opinion word extraction on the Customer Review Dataset.
corpora. Stemming and fuzzy matching are also
performed following previous work (Hu and Liu,
2004).
Evaluation Metrics: We evaluate our method
by precision(P), recall(R) and F-measure(F).
5.2 Our Method vs. the State-of-the-art
Three state-of-the-art unsupervised methods are
used as competitors to compare with our method.
Hu extracts opinion words/targets by using ad-
jacency rules (Hu and Liu, 2004).
DP uses a bootstrapping algorithm named as
Double Propagation (Qiu et al, 2009).
Zhang is an enhanced version of DP and em-
ploys HITS algorithm (Kleinberg, 1999) to rank
opinion targets (Zhang et al, 2010).
Ours-Full is the full implementation of our
method. We employ SVMlight (Joachims, 1999)
as the target refining classifier. Default parameters
are used except the bias item is set 0.
Ours-Stage1 only uses Sentiment Graph Walk-
ing algorithm which does?t have opinion word and
opinion target refinement.
All of the above approaches use same five
common opinion word seeds. The choice of opin-
ion seeds seems reasonable, as most people can
easily come up with 5 opinion words such as
?good?, ?bad?, etc. The performance on five prod-
ucts of CRD dataset is shown in Table 2 and Ta-
ble 3. Zhang does not extract opinion words so
their results for opinion words are not taken into
account. We can see that Ours-Stage1 achieves
superior recall but has some loss in precision com-
pared with DP and Zhang. This may be because
the CRD dataset is too small and our statistic-
based method may suffer from data sparseness.
In spite of this, Ours-Full achieves comparable F-
measure with DP, which is a well-designed rule-
based method.
The results on two larger datasets are shown
in Table 4 and Table 5, from which we can have
the following observation: (i) All syntax-based-
methods outperform Hu, showing the importance
of syntactic information in opinion relation identi-
fication. (ii) Ours-Full outperforms the three com-
petitors on all domains provided. (iii) Ours-Stage1
outperforms Zhang, especially in terms of recall.
We believe it benefits from our automatical pattern
learning algorithm. Moreover, Ours-Stage1 do
not loss much in precision compared with Zhang,
which indicates the applicability to estimate pat-
tern confidence in Sentiment Graph. (iv) Ours-
Full achieves 4-9% improvement in precision over
the most accurate method, which shows the effec-
tiveness of our second stage.
5.3 Detailed Discussions
This section gives several variants of our method
to have a more detailed analysis.
Ours-Bigraph constructs a bi-graph between
opinion words and targets, so opinion patterns
are not included in the graph. Then RWR algo-
rithm is used to only assign confidence to opinion
word/target candidates.
Ours-Stage2 only contains the second stage,
which doesn?t apply Sentiment Graph Walking al-
gorithm. Hence the confidence score conf(?) in
Equations (4) and (5) have no values and they are
set to 1. The initial labeled examples are exactly
the same as Ours-Full. Due to the limitation of
space, we only give analysis on opinion target ex-
traction results in Figure 3.
1769
Methods MP3 Hotel Camera Car Avg.P R F P R F P R F P R F F
Hu 0.53 0.55 0.54 0.55 0.57 0.56 0.63 0.65 0.64 0.62 0.58 0.60 0.58
DP 0.66 0.57 0.61 0.66 0.60 0.63 0.71 0.70 0.70 0.72 0.65 0.68 0.66
Zhang 0.65 0.62 0.63 0.64 0.66 0.65 0.71 0.78 0.74 0.69 0.68 0.68 0.68
Ours-Stage1 0.62 0.68 0.65 0.63 0.71 0.67 0.69 0.80 0.74 0.66 0.71 0.68 0.69
Ours-Full 0.73 0.71 0.72 0.75 0.73 0.74 0.78 0.81 0.79 0.76 0.73 0.74 0.75
Table 4: Results of opinion targets extraction on Large and COAE08.
Methods MP3 Hotel Camera Car Avg.P R F P R F P R F P R F F
Hu 0.48 0.65 0.55 0.51 0.68 0.58 0.72 0.74 0.73 0.70 0.71 0.70 0.64
DP 0.58 0.62 0.60 0.60 0.66 0.63 0.80 0.73 0.76 0.79 0.71 0.75 0.68
Ours-Stage1 0.59 0.69 0.64 0.61 0.71 0.66 0.79 0.78 0.78 0.77 0.77 0.77 0.71
Ours-Full 0.64 0.67 0.65 0.67 0.69 0.68 0.82 0.78 0.80 0.80 0.76 0.78 0.73
Table 5: Results of opinion words extraction on Large and COAE08.
Figure 3: Opinion target extraction results.
5.3.1 The Effect of Sentiment Graph Walking
We can see that our graph-based methods (Ours-
Bigraph and Ours-Stage1) achieve higher recall
than Zhang. By learning patterns automatically,
our method captures opinion relations more ef-
ficiently. Also, Ours-Stage1 outperforms Ours-
Bigraph, especially in precision. We believe it is
because Ours-Stage1 estimated confidence of pat-
terns so false opinion relations are reduced. There-
fore, the consideration of pattern confidence is
beneficial as expected, which alleviates the false
opinion relation problem. On another hand, we
find that Ours-Stage2 has much worse perfor-
mance than Ours-Full. This shows the effective-
ness of Sentiment Graph Walking algorithm since
the confidence scores estimated in the first stage
are indispensable and indeed key to the learning
of the second stage.
5.3.2 The Effect of Self-Learning
Figure 4 shows the average Precision@N curve of
four domains on opinion target extraction. Ours-
GN-Only is implemented by only removing 50
initial negative examples found by our GN cor-
pora. We can see that the GN corpora work quite
well, which find out most top-ranked false opin-
ion targets. At the same time, Ours-Full has much
better performance than Ours-GN-Only which in-
dicates that Ours-Full can filter out more noises
other than the initial negative examples. There-
fore, our self-learning strategy alleviates the short-
coming of false opinion target problem. More-
over, Table 5 shows that the performance of opin-
ion word extraction is also improved based on the
classified results of opinion targets.
Figure 4: The average precision@N curve of the
four domains on opinion target extraction.
1770
ID Pattern Example #Ext. Conf. PrO PrT
#1 <OC>{mod}<TC> it has a clear screen 7344 0.3938 0.59 0.66
#2 <TC>{subj}<OC> the sound quality is excellent 2791 0.0689 0.62 0.70
#3 <TC>{conj}<TC> the size and weight make it convenient 3620 0.0208 N/A 0.67
#4 <TC>{subj}<TC> the button layout is a simplistic plus 1615 0.0096 N/A 0.67
#5 <OC>{pnmod}<TC> the buttons easier to use 128 0.0014 0.61 0.34
#6 <TC>{subj}(V){s}(VBE){subj}<OC> software provided is simple 189 0.0015 0.54 0.33
#7 <OC>{mod}(Prep){pcomp-c}(V){obj}<TC> great for playing audible books 211 0.0013 0.43 0.48
Table 6: Examples of English patterns. #Ext. represent number of terms extracted, Conf. denotes confi-
dence score estimated by RWR and PrO/PrT stand for precisions of extraction on opinion words/targets
of a pattern respectively. Opinion words in examples are in bold and opinion targets are in italic.
Figure 5 gives the recall of long-tail opinion
targets5 extracted, where Ours-Full is shown to
have much better performance than Ours-Stage1
and the three competitors. This observation proves
that our method can improve the limitation of
long-tail opinion target problem.
Figure 5: The recall of long-tail opinion targets.
5.3.3 Analysis on Opinion Patterns
Table 6 shows some examples of opinion pattern
and their extraction accuracy on MP3 reviews in
the first stage. Pattern #1 and #2 are the two
most high-confidence opinion patterns of ?OC-
TC? type, and Pattern #3 and #4 demonstrate two
typical ?TC-TC? patterns. As these patterns ex-
tract too many terms, the overall precision is very
low. We give Precision@400 of them, which is
more meaningful because only top listed terms
in the extracted results are regarded as opinion
targets. Pattern #5 and #6 have high precision
on opinion words but low precision on opinion
targets. This observation demonstrates the false
opinion target problem. Pattern #7 is a pattern ex-
ample that extracts many false opinion relations
and it has low precision for both opinion words
and opinion targets. We can see that Pattern #7 has
5Since there is no explicit definition for the notion ?long-
tail?, we conservatively regard 60% opinion targets with the
lowest frequency as the ?long-tail? terms.
a lower confidence compared with Pattern #5 and
#6 although it extracts more words. It?s because
it has a low probability of walking from opinion
seeds to this pattern. This further proves that our
method can reduce the confidence of low-quality
patterns.
5.3.4 Sensitivity of Parameters
Finally, we study the sensitivity of parameters
when recall is fixed at 0.70. Figure 6 shows the
precision curves at different N initial training ex-
amples and F filtering frequency. We can see that
the performance saturates when N is set to 50 and
it does not vary much under different F , showing
the robustness of our method. We thus set N to
50, and F to 3 for CRD, 5 for COAE08 and 10 for
Large accordingly.
Figure 6: Influence of parameters.
1771
6 Conclusion and Future Work
This paper proposes a novel two-stage framework
for mining opinion words and opinion targets. In
the first stage, we propose a Sentiment Graph
Walking algorithm, which incorporates syntactic
patterns in a Sentiment Graph to improve the ex-
traction performance. In the second stage, we pro-
pose a self-learning method to refine the result of
first stage. The experimental results show that our
method achieves superior performance over state-
of-the-art unsupervised methods.
We further notice that opinion words are not
limited to adjectives but can also be other type of
word such as verbs or nouns. Identifying all kinds
of opinion words is a more challenging task. We
plan to study this problem in our future work.
Acknowledgement
Thanks to Prof. Yulan He for her insightful
advices. This work was supported by the Na-
tional Natural Science Foundation of China (No.
61070106, No. 61272332 and No. 61202329),
the National High Technology Development 863
Program of China (No. 2012AA011102), the
National Basic Research Program of China (No.
2012CB316300), Tsinghua National Laboratory
for Information Science and Technology (TNList)
Cross-discipline Foundation and the Opening
Project of Beijing Key Laboratory of Inter-
net Culture and Digital Dissemination Research
(ICDD201201).
References
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, SSST
?09, pages 51?59.
Sajib Dasgupta and Vincent Ng. 2009. Mine the easy,
classify the hard: a semi-supervised approach to au-
tomatic sentiment classification. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ?09, pages 701?709.
Kok Wee Gan and Ping Wai Wong. 2000. Anno-
tating information structures in chinese texts using
hownet. In Proceedings of the second workshop on
Chinese language processing: held in conjunction
with the 38th Annual Meeting of the Association for
Computational Linguistics - Volume 12, CLPW ?00,
pages 85?92, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 395?
403, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Valentin Jijkoun, Maarten de Rijke, and Wouter
Weerkamp. 2010. Generating focused topic-
specific sentiment lexicons. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 585?594,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Wei Jin and Hung Hay Ho. 2009. A novel lexical-
ized hmm-based learning framework for web opin-
ion mining. In Proceedings of the 26th Annual Inter-
national Conference on Machine Learning, ICML
?09, pages 465?472.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the Sixteenth International Confer-
ence on Machine Learning, pages 200?209.
Jason Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking
of linguistic configurations. In Proceedings of the
Third International AAAI Conference on Weblogs
and Social Media.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-
of relations in opinion mining. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1065?1074, June.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization.
In Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
653?661, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and
Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
410?419, July.
1772
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Workshop on Evaluation of Parsing Sys-
tems at ICLRE.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, EMNLP-CoNLL ?12, pages 1346?1356,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1199?1204.
Ellen Riloff and Janyce Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, EMNLP ?03,
pages 105?112, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD international conference on Knowledge
discovery and data mining, KDD ?11, pages 618?
626, New York, NY, USA. ACM.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the 6th international
conference on Computational Linguistics and Intel-
ligent Text Processing, CICLing?05, pages 486?497.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing:
Volume 3 - Volume 3, pages 1533?1541.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun
Zhao. 2013. Walk and learn: A two-stage approach
for opinion words and opinion targets co-extraction.
In Proceedings of the 22nd International World Wide
Web Conference, WWW ?13.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1462?1470.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In Proceedings of the 18th
ACM conference on Information and knowledge
management, CIKM ?09, pages 1799?1802.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM international conference
on Information and knowledge management, CIKM
?06, pages 43?50.
1773
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 314?324,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Extracting Opinion Targets and Opinion Words from Online Reviews
with Graph Co-ranking
Kang Liu, Liheng Xu and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{kliu, lhxu, jzhao}@nlpr.ia.ac.cn
Abstract
Extracting opinion targets and opinion
words from online reviews are two fun-
damental tasks in opinion mining. This
paper proposes a novel approach to col-
lectively extract them with graph co-
ranking. First, compared to previous
methods which solely employed opinion
relations among words, our method con-
structs a heterogeneous graph to model
two types of relations, including seman-
tic relations and opinion relations. Next,
a co-ranking algorithm is proposed to es-
timate the confidence of each candidate,
and the candidates with higher confidence
will be extracted as opinion targets/words.
In this way, different relations make coop-
erative effects on candidates? confidence
estimation. Moreover, word preference
is captured and incorporated into our co-
ranking algorithm. In this way, our co-
ranking is personalized and each candi-
date?s confidence is only determined by its
preferred collocations. It helps to improve
the extraction precision. The experimen-
tal results on three data sets with differ-
ent sizes and languages show that our ap-
proach achieves better performance than
state-of-the-art methods.
1 Introduction
In opinion mining, extracting opinion targets and
opinion words are two fundamental subtasks.
Opinion targets are objects about which users?
opinions are expressed, and opinion words are
words which indicate opinions? polarities. Ex-
tracting them can provide essential information
for obtaining fine-grained analysis on customers?
opinions. Thus, it has attracted a lot of attentions
(Hu and Liu, 2004b; Liu et al, 2012; Moghaddam
and Ester, 2011; Mukherjee and Liu, 2012).
To this end, previous work usually employed a
collective extraction strategy (Qiu et al, 2009; Hu
and Liu, 2004b; Liu et al, 2013b). Their intuition
is: opinion words usually co-occur with opinion
targets in sentences, and there are strong modifi-
cation relationship between them (called opinion
relation in (Liu et al, 2012)). If a word is an
opinion word, other words with which that word
having opinion relations will have highly proba-
bility to be opinion targets, and vice versa. In this
way, extraction is alternatively performed and mu-
tual reinforced between opinion targets and opin-
ion words. Although this strategy has been widely
employed by previous approaches, it still has sev-
eral limitations.
1) Only considering opinion relations is in-
sufficient. Previous methods mainly focused on
employing opinion relations among words for
opinion target/word co-extraction. They have in-
vestigated a series of techniques to enhance opin-
ion relations identification performance, such as
nearest neighbor rules (Liu et al, 2005), syntactic
patterns (Zhang et al, 2010; Popescu and Etzioni,
2005), word alignment models (Liu et al, 2012;
Liu et al, 2013b; Liu et al, 2013a), etc. How-
ever, we are curious that whether merely employ-
ing opinion relations among words is enough for
opinion target/word extraction? We note that there
are additional types of relations among words. For
example, ?LCD? and ?LED? both denote the same
aspect ?screen? in TV set domain, and they are
topical related. We call such relations between
homogeneous words as semantic relations. If we
have known ?LCD? to be an opinion target, ?LED?
is naturally to be an opinion target. Intuitively,
besides opinion relations, semantic relations may
provide additional rich clues for indicating opin-
ion targets/words. Which kind of relations is more
effective for opinion targets/words extraction? Is it
beneficial to consider these two types of relations
together for the extraction? To our best knowl-
314
edge, these problems have seldom been studied
before (see Section 2).
2) Ignoring word preference. When employ-
ing opinion relations to perform mutual reinforc-
ing extraction between opinion targets and opin-
ion words, previous methods depended on opin-
ion associations among words, but seldom consid-
ered word preference. Word preference denotes
a word?s preferred collocations. Intuitively, the
confidence of a candidate being an opinion tar-
get (opinion word) should mostly be determined
by its word preferences rather than all words hav-
ing opinion relations with it. For example
?This camera?s price is expensive for me.?
?It?s price is good.?
?Canon 40D has a good price.?
In these three sentences, ?price? is modified by
?good? more times than ?expensive?. In tradi-
tional extraction strategy, opinion associations are
usually computed based on the co-occurrence fre-
quency. Thus, ?good? has more strong opinion
association with ?price? than ?expensive?, and it
would have more contributions on determining
?price? to be an opinion target or not. It?s un-
reasonable. ?Expensive? actually has more re-
latedness with ?price? than ?good?, and ?expen-
sive? is likely to be a word preference for ?price?.
The confidence of ?price? being an opinion target
should be influenced by ?expensive? in greater ex-
tent than ?good?. In this way, we argue that the
extraction will be more precise.
????4 ????6 ????5 ????1 ????3 ????2 
????2 ????4 ????3 ????5 ????6 ????1 
?????? 
?????? 
?????? 
Figure 1: Heterogeneous Graph: OC means opin-
ion word candidates. TC means opinion target
candidates. Solid curves and dotted lines respec-
tively mean semantic relations and opinion rela-
tions between two candidates.
Thus, to resolve these two problems, we present
a novel approach with graph co-ranking. The col-
lective extraction of opinion targets/words is per-
formed in a co-ranking process. First, we oper-
ate over a heterogeneous graph to model seman-
tic relations and opinion relations into a unified
model. Specifically, our heterogeneous graph is
composed of three subgraphs which model differ-
ent relation types and candidates, as shown in Fig-
ure 1. The first subgraph G
tt
represents semantic
relations among opinion target candidates, and the
second subgraph G
oo
models semantic relations
among opinion word candidates. The third part
is a bipartite subgraph G
to
, which models opinion
relations among different candidate types and con-
nects the above two subgraphs together. Then we
perform a random walk algorithm onG
tt
, G
oo
and
G
to
separately, to estimate all candidates? confi-
dence, and the entries with higher confidence than
a threshold are correspondingly extracted as opin-
ion targets/words. The results could reflect which
type of relation is more useful for the extraction.
Second, a co-ranking algorithm, which incor-
porates three separate random walks on G
tt
, G
oo
and G
to
into a unified process, is proposed to
perform candidate confidence estimation. Differ-
ent relations may cooperatively affect candidate
confidence estimation and generate more global
ranking results. Moreover, we discover each can-
didate?s preferences through topics. Such word
preference will be different for different candi-
dates. We add word preference information into
our algorithm and make our co-ranking algorithm
be personalized. A candidate?s confidence would
mainly absorb the contributions from its word
preferences rather than its all neighbors with opin-
ion relations, which may be beneficial for improv-
ing extraction precision.
We perform experiments on real-world datasets
from different languages and different domains.
Results show that our approach effectively im-
proves extraction performance compared to the
state-of-the-art approaches.
2 Related Work
There are many significant research efforts on
opinion targets/words extraction (sentence level
and corpus level). In sentence level extraction,
previous methods (Wu et al, 2009; Ma and Wan,
2010; Li et al, 2010; Yang and Cardie, 2013)
mainly aimed to identify all opinion target/word
mentions in sentences. They regarded it as a se-
quence labeling task, where several classical mod-
els were used, such as CRFs (Li et al, 2010) and
SVM (Wu et al, 2009).
This paper belongs to corpus level extraction,
and aims to generate a sentiment lexicon and a
target list rather than to identify mentions in sen-
315
tences. Most of previous corpus-level methods
adopted a co-extraction framework, where opin-
ion targets and opinion words reinforce each other
according to their opinion relations. Thus, how
to improve opinion relations identification perfor-
mance was their main focus. (Hu and Liu, 2004a)
exploited nearest neighbor rules to mine opinion
relations among words. (Popescu and Etzioni,
2005) and (Qiu et al, 2011) designed syntactic
patterns to perform this task. (Zhang et al, 2010)
promoted Qiu?s method. They adopted some spe-
cial designed patterns to increase recall. (Liu et
al., 2012; Liu et al, 2013a; Liu et al, 2013b) em-
ployed word alignment model to capture opinion
relations rather than syntactic parsing. The exper-
imental results showed that these alignment-based
methods are more effective than syntax-based ap-
proaches for online informal texts. However, all
aforementioned methods only employed opinion
relations for the extraction, but ignore consider-
ing semantic relations among homogeneous can-
didates. Moreover, they all ignored word prefer-
ence in the extraction process.
In terms of considering semantic relations
among words, our method is related with sev-
eral approaches based on topic model (Zhao et
al., 2010; Moghaddam and Ester, 2011; Moghad-
dam and Ester, 2012a; Moghaddam and Ester,
2012b; Mukherjee and Liu, 2012). The main
goals of these methods weren?t to extract opin-
ion targets/words, but to categorize all given as-
pect terms and sentiment words. Although these
models could be used for our task according to the
associations between candidates and topics, solely
employing semantic relations is still one-sided and
insufficient to obtain expected performance.
Furthermore, there is little work which consid-
ered these two types of relations globally (Su et
al., 2008; Hai et al, 2012; Bross and Ehrig, 2013).
They usually captured different relations using co-
occurrence information. That was too coarse to
obtain expected results (Liu et al, 2012). In ad-
dition, (Hai et al, 2012) extracted opinion tar-
gets/words in a bootstrapping process, which had
an error propagation problem. In contrast, we per-
form extraction with a global graph co-ranking
process, where error propagation can be effec-
tively alleviated. (Su et al, 2008) used heteroge-
neous relations to find implicit sentiment associ-
ations among words. Their aim was only to per-
form aspect terms categorization but not to extract
opinion targets/words. They extracted opinion tar-
gets/words in advanced through simple phrase de-
tection. Thus, the extraction performance is far
from expectation.
3 The Proposed Method
In this section, we propose our method in detail.
We formulate opinion targets/words extraction as
a co-ranking task. All nouns/noun phrases are re-
garded as opinion target candidates, and all ad-
jectives/verbs are regarded as opinion word candi-
dates, which are widely adopted by pervious meth-
ods (Hu and Liu, 2004a; Qiu et al, 2011; Wang
and Wang, 2008; Liu et al, 2012). Then each can-
didate will be assigned a confidence and ranked,
and the candidates with higher confidence than a
threshold will be extracted as the results.
Different from traditional methods, besides
opinion relations among words, we additionally
capture semantic relations among homogeneous
candidates. To this end, a heterogeneous undi-
rected graph G = (V,E) is constructed. V =
V
t
? V
o
denotes the vertex set, which includes
opinion target candidates v
t
? V
t
and opinion
word candidates v
o
? V
o
. E denotes the edge
set, where e
ij
? E means that there is a relation
between two vertices. E
tt
? E represents the se-
mantic relations between two opinion target candi-
dates. E
oo
? E represents the semantic relations
between two opinion word candidates. E
to
? E
represents the opinion relations between opinion
target candidates and opinion word candidates.
Based on different relation types, we used three
matrices M
tt
? R
|V
t
|?|V
t
|
, M
oo
? R
|V
o
|?|V
o
|
and M
to
? R
|V
t
|?|V
o
|
to record the association
weights between any two vertices, respectively.
Section 3.4 will illustrate how to construct them.
3.1 Only Considering Opinion Relations
To estimate the confidence of each candidate, we
use a random walk algorithm on our graph to per-
form co-ranking. Most previous methods (Hu and
Liu, 2004a; Qiu et al, 2011; Wang and Wang,
2008; Liu et al, 2012) only considered opinion
relations among words. Their basic assumption is
as follows.
Assumption 1: If a word is likely to
be an opinion word, the words which
it has opinion relation with will have
higher confidence to be opinion targets,
and vice versa.
316
In this way, candidates? confidences (v
t
or v
o
) are
collectively determined by each other iteratively.
It equals to making random walk on subgraph
G
to
= (V,E
to
) of G. Thus we have
C
t
= (1? ?)?M
to
? C
o
+ ?? I
t
C
o
= (1? ?)?M
T
to
? C
t
+ ?? I
o
(1)
where C
t
and C
o
respectively represent confi-
dences of opinion targets and opinion words.
m
to
i,j
?M
to
means the association weight between
the ith opinion target and the jth opinion word ac-
cording to their opinion relations.
It?s worthy noting that I
t
and I
o
respectively de-
note prior confidences of opinion target candidates
and opinion word candidates. We argue that opin-
ion targets are usually domain-specific, and there
are remarkably distribution difference of them on
different domains (in-domain D
in
vs. out-domain
D
out
). If a candidate is salient inD
in
but common
in D
out
, it?s likely to be an opinion target in D
in
.
Thus, we use a domain relevance measure (DR)
(Hai et al, 2013) to compute I
t
.
DR(t) =
R(t,D
in
)
R(t,D
out
)
(2)
where R(t,D) =
w?
t
s
t
?
?
N
j=1
(w
tj
?
1
W
j
?
?
W
j
k=1
w
kj
) represents candidate relevance with
domain D. w
tj
= (1 + logTF
tj
) ? log
N
DF
t
is a TF-IDF-like weight of candidate t in doc-
ument j. TF
tj
is the frequency of the candi-
date t in the jth document, and DF
t
is docu-
ment frequency. N means the document num-
ber in domain D. R(t,D) includes two mea-
sures to reflect the salient of a candidate in D. 1)
w
tj
?
1
W
j
?
?
W
j
k=1
w
kj
reflects how frequently a
term is mentioned in a particular document. W
j
denotes the word number in document j. 2)
w?
t
s
t
quantifies how significantly a term is mentioned
across all documents in D. w?
t
=
1
N
?
?
N
k=1
w
tk
denotes average weight across all documents for
t. s
t
=
?
1
N
?
?
N
j=1
(w
tj
? w?
j
)
2
denotes the
standard variance of term t. We use the given
reviews as in-domain collection D
in
and Google
n-gram corpus
1
as out-domain collection D
out
.
Finally, each entry in I
t
is a normalized DR(t)
score. In contrast, opinion words are usually
domain-independent. Users may use same words
to express theirs opinions, like ?good?, ?bad?, etc.
But there are still some domain-dependent opinion
1
http://books.google.com/ngrams/datasets
words, like ?delicious? in the restaurant domain,
?powerful? in the car domain. It?s difficult to dis-
criminate them from other words by using statisti-
cal information. So we simply set al entries in I
o
to be 1. ? ? [0, 1] in Eq.1 determines the impact
of the prior confidence on results.
3.2 Only Considering Semantic Relations
To estimate candidates? confidences by only con-
sidering semantic relations among words, we
make two separately random walks on the sub-
graphs of G, G
tt
= (V,E
tt
) and G
oo
= (V,E
oo
).
The basic assumption is as follows:
Assumption 2: If a word is likely to
be an opinion target (opinion word), the
words which it has strong semantic rela-
tion with will have higher confidence to
be opinion targets (opinion words).
In this way, the confidence of the candidate is
determined only by its homogeneous neighbours.
There is no mutual reinforcement between opinion
targets and opinion words. Thus we have
C
t
= (1? ?)?M
tt
? C
t
+ ? ? I
t
C
o
= (1? ?)?M
oo
? C
o
+ ? ? I
o
(3)
where ? has the same role as ? in Eq.1.
3.3 Considering Semantic Relations and
Opinion Relations Together
To jointly model semantic relations and opinion
relations for opinion targets/words extraction, we
couple two random walking algorithms mentioned
above together. Here, Assumption 1 and As-
sumption 2 are both satisfied. Thus, an opinion
target/word candidate?s confidence is collectively
determined by its neighbours according to differ-
ent relation types. Meanwhile, each item may
make influence on it?s neighbours. It?s an iterative
reinforcement process. Thus, we have
C
t
= (1? ?? ?)?M
to
? C
o
+ ??M
tt
? C
t
+ ?? I
t
C
o
= (1? ?? ?)?M
T
to
? C
t
+ ??M
oo
? C
o
+ ?? I
o
(4)
where ? ? [0, 1] determines which type of rela-
tions dominates candidate confidence estimation.
? = 0 means that each candidate?s confidence
is estimated by only considering opinion relations
among words, which equals to Eq.1. Otherwise,
when ? = 1, candidate confidence estimation only
317
considers semantic relations among words, which
equals to Eq.3. ?, I
o
and I
t
have the same meaning
in Eq.1. Our algorithm will run iteratively until it
converges or in a fixed iteration number Iter. In
experiments, we set Iter = 200.
Obtaining Word Preference. The co-ranking
algorithm in Eq.4 is based on a standard random
walking algorithm, which randomly selects a link
according to the association matrix M
to
, M
tt
and
M
oo
, or jumps to a random node with prior confi-
dence value. However, it generates a global rank-
ing over all candidates without taking the node
preference (word preference) into account. As
mentioned in the first section, each opinion tar-
get/word has its preferred collocations, it?s reason-
able that the confidence of an opinion target (opin-
ion word) candidate should be preferentially de-
termined by its preferences, rather than all of its
neighbors with opinion relations.
To obtain the word preference, we resort to top-
ics. We believe that if an opinion word v
i
o
is
topical related with a target word v
j
t
, v
i
o
can be
regarded as a word preference for v
j
t
, and vice
versa. For example, ?price? and ?expensive? are
topically related in phone?s domain, so they are a
word preference for each other.
Specifically, we use a vector P
T
i
=
[P
T
i
1
, ..., P
T
i
k
, ..., P
T
i
|V
o
|
]
1?|V
o
|
to represent word
preference of the ith opinion target candidate.
P
T
i
k
means the preferred probability of the ith
potential opinion target for the kth potential
opinion words. To compute P
T
i
k
, we first use
Kullback-Leibler divergence to measure the
semantic distance between any two candidates on
the bridge of topics. Thus, we have
D(v
i
, v
j
) =
1
2
?
z
(KL
z
(v
i
||v
j
) +KL
z
(v
j
||v
i
))
whereKL
z
(v
i
||v
j
) = p(z|v
i
)log
p(z|v
i
)
p(z|v
j
)
means the
KL-divergence from candidate v
i
to v
j
based on
topic z. p(z|v) = p(v|z)
p(z)
p(v)
, where p(v|z) is the
probability of the candidate v to topic z (see Sec-
tion 3.4). p(z) is the probability that topic z in
reviews. p(v) is the probability that a candidate
occurs in reviews. Then, a logistic function is used
to map D(v
i
, v
j
) into [0, 1].
SA(v
i
, v
j
) =
1
1 + e
D(v
i
,v
j
)
(5)
Then, we calculate P
T
i
k
by normalize SA(v
i
, v
j
)
score, i.e. P
T
i
k
=
SA(v
t
i
,v
o
k
)
?
|V
o
|
p=1
SA(v
t
i
,v
o
p
)
. For demon-
stration, we give some examples in Table 1, where
each entry denotes a SA(v
i
, v
j
) score between two
candidates. We can see that using topics can suc-
cessfully capture the preference information for
each opinion target/word.
expensive good long colorful
price 0.265 0.043 0.003 0.000
LED 0.002 0.035 0.007 0.098
battery 0.000 0.015 0.159 0.001
Table 1: Examples of Calculated Word Preference
And we use a vector P
O
j
=
[P
O
j
1
, ..., P
O
j
q
, ..., P
O
j
|V
t
|
]
1?|V
t
|
to represent
the preference information of the jth opin-
ion word candidate. Similarly, we have
P
O
j
q
=
SA(v
t
q
,v
o
j
)
?
|V
t
|
k=1
SA(v
t
k
,v
o
j
)
.
Incorporating Word Preference into Co-
ranking. To consider such word preference in
our co-ranking algorithm, we incorporate it into
the random walking on G
to
. Intuitively, prefer-
ence vectors will be different for different can-
didates. Thus, the co-ranking algorithm would
be personalized. It allows that the candidate
confidence propagates to other candidates only
in its preference cluster. Specifically, we make
modification on original transition matrix M
to
=
(M
to
1
,M
to
2
, ...,M
to
|V
t
|
) and add each candidate?s
preference in it. Let
?
M
to
= (
?
M
to
1
,
?
M
to
2
, ...,
?
M
to
|V
t
|
)
be the modified transition matrix, which records
the associations between opinion target candi-
dates and opinion word candidates. Here M
to
k
?
R
1?|V
o
|
and
?
M
to
k
? R
1?|V
o
|
denotes the kth col-
umn vector in M
to
and
?
M
to
, respectively. And
let Diag(P
T
k
) denote a diagonal matrix whose
eigenvalue is vector P
T
k
, we have
?
M
to
k
= M
to
k
Diag(P
T
k
)
Similarly, let U
to
k
? R
1?|V
t
|
and
?
U
to
k
? R
1?|V
t
|
denotes the kth row vector in M
T
to
and
?
M
T
to
, re-
spectively. Diag(P
O
k
) denote a diagonal matrix
whose eigenvalue is vector P
O
k
. Then we have
?
U
to
k
= U
to
k
Diag(P
O
k
)
In this way, each candidate?s preference is in-
corporated into original associations based on
opinion relation M
to
through Diag(P
O
k
) and
Diag(P
T
k
). And candidates? confidences will
mainly come from the contributions of its prefer-
ences. Thus, C
t
and C
o
in Eq.4 become:
318
Ct
= (1? ?? ?)?
?
M
to
? C
o
+ ??M
tt
? C
t
+ ?? I
t
C
o
= (1? ?? ?)?
?
M
T
to
? C
t
+ ??M
oo
? C
o
+ ?? I
o
(6)
3.4 Capturing Semantic and Opinion
Relations
In this section, we explain how to capture seman-
tic relations and opinion relations for constructing
transition matrices M
tt
, M
oo
and M
to
.
Capturing Semantic Relations: For captur-
ing semantic relations among homogenous candi-
dates, we employ topics. We believe that if two
candidates share similar topics in the corpus, there
is a strong semantic relation between them. Thus,
we employ a LDA variation (Mukherjee and Liu,
2012), an extension of (Zhao et al, 2010), to dis-
cover topic distribution on words, which sampled
all words into two separated observations: opinion
targets and opinion words. It?s because that we are
only interested in topic distribution of opinion tar-
gets/words, regardless of other useless words, in-
cluding conjunctions, prepositions etc. This model
has been proven to be better than the standard
LDA model and other LDA variations for opinion
mining (Mukherjee and Liu, 2012).
After topic modeling, we obtain the proba-
bility of the candidates (v
t
and v
o
) to topic z,
i.e. p(z|v
t
) and p(z|v
o
), and topic distribution
p(z). Then, a symmetric Kullback-Leibler diver-
gence as same as Eq.5 is used to calculate the se-
mantical associations between any two homoge-
nous candidates. Thus, we obtain SA(v
t
, v
t
) and
SA(v
o
, v
o
), which correspond to the entries in
M
tt
and M
oo
, respectively.
Capturing Opinion Relations: To capture
opinion relations among words and construct the
transition matrix M
to
, we used an alignment-
based method proposed in (Liu et al, 2013b).
This approach models capturing opinion relations
as a monolingual word alignment process. Each
opinion target can find its corresponding mod-
ifiers in sentences through alignment, in which
multiple factors are considered globally, such as
co-occurrence information, word position in sen-
tence, etc. Moreover, this model adopted a par-
tially supervised framework to combine syntac-
tic information with alignment results, which has
been proven to be more precise than the state-of-
the-art approaches for opinion relations identifica-
tion (Liu et al, 2013b).
After performing word alignment, we obtain
a set of word pairs composed of a noun (noun
phrase) and its corresponding modified word.
Then, we simply employ Pointwise Mutual Infor-
mation (PMI) to calculate the opinion associations
among words as the entries in M
to
. OA(v
t
, v
o
) =
log
p(v
t
,v
o
)
p(v
t
)p(v
o
)
, where v
t
and v
o
denote an opinion
target candidate and an opinion word candidate,
respectively. p(v
t
, v
o
) is the co-occurrence prob-
ability of v
t
and v
o
based on the opinion relation
identification results. p(v
t
) and p(v
o
) give the in-
dependent occurrence probability of of v
t
and v
o
,
respectively
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets: To evaluate the proposed method, we
used three datasets. The first one is Customer
Review Datasets (CRD), used in (Hu and Liu,
2004a), which contains reviews about five prod-
ucts. The second one is COAE2008 dataset2
2
,
which contains Chinese reviews about four prod-
ucts. The third one is Large, also used in (Wang
et al, 2011; Liu et al, 2012; Liu et al, 2013a),
where two domains are selected (Mp3 and Hotel).
As mentioned in (Liu et al, 2012), Large con-
tains 6,000 sentences for each domain. Opinion
targets/words are manually annotated, where three
annotators were involved. Two annotators were
required to annotate out opinion words/targets in
reviews. When conflicts occur, the third annota-
tor make final judgement. In total, we respectively
obtain 1,112, 1,241 opinion targets and 334, 407
opinion words in Hotel, MP3.
Pre-processing: All sentences are tagged to
obtain words? part-of-speech tags using Stanford
NLP tool
3
. And noun phrases are identified using
the method in (Zhu et al, 2009) before extraction.
Evaluation Metrics: We select precision(P),
recall(R) and f-measure(F) as metrics. And a sig-
nificant test is performed, i.e., a t-test with a de-
fault significant level of 0.05.
4.2 Our Method vs. The State-of-the-art
Methods
To prove the effectiveness of the proposed method,
we select some state-of-the-art methods for com-
parison as follows:
2
http://ir-china.org.cn/coae2008.html
3
http://nlp.stanford.edu/software/tagger.shtml
319
Methods
D1 D2 D3 D4 D5 Avg.
P R F P R F P R F P R F P R F F
Hu 0.75 0.82 0.78 0.71 0.79 0.75 0.72 0.76 0.74 0.69 0.82 0.75 0.74 0.80 0.77 0.758
DP 0.87 0.81 0.84 0.90 0.81 0.85 0.90 0.86 0.88 0.81 0.84 0.82 0.92 0.86 0.89 0.856
Zhang 0.83 0.84 0.83 0.86 0.85 0.85 0.86 0.88 0.87 0.80 0.85 0.82 0.86 0.86 0.86 0.846
SAS 0.80 0.79 0.79 0.82 0.76 0.79 0.79 0.74 0.76 0.77 0.78 0.77 0.80 0.76 0.78 0.778
Liu 0.84 0.85 0.84 0.87 0.85 0.86 0.88 0.89 0.88 0.81 0.85 0.83 0.89 0.87 0.88 0.858
Hai 0.77 0.87 0.83 0.79 0.86 0.82 0.79 0.89 0.84 0.72 0.88 0.79 0.74 0.88 0.81 0.818
CR 0.84 0.86 0.85 0.87 0.85 0.86 0.87 0.90 0.88 0.81 0.87 0.83 0.89 0.88 0.89 0.862
CR WP 0.86 0.86 0.86 0.88 0.86 0.87 0.89 0.90 0.89 0.81 0.87 0.83 0.91 0.89 0.90 0.870
Table 2: Results of Opinion Targets Extraction on Customer Review Dataset
Methods
Camera Car Laptop Phone Mp3 Hotel Avg.
P R F P R F P R F P R F P R F P R F F
Hu 0.63 0.65 0.64 0.62 0.58 0.60 0.51 0.67 0.58 0.69 0.60 0.64 0.61 0.68 0.64 0.60 0.65 0.62 0.587
DP 0.71 0.70 0.70 0.72 0.65 0.68 0.58 0.69 0.63 0.78 0.66 0.72 0.69 0.70 0.69 0.67 0.69 0.68 0.683
Zhang 0.71 0.78 0.74 0.69 0.68 0.68 0.57 0.80 0.67 0.80 0.71 0.75 0.67 0.77 0.72 0.67 0.76 0.71 0.712
SAS 0.72 0.72 0.72 0.71 0.64 0.67 0.59 0.72 0.65 0.78 0.69 0.73 0.69 0.75 0.72 0.69 0.74 0.71 0.700
Liu 0.75 0.81 0.78 0.71 0.71 0.71 0.61 0.85 0.71 0.83 0.74 0.78 0.70 0.82 0.76 0.71 0.80 0.75 0.749
Hai 0.68 0.84 0.76 0.69 0.75 0.72 0.58 0.86 0.72 0.75 0.76 0.76 0.65 0.83 0.74 0.62 0.82 0.75 0.742
CR 0.75 0.83 0.79 0.72 0.74 0.73 0.60 0.85 0.70 0.83 0.77 0.80 0.70 0.84 0.76 0.71 0.83 0.77 0.758
CR WP 0.78 0.84 0.81 0.74 0.75 0.74 0.64 0.85 0.73 0.84 0.76 0.80 0.74 0.84 0.79 0.74 0.82 0.78 0.773
Table 3: Results of Opinion Targets Extraction on COAE 2008 and Large
Hu extracted opinion targets/words using asso-
ciation mining rules (Hu and Liu, 2004a).
DP used syntax-based patterns to capture opin-
ion relations in sentences, and then used a boot-
strapping process to extract opinion targets/words
(Qiu et al, 2011),.
Zhang is proposed by (Zhang et al, 2010).
They also used syntactic patterns to capture opin-
ion relations between words. Then a HITS (Klein-
berg, 1999) algorithm is employed to extract opin-
ion targets.
Liu is proposed by (Liu et al, 2013a), an ex-
tension of (Liu et al, 2012). They employed a
word alignment model to capture opinion relations
among words, and then used a random walking al-
gorithm to extract opinion targets.
Hai is proposed by (Hai et al, 2012), which is
similar to our method. They employed both of se-
mantic relations and opinion relations to extract
opinion words/targets in a bootstrapping frame-
work. But they captured relations only using co-
occurrence statistics. Moreover, word preference
was not considered.
SAS is proposed by (Mukherjee and Liu, 2012),
an extended lda-based model of (Zhao et al,
2010). The top K items for each aspect are ex-
tracted as opinion targets/words. It means that
only semantic relations among words are consid-
ered in SAS. And we set aspects number to be 9 as
same as (Mukherjee and Liu, 2012).
CR: is the proposed method in this paper by us-
ing co-ranking, referring to Eq.4. CR doesn?t con-
sider word preference.
CR WP: is the full implementation of our
method, referring to Eq.6.
Hu, DP, Zhang and Liu are the methods which
only consider opinion relations among words.
SAS is the methods which only consider seman-
tic relations among words. Hai, CR and CR WP
consider these two types of relations together. The
parameter settings of state-of-the-art methods are
same as their original paper. In CR and CR WP,
we set ? = 0.4 and ? = 0.1. The experimental
results are shown in Table 2, 3, 4 and 5, where the
last column presents the average F-measure scores
for multiple domains. Since Liu and Zhang aren?t
designed for opinion words extraction, we don?t
present their results in Table 4 and 5. From exper-
imental results, we can see.
1) Our methods (CR and CR WP) outperform
other methods not only on opinion targets extrac-
tion but on opinion words extraction in most do-
mains. It proves the effectiveness of the proposed
method.
2) CR and CR WP have much better perfor-
mance than Liu and Zhang, especially on Recall.
Liu and Zhang also use a ranking framework like
ours, but they only employ opinion relations for
extraction. In contrast, besides opinion relations,
CR and CR WP further take semantic relations
into account. Thus, more opinion targets/words
can be extracted. Furthermore, we observe that
CR and CR WP outperform SAS. SAS only ex-
ploits semantic relations, but ignores opinion re-
lations among words. Its extraction is performed
separately and neglects the reinforcement between
opinion targets and opinion words. Thus, SAS has
worse performance than our methods. It demon-
strates the usefulness of considering multiple rela-
tion types.
320
Methods
D1 D2 D3 D4 D5 Avg.
P R F P R F P R F P R F P R F F
Hu 0.57 0.75 0.65 0.51 0.76 0.61 0.57 0.73 0.64 0.54 0.62 0.58 0.62 0.67 0.64 0.624
DP 0.64 0.73 0.68 0.57 0.79 0.66 0.65 0.70 0.67 0.61 0.65 0.63 0.70 0.68 0.69 0.666
SAS 0.64 0.68 0.66 0.55 0.70 0.62 0.62 0.65 0.63 0.60 0.61 0.60 0.68 0.63 0.65 0.632
Hai 0.62 0.77 0.69 0.52 0.80 0.64 0.60 0.74 0.67 0.56 0.69 0.62 0.66 0.70 0.68 0.660
CR 0.62 0.75 0.68 0.57 0.79 0.67 0.64 0.75 0.69 0.63 0.69 0.66 0.68 0.69 0.69 0.678
CR WP 0.65 0.75 0.70 0.59 0.80 0.68 0.65 0.74 0.70 0.66 0.68 0.67 0.71 0.70 0.70 0.690
Table 4: Results of Opinion Words Extraction on Customer Review Dataset
Methods
Camera Car Laptop Phone Mp3 Hotel Avg.
P R F P R F P R F P R F P R F P R F F
Hu 0.72 0.74 0.73 0.70 0.71 0.70 0.66 0.70 0.68 0.70 0.70 0.70 0.48 0.67 0.56 0.52 0.69 0.59 0.660
DP 0.80 0.73 0.76 0.79 0.71 0.75 0.75 0.69 0.72 0.78 0.68 0.73 0.60 0.65 0.62 0.61 0.66 0.63 0.702
SAS 0.73 0.70 0.71 0.75 0.68 0.71 0.72 0.68 0.69 0.71 0.66 0.68 0.64 0.62 0.63 0.66 0.61 0.63 0.675
Hai 0.76 0.74 0.75 0.72 0.74 0.73 0.69 0.72 0.70 0.72 0.70 0.71 0.61 0.69 0.64 0.59 0.68 0.64 0.690
CR 0.80 0.75 0.77 0.77 0.74 0.75 0.73 0.71 0.72 0.75 0.71 0.73 0.63 0.69 0.64 0.63 0.68 0.66 0.710
CR WP 0.80 0.75 0.77 0.80 0.74 0.77 0.77 0.71 0.74 0.78 0.72 0.75 0.66 0.68 0.67 0.67 0.69 0.68 0.730
Table 5: Results of Opinion Words Extraction on COAE 2008 and Large
3) CR and CR WP both outperform Hai. We
believe the reasons are as follows. First, CR and
CR WP considers multiple relations in a unified
process by using graph co-ranking. In contrast,
Hai adopts a bootstrapping framework which per-
forms extraction step by step and may have the
problem of error propagation. It demonstrates
that our graph co-ranking is more suitable for this
task than bootstrapping-based strategy. Second,
our method captures semantic relations using topic
modeling and captures opinion relations through
word alignments, which are more precise than Hai
which merely uses co-occurrence information to
indicate such relations among words. In addition,
word preference is not handled in Hai, but pro-
cessed in CR WP. The results show the usefulness
of word preference for opinion targets/words ex-
traction.
4) CR WP outperforms CR, especially on pre-
cision. The only difference between them is that
CR WP considers word preference when perform-
ing graph ranking for candidate confidence esti-
mation, but CR does not. Each candidate confi-
dence estimation in CR WP gives more weights
for this candidate?s preferred words than CR.
Thus, the precision can be improved.
4.3 Semantic Relation vs. Opinion Relation
In this section, we discuss which relation type
is more effective for this task. For comparison,
we design two baselines, called OnlySA and On-
lyOA. OnlyOA only employs opinion relations
among words, which equals to Eq.1. OnlySA only
employs semantic relations among words, which
equals to Eq.3. Moreover, Combine is our method
which considers both of opinion relations and se-
mantic relations together, referring to Eq.4 with
 
MP3 Hotel Laptop Phone
R
e
c
a
l
l
.65
.70
.75
.80
.85
.90
.95
OnlySA
OnlyOA
Combine
(a) Opinion Target Extraction Results
 
MP3 Hotel Laptop Phone
R
e
c
a
l
l
.60
.65
.70
.75
.80
OnlySA
OnlyOA
Combine
(b) Opinion Word Extraction Results
Figure 2: Semantic Relations vs. Opinion Rela-
tions
? = 0.5. Figure 2 presents experimental results.
The left graph presents opinion targets extraction
results and the right graph presents opinion words
extraction results. Because of space limitation, we
only shown the results of four domains (MP3, Ho-
tel, Laptop and Phone).
From results, we observe that OnlyOA outper-
forms OnlySA in all domains. It demonstrates
that employing opinion relations are more useful
than semantic relations for co-extracting opinion
targets/words. And it is necessary to utilize the
mutual reinforcement relationship between opin-
ion words and opinion targets. Moreover, Com-
bine outperforms OnlySA and OnlyOA in all do-
mains. It indicates that combining different rela-
tions among words together is effective.
4.4 The Effectiveness of Considering Word
Preference
In this section, we try to prove the necessity of
considering word preference in Eq.6. Besides the
comparison between CR and CR WP performed
321
in the main experiment in Section 4.2, we fur-
ther incorporate word preference in aforemen-
tioned OnlyOA, named as OnlyOA WP, which
only employs opinion relations among words and
equals to Eq.6 with ? = 0. Experimental results
are shown in Figure 3. Because of space limita-
tion, we only show the results of the same domains
in section 4.3,
Form results, we observe that CR WP out-
performs CR, and OnlyOA WP outperforms On-
lyOA in all domains, especially on precision.
These observations demonstrate that considering
word preference is very important for opinion tar-
gets/words extraction. We believe the reason is
that exploiting word preference can provide more
fine information for opinion target/word candi-
dates? confidence estimation. Thus the perfor-
mance can be improved.
 
MP3 Hotel Laptop Phone
P
re
c
i
si
o
n
.60
.65
.70
.75
.80
.85
.90
OnlyOA
OnlyOA_WP
CR
CR_WP
 
MP3 Hotel Laptop Phone
R
e
c
a
l
l
.70
.75
.80
.85
.90
.95
OnlyOA
OnlyOA_WP
CR
CR_WP
(a) Opinion Target Extraction Results
 
MP3 Hotel Laptop Phone
P
re
c
i
si
o
n
.60
.65
.70
.75
.80
OnlyOA
OnlyOA_WP
CR
CR_WP
 
MP3 Hotel Laptop Phone
R
e
c
a
l
l
.60
.65
7
.75
.80
OnlyOA
OnlyOA_WP
CR
CR_WP
(b) Opinion Word Extraction Results
Figure 3: Experimental results when considering
word preference
4.5 Parameter Sensitivity
In this subsection, we discuss the variation of ex-
traction performance when changing ? and ? in
Eq.6. Due to space limitation, we only show the
F-measure of CR WP on four domains. Experi-
mental results are shown in Figure 4 and Figure
5. The left graphs in Figure 4 and 5 present the
performance variation of CR WP with varying ?
from 0 to 0.9 and fixing ? = 0.1. The right graphs
in Figure 4 and 5 present the performance varia-
tion of CR WP with varying ? from 0 to 0.6 and
fixing ? = 0.4.
In the left graphs in Figure 4 and 5, we observe
the best performance is obtained when ? = 0.4.
It indicates that opinion relations and semantic re-
lations are both useful for extracting opinion tar-
gets/words. The extraction performance is benefi-
cial from their combination. In the right graphs in
Figure 4 and 5, the best performance is obtained
when ? = 0.1. It indicates prior knowledge is
useful for extraction. When ? increases, perfor-
mance, however, decreases. It demonstrates that
incorporating more prior knowledge into our al-
gorithm would restrain other useful clues on esti-
mating candidate confidence, and hurt the perfor-
mance.
 
0.0 .1 .2 .3 .4 .5 .6 .7 .8 .9
F-M
e
a
sure
.60
.65
.70
.75
.80
.85
MP3
Hotel 
Laptop
Phone
 
0.0 .1 .2 .3 .4 .5 .6
F-M
e
a
sure
.65
.70
.75
.80
.85
MP3
Hotel 
Laptop
Phone
Figure 4: Opinion targets extraction results
 
0.0 .1 .2 .3 .4 .5 .6 .7 .8 .9
F-M
e
a
sure
.55
.60
.65
.70
.75
.80
MP3
Hotel 
Laptop
Phone
 
0.0 .1 .2 .3 .4 .5 .6
F-M
e
a
sure
.50
.55
.60
.65
.70
.75
.80
MP3
Hotel 
Laptop
Phone
Figure 5: Opinion words extraction results
5 Conclusions
This paper presents a novel method with graph co-
ranking to co-extract opinion targets/words. We
model extracting opinion targets/words as a co-
ranking process, where multiple heterogenous re-
lations are modeled in a unified model to make co-
operative effects on the extraction. In addition, we
especially consider word preference in co-ranking
process to perform more precise extraction. Com-
pared to the state-of-the-art methods, experimental
results prove the effectiveness of our method.
Acknowledgement
This work was sponsored by the National
Basic Research Program of China (No.
2014CB340500), the National Natural Sci-
ence Foundation of China (No. 61272332
and No. 61202329), the National High Tech-
nology Development 863 Program of China
(No. 2012AA011102), and CCF-Tencent Open
Research Fund.
References
Juergen Bross and Heiko Ehrig. 2013. Automatic con-
struction of domain and aspect specific sentiment
322
lexicons for customer review mining. In Proceed-
ings of the 22nd ACM international conference on
Conference on information &#38; knowledge man-
agement, CIKM ?13, pages 1077?1086, New York,
NY, USA. ACM.
Zhen Hai, Kuiyu Chang, and Gao Cong. 2012. One
seed to find them all: mining opinion features via
association. In CIKM, pages 255?264.
Zhen Hai, Kuiyu Chang, Jung-Jae Kim, and Christo-
pher C. Yang. 2013. Identifying features in opinion
mining via intrinsic and extrinsic domain relevance.
IEEE Transactions on Knowledge and Data Engi-
neering, 99(PrePrints):1.
Mingqin Hu and Bing Liu. 2004a. Mining opinion fea-
tures in customer reviews. In Proceedings of Con-
ference on Artificial Intelligence (AAAI).
Minqing Hu and Bing Liu. 2004b. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Yingju Xia, Shu Zhang, and Hao Yu. 2010.
Structure-aware review mining and summarization.
In Chu-Ren Huang and Dan Jurafsky, editors, COL-
ING, pages 653?661. Tsinghua University Press.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In Allan Ellis and Tatsuya Hagino,
editors, WWW, pages 342?351. ACM.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1346?1356, Jeju Island, Korea,
July. Association for Computational Linguistics.
Kang Liu, Liheng Xu, Yang Liu, and Jun Zhao. 2013a.
Opinion target extraction using partially supervised
word alignment model.
Kang Liu, Liheng Xu, and Jun Zhao. 2013b. Syntactic
patterns versus word alignment: Extracting opinion
targets from online reviews.
Tengfei Ma and Xiaojun Wan. 2010. Opinion tar-
get extraction in chinese news comments. In Chu-
Ren Huang and Dan Jurafsky, editors, COLING
(Posters), pages 782?790. Chinese Information Pro-
cessing Society of China.
Samaneh Moghaddam and Martin Ester. 2011. Ilda:
Interdependent lda model for learning latent aspects
and their ratings from online product reviews. In
Proceedings of the 34th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR ?11, pages 665?674, New
York, NY, USA. ACM.
Samaneh Moghaddam and Martin Ester. 2012a.
Aspect-based opinion mining from product reviews.
In Proceedings of the 35th International ACM SIGIR
Conference on Research and Development in In-
formation Retrieval, SIGIR ?12, pages 1184?1184,
New York, NY, USA. ACM.
Samaneh Moghaddam and Martin Ester. 2012b. On
the design of lda models for aspect-based opinion
mining. In CIKM, pages 803?812.
Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, ACL ?12, pages 339?348, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ?05, pages 339?346,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Che. 2009.
Expanding domain sentiment lexicon through dou-
ble propagation.
Guang Qiu, Bing Liu 0001, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Lin-
guistics, 37(1):9?27.
Qi Su, Xinying Xu, Honglei Guo, Zhili Guo, Xian
Wu, Xiaoxun Zhang, Bin Swen, and Zhong Su.
2008. Hidden sentiment association in chinese web
opinion mining. In Jinpeng Huai, Robin Chen,
Hsiao-Wuen Hon, Yunhao Liu, Wei-Ying Ma, An-
drew Tomkins, and Xiaodong Zhang 0001, editors,
WWW, pages 959?968. ACM.
Bo Wang and Houfeng Wang. 2008. Bootstrapping
both product features and opinion words from chi-
nese customer reviews with cross-inducing.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Chid Apt, Joydeep Ghosh,
and Padhraic Smyth, editors, KDD, pages 618?626.
ACM.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In EMNLP, pages 1533?1541. ACL.
Bishan Yang and Claire Cardie. 2013. Joint infer-
ence for fine-grained opinion extraction. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
323
Papers), pages 1640?1649, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking
product features in opinion documents. In Chu-
Ren Huang and Dan Jurafsky, editors, COLING
(Posters), pages 1462?1470. Chinese Information
Processing Society of China.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 56?
65, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In David Wai-Lok Cheung,
Il-Yeol Song, Wesley W. Chu, Xiaohua Hu, and
Jimmy J. Lin, editors, CIKM, pages 1799?1802.
ACM.
324
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 336?346,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Product Feature Mining: Semantic Clues versus Syntactic Constituents
Liheng Xu, Kang Liu, Siwei Lai and Jun Zhao
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China
{lhxu, kliu, swlai, jzhao}@nlpr.ia.ac.cn
Abstract
Product feature mining is a key subtask
in fine-grained opinion mining. Previ-
ous works often use syntax constituents in
this task. However, syntax-based methods
can only use discrete contextual informa-
tion, which may suffer from data sparsity.
This paper proposes a novel product fea-
ture mining method which leverages lexi-
cal and contextual semantic clues. Lexical
semantic clue verifies whether a candidate
term is related to the target product, and
contextual semantic clue serves as a soft
pattern miner to find candidates, which ex-
ploits semantics of each word in context
so as to alleviate the data sparsity prob-
lem. We build a semantic similarity graph
to encode lexical semantic clue, and em-
ploy a convolutional neural model to cap-
ture contextual semantic clue. Then Label
Propagation is applied to combine both se-
mantic clues. Experimental results show
that our semantics-based method signif-
icantly outperforms conventional syntax-
based approaches, which not only mines
product features more accurately, but also
extracts more infrequent product features.
1 Introduction
In recent years, opinion mining has helped cus-
tomers a lot to make informed purchase decisions.
However, with the rapid growth of e-commerce,
customers are no longer satisfied with the over-
all opinion ratings provided by traditional senti-
ment analysis systems. The detailed functions or
attributes of products, which are called product
features, receive more attention. Nevertheless, a
product may have thousands of features, which
makes it impractical for a customer to investigate
them all. Therefore, mining product features au-
tomatically from online reviews is shown to be a
key step for opinion summarization (Hu and Liu,
2004; Qiu et al, 2009) and fine-grained sentiment
analysis (Jiang et al, 2011; Li et al, 2012).
Previous works often mine product features via
syntactic constituent matching (Popescu and Et-
zioni, 2005; Qiu et al, 2009; Zhang et al, 2010).
The basic idea is that reviewers tend to comment
on product features in similar syntactic structures.
Therefore, it is natural to mine product features by
using syntactic patterns. For example, in Figure 1,
the upper box shows a dependency tree produced
by Stanford Parser (de Marneffe et al, 2006), and
the lower box shows a common syntactic pattern
from (Zhang et al, 2010), where <feature/NN>
is a wildcard to be fit in reviews and NN denotes
the required POS tag of the wildcard. Usually, the
product name mp3 is specified, and when screen
matches the wildcard, it is likely to be a product
feature of mp3.
 
Figure 1: An example of syntax-based prod-
uct feature mining procedure. The word screen
matches the wildcard <feature/NN>. Therefore,
screen is likely to be a product feature of mp3.
Generally, such syntactic patterns extract prod-
uct features well but they still have some limita-
tions. For example, the product-have-feature pat-
tern may fail to find the fm tuner in a very similar
case in Example 1(a), where the product is men-
tioned by using player instead of mp3. Similarly,
it may also fail on Example 1(b), just with have re-
placed by support. In essence, syntactic pattern is
336
a kind of one-hot representation for encoding the
contexts, which can only use partial and discrete
features, such as some key words (e.g., have) or
shallow information (e.g., POS tags). Therefore,
such a representation often suffers from the data
sparsity problem (Turian et al, 2010).
One possible solution for this problem is us-
ing a more general pattern such as NP-VB-feature,
where NP represents a noun or noun phrase and
VB stands for any verb. However, this pattern be-
comes too general that it may find many irrelevant
cases such as the one in Example 1(c), which is not
talking about the product. Consequently, it is very
difficult for a pattern designer to balance between
precision and generalization.
Example 1:
(a) This player has an
::
fm
:::::
tuner.
(b) This mp3 supports
::::
wma
:::
file.
(c) This review has helped
:::::
people a lot.
(d) This mp3 has some
:::::
flaws.
To solve the problems stated above, it is ar-
gued that deeper semantics of contexts shall be ex-
ploited. For example, we can try to automatically
discover that the verb have indicates a part-whole
relation (Zhang et al, 2010) and support indicates
a product-function relation, so that both sth. have
and sth. support suggest that terms following them
are product features, where sth. can be replaced
by any terms that refer to the target product (e.g.,
mp3, player, etc.). This is called contextual se-
mantic clue. Nevertheless, only using contexts is
not sufficient enough. As in Example 1(d), we can
see that the word flaws follows mp3 have, but it
is not a product feature. Thus, a noise term may
be extracted even with high contextual support.
Therefore, we shall also verify whether a candi-
date is really related to the target product. We call
it lexical semantic clue.
This paper proposes a novel bootstrapping ap-
proach for product feature mining, which lever-
ages both semantic clues discussed above. Firstly,
some reliable product feature seeds are automat-
ically extracted. Then, based on the assumption
that terms that are more semantically similar to
the seeds are more likely to be product features,
a graph which measures semantic similarities be-
tween terms is built to capture lexical semantic
clue. At the same time, a semi-supervised con-
volutional neural model (Collobert et al, 2011) is
employed to encode contextual semantic clue. Fi-
nally, the two kinds of semantic clues are com-
bined by a Label Propagation algorithm.
In the proposed method, words are represented
by continuous vectors, which capture latent se-
mantic factors of the words (Turian et al, 2010).
The vectors can be unsupervisedly trained on large
scale corpora, and words with similar semantics
will have similar vectors. This enables our method
to be less sensitive to lexicon change, so that the
data sparsity problem can be alleviated . The con-
tributions of this paper include:
? It uses semantics of words to encode contextual
clues, which exploits deeper level information
than syntactic constituents. As a result, it mines
product features more accurately than syntax-
based methods.
? It exploits semantic similarity between words
to capture lexical clues, which is shown to be
more effective than co-occurrence relation be-
tween words and syntactic patterns. In addition,
experiments show that the semantic similarity
has the advantage of mining infrequent product
features, which is crucial for this task. For ex-
ample, one may say ?This hotel has low water
pressure?, where low water pressure is seldom
mentioned, but fatal to someone?s taste.
? We compare the proposed semantics-based ap-
proach with three state-of-the-art syntax-based
methods. Experiments show that our method
achieves significantly better results.
The rest of this paper is organized as follows. Sec-
tion 2 introduces related work. Section 3 describes
the proposed method in details. Section 4 gives the
experimental results. Lastly, we conclude this pa-
per in Section 5.
2 Related Work
In product feature mining task, Hu and Liu (2004)
proposed a pioneer research. However, the asso-
ciation rules they used may potentially introduce
many noise terms. Based on the observation that
product features are often commented on by simi-
lar syntactic structures, it is natural to use patterns
to capture common syntactic constituents around
product features.
Popescu and Etzioni (2005) designed some syn-
tactic patterns to search for product feature candi-
dates and then used Pointwise Mutual Information
(PMI) to remove noise terms. Qiu et al (2009)
proposed eight heuristic syntactic rules to jointly
extract product features and sentiment lexicons,
where a bootstrapping algorithm named Double
337
Propagation was applied to expand a given seed
set. Zhang et al (2010) improved Qiu?s work
by adding more feasible syntactic patterns, and the
HITS algorithm (Kleinberg, 1999) was employed
to rank candidates. Moghaddam and Ester (2010)
extracted product features by automatical opinion
pattern mining. Zhuang et al (2006) used various
syntactic templates from an annotated movie cor-
pus and applied them to supervised movie feature
extraction. Wu et al (2009) proposed a phrase
level dependency parsing for mining aspects and
features of products.
As discussed in the first section, syntactic pat-
terns often suffer from data sparsity. Further-
more, most pattern-based methods rely on term
frequency, which have the limitation of finding
infrequent but important product features. A re-
cent research (Xu et al, 2013) extracted infrequent
product features by a semi-supervised classifier,
which used word-syntactic pattern co-occurrence
statistics as features for the classifier. However,
this kind of feature is still sparse for infrequent
candidates. Our method adopts a semantic word
representation model, which can train dense fea-
tures unsupervisedly on a very large corpus. Thus,
the data sparsity problem can be alleviated.
3 The Proposed Method
We propose a semantics-based bootstrapping
method for product feature mining. Firstly, some
product feature seeds are automatically extracted.
Then, a semantic similarity graph is created to
capture lexical semantic clue, and a Convolutional
Neural Network (CNN) (Collobert et al, 2011) is
trained in each bootstrapping iteration to encode
contextual semantic clue. Finally we use Label
Propagation to find some reliable new seeds for
the training of the next bootstrapping iteration.
3.1 Automatic Seed Generation
The seed set consists of positive labeled examples
(i.e. product features) and negative labeled exam-
ples (i.e. noise terms). Intuitively, popular product
features are frequently mentioned in reviews, so
they can be extracted by simply mining frequently
occurring nouns (Hu and Liu, 2004). However,
this strategy will also find many noise terms (e.g.,
commonly used nouns like thing, one, etc.). To
produce high quality seeds, we employ a Domain
Relevance Measure (DRM) (Jiang and Tan, 2010),
which combines term frequency with a domain-
specific measuring metric called Likelihood Ratio
Test (LRT) (Dunning, 1993). Let ?(t) denotes the
LRT score of a product feature candidate t,
?(t) =
p
k
1
(1? p)
n
1
?k
1
p
k
2
(1? p)
n
2
?k
2
p
k
1
1
(1? p
1
)
n
1
?k
1
p
k
2
2
(1? p
2
)
n
2
?k
2
(1)
where k
1
and k
2
are the frequencies of t in the
review corpus R and a background corpus
1
B, n
1
and n
2
are the total number of terms in R and B,
p = (k
1
+ k
2
)/(n
1
+ n
2
), p
1
= k
1
/n
1
and p
2
=
k
2
/n
2
. Then a modified DRM
2
is proposed,
DRM(t) =
tf(t)
max[tf(?)]
?
1
log df(t)
?
| log ?(t)| ?min| log ?(?)|
max| log ?(?)| ?min| log ?(?)|
(2)
where tf(t) is the frequency of t inR and df(t) is
the frequency of t in B.
All nouns in R are ranked by DRM(t) in de-
scent order, where top N nouns are taken as the
positive example set V
+
s
. On the other hand, Xu
et al (2013) show that a set of general nouns sel-
dom appear to be product features. Therefore, we
employ their General Noun Corpus to create the
negative example set V
?
s
, where N most frequent
terms are selected. Besides, it is guaranteed that
V
+
s
? V
?
s
= ?, i.e., conflicting terms are taken as
negative examples.
3.2 Capturing Lexical Semantic Clue in a
Semantic Similarity Graph
To capture lexical semantic clue, each word is first
converted into word embedding, which is a con-
tinuous vector with each dimension?s value corre-
sponds to a semantic or grammatical interpretation
(Turian et al, 2010). Learning large-scale word
embeddings is very time-consuming (Collobert et
al., 2011), we thus employ a faster method named
Skip-gram model (Mikolov et al, 2013).
3.2.1 Learning Word Embedding for
Semantic Representation
Given a sequence of training words W =
{w
1
, w
2
, ..., w
m
}, the goal of the Skip-gram
model is to learn a continuous vector space EB =
{e
1
, e
2
, ..., e
m
}, where e
i
is the word embedding
of w
i
. The training objective is to maximize the
1
Google-n-Gram (http://books.google.com/ngrams) is
used as the background corpus.
2
The df(t) part of the original DRM is slightly modified
because we want a tf ? idf -like scheme (Liu et al, 2012).
338
average log probability of using word w
t
to pre-
dict a surrounding word w
t+j
,
?
EB = argmax
e
t
?EB
1
m
m
?
t=1
?
?c?j?c,j 6=0
log p(w
t+j
|w
t
; e
t
)
(3)
where c is the size of the training window. Basi-
cally, p(w
t+j
|w
t
; e
t
) is defined as,
p(w
t+j
|w
t
; e
t
) =
exp(e
?
T
t+j
e
t
)
?
m
w=1
exp(e
?
T
w
e
t
)
(4)
where e
?
i
is an additional training vector associ-
ated with e
i
. This basic formulation is impracti-
cal because it is proportional to m. A hierarchical
softmax approximation can be applied to reduce
the computational cost to log
2
(m), see (Morin and
Bengio, 2005) for details.
To alleviate the data sparsity problem, EB is
first trained on a very large corpus
3
(denoted by
C), and then fine-tuned on the target review cor-
pusR. Particularly, for phrasal product features, a
statistic-based method in (Zhu et al, 2009) is used
to detect noun phrases in R. Then, an Unfold-
ing Recursive Autoencoder (Socher et al, 2011) is
trained on C to obtain embedding vectors for noun
phrases. In this way, semantics of infrequent terms
in R can be well captured. Finally, the phrase-
based Skip-gram model in (Mikolov et al, 2013)
is applied onR.
3.2.2 Building the Semantic Similarity Graph
Lexical semantic clue is captured by measuring se-
mantic similarity between terms. The underlying
motivation is that if we have known some product
feature seeds, then terms that are more semanti-
cally similar to these seeds are more likely to be
product features. For example, if screen is known
to be a product feature of mp3, and lcd is of high
semantic similarity with screen, we can infer that
lcd is also a product feature. Analogously, terms
that are semantically similar to negative labeled
seeds are not product features.
Word embedding naturally meets the demand
above: words that are more semantically similar
to each other are located closer in the embedding
space (Collobert et al, 2011). Therefore, we can
use cosine distance between two embedding vec-
tors as the semantic distance measuring metric.
Thus, our method does not rely on term frequency
3
Wikipedia(http://www.wikipedia.org) is used in practice.
to rank candidates. This could potentially improve
the ability of mining infrequent product features.
Formally, we create a semantic similarity graph
G = (V,E,W ), where V = {V
s
? V
c
} is the
vertex set, which contains the labeled seed set V
s
and the unlabeled candidate set V
c
; E is the edge
set which connects every vertex pair (u, v), where
u, v ? V ; W = {w
uv
: cos(EB
u
, EB
v
)} is a
function which associates a weight to each edge.
3.3 Encoding Contextual Semantic Clue
Using Convolutional Neural Network
The CNN is trained on each occurrence of seeds
that is found in review texts. Then for a candidate
term t, the CNN classifies all of its occurrences.
Since seed terms tend to have high frequency in
review texts, only a few seeds will be enough to
provide plenty of occurrences for the training.
3.3.1 The architecture of the Convolutional
Neural Network
The architecture of the Convolutional Neural Net-
work is shown in Figure 2. For a product feature
candidate t in sentence s, every consecutive sub-
sequence q
i
of s that containing t with a window
of length l is fed to the CNN. For example, as
in Figure 2, if t = {screen}, and l = 3, there
are three inputs: q
1
= [the, ipod, screen], q
2
=
[ipod, screen, is], q
3
= [screen, is, impressive].
Partially, t is replaced by a token ?*PF*? to re-
move its lexicon influence
4
.
 
Figure 2: The architecture of the Convolutional
Neural Network.
To get the output score, q
i
is first converted into
a concatenated vector x
i
= [e
1
; e
2
; ...; e
l
], where
e
j
is the word embedding of the j-th word. In
this way, the CNN serves as a soft pattern miner:
4
Otherwise, the CNN will quickly get overfitting on t, be-
cause very few seed lexicons are used for the training.
339
since words that have similar semantics have sim-
ilar low-dimension embedding vectors, the CNN
is less sensitive to lexicon change. The network is
computed by,
y
(1)
i
= tanh(W
(1)
x
i
+ b
(1)
) (5)
y
(2)
= max(y
(1)
i
) (6)
y
(3)
= W
(3)
y
(2)
+ b
(3)
(7)
where y
(i)
is the output score of the i-th layer, and
b
(i)
is the bias of the i-th layer; W
(1)
? R
h?(nl)
and W
(3)
? R
2?h
are parameter matrixes, where
n is the dimension of word embedding, and h is
the size of nodes in the hidden layer.
In conventional neural models, the candidate
term t is placed in the center of the window. How-
ever, from Example 2, when l = 5, we can see that
the best windows should be the bracketed texts
(Because, intuitively, the windows should contain
mp3, which is a strong evidence for finding the
product feature), where t = {screen} is at the
boundary. Therefore, we use Equ. 6 to formulate
a max-convolutional layer, which is aimed to en-
able the CNN to find more evidences in contexts
than conventional neural models.
Example 2:
(a) The [screen of this mp3 is] great.
(b) This [mp3 has a great screen].
3.3.2 Training
Let ? = {EB,W
(?)
, b
(?)
} denotes all the trainable
parameters. The softmax function is used to con-
vert the output score of the CNN to a probability,
p(t|X; ?) =
exp(y
(3)
)
?
|C|
j=1
exp(y
(3)
j
)
(8)
whereX is the input set for term t, andC = {0, 1}
is the label set representing product feature and
non-product feature, respectively.
To train the CNN, we first use V
s
to collect each
occurrence of the seeds in R to form a training
set T
s
. Then, the training criterion is to minimize
cross-entropy over T
s
,
?
? = argmin
?
|T
s
|
?
i=1
? log ?
i
p(t
i
|X
i
; ?) (9)
where ?
i
is the binomial target label distribution
for one entry. Backpropagation algorithm with
mini-batch stochastic gradient descent is used to
solve this optimization problem. In addition, some
useful tricks can be applied during the training.
The weight matrixes W
(?)
are initialized by nor-
malized initialization (Glorot and Bengio, 2010).
W
(1)
is pre-trained by an autoencoder (Hinton,
1989) to capture semantic compositionality. To
speed up the learning, a momentum method is ap-
plied (Sutskever et al, 2013).
3.4 Combining Lexical and Contextual
Semantic Clues by Label Propagation
We propose a Label Propagation algorithm to
combine both semantic clues in a unified process.
Each term t ? V is assumed to have a label dis-
tribution L
t
= (p
+
t
, p
?
t
), where p
+
t
denotes the
probability of the candidate being a product fea-
ture, and on the contrary, p
?
t
= 1? p
+
t
. The clas-
sified results of the CNN which encode contextual
semantic clue serve as the prior knowledge,
I
t
=
?
?
?
(1, 0), if t ? V
+
s
(0, 1), if t ? V
?
s
(r
+
t
, r
?
t
), if t ? V
c
(10)
where (r
+
t
, r
?
t
) is estimated by,
r
+
t
=
count
+
(t)
count
+
(t) + count
?
(t)
(11)
where count
+
(t) is the number of occurrences of
term t that are classified as positive by the CNN,
and count
?
(t) represents the negative count.
Label Propagation is applied to propagate the
prior knowledge distribution I to the product fea-
ture distribution L via semantic similarity graph
G, so that a product feature candidate is deter-
mined by exploring its semantic relations to all of
the seeds and other candidates globally. We pro-
pose an adapted version on the random walking
view of the Adsorption algorithm (Baluja et al,
2008) by updating the following formula until L
converges,
L
i+1
= (1? ?)M
T
L
i
+ ?DI (12)
where M is the semantic transition matrix built
from G; D = Diag[log tf(t)] is a diagonal ma-
trix of log frequencies, which is designed to as-
sign higher ?confidence? scores to more frequent
seeds; and ? is a balancing parameter. Particu-
larly, when ? = 0, we can set the prior knowledge
I without V
c
to L
0
so that only lexical semantic
clue is used; otherwise if ? = 1, only contextual
semantic clue is used.
340
3.5 The Bootstrapping Framework
We summarize the bootstrapping framework of the
proposed method in Algorithm 1. During boot-
strapping, the CNN is enhanced by Label Propaga-
tion which finds more labeled examples for train-
ing, and then the performance of Label Propaga-
tion is also improved because the CNN outputs a
more accurate prior distribution. After running for
several iterations, the algorithm gets enough seeds,
and a final Label Propagation is conducted to pro-
duce the results.
Algorithm 1: Bootstrapping using semantic clues
Input: The review corpusR, a large corpus C
Output: The mined product feature list P
Initialization: Train word embedding set EB first on
C, and then onR
Step 1: Generate product feature seeds V
s
(Section 3.1)
Step 2: Build semantic similarity graph G (Section 3.2)
while iter < MAX ITER do
Step 3: Use V
s
to collect occurrence set T
s
fromR
for training
Step 4: Train a CNNN on T
s
(Section 3.3)
Apply mini-batch SGD on Equ. 9;
Step 5: Run Label Propagation (Section 3.4)
Classify candidates usingN to setup I;
L
0
? I;
repeat
L
i+1
? (1? ?)M
T
L
i
+ ?DI;
until ||L
i+1
? L
i
||
2
< ?;
Step 6: Expand product feature seeds
Move top T terms from V
c
to V
s
;
iter++
end
Step 7: Run Label Propagation for a final result L
f
Rank terms by L
+
f
to get P , where L
+
f
> L
?
f
;
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets: We select two real world datasets to
evaluate the proposed method. The first one
is a benchmark dataset in Wang et al (2011),
which contains English review sets on two do-
mains (MP3 and Hotel)
5
. The second dataset is
proposed by Chinese Opinion Analysis Evalua-
tion 2008 (COAE 2008)
6
, where two review sets
(Camera and Car) are selected. Xu et al (2013)
had manually annotated product features on these
four domains, so we directly employ their annota-
tion as the gold standard. The detailed information
can be found in their original paper.
5
http://timan.cs.uiuc.edu/downloads.html
6
http://ir-china.org.cn/coae2008.html
Evaluation Metrics: We evaluate the proposed
method in terms of precision(P), recall(R) and F-
measure(F). The English results are evaluated by
exact string match. And for Chinese results, we
use an overlap matching metric, because deter-
mining the exact boundaries is hard even for hu-
man (Wiebe et al, 2005).
4.2 Experimental Settings
For English corpora, the pre-processing are the
same as that in (Qiu et al, 2009), and for Chinese
corpora, the Stanford Word Segmenter (Chang
et al, 2008) is used to perform word segmenta-
tion. We select three state-of-the-art syntax-based
methods to be compared with our method:
DP uses a bootstrapping algorithm named as
Double Propagation (Qiu et al, 2009), which is
a conventional syntax-based method.
DP-HITS is an enhanced version of DP pro-
posed by Zhang et al (2010), which ranks product
feature candidates by
s(t) = log tf(t) ? importance(t) (13)
where importance(t) is estimated by the HITS al-
gorithm (Kleinberg, 1999).
SGW is the Sentiment Graph Walking algo-
rithm proposed in (Xu et al, 2013), which first
extracts syntactic patterns and then uses random
walking to rank candidates. Afterwards, word-
syntactic pattern co-occurrence statistic is used
as feature for a semi-supervised classifier TSVM
(Joachims, 1999) to further refine the results. This
two-stage method is denoted as SGW-TSVM.
LEX only uses lexical semantic clue. Label
Propagation is applied alone in a self-training
manner. The dimension of word embedding n =
100, the convergence threshold ? = 10
?7
, and the
number of expanded seeds T = 40. The size of
the seed set N is 40. To output product features,
it ranks candidates in descent order by using the
positive score L
+
f
(t).
CONT only uses contextual semantic clue,
which only contains the CNN. The window size
l is 5. The CNN is trained with a mini-batch size
of 50. The hidden layer size h = 250. Finally,
importance(t) in Equ. 13 is replaced with r
+
t
in
Equ. 11 to rank candidates.
LEX&CONT leverages both semantic clues.
341
Method
MP3 Hotel Camera Car Avg.
P R F P R F P R F P R F F
DP 0.66 0.57 0.61 0.66 0.60 0.63 0.71 0.70 0.70 0.72 0.65 0.68 0.66
DP-HITS 0.65 0.62 0.63 0.64 0.66 0.65 0.71 0.78 0.74 0.69 0.68 0.68 0.68
SGW 0.62 0.68 0.65 0.63 0.71 0.67 0.69 0.80 0.74 0.66 0.71 0.68 0.69
LEX 0.64 0.74 0.69 0.65 0.75 0.70 0.69 0.84 0.76 0.68 0.78 0.73 0.72
CONT 0.68 0.65 0.66 0.69 0.68 0.68 0.74 0.77 0.75 0.74 0.70 0.72 0.71
SGW-TSVM 0.73 0.71 0.72 0.75 0.73 0.74 0.78 0.81 0.79 0.76 0.73 0.74 0.75
LEX&CONT 0.74 0.75 0.74 0.75 0.77 0.76 0.80 0.84 0.82 0.79 0.79 0.79 0.78
Table 1: Experimental results of product feature mining. The precision or recall of CONT is the average
performance over five runs with different random initialization of parameters of the CNN. Avg. stands
for the average score.
4.3 The Semantics-based Methods vs.
State-of-the-art Syntax-based Methods
The experimental results are shown in Table 1,
from which we have the following observations:
(i) Our method achieves the best performance
among all of the compared methods. We
also equally split the dataset into five sub-
sets, and perform one-tailed t-test (p ? 0.05),
which shows that the proposed semantics-
based method (LEX&CONT) significantly out-
performs the three syntax-based strong com-
petitors (DP, DP-HITS and SGW-TSVM).
(ii) LEX&CONT which leverages both lexical and
contextual semantic clues outperforms ap-
proaches that only use one kind of semantic
clue (LEX and CONT), showing that the com-
bination of the semantic clues is helpful.
(iii) Our methods which use only one kind of
semantic clue (LEX and CONT) outperform
syntax-based methods (DP, DP-HITS and
SGW). Comparing DP-HITS with LEX and
CONT, the difference between them is that
DP-HITS uses a syntax-pattern-based algo-
rithm to estimate importance(t) in Equ. 13,
while our methods use lexical or contextual se-
mantic clue instead. We believe the reason that
LEX or CONT is better is that syntactic pat-
terns only use discrete and local information.
In contrast, CONT exploits latent semantics of
each word in context, and LEX takes advantage
of word embedding, which is induced from
global word co-occurrence statistic. Further-
more, comparing SGW and LEX, both methods
are base on random surfer model, but LEX gets
better results than SGW. Therefore, the word-
word semantic similarity relation used in LEX
is more reliable than the word-syntactic pattern
relation used in SGW.
(iv) LEX&CONT achieves the highest recall
among all of the evaluated methods. Since
DP and DP-HITS rely on frequency for rank-
ing product features, infrequent candidates are
ranked low in their extracted list. As for SGW-
TSVM, the features they used for the TSVM
suffer from the data sparsity problem for in-
frequent terms. In contrast, LEX&CONT is
frequency-independent to the review corpus.
Further discussions on this observation are
given in the next section.
4.4 The Results on Extracting Infrequent
Product Features
We conservatively regard 30% product features
with the highest frequencies in R as frequent fea-
tures, so the remaining terms in the gold standard
are infrequent features. In product feature mining
task, frequent features are relatively easy to find.
Table 2 shows the recall of all the four approaches
for mining frequent product features. We can see
that the performance are very close among differ-
ent methods. Therefore, the recall mainly depends
on mining the infrequent features.
Method MP3 Hotel Camera Car
DP 0.89 0.92 0.86 0.84
DP-HITS 0.89 0.91 0.86 0.85
SGW-TSVM 0.87 0.92 0.88 0.87
LEX&CONT 0.89 0.91 0.89 0.87
Table 2: The recall of frequent product features.
Figure 3 gives the recall of infrequent prod-
uct features, where LEX&CONT achieves the best
performance. So our method is less influenced
by term frequency. Furthermore, LEX gets better
recall than CONT and all syntax-based methods,
which indicates that lexical semantic clue does aid
to mine more infrequent features as expected.
342
 1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(a) MP3
 
1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(b) Hotel
 
1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(c) Camera
 
1 2 3 4 5 6 7 8 9
.5
.6
.7
.8
.9
1.0
LEX&CONT
CONT
LEX
(d) Car
Figure 4: Accuracy (y-axis) of product feature seed expansion at each bootstrapping iteration (x-axis).
The error bar shows the standard deviation over five runs.
Method
MP3 Hotel Camera Car
P R F P R F P R F P R F
FW-5 0.62 0.63 0.62 0.64 0.64 0.64 0.68 0.73 0.70 0.67 0.66 0.66
FW-9 0.64 0.65 0.64 0.66 0.68 0.67 0.70 0.76 0.73 0.71 0.70 0.70
CONT 0.68 0.65 0.66 0.69 0.68 0.68 0.74 0.77 0.75 0.74 0.70 0.72
Table 3: The results of convolutional method vs. the results of non-convolutional methods.
MP3 Hotel Camera Car
Reca
ll
.4
.5
.6
.7
.8
.9 DPDP-HITSSGW-TSVMCONTLEXLEX&CONT
Figure 3: The recall of infrequent features. The
error bar shows the standard deviation over five
different runs.
4.5 Lexical Semantic Clue vs. Contextual
Semantic Clue
This section studies the effects of lexical seman-
tic clue and contextual semantic clue during seed
expansion (Step 6 in Algorithm 1), which is con-
trolled by ?. When ? = 1, we get the CONT; and
if ? is set 0, we get the LEX. To take into account
the correctly expanded terms for both positive and
negative seeds, we use Accuracy as the evaluation
metric,
Accuracy =
#TP + #TN
# Extracted Seeds
where TP denotes the true positive seeds, and TN
denotes the true negative seeds.
Figure 4 shows the performance of seed ex-
pansion during bootstrapping, in which the accu-
racy is computed on 40 seeds (20 being positive
and 20 being negative) expanded in each itera-
tion. We can see that the accuracies of CONT and
LEX&CONT retain at a high level, which shows
that they can find reliable new product feature
seeds. However, the performance of LEX oscil-
lates sharply and it is very low for some points,
which indicates that using lexical semantic clue
alone is infeasible. On another hand, comparing
CONT with LEX in Table 1, we can see that LEX
performs generally better than CONT. Although
LEX is not so accurate as CONT during seed ex-
pansion, its final performance surpasses CONT.
Consequently, we can draw conclusion that CONT
is more suitable for the seed expansion, and LEX
is more robust for the final result production.
To combine advantages of the two kinds of se-
mantic clues, we set ? = 0.7 in Step 5 of Algo-
rithm 1, so that contextual semantic clue plays a
key role to find new seeds accurately. For Step 7,
we set ? = 0.3. Thus, lexical semantic clue is
emphasized for producing the final results.
4.6 The Effect of Convolutional Layer
Two non-convolutional variations of the proposed
method are used to be compared with the convo-
lutional method in CONT. FW-5 uses a traditional
neural network with a fixed window size of 5 to
replace the CNN in CONT, and the candidate term
to be classified is placed in the center of the win-
dow. Similarly, FW-9 uses a fixed window size
of 9. Note that CONT uses a 5-term dynamic
window containing the candidate term, so the ex-
ploited number of words in the context is equiva-
lent to FW-9.
343
Table 3 shows the experimental results. We can
see that the performance of FW-5 is much worse
than CONT. The reason is that FW-5 only exploits
half of the context as that of CONT, which is not
sufficient enough. Meanwhile, although FW-9 ex-
ploits equivalent range of context as that of CONT,
it gets lower precisions. It is because FW-9 has
approximately two times parameters in the param-
eter matrix W
(1)
than that in Equ. 5 of CONT,
which makes it more difficult to be trained with
the same amount of data. Also, lengths of many
sentences in the review corpora are shorter than 9.
Therefore, the convolutional approach in CONT is
the most effective way among these settings.
4.7 Parameter Study
We investigate two key parameters of the proposed
method: the initial number of seeds N , and the
size of the window l used by the CNN.
Figure 5 shows the performance under differ-
ent N , where the F-Measure saturates when N
equates to 40 and beyond. Hence, very few seeds
are needed for starting our algorithm.
 
N
10 20 30 40 50 60
F-Measure
.65
.70
.75
.80
.85
MP3
Hote l
Came ra
Car
Figure 5: F-Measure vs. N for the final results.
Figure 6 shows F-Measure under different win-
dow size l. We can see that the performance is
improved little when l is larger than 5. Therefore,
l = 5 is a proper window size for these datasets.
 
l
2 3 4 5 6 7
F-Measure
.5
.6
.7
.8
.9
MP3
Hote l
Came ra
Car
Figure 6: F-Measure vs. l for the final results.
5 Conclusion and Future Work
This paper proposes a product feature mining
method by leveraging contextual and lexical se-
mantic clues. A semantic similarity graph is built
to capture lexical semantic clue, and a convo-
lutional neural network is used to encode con-
textual semantic clue. Then, a Label Propaga-
tion algorithm is applied to combine both seman-
tic clues. Experimental results prove the effec-
tiveness of the proposed method, which not only
mines product features more accurately than con-
ventional syntax-based method, but also extracts
more infrequent product features.
In future work, we plan to extend the proposed
method to jointly mine product features along with
customers? opinions on them. The learnt seman-
tic representations of words may also be utilized
to predict fine-grained sentiment distributions over
product features.
Acknowledgement
This work was sponsored by the National
Basic Research Program of China (No.
2012CB316300), the National Natural Sci-
ence Foundation of China (No. 61272332 and
No. 61202329), the National High Technol-
ogy Development 863 Program of China (No.
2012AA011102), and CCF-Tencent Open Re-
search Fund. This work was also supported in
part by Noahs Ark Lab of Huawei Tech. Ltm.
References
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi
Jing, Jay Yagnik, Shankar Kumar, Deepak
Ravichandran, and Mohamed Aly. 2008. Video
suggestion and discovery for youtube: Taking ran-
dom walks through the view graph. In Proceedings
of the 17th International Conference on World Wide
Web, WWW ?08, pages 895?904, New York, NY,
USA. ACM.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, StatMT ?08, pages 224?232.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493?2537,
November.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
344
dependency parses from phrase structure parses. In
Proceedings of the IEEE / ACL?06 Workshop on
Spoken Language Technology.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Comput. Linguist.,
19(1):61?74, March.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Proceedings of the International Con-
ference on Artificial Intelligence and Statistics.
Geoffrey E. Hinton. 1989. Connectionist learning pro-
cedures. Artificial Intelligence, 40(1C3):185 ? 234.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Xing Jiang and Ah-Hwee Tan. 2010. Crctol: A
semantic-based domain ontology learning system.
Journal of the American Society for Information Sci-
ence and Technology, 61(1):150?168.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 151?160, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the 16th International Conference on
Machine Learning, pages 200?209.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and
Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 410?419, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1346?1356, Jeju Island, Korea,
July. Association for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Samaneh Moghaddam and Martin Ester. 2010. Opin-
ion digger: An unsupervised opinion miner from
unstructured product reviews. In Proceedings of
the 19th ACM International Conference on Informa-
tion and Knowledge Management, CIKM ?10, pages
1825?1828, New York, NY, USA. ACM.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on arti-
ficial intelligence and statistics, AISTATS05, pages
246?252.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1199?1204.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In NIPS?2011, vol-
ume 24, pages 801?809.
Ilya Sutskever, James Martens, George Dahl, and Ge-
offrey Hinton. 2013. Distributed representations of
words and phrases and their compositionality. In
Proceedings of the 30 th International Conference
on Machine Learning.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 384?394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ?11, pages 618?
626, New York, NY, USA. ACM.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing:
Volume 3 - Volume 3, EMNLP ?09, pages 1533?
1541, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
345
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun
Zhao. 2013. Mining opinion words and opinion tar-
gets in a two-stage framework. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1764?1773, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics: Posters, COLING ?10, pages
1462?1470, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou, and
Muhua Zhu. 2009. Multi-aspect opinion polling
from textual reviews. In Proceedings of the 18th
ACM Conference on Information and Knowledge
Management, CIKM ?09, pages 1799?1802, New
York, NY, USA. ACM.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM International Conference
on Information and Knowledge Management, CIKM
?06, pages 43?50, New York, NY, USA. ACM.
346
