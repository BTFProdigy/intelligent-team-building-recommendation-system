Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 440?450,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Non-isomorphic Forest Pair Translation 
 
 
Hui Zhang1, 2, 3   Min Zhang1   Haizhou Li1   Eng Siong Chng2 
 
1Institute for Infocomm Research   
2Nanyang Technological University  
3USC Information Science Institute  
huizhang.fuan@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   aseschng@ntu.edu.sg 
 
 
 
 
 
 
 
 
 
 
Abstract 
This paper studies two issues, non-isomorphic 
structure translation and target syntactic structure 
usage, for statistical machine translation in the 
context of forest-based tree to tree sequence trans-
lation. For the first issue, we propose a novel 
non-isomorphic translation framework to capture 
more non-isomorphic structure mappings than tra-
ditional tree-based and tree-sequence-based trans-
lation methods. For the second issue, we propose a 
parallel space searching method to generate hypo-
thesis using tree-to-string model and evaluate its 
syntactic goodness using tree-to-tree/tree sequence 
model. This not only reduces the search complexity 
by merging spurious-ambiguity translation paths 
and solves the data sparseness issue in training, but 
also serves as a syntax-based target language mod-
el for better grammatical generation. Experiment 
results on the benchmark data show our proposed 
two solutions are very effective, achieving signifi-
cant performance improvement over baselines 
when applying to different translation models. 
1 Introduction 
Recently syntax-based methods have achieved very 
promising results and attracted increasing interests in 
statistical machine translation (SMT) research com-
munity due to their ability to provide informative 
context structure information and convenience in 
carrying out word transformation and sub-span reor-
dering. Fundamentally, syntax-based SMT views 
translation as a structural transformation process. 
Generally speaking, from modeling viewpoint, a 
syntax-based model tries to convert the source struc-
tures into target structures iteratively and recursively 
while from decoding viewpoint a syntax-based sys-
tem segments an input tree/forest into many 
sub-fragments, translates each of them separately, 
combines the translated sub-fragments and then finds 
out the best combinations. Therefore, from bilingual 
viewpoint, we face two fundamental problems: the 
mapping between bilingual structures and the way of 
carrying out the target structures combination.  
For the first issue, a number of models have been 
proposed to model the structure mapping between 
tree and string (Galley et al, 2004; Liu et al, 2006; 
Yamada and Knight, 2001; DeNeefe and Knight, 
2009) and between tree and tree (Eisner, 2003; 
Zhang et al, 2007 & 2008; Liu et al, 2009). How-
ever, one of the major challenges is that all the cur-
rent models only allow one-to-one mapping from one 
source frontier non-terminal node (Galley et al, 2004) 
to one target frontier non-terminal node in a bilingual 
translation rule. Therefore, all those translation equi-
valents with one-to-many frontier non-terminal node 
mapping cannot be covered by the current 
state-of-the-art models. This may largely compro-
mise the modeling ability of translation rules. 
For the second problem, currently, the combina-
tion is driven by only the source side (both 
tree-to-string model and tree-to-tree model only 
check the source span compatibility when combining 
different target structures in decoding) or only the 
440
target side (string to tree model). There is no well 
study in considering both the source side information 
and the compatibility between different target syn-
tactic structures during combination. In addition, it is 
well known that the traditional tree-to-tree models 
suffer heavily from the data sparseness issue in 
training and the spurious-ambiguity translation path 
issue (the same translation with different syntactic 
structures) in decoding. 
In addition, because of the performance limitation 
of automatic syntactic parser, researchers propose 
using packed forest (Tomita, 1987; Klein and Man-
ning, 2001; Huang, 2008)1 instead of 1-best parse 
tree to carry out training (Mi and Huang, 2008) and 
decoding (Mi et al, 2008) in order to reduce the side 
effect caused by parsing errors of the one-best tree. 
However, when we apply the tree-to-tree model to 
the bilingual forest structures, both training and de-
coding become very complicated. 
In this paper, to address the first issue, we propose 
a framework to model the non-isomorphic translation 
process from source tree fragment to target tree se-
quence, allowing any one source frontier 
non-terminal node to be translated into any number 
of target frontier non-terminal nodes. For the second 
issue, we propose a technology to model the combi-
nation task by considering both sides? syntactic 
structure information. We evaluate and integrate the 
two technologies into forest-based tree to tree se-
quence translation. Experimental results on the 
NIST-2003 and NIST-2005 Chinese-English transla-
tion tasks show that our methods significantly out-
perform the forest-based tree to string and previous 
tree to tree models as well as the phrase-based model.  
The remaining of the paper is organized as fol-
lowing. Section 2 reviews the related work. In sec-
tion 3 and section 4, we discuss the proposed for-
est-based rule extraction (non-isomorphic mapping) 
and decoding algorithms (target syntax information 
usage). Finally we report the experimental results in 
section 5 and conclude the paper in section 6. 
2 Related Work 
Much effort has been done in the syntax-based trans-
lation modeling. Yamada and Knight (2001) propose 
                                                          
1 A packed forest is a compact representation of a set of trees 
with sharing substructures; formally, it is defined as a triple a 
triple? ?, ?, ? ?, where ? is non-terminal node set, ? is hy-
per-edge set and ? is leaf node set (i.e. all sentence words). 
Every node in ? covers a consecutive sequence of leaf, every 
hyper-edge in ? connect the father node to its children nodes as 
in a tree. Figure 8 is a packed forest contains two trees. 
a string to tree model. Galley et al (2004) propose 
the GHKM scheme to model the string-to-tree map-
ping. Liu et al (2006) propose a tree-to-string trans-
lation model. Liu et al (2007) propose the tree se-
quence to string model to capture rules covered by 
continuous sequence of trees. Shieber (2007), De-
Neefe and Knight (2009) and Carreras and Collins 
(2009) propose synchronous tree adjoin grammar to 
capture more tree-string mapping beyond the GHKM 
scheme. Zhang et al (2009a) propose the concept of 
virtual node to reform a tree sequence as a tree, and 
design efficient algorithms for tree sequence model 
in forest context. All these works only consider either 
the source side or the target side syntax information. 
To capture both side syntax contexts, Eisner (2003) 
studies the bilingual dependency tree-to-tree map-
ping in conceptual level. Zhang et al (2008) propose 
tree sequence-based tree-to-tree modeling. Liu et al 
(2009) propose efficient algorithms for tree-to-tree 
model in the forest-based training and decoding 
scheme. One common limitation of the above works 
is they only allow the one-to-one mapping between 
each non-terminal frontier node, and thus they suffer 
from the issue of rule coverage. On the other hand, 
due to the data sparseness issue and model coverage 
issue, previous tree-to-tree (Zhang et al, 2008; Liu et 
al., 2009) decoder has to rely solely on the span in-
formation or source side information to combine the 
target syntactic structures, without checking the 
compatibility of the merging nodes, in order not to 
fail many translation paths. Thus, this solution fails 
to effectively utilize the target structure information. 
To address this issue, tree sequence (Liu et al, 
2007; Zhang et al, 2008) and virtual node (Zhang et 
al., 2009a) are two concepts with promising results 
reported. In this paper, with the help of these two 
concepts, we propose a novel framework to solve the 
one-to-many non-isomorphic mapping issue. In addi-
tion, our proposed solution of using target syntax 
information enables our forest-based tree-to-tree se-
quence translation decoding algorithm to not only 
capture bilingual forest information but also have 
almost the same complexity as forest-based 
tree-to-string translation. This reduces the time/space 
complexity exponentially. 
3 Tree to Tree Sequence Rules 
The motivation of introducing tree to tree sequence 
rules is to add target syntax information to 
tree-to-string rules. Following, we first briefly review 
the definition of tree-to-string rules, and then de-
scribe the tree-to-tree sequence rules. 
441
3.1 Tree to String Rules 
VP
ADVP
AD
VP
VV
??
(try hard to)
??
(study)
try to studyhard  
    
Fig. 1. A word-aligned sentence pair with source tree 
 
 
   
   Fig. 2 Examples of tree to string rules 
 
Fig. 2 illustrates the examples of tree to string rules 
extracted from Fig. 1. The tree-to-string rule is very 
simple. Its source side is a sub-tree of source parse 
tree and its target side is a string with only one varia-
ble/non-terminal X. The source side and the target 
side is translation of each other with the constraint of 
word alignments. Please note that there is no any 
target syntactic or linguistic information used in the 
tree-to-string model. 
3.2 Tree to Tree Sequence Rules 
It is more challenging when extracting rules with 
target tree structure as constraint. Fig. 3 extends Fig. 
1 with target tree structure. The problem is that, giv-
en a source tree node, we are able to find its target 
string translation, but these target string may not 
form a linguistic sub-tree. For example, in Fig. 3, the 
source tree node ?ADVP? in solid eclipse is trans-
lated to ?try hard to? in the target sentence, but there 
is no corresponding sub-tree covering and only cov-
ering it in the target side.  
Given the example rules in Fig. 2, what are their 
corresponding rules with target syntax information? 
The answer is that the previous tree or tree se-
quence-based models fail to model the Rule 1 and 
Rule 2 at Fig. 2, since at frontier node level they only 
allow one-to-one node mapping but the solution is 
one-to-many non-terminal frontier node mapping. 
The concept of ?virtual node? (Zhang et al 2009a) is 
a solution to this issue. To facilitate discussion, we 
first introduce three concepts. 
 
 
 
Fig. 3. A word-aligned bi-parsed tree 
 
 
Fig. 4. A restructured tree with a virtual span root 
 
? Def. 1. The ?node sequence? is a sequence of 
nodes (either leaf or internal nodes) covering a 
consecutive span. For example, in Fig 3, ?VBP 
RB TO? and ?VBP ADVP TO? are two ?node 
sequence? covering the same span ?try hard to?. 
 
442
? Def. 2. The ?root node sequence? of a span is 
such a node sequence that any node in this se-
quence could not be a child of a node in other 
node sequence of the span. Intuitively, the ?root 
node sequence? of a span is the node sequence 
with the highest topology level. For example, 
?VBP ADVP TO? is the ?root node sequence? 
of the span of ?try hard to?. It is easy to prove 
that given any span, there exist one and only one 
?root node sequence?. 
 
? Def. 3. The ?span root? of a span is such a node 
that if the ?root node sequence? contains only 
one tree node, then the ?span root? is this tree 
node; otherwise, the ?span root? is the virtual 
father node (Zhang et al, 2009a) of the ?root 
node sequence?. Fig. 4 illustrates the reformed 
Fig. 3 by introducing the virtual node 
?VBP+ADVP+TO? as the ?span root? of the 
span of ?try hard to?. 
 
 
The ?span root? facilitates us to extract rules with 
target side structure information. Given a sub-tree of 
the source tree, we have a set of non-terminal frontier 
nodes. For each such frontier node, we can find its 
corresponding target ?span root?. If the ?span root? 
is a virtual node, then we add it into the target tree as 
a virtual segmentation joint point. After adding the 
?span root? as joint point, we are able to ensure that 
each frontier source node has only one corresponding 
target node, then we can use any traditional rule ex-
traction algorithm to extract rules, including those 
rules with one-to-many non-terminal frontier map-
pings. 
 
 
Fig. 5. Tree-to-tree sequence rules 
Fig. 5 lists the corresponding rules with target 
structure information of the tree-to-string rules in Fig 
2. All the three rules cannot be extracted by previous 
tree-to-tree mapping methods (Liu et al, 2009). The 
previous tree-sequence-based methods (Zhang et al, 
2008; Zhang et al, 2009a) can extracted rule 3 since 
they allow one-to-many mapping in root node level. 
But they cannot extract rule 1 and rule 2. Therefore, 
for any tree-to-string rule, our method can always 
find the corresponding tree-to-tree sequence rule. As 
a result, our rule coverage is the same as 
tree-to-string framework while our rules contain 
more informative target syntax information. Later we 
will show that using our decoding algorithm the 
tree-to-tree sequence search space is exponentially 
reduced to the same as tree-to-string search space. 
That is to say, we do not need to worry about the ex-
ponential search space issue of tree-to-tree sequence 
model existing in previous work. 
3.3 Rule Extraction in Tree Context 
Given a word aligned tree pair, we first extract the 
set of minimum tree to string rules (Galley et al 
2004), then for each tree-to-string rule, we can easily 
extract its corresponding tree-to-tree sequence rule 
by introducing the virtual span root node. After that, 
we generate the composite rules by iteratively com-
bining small rules.  
 
    
 
Fig. 6. Rule combination and virtual node removing 
443
  Please note that in generating composite rules, if 
the joint node is a virtual node, we have to recover 
the original link and remove this virtual node to 
avoid unnecessary ambiguity. Fig. 6 illustrates the 
combination process of rule 2 and rule 3 in Fig. 5. As 
a result, all of our extract rules do not contain any 
internal virtual nodes. 
3.4 Rule Extraction in Forest Context 
In forest pair context, we also first generate the 
minimum tree-to-string rule set as Mi et al (2008), 
and for each tree-to-string rule, we find its corres-
ponding tree-to-tree sequence rules, and then do rule 
composition. 
In tree pair context, given a tree-to-string rule, 
there is one and only one corresponding tree-to-tree 
sequence rule. But in forest pair context, given one 
such tree-to-string rule, there are many correspond-
ing tree-to-tree sequence rules. All these sub-trees 
form one or more sub-forests2 of the entire big target 
forest. If we can identify the sub-forests, i.e., all of 
the hyper-edges of the sub-forests, we can retrieve all 
the sub-trees from the sub-forests as the target sides 
of the corresponding tree-to-tree sequence rules. 
Given a source sub-tree, we can obtain the target 
root span where the target sub-forests start and the 
frontier spans where the target sub-forests stop. To 
indentify all the hyper-edges in the sub-forests, we 
start from every node covering the root span, traverse 
from top to down, mark all the hyper-edges visited 
and stop at the node if its span is a sub-span of one of 
the forest frontier spans or if it is a word node. The 
reason we stop at the node once it fell into a frontier 
span (i.e. the span of the node is a sub-span of the 
frontier span) is to guarantee that given any frontier 
span, we could stop at the ?root node sequence? of 
this span by Def. 2. 
For example, Fig. 7 is a source sub-tree of rule 2 
in Fig. 5 and the circled part in Fig. 8 is one of its 
corresponding target sub-forests. Its corresponding 
target root span is [1,4] (corresponding to source root 
?VP? ) and its corresponding target frontier span is 
{[1,3], study[4,4]}. Now given the target forest, we 
start from node VP[1,4] and traverse from top to 
down, finally stop at following nodes: VBP[1,1], 
ADVP[2,2], TO[3,3], study .  
                                                          
2 All the sub-forests cover the same span. But their roots have 
different grammar tags as the roots? names. The root may be a 
virtual span root node in the case of the one-to-many frontier 
non-terminal node mappings. 
Please note that the starting root node must be a 
single node, being either a normal forest node or a 
virtual ?span root? node. The virtual ?span root? 
node serves as the frontier node of upper rules and 
root node of the currently being extracted rules. Be-
cause we extract rules in a top-to-down manner, the 
necessary virtual ?span root? node for current 
sub-forest has already been added into the global 
forest when extracting upper level rules. 
 
 
 
Figure 7. A source sub-tree in rule 2 
 
 
 
Fig. 8. The corresponding target sub-forest for the tree of 
Figure 7. 
3.5 Fractional Count of Rule 
Following Mi and Huang (2008) and Liu et al 
(2009), we assign a fractional count to a rule to 
measure how likely it appears given the context of 
the forest pair. In following equation, ?S? means 
source sub-tree, ?T? means target sub-tree, ?SF? is 
source forest and ?TF? is the target forest. 
 
 
???, ? |??, ??? ? ???|??, ??? ? ???|??, ???
? ???|??? ? ???|??? 
 
444
The above equation means the fractional count of 
a source-target tree pair is just the product of each of 
their fractional count in corresponding forest context 
in following equation. 
 
????????| ???????? ? ??????????????????? ?????? 
? ?????? ???????? ? ? ????????????? ? ? ????????????????????????????? ??????  
 
 
where ? and ? are the outside and inside probabil-
ities. In addition, if a sub-tree root is a virtual node 
(formed by a root node sequence), then we use fol-
lowing equation to approximate the outside probabil-
ity of the virtual node. 
 
????? ???????? ???? ? ?? ????
? ? ??
# ?? ????? ?? ??
 
4 Decoding 
4.1 Traditional Forest-based Decoding 
A typical translation process of a forest-based system 
is to first convert the source packed forest into a tar-
get translation forest, and then apply search algo-
rithm to find the best translation result from this tar-
get translation forest (Mi et al, 2008).  
For the tree-to-string model, the forest conversion 
process is as following: given an input packed forest, 
we do pattern matching (Zhang et al, 2009b) with 
the source side structures in the rule set. For each 
matched rule, we establish its target side as a hy-
per-edge in the target forest.   
 
 
 
Fig. 9. A forest conversion step in a tree to string model 
 
Fig. 9 exemplifies a conversion step in the tree to 
string model. A sub-tree structure with two hy-
per-edge ?VP[2,4] => ADVP[2,2] VP[3,4]? and 
?VP[3,4] => ADVP[3,4] VP[4,4]? is converted into 
a target hyper-edge ?X-VP[2,4] => X-ADVP[3,3] 
X-ADVP[2,2]  X-VP[4,4] ?.  The node ?X-VP[4,4]? 
in the target forest means that its syntactic label in 
target forest is ?X? and it is translated from the 
source node ?VP[4,4]? in the source forest. In this 
target hyper-edge, ?X-ADVP[3,3] X-ADVP[2,2]? 
means the translation from source node ?ADVP[3,3]? 
is put before the translation from ?ADVP[2,2]?, 
representing a structure reordering. 
4.2 Toward Bilingual Syntax-aware Trans-
lation Generation 
As we could see in section 4.1, there is only one kind 
of non-terminal symbol ?X? in the target side. It is a 
big challenge to rely on such a coarse label to gener-
ate a translation with fine syntactic quality. For ex-
ample, a source node may be translated into a ?NP? 
(noun phrase) in target side. However, in this rule set 
with the only symbol ?X?, it may be merged with 
upper structure as a ?VP? (verb phrase) instead, be-
cause there is no way to favor one over another. In 
this case, the target tree does not well model the 
translation syntactically. In addition, all of the inter-
nal structure information in the target side is ignored 
by the tree-to-string rules. 
One natural solution to the above issue is to use 
the tree to tree/tree sequence model, which have 
richer target syntax structures for more discrimina-
tive probability and finer labels to guide the combi-
nation process. However, the tree to tree/tree se-
quence model may face very severe computational 
problem and so-called ?spurious ambiguities? issue.  
Theoretically, if in the tree-to-tree sequence mod-
el-based decoding, we just give a penalty to the in-
compatible-node combinations instead of pruning out 
the translation paths, then the set of sentences gener-
ated by the tree-to-tree sequence model is identical to 
that of the tree-to-string model since every 
tree-to-tree sequence rule can be projected into a 
tree-to-string rule. Motivated by this, we propose a 
solution call parallel hypothesis spaces searching to 
solve the computational and ?spurious ambiguities? 
issues mentioned above. In the meanwhile, we can 
fully utilize the target structure information to guide 
translation.  
We restructure the tree-to-tree sequence rule set by 
grouping all the rules according to their correspond-
ing tree-to-string rules. This behaves like a 
?tree-to-forest? rule. The ?forest? encodes all the tree 
sequences with same corresponding string. With the 
re-constructed rule set, during decoding, we generate 
two target translation hypothesis spaces (in the form 
of packed forests) synchronously by the tree-to-string 
445
rules and tree-to-tree sequence rules, and maintain 
the projection between them. In other words, we 
generate hypothesis (searching) from the 
tree-to-string forest and calculate the probability 
(evaluating syntax goodness) for each hypothesis by 
the hyper-edges in the tree-to-tree sequence forest.  
4.3 Parallel Hypothesis Spaces 
 
 
Fig. 10. Mapping from tree-to-tree sequence into 
tree-to-string rule 
 
In this subsection, we describe what the parallel 
search spaces are and how to construct them. As 
shown at Fig. 10, given a tree-to-tree sequence rule, 
it is easy to find its corresponding tree-to-string rule 
by simply ignoring the target inside structure and 
renaming the root and leaves non-terminal labels into 
?X?. We iterate through the tree-to-tree sequence rule 
set, find its corresponding tree-to-string rule and then 
group those rules with the same tree-to-string projec-
tion. After that, the original tree-to-tree sequence rule 
set becomes a set of smaller rule sets. Each of them is 
indexed by a unique tree-to-string rule.  
We apply the tree-to-string rules to generate an 
explicit target translation forest to represent the target 
sentences space. At the same time, whenever a 
tree-to-string rule is applied, we also retrieve its cor-
responding tree-to-tree sequence rule set and gener-
ate a set of latent hyper-edges with fine-grained syn-
tax information. In this case, we have two parallel 
forests, one with coarse explicit hyper-edges and the 
other fine and latent. Given a hyper-edge (or a node) 
in the coarse forest, there are a group of correspond-
ing latent hyper-edges (or nodes) with finer syntax 
labels in the fine forest. Accordingly, given a tree in 
the coarse forest, there is a corresponding sub-forest 
in the latent fine forest. We can view the latent fine 
forest as imbedded inside the explicit coarse forest. If 
an explicit hyper-edge is viewed as a big cable, then 
the group of its corresponding latent hyper-edges is 
the small wires inside it. 
We rely on the explicit hyper-edges to enumerate 
possible hypothesis while using the latent hy-
per-edges to measure its translation probability and 
syntax goodness. Thus, the complexity of the search 
space is reduced into the tree-to-string model level, 
while keeping the target language generation syntac-
tic aware. More importantly, we thoroughly avoid 
those spurious ambiguities introduced by the 
tree-to-tree sequence rules. 
4.4 Decoding with Parallel Hypothesis 
Spaces 
 
 
 
Fig. 11. Derivation path and derivation forest 
 
In this subsection, we show exactly how our decoder 
finds the best result from the parallel spaces. We 
generate hypothesis by traversing the coarse forest in 
the parallel spaces with cube-pruning (Huang and 
Chiang, 2007). Given a newly generated hypothesis, 
it is affiliated with a derivation path (tree) in the 
coarse forest and a group of derivation paths 
(sub-forest) in the finer forest. As shown in Fig. 11, 
the left part is the derivation path formed by a coarse 
hyper-edge, consisting the newly-generated sub-tree 
?X => X X X? connecting with three previous-
ly-generated sub paths while the right part is the de-
rivation forest formed by newly-generated finer hy-
per-edges rooted at ?VP? and ?S?, and previous-
ly-generated sub-forests.  
In this paper, we use the sum of probabilities of all 
the derivation paths in the finer forest to measure the 
quality of the candidate translation suggested by the 
hypothesis. From Fig. 11, we can see there may be 
more than one corresponding finer forests, it is easy 
to understand that the sum of all the trees? probabili-
ties in these finer forests is equal to the sum of the 
inside probability of all these root nodes of these fin-
er forests. We adopt the dynamic programming to 
compute the probability of the finer forest: whenever 
we generate a new hypothesis by concatenating a 
446
coarse hyper-edge and its sub-path, we find its cor-
responding finer hyper-edges and sub-forests, do the 
combination and accumulate probabilities from bot-
tom to up. For the coarse hyper-edge, because there 
is only one label ?X?, any sub-path could be easily 
concatenated with upper structure covering the same 
sub-span without the need of checking label compa-
tibility. While for the finer hyper-edges, we only link 
the root nodes of sub-forests to upper hyper-edges 
with the same linking node label. This is to guarantee 
syntactic goodness. In case there are some leaf nodes 
of the upper hyper-edges fail to find corresponding 
sub-forest roots with the same label (e.g. the ?NP? in 
red color in the rightmost of Fig 11), we simply link 
it into the nodes with the least inside probability 
(among these sub-forests), and at the same time give 
a penalty score to this combination. If some root 
nodes of some sub-forest still cannot find upper leaf 
nodes to concatenate (e.g. the ?CP? in red color in 
Fig. 11), we simply ignore them. After the combina-
tion process, it is straightforward to accumulate the 
inside probability dynamically from bottom up. 
5 Experiment 
5.1 Experimental Settings 
We evaluate our method on the Chinese-English 
translation task. We first carry out a series empirical 
study on a set of parallel data with 30K sentence 
pairs, and then do experiment on a larger data set to 
ensure that the effectiveness of our method is consis-
tent across data set of different size. We use the 
NIST 2002 test set as our dev set, and NIST 2003 
and NIST 2005 test sets as our test set. A 3-gram 
language model is trained on the target side of the 
training data by the SRILM Toolkits (Stolcke, 2002) 
with modified Kneser-Ney smoothing (Kneser and 
Ney, 1995). We train Charniak?s parser (Charniak, 
2000) on CTB5.0 for Chinese and ETB3.0 for Eng-
lish and modify it to output packed forest. GIZA++ 
(Och and Ney, 2003) and the heuristics 
?grow-diag-final-and? are used to generate m-to-n 
word alignments. For the MER training (Och, 2003), 
Koehn?s MER trainer (Koehn, 2007) is modified for 
our system. For significance test, we use Zhang et 
al.?s implementation (Zhang et al 2004). Our evalu-
ation metrics is case-sensitive closest BLEU-4 (Pa-
pineni et al, 2002). We use following features in our 
systems: 1) bidirectional tree-to-tree sequence proba-
bility, 2) bidirectional tree-to-string probability, 3) 
bidirectional lexical translation probability, 4) target 
language model, 5) source tree probability 6) the av-
erage number of unmatched nodes in the target forest. 
7) the length of the target translation, 8) the number 
of glue rules used. 
5.2 Empirical Study on Small Data 
We set forest pruning threshold (Mi et al, 2008) to 8 
on both source and target forests for rule extraction. 
For each source sub-tree, we set its height up to 3, 
width up to 7 and extract up to 10-best target struc-
tures. In decoding, we set the pruning threshold to 10 
for the input source forest. Table 1 compares the 
performance in NIST 2003 data set of our method 
and several state-of-the-art systems as our baseline. 
 
1) MOSES: phrase-based system (Koehn et al, 
2007) 
2) FT2S: forest-based tree-to-string system (Mi 
and Huang, 2008; Mi et al, 2008) 
3) FT2T: forest-based tree-to-tree system (Liu et 
al., 2009).  
4) FT2TS (1to1): our forest-based tree-to-tree 
sequence system, where 1to1 means only 
one-to-one frontier non-terminal node map-
ping is allowed, thus the system does not fol-
low our non-isomorphic mapping framework.  
5) FT2TS (1toN): our forest-based tree-to-tree 
sequence system that allows one-to-many 
frontier non-terminal node mapping by fol-
lowing our non-isomorphic mapping frame-
work   
 
In addition, our proposed parallel searching space 
(PSS) technology can be applied to both tree to tree 
and tree to sequence systems. Thus in table 1, for the 
tree-to-tree/tree sequence systems, we report two 
BLEU scores, one uses this technology (withPSS) 
and one does not (noPSS). 
 
Model BLEU-4 
MOSES 23.39 
FT2S 26.10 
FT2T noPSS 23.40 withPSS 24.46 
FT2TS (1to1) noPSS 25.39 withPSS 26.58 
FT2TS (1toN) noPSS 26.30 withPSS 27.70 
 
Table 1. Performance comparison of different methods 
 
 From Table 1, we can see that:  
447
1) All the syntax-based systems (except FT2T 
(noPSS) (23.40)) consistently outperform the 
phrase-based system MOSES significantly 
(? ? 0.01 ), indicating that syntactic know-
ledge is very useful to SMT. 
2) The PSS technology shows significant perfor-
mance improvement ?? ? 0.01? in all mod-
els, which clearly shows effectiveness of the 
PSS technology in utilizing target structures 
for target language generation.  
3) FT2TS (1toN) significantly outperforms 
(? ? 0.01) FT2TS (1to1) in both cases (noPSS 
and withPSS). This convincingly shows the 
effectiveness of our non-isomorphic mapping 
framework in capturing the non-isomorphic 
structure translation equivalences. 
4) Both FT2TS systems significantly outperform 
FT2T( ? ? 0.01). This verifies the effective-
ness of tree sequence rules. 
5) FT2TS shows different level of performance 
improvements over FT2S with the best case 
having 1.6 (27.70-26.10) BLEU score im-
provement over FT2S. This suggests that the 
target structure information is very useful, but 
we need to find a correct way to effectively 
utilize it. 
 
1to1 1toN ratio 
1735871 2363771 1:1.36 
 
Table 2. Statistics on node mapping in forest, where 
?1to1? means the number of nodes in source forest 
that can be translated into one node in target forest 
and ?1toN? means the number of nodes in source 
forest that have to be translated into more than one 
node in target forest, where the node refers to 
non-terminal nodes only 
 
Model # of rules T2S covered 
FT2T 295732 26.8% 
FT2TS(1to1) 631487 57.1% 
FT2TS (1toN) 1945168 100% 
 
Table 3. Statistics of rule coverage, where ?T2S 
covered? means the percentage of tree-to-string 
rules that can be covered by the model 
 
Table 2 studies the node isomorphism between bi-
lingual forest pair. We can see that the 
non-isomorphic node translation mapping (1toN) 
accounts for 57.6% (=1.36/(1+1.36)) of all the forest 
non-terminal nodes with target translation. This 
means that the one-to-many node mapping is a major 
issue in structure transformation. It also empirically 
justifies the importance of our non-isomorphic map-
ping framework.  
Table 3 shows the rule coverage of different bi-
lingual structure mapping model. FT2T only covers 
26.8% tree-to-string rules, so it performs worse than 
FT2S as shown in Table 1. FT2TS (1to1) does not 
allow one-to-many frontier node mapping, so it could 
only recover the non-isomorphic node mapping in 
the root level, while FT2TS (1toN) could make it at 
both root and leaf levels. Therefore, it is not surpris-
ing that in Table 3, FT2TS (1toN) cover many more 
rules than FT2TS (1to1) because given a source tree, 
there are many leaves, if any one of them is 
non-isomorphic, then it could not be covered by the 
FT2TS (1to1).  
 
Decoding Method BLEU-4 Speed (sec/sent)
Traditional: 
FT2TS (1toN) (noPPS) 26.30 152.6 
Ours: 
FT2TS (1toN) (withPPS) 27.70 5.22 
 
Table 4. Performance and speed comparison  
 
Table 4 clearly shows the advantage of our decod-
er over the traditional one. Ours could not only gen-
erate better translation result, but also be 
152.6/5.22>30 times faster. This mainly attributes to 
two reasons: 1) one-to-many frontier node mapping 
equipments the model with more ability to capture 
more non-isomorphic structure mappings than tradi-
tional models, and 2) ?parallel search space? enables 
the decoder to fully utilize target syntactic informa-
tion, but keeping the size of search space the same as 
that a ?tree to string? model explores. 
5.3 Results on Larger Data Set 
We also carry out experiment on a larger dataset 
consisting of the small dataset used in last section 
and the FBIS corpus. In total, there are 280K parallel 
sentence pairs with 9.3M Chinese words and 11.8M 
English words. A 3-gram language model is trained 
on the target side of the parallel corpus and the GI-
GA3 Xinhua portion. We compare our system 
(FT2TS with 1toN and withPPS) with two 
state-of-the-art baselines: the phrase-based system 
MOSES and the forest-based tree-to-string system 
448
implemented by us. Table 5 clearly shows the effec-
tiveness of our method is consistent across small and 
larger corpora, outperforming FT2S by 1.6-1.8 
BLEU and the MOSES by 3.3-4.0 BLEU statistically 
significantly (p<0.01). 
 
Model BLEU 
NIST2003 NIST2005 
MOSES 29.51 27.53 
FT2S 31.21 29.72 
FT2TS 32.88 31.50 
   
Table 5. Performance on larger data set 
6 Conclusions 
In this paper, we propose a framework to address the 
issue of bilingual non-isomorphic structure mapping 
and a novel parallel searching space scheme to effec-
tively utilize target syntactic structure information in 
the context of forest-based tree to tree sequence ma-
chine translation. Based on this framework, we de-
sign an efficient algorithm to extract tree-to-tree se-
quence translation rules from word aligned bilingual 
forest pairs. We also elaborate the parallel searching 
space-based decoding algorithm and the node label 
checking scheme, which leads to very efficient de-
coding speed as fast as the forest-based tree-to-string 
model does, at the same time is able to utilize infor-
mative target structure knowledge. We evaluate our 
methods on both small and large training data sets 
and two NIST test sets. Experimental results show 
our methods statistically significantly outperform the 
state-of-the-art models across different size of cor-
pora and different test sets. In the future, we are in-
terested in testing our algorithm at forest-based tree 
sequence to tree sequence translation. 
References  
Eugene Charniak. 2000. A maximum-entropy inspired 
parser. NAACL-00. 
Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. 
Syntax-based language models for statistical machine 
translation. MT Summit IX. 40?46. 
David Chiang. 2007. Hierarchical phrase-based transla-
tion.Computational Linguistics, 33(2). 
Steve DeNeefe, Kevin Knight. 2009. Synchronous Tree 
Adjoining Machine Translation. EMNLP-2009. 
727-736. 
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for MT. ACL-03 (companion volume). 
Michel Galley, Mark Hopkins, Kevin Knight and Daniel 
Marcu. 2004. What?s in a translation rule? 
HLT-NAACL-04. 273-280. 
Liang Huang. 2008. Forest Reranking: Discriminative 
Parsing with Non-Local Features. ACL-HLT-08. 
586-594 
Liang Huang and David Chiang. 2005. Better k-best Pars-
ing. IWPT-05. 53-64 
Liang Huang and David Chiang. 2007. Forest rescoring: 
Faster decoding with integrated language models. 
ACL-07. 144?151 
Dan Klein and Christopher D. Manning. 2001. Parsing 
and Hypergraphs. IWPT-2001. 
Reinhard Kneser and Hermann Ney. 1995. Improved 
backing-off for M-gram language modeling. 
ICASSP-95, 181-184 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Cal-
lison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Richard 
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin 
and Evan Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. ACL-07. 177-180. 
(poster) 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-String 
Alignment Template for Statistical Machine Transla-
tion. COLING-ACL-06. 609-616. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 2007. 
Forest-to-String Statistical Translation Rules. ACL-07. 
704-711. 
Yang Liu, Yajuan L?, Qun Liu. 2009. Improving 
Tree-to-Tree Translation with Packed Forests. ACL-09. 
558-566 
Haitao Mi, Liang Huang, and Qun Liu. 2008. For-
est-based translation. ACL-HLT-08. 192-199. 
Haitao Mi and Liang Huang. 2008. Forest-based Transla-
tion Rule Extraction. EMNLP-08. 206-214. 
Franz J. Och. 2003. Minimum error rate training in statis-
tical machine translation. ACL-03. 160-167. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics. 29(1) 19-51.  
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. ACL-02. 311-318. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. ICSLP-02. 901-904. 
Masaru Tomita. 1987. An Efficient Aug-
mented-Context-Free Parsing Algorithm. Computation-
al Linguistics 13(1-2): 31-46 
449
Xavier Carreras and Michael Collins. 2009. 
Non-projective Parsing for Statistical Machine Trans-
lation. EMNLP-2009. 200-209. 
K. Yamada and K. Knight. 2001. A Syntax-Based Statis-
tical Translation Model. ACL-01. 523-530. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew 
Lim Tan. 2009a. Forest-based Tree Sequence to String 
Translation Model. ACL-IJCNLP-09. 172-180. 
Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim Tan. 
2009b. Fast Translation Rule Matching for Syn-
tax-based Statistical Machine Translation. EMNLP-09. 
1037-1045. 
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew Lim 
Tan and Sheng Li. 2007. A Tree-to-Tree Align-
ment-based model for statistical Machine translation. 
MT-Summit-07. 535-542 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew 
Lim Tan, Sheng Li. 2008. A Tree Sequence Align-
ment-based Tree-to-Tree Translation Model. 
ACL-HLT-08. 559-567. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do 
we need to have a better system? LREC-04. 2051-2054. 
450
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233?237,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling of term-distance and term-occurrence information for im-
proving n-gram language model performance 
 
Tze Yuang Chong1,2, Rafael E. Banchs3, Eng Siong Chng1,2, Haizhou Li1,2,3 
1Temasek Laboratory, Nanyang Technological University, Singapore 639798 
2School of Computer Engineering, Nanyang Technological University, Singapore 639798 
3Institute for Infocomm Research, Singapore 138632 
tychong@ntu.edu.sg, rembanchs@i2r.a-star.edu.sg, 
aseschng@ntu.edu.sg, hli@i2r.a-star.edu.sg 
  
 
Abstract 
In this paper, we explore the use of distance 
and co-occurrence information of word-pairs 
for language modeling. We attempt to extract 
this information from history-contexts of up to 
ten words in size, and found it complements 
well the n-gram model, which inherently suf-
fers from data scarcity in learning long histo-
ry-contexts. Evaluated on the WSJ corpus, bi-
gram and trigram model perplexity were re-
duced up to 23.5% and 14.0%, respectively. 
Compared to the distant bigram, we show that 
word-pairs can be more effectively modeled in 
terms of both distance and occurrence. 
1 Introduction 
Language models have been extensively studied 
in natural language processing. The role of a lan-
guage model is to measure how probably a (tar-
get) word would occur based on some given evi-
dence extracted from the history-context. The 
commonly used n-gram model (Bahl et al 1983) 
takes the immediately preceding history-word 
sequence, of length   1 , as the evidence for 
prediction. Although n-gram models are simple 
and effective, modeling long history-contexts 
lead to severe data scarcity problems. Hence, the 
context length is commonly limited to as short as 
three, i.e. the trigram model, and any useful in-
formation beyond this window is neglected. 
In this work, we explore the possibility of 
modeling the presence of a history-word in terms 
of: (1) the distance and (2) the co-occurrence, 
with a target-word. These two attributes will be 
exploited and modeled independently from each 
other, i.e. the distance is described regardless the 
actual frequency of the history-word, while the 
co-occurrence is described regardless the actual 
position of the history-word. We refer to these 
two attributes as the term-distance (TD) and the 
term-occurrence (TO) components, respectively. 
The rest of this paper is structured as follows. 
The following section presents the most relevant 
related works. Section 3 introduces and moti-
vates our proposed approach. Section 4 presents 
in detail the derivation of both TD and TO model 
components. Section 5 presents some perplexity 
evaluation results. Finally, section 6 presents our 
conclusions and proposed future work. 
2 Related Work 
The distant bigram model (Huang et.al 1993, 
Simon et al 1997, Brun et al 2007) disassembles 
the n-gram into (n?1) word-pairs, such that each 
pair is modeled by a distance-k bigram model, 
where 1      1 . Each distance-k bigram 
model predicts the target-word based on the oc-
currence of a history-word located k positions 
behind.  
Zhou & Lua (1998) enhanced the effective-
ness of the model by filtering out those word-
pairs exhibiting low correlation, so that only the 
well associated distant bigrams are retained. This 
approach is referred to as the distance-dependent 
trigger model, and is similar to the earlier pro-
posed trigger model (Lau et al 1993, Rosenfeld 
1996) that relies on the bigrams of arbitrary dis-
tance, i.e. distance-independent. 
Latent-semantic language model approaches 
(Bellegarda 1998, Coccaro 2005) weight word 
counts with TFIDF to highlight their semantic 
importance towards the prediction. In this type of 
approach, count statistics are accumulated from 
long contexts, typically beyond ten to twenty 
words. In order to confine the complexity intro-
duced by such long contexts, word ordering is 
ignored (i.e. bag-of-words paradigm). 
Other approaches such as the class-based lan-
guage model (Brown 1992, Kneser & Ney 1993) 
233
use POS or POS-like classes of the history-words 
for prediction. The structured language model 
(Chelba & Jelinek 2000) determines the ?heads? 
in the history-context by using a parsing tree. 
There are also works on skipping irrelevant his-
tory-words in order to reveal more informative n-
grams (Siu & Ostendorf 2000, Guthrie et al 
2006). Cache language models exploit temporal 
word frequencies in the history (Kuhn & Mori 
1990, Clarkson & Robinson 1997). 
3 Motivation of the Proposed Approach 
The attributes of distance and co-occurrence are 
exploited and modeled differently in each lan-
guage modeling approach. In the n-gram model, 
for example, these two attributes are jointly taken 
into account in the ordered word-sequence. Con-
sequently, the n-gram model can only be effec-
tively implemented within a short history-context 
(e.g. of size of three or four). 
Both, the conventional trigger model and the 
latent-semantic model capture the co-occurrence 
information while ignoring the distance informa-
tion. It is reasonable to assume that distance in-
formation at far contexts is less likely to be in-
formative and, hence, can be discarded. Howev-
er, intermediate distances beyond the n-gram 
model limits can be very useful and should not 
be discarded. 
On the other hand, distant-bigram models and 
distance-dependent trigger models make use of 
both, distance and co-occurrence, information up 
to window sizes of ten to twenty. They achieve 
this by compromising inter-dependencies among 
history-words (i.e. the context is represented as 
separated word-pairs). However, similarly to n-
gram models, distance and co-occurrence infor-
mation are implicitly tied within the word-pairs. 
In our proposed approach, we attempt to ex-
ploit the TD and TO attributes, separately, to in-
corporate distant context information into the n-
gram, as a remedy to the data scarcity problem 
when learning the far context. 
4 Language Modeling with TD and TO 
A language model estimates word probabilities 
given their history, i.e.  	 
| 	 
 , 
where  denotes the target word and  denotes its 
corresponding history. Let the word located at ith 
position, 
 , be the target-word and its preceding 
word-sequence 
 	 
?

  of 
length   1, be its history-context. Also, in or-
der to alleviate the data scarcity problem, we as-
sume the occurrences of the history-words to be 
independent from each other, conditioned to the 
occurrence of the target-word 
 , i.e.  
 
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 30?37,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
An Empirical Evaluation of Stop Word Removal                                  
in Statistical Machine Translation 
Chong Tze Yuang 
School of Computer Engi-
neering, Nanyang Technolo-
gical University, 639798 
Singapore 
tychong@ntu.edu.sg 
Rafael E. Banchs 
Institute for Infocomm Re-
search, A*STAR, 138632, 
Singapore 
rembanchs@i2r.a-
star.edu.sg 
Chng Eng Siong 
School of Computer Engi-
neering Nanyang Technolo-
gical University, 639798 
Singapore. 
aseschng@ntu.edu.sg 
 
 
 
 
 
Abstract 
In this paper we evaluate the possibility of 
improving the performance of a statistical 
machine translation system by relaxing the 
complexity of the translation task by remov-
ing the most frequent and predictable terms 
from the target language vocabulary. After-
wards, the removed terms are inserted back 
in the relaxed output by using an n-gram 
based word predictor. Empirically, we have 
found that when these words are omitted 
from the text, the perplexity of the text de-
creases, which may imply the reduction of 
confusion in the text. We conducted some 
machine translation experiments to see if 
this perplexity reduction produced a better 
translation output. While the word predic-
tion results exhibits 77% accuracy in pre-
dicting 40% of the most frequent words in 
the text, the perplexity reduction did not 
help to produce better translations. 
1 Introduction 
It is a characteristic of natural language that a 
large proportion of running words in a corpus 
corresponds to a very small fraction of the voca-
bulary. An analysis of the Brown Corpus has 
shown that the hundred most frequent words 
account for 42% of the corpus, while only 0.1% 
in the vocabulary. On the other hand, words oc-
curring only once account merely 5.7% in the 
corpus but 58% in the vocabulary (Bell et al 
1990). This phenomenon can be explained in 
terms of Zipf?s Law, which states that the prod-
uct of word ranks and their frequencies approx-
imates a constant, i.e. word-frequency plot is 
close to a hyperbolic function, and hence the few 
top ranked words would account for a great por-
tion of the corpus. Also, it appears that the top 
ranked words are mainly function words. For 
instance, the eight most frequent words in the 
Brown Corpus are the, of, and, to, a, in, that and 
is (Bell et al 1990). 
It is a common practice in Information Re-
trieval (IR) to filter the most frequent words out 
from processed documents (which are referred to 
as stop words), as these function words are se-
mantically non-informative and constitute weak 
indexing terms. By removing this great amount 
of stop words, not only space and time complexi-
ties can be reduced, but document content can be 
better discriminated by the remaining content 
words (Fox, 1989; Rijsbergen, 1979; Zou et al, 
2006; Dolamic & Savoy 2009). 
Inspired by the concept of stop word removal 
in Information Retrieval, in this work we study 
the feasibility of stop word removal in Statistical 
Machine Translation (SMT). Different from In-
formation Retrieval, that ranks or classifies doc-
uments; SMT hypothesizes sentences in target 
language. Therefore, without explicitly removing 
frequent words from the documents, we proposed 
to ignore such words in the target language vo-
cabulary, i.e. by replacing those words with a 
null token. We term this process as ?relaxation? 
and the omitted words as ?relaxed words?.  
Relaxed SMT here refers to a translation task 
in which target vocabulary words are intentional-
ly omitted from the training dataset for reducing 
translation complexity. Since the most frequent 
words are targeted to be relaxed, as a result, there 
will be vast amount of null tokens in the output 
text, which later shall be recovered in a post 
processing stage. The idea of relaxation in SMT 
is motivated by one of our experimental findings, 
in which the perplexity measured over a test set 
decreases when most frequent words are relaxed. 
For instance, a 15% of perplexity reduction is 
observed when the twenty most frequent words 
are relaxed in the English EPPS dataset. The 
reduction of perplexity allows us to conjecture 
30
about the decrease of confusion in the text, from 
which a SMT system might be benefited. 
After applying relaxed SMT, the resulting 
null tokens in the translated sentences have to be 
replaced by the corresponding words from the set 
of relaxed words. As relaxed words are chosen 
from the top ranked words, which possess high 
occurrences in the corpus, their n-gram probabili-
ties could be reliably trained to serve for word 
prediction. Also, these words are mainly function 
words and, from the human perspective, function 
words are usually much easier to predict from 
their neighbor context than content words. Con-
sider for instance the sentence the house of the 
president is very nice. Function words like the, 
of, and is, are certainly easier to be predicted than 
content words such as house, president, and nice. 
The rest of the paper is organized into four 
sections. In section 2, we discuss the relaxation 
strategy implemented for a SMT system, which 
generates translation outputs that contain null to-
kens. In section 3, we present the word predic-
tion mechanism used to recover the null tokens 
occurring in the relaxed translation outputs. In 
section 4, we present and discuss the experimen-
tal results. Finally, in section 5 we present the 
most relevant conclusion of this work. 
2 Relaxation for Machine Translation 
In this paper, relaxation refers to the replacement 
of the most frequent words in text by a null to-
ken. In the practice, a set of frequent words is 
defined and the cardinality of such set is referred 
to as the relaxation order. For example, lets the 
relaxation order be two and the two words on the 
top rank are the and is. By relaxing the sample 
sentence previously presented in the introduc-
tion, the following relaxed sentence will be ob-
tained: NULL house of NULL President NULL 
very beautiful. 
From some of our preliminary experimental 
results with the EPPS dataset, we did observe 
that a relaxation order of twenty leaded to a per-
plexity reduction of about a 15%. To see whether 
this contributes to improving the translation per-
formance, we trained a translation system by 
relaxing the top ranked words in the vocabulary 
of the target language. In this way, there will be a 
large number of words in the source language 
that will be translated to a null token. For exam-
ple: la (the in Spanish) and es (is in Spanish) will 
be both translated to a null token in English. 
This relaxation of terms is only applied to the 
target language vocabulary, and it is conducted 
after the word alignment process but before the 
extraction of translation units and the computa-
tion of model probabilities. The main objective 
of this relaxation procedure is twofold: on the 
one hand, it attempts to reduce the complexity of 
the translation task by reducing the size of the 
target vocabulary while affecting a large propor-
tion of the running words in the text; on the other 
hand, it should also help to reduce model sparse-
ness and improve model probability estimates. 
Of course, different from the Information Re-
trieval case, in which stop words are not used at 
all along the search process, in the considered 
machine translation scenario, the removed words 
need to be recovered after decoding in order to 
produce an acceptable translation. The relaxed 
word replacement procedure, which is based on 
an n-gram based predictor, is implemented as a 
post-processing step and applied to the relaxed 
machine translation output in order to produce 
the final translation result.  
Our bet here is that the gain in the translation 
step, which is derived from the relaxation strate-
gy, should be enough to compensate the error 
rate of the word prediction step, producing, in 
overall, a significant improvement in translation 
quality with respect to the non-relaxed baseline 
procedure. 
The next section describes the implemented 
word prediction model in detail. It constitutes a 
fundamental element of our proposed relaxed 
SMT approach. 
3 Frequent Word Prediction 
Word prediction has been widely studied and 
used in several different tasks such as, for exam-
ple, augmented and alternative communication 
(Wandmacher and Antoine, 2007) and spelling 
correction (Thiele et al, 2000). In addition to the 
commonly used word n-gram, various language 
modeling techniques have been applied, such as 
the semantic model (Lu?s and Rosa, 2002; Wand-
macher and Antoine, 2007) and the class-based 
model (Thiele et al, 2000; Zohar and Roth, 
2000; Ruch et al, 2001). 
The role of such a word predictor in our con-
sidered problem is to recover the null tokens in 
the translation output by replacing them with the 
words that best fit the sentence. This task is es-
sentially a classification problem, in which the 
most suitable relaxed word for recovering a giv-
en null token must be selected. In other words, 
  max	
 sentence??, where 
sentence?  is the probabilistic model, e.g. n-
31
gram, that estimates the likelihood of a sentence 
when a null token is recovered with word  , 
drawn from the set of relaxed words . The car-
dinality || is referred to as the relaxation order, 
e.g. ||  5 implies that the five most frequent 
words have been relaxed and are candidates to be 
recovered. 
Notice that the word prediction problem in 
this task is quite different from other works in the 
literature. This is basically because the relaxed 
words to be predicted in this case are mainly 
function words. Firstly, it may not be effective to 
predict a function word semantically. For exam-
ple, we are more certain in predicting equity than 
for given the occurrence of share in the sentence. 
Secondly, although class-based modeling is com-
monly used for prediction, its original intention 
is to tackle the sparseness problem, whereas our 
task focuses only on the most frequent words. 
In this preliminary work, our predicting me-
chanism is based on an n-gram model. It predicts 
the word that yields the maximum a posteriori 
probability, conditioned on its predecessors. For 
the case of the trigram model, it can be expressed 
as follows: 
 
  max	
 |  (1) 
 
Often, there are cases in which more than one 
null token occur consecutively. In such cases 
predicting a null token is conditioned on the pre-
vious recovered null tokens. To prevent a predic-
tion error from being propagated, one possibility 
is to consider the marginal probability (summed 
over the relaxed word set) over the words that 
were previously null tokens. For example, if  
is a relaxed word, which has been recovered 
from earlier predictions, then the prediction of  
should no longer be conditioned by  . This 
can be computed as follows: 
 
  max	
 |