A Maximum Entropy-based Word Sense Disambiguation system

Armando Suarez Manuel Palomar
Departamento de Lenguajes y Sistemas Informaticos
Universidad de Alicante
Apartado de correos, 99
E-03080 Alicante, Spain
farmando, mpalomarg@dlsi.ua.es
http://www.dlsi.ua.es/armando/publicaciones.html
Abstract
In this paper, a supervised learning system of
word sense disambiguation is presented. It is
based on conditional maximum entropy models.
This system acquires the linguistic knowledge
from an annotated corpus and this knowledge
is represented in the form of features. Several
types of features have been analyzed using the
SENSEVAL-2 data for the Spanish lexical sam-
ple task. Such analysis shows that instead of
training with the same kind of information for
all words, each one is more eectively learned
using a dierent set of features. This best-
feature-selection is used to build some systems
based on dierent maximum entropy classiers,
and a voting system helped by a knowledge-
based method.
1 Introduction
Word sense disambiguation (WSD) is an open
research eld in natural language processing
(NLP). The task of WSD consists in assign-
ing the correct sense to words using an elec-
tronic dictionary as the source of word deni-
tions. This is a hard problem that is receiving
a great deal of attention from the research com-
munity.
Currently, there are two main methodologi-
cal approaches in this research area: knowledge-
based methods and corpus-based methods. The
former approach relies on previously acquired
linguistic knowledge, and the latter uses tech-
niques from statistics and machine learning to
induce models of language usage from large
samples of text (Pedersen, 2001). Learning can
be supervised or unsupervised. With supervised

This paper has been partially supported by the
Spanish Government (CICYT) under project number
TIC2000-0664-C02-02.
learning, the actual status (here, sense label)
for each piece of data in the training example is
known, whereas with unsupervised learning the
classication of the data in the training example
is not known (Manning and Schutze, 1999).
At SENSEVAL-2, researchers showed the lat-
est contributions to WSD. Some supervised sys-
tems competed in the Spanish lexical sample
task. The Johns Hopkins University system
(Yarowsky et al, 2001) combined, by means of
a voting-based classier, several WSD subsys-
tems based on dierent methods: decision lists
(Yarowsky, 2000), cosine-based vector models,
and Bayesian classiers. The University of
Maryland system (UMD-SST) (Cabezas et al,
2001) used support vector machines.
Pedersen (2002) proposes a baseline method-
ology for WSD based on decision tree learning
and naive Bayesian classiers, using simple lex-
ical features. Several systems that combine dif-
ferent classiers using distinct sets of features
competed at SENSEVAL-2, both in the English
and Spanish lexical sample tasks.
This paper presents a system that implements
a corpus-based method of WSD. The method
used to perform the learning over a set of sense-
disambiguated examples is that of maximum en-
tropy (ME) probability models. Linguistic in-
formation is represented in the form of feature
vectors, which identify the occurrence of certain
attributes that appear in contexts containing
linguistic ambiguities. The context is the text
surrounding an ambiguity that is relevant to the
disambiguation process. The features used may
be of a distinct nature: word collocations, part-
of-speech (POS) labels, keywords, topic and
domain information, grammatical relationships,
and so on. Instead of training with the same
kind of information for all words, which under-
estimates which information is more relevant to
each word, our research shows that each word is
more eectively learned using a dierent set of
features. Therefore, a more accurate feature se-
lection can be done testing several combinations
of features by means of a n-fold cross-validation
over the training data.
At SENSEVAL-2, Stanford University pre-
sented a metalearner (Ilhan et al, 2001) com-
bining simple classiers (naive-Bayes, vector
space, memory-based and others) that use vot-
ing and conditional ME models. Garca Varea
et al (2001) do machine translation tasks using
ME to perform some kind of semantic classi-
cation, but they also rely on another statistical
training procedure to dene word classes.
In the following discussion, the ME frame-
work will be described. Then, feature imple-
mentation and the complete set of feature de-
nitions used in this work will be detailed. Next,
evaluation results using several combinations of
these features will be shown. Finally, some con-
clusions will be presented, along with a brief
discussion of work in progress and future work
planned.
2 The Maximum Entropy
Framework
ME modeling provides a framework for integrat-
ing information for classication frommany het-
erogeneous information sources (Manning and
Schutze, 1999). ME probability models have
been successfully applied to some NLP tasks,
such as POS tagging or sentence boundary de-
tection (Ratnaparkhi, 1998).
The WSD method used in this work is based
on conditional ME models. It has been im-
plemented using a supervised learning method
that consists of building word-sense classiers
using a semantically tagged corpus. A classi-
er obtained by means of an ME technique con-
sists of a set of parameters or coe?cients which
are estimated using an optimization procedure.
Each coe?cient is associated with one feature
observed in the training data. The main pur-
pose is to obtain the probability distribution
that maximizes the entropy, that is, maximum
ignorance is assumed and nothing apart from
the training data is considered. Some advan-
tages of using the ME framework are that even
knowledge-poor features may be applied accu-
rately; the ME framework thus allows a virtu-
ally unrestricted ability to represent problem-
specic knowledge in the form of features (Rat-
naparkhi, 1998).
Let us assume a set of contexts X and a set
of classes C. The function cl : X ! C chooses
the class c with the highest conditional proba-
bility in the context x: cl(x) = argmax
c
p(cjx).
Each feature is calculated by a function that is
associated to a specic class c
0
, and it takes the
form of equation (1), where cp(x) is some ob-
servable characteristic in the context
1
. The con-
ditional probability p(cjx) is dened by equation
(2), where 
i
is the parameter or weight of the
feature i, K is the number of features dened,
and Z(x) is a value to ensure that the sum of
all conditional probabilities for this context is
equal to 1.
f(x; c) =
An Algorithm for Anaphora Resolution in 
Spanish Texts 
Manuel Palomar* 
University of Alicante 
Lidia Moreno t
Valencia University of Technology 
Jesfis Peral* 
University of Alicante 
Rafael Mufioz* 
University of Alicante 
Antonio Ferr~indez* 
University of Alicante 
Patricio Martinez-Barco* 
University of Alicante 
Maximiliano Saiz-Noeda* 
University of Alicante 
This paper presents an algorithm for identifying noun phrase antecedents ofthird person personal 
pronouns, demonstrative pronouns, reflexive pronouns, and omitted pronouns (zero pronouns) 
in unrestricted Spanish texts. We define a list of constraints and preferences for different ypes 
of pronominal expressions, and we document in detail the importance of each kind of knowledge 
(lexical, morphological, syntactic, and statistical) in anaphora resolution for Spanish. The paper 
also provides a definition for syntactic onditions on Spanish NP-pronoun oncoreference using 
partial parsing. The algorithm has been evaluated on a corpus of 1,677 pronouns and achieved 
a success rate of 76.8%. We have also implemented four competitive algorithms and tested their 
performance in a blind evaluation on the same test corpus. This new approach could easily be 
extended to other languages uch as English, Portuguese, Italian, or Japanese. 
1. Introduction 
We present an algorithm for identifying noun phrase antecedents of personal pro- 
nouns, demonstrative pronouns, reflexive pronouns, and omitted pronouns (zero pro- 
nouns) in Spanish. The algorithm identifies both intrasentential and intersentential 
antecedents and is applied to the syntactic analysis generated by the slot unifica- 
t ion parser (SUP) (Ferr~ndez, Palomar, and Moreno 1998b). It also combines different 
forms of knowledge by distinguishing between constraints and preferences. Whereas 
constraints are used as combinations of several kinds of knowledge (lexical, mor- 
phological, and syntactic), preferences are defined as a combination of heuristic rules 
extracted from a study of different corpora. 
We present he following main contributions in this paper: 
? an algorithm for anaphora resolution in Spanish texts that uses different 
kinds of knowledge 
* Department of Software and Computing Systems, Alicante, Spain. E-mail: (Palomar) 
mpalomar@dlsi.ua.es, (F rr~ndez) antonio@dlsi.ua.es, (Martfnez-Barco) patricio@dlsi.ua.es, (Peral) 
jperal@dlsi.ua.es, (Saiz-Noeda) max@dlsi.ua.es, (Mufioz) rafael@dlsi.ua.es 
t Department of Information Systems and Computation, Valencia, Spain. E-mail: hnoreno@dsic.upv.es 
@ 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 4 
? an exhaustive study of the importance of each kind of knowledge in 
Spanish anaphora resolution 
? a proposal concerning syntactic onditions on NP-pronoun 
noncoreference in Spanish that can be evaluated on a partial parse tree 
? a proposal regarding preferences that are appropriate for resolving 
anaphora in Spanish and that could easily be extended to other 
languages 
? a blind test of the algorithm 
? a comparison with other approaches to anaphora resolution that we have 
applied to Spanish texts using the same blind test 
In Section 2, we show the classification scheme we used to identify the different ypes 
of anaphora that we would be resolving. In Section 3, we present he algorithm and 
discuss its main properties. In Section 4, we evaluate the algorithm. In Section 5, we 
compare our algorithm with several other approaches to anaphora resolution. Finally, 
we present our conclusions. 
2. Our Classification Scheme for Pronominal Expressions in Spanish 
In this section, we present our classification scheme for identifying the different ypes 
of anaphora that we will be resolving. Personal pronouns (PPR), demonstrative pro- 
nouns (DPR), reflexive pronouns (RPR), and omitted pronouns (OPa) are some of the 
most frequent ypes of anaphoric expressions found in Spanish and are the main 
subject of this study. Personal and demonstrative pronouns are further classified ac- 
cording to whether they appear within a prepositional phrase (PP) or whether they 
are complement personal pronouns (clitic pronouns1). We present examples for each 
of the four types of common anaphora. Each example is presented in three forms: as a 
Spanish sentence, as a word-to-word translation into English, and correctly translated 
into English. 2
2.1 Clitic Personal Pronouns (CPPR) 
In the case of clitic personal pronouns, I0, la, le 'him, her, it' and los, las, les 'them', we 
consider that the third person personal pronoun plays the role of the complement. 
(1) Ana abre \[la verja\]i y lai cierra tras de si. 
Ana opens \[the gate\]/ and it/ closes after herself 
'Ana opens the gate and closes it after herself.' 
2.2 Personal Pronouns Not Included in a PP (PPanotPP) 
We include in this class the personal pronouns ~l, ella, ello 'he, she, it' and ellas, ellos 
'they'. 
(2) Andr6si es mi vecino, t~li vive en el segundo piso. 
Andr6si is my neighbor Hei lives on the second floor 
'Andr6s is my neighbor. He lives on the second floor.' 
1 According to Mathews (1997), aclitic pronoun is a pronoun that is treated as an independent word in 
syntax but that forms a phonological unit with the verb that precedes or follows it. 
2 Coindexing indicates coreference b tween anaphor and antecedent. 
546 
Palomar et al Anaphora Resolution in Spanish Texts 
2.3 Personal Pronouns Included in a PP (PPRinPP) 
We include in this class the personal pronouns dl, ella, ello 'him, her, it' and ellas, ellos 
'them'. 
(3) Juan/ debe asistir pero Pedro lo har~i por 61i. 
Juani must attend but Pedro it will do for himi 
'Juan must attend but Pedro will do it for him.' 
2.4 Demonstrative Pronouns Not Included in a PP (DPRnotPP) 
We include in this class the demonstrative pronouns ~ste, dsta, esto 'this'; ~stos, ~stas 
'these'; dse, ~sa, aqu~l, aqu~lla 'that'; and dsos, dsas, aqu~llos, aqudllas 'those'. 
(4) E1 Ferrarii gan6 al Ford. t~stei es el mejor. 
the Ferrarii beat the Ford This/ is the best 
'The Ferrari beat the Ford. This is the best.' 
2.5 Demonstrative Pronouns Included in a PP (DPRinPP) 
We include in this class the demonstrative pronouns ~ste, ~sta, esto 'this'; ~stos, ~stas 
'these'; dse, ~sa, aqudl, aqudlla 'that'; and dsos, ~sas, aqu~llos, aqudllas 'those'. 
(5) Ana vive con Pacoi y cocina para 6stei diariamente. 
Ana lives with Pacoi and cooks for this/ every day 
'Ana lives with Paco and cooks for him every day.' 
2.6 Reflexive Pronouns (RPR) 
We include in this class the reflexive pronouns e, sL si mismo 'himself, herself, itself' 
and consigo, consigo mismo 'themselves'. 
(6) Anai abre la verja y la cierra tras de sfi. 
Anai opens the gate and it closes after herself/ 
'Ana opens the gate and closes it after herself.' 
2.7 Omitted Pronouns (Zero Pronouns OPa) 
The omitted pronoun is the most frequent ype of anaphoric expression in Spanish, as 
we will show in Section 4.2. Omitted pronouns occur when the pronominal subject is 
omitted. This kind of pronoun also occurs in other languages, such as Portuguese or 
Japanese; in these languages, it can also appear in object position, whereas in Spanish 
or Italian, it can appear only in subject position. In the following example, the omission 
is represented by the symbol 13 (the symbol does not appear in the correct ranslation 
into English). 
(7) Anai abre la verja y (~i la cierra tras de sf. 
Anai opens the gate and Oi it closes after herself 
'Ana opens the gate and she closes it after herself.' 
3. Anaphora Resolution Algorithm 
In the algorithm, all the types of anaphora are identified from left to right as they 
appear in the sentence. The most important proposals for anaphora resolution--such 
as those of Baldwin (1997), Lappin and Leass (1994), Hobbs (1978), or Kennedy and 
Boguraev (1996)--are based on a separation between constraints and preferences. 
547 
Computational Linguistics Volume 27, Number 4 
Constraints discard some of the candidates, whereas preferences simply sort the re- 
maining candidates. A constraint defines a property that must be satisfied in order 
for any candidate to be considered as a possible solution of the anaphor. For example, 
pronominal anaphors and antecedents must agree in person, gender, and number. 3 
Otherwise, the candidate is discarded as a possible solution. A preference is a charac- 
teristic that is not always satisfied by the solution of an anaphor. The application of 
preferences usually involves the use of heuristic rules in order to obtain a ranked list 
of candidates. 
Each type of anaphora has its own set of constraints and preferences, although 
they all follow the same general algorithm: constraints are applied first, followed by 
preferences. 
Based on the preceding description, our algorithm contains the following main 
components: 
? identification of the type of pronoun 
? constraints 
- -  morphological greement (person, gender, and number) 
- -  syntactic onditions on NP-pronoun oncoreference 
? preferences 
In order to apply this algorithm to unrestricted texts, it has been necessary to use 
partial parsing. In our partial-parsing scheme, as presented in Ferr~ndez, Palomar, and 
Moreno (1999), we only parse coordinated NPs and PPs, verbal chunks, pronouns, and 
what we have called free conjunctions (i.e., conjunctions that do not join coordinated 
NPs or PPs). Words that do not appear within these constituents are simply ignored. 
The NP constituents include coordinated adjectives, relative clauses, coordinated PPs, 
and appositives as modifiers. 
With this partial-parsing scheme, we divide a sentence into clauses by parsing first 
the free conjunction and then the verbs, as in the following example: 
(8) Pedro compr6 un regalo y se lo dio a Ana. 
Pedro bought a gift and her it gave to Ana 
'Pedro bought a gift and gave it to Ana.' 
In this example, we have parsed the following constituents: np(Pedro), v(comprO), np(un 
regalo),freeconj(y), pron(se), pron(lo), v(dio), pp(a Ana). We are able to divide this sentence 
into two clauses because it contains the free conjunction y 'and' and the two verbs 
compr6 'bought' and clio 'gave'. 
3.1 Identification of the Kind of Pronoun 
The algorithm uses partial-parse trees to automatically identify omitted pronouns by 
employing the following steps: 
? The sentence is divided into clauses (by parsing the free conjunction 
followed by the verbs). 
3 In our implementation, thismorphological information is extracted from the part-of-speech tagger. 
548 
Palomar et al Anaphora Resolution in Spanish Texts 
An NP or pronoun is sought for each clause by analyzing the clause 
constituents on the left-hand side of the verb, unless the verb is 
imperative or impersonal. The chosen NP or pronoun must agree in 
person and number with the clausal verb. (In evaluating this algorithm, 
Ferr~ndez and Peral \[2000\] achieved a success rate of 88% for detecting 
omitted pronouns.) 
The remaining pronouns are identified based on part-of-speech (POS) tagger out- 
puts. 
3.2 Morphological Agreement 
Person, gender, and number agreement are checked in order to discard potential an- 
tecedents. For example, in the sentence 
(9) Juanj vio a Rosa/. Ella/ estaba muy feliz. 
Juanj saw to Rosa/ Shei was very happy 
'Juan saw Rosa. She was very happy.' 
there are two possible antecedents for ella 'she', whose slot structures 4 are 
np (conc (sing, masc), X, Juan) 
np (conc (sing, fem), Y, Rosa) 
whereas the slot structure of the pronoun is 
pron (conc (sing, fem), Z, ella). 
In order to decide between the two antecedents, he unification of both slot struc- 
tures (pronoun and candidate) is carried out by the slot unification parser (Ferr~ndez, 
Palomar, and Moreno 1999). In this example, the candidate Juan is rejected by this 
morphological greement constraint. 
3.3 Syntactic Conditions on NP-Pronoun Noncoreference 
These conditions are based on c-command and minimal-governing-category constraints 
as formulated by Reinhart (1983) and on the noncoreference conditions of Lappin and 
Leass (1994). They are of great importance in any anaphora resolution system that 
does not use semantic information, as is the case with our proposal. In such systems, 
recency is important in selecting the antecedent of an anaphor. That is to say, the 
closest NP to the anaphor has a better chance of being selected as the solution. One 
problem, however, is that such constraints are formulated using full parsing, whereas 
if we want to work with unrestricted texts we should be using partial parsing, as 
previously defined. 
We have therefore proposed a set of noncoreference onditions for Spanish, using 
partial parsing, although they could easily be extended to other languages such as En- 
glish. In our system, the following types of pronouns are noncoreferential with a noun 
phrase (NP) under the conditions noted (noncoindexing indicates that a candidate is
rejected by these conditions). 
4 The term slot structure is defined in Ferr~ndez, Palomar, and Moreno (1998b). The slot structure stores 
morphological and syntactic information related to the different constituents of a sentence. 
549 
Computational Linguistics Volume 27, Number 4 
. 
(a) 
(b) 
(c) 
. 
(a) 
Reflexive pronouns are noncoreferential when: 
(b) 
(10) 
the NP is included in another constituent (e.g., the NP is 
included in a PP) 
Ante Luisj sei frot6 con la toalla. 
in front of Luisj himself/ rubbed with the towel 
'He rubbed himself with the towel in front of Luis.' 
In this sentence, we would have obtained the following sequence 
of constituents after our partial-parsing scheme: pp(prep(ante), 
np(Luis )) , pron(se) , v(frot6 ) , pp(prep( con) , np(la toalla) . Following 
the above-stated condition, the NP Luis cannot corefer with the 
reflexive pronoun se since Luis is included in a PP (ante Luis). 
the NP is in a different clause or sentence 
(11) Anaj trajo un cuchillo y Eva/ sei cort6. 
Anaj brought a knife and Eva/ herself/ cut 
'Ana brought a knife and Eva cut herself.' 
the NP appears after the verb and there is another NP in the 
same clause before the verb 
(12) 
(13) 
Juan/ sei cort6 con el cuchilloj. 
Juan/ himself/ cut with the knifej 
'Juan cut himself with the knife.' 
Under these conditions, coreference is allowed between the NP 
and the reflexive pronoun, since both are in the same clause. For 
example: 
Juan/ queria verlo por s~ mismoi. 
Juan/ wanted see it for himself/ 
'Juan wanted to see it for himself.' 
In this example, Juan and the reflexive pronoun si mismo 
'himself' corefer since Juan is in the same clause as the anaphor, 
it is not included in another constituent, and it appears before 
the verb. 
Clitic pronouns are noncoreferential when: 
the NP is included in a PP (except hose headed by the 
preposition a 'to') 
(14) Con Juan/ loj compr6. 
with Juan/ itj bought 
'I bought it with Juan.' 
the NP is located more than three constituents before the clitic 
pronoun in the same clause 
(15) En casai \[el martillo\]j no se loj di. 
at home/ \[the hammer\]j not him itj gave 
'I didn't give him the hammer at home.' 
550 
Palomar et al Anaphora Resolution in Spanish Texts 
. 
(a) 
(17) 
(b) 
In this example, the direct object el martillo 'the hammer '  has 
been moved from its common position after the verb, and it is 
necessary to fill the resulting gap with the pronoun lo 'it' even 
though it does not appear in the English translation. This 
phenomenon 5 can be considered an exception to the c-command 
constraints as formulated by Reinhart when applied to Spanish 
clitic pronouns. 
Moreover, if the last two conditions are not fulfilled by the NP and the 
verb is in the first or second person, then this NP will necessarily be the 
solution of the pronoun: 
(16) \[El boligrafo\]i 1Oi comprar~s en esa tienda. 
\[The pen\]/ iti will buy in that shop 
'You will buy the pen in that shop.' 
Personal and demonstrative (nonclitic) pronouns are noncoreferential 
when the NP is in the same clause as the anaphor, and: 
the pronoun comes before the verb (in full parsing, this would 
mean that it is the subject of its clause) 
Ante Luisi 61j salud6 a Pedrok. 
in front of Luisi hey greeted to Pedrok 
'He greeted Pedro in front of Luis.' 
the pronoun comes after the verb (in full parsing, this would 
mean that it is the object of the verb) and the NP is not included 
in another NP 
(18) \[El padre de Juanj\]i le venci6 a 41j. 
\[Juanj's father\]/ him beat to himj 
'Juan's father beat him.' 
In this example, the pronoun ~I 'him' cannot corefer with the NP 
el padre de Juan 'Juan's father', but it can corefer with Juan since it 
is a modifier of the NP el padre de Juan. 
It should be mentioned that the clitic pronoun le is another 
form of the pronoun dl 'him'. This is a typical phenomenon i
Spanish, where clitic pronouns occupy the object position. 
Sometimes both the clitic pronoun and the object appear in the 
same clause, as occurs in the previous example and in the 
following one: 
(19) A Pedro/ yo lei vi ayer. 
to Pedroj I himi saw yesterday 
'I saw Pedro yesterday.' 
This example also illustrates the previously mentioned exception 
of c-command constraints for Spanish clitic pronouns. In this 
case, the direct object a Pedro 'to Pedro' has been moved before 
the verb, and the clitic pronoun le 'him' has been added. It 
should also be remarked that, as noted earlier, the clitic pronoun 
does not appear in the English translation. 
5 Mathews (1997) calls this phenomenon "clitic doubling" and defines it as the use of a clitic pronoun 
with the same referent and in the same syntactic function as another element in the same clause. 
551 
Computational Linguistics Volume 27, Number 4 
(c) the pronoun is included in a PP that is not included in another 
constituent and the NP is not included in another constituent 
(NP or PP) 
(20) \[El padre de Luisj\]i juega con 61j. 
\[Luisj's father\]/ plays with himj 
'Luis's father plays with him.' 
In this example, the pronoun ~I 'him' is included in a PP (which 
is not included in another constituent) and the NP el padre de 
Luis is not included in another NP or PP. Therefore, the NP 
cannot corefer with the pronoun. However, the NP Luis can 
corefer because it is included in the NP el padre de Luis. 
(d) the pronoun is included in an NP, so that the NP in which the 
pronoun is included cannot corefer with the pronoun 
(21) Pedro/ vio \[al hermano de ~li\] j. 
Pedro/ saw \[the brother of himi\]j 
'Pedro saw his brother.' 
(e) the pronoun is coordinated with other NPs, so that the other 
coordinated NPs cannot corefer with the pronoun 
(22) Juan/, \[el tio de Ana\]j, y 61k fueron de pesca. 
Juan/, \[Ana's uncle\]j, and hek went fishing 
'He, Juan, and Ana's uncle went fishing.' 
(f) the pronoun is included in a relative clause, and the following 
condition is met: 
. 
(24) 
i. the NP in which the relative clause is included does not 
corefer with the pronoun 
(23) Pedroj vio a \[un amigo que juega con 41j\]i. 
Pedroj saw to \[a friend that plays with himj\]i 
'Pedro saw a friend that he plays with.' 
ii. the NPs that are included in the relative clause follow 
the previous conditions 
iii. the remaining NPs outside the relative clause could 
corefer with the pronoun 
Personal and demonstrative (nonclitic) pronouns are noncoreferential 
when the NP is not in the same clause as the pronoun. (In this case, the 
NP can corefer with the pronoun, except when this NP also appears in 
the same sentence and clause as the pronoun, in which case it will have 
been discarded by the previous noncoreference onditions.) 
Anaj y Evai son amigas. Evai lej ayuda mucho. 
Anay and Evai are friends Evai herj helps a lot 
'Ana and Eva are friends. Eva helps her a lot.' 
It is important o note that the above-mentioned conditions refer to those coor- 
dinated NPs and PPs that have been partially parsed. Moreover, as previously men- 
tioned, NPs can include relative clauses, appositives, coordinated PPs, and adjectives. 
552 
Palomar et al Anaphora Resolution in Spanish Texts 
We should also remark that we consider aconstituent A to be included in a constituent 
B if A modifies the head of B. Let us consider the following NP: 
(25) \[el hombre que ama a \[una mujer que lei ama\]j\]i 
\[the man who loves to \[a woman who him/ loveslj\]i 
'the man who loves a woman who loves him.' 
We consider that the pronoun le 'him' is included in the relative clause that mod- 
ifies the NP una mujer que le ama 'a woman who loves him', which then cannot corefer 
with it due to noncoreference ondition 3(f)i. Under condition 3(f)iii, however, the 
pronoun le 'him' could corefer with the entire NP el hombre que area a una mujer que le 
area 'the man who loves a woman who loves him'. 
Another example might be the following: 
(26) Eva/ tiene \[un tio que lei toma el pelo\]j. 
Evai has \[an uncle that heri teases\]j 
'Eva has an uncle who teases her.' 
In this example, the pronoun is included within the relative clause that modifies un 
tio 'an uncle', and therefore cannot corefer with it. But, following condition 3(f)iii, it 
can corefer with Eva. 
3.4 Preferences 
To obtain the different sets of preferences, we utilized the training corpus to identify 
the importance of each kind of knowledge that is used by humans when tracking 
down the NP antecedent of a pronoun. Our results are shown in Table 1. For our 
analysis, the antecedents for each pronoun in the text were identified, along with their 
configurational characteristics with reference to the pronoun. Thus, the table shows 
how often each configurational characteristic is valid for the solution of a particular 
pronoun. For example, the solution of a reflexive pronoun is a proper noun 53% of the 
time. The total number of pronoun occurrences in the study was 575. Thus, we were 
able to define the different patterns of Spanish pronoun resolution and apply them in 
order to obtain the evaluation results that are presented in this paper. The order of 
importance was determined by first sorting the preferences according to the percentage 
of each configurational characteristic; that is, preferences with higher percentages were 
applied before those with lower percentages. After several experiments on the training 
corpus, an optimal order--the one that produced the best performance--was obtained. 
Since in this evaluation phase we processed texts from different genres and by different 
authors, we can state that the final set of preferences obtained and their order of 
application can be used with confidence on any Spanish text. 
Based on the results presented in Table 1, we have extracted a set of preferences for 
each type of anaphora (listed below). We have distinguished between those pronouns 
that are included within PPs and those that are not. That is because when a pronoun 
is included in a PP, the preposition of this PP sets a preference. 
Preferences of omitted pronouns (OPR): 
1. NPs that are not of time, direction, quantity, or abstract type; that is to 
say, inanimate candidates are rejected (e.g., hal~past en, Market Street, 
three pounds, or a thing) 
2. NPs in the same sentence as the omitted pronotm 
553 
Computat ional  Linguistics Volume 27, Number  4 
Table 1 
Percentage validity of types of pronouns for different configuration characteristics of the 
training corpus (n = 575). 
CPPR RPR OPR PPRinPP DPRinPP PPRnotPP DPRnotPP 
Intrasentential 66 97 57 70 100 60 75 
Intersentential 34 3 43 30 0 40 25 
NPSentAnt ~ 9 3 4 16 50 9 38 
AntPPin b 7 9 14 27 50 20 25 
AntProper c 57 53 63 35 0 43 0 
AntIndef a 13 0 7 0 0 6 13 
AntRepeaff 72 66 79 65 50 71 50 
AntWithVerb f 14 94 20 24 0 26 25 
EqualPP g 100 100 100 78 100 97 100 
EqualPosVerb h 79 84 89 46 0 86 38 
BeforeVerb i 83 91 89 65 50 86 13 
NoTime d 100 100 100 100 100 100 100 
NoQuant i ty  k 100 100 100 100 100 97 100 
NoDirect ion I 100 100 100 97 100 100 100 
NoAbstract m 100 100 100 100 100 100 100 
NoCompany n 100 100 100 100 100 100 100 
a If the NP 
b If the NP 
c If the NP 
d If the NP 
e If the NP 
f If the NP 
g If the NP 
h If the NP 
i If the NP 
j If the NP 
k If the NP 
1 If the NP 
m If the NP 
n If the NP 
is included in another NP 
is included in a PP with the preposition en 'in' 
is a proper noun 
is an indefinite NP 
has been repeated more than once in the text 
has appeared with the verb of the anaphor more than once in the text 
has appeared in a PP more than once in the text 
occupies the same position with reference to the verb as the anaphor (before or after) 
appears before its verb 
is not a time-type 
is not a quantity-type 
is not a direction-type 
is not an abstract-type 
is not a company-type 
3. NPs  that  are in the same sentence  as the  anaphor  and  are also the  
so lu t ion  for  another  omi t ted  pronotm 
4. NPs  that  are in the prev ious  sentence  
5. NPs  that  are not  inc luded  in another  NP  (e.g., when they  appear  ins ide  
a re la t ive  c lause  or  appos i t i ve )  
6. NPs  that  are not  inc luded  in a PP or  are  inc luded  in a PP  when its 
p repos i t ion  is a ' to '  or  de ' o f '  
7. NPs  that  appear  be fore  the  verb  
8. NPs  that  have  been  repeated  more  than  once  in the  text  
Preferences of clitic personal pronouns (CPPR): 
1. NPs  that  are not  of  t ime,  d i rect ion ,  quant i ty ,  or  abst ract  type  
2. NPs  that  are in  the  same sentence  as the  anaphor  
554 
Palomar et al Anaphora Resolution in Spanish Texts 
3. NPs that are in the previous sentence 
4. NPs that are not included in another NP (e.g., when they appear inside 
a clause or appositive) 
5. NPs that are not included in a PP or are included in a PP when its 
preposit ion is a 'to' or de 'of '  
6. NPs that have appeared with the verb of the anaphor more than once 
Preferences of personal and demonstrative pronouns that are included in a PP 
(PPRinPP and DPRinPP): 
1. NPs that are not of time, direction, quantity, or abstract ype; moreover, 
in the case of personal pronouns, the NP cannot be a company type 
2. NPs that are in the same sentence as the anaphor 
3. NPs that are in the previous sentence 
4. NPs that are not included in another NP (e.g., when they appear inside 
a relative clause or appositive) 
5. NPs that have been repeated more than once in the text 
6. NPs that are included in a PP 
7. NPs that occupy the same position (before or after) with respect o the 
verb as the anaphor 
Preferences of personal and demonstrative pronouns that are not included in a PP 
and of reflexive pronouns (PPRnotPP, DPRnotPP, and RPR): 
1. NPs that are not of time, direction, quantity, or abstract ype; moreover, 
in the case of personal pronouns, the NP cannot be a company type 
2. NPs that are in the same sentence as the anaphor 
3. NPs that are in the previous sentence 
4. NPs that are not included in another NP (e.g., when they appear inside 
a relative clause or appositive) 
5. NPs that are not included in a PP or that are included in a PP when its 
preposit ion is a 'to' or de 'of '  
6. For the case of personal pronouns (PPRnotPP), NPs that are not 
included in a PP with the preposit ion en ' in' 
7. NPs that appear before their verbs (i.e., the verb of the sentence in 
which the NP appears) 
3.5 Resolution Procedure 
The resolution procedure consists of the following steps: 
1. Identify the type of anaphora: pronominal  (PPRinPP or PPRnotPP), 
demonstrat ive (DPRinPP or DPRnotPP), reflexive (RPR), or omitted 
(oPR). 
555 
Computational Linguistics Volume 27, Number 4 
2. Identify the NP candidate antecedents of a pronoun in order to create a 
list L. The list created will depend on the type of anaphor and the 
anaphoric accessibility space (empirically obtained from a deep study 
of the training corpus) and will be developed according to the 
following criteria: 
? For pronominal anaphora, demonstrative anaphora, and 
omitted pronouns, NP candidates will appear in the same 
sentence as the anaphor and in the four previous sentences. 
? For reflexive anaphora, NP candidates will appear in the same 
sentence as the anaphor. 
3. Apply constraints to L to obtain LI: 
(a) morphological agreement 
(b) syntactic onditions on NP-pronoun noncoreference 
4. If the number of elements of L1 - 1, then the solution is that element. 
5. If the number of elements of L1 = 0, then the solution is an exophor. 
6. If the number of elements of L1 > 1, then apply preferences to L1 to 
obtain L2. Depending on the type of anaphora, a different set and order 
of preferences will be applied (see Section 3.4). 
7. If the number of elements of L2 = 1, then the solution is that element. 
8. If the number of elements of L2 > 1, then apply the following three 
basic preferences in the order shown until only one candidate remains 
(these three preferences are common to all the pronouns): 
? NPs most repeated in the text 
? NPs that have appeared most with the verb of the anaphor 
? the first candidate of the remaining list (the closest one to the 
anaphor) 
After applying these basic preferences, the antecedent is obtained. 
4. Empirical Evaluation 
4.1 Description of Corpora 
We have tested the algorithm on both technical manuals and literary texts. In the first 
instance, we used a portion of the Spanish edition of the Blue Book corpus. 6 This 
corpus contains the handbook of the International Telecommunications Union CCITT, 
published in English, French, and Spanish; it is one of the most important collections of 
telecommunications texts available and contains 5,000,000 words automatically tagged 
by the Xerox tagger. In the second instance, the algorithm was tested on Lexesp, a 
corpus 7 that contains Spanish literary texts from different genres and by different 
6 CRATER (Proyecto CRATER 1994-1995) Corpus Resources and Terminology Extraction Project. Project 
supported by the European Community Commission (DG-XIII). Computational Linguistics Laboratory, 
Faculty of Philosophy and Fine Arts, Autonomous University of Madrid, Spain. 
7 The Lexesp corpus belongs to the project of the same name carried out by the Psychology Department 
of the University of Oviedo and developed by the Computational Linguistics Group of the University 
of Barcelona, with the collaboration fthe Language Processing Group of the Catalonia University of 
Technology, Spain. 
556 
Palomar et al Anaphora Resolution in Spanish Texts 
Table 2 
Pronoun occurrences in two types of texts. 
Total BB Corpus Lexesp Corpus 
Number of pronoun occurrences 
in the training corpus 575 123 
Number of pronoun occurrences 
in the test corpus 1,677 375 
452 
1,302 
authors. These texts were mainly obtained from newspapers and were automatically 
tagged by a different agger than the one used to tag the Blue Book. The portion of 
the Lexesp corpus that we processed contained various stories, related by a narrator, 
and written by different authors. As was the case for the Blue Book corpus, this 
corpus also contained 5,000,000 words. Since we worked on texts from different genres 
and by different authors, the applicability of our proposal to other kinds of texts is 
assured. 
We selected a subset of the Blue Book corpus and another subset of the Lex- 
esp corpus, and both were annotated with respect o coreference. One portion of the 
coreferentially tagged corpus (training corpus) was used for improving the rules for 
anaphora resolution (constraints and preferences), and another portion was reserved 
for test data (Table 2). 
The annotation phase was accomplished in the following manner: (1) two annota- 
tors were selected, (2) an agreement was reached between the annotators with regard 
to the annotation scheme, (3) each annotator annotated the corpus, and, finally, (4) a 
reliability test (Carletta et al 1997) was done on the annotation in order to guaran- 
tee the results. The reliability test used the kappa statistic that measures agreement 
between the annotations of two annotators in making judgments about categories. In 
this way, the annotation is considered a classification task consisting of defining an ad- 
equate solution among the candidate list. According to Vieira (1998), the classification 
task when tagging anaphora resolution can be reduced to a decision about whether 
each candidate is the solution or not. Thus, two different categories are considered 
for each anaphor: one for the correct antecedent and another for nonantecedents. Our 
experimentation showed one correct antecedent among an average of 14.5 possible 
candidates per anaphor after applying constraints. For computing the kappa statistic 
(k), see Siegel and Castellan (1988). 
According to Carletta et al (1997), a k measurement such as 0.68 < k < 0.8 allows 
us to draw encouraging conclusions, and a measurement k > 0.8 means there is to- 
tal reliability between the results of the two annotators. In our tests, we obtained a 
kappa measurement of k = 0.81. We therefore consider the annotation obtained for the 
evaluation to be totally reliable. 
4.2 Experimental Work 
We conducted a blind test over the entire test corpus of unrestricted Spanish texts by 
applying the algorithm to the partial syntactic structure generated by the slot unifica- 
tion parser. 
Over these corpora, our algorithm attained a success rate for anaphora resolution 
of 76.8%. We define "success rate" as the number of pronouns successfully resolved, 
divided by the total number of resolved pronouns. The total number of resolved pro- 
nouns was 1,677, including personal, demonstrative, reflexive, and omitted pronouns. 
557 
Computational Linguistics Volume 27, Number 4 
Table 3 
Results of blind test. 
CPPR RPR OPR PPRinPP DPRinPP PPRnotPP DPRnotPP Total 
Num. of 
pronoun 
occurrences 228 80 1,099 107 20 94 49 1,677 
Num. of 
cases 
correctly 162 74 868 70 17 64 34 1,289 
resolved 
Success 
rate 71.0% 92.5% 78.9% 65.4% 85.0% 68.0% 69.3% 76.8% 
All of them were in the third person, with a noun phrase that appeared before the 
anaphor as their antecedent. Our algorithm's "recall percentage," defined as the num- 
ber of pronouns correctly resolved, divided by the total number of pronouns in the 
text, was therefore 76.8%. A breakdown of success rate results for each kind of pro- 
noun is also shown in Table 3. The pronouns were classified so as to provide the 
option of applying different kinds of knowledge to resolve each category of pronoun. 
One of the factors that affected the results was the complexity of the Lexesp corpus, 
due mainly to its complex narratives. On average, 16 words per sentence and 27 
candidates per anaphor were found in this corpus. 
In our experiment, a "successful resolution" occurred if the head of the solution 
offered by our algorithm was the same as that offered by two human experts. We 
adopted this definition of "success" because it allowed the system to be totally auto- 
matic: solutions given by the annotators were stored in a file and were later automat- 
ically compared with the solutions given by our system. Since semantic information 
was not used at all, PP attachments were not always correctly disambiguated. Hence, 
at times the differences imply corresponded to different subconstituents. 
After the evaluation process, we tested the results in order to identify the lim- 
itations of the algorithm with respect to the resolution process. We identified the 
following: 
? There were some mistakes in the POS tagging (causing an error rate of 
around 3%). 
? There were some mistakes in the partial parsing with respect o the 
identification of complex noun phrases (causing an error rate of around 
7%) (Palomar et al 1999). 
? Semantic information was not considered (causing an error rate of 
around 32%). An example of this type of error can be seen in the 
following text extracted from the Lexesp corpus: 
(27) Recuerdo, pot ejemplo, \[un pequefio claro en un bosque en 
medio de las montafias canadienses\]i, con tres lagunas diminutas 
que, a causa de los sedimentos del agua. tenfan distintos y chocantes 
colores. Esta rareza habia hecho del sitioi un espacio sagrado al que 
peregrinaron los indios durante siglos y seguramente antes los 
pobladores paleolfticos. Y eso se notaba. 
558 
Palomar et al Anaphora Resolution in Spanish Texts 
(28) 
Canad~i es un pals muy hermoso, y aqu41i no era, ni mucho 
rnenos, el lugar m~s bello: pero guardaba tranquilamente d ntro de sf 
toda su arrnonfa, como los melocotones guardan dentro de sf el duro 
hueso. 
'1 remember, for example, \[a small clearing in the woods in the 
middle of the Canadian mountains\]/, with three tiny lagoons that, 
due to the water sediments, had different and astonishing colors. 
This peculiarity had made the place/into a sacred site, to which the 
Indians made pilgrimages over the centuries, and surely even the 
Paleolithic Indians before them. And you could feel it. 
'Canada is a very beautiful country and that one/was by no 
means the most beautiful place: but it calmly kept within itself all of 
its harmony, like peaches that keep the hard seeds within.' 
In this text, the demonstrative pronoun aqudl 'that one' corefers with the 
antecedent un peque~o claro en un bosque n medio de las monta~as canadienses 
'a small clearing in the woods in the middle of the Canadian mountains', 
which is also linked to the definite noun phrase el sitio 'the place'. Our 
algorithm identified the proper noun Canadd, which is in the same 
sentence, as the anaphor, since the proper noun could only have been 
discarded by means of semantic information. 
As an example of an anaphor that was correctly resolved by the 
algorithm, we present he following sentence xtracted from the Blue 
Book corpus. In this case, the antecedent los sistemas de transmisidn 
analdgica 'the systems of analogue transmission' was correctly chosen for 
the personal pronoun ellos 'them': ' 
En las conexiones largas o de Iongitud media, es probable que la 
fuente principal de ruido de circuito estribe en \[los sistemas de 
transmisi6n anal6gica\]i, ya queen ellosi la potencia de ruido suele 
set proporcional  la Iongitud del circuito. 
'In long or medium connections, it is probable that the main source of 
circuit noise comes from \[the systems of analogue transmission\]/, 
since in them/the noise capacity is usually proportional to the length 
of the circuit.' 
The remainder of the errors were due to split antecedents (10%), 
cataphora (2%), exophora (3%), or exceptions in the application of 
preferences (43%). 
5. Comparison with Other Approaches to Anaphora Resolution 
5.1 Anaphora Resolution Approaches 
Common among all languages i the fact that the anaphora phenomenon requires im- 
ilar strategies for its resolution (e.g., pronouns or definite descriptions). All languages 
employ different kinds of knowledge, but their strategies differ only in the manner by 
which this knowledge is coordinated. For example, in some strategies just one kind 
of knowledge becomes the main selector for identifying the antecedent, with other 
kinds of knowledge being used merely to confirm or reject the proposed antecedent. 
In such cases, the typical kind of knowledge used as the selector is that of discourse 
structure. Centering theory, as employed by Strube and Hahn (1999) or Okumura and 
Tamura (1996), uses this type of approach. Other approaches, however, give equal 
559 
Computational Linguistics Volume 27, Number 4 
importance to each kind of knowledge and generally distinguish between constraints 
and preferences (Baldwin 1997; Lappin and Leass 1994; Carbonell and Brown 1988). 
Whereas constraints tend to be absolute and therefore discard possible antecedents, 
preferences tend to be relative and require the use of additional criteria (e.g., the use of 
heuristics that are not always satisfied by all antecedents). Nakaiwa and Shirai (1996) 
use this sort of resolution model, which involves the use of semantic and pragmatic 
constraints, such as constraints based on modal expressions, or constraints based on 
verbal semantic attributes or conjunctions. 
Our approach to anaphora resolution belongs in the latter category, since it com- 
bines different kinds of knowledge and no knowledge based on discourse structure 
is included. We choose to ignore discourse structure because obtaining this kind of 
knowledge requires not only an understanding of semantics but also knowledge about 
world affairs and the ability to almost perfectly parse any text under discussion (Az- 
zam, Humphreys, and Gaizauskas 1998). 
Still other approaches to anaphora resolution are based either on machine learn- 
ing techniques (Connolly, Burger, and Day 1994; Yamamoto and Sumita 1998; Paul, 
Yamamato, and Sumita 1999) or on the principles of uncertainty reasoning (Mitkov 
1995). 
Computational processing of semantic and domain information is relatively expen- 
sive when compared with other kinds of knowledge. Consequently, current anaphora 
resolution methods rely mainly on constraint and preference heuristics, which employ 
morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov 
\[1998\]). Such approaches have performed notably well. Lappin and Leass (1994) de- 
scribe an algorithm for pronominal anaphora resolution that achieves a high rate of 
correct analyses (85%). Their approach, however, operates almost exclusively on syn- 
tactic information. More recently, Kennedy and Boguraev (1996) proposed an algorithm 
for anaphora resolution that is actually a modified and extended version of the one 
developed by Lappin and Leass (1994). It works from the output of a POS tagger and 
achieves an accuracy rate of 75%. 
There are other approaches based on POS tagger outputs as well. For example, 
Mitkov and Stys (1997) propose a knowledge-poor approach to resolving pronouns 
in technical manuals in both English and Polish. The knowledge mployed in these 
approaches i limited to a small noun phrase grammar, a list of terms, and a set of 
antecedent indicators (definiteness, term preference, lexical reiteration, etc.). 
Still other approaches are based on statistical information, including the work of 
Dagan and Itai (1990, 1991) and Ge, Hale, and Charniak (1998), all of whom present a 
probabilistic model for pronoun resolution. 
We have adopted their ideas and adapted their algorithms to partial parsing and 
to Spanish texts in order to compare our results with their approaches. 
With reference to the differences between English and Spanish anaphora resolu- 
tion, we have made the following observations: 
Syntactic parallelism has played a more important role in English texts 
than in Spanish texts, since Spanish sentence structure is more flexible 
than English sentence structure. Spanish is a free-word-order language 
and has different syntactic onditions, which increases the difficulty of 
resolving Spanish pronouns (hence, the greater accuracy rate for English 
texts). 
? A greater number of possible antecedents was observed for Spanish 
pronouns than for English pronouns, due mainly to the greater average 
560 
Palomar et al Anaphora Resolution in Spanish Texts 
length of Spanish sentences (which also makes the resolution of Spanish 
pronouns more difficult). 
Spanish pronouns usually bear more morphological information. One 
result is that this constraint tends to discard more candidates in Spanish 
than in English. 
For comparison purposes, we implemented the following approaches on the same 
Spanish texts that were tested and described in Section 4.1. 
5.2 Hobbs's Algorithm 
Hobbs's algorithm (Hobbs 1978) is applied to the surface parse trees of sentences in 
a text. A surface parse tree represents the grammatical structure of a sentence. By 
reading the leaves of the parse tree from left to right, the original English sentence is
formed. The algorithm parses the tree in a predefined order and searches for a noun 
phrase of the correct gender and number. Hobbs tested his algorithm for the pronouns 
he, she, it, and they, using 100 examples taken from three different sources. Although 
the algorithm is very simple, it was successful 81.8% of the time. 
We implemented a version of Hobbs's algorithm for slot unification grammar for 
Spanish texts. Since full parsing was not done, our specifications for the algorithm 
were adjusted, as follows: 
? NPs were tested from left to right, as they were parsed in the sentence. 
? Afterward, the NPs that were included in an NP (breadth-first) were 
tested. 
? This test was interrupted when an NP agreed in gender and number 
with the anaphor. 
The problems we encountered in implementing Hobbs's algorithm are similar to 
those found in implementing other approaches: the adaptation to partial parsing, and 
the inherent difficulty of the Spanish language (i.e., its free-word-order characteristics). 
The results of our test of this version of Hobbs's algorithm on the test corpus 
appear in Table 4. 
5.3 Approaches Based on Constraints and Proximity Preference 
Our approach as also been compared with the typical baseline approach consisting of 
constraints and proximity preference; that is, the antecedent that appears closest o the 
anaphor is chosen from among those that satisfy the constraints. For this comparison, 
the same constraints that were used previously (i.e., morphological greement and 
syntactic onditions) were applied here. Then the antecedent a the head of the list of 
antecedents was proposed as the solution of the anaphor. These results are also listed 
in Table 4. As can be seen from the table, success rates were lower than those obtained 
through the joint application of all the preferences. 
5.4 Lappin and Leass's Algorithm 
An algorithm for identifying the noun phrase antecedents of third person pronouns 
and lexical anaphors (reflexive and reciprocal) is presented in Lappin and Leass (1994); 
this algorithm has exhibited a high rate (85%) of correct analyses in English texts. It 
relies on measures of salience that are derived from syntactic structures and on simple 
dynamic models of attentional state to select he antecedent oun phrase of a pronoun 
from a list of candidates. 
561 
Computational Linguistics Volume 27, Number 4 
We have implemented a version of Lappin and Leass's algorithm for Spanish texts. 
The original formulation of the algorithm proposes a syntactic filter on NP-pronoun 
coreference. This filter consists of six conditions for NP-pronoun oncoreference within 
any sentence (Lappin and Leass 1994, page 537). In applying this algorithm to Span- 
ish texts, we changed these conditions o as to capture the appropriate context. As 
mentioned previously, our algorithm does not have access to full syntactic knowledge. 
Accordingly, we employed partial parsing over the text in our application of Lappin 
and Leass's algorithm. The salience parameters were weighted (weight appears in 
parentheses) and applied in the following way: 
? Sentence recency (100): Applied when the NP appeared in the same 
sentence as the anaphor. 
? Subject emphasis (80): Applied when the NP was located before the 
verb of the clause in which it appeared. This heuristic was necessary 
because of our algorithm's lack of syntactic knowledge. It should be 
noted, however, that since Spanish is a nearly free-word-order language 
and the exchange of subject and object positions within Spanish 
sentences i common, the heuristic is often invalid. For example, the two 
Spanish sentences Pedro compr6 un regalo 'Pedro bought a present' and Un 
regalo compr6 Pedro 'A present bought Pedro' are equivalent to one 
another and to the English sentence Pedro bought a present. 
? Existential emphasis (70): In this instance, we applied the parameter in 
the same way as Lappin and Leass, since the entire NP was fully parsed, 
which allowed us to tell when it was a definite or an indefinite NP. 
? Accusative emphasis (50): Applied when the NP appeared after the verb 
of the clause in which it appeared and the NP did not appear inside 
another NP or PP. For example, in the sentence Pedro encontr6 el libro de 
Juana 'Pedro found Juana's book', a value was assigned to el libro de Juana 
'Juana's book' but not to Juana. Once again, it should be noted that this 
heuristic was necessary because of our algorithm's lack of syntactic 
knowledge. 
? Indirect object and oblique complement emphasis (40): Applied when 
the NP appeared in a PP with the Spanish preposition a 'to', which 
usually preceded the indirect object of its sentence. 
? Head noun emphasis (80): Applied when the NP was not contained in 
another NP. 
? Nonadverbial emphasis (50): Applied when the NP was not contained 
in an adverbial PP. In this case, its application depended on the kind of 
preposition in which the NP was included. 
? Parallelism reward (35): Applied when the NP occupied the same 
position as the anaphor with reference to the verb of the sentence (before 
or after the verb). 
Finally, we followed Lappin and Leass in assigning the additional salience value 
to NPs in the current sentence and in degrading the salience of NPs in preceding 
sentences. 
Our results exhibited some similarities with Lappin and Leass's experiments. 
For example, anaphora was strongly preferred over cataphora, and both approaches 
562 
Palomar et al Anaphora Resolution in Spanish Texts 
preferred intrasentential NPs to intersentential ones. These results can be seen in 
Table 4. 
5.5 Centering Approach 
The centering model proposed by Grosz, Joshi, and Weinstein (1983, 1995) provides 
a framework for modeling the local coherence of discourse. The model has two con- 
structs, a list of forward-looking centers and a backward-looking center, that can be 
assigned to each utterance Ui. The list of forward-looking centers Cf(Ui) ranks dis- 
course entities within the utterance Ui. The backward-looking center Cb(Ui+l) con- 
stitutes the most highly ranked element of Cf(Ui) that is finally realized in the next 
utterance Ui+l. In this way, the ranking imposed over Cf(Ui) must reflect he fact that 
the preferred center Cp(U/) (i.e., the most highly ranked element of Cf(Ui)) is most 
likely to be Cb(Ui+l). 
The ranking criteria used by Grosz, Joshi, and Weinstein (1995) order items in 
the Cf list using grammatical roles. Thus, entities with a subject role are preferred to 
entities with an object role, and objects are preferred to others (adjuncts, etc.). 
Grosz, Joshi, and Weinstein (1995) state that if any element of Cf(Ui) is realized 
by a pronoun in Ui+l, then Cb(Ui+l) must also be realized by a pronoun. 
Brennan, Friedman, and Pollard (1987) applied the centering model to pronoun 
resolution. They based their algorithm on the fact that centering transition relations 
will hold across adjacent utterances. 
Moreover, one crucial point in centering is the ranking of the forward-looking 
centers. Grosz, Joshi, and Weinstein (1995) state that Cf may be ordered using different 
factors, but they only use information about grammatical roles. However, both Strube 
(1998) and Strube and Hahn (1999) point out that it is difficult to define grammatical 
roles in free-word-order languages like German or Spanish. For languages like these, 
they propose other anking criteria dependent upon the information status of discourse 
entities. They claim that information about familiarity is crucial for the ranking of 
discourse ntities, at least in free-word-order languages. 
According to Strube's ranking criteria, two different sets of expressions, hearer- 
old discourse ntities (OLD) and hearer-new discourse ntities (NEW), can be distin- 
guished. OLD discourse ntities consist of evoked entities---coreferring resolved ex- 
pressions (pronominal and nominal anaphora, previously mentioned proper names, 
relative pronouns, appositives)--and unused entities (proper names and titles). The re- 
maining entities are assigned to the NEW set. The basic ranking criteria for pronominal 
anaphora resolution prefer OLD entities over NEW entities. 8 
Strube (1998) thus proposes the following adaptation to the centering model: 
The Cf list is replaced by the list of salient discourse ntities (S-list) 
containing discourse ntities that are realized in the current and previous 
utterance. 
? The elements of the S-list are ranked according to the basic ranking 
criteria and position information: 
If X E OLD and y C NEW, then x precedes y. 
If x, y ~ OLD or x, y E NEW, 
8 To resolve functional naphora,  third set, MED, which includes inferable information, must be added 
between the OLD and the NEW sets. However, this set is not needed to resolve pronominal naphora 
(Strube and Hahn 1999). 
563 
Computational Linguistics Volume 27, Number 4 
Table 4 
Comparative r sults of blind test. 
Total CPPR RPR OPR PPRinPP DPRinPP PPRnotPP DPRnotPP 
Num. of 
pronoun 1,677 228 80 1,099 107 20 94 49 
occurrences 
Hobbs's 
algorithm 62.7% 61% 85% 62% 62% 50% 66% 52% 
Lappin & 
Leass's 67.4% 66% 86% 67% 65% 60% 67% 60% 
algorithm 
Proximity 52.9% 55% 86% 47% 65% 85% 61% 65% 
Centering 
approach 62.6% 60% 85% 62% 61% 60% 62% 58% 
Our 
algorithm 76.8% 71% 92% 79% 65% 85% 68% 69% 
then if utterance(y) precedes utterance(x), then x precedes y, 
if utterance(y) = utterance(x) and pos(x) < pos(y), then x precedes y. 
Since there is not a clear definition of what an utterance is, the following 
criteria are assumed: tensed clauses are defined as utterances on their 
own and untensed clauses are processed with the main clause in order to 
constitute only one utterance. 
Incorporating these adaptations, Strube (1998) then proposes the following algo- 
rithm: 
1. If a referring expression is encountered, 
(a) if it is a pronoun, test the elements of the S-list in order until the 
test succeeds; 
(b) update the S-list using information about this referring 
expression. 
2. If the analysis of utterance U is finished, remove all discourse ntities 
from the S-list that are not realized in U. 
The evaluation of this algorithm was performed in Strube (1998) and obtained a 
precision of 85.4% for English, improving upon the results of the centering algorithm 
by Brennan, Friedman, and Pollard (1987), which achieved only 72.9% precision when 
it was applied to the same corpus. 
Consequently, in adapting the centering model to Spanish anaphora resolution, we 
followed Strube's indications. The success rate of the algorithm was not satisfactory, 
as can be seen in Table 4. 
6. Conclus ions 
In this paper, we have presented an algorithm for identifying noun phrase antecedents 
of third person personal pronouns, demonstrative pronouns, reflexive pronouns, and 
564 
Palomar et al Anaphora Resolution in Spanish Texts 
omitted pronouns in Spanish. The algorithm is applied to the syntactic structure gen- 
erated by the slot unification parser--see Ferrdndez, Palomar, and Moreno (1998a, 
1998b, 1999)--and coordinates different kinds of knowledge (lexical, morphological, 
and syntactic) by distinguishing between constraints and preferences. 
The main contribution ofthis paper is the introduction ofan algorithm for anaphora 
resolution for Spanish. In our work, we have undertaken an exhaustive study of the 
importance of each kind of knowledge in anaphora resolution for Spanish. Moreover, 
we have developed a definition of syntactic onditions of NP-pronoun noncorefer- 
ence in Spanish with partial parsing. We have also adapted our anaphora resolution 
algorithm to the problem of partial syntactic knowledge, that is to say, when partial 
parsing of the text is accomplished. 
For unrestricted texts, our approach is somewhat less accurate, since semantic 
information is not taken into account. For such texts, we are dealing with the output 
of a POS tagger, which does not provide this sort of knowledge. In order to test our 
approach with texts of different genres by different authors, we have worked with 
two different Spanish corpora, literary texts (the Lexesp corpus) and technical texts 
(the Blue Book), containing a total of 1,677 pronoun occurrences. 
The algorithm successfully identified the antecedent of the pronoun for 76.8% 
of these pronoun occurrences. Other algorithms usually work with different kinds 
of knowledge, different texts, and different languages. In order to make a more valid 
comparison of our algorithm with others, we adapted the other algorithms so that they 
would operate using only partial-parsing knowledge. In this evaluation, our algorithm 
has always obtained better esults. 
Moreover, based on the results on our study of the importance of each kind 
of knowledge, we can emphasize that constraints are very important for resolving 
anaphora successfully, since they considerably reduce the number of possible candi- 
dates. 
In future studies, we will attempt to evaluate the importance of semantic informa- 
tion in unrestricted texts for anaphora resolution in Spanish texts (Saiz-Noeda, Su~rez, 
and Peral 1999). This information will be obtained from a lexical tool (e.g., Spanish 
WordNet), which can be automatically consulted (since the tagger does not provide 
this information). 
Acknowledgments 
The authors wish to thank Ferran Pla, 
Natividad Prieto, and Antonio Molina for 
contributing their tagger (Pla 2000); and 
Richard Evans, Mikel Forcada, and Rafael 
Carrasco for their helpful revisions of the 
ideas presented in this paper. We are also 
grateful to several anonymous reviewers of 
Computational Linguistics for helpful 
comments on earlier drafts of this paper. 
Our work has been supported by the 
Spanish government (CICYT) with Grant 
TIC97-0671-C02-01/02. 
References 
Azzam, Saliha, Kevin Humphreys, and 
Robert Gaizauskas. 1998. Evaluating a
focus-based approach to anaphora 
resolution. In Proceedings ofthe 36th Annual 
Meeting of the Association for Computational 
Linguistics and 17th International Conference 
on Computational Linguistics 
(COLING-ACL'98), pages 74-78, Montreal 
(Canada). 
Baldwin, Breck. 1997. CogNIAC: High 
precision coreference with limited 
knowledge and linguistic resources. In
Proceedings of the ACL/EACL Workshop on 
Operational Factors in Practical, Robust 
Anaphora Resolution for Unrestricted Texts, 
pages 38--45, Madrid (Spain). 
Brennan, Susan E., Marilyn W. Friedman, 
and Carl J. Pollard. 1987. A centering 
approach to pronouns. In Proceedings ofthe 
25th Annual Meeting of the Association for 
Computational Linguistics (ACL'87), pages 
155-162, Stanford, CA (USA). 
Carbonell, Jaime G. and Ralf D. Brown. 
1988. Anaphora resolution: A
multi-strategy approach. In Proceedings of
the 12th International Conference on 
565 
Computational Linguistics Volume 27, Number 4 
Computational Linguistics (COLING'88), 
pages 96-101, Budapest (Hungary). 
Carletta, Jean, Amy Isard, Stephen Isard, 
Jacqueline C. Kowtko, Gwyneth 
Doherty-Sneddon, and Anne H. 
Anderson. 1997. The reliability of a 
dialogue structure coding scheme. 
Computational Linguistics, 23(1):13-32. 
Connolly, Dennis, John D. Burger, and 
David S. Day. 1994. A machine learning 
approach to anaphoric reference. In
Proceedings ofthe International Conference on 
New Methods in Language Processing 
(NEMLAP'94), pages 255-261, Manchester 
(UK). 
Dagan, Ido and Alon Itai. 1990. Automatic 
processing of large corpora for the 
resolution of anaphora references. In
Proceedings ofthe 13th International 
Conference on Computational Linguistics 
(COLING'90), pages 330-332, Helsinki 
(Finland). 
Dagan, Ido and Alon Itai. 1991. A statistical 
filter for resolving pronoun references. In
Yishai A. Feldman and Alfred Bruckstein, 
editors, Artificial Intelligence and Computer 
Vision. Elsevier Science Publishers B. V. 
(North-Holland), Amsterdam, pages 
125-135. 
Ferrlindez, Antonio, Manuel Palomar, and 
Lidia Moreno. 1998a. A computational 
approach to pronominal anaphora, 
one-anaphora and surface count 
anaphora. In Proceedings ofthe Second 
Colloquium on Discourse Anaphora nd 
Anaphora Resolution (DAARC'98), pages 
117-128, Lancaster (UK). 
Ferr~ndez, Antonio, Manuel Palomar, and 
Lidia Moreno. 1998b. Anaphora 
resolution in unrestricted texts with 
partial parsing. In Proceedings ofthe 36th 
Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics (COLING-ACL'98), pages 
385-391, Montreal (Canada). 
Ferr~fndez, Antonio, Manuel Palomar, and 
Lidia Moreno. 1999. An empirical 
approach to Spanish anaphora resolution. 
Machine Translation, 14(3/4):191-216. 
Ferr~indez, Antonio and Jestis Peral. 2000. A 
computational pproach to zero-pronouns 
in Spanish. In Proceedings ofthe 38th 
Annual Meeting of the Association for 
Computational Linguistics (ACL'O0), pages 
166-172, Hong Kong (China). 
Ge, Niyu, John Hale, and Eugene Charniak. 
1998. A statistical approach to anaphora 
resolution. In Proceedings ofthe Sixth 
Workshop on Ven d Large Corpora, pages 
161-170, Montreal (Canada). 
Grosz, Barbara, Aravind Joshi, and Scott 
Weinstein. 1983. Providing a unified 
account of definite noun phrases in 
discourse. In Proceedings ofthe 21st Annual 
Meeting of the Association for Computational 
Linguistics (ACL'83), pages 44-50, 
Cambridge, MA (USA). 
Grosz, Barbara, Aravind Joshi, and Scott 
Weinstein. 1995. Centering: A framework 
for modeling the local coherence of 
discourse. Computational Linguistics, 
21(2):203-225. 
Hobbs, Jerry R. 1978. Resolving pronoun 
references. Lingua, 44:311-338. 
Kennedy, Christopher and Branimir 
Boguraev. 1996. Anaphora for everyone: 
Pronominal anaphora resolution without 
a parser. In Proceedings ofthe 16th 
International Conference on Computational 
Linguistics (COLING'96), pages 113-118, 
Copenhagen (Denmark). 
Lappin, Shalom and Herbert Leass. 1994. 
An algorithm for pronominal anaphora 
resolution. Computational Linguistics, 
20(4):535-561. 
Mathews, Peter H. 1997. The Concise Oxford 
Dictionary of Linguistics. Oxford University 
Press, Oxford (UK). 
Mitkov, Ruslan. 1995. An uncertainty 
reasoning approach to anaphora 
resolution. In Proceedings ofthe Natural 
Language Pacific Rim Symposium (NLPRS 
"95), pages 149-154, Seoul (Korea). 
Mitkov, Ruslan. 1998. Robust pronoun 
resolution with limited knowledge. In 
Proceedings ofthe 36th Annual Meeting of the 
Association for Computational Linguistics and 
17 th International Conference on 
Computational Linguistics 
(COLING-ACL'98), pages 869-875, 
Montreal (Canada). 
Mitkov, Ruslan and Malgorzata Stys. 1997. 
Robust reference resolution with limited 
knowledge: High precision genre-specific 
approach for English and Polish. In 
Proceedings ofthe International Conference on 
Recent Advances in Natural Language 
Processing (RANLP'97), pages 74-81, 
Tzigov Chark (Bulgaria). 
Nakaiwa, Hiromi and Satoshi Shirai. 1996. 
Anaphora resolution of Japanese zero 
pronouns with deictic reference. In
Proceedings ofthe 16th International 
Conference on Computational Linguistics 
(COLING'96), pages 812-817, Copenhagen 
(Denmark). 
Okumura, Manabu and Kouji Tamura. 1996. 
Zero pronoun resolution in Japanese 
discourse based on centering theory. In 
Proceedings ofthe 16th International 
Conference on Computational Linguistics 
566 
Palomar et al Anaphora Resolution in Spanish Texts 
(COLING'96), pages 871-876, Copenhagen 
(Denmark). 
Palomar, Manuel, Antonio Ferra'ndez, Lidia 
Moreno, Maximiliano Saiz-Noeda, Rafael 
Mu~oz, Patricio Martfnez-Barco, Jestis 
Peral, and Borja Navarro. 1999. A robust 
partial parsing strategy based on the slot 
unification grammars. In Proceedings ofthe 
6th Conference on Natural Language 
Processing (TALN'99), pages 263-272, 
Corsica (France). 
Paul, Michael, Kazuhide Yamamoto, and 
Eiichiro Sumita. 1999. Corpus-based 
anaphora resolution towards antecedent 
preference. In Proceedings ofthe ACL 
Workshop on Coreference and Its Applications, 
pages 47-52, College Park, MD (USA). 
Pla, Ferran. 2000. Etiquetado Ldxico y Andlisis 
Sintdctico Super~'cial Basado en Modelos 
Estadfsticos. Ph.D. thesis, Valencia 
University of Technology, Valencia 
(Spain). 
Proyecto CRATER. 1994-1995. Corpus 
Resources And Terminology ExtRaction. 
MLAP-93/20. http: //www.lllf.uam.es / 
proyectos/crater.html (page visited on 
04/17/01). 
Reinhart, Tanya. 1983. Anaphora nd Semantic 
Interpretation. Croom Hehn Linguistics 
series. Croom Helm Ltd., Beckenham, 
Kent (UK). 
Saiz-Noeda, Maximiliano, Armando Sudrez, 
and Jestis Peral. 1999. Propuesta de 
incorporacidn de informaci6n sem~ntica 
desde Wordnet alandlisis sintdctico 
parcial orientado a la resoluci6n de la 
an~ffora. Procesamiento del Lenguaje Natural, 
25:167-173. 
Siegel, Sidney and John N. Castellan. 1988. 
Nonparametric Statistics for the Behavioral 
Sciences. McGraw-Hill, New York, NY 
(USA), 2nd edition. 
Strube, Michael. 1998. Never look back: An 
alternative to centering. In Proceedings of
the 36th Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics (COLING-ACL'98), pages 
1251-1257, Montreal (Canada). 
Strube, Michael and Udo Hahn. 1999. 
Functional centering: Grounding 
referential coherence in information 
structure. Computational Linguistics, 
25(3):309-344. 
Vieira, Renata. 1998. Processing of Definite 
Descriptions in Unrestricted Texts. Ph.D. 
thesis, University of Edinburgh, 
Edinburgh (UK). 
Yamamoto, Kazuhide and Eiichiro Sumita. 
1998. Feasibility study for ellipsis 
resolution in dialogues by 
machine-learning technique. In 
Proceedings ofthe 36th Annual Meeting of the 
Association for Computational Linguistics and 
17th International Conference on 
Computational Linguistics 
(COLING-ACL'98), pages 385-391, 
Montreal (Canada). 
567 

Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 72?77,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Towards Building a Competitive Opinion Summarization System: 
Challenges and Keys 
 
 
Elena Lloret*, Alexandra Balahur, Manuel Palomar and Andr?s Montoyo 
Department of Software and Computing Systems 
University of Alicante 
Apartado de Correos 99, E-03080, Alicante, Spain 
{elloret, abalahur, mpalomar, montoyo}@dlsi.ua.es 
 
 
 
Abstract 
This paper presents an overview of our participation in 
the TAC 2008 Opinion Pilot Summarization task, as 
well as the proposed and evaluated post-competition 
improvements. We first describe our opinion 
summarization system and the results obtained. Further 
on, we identify the system?s weak points and suggest 
several improvements, focused both on information 
content, as well as linguistic and readability aspects. We 
obtain encouraging results, especially as far as F-
measure is concerned, outperforming the competition 
results by approximately 80%. 
1 Introduction 
The Opinion Summarization Pilot (OSP) task 
within the TAC 2008 competition consisted in 
generating summaries from answers to opinion 
questions retrieved from blogs (the Blog061 
collection). The questions were organized around 
25 targets ? persons, events, organizations etc.  
Additionally, a set of text snippets that contained 
the answers to the questions were provided by the 
organizers, their use being optional. An example of 
target, question and provided snippet is given in 
Figure 1. 
 
 
 
 
 
Figure 1. Examples of target, question and snippet 
 
                                                           
*Elena Lloret is funded by the FPI program (BES-2007-
16268) from the Spanish Ministry of Science and Innovation, 
under the project TEXT-MESS (TIN-2006-15265)  
1http://ir.dcs.gla.ac.uk/test_collections/access_to_data.html 
The techniques employed by the participants were 
mainly based on the already existing 
summarization systems. While most participants 
added new features (sentiment, pos/neg sentiment, 
pos/neg opinion) to account for the presence of 
positive opinions or negative ones - CLASSY 
(Conroy and Schlessinger, 2008); CCNU (He et 
al.,2008);  LIPN (Bossard et al, 2008);  IIITSum08 
(Varma et al, 2008) -, efficient methods were 
proposed focusing on the retrieval and filtering 
stage, based on polarity ? DLSIUAES (Balahur et 
al., 2008) - or on separating information rich 
clauses - italica (Cruz et al, 2008). In general, 
previous work in opinion mining includes 
document level sentiment classification using 
supervised (Chaovalit and Zhou, 2005) and 
unsupervised methods (Turney, 2002), machine 
learning techniques and sentiment classification 
considering rating scales (Pang, Lee and 
Vaithyanathan, 2002), and scoring of features 
(Dave, Lawrence and Pennock, 2003). Other 
research has been conducted in analysing 
sentiment at a sentence level using bootstrapping 
techniques (Riloff and Wiebe, 2003), finding 
strength of opinions (Wilson, Wiebe and Hwa, 
2004), summing up orientations of opinion words 
in a sentence (Kim and Hovy, 2004), and 
identifying opinion holders (Stoyanov and Cardie, 
2006). Finally, fine grained, feature-based opinion 
summarization is defined in (Hu and Liu, 2004).  
2 Opinion Summarization System 
In order to tackle the OSP task, we considered the 
use of two different methods for opinion mining 
and summarization, differing mainly with respect 
to the use of the optional text snippets provided. 
Our first approach (the Snippet-driven Approach) 
Target : George Clooney 
Question: Why do people like George Clooney? 
Snippet 1: 1050 BLOG06-20060125-015-
0025581509 he is a great actor 
72
used these snippets, whereas the second one (Blog-
driven Approach) found the answers directly in the 
corresponding blogs. A general overview of the 
system?s architecture is shown in Figure 2, where 
three main parts can be distinguished: the question 
processing stage, the snippets processing stage 
(only carried out for the first approach), and the 
final summary generation module. Next, the main 
steps involved in each process will be explained in 
more detail.  
 
Figure 2. System architecture 
 
The first step was to determine the polarity of each 
question, extract the keywords from each of them 
and finally, build some patterns of reformulation. 
The latter were defined in order to give the final 
summary an abstract nature, rather than a simple 
joining of sentences. The polarity of the question 
was determined using a set of created patterns, 
whose goal was to extract for further classification 
the nouns, verbs, adverbs or adjectives indicating 
some kind of polarity (positive or negative). These 
extracted words, together with their determiners, 
were classified using the emotions lists in 
WordNet Affect (Strapparava and Valitutti, 2005), 
jointly with the emotions lists of attitudes, triggers 
resource (Balahur and Montoyo, 2008 [1]), four 
created lists of attitudes, expressing criticism, 
support, admiration and rejection and two 
categories for value (good and bad), taking for the 
opinion mining systems in (Balahur and Montoyo, 
2008 [2]). Moreover, the focus of each question 
was automatically extracted using the Freeling2 
Named Entity Recognizer module. This 
information was used to determine whether or not 
all the questions within the same topic had the 
same focus, as well as be able to decide later on 
which text snippet belonged to which question.  
Regarding the given text snippets, we also 
computed their polarity and their focus. The 
                                                           
2
 http://garraf.epsevg.upc.es/freeling/ 
polarity was calculated as a vector similarity 
between the snippets and vectors constructed from 
the list of sentences contained in the ISEAR corpus 
(Scherer and Wallbot, 1997), WordNet Affect 
emotion lists of anger, sadness, disgust and joy and 
the emotion triggers resource, using Pedersen's 
Text Similarity Package.3  
Concerning the blogs, our opinion mining and 
summarization system is focused only on plain 
text; therefore, as pre processing stage, we 
removed all unnecessary tags and irrelevant 
information, such as links, images etc. Further on, 
we split the remaining text into individual 
sentences. A matching between blogs' sentences 
and text snippets was performed so that a 
preliminary set of potential meaningful sentences 
was recorded for further processing. To achieve 
this, snippets not literally contained in the blogs 
were tokenized and stemmed using Porter's 
Stemmer,4 and stop words were removed in order 
to find the most similar possible sentence 
associated with it. Subsequently, by means of the 
same Pedersen Text Similarity Package as for 
computing the snippets' polarity, we computed the 
similarity between the given snippets and this 
created set of potential sentences. We extracted the 
complete blog sentences to which each snippet was 
related. Further on, we extracted the focus for each 
blog phrase sentence as well. Then, we filtered 
redundant sentences using a na?ve similarity based 
approach. Once we obtained the possible answers, 
we used Minipar5 to filter out incomplete 
sentences.  
Having computed the polarity for the questions and 
snippets, and set out the final set of sentences to 
produce the summary, we bound each sentence to 
its corresponding question, and we grouped all 
sentences which were related to the same question 
together, so that we could generate the language 
for this group, according to the patterns of 
reformulation previously mentioned. Finally, the 
speech style was changed to an impersonal one, in 
order to avoid directly expressed opinion 
sentences. A POS-tagger tool (TreeTagger6) was 
used to identify third person verbs and change 
them to a neutral style. A set of rules to identify 
                                                           
3http://www.d.umn.edu/~tpederse/text-similarity.html 
4http://tartarus.org/~martin/PorterStemmer/ 
5http://www.cs.ualberta.ca/~lindek/minipar.htm 
6http://www.ims.uni-tuttgart.de/projekte/corplex/TreeTagger/ 
73
pronouns was created, and they were also changed 
to the more general pronoun ?they? and its 
corresponding forms, to avoid personal opinions.  
3 Evaluation 
Table 1 shows the final results obtained by our 
approaches in the TAC 2008 Opinion Pilot (the 
rank among the 36 participating systems is shown 
in brackets for each evaluation measure). Both of 
our approaches were totally automatic, and the 
only difference between them was the use of the 
given snippets in the first one (A1) and not in the 
second (A2). The column numbers stand for the 
following average scores: summarizerID (1); 
pyramid F-score (Beta=1) (2), grammaticality (3); 
non-redundancy (4); structure/coherence 
(including focus and referential clarity) (5); overall 
fluency/readability (6); overall responsiveness (7). 
 
1 2 3 4 5 6 7 
A1 0.357 
(7) 
4.727 
(8) 
5.364 
(28) 
3.409 
(4) 
3.636 
(16) 
5.045 
(5) 
A2 0.155 
(23) 
3.545 
(36) 
4.364 
(36) 
3.091 
(13) 
2.636 
(36) 
2.227 
(28) 
Table 1. Evaluation results 
 
As it can be noticed from Table 1, our system 
performed well regarding F-measure, the first run 
being classified 7th among the 36 evaluated. As far 
as the structure and coherence are concerned, the 
results were also good, placing the first approach 
in the fourth. Also worth mentioning is the good 
performance obtained regarding the overall 
responsiveness, where A1 ranked 5th. Generally 
speaking, the results for A1 showed well-balanced 
among all the criteria evaluated, except for non 
redundancy and grammaticality.  For the second 
approach, results were not as good, due to the 
difficulty in selecting the appropriate opinion blog 
sentence by only taking into account the keywords 
of the question.  
4 Post-competition tests, experiments 
and improvements 
When an exhaustive examination of the nuggets 
used for evaluating the summaries was done, we 
found some problems that are worth mentioning. 
 
a) Some nuggets with high score did not exist in 
the snippet list (e.g. ?When buying from 
CARMAX, got a better than blue book trade-in 
on old car? (0.9)).  
b) Some nuggets for the same target express the 
same idea, despite their not being identical 
(e.g. ?NAFTA needs to be renegotiated to 
protect Canadian sovereignty? and ?Green 
Party: Renegotiate NAFTA to protect 
Canadian Sovereignty?). 
c) The meaning of one nugget can be deduced 
from another's (e.g. ?reasonably healthy food? 
and ?sandwiches are healthy?). 
d) Some nuggets are not very clear in meaning 
(e.g. ?hot?, ?fun?). 
e) A snippet can be covered by several nuggets 
(e.g. both nuggets ?it is an honest book? and 
?it is a great book? correspond to the same 
snippet ?It was such a great book- honest and 
hard to read (content not language 
difficulty)?). 
 
On the other hand, regarding the use of the 
optional snippets, the main problem to address is to 
remove redundancy, because many of them are 
repeated for the same target, and we have to 
determine which snippet represents better the idea 
for the final summary, in order to avoid noisy 
irrelevant information. 
4.1 Measuring the Performance of a 
Generic Summarization System 
Several participants in the TAC 2008 edition 
performed the OSP task by using generic 
summarization systems. Most were adjusted by 
integrating an opinion classifier module so that the 
task could be fulfilled, but some were not (Bossard 
et al, 2008), (Hendrickx and Bosma, 2008). This 
fact made us realize that a generic summarizer 
could be used to achieve this task. We wanted to 
analyze the effects of such a kind of summarizer to 
produce opinion summaries. We followed the 
approach described in (Lloret et al, 2008). The 
main idea employed is to score sentences of a 
document with regard to the word frequency count 
(WF), which can be combined with a Textual 
Entailment (TE) module.  
Although the first approach suggested for opinion 
summarization obtained much better results in the 
evaluation than the second one (see Section 3.1), 
we decided to run the generic system over both 
approaches, with and without applying TE, to 
74
provide a more extent analysis and conclusions. 
After preprocessing the blogs and having all the 
possible candidate sentences grouped together, we 
considered these as the input for the generic 
summarizer. The goal of these experiments was to 
determine whether the techniques used for a 
generic summarizer would have a positive 
influence in selecting the main relevant 
information to become part of the final summary.  
4.2 Results and Discussion 
We re-evaluated the summaries generated by the 
generic system following the nuggets? list provided 
by the TAC 2008 organization, and counting 
manually the number of nuggets that were covered 
in the summaries. This was a tedious task, but it 
could not be automatically performed because of 
the fact that many of the provided nuggets were 
not found in the original blog collection. After the 
manual matching of nuggets and sentences, we 
computed the average Recall, Precision and F-
measure (Beta =1) in the same way as in the TAC 
2008 was done, according to the number and 
weight of the nuggets that were also covered in the 
summary. Each nugget had a weight ranging from 
0 to 1 reflecting its importance, and it was counted 
only once, even though the information was 
repeated within the summary.  
The average for each value was calculated taking 
into account the results for all the summaries in 
each approach. Unfortunately, we could not 
measure criteria such as readability or coherence as 
they were manually evaluated by human experts.  
Table 2 points out the results for all the approaches 
reported. We have also considered the results 
derived from our participation in the TAC 2008 
conference (OpSum-1 and OpSum-2), in order to 
analyze whether they have been improved or not. 
From these results it can be stated that the TE 
module in conjunction with the WF counts, have 
been very appropriate in selecting the most 
important information of a document. Although it 
can be thought that applying TE can remove some 
meaningful sentences which contained important 
information, results show the opposite. It benefits 
the Precision value, because a shorter summary 
contains greater ratio of relevant information. On 
the other hand, taking into consideration the F-
measure value only, it can be seen that the 
approach combining TE and WF, for the sentences 
in the first approach, has beaten significantly the 
best F-measure result among the participants of 
TAC 2008 (please see Table 3), increasing its 
performance by 20% (with respect to WF only), 
and improving by approximately 80% with respect 
to our first approach submitted to TAC 2008.    
However, a simple generic summarization system 
like the one we have used here is not enough to 
produce opinion oriented summaries, since 
semantic coherence given by the grouping of 
positive and negative opinions is not taken into 
account. Therefore, the opinion classification stage 
must be added in the same manner as used in the 
competition. 
 
SYSTEM RECALL PRECISION F-MEASURE 
OpSum-1 0.592 0.272 0.357 
OpSum-2 0.251 0.141 0.155 
WF-1 0.705 0.392 0.486 
TE+WF -1  0.684 0.630  0.639 
WF -2 0.322 0.234  0.241 
TE+WF-2 0.292 0.282 0.262 
Table 2. Comparison of the results 
4.3 Improving the quality of summaries 
In the evaluation performed by the TAC 
organization, a manual quality evaluation was also 
carried out. In this evaluation the important aspects 
were grammaticality, non-redundancy, structure 
and coherence, readability, and overall 
responsiveness. Although our participating systems 
obtained good F-measure values, in other scores, 
especially in grammaticality and non-redundancy, 
the results achieved were very low. Focusing all 
our efforts in improving the first approach, 
OpSum-1, non-redundancy and grammaticality 
verification had to be performed. In this approach, 
we wanted to test how much of the redundant 
information would be possible to remove by using 
a Textual Entailment system similar to (Iftene and 
Balahur-Dobrescu, 2007), without it affecting the 
quality of the remaining data. As input for the TE 
system, we considered the snippets retrieved from 
the original blog posts. We applied the entailment 
verification on each of the possible pairs, taking in 
turn all snippets as Text and Hypothesis with all 
other snippets as Hypothesis and Text, 
respectively. Thus, as output, we obtained the list 
of snippets from which we eliminated those that 
75
are entailed by any of the other snippets. We 
further eliminated those snippets which had a high 
entailment score with any of the remaining 
snippets. 
 
SYSTEM F-MEASURE 
Best system  0.534 
Second best system 0.490 
OpSum-1 + TE  0.530 
OpSum-1 0.357 
Table 3. F-measure results after improving the system 
 
Table 3 shows that applying TE before generating 
the final summary leads to very good results 
increasing the F-measure by 48.50% with respect 
to the original first approach. Moreover, it can be 
seen form Table 3 that our improved approach 
would have ranked in the second place among all 
the participants, regarding F-measure. The main 
problem with this approach is the long processing 
time. We can apply Textual Entailment in the 
manner described within the generic 
summarization system presented, successively 
testing the relation as Snippet1 entails Snippet2?, 
Snippet1+Snippet2 entails Snippet3? and so on. 
The problem then becomes the fact that this 
approach is random, since different snippets come 
from different sources, so there is no order among 
them. Further on, we have seen that many 
problems arise from the fact that extracting 
information from blogs introduces a lot of noise. In 
many cases, we had examples such as: 
At 4:00 PM John said Starbucks coffee tastes great 
John said Starbucks coffee tastes great, always get one 
when reading New York Times. 
To the final summary, the important information 
that should be added is ?Starbucks coffee tastes 
great?. Our TE system contains a rule specifying 
that the existence or not of a Named Entity in the 
hypothesis and its not being mentioned in the text 
leads to the decision of ?NO? entailment. For the 
example given, both snippets are maintained, 
although they contain the same data.  
Another issue to be addressed is the extra 
information contained in final summaries that is 
not scored as nugget. As we have seen from our 
data, much of this information is also valid and 
correctly answers the questions. Therefore, what 
methods can be employed to give more weight to 
some and penalize others automatically?  
Regarding the grammaticality criteria, once we had 
a summary generated we used the module 
Language Tool7 as a post-processing step. The 
errors that we needed correcting included the 
number matching between nouns and determiners 
as well as among subject and predicate, upper case 
for sentence start, repeated words or punctuation 
marks and lack of punctuation marks. The rules 
present in the module and that we ?switched off?, 
due to the fact that they produced more errors, 
were those concerning the limit in the number of 
consecutive nouns and the need for an article 
before a noun (since it always seemed to want to 
correct ?Vista? for ?the Vista? a.o.). We evaluated 
by observing the mistakes that the texts contained, 
and counting the number of remaining or 
introduced errors in the output. The results 
obtained can be seen in Table 4. 
 
Problem Rightly corrected 
 
Wrongly 
corrected 
Match S-P 90% 10% 
Noun-det 75% 25% 
Upper case 80% 20% 
Repeated words 100% 0% 
Repeated ?.? 80% 20% 
Spelling mistakes 60% 40% 
Unpaired ??/() 100% 0% 
Table 4. Grammaticality analysis 
 
The greatest problem encountered was the fact that 
bigrams are not detected and agreement is not 
made in cases in which the noun does not appear 
exactly after the determiner. All in all, using this 
module, the grammaticality of our texts was 
greatly improved. 
5 Conclusions and future work 
The Opinion Pilot in the TAC 2008 competition 
was a difficult task, involving the development of 
systems including components for QA, IR, polarity 
classification and summarization. Our contribution 
presented in this paper resides in proposing an 
opinion mining and summarization method using 
different approaches and resources, evaluating 
each of them in turn. We have shown that using a 
generic summarization system, we obtain 80% 
improvement over the results obtained in the 
competition, with coherence being maintained by 
using the same polarity classification mechanisms. 
                                                           
7http://community.languagetool.org/ 
76
Using redundancy removal with TE, as opposed to 
our initial polarity strength based sentence filtering 
improved the system performance by almost 50%.    
Finally, we showed that grammaticality can be 
checked and improved using an independent 
solution given by Language Tool.  
Further work includes the improvement of the 
polarity classification component by using 
machine learning over annotated corpora and other 
techniques, such as anaphora resolution. As we 
could see, the well functioning of this component 
ensures logic, structure and coherence to the 
produced summaries. Moreover, we plan to study 
the manner in which opinion sentences of 
blogs/bloggers can be coherently combined. 
References  
Balahur, A., Lloret, E., Ferr?ndez, ?., Montoyo, A., 
Palomar, M., Mu?oz, R., The DLSIUAES Team?s 
Participation in the TAC 2008 Tracks. In 
Proceedings of the Text Analysis Conference (TAC), 
2008. 
Balahur, A. and Montoyo, A. [1]. An Incremental 
Multilingual Approach to Forming a Culture 
Dependent Emotion Triggers Database. In 
Proceedings of the 8th International Conference on 
Terminology and Knowledge Engineering, 2008. 
Balahur, A. and Montoyo, A. [2]. Multilingual Feature--
driven Opinion Mining and Summarization from 
Customer Reviews. In Lecture Notes in Computer 
Science 5039, pg. 345-346. 
Bossard, A., G?n?reux, M. and  Poibeau, T.. Description 
of the LIPN systems at TAC 2008: Summarizing 
information and opinions. In Proceedings of the Text 
Analysis Conference (TAC), 2008. 
Chaovalit, P., Zhou, L. 2005. Movie Review Mining: a 
Comparison between Supervised and Unsupervised 
Classification Approaches. In Proceedings of HICSS-
05, the 38th Hawaii International Conference on 
System Sciences. 
Cruz, F., Troyani, J.A., Ortega, J., Enr?quez, F. The 
Italica System at TAC 2008 Opinion Summarization 
Task. In Proceedings of the Text Analysis 
Conference (TAC), 2008. 
Cui, H., Mittal, V., Datar, M. 2006. Comparative 
Experiments on Sentiment Classification for Online 
Product Reviews. In Proceedings of the 21st National 
Conference on Artificial Intelligence AAAI 2006.  
Dave, K., Lawrence, S., Pennock, D. 2003. Mining the 
Peanut Gallery: Opinion Extraction and Semantic 
Classification of Product Reviews. In Proceedings of 
WWW-03.  
Lloret, E., Ferr??ndez, O., Mu?oz, R. and Palomar, M. A 
Text Summarization Approach under the Influence of 
Textual Entailment. In Proceedings of the 5th 
International Workshop on Natural Language 
Processing and Cognitive Science (NLPCS 2008), 
pages 22?31, 2008. 
Gamon, M., Aue, S., Corston-Oliver, S., Ringger, E. 
2005. Mining Customer Opinions from Free Text. 
Lecture Notes in Computer Science. 
He, T., Chen, J., Gui, Z., Li, F. CCNU at TAC 2008: 
Proceeding on Using Semantic Method for 
Automated Summarization Yield. In Proceedings of 
the Text Analysis Conference (TAC), 2008. 
Hendrickx, I. and Bosma, W.. Using coreference links 
and sentence compression in graph-based 
summarization. In Proceedings of the Text Analysis 
Conference (TAC), 2008.     
Hu, M., Liu, B. 2004. Mining Opinion Features in 
Customer Reviews. In Proceedings of 19th National 
Conference on Artificial Intelligence AAAI. 
Iftene, A., Balahur-Dobrescu, A. Hypothesis 
Transformation and Semantic Variability Rules for 
Recognizing Textual Entailment. In Proceedings of 
the ACL 2007 Workshop on Textual Entailment and 
Paraphrasis, 2007. 
Kim, S.M., Hovy, E. 2004. Determining the Sentiment 
of Opinions. In Proceedings of COLING 2004. 
Pang, B., Lee, L., Vaithyanathan, S. 2002. Thumbs up? 
Sentiment classification using machine learning 
techniques. In Proceedings of EMNLP-02, the 
Conference on Empirical Methods in Natural 
Language Processing. 
Riloff, E., Wiebe, J. 2003 Learning Extraction Patterns 
for Subjective Expressions. In Proceedings of the 
2003 Conference on Empirical Methods in Natural 
Language Processing.  
Scherer, K. and Wallbott, H.G. The ISEAR 
Questionnaire and Codebook, 1997.  
Stoyanov, V., Cardie, C. 2006. Toward Opinion 
Summarization: Linking the Sources. In: COLING-
ACL 2006 Workshop on Sentiment and Subjectivity 
in Text. 
Strapparava, C. and Valitutti, A. "WordNet-Affect: an 
affective extension of WordNet". In Proceedings 
ofthe 4th International Conference on Language 
Resources and Evaluation, 2004, pp. 1083-1086.  
Turney, P., 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews. In Proceedings of the 40th 
Annual Meeting of the ACL 
Varma, V., Pingali, P., Katragadda, R., Krisha, S., 
Ganesh, S., Sarvabhotla, K., Garapati, H., Gopisetty, 
H.,, Reddy, V.B., Bysani, P., Bharadwaj, R. IIT 
Hyderabad at TAC 2008. In Proceedings of the Text 
Analysis Conference (TAC), 2008.  
Wilson, T., Wiebe, J., Hwa, R. 2004. Just how mad are 
you? Finding strong and weak opinion clauses. In: 
Proceedings of AAAI 2004. 
77
Semantic Pattern Learning Through Maximum Entropy-based WSD
technique  
Maximiliano Saiz-Noeda
Depto. de Lenguajes y
Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
max@dlsi.ua.es
Armando Sua?rez
Depto. de Lenguajes y
Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
armando@dlsi.ua.es
Manuel Palomar
Depto. de Lenguajes y
Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
mpalomar@dlsi.ua.es
Abstract
This paper describes a Natural Lan-
guage Learning method that extracts
knowledge in the form of semantic pat-
terns with ontology elements associated
to syntactic components in the text. The
method combines the use of EuroWord-
Net?s ontological concepts and the cor-
rect sense of each word assigned by
a Word Sense Disambiguation(WSD)
module to extract three sets of pat-
terns: subject-verb, verb-direct object
and verb-indirect object. These sets de-
fine the semantic behaviour of the main
textual elements based on their syntac-
tic role. On the one hand, it is shown
that Maximum Entropy models applied
to WSD tasks provide good results. The
evaluation of the WSD module has re-
vealed a accuracy rate of 64% in a pre-
liminary test. On the other hand, we ex-
plain how an adequate set of semantic
or ontological patterns can improve the
success rate of NLP tasks such us pro-
noun resolution. We have implemented
both modules in C++ and although the
evaluation has been performed for En-
glish, their general features allow the
treatment of other languages like Span-
ish.

This paper has been partially supported by the Spanish
Government (CICYT) project number TIC2000-0664-C02-
02.
1 Introduction
Semantic patterns, as defined in this method, con-
figure a system to add a new information source
to Natural Language Processing (NLP) tasks. To
obtain these semantic patterns, it is necessary to
count on different tools. On the one hand, a full
parser must make a syntactic analysis of the text.
This parsing will allow the selection of the differ-
ent syntactic functional elements such as subject,
direct object (DObj) and indirect object (IObj).
On the other hand, a WSD tool must provide the
correct sense in order to ensure the appropriate
selection of the ontological concept associated to
each word. Finally, with the parsing and the cor-
rect sense of each word, the pattern extraction
method will form and store ontological pairs that
define the semantic behaviour of each sentence.
2 Full parsing
The analyzer used for this work is the Conexor?s
FDG Parser (Pasi Tapanainen and Timo Ja?rvinen,
1997). This parser tries to provide a build depen-
dency tree from the sentence. When this is not
possible, the parser tries to build partial trees that
often result from unresolved ambiguity. One vi-
sual example of this dependency trees is shown in
Figure 1 where the parsing tree of sentence (1) is
illustrated.
(1) The minister gave explanations to
the Government.
As seen in Figure 2, the analyzer assigns
to each word a text token (second column), a
base form (third column) and functional link
0
1 The the det:>2 @DN> DET SG/PL
2 minister minister subj:>3 @SUBJ N NOM SG
3 gave give main:>0 @+FMAINV V PAST
4 explanations explanation obj:>3 @OBJ N NOM PL
5 to to dat:>3 @ADVL PREP
6 the the det:>7 @DN> DET SG/PL
7 Government government pcomp:>5 @<P N NOM SG/PL
. .
Figure 2: FDG Analyser?s output example
Figure 1: Parsing tree
names, lexico-syntactic function labels and parts
of speech (fourth column). Figure 1 shows the
parsing tree related to this output. These elements
are enough for the pattern extraction method to be
applied to NLP tasks.
Regarding to the evaluation of the parser, the
authors report an average precision and recall of
95% and 88% respectively in the detection of the
correct head. Furthermore, they report a precision
rate between 89% and 95% and a recall rate be-
tween 83% and 96% in the selection of the func-
tional dependencies.
3 WSD based on Maximum Entropy
A WSD module is applied to this parser?s output,
in order to select the correct sense of each entry.
Maximum Entropy(ME) modeling is a frame-
work for integrating information from many het-
erogeneous information sources for classification
(Manning and Schu?tze, 1999). This WSD sys-
tem is based on conditional ME probability mod-
els. The system implements a supervised learn-
ing method consisting of the building of word
sense classifiers through training on a semanti-
cally tagged corpus. A classifier obtained by
means of a ME technique consist of a set of
parameters or coefficients estimated by an opti-
mization procedure. Each coefficient associates
a weight to one feature observed in the training
data. A feature is a function that gives infor-
mation about some characteristic in a context as-
sociated to a class. The basic idea is to obtain
the probability distribution that maximizes the en-
tropy, that is, maximum ignorance is assumed and
nothing apart of training data is considered. As
advantages of ME framework, knowledge-poor
features applying and accuracy can be mentioned;
ME framework allows a virtually unrestricted
ability to represent problem-specific knowledge
in the form of features (Ratnaparkhi, 1998).
Let us assume a set of contexts

and a
set of classes  . The function 
 	
 that performs the classification in a condi-
tional probability model 
 chooses the class with
the highest conditional probability: 
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 73?80,
Rochester, April 2007 c?2007 Association for Computational Linguistics
DLSITE-2: Semantic Similarity Based on Syntactic
Dependency Trees Applied to Textual Entailment
Daniel Micol, O?scar Ferra?ndez, Rafael Mun?oz, and Manuel Palomar
Natural Language Processing and Information Systems Group
Department of Computing Languages and Systems
University of Alicante
San Vicente del Raspeig, Alicante 03690, Spain
{dmicol, ofe, rafael, mpalomar}@dlsi.ua.es
Abstract
In this paper we attempt to deduce tex-
tual entailment based on syntactic depen-
dency trees of a given text-hypothesis pair.
The goals of this project are to provide an
accurate and fast system, which we have
called DLSITE-2, that can be applied in
software systems that require a near-real-
time interaction with the user. To accom-
plish this we use MINIPAR to parse the
phrases and construct their correspond-
ing trees. Later on we apply syntactic-
based techniques to calculate the seman-
tic similarity between text and hypothe-
sis. To measure our method?s precision we
used the test text corpus set from Second
PASCAL Recognising Textual Entailment
Challenge (RTE-2), obtaining an accuracy
rate of 60.75%.
1 Introduction
There are several methods used to determine tex-
tual entailment for a given text-hypothesis pair. The
one described in this paper uses the information
contained in the syntactic dependency trees of such
phrases to deduce whether there is entailment or
not. In addition, semantic knowledge extracted from
WordNet (Miller et al, 1990) has been added to
achieve higher accuracy rates.
It has been proven in several competitions and
other workshops that textual entailment is a complex
task. One of these competitions is PASCAL Recog-
nising Textual Entailment Challenge (Bar-Haim et
al., 2006), where each participating group develops a
textual entailment recognizing system attempting to
accomplish the best accuracy rate of all competitors.
Such complexity is the reason why we use a combi-
nation of various techniques to deduce whether en-
tailment is produced.
Currently there are few research projects related
to the topic discussed in this paper. Some systems
use syntactic tree matching as the textual entailment
decision core module, such as (Katrenko and Adri-
aans, 2006). It is based on maximal embedded syn-
tactic subtrees to analyze the semantic relation be-
tween text and hypothesis. Other systems use syn-
tactic trees as a collaborative module, not being the
core, such as (Herrera et al, 2006). The application
discussed in this paper belongs to the first set of sys-
tems, since syntactic matching is its main module.
The remainder of this paper is structured as fol-
lows. In the second section we will describe the
methods implemented in our system. The third one
contains the experimental results, and the fourth and
last discusses such results and proposes future work
based on our actual research.
2 Methods
The system we have built aims to provide a good
accuracy rate in a short lapse of time, making it
feasible to be included in applications that require
near-real-time responses due to their interaction with
the user. Such a system is composed of few mod-
ules that behave collaboratively. These include tree
construction, filtering, embedded subtree search and
graph node matching. A schematic representation of
the system architecture is shown in Figure 1.
73
Figure 1: DLSITE-2 system architecture.
Each of the steps or modules of DLSITE-2 is de-
scribed in the following subsections, that are num-
bered sequentially according to their execution or-
der.
2.1 Tree generation
The first module constructs the corresponding syn-
tactic dependency trees. For this purpose, MINI-
PAR (Lin, 1998) output is generated and afterwards
parsed for each text and hypothesis of our corpus.
Phrase tokens, along with their grammatical infor-
mation, are stored in an on-memory data structure
that represents a tree, which is equivalent to the men-
tioned syntactic dependency tree.
2.2 Tree filtering
Once the tree has been constructed, we may want
to discard irrelevant data in order to reduce our sys-
tem?s response time and noise. For this purpose we
have generated a database of relevant grammatical
categories, represented in Table 1, that will allow
us to remove from the tree all those tokens whose
category does not belong to such list. The result-
ing tree will have the same structure as the original,
but will not contain any stop words nor irrelevant to-
kens, such as determinants or auxiliary verbs. The
whole list of ignored grammatical categories is rep-
resented in Table 2.
We have performed tests taking into account and
discarding each grammatical category, which has al-
lowed us to generate both lists of relevant and ig-
nored grammatical categories.
Verbs, verbs with one argument, verbs with two ar-
guments, verbs taking clause as complement, verb
Have, verb Be
Nouns
Numbers
Adjectives
Adverbs
Noun-noun modifiers
Table 1: Relevant grammatical categories.
2.3 Graph embedding detection
The next step of our system consists in determining
whether the hypothesis? tree is embedded into the
text?s. Let us first define the concept of embedded
tree (Katrenko and Adriaans, 2006).
Definition 1: Embedded tree A tree
T1 = (V1, E1) is embedded into another
one T2 = (V2, E2) iff
1. V1 ? V2, and
2. E1 ? E2
where V1 and V2 represent the vertices,
and E1 and E2 the edges.
In other words, a tree, T1, is embedded into an-
other one, T2, if all nodes and branches of T1 are
present in T2.
We believe that it makes sense to reduce the strict-
ness of such a definition to allow the appearance
of intermediate nodes in the text?s branches that are
74
Determiners
Pre-determiners
Post-determiners
Clauses
Inflectional phrases
Preposition and preposition phrases
Specifiers of preposition phrases
Auxiliary verbs
Complementizers
Table 2: Ignored grammatical categories.
not present in the corresponding hypothesis? branch,
which means that we allow partial matching. There-
fore, a match between two branches will be pro-
duced if all nodes of the first one, namely ?1 ? E1,
are present in the second, namely ?2 ? E2, and their
respective order is the same, allowing the possibil-
ity of appearance of intermediate nodes that are not
present in both branches. This is also described in
(Katrenko and Adriaans, 2006).
To determine whether the hypothesis? tree is em-
bedded into the text?s, we perform a top-down
matching process. For this purpose we first compare
the roots of both trees. If they coincide, we then pro-
ceed to compare their respective child nodes, which
are the tokens that have some sort of dependency
with their respective root token.
In order to add more flexibility to our system,
we do not require the pair of tokens to be ex-
actly the same, but rather set a threshold that rep-
resents the minimum similarity value between them.
This is a difference between our approach and the
one described in (Katrenko and Adriaans, 2006).
Such a similarity is calculated by using the Word-
Net::Similarity tool (Pedersen et al, 2004), and,
concretely, the Wu-Palmer measure, as defined in
Equation 1 (Wu and Palmer, 1994).
Sim(C1, C2) =
2N3
N1 +N2 + 2N3
(1)
where C1 and C2 are the synsets whose similarity
we want to calculate, C3 is their least common su-
perconcept, N1 is the number of nodes on the path
from C1 to C3, N2 is the number of nodes on the
path from C2 to C3, and N3 is the number of nodes
on the path from C3 to the root. All these synsets
and distances can be observed in Figure 2.
Figure 2: Distance between two synsets.
If the similarity rate is greater or equal than the
established threshold, which we have set empirically
to 80%, we will consider the corresponding hypoth-
esis? token as suitable to have the same meaning
as the text?s token, and will proceed to compare its
child nodes in the hypothesis? tree. On the other
hand, if such similarity value is less than the cor-
responding threshold, we will proceed to compare
the children of such text?s tree node with the actual
hypothesis? node that was being analyzed.
The comparison between the syntactic depen-
dency trees of both text and hypothesis will be com-
pleted when all nodes of either tree have been pro-
cessed. If we have been able to find a match for all
the tokens within the hypothesis, the corresponding
tree will be embedded into the text?s and we will be-
lieve that there is entailment. If not, we will not be
able to assure that such an implication is produced
and will proceed to execute the next module of our
system.
Next, we will present a text-hypothesis pair sam-
ple where the syntactic dependency tree of the hy-
pothesis (Figure 3(b)) is embedded into the text?s
(Figure 3(a)). The mentioned text-hypothesis pair
is the following:
Text: Mossad is one of the world?s most
well-known intelligence agencies, and is
often viewed in the same regard as the CIA
and MI6.
Hypothesis: Mossad is an intelligence
agency.
75
(a) Mossad is one of the world?s most well-known intelligence agencies, and is often viewed
in the same regard as the CIA and MI6.
(b) Mossad is an intelligence
agency.
Figure 3: Representation of a hypothesis? syntactic dependency tree that is embedded into the text?s.
As one can see in Figure 3, the hypothesis? syn-
tactic dependency tree represented is embedded into
the text?s because all of its nodes are present in
the text in the same order. There is one exception
though, that is the word an. However, since it is a
determinant, the filtering module will have deleted
it before the graph embedding test is performed.
Therefore, in this example the entailment would be
recognized.
2.4 Graph node matching
Once the embedded subtree comparison has fin-
ished, and if its result is negative, we proceed to per-
form a graph node matching process, termed align-
ment, between both the text and the hypothesis. This
operation consists in finding pairs of tokens in both
trees whose lemmas are identical, no matter whether
they are in the same position within the tree. We
would like to point out that in this step we do not
use the WordNet::Similarity tool.
Some authors have already designed similar
matching techniques, such as the ones described in
(MacCartney et al, 2006) and (Snow et al, 2006).
However, these include semantic constraints that we
have decided not to consider. The reason of this
decision is that we desired to overcome the textual
entailment recognition from an exclusively syntactic
perspective. Therefore, we did not want this module
to include any kind of semantic knowledge.
The weight given to a token that has been found
in both trees will depend on the depth in the hypoth-
esis? tree and the token?s grammatical relevance.
The first of these factors depends on an empirically-
calculated weight that assigns less importance to a
node the deeper it is located in the tree. This weight
is defined in Equation 2. The second factor gives
different relevance depending on the grammatical
category and relationship. For instance, a verb will
have the highest weight, while an adverb or an ad-
jective will have less relevance. The values assigned
to each grammatical category and relationship are
also empirically-calculated and are shown in Tables
3 and 4, respectively.
Grammatical category Weight
Verbs, verbs with one argument, verbs
with two arguments, verbs taking
clause as complement
1.0
Nouns, numbers 0.75
Be used as a linking verb 0.7
Adjectives, adverbs, noun-noun mod-
ifiers
0.5
Verbs Have and Be 0.3
Table 3: Weights assigned to the grammatical cate-
gories.
76
Grammatical relationship Weight
Subject of verbs, surface subject, ob-
ject of verbs, second object of ditran-
sitive verbs
1.0
The rest 0.5
Table 4: Weights assigned to the grammatical rela-
tionships.
Let ? and ? represent the text?s and hypothesis?
syntactic dependency trees, respectively. We as-
sume we have found members of a synset, namely ?,
present in both ? and ?. Now let ? be the weight as-
signed to ??s grammatical category (defined in Table
3), ? the weight of ??s grammatical relationship (de-
fined in Table 4), ? an empirically-calculated value
that represents the weight difference between tree
levels, and ?? the depth of the node that contains
the synset ? in ?. We define the function ?(?) as
represented in Equation 2.
?(?) = ? ? ? ? ???? (2)
The value obtained by calculating the expression
of Equation 2 would represent the relevance of a
synset in our system. The experiments performed
reveal that the optimal value for ? is 1.1.
For a given pair (? , ?), we define the set ? as the
one that contains the synsets present in both trees:
? = ? ? ? ?? ? ?, ? ? ? (3)
Therefore, the similarity rate between ? and ?, de-
noted by the symbol ?, would be defined as:
?(?, ?) =
?
???
?(?) (4)
One should note that a requirement of our sys-
tem?s similarity measure would be to be independent
of the hypothesis length. Thus, we must define the
normalized similarity rate, as shown in Equation 5.
?(?, ?) =
?(?, ?)
?
???
?(?)
=
?
???
?(?)
?
???
?(?)
(5)
Once the similarity value, ?(?, ?), has been cal-
culated, it will be provided to the user together with
the corresponding text-hypothesis pair identifier. It
will be his responsibility to choose an appropriate
threshold that will represent the minimum similarity
rate to be considered as entailment between text and
hypothesis. All values that are under such a thresh-
old will be marked as not entailed. For this purpose,
we suggest using a development corpus in order to
obtain the optimal threshold value, as it is done in
the RTE challenges.
3 Experimental results
The experimental results shown in this paper were
obtained processing a set of text-hypothesis pairs
from RTE-2. The organizers of this challenge pro-
vide development and test corpora to the partic-
ipants, both of them containing 800 pairs manu-
ally annotated for logical entailment. It is com-
posed of four subsets, each of them correspond-
ing to typical true and false entailments in different
tasks, such as Information Extraction (IE), Informa-
tion Retrieval (IR), Question Answering (QA), and
Multi-document Summarization (SUM). For each
task, the annotators selected the same amount of true
entailments as negative ones (50%-50% split).
The organizers have also defined two measures to
evaluate the participating systems. All judgments
returned by the systems will be compared to those
manually assigned by the human annotators. The
percentage of matching judgments will provide the
accuracy of the system, i.e. the percentage of cor-
rect responses. As a second measure, the average
precision will be computed. This measure evaluates
the ability of the systems to rank all the pairs in the
corpus according to their entailment confidence, in
decreasing order from the most certain entailment to
the least. Average precision is a common evaluation
measure for system rankings that is defined as shown
in Equation 6.
AP =
1
R
n?
i=1
E(i)
#correct up to pair i
i
(6)
where n is the amount of the pairs in the test corpus,
R is the total number of positive pairs in it, i ranges
over the pairs, ordered by their ranking, and E(i) is
defined as follows:
77
E(i) =
?
?
?
1 if the i? th pair is positive,
0 otherwise.
(7)
As we previously mentioned, we tested our sys-
tem against RTE-2 development corpus, and used
the test one to evaluate it.
First, Table 5 shows the accuracy (ACC) and av-
erage precision (AP), both as a percentage, obtained
processing the development corpus from RTE-2 for
a threshold value of 68.9%, which corresponds to
the highest accuracy that can be obtained using our
system for the mentioned corpus. It also provides
the rate of correctly predicted true and false entail-
ments.
Task ACC AP TRUE FALSE
IE 52.00 51.49 54.00 50.00
IR 55.50 58.99 32.00 79.00
QA 57.50 54.72 53.00 62.00
SUM 65.00 81.35 39.00 91.00
Overall 57.50 58.96 44.50 70.50
Table 5: Results obtained for the development cor-
pus.
Next, let us show in Table 6 the results obtained
processing the test corpus, which is the one used
to compare the different systems that participated in
RTE-2, with the same threshold as before.
Task ACC AP TRUE FALSE
IE 50.50 47.33 75.00 26.00
IR 64.50 67.67 59.00 70.00
QA 59.50 58.16 80.00 39.00
SUM 68.50 75.86 49.00 88.00
Overall 60.75 57.91 65.75 55.75
Table 6: Results obtained for the test corpus.
As one can observe in the previous table, our
system provides a high accuracy rate by using
mainly syntactical measures. The number of text-
hypothesis pairs that succeeded the graph embed-
ding evaluation was three for the development cor-
pus and one for the test set, which reflects the strict-
ness of such module. However, we would like to
point out that the amount of pairs affected by the
mentioned module will depend on the corpus na-
ture, so it can vary significantly between different
corpora.
Let us now compare our results with the ones that
were achieved by the systems that participated in
RTE-2. One should note that the criteria for such
ranking is based exclusively on the accuracy, ignor-
ing the average precision value. In addition, each
participating group was allowed to submit two dif-
ferent systems to RTE-2. We will consider here the
best result of both systems for each group. The men-
tioned comparison is shown in Table 7, and contains
only the systems that had higher accuracy rates than
our approach.
Participant Accuracy
(Hickl et al, 2006) 75.38
(Tatu et al, 2006) 73.75
(Zanzotto et al, 2006) 63.88
(Adams, 2006) 62.62
(Bos and Markert, 2006) 61.62
DLSITE-2 60.75
Table 7: Comparison of some of the teams that par-
ticipated in RTE-2.
As it is reflected in Table 7, our system would
have obtained the sixth position out of twenty-four
participants, which is an accomplishment consider-
ing the limited number of resources that it has built-
in.
Since one of our system?s modules is based on
(Katrenko and Adriaans, 2006), we will compare
their results with ours to analyze whether the modi-
fications we introduced perform correctly. In RTE-
2, they obtained an accuracy rate of 59.00% for the
test corpus. The reason why we believe we have
achieved better results than their system is due to
the fact that we added semantic knowledge to our
graph embedding module. In addition, the syntactic
dependency trees to which we have applied such a
module have been previously filtered to ensure that
they do not contain irrelevant words. This reduces
the system?s noise and allows us to achieve higher
accuracy rates.
In the introduction of this paper we mentioned
that one of the goals of our system was to provide
78
a high accuracy rate in a short lapse of time. This is
one of the reasons why we chose to construct a light
system where one of the aspects to minimize was its
response time. Table 8 shows the execution times1
of our system for both development and test text cor-
pora from RTE-2. These include total and average2
response times.
Development Test
Total 1045 1023
Average 1.30625 1.27875
Table 8: DLSITE-2 response times (in seconds).
As we can see, accurate results can be obtained
using syntactic dependency trees in a short lapse of
time. However, there are some limitations that our
system does not avoid. For instance, the tree em-
bedding test is not applicable when there is no verb
entailment. This is reflected in the following pair:
Text: Tony Blair, the British Prime Minis-
ter, met Jacques Chirac in London.
Hypothesis: Tony Blair is the British
Prime Minister.
The root node of the hypothesis? tree would be
the one corresponding to the verb is. Since the en-
tailment here is implicit, there is no need for such a
verb to appear in the text. However, this is not com-
patible with our system, since is would not match
any node of the text?s tree, and thus the hypothesis?
tree would not be found embedded into the text?s.
The graph matching process would not behave
correctly either. This is due to the fact that the main
verb, which has the maximum weight because it is
the root of the hypothesis? tree and its grammatical
category has the maximum relevance, is not present
in the text, so the overall similarity score would have
a considerable handicap.
The example of limitation of our system that we
have presented is an apposition. To avoid this spe-
cific kind of situations that produce an undesired be-
havior in our system, we could add a preprocess-
ing module that transforms the phrases that have the
1The machine we used to measure the response times had an
Intel Core 2 Duo processor at 2GHz.
2Average response times are calculated diving the totals by
the number of pairs in the corpus.
structureX , Y , Z intoX is Y , andZ. For the shown
example, the resulting text and hypothesis would be
as follows:
Text: Tony Blair is the British Prime Min-
ister, and met Jacques Chirac in London.
Hypothesis: Tony Blair is the British
Prime Minister.
The transformed text would still be syntactically
correct, and the entailment would be detected since
the hypothesis? syntactic dependency tree is embed-
ded into the text?s.
4 Conclusions and future work
The experimental results obtained from this research
demonstrate that it is possible to apply a syntactic-
based approach to deduce textual entailment from a
text-hypothesis pair. We can obtain good accuracy
rates using the discussed techniques with very short
response times, which is very useful for assisting
different kinds of tasks that demand near-real-time
responses to user interaction.
The baseline we set for our system was to achieve
better results than the ones we obtained with our last
participation in RTE-2. As it is stated in (Ferra?ndez
et al, 2006), the maximum accuracy value obtained
by then was 55.63% for the test corpus. Therefore,
our system is 9.20% more accurate compared to the
one that participated in RTE-2, which represents a
considerable improvement.
The authors of this paper believe that if higher ac-
curacy rates are desired, a step-based systemmust be
constructed. This would have several preprocessing
units, such as negation detectors, multi-word associ-
ators and so on. The addition of these units would
definitely increase the response time preventing the
system from being used in real-time tasks.
Future work can be related to the cases where no
verb entailment is produced. For this purpose we
propose to extract a higher amount of semantic in-
formation that would allow us to construct a charac-
terized representation based on the input text, so that
we can deduce entailment even if there is no appar-
ent structure similarity between text and hypothesis.
This would mean to create an abstract conceptual-
ization of the information contained in the analyzed
phrases, allowing us to deduce ideas that are not
79
explicitly mentioned in the parsed text-hypothesis
pairs.
In addition, the weights and thresholds defined
in our system have been established empirically. It
would be interesting to calculate those values by
means of a machine learning algorithm and com-
pare them to the ones we have obtained empirically.
Some authors have already performed this compari-
son, being one example the work described in (Mac-
Cartney et al, 2006).
Acknowledgments
The authors of this paper would like to thank pro-
fessors Borja Navarro and Rafael M. Terol for their
help and critical comments.
This research has been supported by the under-
graduate research fellowships financed by the Span-
ish Ministry of Education and Science, the project
TIN2006-15265-C06-01 financed by such ministry,
and the project ACOM06/90 financed by the Span-
ish Generalitat Valenciana.
References
Rod Adams. 2006. Textual Entailment Through Ex-
tended Lexical Overlap. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The Second PASCAL Recognising Textual En-
tailment Challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Johan Bos, and Katja Markert. 2006. When logical infer-
ence helps determining textual entailment (and when it
doesnt). In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
O?scar Ferra?ndez, Rafael M. Terol, Rafael Mun?oz, Patri-
cio Mart??nez-Barco, and Manuel Palomar. 2006. An
approach based on Logic Forms andWordNet relation-
ships to Textual Entailment performance. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Jesu?s Herrera, Anselmo Pen?as, A?lvaro Rodrigo, and Fe-
lisa Verdejo. 2006. UNED at PASCAL RTE-2 Chal-
lenge. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing Textual Entailment with LCC?s GROUNDHOG
System. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
Sophia Katrenko, and Pieter Adriaans. 2006. Using
Maximal Embedded Syntactic Subtrees for Textual En-
tailment Recognition. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Granada, Spain.
Bill MacCartney, Trond Grenager, Marie-Catherine de
Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the North American
Association of Computational Linguistics (NAACL-
06), NewYork City, NewYork, United States of Amer-
ica.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990. In-
troduction to WordNet: An On-line Lexical Database.
International Journal of Lexicography 1990 3(4):235-
244.
Ted Pedersen, Siddhart Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-04), Boston, Massachus-
sets, United States of America.
Rion Snow, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false
entailment. In Proceedings of the North American As-
sociation of Computational Linguistics (NAACL-06),
New York City, New York, United States of America.
Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi,
and DanMoldovan. 2006. COGEX at the Second Rec-
ognizing Textual Entailment Challenge. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Zhibiao Wu, and Martha Palmer. 1994. Verb Semantics
and Lexical Selection. In Proceedings of the 32nd An-
nual Meeting of the Associations for Computational
Linguistics, pages 133-138, Las Cruces, New Mexico,
United States of America.
Fabio M. Zanzotto, Alessandro Moschitti, Marco Pen-
nacchiotti, and Maria T. Pazienza. 2006. Learning
textual entailment from examples. In Proceedings of
the Second PASCAL Challenges Workshop on Recog-
nising Textual Entailment, Venice, Italy.
80
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 66?71,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Perspective-Based Approach for Solving Textual Entailment Recognition
O?scar Ferra?ndez, Daniel Micol, Rafael Mun?oz, and Manuel Palomar
Natural Language Processing and Information Systems Group
Department of Computing Languages and Systems
University of Alicante
San Vicente del Raspeig, Alicante 03690, Spain
{ofe, dmicol, rafael, mpalomar}@dlsi.ua.es
Abstract
The textual entailment recognition system
that we discuss in this paper represents
a perspective-based approach composed of
two modules that analyze text-hypothesis
pairs from a strictly lexical and syntactic
perspectives, respectively. We attempt to
prove that the textual entailment recognition
task can be overcome by performing indi-
vidual analysis that acknowledges us of the
maximum amount of information that each
single perspective can provide. We compare
this approach with the system we presented
in the previous edition of PASCAL Recognis-
ing Textual Entailment Challenge, obtaining
an accuracy rate 17.98% higher.
1 Introduction
Textual entailment recognition has become a popu-
lar Natural Language Processing task within the last
few years. It consists in determining whether one
text snippet (hypothesis) entails another one (text)
(Glickman, 2005). To overcome this problem sev-
eral approaches have been studied, being the Recog-
nising Textual Entailment Challenge (RTE) (Bar-
Haim et al, 2006; Dagan et al, 2006) the most re-
ferred source for determining which one is the most
accurate.
Many of the participating groups in previous edi-
tions of RTE, including ourselves (Ferra?ndez et al,
2006), designed systems that combined a variety of
lexical, syntactic and semantic techniques. In our
contribution to RTE-3 we attempt to solve the tex-
tual entailment recognition task by analyzing two
different perspectives separately, in order to ac-
knowledge the amount of information that an indi-
vidual perspective can provide. Later on, we com-
bine both modules to obtain the highest possible ac-
curacy rate. For this purpose, we analyze the pro-
vided corpora by using a lexical module, namely
DLSITE-1, and a syntactic one, namely DLSITE-2.
Once all results have been obtained we perform a
voting process in order to take into account all sys-
tem?s judgments.
The remainder of this paper is structured as fol-
lows. Section two describes the system we have
built, providing details of the lexical and syntactic
perspectives, and explains the difference with the
one we presented in RTE-2. Third section presents
the experimental results, and the fourth one provides
our conclusions and describes possible future work.
2 System Specification
This section describes the systemwe have developed
in order to participate in RTE-3. It is based on sur-
face techniques of lexical and syntactic analysis. As
the starting point we have used our previous system
presented in the second edition of the RTE Chal-
lenge (Ferra?ndez et al, 2006). We have enriched
it with two independent modules that are intended
to detect some misinterpretations performed by this
system. Moreover, these new modules can also rec-
ognize entailment relations by themselves. The per-
formance of each separate module and their combi-
nation with our previous system will be detailed in
section three.
Next, Figure 1 represents a schematic view of the
system we have developed.
66
Figure 1: System architecture.
As we can see in the previous Figure, our sys-
tem is composed of three modules that are coordi-
nated by an input scheduler. Its commitment is to
provide the text-hypothesis pairs to each module in
order to extract their corresponding similarity rates.
Once all rates for a given text-hypothesis pair have
been calculated, they will be processed by an output
gatherer that will provide the final judgment. The
method used to calculate the final entailment deci-
sion consists in combining the outputs of both lex-
ical and syntactic modules, and these outputs with
our RTE-2 system?s judgment. The output gatherer
will be detailed later in this paper when we describe
the experimental results.
2.1 RTE-2 System
The approach we presented in the previous edition of
RTE attempts to recognize textual entailment by de-
termining whether the text and the hypothesis are re-
lated using their respective derived logic forms, and
by finding relations between their predicates using
WordNet (Miller et al, 1990). These relations have
a specific weight that provide us a score represent-
ing the similarity of the derived logic forms and de-
termining whether they are related or not.
For our participation in RTE-3 we decided to ap-
ply our previous system because it allows us to han-
dle some kinds of information that are not correctly
managed by the new approaches developed for the
current RTE edition.
2.2 Lexical Module
This method relies on the computation of a wide va-
riety of lexical measures, which basically consists of
overlap metrics. Although in other related work this
kind of metrics have already been used (Nicholson
et al, 2006), the main contribution of this module is
the fact that it only deals with lexical features with-
out taking into account any syntactic nor semantic
information. The following paragraphs list the con-
sidered lexical measures.
Simple matching: initialized to zero. A boolean
value is set to one if the hypothesis word appears in
the text. The final weight is calculated as the sum of
all boolean values and normalized dividing it by the
length of the hypothesis.
Levenshtein distance: it is similar to simple match-
ing. However, in this case we use the mentioned
distance as the similarity measure between words.
When the distance is zero, the increment value is
one. On the other hand, if such value is equal to one,
the increment is 0.9. Otherwise, it will be the inverse
of the obtained distance.
Consecutive subsequence matching: this measure
assigns the highest relevance to the appearance of
consecutive subsequences. In order to perform this,
we have generated all possible sets of consecutive
subsequences, from length two until the length in
words, from the text and the hypothesis. If we pro-
ceed as mentioned, the sets of length two extracted
from the hypothesis will be compared to the sets of
the same length from the text. If the same element is
present in both the text and the hypothesis set, then
a unit is added to the accumulated weight. This pro-
cedure is applied for all sets of different length ex-
tracted from the hypothesis. Finally, the sum of the
weight obtained from each set of a specific length is
normalized by the number of sets corresponding to
67
this length, and the final accumulated weight is also
normalized by the length of the hypothesis in words
minus one. This measure is defined as follows:
CSmatch =
|H|?
i=2
f(SHi)
|H| ? 1
(1)
where SHi contains the hypothesis? subsequences
of length i, and f(SHi) is defined as follows:
f(SHi) =
?
j?SHi
match(j)
|H| ? i+ 1
(2)
being match(j) equal to one if there exists an ele-
ment k that belongs to the set that contains the text?s
subsequences of length i, such that k = j.
One should note that this measure does not con-
sider non-consecutive subsequences. In addition, it
assigns the same relevance to all consecutive sub-
sequences with the same length. Furthermore, the
longer the subsequence is, the more relevant it will
be considered.
Tri-grams: two sets containing tri-grams of letters
belonging to the text and the hypothesis were cre-
ated. All the occurrences in the hypothesis? tri-
grams set that also appear in the text?s will increase
the accumulated weight in a factor of one unit. The
weight is normalized by the size of the hypothesis?
tri-grams set.
ROUGE measures: considering the impact of n-
gram overlap metrics in textual entailment, we be-
lieve that the idea of integrating these measures1 into
our system is very appealing. We have implemented
them as defined in (Lin, 2004).
Each measure is applied to the words, lemmas and
stems belonging to the text-hypothesis pair. Within
the entire set of measures, each one of them is con-
sidered as a feature for the training and test stages
of a machine learning algorithm. The selected one
was a Support Vector Machine due to the fact that its
properties are suitable for recognizing entailment.
2.3 Syntactic Module
The syntactic module we have built is composed of
few submodules that operate collaboratively in order
1The considered measures were ROUGE-N with n=2 and
n=3, ROUGE-L, ROUGE-W and ROUGE-S with s=2 and s=3.
to obtain the highest possible accuracy by using only
syntactic information.
The commitment of the first two submodules is
to generate an internal representation of the syntac-
tic dependency trees generated by MINIPAR (Lin,
1998). For this purpose we obtain the output of such
parser for the text-hypothesis pairs, and then process
it to generate an on-memory internal representation
of the mentioned trees. In order to reduce our sys-
tem?s noise and increase its accuracy rate, we only
keep the relevant words and discard the ones that we
believe do not provide useful information, such as
determinants and auxiliary verbs. After this step has
been performed we can proceed to compare the gen-
erated syntactic dependency trees of the text and the
hypothesis.
The graph node matching, termed alignment, be-
tween both the text and the hypothesis consists in
finding pairs of words in both trees whose lemmas
are identical, no matter whether they are in the same
position within the tree. Some authors have already
designed similar matching techniques, such as the
one described in (Snow et al, 2006). However, these
include semantic constraints that we have decided
not to consider. The reason of this decision is that we
desired to overcome the textual entailment recogni-
tion from an exclusively syntactic perspective. The
formula that provides the similarity rate between the
dependency trees of the text and the hypothesis in
our system, denoted by the symbol ?, is shown in
Equation 3:
?(?, ?) =
?
???
?(?) (3)
where ? and ? represent the text?s and hypothesis?
syntactic dependency trees, respectively, and ? is the
set that contains all synsets present in both trees, be-
ing ? = ? ? ? ?? ? ?, ? ? ?. As we can observe in
Equation 3, ? depends on another function, denoted
by the symbol ?, which provides the relevance of
a synset. Such a weight factor will depend on the
grammatical category and relation of the synset. In
addition, we believe that the most relevant words of
a phrase occupy the highest positions in the depen-
dency tree, so we desired to assign different weights
depending on the depth of the synset. With all these
factors we define the relevance of a word as shown
68
in Equation 4:
?(?) = ? ? ? ? ???? (4)
where ? is a synset present in both ? and ?, ? rep-
resents the weight assigned to ??s grammatical cat-
egory (Table 1), ? the weight of ??s grammatical
relationship (Table 2), ? an empirically calculated
value that represents the weight difference between
tree levels, and ?? the depth of the node that contains
the synset ? in ?. The performed experiments reveal
that the optimal value for ? is 1.1.
Grammatical category Weight
Verbs, verbs with one argument, verbs with
two arguments, verbs taking clause as com-
plement
1.0
Nouns, numbers 0.75
Be used as a linking verb 0.7
Adjectives, adverbs, noun-noun modifiers 0.5
Verbs Have and Be 0.3
Table 1: Weights assigned to the relevant grammati-
cal categories.
Grammatical relationship Weight
Subject of verbs, surface subject, object of
verbs, second object of ditransitive verbs
1.0
The rest 0.5
Table 2: Weights assigned to the grammatical rela-
tionships.
We would like to point out that a requirement of
our system?s similarity measure is to be independent
of the hypothesis length. Therefore, we must de-
fine the normalized similarity rate, as represented in
Equation 5:
?(?, ?) =
?
???
?(?)
?
???
?(?)
(5)
Once the similarity value has been calculated, it
will be provided to the user together with the cor-
responding text-hypothesis pair identifier. It will be
his responsibility to choose an appropriate threshold
that will represent the minimum similarity rate to be
considered as entailment between text and hypothe-
sis. All values that are under such a threshold will
be marked as not entailed.
3 System Evaluation
In order to evaluate our system we have generated
several results using different combinations of all
three mentioned modules. Since the lexical one uses
a machine learning algorithm, it has to be run within
a training environment. For this purpose we have
trained our system with the corpora provided in the
previous editions of RTE, and also with the develop-
ment corpus from the current RTE-3 challenge. On
the other hand, for the remainder modules the devel-
opment corpora was used to set the thresholds that
determine if the entailment holds.
The performed tests have been obtained by per-
forming different combinations of the described
modules. First, we have calculated the accuracy
rates using only each single module separately.
Later on we have combined those developed by our
research group for this year?s RTE challenge, which
are DLSITE-1 (the lexical one) and DLSITE-2 (the
syntactic one). Finally we have performed a voting
process between these two systems and the one we
presented in RTE-2.
The combination of DLSITE-1 and DLSITE-2 is
described as follows. If both modules agree, then the
judgement is straightforward, but if they do not, we
then decide the judgment depending on the accuracy
of each one for true and false entailment situations.
In our case, DLSITE-1 performs better while dealing
with negative examples, so its decision will prevail
over the rest. Regarding the combination of the three
approaches, we have developed a voting strategy.
The results obtained by our system are represented
in Table 3. As it is reflected in such table, the high-
est accuracy rate obtained using the RTE-3 test cor-
pus was achieved applying only the lexical module,
namely DLSITE-1. On the other hand, the syntac-
tic one had a significantly lower rate, and the same
happened with the system we presented in RTE-2.
Therefore, a combination of them will most likely
produce less accurate results than the lexical mod-
ule, as it is shown in Table 3. However, we would
like to point out that these results depend heavily on
the corpus idiosyncrasy. This can be proven with the
results obtained for the RTE-2 test corpus, where the
grouping of the three modules provided the highest
accuracy rates of all possible combinations.
69
RTE-2 test RTE-3 dev RTE-3 test
Overall Overall Overall IE IR QA SUM
RTE-2 system 0.5563 0.5523 0.5400 0.4900 0.6050 0.5100 0.5550
DLSITE-1 0.6188 0.7012 0.6563 0.5150 0.7350 0.7950 0.5800
DLSITE-2 0.6075 0.6450 0.5925 0.5050 0.6350 0.6300 0.6000
DLSITE-1&2 0.6212 0.6900 0.6375 0.5150 0.7150 0.7400 0.5800
Voting 0.6300 0.6900 0.6375 0.5250 0.7050 0.7200 0.6000
Table 3: Results obtained with the corpora from RTE-2 and RTE-3.
3.1 Results Analysis
We will now perform an analysis of the results
shown in the previous section. First, we would like
to mention the fact that our system does not be-
have correctly when it has to deal with long texts.
Roughly 11% and 13% of the false positives of
DLSITE-1 and DLSITE-2, respectively, are caused
by misinterpretations of long texts. The underlying
reason of these failures is the fact that it is easier to
find a lexical and syntactic match when a long text
is present in the pair, even if there is not entailment.
In addition, we consider very appealing to show
the accuracy rates corresponding to true and false
entailment pairs individually. Figure 2 represents the
mentioned rates for all system combinations that we
displayed in Table 3.
Figure 2: Accuracy rates obtained for true and false
entailments using the RTE-3 test corpus.
As we can see in Figure 2, the accuracy rates
for true and false entailment pairs vary significantly.
The modules we built for our participation in RTE-3
obtained high accuracy rates for true entailment text-
hypothesis pairs, but in contrast they behaved worse
in detecting false entailment pairs. This is the oppo-
site to the system we presented in RTE-2, since it has
a much higher accuracy rate for false cases than true
ones. When we combinedDLSITE-1 andDLSITE-2,
their accuracy rate for true entailments diminished,
although, on the other hand, the rate for false ones
raised. The voting between all three modules pro-
vided a higher accuracy rate for false entailments be-
cause the system we presented at RTE-2 performed
well in these cases.
Finally, we would like to discuss some examples
that lead to failures and correct forecasts by our two
new approaches.
Pair 246 entailment=YES task=IR
T: Overall the accident rate worldwide for commercial aviation
has been falling fairly dramatically especially during the period
between 1950 and 1970, largely due to the introduction of new
technology during this period.
H: Airplane accidents are decreasing.
Pair 246 is incorrectly classified by DLSITE-1
due to the fact that some words of the hypothesis do
not appear in the same manner in the text, although
they have similar meaning (e.g. airplane and
aviation). However, DLSITE-2 is able to establish a
true entailment for this pair, since the hypothesis?
syntactic dependency tree can be matched within the
text?s, and the similarity measure applied between
lemmas obtains a high score. This fact produces
that, in this case, the voting also achieves a correct
prediction for pair 246.
Pair 736 entailment=YES task=SUM
T: In a security fraud case, Michael Milken was sentenced to 10
years in prison.
H: Milken was imprisoned for security fraud.
Pair 736 is correctly classified by DLSITE-1 since
there are matches for all hypothesis? words (except
imprisoned) and some subsequences. In contrast,
DLSITE-2 does not behave correctly with this exam-
ple because the main verbs do not match, being this
fact a considerable handicap for the overall score.
70
4 Conclusions and Future Work
This research provides independent approaches con-
sidering mainly lexical and syntactic information. In
order to achieve this, we expose and analyze a wide
variety of lexical measures as well as syntactic struc-
ture comparisons that attempt to solve the textual en-
tailment recognition task. In addition, we propose
several combinations between these two approaches
and integrate them with our previous RTE-2 system
by using a voting strategy.
The results obtained reveal that, although the
combined approach provided the highest accuracy
rates for the RTE-2 corpora, it has not accom-
plished the expected reliability in the RTE-3 chal-
lenge. Nevertheless, in both cases the lexical-based
module achieved better results than the rest of the in-
dividual approaches, being the optimal for our par-
ticipation in RTE-3, and obtaining an accuracy rate
of about 70% and 65% for the development and test
corpus, respectively. One should note that these re-
sults depend on the idiosyncrasies of the RTE cor-
pora. However, these corpora are the most reliable
ones for evaluating textual entailment recognizers.
Future work can be related to the development
of a semantic module. Our system achieves good
lexical and syntactic comparisons between texts, but
we believe that we should take advantage of the se-
mantic resources in order to achieve higher accuracy
rates. For this purpose we plan to build a module
that constructs characterized representations based
on the text using named entities and role labeling in
order to extract semantic information from a text-
hypothesis pair. Another future research line could
consist in applying different recognition techniques
depending on the type of entailment task. We have
noticed that the accuracy of our approach differs
when the entailment is produced mainly by lexical
or syntactic implications. We intend to establish an
entailment typology and tackle each type by means
of different points of view or approaches.
Acknowledgments
This research has been partially funded by the
QALL-ME consortium, which is a 6th Framework
Research Programme of the European Union (EU),
contract number FP6-IST-033860 and by the Span-
ish Government under the project CICyT number
TIN2006-1526-C06-01. It has also been supported
by the undergraduate research fellowships financed
by the Spanish Ministry of Education and Science,
and the project ACOM06/90 financed by the Span-
ish Generalitat Valenciana.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The Second PASCAL Recognising Textual En-
tailment Challenge. Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment, pages 1?9.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In Quin?onero-Candela et al, edi-
tors, MLCW 2005, LNAI Volume 3944, pages 177?190.
Springer-Verlag.
Oscar Ferra?ndez, Rafael M. Terol, Rafael Mun?oz, Patri-
cio Mart??nez-Barco, and Manuel Palomar. 2006. An
approach based on Logic forms and wordnet relation-
ships to textual entailment performance. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 22?26, Venice,
Italy.
Oren Glickman. 2005. Applied Textual Entailment Chal-
lenge. Ph.D. thesis, Bar Ilan University.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Granada, Spain.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Stan Szpakow-
icz Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
pages 74?81, Barcelona, Spain, July. Association for
Computational Linguistics.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990. In-
troduction to WordNet: An On-line Lexical Database.
International Journal of Lexicography, 3(4):235?244.
Jeremy Nicholson, Nicola Stokes, and Timothy Baldwin.
2006. Detecting Entailment Using an Extended Imple-
mentation of the Basic Elements Overlap Metrics. In
Proceedings of the Second PASCAL Challenges Work-
shop on Recognising Textual Entailment, pages 122?
127, Venice, Italy.
Rion Snow, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false
entailment. In Proceedings of the North American
Association of Computational Linguistics, New York
City, New York, United States of America.
71
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 903?911,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Quantifying the Limits and Success of Extractive Summarization Systems
Across Domains
Hakan Ceylan and Rada Mihalcea
Department of Computer Science
University of North Texas
Denton, TX 76203
{hakan,rada}@unt.edu
Umut ?Ozertem
Yahoo! Labs
701 First Avenue
Sunnyvale, CA 94089
umut@yahoo-inc.com
Elena Lloret and Manuel Palomar
Department of
Software and Computing Systems
University of Alicante
San Vicente del Raspeig
Alicante 03690, Spain
{elloret,mpalomar}@dlsi.ua.es
Abstract
This paper analyzes the topic identification
stage of single-document automatic text sum-
marization across four different domains, con-
sisting of newswire, literary, scientific and le-
gal documents. We present a study that ex-
plores the summary space of each domain
via an exhaustive search strategy, and finds
the probability density function (pdf) of the
ROUGE score distributions for each domain.
We then use this pdf to calculate the per-
centile rank of extractive summarization sys-
tems. Our results introduce a new way to
judge the success of automatic summarization
systems and bring quantified explanations to
questions such as why it was so hard for the
systems to date to have a statistically signifi-
cant improvement over the lead baseline in the
news domain.
1 Introduction
Topic identification is the first stage of the gener-
ally accepted three-phase model in automatic text
summarization, in which the goal is to identify the
most important units in a document, i.e., phrases,
sentences, or paragraphs (Hovy and Lin, 1999; Lin,
1999; Sparck-Jones, 1999). This stage is followed
by the topic interpretation and summary generation
steps where the identified units are further processed
to bring the summary into a coherent, human read-
able abstract form. The extractive summarization
systems, however, only employ the topic identifi-
cation stage, and simply output a ranked list of the
units according to a compression ratio criterion. In
general, for most systems sentences are the preferred
units in this stage, as they are the smallest grammat-
ical units that can express a statement.
Since the sentences in a document are reproduced
verbatim in extractive summaries, it is theoretically
possible to explore the search space of this problem
through an enumeration of all possible extracts for
a document. Such an exploration would not only
allow us to see how far we can go with extractive
summarization, but we would also be able to judge
the difficulty of the problem by looking at the dis-
tribution of the evaluation scores for the generated
extracts. Moreover, the high scoring extracts could
also be used to train a machine learning algorithm.
However, such an enumeration strategy has an
exponential complexity as it requires all possible
sentence combinations of a document to be gener-
ated, constrained by a given word or sentence length.
Thus the problem quickly becomes impractical as
the number of sentences in a document increases and
the compression ratio decreases. In this work, we try
to overcome this bottleneck by using a large cluster
of computers, and decomposing the task into smaller
problems by using the given section boundaries or a
linear text segmentation method. As a result of this
exploration, we generate a probability density func-
tion (pdf) of the ROUGE score (Lin, 2004) distri-
butions for four different domains, which shows the
distribution of the evaluation scores for the gener-
ated extracts, and allows us to assess the difficulty
of each domain for extractive summarization.
Furthermore, using these pdfs, we introduce a
new success measure for extractive summarization
systems. Namely, given a system?s average score
over a data set, we show how to calculate the per-
903
centile rank of this system from the corresponding
pdf of the data set. This allows us to see the true
improvement a system achieves over another, such
as a baseline, and provides a standardized scoring
scheme for systems performing on the same data set.
2 Related Work
Despite the large amount of work in automatic
text summarization, there are only a few studies
in the literature that employ an exhaustive search
strategy to create extracts, which is mainly due to
the prohibitively large search space of the prob-
lem. Furthermore, the research regarding the align-
ment of abstracts to original documents has shown
great variations across domains (Kupiec et al, 1995;
Teufel and Moens, 1997; Marcu, 1999; Jing, 2002;
Ceylan and Mihalcea, 2009), which indicates that
the extractive summarization techniques are not ap-
plicable to all domains at the same level.
In order to automate the process of corpus
construction for automatic summarization systems,
(Marcu, 1999) used exhaustive search to generate
the best Extract from a given (Abstract, Text) tuple,
where the best Extract contains a set of clauses from
Text that have the highest similarity to the given Ab-
stract.
In addition, (Donaway et al, 2000) used exhaus-
tive search to create all the sentence extracts of
length three starting with 15 TREC Documents, in
order to judge the performance of several summary
evaluation measures suggested in their paper.
Finally, the study most similar to ours was done
by (Lin and Hovy, 2003), who used the articles with
less than 30 sentences from the DUC 2001 data set
to find oracle extracts of 100 and 150 (?5) words.
These extracts were compared against one summary
source, selected as the one that gave the highest
inter-human agreement. Although it was concluded
that a 10% improvement was possible for extrac-
tive summarization systems, which typically score
around the lead baseline, there was no report on how
difficult it would be to achieve this improvement,
which is the main objective of our paper.
3 Description of the Data Set
Our data set is composed of four different domains:
newswire, literary, scientific and legal. For all the
Domain ?Dw ?Sw ?R ?C ?Cw
Newswire 641 101 84% 1 641
Literary 4973 1148 77% 6 196
Scientific 1989 160 92% 9 221
Legal 3469 865 75% 18 192
Table 1: Statistical properties of the data set. ?Dw, and
?Sw represent the average number of words for each doc-
ument and summary respectively; ?R indicates the av-
erage compression ratio; and ?C and ?Cw represent the
average number of sections for each document, and the
average number of words for each section respectively.
domains we used 50 documents and only one sum-
mary for each document, except for newswire where
we used two summaries per document. For the
newswire domain, we selected the articles and their
summaries from the DUC 2002 data set,1. For the
literary domain, we obtained 10 novels that are lit-
erature classics, and available online in text format.
Further, we collected the corresponding summaries
for these novels from various websites such as
CliffsNotes (www.cliffsnotes.com) and SparkNotes
(www.sparknotes.com), which make available hu-
man generated abstracts for literary works. These
sources give a summary for each chapter of the
novel, so each chapter can be treated as a sepa-
rate document. Thus we evaluate 50 chapters in to-
tal. For the scientific domain, we selected the ar-
ticles from the medical journal Autoimmunity Re-
views2 were selected, and their abstracts are used
as summaries. Finally, for the legal domain, we
gathered 50 law documents and their corresponding
summaries from the European Legislation Website,3
which comprises four types of laws - Council Di-
rectives, Acts, Communications, and Decisions over
several topics, such as society, environment, educa-
tion, economics and employment.
Although all the summaries are human generated
abstracts for all the domains, it is worth mention-
ing that the documents and their corresponding sum-
maries exhibit a specific writing style for each do-
main, in terms of the vocabulary used and the length
of the sentences. We list some of the statistical prop-
erties of each domain in Table 1.
1http://www-nlpir.nist.gov/projects/duc/data.html
2http://www.elsevier.com/wps/product/cws home/622356
3http://eur-lex.europa.eu/en/legis/index.htm
904
4 Experimental Setup
As mentioned in Section 1, an exhaustive search
algorithm requires generating all possible sentence
combinations from a document, and evaluating each
one individually. For example, using the values from
Table 1, and assuming 20 words per sentence, we
find that the search space for the news domain con-
tains approximately
(32
5
)
? 50 = 10, 068, 800 sum-
maries. The same calculation method for the sci-
entific domain gives us
(99
8
)
? 50 = 8.56 ? 1012
summaries. Obviously the search space gets much
bigger for the legal and literary domains due to their
larger text size.
In order to be able to cope with such a huge
search space, the first thing we did was to modify
the ROUGE 1.5.54 Perl script by fixing the parame-
ters to those used in the DUC experiments,5 and also
by modifying the way it handles the input and output
to make it suitable for streaming on the cluster.
The resulting script evaluates around 25-30 sum-
maries per second on an Intel 2.33 GHz processor.
Next, we streamed the resulting ROUGE script for
each (document, summary) pair on a large cluster
of computers running on an Hadoop Map-Reduce
framework.6 Based on the size of the search space
for a (document, summary) pair, the number of com-
puters allocated in the cluster ranged from just a few
to more than one thousand.
Although the combination of a large cluster and a
faster ROUGE is enough to handle most of the doc-
uments in the news domain in just a few hours, a
simple calculation shows that the problem is still im-
practical for the other domains. Hence for the scien-
tific, legal, and literary domains, rather than consid-
ering each document as a whole, we divide them into
sections, and create extracts for each section such
that the length of the extract is proportional to the
length of the section in the original document. For
the legal and scientific domains, we use the given
section boundaries (without considering the subsec-
tions for scientific documents). For the novels, we
treat each chapter as a single document (since each
chapter has its own summary), which is further di-
vided into sections using a publicly available linear
4http://berouge.com
5
-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0
6http://hadoop.apache.org/
text segmentation algorithm by (Utiyama and Isa-
hara, 2001).7 In all cases, we let the algorithm pick
the number of segments automatically.
To evaluate the sections, we modified ROUGE
further so that it applies the length constraint to the
extracts only, not to the model summaries. This is
due to the fact that we evaluate the extracts of each
section individually against the whole model sum-
mary, which is larger than the extract. This way,
we can get an overall ROUGE recall score for a
document extract, simply by summing up the re-
call scores of each section extracts. The precision
score for the entire document can also be found by
adding the weighted precision scores for each sec-
tion, where the weight is proportional to the length
of the section in the original document. In our study,
however, we only use recall scores.
Note that, since for the legal, scientific, and lit-
erary domains we consider each section of a doc-
ument independently, we are not performing a true
exhaustive search for these domains, but rather solv-
ing a suboptimal problem, as we divide the number
of words in the model summary to each section pro-
portional to the section?s length. However, we be-
lieve that this is a fair assumption, as it has been
shown repeatedly in the past that text segmentation
helps improving the performance of text summariza-
tion systems (yen Kan et al, 1998; Nakao, 2000;
Mihalcea and Ceylan, 2007).
5 Exhaustive Search Algorithm
Let Eik = Si1 , Si2 , ..., Sik be the ith extract that
has k sentences, and generated from a document
D with n sentences D = S1, S2, . . . , Sn. Further,
let len(Sj) give the number of words in sentence
Sj . We enforce that Eik satisfies the following con-
straints:
len(Eik) = len(Si1) + . . . + len(Sik) ? L
len(Eik?1) = len(Si1) + . . . + len(Sik?1) < L
where L is the length constraint on all the extracts
of document D. We note that for any Eik , the or-
der of the sentences in Eik?1 does not affect the
ROUGE scores, since only the last sentence may be
7http://mastarpj.nict.go.jp/ mutiyama/software/textseg/textseg-
1.211.tar.gz
905
chopped off due to the length constraint.8 Hence, we
start generating sentence combinations
(n
r
)
in lexico-
graphic order, for r = 1...n, and for each combina-
tion Eik = Si1 , Si2 , ..., Sik where k > 1, we gener-
ate additional extracts E?ik by successfully swapping
Sij with Sik for j = 1, ..., k? 1 and checking to see
if the above constraints are still satisfied. Therefore
from a combination with k sentences that satisfies
the constraints, we might generate up to k ? 1 ad-
ditional extracts. Finally, we stop the process either
when r = n and the last combination is generated,
or we cannot find any extract that satisfies the con-
straints for r.
6 Generating pdfs
Once the extracts for a document are generated and
evaluated, we go through each result and assign its
recall score to a range, which we refer to as a bin.
We use 1, 000 equally spaced bins between 0 and
1. As an example, a recall score of 0.46873 would
be assigned to the bin [0.468, 0.469]. By keeping
a count for each bin, we are in fact building a his-
togram of scores for the document. Let this his-
togram be h, and h[j] be the value in the jth bin of
the histogram. We then define the normalized his-
togram h? as:
h?[j] =
N
?N
i=1 h[j]
h[j] (1)
where N = 1, 000 is the number of bins in the his-
togram. Note that since the width of each bin is 1N ,
the Riemann sum of the normalized histogram h? is
equal to 1, so h? can be used as an approximation
to the underlying pdf. As an example, we show the
histogram h? for the newswire document AP890323-
0218 in Figure 1.
We combine the normalized histograms of all the
documents in a domain in order to find the pdf for
that domain. This requires multiplying the value
of each bin in a document?s histogram, with all
the other possible combinations of bin values taken
from each of the remaining histograms, and assign-
ing the result to the average bin for each combina-
8Note that we do not take the coherence of extracts into ac-
count, i.e. the sentences in an extract do not need to be sorted
in order of their appearance in the original document. We also
do not change the position of the words in a sentence.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 0  100  200  300  400  500  600  700  800  900  1000
"AP890323-0218.dat"
Figure 1: The normalized histogram h? of ROUGE-1 re-
call scores for the newswire document AP890323-0218.
tion. This can be done iteratively by keeping a mov-
ing average. We illustrate this procedure in Algo-
rithm 1, where K represents the number of docu-
ments in a domain.
Algorithm 1 Combine h?i?s for i = 1, . . . ,K to cre-
ate hd, the histogram for domain d.
1: hd := {}
2: for i = 1 to N do
3: hd[i] := h?1[i]
4: end for
5: for i = 2 to K do
6: ht = {}
7: for j = 1 to N do
8: for k = 1 to N do
9: a = round(((k ? (i? 1)) + j)/i)
10: ht[a] = ht[a] + (hd[k] ? h?i[j])
11: end for
12: end for
13: hd := ht
14: end for
The resulting histogram hd, when normalized us-
ing Equation 1, is an approximation to the pdf for
domain d. Furthermore, we used the round() func-
tion in line 9, which rounds a number to the nearest
integer, as the bins are indexed by integers. Note
that this rounding introduces an error, which is dis-
tributed uniformly due to the nature of the round()
function. It is also possible to lower the affect of this
error with higher resolutions (i.e. larger number of
bins). In Figure 2, we show a sample hd, obtained
by combining 10 documents from the newswire do-
906
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
"newswire_10-ROUGE-1.dat"
Figure 2: An example pdf obtained by combining 10 doc-
ument histograms of ROUGE-1 recall scores from the
newswire domain. The x-axis is normalized to [0,1].
main.
Recall from Section 4 that the documents in
the literary, legal, and scientific domains are di-
vided into sections either by using the given section
boundaries or by applying a text segmentation al-
gorithm, and the extracts of each section are then
evaluated individually. Hence for these domains, we
first calculate the histogram of each section individ-
ually, and then combine them to find the histogram
of a document. The combination procedure for the
section histograms is similar to Algorithm 1, except
that in this case we do not keep a moving average,
but rather sum up the bins of the sections. Note
that when bin i and j are added, the resulting val-
ues should be expected to be half the times in bin
i + j, and half the times in i + j ? 1.
7 Calculating Percentile Ranks
Given a pdf for a domain, the success of a system
having a ROUGE recall score of S could be sim-
ply measured by finding the area bounded by S.
This gives us the percentile rank of the system in
the overall distribution. Assuming 0 ? S ? 1, let
S? = ?N ?S?, then the formula to calculate the per-
centile rank can be simply given as:
PR(S) =
100
N
bS?
i=1
h?d[i] (2)
ROUGE-1
Domain ? ? max min
Newswire 39.39 0.87 65.70 20.20
Literary 45.20 0.47 63.90 28.40
Scientific 45.99 0.68 71.90 24.20
Legal 72.82 0.28 82.40 62.80
ROUGE-2
Domain ? ? max min
Newswire 11.57 0.79 37.40 1.60
Literary 5.41 0.34 16.90 1.80
Scientific 10.98 0.60 33.30 1.30
Legal 28.74 0.29 40.90 19.60
ROUGE-SU4
Domain ? ? max min
Newswire 15.33 0.69 38.10 6.40
Literary 13.28 0.30 24.30 6.90
Scientific 16.13 0.50 35.80 6.20
Legal 35.63 0.25 45.70 28.70
Table 2: Statistical properties of the pdfs
8 Results
The ensemble distributions of ROUGE-1 recall
scores per document are shown in Figure 3. The
ensemble distributions tell us that the performance
of the extracts, especially for the news and the sci-
entific domains, are mostly uniform for each docu-
ment. This is due to the fact that documents in these
domains, and their corresponding summaries, are
written with a certain conventional style. There is
however a little scattering in the distributions of the
literary and the legal domains. This is an expected
result for the literary domain, as there is no specific
summarization style for these documents, but some-
how surprising for the legal domain, where the effect
is probably due to the different types of legal docu-
ments in the data set.
The pdf plots resulting from the ROUGE-1 recall
scores are shown in Figure 4.9 In order to analyze
the pdf plots, and better understand their differences,
Table 2 lists the mean (?) and the standard deviation
(?) measures of the pdfs, as well as the average min-
imum and maximum scores that an extractive sum-
marization system can get for each domain.
By looking at the pdf plots and the minimum and
maximum columns from Table 2, we notice that for
9Similar pdfs are obtained for ROUGE-2 and ROUGE-SU4,
even if at a different scale.
907
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  5  10  15  20  25  30  35  40  45  50
"Ensemble-Newswire-50-ROUGE-1.dat"
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25  30  35  40  45  50
"Literary-50-Ensemble-ROUGE-1.dat"
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25  30  35  40  45  50
"Medical-50-Ensemble-ROUGE-1.dat"
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25  30  35  40  45  50
"Legal-50-Ensemble-ROUGE-1.dat"
Figure 3: ROUGE-1 recall score distributions per document for Newswire, Literary, Scientific and Legal Domains,
respectively from left to right.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 0.36  0.38  0.4  0.42  0.44
"Newswire-50-ROUGE-1.dat"
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0.4  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.5
"Literary-50-ROUGE-1.dat"
 0
 10
 20
 30
 40
 50
 60
 70
 0.4  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.5
"Medical-50-ROUGE-1.dat"
 0
 20
 40
 60
 80
 100
 120
 140
 160
 0.7  0.72  0.74  0.76  0.78  0.8
"Legal-50-ROUGE-1.dat"
Figure 4: Probability Density Functions of ROUGE-1 recall scores for the Newswire, Literary, Scientific and Legal
Domains, respectively from left to right. The resolution of the x-axis is increased to 0.1.
all the domains, the pdfs are long-tailed distribu-
tions. This immediately implies that most of the
extracts in a summary space are clustered around
the mean, which means that for automatic summa-
rization systems, it is very easy to get scores around
this range. Furthermore, we can judge the hardness
of each domain by looking at the standard devia-
tion values. A lower standard deviation indicates a
steeper curve, which implies that improving a sys-
tem would be harder. From the table, we can in-
fer that the legal domain is the hardest while the
newswire is the easiest.
Comparing Table 2 with the values in Table 1,
we also notice that the compression ratio affects the
performance differently for each domain. For ex-
ample, although the scientific domain has the high-
est compression ratio, it has a higher mean than
the literary and the newswire domains for ROUGE-
1 and ROUGE-SU4 recall scores. This implies
that although the abstracts of the medical journals
are highly compressed, they have a high overlap
with the document, probably caused by their writ-
ing style. This was in fact confirmed earlier by the
experiments in (Kupiec et al, 1995), where it was
found out that for a data set of 188 scientific arti-
cles, 79% of the sentences in the abstracts could be
perfectly matched with the sentences in the corre-
sponding documents.
Next, we confirm our experiments by testing three
different extractive summarization systems on our
data set. The first system that we implement is called
Random, and gives a random score between 1 and
100 to each sentence in a document, and then se-
lects the top scoring sentences. The second system,
Lead, implements the lead baseline method which
takes the first k sentences of a document until the
length limit is reached. Finally, the last system that
we implement is TextRank, which uses a variation of
the PageRank graph centrality algorithm in order to
identify the most important sentences in a document
(Page et al, 1999; Erkan and Radev, 2004; Mihalcea
and Tarau, 2004). We selected TextRank as it has a
performance competitive with the top systems par-
ticipating in DUC ?02 (Mihalcea and Tarau, 2004).
We would also like to mention that for the literary,
scientific, and legal domains, the systems apply the
algorithms for each section and each section is eval-
uated independently, and their resulting recall scores
are summed up. This is needed in order to be con-
sistent with our exhaustive search experiments.
The ROUGE recall scores of the three systems are
shown in Table 3. As expected, for the literary and
legal domains, the Random, and the Lead systems
score around the mean. This is due to the fact that
the leading sentences for these two domains do not
indicate any significance, hence the Lead system just
behaves like Random. However for the scientific and
newswire domains, the leading sentences do have
908
ROUGE-1
Domain Random Lead TextRank
Newswire 39.13 45.63 44.43
Literary 45.39 45.36 46.12
Scientific 45.75 47.18 49.26
Legal 73.04 72.42 74.82
ROUGE-2
Domain Random Lead TextRank
Newswire 11.39 19.60 17.99
Literary 5.33 5.41 5.92
Scientific 10.73 12.07 12.76
Legal 28.56 28.92 31.06
ROUGE-SU4
Domain Random Lead TextRank
Newswire 15.07 21.58 20.46
Literary 13.21 13.28 13.81
Scientific 15.92 17.12 17.85
Legal 35.41 35.55 37.64
Table 3: ROUGE recall scores of the Lead baseline, Tex-
tRank, and Random sentence selector across domains
importance so the Lead system consistently outper-
forms Random. Furthermore, although TextRank is
the best system for the literary, scientific, and legal
domains, it gets outperformed by the Lead system
on the newswire domain. This is also an expected re-
sult as none of the single-document summarization
systems were able to achieve a statistically signifi-
cant improvement over the lead baseline in the previ-
ous Document Understanding Conferences (DUC).
The ROUGE scoring scheme does not tell us how
much improvement a system achieved over another,
or how far it is from the upper bound. Since we now
have access to the pdf of each domain in our data set,
we can find this information simply by calculating
the percentile rank of each system using the formula
given in Equation 2.
The percentile ranks of all three systems for each
domain are shown in Table 4. Notice how different
the gap is between the scores of each system this
time, compared to the scores in Table 3. For ex-
ample, we see in Table 3 that TextRank on scientific
domain has only a 3.51 ROUGE-1 score improve-
ment over a system that randomly selects sentences
to include in the extract. However, Table 4 tells us
that this improvement is in fact 57.57%.
From Table 4, we see that both TextRank and
the Lead system are in the 99.99% percentile of
ROUGE-1
Domain Random Lead TextRank
Newswire %39.18 %99.99 %99.99
Literary %62.89 %62.89 %97.90
Scientific %42.30 %95.56 %99.87
Legal %79.47 %16.19 %99.99
ROUGE-2
Domain Random Lead TextRank
Newswire %39.57 %99.99 %99.99
Literary %42.20 %54.32 %94.34
Scientific %35.6 %96.03 %99.79
Legal %36.68 %75.38 %99.99
ROUGE-SU4
Domain Random Lead TextRank
Newswire %40.68 %99.99 %99.99
Literary %46.39 %46.39 %96.84
Scientific %36.37 %97.69 %99.94
Legal %23.53 %42.00 %99.99
Table 4: Percentile rankings of the Lead baseline, Tex-
tRank, and Random sentence selector across domains
the newswire domain although the systems have
1.20, 1.61, and 1.12 difference in their ROUGE-1,
ROUGE-2, and ROUGE-SU4 scores respectively.
The high percentile for the Lead system explains
why it was so hard to improve over these baseline in
previous evaluations on newswire data (e.g., see the
evaluations from the Document Understanding Con-
ferences). Furthermore, we see from Table 2 that the
upper bounds corresponding to these scores are 65.7,
37.4, and 38.1 respectively, which are well above
both the TextRank and the Lead systems. There-
fore, the percentile rankings of the Lead and the Tex-
tRank systems for this domain do not seem to give
us clues about how the two systems compare to each
other, nor about their actual distance from the up-
per bounds. There are two reasons for this: First,
as we mentioned earlier, most of the summary space
consists of easy extracts, which make the distribu-
tion long-tailed.10 Therefore even though we have
quite a bit of systems achieving high scores, their
number is negligible compared to the millions of ex-
tracts that are clustered around the mean. Secondly,
we need a higher resolution (i.e. larger number of
bins) in constructing the pdfs in order to be able to
10This also accounts for the fact that even though we might
have two very close ROUGE scores that are not statistically sig-
nificant, their percentile rankings might differ quite a bit.
909
see the difference more clearly between the two sys-
tems. Finally, when comparing two successful sys-
tems using percentile ranks, we believe the use of
error reduction would be more beneficial.
As a final note, we also randomly sampled ex-
tracts from documents in the scientific and legal do-
mains, but this time without considering the section
boundaries and without performing any segmenta-
tion. We kept the number of samples for each doc-
ument equal to the number of extracts we generated
from the same document using a divide-and-conquer
approach. We evaluated the samples using ROUGE-
1 recall scores, and obtained pdfs for each domain
using the same strategy discussed earlier in the pa-
per. The resulting pdfs, although they exhibit simi-
lar characteristics, they have mean values (?) around
10% lower than the ones we listed in Table 2, which
supports the findings from earlier research that seg-
mentation is useful for text summarization.
9 Conclusions and Future Work
In this paper, we described a study that explores the
search space of extractive summaries across four dif-
ferent domains. For the news domain we generated
all possible extracts of the given documents, and
for the literary, scientific, and legal domains we fol-
lowed a divide-and-conquer approach by chunking
the documents into sections, handled each section
independently, and combined the resulting scores at
the end. We then used the distributions of the eval-
uations scores to generate the probability density
functions (pdfs) for each domain. Various statistical
properties of these pdfs helped us asses the difficulty
of each domain. Finally, we introduced a new scor-
ing scheme for automatic text summarization sys-
tems that can be derived from the pdfs. The new
scheme calculates a percentile rank of the ROUGE-
1 recall score of a system, which gives scores in the
range [0-100]. This lets us see how far each sys-
tem is from the upper bound, and thus make a better
comparison among the systems. The new scoring
system showed us that while there is a 20.1% gap
between the upper bound and the lead baseline for
the news domain, closing this gap is difficult, as the
percentile rank of the lead baseline system, 99.99%,
indicates that the system is already very close to the
upper bound.
Furthermore, except for the literary domain, the
percentile rank of the TextRank system is also very
close to the upperbound. This result does not sug-
gest that additional improvements cannot be made
in these domains, but that making further improve-
ments using only extractive summarization will be
considerably difficult. Moreover, in order to see
these future improvements, a higher resolution (i.e.
larger number of bins) will be needed when con-
structing the pdfs.
In all our experiments we used the ROUGE
(Lin, 2004) evaluation package and its ROUGE-
1, ROUGE-2, and ROUGE-SU4 recall scores. We
would like to note that since ROUGE performs its
evaluations based on the n-gram overlap between
the peer and the model summary, it does not take
other summary quality metrics such as coherence
and cohesion into account. However, our goal in this
paper was to analyze the topic-identification stage
only, which concentrates on selecting the right con-
tent from the document to include in the summary,
and the ROUGE scores were found to correlate well
with the human judgments on assessing the content
overlap of summaries.
In the future, we would like to apply a similar ex-
haustive search strategy, but this time with differ-
ent compression ratios, in order to see the impact
of compression ratios on the pdf of each domain.
Furthermore, we would also like to analyze the
high scoring extracts found by the exhaustive search,
in terms of coherence, position and other features.
Such an analysis would allow us to see whether these
extracts exhibit certain properties which could be
used in training machine learning systems.
Acknowledgments
The authors would like to thank the anonymous re-
viewers of NAACL-HLT 2010 for their feedback.
The work of the first author has been partly sup-
ported by an award from Google, Inc. The work of
the fourth and fifth authors has been supported by an
FPI grant (BES-2007-16268) from the Spanish Min-
istry of Science and Innovation, under the project
TEXT-MESS (TIN2006-15265-C06-01) funded by
the Spanish Government, and the project PROME-
TEO Desarrollo de Tcnicas Inteligentes e Interacti-
vas de Minera de Textos (2009/119) from the Valen-
cian Government.
910
References
Hakan Ceylan and Rada Mihalcea. 2009. The decompo-
sition of human-written book summaries. In CICLing
?09: Proceedings of the 10th International Conference
on Computational Linguistics and Intelligent Text Pro-
cessing, pages 582?593, Berlin, Heidelberg. Springer-
Verlag.
Robert L. Donaway, Kevin W. Drummey, and Laura A.
Mather. 2000. A comparison of rankings produced
by summarization evaluation measures. In NAACL-
ANLP 2000 Workshop on Automatic summarization,
pages 69?78, Morristown, NJ, USA. Association for
Computational Linguistics.
G. Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summariza-
tion. Journal of Artificial Intelligence Research, 22.
Eduard H. Hovy and Chin Yew Lin. 1999. Automated
text summarization in summarist. In Inderjeet Mani
and Mark T. Maybury, editors, Advances in Automatic
Text Summarization, pages 81?97. MIT Press.
Hongyan Jing. 2002. Using hidden markov modeling to
decompose human-written summaries. Comput. Lin-
guist., 28(4):527?543.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In SIGIR ?95: Pro-
ceedings of the 18th annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, pages 68?73, New York, NY, USA.
ACM.
Chin-Yew Lin and Eduard Hovy. 2003. The potential
and limitations of automatic sentence extraction for
summarization. In Proceedings of the HLT-NAACL 03
on Text summarization workshop, pages 73?80, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Chin-Yew Lin. 1999. Training a selection function for
extraction. In CIKM ?99: Proceedings of the eighth
international conference on Information and knowl-
edge management, pages 55?62, New York, NY, USA.
ACM.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?
81, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. In
SIGIR ?99: Proceedings of the 22nd annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 137?144, New
York, NY, USA. ACM.
Rada Mihalcea and Hakan Ceylan. 2007. Explorations in
automatic book summarization. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 380?
389, Prague, Czech Republic, June. Association for
Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Conference on Empirical
Methods in Natural Language Processing, Barcelona,
Spain.
Yoshio Nakao. 2000. An algorithm for one-page sum-
marization of a long text based on thematic hierarchy
detection. In ACL ?00: Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 302?309, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford
InfoLab.
Karen Sparck-Jones. 1999. Automatic summarising:
Factors and directions. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Summa-
rization, pages 1?13. MIT Press.
Simone Teufel and Marc Moens. 1997. Sentence ex-
traction as a classification task. In Proceedings of the
ACL?97/EACL?97 Workshop on Intelligent Scallable
Text Summarization, Madrid, Spain, July.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
In Proceedings of the 9th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 491?498.
Min yen Kan, Judith L. Klavans, and Kathleen R. McK-
eown. 1998. Linear segmentation and segment sig-
nificance. In In Proceedings of the 6th International
Workshop of Very Large Corpora, pages 197?205.
911
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 107?115,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Experiments on Summary-based Opinion Classification
Elena Lloret
Department of Software
and Computing Systems
University of Alicante
Apdo. de Correos 99
E-03080, Alicante, Spain
elloret@dlsi.ua.es
Horacio Saggion
Department of Infomation and
Communication Technologies
Grupo TALN
Universitat Pompeu Fabra
C/Ta?nger, 122-134, 2nd floor
08018 Barcelona, Spain
horacio.saggion@upf.edu
Manuel Palomar
Department of Software
and Computing Systems
University of Alicante
Apdo. de Correos 99
E-03080, Alicante, Spain
mpalomar@dlsi.ua.es
Abstract
We investigate the effect of text summarisa-
tion in the problem of rating-inference ? the
task of associating a fine-grained numerical
rating to an opinionated document. We set-up
a comparison framework to study the effect of
different summarisation algorithms of various
compression rates in this task and compare the
classification accuracy of summaries and doc-
uments for associating documents to classes.
We make use of SVM algorithms to associate
numerical ratings to opinionated documents.
The algorithms are informed by linguistic and
sentiment-based features computed from full
documents and summaries. Preliminary re-
sults show that some types of summaries could
be as effective or better as full documents in
this problem.
1 Introduction
Public opinion has a great impact on company and
government decision making. In particular, compa-
nies have to constantly monitor public perception of
their products, services, and key company represen-
tatives to ensure that good reputation is maintained.
Recent cases of public figures making headlines for
the wrong reasons have shown how companies take
into account public opinion to distance themselves
from figures which can damage their public image.
The Web has become an important source for find-
ing information, in the field of business intelligence,
business analysts are turning their eyes to the Web
in order to monitor public perception on products,
services, policies, and managers. The field of senti-
ment analysis has recently emerged (Pang and Lee,
2008) as an important area of research in Natural
Language Processing (NLP) which can provide vi-
able solutions for monitoring public perception on
a number of issues; with evaluation programs such
as the Text REtrieval Conference track on blog min-
ing 1, the Text Analysis Conference 2 track on opin-
ion summarisation, and the DEfi Fouille de Textes
program (Grouin et al, 2009) advances in the state
of the art have been produced. Although sentiment
analysis involves various different problems such as
identifying subjective sentences or identifying posi-
tive and negative opinions in text, here we concen-
trate on the opinion classification task; and more
specifically on rating-inference, the task of identify-
ing the author?s evaluation of an entity with respect
to an ordinal-scale based on the author?s textual eval-
uation of the entity (Pang and Lee, 2005). The spe-
cific problem we study in this paper is that of as-
sociating a fine-grained rating (1=worst,...5=best)
to a review. This is in general considered a dif-
ficult problem because of the fuzziness inherent of
mid-range ratings (Mukras et al, 2007). A consid-
erable body of research has recently been produced
to tackle this problem (Chakraborti et al, 2007; Fer-
rari et al, 2009) and reported figures showing accu-
racies ranging from 30% to 50% for such complex
task; most approaches derive features for the classi-
fication task from the full document. In this research
we ask whether extracting features from document
summaries could help a classification system. Since
text summaries are meant to contain the essential
content of a document (Mani, 2001), we investigate
whether filtering noise through text summarisation
is of any help in the rating-inference task. In re-
1http:trec.nist.gov/
2http://www.nist.gov/tac/
107
cent years, text summarisation has been used to sup-
port both manual and automatic tasks; in the SUM-
MAC evaluation (Mani et al, 1998), text summaries
were tested in document classification and ques-
tion answering tasks where summaries were consid-
ered suitable surrogates for full documents; Bagga
and Baldwin (1998) studied summarisation in the
context of a cross-document coreference task and
found that summaries improved the performance of
a clustering-based coreference mechanism; more re-
cently Latif and McGee (2009) have proposed text
summarisation as a preprocessing step for student
essay assessment finding that summaries could be
used instead of full essays to group ?similar? qual-
ity essays. Summarisation has been studied in the
field of sentiment analysis with the objective of pro-
ducing opinion summaries, however, to the best of
our knowlegde there has been little research on the
study of document summarisation as a text pro-
cessing step for opinion classification. This paper
presents a framework and extensive experiments on
text summarisation for opinion classification, and in
particular, for the rating-inference problem. We will
present results indicating that some types of sum-
maries could be as effective or better than the full
documents in this task.
The remainder of the paper is organised as fol-
lows: Section 2 will compile the existing work with
respect to the inference-rating problem; Section 3
and Section 4 will describe the corpus and the NLP
tools used for all the experimental set-up. Next, the
text summarisation approaches will be described in
Section 5, and then Section 6 will show the exper-
iments conducted and the results obtained together
with a discussion. Finally, we will draw some con-
clusions and address further work in Section 7.
2 Related Work
Most of the literature regarding sentiment analysis
addresses the problem either by detecting and clas-
sifying opinions at a sentence level (Wilson et al,
2005; Du and Tan, 2009), or by attempting to cap-
ture the overall sentiment of a document (McDonald
et al, 2007; Hu et al, 2008). Traditional approaches
tackle the task as binary classification, where text
units (e.g. words, sentences, fragments) are classi-
fied into positive vs. negative, or subjective vs. ob-
jective, according to their polarity and subjectivity
degree, respectively. However, sentiment classifica-
tion taking into account a finer granularity has been
less considered. Rating-inference is a particular task
within sentiment analysis, which aims at inferring
the author?s numerical rating for a review. For in-
stance, given a review and 5-star-rating scale (rang-
ing from 1 -the worst- to 5 -the best), this task should
correctly predict the review?s rating, based on the
language and sentiment expressed in its content.
In (Pang and Lee, 2005), the rating-inference
problem is analysed for the movies domain. In
particular, the utility of employing label and item
similarity is shown by analysing the performance
of three different methods based on SVM (one vs.
all, regression and metric labeling), in order to infer
the author?s implied numerical rating, which ranges
from 1 up to 4 stars, depending on the degree the au-
thor of the review liked or not the film. The approach
described in (Leung et al, 2006) suggests the use of
collaborative filtering algorithms together with sen-
timent analysis techniques to obtain user preferences
expressed in textual reviews, focusing also on movie
reviews. Once the opinion words from user reviews
have been identified, the polarity of those opinion
words together with their strength need to be com-
puted and mapped to the rating scales to be further
input to the collaborative input algorithms.
Apart from these approaches, this problem is
stated from a different point of view in (Shimada
and Endo, 2008). Here it is approached from the
perspective of rating different details of a product
under the same review. Consequently, they rename
the problem as ?seeing several stars? instead of only
one, corresponding to the overall sentiment of the
review. Also, in (Baccianella et al, 2009) the rating
of different features regarding hotel reviews (cleanli-
ness, location, staff, etc.) is addressed by analysing
several aspects involved in the generation of prod-
uct review?s representations, such as part-of-speech
and lexicons. Other approaches (Devitt and Ahmad,
2007), (Turney, 2002) face this problem by group-
ing documents with closer stars under the same cat-
egory, i.e. positive or negative, simplifying the task
into a binary classification problem.
Recently, due to the vast amount of on-line infor-
mation and the subjectivity appearing in documents,
the combination of sentiment analysis and summari-
108
sation task in tandem can result in great benefits
for stand-alone applications of sentiment analysis,
as well as for the potential uses of sentiment analy-
sis as part of other NLP applications (Stoyanov and
Cardie, 2006). Whilst there is much literature com-
bining sentiment analysis and text summarisation
focusing on generating opinion-oriented summaries
for the new textual genres, such as blogs (Lloret
et al, 2009), or reviews (Zhuang et al, 2006), the
use of summaries as substitutes of full documents in
tasks such as rating-inference has been not yet ex-
plored to the best of our knowledge. In contrast to
the existing literature, this paper uses summaries in-
stead of full reviews to tackle the rating-inference
task in the financial domain, and we carry out a pre-
liminary analysis concerning the potential benefits
of text summaries for this task.
3 Dataset for the Rating-inference Task
Since there is no standard dataset for carrying out
the rating-inference task, the corpus used for our ex-
periments was one associated to a current project on
business intelligence we are working on. These data
consisted of 89 reviews of several English banks
(Abbey, Barcalys, Halifax, HSBC, Lloyds TSB, and
National Westminster) gathered from the Internet. In
particular the documents were collected from Ciao3,
a Website where users can write reviews about dif-
ferent products and services, depending on their own
experience.
Table 1 lists some of the statistical properties of
the data. It is worth stressing upon the fact that
the reviews have on average 2,603 words, which
means that we are dealing with long documents
rather than short ones, making the rating-inference
task even more challenging. The shortest document
contains 1,491 words, whereas the longest document
has more than 5,000 words.
# Reviews Avg length Max length Min length
89 2,603 5,730 1,491
Table 1: Corpus Statistics
Since the aim of the task we are pursuing focuses
on classifying correctly the star for a review (rang-
ing from 1 to 5 stars), it is necessary to study how
3http://www.ciao.co.uk/
many reviews we have for each class, in order to see
whether we have a balanced distribution or not. Ta-
ble 2 shows this numbers for each star-rating. It is
worth mentioning that one-third of the reviews be-
long to the 4-star class. In contrast, we have only 9
reviews that have been rated as 3-star, consisting of
the 10% of the corpus, which is a very low number.
Star-rating # reviews %
1-star 17 19
2-star 11 12
3-star 9 10
4-star 28 32
5-star 24 27
Table 2: Class Distribution
4 Natural Language Processing Tools
Linguistic analysis of textual input is carried out
using the General Architecture for Text Engineer-
ing (GATE) ? a framework for the development and
deployment of language processing technology in
large scale (Cunningham et al, 2002). We make use
of typical GATE components: tokenisation, parts of
speech tagging, and morphological analysis to pro-
duce document annotations. From the annotations
we produce a number of features for document rep-
resentation. Features produced from the annotations
are: string ? the original, unmodified text of each
token; root ? the lemmatised, lower-case form of
the token; category ? the part-of-speech (POS) tag, a
symbol that represents a grammatical category such
as determiner, present-tense verb, past-tense verb,
singular noun, etc.; orth ? a code representing the to-
ken?s combination of upper- and lower-case letters.
In addition to these basic features, ?sentiment? fea-
tures based on a lexical resource are computed as
explained below.
4.1 Sentiment Features
SentiWordNet (Esuli and Sebastiani, 2006) is a lexi-
cal resource in which each synset (set of synonyms)
of WordNet (Fellbaum, 1998) is associated with
three numerical scores obj (how objective the word
is), pos (how positive the word is), and neg (how
negative the word is). Each of the scores ranges
from 0 to 1, and their sum equals 1. SentiWord-
Net word values have been semi-automatically com-
puted based on the use of weakly supervised classi-
109
fication algorithms. In this work we compute the
?general sentiment? of a word in the following way:
given a word w we compute the number of times the
word w is more positive than negative (positive >
negative), the number of times is more negative than
positive (positive < negative) and the total number
of entries of word w in SentiWordNet, therefore we
can consider the overall positivity or negativity a
particular word has in SentiWordNet. We are in-
terested in words that are generally ?positive?, gen-
erally ?negative? or generally ?neutral? (not much
variation between positive and negative). For exam-
ple a word such as ?good? has many more entries
where the positive score is greater than the nega-
tivity score while a word such as ?unhelpful? has
more negative occurrences than positive. We use this
aggregated scores in our classification experiments.
Note that we do not apply any word sense disam-
biguation procedure here.
4.2 Machine Learning Tool
For the experiments reported here, we adopt a Sup-
port Vector Machine (SVM) learning paradigm not
only because it has recently been used with suc-
cess in different tasks in natural language processing
(Isozaki and Kazawa, 2002), but it has been shown
particularly suitable for text categorization (Kumar
and Gopal, 2009) where the feature space is huge, as
it is in our case. We rely on the support vector ma-
chines implementation distributed with the GATE
system (Li et al, 2009) which hides from the user
the complexities of feature extraction and conver-
sion from documents to the machine learning imple-
mentation. The tool has been applied with success
to a number of datasets for opinion classification and
rating-inference (Saggion and Funk, 2009).
5 Text Summarisation Approach
In this Section, three approaches for carrying out the
summarisation process are explained in detail. First,
a generic approach is taken as a basis, and then, it is
adapted into a query-focused and a opinion-oriented
approach, respectively.
5.1 Generic Summarisation
A generic text summarisation approach is first taken
as a core, in which three main stages can be distin-
guished: i) document preprocessing; ii) relevance
detection; and ii) summary generation. Since we
work with Web documents, an initial preprocessing
step is essential to remove all unnecessary tags and
noisy information. Therefore, in the first stage the
body of the review out of the whole Web page is
automatically delimitated by means of patterns, and
only this text is used as the input for the next sum-
marisation stages. Further on, a sentence relevance
detection process is carried out employing different
combinations of various techniques. In particular,
the techniques employed are:
Term frequency (tf ): this technique has been
widely used in different summarisation approaches,
showing the the most frequent words in a document
contain relevant information and can be indicative of
the document?s topic (Nenkova et al, 2006)
Textual entailment (te): a te module (Ferra?ndez
et al, 2007) is used to detect redundant information
in the document, by computing the entailment be-
tween two consecutive sentences and discarding the
entailed ones. The identification of these entailment
relations helps to avoid incorporating redundant in-
formation in summaries.
Code quantity principle (cqp): this is a linguis-
tic principle which proves the existence of a propor-
tional relation between how important the informa-
tion is, and the number of coding elements it has
(Givo?n, 1990). In this approach we assume that sen-
tences containing longer noun-phrases are more rel-
evant.
The aforementioned techniques are combined
together taking always into account the term-
frequency, leading to different summarisation strate-
gies (tf, te+tf, cqp+tf, te+cqp+tf ). Finally, the re-
sulting summary is produced by extracting the high-
est scored sentences up to the desired length, accord-
ing the techniques explained.
5.2 Query-focused Summarisation
Through adapting the generic summarisation ap-
proach into a query-focused one, we could benefit
from obtaining more specific sentences with regard
to the topic of the review. As a preliminary work, we
are going to assume that a review is about a bank,
and as a consequence, the name of the bank is con-
sidered to be the topic. It is worth mentioning that a
person can refer to a specific bank in different ways.
For example, in the case of ?The National Westmin-
110
ster Bank?, it can be referred to as ?National West-
minster? or ?NatWest?. Such different denomina-
tions were manually identified and they were used
to biased the content of the generated summaries,
employing the same techniques of tf, te and the cqp
combined together. One limitation of this approach
is that we do not directly deal with the coreference
problem, so for example, sentences containing pro-
nouns referring also to the bank, will not be taken
into consideration in the summarisation process. We
are aware of this limitation and for future work it
would be necessary to run a coreference algorithm
to identify all occurrences of a bank within a review.
However, since the main goal of this paper is to carry
out a preliminary analysis of the usefulness of sum-
maries in contrast to whole reviews in the rating-
inference problem, we did not take this problem into
account at this stage of the research. In addition,
when we do query-focused summarisation only we
rely on the SUMMA toolkit (Saggion, 2008) to pro-
duce a query similarity value for each sentence in the
review which in turn is used to rank sentences for an
extractive summary (qf ). This similarity value is the
cosine similarity between a sentence vector (terms
and weights) and a query vector (terms and weigths)
and where the query is the name of the entity being
reviewed (e.g. National Westminster).
5.3 Opinion-oriented Summarisation
Since reviews are written by people who want to
express their opinion and experience with regard
to a bank, in this particular case, either generic or
query-focused summaries can miss including some
important information concerning their sentiments
and feelings towards this particular entity. There-
fore, a sentiment classification system similar to the
one used in (Balahur-Dobrescu et al, 2009) is used
together with the summarisation approach, in order
to generate opinion-oriented summaries. First of all,
the sentences containing opinions are identified, as-
signing each of them a polarity (positive and neg-
ative) and a numerical value corresponding to the
polarity strength (the higher the negative score, the
more negative the sentence and similarly, the higher
the positive score, the more positive the sentence).
Sentences containing a polarity value of 0 are con-
sidered neutral and are not taken into account. Once
the sentences are classified into positives, negatives
and neutrals, they are grouped together according
to its type. Further on, the same combination of
techniques as for previously explained summarisa-
tion approaches are then used.
Additionally, a summary containing only the most
positive and negative sentences is also generated (we
have called this type of summaries sent) in order to
check whether the polarity strength on its own could
be a relevant feature for the summarisation process.
6 Evaluation Environment
In this Section we are going to describe in detail all
the experimental set-up. Firstly, we will explain the
corpus we used together with some figures regard-
ing some statistics computed. Secondly, we will de-
scribe in-depth all the experiments we ran and the re-
sults obtained. Finally, an extensive discussion will
be given in order to analyse all the results and draw
some conclusions.
6.1 Experiments and Results
The main objective of the paper is to investigate the
influence of summaries in contrast to full reviews for
the rating-inference problem.
The purpose of the experiments is to analyse the
performance of the different suggested text sum-
marisation approaches and compare them to the per-
formance of the full review. Therefore, the experi-
ments conducted were the following: for each pro-
posed summarisation approach, we experimented
with five different types of compression rates for
summaries (ranging from 10% to 50%). Apart from
the full review, we dealt with 14 different sum-
marisation approaches (4 for generic, 5 for query-
focused and 5 for opinion-oriented summarisation),
as well as 2 baselines (lead and final, taking the first
or the last sentences according to a specific compres-
sion rate, respectively). Each experiment consisted
of predicting the correct star of a review, either with
the review as a whole or with one of the summari-
sation approaches. As we previously said in Sec-
tion 4, for predicting the correct star-rating, we used
machine learning techniques. In particular, differ-
ent features were used to train a SVM classifier with
10-fold cross validation4 , using the whole review:
4The classifier used was the one integrated within the GATE
framework: http://gate.ac.uk/
111
the root of each word, its category, and the calcu-
lated value employing the SentiWordNet lexicon, as
well as their combinations. As a baseline for the full
document we took into account a totally uninformed
approach with respect to the class with higher num-
ber of reviews, i.e. considering all documents as if
they were scored with 4 stars. The different results
according different features can be seen in Table 3.
Feature F?=1
baseline 0.300
root 0.378
category 0.367
sentiWN 0.333
root+category 0.356
root+sentiWN 0.333
category+sentiWN 0.389
root+category+sentiWN 0.413
Table 3: F-measure results using the full review for clas-
sification
Regarding the features for training the summaries,
it is worth mentioning that the best performing fea-
ture when no sentiment-based features are taken into
account is the one using the root of the words. Con-
sequently, this feature was used to train the sum-
maries. Moreover, since the best results using the
full review were obtained using the combination of
the all the features (root+category+sentiWN), we
also selected this combination to train the SVM
classifier with our summaries. Conducting both
experiments, we could analyse to what extent the
sentiment-based feature benefit the classification
process.
The results obtained are shown in Table 4 and
Table 5, respectively. These tables show the F-
measure value obtained for the classification task,
when features extracted from summaries are used
instead from the full review. On the one hand,
results using the root feature extracted from sum-
maries can be seen in Table 4. On the other hand,
Table 5 shows the results when the combination
of all the linguistic and sentiment-based features
(root+category+sentiWN), that has been extracted
from summaries, are used for training the SVM clas-
sifier.
We also performed two statistical tests in order
to measure the significance for the results obtained.
The tests we performed were the one-way Analy-
sis of Variance (ANOVA) and the t-test (Spiegel and
Castellan, 1998). Given a group of experiments, we
first run ANOVA for analysing the difference be-
tween their means. In case some differences are
found, we run the t-test between those pairs.
6.2 Discussion
A first analysis derived from the results obtained in
Table 3 makes us be aware of the difficulty associ-
ated to the rating-inference task. As can be seen,
a baseline without any information from the docu-
ment at all, is performing around 30%, which com-
pared to the remaining approaches is not a very bad
number. However, we assumed that dealing with
some information contained in documents, the clas-
sification algorithm will do better in finding the cor-
rect star associated to a review. This was the rea-
son why we experimented with different features
alone or in combination. From these experiments,
we obtained that the combination of linguistic and
semantic-based features leads to the best results, ob-
taining a F-measure value of 41%. If sentiment-
based features are not taken into account, the best
feature is the root of the word on its own. Further-
more, in order to analyse further combinations, we
ran some experiments with bigrams. However, the
results obtained did not improve the ones we already
had, so they are not reported in this paper.
As far as the results is concerned comparing the
use of summaries to the full document, it is worth
mentioning that when using specific summarisation
approaches, such as query-focused summaries com-
bined with term-frequency, we get better results than
using the full document with a 90% confidence in-
terval, according to a t-test. In particular, qf for 10%
is significant with respect to the full document, us-
ing only root as feature for training. For the results
regarding the combination of root, category and Sen-
tiWordNet, qf for 10% and qf+tf for 10% and 20%
are significant with respect to the full document.
Concerning the different summarisation ap-
proaches, it cannot be claimed a general tendency
about which ones may lead to the best results. We
also performed some significance tests between dif-
ferent strategies, and in most of the cases, the t-
test and the ANOVA did not report significance
over 95%. Only a few approaches were significant
at a 95% confidence level, for instance, te+cqp+tf
and sent+te+cqp+tf with respect to sent+cqp+tf
112
Approach Compression Rate
Summarisation method 10% 20% 30% 40% 50%
lead F?=1 0.411 0.378 0.367 0.311 0.322
final F?=1 0.322 0.389 0.300 0.467 0.456
tf F?=1 0.400 0.344 0.400 0.367 0.367
te+tf F?=1 0.367 0.422 0.411 0.389 0.322
cqp+tf F?=1 0.300 0.344 0.311 0.300 0.256
te+cqp+tf F?=1 0.422 0.356 0.333 0.300 0.322
qf F?=1 0.513 0.388 0.375 0.363 0.363
qf+tf F?=1 0.567 0.467 0.311 0.367 0.389
qf+te+tf F?=1 0.389 0.367 0.411 0.378 0.333
qf+cqp+tf F?=1 0.300 0.356 0.378 0.378 0.333
qf+te+cqp+tf F?=1 0.322 0.322 0.367 0.367 0.356
sent F?=1 0.344 0.380 0.391 0.290 0.336
sent+tf F?=1 0.378 0.425 0.446 0.303 0.337
sent+te+tf F?=1 0.278 0.424 0.313 0.369 0.347
sent+cqp+tf F?=1 0.333 0.300 0.358 0.358 0.324
sent+te+cqp+tf F?=1 0.446 0.334 0.358 0.292 0.369
Table 4: Classification results (F-measure) for summaries using root (lead = first sentences; final = last sentences;
tf = term frequency; te = textual entailment; cqp = code quantity principle with noun-phrases; qf = query-focused
summaries; and sent = opinion-oriented summaries)
for 10%; sent+tf in comparison to sent+cqp+tf
for 20%; or sent with respect to cqp+tf for 40%
and 50% compression rates. Other examples of
the approaches that were significant at a 90%
level of confidence are qf for 10% with respect to
sent+te+cqp+tf. Due to the wide range of summari-
sation strategies tested in the experiments, the results
obtained vary a lot and, due to the space limitations,
it is not possible to report all the tables. What it
seems to be clear from the results is that the code
quantity principle (see Section 5) is not contributing
much to the summarisation process, thus obtaining
poor results when it is employed. Intuitively, this
can be due to the fact that after the first mention of
the bank, there is a predominant use of pronouns,
and as a consequence, the accuracy of the tool that
identifies noun-phrases could be affected. The same
reason could be affecting the term-frequency calcu-
lus, as it is computed based on the lemmas of the
words, not taking into account the pronouns that re-
fer also to them.
7 Conclusion and Future Work
This paper presented a preliminary study of
inference-rating task. We have proposed here a new
framework for comparison and extrinsic evaluation
of summaries in a text-based classification task. In
our research, text summaries generated using differ-
ent strategies were used for training a SVM classifier
instead of full reviews. The aim of this task was to
correctly predict the category of a review within a 1
to 5 star-scale. For the experiments, we gathered 89
bank reviews from the Internet and we generated 16
summaries of 5 different compression rates for each
of them (80 different summaries for each review,
having generated in total 7,120 summaries). We also
experimented with several linguistic and sentiment-
based features for the classifier. Although the re-
sults obtained are not significant enough to state
that summaries really help the rating-inference task,
we have shown that in some cases the use of sum-
maries (e.g. query/entity-focused summaries) could
offer competitive advantage over the use of full doc-
uments and we have also shown that some summari-
sation techniques do not degrade the performance of
a rating-inference algorithm when compared to the
use of full documents. We strongly believe that this
preliminary study could serve as a starting point for
future developments.
Although we have carried out extensive experi-
mentation with different summarisation techniques,
compression rates, and document/summary features,
there are many issues that we have not explored. In
the future, we plan to investigate whether the re-
sults could be affected by the class distribution of
the reviews, and in this line we would like to see the
distribution of the documents using clustering tech-
113
Approach Compression Rate
Summarisation method 10% 20% 30% 40% 50%
lead F?=1 0.275 0.422 0.422 0.378 0.322
final F?=1 0.275 0.378 0.333 0.344 0.400
tf F?=1 0.411 0.422 0.411 0.378 0.378
te+tf F?=1 0.411 0.344 0.344 0.344 0.378
cqp+tf F?=1 0.358 0.267 0.333 0.222 0.289
te+cqp+tf F?=1 0.444 0.411 0.411 0.311 0.322
qf F?=1 0.563 0.488 0.400 0.375 0.350
qf+tf F?=1 0.444 0.411 0.433 0.367 0.356
qf+te+tf F?=1 0.322 0.367 0.356 0.344 0.344
qf+cqp+tf F?=1 0.292 0.322 0.367 0.333 0.356
qf+te+cqp+tf F?=1 0.356 0.378 0.356 0.367 0.356
sent F?=1 0.322 0.370 0.379 0.412 0.414
sent+tf F?=1 0.378 0.446 0.359 0.380 0.402
sent+te+tf F?=1 0.333 0.414 0.404 0.380 0.381
sent+cqp+tf F?=1 0.300 0.333 0.347 0.358 0.296
sent+te+cqp+tf F?=1 0.436 0.413 0.425 0.359 0.324
Table 5: Classification results (F-measure) for summaries using root, category and SentiWordNet (lead = first sen-
tences; final = last sentences; tf = term frequency; te = textual entailment; cqp = code quantity principle with
noun-phrases; qf = query-focused summaries; and sent = opinion-oriented summaries)
niques. Moreover, we would also like to investigate
what it would happen if we consider the values of the
star-rating scale as ordinal numbers, and not only as
labels for categories. We will replicate the exper-
iments presented here using as evaluation measure
the ?mean square error? which has been pinpointed
as a more appropriate measure for categorisation in
an ordinal scale. Finally, in the medium to long-
term we plan to extent the experiments and analy-
sis to other available datasets in different domains,
such as movie or book reviews, in order to see if
the results could be influenced by the nature of the
corpus, allowing also further results for comparison
with other approaches and assessing the difficulty of
the task from a perspective of different domains.
Acknowledgments
This research has been supported by the project PROM-
ETEO ?Desarrollo de Te?cnicas Inteligentes e Interacti-
vas de Miner??a de Textos? (2009/119) from the Valencian
Government. Moreover, Elena Lloret is funded by the
FPI program (BES-2007-16268) from the Spanish Min-
istry of Science and Innovation under the project TEXT-
MESS (TIN2006-15265-C06-01), and Horacio Saggion
is supported by a Ramo?n y Cajal Fellowship from the
Ministry of Science and Innovation, Spain. The authors
would also like to thank Alexandra Balahur for helping to
process the dataset with her Opinion Mining approach.
References
S. Baccianella, A. Esuli, and F. Sebastiani. 2009. Multi-
facet Rating of Product Reviews. In Proceedings of
the 31th European Conference on IR Research on Ad-
vances in Information Retrieval, pages 461?472.
A. Bagga and B. Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Space
Model. In Proceedings of the COLING-ACL, pages
79?85.
A. Balahur-Dobrescu, M. Kabadjov, J. Steinberger,
R. Steinberger, and A. Montoyo. 2009. Summarizing
Opinions in Blog Threads. In Proceedings of the Pa-
cific Asia Conference on Language, INformation and
Computation Conference, pages 606?613.
S. Chakraborti, R. Mukras, R. Lothian, N. Wiratunga,
S. Watt, and D Harper. 2007. Supervised Latent Se-
mantic Indexing using Adaptive Sprinkling. In Pro-
ceedings of IJCAI-07, pages 1582?1587.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A Framework and Graphi-
cal Development Environment for Robust NLP Tools
and Applications. In Proceedings of the ACL.
A. Devitt and K. Ahmad. 2007. Sentiment Polarity Iden-
tification in Financial News: A Cohesion-based Ap-
proach. In Proceedings of the ACL, pages 984?991.
W. Du and S. Tan. 2009. An Iterative Reinforcement
Approach for Fine-Grained Opinion Mining. In Pro-
ceedings of the NAACL, pages 486?493.
A. Esuli and F. Sebastiani. 2006. SENTIWORDNET: A
Publicly Available Lexical Resource for Opinion Min-
ing. In Proceedings of LREC, pages 417?422.
114
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database. The MIT Press, Cambridge, MA.
O. Ferra?ndez, D. Micol, R. Mun?oz, and M. Palomar.
2007. A Perspective-Based Approach for Solving Tex-
tual Entailment Recognition. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 66?71, June.
S. Ferrari, T. Charnois, Y. Mathet, F. Rioult, and
D. Legallois. 2009. Analyse de Discours ?Evaluatif,
Mode`le Linguistique et Applications. In Fouille de
donne?es d?opinion, volume E-17, pages 71?93.
T. Givo?n, 1990. Syntax: A functional-typological intro-
duction, II. John Benjamins.
C. Grouin, M. Hurault-Plantet, P. Paroubek, and J. B.
Berthelin. 2009. DEFT?07 : Une Campagne
d?Avaluation en Fouille d?Opinion. In Fouille de
donne?es d?opinion, volume E-17, pages 1?24.
Y. Hu, W. Li, and Q. Lu. 2008. Developing Evalua-
tion Model of Topical Term for Document-Level Sen-
timent Classification. In Proceedings of the 10th Pa-
cific Rim International Conference on Artificial Intel-
ligence, pages 175?186.
H. Isozaki and H. Kazawa. 2002. Efficient Support
Vector Classifiers for Named Entity Recognition. In
Proceedings of the 19th International Conference on
Computational Linguistics, pages 390?396.
M. A. Kumar and M. Gopal. 2009. Text Categorization
Using Fuzzy Proximal SVM and Distributional Clus-
tering of Words. In Proceedings of the 13th Pacific-
Asia Conference on Advances in Knowledge Discovery
and Data Mining, pages 52?61.
S. Latif and M. McGee Wood. 2009. A Novel Technique
for Automated Linguistic Quality Assessment of Stu-
dents? Essays Using Automatic Summarizers. Com-
puter Science and Information Engineering, World
Congress on, 5:144?148.
C. W. K. Leung, S. C. F. Chan, and F. L. Chung.
2006. Integrating Collaborative Filtering and Sen-
timent Analysis: A Rating Inference Approach. In
Proceedings of The ECAI 2006 Workshop on Recom-
mender Systems, pages 62?66.
Y. Li, K. Bontcheva, and H. Cunningham. 2009. Adapt-
ing SVM for Data Sparseness and Imbalance: A Case
Study in Information Extraction. Natural Language
Engineering, 15(2):241?271.
E. Lloret, A. Balahur, M. Palomar, and A. Montoyo.
2009. Towards Building a Competitive Opinion Sum-
marization System: Challenges and Keys. In Proceed-
ings of the NAACL. Student Research Workshop and
Doctoral Consortium, pages 72?77.
I. Mani, D. House, G. Klein, L. Hirshman, L. Obrst,
T. Firmin, M. Chrzanowski, and B. Sundheim. 1998.
The TIPSTER SUMMAC Text Summarization Evalu-
ation. Technical report, The Mitre Corporation.
I. Mani. 2001. Automatic Text Summarization. John
Benjamins Publishing Company.
R. McDonald, K. Hannan, T. Neylon, M. Wells, and
J. Reynar. 2007. Structured Models for Fine-to-
Coarse Sentiment Analysis. In Proceedings of the
ACL, pages 432?439.
R. Mukras, N. Wiratunga, R. Lothian, S. Chakraborti, and
D. Harper. 2007. Information Gain Feature Selection
for Ordinal Text Classification using Probability Re-
distribution. In Proceedings of the Textlink workshop
at IJCAI-07.
A. Nenkova, L. Vanderwende, and K. McKeown. 2006.
A Compositional Context Sensitive Multi-document
Summarizer: Exploring the Factors that Influence
Summarization. In Proceedings of the ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 573?580.
B. Pang and L. Lee. 2005. Seeing Stars: Exploiting
Class Relationships for Sentiment Categorization with
Respect to Rating Scales. In Proceedings of the ACL,
pages 115?124.
B. Pang and L. Lee. 2008. Opinion Mining and Senti-
ment Analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
H. Saggion and A. Funk. 2009. Extracting Opinions and
Facts for Business Intelligence. RNTI, E-17:119?146.
H. Saggion. 2008. SUMMA: A Robust and Adapt-
able Summarization Tool. Traitement Automatique
des Languages, 49:103?125.
K. Shimada and T. Endo. 2008. Seeing Several Stars: A
Rating Inference Task for a Document Containing Sev-
eral Evaluation Criteria. In Proceedings of the 12th
Pacific-Asia Conference on Advances in Knowledge
Discovery and Data Mining, pages 1006?1014.
S. Spiegel and N. J. Castellan, Jr. 1998. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill
International.
V. Stoyanov and C. Cardie. 2006. Toward Opinion Sum-
marization: Linking the Sources. In Proceedings of
the Workshop on Sentiment and Subjectivity in Text,
pages 9?14.
P. D. Turney. 2002. Thumbs Up or Thumbs Down?: Se-
mantic Orientation Applied to Unsupervised Classifi-
cation of Reviews. In Proceedings of the ACL, pages
417?424.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-level Sentiment
Analysis. In Proceedings of the EMNLP, pages 347?
354.
L. Zhuang, F. Jing, and X. Y. Zhu. 2006. Movie Re-
view Mining and Summarization. In Proceedings of
the 15th ACM international conference on Information
and knowledge management, pages 43?50.
115
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 168?174,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Towards a Unified Approach for Opinion Question Answering and
Summarization
Elena Lloret and Alexandra Balahur and Manuel Palomar and Andre?s Montoyo
Department of Software and Computing Systems
University of Alicante
Alicante 03690, Spain
{elloret,abalahur, mpalomar, montoyo}@dlsi.ua.es
Abstract
The aim of this paper is to present an ap-
proach to tackle the task of opinion question
answering and text summarization. Follow-
ing the guidelines TAC 2008 Opinion Sum-
marization Pilot task, we propose new meth-
ods for each of the major components of the
process. In particular, for the information
retrieval, opinion mining and summarization
stages. The performance obtained improves
with respect to the state of the art by approxi-
mately 12.50%, thus concluding that the sug-
gested approaches for these three components
are adequate.
1 Introduction
Since the birth of the Social Web, users play a cru-
cial role in the content appearing on the Internet.
With this type of content increasing at an exponen-
tial rate, the field of Opinion Mining (OM) becomes
essential for analyzing and classifying the sentiment
found in texts.
Nevertheless, real-world applications of OM of-
ten require more than an opinion mining component.
On the one hand, an application should allow a user
to query about opinions in natural language. There-
fore, Question Answering (QA) techniques must be
applied in order to determine the information re-
quired by the user and subsequently retrieve and
analyze it. On the other hand, opinion mining of-
fers mechanisms to automatically detect and classify
sentiments in texts, overcoming the issue given by
the high volume of such information present on the
Internet. However, in many cases, even the result of
the opinion processing by an automatic system still
contains large quantities of information, which are
still difficult to deal with manually. For example,
for questions such as ?Why do people like George
Clooney?? we can find thousands of answers on the
Web. Therefore, finding the relevant opinions ex-
pressed on George Clooney, classifying them and
filtering only the positive opinions is not helpful
enough for the user. He/she will still have to sift
through thousands of texts snippets, containing rele-
vant, but also much redundant information. For that,
we need to use Text Summarization (TS) techniques.
TS provides a condensed version of one or several
documents (i.e., a summary) which can be used as a
substitute of the original ones (Spa?rck Jones, 2007).
In this paper, we will concentrate on proposing ad-
equate solutions to tackle the issue of opinion ques-
tion answering and summarization. Specifically, we
will propose methods to improve the task of ques-
tion answering and summarization over opinionated
data, as defined in the TAC 2008 ?Opinion Sum-
marization pilot?1. Given the performance improve-
ments obtained, we conclude that the approaches we
proposed for these three components are adequate.
2 Related Work
Research focused on building factoid QA systems
has a long tradition, however, it is only recently that
studies have started to focus on the creation and de-
velopment of opinion QA systems. Example of this
can be (Stoyanov et al, 2004) who took advantage of
opinion summarization to support Multi-Perspective
QA system, aiming at extracting opinion-oriented
information of a question. (Yu and Hatzivassiloglou,
2003) separated opinions from facts and summa-
rized them as answer to opinion questions. Apart
from these studies, specialized competitions for sys-
tems dealing with opinion retrieval and QA have
been organized in the past few years. The TAC
2008 Opinion Summarization Pilot track proposed
a mixed setting of factoid and opinion questions.
1http://www.nist.gov/tac/2008/summarization/
168
It is interesting to note that most of the participat-
ing systems only adapted their factual QA systems
to overcome the newly introduced difficulties re-
lated to opinion mining and polarity classification.
Other relevant competition focused on the treatment
of subjective data is the NTCIR MOAT (Multilin-
gual Opinion Analysis Test Collection). The ap-
proaches taken by the participants in this task are rel-
evant to the process of opinion retrieval, which is the
first step performed by an opinion mining question
answering system. For example, (Taras Zabibalov,
2008) used an almost unsupervised approach ap-
plied to two of the sub-tasks: opinionated sentence
and topic relevance detection.(Qu et al, 2008) ap-
plied a sequential tagging approach at the token level
and used the learned token labels in the sentence
level classification task and their formal run submis-
sion was is trained on MPQA (Wiebe et al, 2005).
3 Text Analysis Conferences
In 2008, the Opinion Summarization Pilot task at
the Text Analysis Conferences2 (TAC) consisted in
generating summaries from blogs, according to spe-
cific opinion questions provided by the TAC orga-
nizers. Given a set of blogs from the Blog06 col-
lection3 and a list of questions, participants had to
produce a summary that answered these questions.
The questions generally required determining opin-
ion expressed on a target, each of which dealt with a
single topic (e.g. George Clooney). Additionally, a
set of text snippets were also provided, which con-
tained the answers to the questions. Table 1 depicts
an example of target, question, and optional snippet.
Target: George Clooney
Questions: Why do people like George Clooney?Why do people dislike George Clooney?
Snippets: 1050 BLOG06-20060209-006-0013539097
he?s a great actor.
Table 1: Example of target, question, and snippet.
Following the results obtained in the evaluation
at TAC 2008 (Balahur et al, 2008), we propose
an opinion question answering and summarization
(OQA&S) approach, which is described in detail in
the following sections.
2www.nist.gov/tac/
3http://ir.dcs.gla.ac.uk/test collections/access to data.html
4 An Opinion Question Answering and
Summarization Approach
In order to improve the results of the OQA&S sys-
tem presented at TAC, we propose new methods for
each of the major components of the system: infor-
mation retrieval, opinion mining and text summa-
rization.
4.1 Opinion Question Answering and
Summarization Components
? Information Retrieval
JAVA Information Retrieval system (JIRS) is
a IR system especially suited for QA tasks
(Go?mez, 2007). Its purpose is to find frag-
ments of text (passages) with more probabil-
ity of containing the answer to a user question
made in natural language instead of finding rel-
evant documents for a query. To that end, JIRS
uses the own question structure and tries to
find an equal or similar expression in the docu-
ments. The more similar the structure between
the question and the passage is, the higher the
passage relevance.
JIRS is able to find question structures in a
large document collection quickly and effi-
ciently using different n-gram models. Subse-
quently, each passage is assessed depending on
the extracted n-grams, the weight of these n-
grams, and the relative distance between them.
Finally, it is worth noting that the number of
passages in JIRS is configurable, and in this
research we are going to experiment with pas-
sages of length 1 and 3.
? Opinion Mining
The first step we took in our approach was
to determine the opinionated sentences, as-
sign each of them a polarity (positive or neg-
ative) and a numerical value corresponding to
the polarity strength (the higher the negative
score, the more negative the sentence and vice
versa). In our first approximation (OMaprox1),
we employed a simple, yet efficient method,
presented in Balahur et al (Balahur et al,
2009). As lexicons for affect detection, we
used WordNet Affect (Strapparava and Vali-
tutti, 2004), SentiWordNet (Esuli and Sebas-
169
tiani, 2006), and MicroWNOp (Cerini et al,
2007). Each of the resources we employed
were mapped to four categories, which were
given different scores: positive (1), negative
(-1), high positive (4) and high negative (-4).
First, the score of each of the blog posts was
computed as the sum of the values of the words
that were identified. Subsequently, we per-
formed sentence splitting4 and classified the
sentences we thus obtained according to their
polarity, by adding the individual scores of the
affective words identified.
In the second approach (OMaprox2), we first
filter out the sentences that are associated to
the topic discussed, using LSA. Further on, we
score the sentences identified as relating to the
topic of the blog post, in the same manner as
in the previous approach. The aim of this ap-
proach is to select for further processing only
the sentences which contain opinions on the
post topic. In order to filter these sentences
in, we first create a small corpus of blog posts
on each of the topics included in our collec-
tion5. For each of the corpora obtained, we
apply LSA, using the Infomap NLP Software6.
Subsequently, we compute the 100 most asso-
ciated words with two of the terms that are most
associated with each of the topics and the 100
most associated words with the topic word. The
approach was proven to be successful in (Bal-
ahur et al, 2010).
? Text Summarization
The text summarization approach used in this
paper was presented in (Lloret and Palomar,
2009). In order to generate a summary, the
suggested approach first carries out a basic pre-
processing stage comprising HTML parsing,
sentence segmentation, tokenization, and stem-
ming. Once the input document or documents
have been pre-processed, a relevance detection
stage, which is the core part of the approach, is
applied. The objective of this step is to identify
4http://alias-i.com/lingpipe/
5These small corpora (30 posts for each of the top-
ics) are gathered using the search on topic words on
http://www.blogniscient.com/ and crawling the resulting pages.
6http://infomap-nlp.sourceforge.net/
potential relevant sentences in the document by
means of three techniques: textual entailment,
term frequency and the code quantity principle
(Givo?n, 1990). Then, each potential relevant
sentence is given a score which is computed
on the basis of the aforementioned techniques.
Finally, all sentences are ordered according
to their scores, and the highest ranked ones
(which mean those sentences contain more im-
portant information) are selected and extracted
up to the desired length, thus building the fi-
nal summary. It is worth stressing upon the fact
that in an attempt to maintain the coherence of
the original documents, sentences are shown in
the same order they appear in the original doc-
uments.
4.2 Experimental Framework
The objective of this section is to describe the corpus
used and the experiments performed with the data
provided in TAC 2008 Opinion Summarization Pi-
lot7 task. The approaches analyzed comprise:
? OQA&S: The three components explained
in the previous section (information retrieval,
opinion mining and summarization) were
bound together in order to produce summaries
that include the answer to opinionated ques-
tions. First, the most relevant passages of
length 1 and 3 are retrieved by the IR module,
as in the aforementioned approach, and then
the subjective information is found and classi-
fied within them using the OM approaches de-
scribed in the previous section. Further on, we
incorporate the TS module, to select and ex-
tract the most relevant opinionated facts from
the pool of subjective information identified
by the OM module. We generate opinion-
oriented summaries of compression rates rang-
ing from 10% to 50%. In the end, four dif-
ferent approaches result from the integration
of the three components: IRp1-OMaprox1-
TS; IRp1-OMaprox2-TS; IRp3-OMaprox1-
TS; and IRp3-OMaprox2-TS.
Moreover, apart from these approaches, two base-
lines were also defined. On the one hand, we sug-
7http://www.nist.gov/tac/data/past-
blog06/2008/OpSummQA08.html#OpSumm
170
gest a baseline using the list of snippets provided by
the TAC organization (QA-snippets). This baseline
produces a summary by joining all the answers in the
snippets that related to the same topic On the other
hand, we took as a second baseline the approach
from our participation in TAC 2008 (DLSIUAES),
without not taking into account any information re-
trieval or question answering system to retrieve the
fragments of information which may be relevant to
the query. In contrast, this was performed by com-
puting the cosine similarity8 between each sentence
in the blog and the query. After all the potential rel-
evant sentences for the query were identified, they
were classified in terms of subjectivity and polarity,
and the most relevant ones were selected for the final
summary.
4.3 Evaluation Methodology
Since we used the corpus provided at the Opinion
Summarization Pilot task, and we followed simi-
lar guidelines, we should evaluate our OQA&S ap-
proach in the same way as participant systems were
assessed. However, the evaluation methodology
proposed differs slightly from the one carried out
in the competition. The reason why we took such
decision was due to the fact that the evaluation car-
ried out in TAC had some limitations, and therefore
was not suitable for our purposes. In this manner,
our evaluation is also based on the gold-standard
nuggets provided by TAC, but in addition we pro-
posed an extended version of them, by adding other
pieces of information that are also relevant to the
topics.
In this section, all the issues concerning the eval-
uation are explained. These comprise the original
evaluation method used in the Opinion Summariza-
tion Pilot task at TAC (Section 4.3.1) , its draw-
backs (Section 4.3.2), and the extended version for
the evaluation method we propose (Section 4.3.3).
Further on, the results obtained together with a wide
discussion, as well as its comparison with the base-
lines and the TAC participants is provided in Section
4.4.
4.3.1 Nugget-based Evaluation at TAC
Within the Opinion Summarization Pilot task,
each summary was evaluated according to its con-
8http://www.d.umn.edu/ tpederse/text-similarity.html
tent using the Pyramid method (Nenkova et al,
2007). A list of nuggets was provided and the asses-
sors used such list of nuggets to count the number
of nuggets a summary contained. Depending on the
number of nuggets the summary included and the
importance of each one given by their weight, the
values for recall, precision and F-measure were ob-
tained. An example of several nuggets correspond-
ing to different topics can be seen in Table 2, where
the weight for each one is also shown in brackets.
Topic Nugget (weight)
Carmax CARMAX prices are firm, the price is
the price (0.9)
Jiffy Lube They should have torque wrenches (0.2)
Talk show hosts Funny (0.78)
Table 2: Example of evaluation nuggets and associated
weights.
4.3.2 Limitations of the Nugget Evaluation
The evaluation method suggested at TAC requires
a lot of human effort when it comes to identify
the relevant fragments of information (nuggets) and
compute how many of them a summary contains, re-
sulting in a very costly and time-consuming task.
This is a general problem associated to the evalua-
tion of summaries, which makes the task of summa-
rization evaluation especially hard and difficult.
But, apart from this, when an exhaustive exam-
ination of the nuggets used in TAC is done, some
other problems arised which are worth mentioning.
The average number of nuggets for each topic is
27, and this would mean, that longer summaries
will be highly penalized, because it will contain
more useless information according to the nuggets.
After analyzing in detail all the provided nuggets,
we mainly classified the possible problems into six
groups, which are:
1. Some of the nuggets were expressed differently
from how they appeared in the original blogs.
Since most of the summarization systems are ex-
tractive, this fact forced that humans had to evaluate
the summaries, otherwise it would be very difficult
to account for the presence of such nugget in the
summary, if they are not using the same vocabulary
as the original blogs.
2. Some nuggets for the same topic express the
171
same idea, despite not being identical. In these
cases, we are counting a single piece of informa-
tion in the summary twice, if the idea that nuggets
expressed is included.
3. Moreover, the meaning of one nugget can be de-
duced from another?s, which is also related to the
problem stated before.
4. Some of the nuggets are not very clear in mean-
ing (e.g. ?hot?, ?fun?). This would mean that a
summary might include such terms in a different
context, thus, obtaining incorrectly that it is reve-
lant when might be out of context.
5. A sentence in the original blog can be covered by
several nuggets. For instance, both nuggets ?it is
an honest book? and ?it is a great book? correspond
to the same sentence ?It was such a great book-
honest and hard to read (content not language dif-
ficulty)?. In this case, it is not clear how to proceed
with the evaluation; whether to count both nuggets
or just one of them.
6. Some information which is also relevant for the
topic is not present in any nugget. For instance:
?I go to Starbucks because they generally provide
me better service?. Although it is relevant with re-
spect to the topic and it appears in a number of sum-
maries, it would be not counted because it has not
been chosen as a nugget.
4.3.3 Extended Nugget-based Evaluation
Since we are interested in testing a wide range of
approaches involving IR, OM and TS, sticking to the
rules to the original TAC evaluation would mean that
a lot of time as well as human effort will be required,
as well as not accounting for important information
that summaries may contain in addition to the one
expressed by the nuggets. Therefore, taking as a ba-
sis the nuggets provided at TAC, we set out a modi-
fied version of them.
The underlying idea behind this is to create an ex-
tended set of nuggets that serve as a reference for
assessing the content of the summaries. In this man-
ner, we will map each original nugget with the set of
sentences in the original blogs that are most similar
to it, thus generating a gold-standard summary for
each topic. For creating this extended gold-standard
nuggets we compute the cosine similarity9 between
9The cosine similarity was computed using Pedersen?s
every nugget and all the sentences in the blog related
to the same topic. We empirically established a sim-
ilarity threshold of 0.5, meaning that if a sentence
was equal or above such similarity value, it will be
considered also relevant. One main disadvantage of
such a lower threshold value is that we can consider
relevant sentences that share the same vocabulary
but in fact they are not relevant to the summary. In
order to avoid this, once we had identified all the
most similar sentences to each nugget, we carried
out a manual analysis to discard cases like this. Hav-
ing created the extended set of nuggets, we grouped
all of them pertaining to the same topic, and consid-
ered it a gold-standard summary. Now, the average
number of nuggets per topic is 53, which we have
increased by twice the number of original nuggets
provided at TAC.
Further on, our summaries are compared against
this new gold-standard using ROUGE (Lin, 2004).
This tool computes the number of different kinds
of overlap n-grams between an automatic summary
and a human-made summary. For our evaluation,
we compute ROUGE-1 (unigrams), ROUGE-2 (bi-
grams), ROUGE-SU4 (it measures the overlap of
skip-bigrams between a candidate summary and a
set of reference summaries with a maximum skip
distance of 4), and ROUGE-L (Longest Common
Subsequence between two texts). The results and
discussion are next provided.
4.4 Results and Discussion
This section contains the results obtained for our
OQA&S approach and all the sub-approaches tested.
IRpN refers to the length of the passage employed
in the information retrieval approach, whereas
OMaproxN indicates the approach used for the opin-
ion mining component. Firstly, we show and ana-
lyze the results of our different approaches, and then
we compared the best performing one with the base-
lines and the average Opinion Summarization Pilot
task participants results in TAC.
Table 3 shows the precision (Pre), recall (Rec) and
F-measure results of ROUGE-1 (R-1) for all the ap-
proaches we experimented with.
Generally speaking, the results obtained show
better figures for precision than for recall, and there-
Text Similarity Package: http://www.d.umn.edu/ tpederse/text-
similarity.html
172
Approach Summary length
Name R-1 10% 20% 30% 40% 50%
Pre 24.29 26.17 29.73 30.82 32.54
IRp1 Rec 14.45 18.58 22.32 23.63 26.32
-OMaprox1-TS F?=1 16.53 20.65 24.58 25.75 28.12
Pre 24.29 26.17 29.73 30.82 32.54
IRp1 Rec 16.90 20.02 23.36 24.15 26.77
-OMaprox2-TS F?=1 19.45 22.13 25.36 25.94 28.40
Pre 27.27 30.18 30.91 30.05 30.19
IRp3 Rec 20.56 24.76 28.25 31.67 34.47
-OMaprox1-TS F?=1 22.65 26.23 27.98 29.18 29.74
Pre 30.16 32.11 32.35 32.41 32.11
IRp3 Rec 20.64 24.03 27.25 29.78 32.68
-OMaprox2-TS F?=1 23.28 25.64 27.42 28.44 29.21
Table 3: Results of our OQA&S approaches
Approach Performance (ROUGE)
Name % R-1 R-2 R-L R-SU4
Pre 32.11 7.34 29.00 11.37
IRp3-OMaprox2 Rec 32.68 8.31 33.24 12.76
-TS (50%) F?=1 29.21 7.22 28.60 11.13
Pre 17.97 8.76 17.65 9.98
QA-snippets Rec 71.24 31.30 70.10 37.44
F?=1 24.73 11.58 24.29 13.45
Pre 20.54 7.00 19.46 9.29
DLSIUAES Rec 57.66 18.98 54.61 25.77
F?=1 27.04 9.10 25.59 12.22
Pre 23.74 8.35 22.72 10.81
Average TAC Rec 56.65 19.37 54.56 25.40
participants F?=1 27.45 9.64 26.33 12.46
Pre 20.42 6.06 19.55 8.62
Average TAC Rec 56.45 17.3 54.40 24.11
participants? F?=1 24.31 7.25 23.31 10.29
Table 4: Comparison with other systems
fore the F-measure value, which combines both val-
ues, will be affected. Good precision values means
that the information our approaches select is the cor-
rect one, despite not including all the relevant infor-
mation.
Our best performing approach in general is the
one which uses a length passage of 3 and, as far
as OM is concerned, when topic-sentiment analy-
sis is carried out (IRp3-OMaprox2-TS). This shows
that the approach dealing with topic-sentiment anal-
ysis in opinion mining is more suitable than the one
which does not consider topic relevance. Taking a
look at some individual results, we next try to eluci-
date the reasons why our approach performs better
at some approaches and not so good at others. Con-
cerning the IR module, it is important to mention
that a passage length of 1 always obtains poorer re-
sults that when it is increased to 3, meaning that the
longer the passage, the better.
Regarding the best summary length, we observed
that in general terms, the more content we allow
for the summary, the better. In other words, com-
pression rates of 50% get higher results than 20%
or 10%. However, there are cases in which shorter
summaries (10% and 20%) obtains better results
than longer ones (e.g. IRp3-OMaprox2-TS vs. IRp3-
OMaprox1-TS).
Although the results theirselves are not very high
(around 30%), they are in line with the state-of-the-
art, as can be seen in Table 4, where our best per-
forming approach is compared with respect to other
approaches.
Although the compression rate which obtains best
results is not very high (50%), indeed the final sum-
maries have an average length of 2,333 non-white
space characters. This is really low compared to the
length that TAC organization allowed for the Opin-
ion Summarization Pilot task, which was 7,000 non-
white space characters per question, and most of
the times there were two questions for each topic.
Whereas the results of TAC participants are much
better for the recall value than ours, if we take a look
at the precision, our approach outperforms them ac-
cording to this value in all of the cases. The longer
a summary is, the more chances it has to contain in-
formation related to the topic. However, not all this
information may be relevant, as it is shown in the
results for the precision values, which decrease con-
siderably compared to the recall ones. In contrast,
due to the fact that our approach is missing some
relevant information because we use a rather short
passage length (3 sentences), we do not obtain such
high values for the recall, but we obtain good preci-
sion results, which indicate that the information that
we keep is important.
Moreover, comparing those results with the ones
obtained by our approach, it is worth mentioning
that IRp3-OMaprox2-TS outperforms the F-measure
value for all the ROUGE metrics with respect to Av-
erage TAC participants?. More in detail, when the
ROUGE scores are averaged, IRp3-OMaprox2-TS
improves by 12.50% the Average TAC participants?
for the F-measure value.
173
5 Conclusion and Future Work
In this paper, we tackled the process of OQA&S.
In particular, we analyzed specific methods within
each component of this process, i.e., information
retrieval, opinion mining and text summarization.
These components are crucial in this task, since our
final goal was to provide users with the correct infor-
mation containing the answer of a question. How-
ever, contrary to most research work in question an-
swering, we focus on opinionated questions rather
than factual, increasing the difficulty of the task.
Our analysis comprises different configurations
and approaches: i) varying the length for retrieving
the passages of the documents in the retrieval infor-
mation stage; ii) studying a method that take into
consideration topic-sentiment analysis for detecting
and classifying opinions in the retrieved passages
and comparing it to another that does not; and iii)
generating summaries of different compression rates
(10% to 50%). The results obtained showed that
the proposed methods are appropriate to tackle the
OQA&S task, improving state of the art approaches
by 12.50% approximately.
In the future, we plan to continue investigating
suitable approaches for each of the proposed com-
ponents. Our final goal is to build an integrated and
complete approach.
Acknowledgments
This research work has been funded by the Spanish Gov-
ernment through the research program FPI (BES-2007-
16268) associated to the project TEXT-MESS (TIN2006-
1526-C06-01). Moreover, it has been also partially
funded by projects TEXT-MESS 2.0 (TIN2009-13391-
C04), and PROMETEO (PROMETEO/2009/199) from
the Spanish and the Valencian Government, respectively.
References
A. Balahur, E. Lloret, O. Ferra?ndez, A. Montoyo,
M. Palomar, and R. Mun?oz. 2008. The DLSIUAES
team?s participation in the tac 2008 tracks. In Pro-
ceedings of the Text Analysis Conference.
Alexandra Balahur, Ralf Steinberger, Erik van der Goot,
Bruno Pouliquen, and Mijai Kabadjov. 2009. Opinion
mining from newspaper quotations. In Proceedings of
the Workshop on Intelligent Analysis and Processing
of Web News Content.
A. Balahur, M. Kabadjov, and J. Steinberger. 2010.
Exploiting higher-level semantic information for the
opinion-oriented summarization of blogs. In Proceed-
ings of CICLing?2010.
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini. 2007. Micro-WNOp: A gold stan-
dard for the evaluation of automatically compiled lex-
ical resources for opinion mining. In Language re-
sources and linguistic theory: Typology, second lan-
guage acquisition, English linguistics.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A pub-
licly available resource for opinion mining. In Pro-
ceedings of LREC.
Talmy Givo?n, 1990. Syntax: A functional-typological in-
troduction, II. John Benjamins.
Jose? M. Go?mez. 2007. Recuperacio?n de Pasajes Multil-
ingu?e para la Bu?squeda de Respuestas. Ph.D. thesis.
Chin-Yew Lin. 2004. ROUGE: a Package for Automatic
Evaluation of Summaries. In Proceedings of ACL Text
Summarization Workshop, pages 74?81.
Elena Lloret and Manuel Palomar. 2009. A gradual com-
bination of features for building automatic summarisa-
tion systems. In Proceedings of TSD, pages 16?23.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing, 4(2):4.
Lizhen Qu, Cigdem Toprak, Niklas jakob, and iryna
Gurevych. 2008. Sentence level subjectivity and sen-
timent analysis experiments in ntcir-7 moat challenge.
In Proceedings of NTCIR-7 Workshop meeting.
Karen Spa?rck Jones. 2007. Automatic summarising: The
State of the Art. Information Processing & Manage-
ment, 43(6):1449?1481.
V. Stoyanov, C. Cardie, D. Litman, and J. Wiebe. 2004.
Evaluating an opinion annotation scheme using a new
multi-perspective question and answer corpus. In
AAAI Spring Symposium on Exploring Attitude and Af-
fect in Text: Theories and Applications.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of wordnet. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation, pages 1083?1086.
John Carroll Taras Zabibalov. 2008. Almost-
unsupervised cross-language opinion analysis at ntcis-
7. In Proceedings of NTCIR-7 Workshop meeting.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language. In
Language Resources and Evaluation, volume 39.
D. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
Proceedings of EMNLP.
174
