Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 19?26,
Athens, Greece, 31 March, 2009. c?2009 Association for Computational Linguistics
Revisiting multi-tape automata for Semitic morphological analysis and
generation
Mans Hulden
University of Arizona
Department of Linguistics
mhulden@email.arizona.edu
Abstract
Various methods have been devised to pro-
duce morphological analyzers and gen-
erators for Semitic languages, ranging
from methods based on widely used finite-
state technologies to very specific solu-
tions designed for a specific language
or problem. Since the earliest propos-
als of how to adopt the elsewhere suc-
cessful finite-state methods to root-and-
pattern morphologies, the solution of en-
coding Semitic grammars using multi-tape
automata has resurfaced on a regular ba-
sis. Multi-tape automata, however, require
specific algorithms and reimplementation
of finite-state operators across the board,
and hence such technology has not been
readily available to linguists. This paper,
using an actual Arabic grammar as a case
study, describes an approach to encoding
multi-tape automata on a single tape that
can be implemented using any standard
finite-automaton toolkit.
1 Introduction
1.1 Root-and-pattern morphology and
finite-state systems
The special problems and challenges embodied by
Semitic languages have been recognized from the
early days of applying finite-state methods to nat-
ural language morphological analysis. The lan-
guage model which finite-state methods have been
most successful in describing?a model where
morphemes concatenate in mostly strict linear
order?does not translate congenially to the type
of root-and-pattern morphology found in e.g. Ara-
bic and Hebrew (Kataja and Koskenniemi, 1988;
Lavie et al, 1988).
In Arabic, as in most Semitic languages, verbs
have for a long time been analyzed as consist-
ing of three elements: a (most often) triconsonan-
tal root, such as ktb (H.
H ?), a vowel pattern
containing grammatical information such as voice
(e.g. the vowel a) and a derivational template,
such as CVCVC indicating the class of the verb, all
of which are interdigitated to build a stem, such
as katab (I.

J

?).1 This stem is in turn subject to
more familiar morphological constructions includ-
ing prefixation and suffixation, yielding informa-
tion such as number, person, etc, such as kataba
( I.

J?), the third person singular masculine perfect
form.
The difficulty of capturing this interdigitation
process is not an inherent shortcoming of finite-
state automata or transducers per se, but rather
a result of the methods that are commonly used
to construct automata. Regular expressions that
contain operations such as concatenation, union,
intersection, as well as morphotactic descriptions
through right-linear grammars offer an unwieldy
functionality when it comes to interleaving strings
with one another in a regulated way. But, one
could argue, since large scale morphological ana-
lyzers as finite-state automata/transducers have in-
deed been built (see e.g. Beesley (1996, 1998b,a)),
the question of how to do it becomes one of con-
struction, not feasibility.
1.2 Multitape automata
One early approach, suggested by Kay (1987) and
later pursued in different variants by Kiraz (1994,
2000) among others, was to, instead of modeling
morphology along the more traditional finite-state
transducer, modeling it with a n-tape automaton,
where tapes would carry precisely this interleaving
1Following autosegmental analyses, this paper assumes
the model where the vocalization is not merged with the pat-
tern, i.e. we do not list separate patterns for vocalizations
such as CaCaC as is assumed more traditionally. Which anal-
ysis to choose largely a matter of convenience, and the meth-
ods in this paper apply to either one.
19
that is called for in Semitic interdigitation. How-
ever, large-scale multitape solutions containing the
magnitude of information in standard Arabic dic-
tionaries such as Wehr (1979) have not been re-
ported.
To our knowledge, two large-scale morphologi-
cal analyzers for Arabic that strive for reasonable
completeness have been been built: one by Xerox
and one by Tim Buckwalter (Buckwalter, 2004).
The Xerox analyzer relies on complex extensions
to the finite-state calculus of one and two-tape
automata (transducers) as documented in Beesley
and Karttunen (2003), while Buckwalter?s system
is a procedural approach written in Perl which de-
composes a word and simultaneously consults lex-
ica for constraining the possible decompositions.
Also, in a similar vein to Xerox?s Arabic analyzer,
Yona and Wintner (2008) report on a large-scale
system for Hebrew built on transducer technology.
Most importantly, none of these very large systems
are built around multi-tape automata even though
such a construction from a linguistic perspective
would appear to be a fitting choice when dealing
with root-and-pattern morphology.
1.3 n-tape space complexity
There is a fundamental space complexity problem
with multi-tape automata, which is that when the
number of tapes grows, the required joint sym-
bol alphabet grows with exponential rapidity un-
less special mechanisms are devised to curtail this
growth. This explosion in the number of transi-
tions in an n-tape automaton can in many cases
be more severe than the growth in the number of
states of a complex grammar.
To take a simple, though admittedly slightly ar-
tificial example: suppose we have a 5-tape au-
tomaton, each tape consisting of the same alpha-
bet of, say 22 symbols {s1, . . . , s22}. Now, as-
sume we want to restrict the co-occurrence of s1
on any combination of tapes, meaning s1 can only
occur once on one tape in the same position, i.e.
we would be accepting any strings containing a
symbol such as s1:s2:s2:s2:s2 or s2:s2:s2:s2:s3
but not, s1:s2:s3:s4:s1. Without further treatment
of the alphabet behavior, this yields a multi-tape
automaton which has a single state, but 5,056,506
transitions?each transition naturally representing
a legal combination of symbols on the five tapes.
This kind of transition blow-up is not completely
inevitable: of course one can devise many tricks
to avoid it, such as adding certain semantics to
the transition notation?in our example by per-
haps having a special type of ?failure? transition
which leads to non-acceptance. For the above ex-
ample this would cut down the number of tran-
sitions from 5,056,506 to 97,126. The drawback
with such methods is that any changes will tend
to affect the entire finite-state system one is work-
ing with, requiring adaptations in almost every un-
derlying algorithm to construct automata. One is
then unable to leverage the power of existing soft-
ware designed for finite-state morphological anal-
ysis, but needs to build special-purpose software
for whatever multi-tape implementation one has in
mind.2
1.4 The appeal of the multi-tape solution
The reason multi-tape descriptions of natural lan-
guage morphology are appealing lies not only
in that such solutions seem to be able to han-
dle Semitic verbal interdigitation, but also in
that a multi-tape solution allows for a natural
alignment of information regarding segments and
their grammatical features, something which is
often missing in finite-state-based solutions to
morphological analysis. In the now-classical
way of constructing morphological analyzers, we
have a transducer that maps a string represent-
ing an unanalyzed word form, such as kataba
( I.

J

?) to a string representing an analyzed one,
e.g. ktb +FormI +Perfect +Act +3P
+Masc +Sg. Such transductions seldom pro-
vide grammatical component-wise alignment in-
formation telling which parts of the unanalyzed
words contribute to which parts of the grammat-
ical information. Particularly if morphemes signi-
fying a grammatical category are discontinuous,
this information is difficult to provide naturally
in a finite-automaton based system without many
tapes. A multi-tape solution, on the other hand,
2Two anonymous reviewers point out the work by Habash
et al (2005) and Habash and Rambow (2006) who report an
effort to analyze Arabic with such a multitape system based
on work by Kiraz (2000, 2001) that relies on custom algo-
rithms devised for a multitape alphabet. Although Habash
and Rambow do not discuss the space requirements in their
system, it is to be suspected that the number of transitions
grows quickly using such an method by virtue of the argu-
ment given above. These approaches also use a small number
of tapes (between 3 and 5), and, since the number of tran-
sitions can increase exponentially with the number of tapes
used, such systems do not on the face of it appear to scale
well to more than a handful of tapes without special precau-
tions.
20
Tinput k a t a b a
Troot k t b
Tform Form I
Tptrn C V C V C
Tpaff a
Taffp +3P
+Masc
+Sg
Tvoc a a
Tvocp +Act
. . .
Table 1: A possible alignment of 8 tapes to capture
Arabic verbal morphology.
can provide this information by virtue of its con-
struction. The above example could in an 8-tape
automaton encoding be captured as illustrated in
table 1, assuming here that Tinput is the input tape,
the content of which is provided, and the subse-
quent tapes are output tapes where the parse ap-
pears.
In table 1, we see that the radicals on the root
tape are aligned with the input, as is the pattern on
the pattern tape, the suffix -a on the suffix tape,
which again is aligned with the parse for the suf-
fix on the affix parse tape (affp), and finally the
vocalization a is aligned with the input and the pat-
tern. This is very much in tune with both the type
of analyses linguists seem to prefer (McCarthy,
1981), and more traditional analyses and lexicog-
raphy of root-and-pattern languages such as Ara-
bic.
In what follows, we will present an alternate
encoding for multi-tape automata together with
an implementation of an analyzer for Arabic ver-
bal morphology. The encoding simulates a multi-
tape automaton using a simple one-tape finite-state
machine and can be implemented using standard
toolkits and algorithms given in the literature. The
encoding also avoids the abovementioned blow-up
problems related to symbol combinations on mul-
tiple tapes.
2 Notation
We assume the reader is familiar with the basic
notation regarding finite automata and regular ex-
pressions. We will use the standard operators of
Kleene closure (L?), union (L1 ? L2), intersec-
tion (L1 ? L2), and assume concatenation when-
ever there is no overt operator specified (L1L2).
We use the symbol ? to specify the alphabet, and
the shorthand \a to denote any symbol in the al-
phabet except a. Slight additional notation will be
introduced in the course of elaborating the model.
3 Encoding
In our implementation, we have decided to encode
the multi-tape automaton functionality as consist-
ing of a single string read by a single-tape automa-
ton, where the multiple tapes are all evenly inter-
leaved. The first symbol corresponds to the first
symbol on tape 1, the second to the first on tape 2,
etc.:
T1 ? ? ?
. . .
Tn?1 ? ? ?
Tn ? ? ?
. . .
For instance, the two-tape correspondence:
T1 a
T2 b c
would be encoded as the string ab?c, ? being a spe-
cial symbol used to pad the blanks on a tape to
keep all tapes synchronized.
This means that, for example, for an 8-tape rep-
resentation, every 8th symbol from the beginning
is a symbol representing tape 1.
Although this is the final encoding we wish to
produce, we have added one extra temporary fea-
ture to facilitate the construction: every symbol on
any ?tape? is always preceded by a symbol indi-
cating the tape number drawn from an alphabet
T1, . . . , Tn. These symbols are removed eventu-
ally. That means that during the construction, the
above two-tape example would be represented by
the string T1aT2bT1?T2c. This simple redundancy
mechanism will ease the writing of grammars and
actually limit the size of intermediate automata
during construction.
4 Construction
4.1 Overview
We construct a finite-state n-tape simulation gram-
mar in two steps. Firstly we populate each ?tape?
with all grammatically possible strings. That
means that, for our Arabic example, the root tape
21
should contain all possible roots we wish to ac-
cept, the template tape all the possible templates,
etc. We call this language the Base. The second
step is to constrain the co-occurrence of symbols
on the individual tapes, i.e. a consonant on the root
tape must be matched by a consonant of the input
tape as well as the symbol C on the pattern tape,
etc. Our grammar then consists of all the permit-
ted combinations of tape symbols allowed by a)
the Base and b) the Rules. The resulting lan-
guage is simply their intersection, viz.:
Base ? Rules
4.2 Populating the tapes
We have three auxiliary functions, TapeL(X,Y),
TapeM(X,Y), and TapeA(X,Y), where the ar-
gument X is the tape number, and Y the language
we with to insert on tape X.3 TapeL(X,Y) cre-
ates strings where every symbol from the language
Y is preceded by the tape indicator TX and where
the entire tape is left-aligned, meaning there are
no initial blanks on that tape. TapeM is the same
function, except words on that tape can be pre-
ceded by blanks and succeeded by blanks. TapeA
allows for any alignment of blanks within words
or to the left or right. Hence, to illustrate this
behavior, TapeL(4,C V C V C) will produce
strings like:
XT4CXT4VXT4CXT4VXT4CY
where X is any sequence of symbols not contain-
ing the symbol T4, and Y any sequence possibly
containing T4 but where T4 is always followed by
?, i.e. we pad all tapes at the end to allow for syn-
chronized strings on other tapes containing more
material to the right.
Now, if, as in our grammar, tape 4 is the tem-
plate tape, we would populate that tape by declar-
ing the language:
TapeM(4,Templates)
assuming Templates is the language that ac-
cepts all legal template strings, e.g. CVCVC,
CVCCVC, etc.
Hence, our complete Base language (continu-
ing with the 8-tape example) is:
3See the appendix for exact definitions of these functions.
TapeL(1,Inputs) ?
TapeA(2,Roots) ?
TapeL(3,Forms) ?
TapeM(4,Templates) ?
TapeA(5,Affixes) ?
TapeM(6,Parses) ?
TapeA(7,Voc) ?
TapeL(8,VocParses) ?
(T1?T2?T3?T4?T5?T6?T7?T8?)?
This will produce the language where all strings
are multiples of 16 in length. Every other sym-
bol is the TX tape marker symbol and every other
symbol is the actual symbol on that tape (allowing
for the special symbol ? also to represent blanks on
a tape). Naturally, we will want to define Inputs
occurring on tape 1 as any string containing any
combination of symbols since it represents all pos-
sible input words we wish to parse. Similarly, tape
2 will contain all possible roots, etc. This Base
language is subsequently constrained so that sym-
bols on different tapes align correctly and are only
allowed if they represent a legal parse of the word
on the input tape (tape 1).
4.3 Constructing the rules
When constructing the rules that constrain the co-
occurrence of symbols on the various tapes we
shall primarily take advantage of the ? oper-
ator first introduced for two-level grammars by
Koskenniemi (1983).4 The semantics is as fol-
lows. A statement:
X ? L1 R1, . . . , Ln Rn
where X and Li, Ri are all regular languages
defines the regular language where every instance
of a substring drawn from the languageX must be
surrounded by some pair Li and Ri to the left and
right, respectively.5
Indeed, all of our rules will consist exclusively
of? statements.
To take an example: in order to constrain the
template we need two rules that effectively say that
every C and V symbol occurring in the template
4There is a slight, but subtle difference in notation,
though: the original two-level? operator constrained single
symbols only (such as a:b, which was considered at compile-
time a single symbol); here, the argument X refers to any
arbitrary language.
5Many finite-state toolkits contain this as a separate op-
erator. See Yli-Jyra? and Koskenniemi (2004) and Hulden
(2008) for how such statements can be converted into regular
expressions and finite automata.
22
tape must be matched by 1) a consonant on the root
tape and 2) a vowel on the input tape. Because of
our single-tape encoding the first rule translates to
the idea that every T4 C sequence must be directly
preceded by T2 followed by some consonant fol-
lowed by T3 and any symbol at all:
T4 C ? T2 Cons T3 ? (1)
and the second one translates to:
T4 V ? T1 Vow T2 ? T3 ? (2)
assuming that Vow is the language that contains
any vowel and Cons the language that contains
any consonant.
Similarly, we want to constrain the Forms
parse tape that contains symbols such as Form I,
Form II etc., so that if, for example, Form I oc-
curs on that tape, the pattern CVCVC must occur on
the pattern tape.6
T3 Form I? TapeM(4,C V C V C) (3)
and likewise for all the other forms. It should be
noted that most constraints are very strictly local
to within a few symbols, depending slightly on the
ordering and function of the tapes. In (1), for in-
stance, which constrains a symbol on tape 4 with
a consonant on tape 2, there are only 2 interven-
ing symbols, namely that of tape 3. The ordering
of the tapes thus has some bearing on both how
simple the rules are to write, and the size of the re-
sulting automaton. Naturally, tapes that constrain
each other are ideally placed in adjacent positions
whenever possible.
Of course, some long-distance constraints will
be inevitable. For example, Form II is generally
described as a CVCCVC pattern, where the extra
consonant is a geminate, as in the stem kattab,
where the t of the root associates with both C?s
in the pattern. To distinguish this C behavior
from that of Form X which is also commonly de-
scribed with two adjacent C symbols where, how-
ever, there is no such association (as in the stem
staktab) we need to introduce another symbol.
6To be more exact, to be able to match and parse both
fully vocalized words such as wadarasat (
I ? P

X

?), and un-
vocalized ones, such as wdrst ( I?PX?), we want the pattern
CVCVC to actually be represented by the regular expression
C (V) C (V) C, i.e. where the vowels are optional. Note,
however, that the rule that constrains T4 V above only re-
quires that the V matches if there indeed is one. Hence,
by declaring vowels in patterns (and vocalizations) to be op-
tional, we can always parse any partially, fully, or unvocalized
verb. Of course, fully unvocalized words will be much more
ambiguous and yield more parses.
This symbol C2 occurs in Form II, which becomes
CVCC2VC. We then introduce a constraint to the
effect that any C2-symbol must be matched on the
input by a consonant, which is identical to the pre-
vious consonant on the input tape.7 These long-
distance dependencies can be avoided to some ex-
tent by grammar engineering, but so long as they
do not cause a combinatorial explosion in the num-
ber of states of the resulting grammar automaton,
we have decided to include them for the sake of
clarity.
To give an overview of some of the subsequent
constraints that are still necessary, we include here
a few descriptions and examples (where the starred
(***) tape snippets exemplify illegal configura-
tions):
? Every root consonant has a matching conso-
nant on the input tape
T1 k a t a b a
T2 k t b
T1 k a t a b a
T2*** d r s
? A vowel in the input which is matched by a
V in the pattern, must have a corresponding
vocalization vowel
T1 k a t a b a
T4 C V C V C
T7 a a
T1 k a t a b a
T4 C V C V C
T7*** u i
? A position where there is a symbol in the in-
put either has a symbol in the pattern tape or
a symbol in the affix tape (but not both)
T1 k a t a b a
T4 C V C V C
T5 a
T1 k a t a b a
T4 C V C V C
T5***
7The idea to preserve the gemination in the grammar is
similar to the solutions regarding gemination and spreading
of Forms II, V, and IX documented in Beesley (1998b) and
Habash and Rambow (2006).
23
4.4 The final automaton
As mentioned above, the symbols {T1, . . . , Tn}
are only used during construction of the automa-
ton for the convenience of writing the grammar,
and shall be removed after intersecting the Base
language with the Rules languages. This is a sim-
ple substitution TX ? , i.e. the empty string.
Hence, the grammar is compiled as:
Grammar = h(Base ? Rules)
where h is a homomorphism that replaces TX
symbols with , the empty string.
5 Efficiency Considerations
Because the construction method proposed can
very quickly produce automata of considerable
size, there are a few issues to consider when de-
signing a grammar this way. Of primary concern
is that since one is constructing deterministic au-
tomata, long-distance constraints should be kept
to a minimum. Local constraints, which the ma-
jority of grammar rules encode, yield so-called k-
testable languages when represented as finite au-
tomata, and the state complexity of their inter-
section grows additively. For larger k, however,
growth is more rapid which means that, for ex-
ample, when one is designing the content of the
individual tapes, care should be taken to ensure
that segments or symbols which are related to each
other preferably align very closely on the tapes.
Naturally, this same goal is of linguistic interest as
well and a grammar which does not align gram-
matical information with segments in the input is
likely not a good grammar. However, there are a
couple of ways in which one can go astray. For
instance, in the running example we have pre-
sented, one of the parse tapes has included the
symbol +3P +Masc +Sg, aligned with the affix
that represents the grammatical information:
. . .
T5 a
T6 +3P
+Masc
+Sg
. . .
However, if it be the case that what the parse
tape reflects is a prefix or a circumfix, as will be
the case with the imperfective, subjunctive and
jussive forms, the following alignment would be
somewhat inefficient:
. . .
T5 t a
T6 +3P
+Fem
+Sg
. . .
This is because the prefix ta, which appears
early in the word, is reflected on tape 6 at the end
of the word, in effect unnecessarily producing a
very long-distance dependency and hence dupli-
cates of states in the automaton encoding the in-
tervening material. A more efficient strategy is to
place the parse or annotation tape material as close
as possible to the segments which have a bearing
on it, i.e.:
. . .
T5 t a
T6 +3P
+Fem
+Sg
. . .
This alignment can be achieved by a constraint
in the grammar to the effect that the first non-blank
symbol on the affix tape is in the same position as
the first non-blank symbol on the affix parse tape.
It is also worth noting that our implementation
does not yet restrict the co-occurrence of roots and
forms, i.e. it will parse any word in any root in the
lexicon in any of the forms I-VIII, X. Adding these
restrictions will presumably produce some growth
in the automaton. However, for the time being we
have also experimented with accepting any trilit-
eral root?i.e. any valid consonantal combination.
This has drastically cut the size of the resulting
automaton to only roughly 2,000 states without
much overgeneration in the sense that words will
not incorrectly be matched with the wrong root.
The reason for this small footprint when not hav-
ing a ?real? lexicon is fairly obvious?all depen-
dencies between the root tape and the pattern tape
and the input tape are instantly resolved in the span
of one ?column? or 7 symbols.
6 Algorithmic additions
Naturally, one can parse words by simply inter-
secting TapeL(1, word) ? Grammar, where
24
word is the word at hand and printing out all the
legal strings. Still, this is unwieldy because of
the intersection operation involved and for faster
lookup speeds one needs to consider an algorith-
mic extension that performs this lookup directly
on the Grammar automaton.
6.1 Single-tape transduction
For our implementation, we have simply modified
the automaton matching algorithm in the toolkit
we have used, foma8 to, instead of matching ev-
ery symbol, matching the first symbol as the ?in-
put?, then outputting the subsequent n (where n
is 7 in our example) legal symbols if the subse-
quent input symbols match. Because the grammar
is quite constrained, this produces very little tem-
porary ambiguity in the depth-first search traversal
of the automaton and transduces an input to the
output tapes in nearly linear time.
7 Future work
The transduction mechanism mentioned above
works well and is particularly easy to implement
when the first ?tape? is the input tape containing
the word one wants to parse, since one can simply
do a depth-first search until the the next symbol
on the input tape (in our running example with 8
tapes, that would be 7 symbols forward) and dis-
card the paths where the subsequent tape 1 sym-
bols do not match, resulting in nearly linear run-
ning time. However, for the generation problem,
the solution is less obvious. If one wanted to sup-
ply any of the other tapes with a ready input (such
as form, root, and a combination of grammatical
categories), and then yield a string on tape 1, the
problem would be more difficult. Naturally, one
can intersect various TapeX(n, content) languages
against the grammar, producing all the possible in-
put strings that could have generated such a parse,
but this method is rather slow and results only in
a few parses per second on our system. Devis-
ing a fast algorithm to achieve this would be desir-
able for applications where one wanted to, for in-
stance, generate all possible vocalization patterns
in a given word, or for IR purposes where one
would automatically apply vocalizations to Arabic
words.
8See the appendix.
8 Conclusion
We have described a straightforward method by
which morphological analyzers for languages that
exhibit root-and-pattern morphology can be built
using standard finite-state methods to simulate
multi-tape automata. This enables one to take
advantage of already widely available standard
toolkits designed for construction of single-tape
automata or finite-state transducers. The feasibil-
ity of the approach has been tested with a limited
implementation of Arabic verbal morphology that
contains roughly 2,000 roots, yielding automata of
manageable size. With some care in construction
the method should be readily applicable to larger
projects in Arabic and other languages, in partic-
ular to languages that exhibit root-and-pattern or
templatic morphologies.
References
Beesley, K. and Karttunen, L. (2003). Finite-State
Morphology. CSLI, Stanford.
Beesley, K. R. (1996). Arabic finite-state analysis
and generation. In COLING ?96.
Beesley, K. R. (1998a). Arabic morphology us-
ing only finite-state operations. In Proceedings
of the Workshop on Computational Approaches
to Semitic Languages COLING-ACL, pages 50?
57.
Beesley, K. R. (1998b). Consonant spreading in
Arabic stems. In ACL, volume 36, pages 117?
123. Association for Computational Linguis-
tics.
Beeston, A. F. L. (1968). Written Arabic: An ap-
proach to the basic structures. Cambridge Uni-
versity Press, Cambridge.
Buckwalter, T. (2004). Arabic morphological an-
alyzer 2.0. LDC.
Habash, N. and Rambow, O. (2006). MAGEAD:
A morphological analyzer and generator for the
Arabic dialects. Proceedings of COLING-ACL
2006.
Habash, N., Rambow, O., and Kiraz, G. (2005).
Morphological analysis and generation for Ara-
bic dialects. Proceedings of the Workshop
on Computational Approaches to Semitic Lan-
guages (ACL ?05).
Hulden, M. (2008). Regular expressions and pred-
icate logic in finite-state language processing.
25
In Piskorski, J., Watson, B., and Yli-Jyra?, A.,
editors, Proceedings of FSMNLP 2008.
Kataja, L. and Koskenniemi, K. (1988). Finite-
state description of Semitic morphology: a case
study of ancient Akkadian. In COLING ?88,
pages 313?315.
Kay, M. (1987). Nonconcatenative finite-state
morphology. In Proceedings of the third con-
ference on European chapter of the Association
for Computational Linguistics, pages 2?10. As-
sociation for Computational Linguistics.
Kiraz, G. A. (1994). Multi-tape two-level mor-
phology: A case study in Semitic non-linear
morphology. In COLING ?94, pages 180?186.
Kiraz, G. A. (2000). Multi-tiered nonlinear mor-
phology using multitape finite automata: A case
study on Syriac and Arabic. Computational Lin-
guistics, 26(1):77?105.
Kiraz, G. A. (2001). Computational nonlinear
morphology: with emphasis on Semitic lan-
guages. Cambridge University Press, Cam-
bridge.
Koskenniemi, K. (1983). Two-level morphology:
A general computational model for word-form
recognition and production. Publication 11,
University of Helsinki, Department of General
Linguistics, Helsinki.
Lavie, A., Itai, A., and Ornan, U. (1988). On the
applicability of two level morphology to the in-
flection of Hebrew verbs. In Proceedings of
ALLC III, pages 246?260.
McCarthy, J. J. (1981). A Prosodic Theory of Non-
concatenative Morphology. Linguistic Inquiry,
12(3):373?418.
van Noord, G. (2000). FSA 6 Reference Manual.
Wehr, H. (1979). A Dictionary of Modern Writ-
ten Arabic. Spoken Language Services, Inc.,
Ithaca, NY.
Yli-Jyra?, A. and Koskenniemi, K. (2004). Compil-
ing contextual restrictions on strings into finite-
state automata. The Eindhoven FASTAR Days
Proceedings.
Yona, S. and Wintner, S. (2008). A finite-state
morphological grammar of Hebrew. Natural
Language Engineering, 14(2):173?190.
9 Appendix
The practical implementation described in the pa-
per was done with the freely available (GNU Li-
cence) foma finite-state toolkit.9. However, all of
the techniques used are available in other toolk-
its as well, such as xfst (Beesley and Karttunen,
2003), or fsa (van Noord, 2000)), and translation
of the notation should be straightforward.
The functions for populating the tapes in section
4.2, were defined in foma as follows:
TapeL(X,Y) =
[[Y ? [[0?\X \X]* [0?X]
?
]*]2
[X E|\X \X]*]
TapeM(X,Y) = [[Y ? [0?[\X \X|X E]]*
[0?\X \X]* [0?X]
?
]*]2 [X E|\X \X]*]
TapeA(X,Y) = [[Y ?
[0?\X \X|X E]* 0?X
?
]*]2;
Here, TapeX is a function of two variables, X
and Y. Transducer composition is denoted by ?,
cross-product by ?, the lower projection of a re-
lation by L2, and union by |. Brackets indicate
grouping and ? any symbol. The notation \X de-
notes any single symbol, except X . The symbol ?
here is the special ?blank? symbol used to pad the
tapes and keep them synchronized.
9http://foma.sourceforge.net
26
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 772?780, Dublin, Ireland, August 23-29 2014.
Why implementation matters: Evaluation of an open-source constraint
grammar parser
D
?
avid M
?
ark Nemeskey
Institute for Computer Science and Control,
Hungarian Academy of Sciences,
H-1111 Budapest
nemeskey.david@sztaki.mta.hu
Francis M. Tyers
HSL-fakultetet,
UiT Norgga ?arktala?s universitehta,
9017 Romsa (Norway)
francis.tyers@uit.no
Mans Hulden
Department of Linguistics,
University of Colorado Boulder
80309 Boulder, Colorado
mans.hulden@colorado.edu
Abstract
In recent years, the problem of finite-state constraint grammar (CG) parsing has received renewed
attention. Several compilers have been proposed to convert CG rules to finite-state transducers.
While these formalisms serve their purpose as proofs of the concept, the performance of the
generated transducers lags behind other CG implementations and taggers.
In this paper, we argue that the fault lies with using generic finite-state libraries, and not with
the formalisms themselves. We present an open-source implementation that capitalises on the
characteristics of CG rule application to improve execution time. On smaller grammars our
implementation achieves performance comparable to the current open-source state of the art.
1 Introduction
Constraint grammar (CG), described originally by Karlsson (1990), is a rule-based formalism for vari-
ous linguistics tasks, including morphological analysis, clause boundary detection and surface syntactic
parsing. It has been used in a wide range of application areas, such as morphological disambiguation,
grammar checking and machine translation (Bick, 2011). CG owns its popularity to two reasons: first, it
achieves high accuracy on free text. Second, it works for languages where the annotated corpora required
by statistical parsing methods are not available, but a linguist willing to work on the rules is. The orig-
inal CG has since been superseded by CG-2 (Tapanainen, 1996) and lately, the free/open-source VISL
CG-3 (Bick, 2000; Didriksen, 2011).
Constraint grammar, however, has its drawbacks, one of which is speed. The Apertium machine
translation project (Forcada et al., 2011) uses both CG (via VISL CG-3) and n-gram based models for
morphological disambiguation, and while CG achieves higher accuracy, the n-gram model runs about
ten times faster.
In this paper, we investigate how using finite-state transducers (FST) for CG application can help to
bridge the performance gap. In recent years, several methods have been proposed for compiling a CG to
FST and applying it on text: Hulden (2011) compiles CG rules to transducers and runs them on the input
sentences; Peltonen (2011) converts the sentences into ambiguous automata and attempts to eliminate
branches by intersecting them with the rule FSTs; finally, Yli-Jyr?a (2011) creates a single FST from the
grammar and applies it on featurised input. Unfortunately, none of the authors report exact performance
measurements of their systems. Yli-Jyr?a published promising numbers for the preprocessing step, but
nothing on the overall performance. Peltonen, on the other hand, observed that ?VISL CG-3 was 1,500
times faster? than his implementation (Peltonen, 2011).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
772
We do not attempt here to add a new method to this list; instead, we concentrate on three practical
aspects of FST-based CG. First, we report accurate measurements of the real-world performance of one
of the methods above. Second, we endeavour to optimise the implementation of the selected method.
All three works used foma, an open source FST library (Hulden, 2009b; Hulden, 2009a). We show
that while foma is fast, relying on specialised FST application code instead of a generic library clearly
benefits performance. We also demonstrate what further improvements can be achieved by exploiting the
peculiarities of CG. Lastly, our research also aims to fill the niche left by the lack of openly accessible
finite-state CG implementations.
Section 2 briefly introduces the method we chose to evaluate. In the rest of the paper, we present our
optimisations in a way that mirrors the actual development process. We start out with a simple rule engine
based on foma, and improve it step-by-step, benchmarking its performance after each modification,
instead of a single evaluation chapter. We start in Section 3 by describing our evaluation methodology.
Section 4 follows the evolution of the rule engine, as it improves in terms of speed. Section 5 contains
a complexity analysis and introduces an idea that theoretically allows us to improve the average- and
best-case asymptotic bound. Section 6 demonstrates how memory savings can be derived from the steps
taken in section 4. Finally, Section 7 contains our conclusions and lists the problems that remain for
future work.
2 The fomacg compiler and fomacg-proc
We have chosen Hulden?s fomacg compiler for our study. Our reasons for this are twofold. The transduc-
ers generated by fomacg were meant to be run on the input directly, but they could also be applied to a
finite-state automaton (FSA) representation of the input sentence via FST composition, thereby giving us
more space to experiment. Peltonen?s method, on the other hand, works only through FST intersection.
More importantly, fomacg was the only compiler that is openly available.
1
Here we briefly describe how fomacg works; for further details refer to (Hulden, 2011). A CG used for
morphological disambiguation takes as input a morphologically analysed text, which consists of cohorts:
a word with its possible readings. A reading is represented by a lemma and a set of morphosyntactic
tags. For example, the cohort of the ambiguous Hungarian word sz??v with two readings ?heart? and
?to suck? would be ?sz??v/sz??v<n><sg><nom>/sz??v<vblex><pres><s3p>$.
2
The text is
tokenised into sentences based on a set of delimiters. CG rules operate on a sentence, removing readings
from cohorts based on their context. The rules can be divided into priority levels called section. Most
implementations apply the rules one-by-one in a loop, until no rules can further modify the sentence.
fomacg expects cohorts to be encoded in a different format; the cohort in the example above would be
represented as
$0$ "<sz??v>" #BOC# |
#0# "sz??v" n sg nom |
#0# "sz??v" vblex pres s3p | #EOC#
The rule transducers mark readings for removal by replacing the #0# in front of the reading by #X#;
they act as identity for sentences they cannot be applied to.
fomacg is only a compiler, which reads a CG rule file and emits a foma FST for each rule. The
actual disambiguator program that applies the transducers to text we implemented ourselves. It reads the
morphologically analysed input in the Apertium stream format, converts it into the format expected by
fomacg, applies the transducers to it, and then converts the result back to the stream format. To emphasise
its similarity to cg-proc, VISL CG?s rule applier, we named our program fomacg-proc.
3 Methodology
Apertium includes constraint grammars for several languages.
3
While most of these are wide-coverage
grammars, and are being actually used for morphological disambiguation in Apertium, they are also
1
In the Apertium software repository: https://svn.code.sf.net/p/apertium/svn/branches/fomacg
2
The example is in the Apertium stream format, not in CG-2 style.
3
http://wiki.apertium.org/wiki/Constraint_grammar
773
too big and complex to be easily used for the early stages of parser development. Therefore, we have
written a small Hungarian CG, aimed to fully disambiguate a short Hungarian story, which was used
as the development corpus. Since Hungarian is not properly supported by Apertium yet, morphological
analysis was carried out by Hunmorph (Tr?on et al., 2005), and the tags were translated to the Apertium
tagset with a transducer in foma.
The performance of fomacg-proc has been measured against that of VISL CG. The programs were
benchmarked with three Apertium CG grammars: the toy Hungarian grammar mentioned earlier, the
Breton grammar from the br-fr language pair (Tyers, 2010), and the version of the Finnish grammar
originally written by Karlsson in the North S?ami?Finnish (sme-fin) pair. Seeing that in the early
phases, only the Hungarian grammar was used for development, results for the other two languages are
reported only for the later steps.
Each grammar was run on a test corpus. For Breton, we used the corpus in the br-fr language
pair, which consists of 1,161 sentences. There are no Finnish and Hungarian corpora in Apertium; for
the former, we used a 1,620-sentence excerpt from the 2013-Nov-14 snapshot of the Finnish Wikipedia,
while for the latter, the short test corpus used for grammar development. Since the latter contains a mere
11 sentences, it was repeated 32 times to produce a corpus similar in size to the other two.
4
The Breton
and Finnish corpora were tagged by Apertium?s morphological analyser tools.
Since VISL CG implements CG-3, and fomacg only supports CG-2, a one-to-one comparison with the
grammars above was not feasible. Therefore, we extracted the subset of rules from each that compiled
under fomacg, and carried out the tests on these subsets. Table 1 shows the number of rules in the original
and the CG-2 grammars.
Table 1: Grammar sizes with the running time and binary size of the respective VISL-CG grammars
Language Rules CG-2 rules Binary Time
Hungarian 33 33 8kB 0.284s
Breton 251 226 36kB 0.77s
Finnish 1207 1172 184kB 1.78s
We recorded both initialisation and rule application time for the two programs, via instrumentation in
case of fomacg-proc and by running the grammar first on an empty file and then on the test corpus in
case of cg-proc. However, as initialisation is a one-time cost, in the following we are mainly concerned
with the time required for applying rules. The tests were conducted on a consumer-grade laptop with a
2.2GHz Core2Duo CPU and 4GB RAM, running Linux.
4 Performance optimisations
Our implementation, much like that of fomacg (and indeed, all recent work on finite state CG) is based on
the foma library. We started out with a na??ve implementation that used solely stock foma functions. Most
of the improvements below stem from the fact that we have replaced these functions with custom versions
that run much faster. The final implementation abandons foma entirely, but for the data structures. In the
future, we plan to discard those as well, making our code self-contained.
The program loads the transducers produced by fomacg and applies them to the text. The input is in
the Apertium stream format
5
and it is read cohort-by-cohort. A foma FST is used to convert each cohort
to the format expected by the rule transducers, and to convert the final result back.
To tokenise the text to sentences, we modified fomacg to compile the delimiters set and emit it as the
first FSA in the binary representation of the grammar. fomacg-proc reads the input until a cohort matches
this set and then sends the accumulated sentence to the rule applier engine.
4
Although we used the same corpus for development and testing for Hungarian, the experimental setup was the same for
VISL-CG and fomacg. While the numbers we acquired for Hungarian are not representative of how a proper Hungarian CG
would perform on unseen data, they clearly show which of our steps benefit performance.
5
http://wiki.apertium.org/wiki/Apertium_stream_format
774
The rules are tested one-by-one, section-by-section, to see if any of them can be applied to the text.
Once such a rule is found, the associated FST is executed on the text. As it is possible that a rule that
was not applicable to the original text would now run on the modified one, testing is restarted from the
first section after each rule application. The process ends when no more applicable rules are found.
4.1 Na??ve implementation
The first version of the program used the apply down() foma function both for rule application and
format conversion. As fomacg generated a single FST for a rule, rule testing and execution was done
in the same step, by applying the FST. Whether the rule was actually applied or not was decided by
comparing the original sentence to the one returned by the function.
The first row in Table 2 shows the running time for the Hungarian grammar. At 6.4s, the na??ve
implementation runs more than 20 times slower than VISL-CG (see Table 1). Luckily a far cry from
the 1,500 reported by Peltonen, but clearly too slow to be of practical use.
4.2 FST composition
Another way to apply a rule is to convert the input sentence into a single-path FSA with the same alphabet
as the rules and compose the rule FST on top of it. To check if the rule has actually be applied, the input
automaton was intersected with the result. Unfortunately, this method proved to be much slower than
the application-based one; composition alone took 28.3 seconds on our corpus, while the intersection
pushed it up to 45s. Therefore we decided to abandon this path altogether.
4.3 Deletion of discarded readings
The original transducers replace the #0# in front of discarded readings with #X#. Our first optimisation
comes from the observation that deleting these readings instead would not make the transducers any
more complex, but would shorten the resulting sentence, making subsequent tests faster. Moreover, it
allows the engine to recognise actual rule application by simply testing the length of the output to the
input sentence, an operation slightly faster than byte-for-byte comparison.
Table 2 reports an approximately 8% improvement. While not self-evident, this benefit remained in
effect after our subsequent optimisations.
4.4 FSA-based rule testing
Theoretically, further speed-ups could be achieved by separating rule testing and application, using finite-
state automata for the former. Automata are faster than transducers for two reasons: first, there is no need
to assemble an output; and second, a FSA can be determinised and minimised, while foma can only make
a FST deterministic by treating it as a FSA with an alphabet of the original input:output pairs, which does
not entail determinism in the input.
As the fourth row in table 2 shows, the idea does not immediately translate well to practice. The fault
lies with the apply down() function, which, being the only method of running a finite-state machine
in foma, was designed to support all features of the library. It treats automata as identity transducers, and
fails to capitalise on the aforementioned advantages of the former. In order to benefit from FSA-based
testing then, a custom function is required.
4.5 Custom FSA/FST application
The apply down() function supports the following features (Hulden, 2009a):
? Conversion of the text to symbols (single- and multi-character)
? Regular transitions and flag diacritics
? Three types of search in the transition matrix (linear, binary and indexed)
? Deterministic and non-deterministic operation
? Iterators (multiple invocations iterate the non-deterministic outputs)
775
Our use-case makes most of these features surplus to requirements. fomacg uses multi-character sym-
bols, but not flag diacritics. To maximise the performance gains, the rule testing automata must be
minimal (hence deterministic), so there was no need for non-determinism and iterators. Finally, by mod-
ifying fomacg to sort the edges of all grammar machines, we could ensure that binary transition search
alone suffices.
The custom FSA applier function that implements only the necessary features was employed for both
rule testing and finding the delimiter cohort. As a result, running time went down to 1.45 seconds (see
table 2), a 75% improvement.
A similar function was written for input-deterministic minimal transducers. While not applicable to the
non-deterministic rule FSTs, it could replace apply down() for the conversion between the Apertium
and the fomacg formats, further reducing the running time to 1.275 seconds.
What we can take home from the last two sections is that when speed is paramount, relying blindly
on generic libraries may not only lead to suboptimal performance, but may also produce counterintuitive
results.
Conversely, libraries may benefit from including specialised implementations for different use-cases.
For example, foma has all the information at hand to decide if a FST is deterministic, whether it supports
binary search or not, etc. and so, providing specialised functions (even private ones hidden behind
apply down()) would improve its performance substantially in certain situations.
4.6 Exploiting CG structure
In this chapter, we review the improvements made available by the characteristics of our CG representa-
tion. The first of these is functionality: even though the rule FSTs are non-deterministic, the input-output
mapping is one-to-one (Hulden, 2011). It was thus possible to implement the non-deterministic version
of the FST runner function described in the last section without the need for an iterator feature, and to
use it for rule application. The last usage of the generic apply down() function thus eradicated, the
running time dropped to 1.05 seconds (see table 2).
Internally foma, similarly to other FST toolkits, represents elements of the ? alphabet as integers. The
conversion of text into tokens in ? is a step usually taken for granted in the literature, but it contributes
to the execution time of an FST to a significant extent. In foma, token matching is performed by a trie
built from the symbols in the automaton?s alphabet. Our custom DFSA runner function (see section 4.5)
spends about 60% of its time applying this trie.
The two enhancements below have helped to all but negate the cost of token conversion. The first of
these exploits the fact that in the fomacg format, symbols are separated by space characters. Instead of
passing the input string to each FSM, we split it along the spaces, and pass the resulting string vector to
the machines. This is a rather small change, and while the Hungarian grammar benefited almost nothing,
the running time of the Breton grammar improved by 40%.
The second enhancement came from the observation that all rule testing automata and rule transducers
accept the same CG tags. It is thus possible to generate an automaton whose alphabet is the union of
those of the other machines. This automaton could be used to convert the input sentence into a vector
of ? ids, and then this vector could be sent to the other machines, relinquishing the need of repeated
conversions.
Both fomacg and fomacg-proc had to be modified to account for the changes. The former now creates
the converter FSA and saves it as the second machine in the binary grammar file. Also, since the ids
that correspond to a symbol are unique to each machine, we added a post-processing phase that replaces
the ids with the ?canonical? ones in the converter FSA. fomacg-proc then converts the input to ids using
the converter automaton?s trie, and sends the vector to the rule machines. The rule machines treat the
vector as their input, with a caveat: ids not in the alphabet of the machine in question are replaced by
@IDENTITY SYMBOL@, so that they are handled in the same way as before.
Table 2 shows that factoring the symbol conversion out from the individual machines resulted in huge
savings: the running time of the Hungarian setup improved by 70% to 0.32 second; the Breton one by
40% to 1.55 seconds.
776
Table 2: Effects of the optimisations on running time
Version Hungarian Breton
Na??ve (4.1) 6.4s ?
Composition (4.2) 45s ?
Delete readings (4.3) 5.9s ?
FSA rule testing (4.4) 10s ?
Custom FSA runner (4.5) 1.45s ?
Custom format-FST (4.5) 1.275s 6.8s
Input partitioning (4.6) 1.15s 4s
Custom rule applier (4.6) 1.05s 2.6s
One-time conversion (4.6) 0.32s 1.55s
5 Complexity analysis
Tapanainen (1999) proves that the worst-case time complexity for disambiguating a sentence in his CG-2
parser is O(n
3
k
2
G), where n is the length of the sentence, k is the maximum number of readings per
word, and the grammar consists of G rules. The explanation is as follows: testing a cohort with a single
rule can be done in O(nk); the whole sentence in O(n
2
k). This process must be repeated for each rule,
yielding O(n
2
kG). Finally, in the worst case, a rule only removes a single reading, so it takes n(k ? 1)
rounds to disambiguate the sentence, resulting in the aforementioned bound.
Hulden (2011) showed that if the rules are compiled to transducers, they can be applied to the whole
sentence in O(nk) time, thus decreasing the complexity to O(n
2
k
2
G), instead of the O(n
2
k) suggested
by Tapanainen. To be more precise, applying a rule transducer takes O(nkT ) time, where the constant
T is the size of the FST. While T may be rather large, rule transducers may be factored into bimachines,
which removes the constant. Hence, a disambiguating bimachine for one CG rule can be applied to a
sentence of nk tokens in O(nk) (linear) time. However, fomacg only includes CG rule-to-transducer
compilation and does not include bimachine factorization as of yet.
While this work has left the theoretical limit untouched thus far, it improved on three aspects of the
complexity. First, unlike foma, our specialised FST application functions can take advantage of the
properties of automata and bimachines, and actually run them in O(nk) time. Second, the constant in
the O has been decreased as a result of extensive optimisation. Third, rule testing automata have been
introduced which, being minimal, can also be applied in O(nk) time. Assume that in a round G
a
rules
can be applied to the sentence and G
u
cannot, G
a
+G
u
= G. With minimal automata for rule testing the
round finishes with 2G
a
+G
u
machine applications, instead of the 2G required by bimachines. The facts
that usually G
a
<< G and that automata can be applied faster than transducers result in a performance
improvement over the pure bimachine setup.
5.1 Beyond the O(n
2
k
2
G) bound
This section presents an idea that allows the system to theoretically overcome the O(n
2
k
2
G) average
complexity bound. This section describes the method, and investigates its feasibility; the next section
contains the evaluation.
The idea is based on the fact that regular languages are closed under the union operator. If there are
two automata, FSA
G
a
and FSA
G
b
that test the rules G
a
and G
b
, respectively, then it follows that their
union, FSA
G
ab
, accepts a sentence iff either G
a
or G
b
is applicable to it. If FSA
G
ab
is minimised, it runs
in O(nk) time, the same as FSA
G
a
and FSA
G
b
.
The union FSA allows us to implement hierarchical rule checking. In this example, testing if any of
the two rules match a sentence with only the original automata requires a check with both. Instead, we
can apply FSA
G
ab
first. If neither rule is applicable, the automaton will not accept the sentence, and no
further testing is required. If one of the rules is, FSA
G
a
(or equivalently, FSA
G
b
) must be run against
the sentence to see which. In practice, if we pick two rules from a CG in random, we shall find that the
majority of the sentences will not match either, hence the number of tests may be reduced substantially.
777
There is no need to stop here: we can take two union automata, and merge them again. It is easy to
see that if we represent the rule testing automata in a graph, where a node is a FSA, and two nodes are
connected iff one was created from the other via union, then we get a binary tree. For a grammar of G
rules, a binary tree of logG levels can be built. Such a tree can confirm with a single test if a sentence
does not match any of the rules, or find the matching rule in logG+ 1 tests, if one does. Accordingly, in
theory this method allows us to improve the average- and best-case complexity bounds of the system to
O(n
2
k
2
logG) andO(nk), respectively. (Clearly, for grammars with several sections, instead of a single
tree that contains all rules, one tree must be built for each section to preserve rule priorities. However,
this does not affect the reasoning above).
The bottleneck in this method is memory consumption. The size of the FSA resulting from a non-
deterministic union operation is simply the sum of the sizes of the original automata. To achieve the
speed-up described above, however, the rule checking automata must be determinised, which may cause
them to blow up in size exponentially. Therefore, building a single tree from all rules is not feasible.
A compromise solution is to construct a forest of 2?4 level trees, which still fits into the memory and
provides similar benefits to a single tree, though to a smaller extent.
5.2 Evaluation
The forest can be assembled in several ways; we experimented with two simple algorithms. Both take as
input a list of rule testing automata, which are encapsulated into single-node trees. Before each step, the
trees are sorted by the size of the automata in their roots.
The first algorithm, SmallestFirst, unifies the two smallest trees in each step, until the root FSA in each
tree is above a size limit (1,000 states in our case). The second, FixedLevel, aims to create full, balanced
binary trees: in a single step, it unifies the smallest tree with the largest, the second smallest with the
second largest, etc, and repeats the process until the trees reach a predefined height.
Table 3 lists the running times and memory requirements of the resulting forests. It can be seen that
hierarchical rule testing indeed improves performance: even a single level of merging results in 30-
42% speedup. However, it is also immediately evident that aside from special cases, the disadvantages
overweight the benefits: memory usage and binary size grow exponentially, affecting compilation and
grammar loading time as well, and very soon we run into the limits of physical memory. Unless a method
is found that reduces memory usage substantially, we have to give up on hierarchical rule testing.
Table 3: Performance and storage requirements of rule testing trees
?
State count limit was 500
?
Reached limit of physical memory
Language Algorithm Initialisation Disambiguation Memory File size
Hungarian (flat) 0.028s 0.32s 0.5% 60kB
Hungarian FixedLevel(3) 0.77s 0.235s 2.1% 7.1MB
Hungarian Smallest First 0.62s 0.234s 1.9% 5.9MB
Breton (flat) 0.5s 1.55s 5.1% 1.5MB
Breton FixedLevel(2) 1.8s 1.09s 9.6% 7.4MB
Breton Smallest First 11.14s 1.05s 28.7% 60MB
Finnish (flat) 1.5s 22.87s 21.8% 7.2MB
Finnish FixedLevel(2) 3.64s 13.28s 32.3% 28MB
Finnish SmallestFirst
?
20.75s 9.95s ?
?
198MB
6 Memory savings
The use of a single converter automaton has not only resulted in improved performance, but it has also
opened a way to decrease the storage space requirements of the grammar as well. The trie that converts
the machine?s alphabet to integer ids in foma takes up space; depending on the number and length of the
symbols in bytes, this trie may be responsible for a considerable portion of the memory footprint of an
778
automaton. Given the number of rules in an average CG grammar, it is easy to see how this trivial sub-
task may affect the memory consumption of the application, as well as the size of the grammar binary.
As the job of token matching has been delegated to the symbol automaton (see section 4.6), we no longer
maintain separate tries for all individual FSAs.
Table 4 presents the resulting memory savings. We report numbers for the raw grammars (L1), as
well as for two- and three-level condition trees (L2-3). It is not surprising that the raw grammars see the
largest improvements; here the tries accounted for 70-80% of the memory usage. As the trees get higher,
the number of states and edges grows more rapidly than does the number of tries and the savings become
more modest.
Table 4: Improvements in memory usage due to removing the sigma trie. Memory consumption is
measured as a percentage of the 4GB system memory
Language Method Before After Reduction
Hungarian L1 0.5% 0.1% 80%
Hungarian L3 2.1% 1.5% 28.57%
Breton L1 5.1% 1.3% 74.5%
Breton L2 9.6% 4.4% 54.16%
Finnish L1 21% 4.1% 80.47%
Finnish L2 32.3% 8.9% 72.44%
We explored other options as well to reduce the size of rule condition trees. Unfortunately, most
methods aimed at FSA compression in the literature are either already implemented in foma (e.g. as
row-indexed transition matrix, see Kiraz (2001)), or are aimed at automata with a regular structure, such
as morphological analysers (Huet, 2003; Huet, 2005; Drobac et al., 2014). Without further support, the
approximately 30% saving achieved by our method for a three-level condition tree alone is not enough
to redeem hierarchical rule checking.
A task-specific framework, one based on inward deterministic automata has been proposed for CG
parsing (Yli-Jyr?a, 2011). The paper reports a binary size similar to the original grammar size. However,
as the framework breaks away from the practice of direct rule application followed in this paper and in
related literature (Hulden, 2011; Peltonen, 2011), closer inspection remains as future work.
7 Conclusions
We set out with the goal of creating a fast constraint grammar parser based on finite-state technology.
Our aim was to achieve better performance on the task of morphological disambiguation than the cur-
rent state-of-the-art parser VISL CG-3. We used the CG grammars available in the Apertium machine
translation project.
Our goals were partially fulfilled: while the speed of our parser falls short of that of VISL CG-3 ?
with the exception of the execution of very small grammars ? we have made advances on the state-
of-the-art free/open-source FST implementations of CG. We based our system on the fomacg compiler,
and extended it in several ways. Our parser uses optimised FST application methods instead of the
generic foma variant used by previous implementations, thereby achieving better performance. Further
optimisations, both memory and runtime, were made by exploiting the properties of FSTs generated
from a CG. We report real-world performance measurements with and without these optimisations, so
their efficacy can be accurately evaluated. A new method for rule testing has also been proposed, which
in theory is capable of reducing the worst-case complexity bound of CG application to O(n
2
k
2
logG).
Unfortunately, the method has yet to be proven feasible in practice.
Our main finding is that implementation matters: an FST library which is too generic hinders perfor-
mance and can even make a theoretically faster algorithm slower in practice. Using bimachines and rule
testing automata should have sped up rule application, but only did so after we implemented our own,
specialised FST functions. Since foma has all necessary information about an FST in place to decide
779
the right application method, incorporating our functions into it, or other FST libraries, could benefit
applications beyond the scope of CG.
Acknowledgements
This research was supported through an Apertium project in the 2013 Google Summer of Code.
6
References
Eckhard Bick. 2000. The parsing system? Palavras?: Automatic grammatical analysis of Portuguese in a con-
straint grammar framework. Aarhus Universitetsforlag.
Eckhard Bick. 2011. Constraint grammar applications. In Proceedings of the NODALIDA 2011 Workshop:
Constraint Grammar Applications, page iv.
Tino Didriksen. 2011. Constraint grammar manual: 3rd version of the CG formalism variant.
Senka Drobac, Krister Lind?en, Tommi Pirinen, and Miikka Silfverberg. 2014. Heuristic hyper-minimization
of finite state lexicons. In Proceedings of the Ninth International Conference on Language Resources and
Evaluation (LREC?14), Reykjavik, Iceland. European Language Resources Association (ELRA).
Mikel L Forcada, Mireia Ginest??-Rosell, Jacob Nordfalk, Jim O?Regan, Sergio Ortiz-Rojas, Juan Antonio P?erez-
Ortiz, Felipe S?anchez-Mart??nez, Gema Ram??rez-S?anchez, and Francis M Tyers. 2011. Apertium: a free/open-
source platform for rule-based machine translation. Machine Translation, 25(2):127?144.
G?erard Huet. 2003. Automata mista. Lecture notes in computer science, pages 359?372.
G?erard Huet. 2005. A functional toolkit for morphological and phonological processing, application to a Sanskrit
tagger. Journal of Functional Programming, 15(4):573?614.
Mans Hulden. 2009a. Finite-state machine construction methods and algorithms for phonology and morphology.
Ph.D. thesis.
Mans Hulden. 2009b. Foma: a finite-state compiler and library. In Proceedings of the 12th Conference of the
European Chapter of the Association for Computational Linguistics: Demonstrations Session, pages 29?32.
Association for Computational Linguistics.
Mans Hulden. 2011. Constraint grammar parsing with left and right sequential finite transducers. In Proceedings
of the 9th International Workshop on Finite State Methods and Natural Language Processing, pages 39?47.
Association for Computational Linguistics.
Fred Karlsson. 1990. Constraint grammar as a framework for parsing running text. In Proceedings of the 13th
conference on Computational linguistics-Volume 3, pages 168?173. Association for Computational Linguistics.
George Anton Kiraz. 2001. Compressed storage of sparse finite-state transducers. In Automata Implementation,
pages 109?121. Springer.
Janne Peltonen. 2011. A finite state constraint grammar parser. In Proceedings of the NODALIDA 2011 Workshop:
Constraint Grammar Applications, pages 35?40.
Pasi Tapanainen. 1996. The constraint grammar parser CG-2. University of Helsinki, Department of General
Linguistics.
Pasi Tapanainen. 1999. Parsing in two frameworks: finite-state and functional dependency grammar. Ph.D.
thesis, University of Helsinki, Department of General Linguistics.
Viktor Tr?on, Andr?as Kornai, Gy?orgy Gyepesi, L?aszl?o N?emeth, P?eter Hal?acsy, and D?aniel Varga. 2005. Hun-
morph: open source word analysis. In Proceedings of the Workshop on Software, pages 77?85. Association for
Computational Linguistics.
Francis M Tyers. 2010. Rule-based Breton to French machine translation. In Proceedings of the 14th Annual
Conference of the European Association of Machine Translation, EAMT10, pages 174?181.
Anssi Yli-Jyr?a. 2011. An efficient constraint grammar parser based on inward deterministic automata. In Pro-
ceedings of the NODALIDA 2011 Workshop: Constraint Grammar Applications, pages 50?60.
6
https://google-melange.appspot.com/gsoc/project/details/google/gsoc2013/
davidnemeskey/5764017909923840
780
Proceedings of the EACL 2009 Demonstrations Session, pages 29?32,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
Foma: a finite-state compiler and library
Mans Hulden
University of Arizona
mhulden@email.arizona.edu
Abstract
Foma is a compiler, programming lan-
guage, and C library for constructing
finite-state automata and transducers for
various uses. It has specific support for
many natural language processing appli-
cations such as producing morphologi-
cal and phonological analyzers. Foma is
largely compatible with the Xerox/PARC
finite-state toolkit. It also embraces Uni-
code fully and supports various differ-
ent formats for specifying regular expres-
sions: the Xerox/PARC format, a Perl-like
format, and a mathematical format that
takes advantage of the ?Mathematical Op-
erators? Unicode block.
1 Introduction
Foma is a finite-state compiler, programming lan-
guage, and regular expression/finite-state library
designed for multi-purpose use with explicit sup-
port for automata theoretic research, construct-
ing lexical analyzers for programming languages,
and building morphological/phonological analyz-
ers, as well as spellchecking applications.
The compiler allows users to specify finite-state
automata and transducers incrementally in a simi-
lar fashion to AT&T?s fsm (Mohri et al, 1997) and
Lextools (Sproat, 2003), the Xerox/PARC finite-
state toolkit (Beesley and Karttunen, 2003) and
the SFST toolkit (Schmid, 2005). One of Foma?s
design goals has been compatibility with the Xe-
rox/PARC toolkit. Another goal has been to al-
low for the ability to work with n-tape automata
and a formalism for expressing first-order logi-
cal constraints over regular languages and n-tape-
transductions.
Foma is licensed under the GNU general pub-
lic license: in keeping with traditions of free soft-
ware, the distribution that includes the source code
comes with a user manual and a library of exam-
ples.
The compiler and library are implemented in C
and an API is available. The API is in many ways
similar to the standard C library <regex.h>, and
has similar calling conventions. However, all the
low-level functions that operate directly on au-
tomata/transducers are also available (some 50+
functions), including regular expression primitives
and extended functions as well as automata deter-
minization and minimization algorithms. These
may be useful for someone wanting to build a sep-
arate GUI or interface using just the existing low-
level functions. The API also contains, mainly for
spell-checking purposes, functionality for finding
words that match most closely (but not exactly) a
path in an automaton. This makes it straightfor-
ward to build spell-checkers from morphological
transducers by simply extracting the range of the
transduction and matching words approximately.
Unicode (UTF8) is fully supported and is in
fact the only encoding accepted by Foma. It has
been successfully compiled on Linux, Mac OS X,
and Win32 operating systems, and is likely to be
portable to other systems without much effort.
2 Basic Regular Expressions
Retaining backwards compatibility with Xe-
rox/PARC and at the same time extending the for-
malism means that one is often able to construct
finite-state networks in equivalent various ways,
either through ASCII-based operators or through
the Unicode-based extensions. For example, one
can either say:
ContainsX = ?* X ?*;
MyWords = {cat}|{dog}|{mouse};
MyRule = n -> m || p;
ShortWords = [MyLex1]1 ? ??<6;
or:
29
Operators Compatibility variant Function
[ ] () [ ] () grouping parentheses, optionality
? ? N/A quantifiers
\ ? term negation, substitution/homomorphism
: : cross-product
+ ? + ? Kleene closures
?<n ?>n ?{m,n} ?<n ?>n ?{m,n} iterations
1 2 .1 .2 .u .l domain & range
.f N/A eliminate all unification flags
? $ $. $? ? $ $. $? complement, containment operators
/ ./. /// \\\ /\/ / ./. N/A N/A ?ignores?, left quotient, right quotient, ?inside? quotient
? /? = 6= N/A language membership, position equivalence
 ? < > precedes, follows
? ? ? ? - .P. .p. | & ? .P. .p. union, intersection, set minus, priority unions
=> -> (->) @-> => -> (->) @-> context restriction, replacement rules
? <> shuffle (asynchronous product)
? ? .x. .o. cross-product, composition
Table 1: The regular expressions available in Foma from highest to lower precedence. Horizontal lines
separate precedence classes.
30
define ContainsX ?* X ?*;
define MyWords {cat}|{dog}|{mouse};
define MyRule n -> m || _ p;
define ShortWords Mylex.i.l & ??<6;
In addition to the basic regular expression oper-
ators shown in table 1, the formalism is extended
in various ways. One such extension is the abil-
ity to use of a form of first-order logic to make
existential statements over languages and trans-
ductions (Hulden, 2008). For instance, suppose
we have defined an arbitrary regular language L,
and want to further define a language that contains
only one factor of L, we can do so by:
OneL = (?x)(x ? L ? ?(?y)(y ? L
? ?(x = y)));
Here, quantifiers apply to substrings, and we at-
tribute the usual meaning to ? and ?, and a kind of
concatenative meaning to the predicate S(t1, t2).
Hence, in the above example, OneL defines the
language where there exists a string x such that
x is a member of the language L and there does
not exist a string y, also in L, such that y would
occur in a different position than x. This kind
of logical specification of regular languages can
be very useful for building some languages that
would be quite cumbersome to express with other
regular expression operators. In fact, many of the
internally complex operations of Foma are built
through a reduction to this type of logical expres-
sions.
3 Building morphological analyzers
As mentioned, Foma supports reading and writ-
ing of the LEXC file format, where morphological
categories are divided into so-called continuation
classes. This practice stems back from the earliest
two-level compilers (Karttunen et al, 1987). Be-
low is a simple example of the format:
Multichar_Symbols +Pl +Sing
LEXICON Root
Nouns;
LEXICON Nouns
cat Plural;
church Plural;
LEXICON Plural
+Pl:%?s #;
+Sing #;
4 An API example
The Foma API gives access to basic functions,
such as constructing a finite-state machine from
a regular expression provided as a string, per-
forming a transduction, and exhaustively matching
against a given string starting from every position.
The following basic snippet illustrates how to
use the C API instead of the main interface of
Foma to construct a finite-state machine encod-
ing the language a+b+ and check whether a string
matches it:
1. void check_word(char *s) {
2. fsm_t *network;
3. fsm_match_result *result;
4.
5. network = fsm_regex("a+ b+");
6. result = fsm_match(fsm, s);
7. if (result->num_matches > 0)
8. printf("Regex matches");
9.
10 }
Here, instead of calling the fsm regex() function to
construct the machine from a regular expressions,
we could instead have accessed the beforemen-
tioned low-level routines and built the network en-
tirely without regular expressions by combining
low-level primitives, as follows, replacing line 5
in the above:
network = fsm_concat(
fsm_kleene_plus(
fsm_symbol("a")),
fsm_kleene_plus(
fsm_symbol("b")));
The API is currently under active develop-
ment and future functionality is likely to include
conversion of networks to 8-bit letter transduc-
ers/automata for maximum speed in regular ex-
pression matching and transduction.
5 Automata visualization and
educational use
Foma has support for visualization of the ma-
chines it builds through the AT&T Graphviz li-
brary. For educational purposes and to illustrate
automata construction methods, there is some sup-
port for changing the behavior of the algorithms.
31
For instance, by default, for efficiency reasons,
Foma determinizes and minimizes automata be-
tween nearly every incremental operation. Oper-
ations such as unions of automata are also con-
structed by default with the product construction
method that directly produces deterministic au-
tomata. However, this on-the-fly minimization
and determinization can be relaxed, and a Thomp-
son construction method chosen in the interface so
that automata remain non-deterministic and non-
minimized whenever possible?non-deterministic
automata naturally being easier to inspect and an-
alyze.
6 Efficiency
Though the main concern with Foma has not
been that of efficiency, but of compatibility and
extendibility, from a usefulness perspective it is
important to avoid bottlenecks in the underly-
ing algorithms that can cause compilation times
to skyrocket, especially when constructing and
combining large lexical transducers. With this
in mind, some care has been taken to attempt
to optimize the underlying primitive algorithms.
Table 2 shows a comparison with some exist-
ing toolkits that build deterministic, minimized
automata/transducers. One the whole, Foma
seems to perform particularly well with patho-
logical cases that involve exponential growth in
the number of states when determinizing non-
deterministic machines. For general usage pat-
terns, this advantage is not quite as dramatic, and
for average use Foma seems to perform compa-
rably with e.g. the Xerox/PARC toolkit, perhaps
with the exception of certain types of very large
lexicon descriptions (>100,000 words).
7 Conclusion
The Foma project is multipurpose multi-mode
finite-state compiler geared toward practical con-
struction of large-scale finite-state machines such
as may be needed in natural language process-
ing as well as providing a framework for re-
search in finite-state automata. Several wide-
coverage morphological analyzers specified in the
LEXC/xfst format have been compiled success-
fully with Foma. Foma is free software and will
remain under the GNU General Public License.
As the source code is available, collaboration is
encouraged.
GNU AT&T
Foma xfst flex fsm 4
??a?15 0.216s 16.23s 17.17s 1.884s
??a?20 8.605s nf nf 153.7s
North Sami 14.23s 4.264s N/A N/A
8queens 0.188s 1.200s N/A N/A
sudoku2x3 5.040s 5.232s N/A N/A
lexicon.lex 1.224s 1.428s N/A N/A
3sat30 0.572s 0.648s N/A N/A
Table 2: A relative comparison of running a se-
lection of regular expressions and scripts against
other finite-state toolkits. The first and second en-
tries are short regular expressions that exhibit ex-
ponential behavior. The second results in a FSM
with 221 states and 222 arcs. The others are scripts
that can be run on both Xerox/PARC and Foma.
The file lexicon.lex is a LEXC format English dic-
tionary with 38418 entries. North Sami is a large
lexicon (lexc file) for the North Sami language
available from http://divvun.no.
References
Beesley, K. and Karttunen, L. (2003). Finite-State
Morphology. CSLI, Stanford.
Hulden, M. (2008). Regular expressions and pred-
icate logic in finite-state language processing.
In Piskorski, J., Watson, B., and Yli-Jyra?, A.,
editors, Proceedings of FSMNLP 2008.
Karttunen, L., Koskenniemi, K., and Kaplan,
R. M. (1987). A compiler for two-level phono-
logical rules. In Dalrymple, M., Kaplan, R.,
Karttunen, L., Koskenniemi, K., Shaio, S., and
Wescoat, M., editors, Tools for Morphological
Analysis. CSLI, Palo Alto, CA.
Mohri, M., Pereira, F., Riley, M., and Allauzen, C.
(1997). AT&T FSM Library-Finite State Ma-
chine Library. AT&T Labs?Research.
Schmid, H. (2005). A programming language for
finite-state transducers. In Yli-Jyra?, A., Kart-
tunen, L., and Karhuma?ki, J., editors, Finite-
State Methods and Natural Language Process-
ing FSMNLP 2005.
Sproat, R. (2003). Lextools: a toolkit for
finite-state linguistic analysis. AT&T Labs?
Research.
32
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 569?578,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Semi-supervised learning of morphological paradigms and lexicons
Malin Ahlberg
Spr?akbanken
University of Gothenburg
malin.ahlberg@gu.se
Markus Forsberg
Spr?akbanken
University of Gothenburg
markus.forsberg@gu.se
Mans Hulden
University of Helsinki
mans.hulden@helsinki.fi
Abstract
We present a semi-supervised approach
to the problem of paradigm induction
from inflection tables. Our system ex-
tracts generalizations from inflection ta-
bles, representing the resulting paradigms
in an abstract form. The process is in-
tended to be language-independent, and
to provide human-readable generalizations
of paradigms. The tools we provide can
be used by linguists for the rapid cre-
ation of lexical resources. We evaluate the
system through an inflection table recon-
struction task using Wiktionary data for
German, Spanish, and Finnish. With no
additional corpus information available,
the evaluation yields per word form ac-
curacy scores on inflecting unseen base
forms in different languages ranging from
87.81% (German nouns) to 99.52% (Span-
ish verbs); with additional unlabeled text
corpora available for training the scores
range from 91.81% (German nouns) to
99.58% (Spanish verbs). We separately
evaluate the system in a simulated task of
Swedish lexicon creation, and show that
on the basis of a small number of inflection
tables, the system can accurately collect
from a list of noun forms a lexicon with in-
flection information ranging from 100.0%
correct (collect 100 words), to 96.4% cor-
rect (collect 1000 words).
1 Introduction
Large scale morphologically accurate lexicon con-
struction for natural language is a very time-
consuming task, if done manually. Usually, the
construction of large-scale lexical resources pre-
supposes a linguist who constructs a detailed mor-
phological grammar that models inflection, com-
pounding, and other morphological and phonolog-
ical phenomena, and additionally performs a man-
ual classification of lemmas in the language ac-
cording to their paradigmatic behavior.
In this paper we address the problem of lexicon
construction by constructing a semi-supervised
system that accepts concrete inflection tables as in-
put, generalizes inflection paradigms from the ta-
bles provided, and subsequently allows the use of
unannotated corpora to expand the inflection ta-
bles and the automatically generated paradigms.
1
In contrast to many machine learning ap-
proaches that address the problem of paradigm ex-
traction, the current method is intended to produce
human-readable output of its generalizations. That
is, the paradigms provided by the system can be
inspected for errors by a linguist, and if neces-
sary, corrected and improved. Decisions made by
the extraction algorithms are intended to be trans-
parent, permitting morphological system develop-
ment in tandem with linguist-provided knowledge.
Some of the practical tasks tackled by the sys-
tem include the following:
? Given a small number of known inflection ta-
bles, extract from a corpus a lexicon of those
lemmas that behave like the examples pro-
vided by the linguist.
? Given a large number of inflection tables?
such as those provided by the crowdsourced
lexical resource, Wiktionary?generalize the
tables into a smaller number of abstract
paradigms.
2 Previous work
Automatic learning of morphology has long been a
prominent research goal in computational linguis-
tics. Recent studies have focused on unsupervised
methods in particular?learning morphology from
1
Our programs and the datasets used, including the
evaluation procedure for this paper, are freely avail-
able at https://svn.spraakbanken.gu.se/clt/
eacl/2014/extract
569
unlabeled data (Goldsmith, 2001; Schone and Ju-
rafsky, 2001; Chan, 2006; Creutz and Lagus,
2007; Monson et al., 2008). Hammarstr?om and
Borin (2011) provides a current overview of unsu-
pervised learning.
Previous work with similar semi-supervised
goals as the ones in this paper include Yarowsky
and Wicentowski (2000), Neuvel and Fulop
(2002), Cl?ement et al. (2004). Recent machine
learning oriented work includes Dreyer and Eis-
ner (2011) and Durrett and DeNero (2013), which
documents a method to learn orthographic trans-
formation rules to capture patterns across inflec-
tion tables. Part of our evaluation uses the same
dataset as Durrett and DeNero (2013). Eskander
et al. (2013) shares many of the goals in this paper,
but is more supervised in that it focuses on learn-
ing inflectional classes from richer annotation.
A major departure from much previous work
is that we do not attempt to encode variation
as string-changing operations, say by string edits
(Dreyer and Eisner, 2011) or transformation rules
(Lind?en, 2008; Durrett and DeNero, 2013) that
perform mappings between forms. Rather, our
goal is to encode all variation within paradigms
by presenting them in a sufficiently generic fash-
ion so as to allow affixation processes, phonolog-
ical alternations as well as orthographic changes
to naturally fall out of the paradigm specification
itself. Also, we perform no explicit alignment of
the various forms in an inflection table, as in e.g.
Tchoukalov et al. (2010). Rather, we base our al-
gorithm on extracting the longest common subse-
quence (LCS) shared by all forms in an inflection
table, from which alignment of segments falls out
naturally. Although our paradigm representation
is similar to and inspired by that of Forsberg et al.
(2006) and D?etrez and Ranta (2012), our method
of generalizing from inflection tables to paradigms
is novel.
3 Paradigm learning
In what follows, we adopt the view that words
and their inflection patterns can be organized
into paradigms (Hockett, 1954; Robins, 1959;
Matthews, 1972; Stump, 2001). We essentially
treat a paradigm as an ordered set of functions
(f
1
, . . . , f
n
), where f
i
:x
1
, . . . , x
n
7? ?
?
, that is,
where each entry in a paradigm is a function from
variables to strings, and each function in a partic-
ular paradigm shares the same variables.
3.1 Paradigm representation
We represent the functions in what we call ab-
stract paradigm. In our representation, an ab-
stract paradigm is an ordered collection of strings,
where each string may additionally contain in-
terspersed variables denoted x
1
, x
2
, . . . , x
n
. The
strings represent fixed, obligatory parts of a
paradigm, while the variables represent mutable
parts. These variables, when instantiated, must
contain at least one segment, but may otherwise
vary from word to word. A complete abstract
paradigm captures some generalization where the
mutable parts represented by variables are instan-
tiated the same way for all forms in one particu-
lar inflection table. For example, the fairly simple
paradigm
x
1
x
1
+s x
1
+ed x
1
+ing
could represent a set of English verb forms, where
x
1
in this case would coincide with the infinitive
form of the verb?walk, climb, look, etc.
For more complex patterns, several variable
parts may be invoked, some of them discontinu-
ous. For example, part of an inflection paradigm
for German verbs of the type schreiben (to write)
verbs may be described as:
x
1
+e+x
2
+x
3
+en INFINITIVE
x
1
+e+x
2
+x
3
+end PRESENT PARTICIPLE
ge+x
1
+x
2
+e+x
3
+en PAST PARTICIPLE
x
1
+e+x
2
+x
3
+e PRESENT 1P SG
x
1
+e+x
2
+x
3
+st PRESENT 2P SG
x
1
+e+x
2
+x
3
+t PRESENT 3P SG
If the variables are instantiated as x
1
=schr,
x
2
=i, and x
3
=b, the paradigm corresponds to
the forms (schreiben, schreibend, geschrieben,
schreibe, schreibst, schreibt). If, on the other
hand, x
1
=l, x
2
=i, and x
3
=h, the same paradigm re-
flects the conjugation of leihen (to lend/borrow)?
(leihen, leihend, geliehen, leihe, leihst, leiht).
It is worth noting that in this representation, no
particular form is privileged in the sense that all
other forms can only be generated from some spe-
cial form, say the infinitive. Rather, in the cur-
rent representation, all forms can be derived from
knowing the variable instantiations. Also, given
only a particular word form and a hypothetical
paradigm to fit it in, the variable instantiations can
often be logically deduced unambiguously. For
example, let us say we have a hypothetical form
steigend and need to fit it in the above paradigm,
without knowing which slot it should occupy. We
570
may deduce that it must represent the present par-
ticiple, and that x
1
=st, x
2
=i, and x
3
=g. From this
knowledge, all other forms can subsequently be
derived.
Although we have provided grammatical in-
formation in the above table for illustrative pur-
poses, our primary concern in the current work is
the generalization from inflection tables?which
for our purposes are simply an ordered set of
word forms?to paradigms of the format dis-
cussed above.
3.2 Paradigm induction from inflection tables
The core component of our method consists of
finding, given an inflection table, the maximally
general paradigm that reflects the information in
that table. To this end, we make the assumption
that string subsequences that are shared by dif-
ferent forms in an inflection table are incidental
and can be generalized over. For example, given
the English verb swim, and a simple inflection ta-
ble swim#swam#swum,
2
we make the assump-
tion that the common sequences sw and m are ir-
relevant to the inflection, and that by disregarding
these strings, we can focus on the segments that
vary within the table?in this case the variation
i?a?u. In other words, we can assume sw and
m to be variables that vary from word to word
and describe the table swim#swam#swum as
x
1
+i+x
2
#x
1
+a+x
2
#x
1
+u+x
2
, where x
1
=sw and
x
2
=m in the specific table.
3.2.1 Maximally general paradigms
In order to generalize as much as possible from an
inflection table, we extract from it what we call the
maximally general paradigm by:
1. Finding the longest common subsequence
(LCS) to all the entries in the inflection table.
2. Finding the segmentation into variables of
the LCS(s) (there may be several) in the in-
flection table that results in
(a) The smallest number of variables. Two
segments xy in the LCS must be part of
the same variable if they always occur
together in every form in the inflection
table, otherwise they must be assigned
separate variables.
2
To save space, we will henceforth use the #-symbol as a
delimiter between entries in an inflection table or paradigm.
ring
rang
rung
[r]i[ng]
[r]a[ng]
[r]u[ng]
rng
?	Extract     LCS ?	Fit LCS      to table ?	Generalize     to paradigmsInput:inflectiontables
swim
swam
swum
swm
[sw]i[m]
[sw]a[m]
[sw]u[m]
x
1
+i+x
2
x
1
+a+x
2
x
1
+u+x
2
x
1
+i+x
2
x
1
+a+x
2
x
1
+u+x
2
?	Collapse     paradigms
x
1
+i+x
2
x
1
+a+x
2
x
1
+u+x
2
}} }}
Figure 1: Illustration of our paradigm generaliza-
tion algorithm. In step ? we extract the LCS sep-
arately for each inflection table, attempt to find
a consistent fit between the LCS and the forms
present in the table (step ?), and assign the seg-
ments that participate in the LCS variables (step
?). Finally, resulting paradigms that turn out to be
identical may be collapsed (step ?) (section 3.3).
(b) The smallest total number of infixed
non-variable segments in the inflection
table (segments that occur between vari-
ables).
3. Replacing the discontinuous sequences that
are part of the LCS with variables (every
form in a paradigm will contain the same
number of variables).
These steps are illustrated in figure 1. The
first step, extracting the LCS from a collection of
strings, is the well-known multiple longest com-
mon subsequence problem (MLCS). It is known
to be NP-hard (Maier, 1978). Although the num-
ber of strings to find the LCS from may be rather
large in real-world data, we find that a few sensible
heuristic techniques allow us to solve this problem
efficiently for practical linguistic material, i.e., in-
flection tables. We calculate the LCS by calculat-
ing intersections of finite-state machines that en-
code all subsequences of all words, using the foma
finite-state toolkit (Hulden, 2009).
3
While for most tables there is only one way
to segment the LCS in the various forms, some
ambiguous corner cases need to be resolved by
imposing additional criteria for the segmentation,
given in steps 2(a) and 2(b). As an example,
consider a snippet of a small conjugation table
for the Spanish verb comprar (to buy), com-
prar#compra#compro. Obviously the LCS is
compr?however, this can be distributed in two
different ways across the strings, as seen below.
3
Steps 2 and 3 are implemented using more involved
finite-state techniques that we plan to describe elsewhere.
571
comprar
compra
compro
{ x1
comprar
compra
compro
{
{
x1 x2(a) (b){ {x1 x2x1 {
The obvious difference here is that in the first
assignment, we only need to declare one vari-
able x
1
=compr, while in the second, we need
two, x
1
=comp, x
2
=r. Such cases are resolved by
choosing the segmentation with the smallest num-
ber of variables by step 2(a).
Remaining ambiguities are resolved by mini-
mizing the total number of infixed segments. As
an illustration of where this is necessary, consider
a small extract from the Swedish noun table segel
(sail): segel#seglen#seglet. Here, the LCS, of
which there are two of equal length (sege/segl)
must be assigned to two variables where either
x
1
=seg and x
2
=e, or x
1
=seg and x
2
=l:
segel
seglen
seglet
{ {x1 x2
segel
seglen
seglet
{ {x1 x2(a) (b)
However, in case (a), the number of infixed
segments?the l?s in the second and third form?
total one more than in the distribution in (b), where
only one e needs to be infixed in one form. Hence,
the representation in (b) is chosen in step 2(b).
The need for this type of disambiguation strat-
egy surfaces very rarely and the choice to mini-
mize infix length is largely arbitrary?although it
may be argued that some linguistic plausibility is
encoded in the minimization of infixes. However,
choosing a consistent strategy is important for the
subsequent collapsing of paradigms.
3.3 Collapsing paradigms
If several tables are given as input, and we extract
the maximally general paradigm from each, we
may collapse resulting paradigms that are identi-
cal. This is also illustrated in figure 1.
As paradigms are collapsed, we record the in-
formation about how the various variables were
interpreted prior to collapsing. That is, for the
example in figure 1, we not only store the result-
ing single paradigm, but also the information that
x
1
=r, x
2
=ng in one table and that x
1
=sw, x
2
=m
in another. This allows us to potentially recon-
struct all the inflection tables seen during learn-
Form Input Generalization
[Inf] kaufen x
1
+en
[PresPart] kaufend x
1
+end
[PastPart] gekauft ge+x
1
+t
[Pres1pSg] kaufe x
1
+e
[Pres1pPl] kaufen x
1
+en
[Pres2pSg] kaufst x
1
+st
[Pres2pPl] kauft x
1
+t
[Pres3pSg] kauft x
1
+t
[Pres3pPl] kaufen x
1
+en
. . . . . . . . .
x
1
= kauf
Table 1: Generalization from a German example
verb kaufen (to buy) exemplifying typical render-
ing of paradigms.
ing. Storing this information is also crucial for
paradigm table collection from text, fitting unseen
word forms into paradigms, and reasoning about
unseen paradigms, as will be discussed below.
3.4 MLCS as a language-independent
generalization strategy
There is very little language-specific information
encoded in the strategy of paradigm generaliza-
tion that focuses on the LCS in an inflection
table. That is, we do not explicitly prioritize
processes like prefixation, suffixation, or left-to-
right writing systems. The resulting algorithm
thus generalizes tables that reflect concatenative
and non-concatenative morphological processes
equally well. Tables 1 and 2 show the outputs of
the method for German and Arabic verb conjuga-
tion reflecting the generalization of concatenative
and non-concatenative patterns.
3.5 Instantiating paradigms
As mentioned above, given that the variable in-
stantiations of a paradigm are known, we may gen-
erate the full inflection table. The variable instan-
tiations are retrieved by matching a word form to
one of the patterns in the paradigms. For example,
the German word form b?ucken (to bend down)
may be matched to three patterns in the paradigm
exemplified in table 1, and all three matches yield
the same variable instantiation, i.e., x
1
=b?uck.
Paradigms with more than one variable may
be sensitive to the matching strategy of the vari-
ables. To see this, consider the pattern x
1
+a+x
2
and the word banana. Here, two matches are pos-
sible x
1
=b and x
2
=nana and x
1
=ban and x
2
=na.
In other words, there are three possible matching
572
Form Input Generalization
[Past1SG] katabtu (


I


.


J

?) x
1
+a+x
2
+a+x
3
+tu
[Past2SGM] katabta (


I


.


J

?) x
1
+a+x
2
+a+x
3
+ta
[Past2SGF] katabti (

I



.


J

?) x
1
+a+x
2
+a+x
3
+ti
[Past3SGM] kataba (

I
.


J

?) x
1
+a+x
2
+a+x
3
+a
[Past3SGF] katabat (


I


.


J

?) x
1
+a+x
2
+a+x
3
+at
. . . . . . . . .
[Pres1SG] aktubu (

I
.


J

?

@) a+x
1
+x
2
+u+x
3
+u
[Pres2SGM] taktubu (

I
.


J

?


K) ta+x
1
+x
2
+u+x
3
+u
[Pres2SGF] taktub??na (

	
?



J
.



J

?


K) ta+x
1
+x
2
+u+x
3
+??na
[Pres3SGM] yaktubu (

I
.


J

?

K


) ya+x
1
+x
2
+u+x
3
+u
[Pres3SGF] taktubu (

I
.


J?


K) ta+x
1
+x
2
+u+x
3
+u
. . . . . . . . .
x
1
= k (?), x
2
= t (

H), x
3
= b (H
.
)
Table 2: Generalization from an Arabic con-
jugation table involving the root /k-t-b/ from
which the stems katab (to write/past) and ktub
(present/non-past) are formed, conjugated in Form
I, past and present tenses. Extracting the longest
common subsequence yields a paradigm where
variables correspond to root radicals.
strategies:
4
1. shortest match (x
1
=b and x
2
=nana)
2. longest match (x
1
=ban and x
2
=na)
3. try all matching combinations
The matching strategy that tends to be success-
ful is somewhat language-dependent: for a lan-
guage with a preference for suffixation, longest
match is typically preferred, while for others
shortest match or trying all combinations may be
the best choice. All languages evaluated in this
article have a preference for suffixation, so in our
experiments we have opted for using the longest
match for the sake of convenience. Our imple-
mentation allows for exploring all matches, how-
ever. Even though all matches were to be tried,
?bad? matches will likely result in implausible in-
flections that can be discarded using other cues.
4 Assigning paradigms automatically
The next problem we consider is assigning the cor-
rect paradigms to candidate words automatically.
4
The number of matches may increase quickly for longer
words and many variables in the worst case: e.g. caravan
matches x
1
+a+x
2
in three different ways.
As a first step, we match the current word to a pat-
tern. In the general case, all patterns are tried for a
given candidate word. However, we usually have
access to additional information about the candi-
date words?e.g., that they are in the base form of
a certain part of speech?which we use to improve
the results by only matching the relevant patterns.
From a candidate word, all possible inflection
tables are generated. Following this, a decision
procedure is applied that calculates a confidence
score to determine which paradigm is the most
probable. The score is a weighted combination of
the following calculations:
1. Compute the longest common suffix for the
generated base form (which may be the input
form) with previously seen base forms. If of
equal length, select the paradigm where the
suffix occurs with higher frequency.
2. Compute frequency spread over the set of
unique word forms according to the follow-
ing formula:
?
w?set(W )
log(freq(w) + 1)
3. Use the most frequent paradigm as a tie-
breaker.
Step 1 is a simple memory-based approach,
much in the same spirit as van den Bosch and
Daelemans (1999), where we compare the current
base form with what we have seen before.
For step 2, let us elaborate further why the
frequency spread is computed on unique word
forms. We do this to avoid favoring paradigms
that have the same word forms for many or all
inflected forms. For example, the German noun
Ananas (pineapple) has a syncretic inflection with
one repeated word form across all slots, Ananas.
When trying to assign a paradigm to an unknown
word form that matches x
1
, it will surely fit the
paradigm that Ananas has generated perfectly
since we have encountered every word form in that
paradigm, of which there is only one, namely x
1
.
Hence, we want to penalize low variation of word
forms when assigning paradigms.
The confidence score calculated is not only ap-
plicable for selecting the most probable paradigm
for a given word-form; it may also be used to rank
a list of words so that the highest ranked paradigm
is the most likely to be correct. Examples of such
rankings are found in section 5.3.
573
0 50 100 150 200
Paradigms
0.70
0.75
0.80
0.85
0.90
0.95
1.00
Inflection table coverage
FI-NOUNS-ADJS
FI-VERBS
ES-VERBS
DE-NOUNS
DE-VERBS
Figure 2: Degree of coverage with varying num-
bers of paradigms.
5 Evaluation
To evaluate the method, we have conducted three
experiments. First we repeat an experiment pre-
sented in Durrett and DeNero (2013) using the
same data and experiment setup, but with our
generalization method. In this experiment, we
are given a number of complete inflection tables
scraped from Wiktionary. The task is to recon-
struct complete inflection tables from 200 held-out
base forms. For this task, we evaluate per form
accuracy as well as per table accuracy for recon-
struction. The second experiment is the same as
the first, but with additional access to an unlabeled
text dump for the language from Wikipedia.
In the last experiment we try to mimic the situa-
tion of a linguist starting out to describe a new lan-
guage. The experiment uses a large-scale Swedish
morphology as reference and evaluates how reli-
ably a lexicon can be gathered from a word list us-
ing only a few manually specified inflection tables
generalized into abstract paradigms by our system.
5.1 Experiment 1: Wiktionary
In our first experiment we start from the inflec-
tion tables in the development and test set from
Durrett and DeNero (2013), henceforth D&DN13.
Table 3 shows the number of input tables as well
as the number of paradigms that they result in af-
ter generalization and collapsing. For all cases,
the number of output paradigms are below 10%
of the number of input inflection tables. Figure
2 shows the generalization rate achieved with the
paradigms. For instance, the 20 most common re-
sulting German noun paradigms are sufficient to
model almost 95% of the 2,564 separate inflection
tables given as input.
As described earlier, in the reconstruction task,
the input base forms are compared to the abstract
Input: Output:
Data inflection abstract
tables paradigms
DE-VERBS 1827 140
DE-NOUNS 2564 70
ES-VERBS 3855 97
FI-VERBS 7049 282
FI-NOUNS-ADJS 6200 258
Table 3: Generalization of paradigms. The num-
ber of paradigms produced from Wiktionary in-
flection tables by generalization and collapsing of
abstract paradigms.
paradigms by measuring the longest common suf-
fix length for each input base form compared to
the ones seen during training. This approach is
memory-based: it simply measures the similarity
of a given lemma to the lemmas encountered dur-
ing the learning phase. Table 4 presents our results
juxtaposed with the ones reported by D&DN13.
While scoring slightly below D&DN13 for the
majority of the languages when measuring form
accuracy, our method shows an advantage when
measuring the accuracy of complete tables. In-
terestingly, the only case where we improve upon
the form accuracy of D&DN13 is German verbs,
where we get our lowest table accuracy.
Table 4 further shows an oracle score, giv-
ing an upper bound for our method that would
be achieved if we were always able to pick the
best fitting paradigm available. This upper bound
ranges from 99% (Finnish verbs) to 100% (three
out of five tests).
5.2 Experiment 2: Wiktionary and
Wikipedia
In our second experiment, we extend the previous
experiment by adding access to a corpus. Apart
from measuring the longest common suffix length,
we now also compute the frequency of the hy-
pothetical candidate forms in every generated ta-
ble and use this to favor paradigms that generate
a large number of attested forms. For this, we
use a Wikipedia dump, from which we have ex-
tracted word-form frequencies.
5
In total, the num-
ber of word types in the Wikipedia corpus was
8.9M (German), 3.4M (Spanish), 0.7M (Finnish),
and 2.7M (Swedish). Table 5 presents the results,
5
The corpora were downloaded and extracted as de-
scribed at http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
574
Data Per D&DN13 Per D&DN13 Oracle accuracy
table form per form (per table)
DE-VERBS 68.0 85.0 97.04 96.19 99.70 (198/200)
DE-NOUNS 76.5 79.5 87.81 88.94 100.00 (200/200)
ES-VERBS 96.0 95.0 99.52 99.67 100.00 (200/200)
FI-VERBS 92.5 87.5 96.36 96.43 99.00 (195/200)
FI-NOUNS-ADJS 85.0 83.5 91.91 93.41 100.00 (200/200)
Table 4: Experiment 1: Accuracy of reconstructing 200 inflection tables given only base forms from
held-out data when paradigms are learned from the Wiktionary dataset. For comparison, figures from
Durrett and DeNero (2013) are included (shown as D&DN13).
Data Per Per Oracle acc.
table form per form (table)
DE-VERBS 76.50 97.87 99.70 (198/200)
DE-NOUNS 82.00 91.81 100.00 (200/200)
ES-VERBS 98.00 99.58 100.00 (200/200)
FI-VERBS 92.50 96.63 99.00 (195/200)
FI-NOUNS-ADJS 88.00 93.82 100.00 (200/200)
Table 5: Experiment 2: Reconstructing 200 held-
out inflection tables with paradigms induced from
Wiktionary and further access to raw text from
Wikipedia.
where an increased accuracy is noted for all lan-
guages, as is to be expected since we have added
more knowledge to the system. The bold numbers
mark the cases where we outperform the result in
Durrett and DeNero (2013), which is now the case
in four out of five tests for table accuracy, scoring
between 76.50% for German verbs and 98.00% for
Spanish verbs.
Measuring form accuracy, we achieve scores
between 91.81% and 99.58%. The smallest im-
provement is noted for Finnish verbs, which has
the largest number of paradigms, but also the
smallest corpus.
5.3 Experiment 3: Ranking candidates
In this experiment we consider a task where we
only have a small number of inflection tables,
mimicking the situation where a linguist has man-
ually entered a few inflection tables, allowed the
system to generalize these into paradigms, and
now faces the task of culling from a corpus?in
this case labeled with basic POS information?the
candidate words/lemmas that best fit the induced
paradigms. This would be a typical task during
lexicon creation.
We selected the 20 most frequent noun
paradigms (from a total of 346), with one in-
flection table each, from our gold standard, the
Top-1000 rank Correct/Incorrect
TOP 10% 100/0 (100.0%)
TOP 50% 489/11 (97.8%)
TOP 100% 964/36 (96.4%)
Table 6: Top-1000 rank for all nouns in SALDO
Swedish lexical resource SALDO (Borin et al.,
2013). From this set, we discarded paradigms
that lack plural forms.
6
We also removed from
the paradigms special compounding forms that
Swedish nouns have, since compound informa-
tion is not taken into account in this experiment.
The compounding forms are part of the original
paradigm specification, and after a collapsing pro-
cedure after compound-form removal, we were
left with a total of 11 paradigms.
In the next step we ranked all nouns in SALDO
(79.6k lemmas) according to our confidence score,
which indicates how well a noun fits a given
paradigm. We then evaluated the paradigm assign-
ment for the top-1000 lemmas. Among these top-
1000 words, we found 44 that were outside the
20 most frequent noun paradigms. These words
were not necessarily incorrectly assigned, since
they may only differ in their compound forms; as
a heuristic, we considered them correct if they had
the same declension and gender as the paradigm,
and incorrect otherwise.
Table 6 displays the results, including a total ac-
curacy of 96.4%.
Next, we investigated the top-1000 distribution
for individual paradigms. This corresponds to the
situation where a linguist has just entered a new
inflection table and is looking for words that fit the
resulting paradigm. The result is presented in two
6
The paradigms that lack plural forms are subsets of other
paradigms. In other words: when no plural forms are attested,
we would need a procedure to decide if plural forms are even
possible, which is currently beyond the scope of our method.
575
10 20 30 40 50 60 70
Top ranked H%L
2
4
6
8
10
Error rate H%L
p_kikare
p_mening
p_flicka
10 20 30 40 50 60 70
Top ranked H%L
10
20
30
40
50
60
70
Error rate H%L
p_dike
p_akademi
p_vinge
p_nyckel
Figure 3: Top-1000: high and low precision paradigms.
error rate plots: figure 3 shows the low precision
and high precision paradigms in two plots, where
error rates range from 0-2% and 16-44% for the
top 100 words.
We further investigated the worst-performing
paradigm, p akademi (academy), to determine
the reason for the high error rate for this particular
item. The main source of error (334 out of 1000) is
confusion with p akribi (accuracy), which has no
plural. However, it is on semantic grounds that the
paradigm has no plural; a native Swedish speaker
would pluralize akribi like akademi (disregard-
ing the fact that akribi is defective). The second
main type of error (210 out of 1000) is confusion
with the unseen paradigm of parti (party), which
inflects similarly to akademi, but with a differ-
ence in gender?difficult to predict from surface
forms?that manifests itself in two out of eight
word forms.
6 Future work
The core method of abstract paradigm represen-
tation presented in this paper can readily be ex-
tended in various directions. One obvious topic of
interest is to investigate the use of machine learn-
ing techniques to expand the method to completely
unsupervised learning by first clustering similar
words in raw text into hypothetical inflection ta-
bles. The plausibility of these tables could then be
evaluated using similar techniques as in our exper-
iment 2.
We also plan to explore ways to improve the
techniques for paradigm selection and ranking. In
our experiments we have, for the sake of trans-
parency, used a fairly simple strategy of suffix
matching to reconstruct tables from base forms.
A more involved classifier may be trained for this
purpose. An obvious extension is to use a clas-
sifier based on n-gram, capitalization, and other
standard features to ascertain that word forms in
hypothetical reconstructed inflection tables main-
tain similar shapes to ones seen during training.
One can also investigate ways to collapse
paradigms further by generalizing over phonolog-
ical alternations and by learning alternation rules
from the induced paradigms (Koskenniemi, 1991;
Theron and Cloete, 1997; Koskenniemi, 2013).
Finally, we are working on a separate interactive
graphical morphological tool in which we plan to
integrate the methods presented in this paper.
7 Conclusion
We have presented a language-independent
method for extracting paradigms from inflection
tables and for representing and generalizing the
resulting paradigms.
7
Central to the process of
paradigm extraction is the notion of maximally
general paradigm, which we define as the in-
flection table, with all of the common string
subsequences forms represented by variables.
The method is quite uncomplicated and outputs
human-readable generalizations. Despite the rel-
ative simplicity, we obtain state-of-the art results
in inflection table reconstruction tasks from base
forms.
Because of the plain paradigm representation
format, we believe the model can be used prof-
itably in creating large-scale lexicons from a few
linguist-provided inflection tables.
7
The research presented here was supported by the
Swedish Research Council (the projects Towards a
knowledge-based culturomics, dnr 2012-5738, and Swedish
Framenet++, dnr 2010-6013), the University of Gothenburg
through its support of the Centre for Language Technology
and its support of Spr?akbanken, and the Academy of Finland
under the grant agreement 258373, Machine learning of
rules in natural language morphology and phonology.
576
References
Lars Borin, Markus Forsberg, and Lennart L?onngren.
2013. SALDO: a touch of yin to WordNet?s yang.
Language Resources and Evaluation, May. Online
first publication; DOI 10.1007/s10579-013-9233-4.
Erwin Chan. 2006. Learning probabilistic paradigms
for morphology in a latent class model. In Proceed-
ings of the Eighth Meeting of the ACL Special Inter-
est Group on Computational Phonology and Mor-
phology, pages 69?78. Association for Computa-
tional Linguistics.
Lionel Cl?ement, Bernard Lang, Beno??t Sagot, et al.
2004. Morphology based automatic acquisition of
large-coverage lexica. In LREC 04, pages 1841?
1844.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing (TSLP), 4(1):3.
Gr?egoire D?etrez and Aarne Ranta. 2012. Smart
paradigms and the predictability and complexity of
inflectional morphology. In Proceedings of the 13th
EACL, pages 645?653. Association for Computa-
tional Linguistics.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using
a Dirichlet process mixture model. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 616?627. Association
for Computational Linguistics.
Greg Durrett and John DeNero. 2013. Supervised
learning of complete morphological paradigms. In
Proceedings of NAACL-HLT, pages 1185?1195.
Ramy Eskander, Nizar Habash, and Owen Rambow.
2013. Automatic extraction of morphological lex-
icons from morphologically annotated corpora. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1032?1043. Association for Computational Linguis-
tics.
Markus Forsberg, Harald Hammarstr?om, and Aarne
Ranta. 2006. Morphological lexicon extraction
from raw text data. In Advances in Natural Lan-
guage Processing, pages 488?499. Springer.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
linguistics, 27(2):153?198.
Harald Hammarstr?om and Lars Borin. 2011. Unsuper-
vised learning of morphology. Computational Lin-
guistics, 37(2):309?350.
Charles F Hockett. 1954. Two models of grammati-
cal description. Morphology: Critical Concepts in
Linguistics, 1:110?138.
Mans Hulden. 2009. Foma: a finite-state compiler and
library. In Proceedings of the 12th Conference of the
European Chapter of the European Chapter of the
Association for Computational Linguistics: Demon-
strations Session, pages 29?32, Athens, Greece. As-
sociation for Computational Linguistics.
Kimmo Koskenniemi. 1991. A discovery procedure
for two-level phonology. Computational Lexicol-
ogy and Lexicography: A Special Issue Dedicated
to Bernard Quemada, 1:451?46.
Kimmo Koskenniemi. 2013. An informal discovery
procedure for two-level rules. Journal of Language
Modelling, 1(1):155?188.
Krister Lind?en. 2008. A probabilistic model for guess-
ing base forms of new words by analogy. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 106?116. Springer.
David Maier. 1978. The complexity of some problems
on subsequences and supersequences. Journal of the
ACM (JACM), 25(2):322?336.
Peter H. Matthews. 1972. Inflectional morphology:
A theoretical study based on aspects of Latin verb
conjugation. Cambridge University Press.
Christian Monson, Jaime Carbonell, Alon Lavie, and
Lori Levin. 2008. Paramor: finding paradigms
across morphology. In Advances in Multilingual
and Multimodal Information Retrieval, pages 900?
907. Springer.
Sylvain Neuvel and Sean A Fulop. 2002. Unsuper-
vised learning of morphology without morphemes.
In Proceedings of the ACL-02 workshop on Morpho-
logical and phonological learning-Volume 6, pages
31?40. Association for Computational Linguistics.
Robert H Robins. 1959. In defence of WP. Transac-
tions of the Philological Society, 58(1):116?144.
Patrick Schone and Daniel Jurafsky. 2001.
Knowledge-free induction of inflectional mor-
phologies. In Proceedings of the second meeting
of the North American Chapter of the Association
for Computational Linguistics on Language tech-
nologies, pages 1?9. Association for Computational
Linguistics.
Gregory T. Stump. 2001. A theory of paradigm struc-
ture. Cambridge University Press.
Tzvetan Tchoukalov, Christian Monson, and Brian
Roark. 2010. Morphological analysis by mul-
tiple sequence alignment. In Multilingual Infor-
mation Access Evaluation I. Text Retrieval Experi-
ments, pages 666?673. Springer.
Pieter Theron and Ian Cloete. 1997. Automatic acqui-
sition of two-level morphological rules. In Proceed-
ings of the fifth conference on Applied natural lan-
guage processing, pages 103?110. Association for
Computational Linguistics.
577
Antal van den Bosch and Walter Daelemans. 1999.
Memory-based morphological analysis. In Proceed-
ings of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 285?292. As-
sociation for Computational Linguistics.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of the 38th
Annual Meeting on Association for Computational
Linguistics, pages 207?216. Association for Com-
putational Linguistics.
578
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 39?48,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning word-level dialectal variation as phonological replacement rules
using a limited parallel corpus
Mans Hulden
University of Helsinki
Language Technology
mans.hulden@helsinki.fi
In?aki Alegria
IXA taldea
UPV-EHU
i.alegria@ehu.es
Izaskun Etxeberria
IXA taldea
UPV-EHU
izaskun.etxeberria@ehu.es
Montse Maritxalar
IXA taldea
UPV-EHU
montse.maritxalar@ehu.es
Abstract
This paper explores two different methods of
learning dialectal morphology from a small
parallel corpus of standard and dialect-form
text, given that a computational description
of the standard morphology is available. The
goal is to produce a model that translates in-
dividual lexical dialectal items to their stan-
dard dialect counterparts in order to facili-
tate dialectal use of available NLP tools that
only assume standard-form input. The results
show that a learning method based on induc-
tive logic programming quickly converges to
the correct model with respect to many phono-
logical and morphological differences that are
regular in nature.
1 Introduction
In our work with the Basque language, a morpho-
logical description and analyzer is available for the
standard language, along with other tools for pro-
cessing the language (Alegria et al, 2002). How-
ever, it would be convenient to be able to analyze
variants and dialectal forms as well. As the dialectal
differences within the Basque language are largely
lexical and morphophonological, analyzing the di-
alectal forms would in effect require a separate mor-
phological analyzer that is able to handle the unique
lexical items in the dialect together with the differ-
ing affixes and phonological changes.
Morphological analyzers are traditionally hand-
written by linguists, most commonly using some
variant of the popular finite-state morphology ap-
proach (Beesley and Karttunen, 2002). This entails
having an expert model a lexicon, inflectional and
derivational paradigms as well as phonological al-
ternations, and then producing a morphological an-
alyzer/generator in the form of a finite-state trans-
ducer.
As the development of such wide-coverage mor-
phological analyzers is labor-intesive, the hope is
that an analyzer for a variant could be automatically
learned from a limited parallel standard/dialect cor-
pus, given that an analyzer already exists for the
standard language. This is an interesting problem
because a good solution to it could be applied to
many other tasks as well: to enhancing access to
digital libraries (containing diachronic and dialectal
variants), for example, or to improving treatment of
informal registers such as SMS messages and blogs,
etc.
In this paper we evaluate two methods of learning
a model from a standard/variant parallel corpus that
translates a given word of the dialect to its standard-
form equivalent. Both methods are based on finite-
state phonology. The variant we use for experiments
is Lapurdian,1 a dialect of Basque spoken in the La-
purdi (fr. Labourd) region in the Basque Country.
Because Basque is an agglutinative, highly in-
flected language, we believe some of the results can
be extrapolated to many other languages facing sim-
ilar challenges.
One of the motivations for the current work is
that there are a large number of NLP tools avail-
able and in development for standard Basque (also
called Batua): a morphological analyzer, a POS tag-
ger, a dependency analyzer, an MT engine, among
1Sometimes also called Navarro-Labourdin or Labourdin.
39
others (Alegria et al, 2011). However, these tools
do not work well in processing the different dialects
of Basque where lexical items have a different ortho-
graphic representation owing to slight differences in
phonology and morphology.
Here is a brief contrastive example of the kinds
of differences found in the (a) Lapurdian dialect and
standard Basque (b) parallel corpus:2
(a) Ez gero uste izan nexkatxa guziek tu egiten dautatela
(b) Ez gero uste izan neskatxa guztiek tu egiten didatela
As the example illustrates, the differences are mi-
nor overall?the word order and syntax are unaf-
fected, and only a few lexical items differ. This re-
flects the makeup of our parallel corpus quite well?
in it, slightly less than 20% of the word tokens
are distinct. However, even such relatively small
discrepancies cause great problems in the poten-
tial reuse of current tools designed for the standard
forms only.
We have experimented with two approaches that
attempt to improve on a simple baseline of mem-
orizing word-pairs in the dialect and the standard.
The first approach is based on work by Almeida
et al (2010) on contrasting orthography in Brazil-
ian Portuguese and European Portuguese. In this
approach differences between substrings in distinct
word-pairs are memorized and these transformation
patterns are then applied whenever novel words are
encountered in the evaluation. To prevent over-
generation, the output of this learning process is
later subject to a morphological filter where only ac-
tual standard-form outputs are retained. The second
approach is an Inductive Logic Programming-style
(ILP) (Muggleton and De Raedt, 1994) learning
algorithm where phonological transformation rules
are learned from word-pairs. The goal is to find a
minimal set of transformation rules that is both nec-
essary and sufficient to be compatible with the learn-
ing data, i.e. the word pairs seen in the training data.
The remainder of the paper is organized as fol-
lows. The characteristics of the corpus available to
us are described in section 2. In sections 3, 4, and 5,
we describe the steps and variations of the methods
we have applied and how they are evaluated. Sec-
tion 6 presents the experimental results, and finally,
2English translation of the example: Don?t think all girls spit
on me
we discuss the results and present possibilities for
potential future work in section 7.
1.1 Related work
The general problem of supervised learning of di-
alectal variants or morphological paradigms has
been discussed in the literature with various connec-
tion to computational phonology, morphology, ma-
chine learning, and corpus-based work. For exam-
ple, Kestemont et al (2010) presents a language-
independent system that can ?learn? intra-lemma
spelling variation. The system is used to produce
a consistent lemmatization of texts in Middle Dutch
literature in a medieval corpus, Corpus-Gysseling,
which contains manuscripts dated before 1300 AD.
These texts have enormous spelling variation which
makes a computational analysis difficult.
Koskenniemi (1991) provides a sketch of a dis-
covery procedure for phonological two-level rules.
The idea is to start from a limited number of
paradigms (essentially pairs of input-output forms
where the input is the surface form of a word and the
output a lemmatization plus analysis). The problem
of finding phonological rules to model morpholog-
ical paradigms is essentially similar to the problem
presented in this paper. An earlier paper, Johnson
(1984), presents a ?discovery procedure? for learning
phonological rules from data, something that can be
seen as a precursor to the problem dealt with by our
ILP algorithm.
Mann and Yarowsky (2001) present a method
for inducing translation lexicons based on transduc-
tion models of cognate pairs via bridge languages.
Bilingual lexicons within languages families are in-
duced using probabilistic string edit distance mod-
els. Inspired by that paper, Scherrer (2007) uses
a generate-and-filter approach quite similar to our
first method. He compares different measures of
graphemic similarity applied to the task of bilin-
gual lexicon induction between Swiss German and
Standard German. Stochastic transducers are trained
with the EM algorithm and using handmade trans-
duction rules. An improvement of 11% in F-score is
reported over a baseline method using Levenshtein
Distance.
40
Full corpus 80% part. 20% part.
Sentences 2,117 1,694 423
Words 12,150 9,734 2,417
Unique words
Standard Basque 3,553 3,080 1,192
Lapurdian 3,830 3,292 1,239
Filtered pairs 3,610 3,108 1,172
Identical pairs 2,532 2,200 871
Distinct pairs 1,078 908 301
Table 1: Characteristics of the parallel corpus used for
experiments.
2 The corpus
The parallel corpus used in this research is part of
?TSABL? project developed by the IKER group in
Baiona (fr. Bayonne).3 The researchers of the IKER
project have provided us with examples of the La-
purdian dialect and their corresponding forms in
standard Basque. Our parallel corpus then contains
running text in two variants: complete sentences of
the Lapurdian dialect and equivalent sentences in
standard Basque.
The details of the corpus are presented in table 1.
The corpus consists of 2,117 parallel sentences, to-
taling 12,150 words (roughly 3,600 types). In order
to provide data for our learning algorithms and also
to test their performance, we have divided the cor-
pus into two parts: 80% of the corpus is used for the
learning task (1,694 sentences) and the remaining
20% (423 sentences) for evaluation of the learning
process. As is seen, roughly 23% of the word-pairs
are distinct. Another measure of the average devi-
ation between the word pairs in the corpus is given
by aligning all word-pairs by minimum edit distance
(MED): aligning the 3,108 word-pairs in the learn-
ing corpus can be done at a total MED cost of 1,571.
That is, roughly every 14th character in the dialect
data is different from the standard form.
3 The baseline
The baseline of our experiments is a simple method,
based on a dictionary of equivalent words with the
list of correspondences between words extracted
3Towards a Syntactic Atlas of the Basque Language, web
site: http://www.iker.cnrs.fr/-tsabl-towards-a-syntactic-atlas-
of-.html
from the learning portion (80%) of the corpus. This
list of correspondences contains all different word
pairs in the variant vs. standard corpus. The baseline
approach consists simply of memorizing all the dis-
tinct word pairs seen between the dialectal and stan-
dard forms, and subsequently applying this knowl-
edge during the evaluation task. That is, if an in-
put word during the evaluation has been seen in the
training data, we provide the corresponding previ-
ously known output word as the answer. Otherwise,
we assume that the output word is identical to the
input word.
4 Overview of methods
We have employed two different methods to produce
an application that attempts to extract generaliza-
tions from the training corpus to ultimately be able
to produce the equivalent standard word correspond-
ing to a given dialectal input word. The first method
is based on already existing work by Almeida et al
(2010) that extracts all substrings from lexical pairs
that are different. From this knowledge we then pro-
duce a number of phonological replacement rules
that model the differences between the input and
output words. In the second method, we likewise
produce a set of phonological replacement rules, us-
ing an ILP approach that directly induces the rules
from the pairs of words in the training corpus.
The core difference between the two methods is
that while both extract replacement patterns from
the word-pairs, the first method does not consider
negative evidence in formulating the replacement
rules. Instead, the existing morphological analyzer
is used as a filter after applying the rules to unknown
text. The second method, however, uses negative
evidence from the word-pairs in delineating the re-
placement rules as is standard in ILP-approaches,
and the subsequent morphological filter for the out-
put plays much less of a role. Evaluating and com-
paring both approaches is motivated because the first
method may produce much higher recall by virtue
of generating a large number of input-output candi-
dates during application, and the question is whether
the corresponding loss in precision may be mitigated
by judicious application of post-processing filters.
41
4.1 Format of rules
Both of the methods we have evaluated involve
learning a set of string-transformation rules to
convert words, morphemes, or individual letters
(graphemes) in the dialectal forms to the stan-
dard variant. The rules that are learned are in
the format of so-called phonological replacement
rules (Beesley and Karttunen, 2002) which we have
later converted into equivalent finite-state transduc-
ers using the freely available foma toolkit (Hulden,
2009a). The reason for the ultimate conversion of
the rule set to finite-state transducers is twofold:
first, the transducers are easy to apply rapidly to
input data using available tools, and secondly, the
transducers can further be modified and combined
with the standard morphology already available to
us as a finite transducer.
In its simplest form, a replacement rule is of the
format
A? B || C D (1)
where the arguments A,B,C,D are all single sym-
bols or strings. Such a rule dictates the transfor-
mation of a string A to B, whenever the A occurs
between the strings C and D. Both C and D are
optional arguments in such a rule, and there may
be multiple conditioning environments for the same
rule.
For example, the rule:
h -> 0 || p , t , l , a s o
(2)
would dictate a deletion of h in a number of con-
texts; when the h is preceded by a p, t, or l, or suc-
ceeded by the sequence aso, for instance transform-
ing ongiethorri (Lapurdian) to ongietorri (Batua).
As we will be learning several rules that each tar-
get different input strings, we have a choice as to the
mode of application of the rules in the evaluation
phase. The learned rules could either be applied in
some specific order (sequentially), or applied simul-
taneously without regard to order (in parallel).
For example, the rules:
u -> i || z a (3)
k -> g || z a u (4)
would together (in parallel) change zaukun into zai-
gun. Note that if we imposed some sort of ordering
on the rules and the u ? i rule in the set would
apply first, for example, the conditioning environ-
ment for the second rule would no longer be met
after transforming the word into zaikun. We have
experimented with sequential as well as parallel pro-
cessing, and the results are discussed below.
4.2 Method 1 (lexdiff) details
The first method is based on the idea of identi-
fying sequences inside word pairs where the out-
put differs from the input. This was done through
the already available tool lexdiff which has been
used in automatic migration of texts between differ-
ent Portuguese orthographies (Almeida et al, 2010).
The lexdiff program tries to identify sequences of
changes from seen word pairs and outputs string cor-
respondences such as, for example: 76 ait ->
at ; 39 dautz -> diz (stemming from pairs
such as (joaiten/joaten and dautzut/dizut), indicating
that ait has changed into at 76 times in the cor-
pus, etc., thus directly providing suggestions as to
phonologically regular changes between two texts,
with frequency information included.
With such information about word pairs we gen-
erate a variety of replacement rules which are then
compiled into finite transducers with the foma ap-
plication. Even though the lexdiff program provides
a direct string-to-string change in a format that is
directly compilable into a phonological rule trans-
ducer, we have experimented with some possible
variations of the specific type of phonological rule
we want to output:
? We can restrict the rules by frequency and re-
quire that a certain type of change be seen at
least n times in order to apply that rule. For
example, if we set this threshold to 3, we will
only apply a string-to-string changing rule that
has been seen three or more times.
? We limit the number of rules that can be
applied to the same word. Sometimes the
lexdiff application divides the change be-
tween a pair of words into two separate rules.
For example the word-word correspondence
agerkuntza/agerpena is expressed by two rules:
rkun -> rpen and ntza -> na. Now,
given these two rules, we have to be able to
apply both to produce the correct total change
42
Figure 1: The role of the standard Basque (Batua) ana-
lyzer in filtering out unwanted output candidates created
by the induced rule set produced by method 1.
agerkuntza/agerpena. By limiting the number
of rules that can apply to a single input word we
can avoid creating many spurious outputs, but
also at the same time we may sacrifice some
ability to produce the desired output forms.
? We can also control the application mode of the
rules: sequential or parallel. If the previous
two rules are applied in parallel, the form ob-
tained from agerkuntza will not be correct
since the n overlaps with the two rules. That
is, when applying rules simultaneously in par-
allel, the input characters for two rules may not
overlap. However, if these two rules applied
in sequence (the order in this example is irrel-
evant), the output will be the correct: we first
change rkun -> rpen and later ntza ->
na. We have not a priori chosen to use parallel
or sequential rules and have decided to evaluate
both approaches.
? We can also compact the rules output by lex-
diff by eliminating redundancies and construct-
ing context-sensitive rules. For example: given
a rule such as rkun -> rpen, we can con-
vert this into a context-sensitive rule that only
changes ku into pe when flanked by r and n
to the left and right, respectively, i.e. producing
a rule:
k u -> p e || r n (5)
This has a bearing on the previous point and
will allow more rewritings within a single word
in parallel replacement mode since there are
fewer characters overlapping.
Once a set of rules is compiled with some instanti-
ation of the various parameters discussed above and
converted to a transducer, we modify the transducer
in various ways to improve on the output.
First, since we already have access to a large-scale
morphological transducer that models the standard
Basque (Batua), we restrict the output from the con-
version transducer to only allow those words as out-
put that are legitimate words in standard Basque.
Figure 1 illustrates this idea. In that figure, we see an
input word in the dialect (emaiten) produce a num-
ber of candidates using the rules induced. However,
after adding a morphological filter that models the
Batua, we retain only one output.
Secondly, in the case that even after applying
the Batua filter we retain multiple outputs, we sim-
ply choose the most frequent word (these unigram
counts are gathered from a separate newspaper cor-
pus of standard Basque).
4.3 Method 2 (ILP) details
The second method we have employed works
directly from a collection of word-pairs (di-
alect/standard in this case). We have developed an
algorithm that from a collection of such pairs seeks
a minimal hypothesis in the form of a set of replace-
ment rules that is consistent with all the changes
found in the training data. This approach is gener-
ally in line with ILP-based machine learning meth-
ods (Muggleton and De Raedt, 1994). However, in
contrast to the standard ILP, we do not learn state-
ments of first-order logic that fit a collection of data,
but rather, string-to-string replacement rules.4
4Phonological string-to-string replacement rules can be de-
fined as collections of statements in first-order logic and com-
piled into transducers through such logical statements as well;
43
The two parameters to be induced are (1) the col-
lection of string replacements X ? Y needed to
characterize the training data, and (2) the minimal
conditioning environments for each rule, such that
the collection of rules model the string transforma-
tions found in the training data.
The procedure employed for the learning task is
as follows:
(1) Align all word pairs (using minimum edit dis-
tance by default).
(2) Extract a collection of phonological rewrite
rules.
(3) For each rule, find counterexamples.
(4) For each rule, find the shortest conditioning en-
vironment such that the rule applies to all pos-
itive examples, and none of the negative exam-
ples. Restrict rule to be triggered only in this
environment.
The following simple example should illustrate
the method. Assuming we have a corpus of only
two word pairs:
emaiten ematen
igorri igorri
in step (1) we would perform the alignment and pro-
duce the output
e m a i t e n i g o r r i
e m a ? t e n i g o r r i
From this data we would in step (2) gather that
the only active phonological rule is i ? ?, since
all other symbols are unchanged in the data. How-
ever, we find two counterexamples to this rule (step
3), namely two i-symbols in igorri which do not al-
ternate with ?. The shortest conditioning environ-
ment that accurately models the data and produces
no overgeneration (does not apply to any of the is in
igorri) is therefore:
i -> ? || a (6)
see e.g. Hulden (2009b) for details. In other words, in this
work, we skip the intermediate step of defining our observa-
tions as logical statements and directly convert our observations
into phonological replacement rules.
the length of the conditioning environment being 1
(1 symbol needs to be seen to the left plus zero sym-
bols to the right). Naturally, in this example we have
two competing alternatives to the shortest general-
ization: we could also have chosen to condition the
i-deletion rule by the t that follows the i. Both con-
ditioning environments are exactly one symbol long.
To resolve such cases, we a priori choose to favor
conditioning environments that extend farther to the
left. This is an arbitrary decision?albeit one that
does have some support from phonology as most
phonological assimilation rules are conditioned by
previously heard segments?and very similar results
are obtained regardless of left/right bias in the learn-
ing. Also, all the rules learned with this method are
applied simultaneously (in parallel) in the evaluation
phase.
4.3.1 String-to-string vs. single-symbol rules
In some cases several consecutive input symbols
fail to correspond to the output in the learning data,
as in for example the pairing
d a u t
d i ? t
corresponding to the dialect-standard pair daut/dit.
Since there is no requirement in our formalism of
rewrite rules that they be restricted to single-symbol
rewrites only, there are two ways to handle this: ei-
ther one can create a string-to-string rewriting rule:
au? i / CONTEXT
or create two separate rules
a? i / CONTEXT , u? ? / CONTEXT
where CONTEXT refers to the minimal condition-
ing environment determined by the rest of the data.
We have evaluated both choices, and there is no no-
table difference between them in the final results.
5 Evaluation
We have measured the quality of different ap-
proaches by the usual parameters of precision, re-
call and the harmonic combination of them, the F1-
score, and analyzed how the different options in the
two approaches affect the results of these three pa-
rameters. Given that we, especially in method 1,
extract quite a large number of rules and that each
44
input word generates a very large number of candi-
dates if we use all the rules extracted, it is possible to
produce a high recall on the conversion of unknown
dialect words to the standard form. However, the
downside is that this naturally leads to low precision
as well, which we try to control by introducing a
number of filters to remove some of the candidates
output by the rules. As mentioned above, we use
two filters: (1) an obligatory filter which removes
all candidate words that are not found in the stan-
dard Basque (by using an existing standard Basque
morphological analyzer), and (2) using an optional
filter which, given several candidates in the standard
Basque, picks the most frequently occurring one by
a unigram count from the separate newspaper cor-
pus. This latter filter turns out to serve a much more
prominent role in improving the results of method 1,
while it is almost completely negligible for method
2.
6 Results
As mentioned above, the learning process has made
use of 80% of the corpus, leaving 20% of the corpus
for evaluation of the above-mentioned approaches.
In the evaluation, we have only tested those words
in the dialect that differ from words in the standard
(which are in the minority). In total, in the evalu-
ation part, we have tested the 301 words that differ
between the dialect and the standard in the evalua-
tion part of the corpus.
The results for the baseline?i.e. simple memo-
rization of word-word correspondences?are (in %):
P = 95.62, R = 43.52 and F1 = 59.82. As ex-
pected, the precision of the baseline is high: when
the method gives an answer it is usually the correct
one. But the recall of the baseline is low, as is ex-
pected: slightly less than half the words in the eval-
uation corpus have been encountered before.5
6.1 Results with the lexdiff method
Table 2 shows the initial experiment of method
1 with different variations on the frequency
5The reason the baseline does not show 100% precision is
that the corpus contains minor inconsistencies or accepted al-
ternative spellings, and our method of measuring the precision
suffers from such examples by providing both learned alterna-
tives to a dialectal word, while only one is counted as being
correct.
P R F1
f ? 1 38.95 66.78 49.20
f ? 2 46.99 57.14 51.57
f ? 3 49.39 53.82 51.51
Table 2: Values obtained for Precision, Recall and F-
scores with method 1 by changing the minimum fre-
quency of the correspondences to construct rules for
foma. The rest of the options are the same in all three
experiments: only one rule is applied within a word.
P R F1
f ? 1 70.28 58.13 63.64
f ? 2 70.18 53.16 60.49
f ? 3 71.76 51.50 59.96
Table 3: Values obtained for Precision, Recall and F-
score with method 1 by changing the threshold frequency
of the correspondences and applying a post-filter.
threshold?this is the limit on the number of times
we must see a string-change to learn it. The re-
sults clearly show that the more examples we extract
(frequency 1), the better results we obtain for recall
while at the same time the precision suffers since
many spurious outputs are given?even many differ-
ent ones that each legitimately correspond to a word
in the standard dialect. The F1-score doesn?t vary
very much and it maintains similar values through-
out. The problem with this approach is one which
we have noted before: the rules produce a large
number of outputs for any given input word and
the consequence is that the precision suffers, even
though only those output words are retained that cor-
respond to actual standard Basque.
With the additional unigram filter in place, the
results improve markedly. The unigram-filtered re-
sults are given in table 3.
We have also varied the maximum number of
possible rule applications within a single word as
well as applying the rules in parallel or sequentially,
and compacting the rules to provide more context-
sensitivity. We shall here limit ourselves to present-
ing the best results of all these options in terms of
the F1-score in table 4.
In general, we may note that applying more than
45
P R F1
Exp1 72.20 57.81 64.21
Exp2 72.13 58.47 64.59
Exp3 75.10 60.13 66.79
Table 4: Method 1. Exp1: frequency 2; 2 rules applied;
in parallel; without contextual conditioning. Exp2: fre-
quency 1; 1 rule applied; with contextual conditioning.
Exp3: frequency 2; 2 rules applied; in parallel; with con-
textual conditioning.
one rule within a word has a negative effect on
the precision while not substantially improving the
recall. Applying the unigram filter?choosing the
most frequent candidate?yields a significant im-
provement: much better precision but also slightly
worse recall. Choosing either parallel or sequential
application of rules (when more than one rule is ap-
plied to a word) does not change the results signifi-
cantly. Finally, compacting the rules and producing
context-sensitive ones is clearly the best option.
In all cases the F1-score improves if the unigram
filter is applied; sometimes significantly and some-
times only slightly. All the results of the table 4
which lists the best performing ones come from ex-
periments where the unigram filter was applied.
Figure 2 shows how precision and recall val-
ues change in some of the experiments done with
method 1. There are two different groups of points
depending on if the unigram filter is applied, illus-
trating the tradeoff in precision and recall.
6.2 Results with the ILP method
The ILP-based results are clearly better overall, and
it appears that the gain in recall by using method
1 does not produce F1-scores above those produced
with the ILP-method, irrespective of the frequency
filters applied. Crucially, the negative evidence
and subsequent narrowness of the replacement rules
learned with the ILP method is responsible for the
higher accuracy. Also, the results from the ILP-
based method rely very little on the post-processing
filters, as will be seen.
The only variable parameter with the ILP method
concerns how many times a word-pair must be seen
to be used as learning evidence for creating a re-
placement rule. As expected, the strongest result
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
Re
ca
ll
Precision
P vs R
without filter
with filter
Figure 2: Tradeoffs of precision and recall values in the
experiments with method 1 using various different pa-
rameters. When the unigram filter is applied the precision
is much better, but the recall drops.
P R F1
n = 1 85.02 (86.13) 58.47 (57.80) 69.29 (69.18)
n = 2 82.33 (83.42) 54.15 (53.49) 65.33 (65.18)
n = 3 80.53 (82.07) 50.83 (50.17) 62.32 (62.26)
n = 4 81.19 (82.32) 50.17 (49.50) 62.01 (61.83)
Table 5: Experiments with the ILP method using a thresh-
old of 1?4 (times a word-pair is seen) to trigger rule learn-
ing. The figures in parentheses are the same results with
the added postprocessing unigram filter that, given sev-
eral output candidates of the standard dialect, chooses the
most frequent one.
is obtained by using all word-pairs, i.e. setting the
threshold to 1. Table 5 shows the degradation of per-
formance resulting from using higher thresholds.
Interestingly, adding the unigram filter that im-
proved results markedly in method 1 to the output of
the ILP method slightly worsens the results in most
cases, and gives no discernible advantage in others.
In other words, in those cases where the method pro-
vides multiple outputs, choosing the most frequent
one on a unigram frequency basis gives no improve-
ment over not doing so.
Additionally, there is comparatively little advan-
tage with this method in adding the morphological
filter to the output of the words in method 2 (this
is the filter that rules out non-standard words). The
results in table 5 include the morphological filter,
but omitting it altogether brings down the best F1
46
P R F1
Baseline 95.62 43.52 59.82
Method 1 (lexdiff) 75.10 60.13 66.79
Method 2 (ILP) 85.02 58.47 69.29
Table 6: The best results (per F1-score of the two meth-
ods). The parameters of method 1 included using only
those string transformations that occur at least 2 times in
the training data, and limiting rule application to a maxi-
mum of 2 times within a word, and including a unigram
post-filter. Rules were contextually conditioned. For
method 2, all the examples (threshold 1) in the training
data were used as positive and negative evidence, with-
out a unigram filter.
to 56.14 from 69.29. By contrast, method 1 de-
pends heavily on it and omitting the filter brings
down the F1-score from 66.79 to 11.53 with the
otherwise strongest result of method 1 seen in ta-
ble 6. The most prominent difference between the
two approaches is that while method 1 can be fine-
tuned using frequency information and various fil-
ters to yield results close to method 2, the ILP ap-
proach provides equally robust results without any
additional information?in particular, frequency in-
formation of the target language. We also find a
much lower rate of errors of commission with the
ILP method; this is somewhat obvious as it takes ad-
vantage of negative evidence directly while the first
method only does so indirectly through filters added
later.
7 Conclusions and future work
We have presented a number of experiments to solve
a very concrete task: given a word in the Lapurdian
dialect of Basque, produce the equivalent standard
Basque word. As background knowledge, we have
a complete standard Basque morphological analyzer
and a small parallel corpus of dialect and standard
text. The approach has been based on the idea of
extracting string-to-string transformation rules from
the parallel corpus, and applying these rules to un-
seen words. We have been able to improve on the
results of a naive baseline using two methods to in-
fer phonological rules of the information extracted
from the corpus and applying them with finite state
transducers. In particular, the second method, in-
ferring minimal phonological rewrite rules using
an Inductive Logic Programming-style approach,
seems promising as regards inferring phonological
and morphological differences that are quite regu-
lar in nature between the two language variants. We
expect that a larger parallel corpus in conjunction
with this method could potentially improve the re-
sults substantially?with a larger set of data, thresh-
olds could be set so that morphophonological gener-
alizations are triggered only after a sufficient num-
ber of training examples (avoiding overgeneration),
and, naturally, many more unique, non-regular, lexi-
cal correspondences could be learned.
During the current work, we have also accumu-
lated a small but valuable training and test corpus
which may serve as a future resource for evaluation
of phonological and morphological rule induction
algorithms.
In order to improve the results, we plan to re-
search the combination of the previous methods with
other ones which infer dialectal paradigms and rela-
tions between lemmas and morphemes for the di-
alect and the standard. These inferred relations
could be contrasted with the information of a larger
corpus of the dialect without using an additional par-
allel corpus.
Acknowledgments
We are grateful for the insightful comments
provided by the anonymous reviewers. This re-
search has been partially funded by the Spanish
Science and Innovation Ministry via the OpenMT2
project (TIN2009-14675-C03-01) and the European
Commission?s 7th Framework Program under grant
agreement no. 238405 (CLARA).
References
Alegria, I., Aranzabe, M., Arregi, X., Artola, X.,
D??az de Ilarraza, A., Mayor, A., and Sarasola, K.
(2011). Valuable language resources and applica-
tions supporting the use of Basque. In Vetulani,
Z., editor, Lecture Notes in Artifitial Intelligence,
volume 6562, pages 327?338. Springer.
Alegria, I., Aranzabe, M., Ezeiza, N., Ezeiza, A.,
and Urizar, R. (2002). Using finite state tech-
nology in natural language processing of basque.
47
In LNCS: Implementation and Application of Au-
tomata, volume 2494, pages 1?12. Springer.
Almeida, J. J., Santos, A., and Simoes, A.
(2010). Bigorna?a toolkit for orthography migra-
tion challenges. In Seventh International Con-
ference on Language Resources and Evaluation
(LREC2010), Valletta, Malta.
Beesley, K. R. and Karttunen, L. (2002). Finite-state
morphology: Xerox tools and techniques. Stud-
ies in Natural Language Processing. Cambridge
University Press.
Hulden, M. (2009a). Foma: a finite-state compiler
and library. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association
for Computational Linguistics: Demonstrations
Session, pages 29?32, Athens, Greece. Associa-
tion for Computational Linguistics.
Hulden, M. (2009b). Regular expressions and pred-
icate logic in finite-state language processing. In
Piskorski, J., Watson, B., and Yli-Jyra?, A., edi-
tors, Finite-State Methods and Natural Language
Processing?Post-proceedings of the 7th Interna-
tional Workshop FSMNLP 2008, volume 191 of
Frontiers in Artificial Intelligence and Applica-
tions, pages 82?97. IOS Press.
Johnson, M. (1984). A discovery procedure for cer-
tain phonological rules. In Proceedings of the
10th international conference on Computational
linguistics, COLING ?84, pages 344?347. Asso-
ciation for Computational Linguistics.
Kestemont, M., Daelemans, W., and Pauw, G. D.
(2010). Weigh your words?memory-based
lemmatization for Middle Dutch. Literary and
Linguistic Computing, 25(3):287?301.
Koskenniemi, K. (1991). A discovery procedure for
two-level phonology. Computational Lexicology
and Lexicography: A Special Issue Dedicated to
Bernard Quemada, pages 451?446.
Mann, G. S. and Yarowsky, D. (2001). Multi-
path translation lexicon induction via bridge lan-
guages. In Proceedings of the second meeting of
the North American Chapter of the Association
for Computational Linguistics on Language tech-
nologies, NAACL ?01, pages 1?8.
Muggleton, S. and De Raedt, L. (1994). Inductive
Logic Programming: theory and methods. The
Journal of Logic Programming, 19:629?679.
Scherrer, Y. (2007). Adaptive string distance mea-
sures for bilingual dialect lexicon induction. In
Proceedings of the 45th Annual Meeting of the
ACL: Student Research Workshop, ACL ?07,
pages 55?60. Association for Computational Lin-
guistics.
48
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 13?17,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
BAD: An assistant tool for making verses in Basque
Manex Agirrezabal, In?aki Alegria, Bertol Arrieta
University of the Basque Country (UPV/EHU)
maguirrezaba008@ikasle.ehu.es, i.alegria@ehu.es, bertol@ehu.es
Mans Hulden
Ikerbasque (Basque Science Foundation)
mhulden@email.arizona.edu
Abstract
We present work on a verse-composition
assistant for composing, checking correct-
ness of, and singing traditional Basque
bertsoak?impromptu verses on particular
themes. A performing bertsolari?a verse
singer in the Basque Country?must ad-
here to strict rules that dictate the format
and content of the verses sung. To help
the aspiring bertsolari, we provide a tool
that includes a web interface that is able
to analyze, correct, provide suggestions and
synonyms, and tentatively also sing (using
text-to-speech synthesis) verses composed
by the user.
1 Introduction
In the Basque Country there exists a long-
standing live performance tradition of improvis-
ing verses?a type of ex tempore composition and
singing called bertsolaritza. Verses in bertsolar-
itza can be seen as discourses with strict rules
governing the technical structure of them: verses
must contain a certain number of lines and each
line must have a defined number of syllables, cer-
tain lines have to rhyme in certain patterns, and so
forth.
In this paper we present a web-based assistant
tool for constructing verses (bertsoak) according
to the rules of bertsolaritza (Garzia et al 2001).
If the reader is interested in this topic, we rec-
ommend watching the 2011 film Bertsolari1 2, di-
rected by Asier Altuna.
1IMDB: http://www.imdb.com/title/tt2058583
2Trailer on: http://vimeo.com/9355066
2 Relationship to earlier work
There exist some prior works dealing with Basque
verse-making and computer technologies, such as
BertsolariXa (Arrieta et al, 2001), which is a
rhyme search tool implemented as finite-state au-
tomata using the two-level morphology formal-
ism. The tool also contains other features, includ-
ing semantic categorization of words, narrowing
word-searches to certain themes, etc. While Bert-
solariXa focuses mostly on the word-level, the
current work also includes constraints on over-
all verse structure in its implementation as well
as a synonym search tool, a melody suggestion
system, and possibilities for plugging in text-to-
speech synthesis of verses.
2.1 The Bertsolari tradition
Bertsolaritza is very ingrained in the Basque
Country and championships, competitions and
get-togethers on bertsolaritza are quite common.
Usually the competitors in such event, called bert-
solaris, are given a theme to produce a verse on
under some very limited time constraints.
But the Basque Country is not the only place
that hosts such troubadour traditions?similar
customs are present in many other countries such
as Cuba, Brazil, Argentina, etc. The goal of the
current tool is to be generalizable, and so appli-
cable to various strategies of verse improvisation,
and possibly be useful not only for Basque speak-
ers, but also for others.
Below we briefly present an example of a verse
made in the Basque Country. In 1986 Andoni
Egan?a (a well-known bertsolari) was asked to
sing a bertso and assigned a topic. In the verse,
he was asked to play the role of an old person
who lived alone, and who realized that he could
13
not even tie his shoes. Within a few seconds he
composed and sang three verses. Here, we ana-
lyze the first verse.
Verse:
Gazte aroan ibili arren
gustora tirriki-tarra,
denbora honen joan etorriak
ederki jo dit gitarra,
gorputza daukat ximeldurikan
ta eskuen punta zaharra,
denborarekin seko galdu det
gazte aroko indarra,
ez al da pena gizon mardul bat
hola ibili beharra.
Translation:
Even when I was young
I was always on a spree
over time
I have been punished
I have a crumpled body
and the tip of the hands very old,
Over time I lost
the strength I had when I was young,
It?s a shame that a strong man
has to end up like me.
The special charm of bertsolaritza improvi-
sation is that people proficient in the art can
quickly express a variety of ideas, although they
are working with very restrictive rules concern-
ing the number of syllables in words they use,
and how the words must rhyme. We must take
into account that Andoni Egan?a was able to sing
this verse within a few seconds of being given
the topic, and also, that it complies exactly with
a certain metric. In this case, the verse contains
eight lines, each odd line consisting of ten sylla-
bles, and each even line of eight syllables, with
the even lines rhyming.
Formal training in the bertsolari tradition also
exists in the Basque Country. In the last 20 to
30 years, an important movement has developed
that aims to provide instruction to upcoming gen-
erations on how to create verses (orally or in
writing). This kind of instruction usually takes
place in learning centers called bertso-eskolak,
which in English roughly means, ?verse-making
schools.? The proliferation of this movement has
produced a strong base of young bertsolaris, of
whom many achieve an outstanding level of im-
provisation skills.
3 The BAD tool
BAD is the acronym for ?Bertsotarako Arbel Dig-
itala?, roughly ?Digital verse board.? The aim
of the tool is to serve as a general assistant for
bertsolari-style verse composition and help verse-
making learners in their learning process.
This tool has been developed using the PHP
programming language, but it contains certain
parts developed using finite-state technology. The
main functions of this tool, which will be dis-
cussed in more detail in the next five sections, are
the following: visualization of the verse structure,
structure checking, rhyme and synonym searching
and verse singing.
3.1 Verse structure
The main rules of the bertsolari verse are that a
verse must consist of a certain predefined number
of lines and each line in turn, of a predefined num-
ber of syllables. Traditionally, about a hundred
different schemes are used, and the tool provides
support for all these patterns. For example, the
structure called ?Hamarreko handia? has ten lines
and ten syllables in the odd-numbered lines, and
eight syllables in the even-numbered lines. In this
structure, the even-numbered lines have to rhyme.
Selecting this scheme, the tool will mark the cor-
responding lines with their requirements.
The web interface can be seen in figure 1,
which shows the general layout of the tool, illus-
trated with the example verse referred to above?
we see that each line has been approved in terms
of line length and syllable structure by the tool.
We have designed a database in which the main
verse structures are saved so that when the user se-
lects one verse schema, the system knows exactly
the number of lines it must contain, where must
it rhyme and how many syllables each line should
have. Those schemata are also linked to melodies,
each melody corresponding to one possible struc-
ture.
3.2 Structure checking
After writing the verse, the system can evaluate if
it is technically correct, i.e. if the overall structure
is correct and if each line in the form abides by the
required syllable count and rhyming scheme. The
syllable counter is implemented using the foma
software (Hulden, 2009), and the implementation
(Hulden, 2006) can be found on the homepage of
14
Figure 1: A verse written in the BAD web application.
foma.3
Separately, we have also developed a rhyme
checker, which extracts special patterns in the
lines that must rhyme and checks their confor-
mity.
These patterns are extracted using foma (see
section 3.4) after which some phonological rules
are applied. For example, an example rule era
? {era, eda, ega, eba}, models the fact that any
word ending in era, for example, etxera, will
rhyme with all words that end in era, eda, eba or
ega. These rhyming patterns have been extracted
according to the phonological laws described in
(Amuriza, 1981).
3.3 Synonym search
Usually, people who write verses tend to quickly
exhaust their vocabulary and ideas with to ex-
press what they want to say, or encounter prob-
lems with the number of syllables in various ten-
tative words they have in mind. For example,
if the verse-maker wants to say something con-
taining the word ?family,? (familia in Euskera, a
four-syllable word) but is forced to use a three-
syllable word in a particular context, the inter-
face provides for possibilities to look for three-
syllable synonyms of the word familia, producing
the word sendia? a word whose meaning is oth-
erwise the same, and made up of three syllables.
For developing the synonym search, we used a
modified version of the Basque Wordnet (Pociello
3http://foma.googlecode.com
et al, 2010), originally developed by the IXA
group at the University of the Basque Country.
Within Wordnet we search the synsets for the in-
coming word, and the words that correspond to
those synsets are returned.
3.4 Rhyme search
The classical and most well-known problem in
bertsolaritza concern the rhyming patterns. As
mentioned, various lines within a verse are re-
quired to rhyme, according to certain predefined
schemata. To search for words that rhyme with
other words in a verse, the BAD tool contains a
rhyme search engine. In the interface, this is lo-
cated in the right part of the BAD tool main view,
as seen in figure 2.
The rhyme searcher is built upon finite-state
technology, commonly used for developing mor-
phological and phonological analyzers, and calls
upon the freely available foma-tool, to calculate
matching and nonmatching rhyme schemes.
Its grammar is made up of regular expressions
that are used to identify phonological patterns in
final syllables in the input word. The result of
the search is the intersection of these patterns and
all the words generated from a morphological de-
scription of Basque (Alegria et al, 1996)?that
is, a list of all words that match both the required
phonological constraints given (rhyming) and a
morphological description of Basque.
Based upon figure 2, if we search rhymes for
the word landa (cottage), the system proposes a
15
Figure 2: The response of the rhyme search engine.
set of words that can be filtered depending on the
number of syllables required. Among this list of
words, we can find some words that end in anda,
such as, Irlanda (Ireland) or eztanda (explosion),
but through the application of phonological equiv-
alency rules we also find terms like ganga (vault).
3.5 Singing synthesis
Another characteristic, as mentioned, is that, in
the end, the verses are intended to be sung in-
stead of only being textually represented. Based
on other ongoing work in singing synthesis, we
have designed a system for singing the verses en-
tered into the system in Basque.
This is based on the ?singing mode? of the Fes-
tival text-to-speech system (Taylor et al, 1998).
The advantage of using this is that Festival is
open-source and has given us ample opportunities
to modify its behavior. However, as Festival does
not currently support Basque directly, we have re-
lied on the Spanish support of the Festival sys-
tem.4
4While morphologically and syntactically, Spanish and
Basque have no relationship whatsoever, phonetically the
languages are quite close, with only a few phonemes, syl-
Based on current work by the Aholab research
team in Bilbao?a lab that works on Basque
speech synthesis and recognition?we have im-
plemented a singing module for BAD, based on
the text-to-speech HTS engine (Erro et al, 2010).
Our application is able to sing the composed
verses entered into the system in Basque, with a
choice of various standard melodies for bertsolar-
itza.5
4 Discussion and future work
Now that the BAD tool has been developed, our
intention is to evaluate it. To make a qualita-
tive evaluation we have gotten in touch with some
verse-making schools (bertso-eskola), so that they
can test the system and send us their feedback us-
ing a form. Once the evaluation is made, we will
improve it according to the feedback and the sys-
tem will be made public.
Our ultimate goal is to develop a system able to
create verses automatically. To achieve this long-
term goal, there is plenty of work to do and ba-
sic research to be done. We have in our hands a
good corpus of 3,500 Basque verse transcriptions,
so we intend to study these verses from a mor-
phological, syntactical, semantical and pragmatic
point of view.
In the short term, we also plan to expand the
synonym search to be able to provide searches
for semantically related words and subjects (and
not just synonyms), like hypernyms or hyponyms.
The Basque WordNet provides a good opportu-
nity for this, as one is easily able to traverse the
WordNet to encounter words with varying degrees
of semantic similarity.
Another feature that we want to develop is a
system that receives as input a verse together with
a MIDI file, and where the system automatically
sings the verse to the music provided.
Finally, in order for the system to be
able to provide better proposals for the verse
artist?including perhaps humorous and creative
proposals?we intend to work with approaches
to computational creativity. We are considering
different approaches to this topic, such as in the
work on Hahacronym (Stock et al, 2005) or the
Standup riddle builder (Ritchie et al, 2001).
labification rules, and stress rules being different enough to
disturb the system?s behavior.
5However, this functionality is not available on the web
interface as of yet.
16
Figure 3: The BAD application before entering a verse, showing two possible rhyme patterns.
Acknowledgments
This research has been partially funded by the
Basque Government (Research Groups, IT344-
10).
References
In?aki Alegria, Xabier Artola, Kepa Sarasola and
Miriam Urkia, ?Automatic morphological analysis
of Basque?, Literary and Linguistic Computing,
ALLC, 1996
Xabier Amuriza, ?Hiztegi errimatua?, Alfabetatze Eu-
skalduntze Koordinakundea, 1981
Bertol Arrieta, In?aki Alegria, Xabier Arregi, ?An as-
sistant tool for Verse-Making in Basque based on
Two-Level Morphology?, 2001
Daniel Erro, In?aki Sainz, Ibon Saratxaga, Eva Navas,
Inma Herna?ez, ?MFCC+F0 Extraction and Wave-
form Reconstruction using HNM: Preliminary Re-
sults in an HMM-based Synthesizer?, 2010
Joxerra Garzia, Jon Sarasua, and Andoni Egan?a,
?The art of bertsolaritza: improvised Basque verse
singing?, Bertsolari liburuak, 2001
John E. Hopcroft, Rajeev Motwani, Jeffrey D. Ullman,
?Introduccio?n a la teor??a de Auto?matas, Lenguajes
y Computacio?n?, Pearson educacio?n, 2002
Mans Hulden, ?Foma: a finite-state compiler and li-
brary?, Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Demonstrations Session, p. 29?
32, 2009
Mans Hulden, ?Finite-state syllabification?, Finite-
State Methods and Natural Language Processing, p.
86 ? 96, Springer, 2006
Kimmo Koskenniemi. ?Two-level morphology: A
general computational model for word-form recog-
nition and production?. Publication 11, University
of Helsinki, Department of General Linguistics,
Helsinki, 1983.
Eli Pociello, Eneko Agirre, Izaskun Aldezabal,
?Methodology and construction of the Basque
WordNet?, 2010
Graeme Ritchie, ?Current directions in computational
humour?, Artificial Intelligence Review, Volume
16, Number 2, p. 119 ? 135, Springer, 2001
Graeme Ritchie, Ruli Manurung, Helen Pain, Annalu
Waller, Dave O?Mara, ?The STANDUP interactive
riddle builder?, Volume 2, Number 2, p. 67 ? 69,
IEEE Intelligent Systems, 2006
Oliviero Stock and Carlo Strapparava, ?Hahacronym:
A computational humor system?, Proceedings of
the ACL 2005 on Interactive poster and demonstra-
tion sessions, p. 113 ? 116, Association for Compu-
tational Linguistics, 2005
Paul Taylor, Alan W. Black and Richard Caley, ?The
architecture of the Festival speech synthesis sys-
tem?, International Speech Communication Asso-
ciation, 1998
17
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 10?19,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Practical Finite State Optimality Theory
Dale Gerdemann
University of Tu?bingen
dg@sfs.nphil.uni-tuebingen.de
Mans Hulden
University of the Basque Country
IXA Group
IKERBASQUE, Basque Foundation for Science
mhulden@email.arizona.edu
Abstract
Previous work for encoding Optimality The-
ory grammars as finite-state transducers has
included two prominent approaches: the so-
called ?counting? method where constraint vi-
olations are counted and filtered out to some
set limit of approximability in a finite-state
system, and the ?matching? method, where
constraint violations in alternative strings are
matched through violation alignment in order
to remove suboptimal candidates. In this pa-
per we extend the matching approach to show
how not only markedness constraints, but also
faithfulness constraints and the interaction of
the two types of constraints can be captured
by the matching method. This often produces
exact and small FST representations for OT
grammars which we illustrate with two practi-
cal example grammars. We also provide a new
proof of nonregularity of simple OT gram-
mars.
1 Introduction
The possibility of representing Optimality Theory
(OT) grammars (Prince and Smolensky, 1993) as
computational models and finite-state transducers,
in particular, has been widely studied since the in-
ception of the theory itself. In particular, construct-
ing an OT grammar step-by-step as the composition
of a set of transducers, akin to rewrite rule com-
position in (Kaplan and Kay, 1994), has offered
the attractive possibility of simultaneously model-
ing OT parsing and generation as a natural conse-
quence of the bidirectionality of finite-state trans-
ducers. Two main approaches have received atten-
tion as practical options for implementing OT with
finite-state transducers: that of Karttunen (1998)
and Gerdemann and van Noord (2000).1 Both ap-
1Earlier finite-state approaches do exist, see e.g. Ellison
(1994) and Hammond (1997).
proaches model constraint interaction by construct-
ing a GEN-transducer, which is subsequently com-
posed with filtering transducers that mark violations
of constraints, and remove suboptimal candidates?
candidates that have received more violation marks
than the optimal candidate, with the general tem-
plate:
Grammar = Gen .o. MarkC1 .o. FilterC1 ...
MarkCN .o. FilterCN
In Karttunen?s system, auxiliary ?counting? trans-
ducers are created that first remove candidates with
maximally k violation marks for some fixed k, then
k?1, and so on, until nothing can be removed with-
out emptying the candidate set, using a finite-state
operation called priority union. Gerdemann and van
Noord (2000) present a similar system that they call
a ?matching? approach, but which does not rely on
fixing a maximal number of distinguishable viola-
tions k. The matching method is a procedure by
which we can in many cases (though not always)
distinguish between infinitely many violations in a
finite-state system?something that is not possible
when encoding OT by the alternative approach of
counting violations.
In this paper our primary purpose is to both ex-
tend and simplify this ?matching? method. We
will include interaction of both markedness and
faithfulness constraints (MAX, DEP, and IDENT
violations)?going beyond both Karttunen (1998)
and Gerdemann and van Noord (2000), where only
markedness constraints were modeled. We shall also
clarify the notation and markup used in the matching
approach as well as present a set of generic trans-
ducer templates for EVAL by which modeling vary-
ing OT grammars becomes a simple matter of mod-
ifying the necessary constraint transducers and or-
dering them correctly in a series of compositions.
10
We will first give a detailed explanation of the
?matching? approach in section 2?our encoding,
notation, and tools differ somewhat from that of
Gerdemann and van Noord (2000), although the core
techniques are essentially alike. This is followed by
an illustration of our encoding and method through
a standard OT grammar example in section 3. In
that section we also give examples of debugging OT
grammars using standard finite state calculus meth-
ods. In section 4 we also present an alternate en-
coding of an OT account of prosody in Karttunen
(2006) illustrating devices where GEN is assumed to
add metrical and stress markup in addition to chang-
ing, inserting, or deleting segments. We also com-
pare this grammar to both a non-OT grammar and an
OT grammar of the same phenomenon described in
Karttunen (2006). In section 5, we conclude with a
brief discussion about the limitations of FST-based
OT grammars in light of the method developed in
this paper, as well as show a new proof of nonregu-
larity of some very simple OT constraint systems.
1.1 Notation
All the examples discussed are implemented with
the finite-state toolkit foma (Hulden, 2009b). The
regular expressions are also compilable with the Xe-
rox tools (Beesley and Karttunen, 2003), although
some of the tests of properties of finite-state trans-
ducers, crucial for debugging, are unavailable. The
regular expression formalism used is summarized in
table 1.
2 OT evaluation with matching
In order to clarify the main method used in this pa-
per to model OT systems, we will briefly recapitu-
late the ?matching? approach to filter out suboptimal
candidates, or candidates with more violation marks
in a string representation, developed in Gerdemann
and van Noord (2000).2
2.1 Worsening
The fundamental technique behind the finite-state
matching approach to OT is a device which we call
?worsening?, used to filter out strings from a trans-
ducer containing more occurrences of some desig-
nated special symbol s (e.g. a violation marker),
2Also discussed in Ja?ger (2002).
AB Concatenation
A|B Union
?A Complement
? Any symbol in alphabet
% Escape symbol
[ and ] Grouping brackets
A:B Cross product
T.l Output projection of T
A -> B Rewrite A as B
A (->) B Optionally rewrite A as B
|| C D Context specifier
[..] -> A Insert one instance of A
A -> B ... C Insert B and C around A
.#. End or beginning of string
Table 1: Regular expression notation in foma.
than some other candidate string in the same pool
of strings. This method of transducer manipulation
is perhaps best illustrated through a self-contained
example.
Consider a simple morphological analyzer en-
coded as an FST, say of English, that only
adds morpheme-boundaries?+-symbols?to input
words, perhaps consulting a dictionary of affixes and
stems. Some of the mappings of such a transducer
could be ambiguous: for example, the words decon-
struction or incorporate could be broken down in
two ways by such a morpheme analyzer:
Suppose our task was now to remove alternate
morpheme breakdowns from the transducer so that,
if an analysis with a smaller number of morphemes
was available for any word, a longer analysis would
not be produced. In effect, deconstruction should
only map to deconstruct+ion, since the other al-
ternative has one more morpheme boundary. The
worsening trick is based on the idea that we can
use the existing set of words from the output side
of the morphology, add at least one morpheme
boundary to all of them, and use the resulting set
of words to filter out longer ?candidates? from the
original morphology. For example, one way of
adding a +-symbol to de+construction produces
11
de+construct+ion, which coincides with the orig-
inal output in the morphology, and can now be used
to knock out this suboptimal division. This process
can be captured through:
AddBoundary = [?* 0:%+ ?*]+;
Worsen = Morphology .o. AddBoundary;
Shortest = Morphology .o. ?Worsen.l;
the effect of which is illustrated for the word de-
construction in figure 1. Here, AddBoundary is
a transducer that adds at least one +-symbol to the
input. The Worsen transducer is simply the origi-
nal transducer composed with the AddBoundary
transducer. The Shortest morphology is then
constructed by extracting the output projection of
Worsen, and composing its negation with the orig-
inal morphology.
Figure 1: Illustration of a worsening filter for morpheme
boundaries.
2.2 Worsening in OT
The above ?worsening? maneuver is what the
?matching? approach to model OT syllabification is
based upon. Evaluation of competing candidates
with regard to a single OT constraint can be per-
formed in the same manner. This, of course, pre-
supposes that we are using transducers to mark con-
straint violations in input strings, say by the sym-
bol *. Gerdemann and van Noord (2000) illustrate
this by constructing a GEN-transducer that syllabi-
fies words,3 and another set of transducers that mark
3Although using a much more complex set of markup sym-
bols than here.
violations of some constraint. Then, having a con-
straint, NOCODA, implemented as a transducer that
adds violation marks when syllables end in conso-
nants, we can achieve the following sequence of
markup by composition of GEN and NOCODA, for
a particular example input bebop:
The above transducers could be implemented very
simply, by epenthesis replacement rules:
# Insert periods arbitrarily inside words
Gen = [..] (->) %. || \.#. _ \.#. ;
# Insert *-marks after C . or C .#.
NoCoda = [..] -> %* || C+ [%. | .#.] _ ;
Naturally, at this point in the composition
chain we would like to filter out the suboptimal
candidates?that is, the ones with fewer violation
marks, then remove the marks, and continue with
the next constraint, until all constraints have been
evaluated. The problem of filtering out the subopti-
mal candidates is now analogous to the ?worsening?
scenario above: we can create a ?worsening?-filter
automaton by adding violation marks to the entire
set of candidates. In this example, the candidate
be.bop? would produce a worse candidate be?.bop?,
which (disregarding for the moment syllable bound-
ary marks and the exact position of the violation) can
be used to filter out the suboptimal beb?.op?.
3 An OT grammar with faithfulness and
markedness constraints
As previous work has been limited to working with
only markedness constraints as well as a some-
what impoverished GEN?one that only syllabifies
words?our first task when approaching a more
complete finite-state methodology of OT needs to
address this point. In keeping with the ?richness
of the base?-concept of OT, we require a suitable
12
GEN to be able to perform arbitrary deletions (eli-
sions), insertions (epentheses), and changes to the
input. A GEN-FST that only performs this task
(maps ?? ? ??) on input strings is obviously fairly
easy to construct. However, we need to do more than
this: we also need to keep track of which parts of
the input have been modified by GEN in any way
to later be able to pinpoint and mark faithfulness
violations?places where GEN has manipulated the
input?through an FST.
3.1 Encoding of GEN
Perhaps the simplest possible encoding that meets
the above criteria is to have GEN not only change
the input, but also mark each segment in its output
with a marker whereby we can later distinguish how
the input was changed. To do so, we perform the
following markup:
? Every surface segment (output) is surrounded
by brackets [ . . . ].
? Every input segment that was manipulated by
GEN is surrounded by parentheses ( . . . ).
For example, given the input a, GEN would pro-
duce an infinite number of outputs, and among them:
[a] GEN did nothing
(a)[] GEN deleted the a
(a)[e] GEN changed the a to e
()[d](a)[i] GEN inserted a d and changed a to i
...
This type of generic GEN can be defined through:
Gen = S -> %( ... %) %[ (S) %] ,,
S -> %[ ... %] ,,
[..] (->) [%( %) %[ S %]]* ;
assuming here that S represents the set of segments
available.
3.2 Evaluation of faithfulness and markedness
constraints
As an illustrative grammar, let us consider a standard
OT example of word-final obstruent devoicing?as
in Dutch or German?achieved through the interac-
tion of faithfulness and markedness constraints. The
constraints model the fact that underlyingly voiced
obstruents surface as devoiced in word-final posi-
tion, as in pad ? pat. A set of core constraints to
illustrate this include:
? ?VF: a markedness constraint that disallows fi-
nal voiced obstruents.
? IDENTV: a faithfulness constraint that militates
against change in voicing.
? VOP: a markedness constraint against voiced
obstruents in general.
The interaction of these constraints to achieve de-
voicing can be illustrated by the following tableau.4
bed ?VF IDENTV VOP
+ bet * *
pet **!
bed *! **
ped *! * *
The tableau above represents a kind of shorthand
often given in the linguistic literature where, for the
sake of conciseness, higher-ranked faithfulness con-
straints are omitted. For example, there is nothing
preventing the candidate bede to rank equally with
bet, were it not for an implicit high-ranked DEP-
constraint disallowing epenthesis. As we are build-
ing a complete computational model with an unre-
stricted GEN, and no implicit assumptions, we need
to add a few constraints not normally given when
arguing about OT models. These include:
? DEP: a faithfulness constraint against epenthe-
sis.
? MAX: a faithfulness constraint against dele-
tion.
? IDENTPL: a faithfulness constraint against
changes in place of articulation of segments.
This is crucial to avoid e.g. bat or bap being
equally ranked with bet in the above example.5
4The illustration roughly follows (Kager, 1999), p. 42.
5Note that a generic higher-ranked IDENT will not do, be-
cause then we would never get the desired devoicing in the first
place.
13
Including these constraints explicitly allows us to
rule out unwanted candidates that may otherwise
rank equal with the candidate where word-final ob-
struents are devoiced, as illustrated in the following:
bed DE
P
M
AX
ID
EN
TP
L
? V
F
ID
EN
TV
VO
P
+ bet * *
pet **!
bed *! **
ped *! * *
bat *! * *
bep *! * *
be *! *
bede *! **
Once we have settled for the representation
of GEN, the basic faithfulness constraint markup
transducers?whose job is to insert asterisks wher-
ever violations occur?can be defined as follows:
Dep = [..] -> {*} || %( %) _ ;
Max = [..] -> {*} || %[ %] _ ;
Ident = [..] -> {*} || %( S %) %[ S %] _ ;
That is, DEP inserts a *-symbol after ( )-
sequences, which is how GEN marks epenthesis.
Likewise, MAX-violations are identified by the se-
quence [ ], and IDENT-violations by a parenthesized
segment followed by a bracketed segment. To define
the remaining markup transducers, we shall take ad-
vantage of some auxiliary template definitions, de-
fined as functions:
def Surf(X) [X .o. [0:%[ ? 0:%]]*].l/
[ %( (S) %) | %[ %] ];
def Change(X,Y) [%( X %) %[ Y %]];
Here, Surf(X) in effect changes the language X
so that it can match every possible surface encod-
ing produced by GEN; for example, a surface se-
quence ab may look like [a][b], or [a](a)[b], etc.,
since it may spring from various different underly-
ing forms. This is a useful auxiliary definition that
will serve to identify markedness violations. Like-
wise Change(X,Y) reflects the GEN representa-
tion of changing a segment X to Y needed to con-
cisely identify changed segments. Using the above
we may now define the remaining violation markups
needed.
CVOI = [b|d|g];
Voiced = [b|d|g|V];
Unvoiced = [p|t|k];
define VC Change(Voiced,Unvoiced) |
Change(Unvoiced,Voiced);
define Place Change(p,?-b)|Change(t,?-d)|
Change(k,?-g)|Change(b,?-p)|
Change(d,?-t)|Change(g,?-k)|
Change(a,?)|Change(e,?)|
Change(i,?)|Change(o,?)|
Change(u,?);
VF = [..] -> {*} || Surf(CVOI) _ .#. ;
IdentV = [..] -> {*} || VC _ ;
VOP = [..] -> {*} || Surf(CVOI) _ ;
IdentPl = [..] -> {*} || Place _ ;
The final remaining element for a complete imple-
mentation concerns the question of ?worsening? and
its introduction into a chain of transducer composi-
tion. To this end, we include a few more definitions:
AddViol = [?* 0:%* ?*]+;
Worsen = [Gen.i .o. Gen]/%* .o. AddViol;
def Eval(X) X .o. ?[X .o. Worsen].l .o. %*->0;
Cleanup = %[|%] -> 0 .o. %( \%)* %) -> 0;
Here, AddViol is the basic worsening method
discussed above whereby at least one violation mark
is added. However, because GEN adds markup to
the underlying forms, we need to be a bit more flex-
ible in our worsening procedure when matching up
violations. It may be the case that two different com-
peting surface forms have the same underlying form,
but the violation marks will not align correctly be-
cause of interfering brackets. Given two competing
candidates with a different number of violations, for
example (a)[b]* and [a], we would like the latter to
match the former after adding a violation mark since
they both originate in the same underlying form a.
The way to achieve this is to undo the effect of GEN,
and then redo GEN in every possible configuration
before adding the violation marks. The transducer
Worsen, above, does this by a composition of the
inverse GEN, followed by GEN, ignoring already ex-
isting violations. For the above example, this leads
to representations such as:
[a] Gen.i? a Gen? (a)[b] AddViol? (a)[b]*.
14
0p t k a e i o u 
1b d g 
2g:0 
4
d:0 5
b:0 
p t k a e i o u 
b d g g:0 
d:0 
b:0 
3
0:k 
0:t 
0:p 
Figure 2: OT grammar for devoicing compiled into an
FST.
We also define a Cleanup transducer that re-
moves brackets and parts of the underlying form.
Now we are ready to compile the entire system
into an FST. To apply only GEN and the first con-
straint, for example, we can calculate:
Eval(Gen .o. Dep) .o. Cleanup;
and likewise the entire grammar can be calculated
by:
Eval(Eval(Eval(Eval(Eval(Eval(
Gen .o. Dep) .o. Max) .o. IdentPl) .o.
VF) .o. IdentV) .o. VOP) .o. Cleanup;
This yields an FST of 6 states and 31 transitions
(see figure 2)?it can be ascertained that the FST
indeed does represent a relation where word-final
voiced obstruents are always devoiced.
3.3 Permutation of violations
As mentioned in Gerdemann and van Noord
(2000), there is an additional complication with the
?worsening?-approach. It is not always the case that
in the pool of competing candidates, the violation
markers line up, which is a prerequisite for filtering
out suboptimal ones by adding violations?although
in the above grammar the violations do line up cor-
rectly. However, for the vast majority of OT gram-
mars, this can be remedied by inserting a violation-
permuting transducer that moves violations markers
around before worsening, to attempt to produce a
correct alignment. Such a permuting transducer can
be defined as in figure 3.
If the need for permutation arises, repeated per-
mutations can be included as many times as war-
ranted in the definition of Worsen:
Figure 3: Violation permutation transducer.
Permute = [%*:0 ?* 0:%*|0:%* ?* %*:0]*/?;
Worsen = [Gen.i .o. Gen]/%* .o.
Permute .o. ... .o. Permute .o.
AddViol;
Knowing how many permutations are necessary
for the transducer to be able to distinguish between
any number of violations in a candidate pool is pos-
sible as follows: we can can calculate for some con-
straint ConsN in a sequence of constraints,
Eval(Eval(Gen .o. Cons1) ... .o. ConsN) .o.
ConsN .o. \%* -> 0;
Now, this yields a transducer that maps every un-
derlying form to n asterisks, n being the number
of violations with respect to ConsN in the candi-
dates that have successfully survived ConsN. If this
transducer represents a function (is single-valued),
then we know that two candidates with a different
number of violations have not survived ConsN, and
that the worsening yielded the correct answer. Since
the question of transducer functionality is known
to be decidable (Blattner and Head, 1977), and
an efficient algorithm is given in Hulden (2009a),
which is included in foma (with the command test
functional) we can address this question by cal-
culating the above for each constraint, if necessary,
and then permute the violation markers until the
above transducer is functional.
3.4 Equivalence testing
In many cases, the purpose of an OT grammar is
to capture accurately some linguistic phenomenon
through the interaction of constraints rather than by
other formalisms. However, as has been noted by
15
Karttunen (2006), among others, OT constraint de-
bugging is an arduous task due to the sheer num-
ber of unforeseen candidates. One of the advantages
in encoding an OT grammar through the worsening
approach is that we can produce an exact represen-
tation of the grammar, which is not an approxima-
tion bounded by the number of constraint violations
it can distinguish (as in Karttunen (1998)), or by the
length of strings it can handle. This allows us to
formally calculate, among other things, the equiva-
lence of an OT grammar represented as an FST and
some other transducer. For example, in the above
grammar, the intention was to model end-of-word
obstruent devoicing through optimality constraints.
Another way to model the same thing would be to
compile the replacement rule:
Rule = b -> p, d -> t, g -> k || _ .#. ;
The transducer resulting from this is shown in fig-
ure 4.
0
@ k p t 
1b d g 
2b:p d:t g:k
@ k p t 
b d g 
b:p d:t g:k
Figure 4: Devoicing transducer compiled through a rule.
As is seen, the OT transducer (figure 2) and
the rule transducer (figure 4) are not structurally
identical. However, both transducers represent a
function?i.e. for any given input, there is always
a unique winning candidate. Although transducer
equivalence is not testable by algorithm in the gen-
eral case, it is decidable in the case where one of
two transducers is functional. If this is the case it is
sufficient to test that domain(?1) = domain(?2) and
that ??12 ? ?1 represents identity relations only. As
an algorithm to decide if a transducer is an identity
transducer is also included in foma, it can be used to
ascertain that the two above transducers are in fact
identical, and that the linguistic generalization cap-
tured by the OT constraints is correct:
regex Rule.i .o. Grammar;
test identity
which indeed returns TRUE. For a small grammar,
such as the devoicing grammar, determining the cor-
rectness of the result by other means is certainly fea-
sible. However, for more complex systems the abil-
ity to test for equivalence becomes a valuable tool in
analyzing constraint systems.
4 Variations on GEN: an OT grammar of
stress assignment
Most OT grammars that deal with phonological phe-
nomena with faithfulness and markedness gram-
mars are implementable through the approach given
above, with minor variations according to what spe-
cific constraints are used. In other domains, how-
ever, in may be the case that GEN, as described
above, needs modification. A case in point are gram-
mars that mark prosody or perform syllabification
that often take advantage of only markedness con-
straints. In such cases, there is often no need for
GEN to insert, change, and delete material if all
faithfulness constraints are assumed to outrank all
markedness constraints. Or alternatively, if the OT
grammar is assumed to operate on a different stra-
tum where no faithfulness constraints are present.
However, GEN still needs to insert material into
strings, such as stress marks or syllable boundaries.
To test the approach with a larger ?real-
world? grammar we have reimplemented a Finnish
stress assignment grammar, originally implemented
through the counting approach of Karttunen (1998)
in Karttunen (2006), following a description in
Kiparsky (2003). The grammar itself contains nine
constraints, and is intended to give a complete ac-
count of stress placement in Finnish words. Without
going into a line-by-line analysis of the grammar,
the crucial main differences in this implementation
to that of the previous sections are:
? GEN only inserts symbols ( ) ? and ?
to mark feet and stress
? Violations need to be permuted in Worsen to
yield an exact representation
? GEN syllabifies words correctly through a re-
placement rule (no constraints are given in the
grammar to model syllabification; this is as-
sumed to be already performed)
16
kainostelijat -> (ka?i.nos).(te?.li).jat
kalastelemme -> (ka?.las).te.(le?m.me)
kalasteleminen -> *(ka?.las).te.(le?.mi).nen
kalastelet -> (ka?.las).(te?.let)
kuningas -> (ku?.nin).gas
strukturalismi -> (stru?k.tu).ra.(li?s.mi)
ergonomia -> (e?r.go).(no?.mi).a
matematiikka -> (ma?.te).ma.(ti?ik.ka)
Figure 5: Example outputs of matching implementation
of Finnish OT.
Compiling the entire grammar through the same
procedure as above outputs a transducer with 134
states, and produces the same predictions as Kart-
tunen?s counting OT grammar.6 As opposed to the
previous devoicing grammar, compiling the Finnish
prosody grammar requires permutation of the viola-
tion markers, although only one constraint requires
it (STRESS-TO-WEIGHT, and in that case, compos-
ing Worsen with one round of permutation is suffi-
cient for convergence).
Unlike the counting approach, the current ap-
proach confers two significant advantages. The first
is that we can compile the entire grammar into an
FST that does not restrict the inputs in any way. That
is, the final product is a stand-alone transducer that
accepts as input any sequence of any length of sym-
bols in the Finnish alphabet, and produces an output
where the sequence is syllabified, marked with feet,
and primary and secondary stress placement (see fig-
ure 5). The counting method, in order to compile at
all, requires that the set of inputs be fixed to some
very limited set of words, and that the maximum
number of distinguishable violations (and indirectly
word length) be fixed to some k.7 The second ad-
vantage is that, as mentioned before, we are able to
formally compare the OT grammar (because it is not
an approximation), to a rule-based grammar (FST)
that purports to capture the same phenomena. For
example, Karttunen (2006), apart from the count-
ing OT implementation, also provides a rule-based
account of Finnish stress, which he discovers to be
distinct from an OT account by finding two words
6Including replicating errors in Kiparsky?s OT analysis dis-
covered by Karttunen, as seen in figure 5.
7Also, compiling the grammar is reasonably quick: 7.04s on
a 2.8MHz Intel Core 2, vs. 2.1s for a rewrite-rule-based account
of the same phenomena.
where their respective predictions differ. However,
by virtue of having an exact transducer, we can for-
mally analyze the OT account together with the rule-
based account to see if they differ in their predictions
for any input, without having to first intuit a differ-
ing example:
regex RuleGrammar.i .o. OTGrammar;
test identity
Further, we can subject the two grammars to the
usual finite-state calculus operations to gain possible
insight into what kinds of words yield different pre-
dictions with the two?something useful for linguis-
tic debugging. Likewise, we can use similar tech-
niques to analyze for redundancy in grammars. For
example, we have assumed that the VOP-constraint
plays no role in the above devoicing tableaux. Using
finite-state calculus, we can prove it to be so for any
input if the grammar is constructed with the method
presented here.
5 Limits on FST implementation
We shall conclude the presentation here with a brief
discussion of the limits of FST representability, even
of simple OT grammars. Previous analyses have
shown that OT systems are beyond the generative
capacity of finite-state systems, under some assump-
tions of what GEN looks like. For example, Frank
and Satta (1998) present such a constraint system
where GEN is taken to be defined through a trans-
duction equivalent to:8
Gen = [a:b|b:a]* | [a|b]*;
That is, a relation which either maps all a?s to b?s
and vice versa, or leaves the input unchanged. Now,
let us assume the presence of a single markedness
constraint ?a, militating against the letter a. In that
case, given an input of the format a?b? the effective
mapping of the entire system is one that is an identity
relation if there are fewer a?s than b?s; otherwise the
a?s and b?s are swapped. As is easily seen, this is not
a regular relation.
One possible objection to this analysis of non-
regularity is that linguistically GEN is usually as-
sumed to perform any transformation to the input
8The idea is attributed to Markus Hiller in the article.
17
whatsoever?not just limiting itself to a proper sub-
set of ?? ? ??. However, it is indeed the case
that even with a canonical GEN-function, some very
simple OT systems fall outside the purview of finite-
state expressibility, as we shall illustrate by a differ-
ent example here.
5.1 A simple proof of OT nonregularity
Assume a grammar that has four very basic con-
straints: IDENT, forbidding changes, DEP, for-
bidding epenthesis, ?ab, a markedness constraint
against the sequence ab, and MAX, forbidding dele-
tion, ranked IDENT,DEP  ?ab  MAX. We as-
sume GEN to be as general as possible?performing
arbitrary deletions, insertions, and changes.
It is clear, as is illustrated in table 2, that for all in-
puts of the format anbm the grammar in question de-
scribes a relation that deletes all the a?s or all the b?s
depending on which there are fewer instances of, i.e.
anbm ? an if m < n, and anbm ? bm if n < m.
This can be shown by a simple pumping argument
to not be realizable through an FST.
aaabb IDENT DEP ?ab MAX
aaaaa *!*
aaacbb *!
aaabb *!
aaab *! *
bb ***!
+ aaa **
Table 2: Illustrative tableau for a simple constraint sys-
tem not capturable as a regular relation.
Implementing this constraint system with the
methods presented here is an interesting exercise
and serves to examine the behavior of the method.
We define GEN, DEP, MAX, and IDENT as be-
fore, define a universal alphabet (excluding markup
symbols), and the constraint ?ab naturally as:
S = ? - %( - %) - %[ - %] - %* ;
NotAB = [..] -> {*} || Surf(a b) _ ;
Now, with one round of permutation of the viola-
tion markers in Worsen as follows:
Worsen = [Gen.i .o. Gen]/{*} .o.
AddViol .o. Permute;
we calculate
define Grammar Eval(Eval(Eval(Eval(
Gen .o. Ident) .o. Dep) .o. NotAB) .o.
Max) .o. Cleanup;
which produces an FST that cannot distinguish be-
tween more than two a?s or b?s in a string. While
it correctly maps aab to aa and abb to bb, the
tableau example of aaabb is mapped to both aaa
and bb. However, with one more round of permu-
tation in Worsen, we produce an FST that can in-
deed cover the example, mapping aaabb uniquely
to bb, while failing with aaaabbb (see figure 6).
This illustrates the approximation characteristic of
the matching method: for some grammars (proba-
bly most natural language grammars) the worsening
approach will at some point of permutation of the vi-
olation markers terminate and produce an exact FST
representation of the grammar, while for some gram-
mars such convergence will never happen. How-
ever, if the permutation of markers terminates and
produces a functional transducer when testing each
violation as described above, the FST is guaranteed
to be an exact representation.
0
b @ 
1
a 
5a:0 
@ 2
b:0 
3
a 
@ 
a 
@ 
b:0 
4
a 
@ 
a b:0 
b 
6a:0 b 7
a:0 
b 
a:0 
Figure 6: An non-regular OT approximation.
It is an open question if it is decidable by exam-
ining a grammar whether it will yield an exact FST
representation. We do not expect this question to be
easy, since it cannot be determined by the nature of
the constraints alone. For example, the above four-
constraint system does have an exact FST represen-
tation in some orderings of the constraints, but not
in the particular one given above.
18
6 Conclusion
We have presented a practical method of implement-
ing OT grammars as finite-state transducers. The ex-
amples, definitions, and templates given should be
sufficient and flexible enough to encode a wide vari-
ety of OT grammars as FSTs. Although no method
can encode all OT grammars as FSTs, the funda-
mental advantage with the system outlined is that
for a large majority of practical cases, an FST can
be produced which is not an approximation that can
only tell apart a limited number of violations. As
has been noted elsewhere (e.g. Eisner (2000b,a)),
some OT constraints, such as Generalized Align-
ment constraints, are on the face of it not suitable
for FST implementation. We may add to this that
some very simple constraint systems, assuming a
canonical GEN, and only using the most basic faith-
fulness and markedness constraints, are likewise not
encodable as regular relations, and seem to have the
generative power to encode phenomena not found
in natural language. However, for most practical
purposes?and this includes modeling actual phe-
nomena in phonology and morphology?the present
approach offers a fruitful way to implement, ana-
lyze, and debug OT grammars.
References
Beesley, K. R. and Karttunen, L. (2003). Finite State
Morphology. CSLI Publications, Stanford, CA.
Blattner, M. and Head, T. (1977). Single-valued a-
transducers. Journal of Computer and System Sci-
ences, 15(3):328?353.
Eisner, J. (2000a). Directional constraint evaluation
in optimality theory. In Proceedings of the 18th
conference on Computational linguistics, pages
257?263. Association for Computational Linguis-
tics.
Eisner, J. (2000b). Easy and hard constraint ranking
in optimality theory. In Finite-state phonology:
Proceedings of the 5th SIGPHON, pages 22?33.
Ellison, T. M. (1994). Phonological derivation in op-
timality theory. In Proceedings of COLING?94?
Volume 2, pages 1007?1013.
Frank, R. and Satta, G. (1998). Optimality theory
and the generative complexity of constraint viola-
bility. Computational Linguistics, 24(2):307?315.
Gerdemann, D. and van Noord, G. (2000). Approx-
imation and exactness in finite state optimality
theory. In Proceedings of the Fifth Workshop of
the ACL Special Interest Group in Computational
Phonology.
Hammond, M. (1997). Parsing syllables: Modeling
OT computationally. Rutgers Optimality Archive
(ROA), 222-1097.
Hulden, M. (2009a). Finite-state Machine Construc-
tion Methods and Algorithms for Phonology and
Morphology. PhD thesis, The University of Ari-
zona.
Hulden, M. (2009b). Foma: a finite-state compiler
and library. In EACL 2009 Proceedings, pages
29?32.
Ja?ger, G. (2002). Gradient constraints in finite state
OT: the unidirectional and the bidirectional case.
More than Words. A Festschrift for Dieter Wun-
derlich, pages 299?325.
Kager, R. (1999). Optimality Theory. Cambridge
University Press.
Kaplan, R. M. and Kay, M. (1994). Regular mod-
els of phonological rule systems. Computational
Linguistics, 20(3):331?378.
Karttunen, L. (1998). The proper treatment of op-
timality theory in computational phonology. In
Finite-state Methods in Natural Language Pro-
cessing.
Karttunen, L. (2006). The insufficiency of paper-
and-pencil linguistics: the case of Finnish
prosody. Rutgers Optimality Archive.
Kiparsky, P. (2003). Finnish noun inflection. Gener-
ative approaches to Finnic linguistics. Stanford:
CSLI.
Prince, A. and Smolensky, P. (1993). Optimality
theory: Constraint interaction in generative gram-
mar. ms. Rutgers University Cognitive Science
Center.
Riggle, J. (2004). Generation, recognition, and
learning in finite state Optimality Theory. PhD
thesis, University of California, Los Angeles.
19
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 35?39,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Finite-state technology in a verse-making tool
Manex Agirrezabal, In?aki Alegria, Bertol Arrieta
University of the Basque Country (UPV/EHU)
maguirrezaba008@ikasle.ehu.es, i.alegria@ehu.es, bertol@ehu.es
Mans Hulden
Ikerbasque (Basque Science Foundation)
mhulden@email.arizona.edu
Abstract
This paper presents a set of tools designed to
assist traditional Basque verse writers during
the composition process. In this article we
are going to focus on the parts that have been
created using finite-state technology: this in-
cludes tools such as syllable counters, rhyme
checkers and a rhyme search utility.
1 The BAD tool and the Basque singing
tradition
The BAD tool is an assistant tool for verse-makers
in the Basque bertsolari tradition. This is a form
of improvised verse composition and singing where
participants are asked to produce impromptu com-
positions around themes which are given to them
following one of many alternative verse formats.
The variety of verse schemata that exist all impose
fairly strict structural requirements on the composer.
Verses in the bertsolari tradition must consist of a
specified number of lines, each with a fixed num-
ber of syllables. Also, strict rhyme patterns must
be followed. The structural requirements are con-
sidered the most difficult element in the bertsolar-
itza?however, well-trained bertsolaris can usually
produce verses that fulfill the structural prerequisites
in a very limited time.
The BAD tool presented here is mainly di-
rected at those with less experience in the tradi-
tion such as students. One particular target group
are the bertso-eskola-s (verse-making schools) that
have been growing in popularity?these are schools
found throughout the Basque Country that train
young people in the art of bertsolaritza.
The primary functionality of the tool is illustrated
in figure 1 which shows the main view of the util-
ity. The user is offered a form in which a verse
can be written, after which the system checks the
technical correctness of the poem. To perform this
task, several finite state transducer-based modules,
are used, some of them involving the metrics (syl-
lable counter) of the verse, and others the rhyme
(rhyme searcher and checker). The tool has support
for 150 well known verse meters.
In the following sections, we will outline the tech-
nology used in each of the parts in the system.
2 Related work
Much of the existing technology for Basque mor-
phology and phonology uses finite-state technology,
including earlier work on rhyme patterns (Arrieta
et al, 2001). In our work, we have used the Basque
morphological description (Alegria et al, 1996) in
the rhyme search module. Arrieta et al (2001) de-
velop a system where, among other things, users can
search for words that rhyme with an introduced pat-
tern. It is implemented in the formalism of two-level
morphology (Koskenniemi, 1983) and compiled into
finite-state transducers.
We have used the open-source foma finite-state
compiler to develop all the finite-state based parts
of our tool.1. After compiling the transducers, we
use them in our own application through the C/C++
API provided with foma.
3 Syllable counter
As mentioned, each line in a verse must contain a
specified number of syllables. The syllable counter
module that checks whether this is the case consists
of a submodule that performs the syllabification it-
self as well as a module that yields variants produced
by optional apocope and syncope effects. For the
syllabification itself, we use the approach described
in Hulden (2006), with some modifications to cap-
ture Basque phonology.
1In our examples, FST expressions are written using foma
syntax. For details, visit http://foma.googlecode.com
35
Figure 1: A verse written in the BAD web application.
3.1 Syllabification
Basque syllables can be modeled by assuming a
maximum onset principle together with a sonority
hierarchy where obstruents are the least sonorous el-
ement, followed in sonority by the liquids, the nasals
and the glides. The syllable nuclei are always a sin-
gle vowel (a,e,i,o,u) or a combination of a low vowel
(a,e) and a high vowel (i,o,u) or a high vowel and an-
other high vowel.
The syllabifier relies on a chain of composed re-
placement rules (Beesley and Karttunen, 2003) com-
piled into finite-state transducers. These defini-
tions are shown in figure 2. The overall strategy
is to first mark off the nuclei in a word by the rule
MarkNuclei which takes advantage of a left-to-
right longest replacement rule. This is to ensure that
diphthongs do not get split into separate syllables
by the subsequent syllabification process. Follow-
ing this, syllables are marked off by the markSyll-
rule, which inserts periods after legitimate syllables.
This rule takes advantage of the shortest-leftmost re-
placement strategy?in effect minimizing the coda
and maximizing the size of the onset of a syllable to
the extent permitted by the allowed onsets and co-
das, defined in Onset and Coda, respectively.
To illustrate this process, supposing that we
are syllabifying the Basque word intransitiboa.
The first step in the syllabification process is
to mark the nuclei in the word, resulting in
{i}ntr{a}ns{i}t{i}b{o}{a}. In the more com-
plex syllabification step, the markSyll rule as-
sures that the juncture ntr gets divided as n.tr be-
cause nt.r would produce a non-maximal onset,
and i.ntr would in turn produce an illegal onset in
define Obs [f|h|j|k|p|s|t|t s|t z|t x|x|
z|b|d|g|v|d d|t t];
define LiqNasGli [l|r|r r|y|n|m];
define LowV [a|e|o];
define HighV [i|u];
define V LowV | HighV;
define Nucleus [V | LowV HighV |
[HighV HighV - [i i] - [u u]]];
define Onset (Obs) (LiqNasGli);
define Coda C?<4;
define MarkNuclei Nucleus @-> %{ ... %};
define Syll Onset %{ Nucleus %} Coda;
define markSyll Syll @> ... "." || _ Syll ;
define cleanUp %{|%} -> 0;
regex MarkNuclei .o. markSyll .o. cleanUp;
Figure 2: Syllable definition
the second syllable. The final syllabification, af-
ter markup removal by the Cleanup rule, is then
in.tran.si.ti.bo.a. This process is illustrated in fig-
ure 3
In bertsolaritza, Basque verse-makers follow this
type of syllable counting in the majority if cases;
however, there is some flexibility as regards the syl-
labification process. For example, suppose that the
phrase ta lehenengo urtian needs to fit a line which
must contain six syllables. If we count the sylla-
bles using the algorithm shown above, we receive a
count of eight (ta le.hen.en.go ur.ti.an). However,
in the word lehenengo we can identify the syncope
pattern vowel-h-vowel, with the two vowels being
identical. In such cases, we may simply replace
the entire sequence by a single vowel (ehe ? e).
This is phonetically equivalent to shortening the ehe-
sequence (for those dialects where the orthographi-
cal h is silent). With this modification, we can fit
36
the line in a 7 syllable structure. We can, however,
further reduce the line to 6 syllables by a second
type of process that merges the last syllable of one
word with the first of the next one and then resyl-
labifying. Hence, ta lehenengo urtian, using the
modifications explained above, could be reduced to
ta.le.nen.gour.ti.an, which would fit the 6 syllable
structure. This production of syllabification variants
is shown in figure 4.
transformazioei
tr{a}nsf{o}rm{a}z{i}{o}{ei}
markNuclei
syllabify
tr{a}ns.f{o}r.m{a}.z{i}.{o}.{ei}
cleanUp
trans.for.ma.zi.o.ei
Figure 3: Normal syllabification.
trarnsfomn
trzarnzsfzomn
marrkNuculkeuis
kreybskeym
trzarnzsfzomn trnzsfzomn
tratnsftronnm
tzratznsftzroznnm
marrkNuculkeuis
kreybskeym
tzratznsftzroznnm tzratznstzroznnm
Figure 4: Flexible syllabification.
4 Finite-state technology for rhymes
4.1 Basque rhyme patterns and rules
Similar to the flexibility in syllabification, Basque
rhyme schemes also allows for a certain amount
of leeway that bertsolaris can take advantage of.
The widely consulted rhyming dictionary Hiztegi
Errimatua (Amuriza, 1981) contains documented a
number of phonological alternations that are accept-
able as off-rhymes: for example the stops p, t, and k
are often interchangeable, as are some other phono-
logical groups. Figure 5 illustrates the definitions
for interchangeable phonemes when rhyming. The
interchangeability is done as a prelude to rhyme
checking, whereby phonemes in certain groups,
such as p, are replaced by an abstract symbol de-
noting the group (e.g. PTK).
4.2 Rhyme checker
The rhyme checker itself in BAD was originally de-
veloped as a php-script, and then reimplemented as
define plosvl [p | t | k];
define rplosv [b | d | g | r];
define sib [s | z | x];
define nas [n | m];
define plosvlconv ptk -> PTK;
define rplosvconv bdgr -> BDGR;
define sibconv sib -> SZX;
define nasconv nas -> NM;
define phoRules plosvlconv .o. rplosvconv .o.
sibconv .o. nasconv ;
Figure 5: Conflation of consonant groups before rhyme
checking.
a purely finite-state system. In this section we will
focus on the finite-state based one.
As the php version takes advantage of syllabifica-
tion, the one developed with transducers does not.
Instead, it relies on a series of replacement rules and
the special eq() operator available in foma. An
implementation of this is given in figure 6. As input
to the system, the two words to be checked are as-
sumed to be provided one after the other, joined by
a hyphen. Then, the system (by rule rhympat1)
identifies the segments that do not participate in the
rhyme and marks them off with ?{? and ?}? symbols
(e.g. landa-ganga ? <{l}anda>-<{g}anga>).
The third rule (rhympat3) removes everything
that is between ?{? and ?}?, leaving us only with
the segments relevant for the rhyming pattern (e.g.
<anda>-<anga>). Subsequent to this rule, we
apply the phonological grouping reductions men-
tioned above in section 4.1, producing, for example
(<aNMBDGRa>-<aNMBDGRa>).
After this reduction, we use the eq(X,L,R)-
operator in foma, which from a transducer X, filters
out those words in the output where material be-
tween the specified delimiter symbols L and R are
unequal. In our case, we use the < and > symbols
as delimiters, yielding a final transducer that does
not accept non-rhyming words.
4.3 Rhyme search
The BAD tool also includes a component for search-
ing words that rhyme with a given word. It is devel-
oped in php and uses a finite-state component like-
wise developed with foma.
Similarly to the techniques previously described,
it relies on extracting the segments relevant to the
37
define rhympat1 [0:"{" ?* 0:"}"
[[[V+ C+] (V) V] | [(C) V V]] C* ];
# constraining V V C pattern
define rhympat2 ?[?* V "}" V C];
# cleaning non-rhyme part
define rhympat3 "{" ?* "}" -> 0;
define rhympat rhympat1 .o. rhympat2 .o.
rhympat3;
# rhyming pattern on each word
# and phonological changes
define MarkPattern rhympat .o.
phoRules .o. patroiak;
# verifying if elements between < and >
# are equal
define MarkTwoPatterns
0:%< MarkPattern 0:%> %-
0:%< MarkPattern 0:%> ;
define Verify _eq(MarkTwoPatterns, %<, %>)
regex Verify .o. Clean;
Figure 6: Rhyme checking using foma.
rhyme, after which phonological rules are applied
(as in 4.1) to yield phonetically related forms. For
example, introducing the pattern era, the system re-
turns four phonetically similar forms era, eda, ega,
and eba. Then, these responses are fed to a trans-
ducer that returns a list of words with the same end-
ings. To this end, we take advantage of a finite-state
morphological description of Basque (Alegria et al,
1996).
As this transducer returns a set of words which
may be very comprehensive?including words not
commonly used, or very long compounds?we then
apply a frequency-based filter to reduce the set of
possible rhymes. To construct the filter, we used
a newspaper corpus, (Egunkaria2) and extracted the
frequencies of each word form. Using the frequency
counts, we defined a transducer that returns a word?s
frequency, using which we can extract only the n-
most frequent candidates for rhymes. The system
also offers the possibility to limit the number of syl-
lables that desired rhyming words may contain. The
syllable filtering system and the frequency limiting
parts have been developed in php. Figure 7 shows
the principle of the rhyme search?s finite-state com-
ponent.
5 Evaluation
As we had available to us a rhyme checker written
in php before implementing the finite-state version,
2http://berria.info
regex phoRules .o. phoRules.i .o.
0:?* ?* .o. dictionary ;
Figure 7: Rhyme search using foma
it allowed for a comparison of the application speed
of each. We ran an experiment introducing 250,000
pairs of words to the two rhyme checkers and mea-
sured the time each system needed to reply. The
FST-based checker was roughly 25 times faster than
the one developed in php.
It is also important to mention that these tools
are going to be evaluated in an academic environ-
ment. As that evaluation has not been done yet, we
made another evaluation in our NLP group in or-
der to detect errors in terms of syllabification and
rhyme quality. The general feeling of the experiment
was that the BAD tool works well, but we had some
efficiency problems when many people worked to-
gether. To face this problem some tools are being
implemented as a server.
6 Discussion & Future work
Once the main tools of the BAD have been devel-
oped, we intend to focus on two different lines of
development. The first one is to extend to flexibil-
ity of rhyme checking. There are as of yet patterns
which are acceptable as rhymes to bertsolaris that
the system does not yet recognize. For example,
the words filma and errima will not be accepted by
the current system, as the two rhymes ilma and ima
are deemed to be incompatible. In reality, these two
words are acceptable as rhymes by bertsolaris, as
the l is not very phonetically prominent. However,
adding flexibility also involves controlling for over-
generation in rhymes. Other reduction patterns not
currently covered by the system include phenomena
such as synaloepha?omission of vowels at word
boundaries when one word ends and the next one
begins with a vowel.
Also, we intend to include a catalogue of melodies
in the system. These are traditional melodies that
usually go along with a specific meter. Some 3,000
melodies are catalogued (Dorronsoro, 1995). We are
also using the components described in this article in
another project whose aim is to construct a robot ca-
pable to find, generate and sing verses automatically.
38
Acknowledgments
This research has been partially funded by the Span-
ish Ministry of Education and Science (OpenMT-
2, TIN2009-14675-C03) and partially funded by the
Basque Government (Research Groups, IT344-10).
We would like to acknowledge Aitzol Astigarraga
for his help in the development of this project. He
has been instrumental in our work, and we intend to
continue working with him. Also we must mention
the Association of Friends of Bertsolaritza, whose
verse corpora has been used to test and develop these
tools and to develop new ones.
References
Alegria, I., Artola, X., Sarasola, K., and Urkia,
M. (1996). Automatic morphological analysis
of Basque. Literary and Linguistic Computing,
11(4):193?203.
Amuriza, X. (1981). Hiztegi errimatua [Rhyme Dic-
tionary]. Alfabetatze Euskalduntze Koordinakun-
dea.
Arrieta, B., Alegria, I., and Arregi, X. (2001). An
assistant tool for verse-making in Basque based
on two-level morphology. Literary and linguistic
computing, 16(1):29?43.
Beesley, K. R. and Karttunen, L. (2003). Finite state
morphology. CSLI.
Dorronsoro, J. (1995). Bertso doinutegia [Verse
melodies repository]. Euskal Herriko Bertsolari
Elkartea.
Hulden, M. (2006). Finite-state syllabification.
Finite-State Methods and Natural Language Pro-
cessing, pages 86?96.
Koskenniemi, K. (1983). Two-level morphology:
A general computational model for word-form
production and generation. Publications of the
Department of General Linguistics, University of
Helsinki. Helsinki: University of Helsinki.
39
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 65?69,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Developing an open-source FST grammar for verb chain transfer in a
Spanish-Basque MT System
Aingeru Mayor, Mans Hulden, Gorka Labaka
Ixa Group
University of the Basque Country
aingeru@ehu.es, mhulden@email.arizona.edu, gorka.labaka@ehu.es
Abstract
This paper presents the current status of de-
velopment of a finite state transducer gram-
mar for the verbal-chain transfer module in
Matxin, a Rule Based Machine Translation
system between Spanish and Basque. Due to
the distance between Spanish and Basque, the
verbal-chain transfer is a very complex mod-
ule in the overall system. The grammar is
compiled with foma, an open-source finite-
state toolkit, and yields a translation execution
time of 2000 verb chains/second.
1 Introduction
This paper presents the current status of develop-
ment of an FST (Finite State Transducer) grammar
we have developed for Matxin, a Machine Transla-
tion system between Spanish and Basque.
Basque is a minority language isolate, and it is
likely that an early form of this language was already
present in Western Europe before the arrival of the
Indo-European languages.
Basque is a highly inflected language with free
order of sentence constituents. It is an agglutinative
language, with a rich flexional morphology.
Basque is also a so-called ergative-absolutive lan-
guage where the subjects of intransitive verbs ap-
pear in the absolutive case (which is unmarked),
and where the same case is used for the direct ob-
ject of a transitive verb. The subject of the transi-
tive verb (that is, the agent) is marked differently,
with the ergative case (in Basque by the suffix -k).
The presence of this morpheme also triggers main
and auxiliary verbal agreement. Auxiliary verbs, or
?periphrastic? verbs, which accompany most main
verbs, agree not only with the subject, but also with
the direct object and the indirect object, if present.
Among European languages, this polypersonal sys-
tem (multiple verb agreement) is rare, and found
only in Basque, some Caucasian languages, and
Hungarian.
The fact that Basque is both a morphologically
rich and less-resourced language makes the use of
statistical approaches for Machine Translation dif-
ficult and raises the need to develop a rule-based
architecture which in the future could be combined
with statistical techniques.
The Matxin es-eu (Spanish-Basque) MT engine
is a classic transfer-based system comprising three
main modules: analysis of the Spanish text (based
on FreeLing, (Atserias et al, 2006)), transfer, and
generation of the Basque target text.
In the transfer process, lexical transfer is first
carried out using a bilingual dictionary coded in
the XML format of Apertium dictionary files (.dix)
(Forcada et al, 2009), and compiled, using the FST
library implemented in the Apertium project (the lt-
toolbox library), into a finite-state transducer that
can be processed very quickly.
Following this, structural transfer at the sentence
level is performed, and some information is trans-
ferred from some chunks1 to others while some
chunks may be deleted. Finally, the structural trans-
1A chunk is a non-recursive phrase (noun phrase, preposi-
tional phrase, verbal chain, etc.) which expresses a constituent
(Abney, 1991; Civit, 2003). In our system, chunks play a cru-
cial part in simplifying the translation process, due to the fact
that each module works only at a single level, either inside or
between chunks.
65
fer at the verb chunk level is carried out. The verbal
chunk transfer is a very complex module because of
the nature of Spanish and Basque auxiliary verb con-
structions, and is the main subject of this paper.
This verb chain transfer module is implemented
as a series of ordered replacement rules (Beesley and
Karttunen, 2003) using the foma finite-state toolkit
(Hulden, 2009). In total, the system consists of 166
separate replacement rules that together perform the
verb chunk translation. In practice, the input is given
to the first transducer, after which its output is passed
to the second, and so forth, in a cascade. Each rule in
the system is unambiguous in its output; that is, for
each input in a particular step along the verb chain
transfer, the transducers never produce multiple out-
puts (i.e. the transducers in question are functional).
Some of the rules are joined together with composi-
tion, yielding a total of 55 separate transducers. In
principle, all the rules could be composed together
into one monolithic transducer, but in practice the
size of the composed transducer is too large to be
feasible. The choice to combine some transduc-
ers while leaving others separate is largely a mem-
ory/translation speed tradeoff.
2 Spanish and Basque verb features and
their translation
In the following, we will illustrate some of the main
issues in translating Spanish verb chains to Basque.
Since both languages make frequent use of auxiliary
verb constructions, and since periphrastic verb con-
structions are frequent in Basque, transfer rules can
get quite complex in their design.
For example, in translating the phrase
(Yo) compro (una manzana)
(I) buy (an apple)
[PP1CSN00] [VMIP1S0] [DI0FS0] [NCFS000]
we can translate it using the imperfective partici-
ple form (erosten) of the verb erosi (to buy), and a
transitive auxiliary (dut) which itself contains both
subject agreement information (I: 1st sg.) and num-
ber agreement with the object (an apple: 3rd sg.):
(nik) (sagar bat) erosten dut. The participle carries
information concerning meaning, aspect and tense,
whereas the auxiliaries convey information about ar-
gument structure, tense and mood.
Table 1 illustrates the central idea of the verb
chunk transfer. In the first four examples the form of
the transitive auxiliary changes to express agreement
with different ergative arguments (the subject of the
clause), absolutive arguments (the direct object) and
dative arguments (the indirect object). In the fifth
example the future participle is used. The last ex-
ample shows the translation of a periphrastic con-
struction, in which the the Spanish and the Basque
word orders are completely different: this is re-
flected in the Spanish tengo que-construction (have
to) which appears before the main verb, whereas in
the Basque, the equivalent (behar) appears after the
main verb (erosi).
3 The FST grammar
We carry out the verbal chunk transfer using finite-
state transducers (Alegria et al, 2005). The gram-
mar rules take as input the Spanish verbal chunk,
perform a number of transformations on the input,
and then create and output the verbal chunk for
Basque.
To illustrate the functioning of the grammar, let us
consider the following example sentence in Spanish:
?Un tribunal ha negado los derechos constitu-
cionales a los presos polticos? (A court has denied
constitutional rights to political prisoners). The cor-
rect translation into Basque given by the system for
this example is as follows: Auzitegi batek eskubide
konstituzionalak ukatu dizkie preso politikoei. Fig-
ure 1 shows a detailed overview of how the whole
transfer of the verbal chunk is performed for this par-
ticular example.
First, the input to the grammar is assumed to be a
string containing (separated by the ?&? symbol) the
following information :
? the morphological information (using
EAGLES-style tags Leech and Wilson
(1996)) for all nodes (separated by ?+?
symbol) in the Spanish verbal chunk
(haber[VAIP3S0]+negar[VMP00SM]);
? the morphological information of the subject
([sub3s]), the direct object ([obj3p]) and the
indirect object ([iobj3p]);
? the translation of the main verb in Basque
(ukatu) and information about its transitivity
66
Spanish sentence English Basque translation
(Yo) compro (una manzana) (I) buy (an apple) (Nik) (sagar bat) erosten dut
(Yo) compro (manzanas) (I) buy (apples) (Nik) (sagarrak) erosten ditut
(Tu?) compras (manzanas) (You) buy (apples) (Zuk) (sagarrak) erosten dituzu
(Yo) (te) compro (una manzana) (I) buy (you) (an apple) (Nik) (zuri) (sagar bat) erosten dizut
(Yo) comprare? (una manzana) (I) will buy (an apple) (Nik) (sagar bat) erosiko dut
(Yo) tengo que comprar (manzanas) (I) must buy (apples) (Nik) (sagarrak) erosi behar ditut
Table 1: Examples of translations
  
Un    tribunal     ha negado    los    derechos    constitucionales         a   los    presos    pol?ticosA     court     has denied     (the)   rights           constitutional            to (the)   prisoners  political
uka  +tu              d     +i      +zki   +e    +?
 Subject                Verb                                     Object                                                      Indirect                                                                                                                                           Object
haber[VAIP3S0]+negar[VMP00SM]   &   [sub3s] [obj3p] [iobj3p]   &   ukatu [DIO] 
haber[VAIP3S0]+negar[VMP00SM]  &  [sub3s] [obj3p] [iobj3p]  & ukatu [DIO]SimpleVerb   (main) AspectMain  /  Aux TenseMood Abs Dat Erg
1. Identification      of the schema [ SimpleVerbEsType  -> ...  SimpleVerbEuSchema ]
niega[VMIP3S0]   &   [sub3s] [obj3s] [dat3p]   &  ukatu [DIO] + SimpleVerb   (main)[perfPart]  /  edun(aux) [indPres] [abs3p][dat3p][erg3s]
2. Resolution      of the values Attrib.               ->  Value             || Context                             AspectMain  -> [perfPart]  || ?* VAIP ?* SimpleVerb ?* _Aux  -> edun(aux) || ?* DIO ?* _TenseMood  -> [indPres] || ?* VAIP ?* _Abs  -> [abs3p] || ?* [obj3p] ?* edun(aux) ?* _Dat  -> [dat3p] || ?* [iobj3p] ?* _Erg  -> [erg3s] || ?* V???3S ?* edun(aux) ?* _
3. Elimination of     source information 
ukatu(main)[perfPart]   /  edun(aux) [indPres] [abs3p][dat3p][erg3s]
Input
Output
deny     perf.             ind.    trans.   3rdpl     3rdpl    3rdsg             part.            pres.  aux.   abs.     dat.    erg.           
Figure 1: Example of the transfer of a verbal chunk.
67
([DIO]), indicating a ditransitive construction:
haber[VAIP3S0]+negar[VMP00SM] &
[sub3s][obj3p][iobj3p] & ukatu[DIO]
The grammatical rules are organized into three
groups according to the three main steps defined for
translating verbal chunks:
1. Identification of the Basque verbal chunk
schema corresponding to the source verbal
chunk.
There are twelve rules which perform this task,
each of which corresponds to one of the follow-
ing verbal chunks in Spanish: non-conjugated
verbs, simple non-periphrastic verbs as well
as four different groups reserved for the pe-
riphrastic verbs.
The verbal chunk of the example in figure 1 is
a simple non-periphrastic one, and the rule that
handles this particular case is as follows:
[simpleVerbEsType -> ...
simpleVerbEuSchema]
When this rule matches the input string
representing a simple non-periphrastic ver-
bal chunk (simpleVerbEsType) it adds the
corresponding Basque verbal chunk schema
(simpleVerbEuSchema) to the end of the input
string. simpleVerbEsType is a complex au-
tomaton that has the definition of the Spanish
simple verbs. simpleVerbEuSchema is the type
of the verbal chunk (SimpleVerb) and an au-
tomaton that contains as strings the pattern of
elements (separated by the ?/? symbol) that the
corresponding Basque verb chunk will need to
have (in this case, the main verb and the auxil-
iary verb):
SimpleVerb (main) AspectMain /
Aux TenseMood Abs Dat Erg
2. Resolution of the values for the attributes in the
Basque schema.
A total of 150 replacement rules of this type
have been written in the grammar. Here are
some rules that apply to the above example:
[AspectMain -> [perfPart] || ?* VAIP
?* SimpleVerb ?* ]
[Aux -> edun(aux) || ?* DIO ?* ]
[Abs -> [abs3p] || ?* [obj3p] ?*
edun(aux) ?* ]
3. Elimination of source-language information (4
rules in total).
The output of the grammar for the example is:
ukatu(main)[perfPart] /
edun(aux)[indPres][abs3p][dat3p][erg3s]
The first node has the main verb (ukatu) with
the perfective participle aspect, and the sec-
ond one contains the auxiliary verb (edun) with
all its morphological information: indicative
present and argument structure.
In the output string, each of the elements contains
the information needed by the subsequent syntactic
generation and morphological generation phases.
4 Implementation
When the verbal chunk transfer module was first de-
veloped, there did not exist any efficient open-source
tools for the construction of finite state transduc-
ers. At the time, the XFST-toolkit (Beesley and
Karttunen, 2003) was used to produce the earlier
versions of the module: this included 25 separate
transducers of moderate size, occupying 2,795 kB
in total. The execution speed was roughly 250 verb
chains per second. Since Matxin was designed to be
open source, we built a simple compiler that con-
verted the XFST rules into regular expressions that
could then be applied without FST technology, at the
cost of execution speed. This verbal chunk transfer
module read and applied these regular expressions
at a speed of 50 verbal chunks per second.
In the work presented here, we have reimple-
mented and expanded the original rules written for
XFST with the foma2 toolkit (Hulden, 2009). Af-
ter adapting the grammar and compiling it, the 55
separate transducers occupy 607 kB and operate at
roughly 2,000 complete verb chains per second.3
Passing the strings from one transducer to the next in
the chain of 55 transducers in accomplished by the
depth-first-search transducer chaining functionality
available in the foma API.
2http://foma.sourceforge.net
3On a 2.8MHz Intel Core 2 Duo.
68
References
Abney, S. (1991). Principle-Based Parsing: Com-
putation and Psycholinguistics, chapter Parsing
by Chunks, pages 257?278. Kluwer Academic,
Boston.
Alegria, I., D??az de Ilarraza, A., Labaka, G., Ler-
sundi, M., Mayor, A., and Sarasola, K. (2005).
An FST grammar for verb chain transfer in a
Spanish?Basque MT system. In Finite-State
Methods and Natural Language Processing, vol-
ume 4002, pages 295?296, Germany. Springer
Verlag.
Atserias, J., Casas, B., Comelles, E., Gonza?lez, M.,
Padro?, L., and Padro?, M. (2006). Freeling 1.3:
Syntactic and semantic services in an open-source
NLP library. In Proceedings of LREC, volume 6,
pages 48?55.
Beesley, K. R. and Karttunen, L. (2003). Finite State
Morphology. CSLI Publications, Stanford, CA.
Civit, M. (2003). Criterios de etiquetacio?n y desam-
biguacio?n morfosinta?ctica de corpus en Espan?ol.
PhD thesis, Universidad de Barcelona.
Forcada, M., Bonev, B. I., Ortiz-Rojas, S.,
Pe?rez-Ortiz, J. A., Ram??rez-Sanchez, G.,
Sa?nchez-Mart??nez, F., Armentano-Oller, C.,
Montava, M. A., Tyers, F. M., and Ginest??-
Rosell, M. (2009). Documentation of the
open-source shallow-transfer machine trans-
lation platform Apertium. Technical report,
Departament de Llenguatges i Sistemes In-
formatics. Universitat d?Alacant. Available
at http://xixona.dlsi.ua.es/ fran/apertium2-
documentation.pdf.
Hulden, M. (2009). Foma: a finite-state compiler
and library. In Proceedings of EACL 2009, pages
29?32.
Leech, G. and Wilson, A. (1996). EAGLES rec-
ommendations for the morphosyntactic annota-
tion of corpora. Technical report, EAGLES Expert
Advisory Group on Language Engineering Stan-
dards, Istituto di Linguistica Computazionale,
Pisa, Italy.
69
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 70?74,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Conversion of Procedural Morphologies to Finite-State Morphologies: a
Case Study of Arabic
Mans Hulden
University of the Basque Country
IXA Group
IKERBASQUE, Basque Foundation for Science
mhulden@email.arizona.edu
Younes Samih
Heinrich-Heine-Universita?t Du?sseldorf
samih@phil.uni-duesseldorf.de
Abstract
In this paper we describe a conversion of
the Buckwalter Morphological Analyzer for
Arabic, originally written as a Perl-script,
into a pure finite-state morphological ana-
lyzer. Representing a morphological ana-
lyzer as a finite-state transducer (FST) con-
fers many advantages over running a procedu-
ral affix-matching algorithm. Apart from ap-
plication speed, an FST representation imme-
diately offers various possibilities to flexibly
modify a grammar. In the case of Arabic, this
is illustrated through the addition of the abil-
ity to correctly parse partially vocalized forms
without overgeneration, something not possi-
ble in the original analyzer, as well as to serve
both as an analyzer and a generator.
1 Introduction
Many lexicon-driven morphological analysis sys-
tems rely on a general strategy of breaking down
input words into constituent parts by consulting cus-
tomized lexicons and rules designed for a particu-
lar language. The constraints imposed by the lex-
ica designed are then implemented as program code
that handles co-occurrence restrictions and analysis
of possible orthographic variants, finally producing
a parse of the input word. Some systems designed
along these lines are meant for general use, such as
the hunspell tool (Hala?csy et al, 2004) which allows
users to specify lexicons and constraints, while oth-
ers are language-dependent, such as the Buckwalter
Arabic Morphological Analyzer (BAMA) (Buckwal-
ter, 2004).
In this paper we examine the possibility of con-
verting such morphological analysis tools to FSTs
that perform the same task. As a case study, we have
chosen to implement a one-to-one faithful conver-
sion of the Buckwalter Arabic analyzer into a finite-
state representation using the foma finite state com-
piler (Hulden, 2009b), while also adding some ex-
tensions to the original analyzer. These are useful
extensions which are difficult to add to the original
Perl-based analyzer because of its procedural nature,
but very straightforward to perform in a finite-state
environment using standard design techniques.
There are several advantages to representing mor-
phological analyzers as FSTs, as is well noted in the
literature. Here, in addition to documenting the con-
version, we shall also discuss and give examples of
the flexibility, extensibility, and speed of application
which results from using a finite-state representation
of a morphology.1
2 The Buckwalter Analyzer
Without going into an extensive linguistic discus-
sion, we shall briefly describe the widely used Buck-
walter morphological analyzer for Arabic. The
BAMA accepts as input Arabic words, with or with-
out vocalization, and produces as output a break-
down of the affixes participating in the word, the
stem, together with information about conjugation
classes. For example, for the input word ktb/I. J?,
BAMA returns, among others:
LOOK-UP WORD: ktb
SOLUTION 1: (kataba) [katab-u_1]
katab/VERB_PERFECT
+a/PVSUFF_SUBJ:3MS
(GLOSS): + write + he/it <verb>
1The complete code and analyzer are available at
http://buckwalter-fst.googlecode.com/
70
Figure 1: The Buckwalter Arabic Morphological Analyzer?s lookup process exemplified for the word lilkitAbi.
2.1 BAMA lookup
In the BAMA system, every Arabic word is assumed
to consist of a sometimes optional prefix, an oblig-
atory stem, and a sometimes optional suffix.2 The
system for analysis is performed by a Perl-script that
carries out the following tasks:
1. Strips all diacritics (vowels) from the input
word (since Arabic words may contain vocal-
ization marks which are not included in the lex-
icon lookup). Example: kataba? ktb
2. Factors the input word into all possible
combinations of prefix-stem-suffix. Stems
may not be empty, while affixes are optional.
Example: ktb ? { <k,t,b>,< kt,b,?>,
<k,tb,?>, <?,k,tb>, <?,kt,b>,
<?,ktb,?> }.
3. Consults three lexicons (dictPrefixes, dict-
Stems, dictSuffixes) for ruling out impossi-
ble divisions. For example, <kt,b,?>, is
rejected since kt does not appear as a prefix
in dictPrefixes, while <k,tb,?> is accepted
since k appears in dictPrefixes, tb in dict-
Stems, and ? in dictSuffixes.
4. Consults three co-occurrence constraint lists
for further ruling out incompatible prefix-
stem combinations, stem-suffix combinations,
and prefix-suffix combinations. For example,
2In reality, these are often conjoined prefixes treated as a
single entry within the system.
<k,tb,?>, while accepted in the previous
step, is now rejected because the file dict-
Prefixes lists k as a prefix belonging to class
NPref-Bi, and the stem tb belonging to one of
PV V, IV V, NF, PV C, or IV C. However,
the compatibility file tableAB does not permit
a combination of prefix class NPref-Bi and any
of the above-mentioned stem classes.
5. In the event that the lookup fails, the analyzer
considers various alternative spellings of the in-
put word, and runs through the same steps us-
ing the alternate spellings.
The BAMA lookup process is illustrated using a
different example in figure 1.
3 Conversion
Our goal in the conversion of the Perl-code and the
lookup tables is to produce a single transducer that
maps input words directly to their morphological
analysis, including class and gloss information. In
order to do this, we break the process down into
three major steps:
(1) We construct a transducer Lexicon that ac-
cepts on its output side strings consisting of
any combinations of fully vocalized prefixes,
stems, and suffixes listed in dictPrefixes, dict-
Stems, and dictSuffixes. On the input side,
we find a string that represents the class each
morpheme on the output side corresponds to,
as well as the line number in the correspond-
71
LEXICON Root
Prefixes ;
LEXICON Prefixes
[Pref-%0]{P%:34}:0 Stems;
[Pref-Wa]{P%:37}:wa Stems;
...
LEXICON Stems
[Nprop]{S%:23}:|b Suffixes;
[Nprop]{S%:27}:%<ib? Suffixes;
...
LEXICON Suffixes
[Suff-%0]{X%:34}:0 #;
[CVSuff-o]{X%:37}:o #;
...
Figure 2: Skeleton of basic lexicon transducer in LEXC
generated from BAMA lexicons.
ing file where the morpheme appears. For ex-
ample, the Lexicon transducer would contain
the mapping:
[Pref-0]{P:34}[PV]{S:102658}[NSuff-a]{X:72}
kataba
indicating that for the surface form
kataba/ I.
J?, the prefix class is Pref-0
appearing on line 34 in the file dictPrefixes,
the stem class is PV, appearing on line
102,658 in dictStems, and that the suffix
class is NSuff-a, appearing on line 72 in
dictSuffixes.
To construct the Lexicon, we produced a
Perl-script that reads the contents of the BAMA
files and automatically constructs a LEXC-
format file (Beesley and Karttunen, 2003),
which is compiled with foma into a finite trans-
ducer (see figure 2).
(2) We construct rule transducers that filter out im-
possible combinations of prefix classes based
on the data in the constraint tables tableAB,
tableBC, and tableAC. We then iteratively
compose the Lexicon transducer with each
rule transducer. This is achieved by converting
each suffix class mentioned in each of the class
files to a constraint rule, which is compiled
into a finite automaton. For example, the file
tableBC, which lists co-occurrence constraints
between stems and suffixes contains only the
following lines beginning with Nhy:
Nhy NSuff-h
Nhy NSuff-iy
indicating that the Nhy-class only combines
with Nsuff-h or Nsuff-iy. These lines are
converted by our script into the constraint re-
striction regular expression:
def Rule193 "[Nhy]" => _ ?*
"[NSuff-h]"|"[NSuff-iy]"];
This in effect defines the language where each
instance [Nhy] is always followed some-
time later in the string by either [NSuff-h],
or [NSuff-iy]. By composing this, and
the other constraints, with the Lexicon-
transducer, we can filter out all illegitimate
combinations of morphemes as dictated by the
original Buckwalter files, by calculating:
def Grammar Lexicon.i .o.
Rule1 .o.
...
RuleNNN ;
In this step, it is crucial to note that one cannot
in practice build a separate, single transducer
(or automaton) that models the intersection of
all the lexicon constraints, i.e. Rule1 .o.
Rule2 .o. ... RuleNNN, and then
compose that transducer with the Lexicon
transducer. The reason for this is that the
size of the intersection of all co-occurrence
rules grows exponentially with each rule. To
avoid this intermediate exponential size, the
Lexicon transducer must be composed with
the first rule, whose composition is then com-
posed with the second rule, etc., as above.
(3) As the previous two steps leave us with a trans-
ducer that accepts only legitimate combina-
tions of fully vocalized prefixes, stems, and
suffixes, we proceed to optionally remove short
vowel diacritics as well as perform optional
normalization of the letter Alif ( @) from the
72
output side of the transducer. This means,
for instance, that an intermediate kataba/ I.
J?,
would be mapped to the surface forms kataba,
katab, katba, katb, ktaba, ktab, ktba, and
ktb. This last step assures that we can
parse partially vocalized forms, fully vocal-
ized forms, completely unvocalized forms, and
common variants of Alif.
def RemoveShortVowels
[a|u|i|o|%?|%?] (->) 0;
def NormalizeAlif
["|"|"<"|">"] (->) A .o.
"{" (->) [A|"<"] ;
def RemovefatHatAn [F|K|N] -> 0;
def BAMA 0 <- %{|%} .o.
Grammar .o.
RemoveShortVowels .o.
NormalizeAlif .o.
RemovefatHatAn;
4 Results
Converting the entire BAMA grammar as described
above produces a final FST of 855,267 states and
1,907,978 arcs, which accepts 14,563,985,397 Ara-
bic surface forms. The transducer occupies 8.5Mb.
An optional auxiliary transducer for mapping line
numbers to complete long glosses and class names
occupies an additional 10.5 Mb. This is slightly
more than the original BAMA files which occupy
4.0Mb. However, having a FST representation of
the grammar provides us with a number of advan-
tages not available in the original BAMA, some of
which we will briefly discuss.
4.1 Orthographical variants
The original BAMA deals with spelling variants and
substandard spelling by performing Perl-regex re-
placements to the input string if lookup fails. In the
BAMA documentation, we find replacements such
as:
- word final Y? should be y?
- word final Y? should be }
- word final y? should be }
In a finite-state system, once the grammar is con-
verted, we can easily build such search heuristics
into the FST itself using phonological replacement
rules and various composition strategies such as pri-
ority union (Kaplan, 1987). We can thus mimic the
behavior of the BAMA, albeit without incurring any
extra lookup time.
4.2 Vocalization
As noted above, by constructing the analyzer from
the fully vocalized forms and then optionally remov-
ing vowels in surface variants allows us to more ac-
curately parse partially vocalized Arabic forms. We
thus rectify one of the drawbacks of the original
BAMA, which makes no use of vocalization informa-
tion even when it is provided. For example, given an
input word qabol, BAMA would as a first step strip
off all the vocalization marks, producing qbl. Dur-
ing the parsing process, BAMA could then match qbl
with, for instance, qibal, an entirely different word,
even though vowels were indicated. The FST de-
sign addresses this problem elegantly: if the input
word is qabol, it will never match qibal because the
vocalized morphemes are used throughout the con-
struction of the FST and only optionally removed
from the surface forms, whereas BAMA used the un-
vocalized forms to match input. This behavior is in
line with other finite-state implementations of Ara-
bic, such as Beesley (1996), where diacritics, if they
happen to be present, are taken advantage of in order
to disambiguate and rule out illegitimate parses.
This is of practical importance when parsing Ara-
bic as writers often partially disambiguate words
depending on context. For example, the word
Hsbt/ I.?kis ambiguous (Hasabat = compute,
charge; Hasibat = regard, consider). One would
partially vocalize Hsbt as Hsibt to denote ?she
regards?, or as Hsabt to imply ?she computes.?
The FST-based system correctly narrows down the
parses accordingly, while BAMA would produce all
ambiguities regardless of the vocalization in the in-
put.
4.3 Surface lexicon extraction
Having the BAMA represented as a FST also al-
lows us to extract the output projection of the gram-
mar, producing an automaton that only accepts le-
gitimate words in Arabic. This can be then be
used in spell checking applications, for example,
by integrating the lexicon with weighted transduc-
73
ers reflecting frequency information and error mod-
els (Hulden, 2009a; Pirinen et al, 2010).
4.4 Constraint analysis
Interestingly, the BAMA itself contains a vast
amount of redundant information in the co-
occurrence constraints. That is, some suffix-stem-
lexicon constraints are entirely subsumed by other
constraints and could be removed without affecting
the overall system. This can be observed during the
chain of composition of the various transducers rep-
resenting lexicon constraints. If a constraint X fails
to remove any words from the lexicon?something
that can be ascertained by noting that the number
of paths through the new transducer is the same as
in the transducer before composition?it is an indi-
cation that a previous constraint Y has already sub-
sumed X . In short, the constraint X is redundant.
The original grammar cannot be consistently ana-
lyzed for redundancies as it stands. However, redun-
dant constraints can be detected when compiling the
Lexicon FST together with the set of rules, offer-
ing a way to streamline the original grammar.
5 Conclusion
We have shown a method for converting the table-
based and producedural constraint-driven Buckwal-
ter Arabic Morphological Analyzer into an equiva-
lent finite-state transducer. By doing so, we can take
advantage of established finite-state methods to pro-
vide faster and more flexible parsing and also use the
finite-state calculus to produce derivative applica-
tions that were not possible using the original table-
driven Perl parser, such as spell checkers, normaliz-
ers, etc. The finite-state transducer implementation
also allows us to parse words with any vocalization
without sacrificing accuracy.
While the conversion method in this case is spe-
cific to the BAMA, the general principle illustrated
in this paper can be applied to many other procedu-
ral morphologies that rule out morphological parses
by first consulting a base lexicon and subsequently
applying a batch of serial or parallel constraints over
affix occurrence.
References
Attia, M., Pecina, P., Toral, A., Tounsi, L., and
van Genabith, J. (2011). An open-source finite
state morphological transducer for modern stan-
dard Arabic. In Proceedings of the 9th Interna-
tional Workshop on Finite State Methods and Nat-
ural Language Processing, pages 125?133. Asso-
ciation for Computational Linguistics.
Beesley, K. R. (1996). Arabic finite-state morpho-
logical analysis and generation. In Proceedings
of COLING?96?Volume 1, pages 89?94. Associ-
ation for Computational Linguistics.
Beesley, K. R. and Karttunen, L. (2003). Finite State
Morphology. CSLI Publications, Stanford, CA.
Buckwalter, T. (2004). Arabic Morphological Ana-
lyzer 2.0. Linguistics Data Consortium (LDC).
Habash, N. (2010). Introduction to Arabic natural
language processing. Synthesis Lectures on Hu-
man Language Technologies.
Hala?csy, P., Kornai, A., Ne?meth, L., Rung, A., Sza-
kada?t, I., and Tro?n, V. (2004). Creating open lan-
guage resources for Hungarian. In Proceedings of
Language Resources and Evaluation Conference
(LREC04). European Language Resources Asso-
ciation.
Hulden, M. (2009a). Fast approximate string match-
ing with finite automata. Procesamiento del
lenguaje natural, 43:57?64.
Hulden, M. (2009b). Foma: a finite-state compiler
and library. In Proceedings of the 12th confer-
ence of the European Chapter of the Association
for Computational Linguistics,, pages 29?32. As-
sociation for Computational Linguistics.
Kaplan, R. M. (1987). Three seductions of computa-
tional psycholinguistics. In Whitelock, P., Wood,
M. M., Somers, H. L., Johnson, R., and Bennett,
P., editors, Linguistic Theory and Computer Ap-
plications, London. Academic Press.
Pirinen, T., Linde?n, K., et al (2010). Finite-state
spell-checking with weighted language and error
models. In Proceedings of LREC 2010 Workshop
on creation and use of basic lexical resources for
less-resourced languages.
74
Proceedings of the 14th European Workshop on Natural Language Generation, pages 162?166,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
POS-tag based poetry generation with WordNet
Manex Agirrezabal, Bertol Arrieta, Aitzol Astigarraga
University of the Basque Country (UPV/EHU)
IXA NLP Group
Dept. of Computer Science
20018 Donostia
sgpagzam@ehu.es
bertol@ehu.es
aitzol.astigarraga@ehu.es
Mans Hulden
University of Helsinki
Department of Modern Languages
Helsinki, Finland
mhulden@email.arizona.edu
Abstract
In this paper we present the preliminary work of
a Basque poetry generation system. Basically,
we have extracted the POS-tag sequences from
some verse corpora and calculated the probabil-
ity of each sequence. For the generation process
we have defined 3 different experiments: Based
on a strophe from the corpora, we (a) replace
each word with other according to its POS-tag
and suffixes, (b) replace each noun and adjective
with another equally inflected word and (c) re-
place only nouns with semantically related ones
(inflected). Finally we evaluate those strategies
using a Turing Test-like evaluation.
1 Introduction
Poetry generation is one of the dream tasks of Natural
Language Processing (NLP). In this text we point out
an approach to generate Basque strophes automatically
using some corpora, morphological information and
a lexical database. The presented method is not tied
to a specific language, but it is especially suitable for
inflected languages, as the POS information used in
some tasks with success in non inflected languages is
not enough for inflected ones. We have used the POS-
tags with their inflectional information to learn usual
structures in Basque poetry.
This work is part of a more general and complete
project, called BertsoBOT (Astigarraga et al, 2013).
BertsoBOT is a robot capable of creating and singing
Basque verses automatically. The robot joins together
in a single system techniques from robotics, NLP and
speech synthesis and recognition. The work presented
in this paper comes to improve the generation module
of the mentioned system.
Although our intention is to create whole verses, in
this paper we present the first steps towards it: the cre-
ation of strophes. Additionally, Basque verses have
to rhyme, but in these first experiments we have not
considered it.
Basque language
Basque language is spoken along the Basque Country1
by approximately 700.000 people. Although there is a
standardized form of the language, it is common the use
of non-standard dialects in certain regions, mainly in
spoken language.
Basque is a morphologically rich language, which is
an obvious feature if we analyze the multiple declension
cases2 that can be usedwith only oneword. For example,
the phrase ?with the friends? can be expressed with only
one word, ?lagunekin?.
lagunekin = lagun (friend) + ak (plural determiner) +
kin (with)
Art of bertsolaritza
The art of impromptu verse-making, bertsolaritza, is
very ingrained in the BasqueCountry. The performances
of verse-makers are quite usual and a big championship
is held every four years which congregates 15.000 peo-
ple, approximately. One tipical work to do for the verse-
makers is to sing verses extempore, given a topic. The
particularity of these verses is that they have to follow
strict constraints of meter and rhyme. In the case of
a metric structure of verses known as ?zortziko txikia?
1http://en.wikipedia.org/wiki/Basque Country (greater region)
2en.wikipedia.org/wiki/Basque grammar#Declension
162
(small of eight), the poem must have eight lines. The
union of each odd line with the next even line, form a
strophe. Each strophe, has a small structure3 and must
rhyme with the others. Below, you can see an example
of a verse, with lauko txikia4 stanza:
Neurriz eta errimaz With meter and rhyme
kantatzea hitza, to sing the word
horra hor ze kirol mota bertsolaritza is
den bertsolaritza. that kind of sport
2 State of the art
A good review of computer guided poetry can be found
in (Gerva?s, 2010). Most relevant ones include:
WASP
The WASP system (Gerva?s, 2000) can be considered
one of first serious attempts to build an automatic poetry
generator system. It is based on the generate-and-test
paradigm of problem solving. Simple solutions are
generated and then coupled with an evaluation function
for metric constraints, producing acceptable results.
ASPERA
ASPERA (Gerva?s, 2001) is a case-based reasoning
(CBR) system for poetry generation. It generates poetry
based on the information provided by the user: a prose
description of the intended message, a specific stanza
for the final poem, a set of verse examples on that stanza,
and a group of words that the final poem must contain.
The system was implemented using CLIPS rule-
based system, and follows the four typical CBR steps:
Retrieval, Reuse, Revise and Retain.
POEVOLVE
Levy (Levy, 2001) went on to develop an evolution-
ary model of poetry generation. POEVOLVE creates
limericks taking as a reference the human way of poetry
writing. The POEVOLVE system works as follows:
an initial population is created from a group of words
that include phonetic and stress information. Rhymes
that meet the requirements are selected and then more
words are selected to fill the rest of the verse-line
based on their stress information. A genetic algorithm
is employed to modify the words that compose the
313 syllables with a caesura after the 7th syllable
4Lauko txikia: The same as zortziko txikia but with four lines,
instead of eight.
limerick. Evaluation is performed by a neural network
trained on human judgements. It must be said that this
system does not take syntax and semantics into account.
McGonnagall
Manurung presented also an evolutionary approach
to generate poetry (Manurung, 2003). The poem gen-
eration process is formulated as a state space search
problem using stochastic hill-climbing. The overall pro-
cess is divided in two steps: evaluation and evolution.
During the evaluation phase, a group of individuals is
formed based on initial information, target semantics
and target phonetics. This group of initial individuals
is then evaluated taking into account different aspects
such as phonetics, semantics and surface form. Each
individual receives a score, and in the evolution step, the
subset with higher scores is selected for reproduction.
The resulting mutated individuals derive, hopefully, in
better versions of the poem.
3 Creating strophes
Our goal is to create Basque strophes automatically. But
strophes written by combining words randomly usually
do not have any sense. For words have any meaning
when combined together, they must be organized fol-
lowing particular patterns. Towards this end we have
applied and tested different methodologies. We use a
morphological analyzer to extract POS and inflection
patterns in strophes, and to create new ones following
those schemes. The idea is to find the most commonly
used patterns so that we can use them in new strophes.
We also improve the results taking semantics into ac-
count. In the next lines we are going to describe some
resources we have used.
3.1 Corpora
For the learning process of the usual POS-tag patterns
we have employed some Basque verse corpora yielded
by theAssociation of the Friends of Bertsolaritza5 (AFB).
Those are impromptu verses sung by Basque verse-
makers and the transcriptions of this collection have
been done by members of the information center6 of the
AFB.
For this work, we are going to exploit three corpora,
5http://www.bertsozale.com/en
6http://bdb.bertsozale.com/en/orriak/get/7-xenpelar-
dokumentazio-zentroa
163
each one following a classic stanza in Basque verses: (a)
small stanza, (b) big stanza and (c) habanera.
a) Small stanza
This corpus has approximately 10.000 lines. Each
line of this corpus is composed by a strophe containing
13 syllables with a caesura between the 7th and the 8th
syllable. This stanza is used to sing sprightly verses
composed by compact ideas.
b) Big stanza
In this case, this corpus has about 8.000 lines and
each line has 18 syllables with a caesura after the 10th
syllable. Depending on the chosen melody, this stanza
can also have a complementary pause in the 5th syllable.
The topics of this type of verses tend to be more epic or
dramatic.
c) Habanera
This corpus has just about 1000 lines and they are
composed by 16-syllable lines with a caesura after the
8th syllable. It is commonly used when the verse-maker
has to compose a verse alone about a topic.
3.2 POS sequence extraction
To extract the POS-tags, we use a Basque analyzer de-
veloped by members of IXA NLP group (Aduriz et al,
2004), which involve phrasal morphologic analysis and
disambiguation, among other matters.
Once calculated the POS-tags, we estimated the most
probable POS sequences using POS-tag ngrams. We did
this in order to know which POS-tag sequence would
better fit for each stanza. For example, an acceptable
POS-tag sequence in the small stanza corpus would be
?NN-NN-JJ-VB?. This pattern could be extracted from
this strophe, which is correct.
Mirenekin+NN zakurra+NN zoriontsua+JJ da+VB.
(With Miren)+NN (the dog)+NN is+VB happy+JJ.
But to have the POS-tag pattern is not enough for a
good generation.
Special issues in the categorization of words in
Basque
The gist is that Basque is an agglutinative language,
so there is plenty information included in the suffixes
of the words. Because of that, if we don?t retain any
information about suffixes, we would lose some impor-
tant data. In Basque, we can apply declension to nouns,
pronouns, adjectives and determiners. Therefore, we
need to save the declension case information to do a
correct generation. When a set of words compound a
noun phrase, only one of the words will be inflected.
Some verbs, when they are part of a subourdinate
clause, can also be inflected. In these cases, we have to
extract the suffixes of the verb of that clause, because it
expresses the type of clause.
All this information is essential if we do not want to
lose the meaning of the clause. Below, you can see an
example of generation of strophes in Basque using only
POS-tags:
Mirenekin+NN lagunekin+NN zoriontsua+JJ da+VB.
(With Miren)+NN (with the friends)+NN is+VB happy+JJ.
As you can see, the phrase ?with Miren with the
friends is happy? is not grammatically correct. Storing
the declension information, that creation would not be
allowed and one of the clauses created by the system
could be:
Mirenekin+NN COM mahaia+NN ABS zoriontsua+JJ ABS da+VB.
(With Miren)+NN COM (the desk)+NN ABS is+VB happy+JJ ABS.
The addition of the declension information will avoid
some grammatical errors in the generation process. But
when the changed element is a verb, the system can
insert one that does not follow the same subcategoriza-
tion7, which will lead us to a grammatical error too.
So, changing the verb without more information can be
uncertain.
3.3 Semantic information
On the other hand, if we take a look at the last example,
it is not correct to say that the desk is happy. To avoid
these cases, we posed the use of the Basque WordNet
(Fellbaum, 2010) (Pociello et al, 2011). We used it to
change words with related ones.
3.4 Morphological generation
Finally, it is important the fact that Basque is an inflected
language. So, we need to have a morphological gener-
ator (Alegria et al, 2010) to create the corresponding
inflected forms of the words. This generator is based
on the Basque morphology description (Alegria et al,
1996).
4 Experiments
In this work, we have performed a set of experiments
to analyze different strategies for the generation of stro-
7The subcategorization indicates the syntactic arguments re-
quired or allowed in some lexical items (usually verbs).
164
phes in Basque. In the following lines, we explain the
ameliorations we get in each experiment.
The first experiment creates strophes by inserting
words that are consistent with each POS-tag and its
inflection information. We first get some of the most
common POS-tag sequences and for each POS-tag se-
quence the application returns two strophes. The first
strophe uses words from the same verse corpus to make
substitutions. The second one uses words from the
EPEC corpus (Aduriz et al, 2006).
The second experiment creates clauses, but chang-
ing only the nouns and adjectives from original strophes
from the corpus. We mantain the inflection information.
In this experiment we also get two strophes for each pat-
tern sequence, as in the previous attempt (verse corpus
and EPEC corpus). With this constraint we avoid the
creation of incorrect strophes because of the problem of
subcategorization (explained in section 3.2).
The third experiment makes small changes in the
original strophes (from the corpus), as it only replaces
each noun for a semantically related noun. The related
noun can be: (a) Antonym of the original word or (b)
hyponym of the hypernyms of the original word. In
order of preference, first we try to change each name
with one of its antonyms. If there is no antonym, then
we try to get the hypernyms of the word to return their
hyponims. Once the new word has been found, we
add the needed suffixes (the same ones that had the
words from the corpus) in order to fit correctly in the
strophe, using the morphological generator. The change
of words with related ones gives us the chance to express
semantically similar sentences using different words.
5 Evaluation
Once the experiments were finished, we made an evalu-
ation in order to analyze the quality of the automatically
generated strophes. The evaluation of computer gener-
ated poetry is nowadays fuzzy, so we defined a Turing
Test-like evaluation. We contacted two linguists that had
not done any work on this project, so that the evaluation
be as objective as possible. We prepared 135 strophes
interleaving some created by the machine with others
from the corpus. We asked the evaluators to guess if the
strophe was done by the machine or by a human. We
only draw conclusions using machine-generated stro-
phes, as we want to know how many of them percolate
as human-generated ones. In the next table you can
see the rate of sentences created by the machine and
suposed to be done by humans:
EXPERIMENT
Evaluator 1 1 2 3
Percolated as human 0.033 0.259 0.75
Evaluator 2
Percolated as human 0.333 0.481 0.75
As you can see, according to Evaluator 1, the first
experiment was not very worthy, as the only 3.3% of
the machine generated strophes percolated as human
generated ones. The second experiment got better re-
sults, and the 26% of the strophes were thought to be
human generated ones. As expected, the strophes of
the third experiment are the most trustworthy ones. The
results given by the second evaluator are higher, but the
important fact is the increase of the progression over the
experiments.
6 Discussion & Future Work
In this paper we have presented a set of experiments
for the automatic generation of poetry using POS and
inflectional tag patterns and some semantics. In the
last section we show the Turing Test-like evaluation to
measure the reliability of each experiment. This will be
part of a whole poetry analysis and generation system.
In the future, we intend to change verbs from stro-
phes controlling the subcategorization of them in order
to enable the creation of well-formed strophes about a
constrained topic. Also, we plan to use a frame seman-
tics resource, such as FrameNet, and after creating a
strophe, make some modifications to get an acceptable
semantic meaning.
165
References
Aduriz, I., Aranzabe, M., Arriola, J., de Ilarraza, A.,
Gojenola, K., Oronoz, M., and Uria, L. (2004). A cas-
caded syntactic analyser for Basque. Computational
Linguistics and Intelligent Text Processing, pages 124?
134.
Aduriz, I., Aranzabe, M. J., Arriola, J. M., Atutxa, A.,
de Ilarraza, D. A., Ezeiza, N., Gojenola, K., Oronoz,
M., Soroa, A., and Urizar, R. (2006). Methodology
and steps towards the construction of EPEC, a corpus
of written Basque tagged at morphological and syn-
tactic levels for automatic processing. Language and
Computers, 56(1):1?15.
Alegria, I., Artola, X., Sarasola, K., and Urkia, M.
(1996). Automatic morphological analysis of Basque.
Literary and Linguistic Computing, 11(4):193?203.
Alegria, I., Etxeberria, I., Hulden, M., and Maritxalar,
M. (2010). Porting Basque morphological grammars
to foma, an open-source tool. Finite-State Methods
and Natural Language Processing, pages 105?113.
Astigarraga, A., Agirrezabal, M., Lazkano, E., Jauregi,
E., and Sierra, B. (2013). Bertsobot: the first min-
strel robot. 6th International Conference on Human
System Interaction, Gdansk.
Fellbaum, C. (2010). WordNet. Springer.
Gerva?s, P. (2000). Wasp: Evaluation of different strate-
gies for the automatic generation of Spanish verse. In
Proceedings of the AISB-00 Symposium on Creative
& Cultural Aspects of AI, pages 93?100.
Gerva?s, P. (2001). An expert system for the composition
of formal spanish poetry. Knowledge-Based Systems,
14(3):181?188.
Gerva?s, P. (2010). Engineering linguistic creativity: Bird
flight and jet planes. In Proceedings of the NAACL
HLT 2010 Second Workshop on Computational Ap-
proaches to Linguistic Creativity, pages 23?30. Asso-
ciation for Computational Linguistics.
Levy, R. P. (2001). A computational model of poetic
creativity with neural network as measure of adaptive
fitness. In Proccedings of the ICCBR-01 Workshop
on Creative Systems. Citeseer.
Manurung, R. (2003). An evolutionary algorithm ap-
proach to poetry generation. PhD thesis, School of
informatics, University of Edinburgh.
Pociello, E., Agirre, E., and Aldezabal, I. (2011).
Methodology and construction of the Basque Word-
net. Language resources and evaluation, 45(2):121?
142.
166
Proceedings of the 2014 Joint Meeting of SIGMORPHON and SIGFSM, pages 29?36,
Baltimore, Maryland USA, June 27 2014.
c
?2014 Association for Computational Linguistics
Generalizing inflection tables into paradigms with finite state operations
Mans Hulden
University of Helsinki
mans.hulden@helsinki.fi
Abstract
Extracting and performing an alignment
of the longest common subsequence in in-
flection tables has been shown to be a
fruitful approach to supervised learning
of morphological paradigms. However,
finding the longest subsequence common
to multiple strings is well known to be
an intractable problem. Additional con-
straints on the solution sought complicate
the problem further?such as requiring
that the particular subsequence extracted,
if there is ambiguity, be one that is best
alignable in an inflection table. In this pa-
per we present and discuss the design of a
tool that performs the extraction through
some advanced techniques in finite state
calculus and does so efficiently enough for
the practical purposes of inflection table
generalization.
1 Introduction
Supervised learning of morphological paradigms
from inflection tables has recently been ap-
proached from a number of directions. One ap-
proach is given in Hulden et al. (2014), where
morphological paradigm induction is performed
by extracting the longest common subsequence
(LCS) from a set of words representing an in-
flection table. Although that work presents en-
couraging results as regards learning morphologi-
cal paradigms from inflection tables, no details are
given as to how the paradigms themselves are ex-
tracted. The purpose of this paper is to describe
how such a paradigm extraction procedure can be
performed using only finite state operations.
Extracting the longest common subsequence
from a large number of strings is known as the
multiple longest common subsequence problem
(MLCS), and is computationally intractable. In
fields like bioinformatics specialized heuristic al-
gorithms have been developed for efficiently ex-
tracting common subsequences from DNA se-
quences. In linguistics applications where the goal
is to extract common patterns in an inflection ta-
ble, however, the problem manifests itself in a
different guise. While most applications in other
fields work with a small number of fairly long se-
quences, inflection tables may contain hundreds of
short sequences. Additionally, it is not enough to
extract the LCS from an inflection table. The LCS
itself is often ambiguous and may be factorized in
several different ways in a table. This means that
we operate under the additional constraint that the
LCS must not only be found, but, in case of ambi-
guity, its most contiguous factorization must also
be indicated, as this often produces linguistically
interesting generalizations.
In this paper we will address the problem of
extracting the minimal MLCS through entirely fi-
nite state means. Finite state methods lend them-
selves to solving this kind of an optimization prob-
lem concisely, and, as it turns out, also efficiently
enough for practical purposes.
This paper is laid out as follows. First, we
outline the MLCS-based approach to supervised
learning of morphological paradigms in section
2. We then describe in broad strokes the algo-
rithm required for generalizing inflection tables
into paradigms in section 3. Next, we give a finite
state implementation of the algorithm in section
4, followed by a brief discussion of a stand-alone
software tool based on this that extracts paradigms
from collections of inflection tables in section 5.
2 Supervised learning of morphological
paradigms
In the following, we operate with the central idea
of a model of word formation that organizes word
forms and their inflection patterns into paradigms
(Hockett, 1954; Robins, 1959; Matthews, 1972;
29
Stump, 2001). In particular, we model paradigms
in a slightly more abstract manner than is custom-
arily done. For the purposes of this paper, we dif-
ferentiate between a paradigm and an inflection
table in the following way: an inflection table is
simply a list of words that represents a concrete
manifestation, or instantiation, of a paradigm. A
paradigm is also a list of words, but with spe-
cial symbols that represent variables interspersed.
These variables, when instantiated, represent par-
ticular strings shared across an inflection table.
In our representation, this kind of an abstract
paradigm is an ordered collection of strings,
where each string may additionally contain in-
terspersed variables denoted x
1
, x
2
, . . . , x
n
. The
strings represent fixed, obligatory parts of a
paradigm, while the variables represent mutable
parts. A complete abstract paradigm captures
some generalization where the mutable parts rep-
resented by variables are instantiated the same
way for all forms in one particular inflection table.
For example, the fairly simple paradigm
x
1
x
1
+s x
1
+ed x
1
+ing
could represent a set of English verb forms, where
x
1
in this case would coincide with the infinitive
form of the verb?walk, climb, look, etc.
1
2.1 Learning paradigms from inflection
tables
As is seen from the above example, a general
enough paradigm can encode the inflection pat-
tern of a large number of words. When learning
such paradigms from data?i.e. complete inflec-
tion tables?we intuitively want to find the ?com-
mon? elements of a table and generalize those.
The core of the method is to factor the word
forms in an inflection table in such a manner that
the elements common to all entries are declared
variables, while the non-common elements are as-
sumed to be part of the inflection pattern. To illus-
trate the idea with an example, consider a short-
ened inflection table for the regular German verb
holen (to fetch):
2
1
Our formalization of a paradigm of strings and interven-
ing variables bears many similarities to so-called pattern lan-
guages (Angluin, 1980). In fact, each entry in a paradigm
could be considered a separate pattern language. Addition-
ally, all the individual pattern languages in one paradigm are
constrained to share the same variables and the variables are
constrained to collectively be instantiated the same way.
2
We follow the convention that entries in an inflection ta-
ble are separated by #.
hole#holst#holt#holen#holt#holen#geholt (1)
Obviously, in this example, the element com-
mon to each entry in the inflection table is hol.
Declaring hol to be a variable, we can rewrite the
inflection table as:
x
1
+e#x
1
+st#x
1
+t#x
1
+en#x
1
+t#x
1
+en#ge+x
1
+t (2)
This extraction of the ?common elements? is
formalized in Hulden et al. (2014) to be equivalent
to extraction of the longest common subsequence
of the strings w
1
, . . . , w
n
in an inflection table.
3
The purpose of extracting the common parts and
labeling them variables is to provide a model for
generalization of inflection patterns. Under the as-
sumption that a variable x
i
in this paradigm rep-
resentation corresponds to a nonempty string, we
can instantiate an inflection table by simply pro-
viding the variable strings x
1
, . . . , x
n
. Thus, we
can talk about a paradigm-generating function
f : (x
1
, . . . , x
n
)? ?
?
that maps instantiations of variables to a string rep-
resenting the complete inflection table, in this case
a string where entries are #-separated.
To illustrate this, consider the simple paradigm
in (2). It implicitly defines a function f where, for
example, f(kauf) maps to the string
kaufe#kaufst#kauft#kaufen#kauft#kaufen#gekauft (3)
i.e. produces the inflection table for the regular
verb kaufen (to buy), which behaves like holen.
Likewise, we can also consider the inverse func-
tion. Given an unknown word form, e.g. macht
(to make, 3pSg), we can see that the only way it
fits the paradigm in (2) is if it comes from an in-
flection table:
mache#machst#macht#machen#macht#machen#gemacht
(4)
that is, if macht is part of the output for f (mach).
3
Not to be confused with the longest common substring,
which is a different problem, solvable in polynomial time
for n strings. Subsequences may be discontinuous while
substrings may not. For example, assume s = abcaa and
t = dbcadaa. The longest common substring shared by the
two is bca obtained from s by abcaa and t by dbcadaa. By
contrast, the longest common subsequence is bcaa, obtained
from s by abcaa and t by dbcadaa or dbcadaa or dbcadaa.
30
ring
rang
rung
[r]i[ng]
[r]a[ng]
[r]u[ng]
rng
?	Extract     LCS ?	Fit LCS      to table ?	Generalize     to paradigmsInput:inflectiontables
swim
swam
swum
swm
[sw]i[m]
[sw]a[m]
[sw]u[m]
x
1
+i+x
2
x
1
+a+x
2
x
1
+u+x
2
x
1
+i+x
2
x
1
+a+x
2
x
1
+u+x
2
?	Collapse     paradigms
x
1
+i+x
2
x
1
+a+x
2
x
1
+u+x
2
}} }}
Figure 1: Paradigm extraction strategy.
In other words, the extraction of multiple com-
mon longest subsequences (MLCS) from inflec-
tion tables immediately provides a (simple) gener-
alization mechanism of a grammar, and also sug-
gests a supervised learning strategy for morpho-
logical paradigms. In conjunction with statistical
machine learning methods, Hulden et al. (2014)
has shown that the paradigm extraction and gen-
eralization method provides competitive results
in various supervised and semi-supervised NLP
learning tasks. One such task is to provide a hy-
pothetical reconstruction of a complete inflection
table from an unseen base form after first witness-
ing a number of complete inflection tables. An-
other task is the semi-supervised collection of lex-
ical entries and matching them to paradigms by
observing distributions of word forms across all
the possible paradigms they can fit into. In gen-
eral, there is much current interest in similar tasks
in NLP; see e.g. Dreyer and Eisner (2011); Dur-
rett and DeNero (2013); Eskander et al. (2013) for
a variety of current methods.
3 Learning method
The basic procedure as outlined by Hulden et al.
(2014) for learning paradigms from inflection ta-
bles can be represented by the four-step procedure
given in figure 1. Here, multiple inflection tables
are gathered, and the LCS to each table is found
individually. Following that, the LCS is fit into
the table, and contiguous segments that participate
in the LCS are labeled variables. After paradigm
generalization, it may turn out that several identi-
cal paradigms have been learned, which may then
be collapsed.
The first two steps of the method dictate that
one:
1. Extract the longest common subsequence
(LCS) to all the entries in the inflection table.
2. Split the LCS(s)?of which there may be
several?into variables in such a way that the
number of variables is minimized. Two seg-
ments xy are always part of the same variable
if they occur together in every form of an in-
flection table. If some substring z intervenes
between x and y in some form, x and y must
be assigned separate variables.
These steps represent steps ? and ? in figure
1. After the variables have been identified, steps
? and ? in the figure are easily accomplished by
non-finite-state means.
In the following, we will focus on the previ-
ously unaddressed problem of finding the LCS of
an inflection table (?), and of distributing possible
variables corresponding to contiguous sequences
of the LCS in a way that gives rise to the mini-
mum number of variables (?).
4 Finite-state implementation
The main challenge in producing a paradigm from
an inflection table is not the extraction of the
longest common subsequences, but rather, doing
so with the added criterion of minimizing the num-
ber of variables used. Extracting the LCS from
multiple strings is known to be NP-hard (Maier,
1978) and naive implementations will fail quickly
for even a moderate number of strings found in in-
flection tables. While there exist specialized algo-
rithms that attempt to efficiently either calculate
(Irving and Fraser, 1992) or approximate (Wang
et al., 2010) the LCS, we find that extraction can
easily be accomplished with a simple transducer
calculation. The task of ascertaining that the LCS
is distributed in such a way as to minimize the
number of variables turns out to be more challeng-
ing; at the same time, however, it is a problem to
which the finite state calculus is particularly well
suited, as will be seen below.
4.1 Notation and tool
The paradigm extraction tool was implemented
with the help of the foma toolkit (Hulden, 2009).
In the actual implementation, instead of directly
compiling regular expressions, we make use of
foma?s programming API, but in the following we
give regular expression equivalents to the method
used. Table 1 contains a summary of the regular
expression notation used.
31
0 Empty string
? Any symbol in alphabet
.#. End or beginning of string
{xyz} String
AB Concatenation
A
*
, A+ Kleene star, Kleene plus
A|B Union
A & B Intersection
A - B Difference
?A Complement
A .o. B Composition
% Escape symbol
[ and ] Grouping brackets
A:B Cross product
T.2 Output projection of T
A -> B Rewrite A as B
eq(X,L,R) Strings between L,R are equal
def W {word} Define FSM constant
def F(X,Y) X Y Regular expression macro
Table 1: Regular expression notation in foma.
4.2 LCS extraction
As the first step, we assume that we have encoded
each word w
1
, . . . , w
n
in an inflection table as an
automaton that accepts that word.
4
In general, we can define the set of subse-
quences of any word by a general regular expres-
sion technique:
def SS(X) [X .o. [?|?:0]
*
].2;
SS(w) then contains all of the subsequences
of some word w. Taking advantage of this, we
may calculate the intersection of each set of sub-
sequences SS(w
1
) & ...& SS(w
n
), produc-
ing the language that contains all the common
subsequences to w
1
, . . . , w
n
. From this, extract-
ing the longest subsequence or sequences could in
principle be performed by inspecting the resulting
automaton, but the same can also be done alge-
braically for finite sets:
def Max(X) X -
[[X .o. [?:a]
*
[?:0]+].2 .o. [a:?]
*
].2;
Here, Max(X) is a regular expression tech-
nique of extracting the set of longest strings from
an automaton. We achieve this in practice by first
changing all symbols in X to an arbitrary sym-
bol (a in this case), removing at least one symbol
from the end, and using this intermediate result to
4
We abuse notation slightly by representing by w
i
both a
word and an automaton that accepts that word.
remove from X all strings shorter than the maxi-
mum.
5
An automaton that contains all LCSs for a set of
words w
1
, . . . , w
n
can thus be calculated as:
Max(SS(w
1
) & ... & SS(w
n
)) (5)
The above two lines together represent a
surprisingly efficient manner of calculating the
MLCS for a large number of relatively similar
short sequences (less than 100 characters) and
is essentially equivalent to performing the same
calculation through dynamic programming algo-
rithms with some additional search heuristics.
4.3 Minimizing variables
We can then assume that we have calculated the
LCS or LCSs for an inflection table and can rep-
resent it as an automaton. The following step is to
assign variables to segments that can correspond
to the LCS in a minimal way. The minimality re-
quirement is crucial for good generalization as is
seen in the illustration here:
comprar
compra
compro
{ x1
comprar
compra
compro
{
{
x1 x2(a) (b){ {x1 x2x1 {
The above shows two ways of breaking up the
LCS compr in the hypothetical three-word inflec-
tion table for Spanish. In case (a) the compr
has been located contiguously in inflection entries,
while in (b) there is a gap in the first form, leading
to the inevitable use of two variables to generalize
the table.
In the finite-state string encoding, the overall
intent of our effort to calculate the minimum-
variable MLCS assignment in the table is to
produce an automaton that contains the divi-
sions of variables marked up with brackets.
For example, given a hypothetical two-word ta-
ble holen#geholt, the LCS is obviously hol.
Now, there are several valid divisions of hol
into variables, e.g. [ho][l]en#ge[ho][l]t, which
would represent a two-variable division, while
5
This is a rather inefficient way of extracting the set of
longest strings from an automaton. However, as the runtime
of this part represents only a minute fraction of the complete
procedure, we do so to preserve the benefit of clarity that us-
ing finite-state calculus offers.
32
pextract example
1 def SS(X) [X .o. [?|?:0]
*
].2;
2 def Max(X) X - [[X .o. ?:a
*
?:0+].2 .o. a:?
*
].2;
3 def RedupN(X,Y) [_eq([LEFT X RIGHT [Y LEFT X RIGHT]
*
], LEFT, RIGHT) .o. LEFT|RIGHT -> 0].l;
4 def NOBR ? - %[ - %] - %#;
5 def Order(X) [[X .o. 0:%# ?
*
0:%# .o.
6 ?
*
%# [NOBR | %[:%< | %]:%>]
*
%# ?
*
.o.
7 %[|%] -> 0 .o.
8 [?
*
0:%> 0:%< \[%<|%>|%[|%] ]+ %> ?
*
]
*
.o.
9 %#:0 ?
*
%#:0 .o.
10 0 -> %[|%] .o. %< -> %[ .o. %> -> %]] .o. X ].2;
11 def MarkRoot(X) [X .o. [?|0:%[ ?+ 0:%]]
*
].2;
12 def RandomBracketing(X) [X .o. [? | 0:%[ NOBR
*
0:%]]
*
].2;
13 def AddExtraSegments(X) [X .o. [0:NOBR
*
| %[ \%]
*
%] | %#]
*
].2;
14 def Filter(X) X - Order(X);
15
16 def Table {hole#holst#holt#holen#holt#holen#geholt};
17 def MLCS Max(SS({hole}) & SS({holst}) & SS({holt}) & SS({holen}) & SS({holt}) & SS({holen}) & SS({geholt}));
18 def BracketedMLCS AddExtraSegments(RedupN(MarkRoot(MLCS), %#));
19 def BracketedTable RandomBracketing(Table);
20
21 regex Filter(BracketedMLCS & BracketedTable);
22 print words
Figure 2: Complete implementation of the extraction of the minimum-variable longest common subse-
quences as a foma-script. Here, a small German verb table is hard-coded for illustration purposes on
lines 16 and 17. The output is [hol]e#[hol]st#[hol]t#[hol]en#[hol]t#[hol]en#ge[hol]t
[hol]en#ge[hol]t would represent a one-variable
division.
Naturally, these brackets will have to be divided
in such a way that there is no better way to achieve
the division?i.e. no markup such that fewer vari-
ables are instantiated.
The crux of the method used here is to first pro-
duce an automaton that accepts the set of all valid
markups of the MLCS in the table string, and then
use that set to in turn define the set of suboptimal
markups. Similar finite-state techniques have been
used by Gerdemann and van Noord (2000); Eisner
(2002); Karttunen (2010); Gerdemann and Hulden
(2012), to, among other things, define suboptimal
candidates in Optimality Theory. The trick is to set
up a transducer T that contains the input-output
pair (x, x
?
), iff x
?
represents a worse division of
variables than x does. In effect, T captures the
transitive closure of an ordering relation  of the
various factorizations of the strings into variables,
and T contains the string pair (x, x
?
) when x 
+
x
?
. In general, supposing that we have an identity
transducer, i.e. automaton A, and a transducer T
that maps strings in A according to the transitive
closure of an ordering relation , then we can al-
ways remove the suboptimal strings according to
 from A by calculating A? range(A ? T ).
Apart from this central idea, some bookkeep-
ing is required because we are working with string
representations of inflection tables. A complete
foma listing that captures the behavior of our im-
plementation is given in figure 2. The main com-
plication in the program is to produce the transitive
closure of the ordering by setting up a transducer
Order that, given some bracketed string, breaks
up continuous sequences of brackets into disconti-
nuities, e.g. [xyz]? [x][yz],[xy][z], [x][y][z].
The main logic of the program appears on lines
18?21. The BracketedMLCS is the language
where the MLCS has been bracketed in various
ways and extra segments inserted arbitrarily. An
extra complication is that the MLCS must always
be bracketed the same way within a string, e.g.
[xy][z]#...#[xy][z], or [x][yz]#...#[x][yz] etc. That
is, the variable splits have to be equal across en-
tries.
The BracketedTable language is the lan-
guage that contains a string that represents the in-
flection table at hand, but with arbitrary bracket-
ings. The intersection of the two languages then
contain the valid MLCS bracketings of the inflec-
tion table. After the intersection is calculated, we
apply the ordering transducer and filter out those
strings with suboptimal bracket markup. Figure 3
illustrates the process.
4.4 Optimizations and additions
In addition to the description given above, the
actual implementation contains a number of sec-
ondary optimization strategies. The foremost one
is the simple preprocessing move to locate first
the longest common prefix p in the inflection ta-
ble before any processing is done. This can, of
course, be discovered very efficiently. The prefix
33
[ho][l]e#[ho][l]st#[ho][l]t#[ho][l]en#[ho][l]t#[ho][l]en#ge[ho][l]t[hol]e#[hol]st#[hol]t#[hol]en#[hol]t#[hol]en#ge[hol]t[h][o][l]e#[h][o][l]st#[h][o][l]t#[h][o][l]en#[h][o][l]t#[h][o][l]en#ge[h][o][l]t[h][ol]e#[h][ol]st#[h][ol]t#[h][ol]en#[h][ol]t#[h][ol]en#ge[h][ol]t
[hol]e#[hol]st#[hol]t#[hol]en#[hol]t#[hol]en#ge[hol]tFilter(BracketedMLCS & BracketedTable)
BracketedMLCS & BracketedTable
[h]ole#ho[ls][t]#holt#h[o]len#ho[lt]#[ho][le][n]#[ge]h[ol][t]h[o]le#h[ol][st]#[h]olt#holen#[hol]t#h[o][l]e[n]#g[eh]o[lt]hole#[h]ol[st]#[ho]l[t]#[h][o][len]#holt#hol[en]#[g][eh][o]l[t]
                             ...
BracketedTable X[ho]X[l]X#X[ho]X[l]X#X[ho]X[l]X#X[ho]X[l]X#X[ho]X[l]X#X[ho]X[l]X#X[ho]X[l]XX[hol]X#X[hol]X#X[hol]X#X[hol]X#X[hol]X#X[hol]X#X[hol]XX[h]X[ol]X#X[h]X[ol]X#X[h]X[ol]X#X[h]X[ol]X#X[h]X[ol]X#X[h]X[ol]X#X[h]X[ol]XX[h]X[o]X[l]X#X[h]X[o]X[l]X#X[h]X[o]X[l]X#X[h]X[o]X[l]X#X[h]X[o]X[l]X#X[h]X[o]X[l]XBracketedMLCS
holMLCS
Figure 3: Illustrated steps in the process of extracting and identifying the MLCS. The MLCS language
contains only the longest common subsequence(s). From that language, the language BracketedMLCS
is generated, which contains arbitrary strings with the MLCS bracketed in different ways (X here repre-
sents any string from ?
?
). Intersecting that language with the BracketedTable language and filtering
out suboptimal bracketings yields the final generalization.
can be set aside until the main algorithm is com-
pleted, and then attached as a separate variable to
the paradigm that was extracted without p. This
has little noticeable effect in most cases, but does
speed up the variable minimization with large ta-
bles that contains words more than 30 characters
long. Although not included in the implementa-
tion, the same maneuver can subsequently be per-
formed on the longest common suffix of the re-
maining string after the prefix is extracted.
Additionally, there are still residual cases
where the LCS may be located in several ways
with the same number of variables. An ac-
tual example comes from a Swedish paradigm
with two options: [sege]l#[seg]l[e]n#[seg]l[e]t vs.
[seg]e[l]#[segl]en#[segl]et. The ambiguity here
is due to the two equally long LCSs sege and
segl. These are resolved in our implementation
through non-finite-state means by choosing the di-
vision that results in the smallest number of infix-
segments.
5 Implementation
We have implemented the above paradigm ex-
tractor as a freely available stand-alone tool
pextract.
6
The utility reads inflection tables,
generalizes them into paradigms and collapses re-
sulting identical paradigms. Steps ? and ? in
figure 1 are trivially performed by non-finite state
means. After paradigm generalization, bracketed
sequences are replaced by variable symbols (step
?). As each paradigm is then represented as a sin-
6
http://pextract.googlecode.com
gle string, paradigm collapsing can be performed
by simply testing string equivalence.
The tool also implements some further global
restrictions on the nature of the generalizations al-
lowed. These include, for example, a linguistically
motivated attempt to minimize the number of in-
fixes in paradigms. It also stores information (see
figure 4) about the components of generalizations:
the variable instantiations seen, etc., which may be
useful for subsequent tools that take advantage of
its output.
7
Figure 4 briefly illustrates through a toy exam-
ple the input and output to the extraction tool:
inputs are simply lists of entries in inflection ta-
bles, with or without morphological information,
and the output is a list of paradigms where num-
bers correspond to variables. In the event that sev-
eral paradigms can be collapsed, the tool collapses
them (as indeed is seen in figure 4). The actual in-
stantiations of the variables seen are also stored,
represented by the digits 1, . . . as are the complete
first (often base) forms, represented by 0. In effect,
all the seen inflection tables can in principle be re-
constructed from the resulting abstract paradigms.
Table 2 shows how the pextract tool gener-
alizes with five data sets covering German (DE),
Spanish (ES), and Finnish (FI), provided by Dur-
rett and DeNero (2013), along with running times.
Here, among other things, we see that the tool
has generalized 3,855 Spanish verb inflection ta-
7
Statistical information about what the variables looked
like during generalization can be useful information when
performing classifying tasks, such as attempting to fit pre-
viously unseen words to already learned paradigms, etc.
34
katabtu  perf-1-sgkatabta  perf-2-m-sgkutibu   pass-perf-3-m-plkutibna  pass-perf-3-f-pldarastu  perf-1-sgdarasta  perf-2-m-sgdurisu   pass-perf-3-m-pldurisna  pass-perf-3-f-pl
1+a+2+a+3+tu#1+a+2+a+3+ta#1+u+2+i+3+u#1+u+2+i+3+na0=katabtu1=k2=t3=b0=darastu1=d2=r3=s
pextract
Figure 4: Paradigm extraction tool. For the two toy Arabic inflection tables on the left, the pextract
tool produces one three-variable paradigm as output, and reports how the three variables have been
instantiated in the example data, and also how the first form (presumably often the base form) appeared
in its entirety.
bles into 97 distinct paradigms, and 6,200 Finnish
nouns and adjectives have been reduced to 258
paradigms. For comparison, the fairly com-
plete Thompson (1998) lists 79 classes of Span-
ish verbs, while the Kotus (2007) grammar de-
scription counts 51 Finnish noun and adjective
paradigms.
Much of the remaining redundancy in resulting
paradigms can be attributed to lack of phonologi-
cal modeling. That is, paradigms could be further
collapsed if phonological alternations were added
subsequently to paradigm extraction. Consider a
selection of four forms from the inflection table
for the Finnish verb aidata (to fence):
aidata#aitaan#aitaat#aitasin (6)
This is generalized by the tool into
x
1
+d+x
2
+ta#x
1
+t+x
2
+an#x
1
+t+x
2
+at#x
1
+t+x
2
+sin
(7)
The generalization is indeed correct, but the
method does not take into account a gen-
eral phonological process of consonant gradation
where t and d alternate depending on the syllable
type. With this additional information, paradigm
tables could in principle be collapsed further and
this particular paradigm merged with a more gen-
eral paradigm learned for Finnish verbs. The
same goes for other phonological processes which
sometimes cause the tool to produce superficially
different paradigms that could be collapsed further
by modeling vowel harmony and other phenom-
ena.
We may note that the word lengths and inflec-
tion table sizes encountered in the wild are far
larger than the examples used in this article. For
the Wiktionary data, for example, many inflection
tables have more than 50 entries and word lengths
of 50 characters.
Input: Output: Comp.
Data inflection abstract time(s)
tables paradigms
DE-VERBS 1,827 140 123.6
DE-NOUNS 2,564 70 73.5
ES-VERBS 3,855 97 144.9
FI-VERBS 7,049 282 432.2
FI-NOUNS-ADJS 6,200 258 374.1
Table 2: Paradigm generalization from
Wiktionary-gathered inflection tables.
6 Conclusion
In this work, we have presented a method for
extracting general paradigms from inflection ta-
bles through entirely finite state means. This in-
volves solving a constrained longest common sub-
sequence problem, for which the calculus offered
by modern finite state toolkits is well suited. Al-
though the problem in no way requires a finite
state solution, we find that addressing it with a
general-purpose programming language appears
far more complex a route.
We further note that finite state transducers can
be profitably employed after paradigm generaliza-
tion has occurred?to find all possible paradigms
and slots that an unknown word form might fit
into, to generate paradigms from base forms, and
so forth.
An interesting further potential optimization is
to try to address ambiguous LCS assignments with
the completely different strategy of attempting to
maximize similarity across paradigms, or mini-
mize the number of resulting paradigms, assuming
one is generalizing a batch of inflection tables at
the same time. Additionally, modeling phonolog-
ical phenomena as a separate step after morpho-
logical paradigm generalization provides opportu-
nities for further development of the system.
35
Acknowledgements
This article was much improved by the insightful
comments provided by the anonymous reviewers.
The research was partially funded by the Academy
of Finland under grant agreement 258373, Ma-
chine learning of rules in natural language mor-
phology and phonology. Additional important
support was provided by the Centre for Language
Technology and Spr?akbanken at the University of
Gothenburg, where part of this research was un-
dertaken.
References
Angluin, D. (1980). Finding patterns common to a
set of strings. Journal of Computer and System
Sciences, 21(1):46?62.
Dreyer, M. and Eisner, J. (2011). Discovering
morphological paradigms from plain text using
a Dirichlet process mixture model. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, pages 616?627.
Association for Computational Linguistics.
Durrett, G. and DeNero, J. (2013). Supervised
learning of complete morphological paradigms.
In Proceedings of NAACL-HLT, pages 1185?
1195.
Eisner, J. (2002). Comprehension and compilation
in optimality theory. In Proceedings of the 40th
Annual Meeting on Association for Computa-
tional Linguistics, pages 56?63. Association for
Computational Linguistics.
Eskander, R., Habash, N., and Rambow, O.
(2013). Automatic extraction of morpholog-
ical lexicons from morphologically annotated
corpora. In Proceedings of the 2013 Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, pages 1032?1043. Associa-
tion for Computational Linguistics.
Gerdemann, D. and Hulden, M. (2012). Practi-
cal finite state optimality theory. In 10th Inter-
national Workshop on Finite State Methods and
Natural Language Processing, page 10.
Gerdemann, D. and van Noord, G. (2000). Ap-
proximation and exactness in finite state opti-
mality theory. In Proceedings of the Fifth Work-
shop of the ACL Special Interest Group in Com-
putational Phonology.
Hockett, C. F. (1954). Two models of grammatical
description. Morphology: Critical Concepts in
Linguistics, 1:110?138.
Hulden, M. (2009). Foma: a finite-state compiler
and library. In Proceedings of the 12th Confer-
ence of the European Chapter of the European
Chapter of the Association for Computational
Linguistics: Demonstrations Session, pages 29?
32, Athens, Greece. Association for Computa-
tional Linguistics.
Hulden, M., Forsberg, M., and Ahlberg, M.
(2014). Semi-supervised learning of morpho-
logical paradigms and lexicons. In Proceedings
of the 14th Conference of the European Chap-
ter of the Association for Computational Lin-
guistics, pages 569?578, Gothenburg, Sweden.
Association for Computational Linguistics.
Irving, R. W. and Fraser, C. B. (1992). Two algo-
rithms for the longest common subsequence of
three (or more) strings. In Combinatorial Pat-
tern Matching, pages 214?229. Springer.
Karttunen, L. (2010). Update on finite state mor-
phology tools. Ms., Palo Alto Research Center.
Kotus (2007). Nykysuomen sanalista [Lexicon of
modern Finnish]. Kotus.
Maier, D. (1978). The complexity of some
problems on subsequences and supersequences.
Journal of the ACM (JACM), 25(2):322?336.
Matthews, P. H. (1972). Inflectional morphology:
A theoretical study based on aspects of Latin
verb conjugation. Cambridge University Press.
Robins, R. H. (1959). In defence of WP. Trans-
actions of the Philological Society, 58(1):116?
144.
Stump, G. T. (2001). A theory of paradigm struc-
ture. Cambridge University Press.
Thompson, S. J. (1998). 15,000 Spanish verbs:
fully conjugated in all the tenses using pattern
verbs. Center for Innovative Language Learn-
ing.
Wang, Q., Pan, M., Shang, Y., and Korkin, D.
(2010). A fast heuristic search algorithm for
finding the longest common subsequence of
multiple strings. In AAAI Proc.
36
