Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380?388,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Rule Filtering by Pattern for Efficient Hierarchical Translation
Gonzalo Iglesias? Adria` de Gispert?
? University of Vigo. Dept. of Signal Processing and Communications. Vigo, Spain
{giglesia,erbanga}@gts.tsc.uvigo.es
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
Eduardo R. Banga? William Byrne?
Abstract
We describe refinements to hierarchical
translation search procedures intended to
reduce both search errors and memory us-
age through modifications to hypothesis
expansion in cube pruning and reductions
in the size of the rule sets used in transla-
tion. Rules are put into syntactic classes
based on the number of non-terminals and
the pattern, and various filtering strate-
gies are then applied to assess the impact
on translation speed and quality. Results
are reported on the 2008 NIST Arabic-to-
English evaluation task.
1 Introduction
Hierarchical phrase-based translation (Chiang,
2005) has emerged as one of the dominant cur-
rent approaches to statistical machine translation.
Hiero translation systems incorporate many of
the strengths of phrase-based translation systems,
such as feature-based translation and strong tar-
get language models, while also allowing flexi-
ble translation and movement based on hierarchi-
cal rules extracted from aligned parallel text. The
approach has been widely adopted and reported to
be competitive with other large-scale data driven
approaches, e.g. (Zollmann et al, 2008).
Large-scale hierarchical SMT involves auto-
matic rule extraction from aligned parallel text,
model parameter estimation, and the use of cube
pruning k-best list generation in hierarchical trans-
lation. The number of hierarchical rules extracted
far exceeds the number of phrase translations typ-
ically found in aligned text. While this may lead
to improved translation quality, there is also the
risk of lengthened translation times and increased
memory usage, along with possible search errors
due to the pruning procedures needed in search.
We describe several techniques to reduce mem-
ory usage and search errors in hierarchical trans-
lation. Memory usage can be reduced in cube
pruning (Chiang, 2007) through smart memoiza-
tion, and spreading neighborhood exploration can
be used to reduce search errors. However, search
errors can still remain even when implementing
simple phrase-based translation. We describe a
?shallow? search through hierarchical rules which
greatly speeds translation without any effect on
quality. We then describe techniques to analyze
and reduce the set of hierarchical rules. We do
this based on the structural properties of rules and
develop strategies to identify and remove redun-
dant or harmful rules. We identify groupings of
rules based on non-terminals and their patterns and
assess the impact on translation quality and com-
putational requirements for each given rule group.
We find that with appropriate filtering strategies
rule sets can be greatly reduced in size without im-
pact on translation performance.
1.1 Related Work
The search and rule pruning techniques described
in the following sections add to a growing lit-
erature of refinements to the hierarchical phrase-
based SMT systems originally described by Chi-
ang (2005; 2007). Subsequent work has addressed
improvements and extensions to the search proce-
dure itself, the extraction of the hierarchical rules
needed for translation, and has also reported con-
trastive experiments with other SMT architectures.
Hiero Search Refinements Huang and Chiang
(2007) offer several refinements to cube pruning
to improve translation speed. Venugopal et al
(2007) introduce a Hiero variant with relaxed con-
straints for hypothesis recombination during pars-
ing; speed and results are comparable to those of
cube pruning, as described by Chiang (2007). Li
and Khudanpur (2008) report significant improve-
ments in translation speed by taking unseen n-
grams into account within cube pruning to mini-
mize language model requests. Dyer et al (2008)
380
extend the translation of source sentences to trans-
lation of input lattices following Chappelier et al
(1999).
Extensions to Hiero Blunsom et al (2008)
discuss procedures to combine discriminative la-
tent models with hierarchical SMT. The Syntax-
Augmented Machine Translation system (Zoll-
mann and Venugopal, 2006) incorporates target
language syntactic constituents in addition to the
synchronous grammars used in translation. Shen
at al. (2008) make use of target dependency trees
and a target dependency language model during
decoding. Marton and Resnik (2008) exploit shal-
low correspondences of hierarchical rules with
source syntactic constituents extracted from par-
allel text, an approach also investigated by Chiang
(2005). Zhang and Gildea (2006) propose bina-
rization for synchronous grammars as a means to
control search complexity arising from more com-
plex, syntactic, hierarchical rules sets.
Hierarchical rule extraction Zhang et al (2008)
describe a linear algorithm, a modified version of
shift-reduce, to extract phrase pairs organized into
a tree from which hierarchical rules can be directly
extracted. Lopez (2007) extracts rules on-the-fly
from the training bitext during decoding, search-
ing efficiently for rule patterns using suffix arrays.
Analysis and Contrastive Experiments Zollman
et al (2008) compare phrase-based, hierarchical
and syntax-augmented decoders for translation of
Arabic, Chinese, and Urdu into English, and they
find that attempts to expedite translation by simple
schemes which discard rules also degrade transla-
tion performance. Lopez (2008) explores whether
lexical reordering or the phrase discontiguity in-
herent in hierarchical rules explains improvements
over phrase-based systems. Hierarchical transla-
tion has also been used to great effect in combina-
tion with other translation architectures (e.g. (Sim
et al, 2007; Rosti et al, 2007)).
1.2 Outline
The paper proceeds as follows. Section 2 de-
scribes memoization and spreading neighborhood
exploration in cube pruning intended to reduce
memory usage and search errors, respectively. A
detailed comparison with a simple phrase-based
system is presented. Section 3 describes pattern-
based rule filtering and various procedures to se-
lect rule sets for use in translation with an aim
to improving translation quality while minimizing
rule set size. Finally, Section 4 concludes.
2 Two Refinements in Cube Pruning
Chiang (2007) introduced cube pruning to apply
language models in pruning during the generation
of k-best translation hypotheses via the application
of hierarchical rules in the CYK algorithm. In the
implementation of Hiero described here, there is
the parser itself, for which we use a variant of the
CYK algorithm closely related to CYK+ (Chap-
pelier and Rajman, 1998); it employs hypothesis
recombination, without pruning, while maintain-
ing back pointers. Before k-best list generation
with cube pruning, we apply a smart memoiza-
tion procedure intended to reduce memory con-
sumption during k-best list expansion. Within the
cube pruning algorithm we use spreading neigh-
borhood exploration to improve robustness in the
face of search errors.
2.1 Smart Memoization
Each cell in the chart built by the CYK algorithm
contains all possible derivations of a span of the
source sentence being translated. After the parsing
stage is completed, it is possible to make a very ef-
ficient sweep through the backpointers of the CYK
grid to count how many times each cell will be ac-
cessed by the k-best generation algorithm. When
k-best list generation is running, the number of
times each cell is visited is logged so that, as each
cell is visited for the last time, the k-best list as-
sociated with each cell is deleted. This continues
until the one k-best list remaining at the top of the
chart spans the entire sentence. Memory reduc-
tions are substantial for longer sentences: for the
longest sentence in the tuning set described later
(105 words in length), smart memoization reduces
memory usage during the cube pruning stage from
2.1GB to 0.7GB. For average length sentences of
approx. 30 words, memory reductions of 30% are
typical.
2.2 Spreading Neighborhood Exploration
In generation of a k-best list of translations for
a source sentence span, every derivation is trans-
formed into a cube containing the possible trans-
lations arising from that derivation, along with
their translation and language model scores (Chi-
ang, 2007). These derivations may contain non-
terminals which must be expanded based on hy-
potheses generated by lower cells, which them-
381
HIERO MJ1 HIERO HIERO SHALLOW
X ? ?V2V1,V1V2? X ? ??,?? X ? ??s,?s?
X ? ?V ,V ? ?, ? ? ({X} ?T)+ X ? ?V ,V ?
V ? ?s,t? V ? ?s,t?
s, t ? T+ s, t ? T+; ?s, ?s ? ({V } ? T)+
Table 1: Hierarchical grammars (not including glue rules). T is the set of terminals.
selves may contain non-terminals. For efficiency
each cube maintains a queue of hypotheses, called
here the frontier queue, ranked by translation and
language model score; it is from these frontier
queues that hypotheses are removed to create the
k-best list for each cell. When a hypothesis is ex-
tracted from a frontier queue, that queue is updated
by searching through the neighborhood of the ex-
tracted item to find novel hypotheses to add; if no
novel hypotheses are found, that queue necessar-
ily shrinks. This shrinkage can lead to search er-
rors. We therefore require that, when a hypothe-
sis is removed, new candidates must be added by
exploring a neighborhood which spreads from the
last extracted hypothesis. Each axis of the cube
is searched (here, to a depth of 20) until a novel
hypothesis is found. In this way, up to three new
candidates are added for each entry extracted from
a frontier queue.
Chiang (2007) describes an initialization pro-
cedure in which these frontier queues are seeded
with a single candidate per axis; we initialize each
frontier queue to a depth of bNnt+1, where Nnt is
the number of non-terminals in the derivation and
b is a search parameter set throughout to 10. By
starting with deep frontier queues and by forcing
them to grow during search we attempt to avoid
search errors by ensuring that the universe of items
within the frontier queues does not decrease as the
k-best lists are filled.
2.3 A Study of Hiero Search Errors in
Phrase-Based Translation
Experiments reported in this paper are based
on the NIST MT08 Arabic-to-English transla-
tion task. Alignments are generated over all al-
lowed parallel data, (?150M words per language).
Features extracted from the alignments and used
in translation are in common use: target lan-
guage model, source-to-target and target-to-source
phrase translation models, word and rule penalties,
number of usages of the glue rule, source-to-target
and target-to-source lexical models, and three rule
Figure 1: Spreading neighborhood exploration
within a cube, just before and after extraction
of the item C. Grey squares represent the fron-
tier queue; black squares are candidates already
extracted. Chiang (2007) would only consider
adding items X to the frontier queue, so the queue
would shrink. Spreading neighborhood explo-
ration adds candidates S to the frontier queue.
count features inspired by Bender et al (2007).
MET (Och, 2003) iterative parameter estimation
under IBM BLEU is performed on the develop-
ment set. The English language used model is a
4-gram estimated over the parallel text and a 965
million word subset of monolingual data from the
English Gigaword Third Edition. In addition to the
MT08 set itself, we use a development set mt02-
05-tune formed from the odd numbered sentences
of the NIST MT02 through MT05 evaluation sets;
the even numbered sentences form the validation
set mt02-05-test. The mt02-05-tune set has 2,075
sentences.
We first compare the cube pruning decoder to
the TTM (Kumar et al, 2006), a phrase-based
SMT system implemented with Weighted Finite-
State Tansducers (Allauzen et al, 2007). The sys-
tem implements either a monotone phrase order
translation, or an MJ1 (maximum phrase jump of
1) reordering model (Kumar and Byrne, 2005).
Relative to the complex movement and translation
allowed by Hiero and other models, MJ1 is clearly
inferior (Dreyer et al, 2007); MJ1 was developed
with efficiency in mind so as to run with a mini-
mum of search errors in translation and to be eas-
ily and exactly realized via WFSTs. Even for the
382
large models used in an evaluation task, the TTM
system is reported to run largely without pruning
(Blackwood et al, 2008).
The Hiero decoder can easily be made to
implement MJ1 reordering by allowing only a
restricted set of reordering rules in addition to
the usual glue rule, as shown in left-hand column
of Table 1, where T is the set of terminals.
Constraining Hiero in this way makes it possible
to compare its performance to the exact WFST
TTM implementation and to identify any search
errors made by Hiero.
Table 2 shows the lowercased IBM BLEU
scores obtained by the systems for mt02-05-tune
with monotone and reordered search, and with
MET-optimised parameters for MJ1 reordering.
For Hiero, an N-best list depth of 10,000 is used
throughout. In the monotone case, all phrase-
based systems perform similarly although Hiero
does make search errors. For simple MJ1 re-
ordering, the basic Hiero search procedure makes
many search errors and these lead to degradations
in BLEU. Spreading neighborhood expansion re-
duces the search errors and improves BLEU score
significantly but search errors remain a problem.
Search errors are even more apparent after MET.
This is not surprising, given that mt02-05-tune is
the set over which MET is run: MET drives up the
likelihood of good hypotheses at the expense of
poor hypotheses, but search errors often increase
due to the expanded dynamic range of the hypoth-
esis scores.
Our aim in these experiments was to demon-
strate that spreading neighborhood exploration can
aid in avoiding search errors. We emphasize that
we are not proposing that Hiero should be used to
implement reordering models such as MJ1 which
were created for completely different search pro-
cedures (e.g. WFST composition). However these
experiments do suggest that search errors may be
an issue, particularly as the search space grows
to include the complex long-range movement al-
lowed by the hierarchical rules. We next study
various filtering procedures to reduce hierarchi-
cal rule sets to find a balance between translation
speed, memory usage, and performance.
3 Rule Filtering by Pattern
Hierarchical rules X ? ??,?? are composed of
sequences of terminals and non-terminals, which
Monotone MJ1 MJ1+MET
BLEU SE BLEU SE BLEU SE
a 44.7 - 47.2 - 49.1 -
b 44.5 342 46.7 555 48.4 822
c 44.7 77 47.1 191 48.9 360
Table 2: Phrase-based TTM and Hiero perfor-
mance on mt02-05-tune for TTM (a), Hiero (b),
Hiero with spreading neighborhood exploration
(c). SE is the number of Hiero hypotheses with
search errors.
we call elements. In the source, a maximum of
two non-adjacent non-terminals is allowed (Chi-
ang, 2007). Leaving aside rules without non-
terminals (i.e. phrase pairs as used in phrase-
based translation), rules can be classed by their
number of non-terminals, Nnt, and their number
of elements, Ne. There are 5 possible classes:
Nnt.Ne= 1.2, 1.3, 2.3, 2.4, 2.5.
During rule extraction we search each class sep-
arately to control memory usage. Furthermore, we
extract from alignments only those rules which are
relevant to our given test set; for computation of
backward translation probabilities we log general
counts of target-side rules but discard unneeded
rules. Even with this restriction, our initial ruleset
for mt02-05-tune exceeds 175M rules, of which
only 0.62M are simple phrase pairs.
The question is whether all these rules are
needed for translation. If the rule set can be re-
duced without reducing translation quality, both
memory efficiency and translation speed can be
increased. Previously published approaches to re-
ducing the rule set include: enforcing a mini-
mum span of two words per non-terminal (Lopez,
2008), which would reduce our set to 115M rules;
or a minimum count (mincount) threshold (Zoll-
mann et al, 2008), which would reduce our set
to 78M (mincount=2) or 57M (mincount=3) rules.
Shen et al (2008) describe the result of filter-
ing rules by insisting that target-side rules are
well-formed dependency trees. This reduces their
rule set from 140M to 26M rules. This filtering
leads to a degradation in translation performance
(see Table 2 of Shen et al (2008)), which they
counter by adding a dependency LM in translation.
As another reference point, Chiang (2007) reports
Chinese-to-English translation experiments based
on 5.5M rules.
Zollmann et al (2008) report that filtering rules
383
en masse leads to degradation in translation per-
formance. Rather than apply a coarse filtering,
such as a mincount for all rules, we follow a more
syntactic approach and further classify our rules
according to their pattern and apply different fil-
ters to each pattern depending on its value in trans-
lation. The premise is that some patterns are more
important than others.
3.1 Rule Patterns
Class Rule Pattern
Nnt.Ne ?source , target? Types
?wX1 , wX1? 1185028
1.2 ?wX1 , wX1w? 153130
?wX1 , X1w? 97889
1.3 ?wX1w , wX1w? 32903522
?wX1w , wX1? 989540
2.3 ?X1wX2 , X1wX2? 1554656
?X2wX1 , X1wX2? 39163
?wX1wX2 , wX1wX2? 26901823
?X1wX2w , X1wX2w? 26053969
2.4 ?wX1wX2 , wX1wX2w? 2534510
?wX2wX1 , wX1wX2? 349176
?X2wX1w , X1wX2w? 259459
?wX1wX2w , wX1wX2w? 61704299
?wX1wX2w , wX1X2w? 3149516
2.5 ?wX1wX2w , X1wX2w? 2330797
?wX2wX1w , wX1wX2w? 275810
?wX2wX1w , wX1X2w? 205801
Table 3: Hierarchical rule patterns classed by
number of non-terminals, Nnt, number of ele-
ments Ne, source and target patterns, and types in
the rule set extracted for mt02-05-tune.
Given a rule set, we define source patterns and
target patterns by replacing every sequence of
non-terminals by a single symbol ?w? (indicating
word, i.e. terminal string, w ? T+). Each hierar-
chical rule has a unique source and target pattern
which together define the rule pattern.
By ignoring the identity and the number of ad-
jacent terminals, the rule pattern represents a nat-
ural generalization of any rule, capturing its struc-
ture and the type of reordering it encodes. In to-
tal, there are 66 possible rule patterns. Table 3
presents a few examples extracted for mt02-05-
tune, showing that some patterns are much more
diverse than others. For example, patterns with
two non-terminals (Nnt=2) are richer than pat-
terns with Nnt=1, as they cover many more dis-
tinct rules. Additionally, patterns with two non-
terminals which also have a monotonic relation-
ship between source and target non-terminals are
much more diverse than their reordered counter-
parts.
Some examples of extracted rules and their cor-
responding pattern follow, where Arabic is shown
in Buckwalter encoding.
Pattern ?wX1 , wX1w? :
?w+ qAl X1 , the X1said?
Pattern ?wX1w , wX1? :
?fy X1kAnwn Al>wl , on december X1?
Pattern ?wX1wX2 , wX1wX2w? :
?Hl X1lAzmp X2 , a X1solution to the X2crisis?
3.2 Building an Initial Rule Set
We describe a greedy approach to building a rule
set in which rules belonging to a pattern are added
to the rule set guided by the improvements they
yield on mt02-05-tune relative to the monotone
Hiero system described in the previous section.
We find that certain patterns seem not to con-
tribute to any improvement. This is particularly
significant as these patterns often encompass large
numbers of rules, as with patterns with match-
ing source and target patterns. For instance, we
found no improvement when adding the pattern
?X1w,X1w?, of which there were 1.2M instances
(Table 3). Since concatenation is already possible
under the general glue rule, rules with this pattern
are redundant. By contrast, the much less frequent
reordered counterpart, i.e. the ?wX1,X1w? pat-
tern (0.01M instances), provides substantial gains.
The situation is analogous for rules with two non-
terminals (Nnt=2).
Based on exploratory analyses (not reported
here, for space) an initial rule set was built by
excluding patterns reported in Table 4. In to-
tal, 171.5M rules are excluded, for a remaining
set of 4.2M rules, 3.5M of which are hierarchi-
cal. We acknowledge that adding rules in this way,
by greedy search, is less than ideal and inevitably
raises questions with respect to generality and re-
peatability. However in our experience this is a
robust approach, mainly because the initial trans-
lation system runs very fast; it is possible to run
many exploratory experiments in a short time.
384
Excluded Rules Types
a ?X1w,X1w? , ?wX1,wX1? 2332604
b ?X1wX2,?? 2121594
?X1wX2w,X1wX2w? ,c ?wX1wX2,wX1wX2?
52955792
d ?wX1wX2w,?? 69437146
e Nnt.Ne= 1.3 w mincount=5 32394578
f Nnt.Ne= 2.3 w mincount=5 166969
g Nnt.Ne= 2.4 w mincount=10 11465410
h Nnt.Ne= 2.5 w mincount=5 688804
Table 4: Rules excluded from the initial rule set.
3.3 Shallow versus Fully Hierarchical
Translation
In measuring the effectiveness of rules in transla-
tion, we also investigate whether a ?fully hierarchi-
cal? search is needed or whether a shallow search
is also effective. In constrast to full Hiero, in the
shallow search, only phrases are allowed to be sub-
stituted into non-terminals. The rules used in each
case can be expressed as shown in the 2nd and 3rd
columns of Table 1. Shallow search can be con-
sidered (loosely) to be a form of rule filtering.
As can be seen in Table 5 there is no impact on
BLEU, while translation speed increases by a fac-
tor of 7. Of course, these results are specific to this
Arabic-to-English translation task, and need not
be expected to carry over to other language pairs,
such as Chinese-to-English translation. However,
the impact of this search simplification is easy to
measure, and the gains can be significant enough,
that it may be worth investigation even for lan-
guages with complex long distance movement.
mt02-05- -tune -test
System Time BLEU BLEU
HIERO 14.0 52.1 51.5
HIERO - shallow 2.0 52.1 51.4
Table 5: Translation performance and time (in sec-
onds per word) for full vs. shallow Hiero.
3.4 Individual Rule Filters
We now filter rules individually (not by class) ac-
cording to their number of translations. For each
fixed ? /? T+ (i.e. with at least 1 non-terminal),
we define the following filters over rules X ?
??,??:
? Number of translations (NT). We keep the
NT most frequent ?, i.e. each ? is allowed to
have at most NT rules.
? Number of reordered translations (NRT).
We keep the NRT most frequent ? with
monotonic non-terminals and the NRT most
frequent ? with reordered non-terminals.
? Count percentage (CP). We keep the most
frequent ? until their aggregated number of
counts reaches a certain percentage CP of the
total counts of X ? ??,??. Some ??s are al-
lowed to have more ??s than others, depend-
ing on their count distribution.
Results applying these filters with various
thresholds are given in Table 6, including num-
ber of rules and decoding time. As shown, all
filters achieve at least a 50% speed-up in decod-
ing time by discarding 15% to 25% of the base-
line rules. Remarkably, performance is unaffected
when applying the simple NT and NRT filters
with a threshold of 20 translations. Finally, the
CM filter behaves slightly worse for thresholds of
90% for the same decoding time. For this reason,
we select NRT=20 as our general filter.
mt02-05- -tune -test
Filter Time Rules BLEU BLEU
baseline 2.0 4.20 52.1 51.4
NT=10 0.8 3.25 52.0 51.3
NT=15 0.8 3.43 52.0 51.3
NT=20 0.8 3.56 52.1 51.4
NRT=10 0.9 3.29 52.0 51.3
NRT=15 1.0 3.48 52.0 51.4
NRT=20 1.0 3.59 52.1 51.4
CP=50 0.7 2.56 51.4 50.9
CP=90 1.0 3.60 52.0 51.3
Table 6: Impact of general rule filters on transla-
tion (IBM BLEU), time (in seconds per word) and
number of rules (in millions).
3.5 Pattern-based Rule Filters
In this section we first reconsider whether reintro-
ducing the monotonic rules (originally excluded as
described in rows ?b?, ?c?, ?d? in Table 4) affects
performance. Results are given in the upper rows
of Table 7. For all classes, we find that reintroduc-
ing these rules increases the total number of rules
385
mt02-05- -tune -test
Nnt.Ne Filter Time Rules BLEU BLEU
baseline NRT=20 1.0 3.59 52.1 51.4
2.3 +monotone 1.1 4.08 51.5 51.1
2.4 +monotone 2.0 11.52 51.6 51.0
2.5 +monotone 1.8 6.66 51.7 51.2
1.3 mincount=3 1.0 5.61 52.1 51.3
2.3 mincount=1 1.2 3.70 52.1 51.4
2.4 mincount=5 1.8 4.62 52.0 51.3
2.4 mincount=15 1.0 3.37 52.0 51.4
2.5 mincount=1 1.1 4.27 52.2 51.5
1.2 mincount=5 1.0 3.51 51.8 51.3
1.2 mincount=10 1.0 3.50 51.7 51.2
Table 7: Effect of pattern-based rule filters. Time in seconds per word. Rules in millions.
substantially, despite the NRT=20 filter, but leads
to degradation in translation performance.
We next reconsider the mincount threshold val-
ues for Nnt.Ne classes 1.3, 2.3, 2.4 and 2.5 origi-
nally described in Table 4 (rows ?e? to ?h?). Results
under various mincount cutoffs for each class are
given in Table 7 (middle five rows). For classes
2.3 and 2.5, the mincount cutoff can be reduced
to 1 (i.e. all rules are kept) with slight translation
improvements. In contrast, reducing the cutoff for
classes 1.3 and 2.4 to 3 and 5, respectively, adds
many more rules with no increase in performance.
We also find that increasing the cutoff to 15 for
class 2.4 yields the same results with a smaller rule
set. Finally, we consider further filtering applied to
class 1.2 with mincount 5 and 10 (final two rows
in Table 7). The number of rules is largely un-
changed, but translation performance drops con-
sistently as more rules are removed.
Based on these experiments, we conclude that it
is better to apply separate mincount thresholds to
the classes to obtain optimal performance with a
minimum size rule set.
3.6 Large Language Models and Evaluation
Finally, in this section we report results of our
shallow hierarchical system with the 2.5 min-
count=1 configuration from Table 7, after includ-
ing the following N-best list rescoring steps.
? Large-LM rescoring. We build sentence-
specific zero-cutoff stupid-backoff (Brants et
al., 2007) 5-gram language models, estimated
using ?4.7B words of English newswire text,
and apply them to rescore each 10000-best
list.
? Minimum Bayes Risk (MBR). We then rescore
the first 1000-best hypotheses with MBR,
taking the negative sentence level BLEU
score as the loss function to minimise (Ku-
mar and Byrne, 2004).
Table 8 shows results for mt02-05-tune, mt02-
05-test, the NIST subsets from the MT06 evalu-
ation (mt06-nist-nw for newswire data and mt06-
nist-ng for newsgroup) and mt08, as measured by
lowercased IBM BLEU and TER (Snover et al,
2006). Mixed case NIST BLEU for this system on
mt08 is 42.5. This is directly comparable to offi-
cial MT08 evaluation results1.
4 Conclusions
This paper focuses on efficient large-scale hierar-
chical translation while maintaining good trans-
lation quality. Smart memoization and spreading
neighborhood exploration during cube pruning are
described and shown to reduce memory consump-
tion and Hiero search errors using a simple phrase-
based system as a contrast.
We then define a general classification of hi-
erarchical rules, based on their number of non-
terminals, elements and their patterns, for refined
extraction and filtering.
For a large-scale Arabic-to-English task, we
show that shallow hierarchical decoding is as good
1Full MT08 results are available at
http://www.nist.gov/speech/tests/mt/2008/. It is worth
noting that many of the top entries make use of system
combination; the results reported here are for single system
translation.
386
mt02-05-tune mt02-05-test mt06-nist-nw mt06-nist-ng mt08
HIERO+MET 52.2 / 41.6 51.5 / 42.2 48.4 / 43.6 35.3 / 53.2 42.5 / 48.6
+rescoring 53.2 / 40.8 52.6 / 41.4 49.4 / 42.9 36.6 / 53.5 43.4 / 48.1
Table 8: Arabic-to-English translation results (lower-cased IBM BLEU / TER) with large language mod-
els and MBR decoding.
as fully hierarchical search and that decoding time
is dramatically decreased. In addition, we describe
individual rule filters based on the distribution of
translations with further time reductions at no cost
in translation scores. This is in direct contrast
to recent reported results in which other filtering
strategies lead to degraded performance (Shen et
al., 2008; Zollmann et al, 2008).
We find that certain patterns are of much greater
value in translation than others and that separate
minimum count filters should be applied accord-
ingly. Some patterns were found to be redundant
or harmful, in particular those with two monotonic
non-terminals. Moreover, we show that the value
of a pattern is not directly related to the number of
rules it encompasses, which can lead to discarding
large numbers of rules as well as to dramatic speed
improvements.
Although reported experiments are only for
Arabic-to-English translation, we believe the ap-
proach will prove to be general. Pattern relevance
will vary for other language pairs, but we expect
filtering strategies to be equally worth pursuing.
Acknowledgments
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
Agency, Contract No. HR0011- 06-C-0022. G.
Iglesias supported by Spanish Government re-
search grant BES-2007-15956 (project TEC2006-
13694-C03-03).
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood, Adria` de Gispert, Jamie Brunning,
and William Byrne. 2008. Large-scale statistical
machine translation with weighted finite state trans-
ducers. In Proceedings of FSMNLP, pages 27?35.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-HLT,
pages 200?208.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
Jean-Ce?dric Chappelier, Martin Rajman, Ramo?n
Aragu?e?s, and Antoine Rozenknop. 1999. Lattice
parsing for speech recognition. In Proceedings of
TALN, pages 95?104.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing reordering constraints for SMT
using efficient BLEU oracle computation. In Pro-
ceedings of SSST, NAACL-HLT 2007 / AMTA Work-
shop on Syntax and Structure in Statistical Transla-
tion.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL-HLT, pages 1012?1020.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, pages 144?151.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Shankar Kumar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proceedings of HLT-EMNLP, pages
161?168.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer translation
template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
387
Zhifei Li and Sanjeev Khudanpur. 2008. A scal-
able decoder for parsing-based machine translation
with equivalent language model state maintenance.
In Proceedings of the ACL-HLT Second Workshop
on Syntax and Structure in Statistical Translation,
pages 10?18.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CONLL, pages 976?985.
Adam Lopez. 2008. Tera-scale translation models
via pattern matching. In Proceedings of COLING,
pages 505?512.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of ACL-HLT, pages 1003?
1011.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL, pages 160?167.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of HLT-
NAACL, pages 228?235.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT, pages 577?585.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation
system combination. In Proceedings of ICASSP,
volume 4, pages 105?108.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted human an-
notation. In Proceedings of AMTA, pages 223?231.
Ashish Venugopal, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Pro-
ceedings of HLT-NAACL, pages 500?507.
Hao Zhang and Daniel Gildea. 2006. Synchronous
binarization for machine translation. In Proceedings
of HLT-NAACL, pages 256?263.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In Proceedings of
COLING, pages 1081?1088.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of NAACL Workshop on Statistical
Machine Translation, pages 138?141.
Andreas Zollmann, Ashish Venugopal, Franz Och,
and Jay Ponte. 2008. A systematic comparison
of phrase-based, hierarchical and syntax-augmented
statistical MT. In Proceedings of COLING, pages
1145?1152.
388
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433?441,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hierarchical Phrase-Based Translation with
Weighted Finite State Transducers
Gonzalo Iglesias? Adria` de Gispert?
? University of Vigo. Dept. of Signal Processing and Communications. Vigo, Spain
{giglesia,erbanga}@gts.tsc.uvigo.es
? University of Cambridge. Dept. of Engineering. CB2 1PZ Cambridge, U.K.
{ad465,wjb31}@eng.cam.ac.uk
Eduardo R. Banga? William Byrne?
Abstract
This paper describes a lattice-based decoder
for hierarchical phrase-based translation. The
decoder is implemented with standard WFST
operations as an alternative to the well-known
cube pruning procedure. We find that the
use of WFSTs rather than k-best lists requires
less pruning in translation search, resulting
in fewer search errors, direct generation of
translation lattices in the target language,
better parameter optimization, and improved
translation performance when rescoring with
long-span language models and MBR decod-
ing. We report translation experiments for
the Arabic-to-English and Chinese-to-English
NIST translation tasks and contrast the WFST-
based hierarchical decoder with hierarchical
translation under cube pruning.
1 Introduction
Hierarchical phrase-based translation generates
translation hypotheses via the application of hierar-
chical rules in CYK parsing (Chiang, 2005). Cube
pruning is used to apply language models at each
cell of the CYK grid as part of the search for a
k-best list of translation candidates (Chiang, 2005;
Chiang, 2007). While this approach is very effective
and has been shown to produce very good quality
translation, the reliance on k-best lists is a limita-
tion. We take an alternative approach and describe a
lattice-based hierarchical decoder implemented with
Weighted Finite State Transducers (WFSTs). In ev-
ery CYK cell we build a single, minimal word lattice
containing all possible translations of the source sen-
tence span covered by that cell. When derivations
contain non-terminals, we use pointers to lower-
level lattices for memory efficiency. The pointers
are only expanded to the actual translations if prun-
ing is required during search; expansion is otherwise
only carried out at the upper-most cell, after the full
CYK grid has been traversed.
We describe how this decoder can be easily im-
plemented with WFSTs. For this we employ the
OpenFST libraries (Allauzen et al, 2007). Using
standard FST operations such as composition, ep-
silon removal, determinization, minimization and
shortest-path, we find this search procedure to be
simpler to implement than cube pruning. The main
modeling advantages are a significant reduction in
search errors, a simpler implementation, direct gen-
eration of target language word lattices, and better
integration with other statistical MT procedures. We
report translation results in Arabic-to-English and
Chinese-to-English translation and contrast the per-
formance of lattice-based and cube pruning hierar-
chical decoding.
1.1 Related Work
Hierarchical phrase-based translation has emerged
as one of the dominant current approaches to statis-
tical machine translation. Hiero translation systems
incorporate many of the strengths of phrase-based
translation systems, such as feature-based transla-
tion and strong target language models, while also
allowing flexible translation and movement based
on hierarchical rules extracted from aligned paral-
lel text. We summarize some extensions to the basic
approach to put our work in context.
433
Hiero Search Refinements Huang and Chiang
(2007) offer several refinements to cube pruning to
improve translation speed. Venugopal et al (2007)
introduce a Hiero variant with relaxed constraints
for hypothesis recombination during parsing; speed
and results are comparable to those of cube prun-
ing, as described by Chiang (2007). Li and Khudan-
pur (2008) report significant improvements in trans-
lation speed by taking unseen n-grams into account
within cube pruning to minimize language model re-
quests. Dyer et al (2008) extend the translation of
source sentences to translation of input lattices fol-
lowing Chappelier et al (1999).
Extensions to Hiero Several authors describe ex-
tensions to Hiero, to incorporate additional syntactic
information (Zollmann and Venugopal, 2006; Zhang
and Gildea, 2006; Shen et al, 2008; Marton and
Resnik, 2008), or to combine it with discriminative
latent models (Blunsom et al, 2008).
Analysis and Contrastive Experiments Zollman et
al. (2008) compare phrase-based, hierarchical and
syntax-augmented decoders for translation of Ara-
bic, Chinese, and Urdu into English. Lopez (2008)
explores whether lexical reordering or the phrase
discontiguity inherent in hierarchical rules explains
improvements over phrase-based systems. Hierar-
chical translation has also been used to great effect
in combination with other translation architectures,
e.g. (Sim et al, 2007; Rosti et al, 2007).
WFSTs for Translation There is extensive work in
using Weighted Finite State Transducer for machine
translation (Bangalore and Riccardi, 2001; Casacu-
berta, 2001; Kumar and Byrne, 2005; Mathias and
Byrne, 2006; Graehl et al, 2008).
To our knowledge, this paper presents the first de-
scription of hierarchical phrase-based translation in
terms of lattices rather than k-best lists. The next
section describes hierarchical phrase-based transla-
tion with WFSTs, including the lattice construction
over the CYK grid and pruning strategies. Sec-
tion 3 reports translation experiments for Arabic-to-
English and Chinese-to-English, and Section 4 con-
cludes.
2 Hierarchical Translation with WFSTs
The translation system is based on a variant of the
CYK algorithm closely related to CYK+ (Chappe-
lier and Rajman, 1998). Parsing follows the de-
scription of Chiang (2005; 2007), maintaining back-
pointers and employing hypothesis recombination
without pruning. The underlying model is a syn-
chronous context-free grammar consisting of a set
R = {Rr} of rules Rr : N ? ??r,?r? / pr, with
?glue? rules, S ? ?X,X? and S ? ?S X,S X?. If a
rule has probability pr, it is transformed to a cost cr;
here we use the tropical semiring, so cr = ? log pr.
N denotes a non-terminal; in this paper, N can be
either S, X, or V (see section 3.2). T denotes the
terminals (words), and the grammar builds parses
based on strings ?, ? ? {{S,X, V } ? T}+. Each
cell in the CYK grid is specified by a non-terminal
symbol and position in the CYK grid: (N,x, y),
which spans sx+y?1x on the source sentence.
In effect, the source language sentence is parsed
using a context-free grammar with rules N ? ?.
The generation of translations is a second step that
follows parsing. For this second step, we describe
a method to construct word lattices with all possible
translations that can be produced by the hierarchical
rules. Construction proceeds by traversing the CYK
grid along the backpointers established in parsing.
In each cell (N,x, y) in the CYK grid, we build a
target language word lattice L(N,x, y). This lat-
tice contains every translation of sx+y?1x from every
derivation headed by N . These lattices also contain
the translation scores on their arc weights.
The ultimate objective is the word lattice
L(S, 1, J) which corresponds to all the analyses that
cover the source sentence sJ1 . Once this is built,
we can apply a target language model to L(S, 1, J)
to obtain the final target language translation lattice
(Allauzen et al, 2003).
We use the approach of Mohri (2002) in applying
WFSTs to statistical NLP. This fits well with the use
of the OpenFST toolkit (Allauzen et al, 2007) to
implement our decoder.
2.1 Lattice Construction Over the CYK Grid
In each cell (N,x, y), the set of rule indices used
by the parser is denoted R(N,x, y), i.e. for r ?
R(N,x, y), N ? ??r,?r? was used in at least one
derivation involving that cell.
For each rule Rr, r ? R(N,x, y), we build a lat-
tice L(N,x, y, r). This lattice is derived from the
target side of the rule ?r by concatenating lattices
434
R1: X ? ?s1 s2 s3,t1 t2?
R2: X ? ?s1 s2,t7 t8?
R3: X ? ?s3,t9?
R4: S ? ?X,X?
R5: S ? ?S X,S X?
L(S, 1, 3) = L(S, 1, 3, 4) ? L(S, 1, 3, 5)
L(S, 1, 3, 4) = L(X, 1, 3) = L(X, 1, 3, 1) =
= A(t1)?A(t2)
L(S, 1, 3, 5) = L(S, 1, 2)? L(X, 3, 1)
L(S, 1, 2) = L(S, 1, 2, 4) = L(X, 1, 2) =
= L(X, 1, 2, 2) = A(t7)?A(t8)
L(X, 3, 1) = L(X, 3, 1, 3) = A(t9)
L(S, 1, 3, 5) = A(t7)?A(t8)?A(t9)
L(S, 1, 3) = (A(t1)?A(t2))? (A(t7)?A(t8)?A(t9))
Figure 1: Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3. The grid
is represented here in two dimensions (x, y). In practice only the first column accepts both non-terminals (S,X). For
this reason it is divided in two subcolumns.
corresponding to the elements of ?r = ?r1...?r|?r |.
If an ?ri is a terminal, creating its lattice is straight-
forward. If ?ri is a non-terminal, it refers to a cell
(N ?, x?, y?) lower in the grid identified by the back-
pointer BP (N,x, y, r, i); in this case, the lattice
used is L(N ?, x?, y?). Taken together,
L(N,x, y, r) = ?
i=1..|?r|
L(N,x, y, r, i) (1)
L(N,x, y, r, i) =
{
A(?i) if ?i ? T
L(N ?, x?, y?) else
(2)
where A(t), t ? T returns a single-arc accep-
tor which accepts only the symbol t. The lattice
L(N,x, y) is then built as the union of lattices cor-
responding to the rules in R(N,x, y):
L(N,x, y) = ?
r?R(N,x,y)
L(N,x, y, r) (3)
Lattice union and concatenation are performed
using the ? and ? WFST operations respectively, as
described by Allauzen et al(2007). If a rule Rr has
a cost cr, it is applied to the exit state of the lattice
L(N,x, y, r) prior to the operation of Equation 3.
2.1.1 An Example of Phrase-based Translation
Figure 1 illustrates this process for a three word
source sentence s1s2s3 under monotone phrase-
based translation. The left-hand side shows the state
of the CYK grid after parsing using the rules R1 to
R5. These include 3 rules with only terminals (R1,
R2, R3) and the glue rules (R4, R5). Arrows repre-
sent backpointers to lower-level cells. We are inter-
ested in the upper-most S cell (S, 1, 3), as it repre-
sents the search space of translation hypotheses cov-
ering the whole source sentence. Two rules (R4, R5)
are in this cell, so the lattice L(S, 1, 3) will be ob-
tained by the union of the two lattices found by the
backpointers of these two rules. This process is ex-
plicitly derived in the right-hand side of Figure 1.
2.1.2 An Example of Hierarchical Translation
Figure 2 shows a hierarchical scenario for the
same sentence. Three rules, R6, R7, R8, are added
to the example of Figure 1, thus providing two ad-
ditional derivations. This makes use of sublattices
already produced in the creation of L(S, 1, 3, 5) and
L(X, 1, 3, 1) in Figure 1; these are within {}.
2.2 A Procedure for Lattice Construction
Figure 3 presents an algorithm to build the lattice
for every cell. The algorithm uses memoization: if
a lattice for a requested cell already exists, it is re-
turned (line 2); otherwise it is constructed via equa-
tions 1,2,3. For every rule, each element of the tar-
get side (lines 3,4) is checked as terminal or non-
terminal (equation 2). If it is a terminal element
(line 5), a simple acceptor is built. If it is a non-
terminal (line 6), the lattice associated to its back-
pointer is returned (lines 7 and 8). The complete
lattice L(N,x, y, r) for each rule is built by equa-
tion 1 (line 9). The lattice L(N,x, y) for this cell
is then found by union of all the component rules
(line 10, equation 3); this lattice is then reduced by
435
R6: X ? ?s1,t20?
R7: X ? ?X1 s2 X2,X1 t10 X2?
R8: X ? ?X1 s2 X2,X2 t10 X1?
L(S, 1, 3) = L(S, 1, 3, 4) ?{L(S, 1, 3, 5)}
L(S, 1, 3, 4) = L(X, 1, 3) =
={L(X, 1, 3, 1)} ?L(X, 1, 3, 7)? L(X, 1, 3, 8)
L(X, 1, 3, 7) = L(X, 1, 1, 6)?A(t10)?L(X, 3, 1, 3) =
= A(t20)?A(t10)?A(t9)
L(X, 1, 3, 8) = A(t9)?A(t10)?A(t20)
L(S, 1, 3) = {(A(t1)?A(t2))} ?
?(A(t20)?A(t10)?A(t9))? (A(t9)?A(t10)?A(t20))?
?{(A(t7)?A(t8)?A(t9))}
Figure 2: Translation as in Figure 1 but with additional rules R6,R7,R8. Lattices previously derived appear within {}.
standard WFST operations (lines 11,12,13). It is
important at this point to remove any epsilon arcs
which may have been introduced by the various
WFST union, concatenation, and replacement oper-
ations (Allauzen et al, 2007).
1 function buildFst(N,x,y)
2 if ? L(N,x, y) return L(N,x, y)
3 for r ? R(N,x, y), Rr : N ? ??,??
4 for i = 1...|?|
5 if ?i ? T, L(N,x, y, r, i) = A(?i)
6 else
7 (N ?, x?, y?) = BP (?i)
8 L(N,x, y, r, i) = buildFst(N ?, x?, y?)
9 L(N,x, y, r)=?i=1..|?| L(N,x, y, r, i)
10 L(N,x, y) =?r?R(N,x,y) L(N,x, y, r)
11 fstRmEpsilon L(N,x, y)
12 fstDeterminize L(N,x, y)
13 fstMinimize L(N,x, y)
14 return L(N,x, y)
Figure 3: Recursive Lattice Construction.
2.3 Delayed Translation
Equation 2 leads to the recursive construction of lat-
tices in upper-levels of the grid through the union
and concatenation of lattices from lower levels. If
equations 1 and 3 are actually carried out over fully
expanded word lattices, the memory required by the
upper lattices will increase exponentially.
To avoid this, we use special arcs that serve as
pointers to the low-level lattices. This effectively
builds a skeleton of the desired lattice and delays
the creation of the final word lattice until a single
replacement operation is carried out in the top cell
(S, 1, J). To make this exact, we define a function
g(N,x, y) which returns a unique tag for each lattice
in each cell, and use it to redefine equation 2. With
the backpointer (N ?, x?, y?) = BP (N,x, y, r, i),
these special arcs are introduced as:
L(N,x, y, r, i) =
{
A(?i) if ?i ? T
A(g(N ?, x?, y?)) else
(4)
The resulting lattices L(N,x, y) are a mix of tar-
get language words and lattice pointers (Figure 4,
top). However each still represents the entire search
space of all translation hypotheses covering the
span. Importantly, operations on these lattices ?
such as lossless size reduction via determinization
and minimization ? can still be performed. Owing
to the existence of multiple hierarchical rules which
share the same low-level dependencies, these opera-
tions can greatly reduce the size of the skeleton lat-
tice; Figure 4 shows the effect on the translation ex-
ample. This process is carried out for the lattice at
every cell, even at the lowest level where there are
only sequences of word terminals. As stated, size
reductions can be significant. However not all redu-
dancy is removed, since duplicate paths may arise
through the concatenation and union of sublattices
with different spans.
At the upper-most cell, the lattice L(S, 1, J) con-
tains pointers to lower-level lattices. A single FST
replace operation (Allauzen et al, 2007) recursively
substitutes all pointers by their lower-level lattices
until no pointers are left, thus producing the com-
plete target word lattice for the whole source sen-
tence. The use of the lattice pointer arc was in-
spired by the ?lazy evaluation? techniques developed
by Mohri et al(2000). Its implementation uses the
infrastructure provided by the OpenFST libraries for
436
01
t1
2g(X,1,2)
3
g(X,1,1)
5
g(X,3,1)
7
t2
g(X,3,1)
4
t10
6t10
g(X,3,1)
g(X,1,1)
0
3g(X,1,1)
2g(X,1,2)
1
t1
4
g(X,3,1)
t10
6
g(X,3,1)
t2
5
t10
g(X,1,1)
Figure 4: Delayed translation WFST with derivations
from Figure 1 and Figure 2 before [t] and after minimiza-
tion [b].
delayed composition, etc.
2.4 Pruning in Lattice Construction
The final translation lattice L(S, 1, J) can grow very
large after the pointer arcs are expanded. We there-
fore apply a word-based language model, via WFST
composition, and perform likelihood-based prun-
ing (Allauzen et al, 2007) based on the combined
translation and language model scores.
Pruning can also be performed on sublattices
during search. One simple strategy is to monitor
the number of states in the determinized lattices
L(N,x, y). If this number is above a threshold, we
expand any pointer arcs and apply a word-based lan-
guage model via composition. The resulting lattice
is then reduced by likelihood-based pruning, after
which the LM scores are removed. This search prun-
ing can be very selective. For example, the pruning
threshold can depend on the height of the cell in the
grid. In this way the risk of search errors can be
controlled.
3 Translation Experiments
We report experiments on the NIST MT08 Arabic-
to-English and Chinese-to-English translation tasks.
We contrast two hierarchical phrase-based decoders.
The first decoder, Hiero Cube Pruning (HCP), is a k-
best decoder using cube pruning implemented as de-
scribed by Chiang (2007). In our implementation, k-
best lists contain unique hypotheses. The second de-
coder, Hiero FST (HiFST), is a lattice-based decoder
implemented with Weighted Finite State Transduc-
ers as described in the previous section. Hypotheses
are generated after determinization under the trop-
ical semiring so that scores assigned to hypotheses
arise from single minimum cost / maximum likeli-
hood derivations. We also use a variant of the k-best
decoder which works in alignment mode: given an
input k-best list, it outputs the feature scores of each
hypothesis in the list without applying any pruning.
This is used for Minimum Error Training (MET)
with the HiFST system.
These two language pairs pose very different
translation challenges. For example, Chinese-
to-English translation requires much greater word
movement than Arabic-to-English. In the frame-
work of hierarchical translation systems, we have
found that shallow decoding (see section 3.2) is
as good as full hierarchical decoding in Arabic-
to-English (Iglesias et al, 2009). In Chinese-to-
English, we have not found this to be the case.
Therefore, we contrast the performance of HiFST
and HCP under shallow hierarchical decoding for
Arabic-to-English, while for Chinese-to-English we
perform full hierarchical decoding.
Both hierarchical translation systems share a
common architecture. For both language pairs,
alignments are generated over the parallel data. The
following features are extracted and used in trans-
lation: target language model, source-to-target and
target-to-source phrase translation models, word and
rule penalties, number of usages of the glue rule,
source-to-target and target-to-source lexical models,
and three rule count features inspired by Bender et
al. (2007). The initial English language model is
a 4-gram estimated over the parallel text and a 965
million word subset of monolingual data from the
English Gigaword Third Edition. Details of the par-
allel corpus and development sets used for each lan-
guage pair are given in their respective section.
Standard MET (Och, 2003) iterative parameter
estimation under IBM BLEU (Papineni et al, 2001)
is performed on the corresponding development set.
For the HCP system, MET is done following Chi-
ang (2007). For the HiFST system, we obtain a k-
437
best list from the translation lattice and extract each
feature score with the aligner variant of the k-best
decoder. After translation with optimized feature
weights, we carry out the two following rescoring
steps.
? Large-LM rescoring. We build sentence-
specific zero-cutoff stupid-backoff (Brants et
al., 2007) 5-gram language models, estimated
using ?4.7B words of English newswire text,
and apply them to rescore either 10000-best
lists generated by HCP or word lattices gener-
ated by HiFST. Lattices provide a vast search
space relative to k-best lists, with translation
lattice sizes of 1081 hypotheses reported in the
literature (Tromble et al, 2008).
? Minimum Bayes Risk (MBR). We rescore the
first 1000-best hypotheses with MBR, taking
the negative sentence level BLEU score as the
loss function (Kumar and Byrne, 2004).
3.1 Building the Rule Sets
We extract hierarchical phrases from word align-
ments, applying the same restrictions as introduced
by Chiang (2005). Additionally, following Iglesias
et al (2009) we carry out two rule filtering strate-
gies:
? we exclude rules with two non-terminals with
the same order on the source and target side
? we consider only the 20 most frequent transla-
tions for each rule
For each development set, this produces approx-
imately 4.3M rules in Arabic-to-English and 2.0M
rules in Chinese-to-English.
3.2 Arabic-to-English Translation
We translate Arabic-to-English with shallow hierar-
chical decoding, i.e. only phrases are allowed to be
substituted into non-terminals. The rules used in this
case are, in addition to the glue rules:
X ? ??s,?s?
X ? ?V ,V ?
V ? ?s,t?
s, t ? T+; ?s, ?s ? ({V } ?T)+
For translation model training, we use all allowed
parallel corpora in the NIST MT08 Arabic track
(?150M words per language). In addition to the
MT08 set itself, we use a development set mt02-05-
tune formed from the odd numbered sentences of the
NIST MT02 through MT05 evaluation sets; the even
numbered sentences form the validation set mt02-
05-test. The mt02-05-tune set has 2,075 sentences.
The cube pruning decoder, HCP, employs k-best
lists of depth k=10000 (unique). Using deeper lists
results in excessive memory and time requirements.
In contrast, the WFST-based decoder, HiFST, re-
quires no local pruning during lattice construction
for this task and the language model is not applied
until the lattice is fully built at the upper-most cell of
the CYK grid.
Table 1 shows results for mt02-05-tune, mt02-
05-test and mt08, as measured by lowercased IBM
BLEU and TER (Snover et al, 2006). MET param-
eters are optimized for the HCP decoder. As shown
in rows ?a? and ?b?, results after MET are compara-
ble.
Search Errors Since both decoders use exactly the
same features, we can measure their search errors on
a sentence-by-sentence basis. A search error is as-
signed to one of the decoders if the other has found
a hypothesis with lower cost. For mt02-05-tune, we
find that in 18.5% of the sentences HiFST finds a hy-
pothesis with lower cost than HCP. In contrast, HCP
never finds any hypothesis with lower cost for any
sentence. This is as expected: the HiFST decoder
requires no pruning prior to applying the language
model, so search is exact.
Lattice/k-best Quality Rescoring results are dif-
ferent for cube pruning and WFST-based decoders.
Whereas HCP improves by 0.9 BLEU, HiFST im-
proves over 1.5 BLEU. Clearly, search errors in HCP
not only affect the 1-best output but also the quality
of the resulting k-best lists. For HCP, this limits the
possible gain from subsequent rescoring steps such
as large LMs and MBR.
Translation Speed HCP requires an average of 1.1
seconds per input word. HiFST cuts this time by
half, producing output at a rate of 0.5 seconds per
word. It proves much more efficient to process com-
pact lattices contaning many hypotheses rather than
to independently processing each one of them in k-
best form.
438
decoder mt02-05-tune mt02-05-test mt08
BLEU TER BLEU TER BLEU TER
a HCP 52.2 41.6 51.5 42.2 42.5 48.6
+5gram 53.1 41.0 52.5 41.5 43.3 48.3
+MBR 53.2 40.8 52.6 41.4 43.4 48.1
b HiFST 52.2 41.5 51.6 42.1 42.4 48.7
+5gram 53.3 40.6 52.7 41.3 43.7 48.1
+MBR 53.7 40.4 53.3 40.9 44.0 48.0
Decoding time in secs/word: 1.1 for HCP; 0.5 for HiFST.
Table 1: Constrative Arabic-to-English translation results (lower-cased IBM BLEU | TER) after MET and subsequent
rescoring steps. Decoding time reported for mt02-05-tune.
The mixed case NIST BLEU-4 for the HiFST sys-
tem on mt08 is 42.9. This is directly comparable to
the official MT08 Constrained Training Track eval-
uation results1.
3.3 Chinese-to-English Translation
We translate Chinese-to-English with full hierarchi-
cal decoding, i.e. hierarchical rules are allowed to be
substituted into non-terminals. We consider a maxi-
mum span of 10 words for the application of hierar-
chical rules and only glue rules are allowed at upper
levels of the CYK grid.
For translation model training, we use all avail-
able data for the GALE 2008 evaluation2, approx.
250M words per language. In addition to the MT08
set itself, we use a development set tune-nw and
a validation set test-nw. These contain a mix of
the newswire portions of MT02 through MT05 and
additional developments sets created by translation
within the GALE program. The tune-nw set has
1,755 sentences.
Again, the HCP decoder employs k-best lists of
depth k=10000. The HiFST decoder applies prun-
ing in search as described in Section 2.4, so that any
lattice in the CYK grid is pruned if it covers at least
3 source words and contains more than 10k states.
The likelihood pruning threshold relative to the best
path in the lattice is 9. This is a very broad threshold
so that very few paths are discarded.
1Full MT08 results are available at http://www.nist.gov/
speech/tests/mt/2008/doc/mt08 official results v0.html. It is
worth noting that many of the top entries make use of system
combination; the results reported here are for single system
translation.
2See http://projects.ldc.upenn.edu/gale/data/catalog.html.
Improved Optimization Table 2 shows results for
tune-nw, test-nw and mt08, as measured by lower-
cased IBM BLEU and TER. The first two rows show
results for HCP when using MET parameters opti-
mized over k-best lists produced by HCP (row ?a?)
and by HiFST (row ?b?). We find that using the k-
best list obtained by the HiFST decoder yields bet-
ter parameters during optimization. Tuning on the
HiFST k-best lists improves the HCP BLEU score,
as well. We find consistent improvements in BLEU;
TER also improves overall, although less consis-
tently.
Search Errors Measured over the tune-nw devel-
opment set, HiFST finds a hypothesis with lower
cost in 48.4% of the sentences. In contrast, HCP
never finds any hypothesis with a lower cost for any
sentence, indicating that the described pruning strat-
egy for HiFST is much broader than that of HCP.
Note that HCP search errors are more frequent for
this language pair. This is due to the larger search
space required in fully hierarchical translation; the
larger the search space, the more search errors will
be produced by the cube pruning k-best implemen-
tation.
Lattice/k-best Quality The lattices produced by
HiFST yield greater gains in LM rescoring than the
k-best lists produced by HCP. Including the subse-
quent MBR rescoring, translation improves as much
as 1.2 BLEU, compared to 0.7 BLEU with HCP.
The mixed case NIST BLEU-4 for the HiFST sys-
tem on mt08 is 27.8, comparable to official results
in the UnConstrained Training Track of the NIST
2008 evaluation.
439
decoder MET k-best tune-nw test-nw mt08
BLEU TER BLEU TER BLEU TER
a HCP HCP 31.6 59.7 31.9 59.7 ? ?
b HCP 31.7 60.0 32.2 59.9 27.2 60.2
+5gram HiFST 32.2 59.3 32.6 59.4 27.8 59.3
+MBR 32.4 59.2 32.7 59.4 28.1 59.3
c HiFST 32.0 60.1 32.2 60.0 27.1 60.5
+5gram HiFST 32.7 58.3 33.1 58.4 28.1 59.1
+MBR 32.9 58.4 33.4 58.5 28.9 58.9
Table 2: Contrastive Chinese-to-English translation results (lower-cased IBM BLEU|TER) after MET and subsequent
rescoring steps. The MET k-best column indicates which decoder generated the k-best lists used in MET optimization.
4 Conclusions
The lattice-based decoder for hierarchical phrase-
based translation described in this paper can be eas-
ily implemented using Weighted Finite State Trans-
ducers. We find many benefits in this approach
to translation. From a practical perspective, the
computational operations required can be easily car-
ried out using standard operations already imple-
mented in general purpose libraries. From a model-
ing perspective, the compact representation of mul-
tiple translation hypotheses in lattice form requires
less pruning in hierarchical search. The result is
fewer search errors and reduced overall memory use
relative to cube pruning over k-best lists. We also
find improved performance of subsequent rescor-
ing procedures which rely on the translation scores.
In direct comparison to k-best lists generated un-
der cube pruning, we find that MET parameter opti-
mization, rescoring with large language models, and
MBR decoding, are all improved when applied to
translations generated by the lattice-based hierarchi-
cal decoder.
Acknowledgments
This work was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects
Agency, Contract No. HR0011- 06-C-0022. G.
Iglesias supported by Spanish Government research
grant BES-2007-15956 (project TEC2006-13694-
C03-03).
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of ACL, pages 557?
564.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Srinivas Bangalore and Giuseppe Riccardi. 2001. A
finite-state approach to machine translation. In Pro-
ceedings of NAACL.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language trans-
lation system. In Proceedings of ASRU, pages 396?
401.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-HLT,
pages 200?208.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858?867.
Francisco Casacuberta. 2001. Finite-state transducers
for speech-input translation. In Proceedings of ASRU.
Jean-Ce?dric Chappelier and Martin Rajman. 1998.
A generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
Jean-Ce?dric Chappelier, Martin Rajman, Ramo?n
Aragu?e?s, and Antoine Rozenknop. 1999. Lattice
parsing for speech recognition. In Proceedings of
TALN, pages 95?104.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270.
440
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-HLT, pages 1012?1020.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training tree transducers. Computational Linguistics,
34(3):391?427.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL, pages 144?151.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
EACL.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?176.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of HLT-EMNLP, pages 161?168.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the ACL-HLT Second Workshop on Syntax
and Structure in Statistical Translation, pages 10?18.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of COLING, pages
505?512.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT, pages 1003?1011.
Lambert Mathias and William Byrne. 2006. Statisti-
cal phrase-based speech translation. In Proceedings
of ICASSP.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231:17?32.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language, vol-
ume 16, pages 69?88.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311?318.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007. Combining outputs from multiple machine
translation systems. In Proceedings of HLT-NAACL,
pages 228?235.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT, pages 577?585.
Khe Chai Sim, William Byrne, Mark Gales, Hichem
Sahbi, and Phil Woodland. 2007. Consensus net-
work decoding for statistical machine translation sys-
tem combination. In Proceedings of ICASSP, vol-
ume 4, pages 105?108.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA, pages 223?231.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-Risk
decoding for statistical machine translation. In Pro-
ceedings of EMNLP, pages 620?629.
Ashish Venugopal, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Proceed-
ings of HLT-NAACL, pages 500?507.
Hao Zhang and Daniel Gildea. 2006. Synchronous bi-
narization for machine translation. In Proceedings of
HLT-NAACL, pages 256?263.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL Workshop on Statistical Ma-
chine Translation, pages 138?141.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of COLING, pages 1145?1152.
441
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1373?1383,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Hierarchical Phrase-Based Translation Representations
Gonzalo Iglesias? Cyril Allauzen? William Byrne?
Adria` de Gispert? Michael Riley?
?Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
{gi212,wjb31,ad465}@eng.cam.ac.uk
? Google Research, 76 Ninth Avenue, New York, NY 10011
{allauzen,riley}@google.com
Abstract
This paper compares several translation rep-
resentations for a synchronous context-free
grammar parse including CFGs/hypergraphs,
finite-state automata (FSA), and pushdown
automata (PDA). The representation choice is
shown to determine the form and complex-
ity of target LM intersection and shortest-path
algorithms that follow. Intersection, shortest
path, FSA expansion and RTN replacement al-
gorithms are presented for PDAs. Chinese-to-
English translation experiments using HiFST
and HiPDT, FSA and PDA-based decoders,
are presented using admissible (or exact)
search, possible for HiFST with compact
SCFG rulesets and HiPDT with compact LMs.
For large rulesets with large LMs, we intro-
duce a two-pass search strategy which we then
analyze in terms of search errors and transla-
tion performance.
1 Introduction
Hierarchical phrase-based translation, using a syn-
chronous context-free translation grammar (SCFG)
together with an n-gram target language model
(LM), is a popular approach in machine transla-
tion (Chiang, 2007). Given a SCFG G and an n-
gram language model M , this paper focuses on how
to decode with them, i.e. how to apply them to the
source text to generate a target translation. Decod-
ing has three basic steps, which we first describe
in terms of the formal languages and relations in-
volved, with data representations and algorithms to
follow.
1. Translating the source sentence s with G
to give target translations: T = {s} ? G,
a (weighted) context-free language resulting
from the composition of a finite language and
the algebraic relation G for SCFG G.
2. Applying the language model to these target
translations: L=T ?M, a (weighted) context-
free language resulting from the intersection
of a context-free language and the regular lan-
guage M for M .
3. Searching for the translation and language
model combination with the highest-probablity
path: L?=argmaxl?LL
Of course, decoding requires explicit data represen-
tations and algorithms for combining and searching
them. In common to the approaches we will con-
sider here, s is applied to G by using the CYK algo-
rithm in Step 1 and M is represented by a finite au-
tomaton in Step 2. The choice of the representation
of T in many ways determines the remaining de-
coder representations and algorithms needed. Since
{s} is a finite language and we assume through-
out that G does not allow unbounded insertions,
T and L are, in fact, regular languages. As such,
T and L have finite automaton representations Tf
and Lf . In this case, weighted finite-state intersec-
tion and single-source shortest path algorithms (us-
ing negative log probabilities) can be used to solve
Steps 2 and 3 (Mohri, 2009). This is the approach
taken in (Iglesias et al, 2009a; de Gispert et al,
2010). Instead T and L can be represented by hy-
pergraphs Th and Lh (or very similarly context-free
rules, and-or trees, or deductive systems). In this
case, hypergraph intersection with a finite automa-
ton and hypergraph shortest path algorithms can be
used to solve Steps 2 and 3 (Huang, 2008). This
is the approach taken by Chiang (2007). In this
paper, we will consider another representation for
context-free languages T and L as well, pushdown
automata (PDA) Tp and Lp, familiar from formal
1373
language theory (Aho and Ullman, 1972). We will
describe PDA intersection with a finite automaton
and PDA shortest-path algorithms in Section 2 that
can be used to solve Steps 2 and 3. It cannot be
over-emphasized that the CFG, hypergraph and PDA
representations of T are used for their compactness
rather than for expressing non-regular languages.
As presented so far, the search performed in Step
3 is admissible (or exact) ? the true shortest path
is found. However, the search space in MT can be
quite large. Many systems employ aggressive prun-
ing during the shortest-path computation with little
theoretical or empirical guarantees of correctness.
Further, such pruning can greatly complicate any
complexity analysis of the underlying representa-
tions and algorithms. In this paper, we will exclude
any inadmissible pruning in the shortest-path algo-
rithm itself. This allows us in Section 3 to compare
the computational complexity of using these differ-
ent representations. We show that the PDA represen-
tation is particularly suited for decoding with large
SCFGs and compact LMs.
We present Chinese-English translation results
under the FSA and PDA translation representations.
We describe a two-pass translation strategy which
we have developed to allow use of the PDA repre-
sentation in large-scale translation. In the first pass,
translation is done using a lattice-generating version
of the shortest path algorithm. The full translation
grammar is used but with a compact, entropy-pruned
version (Stolcke, 1998) of the full language model.
This first-step uses admissible pruning and lattice
generation under the compact language model. In
the second pass, the original, unpruned LM is simply
applied to the lattices produced in the first pass. We
find that entropy-pruning and first-pass translation
can be done so as to introduce very few search errors
in the overall process; we can identify search errors
in this experiment by comparison to exact transla-
tion under the full translation grammar and language
model using the FSA representation. We then inves-
tigate a translation grammar which is large enough
that exact translation under the FSA representation
is not possible. We find that translation is possible
using the two-pass strategy with the PDA translation
representation and that gains in BLEU score result
from using the larger translation grammar.
1.1 Related Work
There is extensive prior work on computational ef-
ficiency and algorithmic complexity in hierarchical
phrase-based translation. The challenge is to find al-
gorithms that can be made to work with large trans-
lation grammars and large language models.
Following the original algorithms and analysis of
Chiang (2007), Huang and Chiang (2007) devel-
oped the cube-growing algorithm, and more recently
Huang and Mi (2010) developed an incremental de-
coding approach that exploits left-to-right nature of
the language models.
Search errors in hierarchical translation, and in
translation more generally, have not been as exten-
sively studied; this is undoubtedly due to the diffi-
culties inherent in finding exact translations for use
in comparison. Using a relatively simple phrase-
based translation grammar, Iglesias et al (2009b)
compared search via cube-pruning to an exact FST
implementation (Kumar et al, 2006) and found that
cube-pruning suffered significant search errors. For
Hiero translation, an extensive comparison of search
errors between the cube pruning and FSA imple-
mentation was presented by Iglesias et al (2009a)
and de Gispert et al (2010). Relaxation techniques
have also recently been shown to finding exact so-
lutions in parsing (Koo et al, 2010) and in SMT
with tree-to-string translation grammars and trigram
language models (Rush and Collins, 2011), much
smaller models compared to the work presented in
this paper.
Although entropy-pruned language models have
been used to produce real-time translation sys-
tems (Prasad et al, 2007), we believe our use of
entropy-pruned language models in two-pass trans-
lation to be novel. This is an approach that is widely-
used in automatic speech recognition (Ljolje et al,
1999) and we note that it relies on efficient represen-
tation of very large search spaces T for subsequent
rescoring, as is possible with FSAs and PDAs.
2 Pushdown Automata
In this section, we formally define pushdown au-
tomata and give intersection, shortest-path and re-
lated algorithms that will be needed later.
Informally, pushdown automata are finite au-
tomata that have been augmented with a stack. Typ-
1374
0
1a
2
?
(
3)b
0
1
a
2
?
(
?
3
)
?
b
(a) (b)
0
1
(
3
?
2
a
4(
)
5
b
)
0,?
1,(
?
3,?
?
2,(a
4,(?
?
5,(
b
?
(c) (d)
Figure 1: PDA Examples: (a) Non-regular PDA accept-
ing {anbn|n ? N}. (b) Regular (but not bounded-stack)
PDA accepting a?b?. (c) Bounded-stack PDA accepting
a?b? and (d) its expansion as an FSA.
ically this is done by adding a stack alphabet and la-
beling each transition with a stack operation (a stack
symbol to be pushed onto, popped or read from the
stack) in additon to the usual input label (Aho and
Ullman, 1972; Berstel, 1979) and weight (Kuich
and Salomaa, 1986; Petre and Salomaa, 2009). Our
equivalent representation allows a transition to be la-
beled by a stack operation or a regular input symbol
but not both. Stack operations are represented by
pairs of open and close parentheses (pushing a sym-
bol on and popping it from the stack). The advantage
of this representation is that is identical to the finite
automaton representation except that certain sym-
bols (the parentheses) have special semantics. As
such, several finite-state algorithms either immedi-
ately generalize to this PDA representation or do so
with minimal changes. The algorithms described in
this section have been implemented in the PDT ex-
tension (Allauzen and Riley, 2011) of the OpenFst
library (Allauzen et al, 2007).
2.1 Definitions
A (restricted) Dyck language consist of ?well-
formed? or ?balanced? strings over a finite num-
ber of pairs of parentheses. Thus the string
( [ ( ) ( ) ] { } [ ] ) ( ) is in the Dyck language over 3
pairs of parentheses.
More formally, let A and A be two finite alpha-
bets such that there exists a bijection f from A to
A. Intuitively, f maps an open parenthesis to its cor-
responding close parenthesis. Let a? denote f(a) if
a ? A and f?1(a) if a ? A. The Dyck language
DA over the alphabet A? = A ? A is then the lan-
guage defined by the following context-free gram-
mar: S ? , S ? SS and S ? aSa? for all a?A.
We define the mapping cA : A?? ? A?? as follow.
cA(x) is the string obtained by iteratively deleting
from x all factors of the form aa? with a ? A. Ob-
serve that DA=c?1A ().
Let A and B be two finite alphabets such that
B ? A, we define the mapping rB : A? ? B?
by rB(x1 . . . xn) = y1 . . . yn with yi = xi if xi ?B
and yi= otherwise.
A weighted pushdown automaton (PDA) T over
the tropical semiring (R ? {?},min,+,?, 0) is
a 9-tuple (?,?,?, Q,E, I, F, ?) where ? is the fi-
nite input alphabet, ? and ? are the finite open and
close parenthesis alphabets, Q is a finite set of states,
I ?Q the initial state, F ? Q the set of final states,
E ? Q ? (? ? ?? ? {}) ? (R ? {?}) ? Q a fi-
nite set of transitions, and ? : F ? R ? {?} the
final weight function. Let e= (p[e], i[e], w[e], n[e])
denote a transition in E.
A path pi is a sequence of transitions pi=e1 . . . en
such that n[ei]=p[ei+1] for 1 ? i < n. We then de-
fine p[pi] = p[e1], n[pi] = n[en], i[pi] = i[e1] ? ? ? i[en],
and w[pi]=w[e1] + . . . + w[en].
A path pi is accepting if p[pi] = I and n[pi] ? F .
A path pi is balanced if r??(i[pi]) ?D?. A balanced
path pi accepts the string x ? ?? if it is a balanced
accepting path such that r?(i[pi])=x.
The weight associated by T to a string x ? ??
is T (x) = minpi?P (x) w[pi] + ?(n[pi]) where P (x)
denotes the set of balanced paths accepting x. A
weighted language is recognizable by a weighted
pushdown automaton iff it is context-free. We de-
fine the size of T as |T |= |Q|+|E|.
A PDA T has a bounded stack if there exists K ?
N such that for any sub-path pi of any balanced path
in T : |c?(r??(i[pi]))| ? K . If T has a bounded stack,
then it represents a regular language. Figure 1 shows
non-regular, regular and bounded-stack PDAs.
A weighted finite automaton (FSA) can be viewed
as a PDA where the open and close parentheses al-
phabets are empty, see (Mohri, 2009) for a stand-
alone definition.
1375
2.2 Expansion Algorithm
Given a bounded-stack PDA T , the expansion of T
is the FSA T ? equivalent to T defined as follows.
A state in T ? is a pair (q, z) where q is a state in T
and z ???. A transition (q, a, w, q?) in T results in
a transition ((q, z), a?, w, (q?, z?)) in T ? only when:
(a) a?? ? {}, z? =z and a? =a, (b) a??, z? =za
and a? = , or (c) a ? ?, z? is such that z = z?a
and a? = . The initial state of T ? is I ? = (I, ). A
state (q, z) in T ? is final if q is final in T and z = 
(??((q, ))=?(q)). The set of states of T ? is the set of
pairs (q, z) that can be reached from an initial state
by transitions defined as above. The condition that
T has a bounded stack ensures that this set is finite
(since it implies that for any (q, z), |z| ? K).
The complexity of the algorithm is linear in
O(|T ?|)=O(e|T |). Figure 1d show the result of the
algorithm when applied to the PDA of Figure 1c.
2.3 Intersection Algorithm
The class of weighted pushdown automata is closed
under intersection with weighted finite automata
(Bar-Hillel et al, 1964; Nederhof and Satta, 2003).
Considering a pair (T1, T2) where one element is an
FSA and the other element a PDA, then there exists
a PDA T1?T2, the intersection of T1 and T2, such
that for all x ? ??: (T1?T2)(x) = T1(x)+T2(x).
We assume in the following that T2 is an FSA. We
also assume that T2 has no input- transitions. When
T2 has input- transitions, an epsilon filter (Mohri,
2009; Allauzen et al, 2011) generalized to handle
parentheses can be used.
A state in T =T1?T2 is a pair (q1, q2) where q1 is
a state of T1 and q2 a state of T2. The initial state is
I=(I1, I2). Given a transition e1=(q1, a, w1, q?1) in
T1, transitions out of (q1, q2) in T are obtained using
the following rules.
If a ? ?, then e1 can be matched with a tran-
sition (q2, a, w2, q?2) in T2 resulting a transition
((q1, q2), a, w1+w2, (q?1, q?2)) in T .
If a = , then e1 is matched with staying in q2
resulting in a transition ((q1, q2), , w1, (q?1, q2)).
Finally, if a ? ??, e1 is also matched
with staying in q2, resulting in a transition
((q1, q2), a, w1, (q?1, q2)) in T .
A state (q1, q2) in T is final when both q1 and q2
are final, and then ?((q1, q2))=?1(q1)+?2(q2).
SHORTESTDISTANCE(T )
1 for each q ? Q and a ? ? do
2 B[q, a]? ?
3 GETDISTANCE(T, I)
4 return d[f, I ]
RELAX(q, s, w,S)
1 if d[q, s] > w then
2 d[q, s]? w
3 if q 6? S then
4 ENQUEUE(S , q)
GETDISTANCE(T,s)
1 for each q ? Q do
2 d[q, s]??
3 d[s, s]? 0
4 Ss ? s
5 while Ss 6=? do
6 q ? HEAD(Ss)
7 DEQUEUE(Ss)
8 for each e ? E[q] do
9 if i[e] ? ? ? {} then
10 RELAX(n[e], s, d[q, s] + w[e],Ss)
11 elseif i[e] ? ? then
12 B[s, i[e]]? B[s, i[e]] ? {e}
13 elseif i[e] ? ? then
14 if d[n[e], n[e]] is undefined then
15 GETDISTANCE(T, n[e])
16 for each e? ? B[n[e], i[e]] do
17 w? d[q, s] +w[e] + d[p[e?], n[e]] + w[e?]
18 RELAX(n[e?], s, w,Ss)
Figure 2: PDA shortest distance algorithm. We assume
that F ={f} and ?(f)=0 to simplify the presentation.
The complexity of the algorithm is in O(|T1||T2|).
2.4 Shortest Distance and Path Algorithms
A shortest path in a PDA T is a balanced accepting
path with minimal weight and the shortest distance
in T is the weight of such a path. We show that when
T has a bounded stack, shortest distance and short-
est path can be computed in O(|T |3 log |T |) time
(assuming T has no negative weights) and O(|T |2)
space.
Given a state s in T with at least one incoming
open parenthesis transition, we denote by Cs the set
of states that can be reached from s by a balanced
path. If s has several incoming open parenthesis
transitions, a naive implementation might lead to the
states in Cs to be visited up to exponentially many
times. The basic idea of the algorithm is to memo-
ize the shortest distance from s to states in Cs. The
1376
pseudo-code is given in Figure 2.
GETDISTANCE(T, s) starts a new instance of the
shortest-distance algorithm from s using the queue
Ss, initially containing s. While the queue is not
empty, a state is dequeued and its outgoing transi-
tions examined (line 5-9). Transitions labeled by
non-parenthesis are treated as in Mohri (2009) (line
9-10). When the considered transition e is labeled by
a close parenthesis, it is remembered that it balances
all incoming open parentheses in s labeled by i[e]
by adding e to B[s, i[e]] (line 11-12). Finally, when
e is labeled with an open parenthesis, if its destina-
tion has not already been visited, a new instance is
started from n[e] (line 14-15). The destination states
of all transitions balancing e are then relaxed (line
16-18).
The space complexity of the algorithm is
quadratic for two reasons. First, the number of
non-infinity d[q, s] is |Q|2. Second, the space re-
quired for storing B is at most in O(|E|2) since
for each open parenthesis transition e, the size of
|B[n[e], i[e]]| is O(|E|) in the worst case. This
last observation also implies that the cumulated
number of transitions examined at line 16 is in
O(N |Q| |E|2) in the worst case, where N denotes
the maximal number of times a state is inserted in
the queue for a given call of GETDISTANCE. As-
suming the cost of a queue operation is ?(n) for a
queue containing n elements, the worst-case time
complexity of the algorithm can then be expressed
as O(N |T |3 ?(|T |)). When T contains no negative
weights, using a shortest-first queue discipline leads
to a time complexity in O(|T |3 log |T |). When all
the Cs?s are acyclic, using a topological order queue
discipline leads to a O(|T |3) time complexity.
In effect, we are solving a k-sources shortest-
path problem with k single-source solutions. A po-
tentially better approach might be to solve the k-
sources or k-pairs problem directly (Hershberger et
al., 2003).
When T has been obtained by converting an RTN
or an hypergraph into a PDA (Section 2.5), the poly-
nomial dependency in |T | becomes a linear depen-
dency both for the time and space complexities. In-
deed, for each q in T , there exists a unique s such
that d[q, s] is non-infinity. Moreover, for each close
parenthesis transistion e, there exists a unique open
parenthesis transition e? such that e?B[n[e?], i[e?]].
When each component of the RTN is acyclic, the
complexity of the algorithm is hence in O(|T |) in
time and space.
The algorithm can be modified to compute the
shortest path by keeping track of parent pointers.
2.5 Replacement Algorithm
A recursive transition network (RTN) can be speci-
fied by (N,?, (T?)??N , S) where N is an alphabet
of nonterminals, ? is the input alphabet, (T?)??N is
a family of FSAs with input alphabet ? ? N , and
S?N is the root nonterminal.
A string x ? ?? is accepted by R if there exists
an accepting path pi in TS such that recursively re-
placing any transition with input label ? ?N by an
accepting path in T? leads to a path pi? with input x.
The weight associated by R is the minimum over all
such pi? of w[pi?]+?S(n[pi?]).
Given an RTN R, the replacement of R is the
PDA T equivalent to R defined by the 9-tuple
(?,?,?, Q,E, I, F, ?, ?) with ?=Q=???N Q? ,
I = IS , F = FS , ?= ?S , and E =
?
??N
?
e?E? Ee
where Ee = {e} if i[e] 6? N and Ee =
{(p[e], n[e], w[e], I?), (f, n[e], ??(f), n[e])|f ?F?}
with ?= i[e]?N otherwise.
The complexity of the construction is in O(|T |).
If |F? | = 1, then |T | = O(
?
??N |T? |) = O(|R|).
Creating a superfinal state for each T? would lead to
a T whose size is always linear in the size of R.
3 Hierarchical Phrase-Based Translation
Representation
In this section, we compare several different repre-
sentations for the target translations T of the source
sentence s by synchronous CFG G prior to language
model M application. As discussed in the introduc-
tion, T is a context-free language. For example, sup-
pose it corresponds to:
S?abXdg, S?acXfg, and X?bc.
Figure 3 shows several alternative representations of
T : Figure 3a shows the hypergraph representation of
this grammar; there is a 1:1 correspondence between
each production in the CFG and each hyperedge in
the hypergraph. Figure 3b shows the RTN represen-
tation of this grammar with a 1:1 correspondence be-
tween each production in the CFG and each path in
the RTN; this is the translation representation pro-
1377
SX
3 3
a
1 1
b
2
1
c
2
2
d
4
f
4
g
5 5
(a) Hypergraph
0
1a
6
a
2b
7c
3X 4d 5g
8X 9f 10g 11 12b 13c
S X
(b) RTN
0
1a
6
a
2b
7c
11
(
12b
3 4d 5g
[
8 9f 10g
13c
)
]
(c) PDA
0,?
1,?a
6,?
a
2,?b
7,?c
11,(?
11,[?
12,(b
12,[b
13,(c
13,[c
3,??
8,??
4,?d
9,?f
5,?g
10,?g
(d) FSA
Figure 3: Alternative translation representations
duced by the HiFST decoder (Iglesias et al, 2009a;
de Gispert et al, 2010). Figure 3c shows the push-
down automaton representation generated from the
RTN with the replacement algorithm of Section 2.5.
Since s is a finite language and G does not allow
unbounded insertion, Tp has a bounded stack and
T is, in fact, a regular language. Figure 3d shows
the finite-state automaton representation of T gen-
erated by the PDA using the expansion algorithm
of Section 2.2. The HiFST decoder converts its
RTN translation representation immediately into the
finite-state representation using an algorithm equiv-
alent to converting the RTN into a PDA followed by
PDA expansion.
As shown in Figure 4, an advantage of the RTN,
PDA, and FSA representations is that they can bene-
fit from FSA epsilon removal, determinization and
minimization algorithms applied to their compo-
nents (for RTNs and PDAs) or their entirety (for
FSAs). For the complexity discussion below, how-
ever, we disregard these optimizations. Instead we
focus on the complexity of each MT step described
in the introduction:
1. SCFG Translation: Assuming that the parsing
of the input is performed by a CYK parse, then
the CFG, hypergraph, RTN and PDA represen-
0 1a
2b
3
c
4X
5X
6
d
f 7
g
0 1b 2c
S X
(a) RTN
0 1a
2b
3
c 8
(
[ 9
b
4
6
d
7g
5
f1 0c
)
]
(b) PDA
0 1a
2b
3
c
4b
5b
6c
7c
8
d
f 9
g
(c) FSA
Figure 4: Optimized translation representations
tations can be generated in O(|s|3|G|) time and
space (Aho and Ullman, 1972). The FSA rep-
resentation can require an additional O(e|s|3|G|)
time and space since the PDA expansion can be
exponential.
2. Intersection: The intersection of a CFG Th
with a finite automaton M can be performed by
the classical Bar-Hillel algorithm (Bar-Hillel
et al, 1964) with time and space complex-
ity O(|Th||M |3).1 The PDA intersection algo-
rithm from Section 2.3 has time and space com-
plexity O(|Tp||M |). Finally, the FSA intersec-
tion algorithm has time and space complexity
O(|Tf ||M |) (Mohri, 2009).
3. Shortest Path: The shortest path algorithm on
the hypergraph, RTN, and FSA representations
requires linear time and space (given the under-
lying acyclicity) (Huang, 2008; Mohri, 2009).
As presented in Section 2.4, the PDA rep-
resentation can require time cubic and space
quadratic in |M |.2
Table 1 summarizes the complexity results. Note
the PDA representation is equivalent in time and su-
perior in space to the CFG/hypergraph representa-
tion, in general, and it can be superior in both space
1The modified Bar-Hillel construction described by Chi-
ang (2007) has time and space complexity O(|Th||M |4); the
modifications were introduced presumably to benefit the subse-
quent pruning method employed (but see Huang et al (2005)).
2The time (resp. space) complexity is not cubic (resp.
quadratic) in |Tp||M |. Given a state q in Tp, there exists a
unique sq such that q belongs to Csq . Given a state (q1, q2)
in Tp ?M , (q1, q2) ? C(s1,s2) only if s1 = sq1 , and hence
(q1, q2) belongs to at most |M | components.
1378
Representation Time Complexity Space Complexity
CFG/hypergraph O(|s|3 |G| |M |3) O(|s|3 |G| |M |3)
PDA O(|s|3 |G| |M |3) O(|s|3 |G| |M |2)
FSA O(e|s|3|G| |M |) O(e|s|3|G| |M |)
Table 1: Complexity using various target translation rep-
resentations.
and time to the FSA representation depending on the
relative SCFG and LM sizes. The FSA representa-
tion favors smaller target translation sets and larger
language models. Should a better complexity PDA
shortest path algorithm be found, this conclusion
could change. In practice, the PDA and FSA rep-
resentations benefit hugely from the optimizations
mentioned above, these optimizations improve the
time and space usage by one order of magnitude.
4 Experimental Framework
We use two hierarchical phrase-based SMT de-
coders. The first one is a lattice-based decoder im-
plemented with weighted finite-state transducers (de
Gispert et al, 2010) and described in Section 3. The
second decoder is a modified version using PDAs
as described in Section 2. In order to distinguish
both decoders we call them HiFST and HiPDT, re-
spectively. The principal difference between the two
decoders is where the finite-state expansion step is
done. In HiFST, the RTN representation is immedi-
ately expanded to an FSA. In HiPDT, this expansion
is delayed as late as possible - in the output of the
shortest path algorithm. Another possible configu-
ration is to expand after the LM intersection step but
before the shortest path algorithm; in practice this is
quite similar to HiFST.
In the following sections we report experiments
in Chinese-to-English translation. For translation
model training, we use a subset of the GALE 2008
evaluation parallel text;3 this is 2.1M sentences and
approximately 45M words per language. We re-
port translation results on a development set tune-nw
(1,755 sentences) and a test set test-nw (1,671 sen-
tences). These contain translations produced by the
GALE program and portions of the newswire sec-
tions of MT02 through MT06. In tuning the sys-
3See http://projects.ldc.upenn.edu/gale/data/catalog.html.
We excluded the UN material and the LDC2002E18,
LDC2004T08, LDC2007E08 and CUDonga collections.
0 7.5? 10?9 7.5? 10?8 7.5? 10?7
207.5 20.2 4.1 0.9
Table 2: Number of ngrams (in millions) in the 1st pass 4-gram
language models obtained with different ? values (top row).
tems, standard MERT (Och, 2003) iterative param-
eter estimation under IBM BLEU4 is performed on
the development set.
The parallel corpus is aligned using MTTK (Deng
and Byrne, 2008) in both source-to-target and
target-to-source directions. We then follow stan-
dard heuristics (Chiang, 2007) and filtering strate-
gies (Iglesias et al, 2009b) to extract hierarchical
phrases from the union of the directional word align-
ments. We call a translation grammar the set of rules
extracted from this process. We extract two transla-
tion grammars:
? A restricted grammar where we apply the fol-
lowing additional constraint: rules are only
considered if they have a forward translation
probability p > 0.01. We call this G1. As will
be discussed later, the interest of this grammar
is that decoding under it can be exact, that is,
without any pruning in search.
? An unrestricted one without the previous con-
straint. We call this G2. This is a superset of
the previous grammar, and exact search under
it is not feasible for HiFST: pruning is required
in search.
The initial English language model is a Kneser-
Ney 4-gram estimated over the target side of the par-
allel text and the AFP and Xinhua portions of mono-
lingual data from the English Gigaword Fourth Edi-
tion (LDC2009T13). This is a total of 1.3B words.
We will call this language model M1. For large lan-
guage model rescoring we also use the LM M2 ob-
tained by interpolating M1 with a zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram estimated using
6.6B words of English newswire text.
We next describe how we build translation sys-
tems using entropy-pruned language models.
1. We build a baseline HiFST system that uses M1
and a hierarchical grammar G, parameters be-
ing optimized with MERT under BLEU.
4See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
1379
2. We then use entropy-based pruning of the lan-
guage model (Stolcke, 1998) under a relative
perplexity threshold of ? to reduce the size of
M1. We will call the resulting language model
as M?1 . Table 2 shows the number of n-grams
(in millions) obtained for different ? values.
3. We translate with M?1 using the same param-
eters obtained in MERT in step 1, except for
the word penalty, tuned over the lattices under
BLEU performance. This produces a transla-
tion lattice in the topmost cell that contains hy-
potheses with exact scores under the translation
grammar and M?1 .
4. Translation lattices in the topmost cell are
pruned with a likelihood-based beam width ?.
5. We remove the M?1 scores from the pruned
translation lattices and reapply M1, moving the
word penalty back to the original value ob-
tained in MERT. These operations can be car-
ried out efficiently via standard FSA opera-
tions.
6. Additionally, we can rescore the translation lat-
tices obtained in steps 1 or 5 with the larger
language model M2. Again, this can be done
via standard FSA operations.
Note that if ?=? or if ?=0, the translation lattices
obtained in step 1 should be identical to the ones of
step 5. While the goal is to increase ? to reduce the
size of the language model used at Step 3, ? will
have to increase accordingly so as to avoid pruning
away desirable hypotheses in Step 4. If ? defines
a sufficiently wide beam to contain the hypotheses
which would be favoured by M1, faster decoding
with M?1 would be possible without incurring search
errors M1. This is investigated next.
5 Entropy-Pruned LM in Rescoring
In Table 3 we show translation performance under
grammar G1 for different values of ?. Performance
is reported after first-pass decoding with M?1 (see
step 3 in Section 4), after rescoring with M1 (see
step 5) and after rescoring with M2 (see step 6). The
baseline (experiment number 1) uses ? = 0 (that is,
M1) for decoding.
Under translation grammar G1, HiFST is able to
generate an FSA with the entire space of possible
candidate hypotheses. Therefore, any degradation
in performance is only due to the M?1 involved in
decoding and the ? applied prior to rescoring.
As shown in row number 2, for ? ? 10?9 the
system provides the same performance to the base-
line when ? > 8, while decoding time is reduced
by roughly 40%. This is because M?1 is 10% of the
size of the original language model M1, as shown
in Table 2. As M?1 is further reduced by increas-
ing ? (see rows number 3 and 4), decoding time is
also reduced. However, the beam width ? required
in order to recover the good hypotheses in rescoring
increases, reaching 12 for experiment 3 and 15 for
experiment 4.
Regarding rescoring with the larger M2 (step 6
in Section 4), the system is also able to match the
baseline performance as long as ? is wide enough,
given the particular M?1 used in first-pass decoding.
Interestingly, results show that a similar ? value is
needed when rescoring either with M1 or M2.
The usage of entropy-pruned language models in-
crements speed at the risk of search errors. For in-
stance, comparing the outputs of systems 1 and 2
with ?=10 in Table 3 we find 45 different 1-best hy-
potheses, even though the BLEU score is identical.
In other words, we have 45 cases in which system 2
is not able to recover the baseline output because the
1st-pass likelihood beam ? is not wide enough. Sim-
ilarly, system 3 fails in 101 cases (? =12) and sys-
tem 4 fails in 95 cases. Interestingly, some of these
sentences would require impractically huge beams.
This might be due to the Kneser-Ney smoothing,
which interacts badly with entropy pruning (Chelba
et al, 2010).
6 Hiero with PDAs and FSAs
In this section we contrast HiFST with HiPDT under
the same translation grammar and entropy-pruned
language models. Under the constrained grammar
G1 their performance is identical as both decoders
can generate the entire search space which can then
be rescored with M1 or M2 as shown in the previous
section.
Therefore, we now focus on the unconstrained
grammar G2, where exact search is not feasible for
HiFST. In order to evaluate this problem, we run
both decoders over tune-nw, restricting memory us-
age to 10 gigabytes. If this limit is reached in decod-
1380
HiFST (G1 + M?1 ) +M1 +M2
# ? tune-nw test-nw time ? tune-nw test-nw tune-nw test-nw
1 0 (M1) 34.3 34.5 0.68 - - - 34.8 35.6
2 7.5? 10?9 32.0 32.8 0.38 10 34.8 35.6
9 34.3 34.5 34.9 35.5
8
3 7.5? 10?8 29.5 30.0 0.28 12 34.2 34.5 34.7 35.6
9 34.3 34.4 34.8 35.2
8 34.2 35.1
4 7.5? 10?7 26.0 26.4 0.20 15 34.2 34.5 34.7 35.6
12 34.4 35.5
Table 3: Results (lowercase IBM BLEU scores) under G1 with various M?1 as obtained with several values of ?.
Performance in subsequent rescoring with M1 and M2 after likelihood-based pruning of the translation lattices for
various ? is also reported. Decoding time, in seconds/word over test-nw, refers strictly to first-pass decoding.
Exact search for G2 + M?1 with memory usage under 10 GB
# ? HiFST HiPDT
Success Failure Success Failure
Expand Compose Compose Expand
2 7.5? 10?9 12 51 37 40 8 52
3 7.5? 10?8 16 53 31 76 1 23
4 7.5? 10?7 18 53 29 99.8 0 0.2
Table 4: Percentage of success in producing the 1-best translation under G2 with various M?1 when applying a hard
memory limitation of 10 GB, as measured over tune-nw (1755 sentences). If decoder fails, we report what step was
being done when the limit was reached. HiFST could be expanding into an FSA or composing the FSA with M?1 ;
HiPDT could be PDA composing with M?1 or PDA expanding into an FSA.
HiPDT (G2 + M?1 ) +M1 +M2
? tune-nw test-nw ? tune-nw test-nw tune-nw test-nw
7.5? 10?7 25.7 26.3 15 34.6 34.8 35.2 36.1
Table 5: HiPDT performance on grammar G2 with ? = 7.5 ? 10?7. Exact search with HiFST is not possible under
these conditions: pruning during search would be required.
ing, the process is killed5. We report what internal
decoding operation caused the system to crash. For
HiFST, these include expansion into an FSA (Ex-
pand) and subsequent intersection with the language
model (Compose). For HiPDT, these include PDA
intersection with the language model (Compose) and
subsequent expansion into an FSA (Expand), using
algorithms described in Section 2.
Table 4 shows the number of times each decoder
succeeds in finding a hypothesis given the memory
limit, and the operations being carried out when they
fail to do so, when decoding with various M?1 . With
?=7.5? 10?9 (row 2), HiFST can only decode 218
sentences, while HiPDT succeeds in 703 cases. The
5We used ulimit command. The experiment was carried out
over machines with different configurations and load. There-
fore, these numbers must be considered as approximate values.
differences between both decoders increase as the
M?1 is more reduced, and for ?=7.5?10?7 (row 4),
HiPDT is able to perform exact search over all but
three sentences.
Table 5 shows performance using the latter con-
figuration (Table 4, row 4). After large language
model rescoring, HiPDT improves 0.5 BLEU over
baseline with G1 (Table 3, row 1).
7 Discussion and Conclusion
HiFST fails to decode mainly because the expansion
into an FST leads to far too big search spaces (e.g.
fails 938 times under ? = 7.5 ? 10?8). If it suc-
ceeds in expanding the search space into an FST,
the decoder still has to compose with the language
model, which is also critical in terms of memory us-
1381
age (fails 536 times). In contrast, HiPDT creates a
PDA, which is a more compact representation of the
search space and allows efficient intersection with
the language model before expansion into an FST.
Therefore, the memory usage is considerably lower.
Nevertheless, the complexity of the language model
is critical for the PDA intersection and very specially
the PDA expansion into an FST (fails 403 times for
?=7.5? 10?8).
With the algorithms presented in this paper, de-
coding with PDAs is possible for any translation
grammar as long as an entropy pruned LM is used.
While this allows exact decoding, it comes at the
cost of making decisions based on less complex
LMs, although this has been shown to be an ad-
equate strategy when applying compact CFG rule-
sets. On the other hand, HiFST cannot decode under
large translation grammars, thus requiring pruning
during lattice construction, but it can apply an un-
pruned LM in this process. We find that with care-
fully designed pruning strategies, HiFST can match
the performance of HiPDT reported in Table 5. But
without pruning in search, expansion directly into an
FST would lead to an explosion in terms of memory
usage. Of course, without memory constraints both
strategies would reach the same performance.
Overall, these results suggest that HiPDT is more
robust than HiFST when using complex hierarchi-
cal grammars. Conversely, FSTs might be more
efficient for search spaces described by more con-
strained hierarchical grammars. This suggests that
a hybrid solution could be effective: we could use
PDAs or FSTs e.g. depending on the number of
states of the FST representing the expanded search
space, or other conditions.
8 Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Advanced
Research Projects Agency, Contract No.HR0011-
06-C-0022, and a Google Faculty Research Award,
May 2010.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation and Compiling, volume 1-2.
Prentice-Hall.
Cyril Allauzen and Michael Riley, 2011. Pushdown
Transducers. http://pdt.openfst.org.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of CIAA, pages 11?23.
http://www.openfst.org.
Cyril Allauzen, Michael Riley, and Johan Schalkwyk.
2011. Filters for efficient composition of weighted
finite-state transducers. In Proceedings of CIAA, vol-
ume 6482 of LNCS, pages 28?38. Springer.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Y. Bar-Hillel, editor, Language and Information: Se-
lected Essays on their Theory and Application, pages
116?150. Addison-Wesley.
Jean Berstel. 1979. Transductions and Context-Free
Languages. Teubner.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858?867.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng
Xu. 2010. Study on interaction between entropy prun-
ing and kneser-ney smoothing. In Proceedings of In-
terspeech, pages 2242?2245.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3).
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494?507.
Manfred Drosde, Werner Kuick, and Heiko Vogler, ed-
itors. 2009. Handbook of Weighted Automata.
Springer.
John Hershberger, Subhash Suri, and Amit Bhosle. 2003.
On the difficulty of some shortest path problems. In
Proceedings of STACS, volume 2607 of LNCS, pages
343?354. Springer.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL, pages 144?151.
1382
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273?283.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with hooks.
In Proceedings of the Ninth International Workshop
on Parsing Technology, Parsing ?05, pages 65?73,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Liang Huang. 2008. Advanced dynamic programming in
semiring and hypergraph frameworks. In Proceedings
of COLING, pages 1?18.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-based
translation with weighted finite state transducers. In
Proceedings of NAACL-HLT, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
EACL, pages 380?388.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of EMNLP, pages 1288?1298.
Werner Kuich and Arto Salomaa. 1986. Semirings, au-
tomata, languages. Springer.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Andrej Ljolje, Fernando Pereira, and Michael Riley.
1999. Efficient general lattice generation and rescor-
ing. In Proceedings of Eurospeech, pages 1251?1254.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Drosde et al (Drosde et al, 2009), chapter 6, pages
213?254.
Mark-Jan Nederhof and Giorgio Satta. 2003. Probabilis-
tic parsing as intersection. In Proceedings of 8th In-
ternational Workshop on Parsing Technologies, pages
137?148.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167.
Ion Petre and Arto Salomaa. 2009. Algebraic systems
and pushdown automata. In Drosde et al (Drosde et
al., 2009), chapter 7, pages 257?289.
R. Prasad, K. Krstovski, F. Choi, S. Saleem, P. Natarajan,
M. Decerbo, and D. Stallard. 2007. Real-time speech-
to-speech translation for pdas. In Proceedings of IEEE
International Conference on Portable Information De-
vices, pages 1 ?5.
Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
lagrangian relaxation. In Proceedings of ACL-HLT,
pages 72?82.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270?274.
1383
Hierarchical Phrase-Based Translation with
Weighted Finite-State Transducers and
Shallow-n Grammars
Adria` de Gispert?
University of Cambridge
Gonzalo Iglesias??
University of Vigo
Graeme Blackwood?
University of Cambridge
Eduardo R. Banga??
University of Vigo
William Byrne?
University of Cambridge
In this article we describe HiFST, a lattice-based decoder for hierarchical phrase-based translation
and alignment. The decoder is implemented with standard Weighted Finite-State Transducer
(WFST) operations as an alternative to the well-known cube pruning procedure. We find that
the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in
fewer search errors, better parameter optimization, and improved translation performance. The
direct generation of translation lattices in the target language can improve subsequent rescoring
procedures, yielding further gains when applying long-span language models and Minimum
Bayes Risk decoding. We also provide insights as to how to control the size of the search space
defined by hierarchical rules. We show that shallow-n grammars, low-level rule catenation,
and other search constraints can help to match the power of the translation system to specific
language pairs.
1. Introduction
Hierarchical phrase-based translation (Chiang 2005) is one of the current promising
approaches to statistical machine translation (SMT). Hiero SMT systems are based
on probabilistic synchronous context-free grammars (SCFGs) whose translation rules
? University of Cambridge, Department of Engineering. CB2 1PZ Cambridge, U.K.
E-mail: {ad465,gwb24,wjb31}@eng.cam.ac.uk.
?? University of Vigo, Department of Signal Processing and Communications, 36310 Vigo, Spain.
E-mail: {giglesia,erbanga}@gts.tsc.uvigo.es.
Submission received: 30 October 2009; revised submission received: 24 February 2010; accepted for
publication: 10 April 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
can be extracted automatically from word-aligned parallel text. These grammars can
produce a very rich space of candidate translations and, relative to simpler phrase-
based systems (Koehn, Och, and Marcu 2003), the power of Hiero is most evident in
translation between dissimilar languages, such as English and Chinese (Chiang 2005,
2007). Hiero is able to learn and apply complex patterns in movement and translation
that are not possible with simpler systems. Hiero can also be used to good effect on
?simpler? problems, such as translation between English and Spanish (Iglesias et al
2009c), even though there is not the same need for the full complexity of movement and
translation. If gains in using Hiero are small, however, the computational and modeling
complexity involved are difficult to justify. Such concerns would vanish if there were
reliable methods to match Hiero complexity for specific translation problems. Loosely
put, it would be a good thing if the complexity of a system was somehow proportional
to the improvement in translation quality the system delivers.
Another notable current trend in SMT is system combination. Minimum Bayes
Risk decoding is widely used to rescore and improve hypotheses produced by indi-
vidual systems (Kumar and Byrne 2004; Tromble et al 2008; de Gispert et al 2009),
and more aggressive system combination techniques which synthesize entirely new
hypotheses from those of contributing systems can give even greater translation im-
provements (Rosti et al 2007; Sim et al 2007). It is now commonplace to note that even
the best available individual SMT system can be significantly improved upon by such
techniques. This puts a burden on the underlying SMT systems which is somewhat
unusual in NLP. The requirement is not merely to produce a single hypothesis that
is as good as possible. Ideally, the SMT systems should generate large collections of
candidate hypotheses that are simultaneously diverse and of good quality.
Relative to these concerns, previously published descriptions of Hiero have noted
certain limitations. Spurious ambiguity (Chiang 2005) was described as
a situation where the decoder produces many derivations that are distinct yet have the
same model feature vectors and give the same translation. This can result in n-best lists
with very few different translations which is problematic for the minimum-error-rate
training algorithm ...
This is due in part to the cube pruning procedure (Chiang 2007), which enumerates all
distinct hypotheses to a fixed depth by means of k-best hypothesis lists. If enumeration
was not necessary, or if the lists could be arbitrarily deep, there might still be many
duplicate derivations, but at least the hypothesis space would not be impoverished.
Spurious ambiguity is also related to overgeneration (Varile and Lau 1988; Wu
1997; Setiawan et al 2009). For our purposes we say that overgeneration occurs when
different derivations based on the same set of rules give rise to different translations.
An example is given in Figure 1.
This process is not necessarily a bad thing in that it allows new translations to be
synthesized from rules extracted from training data; a strong target language model,
such as a high order n-gram, is typically relied upon to discard unsuitable hypotheses.
Overgeneration does complicate translation, however, in that many hypotheses are
introduced only to be subsequently discarded. The situation is further complicated by
search errors. Any search procedure which relies on pruning during search is at risk of
search errors and the risk is made worse if the grammars tend to introduce many similar
scoring hypotheses. In particular we have found that cube pruning is very prone to
search errors, that is, the hypotheses produced by cube pruning are not the top scoring
hypotheses which should be found under the Hiero grammar (Iglesias et al 2009b).
506
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 1
Example of multiple translation sequences from a simple grammar fragment showing variability
in reordering in translation of the source sequence abc.
These limitations are clearly related to each other. Moreover, they become more
problematic as the amount of parallel text grows. As the number of rules in the grammar
increases, the grammars become more expressive, but the ability to search them does not
improve. This leads to a widening gap between the expressive power of the grammar
and the ability to search it to find good and diverse hypotheses.
In this article we describe the following two refinements to Hiero which are in-
tended to address some of the limitations in its original formulation.
Lattice-based hierarchical translation We describe how the cube pruning procedure
can be replaced by standard operations with Weighted Finite State Transducers
(WFSTs) so that Hiero uses translation lattices rather than n-best lists in search.
We find that keeping partial translation hypotheses in lattice form greatly reduces
search errors. In some instances it is possible to perform translation without
any pruning at all so that search errors are completely eliminated. Consistent
with the observation by Chiang (2005), this leads to improvements in minimum
error rate training. Furthermore, the direct generation of translation lattices can
improve gains from subsequent language model and Minimum Bayes Risk (MBR)
rescoring.
Shallow-n grammars and additional nonterminal categories Nonterminals can be in-
corporated into hierarchical translation rules for the purpose of tuning the size
of the Hiero search space for individual language pairs. Shallow-n grammars are
described and shown to control the level of rule nesting, low-level rule catenation,
and the minimum and maximum spans of individual translation rules. In trans-
lation experiments we find that a shallow-1 grammar (one level of rule nesting)
is sufficiently expressive for Arabic-to-English translation, but that a shallow-3
grammar is required in Chinese-to-English translation to match the performance
of a full Hiero system that allows arbitrary rule nesting. These nonterminals are
introduced to control the Hiero search space and do not require estimation from
annotated?or parsed?parallel text, as can be required by translation systems
based on linguistically motivated grammars. We use this approach as the basis
of a general approach to SMT modeling. To control overgeneration, we revisit
the synchronous context-free grammar defined by hierarchical rules and take a
shallow-1 grammar as a starting point. We then increase the complexity of the
rules until the desired translation quality is found.
507
Computational Linguistics Volume 36, Number 3
With these refinements we find that hierarchical phrase-based translation can be effi-
ciently carried out with no (or minimal) search errors in large-data tasks and can achieve
state-of-the-art translation performance.
There are many benefits to formulating Hiero translation in terms of WFSTs. Fol-
lowing the manner in which Knight and Al-Onaizan (1998), Kumar, Deng, and Byrne
(2006), and Graehl, Knight, and May (2008) elucidate other machine translation models,
we can use WFST operations to make the operations of the Hiero decoder very clear. The
simplicity of the analysis makes it possible to focus on the underlying grammars and
avoid the complexities of heuristic search procedures. Once the decoder is formulated,
implementation is mostly straightforward using standard WFST techniques developed
for language processing (Mohri, Pereira, and Riley 2002). What difficulties arise are due
to using finite state techniques with grammars which are not themselves finite state.
We will show, however, that the basic operations which need to be performed, such as
extracting sufficient statistics for minimum error rate training, can be done relatively
easily and naturally.
1.1 Overview
In Section 2 we describe HiFST, which is a hierarchical phrase-based translation system
based on the OpenFST WFST libraries (Allauzen et al 2007). We describe how trans-
lation lattices can be generated over the Cocke-Younger-Kasami (CYK) grid used for
parsing under Hiero. We also review some modeling issues needed for practical trans-
lation, such as the efficient handling of source language deletions and the extraction of
statistics for minimum error rate training. This requires running HiFST in ?alignment
mode? (Section 2.3) to find all the rule derivations that generate a given set of translation
hypotheses.
In Section 3 we investigate parameters that control the size and nature of the
hierarchical phrase-based search space as defined by hierarchical translation rules. To
efficiently explore the largest possible space and avoid pruning in search, we introduce
ways to easily adapt the grammar to the reordering needs of each language pair. We
describe the use of additional nonterminal categories to limit the degree of rule nesting,
and can directly control the minimum or maximum span each translation rule can cover.
In Section 4 we report detailed translation results for Arabic-to-English and
Chinese-to-English, and review translation results for Spanish-to-English and Finnish-
to-English translation. In these experiments we contrast the performance of lattice-based
and cube pruning hierarchical decoding and we measure the impact on processing
time and translation performance due to changes in search parameters and grammar
configurations. We demonstrate that it is easy and feasible to compute the marginal
instead of the Viterbi probabilities when using WFSTs, and that this yields gains in
translation performance. And finally, we show that lattice-based translation performs
significantly better than k-best lists for the task of combining translation hypotheses
generated from alternative morphological segmentations of the data via lattice-based
MBR decoding.
2. Hierarchical Translation and Alignment with WFSTs
Hierarchical phrase-based rules define a synchronous context-free grammar (CFG) and
a particular search space of translation candidates. Table 1 shows the type of rules in-
cluded in a standard hierarchical phrase-based grammar, where T denotes the terminals
(words) and ? is a bijective function that relates the source and target nonterminals of
508
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 1
Rules contained in the standard hierarchical grammar.
standard hierarchical grammar
S??X,X? glue rule 1
S??S X,S X? glue rule 2
X???,?,?? , ?,? ? {X ? T}+ hiero rules
each rule (Chiang 2007). This function is defined if there are at least two nonterminals,
and for clarity of presentation may be omitted in general rule discussions. When ?,? ?
{T}+, that is, the rule contains no nonterminals, the rule is a simple lexical phrase pair.
The HiFST translation system is based on a variant of the CYK algorithm closely
related to CYK+ (Chappelier and Rajman 1998). Parsing follows the description of
Chiang (2005, 2007); it maintains back-pointers and employs hypothesis recombination
without pruning. The underlying model is a probabilisitic synchronous CFG consisting
of a set R = {Rr} of rules Rr : Nr ? ??r,?r? / pr, with ?glue? rules, S ? ?X,X? and
S ? ?S X,S X?. N denotes the set of nonterminal categories (examples are given in
Section 3), and pr denotes the rule probability, typically transformed to a cost cr; unless
otherwise noted we use the tropical semiring, so cr = ? log pr. T denotes the terminals
(words), and the grammar builds parses based on strings ?,? ? {N ? T}+. Each cell
in the CYK grid is specified by a nonterminal symbol and position in the CYK grid:
(N, x, y), which spans sx+y?1x on the source sentence.
In effect, the source language sentence is parsed using a CFG with rules N ? ?. The
generation of translations is a second step that follows parsing. For this second step, we
describe a method to construct word lattices with all possible translations that can be
produced by the hierarchical rules. Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In each cell (N, x, y) in the CYK grid, we
build a target language word lattice L(N, x, y). This lattice contains every translation of
sx+y?1x from every derivation headed by N. These lattices also contain the translation
scores on their arc weights.
The ultimate objective is the word lattice L(S, 1, J) which corresponds to all the
analyses that cover the source sentence sJ1. Once this is built, we can apply a target lan-
guage model to L(S, 1, J) to obtain the final target language translation lattice (Allauzen,
Mohri, and Roark 2003).
2.1 Lattice Construction over the CYK Grid
In each cell (N, x, y), the set of rule indices used by the parser is denoted R(N, x, y), that
is, for r ? R(N, x, y), the rule N ? ??r,?r? was used in at least one derivation involving
that cell.
For each rule Rr, r ? R(N, x, y), we build a lattice L(N, x, y, r). This lattice is derived
from the target side of the rule ?r by concatenating lattices corresponding to the ele-
ments of ?r = ?r1...?
r
|?r|. If an ?
r
i is a terminal, creating its lattice is straightforward. If
?ri is a nonterminal, it refers to a cell (N
?, x?, y?) lower in the grid identified by the back-
pointer BP(N, x, y, r, i); in this case, the lattice used is L(N?, x?, y?). Taken together,
L(N, x, y, r) =
?
i=1..|?r|
L(N, x, y, r, i) (1)
509
Computational Linguistics Volume 36, Number 3
Figure 2
Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3.
The grid is represented here in two dimensions (x, y). In practice only the first column accepts
both nonterminals (S,X). For this reason it is divided into two subcolumns.
L(N, x, y, r, i) =
{
A(?i) if ?i ? T
L(N?, x?, y?) otherwise (2)
where A(t), t ? T returns a single-arc acceptor which accepts only the symbol t. The
lattice L(N, x, y) is then built as the union of lattices corresponding to the rules in
R(N, x, y):
L(N, x, y) =
?
r?R(N,x,y)
L(N, x, y, r) (3)
Lattice union and concatenation are performed using the? and?WFST operations,
respectively, as described by Allauzen et al (2007). If a rule Rr has a cost cr, it is applied
to the exit state of the lattice L(N, x, y, r) prior to the operation of Equation (3).
2.1.1 An Example of Phrase-based Translation. Figure 2 illustrates this process for a three-
word source sentence s1s2s3 under monotonic phrase-based translation. The left-hand
side shows the state of the CYK grid after parsing using the rules R1 to R5. These include
three standard phrases, that is, rules with only terminals (R1, R2, R3), and the glue rules
(R4, R5). Arrows represent back-pointers to lower-level cells. We are interested in the
uppermost S cell (S, 1, 3), as it represents the search space of translation hypotheses
covering the whole source sentence. Two rules (R4, R5) are in this cell, so the lattice
L(S, 1, 3) will be obtained by the union of the two lattices found by the back-pointers of
these two rules. This process is explicitly derived in the right-hand side of Figure 2.
2.1.2 An Example of Hierarchical Translation. Figure 3 shows a hierarchical scenario for
the same sentence. Three rules, R6,R7,R8, are added to the example of Figure 2, thus
providing two additional derivations. This makes use of sublattices already produced
in the creation of L(S, 1, 3, 5) and L(X, 1, 3, 1) in Figure 2; these are shown within {}.
510
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 3
Translation as in Figure 2 but with additional rules R6,R7,R8. Lattices previously derived appear
within {}.
2.2 A Procedure for Lattice Construction
Figure 4 presents an algorithm to build the lattice for every cell. The algorithm uses
memoization: If a lattice for a requested cell already exists, it is returned (line 2);
otherwise it is constructed via Equations (1)?(3). For every rule, each element of the
target side (lines 3,4) is checked as terminal or nonterminal (Equation (2)). If it is a
terminal element (line 5), a simple acceptor is built. If it is a nonterminal (line 6), the
lattice associated to its back-pointer is returned (lines 7 and 8). The complete lattice
L(N, x, y, r) for each rule is built by Equation (1) (line 9). The lattice L(N, x, y) for this
cell is then found by union of all the component rules (line 10, Equation (3)); this lattice
is then reduced by standard WFST operations (lines 11, 12, 13). It is important at this
point to remove any epsilon arcs which may have been introduced by the various WFST
union, concatenation, and replacement operations (Allauzen et al 2007).
We now address several important aspects of efficient implementation.
Figure 4
Recursive lattice construction from a CYK grid.
511
Computational Linguistics Volume 36, Number 3
Figure 5
Delayed translation WFST with derivations from Figures 2 and 3 before (left) and after
minimization (right).
2.2.1 Delayed Translation. Equation (2) leads to the recursive construction of lattices in
upper levels of the grid through the union and concatenation of lattices from lower
levels. If Equations (1) and (3) are actually carried out over fully expanded word lattices,
the memory required by the upper lattices will increase exponentially.
To avoid this, we use special arcs that serve as pointers to the low-level lattices. This
effectively builds a skeleton for the desired lattice and delays the creation of the final
word lattice until a single replacement operation is carried out in the top cell (S, 1, J).
To make this exact, we define a function g(N, x, y) which returns a unique tag for each
lattice in each cell, and use it to redefine Equation (2). With the back-pointer (N?, x?, y?) =
BP(N, x, y, r, i), these special arcs are introduced as
L(N, x, y, r, i) =
{
A(?i) if ?i ? T
A(g(N?, x?, y?)) otherwise (4)
The resulting lattices L(N, x, y) are a mix of target language words and lattice
pointers (Figure 5, left). Each still represents the entire search space of all translation
hypotheses covering the span, however. Importantly, operations on these lattices?such
as lossless size reduction via determinization and minimization (Mohri, Pereira, and
Riley 2002)?can still be performed. Owing to the existence of multiple hierarchical rules
which share the same low-level dependencies, these operations can greatly reduce the
size of the skeleton lattice; Figure 5 shows the effect on the translation example. This
process is carried out for the lattice at every cell, even at the lowest level where there
are only sequences of word terminals. As stated, size reductions can be significant. Not
all redundancy is removed, however, because duplicate paths may arise through the
concatenation and union of sublattices with different spans.
At the upper-most cell, the lattice L(S, 1, J) contains pointers to lower-level lattices.
A single FST replace operation (Allauzen et al 2007) recursively substitutes all pointers
by their lower-level lattices until no pointers are left, thus producing the complete
target word lattice for the whole source sentence. The use of the lattice pointer arc was
inspired by the ?lazy evaluation? techniques developed by Mohri, Pereira, and Riley
(2000), closely related to Recursive Transition Networks (Woods 1970; Mohri 1997). Its
implementation uses the infrastructure provided by the OpenFST libraries for delayed
composition.
2.2.2 Top-level Pruning and Search Pruning. The final translation lattice L(S, 1, J) can
be quite large after the pointer arcs are expanded. We therefore apply a word-based
512
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 6
Transducers for filtering up to one (left) or two (right) consecutive deletions.
language model via WFST composition (Allauzen et al 2007) and perform likelihood-
based pruning based on the combined translation and language model scores. We call
this top-level pruning because it is performed over the topmost lattice.
Pruning can also be performed on the sublattices in each cell during search. One
simple strategy is to monitor the number of states in the determinized lattices L(N, x, y).
If this number is above a threshold, we expand any pointer arcs and apply a word-based
language model via composition. The resulting lattice is then reduced by likelihood-
based pruning, after which the LM scores are removed. These pruning strategies can be
very selective, for example allowing the pruning threshold to depend on the height of
the cell in the grid. In this way the risk of search errors can be controlled.
The same n-gram language model can be used for top-level pruning and search
pruning, although different WFST realizations are required. For top-level pruning, a
standard implementation as described by Allauzen et al (2007) is appropriate. For
search pruning, the WFST must allow for incomplete language model histories, because
many sublattice paths are incomplete translation hypotheses which do not begin with
a sentence-start marker. The language model acceptor is constructed so that initial
substrings of length less than the language model order are assigned no weight under
the language model.
2.2.3 Constrained Source Word Deletion. As a practical matter it can be useful to allow SMT
systems to delete some source words rather than to enforce their translation. Deletions
can be allowed in Hiero by including in the grammar a set of special deletion rules
of the type: X??s,NULL?. Unconstrained application of these rules can lead to overly
large and complex search spaces, however. We therefore limit the number of consecutive
source word deletions as we explore each cell of the CYK grid. This is done by standard
composition with an unweighted transducer that maps any word to itself, and up to k
NULL tokens to  arcs. In Figure 6 this simple transducer for k = 1 and k = 2 is drawn.
Composition of the lattice in each cell with this transducer filters out all translations
with more than k consecutive deleted words.
2.3 Hierarchical Phrase-Based Alignment with WFSTs
We now describe a method to apply our decoder in alignment mode. The objective in
alignment is to recover all the derivations which can produce a given translation. We do
this rather than keep track of the rules used during translation, because we find it faster
and more efficient first to generate translations and then, by running the system as an
aligner with a constrained target space, to extract all the relevant derivations with their
costs. As will be discussed in Section 2.3.1, this is useful for minimum error training,
where the contribution of each feature to the overall hypothesis cost is required for
system optimization.
513
Computational Linguistics Volume 36, Number 3
Figure 7
Transducer encoding simultaneously rule derivations R2R1R3R4 and R1R5R6, and the translation
t5t8. The input sentence is s1s2s3 and the grammar considered here contains the following rules:
R1: S??X,X?, R2: S??S X,S X? , R3: X??s1,t5?, R4: X??s2 s3,t8?, R5: X??s1 X s3,X t8? and R6:
X??s2,t5?.
Conceptually, we would like to create a transducer that represents the mapping
from all possible rule derivations to all possible translations, and then compose this
transducer with an acceptor for the translations of interest. Creating this transducer
which maps derivations to translations is not feasible for large translation grammars,
so we instead keep track of rules as they are used to generate a particular translation
output. We introduce two modifications into lattice construction over the CYK grid
described in Section 2.2:
1. In each cell transducers are constructed which map rule sequences to the
target language translation sequences they produce. In each transducer the
output strings are all possible translations of the source sentence span
covered by that cell; the input strings are all the rule derivations that
generate those translations. The rule derivations are expressed as
sequences of rule indices r given the set of rules R = {Rr}.
2. As these transducers are built they are composed with acceptors for
subsequences of the reference translations so that any translations not
present in the given set of reference translations are removed. In effect, this
replaces the general target language model used in translation with an
unweighted acceptor which accepts only specific sentences.
For alignment, Equations (1) and (2) are redefined as
L(N, x, y, r) = AT(r,)
?
i=1..|?r|
L(N, x, y, r, i) (5)
L(N, x, y, r, i) =
{
AT(,?i) if ?i ? T
L(N?, x?, y?) otherwise (6)
where AT(r, t), Rr ? R, t ? T returns a single-arc transducer which accepts the symbol
r in the input language (rule indices) and the symbol t in the output language (target
words). The weight assigned to each arc is the same in alignment as in translation. With
these definitions the goal lattice L(S, 1, J) is now a transducer with rule indices in the
input symbols and target words in the output symbols. A simple example is given
in Figure 7 where two rule derivations for the translation t5t8 are represented by the
transducer.
As we are only interested in those rule derivations that generate the given target
references, we can discard non-desired translations via standard FST composition of
514
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 8
Construction of a substring acceptor. An acceptor for the strings t1t2t4 and t3t4 (left) and its
substring acceptor (right). In alignment the substring acceptor can be used to filter out undesired
partial translations via standard FST composition operations.
the lattice transducer with the given reference acceptor. In principle, this would be done
in the uppermost cell of the CYK, once the complete source sentence has been covered.
However, keeping track of all possible rule derivations and all possible translations until
the last cell may not be computationally feasible for many sentences. It is more desirable
to carry out this filtering composition in lower-level cells while constructing the lattice
over the CYK grid so as to avoid storing an increasing number of undesired translations
and derivations in the lattice. The lattice in each cell should contain translations formed
only from substrings of the references.
To achieve this we build an unweighted substring acceptor that accepts all sub-
strings of each target reference string. For instance, given the reference string t1t2 . . .
tJ, we build an acceptor for all substrings ti . . . tj, where 1 ? i ? j ? J. This is applied
to lattices in all cells (x, y) that do not span the whole sentence. In the uppermost
cell we compose with the reference acceptor which accepts only complete reference
strings. Given a lattice of target references, the unweighted substring acceptor is
built as:
1. change all non-initial states into final states
2. add one initial state and add  arcs from it to all other states
Figure 8 shows an example of a substring acceptor for the two references t1t2t4 and
t3t4. The substring acceptor also accepts an empty string, accounting for those rules
that delete source words, which in other words translate into NULL. In some instances
the final composition with the reference acceptor might return an empty lattice. If this
happens there is no rule sequence in the grammar that can generate the given source
and target sentences simultaneously.
We have described the use of transducers to encode mappings from rule deriva-
tions to translations. These transducers are somewhat impoverished relative to parse
trees and parse forests, which are more commonly used to encode this relationship. It
is easy to map from a parse tree to one of these transducers but the reverse essentially
requires re-parsing to recreate the tree structure. The structures of the parse trees asso-
ciated with a translation are not needed by many algorithms, however. In particular,
parameter optimization by MERT requires only the rules involved in translation. Our
approach keeps only what is needed by such algorithms. This approach also has prac-
tical advantages such as the ability to align directly to k-best lists or lattices of reference
translations.
515
Computational Linguistics Volume 36, Number 3
Figure 9
One arc from a rule acceptor that assigns a vector of K feature weights to each rule (top) and the
result of composition with the transducer of Figure 7 (after weight-pushing) (bottom). The
components of the final K-dimensional weight vector agree with the feature weights of the rule
sequence, for example ck = c2,k + c1,k + c3,k + c4,k for k = 1 . . .K.
2.3.1 Extracting Feature Values from Alignments. As described by Chiang (2007), scores are
associated with hierarchical translation rules through a factoring into features within a
log-linear model (Och and Ney 2002). We assume that we have a collection of K features
and that the cost cr of each rule Rr is cr =
?K
k=1 ?kc
r,k, where cr,k is the value of the kth
feature value for the rth rule and ?k is the weight assigned to the kth feature for all rules.
For a parse which makes use of the rules Rr1 . . .RrN , its cost
?N
n=1 c
rn can therefore
be written as
?K
k=1 ?k
?N
n=1 c
rn,k. The quantity
?N
n=1 c
rn,k is the contribution by the kth
feature to the overall translation score for that parse. These are the quantities which
need to be extracted from alignment lattices for use in procedures such as minimum
error rate training for estimation of the feature weights ?k.
The procedure described in Section 2.3 produces alignment lattices with scores
consistent with the total parse score. Further steps must be taken to factor this over-
all score to identify the contribution due to individual features or translation rules.
We introduce a rule acceptor which accepts sequences of rule indices, such as the
input sequences of the alignment transducer, and assigns weights in the form of
K-dimensional vectors. Each component of the weight vector corresponds to the feature
value for that rule. Arcs have the form 0
Rr/wr?? 0 where wr = [cr,1, . . . , cr,K]. An example
of composition with this rule acceptor is given in Figure 9 to illustrate how feature scores
are mapped to components of the weight vector. The same operations can be applied to
the (unweighted) alignment transducer on a much larger scale to extract the statistics
needed for minimum error rate training.
We typically apply this procedure in the tropical semiring (Viterbi likelihoods), so
that only the best rule derivation that generated each translation candidate is taken
into account when extracting feature contributions for MERT. However, given the
alignment transducer L, this could also be performed in the log semiring (marginal
likelihoods), taking into account the feature contributions from all rule derivations, for
each translation candidate. This would be adequate if the translation system also car-
ried out decoding in the log semiring, an experiment which is partially explored in
Section 4.4.
We note that the contribution of the language model to the overall translation score
cannot be calculated in this scheme, since the language model score cannot be factored
in terms of rules. To obtain the language model contribution, we simply carry out
WFST composition (Allauzen et al 2007) between an unweighted acceptor of the target
516
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 10
Hierarchical translation grammar example and two parse trees with different levels of rule
nesting for the input sentence s1s2s3s4.
sentences and the n-gram language model used in translation. After determinization,
the cost of each path in the acceptor is then the desired LM feature contribution.
3. Shallow-n Translation Grammars: Translation Search Space Refinements
In this section we describe shallow-n grammars in order to reduce Hiero overgeneration
and adapt the grammar complexity to specific language pairs; the ultimate goal is to de-
fine the most constrained grammar that is capable of generating the desired movement
and translation, so that decoding can be performed without search errors.
Hiero can provide varying degrees of complexity in movement and translation.
Consider the example shown in Figure 10, which shows a hierarchical grammar defined
by six rules. For the input sentence s1s2s3s4, there are two possible parse trees as shown;
the rule derivations for each tree are R1R4R3R5 and R2R1R3R5R6. Along with each tree
is shown the translation generated and the phrase-level alignment. Comparing the two
trees and alignments, the leftmost tree makes use of more reordering when translating
from source to target through the nested application of the hierarchical rules R3 and R4.
For some language pairs this level of reordering may be required in translation, but for
other language pairs it may lead to overgeneration of unwanted hypotheses. Suppose
the grammar in this example is modified as follows:
1. A nonterminal X0 is introduced into hierarchical translation rules
R3:X??X0 s3,t5 X0?
R4:X??X0 s4,t3 X0?
2. Rules for lexical phrases are applied to X0
R5:X0??s1 s2,t1 t2?
R6:X0??s4,t7?
These modifications exclude parses in which hierarchical translation rules generate
other hierarchical rules, except at the 0th level which generate lexical phrases. Con-
sequently the left most tree of Figure 10 cannot be generated and t5t1t2t7 is the only
allowable translation of s1s2s3s4. We call this a ?shallow-1? grammar: The maximum
517
Computational Linguistics Volume 36, Number 3
degree of rule nesting allowed is 1 and only the glue rule can be applied above this
level.
The use of additional nonterminal categories is an elegant way to easily control
important aspects that can have a strong impact on the search space. A shallow-n
translation grammar can be formally defined as:
1. the usual nonterminal S
2. a set of nonterminals {X0, . . . ,XN}
3. two glue rules: S ? ?XN,XN? and S ? ?S XN,S XN?
4. hierarchical translation rules for levels n = 1, . . . ,N:
R: Xn???,?,?? , ?,? ? {{Xn?1} ? T}+
with the requirement that ? and ? contain at least one Xn?1
5. translation rules which generate lexical phrases:
R: X0???,?? , ?,? ? T+
3.1 Avoiding Some Spurious Ambiguity
The added requirement in condition (4) in the definition of shallow-n grammars is
included to avoid some instances in which multiple parses lead to the same translation.
It is not absolutely necessary but it can be added without any loss in representational
capability. To see the effect of this constraint, consider the following example with a
source sentence s1 s2 and a shallow-1 grammar defined by these four rules:
R1: S??X1,X1?
R2: X1??s1 s2,t2 t1?
R3: X1??s1 X0,X0 t1?
R4: X0??s2,t2?
There are two derivations R1R2 and R1R3R4 which yield identical translations. However
R2 would not be allowed under the constraint introduced here because it does not
rewrite an X1 to an X0.
3.2 Structured Long-Distance Movement
The basic formulation of shallow-n grammars allows only the upper-level nonterminal
category S to act within the glue rule. This can prevent some useful long-distance
movement, as might be needed to translate Arabic sentences in Verb-Subject-Object
order into English. It often happens that the initial Arabic verb requires long-distance
movement, but the subject which follows can be translated in monotonic order. For
instance, consider the following Romanized Arabic sentence:
TAlb AlwzrA? AlmjtmEyn Alywm fy dm$q <lY ...
(CALLED) (the ministers) (gathered) (today) (in Damascus) (FOR) ...
where the verb ?TAlb? must be translated into English so that it follows the translations
of the five subsequent Arabic words ?AlwzrA? AlmjtmEyn Alywm fy dm$q?, which
518
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
are themselves translated monotonically. A shallow-1 grammar cannot generate this
movement except in the relatively unlikely case that the five words following the verb
can be translated as a single phrase.
A more powerful approach is to define grammars which allow low-level rules to
form movable groups of phrases. Additional nonterminals {Mk} are introduced to allow
successive generation of k nonterminals XN?1 in monotonic order for both languages,
where K1 ? k ? K2. These act in the same manner as the glue rule does in the uppermost
level. Applying Mk nonterminals at the N?1 level allows one hierarchical rule to perform
a long-distance movement over the tree headed by Mk.
We further refine the definition of shallow-n grammars by specifying the allowable
values of k for the successive productions of nonterminals XN?1. There are many pos-
sible ways to formulate and constrain these grammars. If K2 = 1, then the grammar
is equivalent to the previous definition of shallow-n grammars, because monotonic
production is only allowed by the glue rule of level N. If K1 = 1 and K2 > 1, then the
search space defined by the grammar is greater than the standard shallow-n grammar
as it includes structured long-distance movement. Finally, if K1 > 1 then the search
space is different from standard shallow-n as the n level is only used for long-distance
movement.
Introduction of Mk nonterminals redefines shallow-n grammars as:
1. the usual nonterminal S
2. a set of nonterminals {X0, . . . ,XN}
3. a set of nonterminals {MK1 , . . . ,MK2} for K1 = 1, 2; K1 ? K2
4. two glue rules: S ? ?XN,XN? and S ? ?S XN,S XN?
5. hierarchical translation rules for level N:
R: XN???,?,?? , ?,? ? {{MK1 , . . . ,MK2} ? T}+
with the requirement that ? and ? contain at least one Mk
6. hierarchical translation rules for levels n = 1, . . . ,N ? 1:
R: Xn???,?,?? , ?,? ? {{Xn?1} ? T}+
with the requirement that ? and ? contain at least one Xn?1
7. translation rules which generate lexical phrases:
R: X0???,?? , ?,? ? T+
8. rules which generate k nonterminals XN?1:
if K1 == 2 :
R: Mk??XN?1 Mk?1,XN?1 Mk?1? , for k = 3, . . . ,K2
R: M2??XN?1 XN?1,XN?1 XN?1,I?
if K1 == 1 :
R: Mk??XN?1 Mk?1,XN?1 Mk?1? , for k = 2, . . . ,K2
R: M1??XN?1,XN?1?
where I denotes the identity function that enforces monotonocity in the nonterminals.
For example, with a shallow-1 grammar, M3 leads to the monotonic production of three
nonterminals X0, which leads to the production of three lexical phrase pairs; these can be
moved with a hierarchical rule of level 1. This is graphically represented by the leftmost
tree in Figure 11. With a shallow-2 grammar, M2 leads to the monotonic production of
519
Computational Linguistics Volume 36, Number 3
Figure 11
Movement allowed by two grammars: shallow-1, with K1 = 1, K2 = 3 (left), and shallow-2, with
K1 = 1, K2 = 3 (right). Both grammars allow movement of the bracketed term as a unit.
Shallow-1 requires that translation within the object moved be monotonic whereas shallow-2
allows up to two levels of reordering.
two nonterminals X1, a movement represented by the rightmost tree in Figure 11. This
movement cannot be achieved with a shallow-1 grammar.
3.3 Minimum and Maximum Rule Span
It is useful to define two parameters which further control the application of hierarchical
translation rules in generating the search space. Parameters hmax and hmin specify
the maximum and minimum height at which any hierarchical translation rule can be
applied in the CYK grid. In other words, a hierarchical rule can only be applied in cell
(x, y) if hmin? y ?hmax. Note that these parameters can also be set independently for
each nonterminal category.
3.4 Verb Movement Grammars for Arabic-to-English Translation
Following the discussion which motivated this section, we wish to model movement of
Arabic verbs when translating into English. We add to the shallow-n grammars a verb
restriction so that the hierarchical translation rules (5) apply only if the source language
string ? contains a verb. This encourages translations in which the Arabic verb is moved
at the uppermost level N.
3.5 Grammars Used for SMT Experiments
We now define the hierarchical grammars for the translation experiments which we
describe next.
Shallow-1,2,3 : Shallow-n grammars for n = 1, 2, 3. These grammars do not incorporate
any monotonicity constraints, that is K1 = K2 = 1.
Shallow-1, K1 = 1, K2 = 3 : hierarchical rules with one nonterminal can reorder a
monotonic production of up to three target language phrases of level 0.
520
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Shallow-1, K1 = 1, K2 = 3, vo : hierarchical rules with one nonterminal can reorder a
monotonic catenation of up to three target language phrases of level 0, but only if
one of the source terminals is tagged as a verb.
Shallow-2, K1 = 2,K2 = 3, vo : two levels of reordering with monotonic production
of up to three target language phrases of level 1, but only if one of the source
terminals is tagged as a verb.
4. Translation Experiments
In this section we report on hierarchical phrase-based translation experiments with
WFSTs. We focus mainly on the NIST Arabic-to-English and Chinese-to-English trans-
lation tasks; some results for other language pairs are summarized in Section 4.6.
Translation performance is evaluated using the BLEU score (Papineni et al 2001) as
implemented for the NIST 2009 evaluation.1 The experiments are organized as follows:
- Lattice-based and cube pruning hierarchical decoding (Section 4.2)
- Grammar configurations and search parameters and their effect on
translation performance and processing time (Section 4.3)
- Marginalization over translation derivations (Section 4.4)
- Combining translation lattices obtained from alternative morphological
decompositions of the input (Section 4.5)
4.1 Experimental Framework
For Arabic-to-English translation we use all allowed parallel corpora in the NIST MT08
(and MT09) Arabic Constrained Data track (?150M words per language). In addition to
reporting results on the MT08 set itself, we make use of a development set mt02-05-tune
formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation
sets; the even numbered sentences form a validation set mt02-05-test. The mt02-05-tune
set has 2,075 sentences.
For Chinese-to-English translation we use all available parallel text for the GALE
2008 evaluation;2 this is approximately 250M words per language. We report translation
results on the NIST MT08 set, a development set tune-nw, and a validation set test-nw.
These tuning and test sets contain translations produced by the GALE program and
portions of the newswire sections of MT02 through MT05. The tune-nw set has 1,755
sentences, and test-nw set is similar.
The parallel texts for both language pairs are aligned using MTTK (Deng and Byrne
2008). We extract hierarchical rules from the aligned parallel texts using the constraints
developed by Chiang (2007). We further filter the extracted rules by count and pattern
as described by Iglesias et al(2009a). The following features are extracted from the
parallel data and used to assign scores to translation rules: source-to-target and target-
to-source phrase translation models, word and rule penalties, number of usages of the
glue rule, source-to-target and target-to-source lexical models, and three rule count
features inspired by Bender et al (2007).
1 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
2 See http://projects.ldc.upenn.edu/gale/data/catalog.html
521
Computational Linguistics Volume 36, Number 3
We use two types of language model in translation. In first-pass translation we use
4-gram language models estimated over the English side of the parallel text (for each
language pair) and a 965 million word subset of monolingual data from the English
Gigaword Third Edition (LDC2007T07). These are the language models used if pruning
is needed during search. The main language model is a zero-cutoff stupid-backoff
(Brants et al 2007) 5-gram language model, estimated using 6.6B words of English text
from the English Gigaword corpus. These language models are converted to WFSTs
as needed (Allauzen, Mohri, and Roark 2003); failure transitions are used for correct
application of back-off weights. In tuning the systems, standard MERT (Och 2003)
iterative parameter estimation under IBM BLEU is performed on the development sets.
4.2 Contrast between HiFST and Cube Pruning
We contrast two hierarchical phrase-based decoders. The first decoder, HCP, is a k-best
decoder using cube pruning following the description by Chiang (2007); in our im-
plementation, these k-best lists contain only unique hypotheses (Iglesias et al 2009a),
which are obtained by extracting the 10,000 best candidates from each cell (including
the language model cost), using a priority queue to explore the cross-product of the
k-best lists from the cells pointed by nonterminals. We find that deeper k-best lists
(i.e., k = 100, 000) results in impractical decoding times and that fixed k-best list depths
yields better performance than use of a likelihood threshold parameter. The second
decoder, HiFST, is a lattice-based decoder implemented with WFSTs as described earlier.
Hypotheses are generated after determinization under the tropical semiring so that
scores assigned to hypotheses arise from a single minimum cost/maximum likelihood
derivation.
Translation proceeds as follows. After Hiero translation with optimized feature
weights and the first-pass language model, hypotheses are written to disk. For HCP we
save translations as 10,000-best lists, whereas HiFST generates word lattices. The first-
pass results are then rescored with the main 5-gram language model. In this operation
the first-pass language model scores are removed before the main language model
scores are applied. We then perform MBR rescoring. For the n-best lists we rescore
the top 1,000 hypotheses using the negative sentence-level BLEU score as the loss
function (Kumar and Byrne 2004); we have found that using a deeper k-best list is
impractically slow. For the HiFST lattices we use lattice-based MBR search procedures
described by Tromble et al (2008) in an implementation based on standard WFST
operations (Allauzen et al 2007).
4.2.1 Shallow-1 Arabic-to-English Translation. We translate Arabic-to-English with
shallow-1 hierarchical decoding: as described in Section 3, nonterminals are allowed
only to generate target language phrases. Table 2 shows results for mt02-05-tune, mt02-
05-test, and mt08. In this experiment we use MERT to find optimized parameters for
HCP and we use these parameter values in HiFST as well. This allows for a close
comparison of decoder behavior, independent of parameter optimization.
In these experiments, the first-pass translation quality of the two systems (Table 2
a vs. b) is nearly identical. The most notable difference in the first-pass behavior of the
decoders is their memory use. For example, for an input sentence of 105 words, HCP
uses 1.2Gb memory whereas HiFST takes only 900Mb under the same conditions. To
run HCP successfully requires cube pruning with the first-pass 4-gram language model.
By contrast, HiFST requires no pruning during lattice construction and the first pass
language model is not applied until the lattice is fully built at the upper most cell of the
522
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 2
Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) after first-pass
decoding and subsequent rescoring steps. Decoding time reported for mt02-05-tune is in seconds
per word. Both systems are optimized using MERT over the k-best lists generated by HCP.
decoder time mt02-05-tune mt02-05-test mt08
a HCP 1.1 52.5 51.9 42.8
+5g - 53.4 52.9 43.5
+5g+MBR - 53.6 53.0 43.6
b HiFST 0.5 52.5 51.9 42.8
+5g - 53.6 53.2 43.9
+5g+LMBR - 54.3 53.7 44.8
CYK grid. For this grammar, HiFST is able to produce exact translations without any
search errors.
Search Errors Because both decoders are constrained to use exactly the same features,
we can compare their search errors on a sentence-by-sentence basis. A search error is
assigned to one of the decoders if the other has found a hypothesis with lower cost. For
mt02-05-tune, we find that in 18.5% of the sentences HiFST finds a hypothesis with lower
cost than HCP. In contrast, HCP never finds any hypothesis with lower cost for any
sentence. This is as expected: The HiFST decoder requires no pruning prior to applying
the first-pass language model, so search is exact.
Lattice/k-best Quality It is clear from the results that the lattices produced by HiFST
yield better rescoring results than the k-best lists produced by HCP. This is the case for
both 5-gram language model rescoring and MBR. In MT08 rescoring, HCP k-best lists
yield an improvement of 0.8 BLEU relative to the first-pass baseline, whereas rescoring
HiFST lattices yield an improvement of 2.0 BLEU. The advantage of maintaining a large
search space in lattice form during decoding is clear. The use of k-best lists in HCP limits
the gains from subsequent rescoring procedures.
Translation Speed HCP requires an average of 1.1 seconds per input word. HiFST
cuts this time by half, producing output at a rate of 0.5 seconds per word. It proves
much more efficient to process compact lattices containing many hypotheses rather than
independently processing each distinct hypothesis in k-best form.
4.2.2 Fully Hierarchical Chinese-to-English Translation. We translate Chinese-to-English
with full hierarchical decoding: nonterminals are allowed to generate other hierarchical
rules in recursion.
We apply the constraint hmax=10 for nonterminal category X, as described in Sec-
tion 2.2.2, so that only glue rules are allowed at upper levels of the CYK grid; this is
applied in both HCP and HiFST.
In HiFST any lattice in the CYK grid is pruned if it covers at least three source words
and contains more than 10,000 states. The log-likelihood pruning threshold relative to
the best path in the sublattices is 9.0.
Improved Optimization and Generalization Table 3 shows results for tune-nw, test-nw,
and mt08. The first two rows show results for HCP when using MERT parameters
optimized over k-best lists produced by HCP (row a) and by HiFST (row b); in the latter
case, we are tuning HCP parameters over the hypothesis list generated by HiFST. When
measured over test-nw this gives a 0.3 BLEU improvement. HCP benefits from tuning
523
Computational Linguistics Volume 36, Number 3
Table 3
Contrastive Chinese-to-English translation results (lower-cased IBM BLEU) after first-pass
decoding and subsequent rescoring steps. The MERT k-best column indicates which decoder
generated the k-best lists used in MERT optimization. The mt08 set contains 691 sentences of
newswire and 666 sentences of Web text.
decoder MERT k-best tune-nw test-nw mt08
a HCP HCP 32.8 33.1 ?
b HCP 32.9 33.4 28.2
+5g HiFST 33.4 33.8 28.7
+5g+MBR 33.6 34.0 28.9
c HiFST 33.1 33.4 28.1
+5g HiFST 33.8 34.3 29.0
+5g+LMBR 34.5 34.9 30.2
over the HiFST hypotheses and we conclude that using the k-best list obtained by the
HiFST decoder yields better parameters in optimization.
Search Errors Measured over the tune-nw development set, HiFST finds a hypothesis
with lower cost in 48.4% of the sentences. In contrast, HCP never finds any hypothesis
with a lower cost for any sentence, indicating that the described pruning strategy for
HiFST is much broader than that of HCP. Note that HCP search errors are more frequent
for this language pair. This is due to the larger search space required for full hierarchical
translation; the larger the search space, the more likely it is that search errors will be
introduced by the cube pruning algorithm.
Lattice/k-best Quality Large LMs and MBR both benefit from the richer space of
translation hypotheses contained in the lattices. Relative to the first-pass baseline in
MT08, rescoring HiFST lattices yields a gain of 2.1 BLEU, compared to a gain of 0.7
BLEU with HCP k-best lists.
4.2.3 Reliability of n-gram Posterior Distributions. MBR decoding under linear BLEU
(Tromble et al 2008) is driven mainly by the presence of high posterior n-grams in
the lattice; the low posterior n-grams have poor discriminatory power. In the following
experiment, we show that high posterior n-grams are more likely to be found in the
references, and that using the full evidence space of the lattice is much better than even
very large k-best lists for computing posterior probabilities. Let Ni = {w1, . . .,w|Ni|}
denote the set of n-grams of order i in the first-pass translation 1-best, and let Ri =
{w1, . . .,w|Ri|} denote the set of n-grams of order i in the union of the references.
For confidence threshold ?, let Ni,? = {w ? Ni : p(w|L) ? ?} denote the set of all
n-grams in Ni with posterior probability greater than or equal to ?, where p(w|L) is
the posterior probability of the n-gram w, that is, the sum of the posterior probabilities
of all translations containing at least one occurrence of w. The precision at order i for
threshold ? is the proportion of n-grams in Ni,? found in the references:
pi,? =
|Ri ?Ni,?|
|Ni,?|
. (7)
The average per-sentence 4-gram precision at a range of posterior probability thresholds
? is shown in Figure 12. The posterior probabilities are computed using either the full
524
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Figure 12
4-gram precisions for Arabic-to-English mt02-05-tune first-pass 1-best translations computed
using the full evidence space of the lattice and k-best lists of various sizes.
lattice L or a k-best list of the specified size. The 4-gram precision of the 1-best trans-
lations is approximately 0.35. At higher values of ?, the reference precision increases
considerably. Expanding the k-best list size from 1,000 to 10,000 hypotheses only slightly
improves the precision but much higher precisions are observed when the full evidence
space of the lattice is used. The improved precision results from more accurate estimates
of n-gram posterior probabilites and emphasizes once more the advantage of lattice-
based decoding and rescoring techniques.
4.3 Grammar Configurations and Search Parameters
We report translation performance and decoding speed as we vary hierarchical gram-
mar depth and the constraints on low-level rule concatenation (see Section 3). Unless
otherwise noted, hmin = 1 and hmax = 10 throughout (except for the ?S? nonterminal
category, where these constraints are not relevant).
4.3.1 Grammars for Arabic-to-English Translation. Table 4 reports Arabic-to-English trans-
lation results using the alternative grammar configurations described in Section 3.5.
Results are shown in first-pass decoding (HiFST rows), and in rescoring with a larger
5-gram language model for the most promising configurations (+5g rows). Decoding
time is reported for first-pass decoding only; rescoring time is negligible by comparison.
As shown in the upper part of Table 4, translation under a shallow-2 grammar does
not improve relative to a shallow-1 grammar, although decoding is much slower. This
indicates that the additional hypotheses generated when allowing a hierarchical depth
of two are not useful in Arabic-to-English translation. By contrast the shallow gram-
mars that allow long-distance movement for verbs only (shallow-1+K1,K2 = 1, 3, vo and
shallow-2+K1,K2 = 2, 3, vo), perform slightly better than shallow-1 grammar at a similar
decoding time. Performance differences increase when the larger 5-gram is applied
525
Computational Linguistics Volume 36, Number 3
Table 4
Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) with various
grammar configurations. Decoding time reported in seconds per word for mt02-05-tune.
grammar time mt02-05-tune mt02-05-test mt08
HiFST shallow-1 0.8 52.7 52.0 42.9
+K1,K2 = 1, 3 1.3 52.6 51.9 42.8
+K1,K2 = 1, 3, vo 0.9 52.7 52.1 42.9
shallow-2 4.2 52.7 51.9 42.6
+K1,K2 = 2, 3, vo 1.8 52.8 52.2 43.0
+5g shallow-1 - 53.9 53.4 44.9
+K1,K2 = 1, 3, vo - 54.1 53.6 45.0
shallow-2
+K1,K2 = 2, 3, vo
- 54.2 53.8 45.0
(Table 4, bottom). This is expected given that these grammars add valid translation
candidates to the search space with similar costs; a language model is needed to select
the good hypotheses among all those introduced.
4.3.2 Grammars for Chinese-to-English Translation. Table 5 shows contrastive results in
Chinese-to-English translation for full hierarchical and shallow-n (n = 1, 2, 3) gram-
mars.3 Unlike Arabic-to-English translation, Chinese-to-English translation improves
as the hierarchical depth of the grammar is increased (i.e., for larger n). Decoding time
also increases significantly. The shallow-1 grammar constraints which worked well for
Arabic-to-English translation are clearly inadequate for this task; performance degrades
by approximately 1.0 BLEU relative to the full hierarchical grammar.
However, we find that translation under the shallow-3 grammar yields performance
nearly as good as that of the full hiero grammars; translation times are shorter and
yield degradations of only 0.1 to 0.3 BLEU. Translation can be made significantly faster
by constraining the shallow-3 search space with hmin = 9, 5, 2 for X2,X1, and X0, respec-
tively; translation speed is reduced from 10.8 sec/word to 3.8 sec/word at a degradation
of 0.2 to 0.3 BLEU relative to full Hiero.
Shallow-3 grammars describe a restricted search-space but appear to have expressive
power in Chinese-to-English translation which is very similar to that of a full Hiero
grammar. Each cell (x, y) is represented by a bigger set of nonterminals; this allows for
more effective pruning strategies during lattice construction. We note also that hmax
values greater than 10 yield little improvement. As shown in the five bottom rows of
Table 5, differences between grammar configurations tend to carry through after 5-gram
rescoring. In summary, a shallow-3 grammar and filtering with hmin = 9, 5, 2 lead to a
0.4 degradation in BLEU relative to full Hiero. As a final contrast, the mixed-case NIST
BLEU-4 for the HiFST system on mt08 is 28.6. This result is obtained under the same
evaluation conditions as the official NIST MT08 Constrained Training Track.4
3 We note that the scores in full hiero row do not match those of row c in Table 3 which were obtained with
a slightly simplified version of HiFST and optimized according to the 2008 NIST implementation of IBM
BLEU; here we use the 2009 implementation by NIST.
4 Full MT08 results are available at
www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
526
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 5
Contrastive Chinese-to-English translation results (lower-cased IBM BLEU ) with various
grammar configurations and search parameters. Decoding time is reported in sec/word for
tune-nw.
grammar time tune-nw test-nw mt08 (nw)
HiFST shallow-1 0.7 33.6 33.4 32.6
shallow-2 5.9 33.8 34.2 32.7
+hmin=5 5.6 33.8 34.1 32.9
+hmin=7 4.0 33.8 34.3 33.0
shallow-3 8.8 34.0 34.3 33.0
+hmin=7 7.7 34.0 34.4 33.1
+hmin=9 5.9 33.9 34.3 33.1
+hmin=9,5,2 3.8 34.0 34.3 33.0
+hmin=9,5,2+hmax=11 6.1 33.8 34.4 33.0
+hmin=9,5,2+hmax=13 9.8 34.0 34.4 33.1
full hiero 10.8 34.0 34.4 33.3
+5g shallow-1 - 34.1 34.5 33.4
shallow-2 - 34.3 35.1 34.0
shallow-3 - 34.6 35.2 34.4
+hmin=9,5,2 - 34.5 34.8 34.2
full hiero - 34.5 35.2 34.6
4.4 Marginalization Over Translation Derivations
As has been discussed earlier, the translation model in hierarchical phrase-based ma-
chine translation allows for multiple derivations of a target language sentence. Each
derivation corresponds to a particular combination of hierarchical rules and it has been
argued that the correct approach to translation is to accumulate translation probability
by summing over the scores of all derivations (Blunsom, Cohn, and Osborne 2008).
Computing this sum for each of the many translation candidates explored during de-
coding is computationally difficult, however. For this reason the translation probability
is commonly computed using the Viterbi max-derivation approximation. This is the
approach taken in the previous sections in which translations scores were accumulated
under the tropical semiring.
The use of WFSTs allows the sum over alternative derivations of a target string
to be computed efficiently. HiFST generates a translation lattice realized as a weighted
transducer with output labels encoding words and input labels encoding the sequence
of rules corresponding to a particular derivation, and the cost of each path in the lattice
is the negative log probability of the derivation that generated the hypothesis.
Determinization applies the ? operator to all paths with the same word se-
quence (Mohri 1997). When applied in the log semiring, this operator computes the
sum of two paths with the same word sequence as x ? y = ?log(e?x + e?y) so that the
probabilities of alternative derivations can be summed.
Currently this operation is only performed in the top cell of the hierarchical decoder
so it is still an approximation to the true translation probability. Computing the true
translation probability would require the same operation to be repeated in every cell
during decoding, which is very time consuming. Note that the translation lattice was
generated with a language model and so the language model costs must be removed
527
Computational Linguistics Volume 36, Number 3
Table 6
Arabic-to-English results (lower-cased IBM BLEU) when determinizing the lattice at the
upper-most CYK cell with alternative semirings.
semiring mt02-05-tune mt02-05-test mt08
tropical HiFST 52.8 52.2 43.0
+5g 54.2 53.8 44.9
+5g+LMBR 55.0 54.6 45.5
log HiFST 53.1 52.6 43.2
+5g 54.6 54.2 45.2
+5g+LMBR 55.0 54.6 45.5
before determinization to ensure that only the derivation probabilities are included
in the sum. After determinization, the language model is reapplied and the 1-best
translation hypothesis can be extracted from the logarc determinized lattices.
Table 6 compares translation results obtained using the tropical semiring (Viterbi
likelihoods) and the log semiring (marginal likelihoods). First-pass translation shows
small gains in all sets: +0.3 and +0.4 BLEU for mt02-05-tune and mt02-05-test, and +0.2
for mt08. These gains show that the sum over alternative derivations can be easily
obtained in HiFST simply by changing semiring and that these alternative derivations
are beneficial to translation. The gains carry through to the large language model 5-gram
rescoring stage but after LMBR the final BLEU scores are unchanged. The hypotheses
selected by LMBR are in almost all cases exactly the same regardless of the choice of
semiring. This may be due to the fact that our current marginalization procedure is only
an approximation to the true marginal likelihoods, since the log semiring determiniza-
tion operation is applied only in the uppermost cell of the CYK grid and MERT training
is performed using regular Viterbi likelihoods.
We note that a close study of the interaction between LMBR and marginalization
over derivations is beyond the scope of this paper. Our purpose here is to show how
easily these operations can be done using WFSTs.
4.5 Combining Lattices Obtained from Alternative Morphological Decompositions
It has been shown that MBR decoding is a very effective way of combining translation
hypotheses obtained from alternative morphological decompositions of the same source
data. In particular, de Gispert et al (2009) show gains for Arabic-to-English and Finnish-
to-English when taking k-best lists obtained from two morphological decompositions
of the source language. Here we extend this approach to the case of translation lat-
tices and experiment with more than two alternative decompositions. We will show
that working with translation lattices gives significant improvements relative to k-best
lists.
In lattice-based MBR system combination, first-pass decoding results in a set of I
distinct translation lattices L(i), i = 1. . .I for each foreign sentence, with each lattice
produced by translating one of the alternative morphological decompositions. The
evidence space for MBR decoding is formed as the union of these lattices L =
?I
i=1 L(i).
The posterior probability of n-gram w in the union of lattices is computed as a simple
528
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
Table 7
Arabic-to-English results (lower-cased IBM BLEU) when using alternative Arabic
decompositions, and their combination with k-best-based and lattice-based MBR.
configuration mt02-05-tune mt02-05-test mt08
a HiFST+5g 54.2 53.8 44.9
b HiFST+5g 53.8 53.6 45.0
c HiFST+5g 54.1 53.8 44.7
a+b +MBR 55.1 54.7 46.1
+LMBR 55.7 55.4 46.7
a+c +MBR 55.4 54.9 46.5
+LMBR 56.0 55.9 46.9
a+b+c +MBR 55.3 54.9 46.5
+LMBR 56.0 55.7 47.3
linear interpolation of the posterior probabilities according to the evidence space of each
individual lattice so that
p(w|L) =
I
?
i=1
?i pi(w|L(i) ), (8)
where the interpolation parameters 0 ? ?i ? 1 such that
?I
i=1 ?i = 1 specify the weight
associated with each system in the combination and are optimized with respect to
the tuning set. The system-specific posteriors required for the interpolation are com-
puted as
pi(w|L(i) ) =
?
E?L(i)w
Pi(E|F), (9)
where Pi(E|F) is the posterior probability of translation E given source sentence F and
the sum is taken over the subset L(i)w = {E ? L(i) : #w(E) > 0} of the lattice L(i) containing
paths with at least one occurrence of the n-gram w. These posterior probabilities are
used in MBR decoding under the linear approximation to the BLEU score described
in Tromble et al (2008). We find that for system combination, decoding often produces
output that is slightly shorter than required. A fixed per-word factor optimized on the
tuning set is applied when computing the gain and this results in output with improved
BLEU score and reduced brevity penalty.
Table 7 shows translation results in Arabic-to-English using three alternative mor-
phological decompositions of the Arabic text (upper rows a, b, and c). For each decom-
position an independent set of hierarchical rules is obtained from the respective parallel
corpus alignments. The decompositions were generated by the MADA toolkit (Habash
and Rambow 2005) with two alternative tokenization schemes, and by the Sakhr Arabic
Morphological Tagger, developed by Sakhr Software in Egypt.
The following rows show the results when combining with MBR the translation
hypotheses obtained from two or three decompositions. The table also shows a contrast
529
Computational Linguistics Volume 36, Number 3
Figure 13
Average per-sentence 4-gram reference precisions for Arabic-to-English mt02-05-tune
single-system MBR 1-best translations and the 1-best obtained through MBR system
combination.
between decoding the joint k-best lists (rows named MBR, with k = 1, 000) and decod-
ing the unioned translation lattices (rows named LMBR). In line with the findings of
de Gispert et al (2009), we find significant gains from combining k-best lists with respect
to using any one segmentation alone. Interestingly, here we find further gains when
applying lattice-based MBR instead of a k-best approach, obtaining consistent gains of
0.6?0.8 BLEU across all sets.
The results reported in Table 7 are very competitive. The mixed-case NIST BLEU-4
score for a+b+c LMBR system in MT08 is 44.9. This result is obtained under the same
evaluation conditions as the official NIST MT08 Constrained Training Track.5 For MT09,
the mixed-case BLEU-4 is 48.3, which ranks first in the Arabic-to-English NIST 2009
Constrained Data Track.6
4.5.1 System Combination and Reference Precision. We have demonstrated that MBR de-
coding of multiple lattices generated from alternative morphological segmentations
leads to significant improvements in BLEU score. We now show that one reason for
the improved performance is that lattice combination leads to better n-gram posterior
probability estimates. To combine two equally weighted lattices L(1) and L(2), the in-
terpolation weights are ?1 = ?2 = 12 ; Equation (8) simplifies as p(w|L) = 12 (p1(w|L(1) )+
p2(w|L(2))). Figure 13 plots average per-sentence reference precisions for the 4-grams in
the MBR 1-best of systems a and b and their combination (labeled a+b) at a range of
posterior probability thresholds 0 ? ? ? 1. Systems a and b have similar precisions at
all values of ?, confirming that the optimal interpolation weights for this combination
should be equal. The precision obtained using n-gram posterior probabilities computed
5 Full MT08 results are available at
www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
6 Full MT09 results are available at www.itl.nist.gov/iad/mig/tests/mt/2009/ResultsRelease
530
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
from the combined lattices is higher than that of the individual systems. A higher
proportion of the n-grams assigned high posterior probability under the interpolated
distribution are found in the references and this is one of the reasons for the large gains
in BLEU in lattice-based MBR system combination.
4.6 European Language Translation
The HiFST described here has also been found to achieve competitive performance for
other language pairs, such as Spanish-to-English and Finnish-to-English.
For Spanish-to-English we carried out experiments on the shared task of the ACL
2008 Workshop on Statistical Machine Translation (Callison-Burch et al 2008) based on
the Europarl corpus. For the official test2008 evaluation set we obtain a BLEU score of
34.2 using a shallow-1 grammar. Similarly to the Arabic case, deeper grammars are not
found to improve scores for this task.
In Finnish-to-English, we conducted translation experiments based on the Europarl
corpus using 3,000 sentences from the Q4/2000 period for testing with a single ref-
erence. In this case, the shallow-1 grammar obtained a BLEU score of 28.0, whereas
the full hierarchical grammar only achieved 27.6. This is further evidence that full-
hierarchical grammars are not appropriate in all instances. In this case we suspect that
the use of Finnish words without morphological decomposition leads to data sparsity
problems and complicates the task of learning complex translation rules. The lack of
a large English language model suitable for this domain may also make it harder to
select the right hypothesis when the translation grammar produces many more English
alternatives.
5. Conclusions
We have described two linked investigations into hierarchical phrase-based translation.
We investigate the use of weighted finite state transducers rather than k-best lists to
represent the space of translation hypotheses. We describe a lattice-based Hiero de-
coder, with which we find reductions in search errors, better parameter optimization,
and improved translation performance. Relative to these reductions in search errors,
direct generation of target language translation lattices also leads to further translation
improvements through subsequent rescoring steps, such as MBR decoding and the
application of large n-gram language models. These steps can be carried out easily via
standard WFST operations.
As part of the machinery needed for our experiments we develop WFST procedures
for alignment and feature extraction so that statistics needed for system optimization
can be easily obtained and represented as transducers. In particular, we make use of
a lattice-based representation of sequences of rule applications, which proves useful
for minimum error rate training. In all instances we find that using lattices as compact
representations of translation hypotheses offers clear modeling advantages.
We also investigate refinements in translation search space through shallow-n gram-
mars, structured long-distance movement, and constrained word deletion. We find
that these techniques can be used to fit the complexity of Hiero translation systems to
individual language pairs. In translation from Arabic into English, shallow grammars
make it possible to explore the entire search space and to do so more quickly but with the
same translation quality as the full Hiero grammar. Even in complex translation tasks,
such as Chinese to English, we find significant speed improvements with minimal loss
531
Computational Linguistics Volume 36, Number 3
in performance using these methods. We take the view that it is better to perform exact
search of a constrained space than to risk search errors in translation.
We note finally that Chiang introduced Hiero as a model ?based on a synchronous
CFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns
1968).? We have taken this formulation as a starting point for the development of
novel realizations of Hiero. Our motivation has mainly been practical in that we seek
improved translation quality and efficiency through better models and algorithms.
Our approach suggests close links between Hiero and Recursive Transition Net-
works (Woods 1970; Mohri 1997). Although this connection is beyond the scope of this
paper, we do note that Hiero translation requires keeping track of two grammars, one
based on the Hiero translation rules and the other based on n-gram language model
probabilities. These two grammars have very different dependencies which suggests
that a full implementation of Hiero translation such as we have addressed does not
have a simple expression as an RTN.
Acknowledgments
This work was supported in part by the
GALE program of the Defense Advanced
Research Projects Agency, Contract No.
HR0011- 06-C-0022, and in part by the
Spanish government and the ERDF under
projects TEC2006-13694-C03-03 and
TEC2009-14094-C04-04.
References
Allauzen, Cyril, Mehryar Mohri, and Brian
Roark. 2003. Generalized algorithms for
constructing statistical language models.
In Proceedings of ACL, pages 557?564,
Sapporo.
Allauzen, Cyril, Michael Riley, Johan
Schalkwyk, Wojciech Skut, and Mehryar
Mohri. 2007. OpenFst: A general and
efficient weighted finite-state transducer
library. In Proceedings of CIAA,
pages 11?23, Prague.
Bender, Oliver, Evgeny Matusov, Stefan
Hahn, Sasa Hasan, Shahram Khadivi,
and Hermann Ney. 2007. The RWTH
Arabic-to-English spoken language
translation system. In Proceedings of
ASRU, pages 396?401, Kyoto.
Blunsom, Phil, Trevor Cohn, and Miles
Osborne. 2008. A discriminative latent
variable model for statistical machine
translation. In Proceedings of ACL-HLT,
pages 200?208, Columbus, OH.
Brants, Thorsten, Ashok C. Popat, Peng Xu,
Franz J. Och, and Jeffrey Dean. 2007.
Large language models in machine
translation. In Proceedings of EMNLP-ACL,
pages 858?867, Prague.
Callison-Burch, Chris, Cameron Fordyce,
Philipp Koehn, Christof Monz, and Josh
Schroeder. 2008. Further meta-evaluation
of machine translation. In Proceedings
of the ACL Workshop on Statistical
Machine Translation, pages 70?106,
Columbus, OH.
Chappelier, Jean-Ce?dric and Martin Rajman.
1998. A generalized CYK algorithm for
parsing stochastic CFG. In Proceedings of
TAPD, pages 133?137, Paris.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of ACL,
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
de Gispert, Adria`, Sami Virpioja, Mikko
Kurimo, and William Byrne. 2009.
Minimum Bayes-Risk combination of
translation hypotheses from alternative
morphological decompositions. In
Proceedings of HLT/NAACL, Companion
Volume: Short Papers, pages 73?76,
Boulder, CO.
Deng, Yonggang and William Byrne. 2008.
HMM word and phrase alignment for
statistical machine translation. IEEE
Transactions on Audio, Speech, and Language
Processing, 16(3):494?507.
Graehl, Jonathan, Kevin Knight, and
Jonathan May. 2008. Training tree
transducers. Computational Linguistics,
34(3):391?427.
Habash, Nizar and Owen Rambow. 2005.
Arabic tokenization, part-of-speech
tagging and morphological
disambiguation in one fell swoop. In
Proceedings of the ACL, pages 573?580,
Ann Arbor, MI.
Iglesias, Gonzalo, Adria` de Gispert,
Eduardo R. Banga, and William Byrne.
2009a. Rule filtering by pattern for
532
de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammars
efficient hierarchical translation. In
Proceedings of the EACL, pages 380?388,
Athens.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo
R. Banga, and William Byrne. 2009b.
Hierarchical phrase-based translation with
weighted finite state transducers. In
Proceedings of HLT/NAACL, pages 433?441,
Boulder, CO.
Iglesias, Gonzalo, Adria` de Gispert, Eduardo
R. Banga, and William Byrne. 2009c. The
HiFST system for the Europarl
Spanish-to-English task. In Proceedings of
SEPLN, pages 207?214, Donosti.
Knight, Kevin and Yaser Al-Onaizan. 1998.
Translation with finite-state devices. In
Proceedings of the Third Conference of the
AMTA on Machine Translation and the
Information Soup, pages 421?437,
Langhorne, PA.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL,
pages 127?133, Edmonton.
Kumar, Shankar and William Byrne. 2004.
Minimum Bayes-risk decoding for
statistical machine translation. In
Proceedings of HLT-NAACL, pages 169?176,
Boston, MA.
Kumar, Shankar, Yonggang Deng, and
William Byrne. 2006. A weighted finite
state transducer translation template
model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Lewis, P. M., II, and R. E. Stearns. 1968.
Syntax-directed transduction. Journal of the
ACM, 15(3):465?488.
Mohri, Mehryar. 1997. Finite-state
transducers in language and speech
processing. Computational Linguistics,
23:269?311.
Mohri, Mehryar, Fernando Pereira, and
Michael Riley. 2000. The design principles
of a weighted finite-state transducer
library. Theoretical Computer Science,
231:17?32.
Mohri, Mehryar, Fernando Pereira, and
Michael Riley. 2002. Weighted finite-state
transducers in speech recognition.
Computer Speech and Language, 16:69?88.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL, pages 160?167,
Sapporo.
Och, Franz Josef and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the ACL,
pages 295?302, Philadelphia, PA.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. BLEU: A
method for automatic evaluation of
machine translation. In Proceedings of ACL,
pages 311?318, Toulouse.
Rosti, Antti-Veikko, Necip Fazil Ayan, Bing
Xiang, Spyros Matsoukas, Richard
Schwartz, and Bonnie Dorr. 2007.
Combining outputs from multiple
machine translation systems. In
Proceedings of HLT-NAACL, pages 228?235,
Rochester, NY.
Setiawan, Hendra, Min Yen Kan, Haizhou Li,
and Philip Resnik. 2009. Topological
ordering of function words in hierarchical
phrase-based translation. In Proceedings
of the ACL-IJCNLP, pages 324?332,
Singapore.
Sim, Khe Chai, William Byrne, Mark Gales,
Hichem Sahbi, and Phil Woodland. 2007.
Consensus network decoding for statistical
machine translation system combination.
In Proceedings of ICASSP, pages 105?108,
Honolulu, HI.
Tromble, Roy, Shankar Kumar, Franz J. Och,
and Wolfgang Macherey. 2008. Lattice
Minimum Bayes-Risk decoding for
statistical machine translation. In
Proceedings of EMNLP, pages 620?629,
Honolulu, HI.
Varile, Giovanni B. and Peter Lau. 1988.
Eurotra practical experience with a
multilingual machine translation system
under development. In Proceedings of the
Second Conference on Applied Natural
Language Processing, pages 160?167,
Austin, TX.
Woods, W. A. 1970. Transition network
grammars for natural language analysis.
Communications of the ACM,
13(10):591?606.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
533

Pushdown Automata in Statistical
Machine Translation
Cyril Allauzen?
Google Research
Bill Byrne??
University of Cambridge
Adria` de Gispert??
University of Cambridge
Gonzalo Iglesias??
University of Cambridge
Michael Riley?
Google Research
This article describes the use of pushdown automata (PDA) in the context of statistical machine
translation and alignment under a synchronous context-free grammar. We use PDAs to com-
pactly represent the space of candidate translations generated by the grammar when applied to an
input sentence. General-purpose PDA algorithms for replacement, composition, shortest path,
and expansion are presented. We describe HiPDT, a hierarchical phrase-based decoder using the
PDA representation and these algorithms. We contrast the complexity of this decoder with a de-
coder based on a finite state automata representation, showing that PDAs provide a more suitable
framework to achieve exact decoding for larger synchronous context-free grammars and smaller
language models. We assess this experimentally on a large-scale Chinese-to-English alignment
and translation task. In translation, we propose a two-pass decoding strategy involving a weaker
language model in the first-pass to address the results of PDA complexity analysis. We study
in depth the experimental conditions and tradeoffs in which HiPDT can achieve state-of-the-art
performance for large-scale SMT.
? Google Research, 76 Ninth Avenue, New York, NY 10011. E-mail: {allauzen,riley}@google.com.
?? University of Cambridge, Department of Engineering. CB2 1PZ Cambridge, U.K. and SDL Research,
Cambridge U.K. E-mail: {wjb31,ad465,gi212}@eng.cam.ac.uk.
Submission received: 6 August 2012; revised version received: 20 February 2013; accepted for publication:
2 December 2013.
doi:10.1162/COLI a 00197
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
1. Introduction
Synchronous context-free grammars (SCFGs) are nowwidely used in statistical machine
translation, with Hiero as the preeminent example (Chiang 2007). Given an SCFG and
an n-gram language model, the challenge is to decode with them, that is, to apply them
to source text to generate a target translation.
Decoding is complex in practice, but it can be described simply and exactly in
terms of the formal languages and relations involved. We will use this description
to introduce and analyze pushdown automata (PDAs) for machine translation. This
formal description will allow close comparison of PDAs to existing decoders which are
based on other forms of automata. Decoding can be described in terms of the following
steps:
1. Translation: T = ?2({s}?G)
The first step is to compose the finite language {s}, which represents the
source sentence to be translated, with the algebraic relation G for the
translation grammar G. The result of this composition projected on the
output side is T , a weighted context-free grammar that contains all possible
translations of s under G. Following the usual definition of Hiero grammars,
we assume that G does not allow unbounded insertions so that T is a
regular language.
2. Language Model Application: L=T ?M
The next step is to compose the result of Step 1 with the weighted regular
grammarM defined by the n-gram language model, M. The result of this
composition is L, whose paths are weighted by the combined language
model and translation scores.
3. Search: l?=argmaxl?LL
The final step is to find the path through L that has the best combined
translation and language model score.
The composition {s} ? G in Step 1 that generates T can be performed by a modified
CYK algorithm (Chiang 2007). Our interest is in the different types of automata that can
be used to represent T as it is produced by this composition. We focus on three types
of representations: hypergraphs (Chiang 2007), weighted finite state automata (Iglesias
et al. 2009a; de Gispert et al. 2010), and PDAs. We will give a formal definition of PDAs
in Section 2, but we will first illustrate and compare these different representations by
a simple example.
Consider translating a source sentence ?s1 s2 s3? with a simple Hiero grammar G :
X??s1, t2 t3?
S??X s2 s3, t1 t2 X t4 t7?
S??X s2 s3, t1 t3 X t6 t7?
Step 1 yields the translations T = {?t1 t2 t2 t3 t4 t7? , ?t1 t3 t2 t3 t6 t7?}, and Figure 1 gives
examples of the different representations of these translations.We summarize the salient
features of these representations as they are used in decoding.
Hypergraphs. As described by Chiang (2007), a Hiero decoder can generate translations
in the form of a hypergraph, as in Figure 1a. As the figure shows, there is a
1:1 correspondence between each production in the CFG and each hyperedge in
the hypergraph.
688
Allauzen et al. Pushdown Automata in Statistical Machine Translation
(a) Hypergraph
0
1t1
6
t1
2t2
7t3
3X 4t4 5
t7
8X 9t6 10
t7 0 1t2 2t3
S X
(b) RTN
0
1t1
2
t1
3t2
4t3
5eps
6eps
7t2
8t2
9t3
10t3
11eps
12eps
13t4
14t6
15t7
16
t7
(c) FSA
0
1t1
6
t1
2t2
7t3
11
(
12t2
3 4t4 5
t7
[
8 9
t6
10t7
13t3
)
]
(d) PDA
Figure 1
Alternative representations of the regular language of possible translation candidates. Valid
paths through the PDA must have balanced parentheses.
Decoding proceeds by intersecting the translation hypergraph with a language
model, represented as a finite automaton, yielding L as a hypergraph. Step 3
yields a translation by finding the shortest path through the hypergraphL (Huang
2008).
Weighted Finite State Automata (WFSAs). Because T is a regular language and M is
represented by a finite automaton, it follows that T and L can themselves
be represented as finite automata. Consequently, Steps 2 and 3 can be solved
689
Computational Linguistics Volume 40, Number 3
using weighted finite-state intersection and single-source shortest path algo-
rithms, respectively (Mohri 2009). This is the general approach adopted in the
HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010), which first represents
T as a Recursive Transition Network (RTN) and then performs expansion to
generate a WFSA.
Figure 1b shows the space of translations for this example represented as an RTN.
Like the hypergraph, it also has a 1:1 correspondence between each production
in the CFG and paths in the RTN components. The recursive RTN itself can be
expanded into a single WFSA, as shown in Figure 1c. Intersection and shortest
path algorithms are available for both of these WFSAs.
Pushdown Automata. Like WFSAs, PDAs are easily generated from RTNs, as will be
described later, and Figure 1d gives the PDA representation for this example. The
PDA represents the same language as the FSA, but with fewer states. Procedures
to carry out Steps 2 and 3 in decoding will be described in subsequent sections.
We will show that PDAs provide a general framework to describe key aspects
of several existing and novel translation algorithms. We note that PDAs have long
been used to describe parsing algorithms (Aho and Ullman 1972; Lang 1974), and it is
well known that pushdown transducers, the extended version of PDA with input and
output labels in each transition, do not have the expressive power needed to generate
synchronous context-free languages. For this reason, we do not use PDAs to implement
Step 1 in decoding: throughout this article a CYK-like parsing algorithm is always used
for Step 1. However, we do use PDAs to represent the regular languages produced in
Step 1 and in the intersection and shortest distance operations needed for Steps 2 and 3.
1.1 HiPDT: Hierarchical Phrase-Based Translation with PDAs
We introduce HiPDT, a hierarchical phrase-based decoder that uses a PDA representa-
tion for the target language. The architecture of the system is shown in Figure 2, where
CYK parse s with G
Build RTN
Expand RTN to FSA
Intersect FSA with LM
FSA 
Shortest 
Path
FSA 
Pruning
Lattice
1-Best
Hypothesis
RTN to PDA Replacement
Intersect PDA with LM
PDA 
(Pruned) 
Expansion
PDA 
Shortest 
Path
HiPDTHiFST
Figure 2
HiPDT versus HiFST: General flow and high-level operations.
690
Allauzen et al. Pushdown Automata in Statistical Machine Translation
we contrast it with HiFST (de Gispert et al. 2010). Both decoders parse the sentence with
a grammar G using a modified version of the CYK algorithm to generate the translation
search space as an RTN. Each decoder then follows a different path: HiFST expands
the RTN into an FSA, intersects it with the language model, and then prunes the result;
HiPDT performs the following steps:
1. Convert the RTN into PDA using the replacement algorithm. The PDA
representation for the example grammar in Section 1 is shown in Figure 1.
The algorithm will be described in Section 3.2.
2. Apply the language model scores to the PDA by composition. This operation
is described in Section 3.3.
3. Perform either one of the following operations:
(a) Shortest path through the PDA to get the exact best translation under
the model. Shortest distance/path algorithm is described in Section 3.4.
(b) Pruned expansion to an FSA. This expansion uses admissible pruning
and outputs a lattice. We do this for posterior rescoring steps. The
algorithm will be presented in detail in Sections 3.5 and 3.5.2.
The principal difference between the two decoders is the point at which finite-state
expansion is performed. In HiFST, the RTN representation is immediately expanded to
an FSA. In HiPDT, the PDA pruned expansion or shortest path computation is done
after the language model is applied, so that all computation is done with respect to both
the translation and language model scores.
The use of RTNs as an initial translation representation is somewhat influenced by
the development history of our FST and SMT systems. RTN algorithms were available
in OpenFST at the time HiFST was developed. HiPDT was developed as an extension to
HiFST using PDA algorithms, and these have subsequently been included in OpenFST.
A possible alternative approach could be to produce a PDA directly by traversing the
CYK grid. WFSAs could then be generated by PDA expansion, with a computational
complexity in speed and memory usage similar to the RTN-based approach.We present
RTNs as the initial translation representation because the generation of RTNs during
parsing is straightforward and has been previously presented (de Gispert et al. 2010).
We note, however, that RTN composition is algorithmically more complex than PDA
(and FSA) composition, so that RTNs themselves are not ideal representations of T if a
language model is to be applied. Composition of PDAs with FSAs will be discussed in
Section 3.3.
Figure 3 continues the simple translation example from earlier, showing how
HiPDT andHiFST both benefit from the compactness offeredbyWFSA epsilon removal,
determinization, andminimization operations. When applied to PDAs, these operations
treat parentheses as regular symbols. Compact representations of RTNs are shared by
both approaches. Figure 4 illustrates the PDA representation of the translation space
under a slightly more complex grammar that includes rules with alternative orderings
of nonterminals. The rule S??X1 s2 X2, t1 X1 X2? produces the sequence ?t1 t3 t4 t5 t6?,
and S??X1 s2 X2, t2 X2 X1? produces ?t2 t5 t6 t3 t4?. The PDA efficiently represents the
alternative orderings of the phrases ?t3 t4? and ?t5 t6? allowed under this grammar.
In addition to translation, this architecture can also be used directly to carry out
source-to-target alignment, or synchronous parsing, under the SCFG in a two-step
composition rather than one synchronous parsing stage. For example, by using M as the
automata that accepts ?t1 t2 t3 t6 t7?, Step 2 will yield all derivations that yield this string
691
Computational Linguistics Volume 40, Number 3
0 1t1
2t2
3
t3
4X
5X
6
t4
t6 7
t7
0 1t2 2t3
S X
(a) Optimized RTN
0 1t1
2t2
3
t3
4t2
5t2
6t3
7t3
8
t4
t6 9
t7
(b) Optimized FSA
0 1t1
2t2
3
t3 8
(
[ 9
t2
4
6
t4
7t7
5
t610t3
)
]
(c) Optimized PDA
Figure 3
Optimized representations of the regular language of possible translation candidates.
as a translation of the source string. This is the approach taken in Iglesias et al. (2009a)
and de Gispert et al. (2010) for the RTN/FSA and in Dyer (2010b) for hypergraphs. In
Section 4 we analyze how PDAs can be used for alignment.
1.2 Goals
We summarize here the aims of this article.
We will show how PDAs can be used as compact representations of the space T
of candidate translations generated by a hierarchical phrase-based SCFG when
applied to an input sentence s and intersected with a language model M.
We have described the architecture of HiPDT, a hierarchical phrase-based de-
coder based on PDAs, and have identified the general-purpose algorithms needed
0
1
t1
2t2
3(
4
[
5
t3
6t57t4 8t69)
10]
)
11]
(
[
X??s1, t3 t4?
X??s3, t5 t6?
S??X1 s2 X2, t1 X1 X2?
S??X1 s2 X2, t2 X2 X1?
Figure 4
Example of translation grammar with reordered nonterminals and the PDA representing the
result of applying the grammar to input sentence s1 s2 s3.
692
Allauzen et al. Pushdown Automata in Statistical Machine Translation
to perform translation and alignment; in doing so we have highlighted the
similarities and differences relative to translation with FSAs (Section 1.1). We
will provide a formal description of PDAs (Section 2) and present in detail
the associated PDA algorithms required to carry out Steps 2 and 3, including
RTN replacement, composition, shortest path, expansion, and pruned expansion
(Section 3).
We will show both theoretically and experimentally that the PDA representation is
well suited for exact decoding under a large SCFG and a small languagemodel.
An analysis of decoder complexity in terms of the automata used in the repre-
sentation is presented (Section 3). One important aspect of the translation task
is whether the search for the best translation is admissible (or exact) under the
translation and language models. Stated differently, we wish to know whether a
decoder produces the actual shortest path found or whether some form of pruning
might have introduced search errors. In our formulation, we can exclude inadmis-
sible pruning from the shortest-path algorithms, and doing so makes it straight-
forward to compare the computational complexity of a full translation pipeline
using different representations of T (Section 4). We empirically demonstrate that
a PDA representation is superior to an FSA representation in the ability to perform
exact decoding both in an inversion transduction grammar?style word alignment
task and in a translation task with a small language model (Section 4). In these
experiments we take HiFST as a contrastive system for HiPDT, but we do not
present experimental results with hypergraph representations. Hypergraphs are
widely used by the SMT community, and discussions and contrastive experiments
between HiFST and cube pruning decoders are available in the literature (Iglesias
et al. 2009a; de Gispert et al. 2010).
We will propose a two-pass translation decoding strategy for HiPDT based on
entropy-pruned first-pass language models.
Our complexity analysis prompts us to investigate decoding strategies based on
large translation grammars and small language models. We describe, implement,
and evaluate a two-pass decoding strategy for a large-scale translation task using
HiPDT (Section 5). We show that entropy-pruned languagemodels can be used in
first-pass translation, followed by admissible beam pruning of the output lattice
and subsequent rescoring with a full language model. We analyze the search
errors that might be introduced by a two-pass translation approach and show
that these can be negligible if pruning thresholds are set appropriately (Sec-
tion 5.2). Finally, we detail the experimental conditions and speed/performance
tradeoffs that allow HiPDT to achieve state-of-the-art performance for large-
scale SMT under a large grammar (Section 5.3), including lattice rescoring steps
under a vast 5-gram language model and lattice minimum Bayes risk decoding
(Section 5.4).
With this translation strategyHiPDT can yield very good translation performance.
For comparison, the performance of this Chinese-to-English SMT described in
Section 5.4 is equivalent to that of the University of Cambridge submission to the
NIST OpenMT 2012 Evaluation.1
1 For details see http://www.nist.gov/itl/iad/mig/openmt12.cfm.
693
Computational Linguistics Volume 40, Number 3
2. Pushdown Automata
Informally, pushdown transducers are finite-state transducers that have been aug-
mented with a stack. Typically this is done by adding a stack alphabet and labeling
each transition with a stack operation (a stack symbol to be pushed onto, popped, or
read from the stack) in addition to the usual input and output labels (Aho and Ullman
1972; Berstel 1979) and weight (Kuich and Salomaa 1986; Petre and Salomaa 2009). Our
equivalent representation allows a transition to be labeled by a stack operation or a
regular input/output symbol, but not both. Stack operations are represented by pairs
of open and close parentheses (pushing a symbol on and popping it from the stack).
The advantage of this representation is that it is identical to the finite automaton repre-
sentation except that certain symbols (the parentheses) have special semantics. As such,
several finite-state algorithms either immediately generalize to this PDA representation
or do so with minimal changes. In this section we formally define pushdown automata
and transducers.
2.1 Definitions
A (restricted) Dyck language consist of ?well-formed? or ?balanced? strings over a
finite number of pairs of parentheses. Thus the string ( [ ( ) ( ) ] { } [ ] ) ( ) is in the
Dyck language over three pairs of parentheses (see Berstel 1979 for a more detailed
presentation).
More formally, let A and A be two finite alphabets such that there exists a bijection
f from A to A. Intuitively, f maps an open parenthesis to its corresponding close
parenthesis. Let a? denote f (a) if a?A and f?1(a) if a?A. The Dyck language DA
over the alphabet A?=A ? A is then the language defined by the following context-free
grammar: S? ?, S? SS and S? aSa? for all a?A. We define the mapping cA : A?? ? A??
as follows. cA(x) is the string obtained by iteratively deleting from x all factors of the
form aa? with a ? A. Observe that DA=c?1A (?). Finally, for a subset B ? A, we define the
mapping rB : A? ? B? by rB(x1 . . . xn)=y1 . . . yn with yi=xi if xi?B and yi=? otherwise.
A semiring (K,?,?, 0, 1) is a ring that may lack negation. It is specified by a set
of values K, two binary operations ? and ?, and two designated values 0 and 1.
The operation ? is associative, commutative, and has 0 as identity. The operation ?
is associative, has identity 1, distributes with respect to ?, and has 0 as annihilator:
for all a ? K, a? 0 = 0? a = 0. If ? is also commutative, we say that the semiring is
commutative.
The probability semiring (R+,+,?, 0, 1) is used when the weights represent prob-
abilities. The log semiring (R ? {?},?log,+,?, 0), isomorphic to the probability semi-
ring via the negative-log mapping, is often used in practice for numerical stability. The
tropical semiring (R ? {?}, min,+,?, 0) is derived from the log semiring using the
Viterbi approximation. These three semirings are commutative.
A weighted pushdown automaton (PDA) T over a semiring (K,?,?, 0, 1) is an
8-tuple (?,?,?,Q,E, I, F, ?) where ? is the finite input alphabet, ? and ? are the finite
open and close parenthesis alphabets, Q is a finite set of states, I?Q the initial state,
F ? Q the set of final states, E ? Q? (? ? ?? ? {?})?K?Q a finite set of transitions,
and ? : F? K the final weight function. Let e= (p[e], i[e],w[e], n[e]) denote a transition
in E; for simplicity, (p[e], i[e], n[e]) denotes an unweighted transition (i.e., a transition
with weight 1?).
694
Allauzen et al. Pushdown Automata in Statistical Machine Translation
A path ? is a sequence of transitions ?=e1 . . . en such that n[ei]=p[ei+1] for 1 ? i <
n. We then define p[?]=p[e1], n[?]=n[en], i[?]= i[e1] ? ? ? i[en], and w[?]=w[e1]? . . .?
w[en]. A path ? is accepting if p[?]= I and n[?]?F. A path ? is balanced if r??(i[?])?D?.
A balanced path ? accepts the string x??? if it is a balanced accepting path such that
r?(i[?])=x.
The weight associated by T to a string x??? is
T(x)=
?
??P(x)
w[?]??(n[?]) (1)
where P(x) denotes the set of balanced paths accepting x. A weighted language is
recognizable by a weighted pushdown automaton iff it is context-free. We define the
size of T as |T|= |Q|+|E|.
A PDA T has a bounded stack if there exists K ? N such that for any path ? from I
such that c?(r??(i[?])) ? ??:
|c?(r??(i[?]))| ? K (2)
In other words, the number of open parentheses that are not closed along ? is bounded.
If T has a bounded stack, then it represents a regular language. Figure 5 shows non-
regular, regular, and bounded-stack PDAs. A weighted finite automaton (FSA) can be
viewed as a PDA where the open and close parentheses alphabets are empty (see Mohri
2009 for a stand-alone definition).
Finally, a weighted pushdown transducer (PDT) T over a semiring (K,?,?, 0, 1)
is a 9-tuple (?,?,?,?,Q,E, I, F, ?) where ? is the finite input alphabet, ? is the finite
output alphabet, ? and ? are the finite open and close parenthesis alphabets, Q is a
finite set of states, I?Q the initial state, F ? Q the set of final states, E ? Q? (? ? ?? ?
0
1a
2
?
(
3)b
0
1
a
2
?
(
?
3
)
?
b
0
1
(
3
?
2
a
4(
)
5
b
)
(a) (b) (c)
0,?
1,(
?
3,?
?
2,(a
4,(?
?
5,(
b
?
0
1a:c/1
2
?:?
(:(/1
3):)b:c/1
2
0
?:?
1
a:c/1
3
S:  /1?
b:c/1
TS
(d) (e) (f)
Figure 5
PDA Examples: (a) Non-regular PDA accepting {anbn|n ? N}. (b) Regular (but not
bounded-stack) PDA accepting a?b?. (c) Bounded-stack PDA accepting a?b? and (d) its
expansion as an FSA. (e) Weighted PDT T1 over the tropical semiring representing the
weighted transduction (anbn, c2n) 7? 3n and (f) equivalent RTN ({S},{a, b}, {c}, {TS},S).
695
Computational Linguistics Volume 40, Number 3
{?})?K?Q a finite set of transitions, and ? : F? K the final weight function. Let
e= (p[e], i[e], o[e],w[e], n[e]) denote a transition in E. Note that a PDA can be seen as
a particular case of a PDT where i[e] = o[e] for all its transitions. For simplicity, our
following presentation focuses on acceptors, rather than the more general case of trans-
ducers. This is adequate for the translation applications we describe, with the exception
of the treatment of alignment in Section 4.3, for which the intersection algorithm for
PDTs and FSTs is given in Appendix A.
3. PDT Operations
In this section we describe in detail the following PDA algorithms: Replacement, Com-
position, Shortest Path, and (Pruned) Expansion. Although these are needed to implement
HiPDT, these are general purpose algorithms, and suitable for many other applications
outside the focus of this article. The algorithms described in this section have been
implemented in the PDT extension (Allauzen and Riley 2011) of the OpenFst library
(Allauzen et al. 2007). In this section, in order to simplify the presentation we will only
consider machines over the tropical semiring (R+ ? {?}, min,+,?, 0). However, for
each operation, we will specify in which semirings it can be applied.
3.1 Recursive Transition Networks
We briefly give formal definitions for RTNs that will be needed to present the RTN
expansion operation. Examples are shown earlier in Figures 1(b) and 3(a). Informally,
an RTN is an automaton where some labels, nonterminals, are recursively replaced
by other automata. We give the formal definition for acceptors; the extension to RTN
transducers is straightforward.
An RTN R over the tropical semiring (R+ ? {?}, min,+,?, 0) is a 4-tuple
(N,?, (T?)??N, S) where N is the alphabet of nonterminals, ? is the input alpha-
bet, (T?)??N is a family of FSTs with input alphabet ? ?N, and S ? N is the root
nonterminal.
A sequence x ? ?? is accepted by (R,?) if there exists an accepting path ? in T? such
that ? = ?1e1 . . . ?nen?n+1 with i[?k] ? ??, i[ek] ? N and such that there exists sequences
xk such that xk is accepted by (R, i[ek]) and x = i[?1]x1 . . . i[?n]xni[?n+1]. We say that x is
accepted by R when it is accepted by (R, S). The weight associated by (R,?) (and by R)
to x can be defined in the same recursive manner.
As an example of testing whether an RTN accepts a sequence, consider the RTN R
of Figure 6 and the sequence x = a a b. The path in the automata TS can be written as
? = ?1 e1 ?2, with i[?1] = a, i[e1] = X1, and i[?2] = b. In addition, the machine (R, i[e1])
accepts x1 = a. Because x = i[?1] x1 i[?2], it follows that x is accepted by (R, S).
3.2 Replacement
This algorithm converts an RTN into a PDA. As explained in Section 1.1, this PDT
operation is applied by the HiPDT decoder in Step 1, and examples are given in earlier
sections (e.g., in figures 1 and 3).
Replacement acts on every transition of the RTN that is associated with a non-
terminal. The source and destination states of these transitions are used to define the
matched opening and closing parentheses, respectively, in the new PDA. Each RTN
nonterminal transition is deleted and replaced by two new transitions that lead to and
696
Allauzen et al. Pushdown Automata in Statistical Machine Translation
from the automaton indicated by the nonterminal. These new transitions have matched
parentheses, taken from the source and destination states of the RTN transition they
replace. Figure 6 gives a simple example.
Formally, given an RTN R, defined as (N,?, (T?)??N, S), its replacement is the PDA
T equivalent to R defined by the 8-tuple (?,?,?,Q,E, I, F, ?) with Q = ? =
?
??N Q?,
I = IS, F = FS, ? = ?S, and E =
?
??N
?
e?E? E
e where Ee = {e} if i[e] 6? N and
Ee={(p[e], n[e], ?,w[e], I?), (f, n[e], ?, ??(f ), n[e])|f ?F?} (3)
with ? = i[e] ? N otherwise.
The complexity of the construction is in O(|T|). If |F?| = 1 for all ? ? N, then
|T| = O(???N |T?|) = O(|R|). Creating a superfinal state for each T? would lead to a T
whose size is always linear in the size of R. In this article, we assume this optimization
is always performed. We note here that RTNs can be defined and the replacement
operation can be applied in any semiring.
3.3 Composition
Once we have created the PDA with translation scores, Step 2 in Section 1.1 applies the
language model scores to the translation space. This is done by composition with an
FSA containing the relevant language model weights.
The class of weighted pushdown transducers is closed under composition with
weighted finite-state transducers (Bar-Hillel, Perles, and Shamir 1964; Nederhof and
Satta 2003). OpenFST supports composition between automata T1 and T2, where T1
is a weighted pushdown transducer and T2 is a weighted finite-state transducer. If
both T1 and T2 are acceptors, rather than transducers, the composition of a PDA and
an FSA produces a PDA containing their intersection, and so no separate intersection
algorithm is required for these automata. Given this, we describe only the simpler,
special case of intersection between a PDA and an FSA, as this is sufficient for most
of the translation applications described in this article. The alignment experiments of
RTN R
1 2 3 4
a X1 b
TS
5 6
X2
a 7 8
b
TX1 TX2
R accepts a a b and a b b.
PDT T
1 2
a
5 6
3 4
7 8
a
b
b
6
3 3?
6?
T accepts a 3 a 3? b and a 3 6 b 6? 3? b.
Figure 6
Conversion of an RTN R to a PDA T by the replacement operation of Section 3.2. Using the
notation of Section 2.1, in this example ? = {3, 5} and ? = {3?, 5?}, with f (3) = 3? and f (5) = 5?.
The unweighted transition (2,X1, 3) in R is deleted and replaced by two new transitions (2, 3, 5)
and (6, 3?, 3); similarly, (5,X2, 6) is replaced by (5, 6, 7) and (8, 6?, 6). After application of the r?
mapping, the strings accepted by R and by T are the same.
697
Computational Linguistics Volume 40, Number 3
0 1ab 2
a
b 3
a
b 4
a
b
T2
0
1a
2
?
(
3)b T1
0,0
1,1a
2,0
?
0,1(
3,0)
1,2a
2,1
?
b
0,2(
3,1)
1,3a
2,2
?
b
0,3(
3,2)
1,4a
2,3
?
b
0,4(
3,3)
2,4
?
b
T
Figure 7
Composition example: Composition of a PDA T1 accepting {an, bn} with an FSA T2 accepting
{a, b}4 to produce a PDA T = T1 ? T2 . T has only one balanced path, and this path accepts
a(a(?)b)b. Composition is performed by the PDA-FSA intersection described in Section 3.3.
Section 4.3 do require composition of transducers; the algorithm for composition of
transducers is given in Appendix A.
An example of composition by intersection is given in Figure 7. The states of T are
created as the product of all the states in T1 and T2. Transitions are added as illustrated
in Figure 8. These correspond to all paths through T1 and T2 that can be taken by
a synchronized reading of strings from {a, b}?. The algorithm is very similar to the
composition algorithm for finite-state transducers, the difference being the handling
of the parentheses. The parenthesis-labeled transitions are treated similarly to epsilon
transitions, but the parenthesis labels are preserved in the result. This adds many
unbalanced paths to T. In this example, T has five paths but only one balanced path,
so that T accepts the string a a b b.
Formally, given a PDA T1 = (?,?,?,Q1,E1, I1, F1, ?1) and an FSA T2 =
(?,Q2,E2, I2, F2, ?2), intersection constructs a new PDA T = (?,?,?,Q,E, I, F, ?),
where T = T1 ? T2 as follows:
1. The new state space is in the product of the input state spaces: Q ? Q1 ?Q2.
2. The new initial and final states are I = (I1, I2), and F = {(q1, q2) : q1 ? F1, q2 ? F2}.
3. Weights are assigned to final states (q1, q2) ? Q as ?(q1, q2) = ?(q1)+ ?(q2).
4. For pairs of transitions (q1, a1,w1, q?1) ? E1 and (q2, a2,w2, q?2) ? E2, a transition
is added between states (q1, q2) and (q?1, q?2) as specified in Figure 8.
PDT T1 FSA T2 PDT T = T1 ? T2 Input Symbols
q1 q?1
a1/w1
q2 q?2
a2/w2
q1, q2 q?1, q?2
a1/w1 + w2
a1 ? ? and a1 = a2
q1, q2 q?1, q2
a1/w1
a1 ? ? ?? or a1 = ?
Transitions are added to T if and only if the conditions on the input symbols are satisfied.
Figure 8
PDA?FSA intersection under the tropical semiring. The PDA T is created by the intersection of
the PDA T1 and the FSA T2, i.e., T = T1 ? T2.
698
Allauzen et al. Pushdown Automata in Statistical Machine Translation
The intersection algorithm given here assumes that T2 has no input-? transitions.
When T2 has input-? transitions, an epsilon filter (Mohri 2009; Allauzen, Riley, and
Schalkwyk 2011) generalized to handle parentheses can be used. Note that Steps 1 and 2
do not require the construction of all possible pairs of states; only those states reachable
from the initial state and needed in Step 4 are actually generated. The complexity of
the algorithm is in O(|T1| |T2|) in the worst case, as will be discussed in Section 4.
Composition requires the semiring to be commutative.
3.4 Shortest Distance and Path Algorithms
With a PDA including both translation and language model weights, HiPDT can ex-
tract the best translation (Step 3a in Section 1.1). To this end, a general PDA shortest
distance/path algorithm is needed.
A shortest path in a PDA T is a balanced accepting path with minimal weight
and the shortest distance in T is the weight of such a path. We show that when
T has a bounded stack, shortest distance and shortest path can be computed in
O(|T|3 log |T|) time (assuming T has no negative weights) and O(|T|2) space. Figure 9
gives a pseudo-code description of the shortest-distance algorithm, which we now
discuss.
SHORTESTDISTANCE(T)
1 for each q ? Q and a ? ? do
2 B[q, a]? ?
3 for each q ? Q do
4 d[q, q]??
5 GETDISTANCE(T, I) ? I is the unique initial state
6 return d[I, f ] ? f is the unique final state
RELAX(s,q,w,S )
1 if d[s, q] > w then ? if w is a better estimate of the distance from s to q
2 d[s, q]? w ? update d[s, q]
3 if q 6? S then ? enqueue q in S if needed
4 ENQUEUE(S, q)
GETDISTANCE(T,s)
1 for each q ? Q do
2 d[s, q]??
3 d[s, s]? 0
4 Ss ? {s}
5 while Ss 6=? do
6 q? HEAD(Ss )
7 DEQUEUE(Ss )
8 for each e ? E[q] do ? E(q) is the set of transitions leaving state q
9 if i[e] ? ? ? {?} then ? i[e] is a regular symbol
10 RELAX(s,n[e], d[s, q]+w[e],Ss )
11 elseif i[e] ? ? then ? i[e] is a close parenthesis
12 B[s, i[e]]? B[s, i[e]] ? {e}
13 elseif i[e] ? ? then ? i[e] is an open parenthesis
14 if d[n[e], n[e]]=? then ? n[e] is the destination state of transition e
15 GETDISTANCE(T,n[e])
16 for each e? ? B[n[e], i[e]] do
17 w? d[s, q]+w[e]+ d[n[e], p[e?]]+ w[e?]
18 RELAX(s,n[e?],w,Ss )
Figure 9
PDT shortest distance algorithm.
699
Computational Linguistics Volume 40, Number 3
Given a PDA T = (?,?,?,Q,E, I, F, ?), the GETDISTANCE(T) algorithm computes
the shortest distance from the start state I to the final state2 f ? F. The algorithm
recursively calculates
d[q, q?] ? K ? the shortest distance from state q to state q? along a balanced path
At termination, the algorithm returns d[I, f ] as the cost of the shortest path through T.
The core of the shortest distance algorithm is the procedure GETDISTANCE(T, s)
which calculates the distances d[s, q] for all states q that can be reached from s. For an
FSA, this procedure is called once, as GETDISTANCE(T, I), to calculate d[I, q] ?q.
For a PDA, the situation is more complicated. Given a state s in T with at least
one incoming open parenthesis transition, we denote by Cs the set of states that can be
reached by a balanced path starting from s. If s has several incoming open parenthesis
transitions, a naive implementation might lead to the states in Cs to be visited exponen-
tially many times. This is avoided by memoizing the shortest distance from s to states in
Cs. To do this, GETDISTANCE(T, s) calculates d[s, s?] for all s? ? Cs, and it also constructs
sets of transitions
B[s, a] = {e ? E : p[e] ? Cs and i[e] = a} ?a ? ? (4)
These are the transitions with label a leaving states in Cs.
Consider any incoming transition to s, (q, a,w, s), with a ? ?. For every transition
e? = (s?, a,w?, q?), e? ? B[s, a] the following holds3
d[q, q?] = w+ d[s, s?]+ w? (5)
If d[s, s?] is available, the shortest distance from q to q? along any balanced path through
s can be computed trivially by Equation (5). For any state s with incoming open paren-
thesis transitions, only a single call to GETDISTANCE(T, s) is needed to precompute the
necessary values.
Figure 10 gives an example. When transition (2, (1, 0, 5) is processed,
GETDISTANCE(T, 5) is called. The distance d[5, 7] is computed, and following tran-
sitions are logged: B[5, (1]? {(7, )1, 0, 8)} and B[5, (2]? {(7, )2, 0, 9)}. Later, when the
transition (4, (2, 0, 5) is processed, its matching transition (7, )2, 0, 9) is extracted from
B[5, (2]. The distance d[4, 9] is then found by Equation (5) as d[5, 7]. This avoids redun-
dant re-calculation of distances along the shortest balanced path from state 4 to state 9.
We now briefly discuss the shortest distance pseudo-code given in Figure 9. The
description may be easier to follow after reading the worked example in Figure 10. Note
that the sets Cs are not computed explicitly by the algorithm.
The shortest distance calculation proceeds as follows. Self-distances, that is, d[q, q],
are set initially to ?; when GETDISTANCE(T, q) is called it sets d[q, q] = 0 to note that
q has been visited. GETDISTANCE(T, s) starts a new instance of the shortest-distance
algorithm from s using the queue Ss, initially containing s. While the queue is not empty,
a state is dequeued and its outgoing transitions examined (lines 7?11). Transitions
labeled by non-parenthesis are treated as in Mohri (2009) (lines 7?8). When a transition
e is labeled by a close parenthesis, e is added to B[s, i[e]] to indicate that this transition
2 For simplicity, we assume T has only one final state.
3 This assumes all paths from q to q? pass through s. The RELAX operation (Figure 9) handles the
general case.
700
Allauzen et al. Pushdown Automata in Statistical Machine Translation
0
1 2
5 6 7
8
10
3 4 9t1/20
t3/200
(2
t1/10
t2/100
(1 t2/1 t3/1
)1 t4/1, 000
)2
t6/1, 000
GETDISTANCE(T) runs
1. Initialization: d[q, q]??, ?q ? Q
2. GETDISTANCE(T, 0) is called
GETDISTANCE(T, 0) runs
3. Distances are calculated from state 0:
d[0, 0]? 0; d[0, 1]? d[0, 0]+ w[0, 1]; d[0, 2]? d[0, 1]+ w[1, 2]
4. Transition e1 = (2, (1, 0, 5) is reached. e1 has symbol i[e1] = (1 and destination state n[e1] = 5
5. d[5, 5] =? so GETDISTANCE(T, 5) is called
GETDISTANCE(T, 5) runs
6. Distances are calculated from state 5:
d[5, 5]? 0; d[5, 6]? d[5, 5]+ w[5, 6]; d[5, 7]? d[5, 6]+ w[6, 7]
7. The transitions (7, )1, 0, 8) and (7, )2, 0, 9) are reached and memoized
B[5, (1]? {(7, )1, 0, 8)}
B[5, (2]? {(7, )2, 0, 9)}
GETDISTANCE(T, 5) ends
GETDISTANCE(T, 0) resumes
8. Transition e1 = (2, (1, 0, 5) is still being processed, with p[e1] = 2, n[e1] = 5, and i[e1] = (1
9. Transition e2 = (7, )1, 0, 8) matching (1 is extracted from B[n[e1], i[e1]], with p[e2] = 7
and n[e2] = 8
10. Distance d[0, 8] is calculated as d[0, n[e2]] :
d[0, n[e2]]? d[0, p[e1]]+ w[p[e1],n[e1]]+ d[n[e1], p[e2]]+ w[p[e2],n[e2]]
10. Processing of e1 finishes, and calculation of distances from 0 continues:
d[0, 10]? d[0, 8]+ w[8, 10]
10 is a final state. Processing continues with transition (0, t1, 20, 3)
d[0, 3]? d[0, 0]+ w[0, 3]; d[0, 4]? d[0, 3]+ w[3, 4]
13. Transition e3 = (4, (2, 0, 5) is reached
e3 has symbol i[e3] = (2, source state p[e3] = 4, and destination state n[e3] = 5
14. GETDISTANCE(T, 5) is not called, since d[5, 5] = 0 indicates state 5 has been previously
visited
15. Transition e4 = (7, )2, 0, 9) matching (2 is extracted from B[n[e3], i[e3]], with p[e4] = 7 and
n[e4] = 9
16. Distance d[0, 9] is calculated as d[0, n[e4]], using cached values:
d[0, n[e4]]? d[0, p[e3]]+ w[p[e3],n[e3]]+ d[n[e3], p[e4]]+ w[p[e4],n[e4]]
17. d[0, 10] is less than? :
d[0, 10]? min(d[0, 10], d[0, 9]+ w[9, 10])
18. GETDISTANCE(T, 0) ends and returns d[0, 10]
GETDISTANCE(T) ends
Figure 10
Step-by-step description of the shortest distance calculation for the given PDA by the algorithm
of Figure 9. For simplicity, w[q, q?] indicates the weight of the transition connecting q and q?.
balances all incoming open parentheses into s labeled by i[e] (lines 9?10). Finally, if e has
an open parenthesis, and if its destination has not already been visited, a new instance of
GETDISTANCE is started from n[e] (lines 12?13). The destination states of all transitions
balancing e are then relaxed (lines 14?16).
The space complexity of the algorithm is quadratic for two reasons. First, the
number of non-infinity d[q, s] is |Q|2. Second, the space required for storing B is at
most in O(|E|2) because for each open parenthesis transition e, the size of |B[n[e], i[e]]|
701
Computational Linguistics Volume 40, Number 3
is O(|E|) in the worst case. This last observation also implies that the accumulated
number of transitions examined at line 16 is in O(Z|Q| |E|2) in the worst case, where
Z denotes the maximal number of times a state is inserted in the queue for a given
call of GETDISTANCE. Assuming the cost of a queue operation is ?(n) for a queue
containing n elements, the worst-case time complexity of the algorithm can then be
expressed as O(Z|T|3 ?(|T|)). When T contains no negative weights, using a shortest-
first queue discipline leads to a time complexity in O(|T|3 log |T|). When all the
Cs?s are acyclic, using a topological order queue discipline leads to a O(|T|3) time
complexity.
As was shown in Section 3.2, when T has been obtained by converting an RTN
or a hypergraph into a PDA, the polynomial dependency in |T| becomes a linear
dependency both for the time and space complexities. Indeed, for each q in T, there
exists a unique s such that d[s, q] is non-infinity. Moreover, for each open parenthesis
transition e, there exists a unique close parenthesis transition e? such that e??B[n[e], i[e]].
When each component of the RTN is acyclic, the complexity of the algorithm is O(|T|)
in time and space.
The algorithm can be modified (without changing the complexity) to compute the
shortest path by keeping track of parent pointers. The notion of shortest path requires
the semiring (K,?,?, 0, 1) to have the path property: for all a, b in K, a? b ? {a, b}. The
shortest-distance operation as presented here and the shortest-path operation can be
applied in any semiring having the path property by using the natural order defined by
?: a ? b iff a? b = a. However, the shortest distance algorithm given in Figure 9 can be
extended to work for k-closed semirings using the same techniques that were used by
Mohri (2002).
The shortest distance in the intersection of a string s and a PDA T determines if T
recognizes s. PDA recognition is closely related to CFG parsing; a CFG can be repre-
sented as a PDT whose input recognizes the CFG and whose output identifies the parse
(Aho and Ullman 1972). Lang (1974) showed that the cubic tabular method of Earley
can be naturally applied to PDAs; others give the weighted generalizations (Stolcke
1995; Nederhof and Satta 2006). Earley?s algorithm has its analogs in the algorithm in
Figure 9: the scan step corresponds to taking a non-parenthesis transition at line 10, the
predict step to taking an open parenthesis at lines 14?15, and the complete step to taking
the closed parentheses at lines 16?18.
Specialization to Translation. Following the formalism of Section 1, we are interested
in applying shortest distance and shortest path algorithms to automata created as
L = Tp ?M, where Tp, the translation representation, is a PDA derived from an RTN
(via replacement) and M, the language model, is a finite automaton.
For this particular case, the time complexity is O(|Tp||M|3) and the space complexity
is O(|Tp||M2|). The dependence on |Tp| is linear, rather than cubic or quadratic. The
reasoning is as follows. Given a state q in Tp, there exists a unique sq such that q belongs
to Csq. Given a state (q1, q2) in Tp?M, (q1, q2)?C(s1,s2 ) only if s1 = sq1 , and hence (q1, q2)
belongs to at most |M| components.
3.5 Expansion
As explained in Section 1.1, HiPDT can apply Step 3b to generate translation lattices.
This step is typically required for any posterior lattice rescoring strategies. We first
702
Allauzen et al. Pushdown Automata in Statistical Machine Translation
describe the unpruned expansion. However, in practice a pruning strategy of some sort
is required to avoid state explosion. Therefore, we also describe an implementation of
the PDA expansion that includes admissible pruning under a likelihood beam, thus
controlling on-the-fly the size of the output lattice.
3.5.1 Full Expansion. Given a bounded-stack PDA T, the expansion of T is the FSA T?
equivalent to T. A simple example is given in Figure 11.
Expansion starts from the PDA initial state. States and transitions are added to
the FSA as the expansion proceeds along paths through the PDA. In the new FSA,
parentheses are replaced by epsilons, and as open parentheses are encountered on
PDA transitions, they are ?pushed? into the FSA state labels; in this way the stack
depth is maintained along different paths through the PDA. Conversely, when a closing
parenthesis is encountered on a PDA path, a corresponding opening parenthesis is
?popped? from the FSA state label; if this is not possible, for example, as in state (5, ?)
in Figure 11, expansion along that path halts.
The resulting automata accept the same language. The FSA topology changes,
typically with more states and transitions than the original PDA, and the number of
added states is controlled only by the maximum stack depth of the PDA.
Formally, suppose the PDA T = (?,?,?,Q,E, I, F, ?) has a maximum stack depth
of K. The set of states in its FSA expansion T? are then
Q? = {(q, z) : q ? Q , z ? ?? and |z| ? K} (6)
and T? has initial state (I, ?) and final states F? = {(q, ?) : q ? F}. The condition that T
has a bounded stack ensures that Q? is finite. Transitions are added to T? as described in
Figure 12.
The full expansion operation can be applied to PDA over any semiring. The com-
plexity of the algorithm is linear in the size of T?. However, the size of T? can be
exponential in the size of T, which motivates the development of pruned expansion,
as discussed next.
0
1
2 3
4 5 6[ [
[
a
]
b ]
c
0,?
1, [ 2, [[ 3, [[ 4, [ 5, [ 6,??
? a ? b ?
2, [
3, [
?
a
c
4,? 5,?
?
b
Figure 11
Full expansion of a PDA to an equivalent FSA. The PDA maximum stack depth is 2; therefore
the FSA states belong to {0, .., 6} ? {?, [, [[}. Expansion can create incomplete paths in the FSA
(e.g., corresponding here to the unbalanced PDA path [ a ] b ]); however these are guaranteed to
be unconnected, namely, not to lead to a final state. Any unconnected states are removed after
expansion.
703
Computational Linguistics Volume 40, Number 3
Transition in PDA T New transition in FSA T? Conditions Explanation
q, z q?, z
a/w
a ? ? ? {?} a is not a parenthesis; stackdepth is unchanged
q q?
a/w
q, z q?, za
?
a ? ?
a is an open parenthesis; an
epsilon transition is added,
and a is ?pushed? into the
destination state, increas-
ing the stack depth
q, z?a q?, z?
?
a ? ?
a is a closing parenthe-
sis; an epsilon transition
is added, and the match-
ing open parenthesis a is
?popped? from the destina-
tion state, decreasing the
stack depth
Figure 12
PDA Expansion. A states (q, z) and (q?, z? ) in the FSA T? will be connected by a transition if and
only if the above conditions hold on the corresponding transition between q and q? in the PDA T.
3.5.2 Pruned Expansion. Given a bounded-stack PDA T, the pruned expansion of T with
threshold ? is an FST T?? obtained by deleting from T? all states and transitions that do
not belong to any accepting path ? in T? such that w[?]? ?[?] ? d+ ?, where d is the
shortest distance in T.
A naive implementation consisting of fully expanding T and then applying the
FST pruning algorithm would lead to a complexity in O(|T?| log |T?|)=O(e|T||T|).
Assuming that the reverse TR of T is also bounded-stack, an algorithm whose com-
plexity is in O(|T| |T??|+ |T|3 log |T|) can be obtained by first applying the shortest
distance algorithm from the previous section to TR and then using this to prune the
expansion as it is generated. To simplify the presentation, we assume that F={ f} and
?( f )=0.
The motivation for using reversed automaton in pruning is easily seen by looking
at FSAs. For an FSA, the cost of the shortest path through a transition (q, x,w, q?) can
be stated as d[I, q]+ w+ d[q?, f ]. Distances d[I, q] (i.e., distances from the start state) are
computed by the shortest distance algorithm, as discussed in Section 3.4. However,
distances of the form d[q?, f ] are not readily available. To compute these, a shortest
distance algorithm is run over the reversed automaton. Reversal preserves states and
transitions, but swaps the source and destination state (see Figure 13 for a PDA ex-
ample). The start state in the reversed machine is f , so that distances are computed
from f ; these are denoted dR[f, q] and correspond to d[q, f ] in the original FSA. The
cost of the shortest path through an FSA transition (q, x,w, q?) can then be computed as
d[I, q]+ w+ dR[f, q?].
Calculation for PDAs is more complex. Transitions with parentheses must be han-
dled such that distances through them are calculated over balanced paths. For example,
if T in Figure 13 was an FSA, the shortest cost of any path through the transition
e = (4, (2, 0, 5) could be calculated as d[0, 4]+ 0+ d[5, 10]. However, this is not correct,
because d[5, 10], the shortest distance from 5 to 10, is found via a path through the
transition (7, )1, 0, 8).
Correct calculation of the minimum cost of balanced paths through PDA transitions
can be done using quantities computed by the PDA shortest distance algorithm. For a
704
Allauzen et al. Pushdown Automata in Statistical Machine Translation
0
1 2
5 6 7
8
10
3 4 9t1/20
t3/200
(2
t1/10
t2/100
(1 t2/1 t3/1
)1 t4/1, 000
)2
t6/1, 000
T
0
1 2
5 6 7
8
10
3 4 9t1/20
t3/200
(2
t1/10
t2/100
(1 t2/1 t3/1
)1 t4/1, 000
)2
t6/1, 000
TR
Figure 13
PDA T and its reverse TR. TR has start state 10, final state 0, ?R = {)1, )2}, and ?
R = {(1, (2}.
PDA transition e = (q, a,w, q?), a ? ?, the cost of the shortest balanced path through e
can be found as4
c(e) = d[I, q]+ w[e]+ min
e??B[q?,a]
d[q?, p[e?]]+ w[e?]+ dR[n[e?], f ] (7)
where B[q?, a] and d[p[e?], q?] are computed by the PDA shortest distance algorithm over
T, and dR[n[e?], f ] is computed by the PDA shortest distance algorithm over TR.
In Figure 13, the shortest cost of paths through the transition e = (4, (2, 0, 5) is found
as follows: the shortest distance algorithm over T calculates d[0, 4] = 220 , d[5, 7] = 2,
and B[5, (2] = {7, )2, 0, 9}; the shortest distance algorithm over TR calculates dR[10, 9] =
1, 000 (trivially, here); the cost of the shortest path through e is
d[0, 4]+ w[e]+ d[5, 7]+ w[e?]+ dR[10, 9] = 220+ 0+ 2+ 0+ 1, 000
Pruned expansion is therefore able to avoid expanding transitions that would not
contribute to any path that would survive pruning. Prior to expansion of a PDA T to an
FSA T?, the shortest distance d in T is calculated. Transitions e = (q, a,w, q?), a ? ?, are
expanded as transitions e = ((q, z), q,w, (q?, za)) in T? only if c(e) ? d+ ?, as calculated
by Equation (7).
The pruned expansion algorithm implemented in OpenFST is necessarily more
complicated than the simple description given here. Pseudo-code describing the Open-
FST implementation is given in Appendix B.
The pruned expansion operation can be applied in any semiring having the path
property.
4 Note that d[p[e?], q?] could be replaced by dR[q?, p[e?]].
705
Computational Linguistics Volume 40, Number 3
4. HiPDT Analysis and Experiments: Computational Complexity
We now address the following questions:
r What are the differences between the FSA and PDA representations as
observed in a translation/alignment task?
r How do their respective decoding algorithms perform in relation to the
complexity analysis described here?
r How many times is exact decoding achievable in each case?
We will discuss the complexity of both HiPDT and HiFST decoders as well as the
hypergraph representation, with an emphasis on Hiero-style SCFGs. We assess our
analysis for FSA and PDA representations by contrasting HiFST and HiPDT with large
grammars for translation and alignment. For convenience, we refer to the hypergraph
representation as Th, and to the FSA and PDA representations as Tf and Tp.
We first analyze the complexity of each MT step described in the introduction:
1. SCFG Translation: Assuming that the parsing of the input is performed by a
CYK parse, then the CFG, hypergraph, RTN, and PDA representations can
be generated in O(|s|3|G|) time and space (Aho and Ullman 1972). The FSA
representation can require an additional O(e|s|3|G|) time and space because
the RTN expansion to FSA can be exponential.
2. Intersection: The intersection of a CFG Th with a finite automaton M can be
performed by the classical Bar-Hillel algorithm (Bar-Hillel, Perles, and
Shamir 1964) with time and space complexity O(|Th||M|l+1), where l is the
maximum number of symbols on the right-hand side of a grammar rule in
Th. Dyer (2010a) presents a more practical intersection algorithm that avoids
creating rules that are inaccessible from the start symbol. With deterministic
M, the intersection complexity becomes O(|Th||M|lN+1), where lN is the
rank of the SCFG (i.e., lN is the maximum number of nonterminals on the
right-hand side of a grammar rule). With Hiero-styles rules, lN = 2 so the
complexity is O(|Th||M|3) in that case.5 The PDA intersection algorithm
from Section 3.3 has time and space complexity O(|Tp||M|). Finally, the FSA
intersection algorithm has time and space complexity O(|Tf ||M|) (Mohri 2009).
3. Shortest Path: The shortest path algorithm on the hypergraph, RTN, and
FSA representations requires linear time and space (given the underlying
acyclicity) (Huang 2008; Mohri 2009). As presented in Section 3.4, the PDA
representation can require time cubic and space quadratic in |M|.
Table 1 summarizes the complexity results for SCFGs of rank 2. The PDA represen-
tation is equivalent in time and superior in space complexity to the CFG/hypergraph
representation, in general, and it can be superior in both space and time to the FSA
representation depending on the relative SCFG and language model (LM) sizes. The
FSA representation favors smaller target translation grammars and larger language
models.
5 The modified Bar-Hillel construction described by Chiang (2007) has time and space complexity
O(|Th||M|4 ); the modifications were introduced presumably to benefit the subsequent pruning method
employed (but see Huang, Zhong, & Gildea 2005).
706
Allauzen et al. Pushdown Automata in Statistical Machine Translation
Table 1
Translation complexity of target language representations for translation grammars of rank 2.
Representation Time Complexity Space Complexity
CFG/hypergraph O(|s|3 |G| |M|3) O(|s|3 |G| |M|3 )
PDA O(|s|3 |G| |M|3) O(|s|3 |G| |M|2 )
FSA O(e|s|3|G| |M|) O(e|s|3|G| |M|)
In practice, the PDA and FSA representations benefit greatly from the optimiza-
tions mentioned previously (Figure 3 and accompanying discussion). For the FSA
representation, these operations can offset the exponential dependencies in the worst-
case complexity analysis. For example, in a translation of a 15-word sentence taken
at random from the development sets described later, expansion of an RTN yields a
WFSA with 174? 106 states. By contrast, if the RTN is determinized and minimized
prior to expansion, the resulting WFSA has only 34? 103 states. Size reductions of this
magnitude are typical. In general, the original RTN, hypergraph, or CFG representation
can be exponentially larger than the RTN/PDT optimized as described.
Although our interest is primarily in Hiero-style translation grammars, which have
rank 2 and a relatively small number of nonterminals, this complexity analysis can be
extended to other grammars. For SCFGs of arbitrary rank lN, translation complexity in
time for hypergraphs becomes O(|G||s|lN+1|M|lN+1); with FSAs the time complexity be-
comes O(e|G||s|lN+1 |M|); and with PDAs the time complexity becomes O(|G||s|lN+1|M|3).
For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann
and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA represen-
tations may offer computational advantages in the worst case relative to hypergraph
representations, although this must be balanced against other available strategies such
as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and
Langmead 2010). Of course, practical translation systems introduce various pruning
procedures to achieve much better decoding efficiency than the worst cases given here.
We will next describe the translation grammar and language model for our ex-
periments, which will be used throughout the remainder of this article (except when
stated otherwise). In the following sections we assess the complexity discussion with a
contrast between HiFST (FSA representation) and HiPDT (PDA representation) under
large grammars.
4.1 Translation Grammars and Language Models
Translation grammars are extracted from a subset of the GALE 2008 evaluation par-
allel text;6 this is 2.1M sentences and approximately 45M words per language. We
report translation results on a development set tune-nw (1,755 sentences) and a test set
test-nw (1,671 sentences). These contain translations produced by the GALE program
and portions of the newswire sections of the NIST evaluation setsMT02 throughMT06.7
6 See http://projects.ldc.upenn.edu/gale/data/catalog.html.We excluded the UN material and the
LDC2002E18, LDC2004T08, LDC2007E08, and CUDonga collections.
7 See http://www.itl.nist.gov/iad/mig/tests/mt/.
707
Computational Linguistics Volume 40, Number 3
Table 2
Number of n-grams with explicit conditional probability estimates assigned by the 4-gram
language models M?1 after entropy pruning of M1 at threshold values ?. Perplexities over the
(concatenated) tune-nw reference translations are also reported. The Kneser-Ney and Katz
4-gram LM have 416,190 unigrams, which are not removed by pruning.
? 0 7.5? 10?9 7.5? 10?8 7.5? 10?7 7.5? 10?6 7.5? 10?5 7.5? 10?4 7.5? 10?3
KN
2-grams 28M 10M 2.5M 442K 37K 1.3K 21 0
3-grams 61M 6M 969K 74K 2.7K 38 0 0
4-grams 117M 3M 219K 5K 44 0 0 0
perplexity 98.1 122.2 171.5 290.4 605.1 1270.2 1883.6 2200.0
KATZ
2-grams 28M 7M 2M 391K 52K 4K 117 1
3-grams 64M 10M 1.5M 148K 8.4K 197 1 0
4-grams 117M 4.6M 398K 19K 510 1 0 0
perplexity 106.7 120.4 146.9 210.5 336.6 596.5 905.0 1046.1
In tuning the systems, MERT (Och 2003) iterative parameter estimation under IBM
BLEU8 is performed on the development set.
The parallel corpus is aligned using MTTK (Deng and Byrne 2008) in both source-
to-target and target-to-source directions. We then follow published procedures (Chiang
2007; Iglesias et al. 2009b) to extract hierarchical phrases from the union of the
directional word alignments. We call a translation grammar (G) the set of rules
extracted from this process. For reference, the number of rules in G that can apply to the
tune-nw is 1.1M, of which 593K are standard non-hierarchical phrases and 511K are
strictly hierarchical rules.
We will use two English language models in these translation experiments. The
first language model, denoted M1, is a 4-gram estimated over 1.3B words taken
from the target side of the parallel text and the AFP and Xinhua portions of the
English Gigaword Fourth Edition (LDC2009T13). We use both Kneser-Ney (Kneser
and Ney 1995) and Katz (Katz 1987) smoothing in estimating M1. Where language
model reduction is required, we apply Stolcke entropy pruning (Stolcke 1998) to M1
under the relative perplexity threshold ?. The resulting language model is labeled
as M?1 .
The reduction in size in terms of component n-grams is summarized in Table 2.
For aggressive enough pruning, the original 4-gram model can be effectively reduced
to a trigram, bigram, or unigram model. For both the Katz and the Kneser-Ney 4-gram
language models: at ? = 7.5E? 05 the number of 4-grams in the LM is effectively
reduced to zero; at ? = 7.5E? 4 the number of 3-grams is effectively 0; and at
? = 7.5E? 3, only unigrams remain. Development set perplexities increase as entropy
pruning becomes more aggressive, with the Katz smoothed model performing better
under pruning (Chelba et al. 2010; Roark, Allauzen, and Riley 2013).
We will also use a larger language model, denoted M2, obtained by interpolat-
ing M1 with a zero-cutoff stupid-backoff 5-gram model (Brants et al. 2007) estimated
over 6.6B words of English newswire text; M2 is estimated as needed for the n-grams
required for the test sets.
8 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl.
708
Allauzen et al. Pushdown Automata in Statistical Machine Translation
Table 3
Success in finding the 1-best translation under G with various M?1 under a memory size limit of
10GB as measured over tune-nw (1,755 sentences). We note which operations in translation
exceeded the memory limit: either Expansion and Intersection for HiFST, or Intersection and
Shortest Path operation for HiPDT.
Decoding with G + M?1 under a 10GB memory size limit
# ? HiFST HiPDT
Success Failure Success Failure
Expansion Intersection Intersection Shortest Path
2 7.5? 10?9 12% 51% 37% 40% 8% 52%
3 7.5? 10?8 16% 53% 31% 76% 1% 23%
4 7.5? 10?7 18% 53% 29% 99.8% 0% 0.2%
4.2 Exact Decoding with Large Grammars and Small LanguageModels
We now compare HiFST and HiPDT in translation with our large grammar G. In this
case we know that exact search is often not feasible for HiFST.
We run both decoders over tune-nw with a restriction on memory use of 10 GB.
If this limit is reached in decoding, the process is killed.9 Table 3 shows the number
of times each decoder succeeds in finding a hypothesis under the memory limit when
decoding with various entropy-pruned LMs M?1 . With ?=7.5? 10?9 (row 2), HiFST
can only decode 218 sentences, and HiPDT succeeds in 703 cases. The difference in
success rates between the decoders is more pronounced as the language model is more
aggressively pruned: for ?=7.5? 10?7 HiPDT succeeds for all but three sentences.
As Table 3 shows, HiFST fails most frequently in its initial expansion from RTN
to FSA; this operation depends only on the translation grammar and does not benefit
from any reduction in the language model size. Subsequent intersection of the FSA
with the language model can still pose a challenge, although as the language model
is reduced, this intersection fails less often. By contrast, HiPDT intersects the translation
grammar with the language model prior to expansion and this operation nearly always
finishes successfully. The subsequent shortest path (or pruned expansion) operation is
prone to failure, but the risk of this can be greatly reduced by using smaller language
models.
In the next section we contrast both HiPDT and HiFST for alignment.
4.3 Alignment with Inversion Transduction Grammars
We continue to explore applications characterized by large translation grammars G
and small language models M. As an extreme instance of a problem involving a large
translation grammar and a simple target language model, we consider parallel text
alignment under an Inversion Transduction Grammar (ITG) (Wu 1997). This task, or
something like it, is often done in translation grammar induction. The process should
yield the set of derivations, with scores, that generate the target sentence as a translation
9 We use the UNIX ulimit command. The experiment was carried out over machines with different
configurations and loads, so these numbers should be considered as approximate values.
709
Computational Linguistics Volume 40, Number 3
of the source sentence. In alignment the target language model is extremely simple:
It is simply an acceptor for the target language sentence so that |M| is linear in the
length of the target sentence. In contrast, the search space needs now to be represented
with pushdown transducers (instead of pushdown automata) keeping track of both
translations and derivations, that is, indices of the rules in the grammar (Iglesias et al.
2009a; de Gispert et al. 2010; Dyer 2010b).
We define a word-based translation grammar GITG for the alignment problem as
follows. First, we obtain word-to-word translation rules of the form X??s, t? based
on probabilities from IBM Model 1 translation tables estimated over the parallel text,
where s and t are one source and one target word, respectively (?16M rules). Then,
we allow monotonic and inversion transduction of two adjacent nonterminals in the
usual ITG style (i.e., add X??X1 X2, X1 X2? and X??X1 X2, X2 X1?). Additionally,
we allow unrestricted source word deletions (X??s, ??), and restricted target word
insertions (X??X1 X2, X1 t X2?). This restriction, which is solely motivated by ef-
ficiency reasons, disallows the insertion of two consecutive target words. We make
no claims about the suitability or appropriateness of this specific grammar for either
alignment or translation; we introduce this grammar only to define a challenging
alignment task.
A set of 2,500 sentence pairs of up to 50 source and 75 target words was chosen
for alignment. These sentences come from the same Chinese-to-English parallel data
described in Section 4.1. Hard limits on memory usage (10GB) and processing time
(10 minutes) were imposed for processing each sentence pair. If HiPDT or HiFST ex-
ceeded either limit in aligning any sentence pair, alignment was stopped and a ?mem-
ory/time failure? was noted. Even if the resource limits are not exceeded, alignment
may fail due to limitations in the grammar. This happens when either a particular word
pair rule that is not in our Model 1 table, or more than one consecutive target insertions
are needed to reach alignment. In such cases, we record a ?grammar failure,? as opposed
to a ?memory/time failure.?
Results are reported in Table 4. Of the 2,500 sentence pairs, HiFST successfully
aligns only 41% of the sentence pairs under these time and memory constraints. The
reason for this low success rate is that HiFST must generate and expand all possible
derivations under the ITG for a given sentence pair. Even if it is strictly enforced
that the FSA in every CYK cell contains only partial derivations which produce sub-
strings of the target sentence, expansion often exceeds the memory/time constraints.
In contrast, HiPDT succeeds in aligning all sentence pairs that can be aligned under
the grammar (89%), because it never fails due to memory or time constraints. In this
experiment, if alignment is at all possible, HiPDT will find the best derivation. Align-
ment success rate (or coverage) could trivially be improved by modifying the ITG to
allow more consecutive target insertions, or by increasing the number of word-to-word
Table 4
Percentages of success and failure in aligning 2,500 sentence pairs under GITG with HiFST and
HiPDT. HiPDT finds an alignment whenever it is possible under the translation grammar.
HiFST HiPDT
Success Failure Success Failure
memory/time grammar memory/time grammar
41% 53% 6% 89% 0% 11%
710
Allauzen et al. Pushdown Automata in Statistical Machine Translation
rules, but that would not change the conclusion in the contrast between HiFST and
HiPDT.
The computational analysis from the beginning of this section applies to alignment.
The language model M is replaced by an acceptor for the target sentence, and if we
assume that the target sentence length is proportional to the source sentence length, it
follows that |M| ? |s| and the worst-case complexity for HiPDT in alignment mode is
O(|s|6|G|). This is comparable to ITG alignment (Wu 1997) and the intersection algo-
rithm of Dyer (2010b).
Our experimental results support the complexity analysis summarized in Table 1.
HiPDT is more efficient in ITG alignment and this is consistent with its linear depen-
dence on the grammar size, whereas HiFST suffers from its exponential dependence.
This use of PDAs in alignment does not rely on properties specific either to Hiero
or to ITGs. We expect that the approach should be applicable with other types of
SCFGs, although we note that alignment under SCFGs with an arbitrary number of
nonterminals can be NP-hard (Satta and Peserico 2005).
5. HiPDT Two-Pass Translation Architecture and Experiments
The previous complexity analysis suggests that PDAs should excel when used with
large translation grammars and relatively small n-gram language models. In hierar-
chical phrase-based translation, this is a somewhat unusual scenario: It is far more
typical that translation tasks requiring a large translation grammar also require large
language models. To accommodate these requirements we have developed a two-
pass decoding strategy in which a weak version of a large language model is ap-
plied prior to the expansion of the PDA, after which the full language model is
applied to the resulting WFSA in a rescoring pass. An effective way of generating
weak language models is by means of entropy pruning under a threshold ?; these are
the language models M?1 of Section 4.1. Such a two-pass strategy is widely used in
automatic speech recognition (Ljolje, Pereira, and Riley 1999). The steps in two-pass
translation using entropy-pruned language models are given here, and depicted in
Figure 14.
Step 1. We translate with M?1 and G using the same parameters obtained by MERT
for the baseline system, with the exception that the word penalty parameter
is adjusted to produce hypotheses of roughly the correct length. This produces
translation lattices that contain hypotheses with exact scores under G and M?1 :
?2({s} ? G) ?M?1 .
Step 2. These translation lattices are pruned at beamwidth ?: [?2({s} ? G) ?M?1 ]?.
Step 3. We remove the M?1 scores from the pruned translation lattices, reapply the full
language model M1, and restore the word penalty parameter to the baseline
value obtained by MERT. This gives an approximation to ?2({s} ? G) ?M1:
scores are correctly assigned underG andM1, but only hypotheses that survived
pruning at Step 2 are included.
We can rescore the lattices produced by the baseline system or by the two-pass
system with the larger language model M2. If ?=? or if ?=0, the translation lattices
obtained in Step 3 should be identical to lattices produced by the baseline system (i.e.,
the rescoring step is no longer needed). The aim is to increase ? to shrink the language
model used at Step 1, but ? will then have to increase accordingly to avoid pruning
away desirable hypotheses in Step 2.
711
Computational Linguistics Volume 40, Number 3
CYK parse 
s with G Build RTN
RTN to PDA 
Replacement
Intersect PDA 
with WFSA M1
?
PDA to FSA 
Pruned Expansion,
threshold B
Intersect FSA with LM M1
FSA 
Shortest 
Path
FSA 
Pruning
Lattice1-Best Hypothesis
Remove 
LM scores 
Entropy Pruning, 
threshold ?
LM M1
(as WFSA) 
further rescoring
Figure 14
Two-pass HiPDT translation with an entropy pruned language model.
5.1 Efficient Removal of First-Pass Language Model Scores Using
Lexicographic Semirings
The two-pass translation procedure requires removal of the weak language model
scores used in the initial expansion of the translation search space; this is done so
that only the translation scores under G remain after pruning. In the tropical semiring,
the weak LM scores can be ?subtracted? at the path level from the lattice, but this
involves a determinization of an unweighted translation lattice, which can be very
inefficient.
As an alternative we can define a lexicographic semiring (Shafran et al. 2011;
Roark, Sproat, and Shafran 2011) ?w1,w2? over the tropical weights w1 and w2 with the
operations ? and ?:
?w1,w2? ? ?w3,w4? =
{
?w1,w2? if w1 < w3 or (w1 = w3 and w2 < w4)
?w3,w4? otherwise (8)
?w1,w2? ? ?w3,w4? = ?w1 + w3,w2 + w4? (9)
The PDA algorithms described in Section 3 are valid under this new semiring because
it is commutative and has the path property. In particular, the PDA representing {s} ? G
is constructed so that the translation grammar score appears in both w1 and w2 (i.e., it is
duplicated). In the first-pass language model, w1 has the n-gram language model scores
and the w2 are 0. After composition, the resulting automata have the combined trans-
lation grammar score and language model score in the first dimension, and the second
dimension contains the translation grammar scores alone. Pruning can be performed
under the lexicographic semiring with a threshold set so that only the combined scores
in the first dimension are considered. The resulting automata can easily be mapped back
into the regular tropical semiring such that only the translation scores in the second
712
Allauzen et al. Pushdown Automata in Statistical Machine Translation
dimension are retained (this is a linear operation done by the fstmap operation in the
OpenFST library).
5.2 Translation Quality and Modeling Errors in Two-Pass Decoding
We wish to analyze the degree to which the two-pass decoding strategy introduces
?modeling errors? into translation. A modeling error occurs in two-pass decoding
whenever the decoder produces a translation whose score is less than the best attainable
under the grammar and language model (i.e., whenever the best possible translation
is discarded by pruning at Step 2). We refer to these as modeling errors, rather than
search errors, because they are due to differences in scores assigned by the models
M1 and M?1 .
Ideally, we would compare the two-pass translation system against a baseline sys-
tem that performs exact translation, without pruning in search, under the grammar G
and language model M1. This would allow us to address the following questions:
r Is a two-pass decoding procedure that uses entropy-pruned language
models adequate for translation? How many modeling errors are
introduced? Does two-pass decoding impact on translation quality?
r Which smoothing/discounting technique is best suited for the first-pass
language model in two-pass translation, and which smoothing/
discounting technique is best at avoiding modeling errors?
Our grammar G is not suitable for these experiments, as we do not have a system
capable of exact decoding under both G and M1. To create a suitable baseline we there-
fore reduce G by excluding rules that have a forward translation probability p < 0.01,
and refer to this reduced grammar as Gsmall. This process reduces the number of strictly
hierarchical rules that apply to our tune-nw set from 511K to 189K, while the number of
standard phrases is unchanged.
Under Gsmall, both HiFST and HiPDT are able to exactly compose the entire space of
possible candidate hypotheses with the language model and to extract the shortest path
hypothesis. Because an exact decoding baseline is thus available, we can empirically
evaluate the proposed two-pass strategy. Any degradation in translation quality can
only be due to the modeling errors introduced by pruning under ? with respect to the
entropy-pruned M?1 .
Figure 15 shows translation performance under grammar Gsmall for different values
of entropy pruning threshold ?. Performance is reported after first-pass decoding with
M?1 (Step 1, Section 5), and after rescoring with M1 (Step 3, Section 5) the first-pass
lattices pruned at alternative ? beams. The first column reports the baseline for either
Kneser-Ney andKatz languagemodels, which are found by translation without entropy
pruning, that is, performed with M1. Both yield 34.5 on test-nw.
The first and main conclusion from this figure is that the two-pass strategy is ade-
quate because we are always able to recover the baseline performance. As expected, the
harsher the entropy-pruning ofM1 (as we lower ?) the greater?must be to recover from
the significant degradation in first-pass decoding. But even at a harsh ? = 7.5? 10?7,
when first-pass performance drops over 7 BLEU points, a relatively-low value of ? = 15
can recover the baseline performance.
Although this is true independently of the LM smoothing approach, a second
conclusion from the figure is that the choice of LM smoothing does impact first-pass
713
Computational Linguistics Volume 40, Number 3
Figure 15
Results (lower case IBM BLEU scores over test-nw) under Gsmall with various M?1 as obtained
with several values of ?. Performance in subsequent rescoring with M1 after likelihood-based
pruning of the resulting translation lattices for various ? is also reported. In the pipeline, M1
(and M?1 ) are estimated with either Katz or Kneser-Ney smoothing.
translation performance. For entropy pruning at ? = 7.5? 10?7, the Katz LMs perform
better for smaller beamwidths ?. These results are consistent with the test set
perplexities of the entropy pruned LMs (Table 2), and are also in line with other studies
of Kneser-Ney smoothing and entropy pruning (Chelba et al. 2010; Roark, Allauzen,
and Riley 2013).
Modeling errors are reported in Table 5 at the entropy pruning threshold ? = 7.5?
10?7. As expected, modeling errors decrease as the beamwidth ? increases, although
we find that the language model with Katz smoothing has fewer modeling errors.
However, modeling errors do not necessarily impact corpus level BLEU scores. For wide
beamwidths (e.g., ? = 15 here), there are still some modeling errors, but these are either
few enough or subtle enough that two-pass decoding under either smoothing method
yields the same corpus level BLEU score as the exact decoding baseline.
Table 5
Two-pass translation modeling errors as a function of RTN expansion pruning threshold ?. A
modeling error occurs whenever the score of a hypothesis produced by the two-pass translation
differs from the score found by the exact baseline system. Errors are tabulated over systems
reported in Figure 15, at ? = 7.5? 10?7.
? Kneser-Ney Katz
8 814 619
12 343 212
15 240 110
714
Allauzen et al. Pushdown Automata in Statistical Machine Translation
5.3 HIPDT Two-Pass Decoding Speed and Translation Performance
r What are the speed and quality tradeoffs for HiPDT as a function of
first-pass LM size and translation grammar complexity?
r How do these compare against the predicted computational complexity?
In this section we turn back to the original large grammar, for which HiFST cannot
perform exact decoding (see Table 3). In contrast, HiPDT is able to do exact decoding
so we study tradeoffs in speed and translation performance. The speed of two-pass
decoding can be increased by decreasing ? and/or increasing ?, but at the risk of
degradation in translation performance. For grammar G and language model M1 we
plot in Figure 16 the BLEU score against speed as a function of ? for a selection of ?
values. BLEU score is measured over the entire test set test-nw but speed is calculated
only on sentences of length up to 20 words (?500 sentences). In computing speed we
measure not only the PDA operations, but the entire HiPDT decoding process described
in Figure 14, including CYK parsing and the application of M1. We note in passing that
these unusually slow decoding speeds are a consequence of the large grammars, lan-
guage models, and broad pruning thresholds chosen for these experiments; in practice,
translation with either HiPDT or HiFST is much faster.
In these experiments we find that the language model entropy pruning threshold
? and the likelihood beamwidth ? work together to balance speed against translation
quality. For every entropy pruning threshold ? value considered, there is a value of ?
for which there is no degradation in translation quality. For example, suppose we want
to attain a translation quality of 34.5 BLEU: then ? should be set to 12 or greater. If the
goal is to find the fastest system at this level, then we choose ? = 7.5? 10?5.
The interaction between pruning in expansion and pruning of the language model
is explained by Figure 17, where decoding and rescoring times are shown for various
Figure 16
HiPDT translation quality versus speed (decoding with G, M?1 + rescoring with M1) under
different entropy pruning thresholds ? and for likelihood beamwidths ? = 15, 12, 9, 8, 7.
715
Computational Linguistics Volume 40, Number 3
Figure 17
Accumulated decoding+rescoring times for HiPDT under different entropy pruning thresholds,
reaching a performance of at least 34.5 BLEU, for which ? is set to 12.
values of ? and ? that achieve at least the translation quality target of 34.5. As ?
increases, decoding time decreases because a smaller language model is easier to apply;
however, rescoring times increase, because the larger values of ? lead to larger WFSAs
after expansion, and these are costly to rescore. The balance occurs at ? = 7.5? 10?5
and a translation rate of 3.0 words/sec. In this case, entropy pruning yields a severely
shrunken bigram language model, but this may vary depending on the translation
grammar and the original, unpruned LM.
5.4 Rescoring with 5-Gram Language Models and LMBR Decoding
r Does the HiPDT two-pass decoding generate lattices that can be useful in
rescoring?
We now report on rescoring experiments using WFSAs produced by the two-pass
HiPDT translation system under the large translation grammar G. We demonstrate that
HiPDT can be used to generate large, compact representations of the translation space
that are suitable for rescoring with large language models or by alternative decoding
procedures. We investigate translation performance by applying versions of the lan-
guage model M2 estimated with stupid backoff. We also investigate minimum Bayes
risk (MBR) decoding (Kumar and Byrne 2004) as an alternative search strategy. We are
particularly interested in lattice MBR (LMBR) (Tromble et al. 2008), which is well suited
for the large WFSAs that the system can generate; we use the implementation described
by Blackwood, de Gispert, & Byrne (2010). There are two parameters to be tuned: a
scaling parameter to normalize the evidence scores and a word penalty applied to the
hypotheses space; these are tuned jointly on the tune-nw set. Results are reported in
Figure 18.
We note first that rescoring with the large language model M2, which is effectively
interpolated with M1, gives consistent gains over initial results obtained with M1 alone.
After 5-gram rescoring there is already +0.5 BLEU improvement compared with Gsmall.
With a richer translation grammar we have generated a richer lattice that allows gains
to be gotten by our lattice rescoring techniques.
716
Allauzen et al. Pushdown Automata in Statistical Machine Translation
Figure 18
HiPDT decoding with G. Decoding language model M?1 and first pass rescoring language model
M1 are Katz. Results on test-nw are given for ML-Decoding under the 5-gram stupid backoff
language model (?5gML?) and for LMBR and for LMBR decoding. Parameter values are
? = 15, 12, 9, 8 and ? = 7.5? 10?7 , 7.5? 10?5, 7.5? 10?3.
We also find that BLEU scores degrade smoothly as ? decreases and the expansion
pruning beamwidth narrows, and at all values of ? LMBR gives improvement over
the MAP hypotheses. Because LMBR relies on posterior distributions over n-grams, we
conclude that HiPDT is able to generate compact representations of large search spaces
with posteriors that are robust to pruning conditions.
Finally, we find that increasing ? degrades performance quite smoothly for ? ? 9.
Again, with appropriate choices of ? and ? we can easily reach a compromise between
decoding speed and final performance of our HiPDT system. For instance, with ? =
7.5? 10?7 and? = 12, for whichwe decode at a rate of 3words/sec as seen in Figure 16,
we are losing only 0.5 BLEU after LMBR compared to ? = 7.5? 10?7 and ? = 15.
6. RelatedWork
There is extensive prior work on computational efficiency and algorithmic complexity
in hierarchical phrase-based translation. The challenge is to find algorithms that can be
made to work with large translation grammars and large language models.
Following the original algorithms and analysis of Chiang (2007), Huang and
Chiang (2007) developed the cube-growing algorithm, and more recently Huang and
Mi (2010) developed an incremental decoding approach that exploits the left-to-right
nature of n-gram language models.
Search errors in hierarchical translation, and in translation more generally, have
not been as extensively studied; this is undoubtedly due to the difficulties inherent in
finding exact translations for use in comparison. Using a relatively simple phrase-based
translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an
exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning
suffered significant search errors. For Hiero translation, an extensive comparison of
search errors between the cube pruning and FSA implementation was presented by
Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been
717
Computational Linguistics Volume 40, Number 3
studied in phrase-based translation by Zens andNey (2008). Relaxation techniques have
also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrase-
based SMT (Chang and Collins 2011), and in tree-to-string translation under trigram
language models (Rush and Collins 2011); this prior work involved much smaller
grammars and languages models than have been considered here.
Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been
studied previously by Dyer (2010b), who showed that a single synchronous parsing al-
gorithm (Wu 1997) can be significantly improved upon in practice through hypergraph
compositions. We developed similar procedures for our HiFST decoder (Iglesias et al.
2009a; de Gispert et al. 2010) via a different route, after noting that with the space of
translations represented as WFSAs, alignment can be performed using operations over
WFSTs (Kumar and Byrne 2005).
Although entropy-pruned language models have been used to produce real-time
translation systems (Prasad et al. 2007), we believe our use of entropy-pruned language
models in two-pass translation to be novel. This is an approach that is widely used in
automatic speech recognition (Ljolje, Pereira, and Riley 1999) and we note that it relies
on efficient representation of very large search spaces T for subsequent rescoring, as is
possible with FSAs and PDAs.
7. Conclusion
In this article, we have described a novel approach to hierarchical machine translation
using pushdown automata. We have presented fundamental PDA algorithms including
composition, shortest-path, (pruned) expansion, and replacement and have shown how
these can be used in PDA-based machine translation decoding and how this relates to
and compares with hypergraph and FSA-based decoding.
On the basis of the experimental results presented in the previous sections, we can
now address the questions laid out in Sections 4 and 5:
r A two-pass translation decoding procedure in which translation is first
performed with a weak entropy-pruned language model and followed by
admissible likelihood-based pruning and rescoring with a full language
model can yield good quality translations. Translation performance does
not degrade significantly unless the first-pass language model is very
heavily pruned.
r As predicted by the analysis of algorithmic complexity, intersection and
expansion algorithms based on the PDA representation are able to
perform exact decoding with large translation and weak language models.
By contrast, RTN to FSA expansion fails with large translation grammars,
regardless of the size of the language model. With large translation
grammars, language model composition prior to expansion may be more
attractive than expansion prior to language model composition.
r Our experimental results suggest that for a translation grammar and a
language model of a particular size, and given a value of language model
entropy pruning threshold ?, there is a value of the pruned expansion
parameter ? for which there is no degradation in translation quality with
HiPDT. This makes exact decoding under large translation grammars
possible. The values of ? and ? will be grammar- and task-dependent.
718
Allauzen et al. Pushdown Automata in Statistical Machine Translation
r Although there is some interaction between parameter tuning, pruning
thresholds, and language modeling strategies, the variation is not
significant enough to indicate that a particular language model or
smoothing technique is best. This is particularly true if minimum Bayes
risk decoding is applied to the output translation lattices.
Several questions naturally arise about the decoding strategies presented here. One
is whether inadmissible pruning methods can be applied to the PDA-based systems that
are analogous to those used in current hypergraph-based systems such as cube-pruning
(Chiang 2007). Another is whether a hybrid PDA?FSA system, where some parts of the
PDA are pre-expanded and some not, could provide benefits over full pre-expansion
(FSA) or none (PDA). We leave these questions for future work.
Appendix A. Composition of a Weighted PDT and a Weighted FST
Given a pair (T1,T2) where T1 is a weighted pushdown transducer and the T2 is a
weighted finite-state transducer, and such that T1 has input and output alphabets ?
and ? and T2 has input and output alphabets ? and ?, then there exists a weighted
pushdown transducer T1 ? T2, which is the composition of T1 and T2, such that for all
(x, y) ? ?? ? ??:
T = (T1 ? T2)(x, y) = minz???(T1(x, z)+ T2(z, y)) (A.10)
We also assume that T2 has no input-? transitions, noting that for T2 with input-? tran-
sitions, an epsilon filter (Mohri 2009; Allauzen, Riley, and Schalkwyk 2011) generalized
to handle parentheses could be used.
A state in T is a pair (q1, q2) where q1 is a state of T1 and q2 a state of T2. Given a
transition e1 = (q1, a, b,w1, q?1) in T1, transitions out of (q1, q2) in T are obtained using the
following rules. If b ? ?, then e1 can be matched with a transition (q2, b, c,w2, q?2) in T2
resulting in a transition ((q1, q2), a, c,w1 + w2, (q?1, q?2)) in T. If b = ?, then e1 is matched
with staying in q2 resulting in a transition ((q1, q2), a, ?,w1, (q?1, q2)). Finally, if b = a ? ??,
e1 is also matched with staying in q2, resulting in a transition ((q1, q2), a, a,w1, (q?1, q2)) in
T. The initial state is (I1, I2) and a state (q1, q2) in T is final when both q1 and q2 are both
final. Weight values are assigned as ?((q1, q2)) = ?1(q1)+ ?2(q2).
Appendix B. Pruned Expansion
Let dR and BR be the data structures computed by the shortest-distance algorithm
applied to TR. For a state q in T? (or equivalently T??), let d[q] denote the shortest distance
from the initial state to q, d[q] denote the shortest distance from q to the final state, and
s[q] denote the destination state of the last unbalanced open-parenthesis transition on a
shortest path from the initial state to q.
The algorithm is based on the following property: Letting e denote a transition in
T? such that p[e] = (q, z) and z = z?a, the weight of a shortest path through e can be
expressed as:
d[(q, z)]+ w[e]+ min
e??BR[qs ,a]
dR[n[e], p[e?]]+ w[e?]+ d[(n[e?], z?)] (B.11)
719
Computational Linguistics Volume 40, Number 3
PRUNEDEXPANSION(T, ?)
1 (dR,BR)? SHORTESTDISTANCE (TR )
2 ?? dR[I, f ]+? ? Compute the pruning threshold
3 B? REVERSE (BR ) ? Compute the balance information in T from the one in TR
4 (I?, f ? )? ((I,?), ( f,?)) ? I? and f ? are the initial and final states of the pruned expansion
5 (F?,??( f ?))? ({ f ?}, 0)
6 (d[I?], s[I?])? (0, I? )
7 (d[I?], d[ f ?])? (dR[I, f ], 0)
8 (zD,D[ f ])? (?, 0)
9 S? Q?? {I?}
10 while S 6=? do
11 (q, z)? HEAD(S)
12 DEQUEUE (S)
13 if s[(q, z)]= (q, z) then
14 if z 6= zD then ? If the stack has changed, D needs to be cleared and recomputed
15 CLEAR (D)
16 zD? z
17 for each e ? B[q, z|z|] do ? For each close paren. transition balancing the incoming z|z|-labeled open paren. transition in q
18 D[p[e]]? min(D[p[e]],w[e]+ d[(n[e], z1 ? ? ? z|z|?1 )])
19 for each e ? E[q] do
20 if i[e] ? ?? {?} then ? If i[e] is a regular symbol
21 if RETAINPATH (q, z,w[e],n[e]) then
22 E?? E? ? {((q, z), i[e], o[e],w[e], (n[e], z))}
23 elseif i[e] ?? then ? If i[e] is an open parenthesis
24 z?? zi[e]
25 r? false
26 for each e? ? B[n[e], i[e]] do ? For each close paren. transition e? that balances e
27 w? w[e]+ dR[n[e],p[e?]]+w[e?] ? w: weight of the shortest bal. path beginning by e and ending by e? in T
28 r? r? RETAINPATH (q, z,w,n[e?]) ? Does the expansion of that path belong to an accepting path below threshold?
29 wF?min(wF, dR[n[e], p[e?]]+w[e?]+ d[(n[e?], z)])
30 if r then ? If any of the paths considered above are below threshold
31 E?? E? ? {((q, z),?,?,w[e], (n[e], z? ))}
32 PROCESSSTATE ((n[e], z?))
33 s[(n[e], z?)]? (n[e], z? )
34 d[(n[e], z? )]?min(d[(n[e], z? )], d[(q, z)]+w[e])
35 d[(n[e], z? )]?min(d[(n[e], z? )],wF )
36 elseif i[e] ?? and c?(zi[e]) ? ?? then ? If i[e] is the close parenthesis matching the top of the stack
37 z?? c?(zi[e])
38 if d[(q, z)]+w[e]+ d[(n[e], z? )] ? ? then
39 E?? E? ? {((q, z),?,?,w[e], (n[e], z? ))}
40 return (?,?,?,?,Q?,E?, I?,F?,?? )
RETAINPATH(q, z,w, q? )
1 ? Returns true iff a path from (q, z) to (q?, z) with weight w belongs to an accepting path below threshold
2 wI? d[(q, z)]+w ? Shortest distance from I to (q?, z) when taking a path from (q, z) to (q?, z) of weight w
3 wF? min{dR[q?, t]+D[t]|D[t] 6=?}? Current estimate of s. d. from (q?, z) to f ?
4 if wI < d[(q? , z)] then ? If wI is a better estimate of s.-d. from I? to (q?, z), update d[(q?, z)] and s[(q?, z)]
5 d[(q? , z)]? wI
6 s[(q?, z)]? s[(q, z)]
7 if wF < d[(q?, z)] then ? If wF is a better estimate of s. d. from (q?, z) to f ? , update d[(q?, z)]
8 d[(q?, z)]? wF
9 if ? < wI +wF then ? wI +wF: min. weight of an accepting path taking a path of weight w from (q, z) to (q?, z)
10 return false
11 PROCESSSTATE ((q? , z))
12 return true
PROCESSSTATE((q, z))
1 if (q, z) 6? Q? then ? If state (q, z) does not exist yet, create it and add it to the queue
2 Q?? Q? ?{(q, z)}
3 ENQUEUE (S, (q, z))
Figure 19
PDT pruned expansion algorithm. We assume that F={ f} and ?( f )=0 to simplify the
presentation.
720
Allauzen et al. Pushdown Automata in Statistical Machine Translation
where (qs, z) = s[(q, z)]. This implies that assuming when (q, z) is visited, d[(n[e?], z?)] is
known; we then have all the required information for deciding whether e should be
pruned or retained. In order to ensure that each state is visited once, we need to ensure
that d[(q, z)] is known when (q, z) is visited so we can apply an A? queue discipline
among the states sharing the same stack.
Both conditions can be achieved by using a queue discipline defined by a partial
order? such that
z is a prefix of z? ? (q, z) ? (q?, z?) (B.12)
d[(q, z)]+ d[(q, z)] < d[(q?, z)]+ d[(q?, z)]? (q, z) ? (q?, z) (B.13)
We also assume that all states sharing the same stack will be dequeued consecutively
(z 6= z? ? for all (q, q?), (q, z) ? (q?, z?) or for all (q, q?), (q?, z?) ? (q, z)). This allows us to
cache some computations (the D data structure as described subsequently).
The pseudo code of the algorithm is given in Figure 19. First, the shortest distance
algorithm is applied to TR and the absolute pruning threshold is computed accordingly
(lines 1?2). The resulting balanced data information is then reversed (line 3). The initial
and final states are created (lines 4?5) and the d, d, and D data structures are initialized
accordingly (lines 6?8). The default value in these data structures is assumed to be?.
The queue is initialized containing the initial state (line 9).
The state (q, z) at the head of the queue is dequeued (lines 10?12). If (q, z) admits an
incoming open-parenthesis transition, B contains the balance information for that state
and D can be updated accordingly (lines 13?18).
If e is a regular transition, the resulting transition ((q, z), i[e], o[e],w[e], (n[e], z)) in T?
can be pruned using the criterion derived from Equation (B.11). If it is retained, the
transition is created as well as its destination state (n[e], z) if needed (lines 20?22).
If e is an open-parenthesis transition, each balanced path starting by the resulting
transition in T? and ending by a close-parenthesis transition is treated as a meta-
transition and pruned using the same criterion as regular transitions (lines 23?29). If any
of these meta-transitions is retained, the transition ((q, z), ?, ?,w[e], (n[e], zi[e])) resulting
from e is created as well as its destination state (n[e], zi[e]) if needed (lines 30?35).
If e is a closed-parenthesis transition, it is created if it belongs to a balanced path
below the threshold (lines 36?39).
Finally, the resulting transducer T?? is returned (line 40).
Acknowledgments
The research leading to these results has
received funding from the European Union
Seventh Framework Programme
(FP7-ICT-2009-4) under grant agreement
number 247762, and was supported in part
by the GALE program of the Defense
Advanced Research Projects Agency,
contract no. HR0011-06-C-0022, and a
May 2010 Google Faculty Research Award.
References
Aho, Alfred V. and Jeffrey D. Ullman. 1972.
The Theory of Parsing, Translation and
Compiling, volume 1-2. Prentice-Hall.
Allauzen, Cyril and Michael Riley, 2011.
Pushdown Transducers. http://pdt.
openfst.org.
Allauzen, Cyril, Michael Riley, and Johan
Schalkwyk. 2011. Filters for efficient
composition of weighted finite-state
transducers. In Proceedings of CIAA,
volume 6482 of LNCS, pages 28?38. Blois.
Allauzen, Cyril, Michael Riley, Johan
Schalkwyk, Wojciech Skut, and Mehryar
Mohri. 2007. OpenFst: A general and
efficient weighted finite-state transducer
library. In Proceedings of CIAA,
pages 11?23. http://www.openfst.org.
Bar-Hillel, Y., M. Perles, and E. Shamir. 1964.
On formal properties of simple phrase
721
Computational Linguistics Volume 40, Number 3
structure grammars. In Y. Bar-Hillel,
editor, Language and Information: Selected
Essays on their Theory and Application.
Addison-Wesley, pages 116?150.
Berstel, Jean. 1979. Transductions and
Context-Free Languages. Teubner.
Blackwood, Graeme, Adria` de Gispert,
and William Byrne. 2010. Efficient path
counting transducers for minimum
Bayes-risk decoding of statistical machine
translation lattices. In Proceedings of the
ACL: Short Papers, pages 27?32, Uppsala.
Brants, Thorsten, Ashok C. Popat, Peng Xu,
Franz J. Och, and Jeffrey Dean. 2007.
Large language models in machine
translation. In Proceedings of EMNLP-ACL,
pages 858?867, Prague.
Chang, Yin-Wen and Michael Collins. 2011.
Exact decoding of phrase-based translation
models through lagrangian relaxation.
In Proceedings of EMNLP, pages 26?37,
Edinburgh.
Chelba, Ciprian, Thorsten Brants, Will
Neveitt, and Peng Xu. 2010. Study
on interaction between entropy
pruning and Kneser-Ney smoothing.
In Proceedings of Interspeech,
pages 2,242?2,245, Makuhari.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
de Gispert, Adria`, Gonzalo Iglesias, Graeme
Blackwood, Eduardo R. Banga, and
William Byrne. 2010. Hierarchical
phrase-based translation with weighted
finite state transducers and shallow-n
grammars. Computational Linguistics,
36(3):201?228.
Deng, Yonggang and William Byrne. 2008.
HMM word and phrase alignment for
statistical machine translation. IEEE
Transactions on Audio, Speech, and Language
Processing, 16(3):494?507.
Dyer, Chris. 2010a. A Formal Model of
Ambiguity and its Applications in Machine
Translation. Ph.D. thesis, University of
Maryland.
Dyer, Chris. 2010b. Two monolingual parses
are better than one (synchronous parse). In
Proceedings of NAACL-HLT, pages 263?266,
Los Angeles, CA.
Galley, M., M. Hopkins, K. Knight, and
D. Marcu. 2004. What?s in a translation
rule. In Proceedings of HLT-NAACL,
pages 273?280, Boston, MA.
Hopkins, M. and G. Langmead. 2010. SCFG
decoding without binarization. In
Proceedings of EMNLP, pages 646?655,
Cambridge, MA.
Huang, Liang. 2008. Advanced dynamic
programming in semiring and hypergraph
frameworks. In Proceedings of COLING,
pages 1?18, Manchester.
Huang, Liang and David Chiang. 2007.
Forest rescoring: Faster decoding with
integrated language models. In Proceedings
of ACL, pages 144?151, Prague.
Huang, Liang and Haitao Mi. 2010. Efficient
incremental decoding for tree-to-string
translation. In Proceedings of EMNLP,
pages 273?283, Cambridge, MA.
Huang, Liang, Hao Zhang, and Daniel
Gildea. 2005. Machine translation as
lexicalized parsing with hooks. In
Proceedings of the Ninth International
Workshop on Parsing Technology,
Parsing ?05, pages 65?73, Vancouver.
Iglesias, Gonzalo, Adria` de Gispert,
Eduardo R. Banga, and William Byrne.
2009a. Hierarchical phrase-based
translation with weighted finite state
transducers. In Proceedings of NAACL-HLT,
pages 433?441, Boulder, CO.
Iglesias, Gonzalo, Adria` de Gispert,
Eduardo R. Banga, and William Byrne.
2009b. Rule filtering by pattern for efficient
hierarchical translation. In Proceedings of
EACL, pages 380?388, Athens.
Katz, Slava M. 1987. Estimation of
probabilities from sparse data for the
language model component of a speech
recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3):400?401.
Kneser, Reinhard and Herman Ney. 1995.
Improved backing-off for m-gram
language modeling. In Proceedings of
ICASSP, volume 1, pages 181?184,
Detroit, MI.
Koo, Terry, Alexander M. Rush, Michael
Collins, Tommi Jaakkola, and David
Sontag. 2010. Dual decomposition for
parsing with non-projective head
automata. In Proceedings of EMNLP,
pages 1,288?1,298, Cambridge, MA.
Kuich, Werner and Arto Salomaa. 1986.
Semirings, automata, languages. Springer.
Kumar, Shankar and William Byrne.
2004. Minimum Bayes-risk decoding
for statistical machine translation. In
Proceedings of HLT-NAACL, pages 169?176,
Boston, MA.
Kumar, Shankar and William Byrne.
2005. Local phrase reorderingmodels
for statistical machine translation.
In Proceedings of EMNLP-HLT,
pages 161?168, Rochester, NY.
Kumar, Shankar, Yonggang Deng, and
William Byrne. 2006. A weighted finite
722
Allauzen et al. Pushdown Automata in Statistical Machine Translation
state transducer translation template
model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Lang, Bernard. 1974. Deterministic
techniques for efficient non-deterministic
parsers. In Proceedings of ICALP,
pages 255?269, Saarbru?cken.
Ljolje, Andrej, Fernando Pereira, and
Michael Riley. 1999. Efficient general lattice
generation and rescoring. In Proceedings of
Eurospeech, pages 1,251?1,254, Budapest.
Mohri, Mehryar. 2002. Semiring frameworks
and algorithms for shortest-distance
problems. Journal of Automata, Languages
and Combinatorics, 7:321?350.
Mohri, Mehryar. 2009. Weighted automata
algorithms. In M. Drosde, W. Kuick,
and H. Vogler, editors, Handbook of
Weighted Automata. Springer, chapter 6,
pages 213?254.
Nederhof, Mark-Jan and Giorgio Satta. 2003.
Probabilistic parsing as intersection. In
Proceedings of 8th International Workshop on
Parsing Technologies, pages 137?148, Nancy.
Nederhof, Mark-Jan and Giorgio Satta. 2006.
Probabilistic parsing strategies. Journal of
the ACM, 53(3):406?436.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL, pages 160?167,
Sapporo.
Petre, Ion and Arto Salomaa. 2009.
Algebraic systems and pushdown
automata. In M. Drosde, W. Kuick,
and H. Vogler, editors, Handbook of
Weighted Automata. Springer, chapter 7,
pages 257?289.
Prasad, R., K. Krstovski, F. Choi, S. Saleem,
P. Natarajan, M. Decerbo, and D. Stallard.
2007. Real-time speech-to-speech
translation for PDAs. In Proceedings
of IEEE International Conference on
Portable Information Devices, pages 1?5,
Orlando, FL.
Roark, Brian, Cyril Allauzen, and
Michael Riley. 2013. Smoothed marginal
distribution constraints for language
modeling. In Proceedings of ACL,
pages 43?52, Sofia.
Roark, Brian, Richard Sproat, and Izhak
Shafran. 2011. Lexicographic semirings for
exact automata encoding of sequence
models. In Proceedings of ACL-HLT,
pages 1?5, Portland, OR.
Rush, Alexander M. and Michael Collins.
2011. Exact decoding of syntactic
translation models through lagrangian
relaxation. In Proceedings of ACL-HLT,
pages 72?82, Portland, OR.
Satta, Giorgio and Enoch Peserico. 2005.
Some computational complexity results
for synchronous context-free grammars.
In Proceedings of HLT-EMNLP,
pages 803?810, Vancouver.
Shafran, Izhak, Richard Sproat, Mahsa
Yarmohammadi, and Brian Roark.
2011. Efficient determinization of
tagged word lattices using categorial
and lexicographic semirings. In
Proceedings of ASRU, pages 283?288,
Honolulu, HI.
Stolcke, Andreas. 1995. An efficient
probabilistic context-free parsing
algorithm that computes prefix
probabilities. Computational Linguistics,
21(2):165?201.
Stolcke, Andreas. 1998. Entropy-based
pruning of backoff language models.
In Proceedings of DARPA Broadcast
News Transcription and Understanding
Workshop, pages 270?274, Landsdowne,
VA.
Tromble, Roy, Shankar Kumar, Franz J. Och,
and Wolfgang Macherey. 2008. Lattice
minimum Bayes-risk decoding for
statistical machine translation. In
Proceedings of EMNLP, pages 620?629,
Edinburgh.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23:377?403.
Xiao, Tong, Mu Li, Dongdong Zhang,
Jingbo Zhu, and Ming Zhou. 2009. Better
synchronous binarization for machine
translation. In Proceedings of EMNLP,
pages 362?370, Singapore.
Zens, Richard and Hermann Ney. 2008.
Improvements in dynamic programming
beam search for phrase-based statistical
machine translation. In Proceedings of
IWSLT, pages 195?205, Honolulu, HI.
Zhang, Hao, Liang Huang, Daniel Gildea,
and Kevin Knight. 2006. Synchronous
binarization for machine translation. In
Proceedings of HLT-NAACL, pages 256?263,
New York, NY.
Zollmann, Andreas and Ashish Venugopal.
2006. Syntax augmented machine
translation via chart parsing. In Proceedings
of NAACL Workshop on Statistical Machine
Translation, pages 138?141, New York, NY.
723

Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 155?160,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The CUED HiFST System for the WMT10 Translation Shared Task
Juan Pino Gonzalo Iglesias?1 Adria` de Gispert
Graeme Blackwood Jamie Brunning William Byrne
Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
{jmp84,gi212,ad465,gwb24,jjjb2,wjb31}@eng.cam.ac.uk
? Department of Signal Processing and Communications, University of Vigo, Vigo, Spain
Abstract
This paper describes the Cambridge Uni-
versity Engineering Department submis-
sion to the Fifth Workshop on Statistical
Machine Translation. We report results for
the French-English and Spanish-English
shared translation tasks in both directions.
The CUED system is based on HiFST, a
hierarchical phrase-based decoder imple-
mented using weighted finite-state trans-
ducers. In the French-English task, we
investigate the use of context-dependent
alignment models. We also show that
lattice minimum Bayes-risk decoding is
an effective framework for multi-source
translation, leading to large gains in BLEU
score.
1 Introduction
This paper describes the Cambridge University
Engineering Department (CUED) system submis-
sion to the ACL 2010 Fifth Workshop on Statis-
tical Machine Translation (WMT10). Our trans-
lation system is HiFST (Iglesias et al, 2009a), a
hierarchical phrase-based decoder that generates
translation lattices directly. Decoding is guided
by a CYK parser based on a synchronous context-
free grammar induced from automatic word align-
ments (Chiang, 2007). The decoder is imple-
mented with Weighted Finite State Transducers
(WFSTs) using standard operations available in
the OpenFst libraries (Allauzen et al, 2007). The
use of WFSTs allows fast and efficient exploration
of a vast translation search space, avoiding search
errors in decoding. It also allows better integration
with other steps in our translation pipeline such as
5-gram language model (LM) rescoring and lattice
minimum Bayes-risk (LMBR) decoding.
1Now a member of the Department of Engineering, Uni-
versity of Cambridge, Cambridge, CB2 1PZ, U.K.
# Sentences # Tokens # Types
(A)Europarl+News-Commentary
FR 1.7 M 52.4M 139.7kEN 47.6M 121.6k
(B)Europarl+News-Commentary+UN
FR 8.7 M 277.9M 421.0kEN 241.4M 482.1k
(C)Europarl+News-Commentary+UN+Giga
FR 30.2 M 962.4M 2.4MEN 815.3M 2.7M
Table 1: Parallel data sets used for French-to-
English experiments.
We participated in the French-English and
Spanish-English translation shared tasks in each
translation direction. This paper describes the de-
velopment of these systems. Additionally, we re-
port multi-source translation experiments that lead
to very large gains in BLEU score.
The paper is organised as follows. Section 2
describes each step in the development of our sys-
tem for submission, from pre-processing to post-
processing. Section 3 presents and discusses re-
sults and Section 4 describes an additional experi-
ment on multi-source translation.
2 System Development
We built three French-English and two Spanish-
English systems, trained on different portions of
the parallel data sets available for this shared task.
Statistics for the different parallel sets are sum-
marised in Tables 1 and 2. No additional parallel
data was used. As will be shown, the largest paral-
lel corpus gave the best results in French, but this
was not the case in Spanish.
2.1 Pre-processing
The data was minimally cleaned by replacing
HTML-related metatags by their corresponding
155
# Sentences # Tokens # Types
(A) Europarl + News-Commentary
SP 1.7M 49.4M 167.2kEN 47.0M 122.7k
(B) Europarl + News-Commentary + UN
SP 6.5M 205.6M 420.8kEN 192.0M 402.8k
Table 2: Parallel data sets used for Spanish-to-
English experiments.
UTF8 token (e.g., replacing ?&amp? by ?&?) as
this interacts with tokenization. Data was then to-
kenized and lowercased, so mixed case is added as
post-processing.
2.2 Alignments
Parallel data was aligned using the MTTK toolkit
(Deng and Byrne, 2005). In the English-to-French
and English-to-Spanish directions, we trained
a word-to-phrase HMM model with maximum
phrase length of 2. In the French to English and
Spanish to English directions, we trained a word-
to-phrase HMM Model with a bigram translation
table and maximum phrase length of 4.
We also trained context-dependent alignment
models (Brunning et al, 2009) for the French-
English medium-size (B) dataset. The context of
a word is based on its part-of-speech and the part-
of-speech tags of the surrounding words. These
tags were obtained by applying the TnT Tagger
(Brants, 2000) for English and the TreeTagger
(Schmid, 1994) for French. Decision tree clus-
tering based on optimisation of the EM auxiliary
function was used to group contexts that trans-
late similarly. Unfortunately, time constraints pre-
vented us from training context-dependent models
for the larger (C) dataset.
2.3 Language Model
For each target language, we used the SRILM
Toolkit (Stolcke, 2002) to estimate separate 4-
gram LMs with Kneser-Ney smoothing (Kneser
and Ney, 1995), for each of the corpora listed in
Tables 3, 4 and 5. The LM vocabulary was ad-
justed to the parallel data set used. The compo-
nent models of each language pair were then in-
terpolated to form a single LM for use in first-pass
translation decoding. For French-to-English trans-
lation, the interpolation weights were optimised
for perplexity on a development set.
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 246.4M
CNA 1.3M 34.8M
LTW 12.9M 298.7M
XIN 16.0M 352.5M
AFP 30.4M 710.6M
APW 62.1M 1268.6M
NYT 73.6M 1622.5M
Giga 21.4M 573.8M
News 48.7M 1128.4M
Total 275.4M 6236.4M
Table 3: English monolingual training corpora.
Corpus # Sentences # Tokens
EU + NC + UN 9.0M 282.8
AFP 25.2M 696.0M
APW 12.7M 300.6M
News 15.2M 373.5M
Giga 21.4M 684.4M
Total 83.5 M 2337.3M
Table 4: French monolingual training corpora.
Corpus # Sentences # Tokens
NC + News 4.0M 110.8M
EU + Gigaword (5g) 249.4M 1351.5M
Total 253.4 M 1462.3M
Table 5: Spanish monolingual training corpora.
The Spanish-English first pass LM was trained
directly on the NC+News portion of monolingual
data, as we did not find improvements by using
Europarl. The second pass rescoring LM used all
available data.
2.4 Grammar Extraction and Decoding
After unioning the Viterbi alignments, phrase-
based rules of up to five source words in length
were extracted, hierarchical rules with up to two
non-contiguous non-terminals in the source side
were then extracted applying the restrictions de-
scribed in (Chiang, 2007). For Spanish-English
and French-English tasks, we used a shallow-1
grammar where hierarchical rules are allowed to
be applied only once on top of phrase-based rules.
This has been shown to perform as well as a
fully hierarchical grammar for a Europarl Spanish-
English task (Iglesias et al, 2009b).
For translation, we used the HiFST de-
156
coder (Iglesias et al, 2009a). HiFST is a hierarchi-
cal decoder that builds target word lattices guided
by a probabilistic synchronous context-free gram-
mar. Assuming N to be the set of non-terminals
and T the set of terminals or words, then we can
define the grammar as a set R = {Rr} of rules
Rr : N ? ??r,?r? / pr, where N ? N; and
?, ? ? {N ? T}+.
HiFST translates in three steps. The first step
is a variant of the CYK algorithm (Chappelier and
Rajman, 1998), in which we apply hypothesis re-
combination without pruning. Only the source
language sentence is parsed using the correspond-
ing source-side context-free grammar with rules
N ? ?. Each cell in the CYK grid is specified
by a non-terminal symbol and position: (N,x, y),
spanning sx+y?1x on the source sentence s1...sJ .
For the second step, we use a recursive algo-
rithm to construct word lattices with all possi-
ble translations produced by the hierarchical rules.
Construction proceeds by traversing the CYK grid
along the back-pointers established in parsing. In
each cell (N,x, y) of the CYK grid, we build a
target language word lattice L(N,x, y) containing
every translation of sx+y?1x from every derivation
headed by N . For efficiency, this lattice can use
pointers to lattices on other cells of the grid.
In the third step, we apply the word-based LM
via standard WFST composition with failure tran-
sitions, and perform likelihood-based pruning (Al-
lauzen et al, 2007) based on the combined trans-
lation and LM scores.
As explained before, we are using shallow-1 hi-
erarchical grammars (de Gispert et al, 2010) in
our experiments for WMT2010. One very inter-
esting aspect is that HiFST is able to build ex-
act search spaces with this model, i.e. there is no
pruning in search that may lead to spurious under-
generation errors.
2.5 Parameter Optimisation
Minimum error rate training (MERT) (Och, 2003)
under the BLEU score (Papineni et al, 2001) opti-
mises the weights of the following decoder fea-
tures with respect to the newstest2008 develop-
ment set: target LM, number of usages of the
glue rule, word and rule insertion penalties, word
deletion scale factor, source-to-target and target-
to-source translation models, source-to-target and
target-to-source lexical models, and three binary
rule count features inspired by Bender et al (2007)
indicating whether a rule occurs once, twice, or
more than twice in the parallel training data.
2.6 Lattice Rescoring
One of the advantages of HiFST is direct gener-
ation of large translation lattices encoding many
alternative translation hypotheses. These first-pass
lattices are rescored with second-pass higher-order
LMs prior to LMBR.
2.6.1 5-gram LM Lattice Rescoring
We build sentence-specific, zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram LMs esti-
mated over approximately 6.2 billion words for
English, 2.3 billion words for French, and 1.4 bil-
lion words for Spanish. For the English-French
task, the second-pass LM training data is the same
monolingual data used for the first-pass LMs (as
summarised in Tables 3, 4). The Spanish second-
pass 5-gram LM includes an additional 1.4 billion
words of monolingual data from the Spanish Giga-
Word Second Edition (Mendonca et al, 2009) and
Europarl, which were not included in the first-pass
LM (see Table 5).
2.6.2 LMBR Decoding
Minimum Bayes-risk (MBR) decoding (Kumar
and Byrne, 2004) over the full evidence space
of the 5-gram rescored lattices was applied to
select the translation hypothesis that maximises
the conditional expected gain under the linearised
sentence-level BLEU score (Tromble et al, 2008;
Blackwood and Byrne, 2010). The unigram preci-
sion p and average recall ratio r were set as de-
scribed in Tromble et al (2008) using the new-
stest2008 development set.
2.7 Hypothesis Combination
Linearised lattice minimum Bayes-risk decoding
(Tromble et al, 2008) can also be used as an ef-
fective framework for multiple lattice combination
(de Gispert et al, 2010). For the French-English
language pair, we used LMBR to combine transla-
tion lattices produced by systems trained on alter-
native data sets.
2.8 Post-processing
For both Spanish-English and French-English sys-
tems, the recasing procedure was performed with
the SRILM toolkit. For the Spanish-English sys-
tem, we created models from the GigaWord set
corresponding to each system output language.
157
Task Configuration newstest2008 newstest2009 newstest2010
FR ? EN
HiFST (A) 23.4 26.4 ?
HiFST (B) 24.0 27.3 ?
HiFST (B)CD 24.2 27.6 28.0
+5g+LMBR 24.6 28.4 28.9
HiFST (C) 24.7 28.4 28.5
+5g+LMBR 25.3 29.1 29.3
LMBR (B)CD+(C) 25.6 29.3 29.6
EN ? FR
HiFST (A) 22.5 24.2 ?
HiFST (B) 23.4 24.8 ?
HiFST (B)CD 23.3 24.8 26.7
+5g+LMBR 23.7 25.3 27.1
HiFST (C) 23.6 25.6 27.4
+5g+LMBR 23.9 25.8 27.8
LMBR (B)CD+(C) 24.2 26.1 28.2
Table 6: Translation Results for the French-English (FR-EN) language pair, shown in single-reference
lowercase IBM BLEU. Bold results correspond to submitted systems.
For the French-English system, the English model
was trained using the monolingual News corpus
and the target side of the News-Commentary cor-
pus, whereas the French model was trained using
all available constrained French data.
English, Spanish and French outputs were also
detokenized before submission. In French, words
separated by apostrophes were joined.
3 Results and Discussion
French?English Language Pair
Results are reported in Table 6. We can see
that using more parallel data consistently improves
performance. In the French-to-English direction,
the system HiFST (B) improves over HiFST (A)
by +0.9 BLEU and HiFST (C) improves over
HiFST (B) by +1.1 BLEU on the newstest2009
development set prior to any rescoring. The
same trend can be observed in the English-to-
French direction (+0.6 BLEU and +0.8 BLEU im-
provement). The use of context dependent align-
ment models gives a small improvement in the
French-to-English direction: system (B)CD im-
proves by +0.3 BLEU over system (B) on new-
stest2009. In the English-to-French direction,
there is no improvement nor degradation in per-
formance. 5-gram and LMBR rescoring also give
consistent improvement throughout the datasets.
Finally, combination between the medium-size
system (B)CD and the full-size system (C) gives
further small gains in BLEU over LMBR on each
individual system.
Spanish?English Language Pair
Results are reported in Table 7. We report experi-
mental results on two systems. The HiFST(A) sys-
tem is built on the Europarl + News-Commentary
training set. Systems HiFST (B),(B2) and (B3)
use UN data in different ways. System (B) simply
uses all the data for the standard rule extraction
procedure. System HiFST (B2) includes UN data
to build alignment models and therefore reinforce
alignments obtained from smaller dataset (A), but
extracts rules only from dataset (A). HiFST (B3)
combines hierarchical phrases extracted for sys-
tem (A) with phrases extracted from system (B).
Unfortunately, these three larger data strategies
lead to degradation over using only the smaller
dataset (A). For this reason, our best systems only
use the Euparl + News-Commentary parallel data.
This is surprising given that additional data was
helpful for the French-English task. Solving this
issue is left for future work.
4 Multi-Source Translation Experiments
Multi-source translation (Och and Ney, 2001;
Schroeder et al, 2009) is possible whenever mul-
tiple translations of the source language input sen-
tence are available. The motivation for multi-
source translation is that some of the ambiguity
that must be resolved in translating between one
pair of languages may not be present in a differ-
ent pair. In the following experiments, multiple
LMBR is applied for the first time to the task of
multi-source translation.
158
Task Configuration newstest2008 newstest2009 newstest2010
SP ? EN
HiFST (A) 24.6 26.0 29.1
+5g+LMBR 25.4 27.0 30.5
HiFST (B) 23.7 25.4 ?
HiFST (B2) 24.3 25.7 ?
HiFST (B3) 24.2 25.6 ?
EN ? SP HiFST (A) 23.9 24.5 28.0
+5g+LMBR 24.7 25.5 29.1
Table 7: Translation Results for the Spanish-English (SP-EN) language pair, shown in lowercase IBM
BLEU. Bold results correspond to submitted systems.
Configuration newstest2008 newstest2009 newstest2010
FR?EN HiFST+5g 24.8 28.5 28.8
+LMBR 25.3 29.0 29.2
ES?EN HiFST+5g 25.2 26.8 30.1
+LMBR 25.4 26.9 30.3
FR?EN + ES?EN LMBR 27.2 30.4 32.0
Table 8: Lowercase IBM BLEU for single-system LMBR and multiple LMBR multi-source translation
of French (FR) and Spanish (ES) into English (EN).
Separate second-pass 5-gram rescored lattices
EFR and EES are generated for each test set sen-
tence using the French-to-English and Spanish-to-
English HiFST translation systems. The MBR hy-
pothesis space is formed as the union of these lat-
tices. In a similar manner to MBR decoding over
multiple k-best lists in de Gispert et al (2009),
the path posterior probability of each n-gram u re-
quired for linearised LMBR is computed as a lin-
ear interpolation of the posterior probabilities ac-
cording to each individual lattice so that p(u|E) =
?FR p(u|EFR) + ?ES p(u|EES), where p(u|E) is the
sum of the posterior probabilities of all paths con-
taining the n-gram u. The interpolation weights
?FR + ?ES = 1 are optimised for BLEU score on
the development set newstest2008.
The results of single-system and multi-source
LMBR decoding are shown in Table 8. The opti-
mised interpolation weights were ?FR = 0.55 and
?ES = 0.45. Single-system LMBR gives relatively
small gains on these test sets. Much larger gains
are obtained through multi-source MBR combina-
tion. Compared to the best of the single-system 5-
gram rescored lattices, the BLEU score improves
by +2.0 for newstest2008, +1.9 for newstest2009,
and +1.9 for newstest2010. For scoring with re-
spect to a single reference, these are very large
gains indeed.
5 Summary
We have described the CUED submission to
WMT10 using HiFST, a hierarchical phrase-based
translation system. Results are very competitive in
terms of automatic metric for both English-French
and English-Spanish tasks in both directions. In
the French-English task, we have seen that the UN
and Giga additional parallel data are helpful. It
is surprising that UN data did not help for the
Spanish-English language pair.
Future work includes investigating this issue,
developing detokenization tailored to each output
language and applying context dependent align-
ment models to larger parallel datasets.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022. Gonzalo Iglesias was sup-
ported by the Spanish Government research grant
BES-2007-15956 (projects TEC2006-13694-C03-
03 and TEC2009-14094-C04-04).
159
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of CIAA, pages 11?23.
Oliver Bender, Evgeny Matusov, Stefan Hahn, Sasa
Hasan, Shahram Khadivi, and Hermann Ney. 2007.
The RWTH Arabic-to-English spoken language
translation system. In Proceedings of ASRU, pages
396?401.
Graeme Blackwood and William Byrne. 2010. Ef-
ficient Path Counting Transducers for Minimum
Bayes-Risk Decoding of Statistical Machine Trans-
lation Lattices (to appear). In Proceedings of the
ACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-ACL, pages 858?867.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of ANLP, pages 224?
231, April.
Jamie Brunning, Adria` de Gispert, and William Byrne.
2009. Context-dependent alignment models for
statistical machine translation. In Proceedings of
HLT/NAACL, pages 110?118.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
generalized CYK algorithm for parsing stochastic
CFG. In Proceedings of TAPD, pages 133?137.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Adria` de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes Risk Com-
bination of Translation Hypotheses from Alterna-
tive Morphological Decompositions. In Proceed-
ings of HLT/NAACL, Companion Volume: Short Pa-
pers, pages 73?76.
Adria` de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite state transducers and shallow-n grammars (to
appear). In Computational Linguistics.
Yonggang Deng and William Byrne. 2005. HMM
Word and Phrase Alignment for Statistical Machine
Translation. In Proceedings of HLT/EMNLP, pages
169?176.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-
based translation with weighted finite state transduc-
ers. In Proceedings of NAACL, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. The HiFST System for
the EuroParl Spanish-to-English Task. In Proceed-
ings of SEPLN, pages 207?214.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of ICASSP, volume 1, pages 181?184.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Angelo Mendonca, David Graff, and Denise DiPersio.
2009. Spanish Gigaword Second Edition, Linguistic
Data Consortium.
Franz Josef Och and Hermann Ney. 2001. Statisti-
cal multi-source translation. In Machine Translation
Summit 2001, pages 253?258.
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12. Manchester, UK.
Josh Schroeder, Trevor Cohn, and Philipp Koehn.
2009. Word Lattices for Multi-Source Translation.
In Proceedings of EACL, pages 719?727.
Andreas Stolcke. 2002. SRILM?An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP,
volume 3, pages 901?904.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk decoding for statistical machine trans-
lation. In Proceedings of EMNLP, pages 620?629.
160
