TSUBAKI: An Open Search Engine Infrastructure for
Developing New Information Access Methodology
Keiji Shinzato?, Tomohide Shibata?, Daisuke Kawahara?,
Chikara Hashimoto?? and Sadao Kurohashi?
?Graduate School of Informatics, Kyoto University
?National Institute of Information and Communications Technology
??Department of Informatics, Yamagata University
{shinzato, shibata, kuro}@nlp.kuee.kyoto-u.ac.jp
dk@nict.go.jp ch@yz.yamagata-u.ac.jp
Abstract
As the amount of information created by
human beings is explosively grown in the
last decade, it is getting extremely harder
to obtain necessary information by conven-
tional information access methods. Hence,
creation of drastically new technology is
needed. For developing such new technol-
ogy, search engine infrastructures are re-
quired. Although the existing search engine
APIs can be regarded as such infrastructures,
these APIs have several restrictions such as a
limit on the number of API calls. To help the
development of new technology, we are run-
ning an open search engine infrastructure,
TSUBAKI, on a high-performance comput-
ing environment. In this paper, we describe
TSUBAKI infrastructure.
1 Introduction
As the amount of information created by human be-
ings is explosively grown in the last decade (Uni-
versity of California, 2003), it is getting extremely
harder to obtain necessary information by con-
ventional information access methods, i.e., Web
search engines. This is obvious from the fact that
knowledge workers now spend about 30% of their
day on only searching for information (The Del-
phi Group White Paper, 2001). Hence, creation of
drastically new technology is needed by integrating
several disciplines such as natural language process-
ing (NLP), information retrieval (IR) and others.
Conventional search engines such as Google and
Yahoo! are insufficient to search necessary informa-
tion from the current Web. The problems of the con-
ventional search engines are summarized as follows:
Cannot accept queries by natural language sen-
tences: Search engine users have to represent their
needs by a list of words. This means that search
engine users cannot obtain necessary information if
they fail to represent their needs into a proper word
list. This is a serious problem for users who do not
utilize a search engine frequently.
Cannot provide organized search results: A
search result is a simple list consisting of URLs,
titles and snippets of web pages. This type of re-
sult presentation is obviously insufficient consider-
ing explosive growth and diversity of web pages.
Cannot handle synonymous expressions: Exist-
ing search engines ignore a synonymous expression
problem. Especially, since Japanese uses three kinds
of alphabets, Hiragana, Katakana and Kanji, this
problem is more serious. For instance, although both
Japanese words ????? and ???? mean child,
the search engines provide quite different search re-
sults for each word.
We believe that new IR systems that overcome the
above problems give us more flexible and com-
fortable information access and that development
of such systems is an important and interesting re-
search topic.
To develop such IR systems, a search engine in-
frastructure that plays a low-level layer role (i.e., re-
trieving web pages according to a user?s query from
a huge web page collection) is required. The Appli-
cation Programming Interfaces (APIs) provided by
189
commercial search engines can be regarded as such
search engine infrastructures. The APIs, however,
have the following problems:
1. The number of API calls a day and the num-
ber of web pages included in a search result are
limited.
2. The API users cannot know how the acquired
web pages are ranked because the ranking mea-
sure of web pages has not been made public.
3. It is difficult to reproduce previously-obtained
search results via the APIs because search en-
gine?s indices are updated frequently.
These problems are an obstacle to develop new IR
systems using existing search engine APIs.
The research project ?Cyber Infrastructure for the
Information-explosion Era1? gives researchers sev-
eral kinds of shared platforms and sophisticated
tools, such as an open search engine infrastructure,
considerable computational environment and a grid
shell software (Kaneda et al, 2002), for creation of
drastically new IR technology. In this paper, we de-
scribe an open search engine infrastructure TSUB-
AKI, which is one of the shared platforms devel-
oped in the Cyber Infrastructure for the Information-
explosion Era project. The overview of TSUBAKI is
depicted in Figure 1. TSUBAKI is built on a high-
performance computing environment consisting of
128 CPU cores and 100 tera-byte storages, and it
can provide users with search results retrieved from
approximately 100 million Japanese web pages.
The mission of TSUBAKI is to help the develop-
ment of new information access methodology which
solves the problems of conventional information ac-
cess methods. This is achieved by the following
TSUBAKI?s characteristics:
API without any restriction: TSUBAKI pro-
vides its API without any restrictions such as the
limited number of API calls a day and the number
of results returned from an API per query, which are
the typical restrictions of the existing search engine
APIs. Consequently, TSUBAKI API users can de-
velop systems that handle a large number of web
pages. This feature is important for dealing with the
Web that has the long tail aspect.
1http://i-explosion.ex.nii.ac.jp/i-explosion/ctr.php/m/Inde-
xEng/a/Index/
	







 	
SYNGRAPH: A Flexible Matching Method based on Synonymous
Expression Extraction from an Ordinary Dictionary and a Web Corpus
Tomohide Shibata?, Michitaka Odani?, Jun Harashima?,
Takashi Oonishi??, and Sadao Kurohashi?
?Kyoto University, Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan
??NEC Corporation, 1753, Shimonumabe, Nakahara-Ku, Kawasaki, Kanagawa 211-8666, Japan
{shibata,odani,harashima,kuro}@nlp.kuee.kyoto-u.ac.jp
t-onishi@bq.jp.nec.com
Abstract
This paper proposes a flexible matching
method that can assimilate the expressive
divergence. First, broad-coverage syn-
onymous expressions are automatically ex-
tracted from an ordinary dictionary, and
among them, those whose distributional
similarity in a Web corpus is high are used
for the flexible matching. Then, to overcome
the combinatorial explosion problem in the
combination of expressive divergence, an ID
is assigned to each synonymous group, and
SYNGRAPH data structure is introduced to
pack the expressive divergence. We con-
firmed the effectiveness of our method on
experiments of machine translation and in-
formation retrieval.
1 Introduction
In natural language, many expressions have almost
the same meaning, which brings great difficulty to
many NLP tasks, such as machine translation (MT),
information retrieval (IR), and question answering
(QA). For example, suppose an input sentence (1) is
given to a Japanese-English example-based machine
translation system.
(1) hotel ni
hotel
ichiban
best
chikai
near
eki wa
station
doko-desuka
where is
Even if a very similar translation example (TE)
?(2-a) ? (2-b)? exists in the TEs, a simple exact
matching method cannot utilize this example for the
translation.
(2) a. ryokan no
Japanese hotel
moyori no
nearest
eki wa
station
doko-desuka
where is
b. Where?s the nearest station to the hotel?
How to handle these synonymous expressions has
become one of the important research topics in NLP.
This paper presents a flexible matching method,
which can assimilate the expressive divergence, to
solve this problem. This method has the following
two features:
1. Synonymy relations and hypernym-hyponym
relations are automatically extracted from an
ordinary dictionary and a Web corpus.
2. Extracted synonymous expressions are effec-
tively handled by SYNGRAPH data structure,
which can pack the expressive divergence.
An ordinary dictionary is a knowledge source
to provide synonym and hypernym-hyponym rela-
tions (Nakamura and Nagao, 1988; Tsurumaru et al,
1986). A problem in using synonymous expressions
extracted from a dictionary is that some of them are
not appropriate since they are rarely used. For exam-
ple, a synonym pair ?suidou?1 = ?kaikyou(strait)? is
extracted.
Recently, some work has been done on corpus-
based paraphrase extraction (Lin and Pantel, 2001;
Barzilay and Lee, 2003). The basic idea of their
methods is that two words with similar meanings
are used in similar contexts. Although their methods
can obtain broad-coverage paraphrases, the obtained
paraphrases are not accurate enough to be utilized
1This word usually means ?water supply?.
787
for achieving precise matching since they contain
synonyms, near-synonyms, coordinate terms, hyper-
nyms, and inappropriate synonymous expressions.
Our approach makes the best use of an ordi-
nary dictionary and a Web corpus to extract broad-
coverage and precise synonym and hypernym-
hyponym expressions. First, synonymous expres-
sions are extracted from a dictionary. Then, the
distributional similarity of a pair of them is calcu-
lated using a Web corpus. Among extracted syn-
onymous expressions, those whose similarity is high
are used for the flexible matching. By utilizing only
synonymous expressions extracted from a dictionary
whose distributional similarity is high, we can ex-
clude synonymous expressions extracted from a dic-
tionary that are rarely used, and the pair of words
whose distributional similarity is high that is not ac-
tually a synonymous expression (is not listed in a
dictionary).
Another point of our method is to introduce SYN-
GRAPH data structure. So far, the effectiveness
of handling expressive divergence has been shown
for IR using a thesaurus-based query expansion
(Voorhees, 1994; Jacquemin et al, 1997). However,
their methods are based on a bag-of-words approach
and thus does not pay attention to sentence-level
synonymy with syntactic structure. MT requires
such precise handling of synonymy, and advanced
IR and QA also need it. To handle sentence-level
synonymy precisely, we have to consider the combi-
nation of expressive divergence, which may cause
combinatorial explosion. To overcome this prob-
lem, an ID is assigned to each synonymous group,
and then SYNGRAPH data structure is introduced
to pack expressive divergence.
2 Synonymy Database
This section describes a method for constructing a
synonymy database. First, synonym/hypernym re-
lations are automatically extracted from an ordinary
dictionary, and the distributional similarity of a pair
of synonymous expressions is calculated using a
Web corpus. Then, the extracted synonymous ex-
pressions whose similarity is high are used for the
flexible matching.
2.1 Synonym/hypernym Extraction from an
Ordinary Dictionary
Although there were some attempts to extract syn-
onymous expressions from a dictionary (Nakamura
and Nagao, 1988; Tsurumaru et al, 1986), they ex-
tracted only hypernym-hyponym relations from the
limited entries. In contrast, our method extracts not
only hypernym-hyponym relations, but also basic
synonym relations, predicate synonyms, adverbial
synonyms and synonym relations between a word
and a phrase.
The last word of the first definition sentence is
usually the hypernym of an entry word. Some defi-
nition sentences in a Japanese dictionary are shown
below (the left word of ?:? is an entry word, the right
sentence is a definition, and words in bold font is the
extracted words):
yushoku (dinner) : yugata (evening) no (of)
shokuji (meal).
jushin (barycenter) : omosa (weight) ga (is)
tsuriatte (balance) tyushin (center) tonaru
(become) ten (spot).
For example, the last word shokuji (meal) can be
extracted as the hypernym of yushoku (dinner). In
some cases, however, a word other than the last word
can be a hypernym or synonym. These cases can be
detected by sentence-final patterns as follows (the
underlined expressions represent the patterns):
Hypernyms
dosei (Saturn) : wakusei (planet) no (of) hitotsu
(one).
tobi (kite) : taka (hawk) no (of) issyu (kind).
Synonyms / Synonymous Phrases
ice : ice cream no (of) ryaku (abbreviation).
mottomo (most) : ichiban (best). (? one word defi-
nition)
moyori (nearest) : ichiban (best) chikai (near)
tokoro (place)2. (? less than three phrases)
2.2 Calculating the Distributional Similarity
using a Web Corpus
The similarity between a pair of synonymous ex-
pressions is calculated based on distributional sim-
ilarity (J.R.Firth, 1957; Harris, 1968) using the
Web corpus collected by (Kawahara and Kurohashi,
2006). The similarity between two predicates is de-
fined to be one between the patterns of case exam-
ples of each predicate (Kawahara and Kurohashi,
2001). The similarity between two nouns are defined
2If the last word of a sentence is a highly general term such
as koto (thing) and tokoro (place), it is removed from the syn-
onymous expression.
788
gakkou (school)
gakue n (a ca d e m y )
<school>
s h ogakkou (p r i m a r y  school)
s h ogaku (e le m e n t a r y  school)
<p r i m a r y  school>
koukou (hi g h school)
kout ougakkou (se n i or  hi g h)
<hi g h school>
t okor o (p la ce )
<p la ce >
h an t e n (b lob )
m ad ar a (m ot t le )
b uc h i (m a cu la )
<b lob >
t e n  (sp ot )
<sp ot >
j us h i n (b a r y ce n t e r )
<b a r y ce n t e r >
 m oy or i (n e a r e st )
 i c h i b an (b e st )  c h i kaku(n e a r )
<n e a r e st >
m ot t om o (m ost )
i c h i b an (b e st )
<m ost >
p oly se m i c w or dhy p e r n y m -hy p on y m  r e la t i on
 t e n  (sp ot )
<sp ot >
t e n  (sp ot )
p oc h i (d ot )
c h i s an a (sm a ll)   s h i r us h i(m a r k )
<sp ot >
 t e n  (sp ot )
b as h o (a r e a )
i c h i  (loca t i on )
<sp ot >
s h i r us h i  (m a r k )
<m a r k >
sy n on y m ou s g r ou p
Figure 1: An example of synonymy database.
as the ratio of the overlapped co-occurrence words
using the Simpson coefficient. The Simpson coeffi-
cient is computed as |T (w1)?T (w2)|min(|T (w1)|,|T (w2)|) , where T (w) is
the set of co-occurrence words of word w.
2.3 Integrating the Distributional Similarity
into the Synonymous Expressions
Synonymous expressions can be extracted from a
dictionary as described in Section 2.1. However,
some extracted synonyms/hypernyms are not appro-
priate since they are rarely used. Especially, in the
case of that a word has multiple senses, the syn-
onym/hypernym extracted from the second or later
definition might cause the inappropriate matching.
For example, since ?suidou? has two senses, the
two synonym pairs, ?suidou? = ?jyosuidou(water
supply)? and ?suidou? = ?kaikyou(strait)?, are ex-
tracted. The second sense is rarely used, and thus if
the synonymy pair extracted from the second defi-
nition is used as a synonym relation, an inappropri-
ate matching through this synonymmight be caused.
Therefore, only the pairs of synonyms/hypernyms
whose distributional similarity calculated in Section
2.2 is high are utilized for the flexible matching.
The similarity threshold is set to 0.4 for synonyms
and to 0.3 for hypernyms. For example, since the
similarity between ?suidou? and ?kaikyou? is 0.298,
this synonym is not utilized.
2.4 Synonymy Database Construction
With the extracted binomial relations, a synonymy
database can be constructed. Here, polysemic words
should be treated carefully3. When the relations
A=B and B=C are extracted, and B is not polysemic,
3If a word has two or more definition items in the dictionary,
the word can be regarded as polysemic.
they can be merged into A=B=C. However, if B is
polysemic, the synonym relations are not merged
through a polysemic word. In the same way, as for
hypernym-hyponym relations, A ? B and B ? C,
and A ? B and C ? B are not merged if B is pol-
ysemic. By merging binomial synonym relations
with the exception of polysemic words, synony-
mous groups are constructed first. They are given
IDs, hereafter called SYNID4. Then, hypernym-
hyponym relations are established between synony-
mous groups. We call this resulting data as syn-
onymy database. Figure 1 shows examples of syn-
onymous groups in the synonymy database. In this
paper, SYNID is denoted by using English gloss
word, surrounded by ? ? ? ?.
3 SYNGRAPH
3.1 SYNGRAPH Data Structure
SYNGRAPH data structure is an acyclic directed
graph, and the basis of SYNGRAPH is the depen-
dency structure of an original sentence (in this paper,
a robust parser (Kurohashi and Nagao, 1994) is al-
ways employed). In the dependency structure, each
node consists of one content word and zero or more
function words, which is called a basic node here-
after. If the content word of a basic node belongs to
a synonymous group, a new node with the SYNID is
attached to it, and it is called a SYN node hereafter.
For example, in Figure 2, the shaded nodes are basic
nodes and the other nodes are SYN nodes5.
Then, if the expression conjoining two or more
4Spelling variations such as use of Hiragana, Katakana
or Kanji are handled by the morphological analyzer JUMAN
(Kurohashi et al, 1994).
5The reason why we distinguish basic nodes from SYN
nodes is to give priority to exact matching over synonymous
matching.
789
hotel ni
<hotel> ni i c hi b a n( b es t)
<m os t> c hi k a i( n ea r )
<n ea r es t>
0.99
1 .0
0.99
0.99
1 .0
1 .0
m oy or i( n ea r es t)
0.99
1 .0
<n ea r es t>
N M S = 0 . 9 8
N M S = 0 . 9
ek i( s ta ti on ) w a N M S = 1 . 01 .0
ek i( s ta ti on ) w a1 .0
hotel no
<hotel> no0.99
1 .0
Figure 2: SYNGRAPH matching.
nodes corresponds to one synonymous group, a
SYN node is added there. In Figure 2, ?nearest? is
such a SYN node. Furthermore, if one SYN node
has a hyper synonymous group in the synonymy
database, the SYN node with the hyper SYNID is
also added.
In this SYNGRAPH data structure, each node has
a score, NS (Node Score), which reflects how much
the expression of the node is shifted from the orig-
inal expression. We explain how to calculate NSs
later.
3.2 SYNGRAPH Matching
Two SYNGRAPHs match if and only if
? all the nodes in one SYNGRAPH can be
matched to the nodes in the other one,
? the matched nodes in two SYNGRAPHs have
the same dependency structure, and
? the nodes can cover the original sentences.
An example of SYNGRAPH matching is illustrated
in Figure 2. When two SYNGRAPHs match each
other, their matching score is calculated as follows.
First, the matching score of the matching two nodes,
NMS (Node Match Score) is calculated with their
node scores, NS1 and NS2,
NMS = NS 1 ? NS 2 ? FI Penalty,
where we define FI Penalty (Function word Incon-
sistency Penalty) is 0.9 when their function words
are not the same, and 1.0 otherwise.
Then, the matching score of two SYNGRAPHs,
SMS (SYNGRAPH Match Score) is defined as the
average of NMSs weighted by the number of basic
nodes,
SMS =
?
(# of basic nodes ? NMS)?
# of basic nodes
.
In an example shown in Figure 2, the NMS of the
left-hand side hotel node and the right-hand side ho-
tel node is 0.9 (= 1.0 ? 1.0 ? 0.9). The NMS of the
left-hand side ?nearest? node and the right-hand side
?nearest? node is 0.98 (= 0.99 ? 0.99 ? 1.0). Then,
the SMS becomes 0.9?2+0.98?3+1.0?22+3+2 = 0.96.
3.3 SYNGRAPH Transformation of Synonymy
Database
The synonymy database is transformed into SYN-
GRAPHs, where SYNGRAPH matching is itera-
tively applied to interpret the mutual relationships
in the synonymy database, as follows:
Step 1: Each expression in each synonymous group
is parsed and transformed into a fundamental SYN-
GRAPH.
Step 2: SYNGRAPH matching is applied to check
whether a sub-tree of one expression is matched with
any other whole expressions. If there is a match, a
new node with the SYNID of the whole matched ex-
pression is assigned to the partially matched nodes
group. Furthermore, if the SYNID has a hyper syn-
onymous group, another new node with the hyper-
nym SYNID is also assigned. This checking process
starts from small parts to larger parts.
We define the NS of the newly assigned SYN
node as the SMS multiplied by a relation penalty.
Here, we define the synonymy relation penalty as
0.99 and the hypernym relation penalty as 0.7. For
instance, the NS of ?underwater? node is 0.99 and
that of ?inside? node is 0.7.
Step 3: Repeat Step 2, until no more new SYN node
can be assigned to any expressions. In the case of
Figure 3 example, the new SYN node, ?diving? is
given to ?suityu (underwater) ni (to) moguru (dive)?
of ?diving(sport)? at the second iteration.
4 Flexible Matching using SYNGRAPH
We use example-based machine translation (EBMT)
as an example to explain how our flexible matching
method works (Figure 4). EBMT generates a trans-
lation by combining partially matching TEs with an
input6. We use flexible matching to fully exploit the
TEs.
6How to select the best TEs and combine the selected TEs
for generating a translation is omitted in this paper.
790
ni
ni
ni
ni
sport
ni
ni
moguru(dive)
ni
<underwater>
<inside>
<diving>
0.99
0.7
0.99
1.0
1.0
diving1.0
<diving(sport)>
mizu(water)
suityu(underwater)
naka(inside)
1.0
1.0
1.0
<underwater>
<inside>
<inside>0.99
naka(inside)
<inside>
moguru(dive)
0.99
1.0
1.0
<inside>0.7
mizu(water)1.0
sensui(diving)1.0
<diving>Synonymy databaseTranslation example
naka(inside)
suityu(underwater) no
no
1.0
1.0
suru
sport
suru
<diving>
<diving(sport)>
 
 
sensui(diving)
0.99
1.0
0.93
1.0 <underwater> 0.99
Figure 3: SYNGRAPH transformation of synonymy database.input sentence translation examplestransform into a SYNGRAPH
Japanese English
Figure 4: Flexible matching using SYNGRAPH in
EBMT.
First, TEs are transformed into SYNGRAPHs by
SYNGRAPH matching with SYNGRAPHs of the
synonymy database. Since the synonymy database
has been transformed into SYNGRAPHs, we do not
need to care the combinations of synonymous ex-
pressions any more. In the example shown in Fig-
ure 3, ?sensui (diving) suru (do) sport? in the TE is
given ?diving(sport)? node just by looking at SYN-
GRAPHs in ?diving(sport)? synonymous group.
Then, an input sentence is transformed into a
SYNGRAPH by SYNGRAPH matching, and then
the SYNGRAPH matching is applied between all
the sub trees of the input SYNGRAPH and SYN-
GRAPHs of TEs to retrieve the partially matching
TEs.
5 Experiments and Discussion
5.1 Evaluation on Machine Translation Task
To see the effectiveness of the our proposed method,
we conducted our evaluations on a MT task us-
ing Japanese-English translation training corpus
(20,000 sentence pairs) and 506 test sentences of
IWSLT?057. As an evaluation measure, NIST and
BLEU were used based on 16 reference English sen-
tences for each test sentence.
7http://www.is.cs.cmu.edu/iwslt2005/.
Table 1: Size of synonymy database.
# of synonymous group 5,046
# of hypernym-hyponym relation 18,590
The synonymy database used in the experiments
was automatically extracted from the REIKAI-
SHOGAKU dictionary (a dictionary for children),
which consists of about 30,000 entries. Table
1 shows the size of the constructed synonymy
database.
As a base translation system, we used an EBMT
system developed by (Kurohashi et al, 2005). Ta-
ble 2 shows the experimental results. ?None? means
the baseline system without using the synonymy
database. ?Synonym? is the system using only
synonymous relations, and it performed best and
achieved 1.2% improvement for NIST and 0.8%
improvement for BLEU over the baseline. These
differences are statistically significant (p < 0.05).
Some TEs that can be retrieved by our flexible
matching are shown below:
? input: fujin (lady) you (for) toile (toilet) ?
TE: josei (woman) you (for) toile (toilet)
? input: kantan-ni ieba (in short)?TE: tsumari
(in other words)
On the other hand, if the system also uses
hypernym-hyponym relation (?Synonym Hyper-
nym?), the score goes down. It proves that hyper-
nym examples are not necessarily good for trans-
lation. For example, for a translation of depato
(department store), its hypernym ?mise(store)? was
used, and it lowered the score.
Major errors are caused by the deficiency of word
sense disambiguation. When a polysemic word oc-
curs in a sentence, multiple SYNIDs are attached
to the word, and thus, the incorrect matching might
be occurred. Incorporation of unsupervised word-
791
Table 2: Evaluation results on MT task.
Synonymy DB NIST BLEU
None 8.023 0.375
Synonym 8.121 0.378
Synonym Hypernym 8.010 0.374
Table 3: Evaluation results on IR task.
Method Synonymy DB R-prec
Best IREX system ? 0.493
BM25 ? 0.474
None 0.492
Our method Synonym 0.509
Synonym Hypernym 0.514
sense-disambiguation of words in dictionary defini-
tions and matching sentences is one of our future
research targets.
5.2 Evaluation on Information Retrieval Task
To demonstrate the effectiveness of our method
in other NLP tasks, we also evaluated it in IR.
More concretely, we extended word-based impor-
tance weighting of Okapi BM25 (Robertson et al,
1994) to SYN node-based weighting. We used the
data set of IR evaluation workshop IREX, which
contains 30 queries and their corresponding relevant
documents in 2-year volume of newspaper articles8.
Table 3 shows the experimental results, which are
evaluated with R-precision. The baseline system is
our implementation of OKAPI BM25. Differently
from the MT task, the system using both synonym
and hypernym-hyponym relations performed best,
and its improvement over the baseline was 7.8%
relative. This difference is statistically significant
(p < 0.05). This result shows the wide applicabil-
ity of our flexible matching method for NLP tasks.
Some examples that can be retrieved by our flexible
matching are shown below:
? query: gakkou-ni (school) computer-wo
(computer) dounyuu (introduce) ? docu-
ment: shou-gakkou-ni (elementary school)
pasokon-wo (personal computer) dounyuu
(introduce)
6 Conclusion
This paper proposed a flexible matching method by
extracting synonymous expressions from an ordi-
nary dictionary and a Web corpus, and introducing
SYNGRAPH data structure. We confirmed the ef-
fectiveness of our method on experiments of ma-
chine translation and information retrieval.
8http://nlp.cs.nyu.edu/irex/.
Our future research targets are to incorporate
word sense disambiguation to our framework, and
to extend SYNGRAPH matching to more structural
paraphrases.
References
Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence align-
ment. In HLT-NAACL 2003, pages 16?23.
Zellig Harris. 1968. Mathematical Structures of Language.
Wiley.
Christian Jacquemin, Judith L. Klavans, and Evelyne Tzouker-
mann. 1997. Expansion of multi-word terms for indexing
and retrieval using morphology and syntax. In 35th Annual
Meeting of the Association for Computational Linguistics,
pages 24?31.
J.R.Firth. 1957. A synopsis of linguistic theory, 1933-1957. In
Studies in Linguistic Analysis, pages 1?32. Blackwell.
Daisuke Kawahara and Sadao Kurohashi. 2001. Japanese case
frame construction by coupling the verb and its closest case
component. In Proc. of HLT 2001, pages 204?210.
Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame
compilation from the web using high-performance comput-
ing. In Proc. of LREC-06.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic anal-
ysis method of long japanese sentences based on the detec-
tion of conjunctive structures. Computational Linguistics,
20(4):507?534.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and
Makoto Nagao. 1994. Improvements of Japanese mor-
phological analyzer JUMAN. In Proc. of the International
Workshop on Sharable Natural Language, pages 22?28.
Sadao Kurohashi, Toshiaki Nakazawa, Kauffmann Alexis, and
Daisuke Kawahara. 2005. Example-based machine transla-
tion pursuing fully structural NLP. In Proc. of IWSLT?05,
pages 207?212.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question answering. Natural Language Engineer-
ing, 7(4):343?360.
Junichi Nakamura and Makoto Nagao. 1988. Extraction of se-
mantic information from an ordinary english dictionary and
its evaluation. In Proc. of the 12th COLING, pages 459?464.
S. E. Robertson, S. Walker, S. Jones, M.M. Hancock-Beaulieu,
and M. Gatford. 1994. Okapi at TREC-3. In the third Text
REtrieval Conference (TREC-3).
Hiroaki Tsurumaru, Toru Hitaka, and Sho Yoshida. 1986. An
attempt to automatic thesaurus construction from an ordinary
japanese language dictionary. In Proc. of the 11th COLING,
pages 445?447.
Ellen M. Voorhees. 1994. Query expansion using lexical-
semantic relations. In SIGIR, pages 61?69.
792
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 55?62,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Bottom-up Named Entity Recognition
using a Two-stage Machine Learning Method
Hirotaka Funayama Tomohide Shibata Sadao Kurohashi
Kyoto University, Yoshida-honmachi,
Sakyo-ku, Kyoto, 606-8501, Japan
{funayama, shibata, kuro}@nlp.kuee.kyoto-u.ac.jp
Abstract
This paper proposes Japanese bottom-up
named entity recognition using a two-
stage machine learning method. Most
work has formalized Named Entity Recog-
nition as a sequential labeling problem, in
which only local information is utilized
for the label estimation, and thus a long
named entity consisting of several mor-
phemes tends to be wrongly recognized.
Our proposed method regards a compound
noun (chunk) as a labeling unit, and first
estimates the labels of all the chunks in
a phrasal unit (bunsetsu) using a machine
learning method. Then, the best label as-
signment in the bunsetsu is determined
from bottom up as the CKY parsing al-
gorithm using a machine learning method.
We conducted an experimental on CRL
NE data, and achieved an F measure of
89.79, which is higher than previous work.
1 Introduction
Named Entity Recognition (NER) is a task of rec-
ognizing named entities such as person names,
organization names, and location. It is used for
several NLP applications such as Information Ex-
traction (IE) and Question Answering (QA). Most
work uses machine learning methods such as Sup-
port Vector Machines (SVMs) (Vapnik, 1995) and
Conditional Random Field (CRF) (Lafferty et al,
2001) using a hand-annotated corpus (Krishnan
and D.Manning, 2006; Kazama and Torisawa,
2008; Sasano and Kurohashi, 2008; Fukushima et
al., 2008; Nakano and Hirai, 2004; Masayuki and
Matsumoto, 2003).
In general, NER is formalized as a sequential
labeling problem. For example, regarding a mor-
pheme as a basic unit, it is first labeled as S-
PERSON, B-PERSON, I-PERSON, E-PERSON,
S-ORGANIZATION, etc. Then, considering the
labeling results of morphemes, the best NE label
sequence is recognized.
When the label of each morpheme is estimated,
only local information around the morpheme (e.g.,
the morpheme, the two preceding morphemes, and
the two following morphemes) is utilized. There-
fore, a long named entity consisting of several
morphemes tends to be wrongly recognized. Let
us consider the example sentences shown in Fig-
ure 1.
In sentence (1), the label of ?Kazama? can be
recognized to be S-PERSON (PERSON consist-
ing of one morpheme) by utilizing the surrounding
information such as the suffix ?san? (Mr.) and the
verb ?kikoku shita? (return home).
On the other hand, in sentence (2), when the
label of ?shinyou? (credit) is recognized to be
B-ORGANIZATION (the beginning of ORGA-
NIZATION), only information from ?hatsudou?
(invoke) to ?kyusai? (relief) can be utilized, and
thus the information of the morpheme ?ginkou?
(bank) that is apart from ?shinyou? by three mor-
phemes cannot be utilized. To cope with this prob-
lem, Nakano et al (Nakano and Hirai, 2004) and
Sasano et al (Sasano and Kurohashi, 2008) uti-
lized information of the head of bunsetsu1. In their
methods, when the label of ?shinyou? is recog-
nized, the information of the morpheme ?ginkou?
can be utilized.
However, these methods do not work when the
morpheme that we want to refer to is not a head
of bunsetsu as in sentence (3). In this example,
when ?gaikoku? (foreign) is recognized to be B-
ARTIFACT (the beginning of ARTIFACT), we
want to refer to ?hou? (law), not ?ihan? (viola-
tion), which is the head of the bunsetsu.
This paper proposes Japanese bottom-up named
1Bunsetsu is the smallest coherent phrasal unit in
Japanese. It consists of one or more content words followed
by zero or more function words.
55
(1) kikoku-shita
return home
Kazama-san-wa
Mr.Kazama TOP
. . .
?Mr. Kazama who returned home?
(2) hatsudou-shita
invoke
shinyou-kumiai-kyusai-ginkou-no
credit union relief bank GEN
setsuritsu-mo. . .
establishment
?the establishment of the invoking credit union relief bank?
(3) shibunsyo-gizou-to
private document falsification and
gaikoku-jin-touroku-hou-ihan-no
foreigner registration law violation GEN
utagai-de
suspicion INS
?on suspicion of the private document falsification and the violation of the foreigner registra-
tion law?
Figure 1: Example sentences.
entity recognition using a two-stage machine
learning method. Different from previous work,
this method regards a compound noun as a la-
beling unit (we call it chunk, hereafter), and es-
timates the labels of all the chunks in the bun-
setsu using a machine learning method. In sen-
tence (3), all the chunks in the second bunsetsu
(i.e., ?gaikoku?, ?gaikoku-jin?, ? ? ?, ?gaikoku-jin-
touroku-hou-ihan ?, ? ? ?, ?ihan?) are labeled, and
in the case that the chunk ?gaikoku-jin-touroku-
hou? is labeled, the information about ?hou? (law)
is utilized in a natural manner. Then, in the bun-
setsu, the best label assignment is determined. For
example, among the combination of ?gaikoku-jin-
touroku-hou? (ARTIFACT) and ?ihan? (OTHER),
the combination of ?gaikoku-jin? (PERSON) and
?touroku-hou-ihan? (OTHER), etc., the best la-
bel assignment, ?gaikoku-jin-touroku-hou? (AR-
TIFACT) and ?ihan? (OTHER), is chosen based
on a machine learning method. In this determi-
nation of the best label assignment, as the CKY
parsing algorithm, the label assignment is deter-
mined by bottom-up dynamic programming. We
conducted an experimental on CRL NE data, and
achieved an F measure of 89.79, which is higher
than previous work.
This paper is organized as follows. Section 2 re-
views related work of NER, especially focusing on
sequential labeling based method. Section 3 de-
scribes an overview of our proposed method. Sec-
tion 4 presents two machine learning models, and
Section 5 describes an analysis algorithm. Section
6 gives an experimental result.
2 Related Work
In Japanese Named Entity Recognition, the defi-
nition of Named Entity in IREX Workshop (IREX
class example
PERSON Kimura Syonosuke
LOCATION Taiheiyou (Pacific Ocean)
ORGANIZATION Jimin-tou (Liberal Democratic
Party)
ARTIFACT PL-houan (PL bill)
DATE 21-seiki (21 century)
TIME gozen-7-ji (7 a.m.)
MONEY 500-oku-en (50 billions yen)
PERCENT 20 percent
Table 1: NE classes and their examples.
Committee, 1999) is usually used. In this def-
inition, NEs are classified into eight classes:
PERSON, LOCATION, ORGANIZATION, AR-
TIFACT, DATE, TIME, MONEY, and PERCENT.
Table 1 shows example instances of each class.
NER methods are divided into two approaches:
rule-based approach and machine learning ap-
proach. According to previous work, machine
learning approach achieved better performance
than rule-based approach.
In general, a machine learning method is for-
malized as a sequential labeling problem. This
problem is first assigning each token (character or
morpheme) to several labels. In an SE-algorithm
(Sekine et al, 1998), S is assigned to NE com-
posed of one morpheme, B, I, E is assigned to the
beginning, middle, end of NE, respectively, and O
is assigned to the morpheme that is not an NE2.
The labels S, B, I, and E are prepared for each NE
classes, and thus the total number of labels is 33
(= 8 * 4 + 1).
The model for the label estimation is learned
based on machine learning. The following fea-
tures are generally utilized: characters, type of
2Besides, there are IOB1, IOB2 algorithm using only
I,O,B and IOE1, IOE2 algorithm using only I,O,E (Kim and
Veenstra, 1999).
56
HabuPERSON 0.111
Habu-YoshiharuPERSON 0.438
Habu-Yoshiharu-MeijinORGANIZATION0.083
Yoshiharu Yoshiharu-MeijinMONEY0.075 OTHERe0.092
MeijinOTHERe0.245
(a):initial state
HabuPERSON 0.111
Habu-YoshiharuPERSON 0.438
Habu-Yoshiharu + MeijinPSN+OTHERe0.438+0.245
Yoshiharu Yoshiharu + Meijin
final outputanalysis direction
MONEY0.075 MNY+OTHERe0.075+0.245
MeijinOTHERe0.245
(b):final output
Figure 2: An overview of our proposed method. (the bunsetsu ?Habu-Yoshiharu-Meijin?)
character, POS, etc. about the morpheme and the
surrounding two morphemes. The methods utiliz-
ing SVM or CRF are proposed.
Most of NER methods based on sequential la-
beling use only local information. Therefore,
methods utilizing global information are pro-
posed. Nakano et al utilized as a feature the word
sub class of NE on the analyzing direction in the
bunsetsu, the noun in the end of the bunsetsu ad-
jacent to the analyzing direction, and the head of
each bunsetsu (Nakano and Hirai, 2004). Sasano
et al utilized cache feature, coreference result,
syntactic feature, and caseframe feature as struc-
tural features (Sasano and Kurohashi, 2008).
Some work acquired knowledge from unan-
notated large corpus, and applied it to NER.
Kazama et al utilized a Named Entity dic-
tionary constructed from Wikipedia and a noun
clustering result obtained using huge amount of
pairs of dependency relations (Kazama and Tori-
sawa, 2008). Fukushima et al acquired huge
amount of category-instance pairs (e.g., ?po-
litical party - New party DAICHI?,?company-
TOYOTA?) by some patterns from a large Web
corpus (Fukushima et al, 2008).
In Japanese NER researches, CRL NE data are
usually utilized for the evaluation. This data in-
cludes approximately 10 thousands sentences in
news paper articles, in which approximately 20
thousands NEs are annotated. Previous work
achieved an F measure of about 0.89 using this
data.
3 Overview of Proposed Method
Our proposed method first estimates the label of
all the compound nouns (chunk) in a bunsetsu.
Then, the best label assignment is determined
by bottom-up dynamic programming as the CKY
parsing algorithm. Figure 2 illustrates an overview
of our proposed method. In this example, the
bunsetsu ?Habu-Yoshiharu-Meijin? (Grand Mas-
ter Yoshiharu Habu) is analyzed. First, the labels
of all the chunks (?Habu?, ?Habu-Yoshiharu?,
?Habu-Yoshiharu-Meijin?, ? ? ?, ?Meijin?, etc.) in
the bunsetsu are analyzed using a machine learn-
ing method as shown in Figure 2 (a).
We call the state in Figure 2 (a) initial state,
where the labels of all the chunks have been es-
timated. From this state, the best label assign-
ment in the bunsetsu is determined. This pro-
cedure is performed from the lower left (corre-
sponds to each morpheme) to the upper right like
the CKY parsing algorithm as shown in Figure 2
(b). For example, when the label assignment for
?Habu-Yoshiharu? is determined, the label assign-
ment ?Habu-Yoshiharu? (PERSON) and the label
assignment ?Habu? (PERSON) and ?Yoshiharu?
(OTHER) are compared, and the better one is cho-
sen. While grammatical rules are utilized in a
general CKY algorithm, this method chooses bet-
ter label assignment for each cell using a machine
learning method.
The learned models are the followings:
? the model that estimates the label of a chunk
(label estimation model)
? the model that compares two label assign-
ments (label comparison model)
The two models are described in detail in the
next section.
57
Habu Yoshiharu Meijin ga
PERSON OTHERe
invalid invalid
invalid
invalid
Figure 3: Label assignment for all the chunks in
the bunsetsu ?Habu-Yoshiharu-Meijin.?
4 Model Learning
4.1 Label Estimation Model
This model estimates the label for each chunk. An
analysis unit is basically bunsetsu. This is because
93.5% of named entities is located in a bunsetsu
in CRL NE data. Exceptionally, the following ex-
pressions located in multiple bunsetsus tend to be
an NE:
? expressions enclosed in parentheses (e.g., ?
?Himeyuri-no tou? ? (The tower of Himeyuri)
(ARTIFACT))
? expressions that have an entry in Wikipedia
(e.g., ?Nihon-yatyou-no kai? (Wild Bird So-
ciety of Japan) (ORGANIZATION))
Hereafter, bunsetsu is expanded when one of the
above conditions meet. By this expansion, 98.6%
of named entities is located in a bunsetsu3.
For each bunsetsu, the head or tail function
words are deleted. For example, in the bun-
setsu ?Habu-Yoshiharu-Meijin-wa?, the tail func-
tion word ?wa? (TOP) is deleted. In the bunsetsu
?yaku-san-bai? (about three times), the head func-
tion word ?yaku? (about) is deleted.
Next, for learning the label estimation model,
all the chunks in a bunsetsu are attached to the cor-
rect label from a hand-annotated corpus. The la-
bel set is 13 classes, which includes eight NE class
(as shown in Table 1), and five classes: OTHERs,
OTHERb, OTHERi, OTHERe, and invalid.
The chunk that corresponds to a whole bun-
setsu and does not contain any NEs is labeled
as OTHERs, and the head, middle, tail chunk
that does not correspond to an NE is labeled as
OTHERb, OTHERi, OTHERe, respectively4.
3As an example in which an NE is not included by an
expanded bunsetsu, there are ?Toru-no Kimi? (PERSON)
and ?Osaka-fu midori-no kankyo-seibi-shitsu? (ORGANI-
ZATION).
4Each OTHER is assigned to the longest chunk that satis-
fies its condition in a chunk.
1. # of morphemes in the chunk
2. the position of the chunk in its bunsetsu
3. character type5
4. the combination of the character type of adjoining
morphemes
- For the chunk ?Russian Army?, this feature is
?Katakana,Kanji?
5. word class, word sub class, and several features pro-
vided by a morphological analyzer JUMAN
6. several features6 provided by a parser KNP
7. string of the morpheme in the chunk
8. IPADIC7 feature
- If the string of the chunk are registered in the fol-
lowing categories of IPADIC: ?person?, ?lo-
cation?, ?organization?, and ?general?, this
feature fires.
9. Wikipedia feature
- If the string of the chunk has an entry in
Wikipedia, this feature fires.
- the hypernym extracted from its definition sen-
tence using some patterns (e.g., The hyper-
nym of ?the Liberal Democratic Party? is a
political party.)
10. cache feature
- When the same string of the chunk appears in the
preceding context, the label of the preceding
chunk is used for the feature.
11. particles that the bunsetsu includes
12. the morphemes, particles, and head morpheme in the
parent bunsetsu
13. the NE/category ratio in a case slot of predicate/noun
case frame(Sasano and Kurohashi, 2008)
- For example, in the case ga (NOM) of the pred-
icate case frame ?kaiken? (interview), the NE
ratio ?PERSON:0.245? is assigned to the case
slot. Hence, in the sentence ?Habu-ga kaiken-
shita? (Mr. Habu interviewed), the feature
?PERSON:0.245? is utilized for the chunk
?Habu.?
14. parenthesis feature
- When the chunk in a parenthesis, this feature
fires.
Table 2: Features for the label estimation model.
The chunk that is neither any eight NE class nor
the above four OTHER is labeled as invalid.
In an example as shown in Figure 3, ?Habu-
Yoshiharu? is labeled as PERSON, ?Meijin? is la-
beled as OTHERe, and the other chunks are la-
beled as invalid.
Next, the label estimation model is learned from
the data in which the above label set is assigned
5The following five character types are considered: Kanji,
Hiragana, Katakana, Number, and Alphabet.
6When a morpheme has an ambiguity, all the correspond-
ing features fire.
7http://chasen.aist-nara.ac.jp/chasen/distribution.html.ja
58
to all the chunks. The features for the label esti-
mation model are shown in Table 2. Among the
features, as for feature (3), (5)?(8), three cate-
gories according to the position of a morpheme
in the chunk are prepared: ?head?, ?tail?, and
?anywhere.? For example, in the chunk ?Habu-
Yoshiharu-Meijin,? as for the morpheme ?Habu?,
feature (7) is set to be ?Habu? in ?head? and as for
the morpheme ?Yoshiharu?, feature (7) is set to be
?Yoshiharu? in ?anywhere.?
The label estimation model is learned from pairs
of label and feature in each chunk. To classify the
multi classes, the one-vs-rest method is adopted
(consequently, 13 models are learned). The SVM
output is transformed by using the sigmoid func-
tion 11+exp(??x) , and the transformed value is nor-
malized so that the sum of the value of 13 labels
in a chunk is one.
The purpose for setting up the label ?invalid? is
as follows. In the chunk ?Habu? and ?Yoshiharu?
in Figure 3, since the label ?invalid? has a rela-
tively higher score, the score of the label PERSON
is relatively low. Therefore, when the label com-
parison described in Section 4.2 is performed, the
label assignment ?Habu-Yoshiharu? (PERSON) is
likely to be chosen. In the chunk where the score
of the label invalid has the highest score, the label
that has the second highest score is adopted.
4.2 Label Comparison Model
This model compares the two label assignments
for a certain string. For example, in the string
?Habu-Yoshiharu?, the model compares the fol-
lowing two label assignments:
? ?Habu-Yoshiharu? is labeled as PERSON
? ?Habu? is labeled as PERSON and ?Yoshi-
haru? is labeled as MONEY
First, as shown in Figure 4, the two compared
sets of chunks are lined up by sandwiching ?vs.?
(The left one, right one is called the first set, the
second set, respectively.) When the first set is cor-
rect, this example is positive: otherwise, this ex-
ample is negative. The max number of chunks for
each set is five, and thus examples in which the
first or second set has more than five chunks are
not utilized for the model learning.
Then, the feature is assigned to each example.
The feature (13 dimensions) for each chunk is de-
fined as follows: the first 12 dimensions are used
positive:
+1 Habu-Yoshiharu vs Habu + Yoshiharu
PSN PSN + MNY
+1 Habu-Yoshiharu + Meijin vs Habu + Yoshiharu + Meijin
PSN + OTHERe PSN + MONEY + OTHERe
...
negative:
- 1 Habu-Yoshiharu-Meijin vs Habu-Yoshiharu + Meijin
ORG PSN + OTHERe
...
Figure 4: Assignment of positive/negative exam-
ples.
for each label, which is estimated by the label esti-
mation model, and the last 13th dimension is used
for the score of an SVM output. Then, for the first
and second set, the features for each chunk are ar-
ranged from the left, and zero vectors are placed
in the remainder part.
Figure 5 illustrates the feature for ?Habu-
Yoshiharu? vs ?Habu + Yoshiharu.? The label
comparison model is learned from such data us-
ing SVM. Note that only the fact that ?Habu-
Yoshiharu? is PERSON can be found from the
hand-annotated corpus, and thus in the example
?Habu-Yoshiharu-Meijin? vs ?Habu + Yoshiharu-
Meijin?, we cannot determine which one is cor-
rect. Therefore, such example cannot be used for
the model learning.
5 Analysis
First, the label of all the chunks in a bunsetsu is
estimated by using the label estimation model de-
scribed in Section 4.1. Then, the best label assign-
ment in the bunsetsu is determined by applying the
label comparison model described in Section 4.2
iteratively as shown in Figure 2 (b). In this step,
the better label assignment is determined from bot-
tom up as the CKY parsing algorithm.
For example, the initial state shown in Figure
2(a) is obtained using the label estimation model.
Then, the label assignment is determined using the
label comparison model from the lower left (cor-
responds to each morpheme) to the upper right.
In determining the label assignment for the cell
of ?Habu-Yoshiharu? as shown in 6(a), the model
compares the label assignment ?B? with the la-
bel assignment ?A+D.? In this case, the model
chooses the label assignment ?B?, that is, ?Habu
- Yoshiharu? is labeled as PERSON. Similarly,
in determining the label assignment for the cell
of ?Yoshiharu-Meijin?, the model compares the
59
chunk Habu-Yoshiharu Habu Yoshiharu
label PERSON PERSON MONEY
vector V11 0 0 0 0 V21 V22 0 0 0
Figure 5: An example of the feature for the label comparison model. (The example is ?Habu-Yoshiharu
vs Habu + Yoshiharu?, and V11, V21, V22, and 0 is a vector whose dimension is 13.)
HabuPERSON 0.111
Habu-YoshiharuPERSON 0.438
Habu-Yoshiharu-MeijinORGANIZATION0.083Yoshiharu Yoshiharu-Meijin
label assighment ?Habu-Yoshiharu?
label assignment
?Habu? + ?Yoshiharu?
A B C
EDMONEY0.075 OTHERe0.092
MeijinOTHERe0.245
F
(a): label assignment for the cell ?Habu-Yoshiharu?.
HabuPERSON 0.111
Habu-YoshiharuPERSON 0.438
Habu-Yoshiharu-MeijinORGANIZATION0.083Yoshiharu Yoshiharu + Meijin
label assignment
?Habu-Yoshiharu-Meijin?
A B C
EDMONEY0.075 MNY+OTHERe0.075+0.245
MeijinOTHERe0.245
label assignment
?Habu? + ?Yoshiharu? + ?Meijin?
label assignment
?Habu-Yoshiharu? + ?Meijin?
F
(b): label assignment for the cell ?Habu-Yoshiharu-Meijin?.
Figure 6: The label comparison model.
label assignment ?E? with the label assignment
?D+F.? In this case, the model chooses the label
assignment ?D+F?, that is, ?Yoshiharu? is labeled
as MONEY and ?Meijin? is labeled as OTHERe.
When the label assignment consists of multiple
chunks, the content of the cell is updated. In
this case, the cell ?E? is changed from ?Yoshi-
haru-Meijin? (OTHERe) to ?Yoshiharu + Meijin?
(MONEY + OTHERe).
As shown in Figure 6(b), in determining the best
label assignment for the upper right cell, that is,
the final output is determined, the model compares
the label assignment ?A+D+F?, ?B+F?, and ?C?.
When there are more than two candidates of label
assignments for a cell, all the label assignments are
compared in a pairwise, and the label assignment
that obtains the highest score is adopted.
In the label comparing step, the label as-
signment in which OTHER? follows OTHER?
(OTHER? - OTHER?) is not allowed since each
OTHER is assigned to the longest chunk as de-
scribed in Section 4.1. When the first combina-
tion of chunks equals to the second combination
of chunks, the comparison is not performed.
6 Experiment
To demonstrate the effectiveness of our proposed
method, we conducted an experiment on CRL NE
data. In this data, 10,718 sentences in 1,174 news
articles are annotated with eight NEs. The expres-
sion to which it is difficult to annotate manually is
labeled as OPTIONAL, and was not used for both
a b c d e
learn the label estimation model 1
Corpus: CRL NE data
learn the label estimation model 2bobtain features for the label apply
b c d e
b c d e
b c d e
comparison model
learn the label estimation model 2c
learn the label estimation model 2d
learn the label estimation model 2e
learn the label comparison model
Figure 7: 5-fold cross validation.
the model learning8 and the evaluation.
We performed 5-fold cross validation following
previous work. Different from previous work, our
work has to learn the SVM models twice. There-
fore, the corpus was divided as shown in Figure 7.
Let us consider the analysis in the part (a). First,
the label estimation model 1 is learned from the
part (b)-(e). Then, the label estimation model 2b
is learned from the part (c)-(e), and applying the
learned model to the part (b), features for learning
the label comparison model are obtained. Simi-
larly, the label estimation model 2c is learned from
the part (b),(d),(e), and applying it to the part (c),
features are obtained. It is the same with the part
8Exceptionally, ?OPTIONAL? is used when the label es-
timation model for OTHER? and invalid is learned.
60
Recall Precision
ORGANIZATION 81.83 (3008/3676) 88.37 (3008/3404)
PERSON 90.05 (3458/3840) 93.87 (3458/3684)
LOCATION 91.38 (4992/5463) 92.44 (4992/5400)
ARTIFACT 46.72 ( 349/ 747) 74.89 ( 349/ 466)
DATE 93.27 (3327/3567) 93.12 (3327/3573)
TIME 88.25 ( 443/ 502) 90.59 ( 443/ 489)
MONEY 93.85 ( 366/ 390) 97.60 ( 366/ 375)
PERCENT 95.33 ( 469/ 492) 95.91 ( 469/ 489)
ALL-SLOT 87.87 91.79
F-measure 89.79
Table 3: Experimental result.
(d) and (e). Then, the label comparison model is
learned from the obtained features. After that, the
analysis in the part (a) is performed by using both
the label estimation model 1 and the label compar-
ison model.
In this experiment, a Japanese morphological
analyzer, JUMAN9, and a Japanese parser, KNP10
were adopted. The two SVM models were learned
with polynomial kernel of degree 2, and ? in the
sigmoid function was set to be 1.
Table 6 shows an experimental result. An F-
measure in all NE classes is 89.79.
7 Discussion
7.1 Comparison with Previous Work
Table 7 presents the comparison with previ-
ous work, and our method outperformed previ-
ous work. Among previous work, Fukushima
et al acquired huge amount of category-
instance pairs (e.g., ?political party - New party
DAICHI?,?company-TOYOTA?) by some patterns
from a large Web corpus, and Sasano et al uti-
lized the analysis result of corefer resolution as a
feature for the model learning. Therefore, in our
method, by incorporating these knowledge and/or
such analysis result, the performance would be im-
proved.
Compared with Sasano et al, our method
achieved the better performance in analyzing
a long compound noun. For example, in the
bunsetsu ?Oushu-tsuujyou-senryoku-sakugen-
jyouyaku? (Treaty on Conventional Armed
Forces in Europe), while Sasano et al labeled
?Oushu? (Europe) as LOCATION, our method
correctly labeled ?Oushu-tsuujyou-senryoku-
sakugen-jyouyaku? as ARTIFACT. Sasano et
al. incorrectly labeled ?Oushu? as LOCATION
although they utilized the information about
9http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
10http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html
the head of bunsetsu ?jyouyaku? (treaty). In
our method, for the cell ?Oushu?, invalid has
the highest score, and thus the score of LOCA-
TION relatively drops. Similarly, for the cell
?senryoku-sakugen-jyouyaku?, invalid has the
highest score. Consequently, ?Oushu-tsuujyou-
senryoku-sakugen-jyouyaku? is correctly labeled
as ARTIFACT.
In the bunsetsu ?gaikoku-jin-touroku-hou-ihan?
(the violation of the foreigner registration law),
while Sasano et al labeled ?touroku-hou? as AR-
TIFACT, our method correctly labeled ?gaikoku-
jin-touroku-hou? as ARTIFACT. Sasano et al can-
not utilize the information about ?hou? that is use-
ful for the label estimation since the head of this
bunsetsu is ?ihan.? In contrast, in estimating the
label of the chunk ?gaikoku-jin-touroku-hou?, the
information of ?hou? can be utilized.
7.2 Error Analysis
There were some errors in analyzing a Katakana
alphabet word. In the following example, although
the correct is that ?Batistuta? is labeled as PER-
SON, the system labeled it as OTHERs.
(4) Italy-de
Italy LOC
katsuyaku-suru
active
Batistuta-wo
Batistuta ACC
kuwaeta
call
Argentine
Argentine
?Argentine called Batistuta who was active in
Italy.?
There is not an entry of ?Batistuta? in the dictio-
nary of JUMAN nor Wikipedia, and thus only the
surrounding information is utilized. However, the
case analysis of ?katsuyaku? (active) is incorrect,
which leads to the error of ?Batistuta?.
There were some errors in applying the la-
bel comparison model although the analysis of
each chunk is correct. For example, in the
bunsetsu ?HongKong-seityou? (Government of
HongKong), the correct is that ?HongKong-
seityou? is labeled as ORGANIZATION. As
shown in Figure 8 (b), the system incorrectly
labeled ?HongKong? as LOCATION. As shown
in Figure 8(a), although in the initial state,
?HongKong-seityou? was correctly labeled as OR-
GANIZATION, the label assignment ?HongKong
+ seityou? was incorrectly chosen by the label
comparison model. To cope with this problem,
we are planning to the adjustment of the value ?
in the sigmoid function and the refinement of the
61
F1 analysis unit distinctive features
(Fukushima et al, 2008) 89.29 character Web
(Kazama and Torisawa, 2008) 88.93 character Wikipedia,Web
(Sasano and Kurohashi, 2008) 89.40 morpheme structural information
(Nakano and Hirai, 2004) 89.03 character bunsetsu feature
(Masayuki and Matsumoto, 2003) 87.21 character
(Isozaki and Kazawa, 2003) 86.77 morpheme
proposed method 89.79 compound noun Wikipedia,structural information
Table 4: Comparison with previous work. (All work was evaluated on CRL NE data using cross valida-
tion.)
HongKongLOCATION HongKong-seityouORGANIZATION0.266 0.205
seityouOTHERe0.184
(a):initial state
HongKongLOCATION HongKong + seityouLOC+OTHERe0.266 0.266+0.184
seityouOTHERe0.184
(b):the final output
Figure 8: An example of the error in the label com-
parison model.
features for the label comparison model.
8 Conclusion
This paper proposed bottom-up Named Entity
Recognition using a two-stage machine learning
method. This method first estimates the label of
all the chunks in a bunsetsu using a machine learn-
ing, and then the best label assignment is deter-
mined by bottom-up dynamic programming. We
conducted an experiment on CRL NE data, and
achieved an F-measure of 89.79.
We are planning to integrate this method with
the syntactic and case analysis method (Kawa-
hara and Kurohashi, 2007), and perform syntactic,
case, and Named Entity analysis simultaneously to
improve the overall accuracy.
References
Ken?ichi Fukushima, Nobuhiro Kaji, and Masaru
Kitsuregawa. 2008. Use of massive amounts
of web text in Japanese named entity recogni-
tion. In Proceedings of Data Engineering Workshop
(DEWS2008). A3-3 (in Japanese).
IREX Committee, editor. 1999. Proceedings of the
IREX Workshop.
Hideki Isozaki and Hideto Kazawa. 2003. Speeding up
support vector machines for named entity recogni-
tion. Transaction of Information Processing Society
of Japan, 44(3):970?979. (in Japanese).
Daisuke Kawahara and Sadao Kurohashi. 2007. Prob-
abilistic coordination disambiguation in a fully-
lexicalized Japanese parser. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL2007),
pages 304?311.
Jun?ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT, pages 407?415.
Erik F. Tjong Kim and Jorn Veenstra. 1999. Repre-
senting text chunks. In Proceedings of EACL ?99,
pages 173?179.
Vajay Krishnan and Christopher D.Manning. 2006.
An effective two-stage model for exploiting non-
local dependencies in named entity recognition.
pages 1121?1128.
John Lafferty, Andrew McCallun, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference (ICML?01), pages 282?289.
Asahara Masayuki and Yuji Matsumoto. 2003.
Japanese named entity extraction with redundant
morphological analysis. In Proceeding of HLT-
NAACL 2003, pages 8?15.
Keigo Nakano and Yuzo Hirai. 2004. Japanese named
entity extraction with bunsetsu features. Transac-
tion of Information Processing Society of Japan,
45(3):934?941. (in Japanese).
Ryohei Sasano and Sadao Kurohashi. 2008. Japanese
named entity recognition using structural natural
language processing. In Proceeding of Third In-
ternational Joint Conference on Natural Language
Processing, pages 607?612.
Satoshi Sekine, Ralph Grishman, and Hiroyuki Shin-
nou. 1998. A decision tree method for finding and
classifying names in japanese texts. In Proceed-
ings of the Sixth Workshop on Very Large Corpora
(WVLC-6), pages 171?178.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
62
Automatic Slide Generation Based on Discourse
Structure Analysis
Tomohide Shibata and Sadao Kurohashi
Graduate School of Information Science and Technology, University of Tokyo,
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
Abstract. In this paper, we describe a method of automatically gener-
ating summary slides from a text. The slides are generated by itemizing
topic/non-topic parts that are extracted from the text based on syntac-
tic/case analysis. The indentations of the items are controlled according
to the discourse structure, which is detected by cue phrases, identifica-
tion of word chain and similarity between two sentences. Our experiments
demonstrates generated slides are far easier to read in comparison with
original texts.
1 Introduction
A presentation with slides is so effective to pass information to people in many
situations, such as an academic conference or business. Although some softwares,
such as PowerPoint and Keynote, help us with making presentation slides, it is
still cumbersome to make them from scratch.
Some researchers have developed a method of (semi-)automatically making
presentation slides from a technical paper or a news article [1, 2]. However, input
texts of their systems were supposed to be documents whose structure is anno-
tated: in [1], TEX source and in [2], semantically annotated documents by GDA
tag.
In this paper, we propose a method of automatically generating summary
slides from a raw text. An example of a text is shown in Figure 1 and an example
slide that is generated from the text is shown in Figure 2 (the translated slide
is shown in Figure 3). In a slide, topic/non-topic parts that are extracted from
the original text are itemized and each item is indented based on the discourse
structure of the text. In particular, a big contrast/list structure in the text is an
important clue for producing an easy-to-read slide.
The outline of our procedure is as follows:
1. Inputsentences are processedbyJapanese morphologicalanalyzer,JUMAN[3],
and are parsed by Japanese syntactic analyzer, KNP [4].
2. Each sentence is segmented into clauses and the discourse structure of the
text is analyzed.
3. Topic/non-topic parts are extracted from the text.
4. Summary slides are generated by displaying the topic/non-topic parts based
on the discourse structure.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 754?766, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
{shibata, kuro}@kc.t.u-tokyo.ac.jp
Fig. 1. An example of a text
Fig. 2. An example of a slide
Our method not only helps us with making presentation slides but also creates
a full-automatic presentation. That is to say, input texts are spoken via text-
to-speech engine while presenting automatically generated summary slides. We
call this system ?text-to-presentation?, as illustrated in Figure 4. For the input
of text-to-speech engine, written texts are not appropriate, because unnatural
speech might be produced due to difficult words or long compound nouns, which
are unsuitable for speech synthesis. Therefore, written texts are automatically
converted into spoken texts based on paraphrasing technique [5, 6] and then are
inputted into speech synthesis.
The rest of this paper is organized as follows. Section 2 introduces how to
analyze discourse structure. Section 3 explains how to extract topic/non-topic
parts and Section 4 introduces how to generate a slide. Next, in Section 5, we
describe our implemented system and report the evaluation. Finally, in Section
6, we discuss the related work, and in Section 7, we conclude this paper.
Automatic Slide Generation Based on Discourse Structure Analysis 755
Railway Recovery (1)
? Interruption of the three train services, JR Kobe-line, Hankyu Express Kobe-
line and Hanshin Electric Railway
? 450,000 people per day, 120,000 people per hour at the peak of rush,
had no transportation
? Interruption sections in West Japan Railway Toukaidou Line, Sannyou Line,
Hankyu Takarazuka, Imazu and Itami Line and Kobe-Electric Arima-line
? after the earthquake occurred
? transportation by alternate-bus was provided
? from January 23th, when National Route 2 was opened
? transportation by alternate-bus between Osaka and Kobe was pro-
vided
? from January 28th
? the alternate-bus priority lane was set up and smooth transporta-
tion was maintained.
Fig. 3. An example of a slide (in English)
	
	   
      	   	 		    	 
	


Fig. 4. Overview of text-to-presentation system
2 Discourse Structure Analysis
2.1 Model of Discourse Structure
We consider a clause and a sentence as a discourse unit and take into account
the following two types of coherence relations:
1. coherence relations between two clauses in a sentence (four types)
list, contrast, additive, adversative
2. coherence relations between two sentences (eight types)
list, contrast, topic-chaining, topic-dominant chaining, elaboration,
reason, cause, example
An example of discourse structure is shown in Figure 5. The arrows mean
connection of clauses/sentences and the labels on the arrows mean coherence
relation.
In our model, as an initial state, the discourse structure has a starting node.
A sentence connecting to the starting node has the coherence relation ?start?,
which means this sentence is the beginning of a new topic.
756 T. Shibata and S. Kurohashi
 
     	 







      	   

        	    	   
   
 	 
  	        
 	 
  	
 	 
  	
   	      	  
  	          Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 755?762,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Topic Identification by Integrating Linguistic and
Visual Information Based on Hidden Markov Models
Tomohide Shibata
Graduate School of Information Science
and Technology, University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo, 113-8656, Japan
shibata@kc.t.u-tokyo.ac.jp
Sadao Kurohashi
Graduate School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto, 606-8501, Japan
kuro@i.kyoto-u.ac.jp
Abstract
This paper presents an unsupervised topic
identification method integrating linguis-
tic and visual information based on Hid-
den Markov Models (HMMs). We employ
HMMs for topic identification, wherein a
state corresponds to a topic and various
features including linguistic, visual and
audio information are observed. Our ex-
periments on two kinds of cooking TV
programs show the effectiveness of our
proposed method.
1 Introduction
Recent years have seen the rapid increase of mul-
timedia contents with the continuing advance of
information technology. To make the best use
of multimedia contents, it is necessary to seg-
ment them into meaningful segments and annotate
them. Because manual annotation is extremely ex-
pensive and time consuming, automatic annotation
technique is required.
In the field of video analysis, there have been
a number of studies on shot analysis for video
retrieval or summarization (highlight extraction)
using Hidden Markov Models (HMMs) (e.g.,
(Chang et al, 2002; Nguyen et al, 2005; Q.Phung
et al, 2005)). These studies first segmented videos
into shots, within which the camera motion is con-
tinuous, and extracted features such as color his-
tograms and motion vectors. Then, they classi-
fied the shots based on HMMs into several classes
(for baseball sports video, for example, pitch view,
running overview or audience view). In these
studies, to achieve high accuracy, they relied on
handmade domain-specific knowledge or trained
HMMs with manually labeled data. Therefore,
they cannot be easily extended to new domains
on a large scale. In addition, although linguistic
information, such as narration, speech of charac-
ters, and commentary, is intuitively useful for shot
analysis, it is not utilized by many of the previous
studies. Although some studies attempted to uti-
lize linguistic information (Jasinschi et al, 2001;
Babaguchi and Nitta, 2003), it was just keywords.
In the field of Natural Language Processing,
Barzilay and Lee have recently proposed a prob-
abilistic content model for representing topics and
topic shifts (Barzilay and Lee, 2004). This content
model is based on HMMs wherein a state corre-
sponds to a topic and generates sentences relevant
to that topic according to a state-specific language
model, which are learned from raw texts via anal-
ysis of word distribution patterns.
In this paper, we describe an unsupervised topic
identification method integrating linguistic and vi-
sual information using HMMs. Among several
types of videos, in which instruction videos (how-
to videos) about sports, cooking, D.I.Y., and oth-
ers are the most valuable, we focus on cooking
TV programs. In an example shown in Figure 1,
preparation, sauteing, and dishing up are automat-
ically labeled in sequence. Identified topics lead to
video segmentation and can be utilized for video
summarization.
Inspired by Barzilay?s work, we employ HMMs
for topic identification, wherein a state corre-
sponds to a topic, like preparation and frying, and
various features, which include visual and audio
information as well as linguistic information (in-
structor?s utterances), are observed. This study
considers a clause as an unit of analysis and the
following eight topics as a set of states: prepara-
tion, sauteing, frying, baking, simmering, boiling,
dishing up, steaming.
In Barzilay?s model, although domain-specific
755
cut:1 saute:1 add:3 put:2
preparation sauteing dishing up
?
?
?
?
?
?
?
?
preparation
sauteing
dishing up
silencecue phrase
?then?
t
Cut an avocado. We?ll saute. Add spices.
identified
topic:
hidden
states
observed
data
utterance
case frame
image
Put cheese between 
slices of bread.
Figure 1: Topic identification with Hidden Markov Models.
word distribution can be learned from raw texts,
their model cannot utilize discourse features, such
as cue phrases and lexical chains. We incorpo-
rate domain-independent discourse features such
as cue phrases, noun/verb chaining, which indicate
topic change/persistence, into the domain-specific
word distribution.
Our main claim is that we utilize visual and au-
dio information to achieve robust topic identifi-
cation. As for visual information, we can utilize
background color distribution of the image. For
example, frying and boiling are usually performed
on a gas range and preparation and dishing up are
usually performed on a cutting board. This infor-
mation can be an aid to topic identification. As for
audio information, silence can be utilized as a clue
to a topic shift.
2 Related Work
In Natural Language Processing, text segmenta-
tion tasks have been actively studied for infor-
mation retrieval and summarization. Hearst pro-
posed a technique called TextTiling for subdivid-
ing texts into sub-topics (Hearst.M, 1997). This
method is based on lexical co-occurrence. Galley
et al presented a domain-independent topic seg-
mentation algorithm for multi-party speech (Gal-
ley et al, 2003). This segmentation algorithm
uses automatically induced decision rules to com-
bine linguistic features (lexical cohesion and cue
phrases) and speech features (silences, overlaps
and speaker change). These studies aim just at
segmenting a given text, not at identifying topics
of segmented texts.
Marcu performed rhetorical parsing in the
framework of Rhetorical Structure Theory (RST)
based on a discourse-annotated corpus (Marcu,
2000). Although this model is suitable for ana-
lyzing local modification in a text, it is difficult for
this model to capture the structure of topic transi-
tion in the whole text.
In contrast, Barzilay and Lee modeled a con-
tent structure of texts within specific domains,
such as earthquake and finance (Barzilay and Lee,
2004). They used HMMs wherein each state cor-
responds to a distinct topic (e.g., in earthquake
domain, earthquake magnitude or previous earth-
quake occurrences) and generates sentences rel-
evant to that topic according to a state-specific
language model. Their method first create clus-
ters via complete-link clustering, measuring sen-
tence similarity by the cosine metric using word
bigrams as features. They calculate initial proba-
bilities: state si specific language model ps
i
(w?|w)
756
????????? (Cut a Chinese cabbage.)
???????????????? (Cut off its root and wash it.)
???????????????(A Japanese radish would taste delicious.)
??3???????? (Divide it into three equal parts.)
?????????? (Now, we'll  saute.)
??
[individual action]
[individual action] [individual action]
[substitution]
[individual action]
[action declaration]
???????????????????? (Just a little more and go for it!)
[small talk][small talk]
cut:1
cut off:1 wash:1
divide:3
saute:1
Figure 2: An example of closed captions. (The phrase sandwiched by a square bracket means an utterance
type and the word surrounded by a rectangle means an extracted utterance referring to an action. The
bold word means a case frame assigned to the verb.)
and state-transition probability p(sj|si) from state
si to state sj . Then, they continue to estimate
HMM parameters with the Viterbi algorithm un-
til the clustering stabilizes. They applied the con-
structed content model to two tasks: information
ordering and summarization. We differ from this
study in that we utilize multimodal features and
domain-independent discourse features to achieve
robust topic identification.
In the field of video analysis, there have been
a number of studies on shot analysis with HMMs.
Chang et al described a method for classifying
shots into several classes for highlight extraction
in baseball games (Chang et al, 2002). Nguyen
et al proposed a robust statistical framework to
extract highlights from a baseball video (Nguyen
et al, 2005). They applied multi-stream HMMs
to control the weight among different features,
such as principal component features capturing
color information and frame-difference features
for moving objects. Phung et al proposed a prob-
abilistic framework to exploit hierarchy structure
for topic transition detection in educational videos
(Q.Phung et al, 2005).
Some studies attempted to utilize linguistic
information in shot analysis (Jasinschi et al,
2001; Babaguchi and Nitta, 2003). For exam-
ple, Babaguchi and Nitta segmented closed cap-
tion text into meaningful units and linked them to
video streams in sports video. However, linguistic
information they utilized was just keywords.
3 Features for Topic Identification
First, we?ll describe the features that we use for
topic identification, which are listed in Table 1.
They consist of three modalities: linguistic, visual
and audio modality.
We utilize as linguistic information the instruc-
tor?s utterances in video, which can be divided into
various types such as actions, tips, and even small
talk. Among them, actions, such as cut, peel and
grease a pan, are dominant and supposed to be use-
ful for topic identification and others can be noise.
In the case of analyzing utterances in video, it
is natural to utilize visual information as well as
linguistic information for robust analysis. We uti-
lize background image as visual information. For
example, frying and boiling are usually performed
on a gas range and preparation and dishing up are
usually performed on a cutting board.
Furthermore, we utilize cue phrases and silence
as a clue to a topic shift, and noun/verb chaining
as a clue to a topic persistence.
We describe these features in detail in the fol-
lowing sections.
3.1 Linguistic Features
Closed captions of Japanese cooking TV programs
are used as a source for extracting linguistic fea-
757
Table 1: Features for topic identification.
Modality Feature Domain dependent Domain independent
linguistic case frame utterance generalization
cue phrases topic change
noun chaining topic persistence
verb chaining topic persistence
visual background image bottom of image
audio silence topic change
Table 2: Utterance-type classification. (An underlined phrase means a pattern for recognizing utterance
type.)
[action declaration]
ex. ???????????????? (Then, we ?ll cook a steak)
????????????? (OK, we?ll fry.)
[individual action]
ex. ??????????? (Cut off a step of this eggplant.)
??????????? (Pour water into a pan.)
[food state]
ex. ???????????????? (There is no water in the carrot.)
[note]
ex. ??????????? (Don?t cut this core off.)
[substitution]
ex. ?????????? (You may use a leek.)
[food/tool presentation]
ex. ?????????????????? Today, we use this handy mixer.)
[small talk]
ex. ?????? (Hello.)
tures. An example of closed captions is shown in
Figure 2. We first process them with the Japanese
morphological analyzer, JUMAN (Kurohashi et
al., 1994), and make syntactic/case analysis and
anaphora resolution with the Japanese analyzer,
KNP (Kurohashi and Nagao, 1994). Then, we
perform the following process to extract linguis-
tic features.
3.1.1 Extracting Utterances Referring to
Actions
Considering a clause as a basic unit, utterances
referring to an action are extracted in the form
of case frame, which is assigned by case analy-
sis. This procedure is performed for generaliza-
tion and word sense disambiguation. For exam-
ple, ?????? (add salt)? and ???????
?? (add sugar into a pan)? are assigned to case
frame ireru:1 (add) and ??????? (carve with
a knife)? is assigned to case frame ireru:2 (carve).
We describe this procedure in detail below.
Utterance-type recognition
To extract utterances referring to actions, we
classify utterances into several types listed in Ta-
ble 21. Note that actions are supposed to have two
levels: [action declaration] means a declaration of
beginning a series of actions and [individual ac-
tion] means an action that is the finest one.
1In this paper, [ ] means an utterance type.
Input sentences are first segmented into
clauses and their utterance type is recognized.
Among several utterance types, [individual ac-
tion], [food/tool presentation], [substitution],
[note], and [small talk] can be recognized by
clause-end patterns. We prepare approximately
500 patterns for recognizing the utterance type. As
for [individual action] and [food state], consider-
ing the portability of our system, we use general
rules regarding intransitive verbs or adjective + ?
?? (become)? as [food state], and others as [in-
dividual action].
Action extraction
We extract utterances whose utterance type is
recognized as action ([action declaration] or [indi-
vidual action]). For example, ??? (peel)? and ?
?? (cut)? are extracted from the following sen-
tence.
(1) ????????? [individual action]??
???????? [individual action]?(We
peel this carrot and cut it in half.)
We make two exceptions to reduce noises. One
is that clauses are not extracted from the sen-
tence in which sentence-end clause?s utterance-
type is not recognized as an action. In the fol-
lowing example, ??? (simmer)? and ??? (cut)?
are not extracted because the utterance type of
758
Table 3: An example of the automatically con-
structed case frame.
Verb Case
marker Examples
kiru:1 ga <agent>
(cut) wo pork, carrot, vegetable, ? ? ?
ni rectangle, diamonds, ? ? ?
kiru:2 ga <agent>
(drain) wo damp ? ? ?
no eggplant, bean curd, ? ? ?
ireru:1 ga <agent>
(add) wo salt, oil, vegetable, ? ? ?
ni pan, bowl, ? ? ?
ireru:2 ga <agent>
(carve) wo knife ? ? ?
ni fish ? ? ?
the sentence-end clause is recognized as [substi-
tution].
(2) ???? [individual action]???? [indi-
vidual action]????? [substitution]?(It
doesn?t matter if you cut it after simmering.)
The other is that conditional/causal clauses are
not extracted because they sometimes refer to the
previous/next topic.
(3) ?????? ????????(After we
finish cutting it, we?ll fry.)
(4) ???????? ??????????
??????(We cut in this cherry tomato,
because we?ll fry it in oil.)
Note that relations between clauses are recognized
by clause-end patterns.
Verb sense disambiguation by assigning to a
case frame
In general, a verb has multiple mean-
ings/usages. For example, ????? has multiple
usages, ?????? (add salt)? and ????
??? (carve with a knife)? , which appear in
different topics. We do not extract a surface form
of verb but a case frame, which is assigned by
case analysis. Case frames are automatically
constructed from Web cooking texts (12 million
sentences) by clustering similar verb usages
(Kawahara and Kurohashi, 2002). An example of
the automatically constructed case frame is shown
in Table 3. For example, ?????? (add salt)?
is assigned to ireru:1 (add) and ???????
(carve with a knife)? is assigned to case frame
ireru:2 (carve).
3.1.2 Cue phrases
As Grosz and Sidner (Grosz and Sidner, 1986)
pointed out, cue phrases such as now and well
serve to indicate a topic change. We use approx-
imately 20 domain-independent cue phrases, such
as ??? (then)?, ??? (next)? and ??????
?? (then)?.
3.1.3 Noun Chaining
In text segmentation algorithms such as Text-
Tiling (Hearst.M, 1997), lexical chains are widely
utilized for detecting a topic shift. We utilize such
a feature as a clue to topic persistence.
When two continuous actions are performed to
the same ingredient, their topics are often identi-
cal. For example, because ???? (grate)? and ?
??? (raise)? are performed to the same ingredi-
ent ???? (turnip)? , the topics (in this instance,
preparation) in the two utterances are identical.
(5) a. ??????????????????
(We?ll grate a turnip.)
b. ????????????????
(Raise this turnip on this basket.)
However, in the case of spoken language, be-
cause there exist many omissions, it is often the
case that noun chaining cannot be detected with
surface word matching. Therefore, we detect
noun chaining by using the anaphora resolution
result2 of verbs (ex.(6)) and nouns (ex.(7)). The
verb, noun anaphora resolution is conducted by
the method proposed by (Kawahara and Kuro-
hashi, 2004), (Sasano et al, 2004), respectively.
(6) a. ?????????? (Cut a cabbage.)
b. ?? [?????] ????? (Wash it
once.)
(7) a. ??????????????????
(Slice a carrot into 4-cm pieces.)
b. [?????] ???????????
(Peel its skin.)
3.1.4 Verb Chaining
When a verb of a clause is identical with that
of the previous clause, they are likely to have the
same topic. We utilize the fact that the adjoining
two clauses contain an identical verbs or not as an
observed feature.
(8) a. ?????????????(Add some
red peppers.)
2[ ] indicates an element complemented with anaphora
resolution.
759
b. ????????? (Add chicken
wings.)
3.2 Image Features
It is difficult for the current image processing tech-
nique to extract what object appears or what ac-
tion is performing in video unless a detailed ob-
ject/action model for a specific domain is con-
structed by hand. Therefore, referring to (Hamada
et al, 2000), we focus our attention on color dis-
tribution at the bottom of the image, which is com-
paratively easy to exploit. As shown in Figure 1,
we utilize the mass point of RGB in the bottom of
the image at each clause.
3.3 Audio Features
A cooking video contains various types of audio
information, such as instructor?s speech, cutting
sounds and frizzling sound. If cutting sound or
frizzling sound could be distinguished from other
sounds, they could be an aid to topic identification,
but it is difficult to recognize them.
As Galley et al (Galley et al, 2003) pointed
out, a longer silence often appears when topic
changes, and we can utilize it as a clue to topic
change. In this study, silence is automatically ex-
tracted by finding duration below a certain ampli-
tude level which lasts more than one second.
4 Topic Identification based on HMMs
We employ HMMs for topic identification, where
a hidden state corresponds to a topic and vari-
ous features described in Section 3 are observed.
In our model, considering the case frame as a
basic unit, the case frame and background im-
age are observed from the state, and discourse
features indicating to topic shift/persistence (cue
phrases, noun/verb chaining and silence) are ob-
served when the state transits.
4.1 Parameters
HMM parameters are as follows:
? initial state distribution ?i : the probability
that state si is a start state.
? state transition probability aij : the probabil-
ity that state si transits to state sj .
? observation probability bij(ot) : the proba-
bility that symbol ot is emitted when state si
transits to state sj . This probability is given
by the following equation:
bij(ot) = bj(cfk) ? bj(R,G,B)
? bij(discourse features) (1)
- case frame bj(cfk): the probability that
case frame cfk is emitted by state sj .
- background image bj(R,G,B): the prob-
ability that background image bj(R,G,B) is
emitted by state sj . The emission probability
is modeled by a single Gaussian distribution
with mean (Rj ,Gj ,Bj) and variance ?j .
- discourse features : the probability that
discourse features are emitted when state si
transits to state sj . This probability is defined
as multiplication of the observation probabil-
ity of each feature (cue phrase, noun chain-
ing, verb chaining, silence). The observation
probability of each feature does not depend
on state si and sj , but on whether si and sj
are the same or different. For example, in the
case of cue phrase (c), the probability is given
by the following equation:
bij(c) =
{
psame(c)(i = j)
pdiff (c)(i 6= j)
(2)
4.2 Parameters Estimation
We apply the Baum-Welch algorithm for esti-
mating these parameters. To achieve high accu-
racy with the Baum-Welch algorithm, which is
an unsupervised learning method, some labeled
data have been required or proper initial param-
eters have been set depending on domain-specific
knowledge. These requirements, however, make
it difficult to extend to other domains. We auto-
matically extract ?pseudo-labeled? data focusing
on the following linguistic expressions: if a clause
has the utterance-type [action declaration] and an
original form of its verb corresponds to a topic, its
topic is set to that topic. Remind that [action dec-
laration] is a kind of declaration of starting a series
of actions. For example, in Figure 1, the topic of
the clause ?We?ll saute.? is set to sauteing because
its utterance-type is recognized as [action decla-
ration] and the original form of its verb is topic
sauteing.
By using a small amounts of ?pseudo-labeled?
data as well as unlabeled data, we train the
HMM parameters. Once the HMM parameters are
trained, the topic identification is performed using
the standard Viterbi algorithm.
5 Experiments and Discussion
5.1 Data
To demonstrate the effectiveness of our proposed
method, we made experiments on two kinds of
cooking TV programs: NHK ?Today?s Cooking?
760
Table 5: Experimental result of topic identification.
Features Accuracy
case frame background image discourse features silence ?Today?s Cooking? ?Kewpie 3-Min Cooking?
?
61.7% 66.4%
?
56.8% 72.9%
? ?
69.9% 77.1%
? ? ?
70.5% 82.9%
? ? ? ?
70.5% 82.9%
Table 4: Characteristics of the two cooking pro-
grams we used for our experiments.
Program Today?s Cooking Kewpie 3-Min Cooking
Videos 200 70
Duration 25min 10min
# of utterances
per video 249.4 183.4
and NTV ?Kewpie 3-Min Cooking?. Table 4
presents the characteristics of the two programs.
Note that time stamps of closed captions syn-
chronize themselves with the video stream. Ex-
tracted ?pseudo-labeled? data by the expression
mentioned in Section 4.2 are 525 clauses out of
13564 (3.87%) in ?Today?s Cooking?, and 107
clauses out of 1865 (5.74%) in ?Kewpie 3-Min
Cooking?.
5.2 Experiments and Discussion
We conducted the experiment of the topic iden-
tification. We first trained HMM parameters for
each program, and then applied the trained model
to five videos each, in which, we manually as-
signed appropriate topics to clauses. Table 5
gives the evaluation results. The unit of evalua-
tion was a clause. The accuracy was improved
by integrating linguistic and visual information
compared to using linguistic / visual informa-
tion alone. (Note that ?visual information? uses
pseudo-labeled data.) In addition, the accuracy
was improved by using various discourse features.
The reason why silence did not contribute to ac-
curacy improvement is supposed to be that closed
captions and video streams were not synchronized
precisely due to time lagging of closed captions.
To deal with this problem, an automatic closed
caption alignment technique (Huang et al, 2003)
will be applied or automatic speech recognition
will be used as texts instead of closed captions
with the advance of speech recognition technol-
ogy.
Figure 3 illustrates an improved example by
adding visual information. In the case of using
only linguistic information, this topic was rec-
First, saute and 
body.
Chop a garlic 
noisely.
Let?s start cooked 
vegitable.
preparation sauteing
sauteing
linguistic
linguistic
+ visual
Figure 3: An improved example by adding visual
information.
ognized as sauteing, but this topic was actually
preparation, which referred to the next topic. By
using the visual information that background color
was white, this topic was correctly recognized as
preparation.
We conducted another experiment to demon-
strate the validity of several linguistic processes,
such as utterance-type recognition and word sense
disambiguation with case frames, for extracting
linguistic information from closed captions de-
scribed in Section 3.1.1. We compared our method
to three methods: a method that does not per-
form word sense disambiguation with case frames
(w/o cf), a method that does not perform utterance-
type recognition for extracting actions (uses all
utterance-type texts) (w/o utype), a method, in
which a sentence is emitted according to a state-
specific language model (bigram) as Barzilay and
Lee adopted (bigram). Figure 6 gives the exper-
imental result, which demonstrates our method is
appropriate.
One cause of errors in topic identification is that
some case frames are incorrectly constructed. For
example, kiru:1 (cut) contains ?????? (cut
a vegetable)? and ????? (drain oil)?. This
leads to incorrect parameter training. Other cause
is that some verbs are assigned to an inaccurate
case frame by the failure of case analysis.
6 Conclusions
This paper has described an unsupervised topic
identification method integrating linguistic and vi-
sual information based on Hidden Markov Mod-
761
Table 6: Results of the experiment that compares our method to three methods.
Method Accuracy
?Today?s Cooking? ?Kewpie 3-Min Cooking?
proposed method 61.7% 66.4%
w/o cf 57.1% 60.0%
w/o utype 61.7% 62.1%
bigram 54.7% 59.3%
els. Our experiments on the two kinds of cooking
TV programs showed the effectiveness of integra-
tion of linguistic and visual information and in-
corporation of domain-independent discourse fea-
tures to domain-dependent features (case frame
and background image).
We are planning to perform object recognition
using the automatically-constructed object model
and utilize the object recognition results as a fea-
ture for HMM-based topic identification.
References
Noboru Babaguchi and Naoko Nitta. 2003. Intermodal
collaboration: A strategy for semantic content anal-
ysis for broadcasted sports video. In Proceedings of
IEEE International Conference on Image Process-
ing(ICIP2003), pages 13?16.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the NAACL/HLT, pages 113?120.
Peng Chang, Mei Han, and Yihong Gong. 2002.
Extract highlights from baseball game video with
hidden markov models. In Proceedings of the
International Conference on Image Processing
2002(ICIP2002), pages 609?612.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 562?569, 7.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistic, 12:175?204.
Reiko Hamada, Ichiro Ide, Shuichi Sakai, and Hide-
hiko Tanaka. 2000. Associating cooking video with
related textbook. In Proceedings of ACM Multime-
dia 2000 workshops, pages 237?241.
Hearst.M. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64, March.
Chih-Wei Huang, Winston Hsu, and Shin-Fu Chang.
2003. Automatic closed caption alignment based
on speech recognition transcripts. Technical report,
Columbia ADVENT.
Radu Jasinschi, Nevenka Dimitrova, Thomas McGee,
Lalitha Agnihotri, John Zimmerman, and Dongge.
2001. Integrated multimedia processing for topic
segmentation and classification. In Proceedings of
IEEE International Conference on Image Process-
ing(ICIP2003), pages 366?369.
Daisuke Kawahara and Sadao Kurohashi. 2002. Fertil-
ization of case frame dictionary for robust japanese
case analysis. In Proceedings of 19th COLING
(COLING02), pages 425?431.
Daisuke Kawahara and Sadao Kurohashi. 2004. Zero
pronoun resolution based on automatically con-
structed case frames and structural preference of an-
tecedents. In Proceedings of The 1st International
Joint Conference on Natural Language Processing,
pages 334?341.
Sadao Kurohashi and Makoto Nagao. 1994. A syntac-
tic analysis method of long japanese sentences based
on the detection of conjunctive structures. Compu-
tational Linguistics, 20(4).
Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improve-
ments of Japanese morphological analyzer JUMAN.
In Proceedings of the International Workshop on
Sharable Natural Language, pages 22?28.
Daniel Marcu. 2000. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional Linguistics, 26(3):395?448.
Huu Bach Nguyen, Koichi Shinoda, and Sadaoki Fu-
rui. 2005. Robust highlight extraction using multi-
stream hidden markov models for baseball video. In
Proceedings of the International Conference on Im-
age Processing 2005(ICIP2005), pages 173?176.
Dinh Q.Phung, Thi V.T Duong, Hung H.Bui, and
S.Venkatesh. 2005. Topic transition detection using
hierarchical hidden markov and semi-markov mod-
els. In Proceedings of ACM International Confer-
ence on Multimedia(ACM-MM05), pages 6?11.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proceedings of the 20th International
Conference on Computational Linguistics, number
1201?1207, 8.
762
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 269?278, Dublin, Ireland, August 23-29 2014.
Rapid Development of a Corpus with Discourse Annotations
using Two-stage Crowdsourcing
Daisuke Kawahara
??
Yuichiro Machida
?
Tomohide Shibata
??
Sadao Kurohashi
??
Hayato Kobayashi
?
Manabu Sassano
?
?
Graduate School of Informatics, Kyoto University
?
CREST, Japan Science and Technology Agency
?
Yahoo Japan Corporation
{dk, shibata, kuro}@i.kyoto-u.ac.jp, machida@nlp.ist.i.kyoto-u.ac.jp,
{hakobaya, msassano}@yahoo-corp.jp
Abstract
We present a novel approach for rapidly developing a corpus with discourse annotations using
crowdsourcing. Although discourse annotations typically require much time and cost owing to
their complex nature, we realize discourse annotations in an extremely short time while retaining
good quality of the annotations by crowdsourcing two annotation subtasks. In fact, our experi-
ment to create a corpus comprising 30,000 Japanese sentences took less than eight hours to run.
Based on this corpus, we also develop a supervised discourse parser and evaluate its performance
to verify the usefulness of the acquired corpus.
1 Introduction
Humans understand text not by individually interpreting clauses or sentences, but by linking such a text
fragment with another in a particular context. To allow computers to understand text, it is essential to
capture the precise relations between these text fragments. This kind of analysis is called discourse
parsing or discourse structure analysis, and is an important and fundamental task in natural language
processing (NLP). Systems for discourse parsing are, however, available only for major languages, such
as English, owing to the lack of corpora with discourse annotations.
For English, several corpora with discourse annotations have been developed manually, consuming a
great deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al.,
2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson,
2005). Discourse parsers trained on these corpora have also been developed and practically used. To
create the same resource-rich environment for another language, a quicker method than the conventional
time-consuming framework should be sought. One possible approach is to use crowdsourcing, which
has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008;
Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource
the difficult judgments for discourse annotations, which typically consists of two steps: finding a pair of
spans with a certain relation and identifying the relation between the pair.
In this paper, we propose a method for crowdsourcing discourse annotations that simplifies the proce-
dure by dividing it into two steps. The point is that by simplifying the annotation task it is suitable for
crowdsourcing, but does not skew the annotations for use in practical discourse parsing. First, finding a
discourse unit for the span is a costly process, and thus we adopt a clause as the discourse unit, since this
is reliable enough to be automatically detected. We also limit the length of each target document to three
sentences and at most five clauses to facilitate the annotation task. Secondly, we detect and annotate
clause pairs in a document that hold logical discourse relations. However, since this is too complicated
to assign as one task using crowdsourcing, we divide the task into two steps: determining the existence
of logical discourse relations and annotating the type of relation. Our two-stage approach is a robust
method in that it confirms the existence of the discourse relations twice. We also designed the tagset
of discourse relations for crowdsourcing, which consists of two layers, where the upper layer contains
the following three classes: ?CONTINGENCY,? ?COMPARISON? and ?OTHER.? Although the task
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
269
settings are simplified for crowdsourcing, the obtained corpus and knowledge of discourse parsing could
be still useful in general discourse parsing.
In our experiments, we crowdsourced discourse annotations for Japanese, for which there are no pub-
licly available corpora with discourse annotations. The resulting corpus consists of 10,000 documents,
each of which comprises three sentences extracted from the web. Carrying out this two-stage crowd-
sourcing task took less than eight hours. The time elapsed was significantly shorter than the conventional
corpus building method.
We also developed a discourse parser by exploiting the acquired corpus with discourse annotations.
We learned a machine learning-based model for discourse parsing based on this corpus and evaluated its
performance. An F1 value of 37.9% was achieved for contingency relations, which would be roughly
comparable with state-of-the-art discourse parsers on English. This result indicates the usefulness of the
acquired corpus. The resulting discourse parser would be effectively exploited in NLP applications, such
as sentiment analysis (Zirn et al., 2011) and contradiction detection (Murakami et al., 2009; Ennals et
al., 2010).
The novel contributions of this study are summarized below:
? We propose a framework for developing a corpus with discourse annotations using two-stage crowd-
sourcing, which is both cheap and quick to execute, but still retains good quality of the annotations.
? We construct a Japanese discourse corpus in an extremely short time.
? We develop a discourse parser based on the acquired corpus.
The remainder of this paper is organized as follows. Section 2 introduces related work, while Section
3 describes our proposed framework and reports the experimental results for the creation of a corpus with
discourse annotations. Section 4 presents a method for discourse parsing based on the corpus as well as
some experimental results. Section 5 concludes the paper.
2 Related Work
Snow et al. (2008) applied crowdsourcing to five NLP annotation tasks, but the settings of these tasks
are very simple. There have also been several attempts to construct language resources with complex
annotations using crowdsourcing. Negri et al. (2011) proposed a method for developing a cross-lingual
textual entailment (CLTE) corpus using crowdsourcing. They tackled this complex data creation task by
dividing it into several simple subtasks: sentence modification, type annotation and sentence translation.
The creative CLTE task and subtasks are quite different from our non-creative task and subtasks of
discourse annotations. Fossati et al. (2013) proposed FrameNet annotations using crowdsourcing. Their
method is a single-step approach to only detect frame elements. They verified the usefulness of their
approach through an experiment on a small set of verbs with only two frame ambiguities per verb.
Although they seem to be running a larger-scale experiment, its result has not been revealed yet. Hong
and Baker (2011) presented a crowdsourcing method for selecting FrameNet frames, which is a part of
the FrameNet annotation process. Since their task is equivalent to word sense disambiguation, it is not
very complex compared to the whole FrameNet annotation process. These FrameNet annotations are
still different from discourse annotations, which are our target. To the best of our knowledge, there have
been no attempts to crowdsource discourse annotations.
There are several manually-crafted corpora with discourse annotation for English, such as the Penn
Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse
Graphbank (Wolf and Gibson, 2005). These corpora were developed from English newspaper articles.
Several attempts have been made to manually create corpora with discourse annotations for languages
other than English. These include the Potsdam Commentary Corpus (Stede, 2004) for German (news-
paper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents;
1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres;
267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences com-
pared with the English corpora containing several tens of thousands sentences.
270
In recent years, there have been many studies on discourse parsing on the basis of the above hand-
annotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009;
Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty
et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing
can be attributed to the existence of corpora with discourse annotations. However, the target language is
mostly English since English is the only language that has large-scale discourse corpora. To develop and
improve discourse parsers for languages other than English, it is necessary to build large-scale annotated
corpora, especially in a short period if possible.
3 Development of Corpus with Discourse Annotations using Crowdsourcing
3.1 Corpus Specifications
We develop a tagged corpus in which pairs of discourse units are annotated with discourse relations.
To achieve this, it is necessary to determine target documents, discourse units, and a discourse relation
tagset. The following subsections explain the details of these three aspects.
3.1.1 Target Text and Discourse Unit
In previous studies on constructing discourse corpora, the target documents were mainly newspaper
texts, such as the Wall Street Journal for English. However, discourse parsers trained on such newspaper
corpora usually have a problem of domain adaptation. That is to say, while discourse parsers trained on
newspaper corpora are good at analyzing newspaper texts, they generally cannot perform well on texts
of other domains.
To address this problem, we set out to create an annotated corpus covering a variety of domains.
Since the web contains many documents across a variety of domains, we use the Diverse Document
Leads Corpus (Hangyo et al., 2012), which was extracted from the web. Each document in this corpus
consists of the first three sentences of a Japanese web page, making these short documents suitable for
our discourse annotation method based on crowdsourcing.
We adopt the clause as a discourse unit, since spans are too fine-grained to annotate using crowdsourc-
ing and sentences are too coarse-grained to capture discourse relations. Clauses, which are automatically
identified, do not need to be manually modified since they are thought to be reliable enough. Clause
identification is performed using the rules of Shibata and Kurohashi (2005). For example, the following
rules are used to identify clauses as our discourse units:
? clauses that function as a relatively strong boundary in a sentence are adopted,
? relative clauses are excluded.
Since workers involved in our crowdsourcing task need to judge whether clause pairs have discourse
relations, the load of these workers increases combinatorially as the number of clauses in a sentence
increases. To alleviate this problem, we limit the number of clauses in a document to five. This limitation
excludes only about 5% of the documents in the original corpus.
Our corpus consists of 10,000 documents corresponding to 30,000 sentences. The total number of
clauses in this corpus is 39,032, and thus the average number of clauses in a document is 3.9. The total
number of clause pairs is 59,426.
3.1.2 Discourse Relation Tagset
One of our supposed applications of discourse parsing is to automatically generate a bird?s eye view of a
controversial topic as in Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010),
which identify various relations between statements, including contradictory relations. We assume that
expansion relations, such as elaboration and restatement, and temporal relations are not important for this
purpose. This setting is similar to the work of Bethard et al. (2008), which annotated temporal relations
independently of causal relations. We also suppose that temporal relations can be annotated separately
for NLP applications that require temporal information. We determined the tagset of discourse relations
271
Upper type Lower type Example
CONTINGENCY
Cause/Reason ???????????????????
[since (I) pushed the button] [hot water was turned on]
Purpose ?????????????????????
[to pass the exam] [(I) studied a lot]
Condition ?????????????????
[if (you) push the button] [hot water will be turned on]
Ground ??????????????????????????
[here is his/her bag] [he/she would be still in the company]
COMPARISON
Contrast ?????????????????????????????
[at that restaurant, sushi is good] [ramen is so-so]
Concession ??????????????????????????
[that restaurant is surely good] [the price is high]
OTHER (Other) ???????????????????
[After being back home] [it began to rain]
Table 1: Discourse relation tagset with examples.
by referring to the Penn Discourse Treebank. This tagset consists of two layers, where the upper layer
contains three classes and the lower layer seven classes as follows:
? CONTINGENCY
? Cause/Reason (causal relations and not conditional relations)
? Purpose (purpose-action relations where the purpose is not necessarily accomplished)
? Condition (conditional relations)
? Ground (other contingency relations including pragmatic cause/condition)
? COMPARISON (same as the Penn Discourse Treebank)
? Contrast
? Concession
? OTHER (other weak relation or no relation)
Note that we do not consider the direction of relations to simplify the annotation task for crowdsourcing.
Table 1 shows examples of our tagset.
Therefore, our task is to annotate clause pairs in a document with one of the discourse relations given
above. Sample annotations of a document are shown below. Here, clause boundaries are shown by ?::?
and clause pairs that are not explicitly marked are allocated the ?OTHER? relation.
Cause/Reason ?????::??????????::????????????::???????
???????::??????????????
... [the surgery of my father ended safely] [(I) am relieved a little bit]
Contrast ???????????????????????::????????????
????????????????????????::???????????
??????::????????
... [There is tailwind to live,] [there is also headwind.]
3.2 Two-stage Crowdsourcing for Discourse Annotations
We create a corpus with discourse annotations using two-stage crowdsourcing. We divide the annotation
task into the following two subtasks: determining whether a clause pair has a discourse relation excluding
?OTHER,? and then, ascertaining the type of discourse relation for a clause pair that passes the first stage.
272
Probability Number
= 1.0 64
> 0.99 554
> 0.9 1,065
> 0.8 1,379
> 0.5 2,655
> 0.2 4,827
> 0.1 5,895
> 0.01 9,068
> 0.001 12,277
> 0.0001 15,554
Table 2: Number of clause pairs resulting from the judgments of discourse relation existence.
3.2.1 Stage 1: Judgment of Discourse Relation Existence
This subtask determines whether each clause pair in a document has one of the following discourse
relations: Cause/Reason, Purpose, Condition, Ground, Contrast, and Concession (that is, all the relations
except ?OTHER?). Workers are shown examples of these relations and asked to determine only the
existence thereof.
In this subtask, an item presented to a worker at a particular time consists of all the judgments of
clause pairs in a document. By adopting this approach, each worker considers the entire document when
making his/her judgments.
3.2.2 Stage 2: Judgment of Discourse Relation Type
This subtask involves ascertaining the discourse relation type for a clause pair that passes the first stage.
The result of this subtask is one of the seven lower types in our discourse relation tagset. Workers
are shown examples of these types and then asked to select one of the relations. If a worker chooses
?OTHER,? this corresponds to canceling the positive determination of the existence of the discourse
relation in stage one.
In this subtask, an item is the judgment of a clause pair. That is, if a document contains more than
one clause pair that must be judged, the judgments for this document are divided into multiple items,
although this is rare.
3.3 Experiment and Discussion
We conducted an experiment of the two-stage crowdsourcing approach using Yahoo! Crowdsourcing.
1
To increase the reliability of the produced corpus, we set the number of workers for each item for each
task to 10. The reason why we chose this value is as follows. While Snow et al. (2008) claimed that an
average of 4 non-expert labels per item in order to emulate expert-level label quality, the quality of some
tasks increased by increasing the number of workers to 10. We also tested hidden gold-standard items
once every 10 items to examine worker?s quality. If a worker failed these items in serial, he/she would
have to take a test to continue the task.
We obtained judgments for the 59,426 clause pairs in the 10,000 documents of our corpus in the
first stage of crowdsourcing, i.e., the subtask of determining the existence of discourse relations. We
calculated the probability of each label using GLAD
2
(Whitehill et al., 2009), which was proved to
be more reliable than the majority voting. This probability corresponds to the probability of discourse
relation existence of each clause pair. Table 2 lists the results. We set a probability threshold to select
those clause pairs whose types were to be judged in the second stage of crowdsourcing. With this
threshold set to 0.01, 9,068 clause pairs (15.3% of all the clause pairs) were selected. The threshold was
set fairly low to allow low-probability judgments to be re-examined in the second stage.
1
http://crowdsourcing.yahoo.co.jp/
2
http://mplab.ucsd.edu/?jake/OptimalLabelingRelease1.0.3.tar.gz
273
Lower type All prob > 0.8
Cause/Reason 2,104 1,839 (87.4%)
Purpose 755 584 (77.4%)
Condition 1,109 925 (83.4%)
Ground 442 273 (61.8%)
Contrast 437 354 (81.0%)
Concession 80 49 (61.3%)
Sum of the above discourse relations 4,927 4,024 (81.7%)
Other 4,141 3,753 (90.6%)
Total 9,068 7,777 (85.8%)
Table 3: Results of the judgments of lower discourse relation types.
Upper type All prob > 0.8
CONTINGENCY 4,439 3,993 (90.0%)
COMPARISON 516 417 (80.8%)
Sum of the above discourse relations 4,955 4,410 (89.0%)
OTHER 4,113 3,753 (91.2%)
Total 9,068 8,163 (90.0%)
Table 4: Results of the judgments of upper discourse relation types.
The discourse relation types of the 9,068 clause pairs were determined in the second stage of crowd-
sourcing. We extended GLAD (Whitehill et al., 2009) for application to multi-class tasks, and calculated
the probability of the labels of each clause pair. We assigned the label (discourse relation type) with the
highest probability to each clause pair. Table 3 gives some statistics of the results. The second column in
this table denotes the numbers of each discourse relation type, while the third column gives the numbers
of each type of clause pair with a probability higher than 0.80. Table 4 gives statistics of the results when
the lower discourse relation types are merged into the upper types. Table 5 shows some examples of the
resulting annotations.
Carrying out the two separate subtasks using crowdsourcing took approximately three hours and five
hours with 1,458 and 1,100 workers, respectively. If we conduct this task at a single stage, it would take
approximately 33 (5 hours / 0.153) hours. It would be four times longer than our two-stage approach.
Such single-stage approach is also not robust since it does not have a double check mechanism, with
which the two-stage approach is equipped. We spent 111 thousand yen and 113 thousand yen (approx-
imately 1,100 USD, respectively) for these subtasks, which would be extremely less expensive than the
projects of conventional discourse annotations.
For the examples in Table 5, we confirmed that the discourse relation types of the top four examples
were surely correct. However, we judged the type (Contrast) of the bottom example as incorrect. Since
the second clause is an instantiation of the first clause, the correct type should be ?Other.? We found such
errors especially in the clause pairs with a probability lower than 0.80.
4 Development of Discourse Parser based on Acquired Discourse Corpus
To verify the usefulness of the acquired corpus with discourse annotations, we developed a supervised
discourse parser based on the corpus, and evaluated its performance. We built two discourse parsers using
the annotations of the lower and upper discourse relation types, respectively. From the annotations in the
first stage of crowdsourcing (i.e., judging the existence of discourse relations), we assigned annotations
with a probability less than 0.01 as ?OTHER.? Of the annotations acquired in the second stage (i.e.,
judging discourse relation types), we adopted those with a probability greater than 0.80 and discarded
the rest. After this preprocessing, we obtained 58,135 (50,358 + 7,777) instances of clause pairs for
the lower-type discourse parser and 58,521 (50,358 + 8,163) instances of clause pairs for the upper-type
274
Prob # W Type Document
1.00 6/10 Cause/Reason ???????????????????????????????
??????????????????????????????
????
... [Since the flower blooms in the fifth lunar month] [it is called ?Sat-
suki.?] ...
0.99 4/10 Condition ??????????????????????????????
???????????????????????????????
?????????????????????????????
??????
[If you click the balloon on the map] [you can see the recommended
route] ...
0.81 3/10 Purpose ?????????????????????????????
?????????????????????????????
??????????????????????????????
?????????????????????????????
... [And seeking ?Great harvest?] [each country is engaged in a war]
0.61 2/10 Cause/Reason ??????????????????????????????
?????????????????????????????
??????????????????????????????
??????????????????????????????
?????
... [by transmitting power to the front and rear axle with the combina-
tion of gears and shafts] [(it) drives the four wheels.]
0.54 3/10 Contrast ?????????????????????????????
??????????????????????????????
???????????????????????????
... [a scramble for customers by department stores would be severe.]
[What comes out is the possibility of the closure of Fukuoka Mit-
sukoshi.]
Table 5: Examples of Annotations. The first column denotes the estimated label probability and the
second column denotes the number of workers that assigned the designated type. In the fourth column,
the clause pair annotated with the type is marked with?? ([ ] in English translations).
discourse parser. Of these, 4,024 (6.9%) and 4,410 (7.5%) instances, respectively, had one of the types
besides ?OTHER.? We conducted experiments using five-fold cross validation on these instances.
To extract features of machine learning, we applied the Japanese morphological analyzer, JUMAN,
3
and the Japanese dependency parser, KNP,
4
to the corpus. We used the features listed in Table 6, which
are usually used for discourse parsing.
We adopted Opal (Yoshinaga and Kitsuregawa, 2010)
5
for the machine learning implementation. This
tool enables online learning using a polynomial kernel. As parameters for Opal, we used the passive-
aggressive algorithm (PA-I) with a polynomial kernel of degree two as a learner and the extension to
multi-class classification (Matsushima et al., 2010). The numbers of classes were seven and three for the
lower- and upper-type discourse parsers, respectively. We set the aggressiveness parameter C to 0.001,
which generally achieves good performance for many classification tasks. Other parameters were set to
the default values of Opal.
To measure the performance of the discourse parsers, we adopted precision, recall and their harmonic
mean (F1). These metrics were calculated as the proportion of the number of correct clause pairs to the
3
http://nlp.ist.i.kyoto-u.ac.jp/EN/?JUMAN
4
http://nlp.ist.i.kyoto-u.ac.jp/EN/?KNP
5
http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/opal/
275
Name Description
clause distance clause distance between two clauses
sentence distance sentence distance between two clauses
bag of words bag of words (lemmas) for each clause
predicate a content word (lemma) of the predicate of each clause
conjugation form of predicate a conjugation form of the predicate of each clause
conjunction a conjunction if it is located at the beginning of a clause
word overlapping ratio an overlapping ratio of words between the two clauses
clause type a lexical type output by KNP for each clause (about 100 types)
topic marker existence existence of a topic marker in each clause
topic marker cooccurrence existence of a topic marker in both clauses
Table 6: Features for our discourse parsers.
Type Precision Recall F1
Cause/Reason 0.623 (441/708) 0.240 (441/1,839) 0.346
Purpose 0.489 (44/90) 0.075 (44/584) 0.131
Condition 0.581 (256/441) 0.277 (256/925) 0.375
Ground 0.000 (0/12) 0.000 (0/273) 0.000
Contrast 0.857 (6/7) 0.017 (6/354) 0.033
Concession 0.000 (0/0) 0.000 (0/49) 0.000
Other 0.944 (53,702/56,877) 0.992 (53,702/54,111) 0.968
Table 7: Performance of our lower-type discourse parser.
Type Precision Recall F1
CONTINGENCY 0.625 (1,084/1,735) 0.272 (1,084/3,993) 0.379
COMPARISON 0.412 (7/17) 0.017 (7/417) 0.032
OTHER 0.942 (53,454/56,769) 0.988 (53,454/54,111) 0.964
Table 8: Performance of our upper-type discourse parser.
number of all recognized or gold-standard ones for each discourse relation type. Tables 7 and 8 give the
accuracies for the lower- and upper-type discourse parsers, respectively.
From Table 8, we can see that our upper-type discourse parser achieved an F1 of 37.9% for contingency
relations. It is difficult to compare our results with those in previous work due to the use of different data
set and different languages. We, however, anticipate that our results would be comparable with those
of state-of-the-art English discourse parsers. For example, the end-to-end discourse parser of Lin et al.
(2012) achieved an F1 of 20.6% ? 46.8% on the Penn Discourse Treebank.
We also obtained a low F1 for comparison relations. This tendency is similar to the previous results
on the Penn Discourse Treebank. The biggest cause of this low F1 is the lack of unambiguous explicit
discourse connectives for these relations. Although there are explicit discourse connectives in Japanese,
many of them have multiple meanings and cannot be used as a direct clue for discourse relation detection
(e.g., as described in Kaneko and Bekki (2014)). As reported in Pitler et al. (2009) and other studies,
the identification of implicit discourse relations are notoriously difficult. To improve its performance, we
need to incorporate external knowledge sources other than the training data into the discourse parsers.
A promising way is to use large-scale knowledge resources that are automatically acquired from raw
corpora.
276
5 Conclusion
We presented a rapid approach for building a corpus with discourse annotations and a discourse parser
using two-stage crowdsourcing. The acquired corpus is made publicly available and can be used for
research purposes.
6
This corpus can be used not only to build a discourse parser but also to evaluate
its performance. The availability of the corpus with discourse annotations will accelerate the develop-
ment and improvement of discourse parsing. In the future, we intend integrating automatically acquired
knowledge from corpora into the discourse parsers to further enhance their performance. We also aim to
apply our framework to other languages without available corpora with discourse annotations.
References
Steven Bethard, William Corvey, Sara Klingenstein, and James H. Martin. 2008. Building a corpus of temporal-
causal structure. In Proceedings of the 6th International Conference on Language Resources and Evaluation,
pages 908?915.
Or Biran and Kathleen McKeown. 2013. Aggregated word pair features for implicit discourse relation disam-
biguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume
2: Short Papers), pages 69?73.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop on Discourse and
Dialogue.
Iria da Cunha, Juan-Manuel Torres-Moreno, and Gerardo Sierra. 2011. On the development of the RST Spanish
treebank. In Proceedings of the 5th Linguistic Annotation Workshop (LAW V), pages 1?10.
Rob Ennals, Beth Trushkowsky, and John Mark Agosta. 2010. Highlighting disputed claims on the web. In
Proceedings of the 19th international conference on World Wide Web, pages 341?350.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 60?68. Association for Computational Linguistics.
Marco Fossati, Claudio Giuliano, and Sara Tonelli. 2013. Outsourcing FrameNet to the crowd. In Proceedings of
the 51st Annual Meeting of the Association for Computational Linguistics, pages 742?747.
Sucheta Ghosh, Sara Tonelli, Giuseppe Riccardi, and Richard Johansson. 2011. End-to-end discourse parser
evaluation. In Fifth IEEE International Conference on Semantic Computing (ICSC), pages 169?172.
Masatsugu Hangyo, Daisuke Kawahara, and Sadao Kurohashi. 2012. Building a diverse document leads corpus
annotated with semantic relations. In Proceedings of 26th Pacific Asia Conference on Language Information
and Computing, pages 535?544.
Hugo Hernault, Helmut Prendinger, David duVerle, and Mitsuru Ishizuka. 2010. HILDA: A discourse parser
using support vector machine classification. Dialogue & Discourse, 1(3):1?33.
Jisup Hong and Collin F. Baker. 2011. How good is the crowd at ?real? WSD? In Proceedings of the 5th Linguistic
Annotation Workshop, pages 30?37.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A novel discriminative framework for sentence-level
discourse analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 904?915.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and Yashar Mehdad. 2013. Combining intra- and multi-sentential
rhetorical parsing for document-level discourse analysis. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics, pages 486?496.
Kimi Kaneko and Daisuke Bekki. 2014. Building a Japanese corpus of temporal-causal-discourse structures
based on SDRT for extracting causal relations. In Proceedings of the EACL 2014 Workshop on Computational
Approaches to Causality in Language (CAtoCL), pages 33?39.
6
http://nlp.ist.i.kyoto-u.ac.jp/EN/?DDLC
277
Man Lan, Yu Xu, and Zhengyu Niu. 2013. Leveraging synthetic discourse data via multi-task learning for implicit
discourse relation recognition. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 476?485.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, pages 1?34.
Shin Matsushima, Nobuyuki Shimizu, Kazuhiro Yoshida, Takashi Ninomiya, and Hiroshi Nakagawa. 2010. Exact
passive-aggressive algorithm for multiclass classification using support class. In Proceedings of 2010 SIAM
International Conference on Data Mining (SDM2010), pages 303?314.
Koji Murakami, Eric Nichols, Suguru Matsuyoshi, Asuka Sumida, Shouko Masuda, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Statement map: Assisting information credibility analysis by visualizing arguments. In Pro-
ceedings of the 3rd Workshop on Information Credibility on the Web, pages 43?50.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo Giampiccolo, and Alessandro Marchetti. 2011. Divide
and conquer: Crowdsourcing the creation of cross-lingual textual entailment corpora. In Proceedings of the
2011 Conference on Empirical Methods in Natural Language Processing, pages 670?679.
Thiago Alexandre Salgueiro Pardo, Maria das Grac?as Volpe Nunes, and Lucia Helena Machado Rino. 2004.
Dizer: An automatic discourse analyzer for Brazilian Portuguese. In Advances in Artificial Intelligence?SBIA
2004, pages 224?234. Springer.
Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13?16.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in
text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the AFNLP, pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn discourse treebank 2.0. In Proceedings of the 6th International Conference on Language
Resources and Evaluation, pages 2961?2968.
Tomohide Shibata and Sadao Kurohashi. 2005. Automatic slide generation based on discourse structure analysis.
In Proceedings of Second International Joint Conference on Natural Language Processing, pages 754?766.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and AndrewNg. 2008. Cheap and fast ? but is it good? evaluating
non-expert annotations for natural language tasks. In Proceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 254?263.
Manfred Stede. 2004. The Potsdam commentary corpus. In Proceedings of the 2004 ACL Workshop on Discourse
Annotation, pages 96?102.
Rajen Subba and Barbara Di Eugenio. 2009. An effective discourse parser that uses rich linguistic information. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 566?574.
Jacob Whitehill, Paul Ruvolo, Ting fan Wu, Jacob Bergsma, and Javier Movellan. 2009. Whose vote should
count more: Optimal integration of labels from labelers of unknown expertise. In Y. Bengio, D. Schuurmans,
J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22,
pages 2035?2043.
FlorianWolf and Edward Gibson. 2005. Representing discourse coherence: A corpus-based study. Computational
Linguistics, 31(2):249?287.
Naoki Yoshinaga and Masaru Kitsuregawa. 2010. Kernel slicing: Scalable online training with conjunctive fea-
tures. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING2010), pages
1245?1253.
C?acilia Zirn, Mathias Niepert, Heiner Stuckenschmidt, and Michael Strube. 2011. Fine-grained sentiment analysis
with structural features. In Proceedings of 5th International Joint Conference on Natural Language Processing,
pages 336?344.
278
