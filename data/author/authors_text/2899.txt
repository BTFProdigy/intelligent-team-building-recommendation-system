Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 49?56,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Dynamic Path Prediction and Recommendation in a Museum Environment
Karl Grieser??, Timothy Baldwin? and Steven Bird?
? CSSE
University of Melbourne
VIC 3010, Australia
? DIS
University of Melbourne
VIC 3010, Australia
{kgrieser,tim,sb}@csse.unimelb.edu.au
Abstract
This research is concerned with making
recommendations to museum visitors
based on their history within the physical
environment, and textual information
associated with each item in their history.
We investigate a method of providing
such recommendations to users through
a combination of language modelling
techniques, geospatial modelling of
the physical space, and observation of
sequences of locations visited by other
users in the past. This study compares
and analyses different methods of path
prediction including an adapted naive
Bayes method, document similarity, visitor
feedback and measures of lexical similarity.
1 Introduction
Visitors to an information rich environment such as
a museum, are invariably there for a reason, be it
entertainment or education. The visitor has paid
their admission fee, and we can assume they intend
to get the most out of their visit. As with other
information rich environments and systems, first-
time visitors to the museum are at a disadvantage as
they are not familiar with every aspect of the collec-
tion. Conversely, the museum is severely restricted
in the amount of information it can convey to the
visitor in the physical space.
The use of a dynamic, intuitive interface can over-
come some of these issues (Filippini, 2003; Benford
et al, 2001). Such an interface would convention-
ally take the form of a tour guide, audio tour, or a
curator stationed at points throughout the museum.
This research is built around the assumption that the
museum visitor has access to a digital device such
as a PDA and that it is possible for automatic sys-
tems to interact with the user via this device. In this
way we aim to be able to deliver relevant content
to the museum visitor based on observation of their
movements within the physical museum space, as
well as make recommendations of what exhibits they
might like to visit next and why. At present, we are
focusing exclusively on the task of recommendation.
Recommendations can be used to convey predic-
tions about what theme or topic a given visitor is
interested in. They can also help to communicate
unexpected connections between exhibits (Hitzeman
et al, 1997), or explicitly introduce variety into the
visit. For the purposes of this research, we focus
on this first task of providing recommendations con-
sistent with the visitor?s observed behaviour to that
point. We investigate different factors which we
hypothesise impact on the determination of what
exhibits a given visitor will visit, namely: the phys-
ical proximity of exhibits, the conceptual similarity
of exhibits, and the relative sequence in which other
visitors have visited exhibits.
Recommendation systems in physical environ-
ments are notoriously hard to evaluate, as the
recommendation system is only one of many stimuli
which go to determine the actual behaviour of the
visitor. In order to evaluate the relative impact
of different factors in determining actual visitor
behaviour, we separate the stimuli present into
a range of predictive methods. In this paper we
target the task of user prediction, that is prediction
of what exhibit a visitor will visit next based on
49
their previous history. Language based models are
intended to simulate a potentially unobservable
source of information: the visitor?s thought process.
In order to identify the reason for the visitor?s
interest in the multiple part exhibits we parallel this
problem with the task of word sense disambiguation
(WSD). Determining the visitor?s reason for visiting
an exhibit allows a predictive system to more
accurately model the visitor?s future path.
This study aims to arrive at accurate methods of
predicting how a user will act in an information-rich
museum. The space focused on in this research is
the Australia Gallery Collection of the Melbourne
Museum, at Carlton Gardens in Melbourne,
Australia. The predictions take the form of which
exhibits a visitor will visit given a history of
previously visited exhibits. This study analyses
and compares the effectiveness of supervised and
unsupervised learning methods in the museum
domain, drawing on a range of linguistic and
geospatial features. A core contribution of
this study is its focus on the relative import of
heterogeneous information sources a user makes
use of in selecting the next exhibit to visit.
2 Problem Description
In order to recommend exhibits to visitors while they
are going through a museum, the recommendations
need to be accurate/pertinent to the goals that the
visitor has in mind. Without accurate recomme nda-
tions, recommendations given to a visitor are essen-
tially useless, and might as well not have been rec-
ommended at all.
Building a recommender system based on contex-
tual information (Resnick and Varian, 1997) is the
ultimate goal of this research. However the envi-
ronment in this circumstance is physical, and the
actions of visitors are expected to vary within such
a space, as opposed to the usual online or digital
domain of recommender systems. Studies such as
HIPS (Benelli et al, 1999) and the Equator project1
have analysed the importance and difficulty of inte-
grating the virtual environment into the physical, as
well as identifying how non-physical navigation sys-
tems can relate to similar physical systems. For the
purpose of this study, it is sufficient to acknowledge
1http://www.equator.ac.uk
the effect of the physical environment by scaling all
recommendations against their distances from one
another.
The common information that museum exhibits
contain is key in determining how each individual
relates to each other exhibit in the collection. At
the most basic level, the exhibits are simply isolated
elements that share no relationship with one another,
their only similarity being that they occur together
in visitor paths. This interpretation disregards any
meaning or content that each exhibit contains. But
museum exhibits are created with the goal of pro-
viding information, and to disregard the content of
an exhibit is to disregard its purpose.
An exhibit in a museum may be many kinds of
things, and hence most exhibits will differ in presen-
tation and content. The target audience of a museum
is one indicator of the type of content that can be
expected within each exhibit. An art gallery is com-
prised of mainly paintings and sculptures: single
component exhibits with brief descriptions. A chil-
dren?s museum will contain a high proportion of
interactive exhibits, and much audio and visual con-
tent. In these two cases the reason for visiting the
exhibit differs greatly.
Given the diversity of information contained
within each exhibit and the greater diversity of a
museum collection, it can be difficult to see why
visitors only examine certain exhibits during their
tours. It is very difficult to perceive what a visitor?s
intention is without constant feedback, making the
problem of providing relevant recommendations a
question of predicting what a visitor is interested in
based on characteristics of exhibits the visitor has
already seen. The use of both physical attributes and
exhibit information content are used in conjunction
in an effort to account for multiple possible reasons
for visiting as exhibit. Connections between
physical attributes of an exhibit are easier to identify
than connections based on information content.
This is due to the large quantity of information
associated with each exhibit, and the difficulty in
determining what the visitor liked (or disliked)
about the exhibit.
In order to make prediction based on a visitor?s
history, the importance of the exhibits in the visi-
tors path must be known. This is difficult to obtain
directly without the aid of real-time feedback from
50
the user themselves. In an effort to emulate the
difficulty of observing mental processes adopted by
each visitor, language based predictive models are
employed.
3 Resources
The domain in which all experimentation takes place
is the Australia Gallery of the Melbourne Museum.
This exhibition provides a history of the city of Mel-
bourne Melbourne, from its settlement up to the
present day, and includes such exhibits as the taxi-
dermised coat of Phar Lap (Australia?s most famous
race horse) and CSIRAC (Australia?s first, and the
world?s fourth, computer). The Gallery contains
enough variation so that not all exhibits can be clas-
sified into a single category, but is sufficiently spe-
cialised to offer much interaction and commonality
between the exhibits.
The exhibits within the Australia Gallery take
a wide variety of forms, from single items with
a description plaque, to multiple component dis-
plays with interactivity and audio-visual enhance-
ment; note, for our purposes in experimentation,
we do not differentiate between exhibit types or
modalities. The movement of visitors within an
exhibition can be restricted if the positioning of the
exhibits require visitors to take a set path (Peponis
et al, 2004), which can alter how a visitor chooses
between exhibits to view. In the case of the Australia
Gallery, however, the collection is spread out over
a sizeable area, and has an open plan design such
that visitor movement is not restricted or funnelled
through certain areas and there is no predetermined
sequence or selection of exhibits that a given visitor
can be expected to spend time at.
We used several techniques to represent the dif-
ferent aspects of each exhibit. We categorised each
exhibit by way of its physical attributes (e.g. size)
and taxonomic information about the exhibit con-
tent (e.g. clothing or animal). We also described
each exhibit by way of its physical location within
the Australia Gallery, relative to a floorplan of the
Gallery.
The Melbourne Museum also has a sizable
web-site2 which contains much detailed information
about the exhibits within the Australia Gallery. This
2http://www.museum.vic.gov.au/
data is extremely useful in that it provides a rich
vocabulary of information based on the content
of each exhibit. Each exhibit identified within the
Australia Gallery has a corresponding web-page
describing it. The information content of an exhibit
is made up of the text in its corresponding web-page
combined with its attributes. By having a large
source of natural language information associated
with the exhibit, linguistic based predictive methods
can more accurately identify the associations made
by visitors.
The dataset that forms that basis of this research
is a database of 60 visitor paths through the Aus-
tralia Gallery, which was collected by Melbourne
Museum staff over a period of four months towards
the end of 2001. The Australia Gallery contains a
total of fifty-three exhibits. This data is used to eval-
uate both physical and conceptual predictive meth-
ods. If predictive methods are able to accurately
describe how a visitor travels in a museum, then
the predictive method creates an accurate model of
visitor behaviour.
Exhibit components can be combined to form a
description for each exhibit. For this purpose, the
Natural Language Toolkit 3 (Bird, 2005) was used
to analyse and compare the lexical content associ-
ated with each exhibit, so that relationships between
exhibits can be identified.
4 Methodology
Analysis of user history as a method of prediction
(or recommendation) has been examined in
Chalmers et al (1998). Also discussed is the
role that user history plays in anticipating user
goals. This approach can be adapted to a physical
environment by simply substituting in locations
visited in place of web pages visited. Data gathered
from the paths of previous visitors also forms a valid
means of predicting other visitors? paths (Zukerman
and Albrecht, 2001). This approach operates under
the assumption that all visitors behave in a similar
fashion when visiting a museum. However visitors?
goals in visiting a museum can differ widely. For
example, the goals of a student researching a project
will differ to those of a family with young children
on a weekend outing.
3http://nltk.sourceforge.net/
51
A conceptual model of the exhibition space is cre-
ated by visitors with a specific task in mind. Inter-
pretation of this conceptual model is key to creating
accurate recommendations. The building of such a
conceptual model takes place from the moment a
visitor enters an exhibition, until the time they leave,
and skews the visitor towards groups of conceptual
locations and categories.
The representation of these intrinsically dynamic
models is directly related to the task the visitor has
in mind. Students will form a conceptual model
based around their course requirements, children
around the most visually attractive exhibits, and
so forth. This necessitates the need for multiple
exhibit similarity measures, however in the absence
of express knowledge of the ?type? of each visitor in
the sample data, a broad-coverage recommendation
system that functions best in all circumstances is the
desired goal. It is hoped that in future, reevaluation
of the data to classify visitors into broad categories
(e.g. information seeking, entertainment seeking)
will allow for the development of specialised
models tailored to visitor types.
The models of exhibit representation we exam-
ine in this research are exhibit proximity, text-based
exhibit information content, and exhibit popularity
(based on the previous visitor data provided by the
Melbourne Museum), as well as combinations of the
three. Exhibit information content is a two part rep-
resentation: primarily each exhibit has a large body
of text describing the exhibit drawn from the Mel-
bourne Museum website. It is fortunate that this
information is curated, and managed from a cen-
tral source, so that inconsistencies between exhibit
information are extremely rare. The authors were
unable to find any contradictory information in the
web-pages used for experimentation, as may be the
case with larger non-curated document bodies. The
second component of the information content is a
small set of key terms describing the attributes of
the exhibit. Textual content as a means of deter-
mining exhibit similarity has been analysed previ-
ously (Green et al, 1999), both in terms of keyword
attributes and bodies of explanatory text.
In order to form a prediction about which exhibit
a visitor will next visit, the probability of the tran-
sition of the visitor from their current location to
every other exhibit in the collection must be known.
Prediction of the next exhibit by proximity simply
means choosing the closest not-yet-visited exhibit to
the visitor?s current location. In terms of information
content, each exhibit is related to all other exhibits to
a certain degree. To express this we use the attribute
keywords as a query to find the exhibit most simi-
lar. We use the attribute keywords associated with
each document to search the document space of the
exhibits to find the exhibit that is most similar to the
exhibit the visitor is currently located at. To do this
we use a simple tf?idf scheme, using the attribute
keywords as the queries, and the exhibit associated
web pages as the document space. The score of each
query over each document is normalised into a tran-
sitional probability array such that
?
j P (q|dj) = 1
for a query (q) over the j exhibit documents (dj).
In order to determine the popularity of an
exhibit, the visitor paths provided by the Melbourne
Museum were used to form another matrix of
transitional probabilities based on the likelihood
that a visitor will travel to an exhibit from the
exhibit they are currently at. I.e. for each exhibit e
an array of transitional probabilities is formed such
that
?
j P (e|cj) = 1 where cj ? C ? = C/{e}, i.e.
all exhibits other than e. In both cases Laplacian
smoothing was used to remove zero probabilities.
The methods of exhibit popularity and physical
proximity are superficial in scope and do not extend
into the conceptual space adopted by the visitors.
They do however give insight into how a physical
space affects a visitors? mental representation of the
conceptual areas associated with specific exhibit col-
lections, and are more easily observable. Visitor
reaction to exhibit information content is harder to
observe and more problematic to predict. Any accu-
rate recommender systems produced in this fashion
will need to take into account the limitations these
two methods place on the thought processes of visi-
tors.
Connections that visitors make between exhibits
are more fluid, and are harder to represent in terms
of similarity measures. Specifically it is difficult to
see why visitors make connections between exhibits
as there can be multiple similarities between two
exhibits. To this end we have equated this prob-
lem with the task of Word Sense Disambiguation
(WSD). The path that a visitor takes can be seen
as a sentence of exhibits, and each exhibit in the
52
sentence has an associated meaning. WSD is used
to determine the meaning of the next exhibit based
on the meanings of previous exhibits in the path. For
each word in the keyword set of each exhibit, the
WordNet (Fellbaum, 1998) similarity is calculated
against each word in another exhibit. The similar-
ity is the sum of the WordNet similarities between
all attribute keywords in the two exhibits (K1, K2),
normalised over the length of both keyword sets:
?
k1?K1
?
k2?K2 WNsim(k1, k2)
|K1||K2|
For the purposes of this experiment we have
chosen to use three WordNet similarity/relatedness
measures to simulate the conceptual connections
that visitors make between exhibits. The Lin (Lin,
1998) and Leacock-Chodorow (Leacock et al,
1998) similarity measures and the Banerjee-
Pedersen (Patwardhan and Pedersen, 2003)
relatedness measures were used. The similarities
were normalised and transformed into probability
matrices such that
?
j PWNsim(e|cj) = 1 for each
next exhibit ci. The use of WordNet measures is
intended to simulate the mental connections that
visitors make between exhibit content, given that
each visit can interpret content in a number of
different ways.
The history of the visitor at any given time is
essential in keeping the visitor?s conceptual model
of the exhibit space current. The recency of a given
exhibit within a visitor?s history is inversely propor-
tional to how long ago the exhibit was encountered.
To take into account the visitor history, the col-
laborative data, proximity, document vectors, and
conceptual WordNet similarity, we adapt the naive
Bayes approach. The conditional probabilities of
each method are combined along with the temporal
recency of an exhibit to produce a predictive exhibit
recommender. The resultant recommendation to a
visitor can be described as follows:
c? = arg max
ci
P (ci)
t
?
j=1
P (Aj |ci) ? 2?(t?j+1) +
2?t
t
where t is the length of the visitor?s history, Aj ? C
is an exhibit at time j in the visitor history (and C
is the full set of exhibits), and ci ? C ? = C/{Aj}
is each unvisited exhibit. The most probable next
exhibit (c?) is selected from all possible next exhibits
(ci). Any selections made must be compared against
the visitor?s history. In this, we assume that a pre-
viously visited exhibit has already been seen, and
hence should not be recommended again.
The effectiveness of these methods was tested in
multiple combinations, both with history modeling
and without (only the exhibit the visitor is currently
at is considered). Testing was carried out using
the sixty visitor paths supplied by the Melbourne
Museum. For each method two tests were carried
out:
? Predict the next exhibit in the visitor?s path.
? Only make a prediction if the probability of the
prediction is above a given threshold.
Each path was analysed independently of the oth-
ers, and the resulting recommendations evaluated as
a whole. The measures of precision and recall in
the evaluation of recommender systems has been
applied effectively in previous studies (Raskutti et
al., 1997; Basu et al, 1998). In the second test
precision is the measure we are primarily concerned
with: it is not the aim of this recommender system to
predict all elements of a visitor?s path in the correct
order. The correctness of the exhibits predicted is
more important than the quantity of the predictions
the visitor visits, hence only exhibits predicted with
a (relatively) high probability are included in the
final list of predicted exhibits for that visitor.
The thresholds are designed to increase the cor-
rectness of the predictions, by only making a pre-
diction if there is a high probability of the visitor
travelling to the exhibit. As all predictive methods
choose the most probable transition from all possible
transitions, the transition with the highest probabil-
ity is always selected. The threshold values simply
cut off all probabilities below a certain value.
5 Results and Evaluation
The first tests carried out were done only using the
simple probability matrices described in Section 4,
and hence only use the information associated with
the visitor?s current location and not the entirety of
their history. The baseline method being used in all
testing is the naive method of moving to the closest
not-yet-visited exhibit.
53
Method BOE Accuracy
Proximity (baseline) 0.270 0.192
Popularity 0.406 0.313
Tf?Idf 0.130 0.018
Lin 0.129 0.039
Leacock-Chodorow 0.116 0.024
Banerjee-Pedersen 0.181 0.072
Popularity - Tf?Idf 0.196 0.093
Popularity - Lin 0.225 0.114
Popularity - Leacock-Chodorow 0.242 0.130
Popularity - Banerjee-Pedersen 0.163 0.064
Proximity - Tf?Idf 0.205 0.084
Proximity - Lin 0.180 0.114
Proximity - Leacock-Chodorow 0.220 0.151
Proximity - Banerjee-Pedersen 0.205 0.105
Proximity - Popularity 0.232 0.129
Table 1: Single exhibit history using individual and
combined transitional probabilities
In order to prevent specialisation of the methods
over the training data (the aforementioned 60 visitor
paths), 60 fold cross-validation was used. With the
path being used as the test case removed from the
training data at each iteration.
The results of prediction using only the current
exhibit as information can be seen in Table 1. Com-
binations of predictive methods are also included to
add physical environment factors to conceptual sim-
ilarity methods. For example, if two exhibits may
be highly related conceptually but on opposite sides
of the exhibit space, a visitor may forgo the distant
exhibit in favour of a closer exhibit that is slightly
less relevant.
Due to the lengths of the recommendation sets
made for each visitor (a recommendation is made
for each exhibit visited), precision and recall are
identical. The measure of Bag Of Exhibits (BOE)
describes the percentage of exhibits that were visited
by the visitor, but not necessarily in the same order
as they were recommended. The BOE measure is
the same as measuring precision and recall for the
purposes of this evaluation. With the introduction of
thresholds to improve precision, precision and recall
are measured as separate entities.
As seen in Table 1 the performance of the
conceptual or information similarity methods
(the tf?idf method, Lin, Leacock-Chodorow and
Banerjee-Pedersen) is worse than that of the
methods based on static features of the exhibits,
and all perform worse than the baseline. In
order to produce a higher percentage of correct
recommendations, thresholds were introduced.
Using thresholds, a recommendation is only made
if the probability of a visitor visiting an exhibit next
is above a given percentage. The thresholds used
in Table 2 are arbitrary, and were arrived at after
experimentation.
It is worth noting that in both tests, with and
without thresholds, the method of exhibit popularity
based on visitor paths is the most successful. One
expects this trend to continue with the introduction
of the history based model described in Section 4.
Each transitional probability matrix was used in con-
junction with the history model, the results of this
experimentation can be seen in Table 3.
Only single transitional probability matrices are
used in conjunction with the history model. The
physical distance to an exhibit is only relevant to the
current prediction, the distance travelled in the past
from exhibit to exhibit is irrelevant, and so physical
conceptual combinations are not necessary. A model
such as this describes the evolution of a thought pro-
cess, or is able to identify the common conceptual
thread linking the exhibits in a visitor?s path. This
is only true if the visitor has a conceptual model in
mind when touring the museum. Without the aid of
a common information thread, conceptual predictive
methods based on exhibit information content will
always perform poorly.
6 Discussion
The visitor paths supplied by the Melbourne
Museum represent sequential lists of exhibits, and
each visitor is a black box travelling from exhibit
to exhibit. It is this token vs. type problem that
does not allow us to select an appropriate predictive
method with which to make recommendations.
Instead a broad coverage method is necessary. Use
of history models to analyse entire visitor paths are
less successful than analysis of solely the current
location of the visitor. This can be attributed to the
fact that a majority of the visitors tracked may not
have had preconceived tasks in mind when they
entered the museum space, and just moved from
one visually impressive exhibit to the next. The
visitors do not consider their entire history as being
relevant, and only take into account their current
54
Method Threshold Precision Recall F-score
Proximity 0.03 0.271 0.270 0.270
Popularity 0.06 0.521 0.090 0.153
Tf?Idf 0.06 0.133 0.122 0.128
Lin 0.01 0.129 0.129 0.129
Leacock-Chodorow 0.01 0.117 0.117 0.117
Banerjee-Pedersen 0.01 0.182 0.180 0.181
Popularity - Tf?Idf 0.001 0.176 0.154 0.164
Popularity - Lin 0.0005 0.383 0.316 0.348
Popularity - Leacock-Chodorow 0.0005 0.430 0.349 0.385
Popularity - Banerjee-Pedersen 0.001 0.236 0.151 0.184
Proximity - Tf?Idf 0.001 0.189 0.174 0.181
Proximity - Lin 0.0005 0.239 0.237 0.238
Proximity - Leacock-Chodorow 0.0005 0.252 0.250 0.251
Proximity - Banerjee-Pedersen 0.0005 0.182 0.180 0.181
Proximity - Popularity 0.001 0.262 0.144 0.186
Table 2: Single exhibit history predictive methods using thresholds
Method BOE Accuracy
Proximity 0.066 0.0
Popularity 0.016 0.0
Tf?Idf 0.033 0.0
Lin 0.064 0.0
Leacock-Chodorow 0.036 0.0
Banerjee-Pedersen 0.036 0.0
Table 3: Entire visitor history predictive methods.
context. This also explains the relative success of
the predictive method built from analysis of the
visitor paths, presenting a marked improvement
over the baseline of nearest exhibit. In the best case
(as seen in Table 2) the exhibit popularity predictive
method was able to give relevant recommendations
52% of the time.
The interaction between predictive methods here
is highly simplified. The assumption made is that all
aspects of the visitor?s conceptual model are inde-
pendent, or only interact on a superficial level (see
the lower halves of Tables 1?2). More complex
methods of prediction need to be explored fully
take into account the interaction between predictive
methods.
Representations based on physical proximity take
into account little of how a visitor conceptualises a
museum space. They do however describe the fact
that closer exhibits are more visible to visitors, and
are hence more likely to be visited. Proximity can
be used as an augmentation to a conceptual model
designed to be used within a physical space.
Any exhibit is best described by the information it
contains. Visitors with a specific task in mind when
entering an exhibition already have a pre-initialised
conceptual model, relating to a theme. The visitors
seek out content related to their conceptual model,
and separate the bulk of the collection content from
the information they require. The representation of
the content within each exhibit as a vocabulary of
terms allows us to find similarity between exhibits.
The data available at the time of this testing does not
make the distinction between user types, and so only
broad coverage methods result in a improvements.
With the introduction of user types to the data sup-
plied by the museum, specific predictive methods
can be applied to each individual user. This addi-
tional information can be significantly beneficial as
the specialisation of predictive types to visitors is
expected to produce much more accurate predictions
and recommendations. Currently the only method
available to discern the user type is to analyse the
length of time the visitor spends at each each exhibit.
This data is yet to be adapted and annotated from the
raw data supplied by the Melbourne Museum.
7 Conclusion
The above methods are intended to represent base-
line components of possible conceptual models that
represent how a visitor is able to selectively assess
the dynamic context of museum visits. The model
that a visitor generates for themselves is unique, and
is difficult to represent in terms of physical attributes
of exhibits.
55
Being able to predict future actions of a user
within a given environment allows a recommender
system to influence a user?s choices. Key to the pre-
diction of future actions, is the idea that a user has
a conceptual model of how they see content within
the environment in relation to a task. With respect to
a museum environment, the majority of users have
no preconceived conceptual model upon entering an
exhibition and must build one as they explore the
environment. Users with a preconceived task will
more often than not stick to exhibits surrounding
a particular theme. Use of a language-based con-
ceptual model based on the information contained
within an exhibit can be combined with conceptual
models based on geospatial attributes of the exhibit
to create a representation of how a user will react
to an exhibit. The use of heterogeneous information
contained within the exhibit space is only relevant
when the visitor has an information-centric task in
mind.
7.1 Future Work
The methods dealing with a language-based concep-
tual model given here are very basic, and the overall
accuracy and precision of the recommender system
components require improvement. Additional anno-
tation of the paths of visitors to the museum will
enable proper evaluation of conceptual information
based predictive methods. On-site testing of predic-
tive methods at the Melbourne Museum is the ulti-
mate goal of this project, and testing the effects of
visitor feedback on recommendations will also be
analysed. In order to gain more insight into vis-
itor behaviour, the current small-scale set of visi-
tors needs to be expanded to include multiple visitor
types, as well as tasks.
Acknowledgments
This research was supported by Australian Research Council
DP grant no. DP0770931. The authors wish to thank the staff
of the Melbourne Museum for their help in this study. Special
thanks goes to Carolyn Meehan and Alexa Reynolds for their
gathering of data, and helpful suggestions throughout this study.
Thanks also goes to Ingrid Zukerman and Liz Sonenberg for
their input on this research.
References
Chumki Basu, Haym Hirsh, and William Cohen. 1998. Rec-
ommendations as classification: Using social and content-
based information in recommendation. In Proceedings of the
National Conference of Artificial Intelligence, pages 714?
720, Madison, United States.
Giuliano Benelli, Alberto Bianchi, Patrizia Marti, David Sen-
nati, and Elena Not. 1999. HIPS: Hyper-Interaction within
Physical Space. In ICMCS ?99: Proceedings of the IEEE
International Conference on Multimedia Computing and
Systems, volume 2, page 1075. IEEE Computer Society.
Steve Benford, John Bowers, Paul Chandler, Luigina Ciolfi,
Martin Flintham, Mike Fraser, Chris Greenhalgh, Tony Hall,
Sten-Olof Hellstrom, Shahram Izadi, Tom Rodden, Holger
Schnadelbach, and Ian Taylor. 2001. Unearthing virtual
history: using diverse interfaces to reveal hidden worlds. In
Proc Ubicomp, pages 1?6. ACM.
Steven Bird. 2005. NLTK-Lite: Efficient scripting for natural
language processing. In Proceedings of the 4th International
Conference on Natural Language Processing (ICON), pages
11?18, Kanpur, India.
Matthew Chalmers, Kerry Rodden, and Dominique Brodbeck.
1998. The Order of Things: Activity-Centred Information
Access. Computer Networks and ISDN Systems, 30:1?7.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Silvia Filippini. 2003. Personalisation through IT in museums:
Does it really work? Presentation at ICHIM 2003.
Stephen J. Green, Maria Milosavljevic, Robert Dale, and Cecile
Paris. 1999. When virtual documents meet the real world.
In Proc. of WWW8 Workshop: Virtual Documents, Hypertext
Functionality and the Web.
Janet Hitzeman, Chris Mellish, and Jon Oberlander. 1997.
Dynamic generation of museum web pages: The intelli-
gent labelling explorer. Archives and Museum Informatics,
11(2):117?115.
Claudia Leacock, Martin Chodorow, and George A Miller.
1998. Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics, 24(1):147?
65.
Dekang Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In (CoLING)-(ACL), pages 768?774, Montreal,
Canada.
Siddharth Patwardhan and Ted Pedersen. 2003. Extended gloss
overlaps as a measure of semantic relatedness. In Interna-
tional Joint Conference on Artificial Intelligence, pages 805?
810, Acapulco, Mexico.
John Peponis, Ruth Conroy Dalton, Jean Wineman, and Nick
Dalton. 2004. Measuring the effect of layout on visitors?
spatial behaviors in open plan exhibition settings. Environ-
ment and Planning B: Planning and Design, 31:453?473.
Bhavani Raskutti, Anthony Beitz, and Belinda Ward. 1997. A
feature-based approach to recommending selections based
on past preferences. User Modelling and User Adaption,
7(3):179?218.
Paul Resnick and Hal R Varian. 1997. Recommender systems.
Commun. ACM, 40(3):56?58.
Ingrid Zukerman and David W Albrecht. 2001. Predictive
statistical models for user modeling. User Modeling and
User-Adapted Interaction, 11(1?2):5?18.
56
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 100?108,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation of Topic Coherence
David Newman,?? Jey Han Lau,? Karl Grieser?, and Timothy Baldwin,??
? NICTA Victoria Research Laboratory, Australia
? Dept of Computer Science, University of California, Irvine
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? Dept of Information Systems, University of Melbourne, Australia
newman@uci.edu, depthchargex@gmail.com,
kgrieser@csse.unimelb.edu.au, tb@ldwin.net
Abstract
This paper introduces the novel task of topic
coherence evaluation, whereby a set of words,
as generated by a topic model, is rated for
coherence or interpretability. We apply a
range of topic scoring models to the evaluation
task, drawing on WordNet, Wikipedia and the
Google search engine, and existing research
on lexical similarity/relatedness. In compar-
ison with human scores for a set of learned
topics over two distinct datasets, we show a
simple co-occurrence measure based on point-
wise mutual information over Wikipedia data
is able to achieve results for the task at or
nearing the level of inter-annotator correla-
tion, and that other Wikipedia-based lexical
relatedness methods also achieve strong re-
sults. Google produces strong, if less consis-
tent, results, while our results over WordNet
are patchy at best.
1 Introduction
There has traditionally been strong interest within
computational linguistics in techniques for learning
sets of words (aka topics) which capture the latent
semantics of a document or document collection, in
the form of methods such as latent semantic analysis
(Deerwester et al, 1990), probabilistic latent seman-
tic analysis (Hofmann, 2001), random projection
(Widdows and Ferraro, 2008), and more recently, la-
tent Dirichlet alocation (Blei et al, 2003; Griffiths
and Steyvers, 2004). Such methods have been suc-
cessfully applied to a myriad of tasks including word
sense discrimination (Brody and Lapata, 2009), doc-
ument summarisation (Haghighi and Vanderwende,
2009), areal linguistic analysis (Daume III, 2009)
and text segmentation (Sun et al, 2008). In each
case, extrinsic evaluation has been used to demon-
strate the effectiveness of the learned topics in the
application domain, but standardly, no attempt has
been made to perform intrinsic evaluation of the top-
ics themselves, either qualitatively or quantitatively.
In machine learning, on the other hand, researchers
have modified and extended topic models in a vari-
ety of ways, and evaluated intrinsically in terms of
model perplexity (Wallach et al, 2009), but there has
been less effort on qualitative understanding of the
semantic nature of the learned topics.
This research seeks to fill the gap between topic
evaluation in computational linguistics and machine
learning, in developing techniques to perform intrin-
sic qualitative evaluation of learned topics. That
is, we develop methods for evaluating the qual-
ity of a given topic, in terms of its coherence to
a human. After learning topics from a collection
of news articles and a collection of books, we ask
humans to decide whether individual learned top-
ics are coherent, in terms of their interpretability
and association with a single over-arching seman-
tic concept. We then propose models to predict
topic coherence, based on resources such as Word-
Net, Wikipedia and the Google search engine, and
methods ranging from ontological similarity to link
overlap and term co-occurrence. Over topics learned
from two distinct datasets, we demonstrate that there
is remarkable inter-annotator agreement on what is
a coherent topic, and additionally that our methods
based on Wikipedia are able to achieve nearly perfect
agreement with humans over the evaluation of topic
coherence.
This research forms part of a larger research
agenda on the utility of topic modelling in gist-
ing and visualising document collections, and ulti-
mately enhancing search/discovery interfaces over
100
document collections (Newman et al, to appeara).
Evaluating topic coherence is a component of the
larger question of what are good topics, what char-
acteristics of a document collection make it more
amenable to topic modelling, and how can the po-
tential of topic modelling be harnessed for human
consumption (Newman et al, to appearb).
2 Related Work
Most earlier work on intrinsically evaluating learned
topics has been on the basis of perplexity results,
where a model is learned on a collection of train-
ing documents, then the log probability of the un-
seen test documents is computed using that learned
model. Usually perplexity is reported, which is the
inverse of the geometric mean per-word likelihood.
Perplexity is useful for model selection and adjust-
ing parameters (e.g. number of topics T ), and is
the standard way of demonstrating the advantage of
one model over another. Wallach et al (2009) pre-
sented efficient and unbiased methods for computing
perplexity and evaluating almost any type of topic
model.
While statistical evaluation of topic models is
reasonably well understood, there has been much
less work on evaluating the intrinsic semantic qual-
ity of topics learned by topic models, which could
have a far greater impact on the overall value of
topic modeling for end-user applications. Some re-
searchers have started to address this problem, in-
cluding Mei et al (2007) who presented approaches
for automatic labeling of topics (which is core to the
question of coherence and semantic interpretabil-
ity), and Griffiths and Steyvers (2006) who applied
topic models to word sense discrimination tasks.
Misra et al (2008) used topic modelling to identify
semantically incoherent documents within a docu-
ment collection (vs. coherent topics, as targeted in
this research). Chang et al (2009) presented the
first human-evaluation of topic models by creating
a task where humans were asked to identify which
word in a list of five topic words had been ran-
domly switched with a word from another topic.
This work showed some possibly counter-intuitive
results, where in some cases humans preferred mod-
els with higher perplexity. This type of result shows
the need for further exploring measures other than
perplexity for evaluating topic models. In earlier
work, we carried out preliminary experimentation
using pointwise mutual information and Google re-
sults to evaluate topic coherence over the same set
of topics as used in this research (Newman et al,
2009).
Part of this research takes inspiration from the
work on automatic evaluation in machine translation
(Papineni et al, 2002) and automatic summarisation
(Lin, 2004). Here, the development of automated
methods with high correlation with human subjects
has opened the door to large-scale automated evalua-
tion of system outputs, revolutionising the respective
fields. While our aspirations are more modest, the
basic aim is the same: to develop a fully-automated
method for evaluating a well-grounded task, which
achieves near-human correlation.
3 Topic Modelling
In order to evaluate topic modelling, we require a
topic model and set of topics for a given document
collection. While the evaluation methodology we
describe generalises to any method which gener-
ates sets of words, all of our experiments are based
on Latent Dirichlet Allocation (LDA, aka Discrete
Principal Component Analysis), on the grounds that
it is a state-of-the-art method for generating topics.
LDA is a Bayesian graphical model for text docu-
ment collections represented by bags-of-words (see
Blei et al (2003), Griffiths and Steyvers (2004),
Buntine and Jakulin (2004)). In a topic model, each
document in the collection of D documents is mod-
elled as a multinomial distribution over T topics,
where each topic is a multinomial distribution over
W words. Typically, only a small number of words
are important (have high likelihood) in each topic,
and only a small number of topics are present in each
document.
The collapsed Gibbs sampled topic model simul-
taneously learns the topics and the mixture of topics
in documents by iteratively sampling the topic as-
signment z to every word in every document, using
the Gibbs sampling update:
p(zid = t|xid = w, z?id) ?
N?idwt + ?
?
w N?idwt + W?
N?idtd + ?
?
t N?idtd + T?
101
where zid = t is the assignment of the ith word in
document d to topic t, xid = w indicates that the
current observed word is w, and z?id is the vector of
all topic assignments not including the current word.
Nwt represents integer count arrays (with the sub-
scripts denoting what is counted), and ? and ? are
Dirichlet priors.
The maximum a posterior (MAP) estimates of the
topics p(w|t), t = 1 . . . T are given by:
p(w|t) = Nwt + ??
w Nwt + W?
We will follow the convention of representing a
topic via its top-n words, ordered by p(w|t). Here,
we use the top-ten words, as they usually provide
sufficient detail to convey the subject of a topic,
and distinguish one topic from another. For the
remainder of this paper, we will refer to individ-
ual topics by its list of top-ten words, denoted by
w = (w1, . . . , w10).
4 Topic Evaluation Methods
We experiment with scoring methods based on
WordNet (Section 4.1), Wikipedia (Section 4.2) and
the Google search engine (Section 4.3). In the case
of Google, we query for the entire topic, but with
WordNet and Wikipedia, this takes the form of scor-
ing each word-pair in a given topic w based on the
component words (w1, . . . , w10). Given some (sym-
metric) word-similarity measure D(wi, wj), two
straightforward ways of producing a combined score
from the 45 (i.e.
(10
2
)
) word-pair scores are: (1) the
arithmetic mean, and (2) the median, as follows:
Mean-D-Score(w) =
mean{D(wi, wj), ij ? 1 . . . 10, i < j}
Median-D-Score(w) =
median{D(wi, wj), ij ? 1 . . . 10, i < j}
Intuitively, the median seems the more natural rep-
resentation, as it is less affected by outlier scores,
but we experiment with both, and fall back to empir-
ical verification of which is the better combination
method.
4.1 WordNet similarity
WordNet (Fellbaum, 1998) is a lexical ontology
that represents word sense via ?synsets?, which
are structured in a hypernym/hyponym hierarchy
(nouns) or hypernym/troponym hierarchy (verbs).
WordNet additionally links both synsets and words
via lexical relations including antonymy, morpho-
logical derivation and holonymy/meronym.
In parallel with the development of WordNet, a
number of computational methods for calculating
the semantic relatedness/similarity between synset
pairs (i.e. sense-specified word pairs) have been de-
veloped, as we outline below. These methods ap-
ply to synset rather than word pairs, so to generate a
single score for a given word pair, we look up each
word in WordNet and exhaustively generate scores
for each sense pairing defined by them, and calcu-
late their arithmetic mean.1
The majority of the methods (all methods other
than HSO, VECTOR and LESK) are restricted to op-
erating strictly over hierarchical links within a sin-
gle hierarchy. As the verb and noun hierarchies are
not connected (other than via derivational links), this
means that it is generally not possible to calculate
the similarity between noun and verb senses, for ex-
ample. In such cases, we simply drop the synset
pairing in question from our calculation of the mean.
The least common subsumer (LCS) is a common
feature to a number of the measures, and is defined
as the deepest node in the hierarchy that subsumes
both of the synsets under question.
For all our experiments over WordNet, we use the
WordNet::Similarity package.
Path distance (PATH)
The simplest of the WordNet-based measures is
to count the number of nodes visited while going
from one word to another via the hypernym hierar-
chy. The path distance between two nodes is de-
fined as the number of nodes that lie on the short-
est path between two words in the hierarchy. This
1We also experimented with the median, and trialled filter-
ing the set of senses in a variety of ways, e.g. using only the
first sense (the sense with the highest prior) for a given word,
or using only the word senses associated with the POS with the
highest prior. In all cases, the overall trend was for the correla-
tion with the human scores to drop relative to the mean, so we
only present the numbers for the mean in this paper.
102
count of nodes includes the beginning and ending
word nodes.
Leacock-Chodorow (LCH)
The measure of semantic similarity devised by
Leacock et al (1998) finds the shortest path between
two WordNet synsets (sp(c1, c2)) using hypernym
and synonym relationships. This path length is then
scaled by the maximum depth of WordNet (D), and
the log likelihood taken:
simlch(c1, c2) = ? log
sp(c1, c2)
2 ?D
Wu-Palmer (WUP)
Wu and Palmer (1994) proposed to scale the depth
of the two synset nodes (depthc1 and depthc2) by
the depth of their LCS (depth(lcsc1,c2)):
simwup(c1, c2) =
2 ? depth(lcsc1,c2)
depthc1 + depthc2 + 2 ? depth(lcsc1,c2)
The scaling means that specific terms (deeper in the
hierarchy) that are close together are more semanti-
cally similar than more general terms, which have a
short path distance between them. Only hypernym
relationships are used in this measure, as the LCS
is defined by the common member in the concepts?
hypernym path.
Hirst-St Onge (HSO)
Hirst and St-Onge (1998) define a measure of se-
mantic similarity based on length and tortuosity of
the path between nodes. Hirst and St-Onge attribute
directions (up, down and horizontal) to the larger set
of WordNet relationships, and identify the path from
one word to another utilising all of these relation-
ships. The relatedness score is then computed by
the weighted sum of the path length between the two
words (len(c1, c2)) and the number of turns the path
makes (turns(c1, c2)) to take this route:
relhso(c1, c2) =
C ? len(c1, c2)? k ? turns(c1, c2)
where C and k are constants. Additionally, a set of
restrictions is placed on the path so that it may not
be more than a certain length, may not contain more
than a set number of turns, and may only take turns
in certain directions.
Resnik Information Content (RES)
Resnik (1995) presents a method for weighting
edges in WordNet (avoiding the assumption that all
edges between nodes have equal importance), by
weighting edges between nodes by their frequency
of use in textual corpora.
Resnik found that the most effective measure of
comparison using this methodology was to measure
the Information Content (IC(c) = ? log p(c)) of
the subsumer with the greatest Information Content
from the set of all concepts that subsumed the two
initial concepts (S(c1, c2)) being compared:
simres(c1, c2) = max
c?S(c1,c2)
[? log p(c)]
Lin (LIN)
Lin (1998) expanded on the Information Theo-
retic approach presented by Resnik by scaling the
Information Content of each node by the informa-
tion content of their LCS:
simlin(c1, c2) =
2? log p(lcsc1,c2)
log p(c1) + log p(c2)
This measure contrasts the joint content of the two
concepts with the difference between them.
Jiang-Conrath (JCN)
Jiang and Conrath (1997) define a measure that
utilises the components of the information content
of the LCS in a different manner:
simjcn(c1, c2) =
1
IC(a) + IC(b)? 2? IC(lcsa,b)
Instead of defining commonality and difference as
with Lin?s measure, the key determinant is the speci-
ficity of the two nodes compared with their LCS.
Lesk (LESK)
Lesk (1986) proposed a significantly different ap-
proach to lexical similarity to that proposed in the
methods presented above, using the lexical over-
lap in dictionary definitions (or glosses) to disam-
biguate word sense. The sense definitions that con-
tain the most words in common indicate the most
likely sense of the word given its co-occurrence with
similar word senses. Banerjee and Pedersen (2002)
103
adapted this method to utilise WordNet sense glosses
rather than dictionary definitions, and expand the
dictionary definitions via ontological links, and it is
this method we experiment with in this paper.
Vector (VECTOR)
Schu?tze (1998) uses the words surrounding a term
in a piece of text to form a context vector that de-
scribes the context in which the word sense appears.
For a set of words associated with a target sense, a
context vector is computed as the centroid vector of
these words. The centroid context vectors each rep-
resent a word sense. To compare word senses, the
cosine similarity of the context vectors is used.
4.2 Wikipedia
In the last few years, there has been a surge of in-
terest in using Wikipedia to calculate semantic sim-
ilarity, using the Wikipedia article content, in-article
links and document categories (Stru?be and Ponzetto,
2006; Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008). We present a selection of such meth-
ods below. There are a number of Wikipedia-based
scoring methods which we do not present results
for here (notably Stru?be and Ponzetto (2006) and
Gabrilovich and Markovitch (2007)), due to their
computational complexity and uncertainty about the
full implementation details of the methods.
As with WordNet, a given word will often have
multiple entries in Wikipedia, grouped in a disam-
biguation page. For MIW, RACO and DOCSIM,
we apply the same strategy as we did with Word-
Net, in exhaustively calculating the pairwise scores
between the sets of documents associated with each
term, and averaging across them.
Milne-Witten (MIW)
Milne and Witten (2008) adapted the Resnik
(1995) methodology to utilise the count of links
pointing to an article. As Wikipedia is self-
referential (articles link to related articles), no ex-
ternal data is needed to find the ?referred-to-edness?
of a concept. Milne and Witten use an adapted In-
formation Content measure that weights the number
of links from one article to another (c1 ? c2) by the
total number of links to the second article:
w(c1 ? c2) = |c1 ? c2| ? log
?
x?W
|W |
|c1, x)|
where x is an article in W , Wikipedia. This mea-
sure provides the similarity of one article to another,
however this is asymmetrical. The above metric is
used to find the weights of all outlinks from the two
articles being compared:
~c1 = (w(c1 ? l1), w(c1 ? l2), ? ? ? , w(c1 ? ln))
~c2 = (w(c2 ? l1), w(c2 ? l2), ? ? ? , w(c2 ? ln))
for the set of links l that is the union of the sets of
outlinks from both articles. The overall similarity
of the two articles is then calculated by taking the
cosine similarity of the two vectors.
Related Article Concept Overlap (RACO)
We also determine the category overlap of two
articles by examining the outlinks of both articles,
in the form of the Related Article Concept Overlap
(RACO) measure. The concept overlap of the sets
of respective outlinks is given by the union of the
two sets of categories from the outlinks from each
article:
overlap(c1, c1) =
?
?
(
?
l?ol(c1)
cat(l)
)
?
(
?
l?ol(c2)
cat(l)
)
?
?
where ol(c1) is the set of outlinks from article c1,
and cat(l) is the set of categories of which the arti-
cle at outlink l is a member. To account for article
size (and differing number of outlinks), the Jaccard
coefficient is used:
relraco(c1, c2) =
?
?
(
?
l?ol(c1) cat(l)
)
?
(
?
l?ol(c2) cat(l)
)
?
?
?
?
?
l?ol(c1) cat(l)
?
?+
?
?
?
l?ol(c2) cat(l)
?
?
Document Similarity (DOCSIM)
In addition to these two measures of semantic re-
latedness, we experiment with simple cosine simi-
larity of the text of Wikipedia articles as a measure
of semantic relatedness.
Term Co-occurrence (PMI)
Another variant is to treat Wikipedia as a single
meta-document and score word pairs using term co-
occurrence. Here, we calculate the pointwise mu-
tual information (PMI) of each word pair, estimated
104
Selected high-scoring topics (unanimous score=3):
[NEWS] space earth moon science scientist light nasa mission planet mars ...
[NEWS] health disease aids virus vaccine infection hiv cases infected asthma ...
[BOOKS] steam engine valve cylinder pressure piston boiler air pump pipe ...
[BOOKS] furniture chair table cabinet wood leg mahogany piece oak louis ...
Selected low-scoring topics (unanimous score=1):
[NEWS] king bond berry bill ray rate james treas byrd key ...
[NEWS] dog moment hand face love self eye turn young character ...
[BOOKS] soon short longer carried rest turned raised filled turn allowed ...
[BOOKS] act sense adv person ppr plant sax genus applied dis ...
Table 1: A selection of high-scoring and low-scoring topics
from the entire corpus of over two million English
Wikipedia articles (?1 billion words). PMI has been
studied variously in the context of collocation ex-
traction (Pecina, 2008), and is one measure of the
statistical independence of observing two words in
close proximity. Using a sliding window of 10-
words to identify co-occurrence, we computed the
PMI of all a given word pair (wi, wj) as, following
Newman et al (2009):
PMI(wi, wj) = log
p(wi, wj)
p(wi)p(wj)
4.3 Search engine-based similarity
Finally, we present two search engine-based scor-
ing methods, based on Newman et al (2009). In
this case the external data source is the entire World
Wide Web, via the Google search engine. Unlike
the methods presented above, here we query for the
topic in its entirety,2 meaning that we return a topic-
level score rather than scores for individual word or
word sense pairs. In each case, we mark each search
term with the advanced search option + to search
for the terms exactly as is and prevent Google from
using synonyms or lexical variants of the term. An
example query is: +space +earth +moon +science
+scientist +light +nasa +mission +planet +mars.
Google title matches (TITLES)
Firstly, we score topics by the relative occurrence
of their component words in the titles of documents
returned by Google:
Google-titles-match(w) = 1 [wi = vj ]
2All queries were run on 15/09/2009.
where i = 1, . . . , 10 and j = 1, . . . , |V |, vj are
all the unique terms mentioned in the titles from the
top-100 search results, and 1 is the indicator function
to count matches. For example, in the top-100 re-
sults for our query above, there are 194 matches with
the ten topic words, so Google-titles-match(w) =
194.
Google log hits matches (LOGHITS)
Second, we issue queries as above, but return the
log number of hits for our query:
Google-log-hits(w) =
log10(# results from search for w)
where w is the search string +w1 +w2 +w3 . . .
+w10. For example, our query above returns
171,000 results, so Google-log-hits(w) = 5.2. and
the URL titles from the top-100 results include a to-
tal of 194 matches with the ten topic words, so for
this topic Google-titles-match(w)=194.
5 Experimental Setup
We learned topics for two document collections: a
collection of news articles, and a collection of books.
These collections were chosen to produce sets of
topics that have more variable quality than one typi-
cally observes when topic modeling highly uniform
content. The collection of D = 55, 000 news arti-
cles was selected from English Gigaword, and the
collection of D = 12, 000 books was downloaded
from the Internet Archive. We refer to these collec-
tions as NEWS and BOOKS, respectively.
Standard procedures were used to tokenize each
collection and create the bags-of-words. We learned
105
Resource Method Median Mean
WordNet
HSO ?0.29 0.34
JCN 0.08 0.22
LCH ?0.18 ?0.07
LESK 0.38 0.37
LIN 0.18 0.25
PATH 0.19 0.11
RES ?0.10 0.13
VECTOR 0.07 0.20
WUP 0.03 0.10
Wikipedia
RACO 0.61 0.63
MIW 0.69 0.60
DOCSIM 0.45 0.50
PMI 0.78 0.77
Google TITLES 0.80LOGHITS 0.46
Gold-standard IAA 0.79 0.73
Table 2: Spearman rank correlation ? values for the
different scoring methods over the NEWS dataset (best-
performing method for each resource underlined; best-
performing method overall in boldface)
topic models of NEWS and BOOKS using T = 200
and T = 400 topics respectively. We randomly
selected a total of 237 topics from the two collec-
tions for user scoring. We asked N = 9 users to
score each of the 237 topics on a 3-point scale where
3=?useful? (coherent) and 1=?useless? (less coher-
ent).
We provided annotators with a rubric and guide-
lines on how to judge whether a topic was useful
or useless. In addition to showing several examples
of useful and useless topics, we instructed users to
decide whether the topic was to some extent coher-
ent, meaningful, interpretable, subject-heading-like,
and something-you-could-easily-label. For our pur-
poses, the usefulness of a topic can be thought of
as whether one could imagine using the topic in a
search interface to retrieve documents about a par-
ticular subject. One indicator of usefulness is the
ease by which one could think of a short label to de-
scribe a topic.
Table 1 shows a selection of high- and low-
scoring topics, as scored by the N = 9 users. The
first topic illustrates the notion of labelling coher-
ence, as space exploration, e.g., would be an obvi-
ous label for the topic. The low-scoring topics dis-
play little coherence, and one would not expect them
Resource Method Median Mean
WordNet
HSO 0.15 0.59
JCN ?0.20 0.19
LCH ?0.31 ?0.15
LESK 0.53 0.53
LIN 0.09 0.28
PATH 0.29 0.12
RES 0.57 0.66
VECTOR ?0.08 0.27
WUP 0.41 0.26
Wikipedia
RACO 0.62 0.69
MIW 0.68 0.70
DOCSIM 0.59 0.60
PMI 0.74 0.77
Google TITLES 0.51LOGHITS ?0.19
Gold-standard IAA 0.82 0.78
Table 3: Spearman rank correlation ? values for the dif-
ferent scoring methods over the BOOKS dataset (best-
performing method for each resource underlined; best-
performing method overall in boldface)
to be useful as categories or facets in a search inter-
face. Note that the useless topics from both collec-
tions are not chance artifacts produced by the mod-
els, but are in fact stable and robust statistical fea-
tures in the data sets.
6 Results
The results for the different topic scoring methods
over the NEWS and BOOKS collections are pre-
sented in Tables 2 and 3, respectively. In each ta-
ble, we separate out the scoring methods into those
based on WordNet (from Section 4.1), those based
on Wikipedia (from Section 4.2), and those based on
Google (from Section 4.3).
As stated in Section 4, we experiment with two
methods for combining the word-pair scores (for all
methods other than the two Google methods, which
operate natively over a word set), namely the arith-
metic mean and median. We present the numbers
for these two methods in each table. In each case,
we evaluate via Spearman rank correlation, revers-
ing the sign of the calculated ? value for PATH (as it
is the only instance of a distance metric, where the
gold-standard is made up of similarity values).
We include the inter-annotator agreement (IAA)
in the final row of each table, which we consider
106
to be the upper bound for the task. This is calcu-
lated as the average Spearman rank correlation be-
tween each annotator and the mean/median of the
remaining annotators for that topic. Encouragingly,
there is relatively little difference in the IAA be-
tween the two datasets; the median-based calcula-
tion produces slightly higher ? values and is empiri-
cally the method of choice.3
Of all the topic scoring methods tested, PMI
(term co-occurrence via simple pointwise mutual in-
formation) is the most consistent performer, achiev-
ing the best or near-best results over both datasets,
and approaching or surpassing the inter-annotator
agreement. This indicates both that the task of
topic evaluation as defined in this paper is com-
putationally tractable, and that word-pair based co-
occurrence is highly successful at modelling topic
coherence.
Comparing the different resources, Wikipedia is
far and away the most consistent performing, with
PMI producing the best results, followed by MIW
and RACO, and finally DOCSIM. There is rela-
tively little difference in results between NEWS and
BOOKS for the Wikipedia methods. Google achieves
the best results over NEWS, for TITLES (actually
slightly above the IAA), but the results fall away
sharply over BOOKS. The reason for this can be
seen in the sample topics in Table 1: the topics for
BOOKS tend to be more varied in word class than
for NEWS, and contain less proper names; also, the
genre of BOOKS is less well represented on the web.
We hypothesise that Wikipedia?s encyclopedic na-
ture means that it has good coverage over both do-
mains, and thus more robust.
Turning to WordNet, the overall results are
markedly better over BOOKS, again largely because
of the relative sparsity of proper names in the re-
source. The results for individual methods are some-
what surprising. Whereas JCN and LCH have been
shown to be two of the best-performing methods
over lexical similarity tasks (Budanitsky and Hirst,
2005; Agirre et al, 2009), they perform abysmally
at the topic scoring task. Indeed, the spread of re-
sults across the WordNet similarity methods (no-
3Note that the choice of mean or median for IAA is in-
dependent of that for the scoring methods, as they are com-
bining different things: annotator scores in the one hand, and
word/concept pair scores on the other.
tably HSO, JCN, LCH, LIN, RES and WUP) is
much greater than we had expected. The single most
consistent method is LESK, which is based on lexi-
cal overlap in definition sentences and makes rela-
tively modest use of the WordNet hierarchy. Supple-
mentary evaluation where we filtered out all proper
nouns from the topics (based on simple POS priors
for each word learned from an automatically-tagged
version of the British National Corpus) led to a slight
increase in results for the WordNet methods; the full
results are omitted for reasons of space. In future
work, we intend to carry out error analysis to deter-
mine why some of the methods performed so badly,
or inconsistently across the two datasets.
There is no clear answer to the question of
whether the mean or median is the best method for
combining the pair-wise scores.
7 Conclusions
We have proposed the novel task of topic coher-
ence evaluation as a form of intrinsic topic evalu-
ation with relevance in document search/discovery
and visualisation applications. We constructed
a gold-standard dataset of topic coherence scores
over the output of a topic model for two distinct
datasets, and evaluated a wide range of topic scor-
ing methods over this dataset, drawing on WordNet,
Wikipedia and the Google search engine. The sin-
gle best-performing method was term co-occurrence
within Wikipedia based on pointwise mutual infor-
mation, which achieve results very close to the inter-
annotator agreement for the task. Google was also
found to perform well over one of the two datasets,
while the results for the WordNet-based methods
were overall surprisingly low.
Acknowledgements
NICTA is funded by the Australian government as rep-
resented by Department of Broadband, Communication
and Digital Economy, and the Australian Research Coun-
cil through the ICT centre of Excellence programme. DN
has also been supported by a grant from the Institute of
Museum and Library Services, and a Google Research
Award.
References
E Agirre, E Alfonseca, K Hall, J Kravalova, M Pas?ca,
and A Soroa. 2009. A study on similarity and re-
107
latedness using distributional and WordNet-based ap-
proaches. In Proc. of HLT: NAACL 2009, pages 19?
27, Boulder, Colorado.
S Banerjee and T Pedersen. 2002. An adapted Lesk algo-
rithm for word sense disambiguation using WordNet.
Proc. of CICLing?02, pages 136?145.
DM Blei, AY Ng, and MI Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research,
3:993?1022.
S Brody and M Lapata. 2009. Bayesian word sense
induction. In Proc. of EACL 2009, pages 103?111,
Athens, Greece.
A Budanitsky and G Hirst. 2005. Evaluating WordNet-
based Measures of Lexical Sematic Relatedness.
Computational Linguistics, 32(1):13?47.
WL Buntine and A Jakulin. 2004. Applying discrete
PCA in data analysis. In Proc. of UAI 2004, pages
59?66.
J Chang, J Boyd-Graber, S Gerris, C Wang, and D Blei.
2009. Reading tea leaves: How humans interpret topic
models. In Proc. of NIPS 2009.
H Daume III. 2009. Non-parametric bayesian areal lin-
guistics. In Proc. of HLT: NAACL 2009, pages 593?
601, Boulder, USA.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society of Information Science, 41(6).
C Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press, Cambridge, USA.
E Gabrilovich and S Markovitch. 2007. Computing se-
mantic relatedness using Wikipedia-based explicit se-
mantic analysis. In Proc. of IJCAI?07, pages 1606?
1611, Hyderabad, India.
T Griffiths and M Steyvers. 2004. Finding scientific top-
ics. In Proc. of the National Academy of Sciences, vol-
ume 101, pages 5228?5235.
T Griffiths and M Steyvers. 2006. Probabilistic topic
models. In Latent Semantic Analysis: A Road to
Meaning.
A Haghighi and L Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proc. of HLT: NAACL 2009, pages 362?370, Boulder,
USA.
G Hirst and D St-Onge. 1998. Lexical chains as repre-
sentations of context for the detection and correction
of malapropism. In Fellbaum (Fellbaum, 1998), pages
305?332.
T Hofmann. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning,
42(1):177?196.
JJ Jiang and DW Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of COLING?97, pages 19?33, Taipei, Taiwan.
C Leacock, G A Miller, and M Chodorow. 1998. Using
corpus statistics and WordNet relations for sense iden-
tification. Computational Linguistics, 24(1):147?65.
M Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proc. of SIGDOC?86,
pages 24?26, Toronto, Canada.
D Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of COLING/ACL?98, pages 768?
774, Montreal, Canada.
C-Y Lin. 2004. ROUGE: a package for automatic
evaluation of summaries. In Proc. of the ACL 2004
Workshop on Text Summarization Branches Out (WAS
2004), pages 74?81, Barcelona, Spain.
Q Mei, X Shen, and CX Zhai. 2007. Automatic labeling
of multinomial topic models. In Proc. of KDD 2007,
pages 490?499.
D Milne and IH Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
Wikipedia links. In Proc. of AAAI Workshop on
Wikipedia and Artificial Intelligence, pages 25?30,
Chicago, USA.
H Misra, O Cappe, and F Yvon. 2008. Using LDA to
detect semantically incoherent documents. In Proc. of
CoNLL 2008, pages 41?48, Manchester, England.
D Newman, S Karimi, and L Cavedon. 2009. External
evaluation of topic models. In Proc. of ADCS 2009,
pages 11?18, Sydney, Australia.
D Newman, T Baldwin, L Cavedon, S Karimi, D Mar-
tinez, and J Zobel. to appeara. Visualizing docu-
ment collections and search results using topic map-
ping. Journal of Web Semantics.
D Newman, Y Noh, E Talley, S Karimi, and T Bald-
win. to appearb. Evaluating topic models for digital
libraries. In Proc. of JCDL/ICADL 2010, Gold Coast,
Australia.
K Papineni, S Roukos, T Ward, and W-J Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL 2002, pages 311?318,
Philadelphia, USA.
P Pecina. 2008. Lexical Association Measures: Colloca-
tion Extraction. Ph.D. thesis, Charles University.
P Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proc. of IJ-
CAI?95, pages 448?453, Montreal, Canada.
H Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
M Stru?be and SP Ponzetto. 2006. WikiRelate! comput-
ing semantic relateness using Wikipedia. In Proc. of
AAAI?06, pages 1419?1424, Boston, USA.
Q Sun, R Li, D Luo, and X Wu. 2008. Text segmentation
with LDA-based Fisher kernel. In Proc. of ACL-08:
HLT, pages 269?272.
HM Wallach, I Murray, R Salakhutdinov, and
DM Mimno. 2009. Evaluation methods for
topic models. In Proc. of ICML 2009, page 139.
D Widdows and K Ferraro. 2008. Semantic Vectors:
A scalable open source package and online technol-
ogy management application. In Proc. of LREC 2008,
Marrakech, Morocco.
Z Wu and M Palmer. 1994. Verb selection and lexical
selection. In Proc. of ACL?94, pages 133?138, Las
Cruces, USA.
108
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1536?1545,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatic Labelling of Topic Models
Jey Han Lau,?? Karl Grieser,? David Newman,?? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California Irvine
jhlau@csse.unimelb.edu.au, kgrieser@csse.unimelb.edu.au, newman@uci.edu, tb@ldwin.net
Abstract
We propose a method for automatically la-
belling topics learned via LDA topic models.
We generate our label candidate set from the
top-ranking topic terms, titles of Wikipedia ar-
ticles containing the top-ranking topic terms,
and sub-phrases extracted from the Wikipedia
article titles. We rank the label candidates us-
ing a combination of association measures and
lexical features, optionally fed into a super-
vised ranking model. Our method is shown to
perform strongly over four independent sets of
topics, significantly better than a benchmark
method.
1 Introduction
Topic modelling is an increasingly popular frame-
work for simultaneously soft-clustering terms and
documents into a fixed number of ?topics?, which
take the form of a multinomial distribution over
terms in the document collection (Blei et al,
2003). It has been demonstrated to be highly ef-
fective in a wide range of tasks, including multi-
document summarisation (Haghighi and Vander-
wende, 2009), word sense discrimination (Brody
and Lapata, 2009), sentiment analysis (Titov and
McDonald, 2008), information retrieval (Wei and
Croft, 2006) and image labelling (Feng and Lapata,
2010).
One standard way of interpreting a topic is to use
the marginal probabilities p(wi|tj) associated with
each term wi in a given topic tj to extract out the 10
terms with highest marginal probability. This results
in term lists such as:1
stock market investor fund trading invest-
ment firm exchange companies share
1Here and throughout the paper, we will represent a topic tj
via its ranking of top-10 topic terms, based on p(wi|tj).
which are clearly associated with the domain of
stock market trading. The aim of this research is to
automatically generate topic labels which explicitly
identify the semantics of the topic, i.e. which take us
from a list of terms requiring interpretation to a sin-
gle label, such as STOCK MARKET TRADING in the
above case.
The approach proposed in this paper is to first
generate a topic label candidate set by: (1) sourc-
ing topic label candidates from Wikipedia by query-
ing with the top-N topic terms; (2) identifying the
top-ranked document titles; and (3) further post-
processing the document titles to extract sub-strings.
We translate each topic label into features extracted
from Wikipedia, lexical association with the topic
terms in Wikipedia documents, and also lexical fea-
tures for the component terms. This is used as the
basis of a support vector regression model, which
ranks each topic label candidate.
Our contributions in this work are: (1) the genera-
tion of a novel evaluation framework and dataset for
topic label evaluation; (2) the proposal of a method
for both generating and scoring topic label candi-
dates; and (3) strong in- and cross-domain results
across four independent document collections and
associated topic models, demonstrating the ability
of our method to automatically label topics with re-
markable success.
2 Related Work
Topics are conventionally interpreted via their top-
N terms, ranked based on the marginal probability
p(wi|tj) in that topic (Blei et al, 2003; Griffiths and
Steyvers, 2004). This entails a significant cognitive
load in interpretation, prone to subjectivity. Topics
are also sometimes presented with manual post-hoc
labelling for ease of interpretation in research pub-
lications (Wang and McCallum, 2006; Mei et al,
1536
2006). This has obvious disadvantages in terms of
subjectivity, and lack of reproducibility/automation.
The closest work to our method is that of Mei et
al. (2007), who proposed various unsupervised ap-
proaches for automatically labelling topics, based
on: (1) generating label candidates by extracting ei-
ther bigrams or noun chunks from the document col-
lection; and (2) ranking the label candidates based
on KL divergence with a given topic. Their proposed
methodology generates a generic list of label can-
didates for all topics using only the document col-
lection. The best method uses bigrams exclusively,
in the form of the top-1000 bigrams based on the
Student?s t-test. We reimplement their method and
present an empirical comparison in Section 5.3.
In other work, Magatti et al (2009) proposed a
method for labelling topics induced by a hierarchi-
cal topic model. Their label candidate set is the
Google Directory (gDir) hierarchy, and label selec-
tion takes the form of ontological alignment with
gDir. The experiments presented in the paper are
highly preliminary, although the results certainly
show promise. However, the method is only applica-
ble to a hierarchical topic model and crucially relies
on a pre-existing ontology and the class labels con-
tained therein.
Pantel and Ravichandran (2004) addressed the
more specific task of labelling a semantic class
by applying Hearst-style lexico-semantic patterns
to each member of that class. When presented
with semantically homogeneous, fine-grained near-
synonym clusters, the method appears to work well.
With topic modelling, however, the top-ranking
topic terms tended to be associated and not lexically
similar to one another. It is thus highly questionable
whether their method could be applied to topic mod-
els, but it would certainly be interesting to investi-
gate whether our model could conversely be applied
to the labelling of sets of near-synonyms.
In recent work, Lau et al (2010) proposed to ap-
proach topic labelling via best term selection, i.e.
selecting one of the top-10 topic terms to label the
overall topic. While it is often possible to label top-
ics with topic terms (as is the case with the stock
market topic above), there are also often cases where
topic terms are not appropriate as labels. We reuse
a selection of the features proposed by Lau et al
(2010), and return to discuss it in detail in Section 3.
While not directly related to topic labelling,
Chang et al (2009) were one of the first to propose
human labelling of topic models, in the form of syn-
thetic intruder word and topic detection tasks. In the
intruder word task, they include a term w with low
marginal probability p(w|t) for topic t into the top-
N topic terms, and evaluate how well both humans
and their model are able to detect the intruder.
The potential applications for automatic labelling
of topics are many and varied. In document col-
lection visualisation, e.g., the topic model can be
used as the basis for generating a two-dimensional
representation of the document collection (Newman
et al, 2010a). Regions where documents have a
high marginal probability p(di|tj) of being associ-
ated with a given topic can be explicitly labelled
with the learned label, rather than just presented
as an unlabelled region, or presented with a dense
?term cloud? from the original topic. In topic model-
based selectional preference learning (Ritter et al,
2010; O` Se?aghdha, 2010), the learned topics can
be translated into semantic class labels (e.g. DAYS
OF THE WEEK), and argument positions for individ-
ual predicates can be annotated with those labels for
greater interpretability/portability. In dynamic topic
models tracking the diachronic evolution of topics
in time-sequenced document collections (Blei and
Lafferty, 2006), labels can greatly enhance the inter-
pretation of what topics are ?trending? at any given
point in time.
3 Methodology
The task of automatic labelling of topics is a natural
progression from the best topic term selection task
of Lau et al (2010). In that work, the authors use
a reranking framework to produce a ranking of the
top-10 topic terms based on how well each term ? in
isolation ? represents a topic. For example, in our
stock market investor fund trading ... topic example,
the term trading could be considered as a more rep-
resentative term of the overall semantics of the topic
than the top-ranked topic term stock.
While the best term could be used as a topic la-
bel, topics are commonly ideas or concepts that are
better expressed with multiword terms (for example
STOCK MARKET TRADING), or terms that might not
be in the top-10 topic terms (for example, COLOURS
1537
would be a good label for a topic of the form red
green blue cyan ...).
In this paper, we propose a novel method for au-
tomatic topic labelling that first generates topic label
candidates using English Wikipedia, and then ranks
the candidates to select the best topic labels.
3.1 Candidate Generation
Given the size and diversity of English Wikipedia,
we posit that the vast majority of (coherent) topics
or concepts are encapsulated in a Wikipedia article.
By making this assumption, the difficult task of gen-
erating potential topic labels is transposed to find-
ing relevant Wikipedia articles, and using the title of
each article as a topic label candidate.
We first use the top-10 topic terms (based on the
marginal probabilities from the original topic model)
to query Wikipedia, using: (a) Wikipedia?s native
search API; and (b) a site-restricted Google search.
The combined set of top-8 article titles returned
from the two search engines for each topic consti-
tutes the initial set of primary candidates.
Next we chunk parse the primary candidates us-
ing the OpenNLP chunker,2 and extract out all noun
chunks. For each noun chunk, we generate all com-
ponent n-grams (including the full chunk), out of
which we remove all n-grams which are not in them-
selves article titles in English Wikipedia. For exam-
ple, if the Wikipedia document title were the single
noun chunk United States Constitution, we would
generate the bigrams United States and States Con-
stitution, and prune the latter; we would also gen-
erate the unigrams United, States and Constitution,
all of which exist as Wikipedia articles and are pre-
served.
In this way, an average of 30?40 secondary labels
are produced for each topic based on noun chunk n-
grams. A good portion of these labels are commonly
stopwords or unigrams that are only marginally re-
lated to the topic (an artifact of the n-gram gener-
ation process). To remove these outlier labels, we
use the RACO lexical association method of Grieser
et al (2011).
RACO (Related Article Conceptual Overlap) uses
Wikipedia?s link structure and category membership
to identify the strength of relationship between arti-
2http://opennlp.sourceforge.net/
cles via their category overlap. The set of categories
related to an article is defined as the union of the cat-
egory membership of all outlinks in that article. The
category overlap of two articles (a and b) is the in-
tersection of the related category sets of each article.
The formal definition of this measure is as follows:
|(?p?O(a)C(p)) ? (?p?O(b)C(p))|
where O(a) is the set of outlinks from article a, and
C(p) is the set of categories of which article p is a
member. This is then normalised using Dice?s co-
efficient to generate a similarity measure. In the in-
stance that a term maps onto multiple Wikipedia ar-
ticles via a disambiguation page, we return the best
RACO score across article pairings for a given term
pair. The final score for each secondary label can-
didate is calculated as the average RACO score with
each of the primary label candidates. All secondary
labels with an average RACO score of 0.1 and above
are added to the label candidate set.
Finally, we add the top-5 topic terms to the set of
candidates, based on the marginals from the origi-
nal topic model. Doing this ensures that there are
always label candidates for all topics (even if the
Wikipedia searches fail), and also allows the pos-
sibility of labeling a topic using its own topic terms,
which was demonstrated by Lau et al (2010) to be a
baseline source of topic label candidates.
3.2 Candidate Ranking
After obtaining the set of topic label candidates, the
next step is to rank the candidates to find the best la-
bel for each topic. We will first describe the features
that we use to represent label candidates.
3.2.1 Features
A good label should be strongly associated with
the topic terms. To learn the association of a label
candidate with the topic terms, we use several lexical
association measures: pointwise mutual information
(PMI), Student?s t-test, Dice?s coefficient, Pearson?s
?2 test, and the log likelihood ratio (Pecina, 2009).
We also include conditional probability and reverse
conditional probability measures, based on the work
of Lau et al (2010). To calculate the association
measures, we parse the full collection of English
Wikipedia articles using a sliding window of width
1538
20, and obtain term frequencies for the label candi-
dates and topic terms. To measure the association
between a label candidate and a list of topic terms,
we average the scores of the top-10 topic terms.
In addition to the association measures, we in-
clude two lexical properties of the candidate: the raw
number of terms, and the relative number of terms in
the label candidate that are top-10 topic terms.
We also include a search engine score for each
label candidate, which we generate by querying a
local copy of English Wikipedia with the top-10
topic terms, using the Zettair search engine (based
on BM25 term similarity).3 For a given label candi-
date, we return the average score for the Wikipedia
article(s) associated with it.
3.2.2 Unsupervised and Supervised Ranking
Each of the proposed features can be used as the
basis for an unsupervised model for label candidate
selection, by ranking the label candidates for a given
topic and selecting the top-N . Alternatively, they
can be combined in a supervised model, by training
over topics where we have gold-standard labelling
of the label candidates. For the supervised method,
we use a support vector regression (SVR) model
(Joachims, 2006) over all of the features.
4 Datasets
We conducted topic labelling experiments using
document collections constructed from four distinct
domains/genres, to test the domain/genre indepen-
dence of our method:
BLOGS : 120,000 blog articles dated from August
to October 2008 from the Spinn3r blog dataset4
BOOKS : 1,000 English language books from the
Internet Archive American Libraries collection
NEWS : 29,000 New York Times news articles
dated from July to September 1999, from the
English Gigaword corpus
PUBMED : 77,000 PubMed biomedical abstracts
published in June 2010
3http://www.seg.rmit.edu.au/zettair/
4http://www.icwsm.org/data/
The BLOGS dataset contains blog posts that cover
a diverse range of subjects, from product reviews
to casual, conversational messages. The BOOKS
topics, coming from public-domain out-of-copyright
books (with publication dates spanning more than
a century), relate to a wide range of topics includ-
ing furniture, home decoration, religion and art,
and have a more historic feel to them. The NEWS
topics reflect the types and range of subjects one
might expect in news articles such as health, finance,
entertainment, and politics. The PUBMED topics
frequently contain domain-specific terms and are
sharply differentiated from the topics for the other
corpora. We are particularly interested in the perfor-
mance of the method over PUBMED, as it is a highly
specialised domain where we may expect lower cov-
erage of appropriate topic labels within Wikipedia.
We took a standard approach to topic modelling
each of the four document collections: we tokenised,
lemmatised and stopped each document,5 and cre-
ated a vocabulary of terms that occurred at least
ten times. From this processed data, we created a
bag-of-words representation of each document, and
learned topic models with T = 100 topics in each
case.
To focus our experiments on topics that were rela-
tively more coherent and interpretable, we first used
the method of Newman et al (2010b) to calculate
the average PMI-score for each topic, and filtered
all topics that had an average PMI-score lower than
0.4. We additionally filtered any topics where less
than 5 of the top-10 topic terms are default nomi-
nal in Wikipedia.6 The filtering criteria resulted in
45 topics for BLOGS, 38 topics for BOOKS, 60 top-
ics for NEWS, and 85 topics for PUBMED. Man-
ual inspection of the discarded topics indicated that
they were predominantly hard-to-label junk topics or
mixed topics, with limited utility for document/term
clustering.
Applying our label candidate generation method-
ology to these 228 topics produced approximately
6000 labels ? an average of 27 labels per topic.
5OpenNLP is used for tokenization, Morpha for lemmatiza-
tion (Minnen et al, 2001).
6As determined by POS tagging English Wikipedia with
OpenNLP, and calculating the coarse-grained POS priors (noun,
verb, etc.) for each term.
1539
Figure 1: A screenshot of the topic label evaluation task on Amazon Mechanical Turk. This screen constitutes a
Human Intelligence Task (HIT); it contains a topic followed by 10 suggested topic labels, which are to be rated. Note
that been would be the stopword label in this example.
4.1 Topic Candidate Labelling
To evaluate our methods and train the supervised
method, we require gold-standard ratings for the la-
bel candidates. To this end, we used Amazon Me-
chanical Turk to collect annotations for our labels.
In our annotation task, each topic was presented
in the form of its top-10 terms, followed by 10 sug-
gested labels for the topic. This constitutes a Human
Intelligence Task (HIT); annotators are paid based
on the number of HITs they have completed. A
screenshot of a HIT seen by annotator is presented
in Figure 1.
In each HIT, annotators were asked to rate the la-
bels based on the following ordinal scale:
3: Very good label; a perfect description of the
topic.
2: Reasonable label, but does not completely cap-
ture the topic.
1: Label is semantically related to the topic, but
would not make a good topic label.
0: Label is completely inappropriate, and unrelated
to the topic.
To filter annotations from workers who did not
perform the task properly or from spammers, we ap-
1540
Domain Topic Terms Label Candidate AverageRating
BLOGS china chinese olympics gold olympic team win beijing medal sport 2008 summer olympics 2.60
BOOKS church arch wall building window gothic nave side vault tower gothic architecture 2.40
NEWS israel peace barak israeli minister palestinian agreement prime leader palestinians israeli-palestinian conflict 2.63
PUBMED cell response immune lymphocyte antigen cytokine t-cell induce receptor immunity immune system 2.36
Table 1: A sample of topics and topic labels, along with the average rating for each label candidate
plied a few heuristics to automatically detect these
workers. Additionally, we inserted a small num-
ber of stopwords as label candidates in each HIT
and recorded workers who gave high ratings to these
stopwords. Annotations from workers who failed to
passed these tests are removed from the final set of
gold ratings.
Each label candidate was rated in this way by at
least 10 annotators, and ratings from annotators who
passed the filter were combined by averaging them.
A sample of topics, label candidates, and the average
rating is presented in Table 1.7
Finally, we train the regression model over all
the described features, using the human rating-based
ranking.
5 Experiments
In this section we present our experimental results
for the topic labelling task, based on both the unsu-
pervised and supervised methods, and the methodol-
ogy of Mei et al (2007), which we denote MSZ for
the remainder of the paper.
5.1 Evaluation
We use two basic measures to evaluate the perfor-
mance of our predictions. Top-1 average rating is
the average annotator rating given to the top-ranked
system label, and has a maximum value of 3 (where
annotators unanimously rated all top-ranked system
labels with a 3). This is intended to give a sense of
the absolute utility of the top-ranked candidates.
The second measure is normalized discounted
cumulative gain (nDCG: Jarvelin and Kekalainen
(2002), Croft et al (2009)), computed for the top-1
(nDCG-1), top-3 (nDCG-3) and top-5 ranked sys-
tem labels (nDCG-5). For a given ordered list of
7The dataset is available for download from
http://www.csse.unimelb.edu.au/research/
lt/resources/acl2011-topic/.
scores, this measure is based on the difference be-
tween the original order, and the order when the list
is sorted by score. That is, if items are ranked op-
timally in descending order of score at position N ,
nDCG-N is equal to 1. nDCG is a normalised score,
and indicates how close the candidate label ranking
is to the optimal ranking within the set of annotated
candidates, noting that an nDCG-N score of 1 tells
us nothing about absolute values of the candidates.
This second evaluation measure is thus intended to
reflect the relative quality of the ranking, and com-
plements the top-1 average rating.
Note that conventional precision- and recall-based
evaluation is not appropriate for our task, as each
label candidate has a real-valued rating.
As a baseline for the task, we use the unsuper-
vised label candidate ranking method based on Pear-
son?s ?2 test, as it was overwhelmingly found to be
the pick of the features for candidate ranking.
5.2 Results for the Supervised Method
For the supervised model, we present both in-
domain results based on 10-fold cross-validation,
and cross-domain results where we learn a model
from the ratings for the topic model from a given
domain, and apply it to a second domain. In each
case, we learn an SVR model over the full set of fea-
tures described in Section 3.2.1. In practical terms,
in-domain results make the unreasonable assump-
tion that we have labelled 90% of labels in order
to be able to label the remaining 10%, and cross-
domain results are thus the more interesting data
point in terms of the expected results when apply-
ing our method to a novel topic model. It is valuable
to compare the two, however, to gauge the relative
impact of domain on the results.
We present the results for the supervised method
in Table 2, including the unsupervised baseline and
an upper bound estimate for comparison purposes.
The upper bound is calculated by ranking the candi-
1541
Test Domain Training Top-1 Average Rating nDCG-1 nDCG-3 nDCG-5All 1? 2? Top5
BLOGS
Baseline (unsupervised) 1.84 1.87 1.75 1.74 0.75 0.77 0.79
In-domain 1.98 1.94 1.95 1.77 0.81 0.82 0.83
Cross-domain: BOOKS 1.88 1.92 1.90 1.77 0.77 0.81 0.83
Cross-domain: NEWS 1.97 1.94 1.92 1.77 0.80 0.83 0.83
Cross-domain: PUBMED 1.95 1.95 1.93 1.82 0.80 0.82 0.83
Upper bound 2.45 2.26 2.29 2.18 1.00 1.00 1.00
BOOKS
Baseline (unsupervised) 1.75 1.76 1.70 1.72 0.77 0.77 0.79
In-domain 1.91 1.90 1.83 1.74 0.84 0.81 0.83
Cross-domain: BLOGS 1.82 1.88 1.79 1.71 0.79 0.81 0.82
Cross-domain: NEWS 1.82 1.87 1.80 1.75 0.79 0.81 0.83
Cross-domain: PUBMED 1.87 1.87 1.80 1.73 0.81 0.82 0.83
Upper bound 2.29 2.17 2.15 2.04 1.00 1.00 1.00
NEWS
Baseline (unsupervised) 1.96 1.76 1.87 1.70 0.80 0.79 0.78
In-domain 2.02 1.92 1.90 1.82 0.82 0.82 0.84
Cross-domain: BLOGS 2.03 1.92 1.89 1.85 0.83 0.82 0.84
Cross-domain: BOOKS 2.01 1.80 1.93 1.73 0.82 0.82 0.83
Cross-domain: PUBMED 2.01 1.93 1.94 1.80 0.82 0.82 0.83
Upper bound 2.45 2.31 2.33 2.12 1.00 1.00 1.00
PUBMED
Baseline (unsupervised) 1.73 1.74 1.68 1.63 0.75 0.77 0.79
In-domain 1.79 1.76 1.74 1.67 0.77 0.82 0.84
Cross-domain: BLOGS 1.80 1.77 1.73 1.69 0.78 0.82 0.84
Cross-domain: BOOKS 1.77 1.70 1.74 1.64 0.77 0.82 0.83
Cross-domain: NEWS 1.79 1.76 1.73 1.65 0.77 0.82 0.84
Upper bound 2.31 2.17 2.22 2.01 1.00 1.00 1.00
Table 2: Supervised results for all domains
dates based on the annotated human ratings. The up-
per bound for top-1 average rating is thus the high-
est average human rating of all label candidates for
a given topic, while the upper bound for the nDCG
measures will always be 1.
In addition to results for the combined candidate
set, we include results for each of the three candi-
date subsets, namely the primary Wikipedia labels
(?1??), the secondary Wikipedia labels (?2??) and
the top-5 topic terms (?Top5?); the nDCG results
are over the full candidate set only, as the numbers
aren?t directly comparable over the different subsets
(due to differences in the number of candidates and
the distribution of ratings).
Comparing the in-domain and cross-domain re-
sults, we observe that they are largely compara-
ble, with the exception of BOOKS, where there is
a noticeable drop in both top-1 average rating and
nDGC-1 when we use cross-domain training. We
see an appreciable drop in scores when we train
BOOKS against BLOGS (or vice versa), which we
analyse as being due to incompatibility in document
content and structure between these two domains.
Overall though, the results are very encouraging,
and point to the plausibility of using labelled topic
models from independent domains to learn the best
topic labels for a new domain.
Returning to the question of the suitability of la-
bel candidates for the highly specialised PUBMED
document collection, we first notice that the up-
per bound top-1 average rating is comparable to
the other domains, indicating that our method has
been able to extract equivalent-quality label can-
didates from Wikipedia. The top-1 average rat-
ings of the supervised method are lower than the
other domains. We hypothesise that the cause of
the drop is that the lexical association measures are
trained over highly diverse Wikipedia data rather
than biomedical-specific data, and predict that the
results would improve if we trained our features over
PubMed.
The results are uniformly better than the unsuper-
vised baselines for all four corpora, although there
is quite a bit of room for improvement relative to the
upper bound. To better gauge the quality of these
results, we carry out a direct comparison of our pro-
posed method with the best-performing method of
MSZ in Section 5.3.
1542
Looking to the top-1 average score results over the
different candidate sets, we observe first that the up-
per bound for the combined candidate set (?All?) is
higher than the scores for the candidate subsets in all
cases, underlining the complementarity of the differ-
ent candidate sets. We also observe that the top-5
topic term candidate set is the lowest performer out
of the three subsets across all four corpora, in terms
of both upper bound and the results for the super-
vised method. This reinforces our comments about
the inferiority of the topic word selection method of
Lau et al (2010) for topic labelling purposes. For
NEWS and PUBMED, there is a noticeable differ-
ence between the results of the supervised method
over the full candidate set and each of the candidate
subsets. In contrast, for BOOKS and BLOGS, the re-
sults for the primary candidate subset are at times
actually higher than those over the full candidate set
in most cases (but not for the upper bound). This is
due to the larger search space in the full candidate
set, and the higher median quality of candidates in
the primary candidate set.
5.3 Comparison with MSZ
The best performing method out of the suite of
approaches proposed by MSZ method exclusively
uses bigrams extracted from the document collec-
tion, ranked based on Student?s t-test. The potential
drawbacks to this approach are: all labels must be
bigrams, there must be explicit token instances of
a given bigram in the document collection for it to
be considered as a label candidate, and furthermore,
there must be enough token instances in the docu-
ment collection for it to have a high t score.
To better understand the performance difference
of our approach to that of MSZ, we perform direct
comparison of our proposed method with the bench-
mark method of MSZ.
5.3.1 Candidate Ranking
First, we compare the candidate ranking method-
ology of our method with that of MSZ, using the
label candidates extracted by the MSZ method.
We first extracted the top-2000 bigrams using the
N -gram Statistics Package (Banerjee and Pedersen,
2003). We then ranked the bigrams for each topic
using the Student?s t-test. We included the top-5 la-
bels generated for each topic by the MSZ method
in our Mechanical Turk annotation task, and use the
annotations to directly compare the two methods.
To measure the performance of candidate rank-
ing between our supervised method and MSZ?s, we
re-rank the top-5 labels extracted by MSZ using
our SVR methodology (in-domain) and compare the
top-1 average rating and nDCG scores. Results are
shown in Table 3. We do not include results for the
BOOKS domain because the text collection is much
larger than the other domains, and the computation
for the MSZ relevance score ranking is intractable
due to the number of n-grams (a significant short-
coming of the method).
Looking at the results for the other domains, it is
clear that our ranking system has the upper hand:
it consistently outperforms MSZ over every evalu-
ation metric.8 Comparing the top-1 average rating
results back to those in Table 2, we observe that
for all three domains, the results for MSZ are be-
low those of the unsupervised baseline, and well be-
low those of our supervised method. The nDCG re-
sults are more competitive, and the nDCG-3 results
are actually higher than our original results in Ta-
ble 2. It is important to bear in mind, however, that
the numbers are in each case relative to a different la-
bel candidate set. Additionally, the results in Table 3
are based on only 5 candidates, with a relatively flat
gold-standard rating distribution, making it easier to
achieve higher nDCG-5 scores.
5.3.2 Candidate Generation
The method of MSZ makes the implicit assump-
tion that good bigram labels are discoverable within
the document collection. In our method, on the other
hand, we (efficiently) access the much larger and
variable n-gram length set of English Wikipedia ar-
ticle titles, in addition to the top-5 topic terms. To
better understand the differences in label candidate
sets, and the relative coverage of the full label can-
didate set in each case, we conducted another survey
where human users were asked to suggest one topic
label for each topic presented.
The survey consisted, once again, of presenting
annotators with a topic, but in this case, we gave
them the open task of proposing the ideal label for
8Based on a single ANOVA, the difference in results is sta-
tistically significant at the 5% level for BLOGS, and 1% for
NEWS and PUBMED.
1543
Test Domain Candidate Ranking Top-1 nDCG-1 nDCG-3 nDCG-5System Avg. Rating
BLOGS
MSZ 1.26 0.65 0.76 0.87
SVR 1.41 0.75 0.85 0.92
Upper bound 1.87 1.00 1.00 1.00
NEWS
MSZ 1.37 0.73 0.81 0.90
SVR 1.66 0.88 0.90 0.95
Upper bound 1.86 1.00 1.00 1.00
PUBMED
MSZ 1.53 0.77 0.85 0.93
SVR 1.73 0.87 0.91 0.96
Upper bound 1.98 1.00 1.00 1.00
Table 3: Comparison of results for our proposed supervised ranking method (SVR) and that of MSZ
the topic. In this, we did not enforce any restrictions
on the type or size of label (e.g. the number of terms
in the label).
Of the manually-generated gold-standard labels,
approximately 36% were contained in the original
document collection, but 60% were Wikipedia arti-
cle titles. This indicates that our method has greater
potential to generate a label of the quality of the ideal
proposed by a human in a completely open-ended
task.
6 Discussion
On the subject of suitability of using Amazon Me-
chanical Turk for natural language tasks, Snow et al
(2008) demonstrated that the quality of annotation
is comparable to that of expert annotators. With that
said, the PUBMED topics are still a subject of inter-
est, as these topics often contain biomedical terms
which could be difficult for the general populace to
annotate.
As the number of annotators per topic and the
number of annotations per annotator vary, there is
no immediate way to calculate the inter-annotator
agreement. Instead, we calculated the MAE score
for each candidate, which is an average of the ab-
solute difference between an annotator?s rating and
the average rating of a candidate, summed across all
candidates to get the MAE score for a given corpus.
The MAE scores for each corpus are shown in Ta-
ble 4, noting that a smaller value indicates higher
agreement.
As the table shows, the agreement for the
PUBMED domain is comparable with the other
datasets. BLOGS and NEWS have marginally better
Corpus MAE
BLOGS 0.50
BOOKS 0.56
NEWS 0.52
PUBMED 0.56
Table 4: Average MAE score for label candidate rating
over each corpus
agreement, almost certainly because of the greater
immediacy of the topics, covering everyday areas
such as lifestyle and politics. BOOKS topics are oc-
casionally difficult to label due to the breadth of the
domain; e.g. consider a topic containing terms ex-
tracted from Shakespeare sonnets.
7 Conclusion
This paper has presented the task of topic labelling,
that is the generation and scoring of labels for a
given topic. We generate a set of label candidates
from the top-ranking topic terms, titles of Wikipedia
articles containing the top-ranking topic terms, and
also a filtered set of sub-phrases extracted from the
Wikipedia article titles. We rank the label candidates
using a combination of association measures, lexical
features and an Information Retrieval feature. Our
method is shown to perform strongly over four inde-
pendent sets of topics, and also significantly better
than a competitor system.
Acknowledgements
NICTA is funded by the Australian government as rep-
resented by Department of Broadband, Communication
and Digital Economy, and the Australian Research Coun-
cil through the ICT centre of Excellence programme. DN
has also been supported by a grant from the Institute of
Museum and Library Services, and a Google Research
Award.
1544
References
S. Banerjee and T. Pedersen. 2003. The design, im-
plementation, and use of the Ngram Statistic Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381, Mexico City, February.
D.M. Blei and J.D. Lafferty. 2006. Dynamic topic mod-
els. In ICML 2006.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. JMLR, 3:993?1022.
S. Brody and M. Lapata. 2009. Bayesian word sense
induction. In EACL 2009, pages 103?111.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans in-
terpret topic models. In NIPS, pages 288?296.
B. Croft, D. Metzler, and T. Strohman. 2009. Search
Engines: Information Retrieval in Practice. Addison
Wesley.
Y. Feng and M. Lapata. 2010. Topic models for im-
age annotation and text illustration. In Proceedings
of Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL HLT
2010), pages 831?839, Los Angeles, USA, June.
K. Grieser, T. Baldwin, F. Bohnert, and L. Sonenberg.
2011. Using ontological and document similarity to
estimate museum exhibit relatedness. ACM Journal
on Computing and Cultural Heritage, 3(3):1?20.
T. Griffiths and M. Steyvers. 2004. Finding scientific
topics. In PNAS, volume 101, pages 5228?5235.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
HLT: NAACL 2009, pages 362?370.
K. Jarvelin and J. Kekalainen. 2002. Cumulated gain-
based evaluation of IR techniques. ACM Transactions
on Information Systems, 20(4).
T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of the ACM Conference on Knowledge
Discovery and Data Mining (KDD), pages 217?226,
New York, NY, USA. ACM.
J.H. Lau, D. Newman, S. Karimi, and T. Baldwin. 2010.
Best topic word selection for topic labelling. In Coling
2010: Posters, pages 605?613, Beijing, China.
D. Magatti, S. Calegari, D. Ciucci, and F. Stella. 2009.
Automatic labeling of topics. In ISDA 2009, pages
1227?1232, Pisa, Italy.
Q. Mei, C. Liu, H. Su, and C. Zhai. 2006. A probabilistic
approach to spatiotemporal theme pattern mining on
weblogs. In WWW 2006, pages 533?542.
Q. Mei, X. Shen, and C. Zhai. 2007. Automatic labeling
of multinomial topic models. In SIGKDD, pages 490?
499.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Journal of Nat-
ural Language Processing, 7(3):207?223.
D. Newman, T. Baldwin, L. Cavedon, S. Karimi, D. Mar-
tinez, and J. Zobel. 2010a. Visualizing document col-
lections and search results using topic mapping. Jour-
nal of Web Semantics, 8(2-3):169?175.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin.
2010b. Automatic evaluation of topic coherence. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2010), pages 100?108, Los Angeles,
USA, June. Association for Computational Linguis-
tics.
D. O` Se?aghdha. 2010. Latent variable models of selec-
tional preference. In ACL 2010.
P. Pantel and D. Ravichandran. 2004. Automatically
labeling semantic classes. In HLT/NAACL-04, pages
321?328.
P. Pecina. 2009. Lexical Association Measures: Collo-
cation Extraction. Ph.D. thesis, Charles University.
A. Ritter, Mausam, and O. Etzioni. 2010. A la-
tent Dirichlet alocation method for selectional pref-
erences. In ACL 2010.
R. Snow, B. O?Connor, D. Jurafsky, and A. Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In EMNLP
?08: Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 254?
263, Morristown, NJ, USA.
I. Titov and R. McDonald. 2008. Modeling online re-
views with multi-grain topic models. In WWW ?08,
pages 111?120.
X. Wang and A. McCallum. 2006. Topics over time: A
non-Markov continuous-time model of topical trends.
In KDD, pages 424?433.
S. Wei and W.B. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In SIGIR ?06, pages 178?
185.
1545
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 207?215, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UniMelb NLP-CORE: Integrating predictions from multiple domains and
feature sets for estimating semantic textual similarity
Spandana Gella,? Bahar Salehi,?? Marco Lui,??
Karl Grieser,? Paul Cook,? and Timothy Baldwin,??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
sgella@student.unimelb.edu.au, bsalehi@student.unimelb.edu.au
mhlui@unimelb.edu.au, kgrieser@student.unimelb.edu.au
paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
In this paper we present our systems for cal-
culating the degree of semantic similarity be-
tween two texts that we submitted to the Se-
mantic Textual Similarity task at SemEval-
2013. Our systems predict similarity using
a regression over features based on the fol-
lowing sources of information: string similar-
ity, topic distributions of the texts based on
latent Dirichlet alocation, and similarity be-
tween the documents returned by an informa-
tion retrieval engine when the target texts are
used as queries. We also explore methods for
integrating predictions using different training
datasets and feature sets. Our best system was
ranked 17th out of 89 participating systems.
In our post-task analysis, we identify simple
changes to our system that further improve our
results.
1 Introduction
Semantic Textual Similarity (STS) measures the de-
gree of semantic similarity or equivalence between
a pair of short texts. STS is related to many natural
language processing applications such as text sum-
marisation (Aliguliyev, 2009), machine translation,
word sense disambiguation, and question answering
(De Boni and Manandhar, 2003; Jeon et al, 2005).
Two short texts are considered similar if they both
convey similar messages. Often it is the case that
similar texts will have a high degree of lexical over-
lap, although this isn?t always so. For example,
SC dismissed government?s review plea in Vodafone
tax case and SC dismisses govt?s review petition on
Vodafone tax verdict are semantically similar. These
texts have matches in terms of exact words (SC,
Vodafone, tax), morphologically-related words (dis-
missed and dismisses), and abbreviations (govern-
ment?s and govt?s). However, the usages (senses) of
plea and petition, and case and verdict are also sim-
ilar.
One straightforward way of estimating semantic
similarity of two texts is by using approaches based
on the similarity of the surface forms of the words
they contain. However, such methods are not capa-
ble of capturing similarity or relatedness at the lexi-
cal level, and moreover, they do not exploit the con-
text in which individual words are used in a target
text. Nevertheless, a variety of knowledge sources
? including part-of-speech, collocations, syntax,
and domain ? can be used to identify the usage or
sense of words in context (McRoy, 1992; Agirre and
Martinez, 2001; Agirre and Stevenson, 2006) to ad-
dress these issues.
Despite their limitations, string similarity mea-
sures have been widely used in previous seman-
tic similarity tasks (Agirre et al, 2012; Islam and
Inkpen, 2008). Latent variable models have also
been used to estimate the semantic similarity be-
tween words, word usages, and texts (Steyvers and
Griffiths, 2007; Lui et al, 2012; Guo and Diab,
2012; Dinu and Lapata, 2010).
In this paper, we consider three different ways of
measuring semantic similarity based on word and
word usage similarity:
1. String-based similarity to measure surface-
level lexical similarity, taking into account
morphology and abbreviations (e.g., dismisses
and dismissed, and government?s and govt?s);
207
2. Latent variable models of similarity to cap-
ture words that have different surface forms,
but that have similar meanings or that can be
used in similar contexts (e.g., petition and plea,
verdict and case); and
3. Topical/domain similarity of the texts with re-
spect to the similarity of documents in an ex-
ternal corpus (based on information-retrieval
methods) that are relevant to the target texts.
We develop features based on all three of these
knowledge sources to capture semantic similarity
from a variety of perspectives. We build a regres-
sion model, trained on STS training data which has
semantic similarity scores for pairs of texts, to learn
weights for the features and rate the similarity of test
instances. Our approach to the task is to explore the
utility of novel features or features that have not per-
formed well in previous research, rather than com-
bine these features with the myriad of features that
have been proposed by others for the task.
2 Text Similarity Measures
In this section we describe the various features used
in our system.
2.1 String Similarity Measures (SS)
Our first set of features contains various string simi-
larity measures (SS), which compare the target texts
in terms of the words they contain and the order
of the words (Islam and Inkpen, 2008). In the Se-
mEval 2012 STS task (Agirre et al, 2012) such
features were used by several participants (Biggins
et al, 2012; Ba?r et al, 2012; Heilman and Mad-
nani, 2012), including the first-ranked team (Ba?r et
al., 2012) who considered string similarity measures
alongside a wide range of other features.
For our string similarity features, the texts were
lemmatized using the implementation of Lancaster
Stemming in NLTK 2.0 (Bird, 2006), and all punc-
tuation was removed. Limited stopword removal
was carried out by eliminating the words a, and, and
the. The output of each string similarity measure
is normalized to the range of [0, 1], where 0 indi-
cates that the texts are completely different, while 1
means they are identical. The normalization method
for each feature is described in Salehi and Cook (to
appear), wherein the authors applied string similar-
ity measures successfully to the task of predicting
the compositionality of multiword expressions.
Identical Unigrams (IU): This feature measures
the number of words shared between the two texts,
irrespective of word order.
Longest Common Substring (LCS): This mea-
sures the longest sequence of words shared between
the two texts. For example, the longest common
substring between the following sentences is bolded:
A woman and man are dancing in the
rain.
A couple are dancing in the street.
Levenshtein (LEV1): Levenshtein distance (also
known as edit distance) calculates the number of
basic word-level edit operations (insertion, deletion
and substitution) to transform one text into the other:
Levenshtein with substitution penalty (LEV2):
This feature is a variant of LEV1 in which substi-
tution is considered as two edit operations: an inser-
tion and a deletion (Baldwin, 2009).
Smith Waterman (SW): This method is designed
to locally align two sequences of amino acids (Smith
and Waterman, 1981). The algorithm looks for
the longest similar regions by maximizing the num-
ber of matches and minimizing the number of in-
sertion/deletion/substitution operations necessary to
align the two sequences. In other words, it finds the
longest common sequence while tolerating a small
number of differences. We call this sequence, the
?aligned sequence?. It has length equal to or greater
than the longest common sequence.
Not Aligned Words (NAW): As mentioned
above, SW looks for similar regions in the given
texts. Our last string similarity feature shows the
number of identical words not aligned by the SW al-
gorithm. We used this feature to examine how simi-
lar the unaligned words are.
These six features (IU, LCS, LEV1, LEV2, SW,
and NAW) form our string similarity (SS) features.
LEV2, SW, and NAW have not been previously con-
sidered for STS.
208
2.2 Topic Modelling Similarity Measures (TM)
The topic modelling features (TM) are based on La-
tent Dirichlet Allocation (LDA), a generative prob-
abilistic model in which each document is mod-
eled as a distribution over a finite set of topics, and
each topic is represented as a distribution over words
(Blei et al, 2003). We build a topic model on a back-
ground corpus, and then for each target text we cre-
ate a topic vector based on the topic allocations of
its content words, based on the method developed
by Lui et al (2012) for predicting word usage simi-
larity.
The choice of the number of topics, T , can
have a big impact on the performance of this
method. Choosing a small T might give overly-
broad topics, while a large T might lead to un-
interpretable topics (Steyvers and Griffiths, 2007).
Moreover smaller numbers of topics have been
shown to perform poorly on both sentence simi-
larity (Guo and Diab, 2012) and word usage sim-
ilarity tasks (Lui et al, 2012). We therefore build
topic models for 33 values of T in the range
2, 3, 5, 8, 10, 50, 80, 100, 150, 200, ...1350.
The background corpus used for generating the
topic models is similar to the COL-WTMF sys-
tem (Guo and Diab, 2012) from the STS-2012 task,
which outperformed LDA. In particular, we use
sense definitions from WordNet, Wiktionary and all
sentences from the Brown corpus. Similarity be-
tween two texts is measured on the basis of the simi-
larity between their topic distributions. We consider
three vector-based similarity measures here: Cosine
similarity, Jensen-Shannon divergence and KL di-
vergence. Thus for each target text pair we extract
99 features corresponding to the 3 similarity mea-
sures for each of the 33 T settings. These features
are used as the TM feature set in the systems de-
scribed below.
2.3 IR Similarity Measures (IR)
The information retrieval?based features (IR) were
based on a dump of English Wikipedia from Novem-
ber 2009. The entire dump was stripped of markup
and tokenised using the OpenNLP tokeniser. The
tokenised documents were then parsed into TREC
format, with each article forming an individual doc-
ument. These documents were indexed using the
Indri IR engine1 with stopword removal. Each
of the two target texts was issued as a full text
query (without any phrases) to Indri, and the first
1000 documents for each text were returned, based
on Okapi term weighting (Robertson and Walker,
1994). These resultant document lists were then
converted into features using a number of set- and
rank-based measures: Dice?s coefficient, Jaccard in-
dex, average overlap, and rank-biased overlap (the
latter two are described in Webber et al (2010)).
The first two are based on simple set overlap and
ignore the ranks; average overlap takes into account
the rank, but equally weights high- and low-ranking
documents; and rank-biased overlap weights higher-
ranked items higher.
In addition to comparisons of the document rank-
ings for a given target text pair, we also consid-
ered a method that compared the top-ranking doc-
uments themselves. To compare two texts, we ob-
tain the top-100 documents using each text as a
query as above. We then calculate the similarity be-
tween these two sets of resultant documents using
the ?2-based corpus similarity measure of Kilgarriff
(2001). In this method the ?2 statistic is calculated
for the 500 most frequent words in the union of the
two sets of documents (corpora), and is interpreted
as the similarity between the sets of documents.
These 5 IR features (4 rank-based, and 1
document-based) are novel in the context of STS,
and are used in the compound systems described be-
low.
3 Compound systems
3.1 Ridge regression
Each of our features represents a (potentially noisy)
measurement of the semantic textual similarity be-
tween two texts. However, the scale of our fea-
tures varies, e.g., [0, 1] for the string similarity fea-
tures vs. unbounded for KL divergence (one of the
topic modelling features). To learn the mapping be-
tween these features and the graded [0, 5] scale of
the shared task, we made use of a statistical tech-
nique known as ridge regression, as implemented in
scikit-learn.2 Ridge regression is a form of
linear regression where the loss function is the ordi-
1http://www.lemurproject.org/indri/
2http://scikit-learn.org
209
nary least squares, but with an additional L2 regular-
ization term. In our empirical evaluation, we found
that ridge regression outperformed linear regression
on our feature set. For brevity, we only present re-
sults from ridge regression.
3.2 Domain Adaptation
Domain adaptation (Daume? and Marcu, 2006) is the
general term applied to techniques for using labelled
data from a related distribution to label data from a
target distribution. For the 2013 Shared Task, no
training data was provided for the target datasets,
making domain adaptation an important considera-
tion. In this work, we assume that each dataset rep-
resents a different domain, and on this basis develop
approaches that are sensitive to inter-domain differ-
ences.
We tested two simple approaches to including do-
main information in our trained model. The first ap-
proach, which we will refer to as flagging, simply in-
volves appending a boolean vector to each training
instance to indicate which training dataset it came
from. The vector has length D, equal to the number
of training datasets (3 for this task, because we train
on the STS 2012 training data). All the values of the
vector are 0, except for a single 1 according to the
dataset that the training instance is drawn from. For
test data, the entire vector consists of 0s.
The second approach we considered is based on
metalearning, and we will refer to it as domain
stacking. In domain stacking, we train a regressor
for each domain (the level 0 regressors (Wolpert,
1992)). Each of these regressors is then applied
to a test instance to produce a predicted value (the
level 0 prediction). These predictions are then com-
bined using a second regressor (the level 1 regres-
sor), to produce a final prediction for each instance
(the level 1 prediction). This approach is closely
related to feature stacking (Lui, 2012) and stacked
generalization (Wolpert, 1992). A general princi-
ple of metalearning is to combine multiple weaker
(?less accurate?) predictors ? termed level 0 pre-
dictors ? to produce a stronger (?more accurate?)
predictor ? the level 1 predictor. In stacked gener-
alization, the level 0 predictors are different learning
algorithms. In feature stacking, they are the same
algorithm trained on different subsets of features, in
this work corresponding to different methods for es-
timating STS (Section 2). In domain stacking, the
level 0 predictions are obtained from subsets of the
training data, where each subset corresponds to all
the instances from a single dataset (e.g. MSRpar or
SMTeuroparl). In terms of subsampling the training
data, this technique is related to bagging (Breiman,
1996). However, rather than generating new train-
ing sets by uniform sampling across the whole pool
of training data, we treat each domain in the train-
ing dataset as a unique sample. Finally, we also ex-
periment with feature-domain stacking, in which the
level 0 predictions are obtained from the cross prod-
uct of subsets of the training data (as per domain
stacking) and subsets of the feature set (as per fea-
ture stacking). We report results for all 3 variants in
Section 5.
This framework of feature-domain stacking can
be applied with any regression or classification al-
gorithm (indeed, the level 0 and level 1 predictors
could be trained using different algorithms). In this
work, all our regressors are trained using ridge re-
gression (Section 3.1).
4 Submitted Runs
In this section we describe the three official runs we
submitted to the shared task.
4.1 Run1 ? Bahar
For this run we used just the SS feature set, aug-
mented with flagging for domain adaptation. Ridge
regression was used to train a regressor across the
three training datasets (MSRvid, MSRpar, SMTeu-
roparl). Each instance was then labelled using the
output of the regressor, and the output range was lin-
early re-scaled to [0, 5] as it occasionally produced
values outside of this range. Although this approach
approximates STS using only lexical textual similar-
ity, it was our best-performing system on the training
data (Table 1). Furthermore the SS features are ap-
pealing because of their simplicity and because they
do not make use of any external resources.
4.2 Run2 ? Concat
In this run, we concatenated the feature vectors
from all three of our feature sets (SS, TM and
IR), and again trained a regressor on the union of
the MSRvid, MSRpar and SMTeuroparl training
datasets. As in Run1, the output of the regression
210
FSet FL FS DS MSRpar MSRvid SMTeuroparl Ave
SS 0.522 0.537 0.526 0.528
(*) SS X 0.552 0.533 0.562 0.549
TM 0.270 0.479 0.425 0.391
TM X 0.250 0.580 0.427 0.419
IR 0.264 0.759 0.407 0.477
IR X 0.291 0.754 0.400 0.482
(+) ALL 0.401 0.543 0.513 0.485
ALL X 0.377 0.595 0.516 0.496
ALL X 0.385 0.587 0.520 0.497
ALL X 0.452 0.637 0.472 0.521
ALL X X 0.429 0.619 0.526 0.524
ALL X X 0.429 0.627 0.526 0.527
(?) ALL X X X 0.441 0.645 0.527 0.538
Table 1: Pearson?s ? for each feature set (FSet),
as well as combinations of feature sets and adap-
tation strategies, on each training dataset, and the
micro-average over all training datasets. (*), (+),
and (?) denote Run1, Run2, and Run3, respectively,
our submissions to the shared task; FL=Flagging,
FS=Feature stacking, DS=Domain stacking.
was also linearly re-scaled to the [0, 5] range. Un-
like the previous run, the flagging approach to do-
main adaptation was not used. This approach re-
flects a simple application of machine learning to in-
tegrating data from multiple feature sets and training
datasets, and provides a useful point of comparison
against more sophisticated approaches (i.e., Run3).
4.3 Run3 ? Stacking
In this run, we focused on an alternative method
to integrating information from multiple feature sets
and training datasets, namely feature-domain stack-
ing, as discussed in Section 3.2. In this approach, we
train nine regressors using ridge regression on each
combination of the three training datasets and three
feature sets. Thus, the level 1 representation for each
instance is a vector of nine predictions. For the train-
ing data, when computing the level 1 features for the
same training dataset from which a given instance is
drawn, 10-fold cross-validation is used. Ridge re-
gression is again used to combine the level 1 repre-
sentations and produce the final prediction for each
instance. In addition to this, we also simultaneously
apply the flagging approach to domain adaptation.
This approach incorporates all of our domain adap-
tation efforts, and in initial experiments on the train-
ing data (Table 1) it was our second-best system.
FSet FL FS DS OnWN FNWN Headlines SMT Ave
SS 0.340 0.366 0.688 0.325 0.453
(*) SS X 0.349 0.381 0.711 0.350 0.473
TM 0.648 0.358 0.516 0.209 0.433
TM X 0.701 0.368 0.614 0.287 0.506
IR 0.561 -0.006 0.610 0.228 0.419
IR X 0.596 0.002 0.621 0.256 0.441
(+) ALL 0.679 0.337 0.709 0.323 0.542
ALL X 0.704 0.365 0.718 0.344 0.560
ALL X 0.673 0.298 0.714 0.324 0.539
ALL X 0.618 0.264 0.717 0.357 0.534
ALL X X 0.658 0.309 0.721 0.330 0.540
ALL X X 0.557 0.142 0.694 0.280 0.475
(?) ALL X X X 0.614 0.186 0.706 0314 0.509
Table 2: Pearson?s ? for each feature set (FSet),
as well as combinations of feature sets and adap-
tation strategies, on each test dataset, and the
micro-average over all test datasets. (*), (+), and
(?) denote Run1, Run2, and Run3, respectively,
our submissions to the shared task; FL=Flagging,
FS=Feature stacking, DS=Domain stacking.
5 Results
For the STS 2013 task, the organisers advised par-
ticipants to make use of the STS 2012 data; we took
this to mean only the training data. In our post-task
analysis, we realised that the entire 2012 dataset, in-
cluding the testing data, could be used. All our of-
ficial runs were trained only on the training data for
the 2012 task (made up of MSRpar, MSRvid and
SMTeuroparl). We first discuss preliminary find-
ings training and testing on the (STS 2012) training
data, and then present results for the (2013) test data.
Post-submission, we re-trained our systems includ-
ing the 2012 test data.
5.1 Experiments on Training Data
We evaluated our models based on a leave-one-out
cross-validation across the 3 training datasets. Thus,
for each of the training datasets, we trained a sep-
arate model using features from the other two. We
considered approaches based on each individual fea-
ture set, with and without flagging. We further con-
sidered combinations of feature sets using feature
concatenation, as well as feature and domain stack-
ing, again with and without flagging.3 Results are
3We did not consider domain stacking with flagging.
211
FSet FL FS DS OnWN (?) FNWN (?) Headlines (?) SMT (?) Ave (?)
SS 0.3566 (+.0157) 0.3741 (+.0071) 0.6994 (+.0111) 0.3386 (+.0131) 0.4663 (+.0133)
(*) SS X 0.3532 (+.0042) 0.3809 (?.0004) 0.7122 (+.0003) 0.3417 (?.0090) 0.4714 (?.0016)
TM 0.6748 (+.0265) 0.3939 (+.0349) 0.5930 (+.0770) 0.2563 (+.0472) 0.4844 (+.0514)
TM X 0.6269 (?.0743) 0.3519 (?.0162) 0.5999 (?.0142) 0.2653 (?.0223) 0.4743 (?.0317)
IR 0.6632 (+.1015) 0.1026 (+.1093) 0.6383 (?.0281) 0.2987 (+.0701) 0.4863 (+.0673)
IR X 0.6720 (+.0755) 0.0861 (+.0841) 0.6316 (+.0097) 0.2811 (+.0244) 0.4790 (+.0680)
(+) ALL 0.6976 (+.0006) 0.4350 (+.0976) 0.7071 (?.0014) 0.3329 (+.0099) 0.5571 (+.0151)
ALL X 0.6667 (?.0373) 0.4138 (+.0490) 0.7210 (+.0029) 0.3335 (?.0105) 0.5524 (?.0076)
ALL X 0.6889 (+.0149) 0.4620 (+.1636) 0.7309 (+.0167) 0.3538 (+.0295) 0.5721 (+.0331)
ALL X 0.6765 (?.0185) 0.4675 (+.1578) 0.7337 (+.0126) 0.3552 (+.0252) 0.5709 (+.0369)
ALL X X 0.6369 (+.0208) 0.3615 (+.0970) 0.7233 (+.0060) 0.3736 (+.0157) 0.5554 (+.0154)
ALL X X 0.6736 (+.1165) 0.4250 (+.2821) 0.7237 (+0.0297) 0.3404 (+0.0603) 0.5583(+.0833)
(?) ALL X X X 0.6772 (+.0632) 0.3992 (+.2127) 0.7315 (+.0251) 0.3300 (+0.0186) 0.5572 (+.0482)
Table 3: Pearson?s ? for each feature set (FSet), as well as combinations of feature sets and adaptation
strategies, on each test dataset, and the micro-average over all test datasets, using features from all 2012
data (test + train). (*), (+), and (?) denote Run1, Run2, and Run3, respectively, our submissions to the
shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ? denotes the difference in system
performance after adding the additional training data.
reported in Table 1.
The best results on the training data were achieved
using only our SS feature set with flagging (Run1),
with an average Pearson?s ? of 0.549. This fea-
ture set alo gave the best performance on MSR-
par and SMTeuroparl, although the IR feature set
was substantially better on MSRvid. On the training
datasets, our approaches that combine feature sets
did not give an improvement over the best individ-
ual feature set on any dataset, or overall.
5.2 Test Set Results
STS 2013 included four different test sets. Table 2
presents the Pearson?s ? for the same methods as
Section 5.1 ? including our submitted runs ? on
the test data. Run1 drops in performance on the test
set as compared to the training set, where the other
two runs are more consistent, suggesting that lexi-
cal similarity does not generalise well cross-domain.
Table 4 shows that all of our systems performed
above the baseline on each dataset, except Run3 on
FNWN. Table 4 also shows that Run2 consistently
performed well on all the datasets when compared
to the median of all the systems submitted to the task
(Agirre et al, to appear).
Run2, which was based on the concatenation of
all the feature sets, performed well compared to the
stacking-based approaches on the test set, whereas
the stacking approaches all outperformed Run2 on
the training datasets. This is likely due to the
SS features being more effective for STS predic-
tion in the training datasets as compared to the test
datasets. Based on the training datasets, the stack-
ing approaches placed greater weight on the pre-
dictions from the SS feature set. This hypothe-
sis is supported by the result on Headlines, where
the SS feature set does relatively well, and thus the
stacking approaches tend to outperform the simple
concatenation-based method. Finally, an extension
of Run2 with flagging (not submitted to the shared
task) was the best of our methods on the test data.
5.3 Error Analysis
To better understand the behaviour of our systems,
we examined test instances and made the following
observations. Systems based entirely on the TM fea-
tures and domain adaptation consistently performed
well on sentence pairs for which all of our other sys-
tems performed poorly. One example is the follow-
ing OnWN pair, which corresponds to definitions of
newspaper: an enterprise or company that publishes
newsprint and a business firm that publishes news-
papers. Because these texts do not share many com-
mon words, the SS features cannot capture their se-
mantic similarity.
Stacking based approaches performed well on text
pairs which are complex to comprehend, e.g., Two
German tourists, two pilots killed in Kenya air crash
and Senator Reid involved in Las Vegas car crash,
where the individual methods tend to score lower
212
System Headlines OnWN FNWN SMT Ave
(+) Run1 .711 (15) .349 (71) .381 (23) .351 (18) .473 (49)
(+) Run2 .709 (17) .679 (18) .337 (33) .323 (43) .542 (17)
(+) Run3 .706 (18) .614 (28) .187 (71) .314 (47) .509 (29)
Best .718 (14) .704 (15) .365 (28) .344 (24) .560 (7)
(?) Run1 .712 (14) .353 (70) .381 (23) .341 (25) .471 (54)
(?) Run2 .707 (18) .697 (14) .435 (9) .332 (35) .557 (9)
(?) Run3 .731 (11) .677 (19) .399 (17) .330 (38) .557 (8)
(?) Best .730 (11) .688 (17) .462 (7) .353 (18) .572 (4)
Baseline .540 (67) .283 (81) .215 (67) .286 (65) .364 (73)
Median .640 (45) .528 (45) .327 (45) .318 (45) .480 (45)
Best-Score .783 (1) .843 (1) .581 (1) .403 (1) .618 (1)
Table 4: Pearson?s ? (and projected ranking) of runs.
The upper 4 runs are trained only on STS 2012 train-
ing data. (+) denotes runs that were submitted for
evaluation. (?) denotes systems trained on STS 2012
training and test data. For comparison, we include
?Best?, the highest-scoring parametrization of our
system from our post-task analysis (Table 3). We
also include the organiser?s baseline, as well as the
median and best systems for each dataset across all
competitors.
than the human rating, but stacking was able to pre-
dict a higher score (presumably based on the fact
that no method predicted the text pair to be strongly
dissimilar; rather, all methods predicted there to be
somewhat low similarity).
In some cases, the texts are on a similar topic,
but semantically different, e.g., Nigeria mourns over
193 people killed in plane crash and Nigeria opens
probe into deadly air crash. In such cases, systems
based on SS features and stacking perform well.
Systems based on TM and IR features, on the other
hand, tend to predict overly-high scores because the
texts relate to similar topics and tend to have similar
relevant documents in an external corpus.
5.4 Results with the Full Training dataset
We re-trained all the above systems by extending the
training data to include the 2012 test data. Scores on
the 2013 test datasets and the change in Pearson?s ?
after adding the extra training data (denoted ?) are
presented in Table 3.
In general, the addition of the 2012 test data to
the training dataset improves the performance of the
system, though this is often not the case for the flag-
ging approach to domain adaptation, which in some
instances drops in performance after adding the ad-
ditional training data. The biggest improvements
were seen for feature-domain stacking, particularly
on FNWN. This suggests that feature-domain stack-
ing is more sensitive to the similarity between train-
ing data and test data than flagging, but also that it
is better able to cope with variety in training do-
mains than flagging. Given that the pool of anno-
tated data for the STS task continues to increase,
feature-domain stacking is a promising approach to
exploiting the differences between domains to im-
prove overall STS performance.
To facilitate comparison with the published re-
sults for the 2013 STS task, we present a condensed
summary of our results in Table 4, which shows the
absolute score as well as the projected ranking of
each of our systems. It also includes the median and
baseline results for comparison.
6 Conclusions and Future Work
In this paper we described our approach to the
STS SemEval-2013 shared task. While we did not
achieve high scores relative to the other submit-
ted systems on any of the datasets or overall, we
have identified some novel feature sets which we
show to have utility for the STS task. We have
also compared our proposed method?s performance
with a larger training dataset. In future work, we
intend to consider alternative ways for combining
features learned from different domains and training
datasets. Given the strong performance of our string
similarity features on particular datasets, we also in-
tend to consider combining string and distributional
similarity to capture elements of the texts that are not
currently captured by our string similarity features.
Acknowledgments
This work was supported by the European Erasmus
Mundus Masters Program in Language and Commu-
nication Technologies from the European Commis-
sion.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT Centre of Excel-
lence program.
213
References
Eneko Agirre and David Martinez. 2001. Knowl-
edge sources for word sense disambiguation. In Text,
Speech and Dialogue, pages 1?10. Springer.
Eneko Agirre and Mark Stevenson. 2006. Knowledge
sources for wsd. In Eneko Agirre and Philip Edmonds,
editors, Word Sense Disambiguation, volume 33 of
Text, Speech and Language Technology, pages 217?
251. Springer Netherlands.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. to appear. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics,
Atlana, USA. Association for Computational Linguis-
tics.
Ramiz M Aliguliyev. 2009. A new sentence similarity
measure and sentence based extractive technique for
automatic text summarization. Expert Systems with
Applications, 36(4):7764?7772.
Timothy Baldwin. 2009. The hare and the tortoise:
Speed and reliability in translation retrieval. Machine
Translation, 23(4):195?240.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435?440, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Sam Biggins, Shaabi Mohammed, Sam Oakley, Luke
Stringer, Mark Stevenson, and Judita Preiss. 2012.
University of sheffield: Two approaches to semantic
text similarity. In *SEM 2012: The First Joint Confer-
ence on Lexical and Computational Semantics ? Vol-
ume 1: Proceedings of the main conference and the
shared task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation (Se-
mEval 2012), pages 655?661, Montre?al, Canada, 7-8
June. Association for Computational Linguistics.
Steven Bird. 2006. NLTK: The Natural Language
Toolkit. In Proceedings of the COLING/ACL 2006 In-
teractive Presentation Sessions, pages 69?72, Sydney,
Australia, July. Association for Computational Lin-
guistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Leo Breiman. 1996. Bagging predictors. Machine learn-
ing, 24(2):123?140.
Hal Daume?, III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126, May.
Marco De Boni and Suresh Manandhar. 2003. The use
of sentence similarity as a semantic relevance metric
for question answering. In Proceedings of the AAAI
Symposium on New Directions in Question Answering,
Stanford, USA.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Weiwei Guo and Mona Diab. 2012. Weiwei: A sim-
ple unsupervised latent semantics based approach for
sentence similarity. In *SEM 2012: The First Joint
Conference on Lexical and Computational Semantics
? Volume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 586?590, Montre?al, Canada,
7-8 June. Association for Computational Linguistics.
Michael Heilman and Nitin Madnani. 2012. Ets: Dis-
criminative edit models for paraphrase scoring. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics ? Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 529?535, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowledge
management, CIKM ?05, pages 84?90, New York, NY,
USA. ACM.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics, 6(1):97?133.
214
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Association Workshop 2012, pages 33?41,
Dunedin, New Zealand, December.
Marco Lui. 2012. Feature stacking for sentence clas-
sification in evidence-based medicine. In Proceed-
ings of the Australasian Language Technology Associ-
ation Workshop 2012, pages 134?138, Dunedin, New
Zealand, December.
Susan W McRoy. 1992. Using multiple knowledge
sources for word sense discrimination. Computational
Linguistics, 18(1):1?30.
Stephen E Robertson and Steve Walker. 1994. Some
simple effective approximations to the 2-poisson
model for probabilistic weighted retrieval. In Proceed-
ings of the 17th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?94, pages 232?241, Dublin, Ireland.
Bahar Salehi and Paul Cook. to appear. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In *SEM 2013:
The Second Joint Conference on Lexical and Com-
putational Semantics, Atlana, USA. Association for
Computational Linguistics.
TF Smith and MS Waterman. 1981. Identification of
common molecular subsequences. Molecular Biology,
147:195?197.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of latent semantic analysis,
427(7):424?440.
William Webber, Alistair Moffat, and Justin Zobel.
2010. A similarity measure for indefinite rankings.
ACM Transactions on Information Systems (TOIS),
28(4):20.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
215
