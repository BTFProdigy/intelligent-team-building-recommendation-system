Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 688?697, Prague, June 2007. c?2007 Association for Computational Linguistics
The Infinite PCFG using Hierarchical Dirichlet Processes
Percy Liang Slav Petrov Michael I. Jordan Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang, petrov, jordan, klein}@cs.berkeley.edu
Abstract
We present a nonparametric Bayesian model
of tree structures based on the hierarchical
Dirichlet process (HDP). Our HDP-PCFG
model allows the complexity of the grammar
to grow as more training data is available.
In addition to presenting a fully Bayesian
model for the PCFG, we also develop an ef-
ficient variational inference procedure. On
synthetic data, we recover the correct gram-
mar without having to specify its complex-
ity in advance. We also show that our tech-
niques can be applied to full-scale parsing
applications by demonstrating its effective-
ness in learning state-split grammars.
1 Introduction
Probabilistic context-free grammars (PCFGs) have
been a core modeling technique for many as-
pects of linguistic structure, particularly syntac-
tic phrase structure in treebank parsing (Charniak,
1996; Collins, 1999). An important question when
learning PCFGs is how many grammar symbols
to allocate to the learning algorithm based on the
amount of available data.
The question of ?how many clusters (symbols)??
has been tackled in the Bayesian nonparametrics
literature via Dirichlet process (DP) mixture mod-
els (Antoniak, 1974). DP mixture models have since
been extended to hierarchical Dirichlet processes
(HDPs) and HDP-HMMs (Teh et al, 2006; Beal et
al., 2002) and applied to many different types of
clustering/induction problems in NLP (Johnson et
al., 2006; Goldwater et al, 2006).
In this paper, we present the hierarchical Dirich-
let process PCFG (HDP-PCFG). a nonparametric
Bayesian model of syntactic tree structures based
on Dirichlet processes. Specifically, an HDP-PCFG
is defined to have an infinite number of symbols;
the Dirichlet process (DP) prior penalizes the use
of more symbols than are supported by the training
data. Note that ?nonparametric? does not mean ?no
parameters?; rather, it means that the effective num-
ber of parameters can grow adaptively as the amount
of data increases, which is a desirable property of a
learning algorithm.
As models increase in complexity, so does the un-
certainty over parameter estimates. In this regime,
point estimates are unreliable since they do not take
into account the fact that there are different amounts
of uncertainty in the various components of the pa-
rameters. The HDP-PCFG is a Bayesian model
which naturally handles this uncertainty. We present
an efficient variational inference algorithm for the
HDP-PCFG based on a structured mean-field ap-
proximation of the true posterior over parameters.
The algorithm is similar in form to EM and thus in-
herits its simplicity, modularity, and efficiency. Un-
like EM, however, the algorithm is able to take the
uncertainty of parameters into account and thus in-
corporate the DP prior.
Finally, we develop an extension of the HDP-
PCFG for grammar refinement (HDP-PCFG-GR).
Since treebanks generally consist of coarsely-
labeled context-free tree structures, the maximum-
likelihood treebank grammar is typically a poor
model as it makes overly strong independence as-
sumptions. As a result, many generative approaches
to parsing construct refinements of the treebank
grammar which are more suitable for the model-
ing task. Lexical methods split each pre-terminal
symbol into many subsymbols, one for each word,
and then focus on smoothing sparse lexical statis-
688
tics (Collins, 1999; Charniak, 2000). Unlexicalized
methods refine the grammar in a more conservative
fashion, splitting each non-terminal or pre-terminal
symbol into a much smaller number of subsymbols
(Klein and Manning, 2003; Matsuzaki et al, 2005;
Petrov et al, 2006). We apply our HDP-PCFG-GR
model to automatically learn the number of subsym-
bols for each symbol.
2 Models based on Dirichlet processes
At the heart of the HDP-PCFG is the Dirichlet pro-
cess (DP) mixture model (Antoniak, 1974), which is
the nonparametric Bayesian counterpart to the clas-
sical finite mixture model. In order to build up an
understanding of the HDP-PCFG, we first review
the Bayesian treatment of the finite mixture model
(Section 2.1). We then consider the DP mixture
model (Section 2.2) and use it as a building block
for developing nonparametric structured versions of
the HMM (Section 2.3) and PCFG (Section 2.4).
Our presentation highlights the similarities between
these models so that each step along this progression
reflects only the key differences.
2.1 Bayesian finite mixture model
We begin by describing the Bayesian finite mixture
model to establish basic notation that will carry over
the more complex models we consider later.
Bayesian finite mixture model
? ? Dirichlet(?, . . . , ?) [draw component probabilities]
For each component z ? {1, . . . ,K}:
??z ? G0 [draw component parameters]
For each data point i ? {1, . . . , n}:
?zi ? Multinomial(?) [choose component]
?xi ? F (?;?zi) [generate data point]
The model has K components whose prior dis-
tribution is specified by ? = (?1, . . . , ?K). The
Dirichlet hyperparameter ? controls how uniform
this distribution is: as ? increases, it becomes in-
creasingly likely that the components have equal
probability. For each mixture component z ?
{1, . . . ,K}, the parameters of the component ?z are
drawn from some prior G0. Given the model param-
eters (?,?), the data points are generated i.i.d. by
first choosing a component and then generating from
a data model F parameterized by that component.
In document clustering, for example, each data
point xi is a document represented by its term-
frequency vector. Each component (cluster) z
has multinomial parameters ?z which specifies a
distribution F (?;?z) over words. It is custom-
ary to use a conjugate Dirichlet prior G0 =
Dirichlet(??, . . . , ??) over the multinomial parame-
ters, which can be interpreted as adding ???1 pseu-
docounts for each word.
2.2 DP mixture model
We now consider the extension of the Bayesian finite
mixture model to a nonparametric Bayesian mixture
model based on the Dirichlet process. We focus
on the stick-breaking representation (Sethuraman,
1994) of the Dirichlet process instead of the stochas-
tic process definition (Ferguson, 1973) or the Chi-
nese restaurant process (Pitman, 2002). The stick-
breaking representation captures the DP prior most
explicitly and allows us to extend the finite mixture
model with minimal changes. Later, it will enable us
to readily define structured models in a form similar
to their classical versions. Furthermore, an efficient
variational inference algorithm can be developed in
this representation (Section 2.6).
The key difference between the Bayesian finite
mixture model and the DP mixture model is that
the latter has a countably infinite number of mixture
components while the former has a predefined K.
Note that if we have an infinite number of mixture
components, it no longer makes sense to consider
a symmetric prior over the component probabilities;
the prior over component probabilities must decay in
some way. The stick-breaking distribution achieves
this as follows. We write ? ? GEM(?) to mean
that ? = (?1, ?2, . . . ) is distributed according to the
stick-breaking distribution. Here, the concentration
parameter ? controls the number of effective com-
ponents. To draw ? ? GEM(?), we first generate
a countably infinite collection of stick-breaking pro-
portions u1, u2, . . . , where each uz ? Beta(1, ?).
The stick-breaking weights ? are then defined in
terms of the stick proportions:
?z = uz
?
z?<z
(1 ? uz?). (1)
The procedure for generating ? can be viewed as
iteratively breaking off remaining portions of a unit-
689
0 1?1 ?2 ?3 ...
Figure 1: A sample ? ? GEM(1).
length stick (Figure 1). The component probabilities
{?z} will decay exponentially in expectation, but
there is always some probability of getting a smaller
component before a larger one. The parameter ? de-
termines the decay of these probabilities: a larger ?
implies a slower decay and thus more components.
Given the component probabilities, the rest of the
DP mixture model is identical to the finite mixture
model:
DP mixture model
? ? GEM(?) [draw component probabilities]
For each component z ? {1, 2, . . . }:
??z ? G0 [draw component parameters]
For each data point i ? {1, . . . , n}:
?zi ? Multinomial(?) [choose component]
?xi ? F (?;?zi) [generate data point xn]
2.3 HDP-HMM
The next stop on the way to the HDP-PCFG is the
HDP hidden Markov model (HDP-HMM) (Beal et
al., 2002; Teh et al, 2006). An HMM consists of a
set of hidden states, where each state can be thought
of as a mixture component. The parameters of the
mixture component are the emission and transition
parameters. The main aspect that distinguishes it
from a flat finite mixture model is that the transi-
tion parameters themselves must specify a distribu-
tion over next states. Hence, we have not just one
top-level mixture model over states, but also a col-
lection of mixture models, one for each state.
In developing a nonparametric version of the
HMM in which the number of states is infinite, we
need to ensure that the transition mixture models
of each state share a common inventory of possible
next states. We can achieve this by tying these mix-
ture models together using the hierarchical Dirichlet
process (HDP) (Teh et al, 2006). The stick-breaking
representation of an HDP is defined as follows: first,
the top-level stick-breaking weights ? are drawn ac-
cording to the stick-breaking prior as before. Then,
a new set of stick-breaking weights ?? are generated
according based on ?:
?? ? DP(??,?), (2)
where the distribution of DP can be characterized
in terms of the following finite partition property:
for all partitions of the positive integers into sets
A1, . . . , Am,
(??(A1), . . . ,??(Am)) (3)
? Dirichlet
(
???(A1), . . . , ???(Am)
)
,
where ?(A) =
?
k?A ?k.
1 The resulting ?? is an-
other distribution over the positive integers whose
similarity to ? is controlled by a concentration pa-
rameter ??.
HDP-HMM
? ? GEM(?) [draw top-level state weights]
For each state z ? {1, 2, . . . }:
??Ez ? Dirichlet(?) [draw emission parameters]
??Tz ? DP(?
?, ?) [draw transition parameters]
For each time step i ? {1, . . . , n}:
?xi ? F (?;?Ezi) [emit current observation]
?zi+1 ? Multinomial(?Tzi) [choose next state]
Each state z is associated with emission param-
eters ?Ez . In addition, each z is also associated
with transition parameters ?Tz , which specify a dis-
tribution over next states. These transition parame-
ters are drawn from a DP centered on the top-level
stick-breaking weights ? according to Equations (2)
and (3). Assume that z1 is always fixed to a special
START state, so we do not need to generate it.
2.4 HDP-PCFG
We now present the HDP-PCFG, which is the focus
of this paper. For simplicity, we consider Chomsky
normal form (CNF) grammars, which has two types
of rules: emissions and binary productions. We con-
sider each grammar symbol as a mixture component
whose parameters are the rule probabilities for that
symbol. In general, we do not know the appropriate
number of grammar symbols, so our strategy is to
let the number of grammar symbols be infinite and
place a DP prior over grammar symbols.
1Note that this property is a specific instance of the general
stochastic process definition of Dirichlet processes.
690
HDP-PCFG
? ? GEM(?) [draw top-level symbol weights]
For each grammar symbol z ? {1, 2, . . . }:
??Tz ? Dirichlet(?
T ) [draw rule type parameters]
??Ez ? Dirichlet(?
E) [draw emission parameters]
??Bz ? DP(?
B ,??T ) [draw binary production parameters]
For each node i in the parse tree:
?ti ? Multinomial(?Tzi) [choose rule type]
?If ti = EMISSION:
??xi ? Multinomial(?Ezi) [emit terminal symbol]
?If ti = BINARY-PRODUCTION:
??(zL(i), zR(i)) ? Multinomial(?
B
zi) [generate children symbols]
?
?Bz
?Tz
?Ez
z ?
z1
z2
x2
z3
x3
T
Parameters Trees
Figure 2: The definition and graphical model of the HDP-PCFG. Since parse trees have unknown structure,
there is no convenient way of representing them in the visual language of traditional graphical models.
Instead, we show a simple fixed example tree. Node 1 has two children, 2 and 3, each of which has one
observed terminal child. We use L(i) and R(i) to denote the left and right children of node i.
In the HMM, the transition parameters of a state
specify a distribution over single next states; simi-
larly, the binary production parameters of a gram-
mar symbol must specify a distribution over pairs
of grammar symbols for its children. We adapt the
HDP machinery to tie these binary production distri-
butions together. The key difference is that now we
must tie distributions over pairs of grammar sym-
bols together via distributions over single grammar
symbols.
Another difference is that in the HMM, at each
time step, both a transition and a emission are made,
whereas in the PCFG either a binary production or
an emission is chosen. Therefore, each grammar
symbol must also have a distribution over the type
of rule to apply. In a CNF PCFG, there are only
two types of rules, but this can be easily generalized
to include unary productions, which we use for our
parsing experiments.
To summarize, the parameters of each grammar
symbol z consists of (1) a distribution over a finite
number of rule types ?Tz , (2) an emission distribu-
tion ?Ez over terminal symbols, and (3) a binary pro-
duction distribution ?Bz over pairs of children gram-
mar symbols. Figure 2 describes the model in detail.
Figure 3 shows the generation of the binary pro-
duction distributions ?Bz . We draw ?
B
z from a DP
centered on ??T , which is the product distribution
over pairs of symbols. The result is a doubly-infinite
matrix where most of the probability mass is con-
state
right child state
left child state
right child state
left child state
? ? GEM(?)
??T
?Bz ? DP(??
T )
Figure 3: The generation of binary production prob-
abilities given the top-level symbol probabilities ?.
First, ? is drawn from the stick-breaking prior, as
in any DP-based model (a). Next, the outer-product
??T is formed, resulting in a doubly-infinite matrix
matrix (b). We use this as the base distribution for
generating the binary production distribution from a
DP centered on ??T (c).
centrated in the upper left, just like the top-level dis-
tribution ??T .
Note that we have replaced the general
691
G0 and F (?Ezi) pair with Dirichlet(?
E) and
Multinomial(?Ezi) to specialize to natural language,
but there is no difficulty in working with parse
trees with arbitrary non-multinomial observations
or more sophisticated word models.
In many natural language applications, there is
a hard distinction between pre-terminal symbols
(those that only emit a word) and non-terminal sym-
bols (those that only rewrite as two non-terminal or
pre-terminal symbols). This can be accomplished
by letting ?T = (0, 0), which forces a draw ?Tz to
assign probability 1 to one rule type.
An alternative definition of an HDP-PCFG would
be as follows: for each symbol z, draw a distribution
over left child symbols lz ? DP(?) and an inde-
pendent distribution over right child symbols rz ?
DP(?). Then define the binary production distribu-
tion as their cross-product ?Bz = lzr
T
z . This also
yields a distribution over symbol pairs and hence de-
fines a different type of nonparametric PCFG. This
model is simpler and does not require any additional
machinery beyond the HDP-HMM. However, the
modeling assumptions imposed by this alternative
are unappealing as they assume the left child and
right child are independent given the parent, which
is certainly not the case in natural language.
2.5 HDP-PCFG for grammar refinement
An important motivation for the HDP-PCFG is that
of refining an existing treebank grammar to alle-
viate unrealistic independence assumptions and to
improve parsing accuracy. In this scenario, the set
of symbols is known, but we do not know how
many subsymbols to allocate per symbol. We in-
troduce the HDP-PCFG for grammar refinement
(HDP-PCFG-GR), an extension of the HDP-PCFG,
for this task.
The essential difference is that now we have a
collection of HDP-PCFG models for each symbol
s ? S, each one operating at the subsymbol level.
While these HDP-PCFGs are independent in the
prior, they are coupled through their interactions in
the parse trees. For completeness, we have also in-
cluded unary productions, which are essentially the
PCFG counterpart of transitions in HMMs. Finally,
since each node i in the parse tree involves a symbol-
subsymbol pair (si, zi), each subsymbol needs to
specify a distribution over both child symbols and
subsymbols. The former can be handled through
a finite Dirichlet distribution since all symbols are
known and observed, but the latter must be handled
with the Dirichlet process machinery, since the num-
ber of subsymbols is unknown.
HDP-PCFG for grammar refinement (HDP-PCFG-GR)
For each symbol s ? S:
??s ? GEM(?) [draw subsymbol weights]
?For each subsymbol z ? {1, 2, . . . }:
???Tsz ? Dirichlet(?
T ) [draw rule type parameters]
???Esz ? Dirichlet(?
E(s)) [draw emission parameters]
???usz ? Dirichlet(?
u) [unary symbol productions]
???bsz ? Dirichlet(?
b) [binary symbol productions]
??For each child symbol s? ? S:
????Uszs? ? DP(?
U ,?s?) [unary subsymbol prod.]
??For each pair of children symbols (s?, s??) ? S ? S:
????Bszs?s?? ? DP(?
B ,?s??
T
s??) [binary subsymbol]
For each node i in the parse tree:
?ti ? Multinomial(?Tsizi) [choose rule type]
?If ti = EMISSION:
??xi ? Multinomial(?Esizi) [emit terminal symbol]
?If ti = UNARY-PRODUCTION:
??sL(i) ? Multinomial(?
u
sizi) [generate child symbol]
??zL(i) ? Multinomial(?
U
sizisL(i)) [child subsymbol]
?If ti = BINARY-PRODUCTION:
??(sL(i), sR(i)) ? Mult(?sizi) [children symbols]
??(zL(i), zR(i)) ? Mult(?
B
sizisL(i)sR(i)) [subsymbols]
2.6 Variational inference
We present an inference algorithm for the HDP-
PCFG model described in Section 2.4, which can
also be adapted to the HDP-PCFG-GR model with
a bit more bookkeeping. Most previous inference
algorithms for DP-based models involve sampling
(Escobar and West, 1995; Teh et al, 2006). How-
ever, we chose to use variational inference (Blei
and Jordan, 2005), which provides a fast determin-
istic alternative to sampling, hence avoiding issues
of diagnosing convergence and aggregating samples.
Furthermore, our variational inference algorithm es-
tablishes a strong link with past work on PCFG re-
finement and induction, which has traditionally em-
ployed the EM algorithm.
In EM, the E-step involves a dynamic program
that exploits the Markov structure of the parse tree,
and the M-step involves computing ratios based on
expected counts extracted from the E-step. Our vari-
ational algorithm resembles the EM algorithm in
form, but the ratios in the M-step are replaced with
weights that reflect the uncertainty in parameter es-
692
??Bz
?Tz
?Ez
z ?
z1
z2 z3
T
Parameters Trees
Figure 4: We approximate the true posterior p over
parameters ? and latent parse trees z using a struc-
tured mean-field distribution q, in which the distri-
bution over parameters are completely factorized but
the distribution over parse trees is unconstrained.
timates. Because of this procedural similarity, our
method is able to exploit the desirable properties of
EM such as simplicity, modularity, and efficiency.
2.7 Structured mean-field approximation
We denote parameters of the HDP-PCFG as ? =
(?,?), where ? denotes the top-level symbol prob-
abilities and ? denotes the rule probabilities. The
hidden variables of the model are the training parse
trees z. We denote the observed sentences as x.
The goal of Bayesian inference is to compute the
posterior distribution p(?, z | x). The central idea
behind variational inference is to approximate this
intractable posterior with a tractable approximation.
In particular, we want to find the best distribution q?
as defined by
q?
def
= argmin
q?Q
KL(q(?, z)||p(?, z | x)), (4)
where Q is a tractable subset of distributions. We
use a structured mean-field approximation, meaning
that we only consider distributions that factorize as
follows (Figure 4):
Q
def
=
{
q(z)q(?)
K?
z=1
q(?Tz )q(?
E
z )q(?
B
z )
}
. (5)
We further restrict q(?Tz ), q(?
E
z ), q(?
B
z ) to be
Dirichlet distributions, but allow q(z) to be any
multinomial distribution. We constrain q(?) to be a
degenerate distribution truncated at K; i.e., ?z = 0
for z > K. While the posterior grammar does have
an infinite number of symbols, the exponential de-
cay of the DP prior ensures that most of the proba-
bility mass is contained in the first few symbols (Ish-
waran and James, 2001).2 While our variational ap-
proximation q is truncated, the actual PCFG model
is not. AsK increases, our approximation improves.
2.8 Coordinate-wise ascent
The optimization problem defined by Equation (4)
is intractable and nonconvex, but we can use a sim-
ple coordinate-ascent algorithm that iteratively op-
timizes each factor of q in turn while holding the
others fixed. The algorithm turns out to be similar in
form to EM for an ordinary PCFG: optimizing q(z)
is the analogue of the E-step, and optimizing q(?)
is the analogue of the M-step; however, optimizing
q(?) has no analogue in EM. We summarize each
of these updates below (see (Liang et al, 2007) for
complete derivations).
Parse trees q(z): The distribution over parse trees
q(z) can be summarized by the expected suffi-
cient statistics (rule counts), which we denote as
C(z ? zl zr) for binary productions and C(z ?
x) for emissions. We can compute these expected
counts using dynamic programming as in the E-step
of EM.
While the classical E-step uses the current rule
probabilities ?, our mean-field approximation in-
volves an entire distribution q(?). Fortunately, we
can still handle this case by replacing each rule prob-
ability with a weight that summarizes the uncer-
tainty over the rule probability as represented by q.
We define this weight in the sequel.
It is a common perception that Bayesian inference
is slow because one needs to compute integrals. Our
mean-field inference algorithm is a counterexample:
because we can represent uncertainty over rule prob-
abilities with single numbers, much of the existing
PCFG machinery based on EM can be modularly
imported into the Bayesian framework.
Rule probabilities q(?): For an ordinary PCFG,
the M-step simply involves taking ratios of expected
2In particular, the variational distance between the stick-
breaking distribution and the truncated version decreases expo-
nentially as the truncation level K increases.
693
counts:
?Bz (zl, zr) =
C(z ? zl zr)
C(z ? ??)
. (6)
For the variational HDP-PCFG, the optimal q(?) is
given by the standard posterior update for Dirichlet
distributions:3
q(?Bz ) = Dirichlet(?
B
z ;?
B??T + ~C(z)), (7)
where ~C(z) is the matrix of counts of rules with left-
hand side z. These distributions can then be summa-
rized with multinomial weights which are the only
necessary quantities for updating q(z) in the next it-
eration:
WBz (zl, zr)
def
= expEq[log?Bz (zl, zr)] (8)
=
e?(C(z?zl zr)+?
B?zl?zr )
e?(C(z???)+?B)
, (9)
where ?(?) is the digamma function. The emission
parameters can be defined similarly. Inspection of
Equations (6) and (9) reveals that the only difference
between the maximum likelihood and the mean-field
update is that the latter applies the exp(?(?)) func-
tion to the counts (Figure 5).
When the truncation K is large, ?B?zl?zr is near
0 for most right-hand sides (zl, zr), so exp(?(?)) has
the effect of downweighting counts. Since this sub-
traction affects large counts more than small counts,
there is a rich-get-richer effect: rules that have al-
ready have large counts will be preferred.
Specifically, consider a set of rules with the same
left-hand side. The weights for all these rules only
differ in the numerator (Equation (9)), so applying
exp(?(?)) creates a local preference for right-hand
sides with larger counts. Also note that the rule
weights are not normalized; they always sum to at
most one and are equal to one exactly when q(?) is
degenerate. This lack of normalization gives an ex-
tra degree of freedom not present in maximum like-
lihood estimation: it creates a global preference for
left-hand sides that have larger total counts.
Top-level symbol probabilities q(?): Recall that
we restrict q(?) = ???(?), so optimizing ? is
equivalent to finding a single best ??. Unlike q(?)
3Because we have truncated the top-level symbol weights,
the DP prior on ?Bz reduces to a finite Dirichlet distribution.
 
0
 
0.5 1
 
1.5 2  0
 
0.5
 
1
 
1.5
 
2
x
exp(?(x
)) x
Figure 5: The exp(?(?)) function, which is used in
computing the multinomial weights for mean-field
inference. It has the effect of reducing a larger frac-
tion of small counts than large counts.
and q(z), there is no closed form expression for
the optimal ??, and the objective function (Equa-
tion (4)) is not convex in ??. Nonetheless, we can
apply a standard gradient projection method (Bert-
sekas, 1999) to improve ?? to a local maxima.
The part of the objective function in Equation (4)
that depends on ?? is as follows:
L(??) = logGEM(??;?)+ (10)
K?
z=1
Eq[logDirichlet(?Bz ;?
B????T )]
See Liang et al (2007) for the derivation of the gra-
dient. In practice, this optimization has very little ef-
fect on performance. We suspect that this is because
the objective function is dominated by p(x | z) and
p(z | ?), while the contribution of p(? | ?) is mi-
nor.
3 Experiments
We now present an empirical evaluation of the HDP-
PCFG(-GR) model and variational inference tech-
niques. We first give an illustrative example of the
ability of the HDP-PCFG to recover a known gram-
mar and then present the results of experiments on
large-scale treebank parsing.
3.1 Recovering a synthetic grammar
In this section, we show that the HDP-PCFG-GR
can recover a simple grammar while a standard
694
S ? X1X1 | X2X2 | X3X3 | X4X4
X1 ? a1 | b1 | c1 | d1
X2 ? a2 | b2 | c2 | d2
X3 ? a3 | b3 | c3 | d3
X4 ? a4 | b4 | c4 | d4
S
Xi Xi
{ai, bi, ci, di} {ai, bi, ci, di}
(a) (b)
Figure 6: (a) A synthetic grammar with a uniform
distribution over rules. (b) The grammar generates
trees of the form shown on the right.
PCFG fails to do so because it has no built-in con-
trol over grammar complexity. From the grammar in
Figure 6, we generated 2000 trees. The two terminal
symbols always have the same subscript, but we col-
lapsed Xi to X in the training data. We trained the
HDP-PCFG-GR, with truncation K = 20, for both
S and X for 100 iterations. We set al hyperparame-
ters to 1.
Figure 7 shows that the HDP-PCFG-GR recovers
the original grammar, which contains only 4 sub-
symbols, leaving the other 16 subsymbols unused.
The standard PCFG allocates all the subsymbols to
fit the exact co-occurrence statistics of left and right
terminals.
Recall that a rule weight, as defined in Equa-
tion (9), is analogous to a rule probability for stan-
dard PCFGs. We say a rule is effective if its weight
is at least 10?6 and its left hand-side has posterior
is also at least 10?6. In general, rules with weight
smaller than 10?6 can be safely pruned without af-
fect parsing accuracy. The standard PCFG uses all
20 subsymbols of both S and X to explain the data,
resulting in 8320 effective rules; in contrast, the
HDP-PCFG uses only 4 subsymbols for X and 1 for
S, resulting in only 68 effective rules. If the thresh-
old is relaxed from 10?6 to 10?3, then only 20 rules
are effective, which corresponds exactly to the true
grammar.
3.2 Parsing the Penn Treebank
In this section, we show that our variational HDP-
PCFG can scale up to real-world data sets. We ran
experiments on the Wall Street Journal (WSJ) por-
tion of the Penn Treebank. We trained on sections
2?21, used section 24 for tuning hyperparameters,
and tested on section 22.
We binarize the trees in the treebank as follows:
for each non-terminal node with symbol X , we in-
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
0.25
subsymbol
pos
ter
ior
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
0.25
subsymbol
pos
ter
ior
standard PCFG HDP-PCFG
Figure 7: The posteriors over the subsymbols of the
standard PCFG is roughly uniform, whereas the pos-
teriors of the HDP-PCFG is concentrated on four
subsymbols, which is the true number of symbols
in the grammar.
troduce a right-branching cascade of new nodes with
symbol X . The end result is that each node has at
most two children. To cope with unknown words,
we replace any word appearing fewer than 5 times
in the training set with one of 50 unknown word to-
kens derived from 10 word-form features.
Our goal is to learn a refined grammar, where each
symbol in the training set is split into K subsym-
bols. We compare an ordinary PCFG estimated with
maximum likelihood (Matsuzaki et al, 2005) and
the HDP-PCFG estimated using the variational in-
ference algorithm described in Section 2.6.
To parse new sentences with a grammar, we com-
pute the posterior distribution over rules at each span
and extract the tree with the maximum expected cor-
rect number of rules (Petrov and Klein, 2007).
3.2.1 Hyperparameters
There are six hyperparameters in the HDP-PCFG-
GR model, which we set in the following manner:
? = 1, ?T = 1 (uniform distribution over unar-
ies versus binaries), ?E = 1 (uniform distribution
over terminal words), ?u(s) = ?b(s) = 1N(s) , where
N(s) is the number of different unary (binary) right-
hand sides of rules with left-hand side s in the tree-
bank grammar. The two most important hyperpa-
rameters are ?U and ?B , which govern the sparsity
of the right-hand side for unary and binary rules.
We set ?U = ?B although more performance could
probably be gained by tuning these individually. It
turns out that there is not a single ?B that works for
all truncation levels, as shown in Table 1.
If the top-level distribution ? is uniform, the value
of ?B corresponding to a uniform prior over pairs of
children subsymbols is K2. Interestingly, the opti-
mal ?B appears to be superlinear but subquadratic
695
truncation K 2 4 8 12 16 20
best ?B 16 12 20 28 48 80
uniform ?B 4 16 64 144 256 400
Table 1: For each truncation level, we report the ?B
that yielded the highest F1 score on the development
set.
K PCFG PCFG (smoothed) HDP-PCFG
F1 Size F1 Size F1 Size
1 60.47 2558 60.36 2597 60.5 2557
2 69.53 3788 69.38 4614 71.08 4264
4 75.98 3141 77.11 12436 77.17 9710
8 74.32 4262 79.26 120598 79.15 50629
12 70.99 7297 78.8 160403 78.94 86386
16 66.99 19616 79.2 261444 78.24 131377
20 64.44 27593 79.27 369699 77.81 202767
Table 2: Shows development F1 and grammar sizes
(the number of effective rules) as we increase the
truncation K.
in K. We used these values of ?B in the following
experiments.
3.2.2 Results
The regime in which Bayesian inference is most
important is when training data is scarce relative to
the complexity of the model. We train on just sec-
tion 2 of the Penn Treebank. Table 2 shows how
the HDP-PCFG-GR can produce compact grammars
that guard against overfitting. Without smoothing,
ordinary PCFGs trained using EM improve as K in-
creases but start to overfit around K = 4. Simple
add-1.01 smoothing prevents overfitting but at the
cost of a sharp increase in grammar sizes. The HDP-
PCFG obtains comparable performance with a much
smaller number of rules.
We also trained on sections 2?21 to demon-
strate that our methods can scale up and achieve
broadly comparable results to existing state-of-the-
art parsers. When using a truncation level of K =
16, the standard PCFG with smoothing obtains an
F1 score of 88.36 using 706157 effective rules while
the HDP-PCFG-GR obtains an F1 score of 87.08 us-
ing 428375 effective rules. We expect to see greater
benefits from the HDP-PCFG with a larger trunca-
tion level.
4 Related work
The question of how to select the appropriate gram-
mar complexity has been studied in earlier work.
It is well known that more complex models nec-
essarily have higher likelihood and thus a penalty
must be imposed for more complex grammars. Ex-
amples of such penalized likelihood procedures in-
clude Stolcke and Omohundro (1994), which used
an asymptotic Bayesian model selection criterion
and Petrov et al (2006), which used a split-merge
algorithm which procedurally determines when to
switch between grammars of various complexities.
These techniques are model selection techniques
that use heuristics to choose among competing sta-
tistical models; in contrast, the HDP-PCFG relies on
the Bayesian formalism to provide implicit control
over model complexity within the framework of a
single probabilistic model.
Johnson et al (2006) also explored nonparamet-
ric grammars, but they do not give an inference al-
gorithm for recursive grammars, e.g., grammars in-
cluding rules of the form A ? BC and B ? DA.
Recursion is a crucial aspect of PCFGs and our
inference algorithm does handle it. Finkel et al
(2007) independently developed another nonpara-
metric model of grammars. Though their model is
also based on hierarchical Dirichlet processes and is
similar to ours, they present a different inference al-
gorithm which is based on sampling. Kurihara and
Sato (2004) and Kurihara and Sato (2006) applied
variational inference to PCFGs. Their algorithm is
similar to ours, but they did not consider nonpara-
metric models.
5 Conclusion
We have presented the HDP-PCFG, a nonparametric
Bayesian model for PCFGs, along with an efficient
variational inference algorithm. While our primary
contribution is the elucidation of the model and algo-
rithm, we have also explored some important empir-
ical properties of the HDP-PCFG and also demon-
strated the potential of variational HDP-PCFGs on a
full-scale parsing task.
696
References
C. E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric prob-
lems. Annals of Statistics, 2:1152?1174.
M. Beal, Z. Ghahramani, and C. Rasmussen. 2002. The
infinite hidden Markov model. In Advances in Neural
Information Processing Systems (NIPS), pages 577?
584.
D. Bertsekas. 1999. Nonlinear programming.
D. Blei and M. I. Jordan. 2005. Variational inference for
Dirichlet process mixtures. Bayesian Analysis, 1:121?
144.
E. Charniak. 1996. Tree-bank grammars. In Association
for the Advancement of Artificial Intelligence (AAAI).
E. Charniak. 2000. A maximum-entropy-inspired parser.
In North American Association for Computational
Linguistics (NAACL), pages 132?139.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. D. Escobar and M. West. 1995. Bayesian density
estimation and inference using mixtures. Journal of
the American Statistical Association, 90:577?588.
T. S. Ferguson. 1973. A Bayesian analysis of some non-
parametric problems. Annals of Statistics, 1:209?230.
J. R. Finkel, T. Grenager, and C. Manning. 2007. The
infinite tree. In Association for Computational Lin-
guistics (ACL).
S. Goldwater, T. Griffiths, and M. Johnson. 2006. Con-
textual dependencies in unsupervised word segmenta-
tion. In International Conference on Computational
Linguistics and Association for Computational Lin-
guistics (COLING/ACL).
H. Ishwaran and L. F. James. 2001. Gibbs sampling
methods for stick-breaking priors. Journal of the
American Statistical Association, 96:161?173.
M. Johnson, T. Griffiths, and S. Goldwater. 2006. Adap-
tor grammars: A framework for specifying composi-
tional nonparametric Bayesian models. In Advances
in Neural Information Processing Systems (NIPS).
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Association for Computational Linguistics
(ACL), pages 423?430.
K. Kurihara and T. Sato. 2004. An application of the
variational Bayesian approach to probabilistic context-
free grammars. In International Joint Conference on
Natural Language Processing Workshop Beyond Shal-
low Analyses.
K. Kurihara and T. Sato. 2006. Variational Bayesian
grammar induction for natural language. In Interna-
tional Colloquium on Grammatical Inference.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein.
2007. Nonparametric PCFGs using Dirichlet pro-
cesses. Technical report, Department of Statistics,
University of California at Berkeley.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Association for
Computational Linguistics (ACL).
S. Petrov and D. Klein. 2007. Learning and inference
for hierarchically split PCFGs. In Human Language
Technology and North American Association for Com-
putational Linguistics (HLT/NAACL).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In International Conference on Computa-
tional Linguistics and Association for Computational
Linguistics (COLING/ACL).
J. Pitman. 2002. Combinatorial stochastic processes.
Technical Report 621, Department of Statistics, Uni-
versity of California at Berkeley.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4:639?650.
A. Stolcke and S. Omohundro. 1994. Inducing prob-
abilistic grammars by Bayesian model merging. In
Grammatical Inference and Applications.
Y. W. Teh, M. I. Jordan, M. Beal, and D. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566?1581.
697
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 897?905, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Structured Models for Phone Recognition
Slav Petrov Adam Pauls Dan Klein
Computer Science Department, EECS Divison
University of California at Berkeley
Berkeley, CA, 94720, USA
{petrov,adpauls,klein}@cs.berkeley.edu
Abstract
We present a maximally streamlined approach to
learning HMM-based acoustic models for automatic
speech recognition. In our approach, an initial mono-
phone HMM is iteratively refined using a split-merge
EM procedure which makes no assumptions about
subphone structure or context-dependent structure,
and which uses only a single Gaussian per HMM
state. Despite the much simplified training process,
our acoustic model achieves state-of-the-art results
on phone classification (where it outperforms almost
all other methods) and competitive performance on
phone recognition (where it outperforms standard CD
triphone / subphone / GMM approaches). We also
present an analysis of what is and is not learned by
our system.
1 Introduction
Continuous density hiddenMarkov models (HMMs)
underlie most automatic speech recognition (ASR)
systems in some form. While the basic algorithms
for HMM learning and inference are quite general,
acoustic models of speech standardly employ rich
speech-specific structures to improve performance.
For example, it is well known that a monophone
HMM with one state per phone is too coarse an
approximation to the true articulatory and acoustic
process. The HMM state space is therefore refined
in several ways. To model phone-internal dynam-
ics, phones are split into beginning, middle, and end
subphones (Jelinek, 1976). To model cross-phone
coarticulation, the states of the HMM are refined
by splitting the phones into context-dependent tri-
phones. These states are then re-clustered (Odell,
1995) and the parameters of their observation dis-
tributions are tied back together (Young and Wood-
land, 1994). Finally, to model complex emission
densities, states emit mixtures of multivariate Gaus-
sians. This standard structure is shown schemati-
cally in Figure 1. While this rich structure is pho-
netically well-motivated and empirically success-
ful, so much structural bias may be unnecessary, or
even harmful. For example in the domain of syn-
tactic parsing with probabilistic context-free gram-
mars (PCFGs), a surprising recent result is that au-
tomatically induced grammar refinements can out-
perform sophisticated methods which exploit sub-
stantial manually articulated structure (Petrov et al,
2006).
In this paper, we consider a much more automatic,
data-driven approach to learning HMM structure for
acoustic modeling, analagous to the approach taken
by Petrov et al (2006) for learning PCFGs. We start
with a minimal monophone HMM in which there is
a single state for each (context-independent) phone.
Moreover, the emission model for each state is a sin-
gle multivariate Gaussian (over the standard MFCC
acoustic features). We then iteratively refine this
minimal HMM through state splitting, adding com-
plexity as needed. States in the refined HMMs are
always substates of the original HMM and are there-
fore each identified with a unique base phone. States
are split, estimated, and (perhaps) merged, based on
a likelihood criterion. Our model never allows ex-
plicit Gaussian mixtures, though substates may de-
velop similar distributions and thereby emulate such
mixtures.
In principle, discarding the traditional structure
can either help or hurt the model. Incorrect prior
splits can needlessly fragment training data and in-
correct prior tying can limit the model?s expressiv-
ity. On the other hand, correct assumptions can
increase the efficiency of the learner. Empirically,
897
Start
begin end
End
mid begin endmid
d
7 
= c(#-d-ae)
begin endmid
ae
3 
= c(d-ae-d) d
13 
= c(ae-d-#)
Start
a d
End
a d a d
d ae d
b c b c b c
Figure 1: Comparison of the standard model to our model (here
shown with k = 4 subphones per phone) for the word dad.
The dependence of subphones across phones in our model is
not shown, while the context clustering in the standard model is
shown only schematically.
we show that our automatic approach outperforms
classic systems on the task of phone recognition on
the TIMIT data set. In particular, it outperforms
standard state-tied triphone models like Young and
Woodland (1994), achieving a phone error rate of
26.4% versus 27.7%. In addition, our approach
gives state-of-the-art performance on the task of
phone classification on the TIMIT data set, suggest-
ing that our learned structure is particularly effec-
tive at modeling phone-internal structure. Indeed,
our error rate of 21.4% is outperformed only by the
recent structured margin approach of Sha and Saul
(2006). It remains to be seen whether these posi-
tive results on acoustic modeling will facilitate better
word recognition rates in a large vocabulary speech
recognition system.
We also consider the structures learned by the
model. Subphone structure is learned, similar to,
but richer than, standard begin-middle-end struc-
tures. Cross-phone coarticulation is also learned,
with classic phonological classes often emerging
naturally.
Many aspects of this work are intended to sim-
plify rather than further articulate the acoustic pro-
cess. It should therefore be clear that the basic tech-
niques of splitting, merging, and learning using EM
are not in themselves new for ASR. Nor is the basic
latent induction method new (Matsuzaki et al, 2005;
Petrov et al, 2006). What is novel in this paper is (1)
the construction of an automatic system for acous-
tic modeling, with substantially streamlined struc-
ture, (2) the investigation of variational inference for
such a task, (3) the analysis of the kinds of struc-
tures learned by such a system, and (4) the empirical
demonstration that such a system is not only com-
petitive with the traditional approach, but can indeed
outperform even very recent work on some prelimi-
nary measures.
2 Learning
In the following, we propose a greatly simplified
model that does not impose any manually specified
structural constraints. Instead of specifying struc-
ture a priori, we use the Expectation-Maximization
(EM) algorithm for HMMs (Baum-Welch) to auto-
matically induce the structure in a way that maxi-
mizes data likelihood.
In general, our training data consists of sets
of acoustic observation sequences and phone level
transcriptions r which specify a sequence of phones
from a set of phones Y , but does not label each
time frame with a phone. We refer to an observa-
tion sequence as x = x1, . . . , xT where xi ? R39
are standard MFCC features (Davis and Mermel-
stein, 1980). We wish to induce an HMM over a
set of states S for which we also have a function
pi : S ? Y that maps every state in S to a phone
in Y . Note that in the usual formulation of the EM
algorithm for HMMs, one is interested in learning
HMM parameters ? that maximize the likelihood of
the observations P(x|?); in contrast, we aim to max-
imize the joint probability of our observations and
phone transcriptions P(x, r|?) or observations and
phone sequences P(x,y|?) (see below). We now de-
scribe this relatively straightforward modification of
the EM algorithm.
2.1 The Hand-Aligned Case
For clarity of exposition we first consider a simpli-
fied scenario in which we are given hand-aligned
phone labels y = y1, . . . , yT for each time t, as is
the case for the TIMIT dataset. Our procedure does
not require such extensive annotation of the training
data and in fact gives better performance when the
exact transition point between phones are not pre-
specified but learned.
We define forward and backward probabilities
(Rabiner, 1989) in the following way: the forward
probability is the probability of observing the se-
quence x1, . . . , xt with transcription y1, . . . , yt and
898
next
previous
th
dh
p
t
b
g
dx
w
r
l
s
z
sh
f
cl
vcl
m
n
ng
l
r
er
0
(a)
next
previous
th
dh
p
t
b
g
dx
w
r
l
s
z
sh
f
cl
vcl
m
n
ng
l
r
er
0 1
(b)
next
previous
th
dh
p
t
b
g
dx
w
r
l
s
z
sh
f
cl
vcl
m
n
ng
l
r
er
0
3
2
1
(c)
next
previous
th
dh
p
t
b
g
dx
w
r
l
s
z
sh
f
cl
vcl
m
n
ng
l
r
er
1
6
0
3
4
7
25
(d)
Figure 2: Iterative refinement of the /ih/ phone with 1, 2, 4, 8 substates.
ending in state s at time t:
?t(s) = P(x1, . . . , xt, y1, . . . yt, st = s|?),
and the backward probability is the probability of
observing the sequence xt+1, . . . , xT with transcrip-
tion yt+1, . . . , yT , given that we start in state s at
time t:
?t(s) = P(xt+1, . . . , xT , yt+1, . . . , yT |st = s, ?),
where ? are the model parameters. As usual, we
parameterize our HMMs with ass? , the probability
of transitioning from state s to s?, and bs(x) ?
N (?s,?s), the probability emitting the observation
x when in state s.
These probabilities can be computed using the
standard forward and backward recursions (Rabiner,
1989), except that at each time t, we only con-
sider states st for which pi(st) = yt, because we
have hand-aligned labels for the observations. These
quantities also allow us to compute the posterior
counts necessary for the E-step of the EM algorithm.
2.2 Splitting
One way of inducing arbitrary structural annota-
tions would be to split each HMM state in into
m substates, and re-estimate the parameters for the
split HMM using EM. This approach has two ma-
jor drawbacks: for larger m it is likely to converge
to poor local optima, and it allocates substates uni-
formly across all states, regardless of how much an-
notation is required for good performance.
To avoid these problems, we apply a hierarchical
parameter estimation strategy similar in spirit to the
work of Sankar (1998) and Ueda et al (2000), but
here applied to HMMs rather than to GMMs. Be-
ginning with the baseline model, where each state
corresponds to one phone, we repeatedly split and
re-train the HMM. This strategy ensures that each
split HMM is initialized ?close? to some reasonable
maximum.
Concretely, each state s in the HMM is split in
two new states s1, s2 with pi(s1) = pi(s2) = pi(s).
We initialize EM with the parameters of the previ-
ous HMM, splitting every previous state s in two
and adding a small amount of randomness  ? 1%
to its transition and emission probabilities to break
symmetry:
as1s? ? ass? + ,
bs1(o) ? N (?s + ,?s),
and similarly for s2. The incoming transitions are
split evenly.
We then apply the EM algorithm described above
to re-estimate these parameters before performing
subsequent split operations.
2.3 Merging
Since adding substates divides HMM statistics into
many bins, the HMM parameters are effectively es-
timated from less data, which can lead to overfitting.
Therefore, it would be to our advantage to split sub-
899
states only where needed, rather than splitting them
all.
We realize this goal by merging back those splits
s ? s1s2 for which, if the split were reversed, the
loss in data likelihood would be smallest. We ap-
proximate the loss in data likelihood for a merge
s1s2 ? swith the following likelihood ratio (Petrov
et al, 2006):
?(s1 s2 ? s) =
?
sequences
?
t
Pt(x,y)
P(x,y)
.
Here P(x,y) is the joint likelihood of an emission
sequence x and associated state sequence y. This
quantity can be recovered from the forward and
backward probabilities using
P(x,y) =
?
s:pi(s)=yt
?t(s) ? ?t(s).
Pt(x,y) is an approximation to the same joint like-
lihood where states s1 and s2 are merged. We ap-
proximate the true loss by only considering merging
states s1 and s2 at time t, a value which can be ef-
ficiently computed from the forward and backward
probabilities. The forward score for the merged state
s at time t is just the sum of the two split scores:
??t(s) = ?t(s1) + ?t(s2),
while the backward score is a weighted sum of the
split scores:
??t(s) = p1?t(s1) + p2?t(s2),
where p1 and p2 are the relative (posterior) frequen-
cies of the states s1 and s2.
Thus, the likelihood after merging s1 and s2 at
time t can be computed from these merged forward
and backward scores as:
P t(x,y) = ??t(s) ? ??t(s) +
?
s?
?t(s
?) ? ?t(s
?)
where the second sum is over the other substates of
xt, i.e. {s? : pi(s?) = xt, s? /? {s1, s2}}. This
expression is an approximation because it neglects
interactions between instances of the same states at
multiple places in the same sequence. In particular,
since phones frequently occur with multiple consec-
utive repetitions, this criterion may vastly overesti-
mate the actual likelihood loss. As such, we also im-
plemented the exact criterion, that is, for each split,
we formed a new HMM with s1 and s2 merged and
calculated the total data likelihood. This method
is much more computationally expensive, requiring
a full forward-backward pass through the data for
each potential merge, and was not found to produce
noticeably better performance. Therefore, all exper-
iments use the approximate criterion.
2.4 The Automatically-Aligned Case
It is straightforward to generalize the hand-aligned
case to the case where the phone transcription is
known, but no frame level labeling is available. The
main difference is that the phone boundaries are not
known in advance, which means that there is now
additional uncertainty over the phone states. The
forward and backward recursions must thus be ex-
panded to consider all state sequences that yield the
given phone transcription. We can accomplish this
with standard Baum-Welch training.
3 Inference
An HMM over refined subphone states s ? S nat-
urally gives posterior distributions P(s|x) over se-
quences of states s. We would ideally like to ex-
tract the transcription r of underlying phones which
is most probable according to this posterior1. The
transcription is two stages removed from s. First,
it collapses the distinctions between states s which
correspond to the same phone y = pi(s). Second,
it collapses the distinctions between where phone
transitions exactly occur. Viterbi state sequences can
easily be extracted using the basic Viterbi algorithm.
On the other hand, finding the best phone sequence
or transcription is intractable.
As a compromise, we extract the phone sequence
(not transcription) which has highest probability in
a variational approximation to the true distribution
(Jordan et al, 1999). Let the true posterior distri-
bution over phone sequences be P(y|x). We form
an approximation Q(y) ? P(y|x), where Q is an
approximation specific to the sequence x and factor-
1Remember that by ?transcription? we mean a sequence of
phones with duplicates removed.
900
izes as:
Q(y) =
?
t
q(t, xt, yt+1).
We would like to fit the values q, one for each time
step and state-state pair, so as to make Q as close to
P as possible:
min
q
KL(P(y|x)||Q(y)).
The solution can be found analytically using La-
grange multipliers:
q(t, y, y?) =
P(Yt = y, Yt+1 = y?|x)
P(Yt = y|x)
.
where we have made the position-specific random
variables Yt explicit for clarity. This approximation
depends only on our ability to calculate posteriors
over phones or phone-phone pairs at individual po-
sitions t, which is easy to obtain from the state pos-
teriors, for example:
P(Yt = y,Yt+1 = y
?|x) =
?
s:pi(s)=y
?
s?:pi(s?)=y?
?t(s)ass?bs?(xt)?t+1(s
?)
P(x)
Finding the Viterbi phone sequence in the approxi-
mate distribution Q, can be done with the Forward-
Backward algorithm over the lattice of q values.
4 Experiments
We tested our model on the TIMIT database, using
the standard setups for phone recognition and phone
classification. We partitioned the TIMIT data into
training, development, and (core) test sets according
to standard practice (Lee and Hon, 1989; Gunawar-
dana et al, 2005; Sha and Saul, 2006). In particu-
lar, we excluded all sa sentences and mapped the 61
phonetic labels in TIMIT down to 48 classes before
training our HMMs. At evaluation, these 48 classes
were further mapped down to 39 classes, again in
the standard way.
MFCC coefficients were extracted from the
TIMIT source as in Sha and Saul (2006), includ-
ing delta and delta-delta components. For all experi-
ments, our system and all baselines we implemented
used full covariance when parameterizing emission
 
0.24
 
0.26
 
0.28 0.3
 
0.32
 
0.34
 
0.36
 
0.38 0.4
 
0.42  0
 
200 
400 
600 
800 1
000 1
200 1
400 1
600 1
800 2
000
Phone Recognition Error
Numb
er of S
tates
split o
nly
split a
nd me
rge
split a
nd me
rge, au
tomati
c align
ment
Figure 3: Phone recognition error for models of increasing size
models.2 All Gaussians were endowed with weak
inverse Wishart priors with zero mean and identity
covariance.3
4.1 Phone Recognition
In the task of phone recognition, we fit an HMM
whose output, with subsequent states collapsed, cor-
responds to the training transcriptions. In the TIMIT
data set, each frame is manually phone-annotated, so
the only uncertainty in the basic setup is the identity
of the (sub)states at each frame.
We therefore began with a single state for each
phone, in a fully connected HMM (except for spe-
cial treatment of dedicated start and end states). We
incrementally trained our model as described in Sec-
tion 2, with up to 6 split-merge rounds. We found
that reversing 25% of the splits yielded good overall
performance while maintaining compactness of the
model.
We decoded using the variational decoder de-
scribed in Section 3. The output was then scored
against the reference phone transcription using the
standard string edit distance.
During both training and decoding, we used ?flat-
tened? emission probabilities by exponentiating to
some 0 < ? < 1. We found the best setting for ?
to be 0.2, as determined by tuning on the develop-
ment set. This flattening compensates for the non-
2Most of our findings also hold for diagonal covariance
Gaussians, albeit the final error rates are 2-3% higher.
3Following previous work with PCFGs (Petrov et al, 2006),
we experimented with smoothing the substates towards each
other to prevent overfitting, but we were unable to achieve any
performance gains.
901
Method Error Rate
State-Tied Triphone HMM
27.7%1
(Young and Woodland, 1994)
Gender Dependent Triphone HMM
27.1%1
(Lamel and Gauvain, 1993)
This Paper 26.4%
Bayesian Triphone HMM
25.6%
(Ming and Smith, 1998)
Heterogeneous classifiers
24.4%
(Halberstadt and Glass, 1998)
Table 1: Phone recognition error rates on the TIMIT core test
from Glass (2003).
1These results are on a slightly easier test set.
independence of the frames, partially due to over-
lapping source samples and partially due to other
unmodeled correlations.
Figure 3 shows the recognition error as the model
grows in size. In addition to the basic setup de-
scribed so far (split and merge), we also show a
model in which merging was not performed (split
only). As can be seen, the merging phase not only
decreases the number of HMM states at each round,
but also improves phone recognition error at each
round.
We also compared our hierarchical split only
model with a model where we directly split all states
into 2k substates, so that these models had the same
number of states as a a hierarchical model after k
split and merge cycles. While for small k, the dif-
ference was negligible, we found that the error in-
creased by 1% absolute for k = 5. This trend is to
be expected, as the possible interactions between the
substates grows with the number of substates.
Also shown in Figure 3, and perhaps unsurprising,
is that the error rate can be further reduced by allow-
ing the phone boundaries to drift from the manual
alignments provided in the TIMIT training data. The
split and merge, automatic alignment line shows the
result of allowing the EM fitting phase to reposition
each phone boundary, giving absolute improvements
of up to 0.6%.
We investigated how much improvement in accu-
racy one can gain by computing the variational ap-
proximation introduced in Section 3 versus extract-
ing the Viterbi state sequence and projecting that se-
quence to its phone transcription. The gap varies,
Method Error Rate
GMM Baseline (Sha and Saul, 2006) 26.0%
HMM Baseline (Gunawardana et al, 2005) 25.1%
SVM (Clarkson and Moreno, 1999) 22.4%
Hidden CRF (Gunawardana et al, 2005) 21.7%
This Paper 21.4%
Large Margin GMM (Sha and Saul, 2006) 21.1%
Table 2: Phone classification error rates on the TIMIT core test.
but on a model with roughly 1000 states (5 split-
merge rounds), the variational decoder decreases er-
ror from 26.5% to 25.6%. The gain in accuracy
comes at a cost in time: we must run a (possibly
pruned) Forward-Backward pass over the full state
space S, then another over the smaller phone space
Y . In our experiments, the cost of variational decod-
ing was a factor of about 3, which may or may not
justify a relative error reduction of around 4%.
The performance of our best model (split and
merge, automatic alignment, and variational decod-
ing) on the test set is 26.4%. A comparison of our
performance with other methods in the literature is
shown in Table 1. Despite our structural simplic-
ity, we outperform state-tied triphone systems like
Young andWoodland (1994), a standard baseline for
this task, by nearly 2% absolute. However, we fall
short of the best current systems.
4.2 Phone Classification
Phone classification is the fairly constrained task of
classifying in isolation a sequence of frames which
is known to span exactly one phone. In order to
quantify how much of our gains over the triphone
baseline stem from modeling context-dependencies
and how much from modeling the inner structure of
the phones, we fit separate HMM models for each
phone, using the same split and merge procedure as
above (though in this case only manual alignments
are reasonable because we test on manual segmen-
tations). For each test frame sequence, we com-
pute the likelihood of the sequence from the forward
probabilities of each individual phone HMM. The
phone giving highest likelihood to the input was se-
lected. The error rate is a simple fraction of test
phones classified correctly.
Table 2 shows a comparison of our performance
with that of some other methods in the literature.
A minimal comparison is to a GMM with the same
number of mixtures per phone as our model?s maxi-
902
iy ix eh ae ax uw uh aa ey ay oy aw ow er el r w y m n ng dx jh ch z s zh hh v f dh th b p d t g k sil
iy ix eh ae ax uw uh aa ey ay oy aw ow er el r w y m n ng dx jh ch z s zh hh v f dh th b p d t g k sil
iyixehaeaxuwuhaaeyayoyawowerelrwymnngdxjhchzszhhhvfdhthbpdtgksil
iyixehaeaxuwuhaaeyayoyawowerelrwymnngdxjhchzszhhhvfdhthbpdtgksil
Hypothesis
Ref
ere
nce
vowels/semivowels
nasals/flaps
strong fricatives
weak fricatives
stops
Figure 4: Phone confusion matrix. 76% of the substitutions fall
within the shown classes.
mum substates per phone. While these models have
the same number of total Gaussians, in our model
the Gaussians are correlated temporally, while in
the GMM they are independent. Enforcing begin-
middle-end HMM structure (see HMM Baseline) in-
creases accuracy somewhat, but our more general
model clearly makes better use of the available pa-
rameters than those baselines.
Indeed, our best model achieves a surpris-
ing performance of 21.4%, greatly outperform-
ing other generative methods and achieving perfor-
mance competitive with state-of-the-art discrimina-
tive methods. Only the recent structured margin ap-
proach of Sha and Saul (2006) gives a better perfor-
mance than our model. The strength of our system
on the classification task suggests that perhaps it is
modeling phone-internal structure more effectively
than cross-phone context.
5 Analysis
While the overall phone recognition and classifi-
cation numbers suggest that our system is broadly
comparable to and perhaps in certain ways superior
to classical approaches, it is illuminating to investi-
gate what is and is not learned by the model.
Figure 4 gives a confusion matrix over the substi-
tution errors made by our model. The majority of the
next
previous
eh
ow
ao
aa
ey
iy
ix
v
f
k
m
ow
ao
aa
ey
iy
ih
ae
ix
z
f
s
1
4
3
5
62
0
p
Figure 5: Phone contexts and subphone structure. The /l/ phone
after 3 split-merge iterations is shown.
confusions are within natural classes. Some partic-
ularly frequent and reasonable confusions arise be-
tween the consonantal /r/ and the vocalic /er/ (the
same confusion arises between /l/ and /el/, but the
standard evaluation already collapses this distinc-
tion), the reduced vowels /ax/ and /ix/, the voiced
and voiceless alveolar sibilants /z/ and /s/, and the
voiced and voiceless stop pairs. Other vocalic con-
fusions are generally between vowels and their cor-
responding reduced forms. Overall, 76% of the sub-
stitutions are within the broad classes shown in the
figure.
We can also examine the substructure learned for
the various phones. Figure 2 shows the evolution
of the phone /ih/ from a single state to 8 substates
during split/merge (no merges were chosen for this
phone), using hand-alignment of phones to frames.
These figures were simplified from the complete
state transition matrices as follows: (1) adjacent
phones? substates are collapsed, (2) adjacent phones
are selected based on frequency and inbound prob-
ability (and forced to be the same across figures),
(3) infrequent arcs are suppressed. In the first split,
(b), a sonorant / non-sonorant distinction is learned
over adjacent phones, along with a state chain which
captures basic duration (a self-looping state gives
an exponential model of duration; the sum of two
such states is more expressive). Note that the nat-
903
ural classes interact with the chain in a way which
allows duration to depend on context. In further re-
finements, more structure is added, including a two-
track path in (d) where one track captures the distinct
effects on higher formants of r-coloring and nasal-
ization. Figure 5 shows the corresponding diagram
for /l/, where some merging has also occurred. Dif-
ferent natural classes emerge in this case, with, for
example, preceding states partitioned into front/high
vowels vs. rounded vowels vs. other vowels vs. con-
sonants. Following states show a front/back dis-
tinction and a consonant distinction, and the phone
/m/ is treated specially, largely because the /lm/ se-
quence tends to shorten the /l/ substantially. Note
again how context, internal structure, and duration
are simultaneously modeled. Of course, it should
be emphasized that post hoc analysis of such struc-
ture is a simplification and prone to seeing what one
expects; we present these examples to illustrate the
broad kinds of patterns which are detected.
As a final illustration of the nature of the learned
models, Table 3 shows the number of substates allo-
cated to each phone by the split/merge process (the
maximum is 32 for this stage) for the case of hand-
aligned (left) as well as automatically-aligned (right)
phone boundaries. Interestingly, in the hand-aligned
case, the vowels absorb most of the complexity since
many consonantal cues are heavily evidenced on
adjacent vowels. However, in the automatically-
aligned case, many vowel frames with substantial
consontant coloring are re-allocated to those adja-
cent consonants, giving more complex consonants,
but comparatively less complex vowels.
6 Conclusions
We have presented a minimalist, automatic approach
for building an accurate acoustic model for phonetic
classification and recognition. Our model does not
require any a priori phonetic bias or manual spec-
ification of structure, but rather induces the struc-
ture in an automatic and streamlined fashion. Start-
ing from a minimal monophone HMM, we auto-
matically learn models that achieve highly compet-
itive performance. On the TIMIT phone recogni-
tion task our model clearly outperforms standard
state-tied triphone models like Young and Wood-
land (1994). For phone classification, our model
Vowels
aa 31 32
ae 32 17
ah 31 8
ao 32 23
aw 18 6
ax 18 3
ay 32 28
eh 32 16
el 6 4
en 4 3
er 32 31
ey 32 30
ih 32 11
ix 31 16
iy 31 32
ow 26 10
oy 4 4
uh 5 2
uw 21 8
Consonants
b 2 32
ch 13 30
d 2 14
dh 6 31
dx 2 3
f 32 32
g 2 15
hh 3 5
jh 3 16
k 30 32
l 25 32
m 25 25
n 29 32
ng 3 4
p 5 24
r 32 32
s 32 32
sh 30 32
t 24 32
th 8 11
v 23 11
w 10 21
y 3 7
z 31 32
zh 2 2
Other
epi 2 4
sil 32 32
vcl 29 30
cl 31 32
Table 3: Number of substates allocated per phone. The left
column gives the number of substates allocated when training
on manually aligned training sequences, while the right column
gives the number allocated when we automatically determine
phone boundaries.
achieves performance competitive with the state-of-
the-art discriminative methods (Sha and Saul, 2006),
despite being generative in nature. This result to-
gether with our analysis of the context-dependencies
and substructures that are being learned, suggests
that our model is particularly well suited for mod-
eling phone-internal structure. It does, of course
remain to be seen if and how these benefits can be
scaled to larger systems.
References
P. Clarkson and P. Moreno. 1999. On the use of Sup-
port Vector Machines for phonetic classification. In
ICASSP ?99.
S. B. Davis and P. Mermelstein. 1980. Comparison
of parametric representation for monosyllabic word
recognition in continuously spoken sentences. IEEE
Transactions on Acoustics, Speech, and Signal Pro-
cessing, 28(4).
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer Speech and Lan-
guage, 17(2).
A. Gunawardana, M. Mahajan, A. Acero, and J. Platt.
2005. Hidden Conditional Random Fields for phone
recognition. In Eurospeech ?05.
A. K. Halberstadt and J. R. Glass. 1998. Hetero-
geneous measurements and multiple classifiers for
speech recognition. In ICSLP ?98.
F. Jelinek. 1976. Continuous speech recognition by sta-
tistical methods. Proceedings of the IEEE.
904
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. Learning in Graphical Models.
L. Lamel and J. Gauvain. 1993. Cross-lingual experi-
ments with phone recognition. In ICASSP ?93.
K. F. Lee and H. W. Hon. 1989. Speaker-independent
phone recognition using Hidden Markov Models.
IEEE Transactions on Acoustics, Speech, and Signal
Processing, 37(11).
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL ?05.
J. Ming and F.J. Smith. 1998. Improved phone recogni-
tion using Bayesian triphone models. In ICASSP ?98.
J. J. Odell. 1995. The Use of Context in Large Vocab-
ulary Speech Recognition. Ph.D. thesis, University of
Cambridge.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In COLING-ACL ?06.
L. Rabiner. 1989. A Tutorial on hidden Markov mod-
els and selected applications in speech recognition. In
IEEE.
A. Sankar. 1998. Experiments with a Gaussian merging-
splitting algorithm for HMM training for speech
recognition. In DARPA Speech Recognition Workshop
?98.
F. Sha and L. K. Saul. 2006. Large margin Gaussian mix-
ture modeling for phonetic classification and recogni-
tion. In ICASSP ?06.
N. Ueda, R. Nakano, Z. Ghahramani, and G. E. Hinton.
2000. Split andMerge EM algorithm for mixture mod-
els. Neural Computation, 12(9).
S. J. Young and P. C. Woodland. 1994. State clustering
in HMM-based continuous speech recognition. Com-
puter Speech and Language, 8(4).
905
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 108?116,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Coarse-to-Fine Syntactic Machine Translation
using Language Projections
Slav Petrov Aria Haghighi Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov, aria42, klein}@eecs.berkeley.edu
Abstract
The intersection of tree transducer-based
translation models with n-gram language
models results in huge dynamic programs for
machine translation decoding. We propose a
multipass, coarse-to-fine approach in which
the language model complexity is incremen-
tally introduced. In contrast to previous order-
based bigram-to-trigram approaches, we fo-
cus on encoding-based methods, which use
a clustered encoding of the target language.
Across various encoding schemes, and for
multiple language pairs, we show speed-ups of
up to 50 times over single-pass decoding while
improving BLEU score. Moreover, our entire
decoding cascade for trigram language models
is faster than the corresponding bigram pass
alone of a bigram-to-trigram decoder.
1 Introduction
In the absence of an n-gram language model, decod-
ing a synchronous CFG translation model is very
efficient, requiring only a variant of the CKY al-
gorithm. As in monolingual parsing, dynamic pro-
gramming items are simply indexed by a source lan-
guage span and a syntactic label. Complexity arises
when n-gram language model scoring is added, be-
cause items must now be distinguished by their ini-
tial and final few target language words for purposes
of later combination. This lexically exploded search
space is a root cause of inefficiency in decoding, and
several methods have been suggested to combat it.
The approach most relevant to the current work is
Zhang and Gildea (2008), which begins with an ini-
tial bigram pass and uses the resulting chart to guide
a final trigram pass. Substantial speed-ups are ob-
tained, but computation is still dominated by the ini-
tial bigram pass. The key challenge is that unigram
models are too poor to prune well, but bigram mod-
els are already huge. In short, the problem is that
there are too many words in the target language. In
this paper, we propose a new, coarse-to-fine, mul-
tipass approach which allows much greater speed-
ups by translating into abstracted languages. That
is, rather than beginning with a low-order model of
a still-large language, we exploit language projec-
tions, hierarchical clusterings of the target language,
to effectively reduce the size of the target language.
In this way, initial passes can be very quick, with
complexity phased in gradually.
Central to coarse-to-fine language projection is
the construction of sequences of word clusterings
(see Figure 1). The clusterings are deterministic
mappings from words to clusters, with the property
that each clustering refines the previous one. There
are many choice points in this process, including
how these clusterings are obtained and how much
refinement is optimal for each pass. We demon-
strate that likelihood-based hierarchical EM train-
ing (Petrov et al, 2006) and cluster-based language
modeling methods (Goodman, 2001) are superior
to both rank-based and random-projection methods.
In addition, we demonstrate that more than two
passes are beneficial and show that our computa-
tion is equally distributed over all passes. In our
experiments, passes with less than 16-cluster lan-
guage models are most advantageous, and even a
single pass with just two word clusters can reduce
decoding time greatly.
108
To follow related work and to focus on the effects
of the language model, we present translation re-
sults under an inversion transduction grammar (ITG)
translation model (Wu, 1997) trained on the Eu-
roparl corpus (Koehn, 2005), described in detail in
Section 3, and using a trigram language model. We
show that, on a range of languages, our coarse-to-
fine decoding approach greatly outperforms base-
line beam pruning and bigram-to-trigram pruning on
time-to-BLEU plots, reducing decoding times by up
to a factor of 50 compared to single pass decoding.
In addition, coarse-to-fine decoding increases BLEU
scores by up to 0.4 points. This increase is a mixture
of improved search and subtly advantageous coarse-
to-fine effects which are further discussed below.
2 Coarse-to-Fine Decoding
In coarse-to-fine decoding, we create a series of ini-
tially simple but increasingly complex search prob-
lems. We then use the solutions of the simpler prob-
lems to prune the search spaces for more complex
models, reducing the total computational cost.
2.1 Related Work
Taken broadly, the coarse-to-fine approach is not
new to machine translation (MT) or even syntactic
MT. Many common decoder precomputations can
be seen as coarse-to-fine methods, including the A*-
like forward estimates used in the Moses decoder
(Koehn et al, 2007). In an ITG framework like
ours, Zhang and Gildea (2008) consider an approach
in which the results of a bigram pass are used as
an A* heuristic to guide a trigram pass. In their
two-pass approach, the coarse bigram pass becomes
computationally dominant. Our work differs in two
ways. First, we use posterior pruning rather than
A* search. Unlike A* search, posterior pruning
allows multipass methods. Not only are posterior
pruning methods simpler (for example, there is no
need to have complex multipart bounds), but they
can be much more effective. For example, in mono-
lingual parsing, posterior pruning methods (Good-
man, 1997; Charniak et al, 2006; Petrov and Klein,
2007) have led to greater speedups than their more
cautious A* analogues (Klein and Manning, 2003;
Haghighi et al, 2007), though at the cost of guaran-
teed optimality.
L
M
 
O
r
d
e
r
Bits in language model
the,report-NP-these,states
1
pi
2
3
2 3
the-NP-states0-NP-1 01-NP-10 010-NP-100
0,1-NP-0,1 01,10-NP-00,10 010,100-NP-000,100
...
...
?
Figure 2: Possible state projections pi for the target noun
phrase ?the report for these states? using the clusters
from Figure 1. The number of bits used to encode the tar-
get language vocabulary is varied along the x-axis. The
language model order is varied along the y-axis.
Second, we focus on an orthogonal axis of ab-
straction: the size of the target language. The in-
troduction of abstract languages gives better control
over the granularity of the search space and provides
a richer set of intermediate problems, allowing us
to adapt the level of refinement of the intermediate,
coarse passes to minimize total computation.
Beyond coarse-to-fine approaches, other related
approaches have also been demonstrated for syntac-
tic MT. For example, Venugopal et al (2007) con-
siders a greedy first pass with a full model followed
by a second pass which bounds search to a region
near the greedy results. Huang and Chiang (2007)
searches with the full model, but makes assumptions
about the the amount of reordering the language
model can trigger in order to limit exploration.
2.2 Language Model Projections
When decoding in a syntactic translation model with
an n-gram language model, search states are spec-
ified by a grammar nonterminal X as well as the
the n-1 left-most target side words ln?1, . . . , l1 and
right-most target side words r1, . . . , rn?1 of the gen-
erated hypothesis. We denote the resulting lexical-
ized state as ln?1, . . . , l1-X-r1, . . . , rn?1. Assum-
ing a vocabulary V and grammar symbol set G, the
state space size is up to |V |2(n?1)|G|, which is im-
mense for a large vocabulary when n > 1. We
consider two ways to reduce the size of this search
space. First, we can reduce the order of the lan-
guage model. Second, we can reduce the number
of words in the vocabulary. Both can be thought
of as projections of the search space to smaller ab-
109
these
one
we
they
the
a
that
for
states
report
of
to
also
been
will
must
0
1
00
01
000 001
010 011 100 101 110 111
10
11
Figure 1: An example of hierarchical clustering of target language vocabulary (see Section 4). Even with a small
number of clusters our divisive HMM clustering (Section 4.3) captures sensible syntactico-semantic classes.
stracted spaces. Figure 2 illustrates those two or-
thogonal axes of abstraction.
Order-based projections are simple. As shown
in Figure 2, they simply strip off the appropriate
words from each state, collapsing dynamic program-
ming items which are identical from the standpoint
of their left-to-right combination in the lower or-
der language model. However, having only order-
based projections is very limiting. Zhang and Gildea
(2008) found that their computation was dominated
by their bigram pass. The only lower-order pass
possible uses a unigram model, which provides no
information about the interaction of the language
model and translation model reorderings. We there-
fore propose encoding-based projections. These
projections reduce the size of the target language vo-
cabulary by deterministically projecting each target
language word to a word cluster. This projection ex-
tends to the whole search state in the obvious way:
assuming a bigram language model, the state l-X-r
projects to c(l)-X-c(r), where c(?) is the determin-
istic word-to-cluster mapping.
In our multipass approach, we will want a se-
quence c1 . . . cn of such projections. This requires a
hierarchical clustering of the target words, as shown
in Figure 1. Each word?s cluster membership can be
represented by an n-bit binary string. Each prefix of
length k declares that word?s cluster assignment at
the k-bit level. As we vary k, we obtain a sequence
of projections ck(?), each one mapping words to a
more refined clustering. When performing inference
in a k-bit projection, we replace the detailed original
language model over words with a coarse language
model LMk over the k-bit word clusters. In addition,
we replace the phrase table with a projected phrase
table, which further increases the speed of projected
passes. In Section 4, we describe the various clus-
tering schemes explored, as well as how the coarse
LMk are estimated.
2.3 Multipass Decoding
Unlike previous work, where the state space exists
only at two levels of abstraction (i.e. bigram and tri-
gram), we have multiple levels to choose from (Fig-
ure 2). Because we use both encoding-based and
order-based projections, our options form a lattice
of coarser state spaces, varying from extremely sim-
ple (a bigram model with just two word clusters) to
nearly the full space (a trigram model with 10 bits or
1024 word clusters).
We use this lattice to perform a series of coarse
passes with increasing complexity. More formally,
we decode a source sentence multiple times, in a
sequence of state spaces S0, S1, . . . , Sn=S, where
each Si is a refinement of Si?1 in either language
model order, language encoding size, or both. The
state spaces Si and Sj (i < j) are related to each
other via a projection operator pij?i(?) which maps
refined states deterministically to coarser states.
We start by decoding an input x in the simplest
state space S0. In particular, we compute the chart
of the posterior distributions p0(s) = P (s|x) for all
states s ? S0. These posteriors will be used to prune
the search space S1 of the following pass. States s
whose posterior falls below a threshold t trigger the
removal of all more refined states s? in the subse-
quent pass (see Figure 3). This technique is poste-
rior pruning, and is different from A* methods in
two main ways. First, it can be iterated in a multi-
pass setting, and, second, it is generally more effi-
110
0-X-0
11-X-10 10-X-11 11-X-1100-X-11 10-X-1011-X-01 01-X-1010-X-00 11-X-00 10-X-0100-X-00 01-X-00 00-X-01
1-X-0 0-X-1 1-X-1
2-Bit Pass
1-Bit Pass
 < t ?  < t ?  < t ?  < t ?  < t ?  < t ?  < t ?  < t ?
< t ?
< t ? < t ? < t ?
01-X-1100-X-1001-X-01
Figure 3: Example of state pruning in coarse-to-fine decoding using the language encoding projection (see Section 2.2).
During the coarse one-bit word cluster pass, two of the four possible states are pruned. Every extension of the pruned
one-bit states (indicated by the grey shading) are not explored during the two-bit word cluster pass.
cient with a potential cost of increased search errors
(see Section 2.1 for more discussion).
Looking at Figure 2, multipass coarse-to-fine de-
coding can be visualized as a walk from a coarse
point somewhere in the lower left to the most re-
fined point in the upper right of the grid. Many
coarse-to-fine schedules are possible. In practice,
we might start decoding with a 1-bit word bigram
pass, followed by an 3-bit word bigram pass, fol-
lowed by a 5-bit word trigram pass and so on (see
Section 5.3 for an empirical investigation). In terms
if time, we show that coarse-to-fine gives substantial
speed-ups. There is of course an additional mem-
ory requirement, but it is negligible. As we will see
in our experiments (Section 5) the largest gains can
be obtained with extremely coarse language mod-
els. In particular, the largest coarse model we use in
our best multipass decoder uses a 4-bit encoding and
hence has only 16 distinct words (or at most 4096
trigrams).
3 Inversion Transduction Grammars
While our approach applies in principle to a vari-
ety of machine translation systems (phrase-based or
syntactic), we will use the inversion transduction
grammar (ITG) approach of Wu (1997) to facili-
tate comparison with previous work (Zens and Ney,
2003; Zhang and Gildea, 2008) as well as to focus on
language model complexity. ITGs are a subclass of
synchronous context-free grammars (SCFGs) where
there are only three kinds of rules. Preterminal unary
productions produce terminal strings on both sides
(words or phrases): X ? e/f . Binary in-order pro-
ductions combine two phrases monotonically (X ?
[Y Z]). Finally, binary inverted productions invert
the order of their children (X ? ?Y Z?). These pro-
ductions are associated with rewrite weights in the
standard way.
Without a language model, SCFG decoding is just
like (monolingual) CFG parsing. The dynamic pro-
gramming states are specified by iXj , where ?i, j? is
a source sentence span and X is a nonterminal. The
only difference is that whenever we apply a CFG
production on the source side, we need to remem-
ber the corresponding synchronous production on
the target side and store the best obtainable transla-
tion via a backpointer. See Wu (1996) or Melamed
(2004) for a detailed exposition.
Once we integrate an n-gram language model, the
state space becomes lexicalized and combining dy-
namic programming items becomes more difficult.
Each state is now parametrized by the initial and
final n?1 words in the target language hypothesis:
ln?1, ..., l1-iXj-r1, ..., rn?1. Whenever we combine
two dynamic programming items, we need to score
the fluency of their concatentation by incorporat-
ing the score of any language model features which
cross the target side boundaries of the two concate-
nated items (Chiang, 2005). Decoding with an in-
tegrated language model is computationally expen-
sive for two reasons: (1) the need to keep track of
a large number of lexicalized hypotheses for each
source span, and (2) the need to frequently query the
large language model for each hypothesis combina-
tion.
Multipass coarse-to-fine decoding can alleviate
both computational issues. We start by decoding
in an extremely coarse bigram search space, where
there are very few possible translations. We com-
pute standard inside/outside probabilities (iS/oS),
as follows. Consider the application of non-inverted
binary rule: we combine two items lb-iBk-rb and
lc-kCj-rc spanning ?i, k? and ?k, j? respectively to
form a larger item lb-iAj-rc, spanning ?i, j?. The
111
lb-iAj -rc lb-iBk-rb lc-kCj-rc
rclb
+
lb rc
+=
iS(lb-iAj -rc) += iS(lb-iBk-rb) ? iS(lc-kCj-rc)LM(rb, lc) ?p(X?[Y Z]) ?
lcrb
Figure 4: Monotonic combination of two hypotheses dur-
ing the inside pass involves scoring the fluency of the con-
catenation with the language model.
inside score of the new item is incremented by:
iS(lb-iAj-rc) += p(X ? [Y Z]) ? iS(lb-iBk-rb) ?
iS(lc-kCj-rc) ? LM(rb, lc)
This process is also illustrated in Figure 4. Of
course, we also loop over the split point k and ap-
ply the other two rule types (inverted concatenation,
terminal generation). We omit those cases from this
exposition, as well as the update for the outside pass;
they are standard and similar. Once we have com-
puted the inside and outside scores, we compute pos-
terior probabilities for all items:
p(la-iAj-ra) =
iS(la-iAj-ra)oS(la-iAj-ra)
iS(root)
where iS(root) is sum of all translations? scores.
States with low posteriors are then pruned away.
We proceed to compute inside/outside score in the
next, more refined search space, using the projec-
tions pii?i?1 to map between states in Si and Si?1.
In each pass, we skip all items whose projection into
the previous stage had a probability below a stage-
specific threshold. This process is illustrated in Fig-
ure 3. When we reach the most refined search space
S?, we do not prune, but rather extract the Viterbi
derivation instead.1
4 Learning Coarse Languages
Central to our encoding-based projections (see Sec-
tion 2.2) are hierarchical clusterings of the tar-
get language vocabulary. In the present work,
these clusterings are each k-bit encodings and yield
sequences of coarse language models LMk and
phrasetables PTk.
1Other final decoding strategies are possible, of course, in-
cluding variational methods and minimum-risk methods (Zhang
and Gildea, 2008).
Given a hierarchical clustering, we estimate the
corresponding LMk from a corpus obtained by re-
placing each token in a target language corpus with
the appropriate word cluster. As with our original
refined language model, we estimate each coarse
language model using the SRILM toolkit (Stolcke,
2002). The phrasetables PTk are similarly estimated
by replacing the words on the target side of each
phrase pair with the corresponding cluster. This pro-
cedure can potentially map two distinct phrase pairs
to the same coarse translation. In such cases we keep
only one coarse phrase pair and sum the scores of the
colliding originals.
There are many possible schemes for creating hi-
erarchical clusterings. Here, we consider several di-
visive clustering methods, where coarse word clus-
ters are recursively split into smaller subclusters.
4.1 Random projections
The simplest approach to splitting a cluster is to ran-
domly assign each word type to one of two new sub-
clusters. Random projections have been shown to be
a good and computationally inexpensive dimension-
ality reduction technique, especially for high dimen-
sional data (Bingham andMannila, 2001). Although
our best performance does not come from random
projections, we still obtain substantial speed-ups
over a single pass fine decoder when using random
projections in coarse passes.
4.2 Frequency clustering
In frequency clustering, we allocate words to clus-
ters by frequency. At each level, the most frequent
words go into one cluster and the rarest words go
into another one. Concretely, we sort the words in
a given cluster by frequency and split the cluster so
that the two halves have equal token mass. This ap-
proach can be seen as a radically simplified version
of Brown et al (1992). It can, and does, result in
highly imbalanced cluster hierarchies.
4.3 HMM clustering
An approach found to be effective by Petrov and
Klein (2007) for coarse-to-fine parsing is to use
likelihood-based hierarchical EM training. We
adopt this approach here by identifying each clus-
ter with a latent state in an HMM and determiniz-
ing the emissions so that each word type is emitted
112
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 90000
 0  1  2  3  4  5  6  7  8  9  10
Per
ple
xity
Number of bits in coarse language model
HMMJClusterFrequencyRandom
Figure 5: Results of coarse language model perplexity
experiment (see Section 4.5). HMM and JClustering have
lower perplexity than frequency and random clustering
for all number of bits in the language encoding.
by only one state. When splitting a cluster s into
s1 and s2, we initially clone and mildly perturb its
corresponding state. We then use EM to learn pa-
rameters, which splits the state, and determinize the
result. Specifically, each word w is assigned to s1 if
P (w|s1) > P (w|s2) and s2 otherwise. Because of
this determinization after each round of EM, a word
in one cluster will be allocated to exactly one of that
cluster?s children. This process not only guarantees
that the clusters are hierarchical, it also avoids the
state drift discussed by Petrov and Klein (2007). Be-
cause the emissions are sparse, learning is very effi-
cient. An example of some of the words associated
with early splits can be seen in Figure 1.
4.4 JCluster
Goodman (2001) presents a clustering scheme
which aims to minimize the entropy of a word given
a cluster. This is accomplished by incrementally
swapping words between clusters to locally mini-
mize entropy.2 This clustering algorithm was devel-
oped with a slightly different application in mind,
but fits very well into our framework, because the
hierarchical clusters it produces are trained to maxi-
mize predictive likelihood.
4.5 Clustering Results
We applied the above clustering algorithms to our
monolingual language model data to obtain hierar-
2The software for this clustering technique is available at
http://research.microsoft.com/?joshuago/.
 28
 28.2
 28.4
 28.6
 28.8
 29
 29.2
 29.4
 100  1000  10000  100000
BL
EU
Total time in seconds
HMMJClusterFrequenceRandomSingle pass (no clustering)
Figure 6: Coarse-to-fine decoding with HMM or JClus-
tering coarse language models reduce decoding times
while increasing accuracy.
chical clusters. We then trained coarse language
models of varying granularity and evaluated them on
a held-out set. To measure the quality of the coarse
language models we use perplexity (exponentiated
cross-entropy).3 Figure 5 shows that HMM clus-
tering and JClustering have lower perplexity than
frequency and random based clustering for all com-
plexities. In the next section we will present a set of
machine translation experiments using these coarse
language models; the clusterings with better per-
plexities generally produce better decoders.
5 Experiments
We ran our experiments on the Europarl corpus
(Koehn, 2005) and show results on Spanish, French
and German to English translation. We used the
setup and preprocessing steps detailed in the 2008
Workshop on Statistical Machine Translation.4 Our
baseline decoder uses an ITG with an integrated tri-
gram language model. Phrase translation parame-
ters are learned from parallel corpora with approx-
imately 8.5 million words for each of the language
pairs. The English language model is trained on the
entire corpus of English parliamentary proceedings
provided with the Europarl distribution. We report
results on the 2000 development test set sentences
of length up to 126 words (average length was 30
words).
3We assumed that each cluster had a uniform distribution
over all the words in that cluster.
4See http://www.statmt.org/wmt08 for details.
113
 0
 50
 100
 150
 200
 250
 300
1-2-3-f1-3-f2-3-f1-f2-f3-f4-ff
Tot
al t
ime
 in 
min
ute
s
Language model bits for coarse passes
fine4 bits3 bits2 bits1 bit
Figure 7: Many passes with extremely simple language
models produce the highest speed-ups.
Our ITG translation model is broadly competitive
with state-of-the-art phrase-based-models trained on
the same data. For example, on the Europarl devel-
opment test set, we fall short of Moses (Koehn et al,
2007) by less than one BLEU point. On Spanish-
English we get 29.47 BLEU (compared to Moses?s
30.40), on French-English 29.34 (vs. 29.95), and
23.80 (vs. 24.64) on German-English. These differ-
ences can be attributed primarily to the substantially
richer distortion model used by Moses.
The multipass coarse-to-fine architecture that we
have introduced presents many choice points. In
the following, we investigate various axes individu-
ally. We present our findings as BLEU-to-time plots,
where the tradeoffs were generated by varying the
complexity and the number of coarse passes, as well
as the pruning thresholds and beam sizes. Unless
otherwise noted, the experiments are on Spanish-
English using trigram language models. When
different decoder settings are applied to the same
model, MERT weights (Och, 2003) from the unpro-
jected single pass setup are used and are kept con-
stant across runs. In particular, the same MERT
weights are used for all coarse passes; note that this
slightly disadvantages the multipass runs, which use
MERT weights optimized for the single pass de-
coder.
5.1 Clustering
In section Section 4, HMM clustering and JCluster-
ing gave lower perplexities than frequency and ran-
dom clustering when using the same number of bits
for encoding the language model. To test how these
 28
 28.2
 28.4
 28.6
 28.8
 29
 29.2
 29.4
 29.6
 100  1000  10000  100000
BL
EU
Total time in seconds
Encoding+OrderOrderEncodingSingle pass
Figure 8: A combination of order-based and encoding-
based coarse-to-fine decoding yields the best results.
models perform at pruning, we ran our decoder sev-
eral times, varying only the clustering source. In
each case, we used a 2-bit trigram model as a sin-
gle coarse pass, followed by a fine output pass. Fig-
ure 6 shows that we can obtain significant improve-
ments over the single-pass baseline regardless of the
clustering. To no great surprise, HMM clustering
and JClustering yield better results, giving a 30-fold
speed-up at the same accuracy, or improvements of
about 0.3 BLEU when given the same time as the
single pass decoder. We discuss this increase in ac-
curacy over the baseline in Section 5.5. Since the
performance differences between those two cluster-
ing algorithms are negligible, we will use the sim-
pler HMM clustering in all subsequent experiments.
5.2 Spacing
Given a hierarchy of coarse language models, all
trigam for the moment, we need to decide on the
number of passes and the granularity of the coarse
language models used in each pass. Figure 7 shows
how decoding time varies for different multipass
schemes to achieve the same translation quality.
A single coarse pass with a 4-bit language model
cuts decoding time almost in half. However, one
can further cut decoding time by starting with even
coarser language models. In fact, the best results
are achieved by decoding in sequence with 1-, 2-
and 3-bit language models before running the final
fine trigram pass. Interestingly, in this setting, each
pass takes about the same amount of time. A simi-
lar observation was reported in the parsing literature,
where coarse-to-fine inference with multiple passes
114
 28 28.2
 28.4 28.6
 28.8 29
 29.2 29.4
 29.6
 100  1000  10000
BLE
U
Total time in seconds
Spanish
Coarse-To-FineFine Baseline  28
 28.2
 28.4
 28.6
 28.8
 29
 29.2
 29.4
 100  1000  10000
BLE
U
Total time in seconds
French
Coarse-To-FineFine Baseline  22
 22.5
 23
 23.5
 24
 100  1000  10000
BLE
U
Total time in seconds
German
Coarse-To-FineFine Baseline
Figure 9: Coarse-to-fine decoding is faster than single pass decoding with a trigram language model and leads to better
BLEU scores on all language pairs and for all parameter settings.
of roughly equal complexity produces tremendous
speed-ups (Petrov and Klein, 2007).
5.3 Encoding vs. Order
As described in Section 2, the language model com-
plexity can be reduced either by decreasing the vo-
cabulary size (encoding-based projection) or by low-
ering the language model order from trigram to bi-
gram (order-based projection). Figure 7 shows that
both approaches alone yield comparable improve-
ments over the single pass baseline. Fortunately,
the two approaches are complimentary, allowing us
to obtain further improvements by combining both.
We found it best to first do a series of coarse bigram
passes, followed by a fine bigram pass, followed by
a fine trigram pass.
5.4 Final Results
Figure 9 compares our multipass coarse-to-fine de-
coder using language refinement to single pass de-
coding on three different languages. On each lan-
guage we get significant improvements in terms of
efficiency as well as accuracy. Overall, we can
achieve up to 50-fold speed-ups at the same accu-
racy, or alternatively, improvements of 0.4 BLEU
points over the best single pass run.
In absolute terms, our decoder translates on aver-
age about two Spanish sentences per second at the
highest accuracy setting.5 This compares favorably
to the Moses decoder (Koehn et al, 2007), which
takes almost three seconds per sentence.
5Of course, the time for an average sentence is much lower,
since long sentences dominate the overall translation time.
5.5 Search Error Analysis
In multipass coarse-to-fine decoding, we noticed
that in addition to computational savings, BLEU
scores tend to improve. A first hypothesis is
that coarse-to-fine decoding simply improves search
quality, where fewer good items fall off the beam
compared to a simple fine pass. However, this hy-
pothesis turns out to be incorrect. Table 1 shows
the percentage of test sentences for which the BLEU
score or log-likelihood changes when we switch
from single pass decoding to coarse-to-fine multi-
pass decoding. Only about 30% of the sentences
get translated in the same way (if much faster) with
coarse-to-fine decoding. For the rest, coarse-to-fine
decoding mostly finds translations with lower likeli-
hood, but higher BLEU score, than single pass de-
coding.6 An increase of the underlying objectives of
interest when pruning despite an increase in model-
score search errors has also been observed in mono-
lingual coarse-to-fine syntactic parsing (Charniak et
al., 1998; Petrov and Klein, 2007). This effect may
be because coarse-to-fine approximates certain min-
imum Bayes risk objective. It may also be an effect
of model intersection between the various passes?
models. In any case, both possibilities are often per-
fectly desirable. It is also worth noting that the num-
ber of search errors incurred in the coarse-to-fine
approach can be dramatically reduced (at the cost
of decoding time) by increasing the pruning thresh-
olds. However, the fortuitous nature of coarse-to-
fine search errors seems to be a substantial and de-
sirable effect.
6We compared the influence of multipass decoding on the
TM score and the LM score; both decrease.
115
LL
> = <
B
L
E
U > 3.6% - 26.3%
= 1.5% 29.6 % 12.9 %
< 2.2% - 24.1%
Table 1: Percentage of sentences for which the BLEU
score/log-likelihood improves/drops during coarse-to-
fine decoding (compared to single pass decoding).
6 Conclusions
We have presented a coarse-to-fine syntactic de-
coder which utilizes a novel encoding-based lan-
guage projection in conjunction with order-based
projections to achieve substantial speed-ups. Un-
like A* methods, a posterior pruning approach al-
lows multiple passes, which we found to be very
beneficial for total decoding time. When aggres-
sively pruned, coarse-to-fine decoding can incur ad-
ditional search errors, but we found those errors to
be fortuitous more often than harmful. Our frame-
work applies equally well to other translation sys-
tems, though of course interesting new challenges
arise when, for example, the underlying SCFGs be-
come more complex.
References
E. Bingham and H.i Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In KDD ?01.
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-
cer. 1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-
based best-first chart parsing. 6th Workshop on Very
Large Corpora.
E. Charniak, M. Johnson, D. McClosky, et al 2006.
Multi-level coarse-to-fine PCFG Parsing. In HLT-
NAACL ?06.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In ACL ?05.
J. Goodman. 1997. Global thresholding and multiple-
pass parsing. In EMNLP ?97.
J. Goodman. 2001. A bit of progress in language model-
ing. Technical report, Microsoft Research.
A. Haghighi, J. DeNero, and D. Klein. 2007. A* search
via approximate factoring. In NAACL ?07.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In ACL
?07.
D. Klein and C. Manning. 2003. A* parsing: fast exact
viterbi parse selection. In NAACL ?03.
P. Koehn, H. Hoang, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL ?07.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit.
I. D. Melamed. 2004. Statistical machine translation by
parsing. In ACL ?04.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL ?07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In ICSLP ?02.
A. Venugopal, A. Zollmann, and S. Vogel. 2007. An ef-
ficient two-pass approach to synchronous-CFG driven
statistical MT. In HLT-NAACL ?07.
D. Wu. 1996. A polynomial-time algorithm for statisti-
cal machine translation. In ACL ?96.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. In
Computational Linguistics.
R. Zens and H. Ney. 2003. A comparative study on re-
ordering constraints in statistical machine translation.
In ACL ?03.
H. Zhang and D. Gildea. 2008. Efficient multi-pass
decoding for synchronous context free grammars. In
ACL ?08.
116
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867?876,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Sparse Multi-Scale Grammars
for Discriminative Latent Variable Parsing
Slav Petrov and Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov, klein}@eecs.berkeley.edu
Abstract
We present a discriminative, latent variable
approach to syntactic parsing in which rules
exist at multiple scales of refinement. The
model is formally a latent variable CRF gram-
mar over trees, learned by iteratively splitting
grammar productions (not categories). Dif-
ferent regions of the grammar are refined to
different degrees, yielding grammars which
are three orders of magnitude smaller than
the single-scale baseline and 20 times smaller
than the split-and-merge grammars of Petrov
et al (2006). In addition, our discriminative
approach integrally admits features beyond lo-
cal tree configurations. We present a multi-
scale training method along with an efficient
CKY-style dynamic program. On a variety of
domains and languages, this method produces
the best published parsing accuracies with the
smallest reported grammars.
1 Introduction
In latent variable approaches to parsing (Matsuzaki
et al, 2005; Petrov et al, 2006), one models an ob-
served treebank of coarse parse trees using a gram-
mar over more refined, but unobserved, derivation
trees. The parse trees represent the desired output
of the system, while the derivation trees represent
the typically much more complex underlying syntac-
tic processes. In recent years, latent variable meth-
ods have been shown to produce grammars which
are as good as, or even better than, earlier parsing
work (Collins, 1999; Charniak, 2000). In particular,
in Petrov et al (2006) we exhibited a very accurate
category-splitting approach, in which a coarse ini-
tial grammar is refined by iteratively splitting each
grammar category into two subcategories using the
EM algorithm. Of course, each time the number of
grammar categories is doubled, the number of bi-
nary productions is increased by a factor of eight.
As a result, while our final grammars used few cat-
egories, the number of total active (non-zero) pro-
ductions was still substantial (see Section 7). In ad-
dition, it is reasonable to assume that some genera-
tively learned splits have little discriminative utility.
In this paper, we present a discriminative approach
which addresses both of these limitations.
We introduce multi-scale grammars, in which
some productions reference fine categories, while
others reference coarse categories (see Figure 2).
We use the general framework of hidden variable
CRFs (Lafferty et al, 2001; Koo and Collins, 2005),
where gradient-based optimization maximizes the
likelihood of the observed variables, here parse
trees, summing over log-linearly scored derivations.
With multi-scale grammars, it is natural to refine
productions rather than categories. As a result, a
category such as NP can be complex in some re-
gions of the grammar while remaining simpler in
other regions. Additionally, we exploit the flexibility
of the discriminative framework both to improve the
treatment of unknown words as well as to include
span features (Taskar et al, 2004), giving the bene-
fit of some input features integrally in our dynamic
program. Our multi-scale grammars are 3 orders
of magnitude smaller than the fully-split baseline
grammar and 20 times smaller than the generative
split-and-merge grammars of Petrov et al (2006).
867
In addition, we exhibit the best parsing numbers on
several metrics, for several domains and languages.
Discriminative parsing has been investigated be-
fore, such as in Johnson (2001), Clark and Curran
(2004), Henderson (2004), Koo and Collins (2005),
Turian et al (2007), Finkel et al (2008), and, most
similarly, in Petrov and Klein (2008). However, in
all of these cases, the final parsing performance fell
short of the best generative models by several per-
centage points or only short sentences were used.
Only in combination with a generative model was
a discriminative component able to produce high
parsing accuracies (Charniak and Johnson, 2005;
Huang, 2008). Multi-scale grammars, in contrast,
give higher accuracies using smaller grammars than
previous work in this direction, outperforming top
generative models in grammar size and in parsing
accuracy.
2 Latent Variable Parsing
Treebanks are typically not annotated with fully de-
tailed syntactic structure. Rather, they present only
a coarse trace of the true underlying processes. As
a result, learning a grammar for parsing requires
the estimation of a more highly articulated model
than the naive CFG embodied by such treebanks.
A manual approach might take the category NP and
subdivide it into one subcategory NP?S for subjects
and another subcategory NP?VP for objects (John-
son, 1998; Klein and Manning, 2003). However,
rather than devising linguistically motivated features
or splits, latent variable parsing takes a fully auto-
mated approach, in which each symbol is split into
unconstrained subcategories.
2.1 Latent Variable Grammars
Latent variable grammars augment the treebank
trees with latent variables at each node. This cre-
ates a set of (exponentially many) derivations over
split categories for each of the original parse trees
over unsplit categories. For each observed category
A we now have a set of latent subcategories Ax. For
example, NP might be split into NP1 through NP8.
The parameters of the refined productions
Ax ? By Cz, where Ax is a subcategory of A, By
of B, and Cz of C , can then be estimated in var-
ious ways; past work has included both generative
(Matsuzaki et al, 2005; Liang et al, 2007) and dis-
criminative approaches (Petrov and Klein, 2008).
We take the discriminative log-linear approach here.
Note that the comparison is only between estimation
methods, as Smith and Johnson (2007) show that the
model classes are the same.
2.2 Log-Linear Latent Variable Grammars
In a log-linear latent variable grammar, each pro-
duction r = Ax ? By Cz is associated with a
multiplicative weight ?r (Johnson, 2001; Petrov and
Klein, 2008) (sometimes we will use the log-weight
?r when convenient). The probability of a derivation
t of a sentence w is proportional to the product of the
weights of its productions r:
P (t|w) ?
?
r?t
?r
The score of a parse T is then the sum of the scores
of its derivations:
P (T |w) =
?
t?T
P (t|w)
3 Hierarchical Refinement
Grammar refinement becomes challenging when the
number of subcategories is large. If each category
is split into k subcategories, each (binary) produc-
tion will be split into k3. The resulting memory lim-
itations alone can prevent the practical learning of
highly split grammars (Matsuzaki et al, 2005). This
issue was partially addressed in Petrov et al (2006),
where categories were repeatedly split and some
splits were re-merged if the gains were too small.
However, while the grammars are indeed compact
at the (sub-)category level, they are still dense at the
production level, which we address here.
As in Petrov et al (2006), we arrange our subcat-
egories into a hierarchy, as shown in Figure 1. In
practice, the construction of the hierarchy is tightly
coupled to a split-based learning process (see Sec-
tion 5). We use the naming convention that an origi-
nal category A becomes A0 and A1 in the first round;
A0 then becoming A00 and A01 in the second round,
and so on. We will use x? ? x to indicate that the
subscript or subcategory x is a refinement of x?.1 We
1Conversely, x? is a coarser version of x, or, in the language
of Petrov and Klein (2007), x? is a projection of x.
868
+ 7 . 3
+ 5 . 0
+ 7 . 3
+ 1 2
+ 2 . 1
S i n g l e  s c a l e p r o d u c t i o n s
+ 5 . 0
+ 5 . 0
+ 7 . 3
+ 2 . 1
+ 2 . 1
+ 2 . 1
+ 2 . 1
M u l t i  s c a l e p r o d u c t i o n s
+ 2 . 1
+ 1 2
+ 5 . 0
?
0 0
1
0 1 0 0 1 1
+ 5 . 0 + 5 . 0 + 7 . 3 + 1 2
?
0 0 0 1 0 0 1 0 1 1 1 0 1 1 10 0 1 0 1 0 0 1 1
+ 2 . 1 + 2 . 1+ 2 . 1+ 2 . 1
?r? ?r?
0 0
0
*
1
1 00 1 1 1
0
*
0 1
D T 0 0 0 ? t h e
D T 0 0 1 ? t h e
D T 0 1 0 ? t h e
D T 0 1 1 ? t h e
D T 1 0 0 ? t h e
D T 1 0 1 ? t h e
D T 1 1 0 ? t h e
D T 1 1 1 ? t h e
D T 0 0 ? t h e
D T 0 1 0 ? t h e
D T 0 1 1 ? t h e
D T 1 ? t h e}
}
+ 1 2
Figure 1: Multi-scale refinement of the DT ? the production. The multi-scale grammar can be encoded much more
compactly than the equally expressive single scale grammar by using only the shaded features along the fringe.
will also say that x? dominates x, and x will refer to
fully refined subcategories. The same terminology
can be applied to (binary) productions, which split
into eight refinements each time the subcategories
are split in two.
The core observation leading to multi-scale gram-
mars is that when we look at the refinements of a
production, many are very similar in weight. It is
therefore advantageous to record productions only at
the level where they are distinct from their children
in the hierarchy.
4 Multi-Scale Grammars
A multi-scale grammar is a grammar in which some
productions reference fine categories, while others
reference coarse categories. As an example, con-
sider the multi-scale grammar in Figure 2, where the
NP category has been split into two subcategories
(NP0, NP1) to capture subject and object distinc-
tions. Since it can occur in subject and object po-
sition, the production NP ? it has remained unsplit.
In contrast, in a single-scale grammar, two produc-
tions NP0 ? it and NP1 ? it would have been nec-
essary. We use * as a wildcard, indicating that NP?
can combine with any other NP, while NP1 can only
combine with other NP1. Whenever subcategories
of different granularity are combined, the resulting
constituent takes the more specific label.
In terms of its structure, a multi-scale grammar is
a set of productions over varyingly refined symbols,
where each production is associated with a weight.
Consider the refinement of the production shown in
Figure 1. The original unsplit production (at top)
would naively be split into a tree of many subpro-
ductions (downward in the diagram) as the grammar
categories are incrementally split. However, it may
be that many of the fully refined productions share
the same weights. This will be especially common
in the present work, where we go out of our way to
achieve it (see Section 5). For example, in Figure 1,
the productions DTx ? the have the same weight
for all categories DTx which refine DT1.2 A multi-
scale grammar can capture this behavior with just 4
productions, while the single-scale grammar has 8
productions. For binary productions the savings will
of course be much higher.
In terms of its semantics, a multi-scale grammar is
simply a compact encoding of a fully refined latent
variable grammar, in which identically weighted re-
finements of productions have been collapsed to the
coarsest possible scale. Therefore, rather than at-
tempting to control the degree to which categories
are split, multi-scale grammars simply encode pro-
ductions at varying scales. It is hence natural to
speak of refining productions, while considering
the categories to exist at all degrees of refinement.
Multi-scale grammars enable the use of coarse (even
unsplit) categories in some regions of the grammar,
while requiring very specific subcategories in others,
as needed. As we will see in the following, this flex-
ibility results in a tremendous reduction of grammar
parameters, as well as improved parsing time, be-
cause the vast majority of productions end up only
partially split.
Since a multi-scale grammar has productions
which can refer to different levels of the category
hierarchy, there must be constraints on their coher-
ence. Specifically, for each fully refined produc-
tion, exactly one of its dominating coarse produc-
tions must be in the grammar. More formally, the
multi-scale grammar partitions the space of fully re-
fined base rules such that each r maps to a unique
2We define dominating productions and refining productions
analogously as for subcategories.
869
i t
s a w
V P 0
N P 1
V *
S *
N P 0
V 0 N P *
h e r
s a w
V P 0
N P 1V *
S *
N P 0
V 0 N P 1
V P 0
N P 1V *
S *
V P *N P 0
h e r
N P 1
s
h e
N P 0
i t
N P *
s a w
V 0
L
e x i c o n :
G r
a m m a
r :
V P * V P *
s
h e
N P 0
i t
N P *
Figure 2: In multi-scale grammars, the categories exist
at varying degrees of refinement. The grammar in this
example enforces the correct usage of she and her, while
allowing the use of it in both subject and object position.
dominating rule r?, and for all base rules r? such that
r? ? r?, r? maps to r? as well. This constraint is al-
ways satisfied if the multi-scale grammar consists of
fringes of the production refinement hierarchies, in-
dicated by the shading in Figure 1.
A multi-scale grammar straightforwardly assigns
scores to derivations in the corresponding fully re-
fined single scale grammar: simply map each refined
derivation rule to its dominating abstraction in the
multi-scale grammar and give it the corresponding
weight. The fully refined grammar is therefore triv-
ially (though not compactly) reconstructable from
its multi-scale encoding.
It is possible to directly define a derivational se-
mantics for multi-scale grammars which does not
appeal to the underlying single scale grammar.
However, in the present work, we use our multi-
scale grammars only to compute expectations of the
underlying grammars in an efficient, implicit way.
5 Learning Sparse Multi-Scale Grammars
We now consider how to discriminatively learn
multi-scale grammars by iterative splitting produc-
tions. There are two main concerns. First, be-
cause multi-scale grammars are most effective when
many productions share the same weight, sparsity
is very desirable. In the present work, we exploit
L1-regularization, though other techniques such as
structural zeros (Mohri and Roark, 2006) could
also potentially be used. Second, training requires
repeated parsing, so we use coarse-to-fine chart
caching to greatly accelerate each iteration.
5.1 Hierarchical Training
We learn discriminative multi-scale grammars in an
iterative fashion (see Figure 1). As in Petrov et al
(2006), we start with a simple X-bar grammar from
an input treebank. The parameters ? of the grammar
(production log-weights for now) are estimated in a
log-linear framework by maximizing the penalized
log conditional likelihood Lcond ?R(?), where:
Lcond(?) = log
?
i
P(Ti|wi)
R(?) =
?
r
|?r|
We directly optimize this non-convex objective
function using a numerical gradient based method
(LBFGS (Nocedal and Wright, 1999) in our imple-
mentation). To handle the non-diferentiability of the
L1-regularization term R(?) we use the orthant-wise
method of Andrew and Gao (2007). Fitting the log-
linear model involves the following derivatives:
?Lcond(?)
??r
=
?
i
(
E? [fr(t)|Ti] ? E?[fr(t)|wi]
)
where the first term is the expected count fr of a pro-
duction r in derivations corresponding to the correct
parse tree Ti and the second term is the expected
count of the production in all derivations of the sen-
tence wi. Note that r may be of any scale. As we
will show below, these expectations can be com-
puted exactly using marginals from the chart of the
inside/outside algorithm (Lari and Young, 1990).
Once the base grammar has been estimated, all
categories are split in two, meaning that all binary
productions are split in eight. When splitting an al-
ready refined grammar, we only split productions
whose log-weight in the previous grammar deviates
from zero.3 This creates a refinement hierarchy over
productions. Each newly split production r is given
a unique feature, as well as inheriting the features of
its parent productions r? ? r:
?r = exp
(
?
r??r
?r?
)
The parent productions r? are then removed from the
grammar and the new features are fit as described
3L1-regularization drives more than 95% of the feature
weights to zero in each round.
870
V P
N P
S
N P
D T N N V B D D T N N
V P
i k j
S 0 ? N P 1 V P 0 1
I(S0, i, j) I(S11, i, j)
Figure 3: A multi-scale chart can be used to efficiently
compute inside/outside scores using productions of vary-
ing specificity.
above. We detect that we have split a production too
far when all child production features are driven to
zero under L1 regularization. In such cases, the chil-
dren are collapsed to their parent production, which
forms an entry in the multi-scale grammar.
5.2 Efficient Multi-Scale Inference
In order to compute the expected counts needed for
training, we need to parse the training set, score
all derivations and compute posteriors for all sub-
categories in the refinement hierarchy. The in-
side/outside algorithm (Lari and Young, 1990) is an
efficient dynamic program for summing over deriva-
tions under a context-free grammar. It is fairly
straightforward to adapt this algorithm to multi-
scale grammars, allowing us to sum over an expo-
nential number of derivations without explicitly re-
constructing the underlying fully split grammar.
For single-scale latent variable grammars, the in-
side score I(Ax, i, j) of a fully refined category Ax
spanning ?i, j? is computed by summing over all
possible productions r = Ax ? By Cz with weight
?r, spanning ?i, k? and ?k, j? respectively:4
I(Ax, i, j) =
?
r
?r
?
k
I(By, i, k)I(Cz , k, j)
Note that this involves summing over all relevant
fully refined grammar productions.
The key quantities we will need are marginals of
the form I(Ax, i, j), the sum of the scores of all fully
refined derivations rooted at any Ax dominated by
Ax and spanning ?i, j?. We define these marginals
4These scores lack any probabilistic interpretation, but can
be normalized to compute the necessary expectations for train-
ing (Petrov and Klein, 2008).
in terms of the standard inside scores of the most
refined subcategories Ax:
I(Ax, i, j) =
?
x?x
I(Ax, i, j)
When working with multi-scale grammars, we
expand the standard three-dimensional chart over
spans and grammar categories to store the scores of
all subcategories of the refinement hierarchy, as il-
lustrated in Figure 3. This allows us to compute the
scores more efficiently by summing only over rules
r? = Ax? ? By? Cz? ? r:
I(Ax, i, j) =
?
r?
?
r?r?
?r
?
k
I(By, i, k)I(Cz , k, j)
=
?
r?
?r?
?
r?r?
?
k
I(By, i, k)I(Cz , k, j)
=
?
r?
?r?
?
y?y?
?
z?z?
?
k
I(By, i, k)I(Cz , k, j)
=
?
r?
?r?
?
k
?
y?y?
I(By, i, k)
?
z?z?
I(Cz, k, j)
=
?
r?
?r?
?
k
I(By?, i, k)I(Cz? , k, j)
Of course, some of the same quantities are computed
repeatedly in the above equation and can be cached
in order to obtain further efficiency gains. Due to
space constraints we omit these details, and also the
computation of the outside score, as well as the han-
dling of unary productions.
5.3 Feature Count Approximations
Estimating discriminative grammars is challenging,
as it requires repeatedly taking expectations over all
parses of all sentences in the training set. To make
this computation practical on large data sets, we
use the same approach as Petrov and Klein (2008).
Therein, the idea of coarse-to-fine parsing (Charniak
et al, 1998) is extended to handle the repeated pars-
ing of the same sentences. Rather than computing
the entire coarse-to-fine history in every round of
training, the pruning history is cached between train-
ing iterations, effectively avoiding the repeated cal-
culation of similar quantities and allowing the effi-
cient approximation of feature count expectations.
871
6 Additional Features
The discriminative framework gives us a convenient
way of incorporating additional, overlapping fea-
tures. We investigate two types of features: un-
known word features (for predicting the part-of-
speech tags of unknown or rare words) and span fea-
tures (for determining constituent boundaries based
on individual words and the overall sentence shape).
6.1 Unknown Word Features
Building a parser that can process arbitrary sen-
tences requires the handling of previously unseen
words. Typically, a classification of rare words into
word classes is used (Collins, 1999). In such an ap-
proach, the word classes need to be manually de-
fined a priori, for example based on discriminating
word shape features (suffixes, prefixes, digits, etc.).
While this component of the parsing system is
rarely talked about, its importance should not be un-
derestimated: when using only one unknown word
class, final parsing performance drops several per-
centage points. Some unknown word features are
universal (e.g. digits, dashes), but most of them
will be highly language dependent (prefixes, suf-
fixes), making additional human expertise necessary
for training a parser on a new language. It is there-
fore beneficial to automatically learn what the dis-
criminating word shape features for a language are.
The discriminative framework allows us to do that
with ease. In our experiments we extract prefixes
and suffixes of length ? 3 and add those features to
words that occur 25 times or less in the training set.
These unknown word features make the latent vari-
able grammar learning process more language inde-
pendent than in previous work.
6.2 Span Features
There are many features beyond local tree config-
urations which can enhance parsing discrimination;
Charniak and Johnson (2005) presents a varied list.
In reranking, one can incorporate any such features,
of course, but even in our dynamic programming ap-
proach it is possible to include features that decom-
pose along the dynamic program structure, as shown
by Taskar et al (2004). We use non-local span fea-
tures, which condition on properties of input spans
(Taskar et al, 2004). We illustrate our span features
with the following example and the span ?1, 4?:
0 ? 1 [ Yes 2 ? 3 , ] 4 he 5 said 6 . 7
We first added the following lexical features:
? the first (Yes), last (comma), preceding (?) and
following (he) words,
? the word pairs at the left edge ??,Yes?, right
edge ?comma,he?, inside border ?Yes,comma?
and outside border ??,he?.
Lexical features were added for each span of length
three or more. We used two groups of span features,
one for natural constituents and one for synthetic
ones.5 We found this approach to work slightly
better than anchoring the span features to particular
constituent labels or having only one group.
We also added shape features, projecting the
sentence to abstract shapes to capture global sen-
tence structures. Punctuation shape replaces ev-
ery non-punctuation word with x and then further
collapses strings of x to x+. Our example be-
comes #??x??,x+.#, and the punctuation feature
for our span is ??[x??,]x. Capitalization shape
projects the example sentence to #.X..xx.#, and
.[X..]x for our span. Span features are a rich
source of information and our experiments should
be seen merely as an initial investigation of their ef-
fect in our system.
7 Experiments
We ran experiments on a variety of languages and
corpora using the standard training and test splits,
as described in Table 1. In each case, we start
with a completely unannotated X-bar grammar, ob-
tained from the raw treebank by a simple right-
branching binarization scheme. We then train multi-
scale grammars of increasing latent complexity as
described in Section 5, directly incorporating the
additional features from Section 6 into the training
procedure. Hierarchical training starting from a raw
treebank grammar and proceeding to our most re-
fined grammars took three days in a parallel im-
plementation using 8 CPUs. At testing time we
marginalize out the hidden structure and extract the
tree with the highest number of expected correct pro-
ductions, as in Petrov and Klein (2007).
5Synthetic constituents are nodes that are introduced during
binarization.
872
Training Set Dev. Set Test Set
ENGLISH-WSJ Sections Section 22 Section 23(Marcus et al, 1993) 2-21
ENGLISH-BROWN see 10% of 10% of the
(Francis et al 2002) ENGLISH-WSJ the data6 the data6
FRENCH7 Sentences Sentences Sentences
(Abeille et al, 2000) 1-18,609 18,610-19,609 19,609-20,610
GERMAN Sentences Sentences Sentences
(Skut et al, 1997) 1-18,602 18,603-19,602 19,603-20,602
Table 1: Corpora and standard experimental setups.
We compare to a baseline of discriminatively
trained latent variable grammars (Petrov and Klein,
2008). We also compare our discriminative multi-
scale grammars to their generative split-and-merge
cousins, which have been shown to produce the
state-of-the-art figures in terms of accuracy and effi-
ciency on many corpora. For those comparisons we
use the grammars from Petrov and Klein (2007).
7.1 Sparsity
One of the main motivations behind multi-scale
grammars was to create compact grammars. Fig-
ure 4 shows parsing accuracies vs. grammar sizes.
Focusing on the grammar size for now, we see that
multi-scale grammars are extremely compact - even
our most refined grammars have less than 50,000 ac-
tive productions. This is 20 times smaller than the
generative split-and-merge grammars, which use ex-
plicit category merging. The graph also shows that
this compactness is due to controlling production
sparsity, as the single-scale discriminative grammars
are two orders of magnitude larger.
7.2 Accuracy
Figure 4 shows development set results for En-
glish. In terms of parsing accuracy, multi-scale
grammars significantly outperform discriminatively
trained single-scale latent variable grammars and
perform on par with the generative split-and-merge
grammars. The graph also shows that the unknown
word and span features each add about 0.5% in final
parsing accuracy. Note that the span features im-
prove the performance of the unsplit baseline gram-
mar by 8%, but not surprisingly their contribution
6See Gildea (2001) for the exact setup.
7This setup contains only sentences without annotation er-
rors, as in (Arun and Keller, 2005).
90
85
80
75
100000010000010000
Pa
rs
in
g 
ac
cu
ra
cy
 (F
1)
Number of grammar productions
Discriminative Multi-Scale Grammars
+ Lexical Features
+ Span Features
Generative Split-Merge Grammars
Flat Discriminative Grammars
Figure 4: Discriminative multi-scale grammars give sim-
ilar parsing accuracies as generative split-merge gram-
mars, while using an order of magnitude fewer rules.
gets smaller when the grammars get more refined.
Section 8 contains an analysis of some of the learned
features, as well as a comparison between discrimi-
natively and generatively trained grammars.
7.3 Efficiency
Petrov and Klein (2007) demonstrates how the idea
of coarse-to-fine parsing (Charniak et al, 1998;
Charniak et al, 2006) can be used in the context of
latent variable models. In coarse-to-fine parsing the
sentence is rapidly pre-parsed with increasingly re-
fined grammars, pruning away unlikely chart items
in each pass. In their work the grammar is pro-
jected onto coarser versions, which are then used
for pruning. Multi-scale grammars, in contrast, do
not require projections. The refinement hierarchy is
built in and can be used directly for coarse-to-fine
pruning. Each production in the grammar is associ-
ated with a set of hierarchical features. To obtain a
coarser version of a multi-scale grammar, one there-
fore simply limits which features in the refinement
hierarchy can be accessed. In our experiments, we
start by parsing with our coarsest grammar and al-
low an additional level of refinement at each stage of
the pre-parsing. Compared to the generative parser
of Petrov and Klein (2007), parsing with multi-scale
grammars requires the evaluation of 29% fewer pro-
ductions, decreasing the average parsing time per
sentence by 36% to 0.36 sec/sentence.
873
? 40 words all
Parser F1 EX F1 EX
ENGLISH-WSJ
Petrov and Klein (2008) 88.8 35.7 88.3 33.1
Charniak et al (2005) 90.3 39.6 89.7 37.2
Petrov and Klein (2007) 90.6 39.1 90.1 37.1
This work w/o span features 89.7 39.6 89.2 37.2
This work w/ span features 90.0 40.1 89.4 37.7
ENGLISH-WSJ (reranked)
Huang (2008) 92.3 46.2 91.7 43.5
ENGLISH-BROWN
Charniak et al (2005) 84.5 34.8 82.9 31.7
Petrov and Klein (2007) 84.9 34.5 83.7 31.2
This work w/o span features 85.3 35.6 84.3 32.1
This work w/ span features 85.6 35.8 84.5 32.3
ENGLISH-BROWN (reranked)
Charniak et al (2005) 86.8 39.9 85.2 37.8
FRENCH
Arun and Keller (2005) 79.2 21.2 75.6 16.4
This Paper 80.1 24.2 77.2 19.2
GERMAN
Petrov and Klein (2007) 80.8 40.8 80.1 39.1
This Paper 81.5 45.2 80.7 43.9
Table 2: Our final test set parsing accuracies compared to
the best previous work on English, French and German.
7.4 Final Results
For each corpus we selected the grammar that gave
the best performance on the development set to parse
the final test set. Table 2 summarizes our final test
set performance, showing that multi-scale grammars
achieve state-of-the-art performance on most tasks.
On WSJ-English, the discriminative grammars per-
form on par with the generative grammars of Petrov
et al (2006), falling slightly short in terms of F1, but
having a higher exact match score. When trained
on WSJ-English but tested on the Brown corpus,
the discriminative grammars clearly outperform the
generative grammars, suggesting that the highly reg-
ularized and extremely compact multi-scale gram-
mars are less prone to overfitting. All those meth-
ods fall short of reranking parsers like Charniak and
Johnson (2005) and Huang (2008), which, however,
have access to many additional features, that cannot
be used in our dynamic program.
When trained on the French and German tree-
banks, our multi-scale grammars achieve the best
figures we are aware of, without any language spe-
cific modifications. This confirms that latent vari-
able models are well suited for capturing the syn-
tactic properties of a range of languages, and also
shows that discriminative grammars are still effec-
tive when trained on smaller corpora.
8 Analysis
It can be illuminating to see the subcategories that
are being learned by our discriminative multi-scale
grammars and to compare them to generatively es-
timated latent variable grammars. Compared to the
generative case, the lexical categories in the discrim-
inative grammars are substantially less refined. For
example, in the generative case, the nominal cate-
gories were fully refined, while in the discrimina-
tive case, fewer nominal clusters were heavily used.
One reason for this can be seen by inspecting the
first two-way split in the NNP tag. The genera-
tive model split into initial NNPs (San, Wall) and
final NNPs (Francisco, Street). In contrast, the dis-
criminative split was between organizational entities
(Stock, Exchange) and other entity types (September,
New, York). This constrast is unsurprising. Genera-
tive likelihood is advantaged by explaining lexical
choice ? New and York occur in very different slots.
However, they convey the same information about
the syntactic context above their base NP and are
therefore treated the same, discriminatively, while
the systematic attachment distinctions between tem-
porals and named entities are more predictive.
Analyzing the syntactic and semantic patterns
learned by the grammars shows similar trends. In
Table 3 we compare the number of subcategories
in the generative split-and-merge grammars to the
average number of features per unsplit production
with that phrasal category as head in our multi-scale
grammars after 5 split (and merge) rounds. These
quantities are inherently different: the number of
features should be roughly cubic in the number of
subcategories. However, we observe that the num-
bers are very close, indicating that, due to the spar-
sity of our productions, and the efficient multi-scale
encoding, the number of grammar parameters grows
linearly in the number of subcategories. Further-
more, while most categories have similar complex-
ity in those two cases, the complexity of the two
most refined phrasal categories are flipped. Gener-
ative grammars split NPs most highly, discrimina-
874
N
P
V
P
PP S SB
A
R
A
D
JP
A
D
V
P
QP PR
N
Generative 32 24 20 12 12 12 8 7 5
subcategories
Discriminative 19 32 20 14 14 8 7 9 6production parameters
Table 3: Complexity of highly split phrasal categories in
generative and discriminative grammars. Note that sub-
categories are compared to production parameters, indi-
cating that the number of parameters grows cubicly in the
number of subcategories for generative grammars, while
growing linearly for multi-scale grammars.
tive grammars split the VP. This distinction seems
to be because the complexity of VPs is more syntac-
tic (e.g. complex subcategorization), while that of
NPs is more lexical (noun choice is generally higher
entropy than verb choice).
It is also interesting to examine the automatically
learned word class features. Table 4 shows the suf-
fixes with the highest weight for a few different cat-
egories across the three languages that we experi-
mented with. The learning algorithm has selected
discriminative suffixes that are typical derviational
or inflectional morphemes in their respective lan-
guages. Note that the highest weighted suffixes will
typically not correspond to the most common suffix
in the word class, but to the most discriminative.
Finally, the span features also exhibit clear pat-
terns. The highest scoring span features encourage
the words between the last two punctuation marks
to form a constituent (excluding the punctuation
marks), for example ,[x+]. and :[x+]. Words
between quotation marks are also encouraged to
form constituents: ??[x+]?? and x[??x+??]x.
Span features can also discourage grouping words
into constituents. The features with the highest neg-
ative weight involve single commas: x[x,x+],
and x[x+,x+]x and so on (indeed, such spans
were structurally disallowed by the Collins (1999)
parser).
9 Conclusions
Discriminatively trained multi-scale grammars give
state-of-the-art parsing performance on a variety of
languages and corpora. Grammar size is dramati-
cally reduced compared to the baseline, as well as to
ENGLISH GERMAN FRENCH
Adjectives
-ous -los -ien
-ble -bar -ble
-nth -ig -ive
Nouns
-ion -ta?t -te?
-en -ung -eur
-cle -rei -ges
Verbs -ed -st -e?es
-s -eht -e?
Adverbs -ly -mal -ent
Numbers -ty -zig ?
Table 4: Automatically learned suffixes with the highest
weights for different languages and part-of-speech tags.
methods like split-and-merge (Petrov et al, 2006).
Because fewer parameters are estimated, multi-scale
grammars may also be less prone to overfitting, as
suggested by a cross-corpus evaluation experiment.
Furthermore, the discriminative framework enables
the seamless integration of additional, overlapping
features, such as span features and unknown word
features. Such features further improve parsing per-
formance and make the latent variable grammars
very language independent.
Our parser, along with trained grammars
for a variety of languages, is available at
http://nlp.cs.berkeley.edu.
References
A. Abeille, L. Clement, and A. Kinyon. 2000. Building a
treebank for French. In 2nd International Conference
on Language Resources and Evaluation.
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In ICML ?07.
A. Arun and F. Keller. 2005. Lexicalization in crosslin-
guistic probabilistic parsing: the case of french. In
ACL ?05.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
In ACL?05.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-
based best-first chart parsing. 6th Workshop on Very
Large Corpora.
E. Charniak, M. Johnson, D. McClosky, et al 2006.
Multi-level coarse-to-fine PCFG Parsing. In HLT-
NAACL ?06.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In ACL ?04.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, UPenn.
875
J. Finkel, A. Kleeman, and C. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
In ACL ?08.
W. N. Francis and H. Kucera. 2002. Manual of infor-
mation to accompany a standard corpus of present-day
edited american english. In TR, Brown University.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. EMNLP ?01.
J. Henderson. 2004. Discriminative training of a neural
network statistical parser. In ACL ?04.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In ACL ?08.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24:613?632.
M. Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In ACL ?01.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In ACL ?03, pages 423?430.
T. Koo and M. Collins. 2005. Hidden-variable models
for discriminative reranking. In EMNLP ?05.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML ?01.
K. Lari and S. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In EMNLP ?07.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL ?05.
M. Mohri and B. Roark. 2006. Probabilistic context-free
grammar induction based on structural zeros. In HLT-
NAACL ?06.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL ?07.
S. Petrov and D. Klein. 2008. Discriminative log-linear
grammars with latent variables. In NIPS ?08.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In Conf. on Applied Natural Language Processing.
N. A. Smith and M. Johnson. 2007. Weighted and prob-
abilistic context-free grammars are equally expressive.
Computational Lingusitics.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In EMNLP ?04.
J. Turian, B. Wellington, and I. D. Melamed. 2007. Scal-
able discriminative learning for natural language pars-
ing and translation. In NIPS ?07.
876
Proceedings of NAACL HLT 2007, pages 404?411,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Improved Inference for Unlexicalized Parsing
Slav Petrov and Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov,klein}@eecs.berkeley.edu
Abstract
We present several improvements to unlexicalized
parsing with hierarchically state-split PCFGs. First,
we present a novel coarse-to-fine method in which
a grammar?s own hierarchical projections are used
for incremental pruning, including a method for ef-
ficiently computing projections of a grammar with-
out a treebank. In our experiments, hierarchical
pruning greatly accelerates parsing with no loss in
empirical accuracy. Second, we compare various
inference procedures for state-split PCFGs from the
standpoint of risk minimization, paying particular
attention to their practical tradeoffs. Finally, we
present multilingual experiments which show that
parsing with hierarchical state-splitting is fast and
accurate in multiple languages and domains, even
without any language-specific tuning.
1 Introduction
Treebank parsing comprises two problems: learn-
ing, in which we must select a model given a tree-
bank, and inference, in which we must select a
parse for a sentence given the learned model. Pre-
vious work has shown that high-quality unlexical-
ized PCFGs can be learned from a treebank, either
by manual annotation (Klein and Manning, 2003)
or automatic state splitting (Matsuzaki et al, 2005;
Petrov et al, 2006). In particular, we demon-
strated in Petrov et al (2006) that a hierarchically
split PCFG could exceed the accuracy of lexical-
ized PCFGs (Collins, 1999; Charniak and Johnson,
2005). However, many questions about inference
with such split PCFGs remain open. In this work,
we present
1. an effective method for pruning in split PCFGs
2. a comparison of objective functions for infer-
ence in split PCFGs,
3. experiments on automatic splitting for lan-
guages other than English.
In Sec. 3, we present a novel coarse-to-fine pro-
cessing scheme for hierarchically split PCFGs. Our
method considers the splitting history of the final
grammar, projecting it onto its increasingly refined
prior stages. For any projection of a grammar, we
give a new method for efficiently estimating the pro-
jection?s parameters from the source PCFG itself
(rather than a treebank), using techniques for infi-
nite tree distributions (Corazza and Satta, 2006) and
iterated fixpoint equations. We then parse with each
refinement, in sequence, much along the lines of
Charniak et al (2006), except with much more com-
plex and automatically derived intermediate gram-
mars. Thresholds are automatically tuned on held-
out data, and the final system parses up to 100 times
faster than the baseline PCFG parser, with no loss in
test set accuracy.
In Sec. 4, we consider the well-known issue of
inference objectives in split PCFGs. As in many
model families (Steedman, 2000; Vijay-Shanker and
Joshi, 1985), split PCFGs have a derivation / parse
distinction. The split PCFG directly describes a gen-
erative model over derivations, but evaluation is sen-
sitive only to the coarser treebank symbols. While
the most probable parse problem is NP-complete
(Sima?an, 1992), several approximate methods exist,
including n-best reranking by parse likelihood, the
labeled bracket alorithm of Goodman (1996), and
a variational approximation introduced in Matsuzaki
et al (2005). We present experiments which explic-
itly minimize various evaluation risks over a can-
didate set using samples from the split PCFG, and
relate those conditions to the existing non-sampling
algorithms. We demonstrate that n-best reranking
according to likelihood is superior for exact match,
and that the non-reranking methods are superior for
maximizing F1. A specific contribution is to discuss
the role of unary productions, which previous work
has glossed over, but which is important in under-
standing why the various methods work as they do.
404
Finally, in Sec. 5, we learn state-split PCFGs for
German and Chinese and examine out-of-domain
performance for English. The learned grammars are
compact and parsing is very quick in our multi-stage
scheme. These grammars produce the highest test
set parsing figures that we are aware of in each lan-
guage, except for English for which non-local meth-
ods such as feature-based discriminative reranking
are available (Charniak and Johnson, 2005).
2 Hierarchically Split PCFGs
We consider PCFG grammars which are derived
from a raw treebank as in Petrov et al (2006): A
simple X-bar grammar is created by binarizing the
treebank trees. We refer to this grammar as G0.
From this starting point, we iteratively refine the
grammar in stages, as illustrated in Fig. 1. In each
stage, all symbols are split in two, for example DT
might become DT-1 and DT-2. The refined grammar
is estimated using a variant of the forward-backward
algorithm (Matsuzaki et al, 2005). After a split-
ting stage, many splits are rolled back based on (an
approximation to) their likelihood gain. This pro-
cedure gives an ontogeny of grammars Gi, where
G = Gn is the final grammar. Empirically, the
gains on the English Penn treebank level off after 6
rounds. In Petrov et al (2006), some simple smooth-
ing is also shown to be effective. It is interesting to
note that these grammars capture many of the ?struc-
tural zeros? described by Mohri and Roark (2006)
and pruning rules with probability below e?10 re-
duces the grammar size drastically without influenc-
ing parsing performance. Some of our methods and
conclusions are relevant to all state-split grammars,
such as Klein and Manning (2003) or Dreyer and
Eisner (2006), while others apply most directly to
the hierarchical case.
3 Search
When working with large grammars, it is standard to
prune the search space in some way. In the case of
lexicalized grammars, the unpruned chart often will
not even fit in memory for long sentences. Several
proven techniques exist. Collins (1999) combines a
punctuation rule which eliminates many spans en-
tirely, and then uses span-synchronous beams to
prune in a bottom-up fashion. Charniak et al (1998)
G0
G1
G2
G3
G4
G5
G6
X-bar =
G =
pi i
DT:
DT-1: DT-2:
the
that
this
this
0 1 2 3 4
That
5 6 7
some
some
8 9 10 11
these
12 13
the
the
the
14 15
The
16
a
a
17
Figure 1: Hierarchical refinement proceeds top-down while pro-
jection recovers coarser grammars. The top word for the first
refinements of the determiner tag (DT) is shown on the right.
introduces best-first parsing, in which a figure-of-
merit prioritizes agenda processing. Most relevant
to our work is Charniak and Johnson (2005) which
uses a pre-parse phase to rapidly parse with a very
coarse, unlexicalized treebank grammar. Any item
X:[i, j] with sufficiently low posterior probability in
the pre-parse triggers the pruning of its lexical vari-
ants in a subsequent full parse.
3.1 Coarse-to-Fine Approaches
Charniak et al (2006) introduces multi-level coarse-
to-fine parsing, which extends the basic pre-parsing
idea by adding more rounds of pruning. In their
work, the extra pruning was with grammars even
coarser than the raw treebank grammar, such as
a grammar in which all nonterminals are col-
lapsed. We propose a novel multi-stage coarse-to-
fine method which is particularly natural for our hi-
erarchically split grammar, but which is, in princi-
ple, applicable to any grammar. As in Charniak et
al. (2006), we construct a sequence of increasingly
refined grammars, reparsing with each refinement.
The contributions of our method are that we derive
sequences of refinements in a new way (Sec. 3.2),
we consider refinements which are themselves com-
plex, and, because our full grammar is not impossi-
ble to parse with, we automatically tune the pruning
thresholds on held-out data.
3.2 Projection
In our method, which we call hierarchical coarse-
to-fine parsing, we consider a sequence of PCFGs
G0, G1, . . . Gn = G, where each Gi is a refinement
of the preceding grammar Gi?1 and G is the full
grammar of interest. Each grammar Gi is related to
G = Gn by a projection pin?i or pii for brevity. A
405
projection is a map from the non-terminal (including
pre-terminal) symbols of G onto a reduced domain.
A projection of grammar symbols induces a pro-
jection of rules and therefore entire non-weighted
grammars (see Fig. 1).
In our case, we also require the projections to be
sequentially compatible, so that pii?j =pik?j?pii?k.
That is, each projection is itself a coarsening of the
previous projections. In particular, we take the pro-
jection pii?j to be the map that collapses split sym-
bols in round i to their earlier identities in round j.
It is straightforward to take a projection pi and
map a CFG G to its induced projection pi(G). What
is less obvious is how the probabilities associated
with the rules of G should be mapped. In the case
where pi(G) is more coarse than the treebank orig-
inally used to train G, and when that treebank is
available, it is easy to project the treebank and di-
rectly estimate, say, the maximum-likelihood pa-
rameters for pi(G). This is the approach taken by
Charniak et al (2006), where they estimate what in
our terms are projections of the raw treebank gram-
mar from the treebank itself.
However, treebank estimation has several limita-
tions. First, the treebank used to train G may not
be available. Second, if the grammar G is heavily
smoothed or otherwise regularized, its own distri-
bution over trees may be far from that of the tree-
bank. Third, the meanings of the split states can and
do drift between splitting stages. Fourth, and most
importantly, we may wish to project grammars for
which treebank estimation is problematic, for exam-
ple, grammars which are more refined than the ob-
served treebank grammars. Our method effectively
avoids all of these problems by rebuilding and refit-
ting the pruning grammars on the fly from the final
grammar.
3.2.1 Estimating Projected Grammars
Fortunately, there is a well worked-out notion of
estimating a grammar from an infinite distribution
over trees (Corazza and Satta, 2006). In particular,
we can estimate parameters for a projected grammar
pi(G) from the tree distribution induced by G (which
can itself be estimated in any manner). The earli-
est work that we are aware of on estimating models
from models in this way is that of Nederhof (2005),
who considers the case of learning language mod-
els from other language models. Corazza and Satta
(2006) extend these methods to the case of PCFGs
and tree distributions.
The generalization of maximum likelihood esti-
mation is to find the estimates for pi(G) with min-
imum KL divergence from the tree distribution in-
duced by G. Since pi(G) is a grammar over coarser
symbols, we fit pi(G) to the distribution G induces
over pi-projected trees: P (pi(T )|G). The proofs
of the general case are given in Corazza and Satta
(2006), but the resulting procedure is quite intuitive.
Given a (fully observed) treebank, the maximum-
likelihood estimate for the probability of a rule X ?
Y Z would simply be the ratio of the count of X to
the count of the configuration X ? Y Z . If we wish
to find the estimate which has minimum divergence
to an infinite distribution P (T ), we use the same for-
mula, but the counts become expected counts:
P (X ? Y Z) =
EP (T )[X ? Y Z]
EP (T )[X]
with unaries estimated similarly. In our specific
case, X,Y, and Z are symbols in pi(G), and the
expectations are taken over G?s distribution of pi-
projected trees, P (pi(T )|G). We give two practical
methods for obtaining these expectations below.
3.2.2 Calculating Projected Expectations
Concretely, we can now estimate the minimum
divergence parameters of pi(G) for any projection
pi and PCFG G if we can calculate the expecta-
tions of the projected symbols and rules according to
P (pi(T )|G). The simplest option is to sample trees
T from G, project the samples, and take average
counts off of these samples. In the limit, the counts
will converge to the desired expectations, provided
the grammar is proper. However, we can exploit the
structure of our projections to obtain the desired ex-
pectations much more simply and efficiently.
First, consider the problem of calculating the ex-
pected counts of a symbol X in a tree distribution
given by a grammar G, ignoring the issue of projec-
tion. These expected counts obey the following one-
step equations (assuming a unique root symbol):
c(root) = 1
c(X) =
?
Y??X?
P (?X?|Y )c(Y )
406
Here, ?, ?, or both can be empty, and a rule X ? ?
appears in the sum once for each X it contains. In
principle, this linear system can be solved in any
way.1 In our experiments, we solve this system it-
eratively, with the following recurrences:
c0(X)?
{
1 if X = root
0 otherwise
ci+1(X)?
?
Y??X?
P (?X?|Y )ci(Y )
Note that, as in other iterative fixpoint methods, such
as policy evaluation for Markov decision processes
(Sutton and Barto, 1998), the quantities ck(X) have
a useful interpretation as the expected counts ignor-
ing nodes deeper than depth k (i.e. the roots are all
the root symbol, so c0(root) = 1). In our experi-
ments this method converged within around 25 iter-
ations; this is unsurprising, since the treebank con-
tains few nodes deeper than 25 and our base gram-
mar G seems to have captured this property.
Once we have the expected counts of symbols
in G, the expected counts of their projections
X ? = pi(X) according to P (pi(T )|G) are given by
c(X ?) = ?X:pi(X)=X? c(X). Rules can be esti-
mated directly using similar recurrences, or given by
one-step equations:
c(X ? ?) = c(X)P (?|X)
This process very rapidly computes the estimates
for a projection of a grammar (i.e. in a few seconds
for our largest grammars), and is done once during
initialization of the parser.
3.2.3 Hierarchical Projections
Recall that our final state-split grammars G come,
by their construction process, with an ontogeny of
grammars Gi where each grammar is a (partial)
splitting of the preceding one. This gives us a nat-
ural chain of projections pii?j which projects back-
wards along this ontogeny of grammars (see Fig. 1).
Of course, training also gives us parameters for
the grammars, but only the chain of projections is
needed. Note that the projected estimates need not
1Whether or not the system has solutions depends on the
parameters of the grammar. In particular, G may be improper,
though the results of Chi (1999) imply that G will be proper if
it is the maximum-likelihood estimate of a finite treebank.
(and in general will not) recover the original param-
eters exactly, nor would we want them to. Instead
they take into account any smoothing, substate drift,
and so on which occurred by the final grammar.
Starting from the base grammar, we run the pro-
jection process for each stage in the sequence, cal-
culating pii (chained incremental projections would
also be possible). For the remainder of the paper,
except where noted otherwise, all coarser grammars?
estimates are these reconstructions, rather than those
originally learned.
3.3 Experiments
As demonstrated by Charniak et al (2006) parsing
times can be greatly reduced by pruning chart items
that have low posterior probability under a simpler
grammar. Charniak et al (2006) pre-parse with a se-
quence of grammars which are coarser than (parent-
annotated) treebank grammars. However, we also
work with grammars which are already heavily split,
up to half as split as the final grammar, because we
found the computational cost for parsing with the
simple X-bar grammar to be insignificant compared
to the costs for parsing with more refined grammars.
For a final grammar G = Gn, we compute esti-
mates for the n projections Gn?1, . . . , G0 =X-Bar,
where Gi = pii(G) as described in the previous sec-
tion. Additionally we project to a grammar G?1 in
which all nonterminals, except for the preterminals,
have been collapsed. During parsing, we start of
by exhaustively computing the inside/outside scores
with G?1. At each stage, chart items with low poste-
rior probability are removed from the chart, and we
proceed to compute inside/outside scores with the
next, more refined grammar, using the projections
pii?i?1 to map between symbols in Gi and Gi?1. In
each pass, we skip chart items whose projection into
the previous stage had a probability below a stage-
specific threshold, until we reach G = Gn (after
seven passes in our case). For G, we do not prune
but instead return the minimum risk tree, as will be
described in Sec. 4.
Fig. 2 shows the (unlabeled) bracket posteriors af-
ter each pass and demonstrates that most construc-
tions can be ruled out by the simpler grammars,
greatly reducing the amount of computation for the
following passes. The pruning thresholds were em-
pirically determined on a held out set by computing
407
In
flu
en
tia
l
m
e
m
be
rs of th
e
H
ou
se
W
ay
s
a
n
d
M
ea
ns
Co
m
m
itt
ee
in
tro
du
ce
d
le
gi
sla
tio
n
th
at
w
o
u
ld
re
st
ric
t
ho
w
th
e
n
e
w
s&
l
ba
ilo
ut
a
ge
nc
y
ca
n
ra
is
e
ca
pi
ta
l ;
cr
e
a
tin
g
a
n
o
th
er
po
te
nt
ia
l
o
bs
ta
cle to th
e
go
ve
rn
m
en
t
?s
sa
le of
si
ck
th
rif
ts .
G?1 G0=X-bar G1
G2 G3 G4
G5
(G6=G)
Output
Figure 2: Bracket posterior probabilities (black = high) for the
first sentence of our development set during coarse-to-fine
pruning. Note that we compute the bracket posteriors at a much
finer level but are showing the unlabeled posteriors for illustra-
tion purposes. No pruning is done at the finest level (G6 = G)
but the minimum risk tree is returned instead.
the most likely tree under G directly (without prun-
ing) and then setting the highest pruning threshold
for each stage that would not prune the optimal tree.
This setting also caused no search errors on the test
set. We found our projected grammar estimates to be
at least equally well suited for pruning as the orig-
inal grammar estimates which were learned during
the hierarchical training. Tab. 1 shows the tremen-
dous reduction in parsing time (all times are cumu-
lative) and gives an overview over grammar sizes
and parsing accuracies. In particular, in our Java im-
plementation on a 3GHz processor, it is possible to
parse the 1578 development set sentences (of length
40 or less) in less than 1200 seconds with an F1 of
91.2% (no search errors), or, by pruning more, in
680 seconds at 91.1%. For comparison, the Feb.
2006 release of the Charniak and Johnson (2005)
parser runs in 1150 seconds on the same machine
with an F1 of 90.7%.
4 Objective Functions for Parsing
A split PCFG is a grammar G over symbols of the
form X-k where X is an evaluation symbol (such
as NP) and k is some indicator of a subcategory,
such as a parent annotation. G induces a deriva-
tion distribution P (T |G) over trees T labeled with
split symbols. This distribution in turn induces
a parse distribution P (T ?|G) = P (pi(T )|G) over
(projected) trees with unsplit evaluation symbols,
where P (T ?|G) = ?T :T ?=pi(T ) P (T |G). We now
have several choices of how to select a tree given
these posterior distributions over trees. In this sec-
tion, we present experiments with the various op-
tions and explicitly relate them to parse risk mini-
mization (Titov and Henderson, 2006).
G0 G2 G4 G6
Nonterminals 98 219 498 1140
Rules 3,700 19,600 126,100 531,200
No pruning 52 min 99 min 288 min 1612 min
X-bar pruning 8 min 14 min 30 min 111 min
C-to-F (no loss) 6 min 12 min 16 min 20 min
F1 for above 64.8 85.2 89.7 91.2
C-to-F (lossy) 6 min 8 min 9 min 11 min
F1 for above 64.3 84.7 89.4 91.1
Table 1: Grammar sizes, parsing times and accuracies for hier-
archically split PCFGs with and without hierarchical coarse-to-
fine parsing on our development set (1578 sentences with 40 or
less words from section 22 of the Penn Treebank). For compar-
ison the parser of Charniak and Johnson (2005) has an accuracy
of F1=90.7 and runs in 19 min on this set.
The decision-theoretic approach to parsing would
be to select the parse tree which minimizes our ex-
pected loss according to our beliefs:
T ?P = argmin
TP
?
TT
P (TT |w,G)L(TP , TT )
where TT and TP are ?true? and predicted parse
trees. Here, our loss is described by the function L
whose first argument is the predicted parse tree and
the second is the gold parse tree. Reasonable can-
didates for L include zero-one loss (exact match),
precision, recall, F1 (specifically EVALB here), and
so on. Of course, the naive version of this process is
intractable: we have to loop over all (pairs of) pos-
sible parses. Additionally, it requires parse likeli-
hoods P (TP |w,G), which are tractable, but not triv-
ial, to compute for split models. There are two op-
tions: limit the predictions to a small candidate set or
choose methods for which dynamic programs exist.
For arbitrary loss functions, we can approximate
the minimum-risk procedure by taking the min over
only a set of candidate parses TP . In some cases,
each parse?s expected risk can be evaluated in closed
408
Rule score: r(A? B C, i, k, j) =
?
x
?
y
?
z
POUT(Ax, i, j)P(Ax ? By Cz)PIN(By, i, k)PIN(Cy, k, j)
VARIATIONAL: q(A? B C, i, k, j) = r(A? B C, i, k, j)P
x POUT(Ax,i,j)PIN(Ax,i,j)
TG = argmaxT
?
e?T q(e)
MAX-RULE-SUM: q(A? B C, i, k, j) = r(A? B C, i, k, j)PIN(root,0,n) TG = argmaxT
?
e?T q(e)
MAX-RULE-PRODUCT: q(A? B C, i, k, j) = r(A? B C, i, k, j)PIN(root,0,n) TG = argmaxT
?
e?T q(e)
Figure 3: Different objectives for parsing with posteriors, yielding comparable results. A, B, C are nonterminal symbols, x, y, z
are latent annotations and i, j, k are between-word indices. Hence (Ax, i, j) denotes a constituent labeled with Ax spanning from
i to j. Furthermore, we write e = (A? B C, i, j, k) for brevity.
form. Exact match (likelihood) has this property. In
general, however, we can approximate the expecta-
tion with samples from P (T |w,G). The method for
sampling derivations of a PCFG is given in Finkel
et al (2006) and Johnson et al (2007). It requires a
single inside-outside computation per sentence and
is then efficient per sample. Note that for split gram-
mars, a posterior parse sample can be drawn by sam-
pling a derivation and projecting away the substates.
Fig. 2 shows the results of the following exper-
iment. We constructed 10-best lists from the full
grammar G in Sec. 2 using the parser of Petrov et
al. (2006). We then took the same grammar and ex-
tracted 500-sample lists using the method of Finkel
et al (2006). The minimum risk parse candidate was
selected for various loss functions. As can be seen,
in most cases, risk minimization reduces test-set loss
of the relevant quantity. Exact match is problematic,
however, because 500 samples is often too few to
draw a match when a sentence has a very flat poste-
rior, and so there are many all-way ties.2 Since ex-
act match permits a non-sampled calculation of the
expected risk, we show this option as well, which
is substantially superior. This experiment highlights
that the correct procedure for exact match is to find
the most probable parse.
An alternative approach to reranking candidate
parses is to work with inference criteria which ad-
mit dynamic programming solutions. Fig. 3 shows
three possible objective functions which use the eas-
ily obtained posterior marginals of the parse tree dis-
tribution. Interestingly, while they have fairly differ-
ent decision theoretic motivations, their closed-form
solutions are similar.
25,000 samples do not improve the numbers appreciably.
One option is to maximize likelihood in an ap-
proximate distribution. Matsuzaki et al (2005)
present a VARIATIONAL approach, which approxi-
mates the true posterior over parses by a cruder, but
tractable sentence-specific one. In this approximate
distribution there is no derivation / parse distinction
and one can therefore optimize exact match by se-
lecting the most likely derivation.
Instead of approximating the tree distribution we
can use an objective function that decomposes along
parse posteriors. The labeled brackets algorithm of
Goodman (1996) has such an objective function. In
its original formulation this algorithm maximizes
the number of expected correct nodes, but instead
we can use it to maximize the number of correct
rules (the MAX-RULE-SUM algorithm). A worry-
ing issue with this method is that it is ill-defined for
grammars which allow infinite unary chains: there
will be no finite minimum risk tree under recall loss
(you can always reduce the risk by adding one more
cycle). We implement MAX-RULE-SUM in a CNF-
like grammar family where above each binary split
is exactly one unary (possibly a self-loop). With
this limitation, unary chains are not a problem. As
might be expected, this criterion improves bracket
measures at the expense of exact match.
We found it optimal to use a third approach,
in which rule posteriors are multiplied instead of
added. This corresponds to choosing the tree with
greatest chance of having all rules correct, under
the (incorrect) assumption that the rules correct-
ness are independent. This MAX-RULE-PRODUCT
algorithm does not need special treatment of infi-
nite unary chains because it is optimizing a product
rather than a sum. While these three methods yield
409
Objective P R F1 EX
BEST DERIVATION
Viterbi Derivation 89.6 89.4 89.5 37.4
RERANKING
Random 87.6 87.7 87.7 16.4
Precision (sampled) 91.1 88.1 89.6 21.4
Recall (sampled) 88.2 91.3 89.7 21.5
F1 (sampled) 90.2 89.3 89.8 27.2
Exact (sampled) 89.5 89.5 89.5 25.8
Exact (non-sampled) 90.8 90.8 90.8 41.7
Exact/F1 (oracle) 95.3 94.4 95.0 63.9
DYNAMIC PROGRAMMING
VARIATIONAL 90.7 90.9 90.8 41.4
MAX-RULE-SUM 90.5 91.3 90.9 40.4
MAX-RULE-PRODUCT 91.2 91.1 91.2 41.4
Table 2: A 10-best list from our best G can be reordered as to
maximize a given objective either using samples or, under some
restricting assumptions, in closed form.
very similar results (see Fig. 2), the MAX-RULE-
PRODUCT algorithm consistently outperformed the
other two.
Overall, the closed-form options were superior to
the reranking ones, except on exact match, where the
gains from correctly calculating the risk outweigh
the losses from the truncation of the candidate set.
5 Multilingual Parsing
Most research on parsing has focused on English
and parsing performance on other languages is gen-
erally significantly lower.3 Recently, there have
been some attempts to adapt parsers developed for
English to other languages (Levy and Manning,
2003; Cowan and Collins, 2005). Adapting lexi-
calized parsers to other languages in not a trivial
task as it requires at least the specification of head
rules, and has had limited success. Adapting unlexi-
calized parsers appears to be equally difficult: Levy
and Manning (2003) adapt the unlexicalized parser
of Klein and Manning (2003) to Chinese, but even
after significant efforts on choosing category splits,
only modest performance gains are reported.
In contrast, automatically learned grammars like
the one of Matsuzaki et al (2005) and Petrov et al
(2006) require a treebank for training but no addi-
tional human input. One has therefore reason to
3Of course, cross-linguistic comparison of results is com-
plicated by differences in corpus annotation schemes and sizes,
and differences in linguistic characteristics.
ENGLISH GERMAN CHINESE
(Marcus et al, 1993) (Skut et al, 1997) (Xue et al, 2002)
TrainSet Section 2-21 Sentences 1-18,602 Articles 26-270
DevSet Section 22 18,603-19,602 Articles 1-25
TestSet Section 23 19,603-20,602 Articles 271-300
Table 3: Experimental setup.
believe that their performance will generalize bet-
ter across languages than the performance of parsers
that have been hand tailored to English.
5.1 Experiments
We trained models for English, Chinese and Ger-
man using the standard corpora and splits as shown
in Tab. 3. We applied our model directly to each
of the treebanks, without any language dependent
modifications. Specifically, the same model hyper-
parameters (merging percentage and smoothing fac-
tor) were used in all experiments.
Tab. 4 shows that automatically inducing latent
structure is a technique that generalizes well across
language boundaries and results in state of the art
performance for Chinese and German. On English,
the parser is outperformed only by the reranking
parser of Charniak and Johnson (2005), which has
access to a variety of features which cannot be cap-
tured by a generative model.
Space does not permit a thorough exposition of
our analysis, but as in the case of English (Petrov
et al, 2006), the learned subcategories exhibit inter-
esting linguistic interpretations. In German, for ex-
ample, the model learns subcategories for different
cases and genders.
5.2 Corpus Variation
Related to cross language generalization is the gen-
eralization across domains for the same language.
It is well known that a model trained on the Wall
Street Journal loses significantly in performance
when evaluated on the Brown Corpus (see Gildea
(2001) for more details and the exact setup of their
experiment, which we duplicated here). Recently
McClosky et al (2006) came to the conclusion that
this performance drop is not due to overfitting the
WSJ data. Fig. 4 shows the performance on the
Brown corpus during hierarchical training. While
the F1 score on the WSJ is rising we observe a drop
in performance after the 5th iteration, suggesting
that some overfitting is occurring.
410
? 40 words all
Parser LP LR LP LR
ENGLISH
Charniak et al (2005) 90.1 90.1 89.5 89.6
Petrov et al (2006) 90.3 90.0 89.8 89.6
This Paper 90.7 90.5 90.2 89.9
ENGLISH (reranked)
Charniak et al (2005)4 92.4 91.6 91.8 91.0
GERMAN
Dubey (2005) F1 76.3 -
This Paper 80.8 80.7 80.1 80.1
CHINESE5
Chiang et al (2002) 81.1 78.8 78.0 75.2
This Paper 80.8 80.7 78.8 78.5
Table 4: Our final test set parsing performance compared to the
best previous work on English, German and Chinese.
78
80
82
84
86
Grammar Size
F 1
Hierarchically Split PCFGs
Charniak and Johnson (2005) generative parser
Charniak and Johnson (2005) reranking parser
G3
G5 G6G4
Figure 4: Parsing accuracy starts dropping after 5 training iter-
ations on the Brown corpus, while it is improving on the WSJ,
indicating overfitting.
6 Conclusions
The coarse-to-fine scheme presented here, in con-
junction with the risk-appropriate parse selection
methodology, allows fast, accurate parsing, in multi-
ple languages and domains. For training, one needs
only a raw context-free treebank and for decoding
one needs only a final grammar, along with coars-
ening maps. The final parser is publicly available at
http://www.nlp.cs.berkeley.edu.
Acknowledgments We would like to thank Eu-
gene Charniak, Mark Johnson and Noah Smith for
helpful discussions and comments.
References
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-Best
Parsing and MaxEnt Discriminative Reranking. In ACL?05.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-based
best-first chart parsing. 6th Wkshop on Very Large Corpora.
4This is the performance of the updated reranking parser
available at http://www.cog.brown.edu/mj/software.htm
5Sun and Jurafsky (2004) report even better performance on
this dataset but since they assume gold POS tags their work is
not directly comparable (p.c.).
E. Charniak, M. Johnson, et al 2006. Multi-level coarse-to-fine
PCFG Parsing. In HLT-NAACL ?06.
Z. Chi. 1999. Statistical properties of probabilistic context-free
grammars. In Computational Linguistics.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, U. of Pennsylvania.
A. Corazza and G. Satta. 2006. Cross-entropy and estimation
of probabilistic context-free grammars. In HLT-NAACL ?06.
B. Cowan and M. Collins. 2005. Morphology and reranking
for the statistical parsing of Spanish. In HLT-EMNLP ?05.
M. Dreyer and J. Eisner. 2006. Better informed training of
latent syntactic features. In EMNLP ?06, pages 317?326.
A. Dubey. 2005. What to do when lexicalization fails: parsing
German with suffix analysis and smoothing. In ACL ?05.
J. Finkel, C. Manning, and A. Ng. 2006. Solving the prob-
lem of cascading errors: approximate Bayesian inference for
lingusitic annotation pipelines. In EMNLP ?06.
D. Gildea. 2001. Corpus variation and parser performance.
EMNLP ?01, pages 167?202.
J. Goodman. 1996. Parsing algorithms and metrics. ACL ?96.
M. Johnson, T. Griffiths, and S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov Chain Monte Carlo. In
HLT-NAACL ?07.
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. In ACL ?03, pages 423?430.
R. Levy and C. Manning. 2003. Is it harder to parse Chinese,
or the Chinese treebank? In ACL ?03, pages 439?446.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Build-
ing a large annotated corpus of English: The Penn Treebank.
In Computational Linguistics.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG
with latent annotations. In ACL ?05, pages 75?82.
D. McClosky, E. Charniak, and M. Johnson. 2006. Reranking
and self-training for parser adaptation. In COLING-ACL?06.
M. Mohri and B. Roark. 2006. Probabilistic context-free gram-
mar induction based on structural zeros. In HLT-NAACL ?06.
M.-J. Nederhof. 2005. A general technique to train language
models on language models. In Computational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learn-
ing accurate, compact, and interpretable tree annotation. In
COLING-ACL ?06, pages 443?440.
K. Sima?an. 1992. Computatoinal complexity of probabilistic
disambiguation. Grammars, 5:125?151.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997. An anno-
tation scheme for free word order languages. In Conference
on Applied Natural Language Processing.
M. Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, Massachusetts.
H. Sun and D. Jurafsky. 2004. Shallow semantic parsing of
Chinese. In HLT-NAACL ?04, pages 249?256.
R. Sutton and A. Barto. 1998. Reinforcement Learning: An
Introduction. MIT Press.
I. Titov and J. Henderson. 2006. Loss minimization in parse
reranking. In EMNLP ?06, pages 560?567.
K. Vijay-Shanker and A. Joshi. 1985. Some computational
properties of Tree Adjoining Grammars. In ACL ?85.
N. Xue, F.-D. Chiou, and M. Palmer. 2002. Building a large
scale annotated Chinese corpus. In COLING ?02.
411
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433?440,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Accurate, Compact, and Interpretable Tree Annotation
Slav Petrov Leon Barrett Romain Thibaux Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov, lbarrett, thibaux, klein}@eecs.berkeley.edu
Abstract
We present an automatic approach to tree annota-
tion in which basic nonterminal symbols are alter-
nately split and merged to maximize the likelihood
of a training treebank. Starting with a simple X-
bar grammar, we learn a new grammar whose non-
terminals are subsymbols of the original nontermi-
nals. In contrast with previous work, we are able
to split various terminals to different degrees, as ap-
propriate to the actual complexity in the data. Our
grammars automatically learn the kinds of linguistic
distinctions exhibited in previous work on manual
tree annotation. On the other hand, our grammars
are much more compact and substantially more ac-
curate than previous work on automatic annotation.
Despite its simplicity, our best grammar achieves
an F1 of 90.2% on the Penn Treebank, higher than
fully lexicalized systems.
1 Introduction
Probabilistic context-free grammars (PCFGs) underlie
most high-performance parsers in one way or another
(Collins, 1999; Charniak, 2000; Charniak and Johnson,
2005). However, as demonstrated in Charniak (1996)
and Klein and Manning (2003), a PCFG which sim-
ply takes the empirical rules and probabilities off of a
treebank does not perform well. This naive grammar
is a poor one because its context-freedom assumptions
are too strong in some places (e.g. it assumes that sub-
ject and object NPs share the same distribution) and too
weak in others (e.g. it assumes that long rewrites are
not decomposable into smaller steps). Therefore, a va-
riety of techniques have been developed to both enrich
and generalize the naive grammar, ranging from simple
tree annotation and symbol splitting (Johnson, 1998;
Klein and Manning, 2003) to full lexicalization and in-
tricate smoothing (Collins, 1999; Charniak, 2000).
In this paper, we investigate the learning of a gram-
mar consistent with a treebank at the level of evalua-
tion symbols (such as NP, VP, etc.) but split based on
the likelihood of the training trees. Klein and Manning
(2003) addressed this question from a linguistic per-
spective, starting with a Markov grammar and manu-
ally splitting symbols in response to observed linguistic
trends in the data. For example, the symbol NP might
be split into the subsymbol NP?S in subject position
and the subsymbol NP?VP in object position. Recently,
Matsuzaki et al (2005) and also Prescher (2005) ex-
hibited an automatic approach in which each symbol is
split into a fixed number of subsymbols. For example,
NP would be split into NP-1 through NP-8. Their ex-
citing result was that, while grammars quickly grew too
large to be managed, a 16-subsymbol induced grammar
reached the parsing performance of Klein and Manning
(2003)?s manual grammar. Other work has also investi-
gated aspects of automatic grammar refinement; for ex-
ample, Chiang and Bikel (2002) learn annotations such
as head rules in a constrained declarative language for
tree-adjoining grammars.
We present a method that combines the strengths of
both manual and automatic approaches while address-
ing some of their common shortcomings. Like Mat-
suzaki et al (2005) and Prescher (2005), we induce
splits in a fully automatic fashion. However, we use a
more sophisticated split-and-merge approach that allo-
cates subsymbols adaptively where they are most effec-
tive, like a linguist would. The grammars recover pat-
terns like those discussed in Klein and Manning (2003),
heavily articulating complex and frequent categories
like NP and VP while barely splitting rare or simple
ones (see Section 3 for an empirical analysis).
Empirically, hierarchical splitting increases the ac-
curacy and lowers the variance of the learned gram-
mars. Another contribution is that, unlike previous
work, we investigate smoothed models, allowing us to
split grammars more heavily before running into the
oversplitting effect discussed in Klein and Manning
(2003), where data fragmentation outweighs increased
expressivity.
Our method is capable of learning grammars of sub-
stantially smaller size and higher accuracy than previ-
ous grammar refinement work, starting from a simpler
initial grammar. For example, even beginning with an
X-bar grammar (see Section 1.1) with 98 symbols, our
best grammar, using 1043 symbols, achieves a test set
F1 of 90.2%. This is a 27% reduction in error and a sig-
nificant reduction in size1 over the most accurate gram-
1This is a 97.5% reduction in number of symbols. Mat-
suzaki et al (2005) do not report a number of rules, but our
small number of symbols and our hierarchical training (which
433
(a) FRAG
RB
Not
NP
DT
this
NN
year
.
.
(b) ROOT
FRAG
FRAG
RB
Not
NP
DT
this
NN
year
.
.
Figure 1: (a) The original tree. (b) The X-bar tree.
mar in Matsuzaki et al (2005). Our grammar?s accu-
racy was higher than fully lexicalized systems, includ-
ing the maximum-entropy inspired parser of Charniak
and Johnson (2005).
1.1 Experimental Setup
We ran our experiments on the Wall Street Journal
(WSJ) portion of the Penn Treebank using the stan-
dard setup: we trained on sections 2 to 21, and we
used section 1 as a validation set for tuning model hy-
perparameters. Section 22 was used as development
set for intermediate results. All of section 23 was re-
served for the final test. We used the EVALB parseval
reference implementation, available from Sekine and
Collins (1997), for scoring. All reported development
set results are averages over four runs. For the final test
we selected the grammar that performed best on the de-
velopment set.
Our experiments are based on a completely unanno-
tated X-bar style grammar, obtained directly from the
Penn Treebank by the binarization procedure shown in
Figure 1. For each local tree rooted at an evaluation
nonterminal X , we introduce a cascade of new nodes
labeled X so that each has two children. Rather than
experiment with head-outward binarization as in Klein
and Manning (2003), we simply used a left branching
binarization; Matsuzaki et al (2005) contains a com-
parison showing that the differences between binariza-
tions are small.
2 Learning
To obtain a grammar from the training trees, we want
to learn a set of rule probabilities ? on latent annota-
tions that maximize the likelihood of the training trees,
despite the fact that the original trees lack the latent
annotations. The Expectation-Maximization (EM) al-
gorithm allows us to do exactly that.2 Given a sen-
tence w and its unannotated tree T , consider a non-
terminal A spanning (r, t) and its children B and C
spanning (r, s) and (s, t). Let Ax be a subsymbol
of A, By of B, and Cz of C. Then the inside and
outside probabilities PIN(r, t, Ax) def= P (wr:t|Ax) and
POUT(r, t, Ax) def= P (w1:rAxwt:n) can be computed re-
encourages sparsity) suggest a large reduction.
2Other techniques are also possible; Henderson (2004)
uses neural networks to induce latent left-corner parser states.
cursively:
PIN(r, t, Ax) =
?
y,z
?(Ax ? ByCz)
?PIN(r, s, By)PIN(s, t, Cz)
POUT(r, s, By) =
?
x,z
?(Ax ? ByCz)
?POUT(r, t, Ax)PIN(s, t, Cz)
POUT(s, t, Cz) =
?
x,y
?(Ax ? ByCz)
?POUT(r, t, Ax)PIN(r, s, By)
Although we show only the binary component here, of
course there are both binary and unary productions that
are included. In the Expectation step, one computes
the posterior probability of each annotated rule and po-
sition in each training set tree T :
P ((r, s, t, Ax ? ByCz)|w, T ) ? POUT(r, t, Ax)
??(Ax ? ByCz)PIN(r, s, By)PIN(s, t, Cz) (1)
In the Maximization step, one uses the above probabil-
ities as weighted observations to update the rule proba-
bilities:
?(Ax ? ByCz) :=
#{Ax ? ByCz}
?
y?,z? #{Ax ? By?Cz?}
Note that, because there is no uncertainty about the lo-
cation of the brackets, this formulation of the inside-
outside algorithm is linear in the length of the sentence
rather than cubic (Pereira and Schabes, 1992).
For our lexicon, we used a simple yet robust method
for dealing with unknown and rare words by extract-
ing a small number of features from the word and then
computing appproximate tagging probabilities.3
2.1 Initialization
EM is only guaranteed to find a local maximum of the
likelihood, and, indeed, in practice it often gets stuck in
a suboptimal configuration. If the search space is very
large, even restarting may not be sufficient to alleviate
this problem. One workaround is to manually specify
some of the annotations. For instance, Matsuzaki et al
(2005) start by annotating their grammar with the iden-
tity of the parent and sibling, which are observed (i.e.
not latent), before adding latent annotations.4 If these
manual annotations are good, they reduce the search
space for EM by constraining it to a smaller region. On
the other hand, this pre-splitting defeats some of the
purpose of automatically learning latent annotations,
3A word is classified into one of 50 unknown word cate-
gories based on the presence of features such as capital let-
ters, digits, and certain suffixes and its tagging probability is
given by: P?(word|tag) = k P?(class|tag) where k is a con-
stant representing P (word|class) and can simply be dropped.
Rare words are modeled using a combination of their known
and unknown distributions.
4In other words, in the terminology of Klein and Man-
ning (2003), they begin with a (vertical order=2, horizontal
order=1) baseline grammar.
434
DT
the (0.50) a (0.24) The (0.08)
that (0.15) this (0.14) some (0.11)
this (0.39)
that (0.28)
That (0.11)
this (0.52)
that (0.36)
another (0.04)
That (0.38)
This (0.34)
each (0.07)
some (0.20)
all (0.19)
those (0.12)
some (0.37)
all (0.29)
those (0.14)
these (0.27)
both (0.21)
Some (0.15)
the (0.54) a (0.25) The (0.09)
the (0.80)
The (0.15)
a (0.01)
the (0.96)
a (0.01)
The (0.01)
The (0.93)
A(0.02)
No(0.01)
a (0.61)
the (0.19)
an (0.10)
a (0.75)
an (0.12)
the (0.03)
Figure 2: Evolution of the DT tag during hierarchical splitting and merging. Shown are the top three words for
each subcategory and their respective probability.
leaving to the user the task of guessing what a good
starting annotation might be.
We take a different, fully automated approach. We
start with a completely unannotated X-bar style gram-
mar as described in Section 1.1. Since we will evaluate
our grammar on its ability to recover the Penn Treebank
nonterminals, we must include them in our grammar.
Therefore, this initialization is the absolute minimum
starting grammar that includes the evaluation nontermi-
nals (and maintains separate grammar symbols for each
of them).5 It is a very compact grammar: 98 symbols,6
236 unary rules, and 3840 binary rules. However, it
also has a very low parsing performance: 65.8/59.8
LP/LR on the development set.
2.2 Splitting
Beginning with this baseline grammar, we repeatedly
split and re-train the grammar. In each iteration we
initialize EM with the results of the smaller gram-
mar, splitting every previous annotation symbol in two
and adding a small amount of randomness (1%) to
break the symmetry. The results are shown in Fig-
ure 3. Hierarchical splitting leads to better parame-
ter estimates over directly estimating a grammar with
2k subsymbols per symbol. While the two procedures
are identical for only two subsymbols (F1: 76.1%),
the hierarchical training performs better for four sub-
symbols (83.7% vs. 83.2%). This advantage grows
as the number of subsymbols increases (88.4% vs.
87.3% for 16 subsymbols). This trend is to be ex-
pected, as the possible interactions between the sub-
symbols grows as their number grows. As an exam-
ple of how staged training proceeds, Figure 2 shows
the evolution of the subsymbols of the determiner (DT)
tag, which first splits demonstratives from determiners,
then splits quantificational elements from demonstra-
tives along one branch and definites from indefinites
along the other.
5If our purpose was only to model language, as measured
for instance by perplexity on new text, it could make sense
to erase even the labels of the Penn Treebank to let EM find
better labels by itself, giving an experiment similar to that of
Pereira and Schabes (1992).
645 part of speech tags, 27 phrasal categories and the 26
intermediate symbols which were added during binarization
Because EM is a local search method, it is likely to
converge to different local maxima for different runs.
In our case, the variance is higher for models with few
subcategories; because not all dependencies can be ex-
pressed with the limited number of subcategories, the
results vary depending on which one EM selects first.
As the grammar size increases, the important depen-
dencies can be modeled, so the variance decreases.
2.3 Merging
It is clear from all previous work that creating more la-
tent annotations can increase accuracy. On the other
hand, oversplitting the grammar can be a serious prob-
lem, as detailed in Klein and Manning (2003). Adding
subsymbols divides grammar statistics into many bins,
resulting in a tighter fit to the training data. At the same
time, each bin gives a less robust estimate of the gram-
mar probabilities, leading to overfitting. Therefore, it
would be to our advantage to split the latent annota-
tions only where needed, rather than splitting them all
as in Matsuzaki et al (2005). In addition, if all sym-
bols are split equally often, one quickly (4 split cycles)
reaches the limits of what is computationally feasible
in terms of training time and memory usage.
Consider the comma POS tag. We would like to see
only one sort of this tag because, despite its frequency,
it always produces the terminal comma (barring a few
annotation errors in the treebank). On the other hand,
we would expect to find an advantage in distinguishing
between various verbal categories and NP types. Addi-
tionally, splitting symbols like the comma is not only
unnecessary, but potentially harmful, since it need-
lessly fragments observations of other symbols? behav-
ior.
It should be noted that simple frequency statistics are
not sufficient for determining how often to split each
symbol. Consider the closed part-of-speech classes
(e.g. DT, CC, IN) or the nonterminal ADJP. These
symbols are very common, and certainly do contain
subcategories, but there is little to be gained from
exhaustively splitting them before even beginning to
model the rarer symbols that describe the complex in-
ner correlations inside verb phrases. Our solution is
to use a split-and-merge approach broadly reminiscent
of ISODATA, a classic clustering procedure (Ball and
435
Hall, 1967).
To prevent oversplitting, we could measure the util-
ity of splitting each latent annotation individually and
then split the best ones first. However, not only is this
impractical, requiring an entire training phase for each
new split, but it assumes the contributions of multiple
splits are independent. In fact, extra subsymbols may
need to be added to several nonterminals before they
can cooperate to pass information along the parse tree.
Therefore, we go in the opposite direction; that is, we
split every symbol in two, train, and then measure for
each annotation the loss in likelihood incurred when
removing it. If this loss is small, the new annotation
does not carry enough useful information and can be
removed. What is more, contrary to the gain in like-
lihood for splitting, the loss in likelihood for merging
can be efficiently approximated.7
Let T be a training tree generating a sentence w.
Consider a node n of T spanning (r, t) with the label
A; that is, the subtree rooted at n generates wr:t and
has the label A. In the latent model, its label A is split
up into several latent labels, Ax. The likelihood of the
data can be recovered from the inside and outside prob-
abilities at n:
P(w, T ) =
?
x
PIN(r, t, Ax)POUT(r, t, Ax) (2)
Consider merging, at n only, two annotations A1 and
A2. Since A now combines the statistics of A1 and A2,
its production probabilities are the sum of those of A1
and A2, weighted by their relative frequency p1 and p2
in the training data. Therefore the inside score of A is:
PIN(r, t, A) = p1PIN(r, t, A1) + p2PIN(r, t, A2)
Since A can be produced as A1 or A2 by its parents, its
outside score is:
POUT(r, t, A) = POUT(r, t, A1) + POUT(r, t, A2)
Replacing these quantities in (2) gives us the likelihood
Pn(w, T ) where these two annotations and their corre-
sponding rules have been merged, around only node n.
We approximate the overall loss in data likelihood
due to merging A1 and A2 everywhere in all sentences
wi by the product of this loss for each local change:
?ANNOTATION (A1, A2) =
?
i
?
n?Ti
Pn(wi, Ti)
P(wi, Ti)
This expression is an approximation because it neglects
interactions between instances of a symbol at multiple
places in the same tree. These instances, however, are
7The idea of merging complex hypotheses to encourage
generalization is also examined in Stolcke and Omohundro
(1994), who used a chunking approach to propose new pro-
ductions in fully unsupervised grammar induction. They also
found it necessary to make local choices to guide their likeli-
hood search.
often far apart and are likely to interact only weakly,
and this simplification avoids the prohibitive cost of
running an inference algorithm for each tree and an-
notation. We refer to the operation of splitting anno-
tations and re-merging some them based on likelihood
loss as a split-merge (SM) cycle. SM cycles allow us to
progressively increase the complexity of our grammar,
giving priority to the most useful extensions.
In our experiments, merging was quite valuable. De-
pending on how many splits were reversed, we could
reduce the grammar size at the cost of little or no loss
of performance, or even a gain. We found that merging
50% of the newly split symbols dramatically reduced
the grammar size after each splitting round, so that af-
ter 6 SM cycles, the grammar was only 17% of the size
it would otherwise have been (1043 vs. 6273 subcat-
egories), while at the same time there was no loss in
accuracy (Figure 3). Actually, the accuracy even in-
creases, by 1.1% at 5 SM cycles. The numbers of splits
learned turned out to not be a direct function of symbol
frequency; the numbers of symbols for both lexical and
nonlexical tags after 4 SM cycles are given in Table 2.
Furthermore, merging makes large amounts of splitting
possible. It allows us to go from 4 splits, equivalent to
the 24 = 16 substates of Matsuzaki et al (2005), to 6
SM iterations, which take a few days to run on the Penn
Treebank.
2.4 Smoothing
Splitting nonterminals leads to a better fit to the data by
allowing each annotation to specialize in representing
only a fraction of the data. The smaller this fraction,
the higher the risk of overfitting. Merging, by allow-
ing only the most beneficial annotations, helps mitigate
this risk, but it is not the only way. We can further
minimize overfitting by forcing the production proba-
bilities from annotations of the same nonterminal to be
similar. For example, a noun phrase in subject position
certainly has a distinct distribution, but it may benefit
from being smoothed with counts from all other noun
phrases. Smoothing the productions of each subsym-
bol by shrinking them towards their common base sym-
bol gives us a more reliable estimate, allowing them to
share statistical strength.
We perform smoothing in a linear way. The es-
timated probability of a production px = P(Ax ?
By Cz) is interpolated with the average over all sub-
symbols of A.
p?x = (1 ? ?)px + ?p? where p? =
1
n
?
x
px
Here, ? is a small constant: we found 0.01 to be a good
value, but the actual quantity was surprisingly unimpor-
tant. Because smoothing is most necessary when pro-
duction statistics are least reliable, we expect smooth-
ing to help more with larger numbers of subsymbols.
This is exactly what we observe in Figure 3, where
smoothing initially hurts (subsymbols are quite distinct
436
and do not need their estimates pooled) but eventually
helps (as symbols have finer distinctions in behavior
and smaller data support).
2.5 Parsing
When parsing new sentences with an annotated gram-
mar, returning the most likely (unannotated) tree is in-
tractable: to obtain the probability of an unannotated
tree, one must sum over combinatorially many annota-
tion trees (derivations) for each tree (Sima?an, 1992).
Matsuzaki et al (2005) discuss two approximations.
The first is settling for the most probable derivation
rather than most probable parse, i.e. returning the single
most likely (Viterbi) annotated tree (derivation). This
approximation is justified if the sum is dominated by
one particular annotated tree. The second approxima-
tion that Matsuzaki et al (2005) present is the Viterbi
parse under a new sentence-specific PCFG, whose rule
probabilities are given as the solution of a variational
approximation of the original grammar. However, their
rule probabilities turn out to be the posterior probabil-
ity, given the sentence, of each rule being used at each
position in the tree. Their algorithm is therefore the la-
belled recall algorithm of Goodman (1996) but applied
to rules. That is, it returns the tree whose expected
number of correct rules is maximal. Thus, assuming
one is interested in a per-position score like F1 (which
is its own debate), this method of parsing is actually
more appropriate than finding the most likely parse,
not simply a cheap approximation of it, and it need not
be derived by a variational argument. We refer to this
method of parsing as the max-rule parser. Since this
method is not a contribution of this paper, we refer the
reader to the fuller presentations in Goodman (1996)
and Matsuzaki et al (2005). Note that contrary to the
original labelled recall algorithm, which maximizes the
number of correct symbols, this tree only contains rules
allowed by the grammar. As a result, the percentage of
complete matches with the max-rule parser is typically
higher than with the Viterbi parser. (37.5% vs. 35.8%
for our best grammar).
These posterior rule probabilities are still given by
(1), but, since the structure of the tree is no longer
known, we must sum over it when computing the in-
side and outside probabilities:
PIN(r, t, Ax)=
?
B,C,s
?
y,z
?(Ax ? ByCz)?
PIN(r, s, By)PIN(s, t, Cz)
POUT(r, s, By)=
?
A,C,t
?
x,z
?(Ax ? ByCz)?
POUT(r, t, Ax)PIN(s, t, Cz)
POUT(s, t, Cz)=
?
A,B,r
?
x,y
?(Ax ? ByCz)?
POUT(r, t, Ax)PIN(r, s, By)
For efficiency reasons, we use a coarse-to-fine prun-
ing scheme like that of Caraballo and Charniak (1998).
For a given sentence, we first run the inside-outside
algorithm using the baseline (unannotated) grammar,
 74
 76
 78
 80
 82
 84
 86
 88
 90
 200  400  600  800  1000
F1
Total number of grammar symbols
50% Merging and Smoothing
50% Merging
Splitting but no Merging
Flat Training
Figure 3: Hierarchical training leads to better parame-
ter estimates. Merging reduces the grammar size sig-
nificantly, while preserving the accuracy and enabling
us to do more SM cycles. Parameter smoothing leads
to even better accuracy for grammars with high com-
plexity.
producing a packed forest representation of the poste-
rior symbol probabilities for each span. For example,
one span might have a posterior probability of 0.8 of
the symbol NP, but e?10 for PP. Then, we parse with the
larger annotated grammar, but, at each span, we prune
away any symbols whose posterior probability under
the baseline grammar falls below a certain threshold
(e?8 in our experiments). Even though our baseline
grammar has a very low accuracy, we found that this
pruning barely impacts the performance of our better
grammars, while significantly reducing the computa-
tional cost. For a grammar with 479 subcategories (4
SM cycles), lowering the threshold to e?15 led to an F1
improvement of 0.13% (89.03 vs. 89.16) on the devel-
opment set but increased the parsing time by a factor of
16.
3 Analysis
So far, we have presented a split-merge method for
learning to iteratively subcategorize basic symbols
like NP and VP into automatically induced subsym-
bols (subcategories in the original sense of Chomsky
(1965)). This approach gives parsing accuracies of up
to 90.7% on the development set, substantially higher
than previous symbol-splitting approaches, while start-
ing from an extremely simple base grammar. However,
in general, any automatic induction system is in dan-
ger of being entirely uninterpretable. In this section,
we examine the learned grammars, discussing what is
learned. We focus particularly on connections with the
linguistically motivated annotations of Klein and Man-
ning (2003), which we do generally recover.
Inspecting a large grammar by hand is difficult, but
fortunately, our baseline grammar has less than 100
nonterminal symbols, and even our most complicated
grammar has only 1043 total (sub)symbols. It is there-
437
VBZ
VBZ-0 gives sells takes
VBZ-1 comes goes works
VBZ-2 includes owns is
VBZ-3 puts provides takes
VBZ-4 says adds Says
VBZ-5 believes means thinks
VBZ-6 expects makes calls
VBZ-7 plans expects wants
VBZ-8 is ?s gets
VBZ-9 ?s is remains
VBZ-10 has ?s is
VBZ-11 does Is Does
NNP
NNP-0 Jr. Goldman INC.
NNP-1 Bush Noriega Peters
NNP-2 J. E. L.
NNP-3 York Francisco Street
NNP-4 Inc Exchange Co
NNP-5 Inc. Corp. Co.
NNP-6 Stock Exchange York
NNP-7 Corp. Inc. Group
NNP-8 Congress Japan IBM
NNP-9 Friday September August
NNP-10 Shearson D. Ford
NNP-11 U.S. Treasury Senate
NNP-12 John Robert James
NNP-13 Mr. Ms. President
NNP-14 Oct. Nov. Sept.
NNP-15 New San Wall
JJS
JJS-0 largest latest biggest
JJS-1 least best worst
JJS-2 most Most least
DT
DT-0 the The a
DT-1 A An Another
DT-2 The No This
DT-3 The Some These
DT-4 all those some
DT-5 some these both
DT-6 That This each
DT-7 this that each
DT-8 the The a
DT-9 no any some
DT-10 an a the
DT-11 a this the
CD
CD-0 1 50 100
CD-1 8.50 15 1.2
CD-2 8 10 20
CD-3 1 30 31
CD-4 1989 1990 1988
CD-5 1988 1987 1990
CD-6 two three five
CD-7 one One Three
CD-8 12 34 14
CD-9 78 58 34
CD-10 one two three
CD-11 million billion trillion
PRP
PRP-0 It He I
PRP-1 it he they
PRP-2 it them him
RBR
RBR-0 further lower higher
RBR-1 more less More
RBR-2 earlier Earlier later
IN
IN-0 In With After
IN-1 In For At
IN-2 in for on
IN-3 of for on
IN-4 from on with
IN-5 at for by
IN-6 by in with
IN-7 for with on
IN-8 If While As
IN-9 because if while
IN-10 whether if That
IN-11 that like whether
IN-12 about over between
IN-13 as de Up
IN-14 than ago until
IN-15 out up down
RB
RB-0 recently previously still
RB-1 here back now
RB-2 very highly relatively
RB-3 so too as
RB-4 also now still
RB-5 however Now However
RB-6 much far enough
RB-7 even well then
RB-8 as about nearly
RB-9 only just almost
RB-10 ago earlier later
RB-11 rather instead because
RB-12 back close ahead
RB-13 up down off
RB-14 not Not maybe
RB-15 n?t not also
Table 1: The most frequent three words in the subcategories of several part-of-speech tags.
fore relatively straightforward to review the broad be-
havior of a grammar. In this section, we review a
randomly-selected grammar after 4 SM cycles that pro-
duced an F1 score on the development set of 89.11. We
feel it is reasonable to present only a single grammar
because all the grammars are very similar. For exam-
ple, after 4 SM cycles, the F1 scores of the 4 trained
grammars have a variance of only 0.024, which is tiny
compared to the deviation of 0.43 obtained by Mat-
suzaki et al (2005)). Furthermore, these grammars
allocate splits to nonterminals with a variance of only
0.32, so they agree to within a single latent state.
3.1 Lexical Splits
One of the original motivations for lexicalization of
parsers is the fact that part-of-speech (POS) tags are
usually far too general to encapsulate a word?s syntac-
tic behavior. In the limit, each word may well have
its own unique syntactic behavior, especially when, as
in modern parsers, semantic selectional preferences are
lumped in with traditional syntactic trends. However,
in practice, and given limited data, the relationship be-
tween specific words and their syntactic contexts may
be best modeled at a level more fine than POS tag but
less fine than lexical identity.
In our model, POS tags are split just like any other
grammar symbol: the subsymbols for several tags are
shown in Table 1, along with their most frequent mem-
bers. In most cases, the categories are recognizable as
either classic subcategories or an interpretable division
of some other kind.
Nominal categories are the most heavily split (see
Table 2), and have the splits which are most semantic
in nature (though not without syntactic correlations).
For example, plural common nouns (NNS) divide into
the maximum number of categories (16). One cate-
gory consists primarily of dates, whose typical parent
is an NP subsymbol whose typical parent is a root S,
essentially modeling the temporal noun annotation dis-
cussed in Klein and Manning (2003). Another cate-
gory specializes in capitalized words, preferring as a
parent an NP with an S parent (i.e. subject position).
A third category specializes in monetary units, and
so on. These kinds of syntactico-semantic categories
are typical, and, given distributional clustering results
like those of Schuetze (1998), unsurprising. The sin-
gular nouns are broadly similar, if slightly more ho-
mogenous, being dominated by categories for stocks
and trading. The proper noun category (NNP, shown)
also splits into the maximum 16 categories, including
months, countries, variants of Co. and Inc., first names,
last names, initials, and so on.
Verbal categories are also heavily split. Verbal sub-
categories sometimes reflect syntactic selectional pref-
erences, sometimes reflect semantic selectional prefer-
ences, and sometimes reflect other aspects of verbal
syntax. For example, the present tense third person
verb subsymbols (VBZ) are shown. The auxiliaries get
three clear categories: do, have, and be (this pattern
repeats in other tenses), as well a fourth category for
the ambiguous ?s. Verbs of communication (says) and
438
NNP 62 CC 7 WP$ 2 NP 37 CONJP 2
JJ 58 JJR 5 WDT 2 VP 32 FRAG 2
NNS 57 JJS 5 -RRB- 2 PP 28 NAC 2
NN 56 : 5 ? 1 ADVP 22 UCP 2
VBN 49 PRP 4 FW 1 S 21 WHADVP 2
RB 47 PRP$ 4 RBS 1 ADJP 19 INTJ 1
VBG 40 MD 3 TO 1 SBAR 15 SBARQ 1
VB 37 RBR 3 $ 1 QP 9 RRC 1
VBD 36 WP 2 UH 1 WHNP 5 WHADJP 1
CD 32 POS 2 , 1 PRN 4 X 1
IN 27 PDT 2 ? 1 NX 4 ROOT 1
VBZ 25 WRB 2 SYM 1 SINV 3 LST 1
VBP 19 -LRB- 2 RP 1 PRT 2
DT 17 . 2 LS 1 WHPP 2
NNPS 11 EX 2 # 1 SQ 2
Table 2: Number of latent annotations determined by
our split-merge procedure after 6 SM cycles
propositional attitudes (beleives) that tend to take in-
flected sentential complements dominate two classes,
while control verbs (wants) fill out another.
As an example of a less-split category, the superla-
tive adjectives (JJS) are split into three categories,
corresponding principally to most, least, and largest,
with most frequent parents NP, QP, and ADVP, respec-
tively. The relative adjectives (JJR) are split in the same
way. Relative adverbs (RBR) are split into a different
three categories, corresponding to (usually metaphor-
ical) distance (further), degree (more), and time (ear-
lier). Personal pronouns (PRP) are well-divided into
three categories, roughly: nominative case, accusative
case, and sentence-initial nominative case, which each
correlate very strongly with syntactic position. As an-
other example of a specific trend which was mentioned
by Klein and Manning (2003), adverbs (RB) do contain
splits for adverbs under ADVPs (also), NPs (only), and
VPs (not).
Functional categories generally show fewer splits,
but those splits that they do exhibit are known to be
strongly correlated with syntactic behavior. For exam-
ple, determiners (DT) divide along several axes: defi-
nite (the), indefinite (a), demonstrative (this), quantifi-
cational (some), negative polarity (no, any), and var-
ious upper- and lower-case distinctions inside these
types. Here, it is interesting to note that these distinc-
tions emerge in a predictable order (see Figure 2 for DT
splits), beginning with the distinction between demon-
stratives and non-demonstratives, with the other dis-
tinctions emerging subsequently; this echoes the result
of Klein and Manning (2003), where the authors chose
to distinguish the demonstrative constrast, but not the
additional ones learned here.
Another very important distinction, as shown in
Klein and Manning (2003), is the various subdivi-
sions in the preposition class (IN). Learned first is
the split between subordinating conjunctions like that
and proper prepositions. Then, subdivisions of each
emerge: wh-subordinators like if, noun-modifying
prepositions like of, predominantly verb-modifying
ones like from, and so on.
Many other interesting patterns emerge, including
ADVP
ADVP-0 RB-13 NP-2 RB-13 PP-3 IN-15 NP-2
ADVP-1 NP-3 RB-10 NP-3 RBR-2 NP-3 IN-14
ADVP-2 IN-5 JJS-1 RB-8 RB-6 RB-6 RBR-1
ADVP-3 RBR-0 RB-12 PP-0 RP-0
ADVP-4 RB-3 RB-6 ADVP-2 SBAR-8 ADVP-2 PP-5
ADVP-5 RB-5 NP-3 RB-10 RB-0
ADVP-6 RB-4 RB-0 RB-3 RB-6
ADVP-7 RB-7 IN-5 JJS-1 RB-6
ADVP-8 RB-0 RBS-0 RBR-1 IN-14
ADVP-9 RB-1 IN-15 RBR-0
SINV
SINV-0 VP-14 NP-7 VP-14 VP-15 NP-7 NP-9
VP-14 NP-7 .-0
SINV-1 S-6 ,-0 VP-14 NP-7 .-0
S-11 VP-14 NP-7 .-0
Table 3: The most frequent three productions of some
latent annotations.
many classical distinctions not specifically mentioned
or modeled in previous work. For example, the wh-
determiners (WDT) split into one class for that and an-
other for which, while the wh-adverbs align by refer-
ence type: event-based how and why vs. entity-based
when and where. The possesive particle (POS) has one
class for the standard ?s, but another for the plural-only
apostrophe. As a final example, the cardinal number
nonterminal (CD) induces various categories for dates,
fractions, spelled-out numbers, large (usually financial)
digit sequences, and others.
3.2 Phrasal Splits
Analyzing the splits of phrasal nonterminals is more
difficult than for lexical categories, and we can merely
give illustrations. We show some of the top productions
of two categories in Table 3.
A nonterminal split can be used to model an other-
wise uncaptured correlation between that symbol?s ex-
ternal context (e.g. its parent symbol) and its internal
context (e.g. its child symbols). A particularly clean ex-
ample of a split correlating external with internal con-
texts is the inverted sentence category (SINV), which
has only two subsymbols, one which usually has the
ROOT symbol as its parent (and which has sentence fi-
nal puncutation as its last child), and a second subsym-
bol which occurs in embedded contexts (and does not
end in punctuation). Such patterns are common, but of-
ten less easy to predict. For example, possesive NPs get
two subsymbols, depending on whether their possessor
is a person / country or an organization. The external
correlation turns out to be that people and countries are
more likely to possess a subject NP, while organizations
are more likely to possess an object NP.
Nonterminal splits can also be used to relay infor-
mation between distant tree nodes, though untangling
this kind of propagation and distilling it into clean ex-
amples is not trivial. As one example, the subsym-
bol S-12 (matrix clauses) occurs only under the ROOT
symbol. S-12?s children usually include NP-8, which
in turn usually includes PRP-0, the capitalized nomi-
native pronouns, DT-{1,2,6} (the capitalized determin-
439
ers), and so on. This same propagation occurs even
more frequently in the intermediate symbols, with, for
example, one subsymbol of NP symbol specializing in
propagating proper noun sequences.
Verb phrases, unsurprisingly, also receive a full set
of subsymbols, including categories for infinitive VPs,
passive VPs, several for intransitive VPs, several for
transitive VPs with NP and PP objects, and one for
sentential complements. As an example of how lexi-
cal splits can interact with phrasal splits, the two most
frequent rewrites involving intransitive past tense verbs
(VBD) involve two different VPs and VBDs: VP-14 ?
VBD-13 and VP-15 ? VBD-12. The difference is that
VP-14s are main clause VPs, while VP-15s are sub-
ordinate clause VPs. Correspondingly, VBD-13s are
verbs of communication (said, reported), while VBD-
12s are an assortment of verbs which often appear in
subordinate contexts (did, began).
Other interesting phenomena also emerge. For ex-
ample, intermediate symbols, which in previous work
were very heavily, manually split using a Markov pro-
cess, end up encoding processes which are largely
Markov, but more complex. For example, some classes
of adverb phrases (those with RB-4 as their head) are
?forgotten? by the VP intermediate grammar. The rele-
vant rule is the very probable VP-2 ? VP-2 ADVP-6;
adding this ADVP to a growing VP does not change the
VP subsymbol. In essense, at least a partial distinction
between verbal arguments and verbal adjucts has been
learned (as exploited in Collins (1999), for example).
4 Conclusions
By using a split-and-merge strategy and beginning with
the barest possible initial structure, our method reli-
ably learns a PCFG that is remarkably good at pars-
ing. Hierarchical split/merge training enables us to
learn compact but accurate grammars, ranging from ex-
tremely compact (an F1 of 78% with only 147 sym-
bols) to extremely accurate (an F1 of 90.2% for our
largest grammar with only 1043 symbols). Splitting
provides a tight fit to the training data, while merging
improves generalization and controls grammar size. In
order to overcome data fragmentation and overfitting,
we smooth our parameters. Smoothing allows us to
add a larger number of annotations, each specializing
in only a fraction of the data, without overfitting our
training set. As one can see in Table 4, the resulting
parser ranks among the best lexicalized parsers, beat-
ing those of Collins (1999) and Charniak and Johnson
(2005).8 Its F1 performance is a 27% reduction in er-
ror over Matsuzaki et al (2005) and Klein and Man-
ning (2003). Not only is our parser more accurate, but
the learned grammar is also significantly smaller than
that of previous work. While this all is accomplished
with only automatic learning, the resulting grammar is
8Even with the Viterbi parser our best grammar achieves
88.7/88.9 LP/LR.
? 40 words LP LR CB 0CB
Klein and Manning (2003) 86.9 85.7 1.10 60.3
Matsuzaki et al (2005) 86.6 86.7 1.19 61.1
Collins (1999) 88.7 88.5 0.92 66.7
Charniak and Johnson (2005) 90.1 90.1 0.74 70.1
This Paper 90.3 90.0 0.78 68.5
all sentences LP LR CB 0CB
Klein and Manning (2003) 86.3 85.1 1.31 57.2
Matsuzaki et al (2005) 86.1 86.0 1.39 58.3
Collins (1999) 88.3 88.1 1.06 64.0
Charniak and Johnson (2005) 89.5 89.6 0.88 67.6
This Paper 89.8 89.6 0.92 66.3
Table 4: Comparison of our results with those of others.
human-interpretable. It shows most of the manually in-
troduced annotations discussed by Klein and Manning
(2003), but also learns other linguistic phenomena.
References
G. Ball and D. Hall. 1967. A clustering technique for sum-
marizing multivariate data. Behavioral Science.
S. Caraballo and E. Charniak. 1998. New figures of merit
for best?first probabilistic chart parsing. In Computational
Lingusitics, p. 275?298.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In ACL?05,
p. 173?180.
E. Charniak. 1996. Tree-bank grammars. In AAAI ?96, p.
1031?1036.
E. Charniak. 2000. A maximum?entropy?inspired parser. In
NAACL ?00, p. 132?139.
D. Chiang and D. Bikel. 2002. Recovering latent information
in treebanks. In Computational Linguistics.
N. Chomsky. 1965. Aspects of the Theory of Syntax. MIT
Press.
M. Collins. 1999. Head-Driven Statistical Models for Natu-
ral Language Parsing. Ph.D. thesis, U. of Pennsylvania.
J. Goodman. 1996. Parsing algorithms and metrics. In ACL
?96, p. 177?183.
J. Henderson. 2004. Discriminative training of a neural net-
work statistical parser. In ACL ?04.
M. Johnson. 1998. PCFG models of linguistic tree represen-
tations. Computational Linguistics, 24:613?632.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. ACL ?03, p. 423?430.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic
CFG with latent annotations. In ACL ?05, p. 75?82.
F. Pereira and Y. Schabes. 1992. Inside-outside reestimation
from partially bracketed corpora. In ACL ?92, p. 128?135.
D. Prescher. 2005. Inducing head-driven PCFGs with la-
tent heads: Refining a tree-bank grammar for parsing. In
ECML?05.
H. Schuetze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
S. Sekine and M. J. Collins. 1997. EVALB bracket scoring
program. http://nlp.cs.nyu.edu/evalb/.
K. Sima?an. 1992. Computatoinal complexity of probabilis-
tic disambiguation. Grammars, 5:125?151.
A. Stolcke and S. Omohundro. 1994. Inducing probabilistic
grammars by bayesian model merging. In Grammatical
Inference and Applications, p. 106?118.
440
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 14?20, New York City, June 2006. c?2006 Association for Computational Linguistics
Non-Local Modeling with a Mixture of PCFGs
Slav Petrov Leon Barrett Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov, lbarrett, klein}@eecs.berkeley.edu
Abstract
While most work on parsing with PCFGs
has focused on local correlations between
tree configurations, we attempt to model
non-local correlations using a finite mix-
ture of PCFGs. A mixture grammar fit
with the EM algorithm shows improve-
ment over a single PCFG, both in parsing
accuracy and in test data likelihood. We
argue that this improvement comes from
the learning of specialized grammars that
capture non-local correlations.
1 Introduction
The probabilistic context-free grammar (PCFG) for-
malism is the basis of most modern statistical
parsers. The symbols in a PCFG encode context-
freedom assumptions about statistical dependencies
in the derivations of sentences, and the relative con-
ditional probabilities of the grammar rules induce
scores on trees. Compared to a basic treebank
grammar (Charniak, 1996), the grammars of high-
accuracy parsers weaken independence assumptions
by splitting grammar symbols and rules with ei-
ther lexical (Charniak, 2000; Collins, 1999) or non-
lexical (Klein and Manning, 2003; Matsuzaki et al,
2005) conditioning information. While such split-
ting, or conditioning, can cause problems for sta-
tistical estimation, it can dramatically improve the
accuracy of a parser.
However, the configurations exploited in PCFG
parsers are quite local: rules? probabilities may de-
pend on parents or head words, but do not depend
on arbitrarily distant tree configurations. For exam-
ple, it is generally not modeled that if one quantifier
phrase (QP in the Penn Treebank) appears in a sen-
tence, the likelihood of finding another QP in that
same sentence is greatly increased. This kind of ef-
fect is neither surprising nor unknown ? for exam-
ple, Bock and Loebell (1990) show experimentally
that human language generation demonstrates prim-
ing effects. The mediating variables can not only in-
clude priming effects but also genre or stylistic con-
ventions, as well as many other factors which are not
adequately modeled by local phrase structure.
A reasonable way to add a latent variable to a
generative model is to use a mixture of estimators,
in this case a mixture of PCFGs (see Section 3).
The general mixture of estimators approach was first
suggested in the statistics literature by Titterington
et al (1962) and has since been adopted in machine
learning (Ghahramani and Jordan, 1994). In a mix-
ture approach, we have a new global variable on
which all PCFG productions for a given sentence
can be conditioned. In this paper, we experiment
with a finite mixture of PCFGs. This is similar to the
latent nonterminals used in Matsuzaki et al (2005),
but because the latent variable we use is global, our
approach is more oriented toward learning non-local
structure. We demonstrate that a mixture fit with the
EM algorithm gives improved parsing accuracy and
test data likelihood. We then investigate what is and
is not being learned by the latent mixture variable.
While mixture components are difficult to interpret,
we demonstrate that the patterns learned are better
than random splits.
2 Empirical Motivation
It is commonly accepted that the context freedom
assumptions underlying the PCFG model are too
14
VP
VBD
increased
NP
CD
11
NN
%
PP
TO
to
NP
QP
#
#
CD
2.5
CD
billion
PP
IN
from
NP
QP
#
#
CD
2.25
CD
billion
Rule Score
QP? # CD CD 131.6
PRN? -LRB- ADJP -RRB 77.1
VP? VBD NP , PP PP 33.7
VP? VBD NP NP PP 28.4
PRN? -LRB- NP -RRB- 17.3
ADJP? QP 13.3
PP? IN NP ADVP 12.3
NP? NP PRN 12.3
VP? VBN PP PP PP 11.6
ADVP? NP RBR 10.1
Figure 1: Self-triggering: QP? # CD CD. If one British financial occurs in the sentence, the probability of
seeing a second one in the same sentence is highly inreased. There is also a similar, but weaker, correlation
for the American financial ($). On the right hand side we show the ten rules whose likelihoods are most
increased in a sentence containing this rule.
strong and that weakening them results in better
models of language (Johnson, 1998; Gildea, 2001;
Klein and Manning, 2003). In particular, certain
grammar productions often cooccur with other pro-
ductions, which may be either near or distant in the
parse tree. In general, there exist three types of cor-
relations: (i) local (e.g. parent-child), (ii) non-local,
and (iii) self correlations (which may be local or
non-local).
In order to quantify the strength of a correlation,
we use a likelihood ratio (LR). For two rules X? ?
and Y? ?, we compute
LR(X? ?, Y? ?) = P(?, ?|X,Y )P(?|X,Y )P(?|X,Y )
This measures how much more often the rules oc-
cur together than they would in the case of indepen-
dence. For rules that are correlated, this score will
be high (? 1); if the rules are independent, it will
be around 1, and if they are anti-correlated, it will be
near 0.
Among the correlations present in the Penn Tree-
bank, the local correlations are the strongest ones;
they contribute 65% of the rule pairs with LR scores
above 90 and 85% of those with scores over 200.
Non-local and self correlations are in general com-
mon but weaker, with non-local correlations con-
tributing approximately 85% of all correlations1 . By
adding a latent variable conditioning all productions,
1Quantifying the amount of non-local correlation is prob-
lematic; most pairs of cooccuring rules are non-local and will,
due to small sample effects, have LR ratios greater than 1 even
if they were truly independent in the limit.
we aim to capture some of this interdependence be-
tween rules.
Correlations at short distances have been cap-
tured effectively in previous work (Johnson, 1998;
Klein and Manning, 2003); vertical markovization
(annotating nonterminals with their ancestor sym-
bols) does this by simply producing a different dis-
tribution for each set of ancestors. This added con-
text leads to substantial improvement in parsing ac-
curacy. With local correlations already well cap-
tured, our main motivation for introducing a mix-
ture of grammars is to capture long-range rule cooc-
currences, something that to our knowledge has not
been done successfully in the past.
As an example, the rule QP? # CD CD, rep-
resenting a quantity of British currency, cooc-
curs with itself 132 times as often as if oc-
currences were independent. These cooccur-
rences appear in cases such as seen in Figure 1.
Similarly, the rules VP? VBD NP PP , S and
VP? VBG NP PP PP cooccur in the Penn Tree-
bank 100 times as often as we would expect if they
were independent. They appear in sentences of a
very particular form, telling of an action and then
giving detail about it; an example can be seen in Fig-
ure 2.
3 Mixtures of PCFGs
In a probabilistic context-free grammar (PCFG),
each rule X? ? is associated with a conditional
probability P(?|X) (Manning and Schu?tze, 1999).
Together, these rules induce a distribution over trees
P(T ). A mixture of PCFGs enriches the basic model
15
VP
VBD
hit
NP
a record
PP
in 1998
,
,
S
VP
VBG
rising
NP
1.7%
PP
after inflation adjustment
PP
to $13,120
S
NP
DT
No
NX
NX
NNS
lawyers
CC
or
NX
NN
tape
NNS
recorders
VP
were present
.
.
(a) (b)
S
S
NP
DT
These
NN
rate
NNS
indications
VP
VBP
are
RB
n?t
ADJP
directly comparable
:
;
S
NP
NN
lending
NNS
practices
VP
VBP
vary
ADVP
widely
PP
by location
.
.
X
X
SYM
**
ADJP
VBN
Projected
(c) (d)
Figure 2: Tree fragments demonstrating coocurrences. (a) and (c) Repeated formulaic structure in one
grammar: rules VP? VBD NP PP , S and VP? VBG NP PP PP and rules VP? VBP RB ADJP
and VP? VBP ADVP PP. (b) Sibling effects, though not parallel structure, rules: NX? NNS and
NX? NN NNS. (d) A special structure for footnotes has rules ROOT? X and X? SYM coocurring
with high probability.
by allowing for multiple grammars, Gi, which we
call individual grammars, as opposed to a single
grammar. Without loss of generality, we can as-
sume that the individual grammars share the same
set of rules. Therefore, each original rule X? ?
is now associated with a vector of probabilities,
P(?|X, i). If, in addition, the individual grammars
are assigned prior probabilities P(i), then the entire
mixture induces a joint distribution over derivations
P(T, i) = P(i)P(T |i) from which we recover a dis-
tribution over trees by summing over the grammar
index i.
As a generative derivation process, we can think
of this in two ways. First, we can imagine G to be
a latent variable on which all productions are con-
ditioned. This view emphasizes that any otherwise
unmodeled variable or variables can be captured by
the latent variable G. Second, we can imagine se-
lecting an individual grammar Gi and then gener-
ating a sentence using that grammar. This view is
associated with the expectation that there are multi-
ple grammars for a language, perhaps representing
different genres or styles. Formally, of course, the
two views are the same.
3.1 Hierarchical Estimation
So far, there is nothing in the formal mixture model
to say that rule probabilities in one component have
any relation to those in other components. However,
we have a strong intuition that many rules, such as
NP? DT NN, will be common in all mixture com-
ponents. Moreover, we would like to pool our data
across components when appropriate to obtain more
reliable estimators.
This can be accomplished with a hierarchical es-
timator for the rule probabilities. We introduce a
shared grammar Gs. Associated to each rewrite is
now a latent variable L = {S, I} which indicates
whether the used rule was derived from the shared
grammar Gs or one of the individual grammars Gi:
P(?|X, i) =
?P(?|X, i, ?= I) + (1? ?)P(?|X, i, ?= S),
where ? ? P (? = I) is the probability of
choosing the individual grammar and can also
be viewed as a mixing coefficient. Note that
P(?|X, i, ?= S) = P(?|X, ?= S), since the shared
grammar is the same for all individual grammars.
This kind of hierarchical estimation is analogous to
that used in hierarchical mixtures of naive-Bayes for
16
text categorization (McCallum et al, 1998).
The hierarchical estimator is most easily de-
scribed as a generative model. First, we choose a
individual grammar Gi. Then, for each nonterminal,
we select a level from the back-off hierarchy gram-
mar: the individual grammar Gi with probability ?,
and the shared grammar Gs with probability 1 ? ?.
Finally, we select a rewrite from the chosen level. To
emphasize: the derivation of a phrase-structure tree
in a hierarchically-estimated mixture of PCFGs in-
volves two kinds of hidden variables: the grammar
G used for each sentence, and the level L used at
each tree node. These hidden variables will impact
both learning and inference in this model.
3.2 Inference: Parsing
Parsing involves inference for a given sentence S.
One would generally like to calculate the most prob-
able parse ? that is, the tree T which has the high-
est probability P(T |S) ??i P(i)P(T |i). How-
ever, this is difficult for mixture models. For a single
grammar we have:
P(T, i) = P(i)
?
X???T
P(?|X, i).
This score decomposes into a product and it is sim-
ple to construct a dynamic programming algorithm
to find the optimal T (Baker, 1979). However, for a
mixture of grammars we need to sum over the indi-
vidual grammars:
?
i
P(T, i) =
?
i
P(i)
?
X???T
P(?|X, i).
Because of the outer sum, this expression unfor-
tunately does not decompose into a product over
scores of subparts. In particular, a tree which maxi-
mizes the sum need not be a top tree for any single
component.
As is true for many other grammar formalisms in
which there is a derivation / parse distinction, an al-
ternative to finding the most probable parse is to find
the most probable derivation (Vijay-Shankar and
Joshi, 1985; Bod, 1992; Steedman, 2000). Instead
of finding the tree T which maximizes
?
i P(T, i),
we find both the tree T and component i which max-
imize P(T, i). The most probable derivation can be
found by simply doing standard PCFG parsing once
for each component, then comparing the resulting
trees? likelihoods.
3.3 Learning: Training
Training a mixture of PCFGs from a treebank is an
incomplete data problem. We need to decide which
individual grammar gave rise to a given observed
tree. Moreover, we need to select a generation path
(individual grammar or shared grammar) for each
rule in the tree. To learn estimate parameters, we
can use a standard Expectation-Maximization (EM)
approach.
In the E-step, we compute the posterior distribu-
tions of the latent variables, which are in this case
both the component G of each sentence and the hier-
archy level L of each rewrite. Note that, unlike dur-
ing parsing, there is no uncertainty over the actual
rules used, so the E-step does not require summing
over possible trees. Specifically, for the variable G
we have
P(i|T ) = P(T, i)?
j P(T, j)
.
For the hierarchy level L we can write
P(? = I|X ? ?, i, T ) =
?P(?|X, ?= I)
?P(?|X, i, ?= I) + (1? ?)P(?|X, ?= S) ,
where we slightly abuse notation since the rule
X ? ? can occur multiple times in a tree T.
In the M-step, we find the maximum-likelihood
model parameters given these posterior assign-
ments; i.e., we find the best grammars given the way
the training data?s rules are distributed between in-
dividual and shared grammars. This is done exactly
as in the standard single-grammar model using rela-
tive expected frequencies. The updates are shown in
Figure 3.3, where T = {T1, T2, . . . } is the training
set.
We initialize the algorithm by setting the assign-
ments from sentences to grammars to be uniform
between all the individual grammars, with a small
random perturbation to break symmetry.
4 Results
We ran our experiments on the Wall Street Jour-
nal (WSJ) portion of the Penn Treebank using the
standard setup: We trained on sections 2 to 21,
and we used section 22 as a validation set for tun-
ing model hyperparameters. Results are reported
17
P(i)?
?
Tk?T P(i|Tk)
?
i
?
Tk?T P(i|Tk)
=
P
Tk?T
P(i|Tk)
k
P(l = I)?
?
Tk?T
?
X???Tk P(? = I|X ? ?)
?
Tk?T |Tk|
P(?|X, i, ? = I)?
?
Tk?T
?
X???Tk P(i|Tk)P(? = I|Tk, i,X ? ?)
?
??
?
Tk?T
?
X????Tk P(i|Tk)P(? = I|Tk, i,X ? ??)
Figure 3: Parameter updates. The shared grammar?s parameters are re-estimated in the same manner.
on all sentences of 40 words or less from section
23. We use a markovized grammar which was an-
notated with parent and sibling information as a
baseline (see Section 4.2). Unsmoothed maximum-
likelihood estimates were used for rule probabili-
ties as in Charniak (1996). For the tagging proba-
bilities, we used maximum-likelihood estimates for
P(tag|word). Add-one smoothing was applied to
unknown and rare (seen ten times or less during
training) words before inverting those estimates to
give P(word|tag). Parsing was done with a sim-
ple Java implementation of an agenda-based chart
parser.
4.1 Parsing Accuracy
The EM algorithm is guaranteed to continuously in-
crease the likelihood on the training set until conver-
gence to a local maximum. However, the likelihood
on unseen data will start decreasing after a number
of iterations, due to overfitting. This is demonstrated
in Figure 4. We use the likelihood on the validation
set to stop training before overfitting occurs.
In order to evaluate the performance of our model,
we trained mixture grammars with various numbers
of components. For each configuration, we used EM
to obtain twelve estimates, each time with a different
random initialization. We show the F1-score for the
model with highest log-likelihood on the validation
set in Figure 4. The results show that a mixture of
grammars outperforms a standard, single grammar
PCFG parser.2
4.2 Capturing Rule Correlations
As described in Section 2, we hope that the mix-
ture model will capture long-range correlations in
2This effect is statistically significant.
the data. Since local correlations can be captured
by adding parent annotation, we combine our mix-
ture model with a grammar in which node probabil-
ities depend on the parent (the last vertical ancestor)
and the closest sibling (the last horizontal ancestor).
Klein and Manning (2003) refer to this grammar as
a markovized grammar of vertical order = 2 and hor-
izontal order = 1. Because many local correlations
are captured by the markovized grammar, there is a
greater hope that observed improvements stem from
non-local correlations.
In fact, we find that the mixture does capture
non-local correlations. We measure the degree to
which a grammar captures correlations by calculat-
ing the total squared error between LR scores of the
grammar and corpus, weighted by the probability
of seeing nonterminals. This is 39422 for a sin-
gle PCFG, but drops to 37125 for a mixture with
five individual grammars, indicating that the mix-
ture model better captures the correlations present
in the corpus. As a concrete example, in the Penn
Treebank, we often see the rules FRAG? ADJP
and PRN? , SBAR , cooccurring; their LR is 134.
When we learn a single markovized PCFG from the
treebank, that grammar gives a likelihood ratio of
only 61. However, when we train with a hierarchi-
cal model composed of a shared grammar and four
individual grammars, we find that the grammar like-
lihood ratio for these rules goes up to 126, which is
very similar to that of the empirical ratio.
4.3 Genre
The mixture of grammars model can equivalently be
viewed as capturing either non-local correlations or
variations in grammar. The latter view suggests that
the model might benefit when the syntactic structure
18
 0  10  20  30  40  50  60
Lo
g 
Li
ke
lih
oo
d
Iteration
Training data
Validation data
Testing data
 79
 79.2
 79.4
 79.6
 79.8
 80
 1  2  3  4  5  6  7  8  9
F1
Number of Component Grammars
Mixture model
Baseline: 1 grammar
(a) (b)
Figure 4: (a) Log likelihood of training, validation, and test data during training (transformed to fit on the
same plot). Note that when overfitting occurs the likelihood on the validation and test data starts decreasing
(after 13 iterations). (b) The accuracy of the mixture of grammars model with ? = 0.4 versus the number of
grammars. Note the improvement over a 1-grammar PCFG model.
varies significantly, as between different genres. We
tested this with the Brown corpus, of which we used
8 different genres (f, g, k, l, m, n, p, and r). We fol-
low Gildea (2001) in using the ninth and tenth sen-
tences of every block of ten as validation and test
data, respectively, because a contiguous test section
might not be representative due to the genre varia-
tion.
To test the effects of genre variation, we evalu-
ated various training schemes on the Brown corpus.
The single grammar baseline for this corpus gives
F1 = 79.75, with log likelihood (LL) on the testing
data=-242561. The first test, then, was to estimate
each individual grammar from only one genre. We
did this by assigning sentences to individual gram-
mars by genre, without using any EM training. This
increases the data likelihood, though it reduces the
F1 score (F1 = 79.48, LL=-242332). The increase
in likelihood indicates that there are genre-specific
features that our model can represent. (The lack of
F1 improvement may be attributed to the increased
difficulty of estimating rule probabilities after divid-
ing the already scant data available in the Brown cor-
pus. This small quantity of data makes overfitting
almost certain.)
However, local minima and lack of data cause dif-
ficulty in learning genre-specific features. If we start
with sentences assigned by genre as before, but then
train with EM, both F1 and test data log likelihood
drop (F1 = 79.37, LL=-242100). When we use
EM with a random initialization, so that sentences
are not assigned directly to grammars, the scores go
down even further (F1 = 79.16, LL=-242459). This
indicates that the model can capture variation be-
tween genres, but that maximum training data likeli-
hood does not necessarily give maximum accuracy.
Presumably, with more genre-specific data avail-
able, learning would generalize better. So, genre-
specific grammar variation is real, but it is difficult
to capture via EM.
4.4 Smoothing Effects
While the mixture of grammars captures rule cor-
relations, it may also enhance performance via
smoothing effects. Splitting the data randomly could
produce a smoothed shared grammar, Gs, that is
a kind of held-out estimate which could be supe-
rior to the unsmoothed ML estimates for the single-
component grammar.
We tested the degree of generalization by eval-
uating the shared grammar alone and also a mix-
ture of the shared grammar with the known sin-
gle grammar. Those shared grammars were ex-
tracted after training the mixture model with four in-
dividual grammars. We found that both the shared
grammar alone (F1=79.13, LL=-333278) and the
shared grammar mixed with the single grammar
(F1=79.36, LL=-331546) perform worse than a sin-
19
gle PCFG (F1=79.37, LL=-327658). This indicates
that smoothing is not the primary learning effect
contributing to increased F1.
5 Conclusions
We examined the sorts of rule correlations that may
be found in natural language corpora, discovering
non-local correlations not captured by traditional
models. We found that using a model capable of
representing these non-local features gives improve-
ment in parsing accuracy and data likelihood. This
improvement is modest, however, primarily because
local correlations are so much stronger than non-
local ones.
References
J. Baker. 1979. Trainable grammars for speech recog-
nition. Speech Communication Papers for the 97th
Meeting of the Acoustical Society of America, pages
547?550.
K. Bock and H. Loebell. 1990. Framing sentences. Cog-
nition, 35:1?39.
R. Bod. 1992. A computational model of language per-
formance: Data oriented parsing. International Con-
ference on Computational Linguistics (COLING).
E. Charniak. 1996. Tree-bank grammars. In Proc. of
the 13th National Conference on Artificial Intelligence
(AAAI), pages 1031?1036.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In Proc. of the Conference of the North Ameri-
can chapter of the Association for Computational Lin-
guistics (NAACL), pages 132?139.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, Univ. of
Pennsylvania.
Z. Ghahramani and M. I. Jordan. 1994. Supervised
learning from incomplete data via an EM approach. In
Advances in Neural Information Processing Systems
(NIPS), pages 120?127.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. Conference on Empirical Methods in Natural
Language Processing (EMNLP).
M. Johnson. 1998. Pcfg models of linguistic tree repre-
sentations. Computational Linguistics, 24:613?632.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. Proc. of the 41st Meeting of the Association
for Computational Linguistics (ACL), pages 423?430.
C. Manning and H. Schu?tze. 1999. Foundations of Sta-
tistical Natural Language Processing. The MIT Press,
Cambridge, Massachusetts.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proc. of the 43rd
Meeting of the Association for Computational Linguis-
tics (ACL), pages 75?82.
A. McCallum, R. Rosenfeld, T. Mitchell, and A. Ng.
1998. Improving text classification by shrinkage in a
hierarchy of classes. In Int. Conf. on Machine Learn-
ing (ICML), pages 359?367.
M. Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, Massachusetts.
D. Titterington, A. Smith, and U. Makov. 1962. Statisti-
cal Analysis of Finite Mixture Distributions. Wiley.
K. Vijay-Shankar and A. Joshi. 1985. Some computa-
tional properties of tree adjoining grammars. Proc. of
the 23th Meeting of the Association for Computational
Linguistics (ACL), pages 82?93.
20
Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 33?39,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Parsing German with Latent Variable Grammars
Slav Petrov and Dan Klein
{petrov,klein}@cs.berkeley.edu
University of California at Berkeley
Berkeley, CA 94720
Abstract
We describe experiments on learning latent
variable grammars for various German tree-
banks, using a language-agnostic statistical
approach. In our method, a minimal ini-
tial grammar is hierarchically refined using an
adaptive split-and-merge EM procedure, giv-
ing compact, accurate grammars. The learn-
ing procedure directly maximizes the likeli-
hood of the training treebank, without the use
of any language specific or linguistically con-
strained features. Nonetheless, the resulting
grammars encode many linguistically inter-
pretable patterns and give the best published
parsing accuracies on three German treebanks.
1 Introduction
Probabilistic context-free grammars (PCFGs) under-
lie most high-performance parsers in one way or an-
other (Collins, 1999; Charniak, 2000; Charniak and
Johnson, 2005). However, as demonstrated in Char-
niak (1996) and Klein and Manning (2003), a PCFG
which simply takes the empirical rules and probabil-
ities off of a treebank does not perform well. This
naive grammar is a poor one because its context-
freedom assumptions are too strong in some ways
(e.g. it assumes that subject and object NPs share
the same distribution) and too weak in others (e.g.
it assumes that long rewrites do not decompose into
smaller steps). Therefore, a variety of techniques
have been developed to both enrich and generalize
the naive grammar, ranging from simple tree anno-
tation and symbol splitting (Johnson, 1998; Klein
and Manning, 2003) to full lexicalization and intri-
cate smoothing (Collins, 1999; Charniak, 2000).
We view treebank parsing as the search for an
optimally refined grammar consistent with a coarse
training treebank. As a result, we begin with the
provided evaluation symbols (such as NP, VP, etc.)
but split them based on the statistical patterns in
the training trees. A manual approach might take
the symbol NP and subdivide it into one subsymbol
NP?S for subjects and another subsymbol NP?VP
for objects. However, rather than devising linguis-
tically motivated features or splits, we take a fully
automated approach, in which each symbol is split
into unconstrained subsymbols. For example, NP
would be split into NP-1 through NP-8. We use
the Expectation-Maximization (EM) to then fit our
split model to the observed trees; therein the vari-
ous subsymbols will specialize in ways which may
or may not correspond to our linguistic intuitions.
This approach is relatively language independent,
because the hidden subsymbols are induced auto-
matically from the training trees based solely on data
likelihood, though of course it is most applicable to
strongly configurational languages.
In our experiments, we find that we can learn
compact grammars that give the highest parsing ac-
curacies in the 2008 Parsing German shared task.
Our F1-scores of 69.8/84.0 (TIGER/TueBa-D/Z) are
more than four points higher than those of the
second best systems. Additionally, we investigate
the patterns that are learned and show that the la-
tent variable approach recovers linguistically inter-
pretable phenomena. In our analysis, we pay partic-
ular attention to similarities and differences between
33
FRAG
RB
Not
NP
DT
this
NN
year
.
.
(a)
ROOT
FRAG-x
FRAG-x
RB-x
Not
NP-x
DT-x
this
NN-x
year
.-x
.
(b)
Figure 1: (a) The original tree. (b) The binarized tree
with latent variables.
grammars learned from the two treebanks.
2 Latent Variable Parsing
In latent variable parsing (Matsuzaki et al, 2005;
Prescher, 2005; Petrov et al, 2006), we learn
rule probabilities on latent annotations that, when
marginalized out, maximize the likelihood of the
unannotated training trees. We use an automatic ap-
proach in which basic nonterminal symbols are al-
ternately split and merged to maximize the likeli-
hood of the training treebank.
In this section we briefly review the main ideas
in latent variable parsing. This work has been pre-
viously published and we therefore provide only
a short overview. For a more detailed exposi-
tion of the learning algorithm the reader is re-
ferred to Petrov et al (2006). The correspond-
ing inference procedure is described in detail in
Petrov and Klein (2007). The parser, code,
and trained models are available for download at
http://nlp.cs.berkeley.edu.
2.1 Learning
Starting with a simple X-bar grammar, we use the
Expectation-Maximization (EM) algorithm to learn
a new grammar whose nonterminals are subsymbols
of the original evaluation nonterminals. The X-bar
grammar is created by binarizing the treebank trees;
for each local tree rooted at an evaluation nonter-
minal X, we introduce a cascade of new nodes la-
beled X so that each node has at most two children,
see Figure 1. This initialization is the absolute mini-
mum starting grammar that distinguishes the evalua-
tion nonterminals (and maintains separate grammars
for each of them).
In Petrov et al (2006) we show that a hierarchical
split-and-merge strategy learns compact but accurate
grammars, allocating subsymbols adaptively where
they are most effective. Beginning with the base-
line grammar, we repeatedly split and re-train the
grammar. In each iteration, we initialize EM with
the results of the previous round?s grammar, splitting
every previous symbol in two and adding a small
amount of randomness (1%) to break the symme-
try between the various subsymbols. Note that we
split all nonterminal symbols, including the part-of-
speech categories. While creating more latent an-
notations can increase accuracy, it can also lead to
overfitting via oversplitting. Adding subsymbols di-
vides grammar statistics into many bins, resulting in
a tighter fit to the training data. At the same time,
each bin has less support and therefore gives a less
robust estimate of the grammar probabilities. At
some point, the fit no longer generalizes, leading to
overfitting.
To prevent oversplitting, we could measure the
utility of splitting each latent annotation individu-
ally and then split the best ones first. However, not
only is this impractical, requiring an entire training
phase for each new split, but it assumes the contri-
butions of multiple splits are independent. In fact,
extra subsymbols may need to be added to several
nonterminals before they can cooperate to pass in-
formation along the parse tree. This point is cru-
cial to the success of our method: because all splits
are fit simultaneously, local splits can chain together
to propagate information non-locally. We therefore
address oversplitting in the opposite direction; after
training all splits, we measure for each one the loss
in likelihood incurred by removing it. If this loss
is small, the new annotation does not carry enough
useful information and can be removed. Another ad-
vantage of evaluating post-hoc merges is that, unlike
the likelihood gain from splitting, the likelihood loss
from merging can be efficiently approximated.
To summarize, splitting provides an increasingly
tight fit to the training data, while merging improves
generalization and controls grammar size. In order
to further overcome data fragmentation and overfit-
ting, we also smooth our parameters along the split
hierarchy. Smoothing allows us to add a larger num-
ber of annotations, each specializing in only a frac-
tion of the data, without overfitting our training set.
34
2.2 Inference
At inference time, we want to use the learned gram-
mar to efficiently and accurately compute a parse
tree for a give sentence.
For efficiency, we employ a hierarchical coarse-
to-fine inference scheme (Charniak et al, 1998;
Charniak and Johnson, 2005; Petrov and Klein,
2007) which vastly improves inference time with no
loss in test set accuracy. Our method considers the
splitting history of the final grammar, projecting it
onto its increasingly refined prior stages. For each
such projection of the refined grammar, we estimate
the projection?s parameters from the source PCFG
itself (rather than the original treebank), using tech-
niques for infinite tree distributions and iterated fix-
point equations. We then rapidly pre-parse with each
refinement stage in sequence, such that any item
X:[i, j] with sufficiently low posterior probability
triggers the pruning of its further refined variants in
all subsequent finer parses.
Our refined grammars G are over symbols of the
form X-k where X is an evaluation symbol (such as
NP) and k is some indicator of a subsymbol, which
may encode something linguistic like a parent anno-
tation context, but which is formally just an integer.
G therefore induces a derivation distribution over
trees labeled with split symbols. This distribution
in turn induces a parse distribution over (projected)
trees with unsplit evaluation symbols. We have
several choices of how to select a tree given these
posterior distributions over trees. Since computing
the most likely parse tree is NP-complete (Sima?an,
1992), we settle for an approximation that allows us
to (partially) sum out the latent annotation. In Petrov
and Klein (2007) we relate this approximation to
Goodman (1996)?s labeled brackets algorithm ap-
plied to rules and to Matsuzaki et al (2005)?s sen-
tence specific variational approximation. This pro-
cedure is substantially superior to simply erasing the
latent annotations from the the Viterbi derivation.
2.3 Results
In Petrov and Klein (2007) we trained models for
English, Chinese and German using the standard
corpora and setups. We applied our latent variable
model directly to each of the treebanks, without any
? 40 words all
Parser LP LR LP LR
ENGLISH
Charniak et al (2005) 90.1 90.1 89.5 89.6
Petrov and Klein (2007) 90.7 90.5 90.2 89.9
ENGLISH (reranked)
Charniak et al (2005) 92.4 91.6 91.8 91.0
GERMAN (NEGRA)
Dubey (2005) F1 76.3 -
Petrov and Klein (2007) 80.8 80.7 80.1 80.1
CHINESE
Chiang et al (2002) 81.1 78.8 78.0 75.2
Petrov and Klein (2007) 86.9 85.7 84.8 81.9
Table 1: Our split-and-merge latent variable approach
produces the best published parsing performance on
many languages.
language dependent modifications. Specifically, the
same model hyperparameters (merging percentage
and smoothing factor) were used in all experiments.
Table 1 summarizes the results: automatically in-
ducing latent structure is a technique that generalizes
well across language boundaries and results in state
of the art performance for Chinese and German. On
English, the parser is outperformed by the reranked
output of Charniak and Johnson (2005), but it out-
performs their underlying lexicalized parser.
3 Experiments
We conducted experiments on the two treebanks
provided for the 2008 Parsing German shared task.
Both treebanks are annotated collections of Ger-
man newspaper text, covering from similar top-
ics. They are annotated with part-of-speech (POS)
tags, morphological information, phrase structure,
and grammatical functions. TueBa-D/Z addition-
ally uses topological fields to describe fundamental
word order restrictions in German clauses. However,
the treebanks differ significantly in their annotation
schemes: while TIGER relies on crossing branches
to describe long distance relationships, TueBa-D/Z
uses planar tree structures with designated labels
that encode long distance relationships. Addition-
ally, the annotation in TIGER is relatively flat on the
phrasal level, while TueBa-D/Z annotates more in-
ternal phrase structure.
We used the standard splits into training and de-
35
 60
 65
 70
 75
 80
 85
 90
 0  1  2  3  4  5
F1
Split & Merge Iterations
TIGER
TueBa-D/Z
Figure 2: Parsing accuracy improves when the amount of
latent annotation is increased.
velopment set, containing roughly 16,000 training
trees and 1,600 development trees, respectively. All
parsing figures in this section are on the develop-
ment set, evaluating on constituents and grammat-
ical functions using gold part-of-speech tags, un-
less noted otherwise. Note that even when we as-
sume gold evaluation part-of-speech tags, we still
assign probabilities to the different subsymbols of
the provided evaluation tag. The parsing accuracies
in the final results section are the official results of
the 2008 Parsing German shared task.
3.1 Latent Annotation
As described in Section 2.1, we start with a mini-
mal X-Bar grammar and learn increasingly refined
grammars in a hierarchical split-and-merge fashion.
We conjoined the constituency categories with their
grammatical functions, creating initial categories
like NP-PD and NP-OA which were further split
automatically. Figure 2 shows how held-out accu-
racy improves when we add latent annotation. Our
baseline grammars have low F1-scores (63.3/72.8,
TIGER/TueBa-D/Z), but performance increases as
the complexity of latent annotation increases. After
four split-and-merge iterations, performance levels
off. Interestingly, the gap in performance between
the two treebanks increases from 9.5 to 13.4 F1-
points. It appears that the latent variable approach
is better suited for capturing the rich structure of the
TueBa-D/Z treebank.
As languages vary in their phrase-internal head-
TIGER TueBa-D/Z
F1 EX F1 EX
Auto Tags 71.12 28.91 83.18 18.46
Gold Tags 71.74 34.04 85.10 20.98
Table 2: Parsing accuracies (F1-score and exact match)
with gold POS tags and automatic POS tags. Many parse
errors are due to incorrect tagging.
edness, we varied the binarization scheme, but, con-
sistent with our experience in other languages, no-
ticed little difference between right and left bina-
rization. We also experimented with starting from
a more constrained baseline by adding parent and
sibling annotation. Adding initial structural annota-
tion results in a higher baseline performance. How-
ever, since it fragments the grammar, adding latent
annotation has a smaller effect, eventually resulting
in poorer performance compared to starting from a
simple X-Bar grammar. Essentially, the initial gram-
mar is either mis- or oversplit to some degree.
3.2 Part-of-speech tagging
When gold parts-of-speech are not assumed, many
parse errors can be traced back to part-of-speech
(POS) tagging errors. It is therefore interesting to in-
vestigate the influence of tagging errors on the over-
all parsing accuracy. For the shared task, we could
assume gold POS tags: during inference we only al-
lowed (and scored) the different subsymbols of the
correct tags. However, this assumption cannot be
made in a more realistic scenario, where we want to
parse text from an unknown source. Table 2 com-
pares the parsing performance with gold POS tags
and with automatic tagging. While POS tagging er-
rors have little influence on the TIGER treebank,
tagging errors on TueBa-D/Z cause an substantial
number of subsequent parse errors.
3.3 Two pass parsing
In the previous experiments, we conflated the
phrasal categories and grammatical functions into
single initial grammar symbol. An alternative is
to first determine the categorical constituency struc-
ture and then to assign grammatical functions to the
chosen constituents in a separate, second pass. To
achieve this, we trained latent variable grammars
for base constituency parsing by stripping off the
36
grammatical functions. After four rounds of split
and merge training, these grammars achieve very
good constituency accuracies of 85.1/94.1 F1-score
(TIGER/TueBa-D/Z). For the second pass, we es-
timated (but did not split) X-Bar style grammars
on the grammatical functions only. Fixing the con-
stituency structure from the first pass, we used those
to add grammatical functions. Unfortunately, this
approach proved to be inferior to the unified, one
pass approach, giving F1-scores of only 50.0/69.4
(TIGER/TueBa-D/Z). Presumably, the degradation
can be attributed to the fact that grammatical func-
tions model long-distance relations between the con-
stituents, which can only be captured poorly by an
unsplit, highly local X-bar style grammar.
3.4 Final Results
The final results of the shared task evaluation are
shown in Table 3. These results were produced by
a latent variable grammar that was trained for four
split-and-merge iterations, starting from an X-Bar
grammar over conjoined categorical/grammatical
symbols, with a left-branching binarization. Our
automatic latent variable approach serves better for
German disambiguation than the competing ap-
proaches, despite its being very language agnostic.
4 Analysis
In this section, we examine the learned grammars,
discussing what is learned. Because the grammat-
ical functions significantly increase the number of
base categories and make the grammars more diffi-
cult to examine, we show examples from grammars
that were trained for categorical constituency pars-
ing by initially stripping off all grammatical function
annotations.
4.1 Lexical Splits
Since both treebanks use the same part-of-speech
categories, it is easy to compare the learned POS
subcategories. To better understand what is being
learned, we selected two grammars after two split
and merge iterations and examined the word dis-
tributions of the subcategories of various symbols.
The three most likely words for a number of POS
tags are shown in Table 4. Interestingly, the sub-
categories learned from the different treebanks ex-
hibit very similar patterns. For example, in both
cases, the nominal category (NE) has been split
into subcategories for first and last names, abbrevi-
ations and places. The cardinal numbers (CARD)
have been split into subcategories for years, spelled
out numbers, and other numbers. There are of-
ten subcategories distinguishing sentence initial and
sentence medial placement (KOND, PDAT, ART,
APPR, etc.), as well as subcategories capturing case
distinctions (PDAT, ART, etc.).
A quantitative way of analyzing the complexity of
what is learned is to compare the number of subcat-
egories that our split-and-merge procedure has allo-
cated to each category. Table 5 shows the automat-
ically determined number of subcategories for each
POS tag. While many categories have been split into
comparably many of subcategories, the POS tags in
the TIGER treebank have in general been refined
more heavily. This increased refinement can be ex-
plained by our merging criterion. We compute the
loss in likelihood that would be incurred from re-
moving a split, and we merge back the least useful
splits. In this process, lexical and phrasal splits com-
pete with each other. In TueBa-D/Z the phrasal cat-
egories have richer internal structure and therefore
get split more heavily. As a consequence, the lexi-
cal categories are often relatively less refined at any
given stage than in TIGER. Having different merg-
ing thresholds for the lexical and phrasal categories
would eliminate this difference and we might expect
the difference in lexical refinement to become less
pronounced. Of course, because of the different un-
derlying statistics in the two treebanks, we do not
expect the number of subcategories to become ex-
actly equal in any case.
4.2 Phrasal splits
Analyzing the phrasal splits is much more difficult,
as the splits can model internal as well as exter-
nal context (as well as combinations thereof) and,
in general, several splits must be considered jointly
before their patterning can be described. Further-
more, the two treebanks use different annotation
standards and different constituent categories. Over-
all, the phrasal categories of the TueBa-D/Z tree-
bank have been more heavily refined, in order to bet-
ter capture the rich internal structures. In both tree-
banks, the most heavily split categories are the noun,
verb and prepositional phrase categories (NP/NX,
37
TIGER TueBa-D/Z
LP LR F1 LP LR F1
Berkeley Parser 69.23 70.41 69.81 83.91 84.04 83.97
Va?xjo? Parser 67.06 63.40 65.18 76.20 74.56 75.37
Stanford Parser 58.52 57.63 58.07 79.26 79.22 79.24
Table 3: Final test set results of the 2008 Parsing German shared task (labeled precision, labeled recall and F1-score)
on both treebanks (including grammatical functions and using gold part-of-speech tags).
NE
Kohl Klaus SPD Deutschland
Rabin Helmut USA dpa
Lafontaine Peter CDU Bonn
CARD
1996 zwei 000 zwei
1994 drei 100 3
1991 vier 20 2
KOND
Und und sondern und
Doch oder aber oder
Aber aber bis sowie
PDAT
Diese dieser diesem -
Dieser dieses diese -
Dieses diese dieser -
ART
Die der der die
Der des den der
Das Die die den
APPR
In als in von
Von nach von in
Nach vor mit fu?r
PDS
Das dessen das -
Dies deren dies -
Diese die diese -
NE
Milosevic Peter K. Berlin
Mu?ller Wolfgang W. taz
Clinton Klaus de Kosovo
CARD
1998 zwei 500 zwei
1999 drei 100 20
2000 fu?nf 20 18
KOND
Und und sondern und
Aber oder weder Denn
Doch aber sowohl oder
PDAT
Dieser diese diesem dieser
Diese dieser dieser diese
Dieses dieses diesen dieses
ART
Die die die der
die Die der die
Der das den den
APPR
In bis in von
Mit Von auf in
Nach Bis mit fu?r
PDS
dem dessen das Das
das die Das das
jene denen dies diese
Table 4: The three most likely words for several part-of-speech (sub-)categories. The left column corresponds to the
TIGER treebank the right column to the TueBa-D/Z treebank. Similar subcategories are learned for both treebanks.
38
POS Ti Tue
ADJA 32 17
NN 32 32
NE 31 32
ADV 30 15
ADJD 30 19
VVFIN 29 5
VVPP 29 4
APPR 25 24
VVINF 18 7
CARD 18 16
ART 10 7
PIS 9 14
PPER 9 2
PIDAT - 9
POS Ti Tue
PIAT 8 7
VAFIN 8 3
KON 8 8
$[ 7 11
PROAV 7 -
APPRART 6 5
$ 6 2
PDS 5 5
PPOSAT 4 4
$. 4 5
PDAT 4 5
KOUS 4 3
VMFIN 4 1
PRELS 3 1
POS Ti Tue
VVIZU 3 2
VAINF 3 3
PTKNEG 3 1
FM 3 8
PWS 2 2
PWAV 2 5
XY 2 2
TRUNC 2 4
KOUI 2 1
PTKVZ 2 1
VAPP 2 2
KOKOM 2 5
PROP - 2
VVIMP 1 1
POS Ti Tue
VAIMP 1 1
VMPP 1 2
PPOSS 1 1
PRELAT 1 1
NNE 1 -
APPO 1 1
PTKA 1 2
PTKANT 1 2
PWAT 1 2
PRF 1 1
PTKZU 1 1
APZR 1 1
VMINF 1 1
ITJ 1 2
Table 5: Automatically determined number of subcategories for the part-of-speech tags. The left column corresponds
to the TIGER treebank the right column to the TueBa-D/Z treebank. Many categories are split in the same number of
subcategories, but overall the TIGER categories have been more heavily refined.
PP/PX, VP/VX*) as well as the sentential categories
(S/SIMPX). Categories that are rare or that have lit-
tle internal structure, in contrast, have been split
lightly or not at all.
5 Conclusions
We presented a series of experiments on pars-
ing German with latent variable grammars. We
showed that our latent variable approach is very
well suited for parsing German, giving the best
parsing figures on several different treebanks, de-
spite being completely language independent. Ad-
ditionally, we examined the learned grammars
and showed examples illustrating the linguistically
meaningful patterns that were learned. The parser,
code, and models are available for download at
http://nlp.cs.berkeley.edu.
References
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
In ACL?05.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-
based best-first chart parsing. 6th Workshop on Very
Large Corpora.
E. Charniak. 1996. Tree-bank grammars. In AAAI ?96,
pages 1031?1036.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00, pages 132?139.
D. Chiang and D. Bikel. 2002. Recovering latent infor-
mation in treebanks. In COLING ?02, pages 183?189.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, UPenn.
A. Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In ACL ?05.
J. Goodman. 1996. Parsing algorithms and metrics. ACL
?96.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24:613?632.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In ACL ?03, pages 423?430.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL ?05.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL ?07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
D. Prescher. 2005. Inducing head-driven PCFGs with la-
tent heads: Refining a tree-bank grammar for parsing.
In ECML?05.
K. Sima?an. 1992. Computatoinal complexity of proba-
bilistic disambiguation. Grammars, 5:125?151.
39
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 12?22,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Self-training with Products of Latent Variable Grammars
Zhongqiang Huang?
?UMIACS
University of Maryland
College Park, MD
zqhuang@umd.edu
Mary Harper??
?HLT Center of Excellence
Johns Hopkins University
Baltimore, MD
mharper@umd.edu
Slav Petrov?
?Google Research
76 Ninth Avenue
New York, NY
slav@google.com
Abstract
We study self-training with products of latent
variable grammars in this paper. We show
that increasing the quality of the automatically
parsed data used for self-training gives higher
accuracy self-trained grammars. Our genera-
tive self-trained grammars reach F scores of
91.6 on the WSJ test set and surpass even
discriminative reranking systems without self-
training. Additionally, we show that multi-
ple self-trained grammars can be combined in
a product model to achieve even higher ac-
curacy. The product model is most effective
when the individual underlying grammars are
most diverse. Combining multiple grammars
that were self-trained on disjoint sets of un-
labeled data results in a final test accuracy of
92.5% on the WSJ test set and 89.6% on our
Broadcast News test set.
1 Introduction
The latent variable approach of Petrov et al (2006)
is capable of learning high accuracy context-free
grammars directly from a raw treebank. It starts
from a coarse treebank grammar (Charniak, 1997),
and uses latent variables to refine the context-free
assumptions encoded in the grammar. A hierarchi-
cal split-and-merge algorithm introduces grammar
complexity gradually, iteratively splitting (and po-
tentially merging back) each observed treebank cat-
egory into a number of increasingly refined latent
subcategories. The Expectation Maximization (EM)
algorithm is used to train the model, guaranteeing
that each EM iteration will increase the training like-
lihood. However, because the latent variable gram-
mars are not explicitly regularized, EM keeps fit-
ting the training data and eventually begins over-
fitting (Liang et al, 2007). Moreover, EM is a lo-
cal method, making no promises regarding the final
point of convergence when initialized from different
random seeds. Recently, Petrov (2010) showed that
substantial differences between the learned gram-
mars remain, even if the hierarchical splitting re-
duces the variance across independent runs of EM.
In order to counteract the overfitting behavior,
Petrov et al (2006) introduced a linear smoothing
procedure that allows training grammars for 6 split-
merge (SM) rounds without overfitting. The in-
creased expressiveness of the model, combined with
the more robust parameter estimates provided by the
smoothing, results in a nice increase in parsing ac-
curacy on a held-out set. However, as reported by
Petrov (2009) and Huang and Harper (2009), an ad-
ditional 7th SM round actually hurts performance.
Huang and Harper (2009) addressed the issue of
data sparsity and overfitting from a different angle.
They showed that self-training latent variable gram-
mars on their own output can mitigate data spar-
sity issues and improve parsing accuracy. Because
the capacity of the model can grow with the size
of the training data, latent variable grammars are
able to benefit from the additional training data, even
though it is not perfectly labeled. Consequently,
they also found that a 7th round of SM training was
beneficial in the presence of large amounts of train-
ing data. However, variation still remains in their
self-trained grammars and they had to use a held-out
set for model selection.
The observation of variation is not surprising;
EM?s tendency to get stuck in local maxima has been
studied extensively in the literature, resulting in vari-
ous proposals for model selection methods (e.g., see
12
Burnham and Anderson (2002)). What is perhaps
more surprising is that the different latent variable
grammars seem to capture complementary aspects
of the data. Petrov (2010) showed that a simple ran-
domization scheme produces widely varying gram-
mars. Quite serendipitously, these grammars can
be combined into an unweighted product model that
substantially outperforms the individual grammars.
In this paper, we combine the ideas of self-
training and product models and show that both
techniques provide complementary effects. We hy-
pothesize that the main factors contributing to the
final accuracy of the product model of self-trained
grammars are (i) the accuracy of the grammar used
to parse the unlabeled data for retraining (single
grammar versus product of grammars) and (ii) the
diversity of the grammars that are being combined
(self-trained grammars trained using the same auto-
matically labeled subset or different subsets). We
conduct a series of analyses to develop an under-
standing of these factors, and conclude that both di-
mensions are important for obtaining significant im-
provements over the standard product models.
2 Experimental Setup
2.1 Data
We conducted experiments in two genres: newswire
text and broadcast news transcripts. For the
newswire studies, we used the standard setup (sec-
tions 02-21 for training, 22 for development, and 23
for final test) of the WSJ Penn Treebank (Marcus et
al., 1999) for supervised training. The BLLIP cor-
pus (Charniak et al, 2000) was used as a source of
unlabeled data for self-training the WSJ grammars.
We ignored the parse trees contained in the BLLIP
corpus and retained only the sentences, which are
already segmented and tokenized for parsing (e.g.,
contractions are split into two tokens and punctua-
tion is separated from the words). We partitioned
the 1,769,055 BLLIP sentences into 10 equally sized
subsets1.
For broadcast news (BN), we utilized the Broad-
1We corrected some of the most egregious sentence segmen-
tation problems in this corpus, and so the number of sentences is
different than if one simply pulled the fringe of the trees. It was
not uncommon for a sentence split to occur on abbreviations,
such as Adm.
cast News treebank from Ontonotes (Weischedel et
al., 2008) together with the WSJ Penn Treebank for
supervised training, because their combination re-
sults in better parser models compared to using the
limited-sized BN corpus alone (86.7 F vs. 85.2 F).
The files in the Broadcast News treebank represent
news stories collected during different time periods
with a diversity of topics. In order to obtain a rep-
resentative split of train-test-development sets, we
divided them into blocks of 10 files sorted by alpha-
betical filename order. We used the first file in each
block for development, the second for test, and the
remaining files for training. This training set was
then combined with the entire WSJ treebank. We
also used 10 equally sized subsets from the Hub4
CSR 1996 utterances (Garofolo et al, 1996) for self-
training. The Hub 4 transcripts are markedly noisier
than the BLLIP corpus is, in part because it is harder
to sentence segment, but also because it was pro-
duced by human transcription of spoken language.
The treebanks were pre-processed differently for
the two genres. For newswire, we used a slightly
modified version of the WSJ treebank: empty
nodes and function labels were deleted and auxiliary
verbs were replaced with AUXB, AUXG, AUXZ,
AUXD, or AUXN to represent infinitive, progres-
sive, present, past, or past participle auxiliaries2.
The targeted use of the broadcast models is for pars-
ing broadcast news transcripts for language mod-
els in speech recognition systems. Therefore, in
addition to applying the transformations used for
newswire, we also replaced symbolic expressions
with verbal forms (e.g., $5 was replaced with five
dollars) and removed punctuation and case. The
Hub4 data was segmented into utterances, punctua-
tion was removed, words were down-cased, and con-
tractions were tokenized for parsing. Table 1 sum-
marizes the data set sizes used in our experiments,
together with average sentence length and standard
deviation.
2.2 Scoring
Parses from all models are compared with respective
gold standard parses using SParseval bracket scor-
ing (Roark et al, 2006). This scoring tool pro-
2Parsing accuracy is marginally affected. The average over
10 SM6 grammars with the transformation is 90.5 compared to
90.4 F without it, a 0.1% average improvement.
13
Genre Statistics Train Dev Test Unlabeled
Newswire
# sentences 39.8k 1.7k 2.4k 1,769.1k
# words 950.0k 40.1k 56.7k 43,057.0k
length Avg./Std. 28.9/11.2 25.1/11.8 25.1/12.0 24.3/10.9
Broadcast News
# sentences 59.0k 1.0k 1.1k 4,386.5k
# words 1,281.1k 17.1k 19.4k 77,687.9k
length Avg./Std. 17.3/11.3 17.4/11.3 17.7/11.4 17.7/12.8
Table 1: The number of words and sentences, together with average (Avg.) sentence length and its standard deviation
(Std.), for the data sets used in our experiments.
duces scores that are identical to those produced
by EVALB for WSJ. For Broadcast News, SParse-
val applies Charniak and Johnson?s (Charniak and
Johnson, 2001) scoring method for EDITED nodes3.
Using this method, BN scores were slightly (.05-.1)
lower than if EDITED constituents were treated like
any other, as in EVALB.
2.3 Latent Variable Grammars
We use the latent variable grammar (Matsuzaki et
al., 2005; Petrov et al, 2006) implementation of
Huang and Harper (2009) in this work. Latent vari-
able grammars augment the observed parse trees in
the treebank with a latent variable at each tree node.
This effectively splits each observed category into
a set of latent subcategories. An EM-algorithm is
used to fit the model by maximizing the joint like-
lihood of parse trees and sentences. To allocate the
grammar complexity only where needed, a simple
split-and-merge procedure is applied. In every split-
merge (SM) round, each latent variable is first split
in two and the model is re-estimated. A likelihood
criterion is used to merge back the least useful splits
(50% merge rate for these experiments). This itera-
tive refinement proceeds for 7 rounds, at which point
parsing performance on a held-out set levels off and
training becomes prohibitively slow.
Since EM is a local method, different initial-
izations will result in different grammars. In
fact, Petrov (2010) recently showed that this EM-
algorithm is very unstable and converges to widely
varying local maxima. These local maxima corre-
3Non-terminal subconstituents of EDITED nodes are re-
moved so that the terminal constituents become immediate chil-
dren of a single EDITED node, adjacent EDITED nodes are
merged, and they are ignored for span calculations of the other
constituents.
spond to different high quality latent variable gram-
mars that have captured different types of patterns in
the data. Because the individual models? mistakes
are independent to some extent, multiple grammars
can be effectively combined into an unweighted
product model of much higher accuracy. We build
upon this line of work and investigate methods to
exploit products of latent variable grammars in the
context of self-training.
3 Self-training Methodology
Different types of parser self-training have been pro-
posed in the literature over the years. All of them
involve parsing a set of unlabeled sentences with a
baseline parser and then estimating a new parser by
combining this automatically parsed data with the
original training data. McClosky et al (2006) pre-
sented a very effective method for self-training a
two-stage parsing system consisting of a first-stage
generative lexicalized parser and a second-stage dis-
criminative reranker. In their approach, a large
amount of unlabeled text is parsed by the two-stage
system and the parameters of the first-stage lexical-
ized parser are then re-estimated taking the counts
from the automatically parsed data into considera-
tion.
More recently Huang and Harper (2009) pre-
sented a self-training procedure based on an EM-
algorithm. They showed that the EM-algorithm that
is typically used to fit a latent variable grammar
(Matsuzaki et al, 2005; Petrov et al, 2006) to a tree-
bank can also be used for self-training on automati-
cally parsed sentences. In this paper, we investigate
self-training with products of latent variable gram-
mars. We consider three training scenarios:
ST-Reg Training Use the best single grammar to
14
Regular Best Average Product
SM6 90.8 90.5 92.0
SM7 90.4 90.1 92.2
Table 2: Performance of the regular grammars and their
products on the WSJ development set.
parse a single subset of the unlabeled data and
train 10 self-trained grammars using this single
set.
ST-Prod Training Use the product model to parse
a single subset of the unlabeled data and train
10 self-trained grammars using this single set.
ST-Prod-Mult Training Use the product model to
parse all 10 subsets of the unlabeled data and
train 10 self-trained grammars, each using a
different subset.
The resulting grammars can be either used individu-
ally or combined in a product model.
These three conditions provide different insights.
The first experiment allows us to investigate the
effectiveness of product models for standard self-
trained grammars. The second experiment enables
us to quantify how important the accuracy of the
baseline parser is for self-training. Finally, the third
experiment investigates a method for injecting some
additional diversity into the individual grammars to
determine whether a product model is most success-
ful when there is more variance among the individ-
ual models.
Our initial experiments and analysis will focus on
the development set of WSJ. We will then follow
up with an analysis of broadcast news (BN) to de-
termine whether the findings generalize to a second,
less structured type of data. It is important to con-
struct grammars capable of parsing this type of data
accurately and consistently in order to support struc-
tured language modeling (e.g., (Wang and Harper,
2002; Filimonov and Harper, 2009)).
4 Newswire Experiments
In this section, we compare single grammars and
their products that are trained in the standard way
with gold WSJ training data, as well as the three
self-training scenarios discussed in Section 3. We
ST-Reg Best Average Product
SM6 91.5 91.2 92.0
SM7 91.6 91.5 92.4
Table 3: Performance of the ST-Reg grammars and their
products on the WSJ development set.
report the F scores of both SM6 and SM7 grammars
on the development set in order to evaluate the ef-
fect of model complexity on the performance of the
self-trained and product models. Note that we use
6th round grammars to produce the automatic parse
trees for the self-training experiments. Parsing with
the product of the 7th round grammars is slow and
requires a large amount of memory (32GB). Since
we had limited access to such machines, it was in-
feasible for us to parse all of the unlabeled data with
the SM7 product grammars.
4.1 Regular Training
We begin by training ten latent variable models ini-
tialized with different random seeds using the gold
WSJ training set. Results are presented in Table 2.
The best F score attained by the individual SM6
grammars on the development set is 90.8, with an
average score of 90.5. The product of grammars
achieves a significantly improved accuracy at 92.04.
Notice that the individual SM7 grammars perform
worse on average (90.1 vs. 90.5) due to overfitting,
but their product achieves higher accuracy than the
product of the SM6 grammars (92.2 vs. 92.0). We
will further investigate the causes for this effect in
Section 5.
4.2 ST-Reg Training
Given the ten SM6 grammars from the previous sub-
section, we can investigate the three self-training
methods. In the first regime (ST-Reg), we use the
best single grammar (90.8 F) to parse a single subset
of the BLLIP data. We then train ten grammars from
different random seeds, using an equally weighted
combination of the WSJ training set with this sin-
gle set. These self-trained grammars are then com-
bined into a product model. As reported in Table 3,
4We use Dan Bikel?s randomized parsing evaluation com-
parator to determine the significance (p < 0.05) of the differ-
ence between two parsers? outputs.
15
ST-Prod Best Average Product
SM6 91.7 91.4 92.2
SM7 91.9 91.7 92.4
Table 4: Performance of the ST-Prod grammars and their
products on the WSJ development set.
thanks to the use of additional automatically labeled
training data, the individual SM6 ST-Reg grammars
perform significantly better than the individual SM6
grammars (91.2 vs. 90.5 on average), and the indi-
vidual SM7 ST-Reg grammars perform even better,
achieving a high F score of 91.5 on average.
The product of ST-Reg grammars achieves signif-
icantly better performance over the individual gram-
mars, however, the improvement is much smaller
than that obtained by the product of regular gram-
mars. In fact, the product of ST-Reg grammars per-
forms quite similarly to the product of regular gram-
mars despite the higher average accuracy of the in-
dividual grammars. This may be caused by the fact
that self-training on the same data tends to reduce
the variation among the self-trained grammars. We
will show in Section 5 that the diversity among the
individual grammars is as important as average ac-
curacy for the performance attained by the product
model.
4.3 ST-Prod Training
Since products of latent variable grammars perform
significantly better than individual latent variable
grammars, it is natural to try using the product
model for parsing the unlabeled data. To investi-
gate whether the higher accuracy of the automati-
cally labeled data translates into a higher accuracy
of the self-trained grammars, we used the product of
6th round grammars to parse the same subset of the
unlabeled data as in the previous experiment. We
then trained ten self-trained grammars, which we
call ST-Prod grammars. As can be seen in Table 4,
using the product of the regular grammars for label-
ing the self-training data results in improved individ-
ual ST-Prod grammars when compared with the ST-
Reg grammars, with 0.2 and 0.3 improvements for
the best SM6 and SM7 grammars, respectively. In-
terestingly, the best individual SM7 ST-Prod gram-
mar (91.9 F) performs comparably to the product of
ST-Prod-Mult Best Average Product
SM6 91.7 91.4 92.5
SM7 91.8 91.7 92.8
Table 5: Performance of the ST-Prod-Mult grammars and
their products on the WSJ development set.
the regular grammars (92.0 F) that was used to label
the BLLIP subset used for self-training. This is very
useful for practical reasons because a single gram-
mar is faster to parse with and requires less memory
than the product model.
The product of the SM6 ST-Prod grammars also
achieves a 0.2 higher F score compared to the prod-
uct of the SM6 ST-Reg grammars, but the product
of the SM7 ST-Prod grammars has the same perfor-
mance as the product of the SM7 ST-Reg grammars.
This could be due to the fact that the ST-Prod gram-
mars are no more diverse than the ST-Reg grammars,
as we will show in Section 5.
4.4 ST-Prod-Mult Training
When creating a product model of regular gram-
mars, Petrov (2010) used a different random seed for
each model and conjectured that the effectiveness of
the product grammars stems from the resulting di-
versity of the individual grammars. Two ways to
systematically introduce bias into individual mod-
els are to either modify the feature sets (Baldridge
and Osborne, 2008; Smith and Osborne, 2007) or
to change the training distributions of the individual
models (Breiman, 1996). Petrov (2010) attempted to
use the second method to train individual grammars
on either disjoint or overlapping subsets of the tree-
bank, but observed a performance drop in individ-
ual grammars resulting from training on less data,
as well as in the performance of the product model.
Rather than reducing the amount of gold training
data (or having treebank experts annotate more data
to support the diversity), we employ the self-training
paradigm to train models using a combination of the
same gold training data with different sets of the
self-labeled training data. This approach also allows
us to utilize a much larger amount of low-cost self-
labeled data than can be used to train one model by
partitioning the data into ten subsets and then train-
ing ten models with a different subset. Hence, in
16
-3
-2
-1
0
1
2
3
Total VP QP NP SBAR PP ADVP_PRT S WHNP ADJP
D
i
f
f
e
r
e
n
c
e
 
i
n
 
F
G0
G1
G2
G3
G4
G5
G6
G7
G8
G9
(a) Difference in F score between the product and the individual SM6 regular grammars.
-3
-2
-1
0
1
2
3
Total VP QP NP SBAR PP ADVP_PRT S WHNP ADJP
D
i
f
f
e
r
e
n
c
e
 
i
n
 
F
G0
G1
G2
G3
G4
G5
G6
G7
G8
G9
(b) Difference in F score between the product of SM6 regular grammars and the individual SM7 ST-Prod-Mult
grammars.
Figure 1: Difference in F scores between various individual grammars and representative product grammars.
the third self-training experiment, we use the prod-
uct of the regular grammars to parse all ten subsets
of the unlabeled data and train ten grammars, which
we call ST-Prod-Mult grammars, each using a dif-
ferent subset.
As shown in Table 5, the individual ST-Prod-Mult
grammars perform similarly to the individual ST-
Prod grammars. However, the product of the ST-
Prod-Mult grammars achieves significantly higher
accuracies than the product of the ST-Prod gram-
mars, with 0.3 and 0.4 improvements for SM6 and
SM7 grammars, respectively, suggesting that the use
of multiple self-training subsets plays an important
role in model combination.
5 Analysis
We conducted a series of analyses to develop an un-
derstanding of the factors affecting the effectiveness
of combining self-training with product models.
5.1 What Has Improved?
Figure 1(a) depicts the difference between the prod-
uct and the individual SM6 regular grammars on
overall F score, as well as individual constituent F
scores. As can be observed, there are significant
variations among the individual grammars, and the
product of the regular grammars improves almost all
categories, with a few exceptions (some individual
grammars do better on QP and WHNP constituents).
Figure 1(b) shows the difference between the
product of the SM6 regular grammars and the indi-
vidual SM7 ST-Prod-Mult grammars. Self-training
dramatically improves the quality of single gram-
mars. In most of the categories, some individ-
ual ST-Prod-Mult grammars perform comparably or
slightly better than the product of SM6 regular gram-
mars used to automatically label the unlabeled train-
ing set.
5.2 Overfitting vs. Smoothing
Figure 2(a) and 2(b) depict the learning curves of
the regular and the ST-Prod-Mult grammars. As
more latent variables are introduced through the iter-
ative SM training algorithm, the modeling capacity
of the grammars increases, leading to improved per-
formance. However, the performance of the regular
grammars drops after 6 SM rounds, as also previ-
ously observed in (Huang and Harper, 2009; Petrov,
2009), suggesting that the regular SM7 grammars
have overfit the relatively small-sized gold training
17
data. In contrast, the performance of the self-trained
grammars continues to improve in the 7th SM round.
Huang and Harper (2009) argued that the additional
self-labeled training data adds a smoothing effect to
the grammars, supporting an increase in model com-
plexity without overfitting.
Although the performance of the individual gram-
mars, both regular and self-trained, varies signif-
icantly and the product model consistently helps,
there is a non-negligible difference between the im-
provement achieved by the two product models over
their component grammars. The regular product
model improves upon its individual grammars more
than the ST-Prod-Mult product does in the later SM
rounds, as illustrated by the relative error reduction
curves in figures 2(a) and (b). In particular, the prod-
uct of the SM7 regular grammars gains a remarkable
2.1% absolute improvement over the average perfor-
mance of the individual regular SM7 grammars and
0.2% absolute over the product of the regular SM6
grammars, despite the fact that the individual regular
SM7 grammars perform worse than the SM6 gram-
mars. This suggests that the product model is able
to effectively exploit less smooth, overfit grammars.
We will examine this issue further in the next sub-
section.
5.3 Diversity
From the perspective of Products of Experts (Hin-
ton, 1999) or Logarithmic Opinion Pools (Smith et
al., 2005), each individual expert learns complemen-
tary aspects of the training data and the veto power
of product models enforces that the joint prediction
of their product has to be licensed by all individual
experts. One possible explanation of the observa-
tion in the previous subsection is that with the ad-
dition of more latent variables, the individual gram-
mars become more deeply specialized on certain as-
pects of the training data. This specialization leads
to greater diversity in their prediction preferences,
especially in the presence of a small training set.
On the other hand, the self-labeled training set size
is much larger, and so the specialization process is
therefore slowed down.
Petrov (2010) showed that the individually
learned grammars are indeed very diverse by look-
ing at the distribution of latent annotations across the
treebank categories, as well as the variation in over-
all and individual category F scores (see Figure 1).
However, these measures do not directly relate to the
diversity of the prediction preferences of the gram-
mars, as we observed similar patterns in the regular
and self-trained models.
Given a sentence s and a set of grammars G =
{G1, ? ? ? , Gn}, recall that the decoding algorithm of
the product model (Petrov, 2010) searches for the
best tree T such that the following objective function
is maximized:
?
r?T
?
G?G
log p(r|s,G)
where log p(r|s,G) is the log posterior probability
of rule r given sentence s and grammar G. The
power of the product model comes directly from the
diversity in log p(r|s,G) among individual gram-
mars. If there is little diversity, the individual
grammars would make similar predictions and there
would be little or no benefit from using a product
model. We use the average empirical variance of
the log posterior probabilities of the rules among the
learned grammars over a held-out set S as a proxy
of the diversity among the grammars:
?
s?S
?
G?G
?
r?R(G,s)
p(r|s,G)VAR(log(p(r|s,G)))
?
s?S
?
G?G
?
r?R(G,s)
p(r|s,G)
where R(G, s) represents the set of rules extracted
from the chart when parsing sentence s with gram-
mar G, and VAR(log(p(r|s,G))) is the variance of
log(p(r|s,G)) among all grammars G ? G.
Note that the average empirical variance is only
an approximation of the diversity among grammars.
In particular, this measure tends to be biased to pro-
duce larger numbers when the posterior probabili-
ties of rules tend to be small, because small differ-
ences in probability produce large changes in the log
scale. This happens for coarser grammars produced
in early SM stages when there is more uncertainty
about what rules to apply, with the rules remaining
in the parsing chart having low probabilities overall.
As shown in Figure 2(c), the average variances
all start at a high value and then drop, probably due
to the aforementioned bias. However, as the SM
iteration continues, the average variances increase
despite the bias. More interestingly, the variance
18
83
85
87
89
91
93
2 3 4 5 6 7
5%
9%
13%
17%
21%
25%
Regular Grammars
F
(a) SM Rounds
R
e
l
a
t
i
v
e
 
E
r
r
o
r
 
R
e
d
u
c
t
i
o
n
83
85
87
89
91
93
2 3 4 5 6 7
5%
9%
13%
17%
21%
25%
ST-Prod-Mult Grammars
F
(b) SM Rounds
R
e
l
a
t
i
v
e
 
E
r
r
o
r
 
R
e
d
u
c
t
i
o
n
Product Mean Error Reduction
0.1
0.2
0.3
0.4
0.5
2 3 4 5 6 7
Test
A
v
e
r
a
g
e
 
V
a
r
i
a
n
c
e
(c) SM Rounds
Regular
ST-Prod-Mult
ST-Prod
ST-Reg
Figure 2: Learning curves of the individual regular (a) and ST-Prod-Mult (b) grammars (average performance, with
minimum and maximum values indicated by bars) and their products before and after self-training on the WSJ de-
velopment set. The relative error reductions of the products are also reported. (c) The measured average empirical
variance among the grammars trained on WSJ.
among the regular grammars grows at a much faster
speed and is consistently greater when compared to
the self-trained grammars. This suggests that there
is more diversity among the regular grammars than
among the self-trained grammars, and explains the
greater improvement obtained by the regular product
model. It is also important to note that there is more
variance among the ST-Prod-Mult grammars, which
were trained on disjoint self-labeled training data,
and a greater improvement in their product model
relative to the ST-Reg and ST-Prod grammars, fur-
ther supporting the diversity hypothesis. Last but not
the least, the trend seems to indicate that the vari-
ance of the self-trained grammars would continue
increasing if EM training was extended by a few
more SM rounds, potentially resulting in even bet-
ter product models. It is currently impractical to test
this due to the dramatic increase in computational
requirements for an SM8 product model, and so we
leave it for future work.
5.4 Generalization to Broadcast News
We conducted the same set of experiments on the
broadcast news data set. While the development set
results in Table 6 show similar trends to the WSJ
results, the benefits from the combination of self-
training and product models appear even more pro-
nounced here. The best single ST-Prod-Mult gram-
mar (89.2 F) alone is able to outperform the product
of SM7 regular grammars (88.9 F), and their prod-
uct achieves another 0.7 absolute improvement, re-
sulting in a significantly better accuracy at 89.9 F.
Model Rounds Best Product
Regular
SM6 87.1 88.6
SM7 87.1 88.9
ST-Prod
SM6 88.5 89.0
SM7 89.0 89.6
ST-Prod-Mult
SM6 88.8 89.5
SM7 89.2 89.9
Table 6: F-score for various models on the BN develop-
ment set.
Figure 3 shows again that the benefits of self-
training and product models are complementary and
can be stacked. As can be observed, the self-
trained grammars have increasing F scores as the
split-merge rounds increase, while the regular gram-
mars have a slight decrease in F score after round 6.
In contrast to the newswire models, it appears that
the individual ST-Prod-Mult grammars trained on
broadcast news always perform comparably to the
product of the regular grammars at all SM rounds,
including the product of SM7 regular grammars.
This is noteworthy, given that the ST-Prod-Mult
grammars are trained on the output of the worse per-
forming product of the SM6 regular grammars. One
19
79
81
83
85
87
89
91
2 3 4 5 6 7
3%
6%
9%
12%
15%
Regular Gramamrs
F
(a) SM Rounds
R
e
l
a
t
i
v
e
 
E
r
r
o
r
 
R
e
d
u
c
t
i
o
n
79
81
83
85
87
89
91
2 3 4 5 6 7
3%
6%
9%
12%
15%
ST-Prod-Mult Gramamrs
F
(b) SM Rounds
R
e
l
a
t
i
v
e
 
E
r
r
o
r
 
R
e
d
u
c
t
i
o
n
Product Mean Error Reduction
0.1
0.2
0.3
0.4
0.5
2 3 4 5 6 7
Test
A
v
e
r
a
g
e
 
V
a
r
i
a
n
c
e
(c) SM Rounds
Regular
ST-Prod-Mult
ST-Prod
Figure 3: Learning curves of the individual regular (a) and ST-Prod-Mult (b) grammars (average performance, with
minimum and maximum values indicated by bars) and their products before and after self-training on the BN develop-
ment set. The relative error reductions of the products are also reported. (c) The measured average empirical variance
among the grammars trained on BN.
possible explanation is that we used more unlabeled
data for self-training the broadcast news grammars
than for the newswire grammars. The product of the
ST-Prod-Mult grammars provides further and signif-
icant improvement in F score.
6 Final Results
We evaluated the best single self-trained gram-
mar (SM7 ST-Prod), as well as the product of
the SM7 ST-Prod-Mult grammars on the WSJ test
set. Table 7 compares these two grammars to
a large body of related work grouped into sin-
gle parsers (SINGLE), discriminative reranking ap-
proaches (RE), self-training (SELF), and system
combinations (COMBO).
Our best single grammar achieves an accuracy
that is only slightly worse (91.6 vs. 91.8 in F score)
than the product model in Petrov (2010). This is
made possible by self-training on the output of a
high quality product model. The higher quality of
the automatically parsed data results in a 0.3 point
higher final F score (91.6 vs. 91.3) over the self-
training results in Huang and Harper (2009), which
used a single grammar for parsing the unlabeled
data. The product of the self-trained ST-Prod-Mult
grammars achieves significantly higher accuracies
with an F score of 92.5, a 0.7 improvement over the
product model in Petrov (2010).
8Our ST-Reg grammars are trained in the same way as in
Type Parser LP LR EX
S
IN
G
L
E Charniak (2000) 89.9 89.5 37.2
Petrov and Klein (2007) 90.2 90.1 36.7
Carreras et al (2008) 91.4 90.7 -
R
E Charniak and Johnson (2005) 91.8 91.2 44.8
Huang (2008) 92.2 91.2 43.5
S
E
L
F Huang and Harper (2009)8 91.6 91.1 40.4
McClosky et al (2006) 92.5 92.1 45.3
C
O
M
B
O Petrov (2010) 92.0 91.7 41.9
Sagae and Lavie (2006) 93.2 91.0 -
Fossum and Knight (2009) 93.2 91.7 -
Zhang et al (2009) 93.3 92.0 -
This Paper
Best Single 91.8 91.4 40.3
Best Product 92.7 92.2 43.1
Table 7: Final test set accuracies on WSJ.
Although our models are based on purely gen-
erative PCFG grammars, our best product model
performs competitively to the self-trained two-step
discriminative reranking parser of McClosky et al
(2006), which makes use of many non-local rerank-
ing features. Our parser also performs comparably
to other system combination approaches (Sagae and
Lavie, 2006; Fossum and Knight, 2009; Zhang et
al., 2009) with higher recall and lower precision,
Huang and Harper (2009) except that we keep all unary rules.
The reported numbers are from the best single ST-Reg grammar
in this work.
20
but again without using a discriminative reranking
step. We expect that replacing the first-step genera-
tive parsing model in McClosky et al (2006) with a
product of latent variable grammars would give even
higher parsing accuracies.
On the Broadcast News test set, our best perform-
ing single and product grammars (bolded in Table 6)
obtained F scores of 88.7 and 89.6, respectively.
While there is no prior work using our setup, we ex-
pect these numbers to set a high baseline.
7 Conclusions and Future Work
We evaluated methods for self-training high accu-
racy products of latent variable grammars with large
amounts of genre-matched data. We demonstrated
empirically on newswire and broadcast news genres
that very high accuracies can be achieved by training
grammars on disjoint sets of automatically labeled
data. Two primary factors appear to be determin-
ing the efficacy of our self-training approach. First,
the accuracy of the model used for parsing the unla-
beled data is important for the accuracy of the result-
ing single self-trained grammars. Second, the diver-
sity of the individual grammars controls the gains
that can be obtained by combining multiple gram-
mars into a product model. Our most accurate sin-
gle grammar achieves an F score of 91.6 on the WSJ
test set, rivaling discriminative reranking approaches
(Charniak and Johnson, 2005) and products of latent
variable grammars (Petrov, 2010), despite being a
single generative PCFG. Our most accurate product
model achieves an F score of 92.5 without the use of
discriminative reranking and comes close to the best
known numbers on this test set (Zhang et al, 2009).
In future work, we plan to investigate additional
methods for increasing the diversity of our self-
trained models. One possibility would be to utilize
more unlabeled data or to identify additional ways to
bias the models. It would also be interesting to deter-
mine whether further increasing the accuracy of the
model used for automatically labeling the unlabeled
data can enhance performance even more. A simple
but computationally expensive way to do this would
be to parse the data with an SM7 product model.
Finally, for this work, we always used products
of 10 grammars, but we sometimes observed that
subsets of these grammars produce even better re-
sults on the development set. Finding a way to se-
lect grammars from a grammar pool to achieve high
performance products is an interesting area of future
study.
8 Acknowledgments
This research was supported in part by NSF IIS-
0703859. Opinions, findings, and recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the views of the funding
agency or the institutions where the work was com-
pleted.
References
Jason Baldridge and Miles Osborne. 2008. Active learn-
ing and logarithmic opinion pools for HPSG parse se-
lection. Natural Language Engineering.
Leo Breiman. 1996. Bagging predictors. Machine
Learning.
Kenneth P. Burnham and David R. Anderson. 2002.
Model Selection and Multimodel Inference: A Prac-
tical Information-Theoretic Approach. New York:
Springer-Verlag.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
Tag, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In CoNLL, pages 9?16.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In NAACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson, 2000. BLLIP 1987-89
WSJ Corpus Release 1. Linguistic Data Consortium,
Philadelphia.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In ICAI.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In ACL.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP, pages 1114?1123, Singapore, August.
Victoria Fossum and Kevin Knight. 2009. Combining
constituent parsers. In NAACL, pages 253?256.
John Garofolo, Jonathan Fiscus, William Fisher, and
David Pallett, 1996. CSR-IV HUB4. Linguistic Data
Consortium, Philadelphia.
Geoffrey E. Hinton. 1999. Products of experts. In
ICANN.
21
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In EMNLP.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchical
Dirichlet processes. In EMNLP.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor, 1999. Treebank-3.
Linguistic Data Consortium, Philadelphia.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In HLT-
NAACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley.
Slav Petrov. 2010. Products of random latent variable
grammars. In HLT-NAACL.
Brian Roark, Mary Harper, Yang Liu, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie J. Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. SParseval: Evaluation metrics for pars-
ing speech. In LREC.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In NAACL, pages 129?132.
Andrew Smith and Miles Osborne. 2007. Diversity
in logarithmic opinion pools. Lingvisticae Investiga-
tiones.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
fields. In ACL.
Wen Wang and Mary P. Harper. 2002. The superarv lan-
guage model: Investigating the effectiveness of tightly
integrating multiple knowledge sources. In EMNLP,
pages 238?247, Philadelphia, July.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In EMNLP, pages 1552?1560.
22
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 167?176,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Efficient Graph-Based Semi-Supervised Learning
of Structured Tagging Models
Amarnag Subramanya
Google Research
Mountain View, CA 94043
asubram@google.com
Slav Petrov
Google Research
New York, NY 10011
slav@google.com
Fernando Pereira
Google Research
Mountain View, CA 94043
pereira@google.com
Abstract
We describe a new scalable algorithm for
semi-supervised training of conditional ran-
dom fields (CRF) and its application to part-
of-speech (POS) tagging. The algorithm uses
a similarity graph to encourage similar n-
grams to have similar POS tags. We demon-
strate the efficacy of our approach on a do-
main adaptation task, where we assume that
we have access to large amounts of unlabeled
data from the target domain, but no additional
labeled data. The similarity graph is used dur-
ing training to smooth the state posteriors on
the target domain. Standard inference can be
used at test time. Our approach is able to scale
to very large problems and yields significantly
improved target domain accuracy.
1 Introduction
Semi-supervised learning (SSL) is the use of
small amounts of labeled data with relatively large
amounts of unlabeled data to train predictors. In
some cases, the labeled data can be sufficient to pro-
vide reasonable accuracy on in-domain data, but per-
formance on even closely related out-of-domain data
may lag far behind. Annotating training data for all
sub-domains of a varied domain such as all of Web
text is impractical, giving impetus to the develop-
ment of SSL techniques that can learn from unla-
beled data to perform well across domains. The ear-
liest SSL algorithm is self-training (Scudder, 1965),
where one makes use of a previously trained model
to annotate unlabeled data which is then used to
re-train the model. While self-training is widely
used and can yield good results in some applica-
tions (Yarowsky, 1995), it has no theoretical guaran-
tees except under certain stringent conditions, which
rarely hold in practice(Haffari and Sarkar, 2007).
Other SSL methods include co-training (Blum
and Mitchell, 1998), transductive support vector ma-
chines (SVMs) (Joachims, 1999), and graph-based
SSL (Zhu et al, 2003). Several surveys cover a
broad range of methods (Seeger, 2000; Zhu, 2005;
Chapelle et al, 2007; Blitzer and Zhu, 2008). A ma-
jority of SSL algorithms are computationally expen-
sive; for example, solving a transductive SVM ex-
actly is intractable. Thus we have a conflict between
wanting to use SSL with large unlabeled data sets
for best accuracy, but being unable to do so because
of computational complexity. Some researchers at-
tempted to resolve this conflict by resorting to ap-
proximations (Collobert et al, 2006), but those lead
to suboptimal results (Chapelle et al, 2007).
Graph-based SSL algorithms (Zhu et al, 2003;
Joachims, 2003; Corduneanu and Jaakkola, 2003;
Belkin et al, 2005; Subramanya and Bilmes, 2009)
are an important subclass of SSL techniques that
have received much attention in the recent past, as
they outperform other approaches and also scale eas-
ily to large problems. Here one assumes that the data
(both labeled and unlabeled) is represented by ver-
tices in a graph. Graph edges link vertices that are
likely to have the same label. Edge weights govern
how strongly the labels of the nodes linked by the
edge should agree.
Most previous work in SSL has focused on un-
structured classification problems, that is, problems
with a relatively small set of atomic labels. There
167
has been much less work on SSL for structured pre-
diction where labels are composites of many atomic
labels with constraints between them. While the
number of atomic labels might be small, there will
generally be exponentially many ways to combine
them into the final structured label. Structured pre-
diction problems over sequences appear for exam-
ple in speech recognition, named-entity recogni-
tion, and part-of-speech tagging; in machine trans-
lation and syntactic parsing, the output may be tree-
structured.
Altun et al (2005) proposed a max-margin ob-
jective for semi-supervised learning over structured
spaces. Their objective is similar to that of manifold
regularization (Belkin et al, 2005) and they make
use of a graph as a smoothness regularizer. However
their solution involves inverting a matrix whose size
depends on problem size, making it impractical for
very large problems. Brefeld and Scheffer (2006)
present a modified version of the co-training algo-
rithm for structured output spaces. In both of the
above cases, the underlying model is based on struc-
tured SVM, which does not scale well to very large
datasets. More recently Wang et al (2009) proposed
to train a conditional random field (CRF) (Lafferty et
al., 2001) using an entropy-based regularizer. Their
approach is similar to the entropy minimization al-
gorithm (Grandvalet and Bengio, 2005). The prob-
lem here is that their objective is not convex and thus
can pose issues for large problems. Further, graph-
based SSL algorithms outperform algorithms based
on entropy minimization (Chapelle et al, 2007).
In this work, we propose a graph-based SSL
method for CRFs that is computationally practical
for very large problems, unlike the methods in the
studies cited above. Our method is scalable be-
cause it trains with efficient standard building blocks
for CRF inference and learning and also standard
graph label propagation machinery. Graph regular-
izer computations are only used for training, so at
test time, standard CRF inference can be used, un-
like in graph-based transductive methods. Briefly,
our approach starts by training a CRF on the source
domain labeled data, and then uses it to decode unla-
beled data from the target domain. The state posteri-
ors on the target domain are then smoothed using the
graph regularizer. Best state sequences for the unla-
beled target data are then created by Viterbi decod-
ing with the smoothed state posteriors, and this au-
tomatic target domain annotation is combined with
the labeled source domain data to retrain the CRF.
We demonstrate our new method in domain adap-
tation for a CRF part-of-speech (POS) tagger. While
POS tagging accuracies have reached the level of
inter-annotator agreement (>97%) on the standard
PennTreebank test set (Toutanova et al, 2003; Shen
et al, 2007), performance on out-of-domain data is
often well below 90%, impairing language process-
ing tasks that need syntactic information. For exam-
ple, on the question domain used in this paper, the
tagging accuracy of a supervised CRF is only 84%.
Our domain adaptation algorithm improves perfor-
mance to 87%, which is still far below in-domain
performance, but a significant reduction in error.
2 Supervised CRF
We assume that we have a set of labeled source do-
main examples Dl = {(xi,yi)}li=1, but only un-
labeled target domain examples Du = {xi}
l+u
i=l+1.
Here xi = x
(1)
i x
(2)
i ? ? ?x
(|xi|)
i is the sequence of
words in sentence i and yi = y
(1)
i y
(2)
i ? ? ? y
(|xi|)
i is
the corresponding POS tag sequence, with y(j)i ? Y
where Y is the set of POS tags. Our goal is to learn
a CRF of the form:
p(yi|xi; ?)?exp
(Ni?
j=1
K?
k=1
?kfk(y
(j?1)
i ,y
(j)
i ,xi, j)
)
for the target domain. In the above equation, ? =
{?1, . . . , ?K} ? RK , fk(y
(j?1)
i , y
(j)
i ,xi, j) is the k-
th feature function applied to two consecutive CRF
states and some window of the input sequence, and
?k is the weight of that feature. We discuss our fea-
tures in detail in Section 6. Given only labeled data
Dl, the optimal feature weights are given by:
??=argmin
??RK
[
?
l?
i=1
log p(yi|xi; ?)+????2
]
(1)
Here ???2 is the squared `2-norm and acts as the
regularizer, and ? is a trade-off parameter whose set-
ting we discuss in Section 6. In our case, we also
have access to the unlabeled data Du from the target
domain which we would like to use for training the
CRF. We first describe how we construct a similarity
168
graph over the unlabeled which will be used in our
algorithm as a graph regularizer.
3 Graph Construction
Graph construction is the most important step in
graph-based SSL. The standard approach for un-
structured problems is to construct a graph whose
vertices are labeled and unlabeled examples, and
whose weighted edges encode the degree to which
the examples they link should have the same la-
bel (Zhu et al, 2003). Then the main graph con-
struction choice is what similarity function to use
for the weighted edges between examples. How-
ever, in structured problems the situation is more
complicated. Consider the case of sequence tag-
ging we are studying. While we might be able to
choose some appropriate sequence similarity to con-
struct the graph, such as edit distance or a string
kernel, it is not clear how to use whole sequence
similarity to constrain whole tag sequences assigned
to linked examples in the learning algorithm. Al-
tun et al (2005) had the nice insight of doing the
graph construction not for complete structured ex-
amples but instead for the parts of structured exam-
ples (also known as factors in graphical model ter-
minology), which encode the local dependencies be-
tween input data and output labels in the structured
problem. However, their approach is too demanding
computationally (see Section 5), so instead we use
local sequence contexts as graph vertices, exploting
the empirical observation that the part of speech of
a word occurrence is mostly determined by its local
context.
Specifically, the set V of graph vertices consists
of all the word n-grams1 (types) that have occur-
rences (tokens) in training sentences (labeled and
unlabeled). We partition V = Vl ? Vu where Vl cor-
responds to n-grams that occur at least once in the
labeled data, and Vu corresponds to n-grams that oc-
cur only in the unlabeled data.
Given a symmetric similarity function between
types to be defined below, we link types u and v with
1We pad the n-grams at the beginning and end of sentences
with appropriate dummy symbols.
Description Feature
Trigram + Context x1 x2 x3 x4 x5
Trigram x2 x3 x4
Left Context x1 x2
Right Context x4 x5
Center Word x2
Trigram ? Center Word x2 x4
Left Word + Right Context x2 x4 x5
Left Context + Right Word x1 x2 x4
Suffix HasSuffix(x3)
Table 1: Features we extract given a sequence of words
?x1 x2 x3 x4 x5? where the trigram is ?x2 x3 x4?.
an edge of weight wuv, defined as:
wuv =
{
sim(u, v) if v ? K(u) or u ? K(v)
0 otherwise
whereK(u) is the set of k-nearest neighbors of u ac-
cording to the given similarity. For all experiments
in this paper, n = 3 and k = 5.
To define the similarity function, for each token
of a given type in the labeled and unlabeled data,
we extract a set of context features. For example,
for the token x2 x3 x4 occurring in the sequence
x1 x2 x3 x4 x5, we use feature templates that cap-
ture the left (x1 x2) and right contexts (x4 x5). Addi-
tionally, we extract suffix features from the word in
the middle. Table 1 gives an overview of the features
that we used. For each n-gram type, we compute the
vector of pointwise mutual information (PMI) val-
ues between the type and each of the features that
occur with tokens of that type. Finally, we use the
cosine distance between those PMI vectors as our
similarity function.
We have thus circumvented the problem of defin-
ing similarities over sequences by defining the graph
over types that represent local sequence contexts.
Since our CRF tagger only uses local features of the
input to score tag pairs, we believe that the graph
we construct captures all significant context infor-
mation. Figure 1 shows an excerpt from our graph.
The figure shows the neighborhoods of a subset of
the vertices with the center word ?book.? To reduce
clutter, we included only closest neighbors and the
edges that involve the nodes of interest.
169
[the conference on]
[whose book on]
[the auction on]
[U.N.-backed conference on]
[the conference speakers]
[to schedule a]
[to postpone a]
VB
[to ace a]
[to book a]
[to run a]
[to start a]
NN
NN
NN
VB
VB
[you book a]
[you rent a]
[you log a]
[you unrar a]
[to book some]
[to approve some]
VB
[to fly some]
[to approve parental-consent]
6
4
3
[the book that]
[the job that]
[the constituition that]
[the movie that]
[the city that]
NN
NN
[a movie agent]
[a clearing agent]
[a book agent]
7
4
6
Figure 1: Vertices with center word ?book? and their local neighborhoods, as well as the shortest-path distance between
them. Note that the noun (NN) and verb (VB) interpretations form two disjoint connected components.
It is remarkable that the neighborhoods are co-
herent, showing very similar syntactic configura-
tions. Furthermore, different vertices that (should)
have the same label are close to each other, form-
ing connected components for each part-of-speech
category (for nouns and verbs in the figure). We ex-
pect the similarity graph to provide information that
cannot be expressed directly in a sequence model.
In particular, it is not possible in a CRF to directly
enforce the constraint that similar trigrams appear-
ing in different sentences should have similar POS
tags. This constraint however is important dur-
ing (semi-supervised) learning, and is what makes
our approach different and more effective than self-
training.
In practice, we expect two main benefits from
our graph-based approach. First, the graph allows
new features to be discovered. Many words occur
only in the unlabeled data and a purely supervised
CRF would not be able to learn feature weights for
those observations. We could use self-training to
learn weights for those features, but self-training just
tends to reinforce the knowledge that the supervised
model already has. The similarity graph on the other
hand can link events that occur only in the unlabeled
data to similar events in the labeled data. Further-
more, because the graph is built over types rather
than tokens, it will encourage the same interpreta-
tion to be chosen for similar trigrams occurring in
different sentences. For example, the word ?unrar?
will most likely not occur in the labeled training
data. Seeing it in the neighborhood of words for
which we know the POS tag will help us learn the
correct POS tag for this otherwise unknown word
(see Figure 1).
Second, the graph propagates adjustments to the
weights of known features. Many words occur only
a handful of times in our labeled data, resulting in
poor estimates of their contributions. Even for fre-
quently occurring events, their distribution in the tar-
get domain might be different from their distribution
in the source domain. While self-training might be
able to help adapt to such domain changes, its ef-
fectiveness will be limited because the model will
always be inherently biased towards the source do-
main. In contrast, labeled vertices in the similar-
ity graph can help disambiguate ambiguous contexts
and correct (some of) the errors of the supervised
model.
4 Semi-Supervised CRF
Given unlabeled data Du, we only have access to
the prior p(x). As the CRF is a discriminative
model, the lack of label information renders the
CRF weights independent of p(x) and thus we can-
not directly utilize the unlabeled data when train-
ing the CRF. Therefore, semi-supervised approaches
to training discriminative models typically use the
unlabeled data to construct a regularizer that is
used to guide the learning process (Joachims, 1999;
Lawrence and Jordan, 2005). Here we use the graph
as a smoothness regularizer to train CRFs in a semi-
supervised manner.
Our algorithm iterates between the following five
170
Algorithm 1 Semi-Supervised CRF Training
?s = crf-train(Dl, ?0)
Set ?(t)0 = ?
(s)
while not converged do
{p} = posterior decode(Du, ?old)
{q} = token to type({p})
{q?} = graph propagate({q})
D(1)u = viterbi decode({q?}, ?old)
?(t)n+1 = crf-train(Dl ? D
(1)
u , ?
(t)
n )
end while
Return last ?(t)
simple (and convex) steps: Given a set of CRF pa-
rameters, we first compute marginals over the un-
labeled data (posterior decode). The marginals
over tokens are then aggregated to marginals over
types (token to type), which are used to initial-
ize the graph label distributions. After running la-
bel propagation (graph propagate), the posteriors
from the graph are used to smooth the state posteri-
ors. Decoding the unlabeled data (viterbi decode)
produces a new set of automatic annotations that can
be combined with the labeled data to retrain the CRF
using the supervised CRF training objective (crf-
train). These steps, summarized in Algorithm 1, are
iterated until convergence.
4.1 Posterior Decoding
Let ?(t)n (t refers to target domain) represent the esti-
mate of the CRF parameters for the target domain af-
ter the n-th iteration.2 In this step, we use the current
parameter estimates to compute the marginal proba-
bilities
p(y(j)i |xi; ?
(t)
n ) 1 ? j ? |xi|, i ? Dl
over POS tags for every word position j for i index-
ing over sentences in Dl ? Du.
4.2 Token-to-Type Mapping
Recall that our graph is defined over types while
the posteriors computed above involve particular to-
kens. We accumulate token-based marginals to cre-
ate type marginals as follows. For a sentence i and
word position j in that sentence, let T (i, j) be the
2In the first iteration, we initialize the target domain param-
eters to the source domain parameters: ?(t)0 = ?
(s).
trigram (graph node) centered at position j. Con-
versely, for a trigram type u, let T?1(u) be the set
of actual occurrences (tokens) of that trigram u; that
is, all pairs (i, j) where i is the index of a sentence
where u occurs and j is the position of the center
word of an occurrence of u in that sentence. We cal-
culate type-level posteriors as follows:
qu(y) ,
1
|T?1(u)|
?
(i,j)?T?1(u)
p(y(j)i |xi; ?
(t)
n ) .
This combination rule connects the token-centered
CRF with the type-centered graph. Other ways
of combining the token marginals, such as using
weights derived from the entropies of marginals,
might be worth investigating.
4.3 Graph Propagation
We now use our similarity graph (Section 3) to
smooth the type-level marginals by minimizing the
following convex objective:
C(q) =
?
u?Vl
?ru ? qu?
2
+ ?
?
u?V,v?N (i)
wuv?qu ? qv?
2 + ?
?
u?V
?qu ? U?
2
s.t.
?
y
qu(y) = 1 ?u & qu(y) ? 0 ?u, y (2)
where q = {q1, q2, . . . q|V |}. The setting of the
hyperparameters ? and ? will be discussed in Sec-
tion 6, N (u) is the set of neighbors of node u, and
ru is the empirical marginal label distribution for tri-
gram u in the labeled data. We use a squared loss to
penalize neighboring nodes that have different label
distributions: ?qu ? qv?2 =
?
y(qu(y) ? qv(y))
2,
additionally regularizing the label distributions to-
wards the uniform distribution U over all possible
labels Y . It can be shown that the above objective is
convex in q.
Our graph propagation objective can be seen as a
multi-class generalization of the quadratic cost crite-
rion (Bengio et al, 2007). The first term in the above
objective requires that we respect the information
in our labeled data. The second term is the graph
smoothness regularizer which requires that the qi?s
be smooth with respect to the graph. In other words,
if wuv is large, then qu and qv should be close in the
171
squared-error sense. This implies that vertices u and
v are likely to have similar marginals over POS tags.
The last term is a regularizer and encourages all type
marginals to be uniform to the extent that is allowed
by the first two terms. If a unlabeled vertex does
not have a path to any labeled vertex, this term en-
sures that the converged marginal for this vertex will
be uniform over all tags, ensuring that our algorithm
performs at least as well as a standard self-training
based algorithm, as we will see later.
While the objective in Equation 2 admits a closed
form solution, it involves inverting a matrix of or-
der |V | and thus we use instead the simple iterative
update given by
q(m)u (y) =
?u(y)
?u
where
?u(y) = ru(y)?(u ? Vl)
+
?
v?N (u)
wuvq
(m?1)
v (y) + ?U(y),
?u = ?(u ? Vl) + ? + ?
?
v?N (u)
wuv (3)
where m is the iteration index and ? is the indica-
tor function that returns 1 if and only if the con-
dition is true. The iterative procedure starts with
q(0)u (y) = qu(y) as given in the previous section.
In all our experiments we run 10 iterations of the
above algorithm, and we denote the type marginals
at completion by q?u(y).
4.4 Viterbi Decoding
Given the type marginals computed in the previous
step, we interpolate them with the original CRF to-
ken marginals. This interpolation between type and
token marginals encourages similar n-grams to have
similar posteriors, while still allowing n-grams in
different sentences to differ in their posteriors. For
each unlabeled sentence i and word position j in it,
we calculate the following interpolated tag marginal:
p?(y(j)i = y|xi) = ?p(y
(j)
i = y|xi; ?
(t)
n )
+ (1? ?)q?T (m,n)(y) (4)
where ? is a mixing coefficient which reflects the
relative confidence between the original posteriors
from the CRF and the smoothed posteriors from the
graph. We discuss how we set ? in Section 6.
The interpolated marginals summarize all the in-
formation obtained so far about the tag distribution
at each position. However, if we were to use them on
their own to select the most likely POS tag sequence,
the first-order tag dependencies modeled by the CRF
would be mostly ignored. This happens because the
type marginals obtained from the graph after label
propagation will have lost most of the sequence in-
formation. To enforce the first-order tag dependen-
cies we therefore use Viterbi decoding over the com-
bined interpolated marginals and the CRF transition
potentials to compute the best POS tag sequence for
each unlabeled sentence. We refer to these 1-best
transcripts as y?i , i ? Du.
4.5 Re-training the CRF
Now that we have successfully labeled the unlabeled
target domain data, we can use it in conjunction with
the source domain labeled data to re-train the CRF:
?(t)n+1 =argmin
??RK
[
?
l?
i=1
log p(yi|xi; ?(t)n )
? ?
l+u?
i=l+1
log p(y?i |xi; ?
(t)
n )+????
2
]
(5)
where ? and ? are hyper-parameters whose setting
we discuss in Section 6. Given the new CRF pa-
rameters ? we loop back to step 1 (Section 4.1) and
iterate until convergence. It is important to note that
every step of our algorithm is convex, although their
combination clearly is not.
5 Related Work
Our work differs from previous studies of
SSL (Blitzer et al, 2006; III, 2007; Huang
and Yates, 2009) for improving POS tagging in
several ways. First, our algorithm can be general-
ized to other structured semi-supervised learning
problems, although POS tagging is our motivating
task and test application. Unlike III (2007), we
do not require target domain labeled data. While
the SCL algorithm (Blitzer et al, 2006) has been
evaluated without target domain labeled data, that
evaluation was to some extent transductive in that
the target test data (unlabeled) was included in the
unsupervised stage of SCL training that creates the
structural correspondence between the two domains.
172
We mentioned already the algorithm of Altun et
al. (2005), which is unlikely to scale up because
its dual formulation requires the inversion of a ma-
trix whose size depends on the graph size. Gupta
et al (2009) also constrain similar trigrams to have
similar POS tags by forming cliques of similar tri-
grams and maximizing the agreement score over
these cliques. Computing clique agreement poten-
tials however is NP-hard and so they propose ap-
proximation algorithms that are still quite complex
computationally. We achieve similar effects by us-
ing our simple, scalable convex graph regularization
framework. Further, unlike other graph-propagation
algorithms (Alexandrescu and Kirchhoff, 2009), our
approach is inductive. While one might be able
to make inductive extensions of transductive ap-
proaches (Sindhwani et al, 2005), these usually re-
quire extensive computational resources at test time.
6 Experiments and Results
We use the Wall Street Journal (WSJ) section of
the Penn Treebank as our labeled source domain
training set. We follow standard setup procedures
for this task and train on sections 00-18, compris-
ing of 38,219 POS-tagged sentences with a total of
912,344 words. To evaluate our domain-adaptation
approach, we consider two different target domains:
questions and biomedical data. Both target do-
mains are relatively far from the source domain
(newswire), making this a very challenging task.
The QuestionBank (Judge et al, 2006), provides
an excellent corpus consisting of 4,000 questions
that were manually annotated with POS tags and
parse trees. We used the first half as our develop-
ment set and the second half as our test set. Ques-
tions are difficult to tag with WSJ-trained taggers
primarily because the word order is very different
than that of the mostly declarative sentences in the
training data. Additionally, the unknown word rate
is more than twice as high as on the in-domain de-
velopment set (7.29% vs. 3.39%). As our unla-
beled data, we use a set of 10 million questions
collected from anonymized Internet search queries.
These queries were selected to be similar in style
and length to the questions in the QuestionBank.3
3In particular, we selected queries that start with an English
function word that can be used to start a question (what, who,
As running the CRF over 10 million sentences can
be rather cumbersome and probably unnecessary, we
randomly select 100,000 of these queries and treat
this asDu. Because the graph nodes and the features
used in the similarity function are based on n-grams,
data sparsity can be a serious problem, and we there-
fore use the entire unlabeled data set for graph con-
struction. We estimate the mutual information-based
features for each trigram type over all the 10 million
questions, and then construct the graph over only
the set of trigram types that actually occurs in the
100,000 random subset and the WSJ training set.
For our second target domain, we use the Penn
BioTreebank (PennBioIE, 2005). This corpus con-
sists of 1,061 sentences that have been manually an-
notated with POS tags. We used the first 500 sen-
tences as a development set and the remaining 561
sentences as our final test set. The high unknown
word rate (23.27%) makes this corpus very difficult
to tag. Furthermore, the POS tag set for this data is a
super-set of the Penn Treebank?s, including the two
new tags HYPH (for hyphens) and AFX (for com-
mon post-modifiers of biomedical entities such as
genes). These tags were introduced due to the im-
portance of hyphenated entities in biomedical text,
and are used for 1.8% of the words in the test set.
Any tagger trained only on WSJ text will automati-
cally predict wrong tags for those words. For unla-
beled data we used 100,000 sentences that were cho-
sen by searching MEDLINE for abstracts pertaining
to cancer, in particular genomic variations and muta-
tions (Blitzer et al, 2006). Since we did not have ac-
cess to additional unlabeled data, we used the same
set of sentences as target domain unlabeled data,Du.
The graph here was constructed over the 100,000 un-
labeled sentences and the WSJ training set. Finally,
we remind the reader that we did not use label infor-
mation for graph construction in either corpus.
6.1 Baselines
Our baseline supervised CRF is competitive
with state-of-the-art discriminative POS taggers
(Toutanova et al, 2003; Shen et al, 2007), achieving
97.17% on the WSJ development set (sections 19-
21). We use a fairly standard set of features, includ-
ing word identity, suffixes and prefixes and detectors
when, etc.), and have between 30 and 160 characters.
173
Questions Bio
Dev Eval Dev Eval
Supervised CRF 84.8 83.8 86.5 86.2
Self-trained CRF 85.4 84.0 87.5 87.1
Semi-supervised CRF 87.6 86.8 87.5 87.6
Table 2: Domain adaptation experiments. POS tagging accuracies in %.
for special characters such as dashes and digits. We
do not use of observation-dependent transition fea-
tures. Both supervised and semi-supervised models
are regularized with a squared `2-norm regularizer
with weight set to 0.01.
In addition to the supervised baseline trained ex-
clusively on the WSJ, we also consider a semi-
supervised self-trained baseline (?Self-trained CRF?
in Table 2). In this approach, we first train a su-
pervised CRF on the labeled data and then do semi-
supervised training without label propagation. This
is different from plain self-training because it aggre-
gates the posteriors over tokens into posteriors over
types. This aggregation step allows instances of the
same trigram in different sentences to share infor-
mation and works better in practice than direct self-
training on the output of the supervised CRF.
6.2 Domain Adaptation Results
The data set obtained concatenating the WSJ train-
ing set with the 10 million questions had about 20
million trigram types. Of those, only about 1.1 mil-
lion trigram types occurred in the WSJ training set
or in the 100,000 sentence sub-sample. For the
biomedical domain, the graph had about 2.2 mil-
lion trigrams. For all our experiments we set hy-
perparameters as follows: for graph propagation,
? = 0.5, ? = 0.01, for Viterbi decoding mixing,
? = 0.6, for CRF re-training, ? = 0.001, ? = 0.01.
These parameters were chosen based on develop-
ment set performance. All CRF objectives were op-
timized using L-BFGS (Bertsekas, 2004).
Table 2 shows the results for both domains. For
the question corpus, the supervised CRF performs
at only 85% on the development set. While it is al-
most impossible to improve in-domain tagging ac-
curacy and tagging is therefore considered a solved
problem by many, these results clearly show that
the problem is far from solved. Self-training im-
proves over the baseline by about 0.6% on the de-
velopment set. However the gains from self-training
are more modest (0.2%) on the evaluation (test) set.
Our approach is able to provide a more solid im-
provement of about 3% absolute over the super-
vised baseline and about 2% absolute over the self-
trained system on the question development set. Un-
like self-training, on the question evaluation set, our
approach provides about 3% absolute improvement
over the supervised baseline. For the biomedical
data, while the performances of our approach and
self-training are statistically indistinguishable on the
development set, we see modest gains of about 0.5%
absolute on the evaluation set. On the same data, we
see that our approach provides about 1.4% absolute
improvement over the supervised baseline.
7 Analysis & Conclusion
The results suggest that our proposed approach pro-
vides higher gains relative to self-training on the
question data than on the biomedical corpus. We
hypothesize that this caused by sparsity in the graph
generated from the biomedical dataset. For the ques-
tions graph, the PMI statistics were estimated over
10 million sentences while in the case of the biomed-
ical dataset, the same statistics were computed over
just 100,000 sentences. We hypothesize that the lack
of well-estimated features in the case of the biomed-
ical dataset leads to a sparse graph.
To verify the above hypothesis, we measured the
percentage of trigrams that occur in the target do-
main (unlabeled) data that do not have any path to
a trigram in the source domain data, and the aver-
age minimum path length between a trigram in the
target data and a trigram in the source data (when
such a path exists). The results are shown in Ta-
ble 3. For the biomedical data, close to 50% of the
trigrams from the target data do not have a path to
a trigram from the source data. Even when such a
path exists, the average path length is about 22. On
174
Questions Bio
% of unlabeled trigrams
12.4 46.8not connected to
any labeled trigrams
average path length
9.4 22.4
between an unlabeled
trigram and its nearest
labeled trigram
Table 3: Analysis of the graphs constructed for the two
datasets discussed in Section 6. Unlabeled trigrams occur
in the target domain only. Labeled trigrams occur at least
once in the WSJ training data.
the other hand, for the question corpus, only about
12% of the target domain trigrams are disconnected,
and the average path length is about 9. These re-
sults clearly show the sparse nature of the biomed-
ical graph. We believe that it is this sparsity that
causes the graph propagation to not have a more no-
ticeable effect on the final performance. It is note-
worthy that making use of even such a sparse graph
does not lead to any degradation in results, which we
attribute to the choice of graph-propagation regular-
izer (Section 4.3).
We presented a simple, scalable algorithm for
training structured prediction models in a semi-
supervised manner. The approach is based on using
as a regularizer a nearest-neighbor graph constructed
over trigram types. Our results show that the ap-
proach not only scales to large datasets but also pro-
duces significantly improved tagging accuracies.
References
A. Alexandrescu and K. Kirchhoff. 2009. Graph-based
learning for statistical machine translation. In NAACL.
Y. Altun, D. McAllester, and M. Belkin. 2005. Max-
imum margin semi-supervised learning for structured
variables. In Advances in Neural Information Process-
ing Systems 18, page 18.
M. Belkin, P. Niyogi, and V. Sindhwani. 2005. On man-
ifold regularization. In Proc. of the Conference on Ar-
tificial Intelligence and Statistics (AISTATS).
Y. Bengio, O. Delalleau, and N. L. Roux, 2007. Semi-
Supervised Learning, chapter Label Propogation and
Quadratic Criterion. MIT Press.
D Bertsekas. 2004. Nonlinear Programming. Athena
Scientific Publishing.
J. Blitzer and J. Zhu. 2008. ACL 2008 tutorial on Semi-
Supervised learning.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
EMNLP ?06.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT: Proceed-
ings of the Workshop on Computational Learning The-
ory.
U. Brefeld and T. Scheffer. 2006. Semi-supervised learn-
ing for structured output variables. In ICML06, 23rd
International Conference on Machine Learning.
O. Chapelle, B. Scholkopf, and A. Zien. 2007. Semi-
Supervised Learning. MIT Press.
R. Collobert, F. Sinz, J. Weston, L. Bottou, and
T. Joachims. 2006. Large scale transductive svms.
Journal of Machine Learning Research.
A. Corduneanu and T. Jaakkola. 2003. On informa-
tion regularization. In Uncertainty in Artificial Intelli-
gence.
Y. Grandvalet and Y. Bengio. 2005. Semi-supervised
learning by entropy minimization. In CAP.
R. Gupta, S. Sarawagi, and A. A. Diwan. 2009. General-
ized collective inference with symmetric clique poten-
tials. CoRR, abs/0907.0589.
G. R. Haffari and A. Sarkar. 2007. Analysis of semi-
supervised learning with the Yarowsky algorithm. In
UAI.
F. Huang and A. Yates. 2009. Distributional represen-
tations for handling sparsity in supervised sequence-
labeling. In ACL-IJCNLP ?09: Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1. Association for Computational Linguistics.
H. Daume III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
T. Joachims. 1999. Transductive inference for text clas-
sification using support vector machines. In Proc. of
the International Conference on Machine Learning
(ICML).
Thorsten Joachims. 2003. Transductive learning via
spectral graph partitioning. In Proc. of the Interna-
tional Conference on Machine Learning (ICML).
J. Judge, A. Cahill, and J. van Genabith. 2006. Question-
bank: Creating a corpus of parse-annotated questions.
In Proceedings of the 21st International Conference
on Computational Linguist ics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 497?504.
175
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of the In-
ternational Conference on Machine Learning (ICML).
N. D. Lawrence and M. I. Jordan. 2005. Semi-supervised
learning via gaussian processes. In NIPS.
PennBioIE. 2005. Mining the bibliome project. In
http://bioie.ldc.upenn.edu/.
H. J. Scudder. 1965. Probability of Error of some Adap-
tive Pattern-Recognition Machines. IEEE Transac-
tions on Information Theory, 11.
M. Seeger. 2000. Learning with labeled and unlabeled
data. Technical report, University of Edinburgh, U.K.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learning
for bidirectional sequence classification. In ACL ?07.
V. Sindhwani, P. Niyogi, and M. Belkin. 2005. Beyond
the point cloud: from transductive to semi-supervised
learning. In Proc. of the International Conference on
Machine Learning (ICML).
A. Subramanya and J. A. Bilmes. 2009. Entropic graph
regularization in non-parametric semi-supervised clas-
sification. In Neural Information Processing Society
(NIPS), Vancouver, Canada, December.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL ?03.
Y. Wang, G. Haffari, S. Wang, and G. Mori. 2009.
A rate distortion approach for semi-supervised condi-
tional random fields.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In Proc. of the International Con-
ference on Machine Learning (ICML).
X. Zhu. 2005. Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences, Uni-
versity of Wisconsin-Madison.
176
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705?713,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Uptraining for Accurate Deterministic Question Parsing
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, Hiyan Alshawi
Google Research
{slav,pichuan,ringgaard,hiyan}@google.com
Abstract
It is well known that parsing accuracies drop
significantly on out-of-domain data. What is
less known is that some parsers suffer more
from domain shifts than others. We show
that dependency parsers have more difficulty
parsing questions than constituency parsers.
In particular, deterministic shift-reduce depen-
dency parsers, which are of highest interest
for practical applications because of their lin-
ear running time, drop to 60% labeled accu-
racy on a question test set. We propose an
uptraining procedure in which a deterministic
parser is trained on the output of a more ac-
curate, but slower, latent variable constituency
parser (converted to dependencies). Uptrain-
ing with 100K unlabeled questions achieves
results comparable to having 2K labeled ques-
tions for training. With 100K unlabeled and
2K labeled questions, uptraining is able to
improve parsing accuracy to 84%, closing
the gap between in-domain and out-of-domain
performance.
1 Introduction
Parsing accuracies on the popular Section 23 of the
Wall Street Journal (WSJ) portion of the Penn Tree-
bank have been steadily improving over the past
decade. At this point, we have many different pars-
ing models that reach and even surpass 90% depen-
dency or constituency accuracy on this test set (Mc-
Donald et al, 2006; Nivre et al, 2007; Charniak and
Johnson, 2005; Petrov et al, 2006; Carreras et al,
2008; Koo and Collins, 2010). Quite impressively,
models based on deterministic shift-reduce parsing
algorithms are able to rival the other computation-
ally more expensive models (see Nivre (2008) and
references therein for more details). Their linear
running time makes them ideal candidates for large
scale text processing, and our model of choice for
this paper.
Unfortunately, the parsing accuracies of all mod-
els have been reported to drop significantly on out-
of-domain test sets, due to shifts in vocabulary and
grammar usage (Gildea, 2001; McClosky et al,
2006b; Foster, 2010). In this paper, we focus our
attention on the task of parsing questions. Questions
pose interesting challenges for WSJ-trained parsers
because they are heavily underrepresented in the
training data (there are only 334 questions among
the 39,832 training sentences). At the same time,
questions are of particular interest for user facing
applications like question answering or web search,
which necessitate parsers that can process questions
in a fast and accurate manner.
We start our investigation in Section 3 by train-
ing several state-of-the-art (dependency and con-
stituency) parsers on the standard WSJ training set.
When evaluated on a question corpus, we observe
dramatic accuracy drops exceeding 20% for the de-
terministic shift-reduce parsers. In general, depen-
dency parsers (McDonald et al, 2006; Nivre et al,
2007), seem to suffer more from this domain change
than constituency parsers (Charniak and Johnson,
2005; Petrov et al, 2006). Overall, the latent vari-
able approach of Petrov et al (2006) appears to gen-
eralize best to this new domain, losing only about
5%. Unfortunately, the parsers that generalize better
to this new domain have time complexities that are
cubic in the sentence length (or even higher), render-
ing them impractical for web-scale text processing.
705
SBARQ
WHNP
WP
What
SQ
VBZ
does
NP
DT
the
NNP
Peugeot
NN
company
VP
VB
manufacture
.
?
(a)
What does the Peugeot company manufacture ?ROOT
dobj aux det nn p   nsubjroot
(b)
Figure 1: Example constituency tree from the QuestionBank (a) converted to labeled Stanford dependencies (b).
We therefore propose an uptraining method, in
which a deterministic shift-reduce parser is trained
on the output of a more accurate, but slower parser
(Section 4). This type of domain adaptation is rem-
iniscent of self-training (McClosky et al, 2006a;
Huang and Harper, 2009) and co-training (Blum and
Mitchell, 1998; Sagae and Lavie, 2006), except that
the goal here is not to further improve the perfor-
mance of the very best model. Instead, our aim is
to train a computationally cheaper model (a linear
time dependency parser) to match the performance
of the best model (a cubic time constituency parser),
resulting in a computationally efficient, yet highly
accurate model.
In practice, we parse a large amount of unlabeled
data from the target domain with the constituency
parser of Petrov et al (2006) and then train a deter-
ministic dependency parser on this noisy, automat-
ically parsed data. The accuracy of the linear time
parser on a question test set goes up from 60.06%
(LAS) to 76.94% after uptraining, which is compa-
rable to adding 2,000 labeled questions to the train-
ing data. Combining uptraining with 2,000 labeled
questions further improves the accuracy to 84.14%,
fully recovering the drop between in-domain and
out-of-domain accuracy.
We also present a detailed error analysis in Sec-
tion 5, showing that the errors of the WSJ-trained
model are primarily caused by sharp changes in syn-
tactic configurations and only secondarily due to
lexical shifts. Uptraining leads to large improve-
ments across all error metrics and especially on im-
portant dependencies like subjects (nsubj).
2 Experimental Setup
We used the following experimental protocol
throughout the paper.
2.1 Data
Our main training set consists of Sections 02-21 of
the Wall Street Journal portion of the Penn Treebank
(Marcus et al, 1993), with Section 22 serving as de-
velopment set for source domain comparisons. For
our target domain experiments, we evaluate on the
QuestionBank (Judge et al, 2006), which includes
a set of manually annotated questions from a TREC
question answering task. The questions in the Ques-
tionBank are very different from our training data in
terms of grammatical constructions and vocabulary
usage, making this a rather extreme case of domain-
adaptation. We split the 4,000 questions contained
in this corpus in three parts: the first 2,000 ques-
tions are reserved as a small target-domain training
set; the remaining 2,000 questions are split in two
equal parts, the first serving as development set and
the second as our final test set. We report accuracies
on the developments sets throughout this paper, and
test only at the very end on the final test set.
We convert the trees in both treebanks from con-
stituencies to labeled dependencies (see Figure 1)
using the Stanford converter, which produces 46
types of labeled dependencies1 (de Marneffe et al,
2006). We evaluate on both unlabeled (UAS) and
labeled dependency accuracy (LAS).2
Additionally, we use a set of 2 million ques-
tions collected from Internet search queries as unla-
beled target domain data. All user information was
anonymized and only the search query string was re-
tained. The question sample is selected at random
after passing two filters that select queries that are
1We use the Stanford Lexicalized Parser v1.6.2.
2Because the QuestionBank does not contain function tags,
we decided to strip off the function tags from the WSJ be-
fore conversion. The Stanford conversion only uses the -ADV
and -TMP tags, and removing all function tags from the WSJ
changed less than 0.2% of the labels (primarily tmod labels).
706
Training on Evaluating on WSJ Section 22 Evaluating on QuestionBank
WSJ Sections 02-21 F1 UAS LAS POS F1 UAS LAS POS
Nivre et al (2007) ? 88.42 84.89 95.00 ? 74.14 62.81 88.48
McDonald et al (2006) ? 89.47 86.43 95.00 ? 80.01 67.00 88.48
Charniak (2000) 90.27 92.33 89.86 96.71 83.01 85.61 73.59 90.49
Charniak and Johnson (2005) 91.92 93.56 91.24 96.69 84.47 87.13 75.94 90.59
Petrov et al (2006) 90.70 92.91 90.48 96.27 85.52 88.17 79.10 90.57
Petrov (2010) 92.10 93.85 91.60 96.44 86.62 88.77 79.92 91.08
Our shift-reduce parser ? 88.24 84.69 95.00 ? 72.23 60.06 88.48
Our shift-reduce parser (gold POS) ? 90.51 88.53 100.00 ? 78.30 68.92 100.00
Table 1: Parsing accuracies for parsers trained on newswire data and evaluated on newswire and question test sets.
similar in style to the questions in the QuestionBank:
(i) the queries must start with an English function
word that can be used to start a question (what, who
when, how, why, can, does, etc.), and (ii) the queries
have a maximum length of 160 characters.
2.2 Parsers
We use multiple publicly available parsers, as well
as our own implementation of a deterministic shift-
reduce parser in our experiments. The depen-
dency parsers that we compare are the determinis-
tic shift-reduce MaltParser (Nivre et al, 2007) and
the second-order minimum spanning tree algorithm
based MstParser (McDonald et al, 2006). Our shift-
reduce parser is a re-implementation of the Malt-
Parser, using a standard set of features and a lin-
ear kernel SVM for classification. We also train and
evaluate the generative lexicalized parser of Char-
niak (2000) on its own, as well as in combination
with the discriminative reranker of Charniak and
Johnson (2005). Finally, we run the latent variable
parser (a.k.a. BerkeleyParser) of Petrov et al (2006),
as well as the recent product of latent variable gram-
mars version (Petrov, 2010). To facilitate compar-
isons between constituency and dependency parsers,
we convert the output of the constituency parsers to
labeled dependencies using the same procedure that
is applied to the treebanks. We also report their F1
scores for completeness.
While the constituency parsers used in our experi-
ments view part-of-speech (POS) tagging as an inte-
gral part of parsing, the dependency parsers require
the input to be tagged with a separate POS tagger.
We use the TnT tagger (Brants, 2000) in our experi-
ments, because of its efficiency and ease of use. Tag-
ger and parser are always trained on the same data.
3 Parsing Questions
We consider two domain adaptation scenarios in this
paper. In the first scenario (sometimes abbreviated
as WSJ), we assume that we do not have any labeled
training data from the target domain. In practice, this
will always be the case when the target domain is
unknown or very diverse. The second scenario (ab-
breviated as WSJ+QB) assumes a small amount of
labeled training data from the target domain. While
this might be expensive to obtain, it is certainly fea-
sible for narrow domains (e.g. questions), or when a
high parsing accuracy is really important.
3.1 No Labeled Target Domain Data
We first trained all parsers on the WSJ training set
and evaluated their performance on the two domain
specific evaluation sets (newswire and questions).
As can be seen in the left columns of Table 1, all
parsers perform very well on the WSJ development
set. While there are differences in the accuracies,
all scores fall within a close range. The table also
confirms the commonly known fact (Yamada and
Matsumoto, 2003; McDonald et al, 2005) that con-
stituency parsers are more accurate at producing de-
pendencies than dependency parsers (at least when
the dependencies were produced by a deterministic
transformation of a constituency treebank, as is the
case here).
This picture changes drastically when the per-
formance is measured on the QuestionBank devel-
opment set (right columns in Table 1). As one
707
Evaluating on Training on WSJ + QB Training on QuestionBank
QuestionBank F1 UAS LAS POS F1 UAS LAS POS
Nivre et al (2007) ? 83.54 78.85 91.32 ? 79.72 73.44 88.80
McDonald et al (2006) ? 84.95 80.17 91.32 ? 82.52 77.20 88.80
Charniak (2000) 89.40 90.30 85.01 94.17 79.70 76.69 69.69 87.84
Petrov et al (2006) 90.96 90.98 86.90 94.01 86.62 84.09 78.92 87.56
Petrov (2010) 92.81 92.23 88.84 94.48 87.72 85.07 80.08 87.79
Our shift-reduce parser ? 83.70 78.27 91.32 ? 80.44 74.29 88.80
Our shift-reduce parser (gold POS) ? 89.39 86.60 100.00 ? 87.31 84.15 100.00
Table 2: Parsing accuracies for parsers trained on newswire and question data and evaluated on a question test set.
might have expected, the accuracies are significantly
lower, however, the drop for some of the parsers
is shocking. Most notably, the deterministic shift-
reduce parsers lose almost 25% (absolute) on la-
beled accuracies, while the latent variable parsers
lose around 12%.3 Note also that even with gold
POS tags, LAS is below 70% for our determinis-
tic shift-reduce parser, suggesting that the drop in
accuracy is primarily due to a syntactic shift rather
than a lexical shift. These low accuracies are espe-
cially disturbing when one considers that the aver-
age question in the evaluation set is only nine words
long and therefore potentially much less ambiguous
than WSJ sentences. We will examine the main error
types more carefully in Section 5.
Overall, the dependency parsers seem to suf-
fer more from the domain change than the con-
stituency parsers. One possible explanation is that
they lack the global constraints that are enforced by
the (context-free) grammars. Even though the Mst-
Parser finds the globally best spanning tree, all con-
straints are local. This means for example, that it
is not possible to require the final parse to contain
a verb (something that can be easily expressed by
a top-level production of the form S ? NP VP in a
context free grammar). This is not a limitation of de-
pendency parsers in general. For example, it would
be easy to enforce such constraints in the Eisner
(1996) algorithm or using Integer Linear Program-
ming approaches (Riedel and Clarke, 2006; Martins
et al, 2009). However, such richer modeling capac-
ity comes with a much higher computational cost.
Looking at the constituency parsers, we observe
3The difference between our shift-reduce parser and the
MaltParser are due to small differences in the feature sets.
that the lexicalized (reranking) parser of Charniak
and Johnson (2005) loses more than the latent vari-
able approach of Petrov et al (2006). This differ-
ence doesn?t seem to be a difference of generative
vs. discriminative estimation. We suspect that the
latent variable approach is better able to utilize the
little evidence in the training data. Intuitively speak-
ing, some of the latent variables seem to get alo-
cated for modeling the few questions present in the
training data, while the lexicalization contexts are
not able to distinguish between declarative sentences
and questions.
To verify this hypothesis, we conducted two addi-
tional experiments. In the first experiment, we col-
lapsed the question specific phrasal categories SQ
and SBARQ to their declarative sentence equivalents
S and SBAR. When the training and test data are
processed this way, the lexicalized parser loses 1.5%
F1, while the latent variable parser loses only 0.7%.
It is difficult to examine the grammars, but one can
speculate that some of the latent variables were used
to model the question specific constructions and the
model was able to re-learn the distinctions that we
purposefully collapsed. In the second experiment,
we removed all questions from the WSJ training set
and retrained both parsers. This did not make a
significant difference when evaluating on the WSJ
development set, but of course resulted in a large
performance drop when evaluating on the Question-
Bank. The lexicalized parser came out ahead in this
experiment,4 confirming our hypothesis that the la-
tent variable model is better able to pick up the small
amount of relevant evidence that is present in the
WSJ training data (rather than being systematically
4The F1 scores were 52.40% vs. 56.39% respectively.
708
better suited for modeling questions).
3.2 Some Labeled Target Domain Data
In the above experiments, we considered a situation
where we have no labeled training data from the tar-
get domain, as will typically be the case. We now
consider a situation where a small amount of labeled
data (2,000 manually parsed sentences) from the do-
main of interest is available for training.
We experimented with two different ways of uti-
lizing this additional training data. In a first experi-
ment, we trained models on the concatenation of the
WSJ and QuestionBank training sets (we did not at-
tempt to weight the different corpora). As Table 2
shows (left columns), even a modest amount of la-
beled data from the target domain can significantly
boost parsing performance, giving double-digit im-
provements in some cases. While not shown in the
table, the parsing accuracies on the WSJ develop-
ment set where largely unaffected by the additional
training data.
Alternatively, one can also train models exclu-
sively on the QuestionBank data, resulting in ques-
tion specific models. The parsing accuracies of
these domain-specific models are shown in the right
columns of Table 2, and are significantly lower than
those of models trained on the concatenated training
sets. They are often times even lower than the results
of parsers trained exclusively on the WSJ, indicating
that 2,000 sentences are not sufficient to train accu-
rate parsers, even for quite narrow domains.
4 Uptraining for Domain-Adaptation
The results in the previous section suggest that
parsers without global constraints have difficul-
ties dealing with the syntactic differences between
declarative sentences and questions. A possible ex-
planation is that similar word configurations can ap-
pear in both types of sentences, but with very differ-
ent syntactic interpretation. Local models without
global constraints are therefore mislead into dead-
end interpretations from which they cannot recover
(McDonald and Nivre, 2007). Our approach will
therefore be to use a large amount of unlabeled data
to bias the model towards the appropriate distribu-
tion for the target domain. Rather than looking
for feature correspondences between the domains
 70
 75
 80
 85
 90
1M100K10K1K100100
U
A
S
WSJ+QB
WSJ
 60
 65
 70
 75
 80
 85
1M100K10K1K100100
LA
S
Number of unlabeled questions
WSJ+QB
WSJ
Figure 2: Uptraining with large amounts of unlabeled
data gives significant improvements over two different
supervised baselines.
(Blitzer et al, 2006), we propose to use automati-
cally labeled target domain data to learn the target
domain distribution directly.
4.1 Uptraining vs. Self-training
The idea of training parsers on their own output has
been around for as long as there have been statis-
tical parsers, but typically does not work well at
all (Charniak, 1997). Steedman et al (2003) and
Clark et al (2003) present co-training procedures
for parsers and taggers respectively, which are ef-
fective when only very little labeled data is avail-
able. McClosky et al (2006a) were the first to im-
prove a state-of-the-art constituency parsing system
by utilizing unlabeled data for self-training. In sub-
sequent work, they show that the same idea can be
used for domain adaptation if the unlabeled data is
chosen accordingly (McClosky et al, 2006b). Sagae
and Tsujii (2007) co-train two dependency parsers
by adding automatically parsed sentences for which
the parsers agree to the training data. Finally, Suzuki
et al (2009) present a very effective semi-supervised
approach in which features from multiple generative
models estimated on unlabeled data are combined in
a discriminative system for structured prediction.
All of these approaches have in common that their
ultimate goal is to improve the final performance.
Our work differs in that instead of improving the
709
Uptraining with Using only WSJ data Using WSJ + QB data
different base parsers UAS LAS POS UAS LAS POS
Baseline 72.23 60.06 88.48 83.70 78.27 91.32
Self-training 73.62 61.63 89.60 84.26 79.15 92.09
Uptraining on Petrov et al (2006) 86.02 76.94 90.75 88.38 84.02 93.63
Uptraining on Petrov (2010) 85.21 76.19 90.74 88.63 84.14 93.53
Table 3: Uptraining substantially improves parsing accuracies, while self-training gives only minor improvements.
performance of the best parser, we want to build
a more efficient parser that comes close to the ac-
curacy of the best parser. To do this, we parse
the unlabeled data with our most accurate parser
and generate noisy, but fairly accurate labels (parse
trees) for the unlabeled data. We refer to the parser
used for producing the automatic labels as the base
parser (unless otherwise noted, we used the latent
variable parser of Petrov et al (2006) as our base
parser). Because the most accurate base parsers are
constituency parsers, we need to convert the parse
trees to dependencies using the Stanford converter
(see Section 2). The automatically parsed sentences
are appended to the labeled training data, and the
shift-reduce parser (and the part-of-speech tagger)
are trained on this new training set. We did not
increase the weight of the WSJ training data, but
weighted the QuestionBank training data by a fac-
tor of ten in the WSJ+QB experiments.
4.2 Varying amounts of unlabeled data
Figure 2 shows the efficacy of uptraining as a func-
tion of the size of the unlabeled data. Both la-
beled (LAS) and unlabeled accuracies (UAS) im-
prove sharply when automatically parsed sentences
from the target domain are added to the training data,
and level off after 100,000 sentences. Comparing
the end-points of the dashed lines (models having
access only to labeled data from the WSJ) and the
starting points of the solid lines (models that have
access to both WSJ and QuestionBank), one can see
that roughly the same improvements (from 72% to
86% UAS and from 60% to 77% LAS) can be ob-
tained by having access to 2,000 labeled sentences
from the target domain or uptraining with a large
amount of unlabeled data from the target domain.
The benefits seem to be complementary and can be
combined to give the best results. The final accu-
racy of 88.63 / 84.14 (UAS / LAS) on the question
evaluation set is comparable to the in-domain per-
formance on newswire data (88.24 / 84.69).
4.3 Varying the base parser
Table 3 then compares uptraining on the output of
different base parsers to pure self-training. In these
experiments, the same set of 500,000 questions was
parsed by different base parsers. The automatic
parses were then added to the labeled training data
and the parser was retrained. As the results show,
self-training provides only modest improvements of
less than 2%, while uptraining gives double-digit
improvements in some cases. Interestingly, there
seems to be no substantial difference between up-
training on the output of a single latent variable
parser (Petrov et al, 2006) and a product of latent
variable grammars (Petrov, 2010). It appears that
the roughly 1% accuracy difference between the two
base parsers is not important for uptraining.
4.4 POS-less parsing
Our uptraining procedure improves parse quality on
out-of-domain data to the level of in-domain ac-
curacy. However, looking closer at Table 3, one
can see that the POS accuracy is still relatively low
(93.53%), potentially limiting the final accuracy.
To remove this limitation (and also the depen-
dence on a separate POS tagger), we experimented
with word cluster features. As shown in Koo et al
(2008), word cluster features can be used in con-
junction with POS tags to improve parsing accuracy.
Here, we use them instead of POS tags in order to
further reduce the domain-dependence of our model.
Similar to Koo et al (2008), we use the Brown clus-
tering algorithm (Brown et al, 1992) to produce a
deterministic hierarchical clustering of our input vo-
cabulary. We then extract features based on vary-
710
UAS LAS POS
Part-of-Speech Tags 88.35 84.05 93.53
Word Cluster Features 87.92 83.73 ?
Table 4: Parsing accuracies of uptrained parsers with and
without part-of-speech tags and word cluster features.
ing cluster granularities (6 and 10 bits in our experi-
ments). Table 4 shows that roughly the same level of
accuracy can be achieved with cluster based features
instead of POS tag features. This change makes our
parser completely deterministic and enables us to
process sentences in a single left-to-right pass.
5 Error Analysis
To provide a better understanding of the challenges
involved in parsing questions, we analyzed the er-
rors made by our WSJ-trained shift-reduce parser
and also compared them to the errors that are left
after uptraining.
5.1 POS errors
Many parsing errors can be traced back to POS tag-
ging errors, which are much more frequent on out-
of-domain data than on in-domain data (88.8% on
the question data compared to above 95.0% on WSJ
data). Part of the reason for the lower POS tagging
accuracy is the higher unknown word ratio (7.3% on
the question evaluation set, compared to 3.4% on the
WSJ evaluation set). Another reason is a change in
the lexical distribution.
For example, wh-determiners (WDT) are quite
rare in the WSJ training data (relative frequency
0.45%), but five times more common in the Ques-
tionBank training data (2.49%). In addition to this
frequency difference, 52.43% of the WDTs in the
WSJ are the word ?which? and 46.97% are?that?. In
the QuestionBank on the other hand, ?what? is by
far the most common WDT word (81.40%), while
?which? and ?that? account only for 13.65% and
4.94% respectively. Not surprisingly the most com-
mon POS error involves wh-determiners (typically
the word ?what?) being incorrectly labeled as Wh-
pronouns (WP), resulting in head and label errors
like the one shown in Figure 3(a).
To separate out POS tagging errors from parsing
errors, we also ran experiments with correct (gold)
Dep. Label Frequency WSJ Uptrained
nsubj 934 41.02 88.64
amod 556 78.21 86.00
dobj 555 70.10 83.12
attr 471 8.64 93.49
aux 467 77.31 82.56
Table 5: F1 scores for the most frequent labels in the
QuestionBank development set. Uptraining leads to huge
improvements compared to training only on the WSJ.
POS tags. The parsing accuracies of our shift-reduce
parser using gold POS tags are listed in the last rows
of Tables 1 and 2. Even with gold POS tags, the de-
terministic shift-reduce parser falls short of the ac-
curacies of the constituency parsers (with automatic
tags), presumably because the shift-reduce model is
making only local decisions and is lacking the global
constraints provided by the context-free grammar.
5.2 Dependency errors
To find the main error types, we looked at the most
frequent labels in the QuestionBank development
set, and analyzed the ones that benefited the most
from uptraining. Table 5 has the frequency and F-
scores of the dependency types that we are going to
discuss in the following. We also provide examples
which are illustrated in Figure 3.
nsubj: The WSJ-trained model is often producing
parses that are missing a subject (nsubj). Questions
like ?What is the oldest profession?? and ?When
was Ozzy Osbourne born?? should have ?profes-
sion? and ?Osbourne? as nsubjs, but in both cases
the WSJ-trained parser did not label any subj (see
Figures 3(b) and 3(c)). Another common error is to
mislabel nsubj. For example, the nsubj of ?What are
liver enzymes?? should be enzymes, but the WSJ-
trained parser labels ?What? as the nsubj, which
makes sense in a statement but not in a question.
amod: The model is overpredicting ?amod?, re-
sulting in low precision figures for this label. An
example is ?How many points make up a perfect
fivepin bowling score??. The Stanford dependency
uses ?How? as the head of ?many? in noun phrases
like ?How many points?, and the relation is a generic
?dep?. But in the WSJ model prediction, ?many?s?
head is ?points,? and the relation mislabeled as
amod. Since it?s an adjective preceding the noun,
711
What is the oldest profession ?
ROOT
det     amod proot attr nsubj
WP VBZ DT JJS NN .ROOT
det     amod proot   attrdep
WP VBZ DT JJS NN .
What is the oldest profession ?
When was Ozzy Osbourne born ?
ROOT WRB VBZ  NNP NNP VBN .
root padvmod aux nn nsubj
When was Ozzy Osbourne   born ?
ROOT WRB VBZ  NNP NNP   NNP .
   root    nnnn pcompl nsubj
What films featured the character ?
ROOT WDT NNS  VBD DT NN  NNP NNP .
Popeye Doyle
nsubj dep det    nn nn dobj
What films featured the character ?
ROOT WP NNS  VBD DT NN  NNP NNP .
Popeye Doyle
   nsubj    compl det    nn nn   root    ccomp(a)
(b)
(c)
(d)
How many people did Randy ?
ROOT WRB JJ  NNS VBD NNP  NNP VB .
Craft kill
  dobj dep aux    nn nsubj p
?
.
dep
How many people did Randy
ROOT WRB JJ  NNS VBD NNP  NNP VB
Craft kill
compl amod ccomp   nn nsubj pnsubjroot
Figure 3: Example questions from the QuestionBank development set and their correct parses (left), as well as the
predictions of a model trained on the WSJ (right).
the WSJ model often makes this mistake and there-
fore the precision is much lower when it doesn?t see
more questions in the training data.
dobj: The WSJ model doesn?t predict object ex-
traction well. For example, in ?How many people
did Randy Craft kill?? (Figure 3(d)), the direct ob-
ject of kill should be ?How many people.? In the
Stanford dependencies, the correct labels for this
noun phrase are ?dobj dep dep,? but the WSJ model
predicts ?compl amod nsubj.? This is a common
error caused by the different word order in ques-
tions. The uptrained model is much better at han-
dling these type of constructions.
attr: An attr (attributive) is a wh-noun phrase
(WHNP) complement of a copular verb. In the WSJ
training data, only 4,641 out of 950,028 dependen-
cies are attr (0.5%); in the QuestionBank training
data, 1,023 out of 17,069 (6.0%) are attr. As a con-
sequence, the WSJ model cannot predict this label
in questions very well.
aux: ?What does the abbreviation AIDS stand
for?? should have ?stand? as the main head of the
sentence, and ?does? as its aux. However, the WSJ
model labeled ?does? as the main head. Similar
patterns occur in many questions, and therefore the
WSJ has a very low recall rate.
In contrast, mostly local labels (that are not re-
lated to question/statement structure differences)
have a consistently high accuracy. For example: det
has an accuracy of 98.86% with the WSJ-trained
model, and 99.24% with the uptrained model.
6 Conclusions
We presented a method for domain adaptation of de-
terministic shift-reduce parsers. We evaluated mul-
tiple state-of-the-art parsers on a question corpus
and showed that parsing accuracies degrade substan-
tially on this out-of-domain task. Most notably, de-
terministic shift-reduce parsers have difficulty deal-
ing with the modified word order and lose more
than 20% in accuracy. We then proposed a simple,
yet very effective uptraining method for domain-
adaptation. In a nutshell, we trained a deterministic
shift-reduce parser on the output of a more accurate,
but slower parser. Uptraining with large amounts of
unlabeled data gives similar improvements as hav-
ing access to 2,000 labeled sentences from the target
domain. With 2,000 labeled questions and a large
amount of unlabeled questions, uptraining is able to
close the gap between in-domain and out-of-domain
accuracy.
712
Acknowledgements
We would like to thank Ryan McDonald for run-
ning the MstParser experiments and for many fruit-
ful discussions on this topic. We would also like to
thank Joakim Nivre for help with the MatlParser and
Marie-Catherine de Marneffe for help with the Stan-
ford Dependency Converter.
References
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
EMNLP ?06.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT ?98.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP ?00.
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-
cer. 1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In CoNLL ?08.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
In ACL?05.
E. Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In AI ?97.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
S. Clark, J. Curran, and M. Osborne. 2003. Bootstrap-
ping pos-taggers using unlabelled data. In CoNLL ?03.
M.-C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC ?06.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In COLING ?96.
J. Foster. 2010. ?cba to check the spelling?: Investigat-
ing parser performance on discussion forum posts. In
NAACL ?10.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP ?01.
Z. Huang and M. Harper. 2009. Self-training PCFG
grammars with latent annotations across languages. In
EMNLP ?09.
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
bank: creating a corpus of parse-annotated questions.
In ACL ?06.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In ACL ?10.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL ?08.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In ACL ?09.
D. McClosky, E. Charniak, and M. Johnson. 2006a. Ef-
fective self-training for parsing. In NAACL ?06.
D. McClosky, E. Charniak, and M. Johnson. 2006b.
Reranking and self-training for parser adaptation. In
ACL ?06.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
EMNLP ?07.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL
?05.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In CoNLL ?06.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kbler, S. Marinov, and E. Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2).
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
S. Petrov. 2010. Products of random latent variable
grammars. In NAACL ?10.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP ?06.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In NAACL ?06.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with lr models and parser ensem-
bles. In CoNLL ?07.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In EACL ?03.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In EMNLP
?09.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In IWPT
?03.
713
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 62?72,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Multi-Source Transfer of Delexicalized Dependency Parsers
Ryan McDonald
Google
New York, NY
ryanmcd@google.com
Slav Petrov
Google
New York, NY
slav@google.com
Keith Hall
Google
Zu?rich
kbhall@google.com
Abstract
We present a simple method for transferring
dependency parsers from source languages
with labeled training data to target languages
without labeled training data. We first demon-
strate that delexicalized parsers can be di-
rectly transferred between languages, produc-
ing significantly higher accuracies than unsu-
pervised parsers. We then use a constraint
driven learning algorithm where constraints
are drawn from parallel corpora to project the
final parser. Unlike previous work on project-
ing syntactic resources, we show that simple
methods for introducing multiple source lan-
guages can significantly improve the overall
quality of the resulting parsers. The projected
parsers from our system result in state-of-the-
art performance when compared to previously
studied unsupervised and projected parsing
systems across eight different languages.
1 Introduction
Statistical parsing has been one of the most active ar-
eas of research in the computational linguistics com-
munity since the construction of the Penn Treebank
(Marcus et al, 1993). This includes work on phrase-
structure parsing (Collins, 1997; Charniak, 2000;
Petrov et al, 2006), dependency parsing (McDonald
et al, 2005; Nivre et al, 2006) as well as a num-
ber of other formalisms (Clark and Curran, 2004;
Wang and Harper, 2004; Shen and Joshi, 2008).
As underlying modeling techniques have improved,
these parsers have begun to converge to high lev-
els of accuracy for English newswire text. Subse-
quently, researchers have begun to look at both port-
ing these parsers to new domains (Gildea, 2001; Mc-
Closky et al, 2006; Petrov et al, 2010) and con-
structing parsers for new languages (Collins et al,
1999; Buchholz and Marsi, 2006; Nivre et al, 2007).
One major obstacle in building statistical parsers
for new languages is that they often lack the manu-
ally annotated resources available for English. This
observation has led to a vast amount of research
on unsupervised grammar induction (Carroll and
Charniak, 1992; Klein and Manning, 2004; Smith
and Eisner, 2005; Cohen and Smith, 2009; Berg-
Kirkpatrick and Klein, 2010; Naseem et al, 2010;
Spitkovsky et al, 2010; Blunsom and Cohn, 2010).
Grammar induction systems have seen large ad-
vances in quality, but parsing accuracies still signif-
icantly lag behind those of supervised systems. Fur-
thermore, they are often trained and evaluated under
idealized conditions, e.g., only on short sentences
or assuming the existence of gold-standard part-of-
speech (POS) tags.1 The reason for these assump-
tions is clear. Unsupervised grammar induction is
difficult given the complexity of the analysis space.
These assumptions help to give the model traction.
The study of unsupervised grammar induction has
many merits. Most notably, it increases our under-
standing of how computers (and possibly humans)
learn in the absence of any explicit feedback. How-
ever, the gold POS tag assumption weakens any con-
clusions that can be drawn, as part-of-speech are
also a form of syntactic analysis, only shallower.
Furthermore, from a practical standpoint, it is rarely
the case that we are completely devoid of resources
for most languages. This point has been made by
1A notable exception is the work of Seginer (2007).
62
studies that transfer parsers to new languages by
projecting syntax across word alignments extracted
from parallel corpora (Hwa et al, 2005; Ganchev et
al., 2009; Smith and Eisner, 2009). Although again,
most of these studies also assume the existence of
POS tags.
In this work we present a method for creating de-
pendency parsers for languages for which no labeled
training data is available. First, we train a source
side English parser that, crucially, is delexicalized so
that its predictions rely soley on the part-of-speech
tags of the input sentence, in the same vein as Ze-
man and Resnik (2008). We empirically show that
directly transferring delexicalized models (i.e. pars-
ing a foreign language POS sequence with an En-
glish parser) already outperforms state-of-the-art un-
supervised parsers by a significant margin. This re-
sult holds in the presence of both gold POS tags as
well as automatic tags projected from English. This
emphasizes that even for languages with no syntac-
tic resources ? or possibly even parallel data ? sim-
ple transfer methods can already be more powerful
than grammar induction systems.
Next, we use this delexicalized English parser to
seed a perceptron learner for the target language.
The model is trained to update towards parses that
are in high agreement with a source side English
parse based on constraints drawn from alignments in
the parallel data. We use the augmented-loss learn-
ing procedure (Hall et al, 2011) which is closely
related to constraint driven learning (Chang et al,
2007; Chang et al, 2010). The resulting parser con-
sistently improves on the directly transferred delex-
icalized parser, reducing relative errors by 8% on
average, and as much as 18% on some languages.
Finally, we show that by transferring parsers from
multiple source languages we can further reduce er-
rors by 16% over the directly transferred English
baseline. This is consistent with previous work on
multilingual part-of-speech (Snyder et al, 2009) and
grammar (Berg-Kirkpatrick and Klein, 2010; Cohen
and Smith, 2009) induction, that shows that adding
languages leads to improvements.
We present a comprehensive set of experiments
on eight Indo-European languages for which a sig-
nificant amount of parallel data exists. We make
no language specific enhancements in our experi-
ments. We report results for sentences of all lengths,
??????????????????????????????????????????????????????????????
Figure 1: An example (unlabeled) dependency tree.
as well as with gold and automatically induced
part-of-speech tags. We also report results on sen-
tences of length 10 or less with gold part-of-speech
tags to compare with previous work. Our results
consistently outperform the previous state-of-the-art
across all languages and training configurations.
2 Preliminaries
In this paper we focus on transferring dependency
parsers between languages. A dependency parser
takes a tokenized input sentence (optionally part-of-
speech tagged) and produces a connected tree where
directed arcs represent a syntactic head-modifier re-
lationship. An example of such a tree is given in
Figure 1. Dependency tree arcs are often labeled
with the role of the syntactic relationship, e.g., is to
hearing might be labeled as SUBJECT. However, we
focus on unlabeled parsing in order to reduce prob-
lems that arise due to different treebank annotation
schemes. Of course, even for unlabeled dependen-
cies, significant variations in the annotation schemes
remain. For example, in the Danish treebank deter-
miners govern adjectives and nouns in noun phrases,
while in most other treebanks the noun is the head of
the noun phrase. Unlike previous work (Zeman and
Resnik, 2008; Smith and Eisner, 2009), we do not
apply any transformations to the treebanks, which
makes our results easier to reproduce, but systemat-
ically underestimates accuracy.
2.1 Data Sets
The treebank data in our experiments are from the
CoNLL shared-tasks on dependency parsing (Buch-
holz and Marsi, 2006; Nivre et al, 2007). We use
English (en) only as a source language throughout
the paper. Additionally, we use the following eight
languages as both source and target languages: Dan-
ish (da), Dutch (nl), German (de), Greek (el), Italian
(it), Portuguese (pt), Spanish (es) and Swedish (sv).
For languages that were included in both the 2006
and 2007 tasks, we used the treebank from the lat-
63
ter. We focused on this subset of languages because
they are Indo-European and a significant amount of
parallel data exists for each language. By present-
ing results on eight languages our study is already
more comprehensive than most previous work in this
area. However, the restriction to Indo-European lan-
guages does make the results less conclusive when
one wishes to transfer a parser from English to Chi-
nese, for example. To account for this, we report
additional results in the discussion for non-Indo-
European languages. For all data sets we used the
predefined training and testing splits.
Our approach relies on a consistent set of part-
of-speech tags across languages and treebanks. For
this we used the universal tagset from Petrov et
al. (2011), which includes: NOUN (nouns), VERB
(verbs), ADJ (adjectives), ADV (adverbs), PRON
(pronouns), DET (determiners), ADP (prepositions
or postpositions), NUM (numerals), CONJ (conjunc-
tions), PRT (particles), PUNC (punctuation marks)
and X (a catch-all tag). Similar tagsets are used by
other studies on grammar induction and projection
(Naseem et al, 2010; Zeman and Resnik, 2008). For
all our experiments we replaced the language spe-
cific part-of-speech tags in the treebanks with these
universal tags.
Like all treebank projection studies we require a
corpus of parallel text for each pair of languages we
study. For this we used the Europarl corpus version
5 (Koehn, 2005). The corpus was preprocessed in
standard ways and word aligned by running six it-
erations of IBM Model 1 (Brown et al, 1993), fol-
lowed by six iterations of the HMM model (Vogel et
al., 1996) in both directions. We then intersect word
alignments to generate one-to-one alignments.
2.2 Parsing Model
All of our parsing models are based on the
transition-based dependency parsing paradigm
(Nivre, 2008). Specifically, all models use an
arc-eager transition strategy and are trained using
the averaged perceptron algorithm as in Zhang and
Clark (2008) with a beam size of 8. The features
used by all models are: the part-of-speech tags of
the first four words on the buffer and of the top two
words on the stack; the word identities of the first
two words on the buffer and of the top word on the
stack; the word identity of the syntactic head of
the top word on the stack (if available). All feature
conjunctions are included. For treebanks with
non-projective trees we use the pseudo-projective
parsing technique to transform the treebank into
projective structures (Nivre and Nilsson, 2005).
We focus on using this parsing system for two
reasons. First, the parser is near state-of-the-art on
English parsing benchmarks and second, and more
importantly, the parser is extremely fast to train and
run, making it easy to run a large number of exper-
iments. Preliminary experiments using a different
dependency parser ? MSTParser (McDonald et al,
2005) ? resulted in similar empirical observations.
2.3 Evaluation
All systems are evaluated using unlabeled attach-
ment score (UAS), which is the percentage of words
(ignoring punctuation tokens) in a corpus that mod-
ify the correct head (Buchholz and Marsi, 2006).
Furthermore, we evaluate with both gold-standard
part-of-speech tags, as well as predicted part-of-
speech tags from the projected part-of-speech tagger
of Das and Petrov (2011).2 This tagger relies only on
labeled training data for English, and achieves accu-
racies around 85% on the languages that we con-
sider. We evaluate in the former setting to compare
to previous studies that make this assumption. We
evaluate in the latter setting to measure performance
in a more realistic scenario ? when no target lan-
guage resources are available.
3 Transferring from English
To simplify discussion, we first focus on the most
common instantiation of parser transfer in the liter-
ature: transferring from English to other languages.
In the next section we expand our system to allow
for the inclusion of multiple source languages.
3.1 Direct Transfer
We start with the observation that discriminatively
trained dependency parsers rely heavily on part-of-
speech tagging features. For example, when train-
ing and testing a parser on our English data, a parser
with all features obtains an UAS of 89.3%3 whereas
2Available at http://code.google.com/p/pos-projection/
3The best system at CoNLL 2007 achieved 90.1% and used
a richer part-of-speech tagset (Nivre et al, 2007).
64
a delexicalized parser ? a parser that only has non-
lexical features ? obtains an UAS of 82.5%. The
key observation is that part-of-speech tags contain a
significant amount of information for unlabeled de-
pendency parsing.
This observation combined with our universal
part-of-speech tagset, leads to the idea of direct
transfer, i.e., directly parsing the target language
with the source language parser without relying on
parallel corpora. This idea has been previously ex-
plored by Zeman and Resnik (2008) and recently by
S?gaard (2011). Because we use a mapping of the
treebank specific part-of-speech tags to a common
tagset, the performance of a such a system is easy to
measure ? simply parse the target language data set
with a delexicalized parser trained on the source lan-
guage data. We conducted two experiments. In the
first, we assumed that the test set for each target lan-
guage had gold part-of-speech tags, and in the sec-
ond we used predicted part-of-speech tags from the
projection tagger of Das and Petrov (2011), which
also uses English as the source language.
UAS for all sentence lengths without punctuation
are given in Table 1. We report results for both the
English direct transfer parser (en-dir.) as well as a
baseline unsupervised grammar induction system ?
the dependency model with valence (DMV) of Klein
and Manning (2004), as obtained by the implemen-
tation of Ganchev et al (2010). We trained on sen-
tences of length 10 or less and evaluated on all sen-
tences from the test set.4 For DMV, we reversed the
direction of all dependencies if this led to higher per-
formance. From this table we can see that direct
transfer is a very strong baseline and is over 20%
absolute better than the DMV model for both gold
and predicted POS tags. Table 4, which we will dis-
cuss in more detail later, further shows that the direct
transfer parser also significantly outperforms state-
of-the-art unsupervised grammar induction models,
but in a more limited setting of sentences of length
less than 10.
Direct transfer works for a couple of reasons.
First, part-of-speech tags contain a significant
amount of information for parsing unlabeled depen-
dencies. Second, this information can be transferred,
4Training on all sentences results in slightly lower accura-
cies on average.
to some degree, across languages and treebank stan-
dards. This is because, at least for Indo-European
languages, there is some regularity in how syntax
is expressed, e.g., primarily SVO, prepositional, etc.
Even though there are some differences with respect
to relative location of certain word classes, strong
head-modifier POS tag preferences can still help re-
solve these, especially when no other viable alter-
natives are available. Consider for example an arti-
ficial sentence with a tag sequence: ?VERB NOUN
ADJ DET PUNC?. The English parser still predicts
that the NOUN and PUNC modify the VERB and the
ADJ and DET modify the NOUN, even though in the
English data such noun phrases are unlikely.5
3.2 Projected Transfer
Unlike most language transfer systems for parsers,
the direct transfer approach does not rely on project-
ing syntax across aligned parallel corpora (modulo
the fact that non-gold tags come from a system that
uses parallel corpora). In this section we describe
a simple mechanism for projecting from the direct
transfer system using large amounts of parallel data
in a similar vein to Hwa et al (2005), Ganchev et
al. (2009), Smith and Eisner (2009) inter alia. The
algorithm is based on the work of Hall et al (2011)
for training extrinsic parser objective functions and
borrows heavily from ideas in learning with weak
supervision including work on learning with con-
straints (Chang et al, 2007) and posterior regular-
ization (Ganchev et al, 2010). In our case, the
weak signals come from aligned source and target
sentences, and the agreement in their corresponding
parses, which is similar to posterior regularization
or the bilingual view of Smith and Smith (2004) and
Burkett et al (2010).
The algorithm is given in Figure 2. It starts by
labeling a set of target language sentences with a
parser, which in our case is the direct transfer parser
from the previous section (line 1). Next, it uses
these parsed target sentences to ?seed? a new parser
by training a parameter vector using the predicted
parses as a gold standard via standard perceptron
updates for J rounds (lines 3-6). This generates a
parser that emulates the direct transfer parser, but
5This requires a transition-based parser with a beam greater
than 1 to allow for ambiguity to be resolved at later stages.
65
Notation:
x: input sentence
y: dependency tree
a: alignment
w: parameter vector
?(x, y): feature vector
DP : dependency parser, i.e., DP : x? y
Input:
X = {xi}ni=1: target language sentences
P = {(xsi , xti, ai)}mi=1: aligned source-target sentences
DPdelex: delexicalized source parser
DPlex: lexicalized source parser
Algorithm:
1. Let X ? = {(xi, yi)}ni=1 where yi = DPdelex(xi)
2. w = 0
see
d-s
tag
e 3. for j : 1 . . . J
4. for xi : x1 . . . xn
5. Let y = argmaxy w ? ?(xi, y)
6. w = w + ?(xt, yi)? ?(xi, y)
pro
jec
tio
n-s
tag
e 7. for (xsi , xti, ai) : (xs1, xt1, a1) . . . (xsm, xsm, am)8. Let ys = DPlex(xsi )
9. Let Yt = {y1i , . . . , yki }, where:
yki = argmaxy/?{y1i ,...,yk?1i } w ? ?(x
t
i, y)
10. Let yt = argmaxyt?Yt ALIGN(ys, yt, ai)11. w = w + ?(xi, yt)? ?(xi, y1i )
return DP ? such that DP ?(x) = argmaxy w ? ?(x, y)
Figure 2: Perceptron-based learning algorithm for train-
ing a parser by seeding the model with a direct transfer
parser and projecting constraints across parallel corpora.
has now been lexicalized and is working in the space
of target language sentences. Next, the algorithm it-
erates over the sentences in the parallel corpus. It
parses the English sentence with an English parser
(line 8, again a lexicalized parser). It then uses the
current target language parameter vector to create
a k-best parse list for the target sentence (line 9).
From this list, it selects the parse whose dependen-
cies align most closely with the English parse via the
pre-specified alignment (line 10, also see below for
the definition of the ALIGN function). It then uses
this selected parse as a proxy to the gold standard
parse to update the parameters (line 11).
The intuition is simple. The parser starts with
non-random accuracies by emulating the direct
transfer model and slowly tries to induce better pa-
rameters by selecting parses from its k-best list
that are considered ?good? by some external met-
ric. The algorithm then updates towards that out-
put. In this case ?goodness? is determined through
the pre-specified sentence alignment and how well
the target language parse aligns with the English
parse. As a result, the model will, ideally, converge
to a state where it predicts target parses that align as
closely as possible with the corresponding English
parses. However, since we seed the learner with the
direct transfer parser, we bias the parameters to se-
lect parses that both align well and also have high
scores under the direct transfer model. This helps
to not only constrain the search space at the start
of learning, but also helps to bias dependencies be-
tween words that are not part of the alignment.
So far we have not defined the ALIGN function
that is used to score potential parses. Let a =
{(s(1), t(1)), . . . , (s(n), t(n))} be an alignment where
s(i) is a word in the source sentence xs (not nec-
essarily the ith word) and t(i) is similarly a word
in the target sentence xt (again, not necessarily the
ith word). The notation (s(i), t(i)) ? a indicates
two words are the ith aligned pair in a. We define
the ALIGN function to encode the Direct Correspon-
dence Assumption (DCA) from Hwa et al (2005):
ALIGN(ys, yt, a)
=
?
(s(i),t(i))?a
(s(j),t(j))?a
SCORE(ys, yt, (s(i), s(j)), (t(i), t(j)))
SCORE(ys, yt, (s(i), s(j)), (t(i), t(j)))
=
?
???
???
+1 if (s(i), s(j)) ? ys and (t(i), t(j)) ? yt
?1 if (s(i), s(j)) ? ys and (t(i), t(j)) /? yt
?1 if (s(i), s(j)) /? ys and (t(i), t(j)) ? yt
0 otherwise
The notation (i, j) ? y indicates that a dependency
from head i to modifier j is in tree y. The ALIGN
function rewards aligned head-modifier pairs and
penalizes unaligned pairs when a possible alignment
exists. For all other cases it is agnostic, i.e., when
one or both of the modifier or head are not aligned.
Figure 3 shows an example of aligned English-
Greek sentences, the English parse and a potential
Greek parse. In this case the ALIGN function re-
turns a value of 2. This is because there are three
aligned dependencies: took?book, book?the and
66
?????????????????????????????????????
?????????????????????????????????????
Figure 3: A Greek and English sentence pair. Word
alignments are shown as dashed lines, dependency arcs
as solid lines.
from?John. These add 3 to the score. There is
one incorrectly aligned dependency: the preposi-
tion mistakenly modifies the noun on the Greek side.
This subtracts 1. Finally, there are two dependencies
that do not align: the subject on the English side
and a determiner to a proper noun on the Greek side.
These do not effect the result.
The learning algorithm in Figure 2 is an instance
of augmented-loss training (Hall et al, 2011) which
is closely related to the constraint driven learning al-
gorithms of Chang et al (2007). In that work, ex-
ternal constraints on output structures are used to
help guide the learner to good parameter regions.
In our model, we use constraints drawn from paral-
lel data exactly in the same manner. Since posterior
regularization is closely related to constraint driven
learning, this makes our algorithm also similar to the
parser projection approach of Ganchev et al (2009).
There are a couple of differences. First, we bias our
model towards the direct transfer model, which is
already quite powerful. Second, our alignment con-
straints are used to select parses from a k-best list,
whereas in posterior regularization they are used as
soft constraints on full model expectations during
training. The latter is beneficial as the use of k-best
lists does not limit the class of parsers to those whose
parameters and search space decompose neatly with
the DCA loss function. An empirical comparison to
Ganchev et al (2009) is given in Section 5.
Results are given in Table 1 under the column en-
proj. For all experiments we train the seed-stage
perceptron for 5 iterations (J = 5) and we use one
hundred times as much parallel data as seed stage
non-parallel data (m = 100n). The seed-stage non-
parallel data is the training portion of each treebank,
stripped of all dependency annotations. After train-
ing the projected parser we average the parameters
gold-POS pred-POS
DMV en-dir. en-proj. DMV en-dir. en-proj.
da 33.4 45.9 48.2 18.4 44.0 45.5
de 18.0 47.2 50.9 30.3 44.7 47.4
el 39.9 63.9 66.8 21.2 63.0 65.2
es 28.5 53.3 55.8 19.9 50.2 52.4
it 43.1 57.7 60.8 37.7 53.7 56.3
nl 38.5 60.8 67.8 19.9 62.1 66.5
pt 20.1 69.2 71.3 21.0 66.2 67.7
sv 44.0 58.3 61.3 33.8 56.5 59.7
avg 33.2 57.0 60.4 25.3 55.0 57.6
Table 1: UAS for the unsupervised DMV model (DMV),
a delexicalized English direct transfer parser (en-dir.)
and a English projected parser (en-proj.). Measured on
all sentence lengths for both gold and predicted part-of-
speech tags as input.
of the model (Collins, 2002). The parsers evaluated
using predicted part-of-speech tags use the predicted
tags at both training and testing time and are thus
free of any target language specific resources.
When compared with the direct transfer model
(en-dir. in Table 1), we can see that there is an im-
provement for every single language, reducing rela-
tive error by 8% on average (57.0% to 60.4%) and
up to 18% for Dutch (60.8 to 67.8%). One could
wonder whether the true power of the projection
model comes from the re-lexicalization step ? lines
3-6 of the algorithm. However, if just this step is run,
then the average UAS only increases from 57.0%
to 57.4%, showing that most of the improvement
comes from the projection stage. Note that the re-
sults in Table 1 indicate that parsers using predicted
part-of-speech tags are only slightly worse than the
parsers using gold tags (about 2-3% absolute), show-
ing that these methods are robust to tagging errors.
4 Multi-Source Transfer
The previous section focused on transferring an En-
glish parser to a new target language. However,
there are over 20 treebanks available for a variety
of language groups including Indo-European, Altaic
(including Japanese), Semitic, and Sino-Tibetan.
Many of these are even in standardized formats
(Buchholz and Marsi, 2006; Nivre et al, 2007). Past
studies have shown that for both part-of-speech tag-
ging and grammar induction, learning with multiple
comparable languages leads to improvements (Co-
hen and Smith, 2009; Snyder et al, 2009; Berg-
Kirkpatrick and Klein, 2010). In this section we ex-
67
Source Training Language
da de el en es it nl pt sv
Ta
rge
tT
est
La
ng
ua
ge
da 79.2 45.2 44.0 45.9 45.0 48.6 46.1 48.1 47.8
de 34.3 83.9 53.2 47.2 45.8 53.4 55.8 55.5 46.2
el 33.3 52.5 77.5 63.9 41.6 59.3 57.3 58.6 47.5
en 34.4 37.9 45.7 82.5 28.5 38.6 43.7 42.3 43.7
es 38.1 49.4 57.3 53.3 79.7 68.4 51.2 66.7 41.4
it 44.8 56.7 66.8 57.7 64.7 79.3 57.6 69.1 50.9
nl 38.7 43.7 62.1 60.8 40.9 50.4 73.6 58.5 44.2
pt 42.5 52.0 66.6 69.2 68.5 74.7 67.1 84.6 52.1
sv 44.5 57.0 57.8 58.3 46.3 53.4 54.5 66.8 84.8
Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a
delexicalized parser and each row represents which target language test data was used. Bold numbers are when source
equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths
without punctuation.
amine whether this is also true for parser transfer.
Table 2 shows the matrix of source-target lan-
guage UAS for all nine languages we consider (the
original eight target languages plus English). We
can see that there is a wide range from 33.3% to
74.7%. There is also a wide range of values depend-
ing on the source training data and/or target testing
data, e.g., Portuguese as a source tends to parse tar-
get languages much better than Danish, and is also
more amenable as a target testing language. Some
of these variations are expected, e.g., the Romance
languages (Spanish, Italian and Portuguese) tend to
transfer well to one another. However, some are
unexpected, e.g., Greek being the best source lan-
guage for Dutch, as well as German being one of the
worst. This is almost certainly due to different an-
notation schemes across treebanks. Overall, Table 2
does indicate that there are possible gains in accu-
racy through the inclusion of additional languages.
In order to take advantage of treebanks in multi-
ple languages, our multi-source system simply con-
catenates the training data from all non-target lan-
guages. In other words, the multi-source direct
transfer parser for Danish will be trained by first
concatenating the training corpora of the remain-
ing eight languages, training a delexicalized parser
on this data and then directly using this parser to
analyze the Danish test data. For the multi-source
projected parser, the procedure is identical to that
in Section 3.2 except that we use the multi-source
direct transfer model to seed the algorithm instead
of the English-only direct transfer model. For these
experiments we still only use English-target parallel
data because that is the format of the readily avail-
able data in the Europarl corpus.
Table 3 presents four sets of results. The first
(best-source) is the direct transfer results for the ora-
cle single-best source language per target language.
The second (avg-source) is the mean UAS over all
source languages per target language. The third
(multi-dir.) is the multi-source direct transfer sys-
tem. The fourth and final result set (multi-proj.)
is the multi-source projected system. The resulting
parsers are typically much more accurate than the
English direct transfer system (Table 1). On aver-
age, the multi-source direct transfer system reduces
errors by 10% relative over the English-only direct
transfer system. These improvements are not consis-
tent. For Greek and Dutch we see significant losses
relative to the English-only system. An inspection of
Table 2 shows that for these two languages English
is a particularly good source training language.
For the multi-source projected system the results
are mixed. Some languages see basically no change
relative the multi-source direct transfer model, while
some languages see modest to significant increases.
But again, there is an overall trend to better mod-
els. In particular, starting with an English-only di-
rect transfer parser with 57.0% UAS on average,
by adding parallel corpora and multiple source lan-
guages we finish with parser having 63.8% UAS
on average, which is a relative reduction in error
of roughly 16% and more than doubles the perfor-
mance of a DMV model (Table 1).
Interestingly, the multi-source systems provide,
on average, accuracies near that of the single-best
source language and significantly better than the av-
erage source UAS. Thus, even this simple method of
68
best-source avg-source gold-POS pred-POS
source gold-POS gold-POS multi-dir. multi-proj. multi-dir. multi-proj.
da it 48.6 46.3 48.9 49.5 46.2 47.5
de nl 55.8 48.9 56.7 56.6 51.7 52.0
el en 63.9 51.7 60.1 65.1 58.5 63.0
es it 68.4 53.2 64.2 64.5 55.6 56.5
it pt 69.1 58.5 64.1 65.0 56.8 58.9
nl el 62.1 49.9 55.8 65.7 54.3 64.4
pt it 74.8 61.6 74.0 75.6 67.7 70.3
sv pt 66.8 54.8 65.3 68.0 58.3 62.1
avg 63.7 51.6 61.1 63.8 56.1 59.3
Table 3: UAS for multi-source direct (multi-dir.) and projected (multi-proj.) transfer systems. best-source is the best
source model from the languages in Table 2 (excluding the target language). avg-source is the mean UAS over the
source models for the target (excluding target language).
multi-source transfer already provides strong perfor-
mance gains. We expect that more principled tech-
niques will lead to further improvements. For exam-
ple, recent work by S?gaard (2011) explores data set
sub-sampling methods. Unlike our work, S?gaard
found that simply concatenating all the data led to
degradation in performance. Cohen et al (2011) ex-
plores the idea learning language specific mixture
coefficients for models trained independently on the
target language treebanks. However, their results
show that this method often did not significantly out-
perform uniform mixing.
5 Comparison
Comparing unsupervised and parser projection sys-
tems is difficult as many publications use non-
overlapping sets of languages or different evaluation
criteria. We compare to the following three systems
that do not augment the treebanks and report results
for some of the languages that we considered:
? USR: The weakly supervised system of
Naseem et al (2010), in which manually de-
fined universal syntactic rules (USR) are used
to constrain a probabilistic Bayesian model. In
addition to their original results, we also report
results using the same part-of-speech tagset as
the systems described in this paper (USR?).
This is useful for two reasons. First, it makes
the comparison more direct. Second, we can
generate USR results for all eight languages
and not just for the languages that they report.
? PGI: The phylogenetic grammar induction
(PGI) model of Berg-Kirkpatrick and Klein
(2010), in which the parameters of completely
unsupervised DMV models for multiple lan-
guages are coupled via a phylogenetic prior.
? PR: The posterior regularization (PR) approach
of Ganchev et al (2009), in which a supervised
English parser is used to generate constraints
that are projected using a parallel corpus and
used to regularize a target language parser. We
report results without treebank specific rules.
Table 4 gives results comparing the models pre-
sented in this work to those three systems. For this
comparison we use sentences of length 10 or less
after punctuation has been removed in order to be
consistent with reported results. The overall trends
carry over from the full treebank setting to this re-
duced sentence length setup: the projected mod-
els outperform the direct transfer models and multi-
source transfer gives higher accuracy than transfer-
ring only from English. Most previous work has as-
sumed gold part-of-speech tags, but as the code for
USR is publicly available we were able to train it
using the same projected part-of-speech tags used
in our models. These results are also given in Ta-
ble 4 under USR?. Again, we can see that the multi-
source systems (both direct and projected) signifi-
cantly outperform the unsupervised models.
It is not surprising that a parser transferred from
annotated resources does significantly better than
unsupervised systems since it has much more in-
formation from which to learn. The PR system of
Ganchev et al (2009) is similar to ours as it also
projects syntax across parallel corpora. For Span-
ish we can see that the multi-source direct trans-
fer parser is better (75.1% versus 70.6%), and this
is also true for the multi-source projected parser
69
?? gold-POS ?? ? pred-POS?
en-dir. en-proj. multi-dir. multi-proj. USR? USR PGI PR multi-dir. multi-proj. USR?
da 53.2 57.4 58.4 58.8 55.1 51.9 41.6 54.9 54.6 41.7
de 65.9 67.0 74.9 72.0 60.0 63.7 63.4 55.1
el 73.9 73.9 73.5 78.7 60.3 65.2 74.3 53.4
es 58.0 62.3 75.1 73.2 68.3 67.2 58.4 70.6 59.1 56.8 43.3
it 65.5 69.9 75.5 75.5 47.9 65.5 70.2 41.4
nl 67.6 72.2 58.8 70.7 44.0 45.1 56.3 67.2 38.8
pt 77.9 80.6 81.1 86.2 70.9 71.5 63.0 74.0 79.2 66.4
sv 70.4 71.3 76.0 77.6 52.6 58.3 72.0 73.9 59.4
avg 66.6 69.4 71.7 74.1 57.4 63.9 67.5 49.9
Table 4: UAS on sentences of length 10 or less without punctuation, comparing the systems presented in this work
to three representative systems from related work. en-dir./en-proj. are the direct/projected English parsers and multi-
dir./multi-proj. are the multi-source direct/projected parsers. Section 5 contains a description of the baseline systems.
(73.2%). Ganchev et al also report results for
Bulgarian. We trained a multi-source direct trans-
fer parser for Bulgarian which obtained a score of
72.8% versus 67.8% for the PR system. If we only
use English as a source language, as in Ganchev et
al., the English direct transfer model achieves 66.1%
on Bulgarian and 69.3% on Spanish versus 67.8%
and 70.6% for PR. In this setting the English pro-
jected model gets 72.0% on Spanish. Thus, under
identical conditions the direct transfer model obtains
accuracies comparable to PR.6
Another projection based system is that of Smith
and Eisner (2009), who report results for German
(68.5%) and Spanish (64.8%) on sentences of length
15 and less inclusive of punctuation. Smith and Eis-
ner use custom splits of the data and modify a sub-
set of the dependencies. The multi-source projected
parser obtains 71.9% for German and 67.8% for
Spanish on this setup.7 If we cherry-pick the source
language the results can improve, e.g., for Spanish
we can obtain 71.7% and 70.8% by directly transfer-
ring parsers form Italian or Portuguese respectively.
6 Discussion
One fundamental point the above experiments il-
lustrate is that even for languages for which no
resources exist, simple methods for transferring
parsers work remarkably well. In particular, if
6Note that the last set of results was obtained by using the
same English training data as Ganchev et al Using the CoNLL
2007 English data set for training, the English direct transfer
model is 63.2% for Bulgarian and 58.0% for Spanish versus
67.8% and 70.6% for PR, highlighting the large impact that dif-
ference treebank annotation standards can have.
7Data sets and evaluation criteria obtained via communica-
tions with David Smith and Jason Eisner.
one can transfer part-of-speech tags, then a large
part of transferring unlabeled dependencies has been
solved. This observation should lead to a new base-
line in unsupervised and projected grammar induc-
tion ? the UAS of a delexicalized English parser.
Of course, our experiments focus strictly on Indo-
European languages. Preliminary experiments for
Arabic (ar), Chinese (zh), and Japanese (ja) suggest
similar direct transfer methods are applicable. For
example, on the CoNLL test sets, a DMV model
obtains UAS of 28.7/41.8/34.6% for ar/zh/ja re-
spectively, whereas an English direct transfer parser
obtains 32.1/53.8/32.2% and a multi-source direct
transfer parser obtains 39.9/41.7/43.3%. In this
setting only Indo-European languages are used as
source data. Thus, even across language groups di-
rect transfer is a reasonable baseline. However, this
is not necessary as treebanks are available for a num-
ber of language groups, e.g., Indo-European, Altaic,
Semitic, and Sino-Tibetan.
The second fundamental observation is that when
available, multiple sources should be used. Even
through naive multi-source methods (concatenating
data), it is possible to build a system that has compa-
rable accuracy to the single-best source for all lan-
guages. This advantage does not come simply from
having more data. In fact, if we randomly sam-
pled from the multi-source data until the training set
size was equivalent to the size of the English data,
then the results still hold (and in fact go up slightly
for some languages). This suggests that even bet-
ter transfer models can be produced by separately
weighting each of the sources depending on the tar-
get language ? either weighting by hand, if we know
the language group of the target language, or auto-
70
matically, if we do not. As previously mentioned,
the latter has been explored in both S?gaard (2011)
and Cohen et al (2011).
7 Conclusions
We presented a simple, yet effective approach
for projecting parsers from languages with labeled
training data to languages without any labeled train-
ing data. Central to our approach is the idea of
delexicalizing the models, which combined with a
standardized part-of-speech tagset alows us to di-
rectly transfer models between languages. We then
use a constraint driven learning algorithm to adapt
the transferred parsers to the respective target lan-
guage, obtaining an additional 16% error reduc-
tion on average in a multi-source setting. Our final
parsers achieve state-of-the-art accuracies on eight
Indo-European languages, significantly outperform-
ing previous unsupervised and projected systems.
Acknowledgements: We would like to thank Kuz-
man Ganchev, Valentin Spitkovsky and Dipanjan
Das for numerous discussions on this topic and com-
ments on earlier drafts of this paper. We would
also like to thank Shay Cohen, Dipanjan Das, Noah
Smith and Anders S?gaard for sharing early drafts
of their recent related work.
References
T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic
grammar induction. In Proc. of ACL.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
Proc. of EMNLP.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010.
Learning better monolingual models with unannotated
bilingual text. In Proc. of CoNLL.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. In Proc. of the Working Notes of the Workshop
Statistically-Based NLP Techniques.
M.W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
Proc. of ACL.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In Proc. of ICML.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In Proc. of ACL.
S.B. Cohen and N.A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsuper-
vised grammar induction. In Proc. of NAACL.
S.B. Cohen, D. Das, and N.A. Smith. 2011. Unsuper-
vised structure prediction with non-parallel multilin-
gual guidance. In Proc. of EMNLP.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999.
A statistical parser for Czech. In Proc. of ACL.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proc. of ACL.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of ACL.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proc. of ACL-HLT.
K. Ganchev, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection con-
straints. In Proc. of ACL-IJCNLP.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc of EMNLP.
K. Hall, R. McDonald, J. Katz-Brown, and M. Ringgaard.
2011. Training dependency parsers by jointly optimiz-
ing multiple objectives. In Proc. of EMNLP.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion across parallel texts. Natural Language Engineer-
ing, 11(03):311?325.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: models of dependency and
constituency. In Proc. of ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit.
M. P. Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
English: the Penn treebank. Computational Linguis-
tics, 19.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL.
71
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010.
Using universal linguistic knowledge to guide gram-
mar induction. In Proc. of EMNLP.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. of ACL.
J. Nivre, J. Hall, and J. Nilsson. 2006. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proc. of LREC.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
EMNLP-CoNLL.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513?553.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. of ACL.
S. Petrov, P. Chang, M. Ringgaard, and H. Alshawi.
2010. Uptraining for accurate deterministic question
parsing. In EMNLP ?10.
S. Petrov, D. Das, and R. McDonald. 2011. A universal
part-of-speech tagset. In ArXiv:1104.2086.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In Proc. of ACL.
L. Shen and A.K. Joshi. 2008. Ltag dependency parsing
with bidirectional incremental construction. In Proc.
of EMNLP.
N.A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL.
D.A. Smith and J. Eisner. 2009. Parser adaptation and
projection with quasi-synchronous grammar features.
In Proc. of EMNLP.
D.A. Smith and N.A. Smith. 2004. Bilingual parsing
with factored estimation: Using english to parse ko-
rean. In Proc. of EMNLP.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2009. Adding more languages improves unsupervised
multilingual part-of-speech tagging: A Bayesian non-
parametric approach. In Proc. of NAACL.
A. S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Proc.
ACL.
V.I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010.
From baby steps to leapfrog: How ?less is more? in un-
supervised dependency parsing. In Proc. of NAACL-
HLT.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. of
COLING.
W. Wang and M. P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proc. of
the Workshop on Incremental Parsing: Bringing Engi-
neering and Cognition Together.
D. Zeman and P. Resnik. 2008. Cross-language parser
adaptation between related languages. In NLP for Less
Privileged Languages.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
72
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183?192,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Training a Parser for Machine Translation Reordering
Jason Katz-Brown Slav Petrov Ryan McDonald Franz Och
David Talbot Hiroshi Ichikawa Masakazu Seno Hideto Kazawa
Google
{jasonkb|slav|ryanmcd|och|talbot|ichikawa|seno|kazawa}@google.com
Abstract
We propose a simple training regime that can
improve the extrinsic performance of a parser,
given only a corpus of sentences and a way
to automatically evaluate the extrinsic quality
of a candidate parse. We apply our method
to train parsers that excel when used as part
of a reordering component in a statistical ma-
chine translation system. We use a corpus of
weakly-labeled reference reorderings to guide
parser training. Our best parsers contribute
significant improvements in subjective trans-
lation quality while their intrinsic attachment
scores typically regress.
1 Introduction
The field of syntactic parsing has received a great
deal of attention and progress since the creation of
the Penn Treebank (Marcus et al, 1993; Collins,
1997; Charniak, 2000; McDonald et al, 2005;
Petrov et al, 2006; Nivre, 2008). A common?
and valid?criticism, however, is that parsers typi-
cally get evaluated only on Section 23 of the Wall
Street Journal portion of the Penn Treebank. This
is problematic for many reasons. As previously ob-
served, this test set comes from a very narrow do-
main that does not necessarily reflect parser perfor-
mance on text coming from more varied domains
(Gildea, 2001), especially web text (Foster, 2010).
There is also evidence that after so much repeated
testing, parsers are indirectly over-fitting to this set
(Petrov and Klein, 2007). Furthermore, parsing was
never meant as a stand-alone task, but is rather a
means to an end, towards the goal of building sys-
tems that can process natural language input.
This is not to say that parsers are not used in larger
systems. All to the contrary, as parsing technology
has become more mature, parsers have become ef-
ficient and accurate enough to be useful in many
natural language processing systems, most notably
in machine translation (Yamada and Knight, 2001;
Galley et al, 2004; Xu et al, 2009). While it has
been repeatedly shown that using a parser can bring
net gains on downstream application quality, it is of-
ten unclear how much intrinsic parsing accuracy ac-
tually matters.
In this paper we try to shed some light on this is-
sue by comparing different parsers in the context of
machine translation (MT). We present experiments
on translation from English to three Subject-Object-
Verb (SOV) languages,1 because those require ex-
tensive syntactic reordering to produce grammatical
translations. We evaluate parse quality on a num-
ber of extrinsic metrics, including word reordering
accuracy, BLEU score and a human evaluation of fi-
nal translation quality. We show that while there is
a good correlation between those extrinsic metrics,
parsing quality as measured on the Penn Treebank
is not a good indicator of the final downstream ap-
plication quality. Since the word reordering metric
can be computed efficiently offline (i.e. without the
use of the final MT system), we then propose to tune
parsers specifically for that metric, with the goal of
improving the performance of the overall system.
To this end we propose a simple training regime
1We experiment with Japanese, Korean and Turkish, but
there is nothing language specific in our approach.
183
which we refer to as targeted self-training (Sec-
tion 2). Similar to self-training, a baseline model
is used to produce predictions on an unlabeled data
set. However, rather than directly training on the
output of the baseline model, we generate a list of
hypotheses and use an external signal to select the
best candidate. The selected parse trees are added
to the training data and the model is then retrained.
The experiments in Section 5 show that this simple
procedure noticeably improves our parsers for the
task at hand, resulting in significant improvements
in downstream translation quality, as measured in a
human evaluation on web text.
This idea is similar in vein to McClosky. et al
(2006) and Petrov et al (2010), except that we use an
extrinsic quality metric instead of a second parsing
model for making the selection. It is also similar to
Burkett and Klein (2008) and Burkett et al (2010),
but again avoiding the added complexity introduced
by the use of additional (bilingual) models for can-
didate selection.
It should be noted that our extrinsic metric is com-
puted from data that has been manually annotated
with reference word reorderings. Details of the re-
ordering metric and the annotated data we used are
given in Sections 3 and 4. While this annotation re-
quires some effort, such annotations are much easier
to obtain than full parse trees. In our experiments
in Section 6 we show that we can obtain similar
improvements on downstream translation quality by
targeted self-training with weakly labeled data (in
form of word reorderings), as with training on the
fully labeled data (with full syntactic parse trees).
2 Targeted Self-Training
Our technique for retraining a baseline parser is an
extension of self-training. In standard parser self-
training, one uses the baseline parsing model to
parse a corpus of sentences, and then adds the 1-best
output of the baseline parser to the training data. To
target the self-training, we introduce an additional
step, given as Algorithm 1. Instead of taking the 1-
best parse, we produce a ranked n-best list of predic-
tions and select the parser which gives the best score
according to an external evaluation function. That
is, instead of relying on the intrinsic model score,
we use an extrinsic score to select the parse towards
Algorithm 1 Select parse that maximizes an extrin-
sic metric.
Input: baseline parser B
Input: sentence S
Input: function COMPUTEEXTRINSIC(parse P )
Output: a parse for the input sentence
Pn = {P1, . . . , Pn} ? n-best parses of S by B
maxScore = 0
bestParse = ?
for k = 1 to n do
extrinsicScore = COMPUTEEXTRINSIC(Pk)
if extrinsicScore > maxScore then
maxScore = extrinsicScore
bestParse = Pk
end if
end for
return bestParse
which to update. In the case of a tie, we prefer the
parse ranked most highly in the n-best list.
The motivation of this selection step is that good
performance on the downstream external task, mea-
sured by the extrinsic metric, should be predictive
of an intrinsically good parse. At the very least,
even if the selected parse is not syntactically cor-
rect, or even if it goes against the original treebank-
ing guidelines, it results in a higher extrinsic score
and should therefore be preferred.
One could imagine extending this framework by
repeatedly running self-training on successively im-
proving parsers in an EM-style algorithm. A recent
work by Hall et al (2011) on training a parser with
multiple objective functions investigates a similar
idea in the context of online learning.
In this paper we focus our attention on machine
translation as the final application, but one could en-
vision applying our techniques to other applications
such as information extraction or question answer-
ing. In particular, we explore one application of
targeted self-training, where computing the extrin-
sic metric involves plugging the parse into an MT
system?s reordering component and computing the
accuracy of the reordering compared to a reference
word order. We now direct our attention to the de-
tails of this application.
184
3 The MT Reordering Task
Determining appropriate target language word or-
der for a translation is a fundamental problem in
MT. When translating between languages with sig-
nificantly different word order such as English and
Japanese, it has been shown that metrics which ex-
plicitly account for word-order are much better cor-
related with human judgments of translation qual-
ity than those that give more weight to word choice,
like BLEU (Lavie and Denkowski, 2009; Isozaki et
al., 2010a; Birch and Osborne, 2010). This demon-
strates the importance of getting reordering right.
3.1 Reordering as a separately evaluable
component
One way to break down the problem of translat-
ing between languages with different word order
is to handle reordering and translation separately:
first reorder source-language sentences into target-
language word order in a preprocessing step, and
then translate the reordered sentences. It has been
shown that good results can be achieved by reorder-
ing each input sentence using a series of tree trans-
formations on its parse tree. The rules for tree
transformation can be manually written (Collins et
al., 2005; Wang, 2007; Xu et al, 2009) or auto-
matically learned (Xia and McCord, 2004; Habash,
2007; Genzel, 2010).
Doing reordering as a preprocessing step, sepa-
rately from translation, makes it easy to evaluate re-
ordering performance independently from the MT
system. Accordingly, Talbot et al (2011) present a
framework for evaluating the quality of reordering
separately from the lexical choice involved in trans-
lation. They propose a simple reordering metric
based on METEOR?s reordering penalty (Lavie and
Denkowski, 2009). This metric is computed solely
on the source language side. To compute it, one
takes the candidate reordering of the input sentence
and partitions it into a set C of contiguous spans
whose content appears contiguously in the same or-
der in the reference. The reordering score is then
computed as
?(esys, eref) = 1?
|C| ? 1
|e| ? 1 .
This metric assigns a score between 0 and 1 where 1
indicates that the candidate reordering is identical to
the reference and 0 indicates that no two words that
are contiguous in the candidate reordering are con-
tiguous in the reference. For example, if a reference
reordering is A B C D E, candidate reordering A
B E C D would get score 1?(3?1)/(5?1) = 0.5.
Talbot et al (2011) show that this reordering score
is strongly correlated with human judgment of trans-
lation quality. Furthermore, they propose to evalu-
ate the reordering quality of an MT system by com-
puting its reordering score on a test set consisting
of source language sentences and their reference re-
orderings. In this paper, we take the same approach
for evaluation, and in addition, we use corpora of
source language sentences and their reference re-
orderings for training the system, not just testing
it. We describe in more detail how the reference re-
ordering data was prepared in Section 4.1.
3.2 Reordering quality as predictor of parse
quality
Figure 1 gives concrete examples of good and bad
reorderings of an English sentence into Japanese
word order. It shows that a bad parse leads to a bad
reordering (lacking inversion of verb ?wear? and ob-
ject ?sunscreen?) and a low reordering score. Could
we flip this causality around, and perhaps try to iden-
tify a good parse tree based on its reordering score?
With the experiments in this paper, we show that in-
deed a high reordering score is predictive of the un-
derlying parse tree that was used to generate the re-
ordering being a good parse (or, at least, being good
enough for our purpose).
In the case of translating English to Japanese or
another SOV language, there is a large amount of
reordering required, but with a relatively small num-
ber of reordering rules one can cover a large pro-
portion of reordering phenomena. Isozaki et al
(2010b), for instance, were able to get impressive
English?Japanese results with only a single re-
ordering rule, given a suitable definition of a head.
Hence, the reordering task depends crucially on a
correct syntactic analysis and is extremely sensitive
to parser errors.
185
4 Experimental Setup
4.1 Treebank data
In our experiments the baseline training corpus is
the Wall Street Journal (WSJ) section of the Penn
Treebank (Marcus et al, 1993) using standard train-
ing/development/testing splits. We converted the
treebank to match the tokenization expected by our
MT system. In particular, we split tokens containing
hyphens into multiple tokens and, somewhat sim-
plistically, gave the original token?s part-of-speech
tag to all newly created tokens. In Section 6 we
make also use of the Question Treebank (QTB)
(Judge et al, 2006), as a source of syntactically an-
notated out-of-domain data. Though we experiment
with both dependency parsers and phrase structure
parsers, our MT system assumes dependency parses
as input. We use the Stanford converter (de Marneffe
et al, 2006) to convert phrase structure parse trees to
dependency parse trees (for both treebank trees and
predicted trees).
4.2 Reference reordering data
We aim to build an MT system that can accurately
translate typical English text that one finds on the
Internet to SOV langauges. To this end, we ran-
domly sampled 13595 English sentences from the
web and created Japanese-word-order reference re-
orderings for them. We split the sentences arbitrarily
into a 6268-sentence Web-Train corpus and a 7327-
sentence Web-Test corpus.
To make the reference alignments we used the
technique suggested by Talbot et al (2011): ask
annotators to translate each English sentence to
Japanese extremely literally and annotate which En-
glish words align to which Japanese words. Golden
reference reorderings can be made programmati-
cally from these annotations. Creating a large set
of reference reorderings is straightforward because
annotators need little special background or train-
ing, as long as they can speak both the source and
target languages. We chose Japanese as the target
language through which to create the English refer-
ence reorderings because we had access to bilingual
annotators fluent in English and Japanese.
Good parse
Reordered:
15 or greater of an SPF has that sunscreen Wear
Reordering score: 1.0 (matches reference)
Bad parse
Reordered:
15 or greater of an SPF has that Wear sunscreen
Reordering score: 0.78 (?Wear? is out of place)
Figure 1: Examples of good and bad parses and cor-
responding reorderings for translation from English to
Japanese. The good parse correctly identifies ?Wear? as
the main verb and moves it to the end of the sentence; the
bad parse analyses ?Wear sunscreen? as a noun phrase
and does not reorder it. This example was one of the
wins in the human evaluation of Section 5.2.
4.3 Parsers
The core dependency parser we use is an implemen-
tation of a transition-based dependency parser using
an arc-eager transition strategy (Nivre, 2008). The
parser is trained using the averaged perceptron algo-
rithm with an early update strategy as described in
Zhang and Clark (2008). The parser uses the fol-
lowing features: word identity of the first two words
on the buffer, the top word on the stack and the head
of the top word on the stack (if available); part-of-
speech identities of the first four words on the buffer
and top two words on the stack; dependency arc la-
bel identities for the top word on the stack, the left
and rightmost modifier of the top word on the stack,
and the leftmost modifier of the first word in the
buffer. We also include conjunctions over all non-
lexical features.
We also give results for the latent variable parser
(a.k.a. BerkeleyParser) of Petrov et al (2006). We
convert the constituency trees output by the Berke-
leyParser to labeled dependency trees using the same
procedure that is applied to the treebanks.
While the BerkeleyParser views part-of-speech
(POS) tagging as an integral part of parsing, our
dependency parser requires the input to be tagged
186
with a separate POS tagger. We use the TnT tag-
ger (Brants, 2000) in our experiments, because of
its efficiency and ease of use. Tagger and parser are
always trained on the same data.
For all parsers, we lowercase the input at train and
test time. We found that this improves performance
in parsing web text. In addition to general upper-
case/lowercase noisiness of the web text negatively
impacting scores, we found that the baseline case-
sensitive parsers are especially bad at parsing imper-
ative sentences, as discussed in Section 5.3.2.
4.4 Reordering rules
In this paper we focus on English to Japanese, Ko-
rean, and Turkish translation. We use a superset of
the reordering rules proposed by Xu et al (2009),
which flatten a dependency tree into SOV word or-
der that is suitable for all three languages. The rules
define a precedence order for the dependents of each
part of speech. For example, a slightly simplified
version of the precedence order of child labels for
a verbal head HEADVERB is: advcl, nsubj, prep,
[other children], dobj, prt, aux, neg, HEADVERB,
mark, ref, compl.
Alternatively, we could have used an automatic
reordering-rule learning framework like that of Gen-
zel (2010). Because the reordering accuracy met-
ric can be computed for any source/target language
pair, this would have made our approach language
completely independent and applicable to any lan-
guage pair. We chose to use manually written rules
to eliminate the variance induced by the automatic
reordering-rule learning framework.
4.5 MT system
We carried out all our translation experiments on a
state-of-the-art phrase-based statistical MT system.
During both training and testing, the system reorders
source-language sentences in a preprocessing step
using the above-mentioned rules. During decoding,
we used an allowed jump width of 4 words. In ad-
dition to the regular distance distortion model, we
incorporate a maximum entropy based lexicalized
phrase reordering model (Zens and Ney, 2006) as
a feature used in decoding.
Overall for decoding, we use between 20 to
30 features, whose weights are optimized using
MERT (Och, 2003). All experiments for a given lan-
guage pair use the same set of MERT weights tuned
on a system using a separate parser (that is neither
the baseline nor the experiment parser). This po-
tentially underestimates the improvements that can
be obtained, but also eliminates MERT as a pos-
sible source of improvement, allowing us to trace
back improvements in translation quality directly to
parser changes.2
For parallel training data, we use a custom collec-
tion of parallel documents. They come from vari-
ous sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. For all language pairs, we
trained on approximately 300 million source words
each.
5 Experiments Reordering Web Text
We experimented with parsers trained in three dif-
ferent ways:
1. Baseline: trained only on WSJ-Train.
2. Standard self-training: trained on WSJ-Train
and 1-best parse of the Web-Train set by base-
line parser.
3. Targeted self-training: trained on WSJ-Train
and, for each sentence in Web-Train, the parse
from the baseline parser?s 512-best list that
when reordered gives the highest reordering
score.3
5.1 Standard self-training vs targeted
self-training
Table 1 shows that targeted self-training on Web-
Train significantly improves Web-Test reordering
score more than standard self-training for both the
shift-reduce parser and for the BerkeleyParser. The
reordering score is generally divorced from the at-
tachment scores measured on the WSJ-Test tree-
bank: for the shift-reduce parser, Web-Test reorder-
ing score and WSJ-Test labeled attachment score
2We also ran MERT on all systems and the pattern of im-
provement is consistent, but sometimes the improvement is big-
ger or smaller after MERT. For instance, the BLEU delta for
Japanese is +0.0030 with MERT on both sides as opposed to
+0.0025 with no MERT.
3We saw consistent but diminishing improvements as we in-
creased the size of the n-best list.
187
Parser Web-Test reordering WSJ-Test LAS
Shift-reduce WSJ baseline 0.757 85.31%
+ self-training 1x 0.760 85.26%
+ self-training 10x 0.756 84.14%
+ targeted self-training 1x 0.770 85.19%
+ targeted self-training 10x 0.777 84.48%
Berkeley WSJ baseline 0.780 88.66%
+ self-training 1x 0.785 89.21%
+ targeted self-training 1x 0.790 89.32%
Table 1: English?Japanese reordering scores on Web-Test for standard self-training and targeted self-training on
Web-Train. Label ?10x? indicates that the self-training data was weighted 10x relative to the WSJ training data.
Bolded reordering scores are different from WSJ-only baseline with 95% confidence but are not significantly different
from each other within the same group.
English to BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.1777 0.1802 2.56 2.69 yes (at 95% level)
Korean 0.3229 0.3259 2.61 2.70 yes (at 90% level)
Turkish 0.1344 0.1370 2.10 2.20 yes (at 95% level)
Table 2: BLEU scores and human evaluation results for translation between three language pairs, varying only the
parser between systems. ?WSJ-only? corresponds to the baseline WSJ-only shift-reduce parser; ?Targeted? corre-
sponds to the Web-Train targeted self-training 10x shift-reduce parser.
(LAS) are anti-correlated, but for BerkeleyParser
they are correlated. Interestingly, weighting the self-
training data more seems to have a negative effect on
both metrics.4
One explanation for the drops in LAS is that some
parts of the parse tree are important for downstream
reordering quality while others are not (or only to
a lesser extent). Some distinctions between labels
become less important; for example, arcs labeled
?amod? and ?advmod? are transformed identically
by the reordering rules. Some semantic distinctions
also become less important; for example, any sane
interpretation of ?red hot car? would be reordered
the same, that is, not at all.
5.2 Translation quality improvement
To put the improvement of the MT system in terms
of BLEU score (Papineni et al, 2002), a widely used
metric for automatic MT evaluation, we took 5000
sentences from Web-Test and had humans gener-
ate reference translations into Japanese, Korean, and
4We did not attempt this experiment for the BerkeleyParser
since training was too slow.
Turkish. We then trained MT systems varying only
the parser used for reordering in training and decod-
ing. Table 2 shows that targeted self-training data
increases BLEU score for translation into all three
languages.
In addition to BLEU increase, a side-by-side hu-
man evaluation on 500 sentences (sampled from
the 5000 used to compute BLEU scores) showed
a statistically significant improvement for all three
languages (see again Table 2). For each sen-
tence, we asked annotators to simultaneously score
both translations from 0 to 6, with guidelines
that 6=?Perfect?, 4=?Most Meaning/Grammar?,
2=?Some Meaning/Grammar?, 0=?Nonsense?. We
computed confidence intervals for the average score
difference using bootstrap resampling; a difference
is significant if the two-sided confidence interval
does not include 0.
5.3 Analysis
As the divergence between the labeled attachment
score on the WSJ-Test data and the reordering score
on the WSJ-Test data indicates, parsing web text
188
Parser Click as N Click as V Imperative rate
case-sensitive shift-reduce WSJ-only 74 0 6.3%
case-sensitive shift-reduce + Web-Train targeted self-training 75 0 10.5%
case-insensitive shift-reduce WSJ-only 75 0 10.3%
case-insensitive shift-reduce + Web-Train targeted self-training 75 0 11.6%
Berkeley WSJ-only 35 35 11.9%
Berkeley + Web-Train targeted self-training 13 58 12.5%
(WSJ-Train) 1 0 0.7%
Table 3: Counts on Web-Test of ?click? tagged as a noun and verb and percentage of sentences parsed imperatively.
poses very different challenges compared to parsing
newswire. We show how our method improves pars-
ing performance and reordering performance on two
examples: the trendy word ?click? and imperative
sentences.
5.3.1 Click
The word ?click? appears only once in the train-
ing portion of the WSJ (as a noun), but appears many
times in our Web test data. Table 3 shows the distri-
bution of part-of-speech tags that different parsers
assign to ?click?. The WSJ-only parsers tag ?click?
as a noun far too frequently. The WSJ-only shift-
reduce parser refuses to tag ?click? as a verb even
with targeted self-training, but BerkeleyParser does
learn to tag ?click? more often as a verb.
It turns out that the shift-reduce parser?s stub-
bornness is not due to a fundamental problem of
the parser, but due to an artifact in TnT. To in-
crease speed, TnT restricts the choices of tags for
known words to previously-seen tags. This causes
the parser?s n-best lists to never hypothesize ?click?
as a verb, and self-training doesn?t click no matter
how targeted it is. This shows that the targeted self-
training approach heavily relies on the diversity of
the baseline parser?s n-best lists.
It should be noted here that it would be easy to
combine our approach with the uptraining approach
of Petrov et al (2010). The idea would be to use the
BerkeleyParser to generate the n-best lists; perhaps
we could call this targeted uptraining. This way, the
shift-reduce parser could benefit both from the gen-
erally higher quality of the parse trees produced by
the BerkeleyParser, as well as from the information
provided by the extrinsic scoring function.
5.3.2 Imperatives
As Table 3 shows, the WSJ training set contains
only 0.7% imperative sentences.5 In contrast, our
test sentences from the web contain approximately
10% imperatives. As a result, parsers trained exclu-
sively on the WSJ underproduce imperative parses,
especially a case-sensitive version of the baseline.
Targeted self-training helps the parsers to predict im-
perative parses more often.
Targeted self-training works well for generating
training data with correctly-annotated imperative
constructions because the reordering of main sub-
jects and verbs in an SOV language like Japanese
is very distinct: main subjects stay at the begin-
ning of the sentence, and main verbs are reordered
to the end of the sentence. It is thus especially easy
to know whether an imperative parse is correct or
not by looking at the reference reordering. Figure 1
gives an example: the bad (WSJ-only) parse doesn?t
catch on to the imperativeness and gets a low re-
ordering score.
6 Targeted Self-Training vs Training on
Treebanks for Domain Adaptation
If task-specific annotation is cheap, then it is rea-
sonable to consider whether we could use targeted
self-training to adapt a parser to a new domain as
a cheaper alternative to making new treebanks. For
example, if we want to build a parser that can reorder
question sentences better than our baseline WSJ-
only parser, we have these two options:
1. Manually construct PTB-style trees for 2000
5As an approximation, we count every parse that begins with
a root verb as an imperative.
189
questions and train on the resulting treebank.
2. Create reference reorderings for 2000 questions
and then do targeted self-training.
To compare these approaches, we created reference
reordering data for our train (2000 sentences) and
test (1000 sentences) splits of the Question Tree-
bank (Judge et al, 2006). Table 4 shows that both
ways of training on QTB-Train sentences give sim-
ilarly large improvements in reordering score on
QTB-Test. Table 5 confirms that this corresponds
to very large increases in English?Japanese BLEU
score and subjective translation quality. In the hu-
man side-by-side comparison, the baseline transla-
tions achieved an average score of 2.12, while the
targeted self-training translations received a score of
2.94, where a score of 2 corresponds to ?some mean-
ing/grammar? and ?4? corresponds to ?most mean-
ing/grammar?.
But which of the two approaches is better? In
the shift-reduce parser, targeted self-training gives
higher reordering scores than training on the tree-
bank, and in BerkeleyParser, the opposite is true.
Thus both approaches produce similarly good re-
sults. From a practical perspective, the advantage of
targeted self-training depends on whether the extrin-
sic metric is cheaper to calculate than treebanking.
For MT reordering, making reference reorderings is
cheap, so targeted self-training is relatively advanta-
geous.
As before, we can examine whether labeled at-
tachment score measured on the test set of the
QTB is predictive of reordering quality. Table 4
shows that targeted self-training raises LAS from
64.78?69.17%. But adding the treebank leads
to much larger increases, resulting in an LAS of
84.75%, without giving higher reordering score. We
can conclude that high LAS is not necessary to
achieve top reordering scores.
Perhaps our reordering rules are somehow defi-
cient when it comes to reordering correctly-parsed
questions, and as a result the targeted self-training
process steers the parser towards producing patho-
logical trees with little intrinsic meaning. To explore
this possibility, we computed reordering scores after
reordering the QTB-Test treebank trees directly. Ta-
ble 4 shows that this gives reordering scores similar
to those of our best parsers. Therefore it is at least
possible that the targeted self-training process could
have resulted in a parser that achieves high reorder-
ing score by producing parses that look like those in
the QuestionBank.
7 Related Work
Our approach to training parsers for reordering is
closely related to self/up-training (McClosky. et al,
2006; Petrov et al, 2010). However, unlike uptrain-
ing, our method does not use only the 1-best output
of the first-stage parser, but has access to the n-best
list. This makes it similar to the work of McClosky.
et al (2006), except that we use an extrinsic metric
(MT reordering score) to select a high quality parse
tree, rather than a second, reranking model that has
access to additional features.
Targeted self-training is also similar to the re-
training of Burkett et al (2010) in which they
jointly parse unannotated bilingual text using a mul-
tiview learning objective, then retrain the monolin-
gual parser models to include each side of the jointly
parsed bitext as monolingual training data. Our ap-
proach is different in that it doesn?t use a second
parser and bitext to guide the creation of new train-
ing data, and instead relies on n-best lists and an
extrinsic metric.
Our method can be considered an instance of
weakly or distantly supervised structured prediction
(Chang et al, 2007; Chang et al, 2010; Clarke et al,
2010; Ganchev et al, 2010). Those methods attempt
to learn structure models from related external sig-
nals or aggregate data statistics. This work differs
in two respects. First, we use the external signals
not as explicit constraints, but to compute an ora-
cle score used to re-rank a set of parses. As such,
there are no requirements that it factor by the struc-
ture of the parse tree and can in fact be any arbitrary
metric. Second, our final objective is different. In
weakly/distantly supervised learning, the objective
is to use external knowledge to build better struc-
tured predictors. In our case this would mean using
the reordering metric as a means to train better de-
pendency parsers. Our objective, on the other hand,
is to use the extrinsic metric to train parsers that are
specifically better at the reordering task, and, as a re-
sult, better suited for MT. This makes our work more
in the spirit of Liang et al (2006), who train a per-
190
Parser QTB-Test reordering QTB-Test LAS
Shift-reduce WSJ baseline 0.663 64.78%
+ treebank 1x 0.704 77.12%
+ treebank 10x 0.768 84.75%
+ targeted self-training 1x 0.746 67.84%
+ targeted self-training 10x 0.779 69.17%
Berkeley WSJ baseline 0.733 76.50%
+ treebank 1x 0.800 87.79%
+ targeted self-training 1x 0.775 80.64%
(using treebank trees directly) 0.788 100%
Table 4: Reordering and labeled attachment scores on QTB-Test for treebank training and targeted self-training on
QTB-Train.
English to QTB-Test BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.2379 0.2615 2.12 2.94 yes (at 95% level)
Table 5: BLEU scores and human evaluation results for English?Japanese translation of the QTB-Test corpus, varying
only the parser between systems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training
10x shift-reduce parser.
ceptron model for an end-to-end MT system where
the alignment parameters are updated based on se-
lecting an alignment from a n-best list that leads to
highest BLEU score. As mentioned earlier, this also
makes our work similar to Hall et al (2011) who
train a perceptron algorithm on multiple objective
functions with the goal of producing parsers that are
optimized for extrinsic metrics.
It has previously been observed that parsers of-
ten perform differently for downstream applications.
Miyao et al (2008) compared parser quality in the
biomedical domain using a protein-protein interac-
tion (PPI) identification accuracy metric. This al-
lowed them to compare the utility of extant depen-
dency parsers, phrase structure parsers, and deep
structure parsers for the PPI identification task. One
could apply the targeted self-training technique we
describe to optimize any of these parsers for the PPI
task, similar to how we have optimized our parser
for the MT reordering task.
8 Conclusion
We introduced a variant of self-training that targets
parser training towards an extrinsic evaluation met-
ric. We use this targeted self-training approach to
train parsers that improve the accuracy of the word
reordering component of a machine translation sys-
tem. This significantly improves the subjective qual-
ity of the system?s translations from English into
three SOV languages. While the new parsers give
improvements in these external evaluations, their in-
trinsic attachment scores go down overall compared
to baseline parsers trained only on treebanks. We
conclude that when using a parser as a component
of a larger external system, it can be advantageous
to incorporate an extrinsic metric into parser train-
ing and evaluation, and that targeted self-training is
an effective technique for incorporating an extrinsic
metric into parser training.
References
A. Birch and M. Osborne. 2010. LRscore for evaluating
lexical and reordering quality in MT. In ACL-2010
WMT.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP ?00.
D. Burkett and D. Klein. 2008. Two languages are better
than one (for syntactic parsing). In EMNLP ?08.
D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010.
Learning better monolingual models with unannotated
bilingual text. In CoNLL ?10.
191
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In ACL
?07.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In ICML ?10.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world?s response.
In CoNLL ?10.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In ACL
?05.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL ?97.
M.-C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC ?06.
J. Foster. 2010. ?cba to check the spelling?: Investigat-
ing parser performance on discussion forum posts. In
NAACL ?10.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In HLT-NAACL ?04.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
COLING ?10.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP ?01.
N. Habash. 2007. Syntactic preprocessing for statistical
machine translation. In MTS ?07.
K. Hall, R. McDonald, J. Katz-Brown, and M. Ringgaard.
2011. Training dependency parsers by jointly optimiz-
ing multiple objectives. In EMNLP ?11.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010a. Automatic evaluation of translation quality for
distant language pairs. In EMNLP ?10.
H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh. 2010b.
Head finalization: A simple reordering rule for SOV
languages. In ACL-2010 WMT.
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
Bank: creating a corpus of parse-annotated questions.
In ACL ?06.
A. Lavie and M. Denkowski. 2009. The Meteor metric
for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3).
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL ?06.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
D. McClosky., E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In NAACL ?06.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL
?05.
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and J. Tsu-
jii. 2008. Task-oriented evaluation of syntactic parsers
and their representations. In ACL ?08.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL ?02.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL ?07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
S. Petrov, P. Chang, and M. Ringgaard H. Alshawi. 2010.
Uptraining for accurate deterministic question parsing.
In EMNLP ?10.
D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalua-
tion framework for machine translation reordering. In
EMNLP-2011 WMT.
C. Wang. 2007. Chinese syntactic reordering for statisti-
cal machine translation. In EMNLP ?07.
F. Xia and M. McCord. 2004. Improving a statistical MT
system with automatically learned rewrite patterns. In
Coling ?04.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
dependency parser to improve SMT for subject-object-
verb languages. In NAACL-HLT ?09.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL ?01.
R. Zens and H. Ney. 2006. Discriminative reordering
models for statistical machine translation. In NAACL-
06 WMT.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing. In EMNLP ?08.
192
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 513?523,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Source-Side Classifier Preordering for Machine Translation
Uri Lerner
Google Inc.
Mountain View, CA, USA
uri@google.com
Slav Petrov
Google Inc.
New York, NY, USA
slav@google.com
Abstract
We present a simple and novel classifier-based
preordering approach. Unlike existing pre-
ordering models, we train feature-rich dis-
criminative classifiers that directly predict the
target-side word order. Our approach com-
bines the strengths of lexical reordering and
syntactic preordering models by performing
long-distance reorderings using the structure
of the parse tree, while utilizing a discrimina-
tive model with a rich set of features, includ-
ing lexical features. We present extensive ex-
periments on 22 language pairs, including pre-
ordering into English from 7 other languages.
We obtain improvements of up to 1.4 BLEU
on language pairs in the WMT 2010 shared
task. For languages from different families the
improvements often exceed 2 BLEU. Many of
these gains are also significant in human eval-
uations.
1 Introduction
Generating the appropriate word order for the tar-
get language has been one of the fundamental prob-
lems in machine translation since the ground setting
work of Brown et al (1990). Lexical reordering ap-
proaches (Tillmann, 2004; Zens and Ney, 2006) add
a reordering component to standard phrase-based
translation systems (Och and Ney, 2004). Because
the reordering model is trained discriminatively, it
can use a rich set of lexical features. However,
it only has access to the local context which often
times is insufficient to make the long-distance re-
ordering decisions that are necessary for language
pairs with significantly different word order.
Preordering (sometimes called pre-reordering or
simply reordering) approaches (Xia and McCord,
2004; Collins et al, 2005) preprocess the input in
such a way that the words on the source side appear
closer to their final positions on the target side. Be-
cause preordering is performed prior to word align-
ment, it can improve the alignment process and can
then be combined with any subsequent translation
model. Most preordering models use a source-side
syntactic parser and perform a series of tree trans-
formations. Approaches that do not use a parser ex-
ist as well and typically induce a hierarchical rep-
resentation that also allows them to perform long-
distance changes (Tromble and Eisner, 2009; DeN-
ero and Uszkoreit, 2011; Neubig et al, 2012).
Models that use a source-side parser differ on two
main dimensions: the way tree transformations are
expressed, and whether they are built manually or
learned from data. One common type of tree trans-
formation are rewrite rules. These typically involve
some condition under which the transformation can
be applied (e.g., a noun and an adjective found in
the same clause) and the transformation itself (e.g.,
move the adjective after the noun). These rules can
be designed manually (Collins et al, 2005; Wang
et al, 2007) or learned from data (Xia and McCord,
2004; Habash, 2007; Genzel, 2010; Wu et al, 2011).
Another type of tree transformations uses ranking
functions to implement precedence-based reorder-
ing. Here, a function assigns a numerical value
to every word in a clause, intended to express the
precedence of the word in the target language. The
reordering operation is then to sort the words accord-
ing to their assigned values. The ranking function
513
can be designed manually (Xu et al, 2009) or trained
from data (Yang et al, 2012). This approach is
particularly effective for Subject-Object-Verb (SOV)
languages.
In this work we present a simple classifier-based
preordering model. Our model operates over de-
pendency parse trees and is therefore able to per-
form long-distance reordering decisions, as is typ-
ical for preordering models. But instead of deter-
ministic rules or ranking functions, we use discrim-
inative classifiers to directly predict the final word
order, using rich (bi-)lexical and syntactic features.
We present two models. The first model uses a
classifier to directly predict the permutation order in
which a family of words (a head word and all its
children) will appear on the target side. This ap-
proach is similar in spirit to the work of Li et al
(2007), except that they use constituency parse trees
and consider only nodes with 2 or 3 children. We
instead work with dependency trees and consider
much larger head-children sets. Our second model is
designed to decompose the exponential search space
of all possible permutations. The prediction task is
broken into two separate steps. In the first step, for
each child word a binary classifier decides whether
it appears before or after its parent in the target lan-
guage. In the second step, we predict the best order
of the words on each side of the parent. We show
that the second approach is never worse than the first
one and sometimes significantly better.
We present experiments on 22 language pairs
from different language families using our preorder-
ing approach in a phrase-based system (Och and
Ney, 2004), as well as a forest-to-string system
(Zhang et al, 2011). In a first set of experiments,
we use the WMT 2010 shared task data (Callison-
Burch et al, 2010) and show significant improve-
ments of up to 1.4 BLEU (Papineni et al, 2002)
on three out of eight language pairs. In a second
set of experiments, we use automatically mined par-
allel data from the web and build translation sys-
tems for languages from various language families.
We obtain especially big improvements in transla-
tion quality (2-7 BLEU) when the language pairs
have divergent word order (for example English to
Indonesian, Japanese, Korean or Malay). In our ex-
periments on English to and from Hungarian, Dutch,
and Portuguese translation, we find that we can ob-
tain consistent improvements in both translation di-
rections. To additionally verify our improvements
we use human raters, who confirm the significance
of the BLEU score improvements.
Finally, we compare training the preordering clas-
sifiers on small amounts of manually aligned data to
training on large quantities of automatically aligned
data for English to Arabic, Hebrew, and Japanese.
When evaluated on a pure reordering task, the mod-
els trained on manually aligned data perform slightly
better, but similar BLEU scores are obtained in both
scenarios on an end-to-end translation task.
2 Classifier Reordering
Our goal is to learn a model that can transform the
word order of an input sentence to an order that is
natural in the target language. For example, when
translating the English sentence:
The black cat climbed to the tree top.
to Spanish, we would like to reorder it as:
The cat black climbed to the top tree.
When translating to Japanese, we would like to get:
The black cat the tree top to climbed.
Such a model can then be used in combination with
any translation model.
In our approach we first part-of-speech (POS) tag
and parse the input sentence, producing the POS
tags and head-modifier dependencies shown in Fig-
ure 1. Reordering is then done by traversing the
dependency tree starting at the root. For each head
word we determine the order of the head and its chil-
dren (independently of other decisions) and continue
the traversal recursively in that order. In the exam-
ple, we first need to decide on the order of the head
?climbed? and the children ?cat?, ?to?, and ?.?.
2.1 Classification Model & Features
The reordering decisions are made by multi-class
classifiers where class labels correspond to permu-
tation sequences. We train a separate classifier for
each number of possible children. Crucially, we do
not learn explicit tree transformations rules, but let
the classifiers learn to trade off between a rich set of
overlapping features.
514
Obviously, it is possible to use any classification
model and learning algorithm. We use maximum
entropy classifiers with l1/l? regularization trained
with the GradBoost algorithm (Duchi and Singer,
2009). We chose this setup since it naturally sup-
ports multi-class prediction and can therefore be
used to select one out of many possible permuta-
tions. Additionally, the learning algorithm produces
a sparse set of features. In our experiments the final
models have typically only a few 100K non-zero fea-
ture weights per language pair. Given this relatively
small number of features, it is possible to manually
inspect the feature weights and gain insights into the
behavior of the model. We show an example analy-
sis in Section 5.
Our features encode information about the context
in which a word occurs in the sentence. We model
context as ?informative? words:
? The head itself.
? The children. We indicate whether each child is
before, immediately before, immediately after,
or after the head.
? For every child, if there is a gap between it and
the head, then the first and last word of that gap.
? For every pair of consecutive children, if there
is a gap between them, then the first and last
word of that gap.
? The head?s immediate sibling to the left/right or
an indication that none exists.
When extracting the features, every word can be rep-
resented by its word identity, its fine-grained POS
tag from the treebank, and a coarse-grained POS cat-
egory, similar to the universal categories described
in Petrov et al (2012). We also include pairs of these
features, resulting in potentially bilexical features.
2.2 Training Data
The training data for the classifiers is generated from
the word aligned parallel text. Since parallel data
is plentiful, we can afford to be selective. We first
construct the intersection of high-confidence source-
to-target and target-to-source alignments. For every
family in the source dependency tree we generate a
training instance if and only if the intersection de-
fines a full order on the source words:
? Every source word must be aligned to at least
one target word.
The black cat climbed to the tree top .
DT JJ NN VBD IN DT NN NN .
DET ADJ NOUN VERB ADP DET NOUN NOUN .
det
amod nsubj prep
det
nc
pobj
p
ROOT
Figure 1: A sentence, its dependency parse and its fine-
grained and coarse-grained POS tags.
? No two source words can be aligned to the
same target word.
? If a source word is aligned to multiple target
words, then no target word in this range can be
aligned to a different source word.
While this might sound restrictive, we can usually
generate at least some training instances from every
sentence and discard the remaining families in the
tree. In particular, we do not need to extract train-
ing instances for all words in a given sentence since
the reordering decisions are made independently for
every head word.
A potential concern might be that our method for
selecting training data can exclude all instances of
certain words. Consider the English phrase ?the
boy?. For languages without articles (e.g. Russian
or Japanese) the determiner ?the? may either not be
aligned to any word or get algned to the foreign
word for ?boy?. In both cases the family will be
discarded according to either the first or the second
condition above. The concern is therefore that we
would have no training data with the English word
?the?. In practice, however, this does not seem to be
a problem. First, there are instances where the En-
glish word ?the? gets aligned to something (perhaps
a preposition), and second, since the word ?the? is
omitted in the target language its location in the re-
ordered sentence is not very important.
Naturally we learn better classifier models from
better alignments. The other direction is also true
? if we run preordering on the source side then the
alignment task becomes easier and tends to produce
better results. Therefore it can be useful to iter-
ate between generating the alignment and learning
a preordering model. Empirically, the gains from
this bootstrapping approach are not dramatic and are
realized after just one iteration, i.e., create the align-
515
ment, train a preordering model, use the preordering
model to learn a new alignment, and then train the
final preordering model.
2.3 1-Step Classifier
As a first approach we use a single classifier to di-
rectly predict the correct permutation of a given fam-
ily. Consider the family headed by ?climbed? in
Figure 1. There are three children and the original
order of the words is ?cat?, ?climbed?, ?to?, and
?.?. A possible outcome of the classifier can be the
permutation 0-2-1-3, representing the order ?cat?,
?to?, ?climbed?, and ?.?.
The number of permutations for the head and n
children is of course (n + 1)!, which becomes large
very quickly and causes some problems. In practice
we therefore limit ourselves to the K most common
permutations. Unfortunately, this means that when-
ever there are many children, the correct permuta-
tion order might not be available as an option. Even
when the correct permutation is available, classifica-
tion accuracy typically deteriorates as the number of
possible classes increases.
An additional subtle issue is that the 1-step classi-
fier cannot share useful information across different
numbers of children. For example, in Spanish adjec-
tives usually appear after the noun, but sometimes
they appear before the noun. The decision depends
on the adjective itself and sometimes the head noun,
but does not depend on other children. Ideally, if for
some adjective we have enough examples with 1 or
2 children we would like to make the same decision
for a larger number of children, but these classifiers
may not have enough relevant examples.
2.4 2-Step Classifier
Our 2-step approach addresses the exponential
blowup of the number of children by decomposing
the prediction into two steps:
1. For every child, decide whether it should ap-
pear before or after the head.
2. Determine the order of the children that appear
before the head and the order of the children
after the head.
The two steps make the reordering of the modifiers
before and after the head independent of each other,
which is reminiscent of the lexicalized parse tree
generation approach of Collins (1997). In the run-
ning example, for the head ?climbed? we might first
make the following three binary decisions: the word
?cat? should appear before the head and the words
?to? and ?.? should appear after the head. In the
second step there is only one word before the head
so there is nothing to do. There are two words after
the head, so we use another classifier to determine
their order. The first step is implemented using a bi-
nary classifier, called the pivot classifier (since the
head functions like the pivot in quicksort). The sec-
ond step classifiers directly predict the correct per-
mutation of the children before / after the head.
To illustrate the effectiveness of the 2-step ap-
proach, consider a head word with 4 children. The 1-
step approach must predict 1 of 5! = 120 outcomes.
In the 2-step approach, in the worst case the second
step must predict 1 of 4! = 24 outcomes (if all the
children are on one side of the head); if we are lucky
and the children split evenly, then we only need two
binary decisions in the second step (for the two pairs
before and after the head). If we define hard cases as
cases involving 5 or more words, 5.54% of the non-
leaves are hard cases with the 1-step approach, but
only 1.07% are hard cases with the 2-step approach.
3 Experimental Setup
To provide a through evaluation of our approach, we
conduct experiments on two sets of data and with
two translation systems. The first translation system
is a phrase-based system (Och and Ney, 2004). In
addition to the regular distance distortion model, we
incorporate a maximum entropy based lexicalized
phrase reordering model (Zens and Ney, 2006). Our
second system is a forest-to-string system (Zhang et
al., 2011). The forest-to-string system uses a one-
best parse tree but factorizes it into a packed forest
of binary elementary trees ? hence the name forest-
to-string rather than tree-to-string.
The systems are configured and tuned for each
language pair to produce the best results. We then
add our 1-step and 2-step preordering classifiers as
preprocessing steps at training and test time. We
train the reordering classifiers on up to 15M train-
ing instances. We train separate classifiers for every
number of involved words, and restrict each one to
the K = 20 most frequent outcomes.
516
In our implementation, in the 1-step approach we
did not do any reordering for nodes with 7 or more
children. In the 2-step approach we did not reorder
the children on either side of the head if there were 7
or more of them. Even though there was no techni-
cal reason that prevented us from raising the thresh-
olds, there was no good reason to do so. There were
very few cases where children were not reordered
because of these thresholds, many of them corre-
sponded to bad parses, and they had very little im-
pact on the final scores. Thus, for the 1-step ap-
proach we had 6 classifiers: 1 binary classifier for a
head and a single child and 5 multi-class classifiers
for 3?7 words. For the 2-step approach we had 11
classifiers: 1 pivot classifier, 5 classifiers for words
before the head, and 5 for words after the head.
For a direct comparison to a strong preordering
system, we compare to the system of Genzel (2010),
which learns a set of unlexicalized reordering rules
from automatically aligned data by minimizing the
number of crossing alignments. We used a sliding
window of size 3 and tried all three of their vari-
ants. There were about 40-50 rules per language
pair. While conceptually possible, it is not practi-
cal to learn more rules (including lexicalized rules)
with this system, because of the computational com-
plexity of the learning algorithm and the incremental
nature in which the rules are learned and applied.
3.1 WMT Setup
In our first set of experiments, we use the data pro-
vided for the WMT 2010 shared task (Callison-
Burch et al, 2010). We build systems for all lan-
guage pairs: English to and from Czech, French,
German, and Spanish. Since this is a publicly avail-
able dataset, it is easy to compare our results to other
submissions to the shared task.
During word alignment, we filter out sentences
exceeding 60 words in the parallel texts and per-
form 6 iterations of IBM Model-1 training (Brown
et al, 1993), followed by 6 iterations of HMM train-
ing (Vogel et al, 1996). We do not use Model-4
because it is slow and did not add much value to our
systems in a pilot study. Standard phrase extraction
heuristics (Koehn et al, 2003) are applied to extract
phrase pairs with a length limit of 6 from alignments
symmetrized with the ?union? heuristic. Maximum
jump width is set to 8. Rule extraction for the forest-
to-string system is limited to 16 rules per tree node.
There are no length-based reordering constraints in
the forest-to-string system. We train two 5-gram lan-
guage models with Kneser-Ney smoothing for each
of the target languages. One is trained on the tar-
get side of the parallel text, the other on a news cor-
pus provided by the shared task. We tune the fea-
ture weights for every configuration with 10 rounds
of hypergraph-based Minimum Error Rate Training
(MERT) (Kumar et al, 2009).
3.2 Additional Languages
In our second set of experiments, we explore the im-
pact of classifier preordering for a number of lan-
guages with different word orders. Some of the lan-
guages included in our study are verb-subject-object
(VSO) languages (Arabic, Irish, Welsh), subject-
object-verb (SOV) languages (Japanese, Korean),
and fairly free word order languages (Dutch, Hun-
garian). Where a parser is available, we also conduct
experiments on translating into English.
Since there are no standard training sets for many
of these language pairs, we use parallel data auto-
matically mined from the web. The amount of par-
allel text for each language pair is between 120M
and 160M words. For evaluation, we use a set of 9K
English sentences collected from the web and trans-
lated by humans into each of the target languages.
Each sentence has one reference translation. We use
5K sentences for evaluation and the rest for tuning.
The systems and training configurations are sim-
ilar to the WMT setup. The word alignment step
includes 3 iterations of IBM Model-1 training and
2 iterations of HMM training. Lexical reordering is
included where it helps, but typically makes only a
small difference. We again use a 5-gram language
model trained on a large amount of monolingual
text. Overall, we use between 20 and 30 features,
whose weights are optimized using hypergraph-
based MERT. All experiments for a given language
pair use the same set of MERT weights. This po-
tentially underestimates the improvements that can
be obtained, but also eliminates MERT as a pos-
sible source of improvement, allowing us to trace
back improvements in translation quality directly to
changes in preordering of the input data.
517
3.3 Evaluation
We use case-sensitive BLEU (Papineni et al, 2002)
to assess translation quality. For Japanese and Ko-
rean we use character-level BLEU. We use bootstrap
resampling to compute confidence intervals.
Additionally, we also conduct a side-by-side hu-
man evaluation on 750 sentences for each language
pair (sampled from the same sentences used for
computing BLEU). For each sentence, we ask bilin-
gual annotators to compare the translations from two
different systems and say whether one is better, lead-
ing to three possible scores of -1, 0, and +1. We fo-
cus on this relative comparison since absolute scores
are difficult to calibrate across languages and raters.
3.4 Syntactic Parsers
Table 1 shows our treebank sources and parsing ac-
curacies. For English, we use the updated WSJ with
OntoNotes-style annotations converted to Stanford
dependencies (de Marneffe et al, 2006). The re-
maining treebanks are all available in dependency
format. In all cases, we apply a set of heuristics to
the treebank data to make the tokenization as simi-
lar as possible to the one of the bitext. Our heuristics
can split treebank tokens but do not merge treebank
tokens. We found that adjusting the treebank tok-
enization is crucial for obtaining good results. How-
ever, this makes the reported parsing accuracies not
comparable to other numbers in the literature. When
necessary, we projectivize the treebanks by raising
arcs until the tree becomes projective, as described
in Nivre and Nilsson (2005); we do not reconstruct
non-projective arcs at parsing time, since our subse-
quent systems expect projective trees.
Our part-of-speech tagger is a conditional random
field model (Lafferty et al, 2001) with simple word-
identity and affix features. The parsing model is
a shift-reduce dependency parser, using the higher-
order features from Zhang and Nivre (2011). Addi-
tionally, we include 256 word-cluster features (Koo
et al, 2008) trained on a large amount of unlabeled
monolingual text (Uszkoreit and Brants, 2008).
4 Experiments
Due to the large number of experiments and lan-
guage pairs we divide the experiments into groups
and discuss each in turn. We only include the results
UAS LAS POS
en: English1 92.28 90.28 97.05
cs: Czech2 84.66 72.01 98.97
de: German3 89.30 86.98 97.69
es: Spanish4 86.24 82.32 96.62
fr: French5 88.57 86.40 97.48
hu: Hungarian2 87.66 82.51 94.47
nl: Dutch3 86.09 82.31 97.38
pt: Portuguese4 90.22 87.26 98.10
Table 1: Parsing accuracies on the retokenized treebanks.
UAS is unlabeled attachment score, LAS is labeled at-
tachment score, and POS is part-of-speech tagging accu-
racy. The treebank sources are (1): Marcus et al (1993)
+ Judge et al (2006) + Petrov and McDonald (2012), (2):
Nivre et al (2007), (3): Buchholz and Marsi (2006), (4):
McDonald et al (2013), (5): Abeille? et al (2003).
from the forest-to-string system when they are bet-
ter than the phrase-based results. We use * to denote
results from the forest-to-string system.
4.1 WMT Experiments
Table 2 presents detailed results on the WMT setup.
Lexical reordering (Zens and Ney, 2006) never hurts
and is thus included in all systems. Overall, our re-
sults are a little better than the best results of the
WMT 2010 shared task for two language pairs and
within reach of the best results in most other cases.
The 2-step classifier preordering approach pro-
vides statistically significant improvements over the
lexical reordering baseline on three out of the eight
language pairs: English-Spanish (en-es: 1.4 BLEU),
German-English (de-en: 1.2 BLEU), and English-
French (en-fr: 1.0 BLEU). These improvements are
significant in our human side-by-side evaluation.
We also observe gains when combining our pre-
ordering approach with the forest-to-string system
for English-Spanish and German-English. While the
forest-to-string system is capable of performing long
distance reordering in the decoder, it appears that
an explicitly trained lexicalized preordering model
can provide complementary benefits. These bene-
fits are especially pronounced for German-English
where long distance verb movement is essential. For
the romance languages (Spanish and French), word
ordering depends highly on lexical choice which is
captured by the lexical features in our classifiers.
518
base lexical rule 1-step 2-step wmt best
en-cs 14.9 15.1 15.2 15.2 15.2 15.4
en-de 15.3 15.6 15.9 15.9 15.7 16.3
en-es 27.4 27.8s 28.4p 29.0 28.8?? 28.6
en-es* 28.9 - 28.7 29.0 29.2 28.6
en-fr 26.3 26.5s 26.8p 27.2 27.3?p 27.6
cs-en 21.6 21.6 21.5 21.6 21.7 21.9
de-en 20.6 21.1s 21.9 21.9 21.8? 22.8
de-en* 22.1 - 22.5 22.5 22.7 22.8
es-en 28.3 28.7 28.7 28.8 28.9 28.8
fr-en 26.8 27.0 26.9 26.9 27.0 28.3
Table 2: BLEU scores on the WMT 2010 setup. Results from the forest-to-string system are marked with * and are
only included when better than the phrase-based results. The base system includes a distance distortion model; the
lexical system adds lexical reordering; rule is the rule preordering system of Genzel (2010) plus lexical reordering;
1-step and 2-step are our classifier-based systems plus lexical reordering. Bolded results are statistically significantly
better than non-bolded results as measured by a bootstrap sample test with a 99% confidence interval. Human evals
are conducted only where indicated; we use ? and ? to indicate a significantly better result than s and p in the human
eval at 95%. Also included are the best results from the WMT 2010 task.
Compared to a state-of-the-art preordering sys-
tem, the automatic rule extraction system of Gen-
zel (2010), we observe significant gains in several
cases and no losses at all. The improvements on
English-Spanish are significant also in the human
evaluation, while the English-French improvements
are positive, but not statistically significant.
Comparing the different languages, Czech (cs) ap-
pears the most immune to improvements from pre-
ordering (and lexical reordering). One possible ex-
planation is that Czech has a relatively free word
order with a default SVO structure. It is therefore
difficult to learn reordering changes from English
to Czech. Additionally, the accuracy (LAS) of our
Czech parser is by far the lowest of all parsers that
we used, potentially limiting the benefits that can be
obtained when translating from Czech into English.
On this setup there is fairly little difference in per-
formance between the 1-step and 2-step approaches.
The main benefit of the 2-step approach is compact-
ness: the set of 2-step classifiers has about half the
number of non-zero features as the 1-step classifiers.
4.2 Additional Languages Experiments
Table 3 shows our first set of results on the additional
languages, including some languages with a wide
disparity in word order relative to English. The SOV
languages Korean (ko) and Japanese (ja) benefit the
most from preordering and gain more than 7 BLEU
relative to the phrase-based baseline and still more
than 3 BLEU for the forest-to-string system. Simi-
lar improvements were reported by Xu et al (2009)
with manual reordering rules. Indonesian (id) and
Malay (ms) are next with gains of 2.5 BLEU. Malay
does not have a grammatical subject in the sense
that English does, but instead uses a concept of an
agent and an object, whose order is determined by
the voice of the verb. It appears that our classifiers
have learned to model some of these highly lexical,
but systematic ordering preferences. Welsh (cy) and
Irish (ga) as VSO languages also exhibit large gains
of 2.1 BLEU. For Arabic (ar) and Hebrew (iw), the
gains are smaller, but still significant and exceed 1
BLEU relative to the baseline.
The benefits of our 2-step approach over the 1-
step approach become apparent on this set of lan-
guages where reordering is most important. By pre-
dicting the target word order in two steps, we reduce
sparsity and make two easier decisions in place of
a single difficult high entropy decision. Indeed, the
2-step approach produces improvements over the 1-
step approach on five out of nine language pairs. The
improvements are as large as 0.9 BLEU for Korean
and 0.5 BLEU for Japanese and Welsh. We per-
formed human evaluation for all language pairs with
a noticeable BLEU gain for the 2-step system over
519
base rule 1-step 2-step
en-ar 11.4 12.3 12.5 12.6
en-cy 29.3 31.1 31.9p 32.4?
en-ga 17.0 18.5 18.8p 19.1?
en-iw 18.8 19.7 20.2 20.2
en-id 31.0 33.4 34.0p 34.3p
en-ja 10.4 16.4 17.5p 18.0?
en-ja* 14.9 18.0 18.2p 18.6?
en-ko 24.1 31.8 31.8p 32.7?
en-ms 20.4 22.5 22.9 22.9
Table 3: BLEU scores for language from various lan-
guage families: Arabic (ar), Welsh (cy), Irish (ga), In-
donesian (id), Hebrew (iw), Japanese (ja), Korean (ko),
and Malay (ms). Lexical reordering is not included in
any of the systems. Bolded results are significant at 99%.
? is significantly better than p in a human eval at 95%.
the 1-step system. The human judgments exactly
agree with the results of the BLEU significance tests.
The gains relative to the rule reordering system of
Genzel (2010) and the no-preordering baseline are
even larger and therefore clearly also significant.
In Table 4 we show results for Hungarian (hu),
Dutch (nl), and Portuguese (pt). In all cases but
English-Hungarian we observe significant improve-
ments over the no preordering baseline. It should be
noted that the gains are not symmetric ? sometimes
there are larger gains for translating out of English,
while for Hungarian the gains are higher for trans-
lating into English. Hungarian has a free word order
which is difficult to predict which might partially ex-
plain why there are no improvements for translating
into Hungarian. For Dutch-English, the forest-to-
string system yields the best results, which was also
the case for German-English, further supporting the
observation that combining different types of syn-
tactic reordering approaches can be beneficial.
4.3 Manually Aligned Data
For Arabic (ar), Hebrew (iw), and Japanese (ja) we
conducted some additional experiments with man-
ually aligned data. We asked bilingual speakers to
translate about 20K English sentences into the re-
spective target language and to mark the alignment
between the words. We reserved 20% of this data for
evaluation and used the rest for training. For evalu-
ation we used the fuzzy metric defined by Talbot et
base rule 1-step 2-step
en-hu 12.7 12.6 12.8 12.7
en-nl 25.3 26.1 26.4 26.4
en-pt 30.2 31.9 32.6 32.8
hu-en 22.0 22.2 22.7 22.7
nl-en 34.9 35.7 35.2 35.1
nl-en* 36.3 36.5 36.6 36.7
pt-en 39.8 40.1 40.1 40.1
Table 4: BLEU scores for translating to and from En-
glish for: Hungarian (hu), Dutch (nl), and Portuguese
(pt). Lexical reordering is not used for any language pair.
Bolded results are significant at 99%.
al. (2011), which counts the fraction of words that
are reordered into the correct position.
The BLEU scores in Table 5 show that training
from small amounts of manually aligned data or
large amounts of automatically aligned data results
in models of similar quality. In terms of the fuzzy
metric, the models trained from manually aligned
data were better. A possible explanation is that these
models were trained on data which was much more
similar to the evaluation data (both were subsets of
the manually aligned data), biasing the metric in
their favor. In absolute terms, the reordering ac-
curacy is around 80% for Arabic and Japanese and
close to 90% for Hebrew. Most impressively, more
than 60% of the Hebrew sentences are exactly in the
correct word order, implying that monotonic trans-
lation may suffice.
We also examined the accuracy of the individual
classifiers and found that the pivot classifier has an
accuracy around 95%. It is therefore unlikely that
a word is reordered to the wrong side of its head in
the 2-step reordering approach. The classifiers that
predict the final word order have an accuracy above
90% when there are only two words and drop to still
respectable 70%-80% when there are 4 or more chil-
dren or 20 possible options.
5 Analysis
In this section, we analyze an example whose trans-
lation is significantly improved by our preordering
approach, demonstrating the usefulness of our lexi-
calized features. Consider the English sentence:
It was a real whirlwind.
520
no reordering manual automatic
fuzzy exact BLEU fuzzy exact BLEU fuzzy exact BLEU
en-ar 63.2 19.8 11.4 83.5 47.6 12.4 79.0 38.9 12.6
en-iw 67.9 22.2 18.8 89.8 62.4 20.3 89.2 61.2 20.2
en-ja* 44.1 0.0 14.9 80.9 41.5 18.4 78.5 36.8 18.6
Table 5: Preordering accuracy for the 2-step classifiers using manual alignments vs. automatic alignments. Fuzzy
refers to the metric defined in Talbot et al (2011) and exact is the percentage of sentences with a perfect preordering.
taken from the WMT test set. The dependency parse
tree is shown in Figure 2. In our experiments the
rule-based approach of (Genzel, 2010) reordered the
source sentence into:
It was a whirlwind real.
and produced the translation:
Es un torbellino real.
In comparison, our 2-step system kept the English
sentence unchanged and produced the translation:
Fue un aute?ntico torbellino.
The second translation is better than the first because
of the correct tense (which is not related directly to
the preordering) and because the noun phrase ?real
whirlwind? is ordered correctly.
The main reason for the difference in the ordering
is that the rule-based system can only use the unlex-
icalized information from the parse tree. The head
?whirlwind? is a noun and the child ?real? is an ad-
jective; since adjectives typically appear after nouns
in Spanish, their order is reversed.
To understand why the classifier-based system
keeps ?real? before ?whirlwind? we can examine
the features used by the classifier to make this deci-
sion. In Table 6 we consider the 3 strongest features
in favor of the child ?real? appearing after the head
?whirlwind? and the three strongest features in favor
of the child appearing before the head. Recall that
the pivot is a binary classifier: positive features sup-
port one decision (in our case: the child should be
after the head) and the negative features support the
other decision (the child should be before the head).
The three features that have the highest positive
weight encode the fact that the child is an adjective,
since in general, adjectives in Spanish appear after
the noun. On the other hand, the three features with
the most negative weights all encode the fact that the
child is the word ?real? which unlike most adjec-
tives tends to appear before the noun. It is interesting
to note that for this particular ordering decision the
child word is much more informative than the head
word and indeed, all the important features contain
information about the child and none of them con-
tains any information about the head.
6 Conclusions & Future Work
We presented a simple and novel preordering ap-
proach that produces substantial improvements in
translation accuracy on a large number of languages.
We use a source-side syntactic parser and train dis-
criminative classifiers to predict the order of a parent
and its children in the target language, using features
from the dependency tree as well as (bi-)lexical fea-
tures. To decompose the exponential space of all
possible permutations, we introduce the 2-step ap-
proach. We show empirically that this approach is
significantly better than directly predicting the full
permutation for some languages, and never signifi-
cantly worse.
We obtain strong results on the WMT 2010 shared
task data, observing gains of up to 1.4 BLEU over a
state-of-the-art system. We also show gains of up
to 0.5 BLEU over a strong directly comparable pre-
ordering system that is based on learning unlexical-
ized reordering rules. We obtain improvements of
more than 2 BLEU in experiments on additional lan-
guages. The gains are especially large for languages
where the sentence structure is very different from
English. These positive results are confirmed in hu-
man side-by-side evaluations.
When comparing our approach to syntax-based
translation systems (Yamada and Knight, 2001; Gal-
ley et al, 2004; Huang et al, 2006; Dyer and Resnik,
2010) we note that both approaches use syntactic in-
formation for reordering decisions. Our preorder-
ing approach has several advantages. First, be-
521
It was a real whirlwind .
NN VBD DT JJ NN .
NOUN VERB DET ADJ NOUN .
nsubj
det
amod
attr
p
ROOT
Figure 2: An example where lexical information is nec-
essary for choosing the correct word order.
cause preordering is performed before learning word
alignments, it has the potential to improve the word
alignments. Second, by using discriminative clas-
sifiers we can take advantage of lexical features.
Finally, preordering can be combined with syntax-
based translation models and our results confirm the
complementary benefits that can be obtained.
Compared to other preordering models, our ap-
proach has the obvious problem of having to make
predictions over an exponential set of permutations.
We show that this is not an insurmountable diffi-
culty: our 2-step approach decomposes the exponen-
tial space, often leading to much easier prediction
tasks. Even when the number of possible permuta-
tions is large we can limit ourselves to the K most
popular permutations.
On the other hand, our approach provides im-
portant advantages. Compared to systems that use
rewrite rules, it is much easier to encode useful
knowledge that by itself is not enough to determine
a full rewrite rule, such as ?a determiner is unlikely
to be the last word in a clause.? Perhaps more im-
portantly, our model provides an elegant answer to
the question of what to do when multiple rewrite
rules can be applied. Previous work has employed
different heuristics: use the most specific rule (Xia
and McCord, 2004), use all applicable rules (Gen-
zel, 2010), or use the most frequent rule (Wu et al,
2011). In our model there is no need for such heuris-
tics ? all the ?rules? are treated as features to a dis-
criminative classifier, and the task of analyzing their
interactions is handled by the learning algorithm.
Compared to preordering systems that use rank-
ing functions, our model has the advantage that it
can encode information about the complete permu-
tation. For example, for three source words A, B,
and C, we can naturally express the useful prior that
Feature Weight
PrevChild:tag=JJ,PrevSibling:a 0.448
PrevChild:cat=ADJ,PrevSibling:a 0.292
PrevChild:cat=ADJ,NoNextSibling 0.212
...
PrevChild:real,NoNextHeadSibling -0.310
PrevChild:real,PrevSibling:cat=DET -0.516
PrevChild:real,PrevSibling:a -0.979
Table 6: The three features with the highest and low-
est weights for choosing the position of ?real? relative
to ?whirlwind.? PrevChild means that the child is the
immediate word before the head. PrevSibling refers to
the child?s sibling immediately to the left (the determiner
?a?). NoNextSibling and NoNextHeadSibling mean that
the child and head do not have a sibling to the right.
A-B-C and C-B-A are likely orders but C-A-B is not.
Promising directions for future work are joint
parsing and reordering models, and measuring the
influence of parsing accuracy on preordering and fi-
nal translation quality.
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Build-
ing a Treebank for French. In A. Abeille?, editor, Tree-
banks: Building and Using Parsed Corpora, chap-
ter 10. Kluwer.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2).
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL ?06.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O. Zaidan. 2010. Findings of
the 2010 joint workshop on statistical machine trans-
lation and metrics for machine translation. In Proc. of
ACL?05 WMT.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of ACL ?05.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL ?97.
M.-C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC ?06.
522
J. DeNero and J. Uszkoreit. 2011. Inducing sentence
structure from parallel corpora for reordering. In Proc.
of EMNLP ?11.
J. Duchi and Y. Singer. 2009. Boosting with structural
sparsity. In Proc. of ICML ?09.
C. Dyer and P. Resnik. 2010. Context-free reordering,
finite-state translation. In Proc. of NAACL-HLT ?10.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In Proc. of NAACL-HLT
?04.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
Proc. of COLING ?10.
N. Habash. 2007. Syntactic preprocessing for statistical
machine translation. In Proc. of MTS ?07.
L. Huang, K. Knight, and A. Joshi. 2006. Statistical
syntax-directed translation with extended domain of
locality. In Proc. of AMTA ?06.
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
Bank: creating a corpus of parse-annotated questions.
In Proc. of ACL ?06.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase based translation. In Proc. of NAACL-HLT ?03.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. of ACL-HLT
?08.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. of ACL ?09.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML
?01.
C. H. Li, M. Li, D. Zhang, M. Li, M. Zhou, and Y. Guan.
2007. A Probabilistic Approach to Syntax-based Re-
ordering for Statistical Machine Translation. In Proc.
of ACL ?07.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
R. McDonald, J. Nivre, Y. Quirmbach-Brundagez,
Y. Goldberg, D. Das, K. Ganchev, K. Hall, S. Petrov,
H. Zhang, O. Ta?ckstro?m, C. Bedini, N. Bertomeu
Castello?, and J. Lee. 2013. Universal dependency an-
notation for multilingual parsing. In Proc. of ACL ?13.
G. Neubig, T. Watanabe, and S. Mori. 2012. Inducing a
discriminative parser to optimize machine translation
reordering. In Proc. of EMNLP-CoNLL ?12.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. of ACL ?05.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proc. EMNLP-
CoNLL ?07.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30(4).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL ?02.
S. Petrov and R. McDonald. 2012. Overview of the 2012
shared task on parsing the web. In Proc. of NAACL ?12
SANCL.
S. Petrov, D. Das, and R. McDonald. 2012. A universal
part-of-speech tagset. In Proc. of LREC ?12.
D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalua-
tion framework for machine translation reordering. In
Proc. of EMNLP ?11 WMT.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proc. of NAACL-HLT
?04.
R. Tromble and J. Eisner. 2009. Learning linear ordering
problems for better translation. In Proc. of EMNLP
?09.
J. Uszkoreit and T. Brants. 2008. Distributed word clus-
tering for large scale class-based language modeling in
machine translation. In Proc. of ACL-HLT ?08.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In In Proc. of
COLING ?96.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
Proc. of EMNLP-CoNLL ?07.
X. Wu, K. Sudoh, K. Duh, H. Tsukada, and M. Nagata.
2011. Extracting pre-ordering rules from predicate-
argument structures. In Proc. of IJCNLP ?11.
F. Xia and M. McCord. 2004. Improving a statistical MT
system with automatically learned rewrite patterns. In
Proc. of COLING ?04.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
dependency parser to improve SMT for subject-object-
verb languages. In Proc. of NAACL-HLT ?09.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In Proc. of ACL ?01.
N. Yang, M. Li, D. Zhang, and N. Yu. 2012. A ranking-
based approach to word reordering for statistical ma-
chine translation. In Proc. of ACL ?12.
R. Zens and H. Ney. 2006. Discriminative reordering
models for statistical machine translation. In Proc. of
NAACL ?06 WMT.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc. of
ACL-HLT ?11.
H. Zhang, L. Fang, P. Xu, and X. Wu. 2011. Binarized
forest to string translation. In Proc. of ACL-HLT ?11.
523
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1273?1283,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning Compact Lexicons for CCG Semantic Parsing
Yoav Artzi
?
Computer Science & Engineering
University of Washington
Seattle, WA 98195
yoav@cs.washington.edu
Dipanjan Das Slav Petrov
Google Inc.
76 9th Avenue
New York, NY 10011
{dipanjand,slav}@google.com
Abstract
We present methods to control the lexicon
size when learning a Combinatory Cate-
gorial Grammar semantic parser. Existing
methods incrementally expand the lexicon
by greedily adding entries, considering a
single training datapoint at a time. We pro-
pose using corpus-level statistics for lexi-
con learning decisions. We introduce vot-
ing to globally consider adding entries to
the lexicon, and pruning to remove entries
no longer required to explain the training
data. Our methods result in state-of-the-art
performance on the task of executing se-
quences of natural language instructions,
achieving up to 25% error reduction, with
lexicons that are up to 70% smaller and are
qualitatively less noisy.
1 Introduction
Combinatory Categorial Grammar (Steedman,
1996, 2000, CCG, henceforth) is a commonly
used formalism for semantic parsing ? the task
of mapping natural language sentences to for-
mal meaning representations (Zelle and Mooney,
1996). Recently, CCG semantic parsers have been
used for numerous language understanding tasks,
including querying databases (Zettlemoyer and
Collins, 2005), referring to physical objects (Ma-
tuszek et al., 2012), information extraction (Kr-
ishnamurthy and Mitchell, 2012), executing in-
structions (Artzi and Zettlemoyer, 2013b), gen-
erating regular expressions (Kushman and Barzi-
lay, 2013), question-answering (Cai and Yates,
2013) and textual entailment (Lewis and Steed-
man, 2013). In CCG, a lexicon is used to map
words to formal representations of their meaning,
which are then combined using bottom-up opera-
tions. In this paper we present learning techniques
?
This research was carried out at Google.
chair ` N : ?x.chair(x)
chair ` N : ?x.sofa(x)
chair ` AP : ?a.len(a, 3)
chair ` NP : A(?x.corner(x))
chair ` ADJ : ?x.hall(x)
Figure 1: Lexical entries for the word chair as learned
with no corpus-level statistics. Our approach is able to
correctly learn only the top two bolded entries.
to explicitly control the size of the CCG lexicon,
and show that this results in improved task perfor-
mance and more compact models.
In most approaches for inducing CCGs for se-
mantic parsing, lexicon learning and parameter es-
timation are performed jointly in an online algo-
rithm, as introduced by Zettlemoyer and Collins
(2007). To induce the lexicon, words extracted
from the training data are paired with CCG cat-
egories one sample at a time (for an overview of
CCG, see ?2). Joint approaches have the potential
advantage that only entries participating in suc-
cessful parses are added to the lexicon. However,
new entries are added greedily and these decisions
are never revisited at later stages. In practice, this
often results in a large and noisy lexicon.
Figure 1 lists a sample of CCG lexical entries
learned for the word chair with a greedy joint al-
gorithm (Artzi and Zettlemoyer, 2013b). In the
studied navigation domain, the word chair is often
used to refer to chairs and sofas, as captured by the
first two entries. However, the system also learns
several spurious meanings: the third shows an er-
roneous usage of chair as an adverbial phrase de-
scribing action length, while the fourth treats it as
a noun phrase and the fifth as an adjective. In con-
trast, our approach is able to correctly learn only
the top two lexical entries.
We present a batch algorithm focused on con-
trolling the size of the lexicon when learning CCG
semantic parsers (?3). Because we make updates
only after processing the entire training set, we
1273
can take corpus-wide statistics into account be-
fore each lexicon update. To explicitly control
the size of the lexicon, we adopt two complemen-
tary strategies: voting and pruning. First, we con-
sider the lexical evidence each sample provides as
a vote towards potential entries. We describe two
voting strategies for deciding which entries to add
to the model lexicon (?4). Second, even though
we use voting to only conservatively add new lex-
icon entries, we also prune existing entries if they
are no longer necessary for parsing the training
data. These steps are incorporated into the learn-
ing framework, allowing us to apply stricter crite-
ria for lexicon expansion while maintaining a sin-
gle learning algorithm.
We evaluate our approach on the robot navi-
gation semantic parsing task (Chen and Mooney,
2011; Artzi and Zettlemoyer, 2013b). Our exper-
imental results show that we outperform previous
state of the art on executing sequences of instruc-
tions, while learning significantly more compact
lexicons (?6 and Table 3).
2 Task and Inference
To present our lexicon learning techniques, we
focus on the task of executing natural language
navigation instructions (Chen and Mooney, 2011).
This domain captures some of the fundamental
difficulties in recent semantic parsing problems.
In particular, it requires learning from weakly-
supervised data, rather than data annotated with
full logical forms, and parsing sentences in a
situated environment. Additionally, successful
task completion requires interpreting and execut-
ing multiple instructions in sequence, requiring
accurate models to avoid cascading errors. Al-
though this overview centers around the aforemen-
tioned task, our methods are generalizable to any
semantic parsing approach that relies on CCG.
We approach the navigation task as a situated
semantic parsing problem, where the meaning of
instructions is represented with lambda calculus
expressions, which are then deterministically ex-
ecuted. Both the mapping of instructions to logi-
cal forms and their execution consider the current
state of the world. This problem was recently ad-
dressed by Artzi and Zettlemoyer (2013b) and our
experimental setup mirrors theirs. In this section,
we provide a brief background on CCG and de-
scribe the task and our inference method.
walk forward twice
S/NP NP AP
?x.?a.move(a) ? direction(a, x) forward ?a.len(a, 2)
>
S S\S
?a.move(a) ? direction(a, forward) ?f.?a.f(a) ? len(a, 2)
<
S
?a.move(a) ? direction(a, forward) ? len(a, 2)
in the red hallway
PP/NP NP/N ADJ N
?x.?y.intersect(y, x) ?f.?(f) ?x.brick(x) ?x.hall(x)
N/N
?f.?x.f(x)?
brick(x)
<
N
?x.hall(x) ? brick(x)
>
NP
?(?x.hall(x) ? brick(x)
>
PP
?y.intersect(y, ?(?x.hall(x) ? brick(x)))
Figure 2: Two CCG parses. The top shows a complete
parse with an adverbial phrase (AP ), including unary
type shifting and forward (>) and backward (<) ap-
plication. The bottom fragment shows a prepositional
phrase (PP ) with an adjective (ADJ).
2.1 Combinatory Categorial Grammar
CCG is a linguistically-motivated categorial for-
malism for modeling a wide range of language
phenomena (Steedman, 1996; Steedman, 2000).
In CCG, parse tree nodes are categories, which are
assigned to strings (single words or n-grams) and
combined to create a complete derivation. For ex-
ample, S/NP : ?x.?a.move(a)? direction(a, x)
is a CCG category describing an imperative verb
phrase. The syntactic type S/NP indicates the
category is expecting an argument of type NP
on its right, and the returned category will have
the syntax S. The directionality is indicated by
the forward slash /, where a backward slash \
would specify the argument is expected on the left.
The logical form in the category represents its se-
mantic meaning. For example, ?x.?a.move(a) ?
direction(a, x) in the category above is a function
expecting an argument, the variable x, and return-
ing a function from events to truth-values, the se-
mantic representation of imperatives. In this do-
main, the conjunction in the logical form specifies
conditions on events. Specifically, the event must
be a move event and have a specified direction.
A CCG is defined by a lexicon and a set of com-
binators. The lexicon provides a mapping from
strings to categories. Figure 2 shows two CCG
parses in the navigation domain. Parse trees are
read top to bottom. Parsing starts by matching cat-
egories to strings in the sentence using the lexicon.
For example, the lexical entry walk ` S/NP :
?x.?a.move(a) ? direction(a, x) pairs the string
walk with the example category above. Each in-
termediate parse node is constructed by applying
1274
one of a small set of binary CCG combinators or
unary operators. For example, in Figure 2 the cat-
egory of the span walk forward is combined with
the category of twice using backward application
(<). Parsing concludes with a logical form that
captures the meaning of the complete sentence.
We adopt a factored representation for CCG
lexicons (Kwiatkowski et al., 2011), where
entries are dynamically generated by combining
lexemes and templates. A lexeme is a pair
that consists of a natural language string and
a set of logical constants, while the template
contains the syntactic and semantic components
of a CCG category, abstracting over logical
constants. For example, consider the lexical entry
walk ` S/NP : ?x.?a.move(a) ? direction(a, x).
Under the factored representation, this entry
can be constructed by combining the lexeme
?walk, {move,direction}? and the template
?v
1
.?v
2
.[S/NP : ?x.?a.v
1
(a) ? v
2
(a, x)]. This
representation allows for better generalization
over unseen lexical entries at inference time,
allowing for pairings of templates and lexemes
not seen during training.
2.2 Situated Log-Linear CCGs
We use a CCG to parse sentences to logical forms,
which are then executed. Let S be a set of states,
X be the set of all possible sentences, and E be
the space of executions, which are S ? S func-
tions. For example, in the navigation task from
Artzi and Zettlemoyer (2013b), S is a set of po-
sitions on a map, as illustrated in Figure 3. The
map includes an agent that can perform four ac-
tions: LEFT, RIGHT, MOVE, and NULL. An execu-
tion e is a sequence of actions taken consecutively.
Given a state s ? S and a sentence x ? X , we aim
to find the execution e ? E described in x. Let Y
be the space of CCG parse trees and Z the space
of all possible logical forms. Given a sentence x
we generate a CCG parse y ? Y , which includes a
logical form z ? Z . An execution e is then gener-
ated from z using a deterministic process.
Parsing with a CCG requires choosing appro-
priate lexical entries from an often ambiguous lex-
icon and the order in which operations are ap-
plied. In a situated scenario such choices must
account for the current state of the world. In gen-
eral, given a CCG, there are many parses for each
sentence-state pair. To discriminate between com-
peting parses, we use a situated log-linear CCG,
facing the chair in the intersection move forward twice
?a.pre(a, front(you, ?(?x.chair(x)?
intersect(x, ?(?y.intersection(y))))))?
move(a) ? len(a, 2)
?FORWARD, FORWARD?
turn left
?a.turn(a) ? direction(a, left)
?LEFT?
go to the end of the hall
?x.move(a) ? to(a, ?(?x.end(x, ?(?y.hall(y)))))
?FORWARD, FORWARD?
Figure 3: Fragment of a map and instructions for the
navigation domain. The fragment includes two inter-
secting hallways (red and blue), two chairs and an agent
facing left (green pentagon), which follows instructions
such as these listed below. Each instruction is paired
with a logical form representing its meaning and its ex-
ecution in the map.
inspired by Clark and Curran (2007).
Let GEN(x, s; ?) ? Y be the set of all possi-
ble CCG parses given the sentence x, the current
state s and the lexicon ?. In GEN(x, s; ?), multi-
ple parse trees may have the same logical form;
let Y(z) ? GEN(x, s; ?) be the subset of such
parses with the logical form z at the root. Also,
let ? ? R
d
be a d-dimensional parameter vector.
We define the probability of the logical form z as:
p(z|x, s; ?,?) =
?
y?Y(z)
p(y|x, s; ?,?) (1)
Above, we marginalize out the probabilities of all
parse trees with the same logical form z at the root.
The probability of a parse tree y is defined as:
p(y|x, s; ?,?) =
e
???(x,s,y)
?
y
?
?GEN(x,s;?)
e
???(x,s,y
?
)
(2)
Where ?(x, s, y) ? R
d
is a feature vector. Given
a logical form z, we deterministically map it to an
execution e ? E . At inference time, given a sen-
tence x and state s, we find the best logical form
z
?
(and its corresponding execution) by solving:
z
?
= arg max
z
p(z|x, s; ?,?) (3)
1275
The above arg max operation sums over all trees
y ? Y(z), as described in Equation 1. We use a
CKY chart for this computation. The chart signa-
ture in each span is a CCG category. Since ex-
act inference is prohibitively expensive, we fol-
low previous work and perform bottom-up beam
search, maintaining only the k-best categories for
each span in the chart. The logical form z
?
is taken
from the k-best categories at the root of the chart.
The partition function in Equation 2 is approxi-
mated by summing the inside scores of all cate-
gories at the root. We describe the choices of hy-
perparameters and details of our feature set in ?5.
3 Learning
Learning a CCG semantic parser requires inducing
the entries of the lexicon ? and estimating pars-
ing parameters ?. We describe a batch learning
algorithm (Figure 4), which explicitly attempts to
induce a compact lexicon, while fully explaining
the training data. At training time, we assume ac-
cess to a set of N examples D =
{
d
(i)
}
N
1
, where
each datapoint d
(i)
= ?x
(i)
, s
(i)
, e
(i)
?, consists of
an instruction x
(i)
, the state s
(i)
where the instruc-
tion is issued and its execution demonstration e
(i)
.
In particular, we know the correct execution for
each state and instruction, but we do not know the
correct CCG parse and logical form. We treat the
choices that determine them, including selection
of lexical entries and parsing operators, as latent.
Since there can be many logical forms z ? Z that
yield the same execution e
(i)
, we marginalize over
the logical forms (using Equation 1) when maxi-
mizing the following regularized log-likelihood:
L (?,?,D) = (4)
?
d
(i)
?D
?
z?Z(e
(i)
)
p(z|x
(i)
, s
(i)
; ?,?)?
?
2
???
2
2
WhereZ(e
(i)
) is the set of logical forms that result
in the execution e
(i)
and the hyperparameter ? is
a regularization constant. Due to the large number
of potential combinations,
1
it is impractical to con-
sider the complete set of lexical entries, where all
strings (single words and n-grams) are associated
with all possible CCG categories. Therefore, simi-
lar to prior work, we gradually expand the lexicon
during learning. As a result, the parameter space
1
For the navigation task, given the set of CCG category
templates (see ?2.1) and parameters used there would be be-
tween 7.5-10.2M lexical entries to consider, depending on the
corpus used (?5).
Algorithm 1 Batch algorithm for maximizing L (?,?,D).
See ?3.1 for details.
Input: Training dataset D =
{
d
(i)
}
N
1
, number of learning
iterations T , seed lexicon ?
0
, a regularization constant
?, and a learning rate ?. VOTE is defined in ?4.
Output: Lexicon ? and model parameters ?
1: ?? ?
0
2: for t = 1 to T do
?
Generate lexical entries for all datapoints.
3: for i = 1 to N do
4: ?
(i)
? GENENTRIES(d
(i)
, ?,?)
?
Add corpus-wide voted entries to model lexicon.
5: ?? ? ? VOTE(?, {?
(1)
, . . . , ?
(N)
})
?
Compute gradient and entries to prune.
6: for i = 1 to N do
7: ??
(i)
?
,?
(i)
? ? COMPUTEUPDATE(d
(i)
, ?,?)
?
Prune lexicon.
8: ?? ? \
N?
i=1
?
(i)
?
?
Update model parameters.
9: ? ? ? + ?
N?
i=1
?
(i)
? ??
10: return ? and ?
Algorithm 2 GENENTRIES: Algorithm to generate lexical
entries from one training datapoint. See ?3.2 for details.
Input: Single datapoint d = ?x, s, e?, current model param-
eters ? and lexicon ?.
Output: Datapoint-specific lexicon entries ?.
?
Augment lexicon with sentence-specific entries.
1: ?
+
? ? ? GENLEX(d,?, ?)
?
Get max-scoring parses producing correct execution.
2: y
+
? GENMAX(x, s, e; ?
+
, ?)
?
Extract lexicon entries from max-scoring parses.
3: ??
?
y?y
+
LEX(y)
4: return ?
Algorithm 3 COMPUTEUPDATE: Algorithm to compute the
gradient and the set of lexical entries to prune for one data-
point. See ?3.3 for details.
Input: Single datapoint d = ?x, s, e?, current model param-
eters ? and lexicon ?.
Output: ??
?
,??, lexical entries to prune for d and gradient.
?
Get max-scoring correct parses given ? and ?.
1: y
+
? GENMAX(x, s, e; ?, ?)
?
Create the set of entries to prune.
2: ?
?
? ? \
?
y?y
+
LEX(y)
?
Compute gradient.
3: ?? E(y | x, s, e; ?,?)? E(y | x, s; ?,?)
4: return ??
?
,??
Figure 4: Our learning algorithm and its subroutines.
changes throughout training whenever the lexicon
is modified. The learning problem involves jointly
finding the best set of parameters and lexicon en-
tries. In the remainder of this section, we describe
how we optimize Equation 4, while explicitly con-
trolling the lexicon size.
1276
3.1 Optimization Algorithm
We present a learning algorithm to optimize the
data log-likelihood, where both lexicon learning
and parameter updates are performed in batch, i.e.,
after observing all the training corpus. The batch
formulation enables us to use information from the
entire training set when updating the model lexi-
con. Algorithm 1 presents the outline of our op-
timization procedure. It takes as input a training
dataset D, number of iterations T , seed lexicon
?
0
, learning rate ? and regularization constant ?.
Learning starts with initializing the model lex-
icon ? using ?
0
(line 1). In lines 2-9, we run T
iterations; in each, we make two passes over the
corpus, first to generate lexical entries, and second
to compute gradient updates and lexical entries to
prune. To generate lexical entries (lines 3-4) we
use the subroutine GENENTRIES to independently
generate entries for each datapoint, as described
in ?3.2. Given the entries for each datapoint, we
vote on which to add to the model lexicon. The
subroutine VOTE (line 5) chooses a subset of the
proposed entries using a particular voting strategy
(see ?4). Given the updated lexicon, we process
the corpus a second time (lines 6-7). The sub-
routine COMPUTEUPDATE, as described in ?3.3,
computes the gradient update for each datapoint
d
(i)
, and also generates the set of lexical entries not
included in the max-scoring parses of d
(i)
, which
are candidates for pruning. We prune from the
model lexicon all lexical entries not used in any
correct parse (line 8). During this pruning step, we
ensure that no entries from ?
0
are removed from
?. Finally, the gradient updates are accumulated
to update the model parameters (line 9).
3.2 Lexical Entries Generation
For each datapoint d = ?x, s, e?, the subroutine
GENENTRIES, as described in Algorithm 2, gen-
erates a set of potential entries. The subroutine
uses the function GENLEX, originally proposed
by Zettlemoyer and Collins (2005), to generate
lexical entries from sentences paired with logical
forms. We use the weakly-supervised variant of
Artzi and Zettlemoyer (2013b). Briefly, GENLEX
uses the sentence and expected execution to gen-
erate new lexemes, which are then paired with a
set of templates factored from ?
0
to generate new
lexical entries. For more details, see ?8 of Artzi
and Zettlemoyer (2013b).
Since GENLEX over-generates entries, we need
to determine the set of entries that participate
in max-scoring parses that lead to the correct
execution e. We therefore create a sentence-
specific lexicon ?
+
by taking the union of the
GENLEX-generated entries for the current sen-
tence and the model lexicon (line 1). We define
GENMAX(x, s, e; ?
+
, ?) to be the set of all max-
scoring parses according to the parameters ? that
are in GEN(x, s; ?
+
) and result in the correct ex-
ecution e (line 2). In line 3 we use the function
LEX(y), which returns the lexical entries used in
the parse y, to compute the set of all lexical en-
tries used in these parses. This final set contains
all newly generated entries for this datapoint and
is returned to the optimization algorithm.
3.3 Pruning and Gradient Computation
Algorithm 3 describes the subroutine COMPUTE-
UPDATE that, given a datapoint d, the current
model lexicon ? and model parameters ?, returns
the gradient update and the set of lexical entries
to prune for d. First, similar to GENENTRIES we
compute the set of correct max-scoring parses us-
ing GENMAX (line 1). This time, however, we do
not use a sentence-specific lexicon, but instead use
the model lexicon that has been expanded with all
voted entries. As a result, the set of max-scoring
parses producing the correct execution may be
different compared to GENENTRIES. LEX(y) is
then used to extract the lexical entries from these
parses, and the set difference (?
?
) between the
model lexicon and these entries is set to be pruned
(line 2). Finally, the partial derivative for the data-
point is computed using the difference of two ex-
pected feature vectors, according to two distribu-
tions (line 3): (a) parses conditioned on the correct
execution e, the sentence x, state s and the model,
and (b) all parses not conditioned on the execution
e. The derivatives are approximate due to the use
of beam search, as described in ?2.2.
4 Global Voting for Lexicon Learning
Our goal is to learn compact and accurate CCG
lexicons. To this end, we globally reason about
adding new entries to the lexicon by voting (VOTE,
Algorithm 1, line 5), and remove entries by prun-
ing the ones no longer required for explaining the
training data (Algorithm 1, line 8). In voting, each
datapoint can be considered as attempting to in-
fluence the learning algorithm to update the model
lexicon with the entries required to parse it. In this
1277
Round 1 Round 2 Round 3 Round 4
d
(1)
?chair, {chair}?
?chair, {hatrack}?
?chair, {turn,direction}?
1
/3
1
/3
1
/3
?chair, {chair}?
?chair, {hatrack}?
1
/2
1
/2
?chair, {chair}? 1 ?chair, {chair}? 1
d
(2)
?chair, {chair}?
?chair, {hatrack}?
1
/2
1
/2
?chair, {chair}?
?chair, {hatrack}?
1
/2
1
/2
?chair, {chair}? 1 ?chair, {chair}? 1
d
(3)
?chair, {chair}?
?chair, {easel}?
1
/2
1
/2
?chair, {chair}?
?chair, {easel}?
1
/2
1
/2
?chair, {chair}?
?chair, {easel}?
1
/2
1
/2
?chair, {chair}? 1
d
(4)
?chair, {easel}? 1 ?chair, {easel}? 1 ?chair, {easel}? 1 ?chair, {easel}? 1
Votes
?chair, {chair}?
?chair, {easel}?
?chair, {hatrack}?
?chair, {turn,direction}?
1
1
/3
1
1
/2
5
/6
1
/3
?chair, {chair}?
?chair, {easel}?
?chair, {hatrack}?
1
1
/2
1
1
/2
1
?chair, {chair}?
?chair, {easel}?
2
1
/2
1
1
/2
?chair, {chair}?
?chair, {easel}?
3
1
Discard ?chair, {turn, direction}? ?chair, {hatrack}? ?chair, {easel}?
Figure 5: Four rounds of CONSENSUSVOTE for the string chair for four training datapoints. For each datapoint,
we specify the set of lexemes generated in the Round 1 column, and update this set after each round. At the end,
the highest voted new lexeme according to the final votes is returned. In this example, MAXVOTE and CONSEN-
SUSVOTE lead to different outcomes. MAXVOTE, based on the initial sets only, will select ?chair, {easel}?.
section we describe two alternative voting strate-
gies. Both strategies ensure that new entries are
only added when they have wide support in the
training data, but count this support in different
ways. For reproducibility, we also provide step-
by-step pseudocode for both methods in the sup-
plementary material.
Since we only have access to executions and
treat parse trees as latent, we consider as correct
all parses that produce correct executions. Fre-
quently, however, incorrect parses spuriously lead
to correct executions. Lexical entries extracted
from such spurious parses generalize poorly. The
goal of voting is to eliminate such entries.
Voting is formulated on the factored lexicon
representation, where each lexical entry is factored
into a lexeme and a template, as described in ?2.1.
Each lexeme is a pair containing a natural lan-
guage string and a set of logical constants.
2
A lex-
eme is combined with a template to create a lexical
entry. In our lexicon learning approach only new
lexemes are generated, while the set of templates
is fixed; hence, our voting strategies reason over
lexemes and only create complete lexicon entries
at the end. Decisions are made for each string in-
dependently of all other strings, but considering all
occurrences of that string in the training data.
In lines 3-4 of Algorithm 1 GENENTRIES is
used to propose new lexical entries for each train-
ing datapoint d
(i)
. For each d
(i)
a set ?
(i)
, that
includes all lexical entries participating in parses
that lead to the correct execution, is generated. In
these sets, the same string can appear in multiple
2
Recall, for example, that in one lexeme the string walk
may be paired with the set of constants {move, direction}.
lexemes. To normalize its influence, each data-
point is given a vote of 1.0 for each string, which
is distributed uniformly among all lexemes con-
taining the same string.
For example, a specific ?
(i)
may consist of
the following three lexemes: ?chair, {chair}?,
?chair, {hatrack}?, ?face, {post, front, you}?. In
this set, the phrase chair has two possible mean-
ings, which will therefore each receive a vote of
0.5, while the third lexeme will be given a vote of
1.0. Such ambiguity is common and occurs when
the available supervision is insufficient to discrim-
inate between different parses, for example, if they
lead to identical executions.
Each of the two following strategies reasons
over these votes to globally select the best lex-
emes. To avoid polluting the model lexicon, both
strategies adopt a conservative approach and only
select at most one lexeme for each string in each
training iteration.
4.1 Strategy 1: MAXVOTE
The first strategy for selecting voted lexical entries
is straightforward. For each string it simply aggre-
gates all votes and selects the new lexeme with the
most votes. A lexeme is considered new if it is
not already in the model lexicon. If no such sin-
gle lexeme exists (e.g., no new entries were used
in correctly executing parses or in the case of a tie)
no lexeme is selected in this iteration.
A potential limitation of MAXVOTE is that the
votes for all rejected lexemes are lost. However,
it is often reasonable to re-allocate these votes to
other lexemes. For example, consider the sets of
lexemes for the word chair in the Round 1 col-
1278
umn of Figure 5. Using MAXVOTE on these sets
will select the lexeme ?chair, {easel}?, rather than
the correct lexeme ?chair, {chair}?. This occurs
when the datapoints supporting the correct lexeme
distribute their votes over many spurious lexemes.
4.2 Strategy 2: CONSENSUSVOTE
Our second strategy CONSENSUSVOTE aims to
capture the votes that are lost in MAXVOTE. In-
stead of discarding votes that do not go to the max-
imum scoring lexeme, voting is done in several
rounds. In each round the lowest scoring lexeme
is discarded and votes are re-assigned uniformly
to the remaining lexemes. This procedure is con-
tinued until convergence. Finally, given the sets of
lexemes in the last round, the votes are computed
and the new lexeme with most votes is selected.
Figure 5 shows a complete voting process for
four training datapoints. In each round, votes
are aggregated over the four sets of lexemes, and
the lexeme with the fewest votes is discarded.
For each set of lexemes, the discarded lexeme
is removed, unless it will lead to an empty set.
3
In the example, while ?chair, {easel}? is dis-
carded in Round 3, it remains in the set of d
(4)
.
The process converges in the fourth round, when
there are no more lexemes to discard. The fi-
nal sets include two entries: ?chair, {chair}? and
?chair, {easel}?. By avoiding wasting votes on
lexemes that have no chance of being selected, the
more widely supported lexeme ?chair, {chair}?
receives the most votes, in contrast to Round 1,
where ?chair, {easel}? was the highest voted one.
5 Experimental Setup
To isolate the effect of our lexicon learning tech-
niques we closely follow the experimental setup of
previous work (Artzi and Zettlemoyer, 2013b, ?9)
and use its publicly available code.
4
This includes
the provided beam-search CKY parser, two-pass
parsing for testing, beam search for executing se-
quences of instructions and the same seed lexicon,
weight initialization and features. Finally, except
3
This restriction is meant to ensure that discarding lex-
emes will not change the set of sentences that can be parsed.
In addition, it means that the total amount of votes given to a
string is invariant between rounds. Allowing for empty sets
will change the sum of votes, and therefore decrease the num-
ber of datapoints contributing to the decision.
4
Their implementation, based on the University of Wash-
ington Semantic Parsing Framework (Artzi and Zettlemoyer,
2013a), is available at http://yoavartzi.com/navi.
the optimization parameters specified below, we
use the same parameter settings.
Data For evaluation we use two related cor-
pora: SAIL (Chen and Mooney, 2011) and ORA-
CLE (Artzi and Zettlemoyer, 2013b). Due to how
the original data was collected (MacMahon et al.,
2006), SAIL includes many wrong executions and
about 30% of all instruction sequences are infeasi-
ble (e.g., instructing the agent to walk into a wall).
To better understand system performance and the
effect of noise, ORACLE was created with the
subset of valid instructions from SAIL paired with
their gold executions. Following previous work,
we use a held-out set for the ORACLE corpus and
cross-validation for the SAIL corpus.
Systems We report two baselines. Our batch
baseline uses the same regularized algorithm, but
updates the lexicon by adding all entries without
voting and skips pruning. Additionally, we added
post-hoc pruning to the algorithm of Artzi and
Zettlemoyer (2013b) by discarding all learned en-
tries that are not participating in max-scoring cor-
rect parses at the end of training. For ablation,
we study the influence of the two voting strategies
and pruning, while keeping the same regulariza-
tion setting. Finally, we compare our approach to
previous published results on both corpora.
Optimization Parameters We optimized the
learning parameters using cross validation on the
training data to maximize recall of complete se-
quence execution and minimize lexicon size. We
use 10 training iterations and the learning rate
? = 0.1. For SAIL we set the regularization pa-
rameter ? = 1.0 and for ORACLE ? = 0.5.
Full Sequence Inference To execute sequences
of instructions we use the beam search procedure
of Artzi and Zettlemoyer (2013b) with an identical
beam size of 10. The beam stores states, and is
initialized with the starting state. Instructions are
executed in order, each is attempted from all states
currently in the beam, the beam is then updated
and pruned to keep the 10-best states. At the end,
the best scoring state in the beam is returned.
Evaluation Metrics We evaluate the end-to-end
task of executing complete sequences of instruc-
tions against an oracle final state. In addition, to
better understand the results, we also measure task
completion for single instructions. We repeated
1279
ORACLE corpus cross-validation
Single sentence Sequence Lexicon
P R F1 P R F1 size
Artzi and Zettlemoyer (2013b) 84.59 82.74 83.65 68.35 58.95 63.26 5383
w/ post-hoc pruning 84.32 82.89 83.60 66.83 61.23 63.88 3104
Batch baseline 85.14 81.91 83.52 72.64 60.13 65.76 6323
w/ MAXVOTE 84.04 82.25 83.14 72.79 64.86 68.55 2588
w/ CONSENSUSVOTE 84.51 82.23 83.36 72.99 63.45 67.84 2446
w/ pruning 85.58 83.51 84.53 75.15 65.97 70.19 2791
w/ MAXVOTE + pruning 84.50 82.89 83.69 72.91 66.40 69.47 2186
w/ CONSENSUSVOTE + pruning 85.22 83.00 84.10 75.65 66.15 70.55 2101
Table 1: Ablation study using cross-validation on the ORACLE corpus training data. We report mean precision
(P), recall (R) and harmonic mean (F1) of execution accuracy on single sentences and sequences of instructions
and mean lexicon sizes. Bold numbers represent the best performing method on a given metric.
Final results
Single sentence Sequence Lexicon
P R F1 P R F1 size
SAIL
Chen and Mooney (2011) 54.40 16.18
Chen (2012) 57.28 19.18
+ additional data 57.62 20.64
Kim and Mooney (2012) 57.22 20.17
Kim and Mooney (2013) 62.81 26.57
Artzi and Zettlemoyer (2013b) 67.60 65.28 66.42 38.06 31.93 34.72 10051
Our Approach 66.67 64.36 65.49 41.30 35.44 38.14 2873
ORACLE
Artzi and Zettlemoyer (2013b) 81.17 (0.68) 78.63 (0.84) 79.88 (0.76) 68.07 (2.72) 58.05 (3.12) 62.65 (2.91) 6213 (217)
Our Approach 79.86 (0.50) 77.87 (0.41) 78.85 (0.45) 76.05 (1.79) 68.53 (1.76) 72.10 (1.77) 2365 (57)
Table 2: Our final results compared to previous work on the SAIL and ORACLE corpora. We report mean precision
(P), recall (R), harmonic mean (F1) and lexicon size results and standard deviation between runs (in parenthesis)
when appropriate. Our Approach stands for batch learning with a consensus voting and pruning. Bold numbers
represent the best performing method on a given metric.
each experiment five times and report mean preci-
sion, recall,
5
harmonic mean (F1) and lexicon size.
For held-out test results we also report standard
deviation. For the baseline online experiments we
shuffled the training data between runs.
6 Results
Table 1 shows ablation results for 5-fold cross-
validation on the ORACLE training data. We
evaluate against the online learning algorithm of
Artzi and Zettlemoyer (2013b), an extension of it
to include post-hoc pruning and a batch baseline.
Our best sequence execution development result
is obtained with CONSENSUSVOTE and pruning.
The results provide a few insights. First, sim-
ply switching to batch learning provides mixed re-
sults: precision increases, but recall drops and the
learned lexicon is larger. Second, adding pruning
results in a much smaller lexicon, and, especially
in batch learning, boosts performance. Adding
voting further reduces the lexicon size and pro-
vides additional gains for sequence execution. Fi-
nally, while MAXVOTE and CONSENSUSVOTE
give comparable performance on their own, CON-
SENSUSVOTE results in more precise and compact
5
Recall is identical to accuracy as reported in prior work.
models when combined with pruning.
Table 2 lists our test results. We significantly
outperform previous state of the art on both cor-
pora when evaluating sequence accuracy. In both
scenarios our lexicon is 60-70% smaller. In con-
trast to the development results, single sentence
performance decreases slightly compared to Artzi
and Zettlemoyer (2013b). The discrepancy be-
tween single sentence and sequence results might
be due to the beam search performed when execut-
ing sequences of instructions. Models with more
compact lexicons generate fewer logical forms for
each sentence: we see a decrease of roughly 40%
in our models compared to Artzi and Zettlemoyer
(2013b). This is especially helpful during se-
quence execution, where we use a beam size of
10, resulting in better sequences of executions. In
general, this shows the potential benefit of using
more compact models in scenarios that incorpo-
rate reasoning about parsing uncertainty.
To illustrate the types of errors avoided with
voting and pruning, Table 3 describes common
error classes and shows example lexical entries
for batch trained models with CONSENSUSVOTE
and pruning and without. Quantitatively, the mean
number of entries per string on development folds
1280
String
# lexical entries
Example categoriesBatch With voting
baseline and pruning
The algorithm often treats common bigrams as multiword phrases, and later learns the more general separate entries.
Without pruning the initial entries remain in the lexicon and compete with the correct ones during inference.
octagon carpet 45 0 N : ?x.wall(x) N : ?x.hall(x)
N : ?x.honeycomb(x)
carpet 51 5 N : ?x.hall(x)
N/N : ?f.?x.x == argmin(f, ?y.dist(y))
octagon 21 5 N : ?x.honeycomb(x) N : ?x.cement(x)
ADJ : ?x.honeycomb(x)
We commonly see in the lexicon a long tail of erroneous entries, which compete with correctly learned ones. With voting
and pruning we are often able to avoid such noisy entries. However, some noise still exists, e.g., the entry for ?intersection?.
intersection 45 7 N : ?x.intersection(x) S\N : ?f.intersect(you, (f))
AP : ?a.len(a, 1) N/NP : ?x.?y.intersect(y, x)
twice 46 2 AP : ?a.len(a, 2) AP : ?a.pass(a,A(?x.empty(x)))
AP : ?a.pass(a,A(?x.hall(x)))
stone 31 5 ADJ : ?x.stone(x) ADJ : ?x.brick(x)
ADJ : ?x.honeycomb(x) NP/N : ?f.A(f)
Not all concepts mentioned in the corpus are relevant to the task and some of these are not semantically modeled. However,
the baseline learner doesn?t make this distinction and induces many erroneous entries. With voting the model better handles
such cases, either by pairing such words with semantically empty entries or learning no entries for them. During inference
the system can then easily skip such words.
now 28 0 AP : ?a.len(a, 3) AP : ?a.direction(a, forward)
only 38 0 N/NP : ?x.?y.intersect(y, x)
N/NP : ?x.?y.front(y, x)
here 31 8 NP : you S/S : ?x.x
S\N : ?f.intersect(you,A(f))
Without pruning the learner often over-splits multiword phrases and has no way to reverse such decisions.
coat 25 0 N : ?x.intersection(x) ADJ : ?x.hatrack(x)
rack 45 0 N : ?x.hatrack(x) N : ?x.furniture(x)
coat rack 55 5 N : ?x.hatrack(x) N : ?x.wall(x)
N : ?x.furniture(x)
Voting helps to avoid learning entries for rare words when the learning signal is highly ambiguous.
orange 20 0 N : ?x.cement(x) N : ?x.grass(x)
pics of towers 26 0 N?x.intersection(x) N : ?x.hall(x)
Table 3: Example entries from a learned ORACLE corpus lexicon using batch learning. For each string we
report the number of lexical entries without voting (CONSENSUSVOTE) and pruning and with, and provide a few
examples. Struck entries were successfully avoided when using voting and pruning.
decreases from 16.77 for online training to 8.11.
Finally, the total computational cost of our ap-
proach is roughly equivalent to online approaches.
In both approaches, each pass over the data makes
the same number of inference calls, and in prac-
tice, Artzi and Zettlemoyer (2013b) used 6-8 it-
erations for online learning while we used 10. A
benefit of the batch method is its insensitivity to
data ordering, as expressed by the lower standard
deviation between randomized runs in Table 2.
6
7 Related Work
There has been significant work on learning for se-
mantic parsing. The majority of approaches treat
grammar induction and parameter estimation sep-
arately, e.g. Wong and Mooney (2006), Kate and
Mooney (2006), Clarke et al. (2010), Goldwasser
et al. (2011), Goldwasser and Roth (2011), Liang
6
Results still vary slightly due to multi-threading.
et al. (2011), Chen and Mooney (2011), and Chen
(2012). In all these approaches the grammar struc-
ture is fixed prior to parameter estimation.
Zettlemoyer and Collins (2005) proposed the
learning regime most related to ours. Their learner
alternates between batch lexical induction and on-
line parameter estimation. Our learning algo-
rithm design combines aspects of previously stud-
ied approaches into a batch method, including
gradient updates (Kwiatkowski et al., 2010) and
using weak supervision (Artzi and Zettlemoyer,
2011). In contrast, Artzi and Zettlemoyer (2013b)
use online perceptron-style updates to optimize a
margin-based loss. Our work also focuses on CCG
lexicon induction but differs in the use of corpus-
level statistics through voting and pruning for ex-
plicitly controlling the size of the lexicon.
Our approach is also related to the grammar in-
duction algorithm introduced by Carroll and Char-
1281
niak (1992). Similar to our method, they process
the data using two batch steps: the first proposes
grammar rules, analogous to our step that gener-
ates lexical entries, and the second estimates pars-
ing parameters. Both methods use pruning after
each iteration, to remove unused entries in our ap-
proach, and low probability rules in theirs. How-
ever, while we use global voting to add entries
to the lexicon, they simply introduce all the rules
generated by the first step. Their approach also
relies on using disjoint subsets of the data for the
two steps, while we use the entire corpus.
Using voting to aggregate evidence has been
studied for combining decisions from an ensem-
ble of classifiers (Ho et al., 1994; Van Erp and
Schomaker, 2000). MAXVOTE is related to ap-
proval voting (Brams and Fishburn, 1978), where
voters are required to mark if they approve each
candidate or not. CONSENSUSVOTE combines
ideas from approval voting, Borda counting, and
instant-runoff voting. Van Hasselt (2011) de-
scribed all three systems and applied them to pol-
icy summation in reinforcement learning.
8 Conclusion
We considered the problem of learning for se-
mantic parsing, and presented voting and pruning
methods based on corpus-level statistics for induc-
ing compact CCG lexicons. We incorporated these
techniques into a batch modification of an exist-
ing learning approach for joint lexicon induction
and parameter estimation. Our evaluation demon-
strates that both voting and pruning contribute to-
wards learning a compact lexicon and illustrates
the effect of lexicon quality on task performance.
In the future, we wish to study various aspects
of learning more robust lexicons. For example, in
our current approach, words not appearing in the
training set are treated as unknown and ignored at
inference time. We would like to study the bene-
fit of using large amounts of unlabeled text to al-
low the model to better hypothesize the meaning
of such previously unseen words. Moreover, our
model?s performance is currently sensitive to the
set of seed lexical templates provided. While we
are able to learn the meaning of new words, the
model is unable to correctly handle syntactic and
semantic structures not covered by the seed tem-
plates. To alleviate this problem, we intend to fur-
ther explore learning novel lexical templates.
Acknowledgements
We thank Kuzman Ganchev, Emily Pitler, Luke
Zettlemoyer, Tom Kwiatkowski and Nicholas
FitzGerald for their comments on earlier drafts,
and the anonymous reviewers for their valuable
feedback. We also wish to thank Ryan McDon-
ald and Arturas Rozenas for their valuable input
about voting procedures.
References
Yoav Artzi and Luke S. Zettlemoyer. 2011. Bootstrap-
ping semantic parsers from conversations. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Yoav Artzi and Luke S. Zettlemoyer. 2013a. UW
SPF: The University of Washington Semantic Pars-
ing Framework.
Yoav Artzi and Luke S. Zettlemoyer. 2013b. Weakly
supervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Steven J. Brams and Peter C. Fishburn. 1978. Ap-
proval voting. The American Political Science Re-
view, pages 831?847.
Qingqing Cai and Alexander Yates. 2013. Seman-
tic parsing freebase: Towards open-domain semantic
parsing. In Proceedings of the Joint Conference on
Lexical and Computational Semantics.
Gelnn Carroll and Eugene Charniak. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. Working Notes of the Workshop
Statistically-Based NLP Techniques.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the Na-
tional Conference on Artificial Intelligence.
David L. Chen. 2012. Fast online lexicon learning for
grounded language acquisition. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world?s response. In Proceedings of the Conference
on Computational Natural Language Learning.
Dan Goldwasser and Dan Roth. 2011. Learning from
natural instructions. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence.
1282
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proceedings of the Association
of Computational Linguistics.
Tin K. Ho, Jonathan J. Hull, and Sargur N. Srihari.
1994. Decision combination in multiple classifier
systems. IEEE Transactions on Pattern Analysis
and Machine Intelligence, pages 66?75.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the Conference of the Association for
Computational Linguistics.
Joohyun Kim and Raymond J. Mooney. 2012. Un-
supervised pcfg induction for grounded language
learning with highly ambiguous supervision. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Joohyun Kim and Raymond J. Mooney. 2013. Adapt-
ing discriminative reranking to grounded language
learning. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Nate Kushman and Regina Barzilay. 2013. Using se-
mantic unification to generate regular expressions
from natural language. In Proceedings of the Hu-
man Language Technology Conference of the North
American Association for Computational Linguis-
tics.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing prob-
abilistic CCG grammars from logical form with
higher-order unification. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2011. Lexical Gener-
alization in CCG Grammar Induction for Semantic
Parsing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics,
1(1):179?192.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the Conference of the As-
sociation for Computational Linguistics.
Matt MacMahon, Brian Stankiewics, and Benjamin
Kuipers. 2006. Walk the talk: Connecting language,
knowledge, action in route instructions. In Proceed-
ings of the National Conference on Artificial Intelli-
gence.
Cynthia Matuszek, Nicholas FitzGerald, Luke S.
Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A
joint model of language and perception for grounded
attribute learning. In Proceedings of the Interna-
tional Conference on Machine Learning.
Mark Steedman. 1996. Surface Structure and Inter-
pretation. The MIT Press.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press.
Merijn Van Erp and Lambert Schomaker. 2000.
Variants of the borda count method for combining
ranked classifier hypotheses. In In the International
Workshop on Frontiers in Handwriting Recognition.
Hado Van Hasselt. 2011. Insights in Reinforcement
Learning: formal analysis and empirical evaluation
of temporal-difference learning algorithms. Ph.D.
thesis, University of Utrecht.
Yuk W. Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the Human Language
Technology Conference of the North American Asso-
ciation for Computational Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Conference on Un-
certainty in Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning.
1283
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 19?27,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Products of Random Latent Variable Grammars
Slav Petrov
Google Research
New York, NY, 10011
slav@google.com
Abstract
We show that the automatically induced latent
variable grammars of Petrov et al (2006) vary
widely in their underlying representations, de-
pending on their EM initialization point. We
use this to our advantage, combining multiple
automatically learned grammars into an un-
weighted product model, which gives signif-
icantly improved performance over state-of-
the-art individual grammars. In our model,
the probability of a constituent is estimated as
a product of posteriors obtained from multi-
ple grammars that differ only in the random
seed used for initialization, without any learn-
ing or tuning of combination weights. Despite
its simplicity, a product of eight automatically
learned grammars improves parsing accuracy
from 90.2% to 91.8% on English, and from
80.3% to 84.5% on German.
1 Introduction
Learning a context-free grammar for parsing re-
quires the estimation of a more highly articulated
model than the one embodied by the observed tree-
bank. This is because the naive treebank grammar
(Charniak, 1996) is too permissive, making unreal-
istic context-freedom assumptions. For example, it
postulates that there is only one type of noun phrase
(NP), which can appear in all positions (subject, ob-
ject, etc.), regardless of case, number or gender. As
a result, the grammar can generate millions of (in-
correct) parse trees for a given sentence, and has a
flat posterior distribution. High accuracy grammars
therefore add soft constraints on the way categories
can be combined, and enrich the label set with addi-
tional information. These constraints can be lexical-
ized (Collins, 1999; Charniak, 2000), unlexicalized
(Johnson, 1998; Klein and Manning, 2003b) or au-
tomatically learned (Matsuzaki et al, 2005; Petrov
et al, 2006). The constraints serve the purpose of
weakening the independence assumptions, and re-
duce the number of possible (but incorrect) parses.
Here, we focus on the latent variable approach of
Petrov et al (2006), where an Expectation Maxi-
mization (EM) algorithm is used to induce a hier-
archy of increasingly more refined grammars. Each
round of refinement introduces new constraints on
how constituents can be combined, which in turn
leads to a higher parsing accuracy. However, EM is a
local method, and there are no guarantees that it will
find the same grammars when initialized from dif-
ferent starting points. In fact, it turns out that even
though the final performance of these grammars is
consistently high, there are significant variations in
the learned refinements.
We use these variations to our advantage, and
treat grammars learned from different random seeds
as independent and equipotent experts. We use a
product distribution for joint prediction, which gives
more peaked posteriors than a sum, and enforces all
constraints of the individual grammars, without the
need to tune mixing weights. It should be noted here
that our focus is on improving parsing performance
using a single underlying grammar class, which is
somewhat orthogonal to the issue of parser combina-
tion, that has been studied elsewhere in the literature
(Sagae and Lavie, 2006; Fossum and Knight, 2009;
Zhang et al, 2009). In contrast to that line of work,
we also do not restrict ourselves to working with k-
best output, but work directly with a packed forest
representation of the posteriors, much in the spirit
of Huang (2008), except that we work with several
forests rather than rescoring a single one.
19
In our experimental section we give empirical an-
swers to some of the remaining theoretical ques-
tions. We address the question of averaging versus
multiplying classifier predictions, we investigate dif-
ferent ways of introducing more diversity into the
underlying grammars, and also compare combining
partial (constituent-level) and complete (tree-level)
predictions. Quite serendipitously, the simplest ap-
proaches work best in our experiments. A product
of eight latent variable grammars, learned on the
same data, and only differing in the seed used in
the random number generator that initialized EM,
improves parsing accuracy from 90.2% to 91.8%
on English, and from 80.3% to 84.5% on German.
These parsing results are even better than those ob-
tained by discriminative systems which have access
to additional non-local features (Charniak and John-
son, 2005; Huang, 2008).
2 Latent Variable Grammars
Before giving the details of our model, we briefly
review the basic properties of latent variable gram-
mars. Learning latent variable grammars consists of
two tasks: (1) determining the data representation
(the set of context-free productions to be used in the
grammar), and (2) estimating the parameters of the
model (the production probabilities). We focus on
the randomness introduced by the EM algorithm and
refer the reader to Matsuzaki et al (2005) and Petrov
et al (2006) for a more general introduction.
2.1 Split & Merge Learning
Latent variable grammars split the coarse (but ob-
served) grammar categories of a treebank into more
fine-grained (but hidden) subcategories, which are
better suited for modeling the syntax of natural
languages (e.g. NP becomes NP1 through NPk).
Accordingly, each grammar production A?BC
over observed categories A,B,C is split into a set
of productions Ax?ByCz over hidden categories
Ax,By,Cz. Computing the joint likelihood of the ob-
served parse trees T and sentences w requires sum-
ming over all derivations t over split subcategories:
?
i
P(wi, Ti) =
?
i
?
t:Ti
P(wi, t) (1)
Matsuzaki et al (2005) derive an EM algorithm
for maximizing the joint likelihood, and Petrov et
al. (2006) extend this algorithm to use a split&merge
procedure to adaptively determine the optimal num-
ber of subcategories for each observed category.
Starting from a completely markovized X-Bar gram-
mar, each category is split in two, generating eight
new productions for each original binary production.
To break symmetries, the production probabilities
are perturbed by 1% of random noise. EM is then
initialized with this starting point and used to climb
the highly non-convex objective function given in
Eq. 1. Each splitting step is followed by a merging
step, which uses a likelihood ratio test to reverse the
least useful half of the splits. Learning proceeds by
iterating between those two steps for six rounds. To
prevent overfitting, the production probabilities are
linearly smoothed by shrinking them towards their
common base category.
2.2 EM induced Randomness
While the split&merge procedure described above
is shown in Petrov et al (2006) to reduce the vari-
ance in final performance, we found after closer
examination that there are substantial differences
in the patterns learned by the grammars. Since
the initialization is not systematically biased in any
way, one can obtain different grammars by simply
changing the seed of the random number genera-
tor. We trained 16 different grammars by initial-
izing the random number generator with seed val-
ues 1 through 16, but without biasing the initial-
ization in any other way. Figure 1 shows that the
number of subcategories allocated to each observed
category varies significantly between the different
initialization points, especially for the phrasal cate-
gories. Figure 2 shows posteriors over the most fre-
quent subcategories given their base category for the
first four grammars. Clearly, EM is allocating the la-
tent variables in very different ways in each case.
As a more quantitative measure of difference,1 we
evaluated all 16 grammars on sections 22 and 24 of
the Penn Treebank. Figure 3 shows the performance
on those two sets, and reveals that there is no single
grammar that achieves the best score on both. While
the parsing accuracies are consistently high,2 there
1While cherry-picking similarities is fairly straight-forward,
it is less obvious how to quantify differences.
2Note that despite their variance, the performance is always
higher than the one of the lexicalized parser of Charniak (2000).
20
 10
 20
 30
 40
 50
 60
N
P
V
P PP
A
D
V
P
A
D
JP S
SB
A
R QP
N
N
P JJ
N
N
S
N
N RB
V
BN
V
BG V
B IN CD
V
BD V
BZ D
T
V
BP
Automatically determined number of subcategories
Figure 1: There is large variance in the number of subcat-
egories (error bars correspond to one standard deviation).
is only a weak correlation between the accuracies
on the two evaluation sets (Pearson coefficient 0.34).
This suggests that no single grammar should be pre-
ferred over the others. In previous work (Petrov et
al., 2006; Petrov and Klein, 2007) the final grammar
was chosen based on its performance on a held-out
set (section 22), and corresponds to the second best
grammar in Figure 3 (because only 8 different gram-
mars were trained).
A more detailed error analysis is given in Fig-
ure 4, where we show a breakdown of F1 scores for
selected phrasal categories in addition to the overall
F1 score and exact match (on the WSJ development
set). While grammar G2 has the highest overall F1
score, its exact match is not particularly high, and
it turns out to be the weakest at predicting quanti-
fier phrases (QP). Similarly, the performance of the
other grammars varies between the different error
measures, indicating again that no single grammar
dominates the others.
3 A Simple Product Model
It should be clear by now that simply varying the
random seed used for initialization causes EM to
discover very different latent variable grammars.
While this behavior is worrisome in general, it turns
out that we can use it to our advantage in this partic-
ular case. Recall that we are using EM to learn both,
the data representation, as well as the parameters of
the model. Our analysis showed that changing the
initialization point results in learning grammars that
vary quite significantly in the errors they make, but
have comparable overall accuracies. This suggests
that the different local maxima found by EM corre-
spond to different data representations rather than to
4%
7%
10%
1 2 3 4 5 6 7 8
NP
0%
15%
25%
1 2 3 4 5 6 7 8
PP
0%
15%
30%
1 2 3 4 5 6 7 8
IN
0%
30%
60%
1 2 3 4 5 6 7 8
DT
Figure 2: Posterior probabilities of the eight most fre-
quent hidden subcategories given their observed base cat-
egories. The four grammars (indicated by shading) are
populating the subcategories in very different ways.
suboptimal parameter estimates.
To leverage the strengths of the individual gram-
mars, we combine them in a product model. Product
models have the nice property that their Kullback-
Liebler divergence from the true distribution will
always be smaller than the average of the KL di-
vergences of the individual distributions (Hinton,
2001). Therefore, as long as no individual gram-
mar Gi is significantly worse than the others, we can
only benefit from combining multiple latent variable
grammars and searching for the tree that maximizes
P(T |w) ?
?
i
P(T |w, Gi) (2)
Here, we are making the assumption that the individ-
ual grammars are conditionally independent, which
is of course not true in theory, but holds surprisingly
well in practice. To avoid this assumption, we could
use a sum model, but we will show in Section 4.1
that the product formulation performs significantly
better. Intuitively speaking, products have the ad-
vantage that the final prediction has a high poste-
rior under all models, giving each model veto power.
This is exactly the behavior that we need in the case
of parsing, where each grammar has learned differ-
ent constraints for ruling out improbable parses.
3.1 Learning
Joint training of our product model would couple the
parameters of the individual grammars, necessitat-
ing the computation of an intractable global parti-
tion function (Brown and Hinton, 2001). Instead,
we use EM to train each grammar independently,
21
 89.5
 89.6
 89.7
 89.8
 89.9
 90
 90.1
 90.2
 90.6  90.7  90.8  90.9  91  91.1  91.2  91.3  91.4
F1
 S
co
re
 o
n 
Se
ct
io
n 
24
F1 Score on Section 22
Figure 3: Parsing accuracies for grammars learned from
different random seeds. The large variance and weak cor-
relation suggest that no single grammar is to be preferred.
but from a different, randomly chosen starting point.
To emphasize, we do not introduce any systematic
bias (but see Section 4.3 for some experiments), or
attempt to train the models to be maximally dif-
ferent (Hinton, 2002) ? we simply train a random
collection of grammars by varying the random seed
used for initialization. We found in our experiments
that the randomness provided by EM is sufficient
to achieve diversity among the individual grammars,
and gives results that are as good as more involved
training procedures. Xu and Jelinek (2004) made
a similar observation when learning random forests
for language modeling.
Our model is reminiscent of Logarithmic Opinion
Pools (Bordley, 1982) and Products of Experts (Hin-
ton, 2001).3 However, because we believe that none
of the underlying grammars should be favored, we
deliberately do not use any combination weights.
3.2 Inference
Computing the most likely parse tree is intractable
for latent variable grammars (Sima?an, 2002), and
therefore also for our product model. This is because
there are exponentially many derivations over split
subcategories that correspond to a single parse tree
over unsplit categories, and there is no dynamic pro-
gram to efficiently marginalize out the latent vari-
ables. Previous work on parse risk minimization has
addressed this problem in two different ways: by
changing the objective function, or by constraining
3As a matter of fact, Hinton (2001) mentions syntactic pars-
ing as one of the motivating examples for Products of Experts.
G1
G2
G3
G4
P
90% 91.5% 93%
F1 Score
G1
G2
G3
G4
P
40% 45% 50%
Exact Match
G1
G2
G3
G4
P
91% 93% 95%
NP
G1
G2
G3
G4
P
90% 92% 94%
VP
G1
G2
G3
G4
P
85% 88% 91%
PP
G1
G2
G3
G4
P
90% 92.5% 95%
QP
Figure 4: Breakdown of different accuracy measures for
four randomly selected grammars (G1-G4), as well as a
product model (P) that uses those four grammars. Note
that no single grammar does well on all measures, while
the product model does significantly better on all.
the search space (Goodman, 1996; Titov and Hen-
derson, 2006; Petrov and Klein, 2007).
The simplest approach is to stick to likelihood as
the objective function, but to limit the search space
to a set of high quality candidates T :
T ? = argmax
T?T
P(T |w) (3)
Because the likelihood of a given parse tree can be
computed exactly for our product model (Eq. 2), the
quality of this approximation is only limited by the
quality of the candidate list. To generate the candi-
date list, we produce k-best lists of Viterbi deriva-
tions with the efficient algorithm of Huang and Chi-
ang (2005), and erase the subcategory information
to obtain parse trees over unsplit categories. We re-
fer to this approximation as TREE-LEVEL inference,
because it considers a list of complete trees from
the underlying grammars, and selects the tree that
has the highest likelihood under the product model.
While the k-best lists are of very high quality, this is
a fairly crude and unsatisfactory way of approximat-
ing the posterior distribution of the product model,
as it does not allow the synthesis of new trees based
on tree fragments from different grammars.
An alternative is to use a tractable objective func-
tion that allows the efficient exploration of the entire
22
SINV
? S
NP
Su
ch
a
ge
n
cy
?
se
lf-
he
lp
?
bo
rr
o
w
in
g
VP
is ADJP
ADJP
u
n
a
u
th
o
ri
ze
d
a
n
d
ex
pe
n
si
ve
, ADJP
ADJP
far mo
re
ex
pe
n
si
ve
ADJP
ADVP
far mo
re
PP
th
a
n
di
re
ct
Tr
ea
su
ry
bo
rr
o
w
in
g
,
? VP
sa
id
NP
NP
Re
p.
Fo
rt
n
ey
St
a
rk
PRN
(D
.
Ca
lif.
)
NP
NP
Re
p.
Fo
rt
n
ey
St
a
rk
PRN
(D
.
Ca
lif.
)
, NP
th
e
bi
ll?
s
ch
ie
f
sp
o
n
so
r
.
ex
pe
n
si
ve
? ?
G1
-11.7 -12.4
G2
-12.9 -11.5
z
}
|
{
,
|
{
z
}
? ?
G1 G2
-68.8 -65.9 -66.7 -67.4
z
}
|
{
,
|
{
z
}
Legend: log G1-score log G2-score
Figure 5: Grammar G1 has a preference for flat structures, while grammar G2 prefers deeper hierarchical structures.
Both grammars therefore make one mistake each on their own. However, the correct parse tree (which uses a flat
ADJP in the first slot and a hierarchical NP in the second) scores highest under the product model.
search space. Petrov and Klein (2007) present such
an objective function, which maximizes the product
of expected correct productions r:
T ? = argmax
T
?
r?T
E(r|w) (4)
These expectations can be easily computed from the
inside/outside scores, similarly as in the maximum
bracket recall algorithm of Goodman (1996), or in
the variational approximation of Matsuzaki et al
(2005). We extend the algorithm to work over poste-
rior distributions from multiple grammars, by aggre-
gating their expectations into a product. In practice,
we use a packed forest representation to approxi-
mate the posterior distribution, as in Huang (2008).
We refer to this approximation as CONSTITUENT-
LEVEL, because it allows us to form new parse trees
from individual constituents.
Figure 5 illustrates a real case where the prod-
uct model was able to construct a completely correct
parse tree from two partially correct ones. In the ex-
ample, one of the underlying grammars (G1) had an
imperfect recall score, because of its preference for
flat structures (it missed an NP node in the second
part of the sentence). In contrast, the other gram-
mar (G2) favors deeper structures, and therefore in-
troduced a superfluous ADVP node. The product
model gives each underlying grammar veto power,
and picks the least controversial tree (which is the
correct one in this case). Note that a sum model al-
lows the most confident model to dominate the de-
cision, and would chose the incorrect hierarchical
ADJP construction here (as one can verify using the
provided model scores).
To make inference efficient, we can use the
same coarse-to-fine pruning techniques as Petrov
and Klein (2007). We generate a hierarchy of pro-
jected grammars for each individual grammar and
parse with each one in sequence. Because only the
very last pass requires scores from the different un-
derlying grammars, this computation can be trivially
parallelized across multiple CPUs. Additionally, the
first (X-Bar) pruning pass needs to be computed
only once because it is shared among all grammars.
Since the X-Bar pass is the bottleneck of the multi-
pass scheme (using nearly 50% of the total process-
ing time), the overhead of using a product model is
quite manageable. It would have also been possi-
ble to use A*-search for factored models (Klein and
Manning, 2003a; Sun and Tsujii, 2009), but we did
not attempt this in the present work.
4 Experiments
In our experiments, we follow the standard setups
described in Table 1, and use the EVALB tool for
computing parsing figures. Unless noted other-
wise, we use CONSTITUENT-LEVEL inference. All
our experiments are based on the publicly available
BerkeleyParser.4
4http://code.google.com/p/berkeleyparser
23
Training Set Dev. Set Test Set
ENGLISH-WSJ Sections Section 22 Section 23(Marcus et al, 1993) 2-21
ENGLISH-BROWN see 10% of 10% of the
(Francis et al 1979) ENGLISH-WSJ the data5 the data5
GERMAN Sentences Sentences Sentences
(Skut et al, 1997) 1-18,602 18,603-19,602 19,603-20,602
Table 1: Corpora and standard experimental setups.
4.1 (Weighted) Product vs. (Weighted) Sum
A great deal has been written on the topic of prod-
ucts versus sums of probability distributions for joint
prediction (Genest and Zidek, 1986; Tax et al,
2000). However, those theoretical results do not
apply directly here, because we are using multi-
ple randomly permuted models from the same class,
rather models from different classes. To shed some
light on this issue, we addressed the question em-
pirically, and combined two grammars into an un-
weighted product model, and also an unweighted
sum model. The individual grammars had parsing
accuracies (F1) of 91.2 and 90.7 respectively, and
their product (91.7) clearly outperformed their sum
(91.3). When more grammars are added, the gap
widens even further, and the trends persist indepen-
dently of whether the models use TREE-LEVEL or
CONSTITUENT-LEVEL inference. At least for the
case of unweighted combinations, the product dis-
tribution seems to be superior.
In related work, Zhang et al (2009) achieve ex-
cellent results with a weighted sum model. Using
weights learned on a held-out set and rescoring 50-
best lists from Charniak (2000) and Petrov et al
(2006), they obtain an F1 score of 91.0 (which they
further improve to 91.4 using a voting scheme). We
replicated their experiment, but used an unweighted
product of the two model scores. Using TREE-
LEVEL inference, we obtained an F1 score of 91.6,
suggesting that weighting is not so important in the
product case, as long as the classifiers are of compa-
rable quality.6 This is in line with previous work on
product models, where weighting has been impor-
tant when combining heterogenous classifiers (Hes-
kes, 1998), and less important when the classifiers
are of similar accuracy (Smith et al, 2005).
5See Gildea (2001) for the exact setup.
6The unweighted sum model, however, underperforms the
individual models with an F1 score of only 90.3.
 90.5
 91
 91.5
 92
 92.5
1 2 4 8 16
Number of grammars in product model
Parsing accuracy on the WSJ development set
Constituent-Level Inference
Tree-Level Inference
Figure 6: Adding more grammars to the product model
improves parsing accuracy, while CONSTITUENT-LEVEL
inference gives consistently better results.
4.2 Tree-Level vs. Constituent-Level Inference
Figure 6 shows that accuracy increases when more
grammars are added to the product model, but levels
off after eight grammars. The plot also compares
our two inference approximations, and shows that
CONSTITUENT-LEVEL inference results in a small
(0.2), but consistent improvement in F1 score.
A first thought might be that the improvement is
due to the limited scope of the k-best lists. How-
ever, this is not the case, as the results hold even
when the candidate set for CONSTITUENT-LEVEL
inference is constrained to trees from the k-best lists.
While the packed forrest representation can very ef-
ficiently encode an exponential set of parse trees, in
our case the k-best lists appear to be already very di-
verse because they are generated by multiple gram-
mars. Starting at 96.1 for a single latent variable
grammar, merging two 50-best lists from different
grammars gives an oracle score of 97.4, and adding
more k-best lists further improves the oracle score to
98.6 for 16 grammars. This compares favorably to
the results of Huang (2008), where the oracle score
over a pruned forest is shown to be 97.8 (compared
to 96.7 for a 50-best list).
The accuracy improvement can instead be ex-
plained by the change in the objective function. Re-
call from section Section 3.2, that CONSTITUENT-
LEVEL inference maximizes the expected number
of correct productions, while TREE-LEVEL infer-
ence maximizes tree-likelihood. It is therefore not
too surprising that the two objective functions se-
lect the same tree only 41% of the time, even when
limited to the same candidate set. Maximizing the
24
expected number of correct productions is superior
for F1 score (see the one grammar case in Figure 6).
However, as to be expected, likelihood is better for
exact match, giving a score of 47.6% vs. 46.8%.
4.3 Systematic Bias
Diversity among the underlying models is what
gives combined models their strength. One way of
increasing diversity is by modifying the feature sets
of the individual models (Baldridge and Osborne,
2008; Smith and Osborne, 2007). This approach
has the disadvantage that it reduces the performance
of the individual models, and is not directly appli-
cable for latent variable grammars because the fea-
tures are automatically learned. Alternatively, one
can introduce diversity by changing the training dis-
tribution. Bagging (Breiman, 1996) and Boosting
(Freund and Shapire, 1996) fall into this category,
but have had limited success for parsing (Hender-
son and Brill, 2000). Furthermore boosting is im-
practical here, because it requires training dozens of
grammars in sequence.
Since training a single grammar takes roughly one
day, we opted for a different, parallelizable way of
changing the training distribution. In a first exper-
iment, we divided the training set into two disjoint
sets, and trained separate grammars on each half.
These truly disjoint grammars had low F1 scores
of 89.4 and 89.6 respectively (because they were
trained on less data). Their combination unfortu-
nately also achieves only an accuracy of 90.9, which
is lower than what we get when training a single
grammar on the entire training set. In another exper-
iment, we used a cross-validation setup where indi-
vidual sections of the treebank were held out. The
resulting grammars had parsing accuracies of about
90.5, and the product model was again not able to
overcome the lower starting point, despite the poten-
tially larger diversity among the underlying gram-
mars. It appears that any systematic bias that lowers
the accuracy of the individual grammars also hurts
the final performance of the product model.
4.4 Product Distribution as Smoothing
Smith et al (2005) interpret Logarithmic Opinion
Pools (LOPs) as a smoothing technique. They
compare regularizing Conditional Random Fields
(CRFs) with Gaussian priors (Lafferty et al, 2001),
to training a set of unregularized CRFs over differ-
ent feature sets and combining them in an LOP. In
their experiments, both approaches work compara-
bly well, but their combination, an LOP of regular-
ized CRFs works best.
Not too surprisingly, we find this to be the case
here as well. The parameters of each latent vari-
able grammar are typically smoothed in a linear
fashion to prevent excessive overfitting (Petrov et
al., 2006). While all the experiments so far used
smoothed grammars, we reran the experiments also
with a set of unsmoothed grammars. The individ-
ual unsmoothed grammars have on average an 1.2%
lower accuracy. Even though our product model
is able to increase accuracy by combining multiple
grammars, the gap to the smoothed models remains
consistent. This suggests that the product model is
doing more than just smoothing. In fact, because the
product distribution is more peaked, it seems to be
doing the opposite of smoothing.
4.5 Final Results
Our final model uses an unweighted product of eight
grammars trained by initializing the random number
generator with seeds 1 through 8. Table 2 shows
our test set results (obtained with CONSTITUENT-
LEVEL inference), and compares them to related
work. There is a large body of work that has re-
ported parsing accuracies for English, and we have
grouped the different methods into categories for
better overview.
Our results on the English in-domain test set are
higher than those obtained by any single component
parser (SINGLE). The other methods quoted in Ta-
ble 2 operate over the output of one or more single
component parsers and are therefore largely orthog-
onal to our line of work. It is nonetheless exciting
to see that our product model is competitive with
the discriminative rescoring methods (RE) of Char-
niak and Johnson (2005) and Huang (2008), achiev-
ing higher F1 scores but lower exact match. These
two methods work on top of the Charniak (2000)
parser, and it would be possible to exchange that
parser with our product model. We did not attempt
this experiment, but we expect that those methods
would stack well with our model, because they use
primarily non-local features that are not available in
a context-free grammar.
25
Techniques like self-training (SELF) and system
combinations (COMBO) can further improve pars-
ing accuracies, but are also orthogonal to our work.
In particular the COMBO methods seem related to
our work, but are very different in their nature.
While we use multiple grammars in our work, all
grammars are from the same model class for us. In
contrast, those methods rely on a diverse set of in-
dividual parsers, each of which requires a signifi-
cant effort to build. Furthermore, those techniques
have largely relied on different voting schemes in the
past (Henderson and Brill, 1999; Sagae and Lavie,
2006), and only more recently have started using ac-
tual posteriors from the underlying models (Fossum
and Knight, 2009; Zhang et al, 2009). Even then,
those methods operate only over k-best lists, and we
are the first to work directly with parse forests from
multiple grammars.
It is also interesting to note that the best results
in Zhang et al (2009) are achieved by combining k-
best lists from a latent variable grammar of Petrov
et al (2006) with the self-trained reranking parser of
McClosky et al (2006). Clearly, replacing the sin-
gle latent variable grammar with a product of latent
variable grammars ought to improve performance.
The results on the other two corpora are similar.
A product of latent variable grammars very signifi-
cantly outperforms a single latent variable grammar
and sets new standards for the state-of-the-art.
We also analyzed the errors of the product mod-
els. In addition to the illustrative example in Fig-
ure 5, we computed detailed error metrics for differ-
ent phrasal categories. Figure 4 shows that a product
of four random grammars is always better than even
the best underlying grammar. The individual gram-
mars seem to learn different sets of constraints, and
the product model is able to model them all at once,
giving consistent accuracy improvements across all
metrics.
5 Conclusions
We presented a simple product model that signifi-
cantly improves parsing accuracies on different do-
mains and languages. Our model leverages multi-
ple automatically learned latent variable grammars,
which differ only in the seed of the random num-
ber generator used to initialize the EM learning al-
Ty
pe all sentences
Parser LP LR EX
ENGLISH-WSJ
This Paper 92.0 91.7 41.9
SI
N
G
LE Charniak (2000) 89.9 89.5 37.2
Petrov and Klein (2007) 90.2 90.1 36.7
Carreras et al (2008) 91.4 90.7 -
R
E Charniak et al (2005) 91.8 91.2 44.8
Huang (2008) 92.2 91.2 43.5
SE
LF Huang and Harper (2009) 91.37 91.57 39.37
McClosky et al (2006) 92.5 92.1 45.3
CO
M
B
O Sagae and Lavie (2006) 93.2 91.0 -
Fossum and Knight (2009) 93.2 91.7 -
Zhang et al (2009) 93.3 92.0 -
ENGLISH-BROWN
This Paper 86.5 86.3 35.8
SI
N
G Charniak (2000) 82.9 82.9 31.7
Petrov and Klein (2007) 83.9 83.8 29.6
R
E Charniak et al (2005) 86.1 85.2 36.8
GERMAN
This Paper 84.5 84.0 51.2
SI
N
G Petrov and Klein (2007) 80.0 80.2 42.4
Petrov and Klein (2008) 80.6 80.8 43.9
Table 2: Final test set accuracies for English and German.
gorithm. As our analysis showed, the grammars vary
widely, making very different errors. This is in part
due to the fact that EM is used not only for estimat-
ing the parameters of the grammar, but also to deter-
mine the set of context-free productions that under-
lie it. Because the resulting data representations are
largely independent, they can be easily combined in
an unweighted product model. The product model
does not require any additional training and is ca-
pable of significantly improving the state-of-the-art
in parsing accuracy. It remains to be seen if a sim-
ilar approach can be used in other cases where EM
converges to widely varying local maxima.
Acknowledgements
I would like to thank Ryan McDonald for numerous
discussions on this topic and his feedback on earlier
versions of this paper. This work also benefited from
conversations with Gideon Mann, Fernando Pereira,
Dan Klein and Mehryar Mohri.
7Note that these results are on a modified version of the tree-
bank where unary productions are removed.
26
References
J. Baldridge and M. Osborne. 2008. Active learning and
logarithmic opinion pools for HPSG parse selection.
Natural Language Engineering.
R. F. Bordley. 1982. A multiplicative formula for aggre-
gating probability assessments. Management Science.
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing.
A. Brown and G. Hinton. 2001. Products of hidden
Markov models. In AISTATS ?01.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In CoNLL ?08.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
In ACL?05.
E. Charniak. 1996. Tree-bank grammars. In AAAI ?96.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, UPenn.
V. Fossum and K. Knight. 2009. Combining constituent
parsers. In NAACL ?09.
W. N. Francis and H. Kucera. 1979. Manual of infor-
mation to accompany a standard corpus of present-day
edited American English. Technical report, Brown
University.
Y. Freund and R. E. Shapire. 1996. Experiments with a
new boosting algorithm. In ICML ?96.
C. Genest and J. V. Zidek. 1986. Combining probability
distributions: A critique and an annotated bibliogra-
phy. Statistical Science.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. EMNLP ?01.
J. Goodman. 1996. Parsing algorithms and metrics. ACL
?96.
J. Henderson and E. Brill. 1999. Exploiting diversity
in natural language processing: combining parsers. In
EMNLP ?99.
J. Henderson and E. Brill. 2000. Bagging and boosting a
treebank parser. In NAACL ?00.
T. Heskes. 1998. Selecting weighting factors in logarith-
mic opinion pools. In NIPS ?98.
G. Hinton. 2001. Products of experts. In ICANN ?01.
G. Hinton. 2002. Training products of experts by mini-
mizing contrastive divergence. Neural Computation.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
IWPT ?05.
Z. Huang and M. Harper. 2009. Self-training PCFG
grammars with latent annotations across languages. In
EMNLP ?09.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In ACL ?08.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24.
D. Klein and C. Manning. 2003a. A* parsing: fast exact
viterbi parse selection. In NAACL ?03.
D. Klein and C. Manning. 2003b. Accurate unlexicalized
parsing. In ACL ?03.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML ?01.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL ?05.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In NAACL ?06.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL ?07.
S. Petrov and D. Klein. 2008. Sparse multi-scale gram-
mars for discriminative latent variable parsing. In
EMNLP ?08.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In NAACL ?06.
K. Sima?an. 2002. Computatoinal complexity of proba-
bilistic disambiguation. Grammars.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In ANLP ?97.
A. Smith and M. Osborne. 2007. Diversity in logarith-
mic opinion pools. Lingvisticae Investigationes.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In ACL
?05.
X. Sun and J. Tsujii. 2009. Sequential labeling with la-
tent variables: An exact inference algorithm and its
efficient approximation. In EACL ?09.
D. Tax, M. Van Breukelen, R. Duin, and J. Kittler. 2000.
Combining multiple classifiers by averaging or by
multiplying? Pattern Recognition.
I. Titov and J. Henderson. 2006. Loss minimization in
parse reranking. In EMNLP ?06.
P. Xu and F. Jelinek. 2004. Random forests in language
modeling. In EMNLP ?04.
H. Zhang, M. Zhang, C. L. Tan, and H. Li. 2009. K-best
combination of syntactic parsers. In EMNLP ?09.
27
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 498?507,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Vine Pruning for Efficient Multi-Pass Dependency Parsing
Alexander M. Rush?
MIT CSAIL
Cambridge, MA 02139, USA
srush@csail.mit.edu
Slav Petrov
Google
New York, NY 10027, USA
slav@google.com
Abstract
Coarse-to-fine inference has been shown to be
a robust approximate method for improving
the efficiency of structured prediction models
while preserving their accuracy. We propose
a multi-pass coarse-to-fine architecture for de-
pendency parsing using linear-time vine prun-
ing and structured prediction cascades. Our
first-, second-, and third-order models achieve
accuracies comparable to those of their un-
pruned counterparts, while exploring only a
fraction of the search space. We observe
speed-ups of up to two orders of magnitude
compared to exhaustive search. Our pruned
third-order model is twice as fast as an un-
pruned first-order model and also compares
favorably to a state-of-the-art transition-based
parser for multiple languages.
1 Introduction
Coarse-to-fine inference has been extensively used
to speed up structured prediction models. The gen-
eral idea is simple: use a coarse model where in-
ference is cheap to prune the search space for more
complex models. In this work, we present a multi-
pass coarse-to-fine architecture for graph-based de-
pendency parsing. We start with a linear-time vine
pruning pass and build up to higher-order models,
achieving speed-ups of two orders of magnitude
while maintaining state-of-the-art accuracies.
In constituency parsing, exhaustive inference for
all but the simplest grammars tends to be pro-
hibitively slow. Consequently, most high-accuracy
constituency parsers routinely employ a coarse
grammar to prune dynamic programming chart cells
? Research conducted at Google.
of the final grammar of interest (Charniak et al,
2006; Carreras et al, 2008; Petrov, 2009). While
there are no strong theoretical guarantees for these
approaches,1 in practice one can obtain significant
speed improvements with minimal loss in accuracy.
This benefit comes primarily from reducing the large
grammar constant |G| that can dominate the runtime
of the cubic-time CKY inference algorithm. De-
pendency parsers on the other hand do not have a
multiplicative grammar factor |G|, and until recently
were considered efficient enough for exhaustive in-
ference. However, the increased model complex-
ity of a third-order parser forced Koo and Collins
(2010) to prune with a first-order model in order to
make inference practical. While fairly effective, all
these approaches are limited by the fact that infer-
ence in the coarse model remains cubic in the sen-
tence length. The desire to parse vast amounts of
text necessitates more efficient dependency parsing
algorithms.
We thus propose a multi-pass coarse-to-fine ap-
proach where the initial pass is a linear-time sweep,
which tries to resolve local ambiguities, but leaves
arcs beyond a fixed length b unspecified (Section
3). The dynamic program is a form of vine parsing
(Eisner and Smith, 2005), which we use to compute
parse max-marginals, rather than for finding the 1-
best parse tree. To reduce pruning errors, the param-
eters of the vine parser (and all subsequent pruning
models) are trained using the structured prediction
cascades of Weiss and Taskar (2010) to optimize
for pruning efficiency, and not for 1-best prediction
(Section 4). Despite a limited scope of b = 3, the
1This is in contrast to optimality preserving methods such as
A* search, which typically do not provide sufficient speed-ups
(Pauls and Klein, 2009).
498
vine pruning pass is able to preserve >98% of the
correct arcs, while ruling out ?86% of all possible
arcs. Subsequent i-th order passes introduce larger
scope features, while further constraining the search
space. In Section 5 we present experiments in multi-
ple languages. Our coarse-to-fine first-, second-, and
third-order parsers preserve the accuracy of the un-
pruned models, but are faster by up to two orders of
magnitude. Our pruned third-order model is faster
than an unpruned first-order model, and compares
favorably in speed to the state-of-the-art transition-
based parser of Zhang and Nivre (2011).
It is worth noting the relationship to greedy
transition-based dependency parsers that are also
linear-time (Nivre et al, 2004) or quadratic-time
(Yamada and Matsumoto, 2003). It is their success
that motivates building explicitly trained, linear-time
pruning models. However, while a greedy solu-
tion for arc-standard transition-based parsers can be
computed in linear-time, Kuhlmann et al (2011)
recently showed that computing exact solutions or
(max-)marginals has time complexity O(n4), mak-
ing these models inappropriate for coarse-to-fine
style pruning. As an alternative, Roark and Holling-
shead (2008) and Bergsma and Cherry (2010)
present approaches where individual classifiers are
used to prune chart cells. Such approaches have the
drawback that pruning decisions are made locally
and therefore can rule out all valid structures, despite
explicitly evaluating O(n2) chart cells. In contrast,
we make pruning decisions based on global parse
max-marginals using a vine pruning pass, which is
linear in the sentence length, but nonetheless guar-
antees to preserve a valid parse structure.
2 Motivation & Overview
The goal of this work is fast, high-order, graph-
based dependency parsing. Previous work on con-
stituency parsing demonstrates that performing sev-
eral passes with increasingly more complex mod-
els results in faster inference (Charniak et al, 2006;
Petrov and Klein, 2007). The same technique ap-
plies to dependency parsing with a cascade of mod-
els of increasing order; however, this strategy is
limited by the speed of the simplest model. The
algorithm for first-order dependency parsing (Eis-
ner, 2000) already requires O(n3) time, which Lee
1 2 3 4 5 6 7 8 9
modifier index
0
1
2
3
4
5
6
7
8
9
hea
din
dex
(a)
dependency length
freq
uen
cy
1 2 3 4 5 60.00.1
0.20.30.4
0.5 ADJNOUNVERB
(b)
Figure 1: (a) Heat map indicating how likely a par-
ticular head position is for each modifier position.
Greener/darker is likelier. (b) Arc length frequency for
three common modifier tags. Both charts are computed
from all sentences in Section 22 of the PTB.
(2002) shows is a practical lower bound for parsing
of context-free grammars. This bound implies that
it is unlikely that there can be an exhaustive pars-
ing algorithm that is asymptotically faster than the
standard approach.
We thus need to leverage domain knowledge to
obtain faster parsing algorithms. It is well-known
that natural language is fairly linear, and most head-
modifier dependencies tend to be short. This prop-
erty is exploited by transition-based dependency
parsers (Yamada and Matsumoto, 2003; Nivre et
al., 2004) and empirically demonstrated in Figure 1.
The heat map on the left shows that most of the
probability mass of modifiers is concentrated among
nearby words, corresponding to a diagonal band in
the matrix representation. On the right we show the
frequency of arc lengths for different modifier part-
of-speech tags. As one can expect, almost all arcs
involving adjectives (ADJ) are very short (length 3
or less), but even arcs involving verbs and nouns are
often short. This structure suggests that it may be
possible to disambiguate most dependencies by con-
sidering only the ?banded? portion of the sentence.
We exploit this linear structure by employing a
variant of vine parsing (Eisner and Smith, 2005).2
Vine parsing is a dependency parsing algorithm that
considers only close words as modifiers. Because of
this assumption it runs in linear time. Of course, any
parse tree with hard limits on dependency lengths
will contain major parse errors. We therefore use the
2The term vine parsing is a slight misnomer, since the un-
derlying vine models are as expressive as finite-state automata.
However, this allows them to circumvent the cubic-time bound.
499
As McGwire neared , fans went wild* As McGwire neared , fans went wild* As McGwire neared , fans went wild*
modifiers
heads
As McGwire
neared
, fans went wild
*AsMcGwireneared,
fanswentwild
modifiers
heads
As McGwire
neared
, fans went wild
*AsMcGwireneared,
fanswentwild
modifiers
heads
As McGwire
neared
, fans went wild
*AsMcGwireneared,
fanswentwild
Figure 2: Multi-pass pruning with a vine, first-order, and second-order model shown as dependencies and filtered
index sets after each pass. Darker cells have higher max-marginal values, while empty cells represent pruned arcs.
vine parser only for pruning and augment it to allow
arcs to remain unspecified (by including so called
outer arcs). The vine parser can thereby eliminate
a possibly quadratic number of arcs, while having
the flexibility to defer some decisions and preserve
ambiguity to be resolved by later passes. In Figure 2
for example, the vine pass correctly determined the
head-word of McGwire as neared, limited the head-
word candidates for fans to neared and went, and
decided that the head-word for went falls outside the
band by proposing an outer arc. A subsequent first-
order pass needs to score only a small fraction of all
possible arcs and can be used to further restrict the
search space for the following higher-order passes.
3 Graph-Based Dependency Parsing
Graph-based dependency parsing models factor all
valid parse trees for a given sentence into smaller
units, which can be scored independently. For in-
stance, in a first-order factorization, the units are just
dependency arcs. We represent these units by an in-
dex set I and use binary vectors Y ? {0, 1}|I| to
specify a parse tree y ? Y such that y(i) = 1 iff the
index i exists in the tree. The index sets of higher-
order models can be constructed out of the index sets
of lower-order models, thus forming a hierarchy that
we will exploit in our coarse-to-fine cascade.
The inference problem is to find the 1-best parse
tree arg maxy?Y y ? w, where w ? R|I| is a weight
vector that assigns a score to each index i (we dis-
cuss how w is learned in Section 4). A general-
ization of the 1-best inference problem is to find
the max-marginal score for each index i. Max-
marginals are given by the function M : I ? Y de-
fined as M(i;Y, w) = arg maxy?Y:y(i)=1 y ?w. For
first-order parsing, this corresponds to the best parse
utilizing a given dependency arc. Clearly there are
exponentially many possible parse tree structures,
but fortunately there exist well-known dynamic pro-
gramming algorithms for searching over all possible
structures. We review these below, starting with the
first-order factorization for ease of exposition.
Throughout the paper we make use of some ba-
sic mathematical notation. We write [c] for the enu-
meration {1, . . . , c} and [c]a for {a, . . . , c}. We use
1[c] for the indicator function, equal to 1 if con-
dition c is true and 0 otherwise. Finally we use
[c]+ = max{0, c} for the positive part of c.
3.1 First-Order Parsing
The simplest way to index a dependency parse struc-
ture is by the individual arcs of the parse tree. This
model is known as first-order or arc-factored. For a
sentence of length n the index set is:
I1 = {(h,m) : h ? [n]0,m ? [n]}
Each dependency tree has y(h,m) = 1 iff it includes
an arc from head h to modifier m. We follow com-
mon practice and use position 0 as the pseudo-root
(?) of the sentence. The full set I1 has cardinality
|I1| = O(n2).
500
(a)
h m
?I
h r
+C
mr + 1
C
(b)
h e
?C
h m
+I
m e
C
Figure 3: Parsing rules for first-order dependency pars-
ing. The complete items C are represented by triangles
and the incomplete items I are represented by trapezoids.
Symmetric left-facing versions are also included.
The first-order bilexical parsing algorithm of Eis-
ner (2000) can be used to find the best parse tree
and max-marginals. The algorithm defines a dy-
namic program over two types of items: incom-
plete items I(h,m) that denote the span between
a modifier m and its head h, and complete items
C(h, e) that contain a full subtree spanning from the
head h and to the word e on one side. The algo-
rithm builds larger items by applying the composi-
tion rules shown in Figure 3. Rule 3(a) builds an
incomplete item I(h,m) by attaching m as a modi-
fier to h. This rule has the effect that y(h,m) = 1 in
the final parse. Rule 3(b) completes item I(h,m) by
attaching item C(m, e). The existence of I(h,m)
implies that m modifies h, so this rule enforces that
the constituents of m are also constituents of h.
We can find the best derivation for each item
by adapting the standard CKY parsing algorithm
to these rules. Since both rule types contain three
variables that can range over the entire sentence
(h,m, e ? [n]0), the bottom-up, inside dynamic pro-
gramming algorithm requires O(n3) time. Further-
more, we can find max-marginals with an additional
top-down outside pass also requiring cubic time. To
speed up search, we need to filter indices from I1
and reduce possible applications of Rule 3(a).
3.2 Higher-Order Parsing
Higher-order models generalize the index set by us-
ing siblings s (modifiers that previously attached to
a head word) and grandparents g (head words above
the current head word). For compactness, we use g1
for the head word and sk+1 for the modifier and pa-
rameterize the index set to capture arbitrary higher-
(c) V?
0 e
? C
0 e? 1
+
ee? 1
C
(d)
0 e
?V?
0 m
+V?
em
I
(e)
0 e
?V?
0 e
V?
(f)
0 e
?V?
0 m
+V?
em
I
(g)
0 e
?C
0 e? 1
+V?
e? 1 e
C
Figure 4: Additional rules for vine parsing. Vine left
(V?) items are pictured as right-facing triangles and vine
right (V?) items are marked trapezoids. Each new item
is anchored at the root and grows to the right.
order decisions in both directions:
Ik,l = {(g, s) : g ? [n]l+10 , s ? [n]k+1}
where k + 1 is the sibling order, l + 1 is the par-
ent order, and k + l + 1 is the model order. The
canonical second-order model uses I1,0, which has
a cardinality of O(n3). Although there are several
possibilities for higher-order models, we use I1,1 as
our third-order model. Generally, the parsing index
set has cardinality |Ik,l| = O(n2+k+l). Inference
in higher-order models uses variants of the dynamic
program for first-order parsing, and we refer to pre-
vious work for the full set of rules. For second-order
models with index set I1,0, parsing can be done in
O(n3) time (McDonald and Pereira, 2006) and for
third-order models in O(n4) time (Koo and Collins,
2010). Even though second-order parsing has the
same asymptotic time complexity as first-order pars-
ing, inference is significantly slower due to the cost
of scoring the larger index set.
We aim to prune the index set, by mapping each
higher-order index down to a set of small set indices
501
that can be pruned using a coarse pruning model.
For example, to use a first-order model for pruning,
we would map the higher-order index to the individ-
ual indices for its arc, grandparents, and siblings:
pk,l?1(g, s) = {(g1, sj) : j ? [k + 1]}
? {(gj+1, gj) : j ? [l]}
The first-order pruning model can then be used
to score these indices, and to produce a filtered in-
dex set F (I1) by removing low-scoring indices (see
Section 4). We retain only the higher-order indices
that are supported by the filtered index set:
{(g, s) ? Ik,l : pk,l?1(g, s) ? F (I1)}
3.3 Vine Parsing
To further reduce the cost of parsing and produce
faster pruning models, we need a model with less
structure than the first-order model. A natural
choice, following Section 2, is to only consider
?short? arcs:
S = {(h,m) ? I1 : |h?m| ? b}
where b is a small constant. This constraint reduces
the size of the set to |S| = O(nb).
Clearly, this index set is severely limited; it is nec-
essary to have some long arcs for even short sen-
tences. We therefore augment the index set to in-
clude outer arcs:
I0 = S ? {(d,m) : d ? {?,?},m ? [n]}
? {(h, d) : h ? [n]0, d ? {?,?}}
The first set lets modifiers choose an outer head-
word and the second set lets head words accept outer
modifiers, and both sets distinguish the direction of
the arc. Figure 5 shows a right outer arc. The size of
I0 is linear in the sentence length. To parse the in-
dex set I0, we can modify the parse rules in Figure 3
to enforce additional length constraints (|h? e| ? b
for I(h, e) and |h?m| ? b for C(h,m)). This way,
only indices in S are explored. Unfortunately, this is
not sufficient since the constraints also prevent the
algorithm from producing a full derivation, since no
item can expand beyond length b.
Eisner and Smith (2005) therefore introduce vine
parsing, which includes two new items, vine left,
As McGwire neared , fans went wild*
Figure 5: An outer arc (1,?) from the word ?As? to pos-
sible right modifiers.
V?(e), and vine right, V?(e). Unlike the previous
items, these new items are left-anchored at the root
and grow only towards the right. The items V?(e)
and V?(e) encode the fact that a word e has not
taken a close (within b) head word to its left or right.
We incorporate these items by adding the five new
parsing rules shown in Figure 4.
The major addition is Rule 4(e) which converts a
vine left item V?(e) to a vine right item V?(e). This
implies that word e has no close head to either side,
and the parse has outer head arcs, y(?, e) = 1 or
y(?, e) = 1. The other rules are structural and dic-
tate creation and extension of vine items. Rules 4(c)
and 4(d) create vine left items from items that can-
not find a head word to their left. Rules 4(f) and
4(g) extend and finish vine right items. Rules 4(d)
and 4(f) each leave a head word incomplete, so they
may set y(e,?) = 1 or y(m,?) = 1 respec-
tively. Note that for all the new parse rules, e ? [n]0
and m ? {e ? b . . . n}, so parsing time of this so
called vine parsing algorithm is linear in the sen-
tence length O(nb2).
Alone, vine parsing is a poor model of syntax - it
does not even score most dependency pairs. How-
ever, it can act as a pruning model for other parsers.
We prune a first-order model by mapping first-order
indices to indices in I0.
p1?0(h,m) =
?
?
?
{(h,m)} if |h?m| ? b
{(?,m), (h,?)} if h < m
{(?,m), (h,?)} if h > m
The remaining first-order indices are then given by:
{(h,m) ? I1 : p1?0(h,m) ? F (I0)}
Figure 2 depicts a coarse-to-fine cascade, incor-
porating vine and first-order pruning passes and fin-
ishing with a higher-order parse model.
502
4 Training Methods
Our coarse-to-fine parsing architecture consists of
multiple pruning passes followed by a final pass
of 1-best parsing. The training objective for the
pruning models comes from the prediction cascade
framework of Weiss and Taskar (2010), which ex-
plicitly trades off pruning efficiency versus accuracy.
The models used in the final pass on the other hand
are trained for 1-best prediction.
4.1 Max-Marginal Filtering
At each pass of coarse-to-fine pruning, we apply an
index filter function F to trim the index set:
F (I) = {i ? I : f(i) = 1}
Several types of filters have been proposed in the
literature, with most work in coarse-to-fine pars-
ing focusing on predicates that threshold the poste-
rior probabilities. In structured prediction cascades,
we use a non-probabilistic filter, based on the max-
marginal value of the index:
f(i;Y, w) = 1[ M(i;Y, w) ? w < t?(Y, w) ]
where t?(Y, w) is a sentence-specific threshold
value. To counteract the fact that the max-marginals
are not normalized, the threshold t?(Y, w) is set as
a convex combination of the 1-best parse score and
the average max-marginal value:
t?(Y, w) = ?max
y?Y
(y ? w)
+ (1? ?) 1|I|
?
i?I
M(i;Y, w) ? w
where the model-specific parameter 0 ? ? ? 1 is
the tradeoff between ? = 1, pruning all indices i not
in the best parse, and ? = 0, pruning all indices with
max-marginal value below the mean.
The threshold function has the important property
that for any parse y, if y ?w ? t?(Y, w) then y(i) =
1 implies f(i) = 0, i.e. if the parse score is above
the threshold, then none of its indices will be pruned.
4.2 Filter Loss Training
The aim of our pruning models is to filter as many
indices as possible without losing the gold parse. In
structured prediction cascades, we incorporate this
pruning goal into our training objective.
Let y be the gold output for a sentence. We define
filter loss to be an indicator of whether any i with
y(i) = 1 is filtered:
?(y,Y, w) = 1[?i ? y,M(i;Y, w) ?w < t?(Y, w)]
During training we minimize the expected filter loss
using a standard structured SVM setup (Tsochan-
taridis et al, 2006). First we form a convex, con-
tinuous upper-bound of our loss function:
?(y,Y, w) ? 1[y ? w < t?(Y, w)]
? [1? y ? w + t?(Y, w)]+
where the first inequality comes from the proper-
ties of max-marginals and the second is the standard
hinge-loss upper-bound on an indicator.
Now assume that we have a corpus of P train-
ing sentences. Let the sequence (y(1), . . . , y(P )) be
the gold parses for each sentences and the sequence
(Y(1), . . . ,Y(P )) be the set of possible output struc-
tures. We can form the regularized risk minimiza-
tion for this upper bound of filter loss:
min
w
??w?2 + 1
P
P?
p=1
[1? y(p) ? w + t?(Y(p), w)]+
This objective is convex and non-differentiable, due
to the max inside t. We optimize using stochastic
subgradient descent (Shalev-Shwartz et al, 2007).
The stochastic subgradient at example p, H(w, p) is
0 if y(p) ? 1 ? t?(Y, w) otherwise,
H(w, p) =
2?w
P
? y(p) + ? arg max
y?Y(p)
y ? w
+ (1? ?) 1|I(p)|
?
i?I(p)
M(i;Y(p), w)
Each step of the algorithm has an update of the form:
wk = wk?1 ? ?kH(w, p)
where ? is an appropriate update rate for subgradi-
ent convergence. If ? = 1 the objective is identical
to structured SVM with 0/1 hinge loss. For other
values of ?, the subgradient includes a term from
the features of all max-marginal structures at each
index. These feature counts can be computed using
dynamic programming.
503
First-order Second-order Third-order
Setup Speed PE Oracle UAS Speed PE Oracle UAS Speed PE Oracle UAS
NOPRUNE 1.00 0.00 100 91.4 0.32 0.00 100 92.7 0.01 0.00 100 93.3
LENGTHDICTIONARY 1.94 43.9 99.9 91.5 0.76 43.9 99.9 92.8 0.05 43.9 99.9 93.3
LOCALSHORT 3.08 76.6 99.1 91.4 1.71 76.4 99.1 92.6 0.31 77.5 99.0 93.1
LOCAL 4.59 89.9 98.8 91.5 2.88 83.2 99.5 92.6 1.41 89.5 98.8 93.1
FIRSTONLY 3.10 95.5 95.9 91.5 2.83 92.5 98.4 92.6 1.61 92.2 98.5 93.1
FIRSTANDSECOND - - 1.80 97.6 97.7 93.1
VINEPOSTERIOR 3.92 94.6 96.5 91.5 3.66 93.2 97.7 92.6 1.67 96.5 97.9 93.1
VINECASCADE 5.24 95.0 95.7 91.5 3.99 91.8 98.7 92.6 2.22 97.8 97.4 93.1
k=8 k=16 k=64
ZHANGNIVRE 4.32 - - 92.4 2.39 - - 92.5 0.64 - - 92.7
Table 1: Results comparing pruning methods on PTB Section 22. Oracle is the max achievable UAS after pruning.
Pruning efficiency (PE) is the percentage of non-gold first-order dependency arcs pruned. Speed is parsing time relative
to the unpruned first-order model (around 2000 tokens/sec). UAS is the unlabeled attachment score of the final parses.
4.3 1-Best Training
For the final pass, we want to train the model for 1-
best output. Several different learning methods are
available for structured prediction models including
structured perceptron (Collins, 2002), max-margin
models (Taskar et al, 2003), and log-linear mod-
els (Lafferty et al, 2001). In this work, we use the
margin infused relaxed algorithm (MIRA) (Cram-
mer and Singer, 2003; Crammer et al, 2006) with
a hamming-loss margin. MIRA is an online algo-
rithm with similar benefits as structured perceptron
in terms of simplicity and fast training time. In prac-
tice, we found that MIRA with hamming-loss mar-
gin gives a performance improvement over struc-
tured perceptron and structured SVM.
5 Parsing Experiments
To empirically demonstrate the effectiveness of our
approach, we compare our vine pruning cascade
with a wide range of common pruning methods on
the Penn WSJ Treebank (PTB) (Marcus et al, 1993).
We then also show that vine pruning is effective
across a variety of different languages.
For English, we convert the PTB constituency
trees to dependencies using the Stanford dependency
framework (De Marneffe et al, 2006). We then
train on the standard PTB split with sections 2-21
as training, section 22 as validation, and section 23
as test. Results are similar using the Yamada and
Matsumoto (2003) conversion. We additionally se-
lected six languages from the CoNLL-X shared task
(Buchholz and Marsi, 2006) that cover a number
of different language families: Bulgarian, Chinese,
Japanese, German, Portuguese, and Swedish. We
use the standard CoNLL-X training/test split and
tune parameters with cross-validation.
All experiments use unlabeled dependencies for
training and test. Accuracy is reported as unlabeled
attachment score (UAS), the percentage of tokens
with the correct head word. For English, UAS ig-
nores punctuation tokens and the test set uses pre-
dicted POS tags. For the other languages we fol-
low the CoNLL-X setup and include punctuation in
UAS and use gold POS tags on the set set. Speed-
ups are given in terms of time relative to a highly
optimized C++ implementation. Our unpruned first-
order baseline can process roughly two thousand to-
kens a second and is comparable in speed to the
greedy shift-reduce parser of Nivre et al (2004).
5.1 Models
Our parsers perform multiple passes over each sen-
tence. In each pass we first construct a (pruned) hy-
pergraph (Klein and Manning, 2005) and then per-
form feature computation and inference. We choose
the highest ? that produces a pruning error of no
more than 0.2 on the validation set (typically ? ?
0.6) to filter indices for subsequent rounds (similar
to Weiss and Taskar (2010)). We compare a variety
of pruning models:
LENGTHDICTIONARY a deterministic prun-
ing method that eliminates all arcs longer
than the maximum length observed for each
504
head-modifier POS pair.
LOCAL an unstructured arc classifier that chooses
indices from I1 directly without enforcing
parse constraints. Similar to the quadratic-time
filter from Bergsma and Cherry (2010).
LOCALSHORT an unstructured arc classifier that
chooses indices from I0 directly without en-
forcing parse constraints. Similar to the linear-
time filter from Bergsma and Cherry (2010).
FIRSTONLY a structured first-order model trained
with filter loss for pruning.
FIRSTANDSECOND a structured cascade with
first- and second-order pruning models.
VINECASCADE the full cascade with vine, first-
and second-order pruning models.
VINEPOSTERIOR the vine parsing cascade trained
as a CRF with L-BFGS (Nocedal and Wright,
1999) and using posterior probabilities for fil-
tering instead of max-marginals.
ZHANGNIVRE an unlabeled reimplementation of
the linear-time, k-best, transition-based parser
of Zhang and Nivre (2011). This parser uses
composite features up to third-order with a
greedy decoding algorithm. The reimplemen-
tation is about twice as fast as their reported
speed, but scores slightly lower.
We found LENGTHDICTIONARY pruning to give
significant speed-ups in all settings and therefore al-
ways use it as an initial pass. The maximum number
of passes in a cascade is five: dictionary, vine, first-,
and second-order pruning, and a final third-order 1-
best pass.3 We tune the pruning thresholds for each
round and each cascade separately. This is because
we might be willing to do a more aggressive vine
pruning pass if the final model is a first-order model,
since these two models tend to often agree.
5.2 Features
For the non-pruning models, we use a standard set
of features proposed in the discriminative graph-
based dependency parsing literature (McDonald et
al., 2005; Carreras, 2007; Koo and Collins, 2010).
3For the first-order parser, we found it beneficial to employ a
reduced feature first-order pruner before the final model, i.e. the
cascade has four rounds: dictionary, vine, first-order pruning,
and first-order 1-best.
sentence length
10 20 30 40 50
No Prune [2.8]Length [1.9]Cascade [1.4]
me
an
tim
e
first-order
sentence length
10 20 30 40 50
No Prune [2.8]Length [2.0]Cascade [1.8]
me
an
tim
e
second-order
sentence length
10 20 30 40 50
No Prune [3.8]Length [2.4]Cascade [1.9]
me
an
tim
e
third-order
sentence length
10 20 30 40 50
Length [1.9]Local [1.8]Cascade [1.4]
me
an
tim
e
pruning methods
Figure 6: Mean parsing speed by sentence length for
first-, second-, and third-order parsers as well as differ-
ent pruning methods for first-order parsing. [b] indicates
the empirical complexity obtained from fitting axb.
Included are lexical features, part-of-speech fea-
tures, features on in-between tokens, as well as fea-
ture conjunctions, surrounding part-of-speech tags,
and back-off features. In addition, we replicate each
part-of-speech (POS) feature with an additional fea-
ture using coarse POS representations (Petrov et al,
2012). Our baseline parsing models replicate and,
for some experiments, surpass previous best results.
The first- and second-order pruning models have
the same structure, but for efficiency use only the
basic features from McDonald et al (2005). As fea-
ture computation is quite costly, future work may
investigate whether this set can be reduced further.
VINEPRUNE and LOCALSHORT use the same fea-
ture sets for short arcs. Outer arcs have features of
the unary head or modifier token, as well as features
for the POS tag bordering the cutoff and the direc-
tion of the arc.
5.3 Results
A comparison between the pruning methods is
shown in Table 1. The table gives relative speed-
ups, compared to the unpruned first-order baseline,
as well as accuracy, pruning efficiency, and ora-
cle scores. Note particularly that the third-order
cascade is twice as fast as an unpruned first-order
model and >200 times faster than the unpruned
third-order baseline. The comparison with poste-
505
1-Best Model
Round First Second Third
Vine 37% 27% 16%
First 63% 30% 17%
Second - 43% 18%
Third - - 49%
Table 2: Relative speed of pruning models in a multi-pass
cascade. Note that the 1-best models use richer features
than the corresponding pruning models.
rior pruning is less pronounced. Filter loss train-
ing is faster than VINEPOSTERIOR for first- and
third-order parsing, but the two models have similar
second-order speeds. It is also noteworthy that ora-
cle scores are consistently high even after multiple
pruning rounds: the oracle score of our third-order
model for example is 97.4%.
Vine pruning is particularly effective. The vine
pass is faster than both LOCAL and FIRSTONLY
and prunes more effectively than LOCALSHORT.
Vine pruning benefits from having a fast, linear-time
model, but still maintaining enough structure for
pruning. While our pruning approach does not pro-
vide any asymptotic guarantees, Figure 6 shows that
in practice our multi-pass parser scales well even
for long sentences: Our first-order cascade scales
almost linearly with the sentence length, while the
third-order cascade scales better than quadratic. Ta-
ble 2 shows that the final pass dominates the compu-
tational cost, while each of the pruning passes takes
up roughly the same amount of time.
Our second- and third-order cascades also signif-
icantly outperform ZHANGNIVRE. The transition-
based model with k = 8 is very efficient and effec-
tive, but increasing the k-best list size scales much
worse than employing multi-pass pruning. We also
note that while direct speed comparison are difficult,
our parser is significantly faster than the published
results for other high accuracy parsers, e.g. Huang
and Sagae (2010) and Koo et al (2010).
Table 3 shows our results across a subset of the
CoNLL-X datasets, focusing on languages that dif-
fer greatly in structure. The unpruned models per-
form well across datasets, scoring comparably to the
top results from the CoNLL-X competition. We see
speed increases for our cascades with almost no loss
in accuracy across all languages, even for languages
with fairly free word order like German. This is
First-order Second-order Third-order
Setup Speed UAS Speed UAS Speed UAS
BG B 1.90 90.7 0.67 92.0 0.05 92.1V 6.17 90.5 5.30 91.6 1.99 91.9
DE B 1.40 89.2 0.48 90.3 0.02 90.8V 4.72 89.0 3.54 90.1 1.44 90.8
JA B 1.77 92.0 0.58 92.1 0.04 92.4V 8.14 91.7 8.64 92.0 4.30 92.3
PT B 0.89 90.1 0.28 91.2 0.01 91.7V 3.98 90.0 3.45 90.9 1.45 91.5
SW B 1.37 88.5 0.45 89.7 0.01 90.4V 6.35 88.3 6.25 89.4 2.66 90.1
ZH B 7.32 89.5 3.30 90.5 0.67 90.8V 7.45 89.3 6.71 90.3 3.90 90.9
EN B 1.0 91.2 0.33 92.4 0.01 93.0V 5.24 91.0 3.92 92.2 2.23 92.7
Table 3: Speed and accuracy results for the vine prun-
ing cascade across various languages. B is the un-
pruned baseline model, and V is the vine pruning cas-
cade. The first section of the table gives results for
the CoNLL-X test datasets for Bulgarian (BG), German
(DE), Japanese (JA), Portuguese (PT), Swedish (SW),
and Chinese (ZH). The second section gives the result
for the English (EN) test set, PTB Section 23.
encouraging and suggests that the outer arcs of the
vine-pruning model are able to cope with languages
that are not as linear as English.
6 Conclusion
We presented a multi-pass architecture for depen-
dency parsing that leverages vine parsing and struc-
tured prediction cascades. The resulting 200-fold
speed-up leads to a third-order model that is twice
as fast as an unpruned first-order model for a vari-
ety of languages, and that also compares favorably
to a state-of-the-art transition-based parser. Possible
future work includes experiments using cascades to
explore much higher-order models.
Acknowledgments
We would like to thank the members of the Google
NLP Parsing Team for comments, suggestions, bug-
fixes and help in general: Ryan McDonald, Hao
Zhang, Michael Ringgaard, Terry Koo, Keith Hall,
Kuzman Ganchev and Yoav Goldberg. We would
also like to thank Andre Martins for showing that
MIRA with hamming-loss margin performs better
than other 1-best training algorithms.
506
References
S. Bergsma and C. Cherry. 2010. Fast and accurate arc
filtering for dependency parsing. In Proc. of COLING,
pages 53?61.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
X. Carreras, M. Collins, and T. Koo. 2008. Tag, dynamic
programming, and the perceptron for efficient, feature-
rich parsing. In Proc. of CoNLL, pages 9?16.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of CoNLL
Shared Task Session of EMNLP-CoNLL, volume 7,
pages 957?961.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil,
D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore,
M. Pozar, et al 2006. Multilevel coarse-to-fine PCFG
parsing. In Proc. of NAACL/HLT, pages 168?175.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of EMNLP, pages 1?8.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. The Journal
of Machine Learning Research, 3:951?991.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive algo-
rithms. The Journal of Machine Learning Research,
7:551?585.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC, volume 6,
pages 449?454.
J. Eisner and N.A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In Proc. of
IWPT, pages 30?41.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. Advances in Probabilistic
and Other Parsing Technologies, pages 29?62.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. of ACL,
pages 1077?1086.
D. Klein and C.D. Manning. 2005. Parsing and hy-
pergraphs. New developments in parsing technology,
pages 351?372.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL, pages 1?11.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP, pages
1288?1298.
M. Kuhlmann, C. Go?mez-Rodr??guez, and G. Satta. 2011.
Dynamic programming algorithms for transition-
based dependency parsers. In Proc. of ACL/HLT,
pages 673?682.
J. Lafferty, A. McCallum, and F.C.N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc. of
ICML, pages 282?289.
L. Lee. 2002. Fast context-free grammar parsing re-
quires fast boolean matrix multiplication. Journal of
the ACM, 49(1):1?15.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313?330.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of EACL, volume 6, pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL, pages 91?98.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proc. of CoNLL, pages 49?56.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
A. Pauls and D. Klein. 2009. Hierarchical search for
parsing. In Proc. of NAACL/HLT, pages 557?565.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of NAACL/HLT, pages
404?411.
S. Petrov, D. Das, and R. McDonald. 2012. A universal
part-of-speech tagset. In LREC.
S. Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA, USA.
B. Roark and K. Hollingshead. 2008. Classifying chart
cells for quadratic complexity context-free inference.
In Proc. of COLING, pages 745?751.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pe-
gasos: Primal estimated sub-gradient solver for svm.
In Proc. of ICML, pages 807?814.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
markov networks. Advances in neural information
processing systems, 16:25?32.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2006. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6(2):1453.
D. Weiss and B. Taskar. 2010. Structured prediction cas-
cades. In Proc. of AISTATS, volume 1284, pages 916?
923.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of IWPT, volume 3, pages 195?206.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc. of
ACL, pages 188?193.
507
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 600?609,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Part-of-Speech Tagging
with Bilingual Graph-Based Projections
Dipanjan Das?
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dipanjan@cs.cmu.edu
Slav Petrov
Google Research
New York, NY 10011, USA
slav@google.com
Abstract
We describe a novel approach for inducing
unsupervised part-of-speech taggers for lan-
guages that have no labeled training data, but
have translated text in a resource-rich lan-
guage. Our method does not assume any
knowledge about the target language (in par-
ticular no tagging dictionary is assumed),
making it applicable to a wide array of
resource-poor languages. We use graph-based
label propagation for cross-lingual knowl-
edge transfer and use the projected labels
as features in an unsupervised model (Berg-
Kirkpatrick et al, 2010). Across eight Eu-
ropean languages, our approach results in an
average absolute improvement of 10.4% over
a state-of-the-art baseline, and 16.7% over
vanilla hidden Markov models induced with
the Expectation Maximization algorithm.
1 Introduction
Supervised learning approaches have advanced the
state-of-the-art on a variety of tasks in natural lan-
guage processing, resulting in highly accurate sys-
tems. Supervised part-of-speech (POS) taggers,
for example, approach the level of inter-annotator
agreement (Shen et al, 2007, 97.3% accuracy for
English). However, supervised methods rely on la-
beled training data, which is time-consuming and
expensive to generate. Unsupervised learning ap-
proaches appear to be a natural solution to this prob-
lem, as they require only unannotated text for train-
?This research was carried out during an internship at Google
Research.
ing models. Unfortunately, the best completely un-
supervised English POS tagger (that does not make
use of a tagging dictionary) reaches only 76.1% ac-
curacy (Christodoulopoulos et al, 2010), making its
practical usability questionable at best.
To bridge this gap, we consider a practically mo-
tivated scenario, in which we want to leverage ex-
isting resources from a resource-rich language (like
English) when building tools for resource-poor for-
eign languages.1 We assume that absolutely no la-
beled training data is available for the foreign lan-
guage of interest, but that we have access to parallel
data with a resource-rich language. This scenario is
applicable to a large set of languages and has been
considered by a number of authors in the past (Al-
shawi et al, 2000; Xi and Hwa, 2005; Ganchev et
al., 2009). Naseem et al (2009) and Snyder et al
(2009) study related but different multilingual gram-
mar and tagger induction tasks, where it is assumed
that no labeled data at all is available.
Our work is closest to that of Yarowsky and Ngai
(2001), but differs in two important ways. First,
we use a novel graph-based framework for project-
ing syntactic information across language bound-
aries. To this end, we construct a bilingual graph
over word types to establish a connection between
the two languages (?3), and then use graph label
propagation to project syntactic information from
English to the foreign language (?4). Second, we
treat the projected labels as features in an unsuper-
1For simplicity of exposition we refer to the resource-poor lan-
guage as the ?foreign language.? Similarly, we use English
as the resource-rich language, but any other language with la-
beled resources could be used instead.
600
vised model (?5), rather than using them directly for
supervised training. To make the projection practi-
cal, we rely on the twelve universal part-of-speech
tags of Petrov et al (2011). Syntactic universals are
a well studied concept in linguistics (Carnie, 2002;
Newmeyer, 2005), and were recently used in similar
form by Naseem et al (2010) for multilingual gram-
mar induction. Because there might be some contro-
versy about the exact definitions of such universals,
this set of coarse-grained POS categories is defined
operationally, by collapsing language (or treebank)
specific distinctions to a set of categories that ex-
ists across all languages. These universal POS cat-
egories not only facilitate the transfer of POS in-
formation from one language to another, but also
relieve us from using controversial evaluation met-
rics,2 by establishing a direct correspondence be-
tween the induced hidden states in the foreign lan-
guage and the observed English labels.
We evaluate our approach on eight European lan-
guages (?6), and show that both our contributions
provide consistent and statistically significant im-
provements. Our final average POS tagging accu-
racy of 83.4% compares very favorably to the av-
erage accuracy of Berg-Kirkpatrick et al?s mono-
lingual unsupervised state-of-the-art model (73.0%),
and considerably bridges the gap to fully supervised
POS tagging performance (96.6%).
2 Approach Overview
The focus of this work is on building POS taggers
for foreign languages, assuming that we have an En-
glish POS tagger and some parallel text between
the two languages. Central to our approach (see
Algorithm 1) is a bilingual similarity graph built
from a sentence-aligned parallel corpus. As dis-
cussed in more detail in ?3, we use two types of
vertices in our graph: on the foreign language side
vertices correspond to trigram types, while the ver-
tices on the English side are individual word types.
Graph construction does not require any labeled
data, but makes use of two similarity functions. The
edge weights between the foreign language trigrams
are computed using a co-occurence based similar-
ity function, designed to indicate how syntactically
2See Christodoulopoulos et al (2010) for a discussion of met-
rics for evaluating unsupervised POS induction systems.
Algorithm 1 Bilingual POS Induction
Require: Parallel English and foreign language
data De and Df , unlabeled foreign training data
?f ; English tagger.
Ensure: ?f , a set of parameters learned using a
constrained unsupervised model (?5).
1: De?f ? word-align-bitext(De,Df )
2: D?e ? pos-tag-supervised(De)
3: A ? extract-alignments(De?f , D?e)
4: G? construct-graph(?f ,Df ,A)
5: G?? graph-propagate(G)
6: ?? extract-word-constraints(G?)
7: ?f ? pos-induce-constrained(?f ,?)
8: Return ?f
similar the middle words of the connected trigrams
are (?3.2). To establish a soft correspondence be-
tween the two languages, we use a second similar-
ity function, which leverages standard unsupervised
word alignment statistics (?3.3).3
Since we have no labeled foreign data, our goal
is to project syntactic information from the English
side to the foreign side. To initialize the graph we
tag the English side of the parallel text using a su-
pervised model. By aggregating the POS labels of
the English tokens to types, we can generate label
distributions for the English vertices. Label propa-
gation can then be used to transfer the labels to the
peripheral foreign vertices (i.e. the ones adjacent to
the English vertices) first, and then among all of the
foreign vertices (?4). The POS distributions over the
foreign trigram types are used as features to learn a
better unsupervised POS tagger (?5). The follow-
ing three sections elaborate these different stages is
more detail.
3 Graph Construction
In graph-based learning approaches one constructs
a graph whose vertices are labeled and unlabeled
examples, and whose weighted edges encode the
degree to which the examples they link have the
same label (Zhu et al, 2003). Graph construction
for structured prediction problems such as POS tag-
ging is non-trivial: on the one hand, using individ-
ual words as the vertices throws away the context
3The word alignment methods do not use POS information.
601
necessary for disambiguation; on the other hand,
it is unclear how to define (sequence) similarity if
the vertices correspond to entire sentences. Altun
et al (2005) proposed a technique that uses graph
based similarity between labeled and unlabeled parts
of structured data in a discriminative framework for
semi-supervised learning. More recently, Subra-
manya et al (2010) defined a graph over the cliques
in an underlying structured prediction model. They
considered a semi-supervised POS tagging scenario
and showed that one can use a graph over trigram
types, and edge weights based on distributional sim-
ilarity, to improve a supervised conditional random
field tagger.
3.1 Graph Vertices
We extend Subramanya et al?s intuitions to our
bilingual setup. Because the information flow in
our graph is asymmetric (from English to the foreign
language), we use different types of vertices for each
language. The foreign language vertices (denoted by
Vf ) correspond to foreign trigram types, exactly as
in Subramanya et al (2010). On the English side,
however, the vertices (denoted by Ve) correspond to
word types. Because all English vertices are going
to be labeled, we do not need to disambiguate them
by embedding them in trigrams. Furthermore, we do
not connect the English vertices to each other, but
only to foreign language vertices.4
The graph vertices are extracted from the differ-
ent sides of a parallel corpus (De, Df ) and an ad-
ditional unlabeled monolingual foreign corpus ?f ,
which will be used later for training. We use two dif-
ferent similarity functions to define the edge weights
among the foreign vertices and between vertices
from different languages.
3.2 Monolingual Similarity Function
Our monolingual similarity function (for connecting
pairs of foreign trigram types) is the same as the one
used by Subramanya et al (2010). We briefly re-
view it here for completeness. We define a sym-
metric similarity function K(ui, uj) over two for-
4This is because we are primarily interested in learning foreign
language taggers, rather than improving supervised English
taggers. Note, however, that it would be possible to use our
graph-based framework also for completely unsupervised POS
induction in both languages, similar to Snyder et al (2009).
Description Feature
Trigram + Context x1 x2 x3 x4 x5
Trigram x2 x3 x4
Left Context x1 x2
Right Context x4 x5
Center Word x3
Trigram ? Center Word x2 x4
Left Word + Right Context x2 x4 x5
Left Context + Right Word x1 x2 x4
Suffix HasSuffix(x3)
Table 1: Various features used for computing edge
weights between foreign trigram types.
eign language vertices ui, uj ? Vf based on the
co-occurrence statistics of the nine feature concepts
given in Table 1. Each feature concept is akin to a
random variable and its occurrence in the text corre-
sponds to a particular instantiation of that random
variable. For each trigram type x2 x3 x4 in a se-
quence x1 x2 x3 x4 x5, we count how many times
that trigram type co-occurs with the different instan-
tiations of each concept, and compute the point-wise
mutual information (PMI) between the two.5 The
similarity between two trigram types is given by
summing over the PMI values over feature instan-
tiations that they have in common. This is similar to
stacking the different feature instantiations into long
(sparse) vectors and computing the cosine similarity
between them. Finally, note that while most feature
concepts are lexicalized, others, such as the suffix
concept, are not.
Given this similarity function, we define a near-
est neighbor graph, where the edge weight for the n
most similar vertices is set to the value of the simi-
larity function and to 0 for all other vertices. We use
N (u) to denote the neighborhood of vertex u, and
fixed n = 5 in our experiments.
3.3 Bilingual Similarity Function
To define a similarity function between the English
and the foreign vertices, we rely on high-confidence
word alignments. Since our graph is built from a
parallel corpus, we can use standard word align-
ment techniques to align the English sentences De
5Note that many combinations are impossible giving a PMI
value of 0; e.g., when the trigram type and the feature instanti-
ation don?t have words in common.
602
and their foreign language translations Df .6 Label
propagation in the graph will provide coverage and
high recall, and we therefore extract only intersected
high-confidence (> 0.9) alignments De?f .
Based on these high-confidence alignments we
can extract tuples of the form [u ? v], where u is
a foreign trigram type, whose middle word aligns
to an English word type v. Our bilingual similarity
function then sets the edge weights in proportion to
these tuple counts.
3.4 Graph Initialization
So far the graph has been completely unlabeled. To
initialize the graph for label propagation we use a su-
pervised English tagger to label the English side of
the bitext.7 We then simply count the individual la-
bels of the English tokens and normalize the counts
to produce tag distributions over English word types.
These tag distributions are used to initialize the label
distributions over the English vertices in the graph.
Note that since all English vertices were extracted
from the parallel text, we will have an initial label
distribution for all vertices in Ve.
3.5 Graph Example
A very small excerpt from an Italian-English graph
is shown in Figure 1. As one can see, only the
trigrams [suo incarceramento ,], [suo iter ,] and
[suo carattere ,] are connected to English words. In
this particular case, all English vertices are labeled
as nouns by the supervised tagger. In general, the
neighborhoods can be more diverse and we allow a
soft label distribution over the vertices. It is worth
noting that the middle words of the Italian trigrams
are nouns too, which exhibits the fact that the sim-
ilarity metric connects types having the same syn-
tactic category. In the label propagation stage, we
propagate the automatic English tags to the aligned
Italian trigram types, followed by further propaga-
tion solely among the Italian vertices.
6We ran six iterations of IBM Model 1 (Brown et al, 1993),
followed by six iterations of the HMM model (Vogel et al,
1996) in both directions.
7We used a tagger based on a trigram Markov model (Brants,
2000) trained on the Wall Street Journal portion of the Penn
Treebank (Marcus et al, 1993), for its fast speed and reason-
able accuracy (96.7% on sections 22-24 of the treebank, but
presumably much lower on the (out-of-domain) parallel cor-
pus).
[ suo iter , ]
[ suo incarceramento , ]
[ suo fidanzato , ]
[ suo carattere , ]
[ imprisonment ]
[ enactment ]
[ character ]
[ del fidanzato , ]
[ il fidanzato , ]
NOUN
NOUN
NOUN
[ al fidanzato e ]
Figure 1: An excerpt from the graph for Italian. Three of
the Italian vertices are connected to an automatically la-
beled English vertex. Label propagation is used to propa-
gate these tags inwards and results in tag distributions for
the middle word of each Italian trigram.
4 POS Projection
Given the bilingual graph described in the previous
section, we can use label propagation to project the
English POS labels to the foreign language. We use
label propagation in two stages to generate soft la-
bels on all the vertices in the graph. In the first stage,
we run a single step of label propagation, which
transfers the label distributions from the English
vertices to the connected foreign language vertices
(say, V lf ) at the periphery of the graph. Note that
because we extracted only high-confidence align-
ments, many foreign vertices will not be connected
to any English vertices. This stage of label propa-
gation results in a tag distribution ri over labels y,
which encodes the proportion of times the middle
word of ui ? Vf aligns to English words vy tagged
with label y:
ri(y) =
?
vy
#[ui ? vy]
?
y?
?
vy?
#[ui ? vy? ]
(1)
The second stage consists of running traditional
label propagation to propagate labels from these pe-
ripheral vertices V lf to all foreign language vertices
603
in the graph, optimizing the following objective:
C(q) =
?
ui?Vf\V lf ,uj?N (ui)
wij?qi ? qj?
2
+ ?
?
ui?Vf\V lf
?qi ? U?
2
s.t.
?
y
qi(y) = 1 ?ui
qi(y) ? 0 ?ui, y
qi = ri ?ui ? V
l
f (2)
where the qi (i = 1, . . . , |Vf |) are the label distribu-
tions over the foreign language vertices and ? and
? are hyperparameters that we discuss in ?6.4. We
use a squared loss to penalize neighboring vertices
that have different label distributions: ?qi ? qj?2 =?
y(qi(y)?qj(y))
2, and additionally regularize the
label distributions towards the uniform distribution
U over all possible labels Y . It can be shown that
this objective is convex in q.
The first term in the objective function is the graph
smoothness regularizer which encourages the distri-
butions of similar vertices (large wij) to be similar.
The second term is a regularizer and encourages all
type marginals to be uniform to the extent that is al-
lowed by the first two terms (cf. maximum entropy
principle). If an unlabeled vertex does not have a
path to any labeled vertex, this term ensures that the
converged marginal for this vertex will be uniform
over all tags, allowing the middle word of such an
unlabeled vertex to take on any of the possible tags.
While it is possible to derive a closed form so-
lution for this convex objective function, it would
require the inversion of a matrix of order |Vf |. In-
stead, we resort to an iterative update based method.
We formulate the update as follows:
q(m)i (y) =
?
?
?
ri(y) if ui ? V lf
?i(y)
?i
otherwise
(3)
where ?ui ? Vf \ V lf , ?i(y) and ?i are defined as:
?i(y) =
?
uj?N (ui)
wijq
(m?1)
j (y) + ? U(y) (4)
?i = ? +
?
uj?N (ui)
wij (5)
We ran this procedure for 10 iterations.
5 POS Induction
After running label propagation (LP), we com-
pute tag probabilities for foreign word types x by
marginalizing the POS tag distributions of foreign
trigrams ui = x? x x+ over the left and right con-
text words:
p(y|x) =
?
x?,x+
qi(y)
?
x?,x+,y?
qi(y
?)
(6)
We then extract a set of possible tags tx(y) by elimi-
nating labels whose probability is below a threshold
value ? :
tx(y) =
{
1 if p(y|x) ? ?
0 otherwise
(7)
We describe how we choose ? in ?6.4. This vector
tx is constructed for every word in the foreign vo-
cabulary and will be used to provide features for the
unsupervised foreign language POS tagger.
We develop our POS induction model based on
the feature-based HMM of Berg-Kirkpatrick et al
(2010). For a sentence x and a state sequence z, a
first order Markov model defines a distribution:
P?(X = x,Z = z) = P?(Z1 = z1)?
?|x|
i=1 P?(Zi+1 = zi+1 | Zi = zi)? ?? ?
transition
?
P?(Xi = xi | Zi = zi)
? ?? ?
emission
(8)
In a traditional Markov model, the emission distri-
bution P?(Xi = xi | Zi = zi) is a set of multinomi-
als. The feature-based model replaces the emission
distribution with a log-linear model, such that:
P?(X = x | Z = z) =
exp ?>f(x, z)
?
x??Val(X)
exp ?>f(x?, z)
(9)
where Val(X) corresponds to the entire vocabulary.
This locally normalized log-linear model can look at
various aspects of the observation x, incorporating
overlapping features of the observation. In our ex-
periments, we used the same set of features as Berg-
Kirkpatrick et al (2010): an indicator feature based
604
on the word identity x, features checking whether x
contains digits or hyphens, whether the first letter of
x is upper case, and suffix features up to length 3.
All features were conjoined with the state z.
We trained this model by optimizing the following
objective function:
L(?) =
N?
i=1
log
?
z
P?(X = x
(i),Z = z(i))
?C???22 (10)
Note that this involves marginalizing out all possible
state configurations z for a sentence x, resulting in
a non-convex objective. To optimize this function,
we used L-BFGS, a quasi-Newton method (Liu and
Nocedal, 1989). For English POS tagging, Berg-
Kirkpatrick et al (2010) found that this direct gra-
dient method performed better (>7% absolute ac-
curacy) than using a feature-enhanced modification
of the Expectation-Maximization (EM) algorithm
(Dempster et al, 1977).8 Moreover, this route of
optimization outperformed a vanilla HMM trained
with EM by 12%.
We adopted this state-of-the-art model because it
makes it easy to experiment with various ways of
incorporating our novel constraint feature into the
log-linear emission model. This feature ft incor-
porates information from the smoothed graph and
prunes hidden states that are inconsistent with the
thresholded vector tx. The function ? : F ? C
maps from the language specific fine-grained tagset
F to the coarser universal tagset C and is described
in detail in ?6.2:
ft(x, z) = log(tx(y)), if ?(z) = y (11)
Note that when tx(y) = 1 the feature value is 0
and has no effect on the model, while its value is
?? when tx(y) = 0 and constrains the HMM?s
state space. This formulation of the constraint fea-
ture is equivalent to the use of a tagging dictionary
extracted from the graph using a threshold ? on the
posterior distribution of tags for a given word type
(Eq. 7). It would have therefore also been possible to
use the integer programming (IP) based approach of
8See ?3.1 of Berg-Kirkpatrick et al (2010) for more details
about their modification of EM, and how gradients are com-
puted for L-BFGS.
Ravi and Knight (2009) instead of the feature-HMM
for POS induction on the foreign side. However, we
do not explore this possibility in the current work.
6 Experiments and Results
Before presenting our results, we describe the
datasets that we used, as well as two baselines.
6.1 Datasets
We utilized two kinds of datasets in our experiments:
(i) monolingual treebanks9 and (ii) large amounts of
parallel text with English on one side. The availabil-
ity of these resources guided our selection of foreign
languages. For monolingual treebank data we re-
lied on the CoNLL-X and CoNLL-2007 shared tasks
on dependency parsing (Buchholz and Marsi, 2006;
Nivre et al, 2007). The parallel data came from the
Europarl corpus (Koehn, 2005) and the ODS United
Nations dataset (UN, 2006). Taking the intersection
of languages in these resources, and selecting lan-
guages with large amounts of parallel data, yields
the following set of eight Indo-European languages:
Danish, Dutch, German, Greek, Italian, Portuguese,
Spanish and Swedish.
Of course, we are primarily interested in apply-
ing our techniques to languages for which no la-
beled resources are available. However, we needed
to restrict ourselves to these languages in order to
be able to evaluate the performance of our approach.
We paid particular attention to minimize the number
of free parameters, and used the same hyperparam-
eters for all language pairs, rather than attempting
language-specific tuning. We hope that this will al-
low practitioners to apply our approach directly to
languages for which no resources are available.
6.2 Part-of-Speech Tagset and HMM States
We use the universal POS tagset of Petrov et al
(2011) in our experiments.10 This set C consists
of the following 12 coarse-grained tags: NOUN
(nouns), VERB (verbs), ADJ (adjectives), ADV
(adverbs), PRON (pronouns), DET (determiners),
ADP (prepositions or postpositions), NUM (numer-
als), CONJ (conjunctions), PRT (particles), PUNC
9We extracted only the words and their POS tags from the tree-
banks.
10Available at http://code.google.com/p/universal-pos-tags/.
605
(punctuation marks) and X (a catch-all for other
categories such as abbreviations or foreign words).
While there might be some controversy about the
exact definition of such a tagset, these 12 categories
cover the most frequent part-of-speech and exist in
one form or another in all of the languages that we
studied.
For each language under consideration, Petrov et
al. (2011) provide a mapping ? from the fine-grained
language specific POS tags in the foreign treebank
to the universal POS tags. The supervised POS tag-
ging accuracies (on this tagset) are shown in the last
row of Table 2. The taggers were trained on datasets
labeled with the universal tags.
The number of latent HMM states for each lan-
guage in our experiments was set to the number of
fine tags in the language?s treebank. In other words,
the set of hidden states F was chosen to be the fine
set of treebank tags. Therefore, the number of fine
tags varied across languages for our experiments;
however, one could as well have fixed the set of
HMM states to be a constant across languages, and
created one mapping to the universal POS tagset.
6.3 Various Models
To provide a thorough analysis, we evaluated three
baselines and two oracles in addition to two variants
of our graph-based approach. We were intentionally
lenient with our baselines:
? EM-HMM: A traditional HMM baseline, with
multinomial emission and transition distribu-
tions estimated by the Expectation Maximiza-
tion algorithm. We evaluated POS tagging ac-
curacy using the lenient many-to-1 evaluation
approach (Johnson, 2007).
? Feature-HMM: The vanilla feature-HMM of
Berg-Kirkpatrick et al (2010) (i.e. no ad-
ditional constraint feature) served as a sec-
ond baseline. Model parameters were esti-
mated with L-BFGS and evaluation again used
a greedy many-to-1 mapping.
? Projection: Our third baseline incorporates
bilingual information by projecting POS tags
directly across alignments in the parallel data.
For unaligned words, we set the tag to the most
frequent tag in the corresponding treebank. For
each language, we took the same number of
sentences from the bitext as there are in its tree-
bank, and trained a supervised feature-HMM.
This can be seen as a rough approximation of
Yarowsky and Ngai (2001).
We tried two versions of our graph-based approach:
? No LP: Our first version takes advantage of
our bilingual graph, but extracts the constraint
feature after the first stage of label propagation
(Eq. 1). Because many foreign word types are
not aligned to an English word (see Table 3),
and we do not run label propagation on the for-
eign side, we expect the projected information
to have less coverage. Furthermore we expect
the label distributions on the foreign to be fairly
noisy, because the graph constraints have not
been taken into account yet.
? With LP: Our full model uses both stages
of label propagation (Eq. 2) before extracting
the constraint features. As a result, we are
able to extract the constraint feature for all for-
eign word types and furthermore expect the
projected tag distributions to be smoother and
more stable.
Our oracles took advantage of the labeled treebanks:
? TB Dictionary: We extracted tagging dictio-
naries from the treebanks and and used them as
constraint features in the feature-based HMM.
Evaluation was done using the prespecified
mappings.
? Supervised: We trained the supervised model
of Brants (2000) on the original treebanks and
mapped the language-specific tags to the uni-
versal tags for evaluation.
6.4 Experimental Setup
While we tried to minimize the number of free pa-
rameters in our model, there are a few hyperparam-
eters that need to be set. Fortunately, performance
was stable across various values, and we were able
to use the same hyperparameters for all languages.
We used C = 1.0 as the L2 regularization con-
stant in (Eq. 10) and trained both EM and L-BFGS
for 1000 iterations. When extracting the vector
606
Model Danish Dutch German Greek Italian Portuguese Spanish Swedish Avg
baselines
EM-HMM 68.7 57.0 75.9 65.8 63.7 62.9 71.5 68.4 66.7
Feature-HMM 69.1 65.1 81.3 71.8 68.1 78.4 80.2 70.1 73.0
Projection 73.6 77.0 83.2 79.3 79.7 82.6 80.1 74.7 78.8
our approach
No LP 79.0 78.8 82.4 76.3 84.8 87.0 82.8 79.4 81.3
With LP 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
oracles
TB Dictionary 93.1 94.7 93.5 96.6 96.4 94.0 95.8 85.5 93.7
Supervised 96.9 94.9 98.2 97.8 95.8 97.2 96.8 94.8 96.6
Table 2: Part-of-speech tagging accuracies for various baselines and oracles, as well as our approach. ?Avg? denotes
macro-average across the eight languages.
tx used to compute the constraint feature from the
graph, we tried three threshold values for ? (see
Eq. 7). Because we don?t have a separate develop-
ment set, we used the training set to select among
them and found 0.2 to work slightly better than 0.1
and 0.3. For seven out of eight languages a thresh-
old of 0.2 gave the best results for our final model,
which indicates that for languages without any val-
idation set, ? = 0.2 can be used. For graph prop-
agation, the hyperparameter ? was set to 2 ? 10?6
and was not tuned. The graph was constructed using
2 million trigrams; we chose these by truncating the
parallel datasets up to the number of sentence pairs
that contained 2 million trigrams.
6.5 Results
Table 2 shows our complete set of results. As ex-
pected, the vanilla HMM trained with EM performs
the worst. The feature-HMM model works better for
all languages, generalizing the results achieved for
English by Berg-Kirkpatrick et al (2010). Our ?Pro-
jection? baseline is able to benefit from the bilingual
information and greatly improves upon the mono-
lingual baselines, but falls short of the ?No LP?
model by 2.5% on an average. The ?No LP? model
does not outperform direct projection for German
and Greek, but performs better for six out of eight
languages. Overall, it gives improvements ranging
from 1.1% for German to 14.7% for Italian, for an
average improvement of 8.3% over the unsupervised
feature-HMM model. For comparison, the com-
pletely unsupervised feature-HMM baseline accu-
racy on the universal POS tags for English is 79.4%,
and goes up to 88.7% with a treebank dictionary.
Our full model (?With LP?) outperforms the un-
supervised baselines and the ?No LP? setting for all
languages. It falls short of the ?Projection? base-
line for German, but is statistically indistinguish-
able in terms of accuracy. As indicated by bolding,
for seven out of eight languages the improvements
of the ?With LP? setting are statistically significant
with respect to the other models, including the ?No
LP? setting.11 Overall, it performs 10.4% better
than the hitherto state-of-the-art feature-HMM base-
line, and 4.6% better than direct projection, when we
macro-average the accuracy over all languages.
6.6 Discussion
Our full model outperforms the ?No LP? setting
because it has better vocabulary coverage and al-
lows the extraction of a larger set of constraint fea-
tures. We tabulate this increase in Table 3. For all
languages, the vocabulary sizes increase by several
thousand words. Although the tag distributions of
the foreign words (Eq. 6) are noisy, the results con-
firm that label propagation within the foreign lan-
guage part of the graph adds significant quality for
every language.
Figure 2 shows an excerpt of a sentence from the
Italian test set and the tags assigned by four different
models, as well as the gold tags. While the first three
models get three to four tags wrong, our best model
gets only one word wrong and is the most accurate
among the four models for this example. Examin-
ing the word fidanzato for the ?No LP? and ?With
LP? models is particularly instructive. As Figure 1
shows, this word has no high-confidence alignment
in the Italian-English bitext. As a result, its POS tag
needs to be induced in the ?No LP? case, while the
11A word level paired-t-test is significant at p < 0.01 for Dan-
ish, Greek, Italian, Portuguese, Spanish and Swedish, and
p < 0.05 for Dutch.
607
Gold:
si          trovava          in           un          parco          con          il            fidanzato         Paolo        F.     ,     27    anni       ,   rappresentante   
EM-HMM:
Feature-HMM:
No LP:
With LP:
CONJ NOUN DET DET NOUN ADP DET NOUN .
NOUN
.
NUM NOUN .
NOUN
PRON VERB ADP DET NOUN CONJ DET NOUN NOUN
NOUN
.
ADP NOUN .
VERB
PRON VERB ADP DET NOUN ADP DET NOUN NOUN
NOUN
.
NUM NOUN .
NOUN
VERB VERB ADP DET NOUN ADP DET ADJ NOUN
ADJ
.
NUM NOUN .
NOUN
VERB VERB ADP DET NOUN ADP DET NOUN NOUN
NOUN
.
NUM NOUN .
NOUN
Figure 2: Tags produced by the different models along with the reference set of tags for a part of a sentence from the
Italian test set. Italicized tags denote incorrect labels.
Language
# words with constraints
?No LP? ?With LP?
Danish 88,240 128, 391
Dutch 51,169 74,892
German 59,534 107,249
Greek 90,231 114,002
Italian 48,904 62,461
Portuguese 46,787 65,737
Spanish 72,215 82,459
Swedish 70,181 88,454
Table 3: Size of the vocabularies for the ?No LP? and
?With LP? models for which we can impose constraints.
correct tag is available as a constraint feature in the
?With LP? case.
7 Conclusion
We have shown the efficacy of graph-based label
propagation for projecting part-of-speech informa-
tion across languages. Because we are interested in
applying our techniques to languages for which no
labeled resources are available, we paid particular
attention to minimize the number of free parame-
ters and used the same hyperparameters for all lan-
guage pairs. Our results suggest that it is possible to
learn accurate POS taggers for languages which do
not have any annotated data, but have translations
into a resource-rich language. Our results outper-
form strong unsupervised baselines as well as ap-
proaches that rely on direct projections, and bridge
the gap between purely supervised and unsupervised
POS tagging models.
Acknowledgements
We would like to thank Ryan McDonald for numer-
ous discussions on this topic. We would also like to
thank Amarnag Subramanya for helping us with the
implementation of label propagation and Shankar
Kumar for access to the parallel data. Finally, we
thank Kuzman Ganchev and the three anonymous
reviewers for helpful suggestions and comments on
earlier drafts of this paper.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Head-transducer models for speech translation
and their automatic acquisition from bilingual data.
Machine Translation, 15.
Yasemin Altun, David McAllester, and Mikhail Belkin.
2005. Maximum margin semi-supervised learning for
structured variables. In Proc. of NIPS.
Taylor Berg-Kirkpatrick, Alexandre B. Co?te?, John DeN-
ero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proc. of NAACL-HLT.
Thorsten Brants. 2000. TnT - a statistical part-of-speech
tagger. In Proc. of ANLP.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Andrew Carnie. 2002. Syntax: A Generative Introduc-
tion (Introducing Linguistics). Blackwell Publishing.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proc. of
EMNLP.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proc. of ACL-IJCNLP.
608
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP-CoNLL.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. JAIR, 36.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowledge
to guide grammar induction. In Proc. of EMNLP.
Frederick J. Newmeyer. 2005. Possible and Probable
Languages: A Generative Perspective on Linguistic
Typology. Oxford University Press.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. ArXiv:1104.2086.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proc. of
ACL-IJCNLP.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proc. of ACL.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proc. of ACL-IJCNLP.
Amar Subramanya, Slav Petrov, and Fernando Pereira.
2010. Efficient graph-based semi-supervised learning
of structured tagging models. In Proc. of EMNLP.
UN. 2006. ODS UN parallel corpus.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. of COLING.
Chenhai Xi and Rebecca Hwa. 2005. A backoff
model for bootstrapping resources for non-English
languages. In Proc. of HLT-EMNLP.
David Yarowsky and Grace Ngai. 2001. Inducing multi-
lingual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. In Proc. of NAACL.
Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty.
2003. Semi-supervised learning using gaussian fields
and harmonic functions. In Proc. of ICML.
609
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 238?242,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Using Search-Logs to Improve Query Tagging
Kuzman Ganchev Keith Hall Ryan McDonald Slav Petrov
Google, Inc.
{kuzman|kbhall|ryanmcd|slav}@google.com
Abstract
Syntactic analysis of search queries is im-
portant for a variety of information-retrieval
tasks; however, the lack of annotated data
makes training query analysis models diffi-
cult. We propose a simple, efficient proce-
dure in which part-of-speech tags are trans-
ferred from retrieval-result snippets to queries
at training time. Unlike previous work, our
final model does not require any additional re-
sources at run-time. Compared to a state-of-
the-art approach, we achieve more than 20%
relative error reduction. Additionally, we an-
notate a corpus of search queries with part-
of-speech tags, providing a resource for future
work on syntactic query analysis.
1 Introduction
Syntactic analysis of search queries is important for
a variety of tasks including better query refinement,
improved matching and better ad targeting (Barr
et al, 2008). However, search queries differ sub-
stantially from traditional forms of written language
(e.g., no capitalization, few function words, fairly
free word order, etc.), and are therefore difficult
to process with natural language processing tools
trained on standard corpora (Barr et al, 2008). In
this paper we focus on part-of-speech (POS) tagging
queries entered into commercial search engines and
compare different strategies for learning from search
logs. The search logs consist of user queries and
relevant search results retrieved by a search engine.
We use a supervised POS tagger to label the result
snippets and then transfer the tags to the queries,
producing a set of noisy labeled queries. These la-
beled queries are then added to the training data and
the tagger is retrained. We evaluate different strate-
gies for selecting which annotation to transfer and
find that using the result that was clicked by the user
gives comparable performance to using just the top
result or to aggregating over the top-k results.
The most closely related previous work is that of
Bendersky et al (2010, 2011). In their work, un-
igram POS tag priors generated from a large cor-
pus are blended with information from the top-50
results from a search engine at prediction time. Such
an approach has the disadvantage that it necessitates
access to a search engine at run-time and is com-
putationally very expensive. We re-implement their
method and show that our direct transfer approach is
more effective, while being simpler to instrument:
since we use information from the search engine
only during training, we can train a stand-alone POS
tagger that can be run without access to additional
resources. We also perform an error analysis and
find that most of the remaining errors are due to er-
rors in POS tagging of the snippets.
2 Direct Transfer
The main intuition behind our work, Bendersky et
al. (2010) and Ru?d et al (2011), is that standard NLP
annotation tools work better on snippets returned by
a search engine than on user supplied queries. This
is because snippets are typically well-formed En-
glish sentences, while queries are not. Our goal is to
leverage this observation and use a supervised POS
tagger trained on regular English sentences to gen-
erate annotations for a large set of queries that can
be used for training a query-specific model. Perhaps
the simplest approach ? but also a surprisingly pow-
erful one ? is to POS tag some relevant snippets for
238
a given query, and then to transfer the tags from the
snippet tokens to matching query tokens. This ?di-
rect? transfer idea is at the core of all our experi-
ments. In this work, we provide a comparison of
techniques for selecting snippets associated with the
query, as well as an evaluation of methods for align-
ing the matching words in the query to those in the
selected snippets.
Specifically, for each query1 with a corresponding
set of ?relevant snippets,? we first apply the baseline
tagger to the query and all the snippets. We match
any query terms in these snippets, and copy over the
POS tag to the matching query term. Note that this
can produce multiple labelings as the relevant snip-
pet set can be very diverse and varies even for the
same query. We choose the most frequent tagging
as the canonical one and add it to our training set.
We then train a query tagger on all our training data:
the original human annotated English sentences and
also the automatically generated query training set.
The simplest way to match query tokens to snip-
pet tokens is to allow a query token to match any
snippet token. This can be problematic when we
have queries that have a token repeated with differ-
ent parts-of-speech such as in ?tie a tie.? To make a
more precise matching we try a sequence of match-
ing rules: First, exact match of the query n-gram.
Then matching the terms in order, so the query ?tiea
a tieb? matched to the snippet ?to tie1 a neck tie2?
would match tiea:tie1 and tieb:tie2. Finally, we
match as many query terms as possible. An early
observation showed that when a query term occurs
in the result URL, e.g., searching for ?irs mileage
rate? results in the page irs.gov, the query term
matching the URL domain name is usually a proper
noun. Consequently we add this rule.
In the context of search logs, a relevant snippet
set can refer to the top k snippets (including the case
where k = 1) or the snippet(s) associated with re-
sults clicked by users that issued the query. In our
experiments we found that different strategies for se-
lecting relevant snippets, such as selecting the snip-
pets of the clicked results, using the top-10 results
or using only the top result, perform similarly (see
Table 1).
1We skip navigational queries, e.g, amazon or amazon.com,
since syntactic analysis of such queries is not useful.
Query budget/NN rent/VB a/DET car/NN Clicks
Snip 1 . . . Budget/NNP Rent/NNP 2
A/NNP Car/NNP . . .
Snip 2 . . . Go/VB to/TO Budget/NNP 1
to/TO rent/VB a/DET car/NN . . .
Snip 3 . . . Rent/VB a/DET car/NN 1
from/IN Budget/NNP . . .
Figure 1: Example query and snippets as tagged by a
baseline tagger as well as associated clicks.
By contrast Bendersky et al (2010) use a lin-
ear interpolation between a prior probability and the
snippet tagging. They define pi(t|w) as the relative
frequency of tag t given by the baseline tagger to
word w in some corpus and ?(t|w, s) as the indica-
tor function for word w in the context of snippet s
has tag t. They define the tagging of a word as
argmax
t
0.2pi(t|w) + 0.8mean
s:w?s
?(t|w, s) (1)
We illustrate the difference between the two ap-
proaches in Figure 1. The numbered rows of the
table correspond to three snippets (with non-query
terms elided). The strategy that uses the clicks to se-
lect the tagging would count two examples of ?Bud-
get/NNP Rent/NNP A/NNP Car/NNP? and one for
each of two other taggings. Note that snippet 1
and the query get different taggings primarily due
to orthographic variations. It would then add ?bud-
get/NNP rent/NNP a/NNP car/NNP? to its training
set. The interpolation approach of Bendersky et al
(2010) would tag the query as ?budget/NNP rent/VB
a/DET car/NN?. To see why this is the case, consider
the probability for rent/VB vs rent/NNP. For rent/VB
we have 0.2 + 0.8? 23 , while for rent/NNP we have
0 + 0.8? 13 assuming that pi(VB|rent) = 1.
3 Experimental Setup
We assume that we have access to labeled English
sentences from the PennTreebank (Marcus et al,
1993) and the QuestionBank (Judge et al, 2006), as
well as large amounts of unlabeled search queries.
Each query is paired with a set of relevant results
represented by snippets (sentence fragments con-
taining the search terms), as well as information
about the order in which the results were shown to
the user and possibly the result the user clicked on.
Note that different sets of results are possible for the
239
same query, because of personalization and ranking
changes over time.
3.1 Evaluation Data
We use two data sets for evaluation. The first is the
set of 251 queries from Microsoft search logs (MS-
251) used in Bendersky et al (2010, 2011). The
queries are annotated with three POS tags represent-
ing nouns, verbs and ?other? tags (MS-251 NVX).
We additionally refine the annotation to cover 14
POS tags comprising the 12 universal tags of Petrov
et al (2012), as well as proper nouns and a special
tag for search operator symbols such as ?-? (for
excluding the subsequent word). We refer to this
evaluation set as MS-251 in our experiments. We
had two annotators annotate the whole of the MS-
251 data set. Before arbitration, the inter-annotator
agreement was 90.2%. As a reference, Barr et al
(2008) report 79.3% when annotating queries with
19 POS tags. We then examined all the instances
where the annotators disagreed, and corrected
the discrepancy. Our annotations are available at
http://code.google.com/p/query-syntax/.
The second evaluation set consists of 500 so
called ?long-tail? queries. These are queries that oc-
curred rarely in the search logs, and are typically
difficult to tag because they are searching for less-
frequent information. They do not contain naviga-
tional queries.
3.2 Baseline Model
We use a linear chain tagger trained with the aver-
aged perceptron (Collins, 2002). We use the follow-
ing features for our tagger: current word, suffixes
and prefixes of length 1 to 3; additionally we use
word cluster features (Uszkoreit and Brants, 2008)
for the current word, and transition features of the
cluster of the current and previous word. When
training on Sections 1-18 of the Penn Treebank
and testing on sections 22-24, our tagger achieves
97.22% accuracy with the Penn Treebank tag set,
which is state-of-the-art for this data set. When we
evaluate only on the 14 tags used in our experiments,
the accuracy increases to 97.88%.
We experimented with 4 baseline taggers (see Ta-
ble 2). WSJ corresponds to training on only the
standard training sections of Wall Street Journal por-
tion of the Penn Treebank. WSJ+QTB adds the
Method
MS-251
NVX
MS-251 long-tail
DIRECT-CLICK 93.43 84.11 78.15
DIRECT-ALL 93.93 84.39 77.73
DIRECT-TOP-1 93.93 84.60 77.60
Table 1: Evaluation of snippet selection strategies.
QuestionBank as training data. WSJ NOCASE and
WSJ+QTB NOCASE use case-insensitive version of
the tagger (conceptually lowercasing the text before
training and before applying the tagger). As we will
see, all our baseline models are better than the base-
line reported in Bendersky et al (2010); our lower-
cased baseline model significantly outperforms even
their best model.
4 Experiments
First, we compared different strategies for selecting
relevant snippets from which to transfer the tags.
These systems are: DIRECT-CLICK, which uses
snippets clicked on by users; DIRECT-ALL, which
uses all the returned snippets seen by the user;2
and DIRECT-TOP-1, which uses just the snippet in
the top result. Table 1 compares these systems on
our three evaluation sets. While DIRECT-ALL and
DIRECT-TOP-1 perform best on the MS-251 data
sets, DIRECT-CLICK has an advantage on the long
tail queries. However, these differences are small
(<0.6%) suggesting that any strategy for selecting
relevant snippet sets will return comparable results
when aggregated over large amounts of data.
We then compared our method to the baseline
models and a re-implementation of Bendersky et al
(2010), which we denote BSC. We use the same
matching scheme for both BSC and our system, in-
cluding the URL matching described in Section 2.
The URL matching improves performance by 0.4-
3.0% across all models and evaluation settings.
Table 2 summarizes our final results. For com-
parison, Bendersky et al (2010) report 91.6% for
their final system, which is comparable to our im-
plementation of their system when the baseline tag-
ger is trained on just the WSJ corpus. Our best sys-
tem achieves a 21.2% relative reduction in error on
their annotations. Some other trends become appar-
2Usually 10 results, but more if the user viewed the second
page of results.
240
Method
MS-251
NVX
MS-251 long-tail
WSJ 90.54 75.07 53.06
BSC 91.74 77.82 57.65
DIRECT-CLICK 93.36 85.81 76.13
WSJ + QTB 90.18 74.86 53.48
BSC 91.74 77.54 57.65
DIRECT-CLICK 93.01 85.03 76.97
WSJ NOCASE 92.87 81.92 74.31
BSC 93.71 84.32 76.63
DIRECT-CLICK 93.50 84.46 77.48
WSJ + QTB NOCASE 93.08 82.70 74.65
BSC 93.57 83.90 77.27
DIRECT-CLICK 93.43 84.11 78.15
Table 2: Tagging accuracies for different baseline settings
and two transfer methods.DIRECT-CLICK is the approach
we propose (see text). Column MS-251 NVX evaluates
with tags from Bendersky et al (2010). Their baseline
is 89.3% and they report 91.6% for their method. MS-
251 and Long-tail use tags from Section 3.1. We observe
snippets for 2/500 long-tail queries and 31/251 MS-251
queries.
ent in Table 2. Firstly, a large part of the benefit of
transfer has to do with case information that is avail-
able in the snippets but is missing in the query. The
uncased tagger is insensitive to this mismatch and
achieves significantly better results than the cased
taggers. However, transferring information from the
snippets provides additional benefits, significantly
improving even the uncased baseline taggers. This
is consistent with the analysis in Barr et al (2008).
Finally, we see that the direct transfer method from
Section 2 significantly outperforms the method de-
scribed in Bendersky et al (2010). Table 3 confirms
this trend when focusing on proper nouns, which are
particularly difficult to identify in queries.
We also manually examined a set of 40 queries
with their associated snippets, for which our best
DIRECT-CLICK system made mistakes. In 32 cases,
the errors in the query tagging could be traced back
to errors in the snippet tagging. A better snippet
tagger could alleviate that problem. In the remain-
ing 8 cases there were problems with the matching
? either the mis-tagged word was not found at all,
or it was matched incorrectly. For example one of
the results for the query ?bell helmet? had a snippet
containing ?Bell cycling helmets? and we failed to
match helmet to helmets.
Method P R F
WSJ + QTB NOCASE 72.12 79.80 75.77
BSC 82.87 69.05 75.33
BSC + URL 83.01 70.80 76.42
DIRECT-CLICK 79.57 76.51 78.01
DIRECT-ALL 75.88 78.38 77.11
DIRECT-TOP-1 78.38 76.40 77.38
Table 3: Precision and recall of the NNP tag on the long-
tail data for the best baseline method and the three trans-
fer methods using that baseline.
5 Related Work
Barr et al (2008) manually annotate a corpus of
2722 queries with 19 POS tags and use it to train
and evaluate POS taggers, and also describe the lin-
guistic structures they find. Unfortunately their data
is not available so we cannot use it to compare to
their results. Ru?d et al (2011) create features based
on search engine results, that they use in an NER
system applied to queries. They report report sig-
nificant improvements when incorporating features
from the snippets. In particular, they exploit capital-
ization and query terms matching URL components;
both of which we have used in this work. Li et al
(2009) use clicks in a product data base to train a tag-
ger for product queries, but they do not use snippets
and do not annotate syntax. Li (2010) and Manshadi
and Li (2009) also work on adding tags to queries,
but do not use snippets or search logs as a source of
information.
6 Conclusions
We described a simple method for training a search-
query POS tagger from search-logs by transfer-
ring context from relevant snippet sets to query
terms. We compared our approach to previous work,
achieving an error reduction of 20%. In contrast to
the approach proposed by Bendersky et al (2010),
our approach does not require access to the search
engine or index when tagging a new query. By ex-
plicitly re-training our final model, it has the ability
to pool knowledge from several related queries and
incorporate the information into the model param-
eters. An area for future work is to transfer other
syntactic information, such as parse structures or su-
pertags using a similar transfer approach.
241
References
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
The linguistic structure of English web-search queries.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1021?1030, Honolulu, Hawaii, October. Association
for Computational Linguistics.
M. Bendersky, W.B. Croft, and D.A. Smith. 2010.
Structural annotation of search queries using pseudo-
relevance feedback. In Proceedings of the 19th ACM
international conference on Information and knowl-
edge management, pages 1537?1540. ACM.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 497?504, Sydney, Australia, July.
Association for Computational Linguistics.
X. Li, Y.Y. Wang, and A. Acero. 2009. Extracting
structured information from user queries with semi-
supervised conditional random fields. In Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 572?579. ACM.
X. Li. 2010. Understanding the semantic structure of
noun phrase queries. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1337?1345. Association for Com-
putational Linguistics.
M. Manshadi and X. Li. 2009. Semantic tagging of web
search queries. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2-Volume 2, pages
861?869. Association for Computational Linguistics.
M. P. Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
English: the Penn treebank. Computational Linguis-
tics, 19.
S. Petrov, D. Das, and R. McDonald. 2012. A universal
part-of-speech tagset. In Proc. of LREC.
Stefan Ru?d, Massimiliano Ciaramita, Jens Mu?ller, and
Hinrich Schu?tze. 2011. Piggyback: Using search en-
gines for robust cross-domain named entity recogni-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 965?975, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
J. Uszkoreit and T. Brants. 2008. Distributed word clus-
tering for large scale class-based language modeling in
machine translation. In Proc. of ACL.
242
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 169?174,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Syntactic Annotations for the Google Books Ngram Corpus
Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,
Jon Orwant, Will Brockman and Slav Petrov?
Google Inc.
{yurilin,jbmichel,drerez,orwant,brockman,slav}@google.com
Abstract
We present a new edition of the Google Books
Ngram Corpus, which describes how often
words and phrases were used over a period
of five centuries, in eight languages; it reflects
6% of all books ever published. This new edi-
tion introduces syntactic annotations: words
are tagged with their part-of-speech, and head-
modifier relationships are recorded. The an-
notations are produced automatically with sta-
tistical models that are specifically adapted to
historical text. The corpus will facilitate the
study of linguistic trends, especially those re-
lated to the evolution of syntax.
1 Introduction
The Google Books Ngram Corpus (Michel et al,
2011) has enabled the quantitative analysis of lin-
guistic and cultural trends as reflected in millions
of books written over the past five centuries. The
corpus consists of words and phrases (i.e., ngrams)
and their usage frequency over time. The data is
available for download, and can also be viewed
through the interactive Google Books Ngram Viewer
at http://books.google.com/ngrams.
The sheer quantity of and broad historical scope
of the data has enabled a wide range of analyses
(Michel et al, 2011; Ravallion, 2011). Of course,
examining raw ngram frequencies is of limited util-
ity when studying many aspects of linguistic change,
particularly the ones related to syntax. For instance,
most English verbs are regular (their past tense is
formed by adding -ed), and the few exceptions,
known as irregular verbs, tend to regularize over the
? Corresponding author.
1800 1850 1900 1950 2000
Rel
ativ
e F
req
uen
cy
burntburnt_VERBburnt_ADJburnedburned_VERBburned_ADJ
Figure 1: Usage frequencies of burned and burnt over
time, showing that burned became the dominant spelling
around 1880. Our new syntactic annotations enable a
more refined analysis, suggesting that the crossing-point
for the verb usage (burned VERB vs. burnt VERB) was
decades earlier.
centuries (Lieberman et al, 2007). Figure 1 illus-
trates how burned gradually overtook burnt, becom-
ing more frequent around 1880. Unfortunately, as a
study of verb regularization, this analysis is skewed
by a significant confound: both words can serve
as either verbs (e.g., the house burnt) or adjectives
(e.g., the burnt toast). Because many words have
multiple syntactic interpretations, such confounds
often limit the utility of raw ngram frequency data.
In this work we provide a new edition of the
Google Books Ngram Corpus that contains over 8
million books, or 6% of all books ever published (cf.
Section 3). Moreover, we include syntactic anal-
ysis in order to facilitate a fine-grained analysis of
the evolution of syntax. Ngrams are annotated with
part-of-speech tags (e.g., in the phrase he burnt the
toast, burnt is a verb; in the burnt toast, burnt is an
adjective) and head-modifier dependencies (e.g., in
the phrase the little black book, little modifies book).
The annotated ngrams are far more useful for ex-
169
amining the evolution of grammar and syntax. For
our study of the regularization of the verb burn,
the availability of syntactic annotations resolves the
verb vs. adjective ambiguity in the original data, al-
lowing us to only examine instances where burnt
and burned appear as verbs. This more refined anal-
ysis suggests a crossover date for the frequency of
the verb forms that is several decades earlier than
the overall (verbs and adjectives) crossover.
We use state-of-the-art statistical part-of-speech
taggers and dependency parsers to produce syntac-
tic annotations for eight languages in the Google
Books collection. The annotations consist of 12 lan-
guage universal part-of-speech tags and unlabeled
head-modifier dependencies. Section 4 describes the
models that we used and the format of the annota-
tions in detail. We assess the expected annotation
accuracies experimentally and discuss how we adapt
the taggers and parsers to historical text in Section 5.
The annotated ngrams are available as a new edition
of the Google Books Ngram Corpus; we provide
some examples from the new corpus in Figure 3.
2 Related Work
Michel et al (2011) described the construction of
the first edition of the Google Books Ngram Corpus
and used it to quantitatively analyze a variety of top-
ics ranging from language growth to public health.
The related Ngram Viewer has become a popular
tool for examining language trends by experts and
non-experts alike.
In addition to studying frequency patterns in the
data, researchers have also attempted to analyze the
grammatical function of the ngrams (Davies, 2011).
Such endeavors are hampered by the fact that the
Ngram Corpus provides only aggregate statistics in
the form of ngram counts and not the full sen-
tences. Furthermore, only ngrams that pass certain
occurrence thresholds are publicly available, making
any further aggregation attempt futile: in heavy tail
distributions like the ones common in natural lan-
guages, the counts of rare events (that do not pass
the frequency threshold) can have a large cumula-
tive mass.
In contrast, because we have access to the full
text, we can annotate ngrams to reflect the particu-
lar grammatical functions they take in the sentences
Language #Volumes #Tokens
English 4,541,627 468,491,999,592
Spanish 854,649 83,967,471,303
French 792,118 102,174,681,393
German 657,991 64,784,628,286
Russian 591,310 67,137,666,353
Italian 305,763 40,288,810,817
Chinese 302,652 26,859,461,025
Hebrew 70,636 8,172,543,728
Table 1: Number of volumes and tokens for each lan-
guage in our corpus. The total collection contains more
than 6% of all books ever published.
they were extracted from, and can also account for
the contribution of rare ngrams to otherwise frequent
grammatical functions.
3 Ngram Corpus
The Google Books Ngram Corpus has been avail-
able at http://books.google.com/ngrams
since 2010. This work presents new corpora that
have been extracted from an even larger book collec-
tion, adds a new language (Italian), and introduces
syntactically annotated ngrams. The new corpora
are available in addition to the already existing ones.
3.1 Books Data
The new edition of the Ngram Corpus supports the
eight languages shown in Table 1. The book vol-
umes were selected from the larger collection of all
books digitized at Google following exactly the pro-
cedure described in Michel et al (2011). The new
edition contains data from 8,116,746 books, or over
6% of all books ever published. The English cor-
pus alone comprises close to half a trillion words.
This collection of books is much larger than any
other digitized collection; its generation required a
substantial effort involving obtaining and manually
scanning millions of books.
3.2 Raw Ngrams
We extract ngrams in a similar way to the first edi-
tion of the corpus (Michel et al, 2011), but with
some notable differences. Previously, tokenization
was done on whitespace characters and all ngrams
occurring on a given page were extracted, includ-
ing ones that span sentence boundaries, but omitting
170
Tag English Spanish French German Russian1 Italian Chinese Hebrew
ADJ other, such mayor, gran tous, me?me anderen, ersten vse,toProceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92?97,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Universal Dependency Annotation for Multilingual Parsing
Ryan McDonald? Joakim Nivre?? Yvonne Quirmbach-Brundage? Yoav Goldberg??
Dipanjan Das? Kuzman Ganchev? Keith Hall? Slav Petrov? Hao Zhang?
Oscar Ta?ckstro?m?? Claudia Bedini? Nu?ria Bertomeu Castello?? Jungmee Lee?
Google, Inc.? Uppsala University? Appen-Butler-Hill? Bar-Ilan University?
Contact: ryanmcd@google.com
Abstract
We present a new collection of treebanks
with homogeneous syntactic dependency
annotation for six languages: German,
English, Swedish, Spanish, French and
Korean. To show the usefulness of such a
resource, we present a case study of cross-
lingual transfer parsing with more reliable
evaluation than has been possible before.
This ?universal? treebank is made freely
available in order to facilitate research on
multilingual dependency parsing.1
1 Introduction
In recent years, syntactic representations based
on head-modifier dependency relations between
words have attracted a lot of interest (Ku?bler et
al., 2009). Research in dependency parsing ? com-
putational methods to predict such representations
? has increased dramatically, due in large part to
the availability of dependency treebanks in a num-
ber of languages. In particular, the CoNLL shared
tasks on dependency parsing have provided over
twenty data sets in a standardized format (Buch-
holz and Marsi, 2006; Nivre et al, 2007).
While these data sets are standardized in terms
of their formal representation, they are still hetero-
geneous treebanks. That is to say, despite them
all being dependency treebanks, which annotate
each sentence with a dependency tree, they sub-
scribe to different annotation schemes. This can
include superficial differences, such as the renam-
ing of common relations, as well as true diver-
gences concerning the analysis of linguistic con-
structions. Common divergences are found in the
1Downloadable at https://code.google.com/p/uni-dep-tb/.
analysis of coordination, verb groups, subordinate
clauses, and multi-word expressions (Nilsson et
al., 2007; Ku?bler et al, 2009; Zeman et al, 2012).
These data sets can be sufficient if one?s goal
is to build monolingual parsers and evaluate their
quality without reference to other languages, as
in the original CoNLL shared tasks, but there are
many cases where heterogenous treebanks are less
than adequate. First, a homogeneous represen-
tation is critical for multilingual language tech-
nologies that require consistent cross-lingual anal-
ysis for downstream components. Second, consis-
tent syntactic representations are desirable in the
evaluation of unsupervised (Klein and Manning,
2004) or cross-lingual syntactic parsers (Hwa et
al., 2005). In the cross-lingual study of McDonald
et al (2011), where delexicalized parsing models
from a number of source languages were evalu-
ated on a set of target languages, it was observed
that the best target language was frequently not the
closest typologically to the source. In one stun-
ning example, Danish was the worst source lan-
guage when parsing Swedish, solely due to greatly
divergent annotation schemes.
In order to overcome these difficulties, some
cross-lingual studies have resorted to heuristics to
homogenize treebanks (Hwa et al, 2005; Smith
and Eisner, 2009; Ganchev et al, 2009), but we
are only aware of a few systematic attempts to
create homogenous syntactic dependency anno-
tation in multiple languages. In terms of auto-
matic construction, Zeman et al (2012) attempt
to harmonize a large number of dependency tree-
banks by mapping their annotation to a version of
the Prague Dependency Treebank scheme (Hajic?
et al, 2001; Bo?hmova? et al, 2003). Addition-
ally, there have been efforts to manually or semi-
manually construct resources with common syn-
92
tactic analyses across multiple languages using al-
ternate syntactic theories as the basis for the repre-
sentation (Butt et al, 2002; Helmreich et al, 2004;
Hovy et al, 2006; Erjavec, 2012).
In order to facilitate research on multilingual
syntactic analysis, we present a collection of data
sets with uniformly analyzed sentences for six lan-
guages: German, English, French, Korean, Span-
ish and Swedish. This resource is freely avail-
able and we plan to extend it to include more data
and languages. In the context of part-of-speech
tagging, universal representations, such as that of
Petrov et al (2012), have already spurred numer-
ous examples of improved empirical cross-lingual
systems (Zhang et al, 2012; Gelling et al, 2012;
Ta?ckstro?m et al, 2013). We aim to do the same for
syntactic dependencies and present cross-lingual
parsing experiments to highlight some of the bene-
fits of cross-lingually consistent annotation. First,
results largely conform to our expectations of
which target languages should be useful for which
source languages, unlike in the study of McDon-
ald et al (2011). Second, the evaluation scores
in general are significantly higher than previous
cross-lingual studies, suggesting that most of these
studies underestimate true accuracy. Finally, un-
like all previous cross-lingual studies, we can re-
port full labeled accuracies and not just unlabeled
structural accuracies.
2 Towards A Universal Treebank
The Stanford typed dependencies for English
(De Marneffe et al, 2006; de Marneffe and Man-
ning, 2008) serve as the point of departure for our
?universal? dependency representation, together
with the tag set of Petrov et al (2012) as the under-
lying part-of-speech representation. The Stanford
scheme, partly inspired by the LFG framework,
has emerged as a de facto standard for depen-
dency annotation in English and has recently been
adapted to several languages representing different
(and typologically diverse) language groups, such
as Chinese (Sino-Tibetan) (Chang et al, 2009),
Finnish (Finno-Ugric) (Haverinen et al, 2010),
Persian (Indo-Iranian) (Seraji et al, 2012), and
Modern Hebrew (Semitic) (Tsarfaty, 2013). Its
widespread use and proven adaptability makes it a
natural choice for our endeavor, even though ad-
ditional modifications will be needed to capture
the full variety of grammatical structures in the
world?s languages.
Alexandre re?side avec sa famille a` Tinqueux .
NOUN VERB ADP DET NOUN ADP NOUN P
NSUBJ ADPMOD
ADPOBJ
POSS
ADPMOD
ADPOBJ
P
Figure 1: A sample French sentence.
We use the so-called basic dependencies (with
punctuation included), where every dependency
structure is a tree spanning all the input tokens,
because this is the kind of representation that most
available dependency parsers require. A sample
dependency tree from the French data set is shown
in Figure 1. We take two approaches to generat-
ing data. The first is traditional manual annotation,
as previously used by Helmreich et al (2004) for
multilingual syntactic treebank construction. The
second, used only for English and Swedish, is to
automatically convert existing treebanks, as in Ze-
man et al (2012).
2.1 Automatic Conversion
Since the Stanford dependencies for English are
taken as the starting point for our universal annota-
tion scheme, we begin by describing the data sets
produced by automatic conversion. For English,
we used the Stanford parser (v1.6.8) (Klein and
Manning, 2003) to convert the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993) to basic dependency trees, including punc-
tuation and with the copula verb as head in cop-
ula constructions. For Swedish, we developed a
set of deterministic rules for converting the Tal-
banken part of the Swedish Treebank (Nivre and
Megyesi, 2007) to a representation as close as pos-
sible to the Stanford dependencies for English.
This mainly consisted in relabeling dependency
relations and, due to the fine-grained label set used
in the Swedish Treebank (Teleman, 1974), this
could be done with high precision. In addition,
a small number of constructions required struc-
tural conversion, notably coordination, which in
the Swedish Treebank is given a Prague style anal-
ysis (Nilsson et al, 2007). For both English and
Swedish, we mapped the language-specific part-
of-speech tags to universal tags using the map-
pings of Petrov et al (2012).
2.2 Manual Annotation
For the remaining four languages, annotators were
given three resources: 1) the English Stanford
93
guidelines; 2) a set of English sentences with Stan-
ford dependencies and universal tags (as above);
and 3) a large collection of unlabeled sentences
randomly drawn from newswire, weblogs and/or
consumer reviews, automatically tokenized with a
rule-based system. For German, French and Span-
ish, contractions were split, except in the case of
clitics. For Korean, tokenization was more coarse
and included particles within token units. Annota-
tors could correct this automatic tokenization.
The annotators were then tasked with producing
language-specific annotation guidelines with the
expressed goal of keeping the label and construc-
tion set as close as possible to the original English
set, only adding labels for phenomena that do not
exist in English. Making fine-grained label dis-
tinctions was discouraged. Once these guidelines
were fixed, annotators selected roughly an equal
amount of sentences to be annotated from each do-
main in the unlabeled data. As the sentences were
already randomly selected from a larger corpus,
annotators were told to view the sentences in or-
der and to discard a sentence only if it was 1) frag-
mented because of a sentence splitting error; 2) not
from the language of interest; 3) incomprehensible
to a native speaker; or 4) shorter than three words.
The selected sentences were pre-processed using
cross-lingual taggers (Das and Petrov, 2011) and
parsers (McDonald et al, 2011).
The annotators modified the pre-parsed trees us-
ing the TrEd2 tool. At the beginning of the annota-
tion process, double-blind annotation, followed by
manual arbitration and consensus, was used itera-
tively for small batches of data until the guidelines
were finalized. Most of the data was annotated
using single-annotation and full review: one an-
notator annotating the data and another reviewing
it, making changes in close collaboration with the
original annotator. As a final step, all annotated
data was semi-automatically checked for annota-
tion consistency.
2.3 Harmonization
After producing the two converted and four an-
notated data sets, we performed a harmonization
step, where the goal was to maximize consistency
of annotation across languages. In particular, we
wanted to eliminate cases where the same label
was used for different linguistic relations in dif-
ferent languages and, conversely, where one and
2Available at http://ufal.mff.cuni.cz/tred/.
the same relation was annotated with different la-
bels, both of which could happen accidentally be-
cause annotators were allowed to add new labels
for the language they were working on. Moreover,
we wanted to avoid, as far as possible, labels that
were only used in one or two languages.
In order to satisfy these requirements, a number
of language-specific labels were merged into more
general labels. For example, in analogy with the
nn label for (element of a) noun-noun compound,
the annotators of German added aa for compound
adjectives, and the annotators of Korean added vv
for compound verbs. In the harmonization step,
these three labels were merged into a single label
compmod for modifier in compound.
In addition to harmonizing language-specific la-
bels, we also renamed a small number of relations,
where the name would be misleading in the uni-
versal context (although quite appropriate for En-
glish). For example, the label prep (for a mod-
ifier headed by a preposition) was renamed adp-
mod, to make clear the relation to other modifier
labels and to allow postpositions as well as prepo-
sitions.3 We also eliminated a few distinctions in
the original Stanford scheme that were not anno-
tated consistently across languages (e.g., merging
complm with mark, number with num, and purpcl
with advcl).
The final set of labels is listed with explanations
in Table 1. Note that relative to the universal part-
of-speech tagset of Petrov et al (2012) our final
label set is quite rich (40 versus 12). This is due
mainly to the fact that the the former is based on
deterministic mappings from a large set of annota-
tion schemes and therefore reduced to the granu-
larity of the greatest common denominator. Such a
reduction may ultimately be necessary also in the
case of dependency relations, but since most of our
data sets were created through manual annotation,
we could afford to retain a fine-grained analysis,
knowing that it is always possible to map from
finer to coarser distinctions, but not vice versa.4
2.4 Final Data Sets
Table 2 presents the final data statistics. The num-
ber of sentences, tokens and tokens/sentence vary
3Consequently, pobj and pcomp were changed to adpobj
and adpcomp.
4The only two data sets that were created through con-
version in our case were English, for which the Stanford de-
pendencies were originally defined, and Swedish, where the
native annotation happens to have a fine-grained label set.
94
Label Description
acomp adjectival complement
adp adposition
adpcomp complement of adposition
adpmod adpositional modifier
adpobj object of adposition
advcl adverbial clause modifier
advmod adverbial modifier
amod adjectival modifier
appos appositive
attr attribute
aux auxiliary
auxpass passive auxiliary
cc conjunction
ccomp clausal complement
Label Description
compmod compound modifier
conj conjunct
cop copula
csubj clausal subject
csubjpass passive clausal subject
dep generic
det determiner
dobj direct object
expl expletive
infmod infinitival modifier
iobj indirect object
mark marker
mwe multi-word expression
neg negation
Label Description
nmod noun modifier
nsubj nominal subject
nsubjpass passive nominal subject
num numeric modifier
p punctuation
parataxis parataxis
partmod participial modifier
poss possessive
prt verb particle
rcmod relative clause modifier
rel relative
xcomp open clausal complement
Table 1: Harmonized label set based on Stanford dependencies (De Marneffe et al, 2006).
source(s) # sentences # tokens
DE N, R 4,000 59,014
EN PTB? 43,948 1,046,829
SV STB? 6,159 96,319
ES N, B, R 4,015 112,718
FR N, B, R 3,978 90,000
KO N, B 6,194 71,840
Table 2: Data set statistics. ?Automatically con-
verted WSJ section of the PTB. The data release
includes scripts to generate this data, not the data
itself. ?Automatically converted Talbanken sec-
tion of the Swedish Treebank. N=News, B=Blogs,
R=Consumer Reviews.
due to the source and tokenization. For example,
Korean has 50% more sentences than Spanish, but
?40k less tokens due to a more coarse-grained to-
kenization. In addition to the data itself, anno-
tation guidelines and harmonization rules are in-
cluded so that the data can be regenerated.
3 Experiments
One of the motivating factors in creating such a
data set was improved cross-lingual transfer eval-
uation. To test this, we use a cross-lingual transfer
parser similar to that of McDonald et al (2011).
In particular, it is a perceptron-trained shift-reduce
parser with a beam of size 8. We use the features
of Zhang and Nivre (2011), except that all lexical
identities are dropped from the templates during
training and testing, hence inducing a ?delexical-
ized? model that employs only ?universal? proper-
ties from source-side treebanks, such as part-of-
speech tags, labels, head-modifier distance, etc.
We ran a number of experiments, which can be
seen in Table 3. For these experiments we ran-
domly split each data set into training, develop-
ment and testing sets.5 The one exception is En-
glish, where we used the standard splits. Each
row in Table 3 represents a source training lan-
guage and each column a target evaluation lan-
guage. We report both unlabeled attachment score
(UAS) and labeled attachment score (LAS) (Buch-
holz and Marsi, 2006). This is likely the first re-
liable cross-lingual parsing evaluation. In partic-
ular, previous studies could not even report LAS
due to differences in treebank annotations.
We can make several interesting observations.
Most notably, for the Germanic and Romance tar-
get languages, the best source language is from
the same language group. This is in stark contrast
to the results of McDonald et al (2011), who ob-
serve that this is rarely the case with the heteroge-
nous CoNLL treebanks. Among the Germanic
languages, it is interesting to note that Swedish
is the best source language for both German and
English, which makes sense from a typological
point of view, because Swedish is intermediate be-
tween German and English in terms of word or-
der properties. For Romance languages, the cross-
lingual parser is approaching the accuracy of the
supervised setting, confirming that for these lan-
guages much of the divergence is lexical and not
structural, which is not true for the Germanic lan-
guages. Finally, Korean emerges as a very clear
outlier (both as a source and as a target language),
which again is supported by typological consider-
ations as well as by the difference in tokenization.
With respect to evaluation, it is interesting to
compare the absolute numbers to those reported
in McDonald et al (2011) for the languages com-
5These splits are included in the release of the data.
95
Source
Training
Language
Target Test Language
Unlabeled Attachment Score (UAS) Labeled Attachment Score (LAS)
Germanic Romance Germanic Romance
DE EN SV ES FR KO DE EN SV ES FR KO
DE 74.86 55.05 65.89 60.65 62.18 40.59 64.84 47.09 53.57 48.14 49.59 27.73
EN 58.50 83.33 70.56 68.07 70.14 42.37 48.11 78.54 57.04 56.86 58.20 26.65
SV 61.25 61.20 80.01 67.50 67.69 36.95 52.19 49.71 70.90 54.72 54.96 19.64
ES 55.39 58.56 66.84 78.46 75.12 30.25 45.52 47.87 53.09 70.29 63.65 16.54
FR 55.05 59.02 65.05 72.30 81.44 35.79 45.96 47.41 52.25 62.56 73.37 20.84
KO 33.04 32.20 27.62 26.91 29.35 71.22 26.36 21.81 18.12 18.63 19.52 55.85
Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
mon to both studies (DE, EN, SV and ES). In that
study, UAS was in the 38?68% range, as compared
to 55?75% here. For Swedish, we can even mea-
sure the difference exactly, because the test sets
are the same, and we see an increase from 58.3%
to 70.6%. This suggests that most cross-lingual
parsing studies have underestimated accuracies.
4 Conclusion
We have released data sets for six languages with
consistent dependency annotation. After the ini-
tial release, we will continue to annotate data in
more languages as well as investigate further au-
tomatic treebank conversions. This may also lead
to modifications of the annotation scheme, which
should be regarded as preliminary at this point.
Specifically, with more typologically and morpho-
logically diverse languages being added to the col-
lection, it may be advisable to consistently en-
force the principle that content words take func-
tion words as dependents, which is currently vi-
olated in the analysis of adpositional and copula
constructions. This will ensure a consistent analy-
sis of functional elements that in some languages
are not realized as free words or are not obliga-
tory, such as adpositions which are often absent
due to case inflections in languages like Finnish. It
will also allow the inclusion of language-specific
functional or morphological markers (case mark-
ers, topic markers, classifiers, etc.) at the leaves of
the tree, where they can easily be ignored in appli-
cations that require a uniform cross-lingual repre-
sentation. Finally, this data is available on an open
source repository in the hope that the community
will commit new data and make corrections to ex-
isting annotations.
Acknowledgments
Many people played critical roles in the pro-
cess of creating the resource. At Google, Fer-
nando Pereira, Alfred Spector, Kannan Pashu-
pathy, Michael Riley and Corinna Cortes sup-
ported the project and made sure it had the re-
quired resources. Jennifer Bahk and Dave Orr
helped coordinate the necessary contracts. Andrea
Held, Supreet Chinnan, Elizabeth Hewitt, Tu Tsao
and Leigha Weinberg made the release process
smooth. Michael Ringgaard, Andy Golding, Terry
Koo, Alexander Rush and many others provided
technical advice. Hans Uszkoreit gave us per-
mission to use a subsample of sentences from the
Tiger Treebank (Brants et al, 2002), the source of
the news domain for our German data set. Anno-
tations were additionally provided by Sulki Kim,
Patrick McCrae, Laurent Alamarguy and He?ctor
Ferna?ndez Alcalde.
References
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Parsed Cor-
pora, pages 103?127. Kluwer.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The parallel grammar project. In Proceedings of
the 2002 workshop on Grammar engineering and
evaluation-Volume 15.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009.
96
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Marie-Catherine De Marneffe, Bill MacCartney, and
Chris D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Pro-
ceedings of LREC.
Tomaz Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46:131?142.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Grac?a. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure.
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,
Eva Hajic?ova?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking finnish. In Proceedings of
The Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9).
Stephen Helmreich, David Farwell, Bonnie Dorr, Nizar
Habash, Lori Levin, Teruko Mitamura, Florence
Reeder, Keith Miller, Eduard Hovy, Owen Rambow,
and Advaith Siddharthan. 2004. Interlingual anno-
tation of multilingual text corpora. In Proceedings
of the HLT-EACL Workshop on Frontiers in Corpus
Annotation.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of ACL.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of ACL.
Joakim Nivre and Bea?ta Megyesi. 2007. Bootstrap-
ping a Swedish treebank using cross-corpus harmo-
nization and annotation projection. In Proceedings
of the 6th International Workshop on Treebanks and
Linguistic Theories.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Mojgan Seraji, Bea?ta Megyesi, and Nivre Joakim.
2012. Bootstrapping a Persian dependency tree-
bank. Linguistic Issues in Language Technology,
7(18):1?10.
David A. Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the ACL.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivn-
ing av talad och skriven svenska. Studentlitteratur.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. Proceedings of
ACL.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan S?tepa?nek, Zdene?k
Z?abokrtsky`, and Jan Hajic. 2012. Hamledt: To
parse or not to parse. In Proceedings of LREC.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a universal
pos tagset. In Proceedings of EMNLP.
97
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 115?120,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Enhanced Search with Wildcards and Morphological Inflections
in the Google Books Ngram Viewer
Jason Mann
??
David Zhang
??
Lu Yang
??
Dipanjan Das
?
Slav Petrov
?
?
Columbia University
?
USC
?
Cornell University
?
Google Inc.
Contact: dipanjand@google.com, slav@google.com
Abstract
We present a new version of the Google
Books Ngram Viewer, which plots the fre-
quency of words and phrases over the last
five centuries; its data encompasses 6%
of the world?s published books. The new
Viewer adds three features for more pow-
erful search: wildcards, morphological in-
flections, and capitalization. These addi-
tions allow the discovery of patterns that
were previously difficult to find and fur-
ther facilitate the study of linguistic trends
in printed text.
1 Introduction
The Google Books Ngram project facilitates the
analysis of cultural, social and linguistic trends
through five centuries of written text in eight
languages. The Ngram Corpus (Michel et al.,
2011; Lin et al., 2012) consists of words and
phrases (i.e., ngrams) and their usage frequency
over time.
1
The interactive Ngram Viewer
2
allows
users to retrieve and plot the frequency of mul-
tiple ngrams on a simple webpage. The Viewer
is widely popular and can be used to efficiently
explore and visualize patterns in the underlying
ngram data. For example, the ngram data has
been used to detect emotion trends in 20th cen-
tury books (Acerbi et al., 2013), to analyze text
focusing on market capitalism throughout the past
century (Schulz and Robinson, 2013), detect so-
cial and cultural impact of historical personalities
(Skiena and Ward, 2013), or to analyze the corre-
lation of economic crises with a literary ?misery
?
The majority of this work was carried out during an
internship at Google.
1
The Ngram Corpus is freely available for download at
http://books.google.com/ngrams/datasets.
2
See http://books.google.com/ngrams.
1930 1965 2000
Rel
ativ
e Fr
equ
enc
y
Query: "President Kennedy, President Reagan, President Nixon"
"President Kennedy"
"President Reagan"
"President Nixon"
Figure 1: Mention frequencies for three different American
presidents queried one-by-one.
index? reflected in printed text during crises peri-
ods (Bentley et al., 2014).
A limitation of the Viewer, however, is that all
the reasoning has to be done by the user, and
only individual, user-specified ngrams can be re-
trieved and plotted. For example, to compare
the popularity of different presidents, one needs
to come up with a list of presidents and then
search for them one-by-one. The result of the
query ?President Kennedy, President
Nixon, President Reagan? is shown in
Figure 1. To determine the most popular president,
one would need to search for all presidents, which
is cumbersome and should ideally be automated.
In this paper, we therefore present an updated
version of the Viewer that enhances its search
functionality. We introduce three new features
that automatically expand a given query and re-
trieve a collection of ngrams, to facilitate the dis-
covery of patterns in the underlying data. First,
users can replace one query term with a place-
holder symbol ?
*
? (wildcard, henceforth), which
will return the ten most frequent expansions of
the wildcard in the corpus for the specified year
range. Second, by adding a specific marker to
any word in a query (? INF?), ngrams with all
115
morphological inflections of that word will be re-
trieved. Finally, the new Viewer supports capi-
talization searches, which return all capitalization
variants of the query ngram. Figure 2 provides ex-
amples for these three new types of queries.
While it is fairly obvious how the above search
features can be implemented via brute-force com-
putation, supporting an interactive application
with low latency necessitates some precomputa-
tion. In particular, the wildcard search feature
poses some challenges because the most frequent
expansions depend on the selected year range
(consider the frequency with which presidents are
mentioned during different decades, for example).
To this end, we provide details of our system ar-
chitecture in ?2 and discuss how the new search
features are implemented in ?3. In addition, we
present an overhaul of the Ngram Viewer?s user
interface with interactive features that allow for
easier management of the increase in data points
returned.
Detailed analysis and interpretation of trends
uncovered with the new search interface is beyond
the scope of this paper. We highlight some in-
teresting use cases in ?4; many of the presented
queries were difficult (or impossible) to execute in
the previous versions of the system. We emphasize
that this demonstration updates only the Viewer,
providing tools for easier analysis of the underly-
ing corpora. The ngram corpora themselves are
not updated.
2 System Overview
We first briefly review the two editions of the
Ngram Corpus (Michel et al., 2011; Lin et al.,
2012) and then describe the extensions to the ar-
chitecture of the Viewer that are needed to support
the new search features.
2.1 The Ngram Corpus
The Google Books Ngram Corpus provides ngram
counts for eight different languages over more
than 500 years; additionally, the English corpus
is split further into British vs. American English
and Fiction to aid domain-specific analysis. This
corpus is a subset of all books digitized at Google
and represents more than 6% of all publicized texts
(Lin et al., 2012). Two editions of the corpus are
available: the first edition dates from 2009 and is
described in Michel et al. (2011); the second edi-
tion is from 2012 and is described in Lin et al.
1900 1950 2000
Re
lat
ive
 F
req
ue
nc
y
Query: "University of *"
University of California
University of Chicago
University of Wisconsin
University of Michigan
University of Pennsylvania
1950 1975 2000
Re
lat
ive
 F
req
ue
nc
y
Query: "book_INF a hotel"
book a hotel
booked a hotel
booking a hotel
books a hotel
1800 1900 2000
Re
lat
ive
 F
req
ue
nc
y
Query: "fitzgerald [case-insensitive]"
Fitzgerald
FitzGerald
FITZGERALD
Figure 2: In the new enhanced search features of the Ngram
Viewer, a single query is automatically expanded to retrieve
multiple related ngrams. From top to bottom, we show ex-
amples of the wildcard operator (?
*
?), the ? INF? marker that
results in morphological inflections, and the case insensitive
search functionality. Due to space considerations we show
only a subset of the results returned by the Ngram Viewer.
(2012). The new search features presented here
are available for both editions.
Michel et al. (2011) extract ngrams for each
page in isolation. More specifically, they use
whitespace tokenization and extract all ngrams up
to length five. These ngrams include ones that po-
tentially span sentence boundaries, but do not in-
clude ngrams that span across page breaks (even
when they are part of the same sentence). Lin
et al. (2012) on the other hand perform tokeniza-
tion, text normalization and segmentation into sen-
tences. They then add synthetic START and
END tokens to the beginning and end of the sen-
116
tences to enable the distinction of sentence me-
dial ngrams from those near sentence boundaries.
They also ensure that sentences that span across
page boundaries are included. Due to these dif-
ferences, as well as the availability of additional
book data, improvements to the optical character
recognition algorithms and metadata extraction for
dating the books, the ngrams counts from the two
editions are not the same.
The edition from Lin et al. (2012) additionally
includes syntactic ngrams. The corpus is tagged
using the universal part-of-speech (POS) tag set
of Petrov et al. (2012): NOUN (nouns), VERB
(verbs), ADJ (adjectives), ADV (adverbs), PRON
(pronouns), DET (determiners and articles), ADP
(prepositions and postpositions), CONJ (conjunc-
tions). Words can be disambiguated by their POS
tag by simply appending the tag to the word with
an underscore (e.g. book NOUN) and can also be
replaced by POS tags in the ngrams, see Lin et
al. (2012) for details. The corpus is parsed with
a dependency parser and head-modifier syntactic
relations between words in the same sentence are
extracted. Dependency relations are represented
as ?=>? in the corpus. Our new enhanced search
features for automatic expansions can also be ap-
plied to these syntactic ngrams. In fact, some of
the most interesting queries use expansions to au-
tomatically uncover related ngrams, while using
syntax to focus on particular patterns.
The Viewer supports the composition of ngram
frequencies via arithmetic operators. Addition (+),
subtraction (-) and division (/) of ngrams are car-
ried out on a per year basis, while multiplication
(
*
) is performed by a scalar that is applied to all
counts in the time series. Where ambiguous, the
wildcard operator takes precedence over the mul-
tiplication operator. Parentheses can be used to
disambiguate and to force the interpretation of a
mathematical operation.
2.2 Architecture
The Ngram Viewer provides a lightweight inter-
face to the underlying ngram corpora. In its basic
form, user requests are directed through the server
to a simple lookup table containing the raw ngrams
and their frequencies. This data flow is displayed
in the top part of Figure 3 and is maintained for
queries that do not involve the new expansion fea-
tures introduced in this work.
The expansion queries could in principle be
Raw Ngrams
?King James? :
{(1900, 234), 
(1901, 122), ?}
?Kinged James?: 
{(1900, 20), 
(1901, 15), ?}
?
Inflections
?King_INF?: 
{King, Kinged, 
Kings,
 ? }
Wildcards
?King *?:
{King James,
 King George,
 ? }
Capitalizations
?king james?: 
{king James, 
King James,
? }
Ngram 
Viewer
Server
User
new in this version
Ngram Viewer System Architecture
Figure 3: Overview of the Ngram Viewer architecture.
implemented by scanning the raw ngrams on
the fly and returning the matching subset: to
answer the query ?President
*
?, one would
need to obtain all bigrams starting with the word
President (there are 23,693) and extract the
most frequent ten. Given the large number of
ngrams (especially for larger n), such an approach
turns out to be too slow for an interactive appli-
cation. We therefore pre-compute intermediate re-
sults that can be used to more efficiently retrieve
the results for expansion queries. The intermedi-
ate results are stored in additional lookup tables
(shown at the bottom in Figure 3). When the user
executes an expansion search, the query is first
routed to the appropriate lookup table which stores
all possible expansions (including expansions that
might not appear in the corpus). These expanded
ngrams are then retrieved from the raw ngram ta-
ble, sorted by frequency and returned to he user.
We describe the intermediate results tables and
how they are generated in the next section.
Note that we only support one expansion oper-
ation per query ngram. This is needed in order to
avoid the combinatorial explosion that would re-
sult from mixing multiple expansion operators in
the same query.
3 New Features
The three new search features are implemented via
the same two-step approach. As shown in Fig-
ure 3, we add three new lookup tables that store
intermediate results needed for efficiently support-
117
1800 1900 2000
Rela
tive 
Freq
uenc
y
Query: "President *"
President ofPresident ?sPresident andPresident toPresident Roosevelt
1800 1900 2000
Rela
tive 
Freq
uenc
y
Query: "President *_NOUN, 1800-2000"
President Roosevelt_NOUNPresident Wilson_NOUNPresident Lincoln_NOUNPresident Johnson_NOUNPresident Truman_NOUN
1950 1975 2000
Rela
tive 
Freq
uenc
y
Query: "President *_NOUN, 1950-2000"
President Roosevelt_NOUNPresident Truman_NOUNPresident Kennedy_NOUNPresident Johnson_NOUNPresident Eisenhower_NOUN
Figure 4: Different wildcard queries for bigrams starting with President. Specification of a POS tag along with the wildcard
operator results in more specific results, and the results vary depending on the selected yaer range.
ing the new search types. In all cases the lookup
tables provide a set of possible expansions that are
then retrieved in the original raw ngram table. Be-
low we describe how these intermediate results are
generated and how they are used to retrieve the fi-
nal results.
3.1 Wildcards
Wildcards provide a convenient way to automat-
ically retrieve and explore related ngrams. Be-
cause of the large number of possibilities that can
fill a wildcard slot, returning anything but the top
few expansions is likely to be overwhelming. We
therefore return only the ten most frequent expan-
sions. Determining the most frequent expansions
is unfortunately computationally very expensive
because of the large number of ngrams; the query
?the
*
? for example has 2,353,960 expansions.
To avoid expensive on-the-fly computations,
we precompute the most frequent expansions for
all possible queries. The problem that arises
is that the ten most frequent expansions depend
on the selected year range. Consider the query
?President
*
?; we would like to be able get
the correct result for any year range. Since our
data spans more than 500 years, precomputing the
results for all year ranges is not a possibility. In-
stead, we compute the possible wildcard expan-
sions for each year. The top expansions for the
entire range are then taken from the union of top
expansions for each year. This set is at most of
size 10n (where n is the year range) and in practice
typically a lot smaller. Theoretically it is possible
for this approximation to miss an expansion that is
never among the top ten for a particular year, but
is cumulatively in the top ten for the entire range.
This would happen if there were many spikes in
the data, which is not the case.
To make the wildcard expansions more rele-
vant, we filter expansions that consist entirely of
punctuation symbols. To further narrow down
the expansions and focus on particular patterns,
we allow wildcards to be qualified via POS
tags. Figure 4 shows some example wildcard
queries involving bigrams that start with the word
?President.? See also Table 1 for some addi-
tional examples. Note that it is possible to replace
POS tags with wildcards (e.g., cook
*
) which
will find all POS tags that the query word can take.
3.2 Morphological Inflections
When comparing ngram frequencies (especially
across languages, but also for the same language),
it can be useful to examine and potentially aggre-
gate the frequencies of all inflected forms. This
can be accomplished by manually deriving all in-
flected forms and then using arithmetic operations
to aggregate their counts. Our new inflected form
search accomplishes this automatically. By ap-
pending the keyword INF to a word, a set of
ngrams with all inflected forms of the word will
be retrieved. To generate the inflected forms we
make use of Wiktionary
3
and supplement it with
automatically generated inflection tables based on
the approach of Durrett and DeNero (2013).
Because there are at most a few dozen inflected
forms for any given word, we can afford to sub-
stitute and retrieve all inflections of the marked
word, even the ones that are not grammatical in a
given ngram context. This has the advantage that
we only need to store inflected forms for individ-
ual words rather than entire ngrams. If a generated
ngram has no support in the corpus, we simply
omit it from the final set of results. We do not per-
form any additional filtering; as a result, an inflec-
tion search can produce many results, especially
for morphologically rich languages like Russian.
We have therefore updated the user interface to
better deal with many data lines (?4).
3
See http://www.wiktionary.org/. Because
Wiktionary is an evolving resource, results for a particular
query may change over time.
118
Query Possible Replacements
*
?s Theorem
Lagrange ?s Theorem, Gauss ?s Theorem,
Euler ?s Theorem, Pascal ?s Theorem
War=>
*
NOUN
War=>World NOUN, War=>Civil NOUN,
War=>Second NOUN, War=>Cold NOUN
lubov~ INF lubil, lublu, lubit, lubit~, lubila, lubimyi?, lubix~
book INF book, books, booked, booking
book INF NOUN book, books
cook
*
cook NOUN, cook VERB
the cook (case insensitive)
THE COOK, the cook, The Cook, the Cook, The cook
Table 1: Examples expansions for wildcard, inflection, and capitalization queries.
3.3 Capitalization
By aggregating different capitalizations of the
same word, one can normalize between sentence-
initial and sentence-medial occurrences of a given
word. A simple way to accomplish this is by
searching for a lowercased, capitalized and all
caps spelling of the query. This however can miss
CamelCase spelling and other capitalization vari-
ants (consider FitzGerald for example). It is
of course not feasible to try all case variants of ev-
ery letter in the query. Instead, we perform an of-
fline precomputation step in which we collect all
ngrams that map to the same lowercased string.
Due to scanning errors and spelling mistakes there
can be many extremely rare capitalization variants
for a given query. We therefore filter out all vari-
ants that have a cumulative count of less than 1%
of the most frequent variant for a given year range.
Capitalization searches are enabled by selecting a
case-insensitive check box on the new interface.
4 Use Cases
The three features introduced in this paper repre-
sent a major extension of the capabilities of the
Ngram Viewer. While the second edition of the
Ngram Corpus (Lin et al., 2012) introduced syn-
tactic ngrams, the functionality of the Viewer had
remained largely unchanged since its first launch
five years ago. Together, the updated Corpus and
Viewer enable a much more detailed analysis of
the underlying data. Below we provide some uses
cases highlighting the ways in which sophisticated
queries can be crafted. While the results produce
some intriguing patterns, we leave their analysis to
the experts.
Since we have made no modifications to the un-
derlying raw ngrams, all of the plots in this pa-
per could have also been generated with the pre-
vious version of the Viewer. They would, how-
ever, have required the user to manually generate
and issue all query terms. For example, Figure 1
shows manually created queries searching for spe-
cific presidents; contrarily, Figure 4 shows single
wildcard queries that automatically retrieve the ten
most frequently mentioned presidents and uncover
additional trends that would have required extra
work on behalf of the user.
The wildcard feature used on its own can be a
powerful tool for the analysis of top expansions
for a certain context. Although already useful on
its own, it becomes really powerful when com-
bined with POS tags. The user can attach an un-
derscore and POS tag to either a wildcard-based
or inflection-based query to specify that the ex-
pansions returned should be of a specific part of
speech. Compare the utility of the generic wild-
card and a search with a noun part-of-speech spec-
ification in a query examining president names,
?President
*
? vs. ?President
*
NOUN?
shown in Figure 4. The former gives a mixture
of prepositions, particles, and verbs along with
names of presidents, and because the latter spec-
ifies the noun tag, the top expansions turn out to
be names and more in line with the intention of
the search. Also, note in Figure 4 the difference in
expansions that searching over two different time
ranges provides. In Table 2, we compare the com-
bination of the wildcard feature with the existing
dependency link feature to highlight a comparison
of context across several languages.
It is worth noting that the newly introduced fea-
tures could result in many lines in the resulting
chart. Hence, we have updated the Viewer?s user
interface to better handle charts involving many
ngrams. The new interactive functionality allows
the user to highlight a line by hovering over it,
keep that focus by left clicking, and clear all fo-
cused lines by double clicking. A right click on
any of the expansions returned by an issued query
combines them into the year-wise sum total of all
the expansions. We added another feature to the
119
1700 1850 2000
Rela
tive
 Fre
que
ncy
Query: "light_INF"
"light"
"lights"
"lighted"
"lighter"
"lit"
"lighting"
"lightest"
1700 1850 2000
Rela
tive
 Fre
que
ncy
Query: "light_VERB_INF"
"light_VERB"
"lighted_VERB"
"lit_VERB"
"lighting_VERB"
"lights_VERB"
Figure 5: Comparison of specification of POS tag in wildcard search.
English American British
German French Russian Italian
Chinese
Spanish Hebrew
(All) English English (Simplified)
drinks drinks drinks trinkt boit p~t beve ? bebe dzy
water water water Bier (beer) vin (wine) on (he) vino (wine) ? (wine) agua (water) oii (wine)
wine wine wine Kaffee (coffee) sang (blood) qai? (tea) acqua (water) ? (tea) vino (wine) min (water)
milk coffee tea Wein (wine) eau (water) vodu (water) sangue (blood) ? (water) sangre (blood) d (the)
coffee beer blood Wasser (water) cafe (coffee) On (He) birra (beer) ?? (coffee) vaso (glass) qek (cup)
beer milk beer Tee (tea) verre (glass) vino (wine) caff?e (coffee) ? (person) cerveza (beer) dz (tea)
Table 2: Comparison of the top modifiers of the verb drinks, or its equivalent in translation, in all corpora, retrieved via
the query drinks VERB=>
*
NOUN and equivalents in the other languages. The modifiers can appear both in subject and in
object position because we have access only to unlabeled dependencies.
interface that creates static URLs maintaining all
the raw ngrams retrieved from any query. This pre-
vents statically linked charts from changing over
time, and allowing for backwards compatibility.
One of the primary benefits of the capitalization
feature is the combination of multiple searches
in one, which allows the user to compare case-
insensitive usages of two different phrases. An
alternative use is in Figure 2(c), where capitaliza-
tion search allows the immediate identification of
changing orthographic usage of a word or phrase;
in this case the figure shows the arrival of F. Scott
Fitzgerald in the early to mid 20th century, as well
as the rise in popularity of the CamelCase variety
of his surname at the turn of the 19th century.
Searches using inflections can be useful for the
same reasons as the capitalization feature, and also
be used to compare changes in spelling; it is par-
ticularly useful for the analysis of irregular verbs,
where the query can return both the regular and
irregular forms of a verb.
5 Conclusions
We have presented an update to the Ngram Viewer
that introduces new search features. Users can
now perform more powerful searches that auto-
matically uncover trends which were previously
difficult or impossible to extract. We look forward
to seeing what users of the Viewer will discover.
6 Acknowledgements
We would like to thank John DeNero, Jon Orwant,
Karl Moritz Hermann for many useful discussions.
References
A. Acerbi, V. Lampos, and R. A. Bentley. 2013. Ro-
bustness of emotion extraction from 20th century en-
glish books. In Proceedings of the IEEE Interna-
tional Conference on Big Data.
A. R. Bentley, A. Acerbi, P. Ormerod, and V. Lampos.
2014. Books average previous decade of economic
misery. PLOS One, 9(1).
G. Durrett and J. DeNero. 2013. Supervised learning
of complete morphological paradigms. In Proceed-
ings of NAACL-HLT.
Y. Lin, J.-B. Michel, E. L. Aiden, J. Orwant, W. Brock-
man, and S. Petrov. 2012. Syntactic annotations for
the Google Books Ngram Corpus. In Proceedings
of the ACL.
J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres,
M. K. Gray, The Google Books Team, J. P. Pick-
ett, D. Hoiberg, D. Clancy, P. Norvig, J. Orwant,
S. Pinker, M. A. Nowak, and E. Lieberman Aiden.
2011. Quantitative analysis of culture using millions
of digitized books. Science.
S. Petrov, D. Das, and R. McDonald. 2012. A univer-
sal part-of-speech tagset. In Proc. of LREC.
J. Schulz and L. Robinson. 2013. Shifting grounds and
evolving battlegrounds. American Journal of Cul-
tural Sociology, 1(3):373?402.
S. Skiena and C. Ward. 2013. Who?s Bigger?: Where
Historical Figures Really Rank. Cambridge Univer-
sity Press.
120
Transactions of the Association for Computational Linguistics, 1 (2013) 1?12. Action Editor: Sharon Goldwater.
Submitted 11/2012; Revised 1/2013; Published 3/2013. c?2013 Association for Computational Linguistics.
Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging
Oscar Ta?ckstro?m?? Dipanjan Das? Slav Petrov? Ryan McDonald? Joakim Nivre??
 Swedish Institute of Computer Science
?Department of Linguistics and Philology, Uppsala University
?Google Research, New York
oscar@sics.se
{dipanjand|slav|ryanmcd}@google.com
joakim.nivre@lingfil.uu.se
Abstract
We consider the construction of part-of-speech
taggers for resource-poor languages. Recently,
manually constructed tag dictionaries from
Wiktionary and dictionaries projected via bitext
have been used as type constraints to overcome
the scarcity of annotated data in this setting.
In this paper, we show that additional token
constraints can be projected from a resource-
rich source language to a resource-poor target
language via word-aligned bitext. We present
several models to this end; in particular a par-
tially observed conditional random field model,
where coupled token and type constraints pro-
vide a partial signal for training. Averaged
across eight previously studied Indo-European
languages, our model achieves a 25% relative
error reduction over the prior state of the art.
We further present successful results on seven
additional languages from different families,
empirically demonstrating the applicability of
coupled token and type constraints across a
diverse set of languages.
1 Introduction
Supervised part-of-speech (POS) taggers are avail-
able for more than twenty languages and achieve ac-
curacies of around 95% on in-domain data (Petrov et
al., 2012). Thanks to their efficiency and robustness,
supervised taggers are routinely employed in many
natural language processing applications, such as syn-
tactic and semantic parsing, named-entity recognition
and machine translation. Unfortunately, the resources
required to train supervised taggers are expensive to
create and unlikely to exist for the majority of written
?Work primarily carried out while at Google Research.
languages. The necessity of building NLP tools for
these resource-poor languages has been part of the
motivation for research on unsupervised learning of
POS taggers (Christodoulopoulos et al, 2010).
In this paper, we instead take a weakly supervised
approach towards this problem. Recently, learning
POS taggers with type-level tag dictionary constraints
has gained popularity. Tag dictionaries, noisily pro-
jected via word-aligned bitext, have bridged the gap
between purely unsupervised and fully supervised
taggers, resulting in an average accuracy of over 83%
on a benchmark of eight Indo-European languages
(Das and Petrov, 2011). Li et al (2012) further im-
proved upon this result by employing Wiktionary1 as
a tag dictionary source, resulting in the hitherto best
published result of almost 85% on the same setup.
Although the aforementioned weakly supervised
approaches have resulted in significant improvements
over fully unsupervised approaches, they have not
exploited the benefits of token-level cross-lingual
projection methods, which are possible with word-
aligned bitext between a target language of interest
and a resource-rich source language, such as English.
This is the setting we consider in this paper (?2).
While prior work has successfully considered both
token- and type-level projection across word-aligned
bitext for estimating the model parameters of genera-
tive tagging models (Yarowsky and Ngai, 2001; Xi
and Hwa, 2005, inter alia), a key observation under-
lying the present work is that token- and type-level
information offer different and complementary sig-
nals. On the one hand, high confidence token-level
projections offer precise constraints on a tag in a
particular context. On the other hand, manually cre-
1http://www.wiktionary.org/.
1
ated type-level dictionaries can have broad coverage
and do not suffer from word-alignment errors; they
can therefore be used to filter systematic as well as
random noise in token-level projections.
In order to reap these potential benefits, we pro-
pose a partially observed conditional random field
(CRF) model (Lafferty et al, 2001) that couples to-
ken and type constraints in order to guide learning
(?3). In essence, the model is given the freedom to
push probability mass towards hypotheses consistent
with both types of information. This approach is flex-
ible: we can use either noisy projected or manually
constructed dictionaries to generate type constraints;
furthermore, we can incorporate arbitrary features
over the input. In addition to standard (contextual)
lexical features and transition features, we observe
that adding features from a monolingual word cluster-
ing (Uszkoreit and Brants, 2008) can significantly im-
prove accuracy. While most of these features can also
be used in a generative feature-based hidden Markov
model (HMM) (Berg-Kirkpatrick et al, 2010), we
achieve the best accuracy with a globally normalized
discriminative CRF model.
To evaluate our approach, we present extensive
results on standard publicly available datasets for 15
languages: the eight Indo-European languages pre-
viously studied in this context by Das and Petrov
(2011) and Li et al (2012), and seven additional lan-
guages from different families, for which no compa-
rable study exists. In ?4 we compare various features,
constraints and model types. Our best model uses
type constraints derived from Wiktionary, together
with token constraints derived from high-confidence
word alignments. When averaged across the eight
languages studied by Das and Petrov (2011) and Li
et al (2012), we achieve an accuracy of 88.8%. This
is a 25% relative error reduction over the previous
state of the art. Averaged across all 15 languages,
our model obtains an accuracy of 84.5% compared to
78.5% obtained by a strong generative baseline. Fi-
nally, we provide an in depth analysis of the relative
contributions of the two types of constraints in ?5.
2 Coupling Token and Type Constraints
Type-level information has been amply used in
weakly supervised POS induction, either via pure
manually crafted tag dictionaries (Smith and Eisner,
2005; Ravi and Knight, 2009; Garrette and Baldridge,
2012), noisily projected tag dictionaries (Das and
Petrov, 2011) or through crowdsourced lexica, such
as Wiktionary (Li et al, 2012). At the other end
of the spectrum, there have been efforts that project
token-level information across word-aligned bitext
(Yarowsky and Ngai, 2001; Xi and Hwa, 2005). How-
ever, systems that combine both sources of informa-
tion in a single model have yet to be fully explored.
The following three subsections outline our overall
approach for coupling these two types of information
to build robust POS taggers that do not require any
direct supervision in the target language.
2.1 Token Constraints
For the majority of resource-poor languages, there
is at least some bitext with a resource-rich source
language; for simplicity, we choose English as our
source language in all experiments. It is then nat-
ural to consider using a supervised part-of-speech
tagger to predict part-of-speech tags for the English
side of the bitext. These predicted tags can subse-
quently be projected to the target side via automatic
word alignments. This approach was pioneered by
Yarowsky and Ngai (2001), who used the resulting
partial target annotation to estimate the parameters
of an HMM. However, due to the automatic nature
of the word alignments and the POS tags, there will
be significant noise in the projected tags. To conquer
this noise, they used very aggressive smoothing tech-
niques when training the HMM. Fossum and Abney
(2005) used similar token-level projections, but in-
stead combined projections from multiple source lan-
guages to filter out random projection noise as well
as the systematic noise arising from different source
language annotations and syntactic divergences.
2.2 Type Constraints
It is well known that given a tag dictionary, even if
it is incomplete, it is possible to learn accurate POS
taggers (Smith and Eisner, 2005; Goldberg et al,
2008; Ravi and Knight, 2009; Naseem et al, 2009).
While widely differing in the specific model struc-
ture and learning objective, all of these approaches
achieve excellent results. Unfortunately, they rely
on tag dictionaries extracted directly from the un-
derlying treebank data. Such dictionaries provide in
depth coverage of the test domain and also list all
2
	
 
     
   

 
   	
 
  	  	  

	

	
 



	



 

	    


Figure 1: Lattice representation of the inference search space Y(x) for an authentic sentence in Swedish (?The farming
products must be pure and must not contain any additives?), after pruning with Wiktionary type constraints. The
correct parts of speech are listed underneath each word. Bold nodes show projected token constraints y?. Underlined
text indicates incorrect tags. The coupled constraints lattice Y?(x, y?) consists of the bold nodes together with nodes for
words that are lacking token constraints; in this case, the coupled constraints lattice thus defines exactly one valid path.
inflected forms ? both of which are difficult to obtain
and unrealistic to expect for resource-poor languages.
In contrast, Das and Petrov (2011) automatically
create type-level tag dictionaries by aggregating over
projected token-level information extracted from bi-
text. To handle the noise in these automatic dictionar-
ies, they use label propagation on a similarity graph
to smooth (and also expand) the label distributions.
While their approach produces good results and is
applicable to resource-poor languages, it requires a
complex multi-stage training procedure including the
construction of a large distributional similarity graph.
Recently, Li et al (2012) presented a simple and
viable alternative: crowdsourced dictionaries from
Wiktionary. While noisy and sparse in nature, Wik-
tionary dictionaries are available for 170 languages.2
Furthermore, their quality and coverage is growing
continuously (Li et al, 2012). By incorporating type
constraints from Wiktionary into the feature-based
HMM of Berg-Kirkpatrick et al (2010), Li et al were
able to obtain the best published results in this setting,
surpassing the results of Das and Petrov (2011) on
eight Indo-European languages.
2.3 Coupled Constraints
Rather than relying exclusively on either token or
type constraints, we propose to complement the one
with the other during training. For each sentence in
our training set, a partially constrained lattice of tag
sequences is constructed as follows:
2http://meta.wikimedia.org/wiki/
Wiktionary ? October 2012.
1. For each token whose type is not in the tag dic-
tionary, we allow the entire tag set.
2. For each token whose type is in the tag dictio-
nary, we prune all tags not licensed by the dictio-
nary and mark the token as dictionary-pruned.
3. For each token that has a tag projected via a
high-confidence bidirectional word alignment:
if the projected tag is still present in the lattice,
then we prune every tag but the projected tag for
that token; if the projected tag is not present in
the lattice, which can only happen for dictionary-
pruned tokens, then we ignore the projected tag.
Figure 1 provides a running example. The lattice
shows tags permitted after constraining the words
to tags licensed by the dictionary (up until Step 2
from above). There is only a single token ?Jordbruk-
sprodukterna? (?the farming products?) not in the
dictionary; in this case the lattice permits the full
set of tags. With token-level projections (Step 3;
nodes with bold border in Figure 1), the lattice can
be further pruned. In most cases, the projected tag
is both correct and is in the dictionary-pruned lattice.
We thus successfully disambiguate such tokens and
shrink the search space substantially.
There are two cases we highlight in order to show
where our model can break. First, for the token
?Jordbruksprodukterna?, the erroneously projected
tag ADJ will eliminate all other tags from the lattice,
including the correct tag NOUN. Second, the token
?na?gra? (?any?) has a single dictionary entry PRON
and is missing the correct tag DET. In the case where
3
DET is the projected tag, we will not add it to the
lattice and simply ignore it. This is because we hy-
pothesize that the tag dictionary can be trusted more
than the tags projected via noisy word alignments. As
we will see in ?4, taking the union of tags performs
worse, which supports this hypothesis.
For generative models, such as HMMs (?3.1), we
need to define only one lattice. For our best gen-
erative model this is the coupled token- and type-
constrained lattice.3 At prediction time, in both the
discriminative and the generative cases, we find the
most likely label sequence using Viterbi decoding.
For discriminative models, such as CRFs (?3.2),
we need to define two lattices: one that the model
moves probability mass towards and another one
defining the overall search space (or partition func-
tion). In traditional supervised learning without a
dictionary, the former is a trivial lattice containing
the gold standard tag sequence and the latter is the
set of all possible tag sequences spanning the tokens.
With our best model, we will move mass towards the
coupled token- and type-constrained lattice, such that
the model can freely distribute mass across all paths
consistent with these constraints. The lattice defining
the partition function will be the full set of possible
tag sequences when no dictionary is used; when a
dictionary is used it will consist of all dictionary-
pruned tag sequences (sans Step 3 above; the full set
of possibilities shown in Figure 1 for our running
example).
Figures 2 and 3 provide statistics regarding the
supervision coverage and remaining ambiguity. Fig-
ure 2 shows that more than two thirds of all tokens in
our training data are in Wiktionary. However, there is
considerable variation between languages: Spanish
has the highest coverage with over 90%, while Turk-
ish, an agglutinative language with a vast number
of word forms, has less than 50% coverage. Fig-
ure 3 shows that there is substantial uncertainty left
after pruning with Wiktionary, since tokens are rarely
fully disambiguated: 1.3 tags per token are allowed
on average for types in Wiktionary.
Figure 2 further shows that high-confidence align-
ments are available for about half of the tokens for
most languages (Japanese is a notable exception with
3Other training methods exist as well, for example, con-
trastive estimation (Smith and Eisner, 2005).
0
25
50
75
100
avg bg cs da de el es fr it ja nl pt sl sv tr zh
Pe
rc
en
t o
f to
ke
ns
 c
ov
er
ed
Token
coverage Wiktionary Projected Projected+Filtered
Figure 2: Wiktionary and projection dictionary coverage.
Shown is the percentage of tokens in the target side of the
bitext that are covered by Wiktionary, that have a projected
tag, and that have a projected tag after intersecting the two.
0.0
0.5
1.0
1.5
avg bg cs da de el es fr it ja nl pt sl sv tr zh
Nu
mb
er 
of 
tag
s p
er 
tok
en
Figure 3: Average number of licensed tags per token on
the target side of the bitext, for types in Wiktionary.
less than 30% of the tokens covered). Intersecting the
Wiktionary tags and the projected tags (Step 2 and 3
above) filters out some of the potentially erroneous
tags, but preserves the majority of the projected tags;
the remaining, presumably more accurate projected
tags cover almost half of all tokens, greatly reducing
the search space that the learner needs to explore.
3 Models with Coupled Constraints
We now formally present how we couple token and
type constraints and how we use these coupled con-
straints to train probabilistic tagging models. Let
x = (x1x2 . . . x|x|) ? X denote a sentence, where
each token xi ? V is an instance of a word type from
the vocabulary V and let y = (y1y2 . . . y|x|) ? Y de-
note a tag sequence, where yi ? T is the tag assigned
to token xi and T denotes the set of all possible part-
of-speech tags. We denote the lattice of all admissible
tag sequences for the sentence x by Y(x). This is the
4
inference search space in which the tagger operates.
As we shall see, it is crucial to constrain the size of
this lattice in order to simplify learning when only
incomplete supervision is available.
A tag dictionary maps a word type xj ? V to
a set of admissible tags T (xj) ? T . For word
types not in the dictionary we allow the full set of
tags T (while possible, in this paper we do not at-
tempt to distinguish closed-class versus open-class
words). When provided with a tag dictionary, the
lattice of admissible tag sequences for a sentence x
is Y(x) = T (x1) ? T (x2) ? . . . ? T (x|x|). When
no tag dictionary is available, we simply have the full
lattice Y(x) = T |x|.
Let y? = (y?1y?2 . . . y?|x|) be the projected tags for
the sentence x. Note that {y?i} = ? for tokens without
a projected tag. Next, we define a piecewise operator
_ that couples y? and Y(x) with respect to every
sentence index, which results in a token- and type-
constrained lattice. The operator behaves as follows,
coherent with the high level description in ?2.3:
T? (xi, y?i) = y?i _ T (xi) =
{
{y?i} if y?i ? T (xi)
T (xi) otherwise .
We denote the token- and type-constrained lattice as
Y?(x, y?) = T? (x1, y?1)?T? (x2, y?2)?. . .?T? (x|x|, y?|x|).
Note that when token-level projections are not used,
the dictionary-pruned lattice and the lattice with cou-
pled constraints are identical, that is Y?(x, y?) = Y(x).
3.1 HMMs with Coupled Constraints
A first-order hidden Markov model (HMM) specifies
the joint distribution of a sentence x ? X and a
tag-sequence y ? Y(x) as:
p?(x, y) =
|x|?
i=1
p?(xi | yi)? ?? ?
emission
p?(yi | yi?1)? ?? ?
transition
.
We follow the recent trend of using a log-linear
parametrization of the emission and the transition
distributions, instead of a multinomial parametriza-
tion (Chen, 2003). This allows model parameters ?
to be shared across categorical events, which has
been shown to give superior performance (Berg-
Kirkpatrick et al, 2010). The categorical emission
and transition events are represented by feature vec-
tors ?(xi, yi) and ?(yi, yi?1). Each element of the
parameter vector ? corresponds to a particular fea-
ture; the component log-linear distributions are:
p?(xi | yi) =
exp
(
?>?(xi, yi)
)
?
x?i?V exp (?
>?(x?i, yi))
,
and
p?(yi | yi?1) =
exp
(
?>?(yi, yi?1)
)
?
y?i?T exp (?
>?(y?i, yi?1))
.
In maximum-likelihood estimation of the parameters,
we seek to maximize the likelihood of the observed
parts of the data. For this we need the joint marginal
distribution p?(x, Y?(x, y?)) of a sentence x, and its
coupled constraints lattice Y?(x, y?), which is obtained
by marginalizing over all consistent outputs:
p?(x, Y?(x, y?)) =
?
y?Y?(x,y?)
p?(x, y) .
If there are no projections and no tag dictionary, then
Y?(x, y?) = T |x|, and thus p?(x, Y?(x, y?)) = p?(x),
which reduces to fully unsupervised learning. The
`2-regularized marginal joint log-likelihood of the
constrained training data D = {(x(i), y?(i))}ni=1 is:
L(?;D) =
n?
i=1
log p?(x(i), Y?(x(i), y?(i)))?? ???22 .
(1)
We follow Berg-Kirkpatrick et al (2010) and take a
direct gradient approach for optimizing Eq. 1 with
L-BFGS (Liu and Nocedal, 1989). We set ? = 1 and
run 100 iterations of L-BFGS. One could also em-
ploy the Expectation-Maximization (EM) algorithm
(Dempster et al, 1977) to optimize this objective, al-
though the relative merits of EM versus direct gradi-
ent training for these models is still a topic of debate
(Berg-Kirkpatrick et al, 2010; Li et al, 2012).4 Note
that since the marginal likelihood is non-concave, we
are only guaranteed to find a local maximum of Eq. 1.
After estimating the model parameters ?, the tag-
sequence y? ? Y(x) for a sentence x ? X is pre-
dicted by choosing the one with maximal joint prob-
ability:
y? ? arg max
y?Y(x)
p?(x, y) .
4We trained the HMM with EM as well, but achieved better
results with direct gradient training and hence omit those results.
5
3.2 CRFs with Coupled Constraints
Whereas an HMM models the joint probability of
the input x ? X and output y ? Y(x), using locally
normalized component distributions, a conditional
random field (CRF) instead models the probability of
the output conditioned on the input as a globally nor-
malized log-linear distribution (Lafferty et al, 2001):
p?(y | x) =
exp
(
?>?(x, y)
)
?
y??Y(x) exp (?>?(x, y?))
,
where ? is a parameter vector. As for the HMM,
Y(x) is not necessarily the full space of possible
tag-sequences; specifically, for us, it is the dictionary-
pruned lattice without the token constraints.
With a first-order Markov assumption, the feature
function factors as:
?(x, y) =
|x|?
i=1
?(x, yi, yi?1) .
This model is more powerful than the HMM in that
it can use richer feature definitions, such as joint in-
put/transition features and features over a wider input
context. We model a marginal conditional probabil-
ity, given by the total probability of all tag sequences
consistent with the lattice Y?(x, y?):
p?(Y?(x, y?) | x) =
?
y?Y?(x,y?)
p?(y | x) .
The parameters of this constrained CRF are estimated
by maximizing the `2-regularized marginal condi-
tional log-likelihood of the constrained data (Riezler
et al, 2002):
L(?;D) =
n?
i=1
log p?(Y?(x(i), y?(i)) | x(i))? ????22 .
(2)
As with Eq. 1, we maximize Eq. 2 with 100 itera-
tions of L-BFGS and set ? = 1. In contrast to the
HMM, after estimating the model parameters ?, the
tag-sequence y? ? Y(x) for a sentence x ? X is
chosen as the sequence with the maximal conditional
probability:
y? ? arg max
y?Y(x)
p?(y | x) .
4 Empirical Study
We now present a detailed empirical study of the mod-
els proposed in the previous sections. In addition to
comparing with the state of the art in Das and Petrov
(2011) and Li et al (2012), we present models with
several combinations of token and type constraints,
additional features incorporating word clusters. Both
generative and discriminative models are explored.
4.1 Experimental Setup
Before delving into the experimental details, we
present our setup and datasets.
Languages. We evaluate on eight target languages
used in previous work (Das and Petrov, 2011; Li et
al., 2012) and on seven additional languages (see Ta-
ble 1). While the former eight languages all belong to
the Indo-European family, we broaden the coverage
to language families more distant from the source
language (for example, Chinese, Japanese and Turk-
ish). We use the treebanks from the CoNLL shared
tasks on dependency parsing (Buchholz and Marsi,
2006; Nivre et al, 2007) for evaluation.5 The two-
letter abbreviations from the ISO 639-1 standard are
used when referring to these languages in tables and
figures.
Tagset. In all cases, we map the language-specific
POS tags to universal POS tags using the mapping
of Petrov et al (2012).6 Since we use indirect super-
vision via projected tags or Wiktionary, the model
states induced by all models correspond directly to
POS tags, enabling us to compute tagging accuracy
without a greedy 1-to-1 or many-to-1 mapping.
Bitext. For all experiments, we use English as the
source language. Depending on availability, there
are between 1M and 5M parallel sentences for each
language. The majority of the parallel data is gath-
ered automatically from the web using the method
of Uszkoreit et al (2010). We further include data
from Europarl (Koehn, 2005) and from the UN par-
allel corpus (UN, 2006), for languages covered by
these corpora. The English side of the bitext is
POS tagged with a standard supervised CRF tagger,
trained on the Penn Treebank (Marcus et al, 1993),
with tags mapped to universal tags. The parallel sen-
5For French we use the treebank of Abeille? et al (2003).
6We use version 1.03 of the mappings available at http:
//code.google.com/p/universal-pos-tags/.
6
tences are word aligned with the aligner of DeNero
and Macherey (2011). Intersected high-confidence
alignments (confidence >0.95) are extracted and ag-
gregated into projected type-level dictionaries. For
purely practical reasons, the training data with token-
level projections is created by randomly sampling
target-side sentences with a total of 500K tokens.
Wiktionary. We use a snapshot of the Wiktionary
word definitions, and follow the heuristics of Li et
al. (2012) for creating the Wiktionary dictionary by
mapping the Wiktionary tags to universal POS tags.7
Features. For all models, we use only an identity
feature for tag-pair transitions. We use five features
that couple the current tag and the observed word
(analogous to the emission in an HMM): word iden-
tity, suffixes of up to length 3, and three indicator
features that fire when the word starts with a capital
letter, contains a hyphen or contains a digit. These are
the same features as those used by Das and Petrov
(2011). Finally, for some models we add a word
cluster feature that couples the current tag and the
word cluster identity of the word. These (monolin-
gual) word clusters are induced with the exchange
algorithm (Uszkoreit and Brants, 2008). We set the
number of clusters to 256 across all languages, as this
has previously been shown to produce robust results
for similar tasks (Turian et al, 2010; Ta?ckstro?m et
al., 2012). The clusters for each language are learned
on a large monolingual newswire corpus.
4.2 Models with Type Constraints
To examine the sole effect of type constraints, we
experiment with the HMM, drawing constraints from
three different dictionaries. Table 1 compares the per-
formance of our models with the best results of Das
and Petrov (2011, D&P) and Li et al (2012, LG&T).
As in previous work, training is done exclusively on
the training portion of each treebank, stripped of any
manual linguistic annotation.
We first use all of our parallel data to generate
projected tag dictionaries: the English POS tags are
projected across word alignments and aggregated to
tag distributions for each word type. As in Das and
Petrov (2011), the distributions are then filtered with
a threshold of 0.2 to remove noisy tags and to create
7The definitions were downloaded on August 31, 2012 from
http://toolserver.org/?enwikt/definitions/.
This snapshot is more recent than that used by Li et al
Prior work HMM with type constraints
Lang. D&P LG&T YHMMproj. YHMMwik. YHMMunion YHMMunion +C
bg ? ? 84.2 68.1 87.2 87.9
cs ? ? 75.4 70.2 75.4 79.2
da 83.2 83.3 87.7 82.0 78.4 89.5
de 82.8 85.8 86.6 85.1 80.0 88.3
el 82.5 79.2 83.3 83.8 86.0 83.2
es 84.2 86.4 83.9 83.7 88.3 87.3
fr ? ? 88.4 75.7 75.6 86.6
it 86.8 86.5 89.0 85.4 89.9 90.6
ja ? ? 45.2 76.9 74.4 73.7
nl 79.5 86.3 81.7 79.1 83.8 82.7
pt 87.9 84.5 86.7 79.0 83.8 90.4
sl ? ? 78.7 64.8 82.8 83.4
sv 80.5 86.1 80.6 85.9 85.9 86.7
tr ? ? 66.2 44.1 65.1 65.7
zh ? ? 59.2 73.9 63.2 73.0
avg (8) 83.4 84.8 84.9 83.0 84.5 87.3
avg ? ? 78.5 75.9 80.0 83.2
Table 1: Tagging accuracies for type-constrained HMM
models. D&P is the ?With LP? model in Table 2 of
Das and Petrov (2011), while LG&T is the ?SHMM-ME?
model in Table 2 of Li et al (2012). YHMMproj. , YHMMwik. and
YHMMunion are HMMs trained solely with type constraints
derived from the projected dictionary, Wiktionary and
the union of these dictionaries, respectively. YHMMunion +C is
equivalent to YHMMunion with additional cluster features. All
models are trained on the treebank of each language,
stripped of gold labels. Results are averaged over the
8 languages from Das and Petrov (2011), denoted avg (8),
as well as over the full set of 15 languages, denoted avg.
an unweighted tag dictionary. We call this model
YHMMproj. ; its average accuracy of 84.9% on the eight
languages is higher than the 83.4% of D&P and on
par with LG&T (84.8%).8 Our next model (YHMMwik. )
simply draws type constraints from Wiktionary. It
slightly underperforms LG&T (83.0%), presumably
because they used a second-order HMM. As a simple
extension to these two models, we take the union
of the projected dictionary and Wiktionary to con-
strain an HMM, which we name YHMMunion . This model
performs a little worse on the eight Indo-European
languages (84.5), but gives an improvement over the
projected dictionary when evaluated across all 15
languages (80.0% vs. 78.5%).
8Our model corresponds to the weaker, ?No LP? projection
of Das and Petrov (2011). We found that label propagation was
only beneficial when small amounts of bitext were available.
7
Token constraints HMM with coupled constraints CRF with coupled constraints
Lang. YHMMunion +C+L y?HMM+C+L y?CRF+C+L Y?HMMproj. +C+L Y?HMMwik. +C+L Y?HMMunion +C+L Y?CRFproj. +C+L Y?CRFwik. +C+L Y?CRFunion+C+L
bg 87.7 77.9 84.1 84.5 83.9 86.7 86.0 87.8 85.4
cs 78.3 65.4 74.9 74.8 81.1 76.9 74.7 80.3** 75.0
da 87.3 80.9 85.1 87.2 85.6 88.1 85.5 88.2* 86.0
de 87.7 81.4 83.3 85.0 89.3 86.7 84.4 90.5** 85.5
el 85.9 81.1 77.8 80.1 87.0 83.9 79.6 89.5** 79.7
es 89.1** 84.1 85.5 83.7 85.9 88.0 85.7 87.1 86.0
fr 88.4** 83.5 84.7 85.9 86.4 87.4 84.9 87.2 85.6
it 89.6 85.2 88.5 88.7 87.6 89.8 88.3 89.3 89.4
ja 72.8 47.6 54.2 43.2 76.1 70.5 44.9 81.0** 68.0
nl 83.1 78.4 82.4 82.3 84.2 83.2 83.1 85.9** 83.2
pt 89.1 84.7 87.0 86.6 88.7 88.0 87.9 91.0** 88.3
sl 82.4 69.8 78.2 78.5 81.8 80.1 79.7 82.3 80.0
sv 86.1 80.1 84.2 82.3 87.9 86.9 84.4 88.9** 85.5
tr 62.4 58.1 64.5 64.6 61.8 64.8 65.0 64.1** 65.2
zh 72.6 52.7 39.5 56.0 74.1 73.3 59.7 74.4** 73.4
avg (8) 87.2 82.0 84.2 84.5 87.0 86.8 84.9 88.8 85.4
avg 82.8 74.1 76.9 77.6 82.8 82.3 78.2 84.5 81.1
Table 2: Tagging accuracies for models with token constraints and coupled token and type constraints. All models use
cluster features (. . . +C) and are trained on large training sets each containing 500k tokens with (partial) token-level
projections (. . . +L). The best type-constrained model, trained on the larger datasets, YHMMunion +C+L, is included for
comparison. The remaining columns correspond to HMM and CRF models trained only with token constraints (y? . . .)
and with coupled token and type constraints (Y? . . .). The latter are trained using the projected dictionary (?proj.),
Wiktionary (?wik.) and the union of these dictionaries (?union), respectively. The search spaces of the models trained with
coupled constraints (Y? . . .) are each pruned with the respective tag dictionary used to derive the coupled constraints.
The observed difference between Y?CRFwik. +C+L and YHMMunion +C+L is statistically significant at p < 0.01 (**) and p < 0.015(*) according to a paired bootstrap test (Efron and Tibshirani, 1993). Significance was not assessed for avg or avg (8).
We next add monolingual cluster features to
the model with the union dictionary. This model,
YHMMunion +C, significantly outperforms all other type-
constrained models, demonstrating the utility of
word-cluster features.9 For further exploration, we
train the same model on the datasets containing 500K
tokens sampled from the target side of the parallel
data (YHMMunion +C+L); this is done to explore the effects
of large data during training. We find that training
on these datasets result in an average accuracy of
87.2% which is comparable to the 87.3% reported
for YHMMunion +C in Table 1. This shows that the different
source domain and amount of training data does not
influence the performance of the HMM significantly.
Finally, we train CRF models where we treat type
constraints as a partially observed lattice and use the
full unpruned lattice for computing the partition func-
9These are monolingual clusters. Bilingual clusters as intro-
duced in Ta?ckstro?m et al (2012) might bring additional benefits.
tion (?3.2). Due to space considerations, the results
of these experiments are not shown in table 1. We ob-
serve similar trends in these results, but on average,
accuracies are much lower compared to the type-
constrained HMM models; the CRF model with the
union dictionary along with cluster features achieves
an average accuracy of 79.3% when trained on same
data. This result is not unsurprising. First, the CRF?s
search space is fully unconstrained. Second, the dic-
tionary only provides a weak set of observation con-
straints, which do not provide sufficient information
to successfully train a discriminative model. How-
ever, as we will observe next, coupling the dictionary
constraints with token-level information solves this
problem.
4.3 Models with Token and Type Constraints
We now proceed to add token-level information,
focusing in particular on coupled token and type
8
constraints. Since it is not possible to generate
projected token constraints for our monolingual
treebanks, we train all models in this subsection
on the 500K-tokens datasets sampled from the bi-
text. As a baseline, we first train HMM and CRF
models that use only projected token constraints
(y?HMM+C+L and y?CRF+C+L). As shown in Table 2,
these models underperform the best type-level model
(YHMMunion +C+L),10 which confirms that projected to-
ken constraints are not reliable on their own. This
is in line with similar projection models previously
examined by Das and Petrov (2011).
We then study models with coupled token and type
constraints. These models use the same three dictio-
naries as used in ?4.2, but additionally couple the
derived type constraints with projected token con-
straints; see the caption of Table 2 for a list of these
models. Note that since we only allow projected tags
that are licensed by the dictionary (Step 3 of the trans-
fer, ?2.3), the actual token constraints used in these
models vary with the different dictionaries.
From Table 2, we see that coupled constraints are
superior to token constraints, when used both with
the HMM and the CRF. However, for the HMM, cou-
pled constraints do not provide any benefit over type
constraints alone, in particular when the projected
dictionary or the union dictionary is used to derive the
coupled constraints (Y?HMMproj. +C+L and Y?HMMunion +C+L).
We hypothesize that this is because these dictionar-
ies (in particular the former) have the same bias as
the token-level tag projections, so that the dictionary
is unable to correct the systematic errors in the pro-
jections (see ?2.1). Since the token constraints are
stronger than the type constraints in the coupled mod-
els, this bias may have a substantial impact. With
the Wiktionary dictionary, the difference between the
type-constrained and the coupled-constrained HMM
is negligible: YHMMunion +C+L and Y?HMMwik. +C+L both av-
erage at an accuracy of 82.8%.
The CRF model, on the other hand, is able to take
advantage of the complementary information in the
coupled constraints, provided that the dictionary is
able to filter out the systematic token-level errors.
With a dictionary derived from Wiktionary and pro-
jected token-level constraints, Y?CRFwik. +C+L performs
10To make the comparison fair vis-a-vis potential divergences
in training domains, we compare to the best type-constrained
model trained on the same 500K tokens training sets.
0 1 2 3
0
25
50
75
100
0 1 10 100 0 1 10 100 0 1 10 100 0 1 10 100
Number of token?level projections
Ta
gg
ing
 ac
cur
ac
y
Number of tags listed in Wiktionary
Figure 4: Relative influence of token and type constraints
on tagging accuracy in the Y?CRFwik. +C+L model. Word typesare categorized according to a) their number of Wiktionary
tags (0,1,2 or 3+ tags, with 0 representing no Wiktionary
entry; top-axis) and b) the number of times they are token-
constrained in the training set (divided into buckets of
0, 1-9, 10-99 and 100+ occurrences; x-axis). The boxes
summarize the accuracy distributions across languages
for each word type category as defined by a) and b). The
horizontal line in each box marks the median accuracy,
the top and bottom mark the first and third quantile, re-
spectively, while the whiskers mark the minimum and
maximum values of the accuracy distribution.
better than all the remaining models, with an average
accuracy of 88.8% across the eight Indo-European
languages available to D&P and LG&T. Averaged
over all 15 languages, its accuracy is 84.5%.
5 Further Analysis
In this section we provide a detailed analysis of the
impact of token versus type constraints and we study
the pruning and filtering mistakes resulting from in-
complete Wiktionary entries in detail. This analysis
is based on the training portion of each treebank.
5.1 Influence of Token and Type Constraints
The empirical success of the model trained with cou-
pled token and type constraints confirms that these
constraints indeed provide complementary signals.
Figure 4 provides a more detailed view of the rela-
tive benefits of each type of constraint. We observe
several interesting trends.
First, word types that occur with more token con-
straints during training are generally tagged more
accurately, regardless of whether these types occur
9
90.0
92.5
95.0
97.5
100.0
0 50 100 150 200 250
Number of corrected Wiktionary entries
Pr
un
ing
 ac
cur
ac
y
Figure 5: Average pruning accuracy (line) across lan-
guages (dots) as a function of the number of hypotheti-
cally corrected Wiktionary entries for the k most frequent
word types. For example, position 100 on the x-axis cor-
responds to manually correcting the entries for the 100
most frequent types, while position 0 corresponds to ex-
perimental conditions.
in Wiktionary. The most common scenario is for a
word type to have exactly one tag in Wiktionary and
to occur with this projected tag over 100 times in
the training set (facet 1, rightmost box). These com-
mon word types are typically tagged very accurately
across all languages.
Second, the word types that are ambiguous accord-
ing to Wiktionary (facets 2 and 3) are predominantly
frequent ones. The accuracy is typically lower for
these words compared to the unambiguous words.
However, as the number of projected token con-
straints is increased from zero to 100+ observations,
the ambiguous words are effectively disambiguated
by the token constraints. This shows the advantage
of intersecting token and type constraints.
Finally, projection generally helps for words that
are not in Wiktionary, although the accuracy for these
words never reach the accuracy of the words with
only one tag in Wiktionary. Interestingly, words that
occur with a projected tag constraint less than 100
times are tagged more accurately for types not in the
dictionary compared to ambiguous word types with
the same number of projected constraints. A possible
explanation for this is that the ambiguous words are
inherently more difficult to predict and that most of
the words that are not in Wiktionary are less common
words that tend to also be less ambiguous.
zh
tr
sv
sl
pt
nl
jait
fr
es
el
de
da
cs
bg
avg
0 25 50 75 100
Proportion of pruning errors
PRON
NOUN
DET
ADP
PRT
ADV
NUM
CONJ
ADJ
VERB
X
.
Figure 6: Prevalence of pruning mistakes per POS tag,
when pruning the inference search space with Wiktionary.
5.2 Wiktionary Pruning Mistakes
The error analysis by Li et al (2012) showed that the
tags licensed by Wiktionary are often valid. When
using Wiktionary to prune the search space of our
constrained models and to filter token-level projec-
tions, it is also important that correct tags are not
mistakenly pruned because they are missing from
Wiktionary. While the accuracy of filtering is more
difficult to study, due to the lack of a gold standard
tagging of the bitext, Figure 5 (position 0 on the x-
axis) shows that search space pruning errors are not
a major issue for most languages; on average the
pruning accuracy is almost 95%. However, for some
languages such as Chinese and Czech the correct tag
is pruned from the search space for nearly 10% of all
tokens. When using Wiktionary as a pruner, the upper
bound on accuracy for these languages is therefore
only around 90%. However, Figure 5 also shows that
with some manual effort we might be able to remedy
many of these errors. For example, by adding miss-
ing valid tags to the 250 most common word types in
the worst language, the minimum pruning accuracy
would rise above 95% from below 90%. If the same
was to be done for all of the studied languages, the
mean pruning accuracy would reach over 97%.
Figure 6 breaks down pruning errors resulting from
incorrect or incomplete Wiktionary entries across
the correct POS tags. From this we observe that,
for many languages, the pruning errors are highly
skewed towards specific tags. For example, for Czech
over 80% of the pruning errors are caused by mistak-
enly pruned pronouns.
10
6 Conclusions
We considered the problem of constructing multilin-
gual POS taggers for resource-poor languages. To
this end, we explored a number of different models
that combine token constraints with type constraints
from different sources. The best results were ob-
tained with a partially observed CRF model that ef-
fectively integrates these complementary constraints.
In an extensive empirical study, we showed that this
approach substantially improves on the state of the
art in this context. Our best model significantly out-
performed the second-best model on 10 out of 15
evaluated languages, when trained on identical data
sets, with an insignificant difference on 3 languages.
Compared to the prior state of the art (Li et al, 2012),
we observed a relative reduction in error by 25%,
averaged over the eight languages common to our
studies.
Acknowledgments
We thank Alexander Rush for help with the hyper-
graph framework that was used to implement our
models and Klaus Macherey for help with the bi-
text extraction. This work benefited from many dis-
cussions with Yoav Goldberg, Keith Hall, Kuzman
Ganchev and Hao Zhang. We also thank the editor
and the three anonymous reviewers for their valuable
feedback. The first author is grateful for the financial
support from the Swedish National Graduate School
of Language Technology (GSLT).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a Treebank for French. In A. Abeille?,
editor, Treebanks: Building and Using Parsed Corpora,
chapter 10. Kluwer.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?, John
DeNero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proceedings of NAACL-HLT.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Stanley F Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proceedings of
Eurospeech.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT.
Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin.
1977. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39.
John DeNero and Klaus Macherey. 2011. Model-based
aligner combination using dual decomposition. In Pro-
ceedings of ACL-HLT.
Brad Efron and Robert J. Tibshirani. 1993. An Introduc-
tion to the Bootstrap. Chapman & Hall, New York, NY,
USA.
Victoria Fossum and Steven Abney. 2005. Automatically
inducing a part-of-speech tagger by projecting from
multiple source languages across aligned corpora. In
Proceedings of IJCNLP.
Dan Garrette and Jason Baldridge. 2012. Type-supervised
hidden markov models for part-of-speech tagging with
incomplete tag dictionaries. In Proceedings of EMNLP-
CoNLL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings of ACL-HLT.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of ICML.
Shen Li, Joa?o Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings of
EMNLP-CoNLL.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2).
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. JAIR, 36.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of EMNLP-CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of ACL-IJCNLP.
11
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, III, and Mark Johnson. 2002.
Parsing the wall street journal using a lexical-functional
grammar and discriminative estimation techniques. In
Proceedings of ACL.
Noah Smith and Jason Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data. In
Proceedings of ACL.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual word clusters for direct transfer of
linguistic structure. In Proceedings of NAACL-HLT.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceedings
of ACL.
UN. 2006. ODS UN parallel corpus.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL-HLT.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe
Dubiner. 2010. Large scale parallel document mining
for machine translation. In Proceedings of COLING.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-English languages.
In Proceedings of HLT-EMNLP.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL.
12
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 46?54,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Learning Better Monolingual Models with Unannotated Bilingual Text
David Burkett? Slav Petrov? John Blitzer? Dan Klein?
?University of California, Berkeley ?Google Research
{dburkett,blitzer,klein}@cs.berkeley.edu slav@google.com
Abstract
This work shows how to improve state-of-the-art
monolingual natural language processing models
using unannotated bilingual text. We build a mul-
tiview learning objective that enforces agreement
between monolingual and bilingual models. In
our method the first, monolingual view consists of
supervised predictors learned separately for each
language. The second, bilingual view consists of
log-linear predictors learned over both languages
on bilingual text. Our training procedure estimates
the parameters of the bilingual model using the
output of the monolingual model, and we show how
to combine the two models to account for depen-
dence between views. For the task of named entity
recognition, using bilingual predictors increases F1
by 16.1% absolute over a supervised monolingual
model, and retraining on bilingual predictions
increases monolingual model F1 by 14.6%. For
syntactic parsing, our bilingual predictor increases
F1 by 2.1% absolute, and retraining a monolingual
model on its output gives an improvement of 2.0%.
1 Introduction
Natural language analysis in one language can be
improved by exploiting translations in another lan-
guage. This observation has formed the basis for
important work on syntax projection across lan-
guages (Yarowsky et al, 2001; Hwa et al, 2005;
Ganchev et al, 2009) and unsupervised syntax
induction in multiple languages (Snyder et al,
2009), as well as other tasks, such as cross-lingual
named entity recognition (Huang and Vogel, 2002;
Moore, 2003) and information retrieval (Si and
Callan, 2005). In all of these cases, multilingual
models yield increased accuracy because differ-
ent languages present different ambiguities and
therefore offer complementary constraints on the
shared underlying labels.
In the present work, we consider a setting where
we already possess supervised monolingual mod-
els, and wish to improve these models using unan-
notated bilingual parallel text (bitext). We cast this
problem in the multiple-view (multiview) learning
framework (Blum and Mitchell, 1998; Collins and
Singer, 1999; Balcan and Blum, 2005; Ganchev et
al., 2008). Our two views are a monolingual view,
which uses the supervised monolingual models but
not bilingual information, and a bilingual view,
which exploits features that measure agreement
across languages. The parameters of the bilin-
gual view are trained to reproduce the output of
the monolingual view. We show that by introduc-
ing weakened monolingual models into the bilin-
gual view, we can optimize the parameters of the
bilingual model to improve monolingual models.
At prediction time, we automatically account for
the between-view dependence introduced by the
weakened monolingual models with a simple but
effective view-combination heuristic.
We demonstrate the performance of this method
on two problems. The first is named en-
tity recognition (NER). For this problem, our
method automatically learns (a variation on) ear-
lier hand-designed rule-based bilingual NER pre-
dictors (Huang and Vogel, 2002; Moore, 2003),
resulting in absolute performance gains of up to
16.1% F1. The second task we consider is statis-
tical parsing. For this task, we follow the setup
of Burkett and Klein (2008), who improved Chi-
nese and English monolingual parsers using par-
allel, hand-parsed text. We achieve nearly iden-
tical improvements using a purely unlabeled bi-
text. These results carry over to machine transla-
tion, where we can achieve slightly better BLEU
improvements than the supervised model of Bur-
kett and Klein (2008) since we are able to train
our model directly on the parallel data where we
perform rule extraction.
Finally, for both of our tasks, we use our bilin-
gual model to generate additional automatically
labeled monolingual training data. We compare
46
this approach to monolingual self-training and
show an improvement of up to 14.4% F1 for entity
recognition. Even for parsing, where the bilingual
portion of the treebank is much smaller than the
monolingual, our technique still can improve over
purely monolingual self-training by 0.7% F1.
2 Prior Work on Learning from
Bilingual Text
Prior work in learning monolingual models from
bitexts falls roughly into three categories: Unsu-
pervised induction, cross-lingual projection, and
bilingual constraints for supervised monolingual
models. Two recent, successful unsupervised
induction methods are those of Blunsom et al
(2009) and Snyder et al (2009). Both of them es-
timate hierarchical Bayesian models and employ
bilingual data to constrain the types of models that
can be derived. Projection methods, on the other
hand, were among the first applications of parallel
text (after machine translation) (Yarowsky et al,
2001; Yarowsky and Ngai, 2001; Hwa et al, 2005;
Ganchev et al, 2009). They assume the existence
of a good, monolingual model for one language
but little or no information about the second lan-
guage. Given a parallel sentence pair, they use the
annotations for one language to heavily constrain
the set of possible annotations for the other.
Our work falls into the final category: We wish
to use bilingual data to improve monolingual mod-
els which are already trained on large amounts of
data and effective on their own (Huang and Vo-
gel, 2002; Smith and Smith, 2004; Snyder and
Barzilay, 2008; Burkett and Klein, 2008). Proce-
durally, our work is most closely related to that
of Burkett and Klein (2008). They used an an-
notated bitext to learn parse reranking models for
English and Chinese, exploiting features that ex-
amine pieces of parse trees in both languages. Our
method can be thought of as the semi-supervised
counterpart to their supervised model. Indeed, we
achieve nearly the same results, but without anno-
tated bitexts. Smith and Smith (2004) consider
a similar setting for parsing both English and Ko-
rean, but instead of learning a joint model, they
consider a fixed combination of two parsers and
a word aligner. Our model learns parameters for
combining two monolingual models and poten-
tially thousands of bilingual features. The result
is that our model significantly improves state-of-
the-art results, for both parsing and NER.
3 A Multiview Bilingual Model
Given two input sentences x = (x1, x2) that
are word-aligned translations of each other, we
consider the problem of predicting (structured)
labels y = (y1, y2) by estimating conditional
models on pairs of labels from both languages,
p(y1, y2|x1, x2). Our model consists of two views,
which we will refer to as monolingual and bilin-
gual. The monolingual view estimates the joint
probability as the product of independent marginal
distributions over each language, pM (y|x) =
p1(y1|x1)p2(y2|x2). In our applications, these
marginal distributions will be computed by state-
of-the-art statistical taggers and parsers trained on
large monolingual corpora.
This work focuses on learning parameters for
the bilingual view of the data. We parameterize
the bilingual view using at most one-to-one match-
ings between nodes of structured labels in each
language (Burkett and Klein, 2008). In this work,
we use the term node to indicate a particular com-
ponent of a label, such as a single (multi-word)
named entity or a node in a parse tree. In Fig-
ure 2(a), for example, the nodes labeled NP1 in
both the Chinese and English trees are matched.
Since we don?t know a priori how the components
relate to one another, we treat these matchings as
hidden. For each matching a and pair of labels
y, we define a feature vector ?(y1, a, y2) which
factors on edges in the matching. Our model is
a conditional exponential family distribution over
matchings and labels:
p?(y, a|x) = exp
[
?>?(y1, a, y2)?A(?;x)
]
,
where ? is a parameter vector, and A(?;x) is the
log partition function for a sentence pair x. We
must approximate A(?;x) because summing over
all at most one-to-one matchings a is #P-hard. We
approximate this sum using the maximum-scoring
matching (Burkett and Klein, 2008):
A?(?;x) = log
?
y
max
a
(
exp
[
?>?(y1, a, y2)
])
.
In order to compute the distribution on labels y, we
must marginalize over hidden alignments between
nodes, which we also approximate by using the
maximum-scoring matching:
q?(y|x)
def
= max
a
exp
[
?>?(y1, a, y2)?A?(?;x)
]
.
47
the reports of European Court
ORG1
of Auditors
die Berichte des Europ?ischen Rechnungshofes
ORG1
the
Figure 1: An example where English NER can be
used to disambiguate German NER.
We further simplify inference in our model by
working in a reranking setting (Collins, 2000;
Charniak and Johnson, 2005), where we only con-
sider the top k outputs from monolingual models
in both languages, for a total of k2 labels y. In
practice, k2 ? 10, 000 for our largest problem.
3.1 Including Weakened Models
Now that we have defined our bilingual model, we
could train it to agree with the output of the mono-
lingual model (Collins and Singer, 1999; Ganchev
et al, 2008). As we will see in Section 4, however,
the feature functions ?(y1, a, y2) make no refer-
ence to the input sentences x, other than through a
fixed word alignment. With such limited monolin-
gual information, it is impossible for the bilingual
model to adequately capture all of the information
necessary for NER or parsing. As a simple ex-
ample, a bilingual NER model will be perfectly
happy to label two aligned person names as ORG
instead of PER: both labelings agree equally well.
We briefly illustrate how poorly such a basic bilin-
gual model performs in Section 10.
One way to solve this problem is to include the
output of the full monolingual models as features
in the bilingual view. However, we are training the
bilingual view to match the output of these same
models, which can be trivially achieved by putting
weight on only the monolingual model scores and
never recruiting any bilingual features. There-
fore, we use an intermediate approach: we intro-
duce the output of deliberately weakened mono-
lingual models as features in the bilingual view.
A weakened model is from the same class as the
full monolingual models, but is intentionally crip-
pled in some way (by removing feature templates,
for example). Crucially, the weakened models will
make predictions that are roughly similar to the
full models, but systematically worse. Therefore,
model scores from the weakened models provide
enough power for the bilingual view to make accu-
Feat. types Examples
Algn Densty INSIDEBOTH=3 INENONLY=0
Indicators LBLMATCH=true BIAS=true
Table 1: Sample features used for named entity
recognition for the ORG entity in Figure 1.
rate predictions, but ensure that bilingual features
will be required to optimize the training objective.
Let `W1 = log p
W
1 (y1|x1), `
W
2 = log p
W
2 (y2|x2)
be the log-probability scores from the weakened
models. Our final approximation to the marginal
distribution over labels y is:
q?1,?2,?(y|x)
def
= max
a
exp
h
?1`
W
1 + ?2`
W
2 +
?>?(y1, a, y2)? A?(?1, ?2,?;x)
i
.
(1)
Where
A?(?1, ?2,?;x) =
log
X
y
max
a
exp
h
?1`
W
1 + ?2`
W
2 + ?
>?(y1, a, y2)
i
is the updated approximate log partition function.
4 NER and Parsing Examples
Before formally describing our algorithm for find-
ing the parameters [?1, ?2,?], we first give exam-
ples of our problems of named entity recognition
and syntactic parsing, together with node align-
ments and features for each. Figure 1 depicts a
correctly-labeled sentence fragment in both En-
glish and German. In English, the capitalization of
the phrase European Court of Auditors helps iden-
tify the span as a named entity. However, in Ger-
man, all nouns are capitalized, and capitalization
is therefore a less useful cue. While a monolin-
gual German tagger is likely to miss the entity in
the German text, by exploiting the parallel English
text and word alignment information, we can hope
to improve the German performance, and correctly
tag Europa?ischen Rechnungshofes.
The monolingual features are standard features
for discriminative, state-of-the-art entity recogniz-
ers, and we can produce weakened monolingual
models by simply limiting the feature set. The
bilingual features, ?(y1, a, y2), are over pairs of
aligned nodes, where nodes of the labels y1 and
y2 are simply the individual named entities. We
use a small bilingual feature set consisting of two
types of features. First, we use the word alignment
density features from Burkett and Klein (2008),
which measure how well the aligned entity pair
matches up with alignments from an independent
48
Input: full and weakened monolingual models:
p1(y1|x1), p2(y2|x2), p
w
1 (y1|x1), p
w
2 (y2|x2)
unannotated bilingual data: U
Output: bilingual parameters: ??, ??1, ??2
1. Label U with full monolingual models:
?x ? U, y?M = argmaxy p1(y1|x1)p2(y2|x2).
2. Return argmax?1,?2,?
Q
x?U q?,?1,?2 (y?M |x),
where q?,?1,?2 has the form in Equation 1.
Figure 3: Bilingual training with multiple views.
word aligner. We also include two indicator fea-
tures: a bias feature that allows the model to learn
a general preference for matched entities, and a
feature that is active whenever the pair of nodes
has the same label. Figure 1 contains sample val-
ues for each of these features.
Another natural setting where bilingual con-
straints can be exploited is syntactic parsing. Fig-
ure 2 shows an example English prepositional
phrase attachment ambiguity that can be resolved
bilingually by exploiting Chinese. The English
monolingual parse mistakenly attaches to to the
verb increased. In Chinese, however, this ambi-
guity does not exist. Instead, the word ?, which
aligns to to, has strong selectional preference for
attaching to a noun on the left.
In our parsing experiments, we use the Berke-
ley parser (Petrov et al, 2006; Petrov and Klein,
2007), a split-merge latent variable parser, for our
monolingual models. Our full model is the re-
sult of training the parser with five split-merge
phases. Our weakened model uses only two. For
the bilingual model, we use the same bilingual fea-
ture set as Burkett and Klein (2008). Table 2 gives
some examples, but does not exhaustively enumer-
ate those features.
5 Training Bilingual Models
Previous work in multiview learning has focused
on the case of agreement regularization (Collins
and Singer, 1999; Ganchev et al, 2008). If we had
bilingual labeled data, together with our unlabeled
data and monolingual labeled data, we could ex-
ploit these techniques. Because we do not possess
bilingual labeled data, we must train the bilingual
model in another way. Here we advocate train-
ing the bilingual model (consisting of the bilin-
gual features and weakened monolingual models)
to imitate the full monolingual models. In terms
of agreement regularization, our procedure may be
thought of as ?regularizing? the bilingual model to
be similar to the full monolingual models.
Input: full and weakened monolingual models:
p1(y1|x1), p2(y2|x2), p
w
1 (y1|x1), p
w
2 (y2|x2)
bilingual parameters: ??, ??1, ??2
bilingual input: x = (x1, x2)
Output: bilingual label: y?
Bilingual w/ Weak Bilingual w/ Full
1a. l1 = log
`
pw1 (y1|x1)
?
1b. l1 = log
`
p1(y1|x1)
?
2a. l2 = log
`
pw2 (y2|x2)
?
2b. l2 = log
`
p2(y2|x2)
?
3. Return argmaxy maxa ??1l1 + ??2l2+??
>
?(y1, a, y2)
Figure 4: Prediction by combining monolingual
and bilingual models.
Our training algorithm is summarized in Fig-
ure 3. For each unlabeled point x = (x1, x2), let
y?M be the joint label which has the highest score
from the independent monolingual models (line
1). We then find bilingual parameters ??, ??1, ??2
that maximize q??,??1,??2(y?x|x) (line 2). This max-
likelihood optimization can be solved by an EM-
like procedure (Burkett and Klein, 2008). This
procedure iteratively updates the parameter esti-
mates by (a) finding the optimum alignments for
each candidate label pair under the current pa-
rameters and then (b) updating the parameters to
maximize a modified version of Equation 1, re-
stricted to the optimal alignments. Because we re-
strict alignments to the set of at most one-to-one
matchings, the (a) step is tractable using the Hun-
garian algorithm. With the alignments fixed, the
(b) step just involves maximizing likelihood under
a log-linear model with no latent variables ? this
problem is convex and can be solved efficiently
using gradient-based methods. The procedure has
no guarantees, but is observed in practice to con-
verge to a local optimum.
6 Predicting with Monolingual and
Bilingual Models
Once we have learned the parameters of the bilin-
gual model, the standard method of bilingual pre-
diction would be to just choose the y that is most
likely under q??,??1,??2 :
y? = argmax
y
q??,??1,??2(y|x) . (2)
We refer to prediction under this model as ?Bilin-
gual w/ Weak,? to evoke the fact that the model is
making use of weakened monolingual models in
its feature set.
Given that we have two views of the data,
though, we should be able to leverage additional
information in order to make better predictions. In
49
VB 
NP1 
NP 
VP 
S 
These measures increased the attractiveness of Tianjin to Taiwanese merchants 
(a) 
NP PP PP 
These measures increased the attractiveness of Tianjin to Taiwanese merchants 
VB 
NP 
NP 
VP1 
S 
NP PP PP 
?? ? ?? ? ? ? ?? ? ?? ?? ?
S 
NP 
VB NNP 
PP 
DE NN 
NP1 
VP 
?? ? ?? ? ? ? ?? ? ?? ?? ?
S 
NP 
VB NNP 
PP 
DE NN 
NP1 
VP 
(b) 
Figure 2: An example of PP attachment that is ambiguous in English, but simple in Chinese. In (a) the
correct parses agree (low PP attachment), whereas in (b) the incorrect parses disagree.
Feature Types Feature Templates
Examples
Correct Incorrect
Alignment Density INSIDEBOTH, INSIDEENONLY INSIDEENONLY=0 INSIDEENONLY=1
Span Difference ABSDIFFERENCE ABSDIFFERENCE=3 ABSDIFFERENCE=4
Syntactic Indicators LABEL?E,C?, NUMCHILDREN?E,C? LABEL?NP,NP?=true LABEL?VP,NP?=true
Table 2: Sample bilingual features used for parsing. The examples are features that would be extracted
by aligning the parents of the PP nodes in Figure 2(a) (Correct) and Figure 2(b) (Incorrect).
particular, the monolingual view uses monolingual
models that are known to be superior to the mono-
lingual information available in the bilingual view.
Thus, we would like to find some way to incorpo-
rate the full monolingual models into our predic-
tion method. One obvious choice is to choose the
labeling that maximizes the ?agreement distribu-
tion? (Collins and Singer, 1999; Ganchev et al,
2008). In our setting, this amounts to choosing:
y? = argmax
y
pM (y|x) q??,??1??2(y|x) . (3)
This is the correct decision rule if the views are
independent and the labels y are uniformly dis-
tributed a priori,1 but we have deliberately in-
troduced between-view dependence in the form
of the weakened monolingual models. Equa-
tion 3 implicitly double-counts monolingual infor-
mation.
One way to avoid this double-counting is to
simply discard the weakened monolingual models
when making a joint prediction:
y? = argmax
y
max
a
pM (y|x)
exp
[
??
>
?(y1, a, y2)
]
.
(4)
1See, e.g. Ando & Zhang(Ando and Zhang, 2007) for a
derivation of the decision rule from Equation 3 under these
assumptions.
This decision rule uniformly combines the two
monolingual models and the bilingual model.
Note, however, that we have already learned non-
uniform weights for the weakened monolingual
models. Our final decision rule uses these weights
as weights for the full monolingual models:
y? = argmax
y
max
a
exp
[
??1 log
(
p1(y1|x1)
)
+
??2 log
(
p2(y2|x2)
)
+??
>
?(y1, a, y2)
]
. (5)
As we will show in Section 10, this rule for com-
bining the monolingual and bilingual views per-
forms significantly better than the alternatives, and
comes close to the optimal weighting for the bilin-
gual and monolingual models.
We will refer to predictions made with Equa-
tion 5 as ?Bilingual w/ Full?, to evoke the use of
the full monolingual models alongside our bilin-
gual features. Prediction using ?Bilingual w/
Weak? and ?Bilingual w/ Full? is summarized in
Figure 4.
7 Retraining Monolingual Models
Although bilingual models have many direct ap-
plications (e.g. in machine translation), we also
wish to be able to apply our models on purely
monolingual data. In this case, we can still take
50
Input: annotated monolingual data: L1, L2
unannotated bilingual data: U
monolingual models: p1(y1|x1), p2(y2|x2)
bilingual parameters: ??, ??1, ??2
Output: retrained monolingual models:
pr1(y1|x1), p
r
2(y2|x2)
?x = (x1, x2) ? U:
Self-Retrained Bilingual-Retrained
1a. y?x1 = argmaxy1 p1(y1|x1) 1b. Pick y?x, Fig. 4
y?x2 = argmaxy2 p2(y2|x2) (Bilingual w/ Full)
2. Add (x1, y?x1 ) to L1 and add (x2, y?x2 ) to L2.
3. Return full monolingual models pr1(y1|x1),
pr2(y2|x2) trained on newly enlarged L1, L2.
Figure 5: Retraining monolingual models.
advantage of parallel corpora by using our bilin-
gual models to generate new training data for the
monolingual models. This can be especially use-
ful when we wish to use our monolingual models
in a domain for which we lack annotated data, but
for which bitexts are plentiful.2
Our retraining procedure is summarized in Fig-
ure 5. Once we have trained our bilingual param-
eters and have a ?Bilingual w/ Full? predictor (us-
ing Equation 5), we can use that predictor to an-
notate a large corpus of parallel data (line 1b). We
then retrain the full monolingual models on a con-
catenation of their original training data and the
newly annotated data (line 3). We refer to the new
monolingual models retrained on the output of the
bilingual models as ?Bilingual-Retrained,? and we
tested such models for both NER and parsing. For
comparison, we also retrained monolingual mod-
els directly on the output of the original full mono-
lingual models, using the same unannotated bilin-
gual corpora for self-training (line 1a). We refer to
these models as ?Self-Retrained?.
We evaluated our retrained monolingual mod-
els on the same test sets as our bilingual mod-
els, but using only monolingual data at test time.
The texts used for retraining overlapped with the
bitexts used for training the bilingual model, but
both sets were disjoint from the test sets.
8 NER Experiments
We demonstrate the utility of multiview learn-
ing for named entity recognition (NER) on En-
glish/German sentence pairs. We built both our
full and weakened monolingual English and Ger-
man models from the CoNLL 2003 shared task
2Of course, unannotated monolingual data is even more
plentiful, but as we will show, with the same amount of data,
our method is more effective than simple monolingual self-
training.
training data. The bilingual model parameters
were trained on 5,000 parallel sentences extracted
from the Europarl corpus. For the retraining
experiments, we added an additional 5,000 sen-
tences, for 10,000 in all. For testing, we used
the Europarl 2006 development set and the 2007
newswire test set. Neither of these data sets were
annotated with named entities, so we manually an-
notated 200 sentences from each of them.
We used the Stanford NER tagger (Finkel et
al., 2005) with its default configuration as our full
monolingual model for each language. We weak-
ened both the English and German models by re-
moving several non-lexical and word-shape fea-
tures. We made one more crucial change to our
monolingual German model. The German entity
recognizer has extremely low recall (44 %) when
out of domain, so we chose y?x from Figure 3 to
be the label in the top five which had the largest
number of named entities.
Table 3 gives results for named entity recogni-
tion. The first two rows are the full and weak-
ened monolingual models alone. The second two
are the multiview trained bilingual models. We
first note that for English, using the full bilin-
gual model yields only slight improvements over
the baseline full monolingual model, and in prac-
tice the predictions were almost identical. For this
problem, the monolingual German model is much
worse than the monolingual English model, and so
the bilingual model doesn?t offer significant im-
provements in English. The bilingual model does
show significant German improvements, however,
including a 16.1% absolute gain in F1 over the
baseline for parliamentary proceedings.
The last two rows of Table 3 give results for
monolingual models which are trained on data that
was automatically labeled using the our models.
English results were again mixed, due to the rel-
atively weak English performance of the bilin-
gual model. For German, though, the ?Bilingual-
Retrained? model improves 14.4% F1 over the
?Self-Retrained? baseline.
9 Parsing Experiments
Our next set of experiments are on syntactic pars-
ing of English and Chinese. We trained both our
full and weakened monolingual English models
on the Penn Wall Street Journal corpus (Marcus
et al, 1993), as described in Section 4. Our full
and weakened Chinese models were trained on
51
Eng Parliament Eng Newswire Ger Parliament Ger Newswire
Prec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1
Monolingual Models (Baseline)
Weak Monolingual 52.6 65.9 58.5 67.7 83.0 74.6 71.3 36.4 48.2 80.0 51.5 62.7
Full Monolingual 65.7 71.4 68.4 80.1 88.7 84.2 69.8 44.0 54.0 73.0 56.4 63.7
Multiview Trained Bilingual Models
Bilingual w/ Weak 56.2 70.8 62.7 71.4 86.2 78.1 70.1 66.3 68.2 76.5 76.1 76.3
Bilingual w/ Full 65.4 72.4 68.7 80.6 88.7 84.4 70.1 70.1 70.1 74.6 77.3 75.9
Retrained Monolingual Models
Self-Retrained 71.7 74.0 72.9 79.9 87.4 83.5 70.4 44.0 54.2 79.3 58.9 67.6
Bilingual-Retrained 68.6 70.8 69.7 80.7 89.3 84.8 74.5 63.6 68.6 77.9 69.3 73.4
Table 3: NER Results. Rows are grouped by data condition. We bold all entries that are best in their
group and beat the strongest monolingual baseline.
Chinese English
Monolingual Models (Baseline)
Weak Monolingual 78.3 67.6
Full Monolingual 84.2 75.4
Multiview Trained Bilingual Models
Bilingual w/ Weak 80.4 70.8
Bilingual w/ Full 85.9 77.5
Supervised Trained Bilingual Models
Burkett and Klein (2008) 86.1 78.2
Retrained Monolingual Models
Self-Retrained 83.6 76.7
Bilingual-Retrained 83.9 77.4
Table 4: Parsing results. Rows are grouped by data
condition. We bold entries that are best in their
group and beat the the Full Monolingual baseline.
the Penn Chinese treebank (Xue et al, 2002) (ar-
ticles 400-1151), excluding the bilingual portion.
The bilingual data consists of the parallel part of
the Chinese treebank (articles 1-270), which also
includes manually parsed English translations of
each Chinese sentence (Bies et al, 2007). Only
the Chinese sentences and their English transla-
tions were used to train the bilingual models ? the
gold trees were ignored. For retraining, we used
the same data, but weighted it to match the sizes
of the original monolingual treebanks. We tested
on the standard Chinese treebank development set,
which also includes English translations.
Table 4 gives results for syntactic parsing. For
comparison, we also show results for the super-
vised bilingual model of Burkett and Klein (2008).
This model uses the same features at prediction
time as the multiview trained ?Bilingual w/ Full?
model, but it is trained on hand-annotated parses.
We first examine the first four rows of Table 4. The
?Bilingual w/ Full? model significantly improves
performance in both English and Chinese relative
to the monolingual baseline. Indeed, it performs
Phrase-Based System
Moses (No Parser) 18.8
Syntactic Systems
Monolingual Parser 18.7
Supervised Bilingual (Treebank Bi-trees) 21.1
Multiview Bilingual (Treebank Bitext) 20.9
Multiview Bilingual (Domain Bitext) 21.2
Table 5: Machine translation results.
only slightly worse than the supervised model.
The last two rows of Table 4 are the results of
monolingual parsers trained on automatically la-
beled data. In general, gains in English, which
is out of domain relative to the Penn Treebank,
are larger than those in Chinese, which is in do-
main. We also emphasize that, unlike our NER
data, this bitext was fairly small relative to the an-
notated monolingual data. Therefore, while we
still learn good bilingual model parameters which
give a sizable agreement-based boost when doing
bilingual prediction, we don?t expect retraining to
result in a coverage-based boost in monolingual
performance.
9.1 Machine Translation Experiments
Although we don?t have hand-labeled data for our
largest Chinese-English parallel corpora, we can
still evaluate our parsing results via our perfor-
mance on a downstream machine translation (MT)
task. Our experimental setup is as follows: first,
we used the first 100,000 sentences of the English-
Chinese bitext from Wang et al (2007) to train
Moses (Koehn et al, 2007), a phrase-based MT
system that we use as a baseline. We then used the
same sentences to extract tree-to-string transducer
rules from target-side (English) trees (Galley et al,
2004). We compare the single-reference BLEU
scores of syntactic MT systems that result from
using different parsers to generate these trees.
52
0.0 0.2 
0.4 0.6 
0.8 1.0 
1.2 1.4 
0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 
68-71 65-68 62-65 59-62 56-59 
English Weight 
German
 Weigh
t 
German F1 
70.3 70.1 59.1 
* + * + 
(a) 
0.0 0.2 
0.4 0.6 
0.8 1.0 
1.2 1.4 
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 
81.8-82.1 81.5-81.8 81.2-81.5 80.9-81.2 80.6-80.9 
English Weight 
Chines
e Weig
ht 
Combined F1 
82.1 82.0 81.4 
* + ? 
* + 
? 
(b) 
Figure 6: (a) NER and (b) parsing results for different values of ?1 and ?2 (see Equation 6). ?*? shows
optimal weights, ?+? shows our learned weights, and ?-? shows uniform combination weights.
For our syntactic baseline, we used the mono-
lingual English parser. For our remaining experi-
ments, we parsed both English and Chinese simul-
taneously. The supervised model and the first mul-
tiview trained model are the same Chinese tree-
bank trained models for which we reported pars-
ing results. We also used our multiview method to
train an additional bilingual model on part of the
bitext we used to extract translation rules.
The results are shown in Table 5. Once again,
our multiview trained model yields comparable re-
sults to the supervised model. Furthermore, while
the differences are small, our best performance
comes from the model trained on in-domain data,
for which no gold trees exist.
10 Analyzing Combined Prediction
In this section, we explore combinations of the full
monolingual models, p1(y1|x1) and p2(y2|x2),
and the bilingual model, max
a
??
>
?(y1, a, y2). For
parsing, the results in this section are for combined
F1. This simply computes F1 over all of the sen-
tences in both the English and Chinese test sets.
For NER, we just use German F1, since English is
relatively constant across runs.
We begin by examining how poorly our model
performs if we do not consider monolingual in-
formation in the bilingual view. For parsing, the
combined Chinese and English F1 for this model
is 78.7%. When we combine this model uniformly
with the full monolingual model, as in Equation 4,
combined F1 improves to 81.2%, but is still well
below our best combined score of 82.1%. NER
results for a model trained without monolingual
information show an even larger decline.
Now let us consider decision rules of the form:
y? = argmax
y
max
a
exp[?1 log
`
p1(y1|x1)
?
+
?2 log
`
p2(y2|x2)
?
+??
>
?(y1, a, y2)] .
Note that when ?1 = ?2 = 1, this is exactly
the uniform decision rule (Equation 4). When
?1 = ??1 and ?2 = ??2, this is the ?Bilingual w/
Full? decision rule (Equation 5). Figure 6 is a
contour plot of F1 with respect to the parameters
?1 and ?2. Our decision rule ?Bilingual w/ Full?
(Equation 5, marked with a ?+?) is near the opti-
mum (?*?), while the uniform decision rule (?-?)
performs quite poorly. This is true for both NER
(Figure 6a) and parsing (Figure 6b).
There is one more decision rule which we have
yet to consider: the ?conditional independence?
decision rule from Equation 3. While this rule can-
not be shown on the plots in Figure 6 (because
it uses both the full and weakened monolingual
models), we note that it also performs poorly in
both cases (80.7% F1 for parsing, for example).
11 Conclusions
We show for the first time that state-of-the-art,
discriminative monolingual models can be signifi-
cantly improved using unannotated bilingual text.
We do this by first building bilingual models that
are trained to agree with pairs of independently-
trained monolingual models. Then we combine
the bilingual and monolingual models to account
for dependence across views. By automatically
annotating unlabeled bitexts with these bilingual
models, we can train new monolingual models that
do not rely on bilingual data at test time, but still
perform substantially better than models trained
using only monolingual resources.
Acknowledgements
This project is funded in part by NSF grants
0915265 and 0643742, an NSF graduate research
fellowship, the DNI under grant HM1582-09-1-
0021, and BBN under DARPA contract HR0011-
06-C-0022.
53
References
Rie Kubota Ando and Tong Zhang. 2007. Two-view
feature generation model for semi-supervised learn-
ing. In ICML.
Maria-Florina Balcan and Avrim Blum. 2005. A pac-
style model for learning from labeled and unlabeled
data. In COLT.
Ann Bies, Martha Palmer, Justin Mott, and Colin
Warner. 2007. English chinese translation treebank
v 1.0. Web download. LDC2007T02.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009.
Bayesian synchronous grammar induction. In NIPS.
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
EMNLP.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Kuzman Ganchev, Joao Graca, John Blitzer, and Ben
Taskar. 2008. Multi-view learning over structured
and non-identical outputs. In UAI.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In ACL.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In ICMI.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Special Issue of the Journal of Natural Language
Engineering on Parallel Texts, 11(3):311?325.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Robert Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In EACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In COLING-ACL.
Luo Si and Jamie Callan. 2005. Clef 2005: Multi-
lingual retrieval by combining multiple multilingual
ranked lists. In CLEF.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: using english to
parse korean. In EMNLP.
Benjamin Snyder and Regina Barzilay. 2008. Cross-
lingual propagation for morphological analysis. In
AAAI.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In ACL.
Wen Wang, Andreas Stolcke, and Jing Zheng. 2007.
Reranking machine translation hypotheses with
structured and web-based language models. In IEEE
ASRU Workshop.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In COLING.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In NAACL.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Human Language Technologies.
54
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 61?65,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Temporal Analysis of Language through Neural Language Models
Yoon Kim
?
Yi-I Chiu
?
Kentaro Hanaki
?
Darshan Hegde
?
Slav Petrov

?
New York University, New York

Google Inc., New York
{yhk255, yic211, kh1615, dh1806}@nyu.edu
slav@google.com
Abstract
We provide a method for automatically
detecting change in language across time
through a chronologically trained neural
language model. We train the model on
the Google Books Ngram corpus to ob-
tain word vector representations specific
to each year, and identify words that have
changed significantly from 1900 to 2009.
The model identifies words such as cell
and gay as having changed during that
time period. The model simultaneously
identifies the specific years during which
such words underwent change.
1 Introduction
Language changes across time. Existing words
adopt additional senses (gay), new words are cre-
ated (internet), and some words ?die out? (many
irregular verbs, such as burnt, are being replaced
by their regularized counterparts (Lieberman et al.,
2007)). Traditionally, scarcity of digitized histori-
cal corpora has prevented applications of contem-
porary machine learning algorithms?which typi-
cally require large amounts of data?in such tem-
poral analyses. Publication of the Google Books
Ngram corpus in 2009, however, has contributed
to an increased interest in culturomics, wherein
researchers analyze changes in human culture
through digitized texts (Michel et al., 2011).
Developing computational methods for detect-
ing and quantifying change in language is of in-
terest to theoretical linguists as well as NLP re-
searchers working with diachronic corpora. Meth-
ods employed in previous work have been var-
ied, from analyses of word frequencies to more in-
volved techniques (Guolordava et al. (2011); Mi-
halcea and Nataste (2012)). In our framework,
we train a Neural Language Model (NLM) on
yearly corpora to obtain word vectors for each year
from 1900 to 2009. We chronologically train the
model by initializing word vectors for subsequent
years with the word vectors obtained from previ-
ous years.
We compare the cosine similarity of the word
vectors for same words in different years to iden-
tify words that have moved significantly in the
vector space during that time period. Our model
identifies words such as cell and gay as having
changed between 1900?2009. The model addi-
tionally identifies words whose change is more
subtle. We also analyze the yearly movement of
words across the vector space to identify the spe-
cific periods during which they changed. The
trained word vectors are publicly available.
1
2 Related Work
Previously, researchers have computationally in-
vestigated diachronic language change in various
ways. Mihalcea and Nastase (2012) take a super-
vised learning approach and predict the time pe-
riod to which a word belongs given its surrounding
context. Sagi et al. (2009) use a variation of Latent
Semantic Analysis to identify semantic change of
specific words from early to modern English. Wi-
jaya and Yeniterzi (2011) utilize a Topics-over-
Time model and K-means clustering to identify
periods during which selected words move from
one topic/cluster to another. They correlate their
findings with the underlying historical events dur-
ing that time. Gulordava and Baroni (2011) use
co-occurrence counts of words from 1960s and
1990s to detect semantic change. They find that
the words identified by the model are consistent
with evaluations from human raters. Popescu and
Strapparava (2013) employ statistical tests on fre-
quencies of political, social, and emotional words
to identify and characterize epochs.
Our work contributes to the domain in sev-
1
http://www.yoon.io
61
eral ways. Whereas previous work has generally
involved researchers manually identifying words
that have changed (with the exception of Gulor-
dava and Baroni (2011)), we are able to automat-
ically identify them. We are additionally able to
capture a word?s yearly movement and identify
periods of rapid change. In contrast to previous
work, we simultaneously identify words that have
changed and also the specific periods during which
they changed.
3 Neural Language Models
Similar to traditional language models, NLMs in-
volve predicting a set of future word given some
history of previous words. In NLMs however,
words are projected from a sparse, 1-of-V encod-
ing (where V is the size of the vocabulary) onto a
lower dimensional vector space via a hidden layer.
This allows for better representation of semantic
properties of words compared to traditional lan-
guage models (wherein words are represented as
indices in a vocabulary set). Thus, words that
are semantically close to one another would have
word vectors that are likewise ?close? (as measured
by a distance metric) in the vector space. In fact,
Mikolov et al. (2013a) report that word vectors ob-
tained through NLMs capture much deeper level
of semantic information than had been previously
thought. For example, if x
w
is the word vector
for word w, they note that x
apple
? x
apples
?
x
car
? x
cars
? x
family
? x
families
. That is, the
concept of pluralization is learned by the vector
representations (see Mikolov et al. (2013a) for
more examples).
NLMs are but one of many methods to ob-
tain word vectors?other techniques include La-
tent Semantic Analysis (LSA) (Deerwester et al.,
1990), Latent Dirichlet Allocation (LDA) (Blei et
al., 2003), and variations thereof. And even within
NLMs there exist various architectures for learn-
ing word vectors (Bengio et al. (2003); Mikolov
et al. (2010); Collobert et al. (2011); Yih et
al. (2011)). We utilize an architecture introduced
by Mikolov et al. (2013b), called the Skip-gram,
which allows for efficient estimation of word vec-
tors from large corpora.
In a Skip-gram model, each word in the corpus
is used to predict a window of surrounding words
(Figure 1). To ensure that words closer to the cur-
rent word are given more weight in training, dis-
Figure 1: Architecture of a Skip-gram model (Mikolov et al.,
2013b).
tant words are sampled less frequently.
2
Training
is done through stochastic gradient descent and
backpropagation. The word representations are
found in the hidden layer. Despite its simplicity?
and thus, computational efficiency?compared to
other NLMs, Mikolov et al. (2013b) note that the
Skip-gram is competitive with other vector space
models in the Semantic-Syntactic Word Relation-
ship test set when trained on the same data.
3.1 Training
The Google Books Ngram corpus contains
Ngrams from approximately 8 million books, or
6% of all books published (Lin et al., 2012). We
sample 10 million 5-grams from the English fic-
tion corpus for every year from 1850?2009. We
lower-case all words after sampling and restrict the
vocabulary to words that occurred at least 10 times
in the 1850?2009 corpus.
For the model, we use a window size of 4 and
dimensionality of 200 for the word vectors. Within
each year, we iterate over epochs until conver-
gence, where the measure of convergence is de-
fined as the average angular change in word vec-
tors between epochs. That is, if V (y) is the vo-
cabulary set for year y, and x
w
(y, e) is the word
vector for word w in year y and epoch number e,
we continue iterating over epochs until,
1
|V (y)|
?
w?V (y)
arccos
x
w
(y, e) ? x
w
(y, e? 1)
?x
w
(y, e)??x
w
(y, e? 1)?
is below some threshold. The learning rate is set
to 0.01 at the start of each epoch and linearly de-
creased to 0.0001.
2
Specifically, given a maximum window size of W , a ran-
dom integer R is picked from range [1, W ] for each training
word. The current training word is used to predict R previous
and R future words.
62
Most Changed Least Changed
Word Similarity Word Similarity
checked 0.3831 by 0.9331
check 0.4073 than 0.9327
gay 0.4079 for 0.9313
actually 0.4086 more 0.9274
supposed 0.4232 other 0.9272
guess 0.4233 an 0.9268
cell 0.4413 own 0.9259
headed 0.4453 with 0.9257
ass 0.4549 down 0.9252
mail 0.4573 very 0.9239
Table 1: Top 10 most/least changed words from 1900?2009,
based on cosine similarity of words in 2009 against their 1900
counterparts. Infrequent words (words that occurred less than
500 times) are omitted.
Once the word vectors for year y have con-
verged, we initialize the word vectors for year y+1
with the previous year?s word vectors and train
on the y + 1 data until convergence. We repeat
this process for 1850?2009. Using an open source
implementation in the gensim package, training
took approximately 4 days on a 2.9 GHz machine.
4 Results and Discussion
For the analysis, we treat 1850?1899 as an initial-
ization period and begin our study from 1900.
4.1 Word Comparisons
By comparing the cosine similarity between same
words across different time periods, we are able
to detect words whose usage has changed. We are
also able to identify words that did not change. Ta-
ble 1 has a list of 10 most/least changed words be-
tween 1900 and 2009. We note that almost all of
the least changed words are function words. For
the changed words, many of the identified words
agree with intuition (e.g. gay, cell, ass). Oth-
ers are not so obvious (e.g. checked, headed, ac-
tually). To better understand how these words
have changed, we look at the composition of their
neighboring words for 1900 and 2009 (Table 2).
As a further check, we search Google Books
for sentences that contain the above words. Below
are some example sentences from 1900 and 2009
with the word checked:
1900: ?However, he checked himself in time, saying ??
1900: ?She was about to say something further, but she
checked herself.?
2009: ?He?d checked his facts on a notepad from his back
pocket.?
2009: ?I checked out the house before I let them go inside.?
Word
Neighboring Words in
1900 2009
gay
cheerful lesbian
pleasant bisexual
brilliant lesbians
cell
closet phone
dungeon cordless
tent cellular
checked
checking checking
recollecting consulted
straightened check
headed
haired heading
faced sprinted
skinned marched
actually
evidently really
accidentally obviously
already nonetheless
Table 2: Top 3 neighboring words (based on cosine similar-
ity) specific to each time period for the words identified as
having changed.
At the risk of oversimplifying, the resulting sen-
tences indicate that in the past, checked was more
frequently used with the meaning ?to hold in re-
straint?, whereas now, it is more frequently used
with the meaning ?to verify by consulting an au-
thority? or ?to inspect so as to determine accu-
racy?. Given that check is a highly polysemous
word, this seems to be a case in which the popu-
larity of a word?s sense changed over time.
Conducting a similar exercise for actually, we
obtain the following sentences:
1900: ?But if ever he actually came into property, she must
recognize the change in his position.?
1900: ?Whenever a young gentleman was not actually
engaged with his knife and fork or spoon ??
2009: ?I can?t believe he actually did that!?
2009: ?Our date was actually one of the most fun and
creative ones I had in years.?
Like the above, this seems to be a case in
which the popularity of a word?s sense changed
over time (from ?to refer to what is true or real?
to ?to express wonder or surprise?).
4.2 Periods of Change
As we chronologically train the model year-by-
year, we can plot the time series of a word?s
distance to its neighboring words (from differ-
ent years) to detect periods of change. Figure 2
(above) has such a plot for the word cell compared
to its early neighbors, closet and dungeon, and the
more recent neighbors, phone and cordless. Fig-
ure 2 (below) has a similar plot for gay.
Such plots allow us to identify a word?s pe-
riod of change relative to its neighboring words,
63
Figure 2: (Above) Time trend of the cosine similarity be-
tween cell and its neighboring words in 1900 (closet, dun-
geon) and 2009 (phone, cordless). (Below) Similar plot of
gay and its neighboring words in 1900 (cheerful, pleasant)
and 2009 (lesbian, bisexual).
and thus provide context as to how it evolved.
This may be of use to researchers interested in
understanding (say) when gay started being used
as a synonym for homosexual. We can also iden-
tify periods of change independent of neighboring
words by analyzing the cosine similarity of a word
against itself from a reference year (Figure 3). As
some of the change is due to sampling and random
drift, we additionally plot the average cosine simi-
larity of all words against their reference points in
Figure 3. This allows us to detect whether a word?s
change during a given period is greater (or less)
than would be expected from chance. We note
that for cell, the identified period of change (1985?
2009) coincides with the introduction?and sub-
sequent adoption?of the cell phone by the gen-
eral public.
3
Likewise, the period of change for
gay agrees with the gay movement which began
around the 1970s (Wijaya and Yeniterzi, 2011).
4.3 Limitations
In the present work, identification of a changed
word is conditioned on its occurring often enough
3
http://library.thinkquest.org/04oct/02001/origin.htm
Figure 3: Plot of the cosine similarity of changed (gay, cell)
and unchanged (by, than) words against their 1900 starting
points. Middle line is the average cosine similarity of all
words against their starting points in 1900. Shaded region
corresponds to one standard deviation of errors.
in the study period. If a word?s usage decreased
dramatically (or stopped being used altogether),
its word vector will have remained the same and
hence it will not show up as having changed.
One way to overcome this may be to combine
the cosine distance and the frequency to define a
new metric that measures how a word?s usage has
changed.
5 Conclusions and Future Work
In this paper we provided a method for analyz-
ing change in the written language across time
through word vectors obtained from a chronolog-
ically trained neural language model. Extending
previous work, we are able to not only automat-
ically identify words that have changed but also
the periods during which they changed. While we
have not extensively looked for connections be-
tween periods identified by the model and real his-
torical events, they are nevertheless apparent.
An interesting direction of research could in-
volve analysis and characterization of the differ-
ent types of change. With a few exceptions, we
have been deliberately general in our analysis by
saying that a word?s usage has changed. We have
avoided inferring the type of change (e.g. semantic
vs syntactic, broadening vs narrowing, pejoration
vs amelioration). It may be the case that words that
undergo (say) a broadening in senses exhibit reg-
ularities in how they move about the vector space,
allowing researchers to characterize the type of
change that occurred.
64
References
Y. Bengio, R. Ducharme, P. Vincent. 2003. Neu-
ral Probabilitistic Language Model. Journal of Ma-
chine Learning Research 3:1137?1155.
D. Blei, A. Ng, M. Jordan, J. Lafferty. 2003. Latent
Dirichlet Allocation. Journal of Machine Learning
Research 3:993?1022.
R. Collobert, J. Weston, L. Bottou, M. Karlen, K.
Kavukcuglu, P. Kuksa. 2011. Natural Language
Processing (Almost) from Scratch. Journal of Ma-
chine Learning Research 12:2493?2537.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, R.
Harshman. 2011. Indexing by Latent Semantic
Analysis. Journal of the American Society for In-
formation Science, 41(6):391?407.
K. Gulordava, M. Baroni. 2011. A Distributional
Similarity Approach to the Detection of Semantic
Change in the Google Books Ngram Corpus. Pro-
ceedings of the GEMS 2011 Workshop.
E. Lieberman, J.B. Michel, J. Jackson, T. Tang, M.A.
Nowak. 2007. Quantifying the evolutionary dynam-
ics of language. Nature, 449: 716?716, October.
Y. Lin, J.B. Michel, E.L. Aiden, J. Orwant, W. Brock-
man, S. Petrov. 2012. Syntactic Annotations for the
Google Books Ngram Corpus. Proceedings of the
Association for Computational Linguistics 2012.
J.B Michel, Y.K. Shen, A.P. Aiden, A. Veres, M.K.
Gray, J.P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,
J. Orwant, S.Pinker, M.A. Nowak, E.L. Aiden.
2011. Quantitative Analysis of Culture Using Mil-
lions of Digitized Books. Science, 331(6014): 176?
182, January.
R. Mihalcea, V. Nastase. 2012. Word Epoch Disam-
biguation: Finding How Words Change Over Time.
Proceedings of the Association for Computational
Linguistics 2012.
T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, S.
Khudanpur. 2010. Recurrent Neural Network
Based Language Model. Proceedings of Inter-
speech.
T. Mikolov, W.T Yih, G. Zweig. 2013a. Linguistic
Regularities in Continuous Space Word Representa-
tions. Proceedings of NAACL-HLT 2013, 746?751.
T. Mikolov, K. Chen, G. Corrado, J.Dean. 2013b. Effi-
cient Estimation of Word Representations in Vector
Space arXiv Preprint.
O. Popescu, C. Strapparava. 2013. Behind the Times:
Detecting Epoch Changes using Large Corpora. In-
ternational Joint Conference on Natural Language
Processing, 347?355
E. Sagi, S. Kaufmann, B. Clark 2009. Semantic
Density Analysis: Comparing Word Meaning across
Time and Phonetic Space. Proceedings of the EACL
2009 Workshop on GEMS: 104?111.
D.T. Wijaya, R. Yeniterzi. 2011. Understanding se-
mantic change of words over centuries. Proceed-
ings of the 2011 international workshop on DEtect-
ing and Exploiting Cultural diversiTy on the social
web: 35?40.
W. Yih, K. Toutanova, J. Platt, C. Meek. 2011. Learn-
ing Discriminative Projections for Text Similarity
Measures. Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning,
247?256.
65
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), page 66,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Towards Universal Syntactic Processing of Natural Language
(invited talk)
Slav Petrov
Google Inc.
76 9th Avenue
New York, NY 10011
slav@google.com
1 Abstract
In this talk I will first describe some techniques
for projecting syntactic information across lan-
guage boundaries, allowing us to build models for
languages with no labeled training data. I will
then present some ongoing work towards a univer-
sal representation of morphology and syntax that
makes it possible to model language phenomena
across language boundaries in a consistent way.
Finally, I will highlight some examples of how
we have successfully used syntax at Google to im-
prove downstream applications like question an-
swering and machine translation.
2 Author?s Biography
Slav Petrov is a researcher in Google?s New York
office, leading a team that works on syntactic pars-
ing and its applications to information extraction,
question answering and machine translation. He
holds a PhD degree from UC Berkeley, where he
worked with Dan Klein. Before that he completed
a Master?s degree at the Free University of Berlin
and was a member of the FU-Fighters team that
won the RoboCup world championship in 2004.
His work on fast and accurate multilingual syntac-
tic analysis has been recognized with best paper
awards at ACL 2011 and NAACL 2012. Slav also
teaches Statistical Natural Language Processing at
New York University.
66
