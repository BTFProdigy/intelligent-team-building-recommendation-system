Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 1?9,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
A Morphological Analyzer for Egyptian Arabic
Nizar Habash and Ramy Eskander and Abdelati Hawwari
Center for Computational Learning Systems
Columbia University
New York, NY, USA
{habash,reskander,ahawwari}@ccls.columbia.edu
Abstract
Most tools and resources developed for nat-
ural language processing of Arabic are de-
signed for Modern Standard Arabic (MSA)
and perform terribly on Arabic dialects, such
as Egyptian Arabic. Egyptian Arabic differs
from MSA phonologically, morphologically
and lexically and has no standardized orthog-
raphy. We present a linguistically accurate,
large-scale morphological analyzer for Egyp-
tian Arabic. The analyzer extends an existing
resource, the Egyptian Colloquial Arabic Lex-
icon, and follows the part-of-speech guide-
lines used by the Linguistic Data Consortium
for Egyptian Arabic. It accepts multiple or-
thographic variants and normalizes them to a
conventional orthography.
1 Introduction
Dialectal Arabic (DA) refers to the day-to-day na-
tive vernaculars spoken in the Arab World. DA
is used side by side with Modern Standard Ara-
bic (MSA), the official language of the media and
education (Holes, 2004). Although DAs are his-
torically related to MSA, there are many phono-
logical, morphological and lexical differences be-
tween them. Unlike MSA, DAs have no stan-
dard orthographies or language academies. Fur-
thermore, different DAs, such as Egyptian Arabic
(henceforth, EGY), Levantine Arabic or Moroccan
Arabic have important differences among them sim-
ilar to those seen among Romance languages (Er-
win, 1963; Cowell, 1964; Abdel-Massih et al, 1979;
Holes, 2004). Most tools and resources developed
for natural language processing (NLP) of Arabic are
designed for MSA. Such resources are quite limited
when it comes to processing DA, e.g., a state-of-the-
art MSA morphological analyzer has been reported
to only have 60% coverage of Levantine Arabic verb
forms (Habash and Rambow, 2006). Most efforts to
address this gap have been lacking. Some have taken
a quick-and-dirty approach to model shallow mor-
phology in DA by extending MSA tools, resulting
in linguistically inaccurate models (Abo Bakr et al,
2008; Salloum and Habash, 2011). Others have at-
tempted to build linguistically accurate models that
are lacking in coverage (at the lexical or inflectional
levels) or focusing on representations that are not
readily usable for NLP text processing, e.g., phono-
logical lexicons (Kilany et al, 2002).
In this paper we present the Columbia Ara-
bic Language and dIalect Morphological Analyzer
(CALIMA) for EGY.1 We built this tool by ex-
tending an existing resource for EGY, the Egyptian
Colloquial Arabic Lexicon (ECAL) (Kilany et al,
2002). CALIMA is a linguistically accurate, large-
scale morphological analyzer. It follows the part-of-
speech (POS) guidelines used by the Linguistic Data
Consortium for EGY (Maamouri et al, 2012b). It
accepts multiple orthographic variants and normal-
izes them to CODA, a conventional orthography for
DA (Habash et al, 2012).
The rest of the paper is structured as follows: Sec-
tion 2 presents relevant motivating linguistic facts.
Section 3 discusses related work. Section 4 details
the steps taken to create CALIMA starting with
ECAL. Section 5 presents a preliminary evaluation
and statistics about the coverage of CALIMA. Fi-
nally, Section 6 outlines future plans and directions.
1Although we focus on Egyptian Arabic in this paper, the
CALIMA name will be used in the future to cover a variety of
dialects.
1
2 Motivating Linguistic Facts
We present some general Arabic (MSA/DA) NLP
challenges. Then we discuss differences between
MSA and DA ? specifically EGY.
2.1 General Arabic Linguistic Challenges
Arabic, as MSA or DA, poses many challenges
for NLP. Arabic is a morphologically complex lan-
guage which includes rich inflectional morphology,
expressed both templatically and affixationally, and
several classes of attachable clitics. For example, the
MSA word A? 	E?J.

J?J
?? wa+sa+ya-ktub-uwna+hA
2
?and they will write it? has two proclitics (+? wa+
?and? and +? sa+ ?will?), one prefix -?


ya- ?3rd
person?, one suffix 	??- -uwna ?masculine plural?
and one pronominal enclitic A?+ +hA ?it/her?. The
stem ktub can be further analyzed into the root ktb
and pattern 12u3.
Additionally, Arabic is written with optional dia-
critics that primarily specify short vowels and con-
sonantal doubling, e.g., the example above will most
certainly be written as wsyktbwnhA. The absence of
these diacritics together with the language?s com-
plex morphology lead to a high degree of ambiguity,
e.g., the Standard Arabic Morphological Analyzer
(SAMA) (Graff et al, 2009) produces an average of
12 analyses per MSA word.
Moreover, some letters in Arabic are often spelled
inconsistently which leads to an increase in both
sparsity (multiple forms of the same word) and
ambiguity (same form corresponding to multiple
words), e.g., variants of the Hamzated Alif,

@ ? or
@ A?, are often written without their Hamza (Z ?): @ A.
and the Alif-Maqsura (or dot-less Ya) ? ? and the
regular dotted Ya ?


y are often used interchangeably
in the word-final position (Buckwalter, 2007).
Arabic complex morphology and ambiguity are
handled using tools for disambiguation and tok-
enization (Habash and Rambow, 2005; Diab et al,
2007).
2Arabic orthographic transliteration is presented in the HSB
scheme (Habash et al, 2007): (in alphabetical order)
@ H.
H H h. h p X
	
XP 	P ? ? ?
	
? ?
	
? ?
	
?
	
?

? ? ? ?
	
? ? ? ?


A b t ? j H x d ? r z s ? S D T D? ? ? f q k l m n h w y
and the additional letters: ? Z, ?

@, A? @, A?

@, w? ?', y? Z?', ~ ?, ? ?.
We distinguish between morphological analysis,
whose target is to produce all possible morphologi-
cal/POS readings of a word out of context, and mor-
phological disambiguation, which attempts to tag
the word in context (Habash and Rambow, 2005).
The work presented in this paper is only about mor-
phological analysis.
2.2 Differences between MSA and DA
Contemporary Arabic is in fact a collection of vari-
eties: MSA, which has a standard orthography and
is used in formal settings, and DAs, which are com-
monly used informally and with increasing presence
on the web, but which do not have standard or-
thographies. DAs mostly differ from MSA phono-
logically, morphologically, and lexically (Gadalla,
2000; Holes, 2004). These difference are not mod-
eled as part of MSA NLP tools, leaving a gap in
coverage when using them to process DAs. All ex-
amples below are in Egyptian Arabic (EGY).
Phonologically, the profile of EGY is quite simi-
lar to MSA, except for some important differences.
For example, the MSA consonants

?/
	
X/ H q/?/?
are generally pronounced in EGY (Cairene) as ?/z/s
(Holes, 2004). Some of these consonants shift in dif-
ferent ways in different words: e.g., MSA I.
	
K
	
X ?anb
?fault? and H.
	
Y? ki?b ?lying? are pronounced zanb
and kidb. EGY has five long vowels compared with
MSA?s three long vowels. Unlike MSA, long vow-
els in EGY predictably shorten under certain condi-
tions, often as a result of cliticization. For example,
compare the following forms of the same verb:
	
?A ?
?Af /?a?f/ ?he saw? and A? 	?A ? ?Af+hA /?afha/ ?he saw
her? (Habash et al, 2012).
Morphologically, the most important difference
is in the use of clitics and affixes that do not ex-
ist in MSA. For instance, the EGY equivalent of
the MSA example above is A??J.

J?J
k? wi+Ha+yi-
ktib-uw+hA ?and they will write it?. The optionality
of vocalic diacritics helps hide some of the differ-
ences resulting from vowel changes; compare the
undiacritized forms: EGY wHyktbwhA and MSA
wsyktbwnhA. In this example, the forms of the cli-
tics and affixes are different in EGY although they
have the same meaning; however, EGY has clitics
that are not part of MSA morphology, e.g., the in-
direct pronominal object clitic (+l+uh ?for him?)
2
??A??J.

J?J
k? wi+Ha+yi-ktib-uw+hA+l+uh ?and they
will write it for him?. Another important example is
the circumfix negation ?+ + A? mA+ +? which sur-
rounds some verb forms: ?.

J? A? mA+katab+? ?he
did not write? (the MSA equivalent is two words:
I.

J?K
 ?? lam yaktub). Another important morpho-
logical difference from MSA is that DAs in general
and not just EGY drop the case and mood features
almost completely.
Lexically, the number of differences is very
large. Examples include ?. bas ?only?,

?
	Q
K. Q?
tarabayza~ ?table?, H@Q? mirAt ?wife [of]? and ??X
dawl ?these?, which correspond to MSA ?

?
	
? faqaT,

???A? TAwila~, ?k. ? 	P zawja~ and ZB

?? haw?lA?, re-
spectively.
An important challenge for NLP work on DAs
in general is the lack of an orthographic standard.
EGY writers are often inconsistent even in their
own writing. The differences in phonology between
MSA and EGY are often responsible: words can
be spelled as pronounced or etymologically in their
related MSA form, e.g., H. Y? kidb or H.
	
Y? ki?b.
Some clitics have multiple common forms, e.g., the
future particle h Ha appears as a separate word or as
a proclitic +h/+? Ha+/ha+, reflecting different pro-
nunciations. The different spellings may add some
confusion, e.g., ?J.

J? ktbw may be @?J.

J? katabuwA
?they wrote? or ?J.

J? katabuh ?he wrote it?. Finally,
shortened long vowels can be spelled long or short,
e.g., A?
	
?A ?/ A?
	
? ? ?Af+hA/?f+hA ?he saw her?.
3 Related Work
3.1 Approaches to Arabic Morphology
There has been a considerable amount of work on
Arabic morphological analysis (Al-Sughaiyer and
Al-Kharashi, 2004; Habash, 2010). Altantawy et al
(2011) characterize the various approaches explored
for Arabic and Semitic computational morphology
as being on a continuum with two poles: on one end,
very abstract and linguistically rich representations
and morphological rules are used to derive surface
forms; while on the other end, simple and shallow
techniques focus on efficient search in a space of
precompiled (tabulated) solutions. The first type is
typically implemented using finite-state technology
and can be at many different degrees of sophistica-
tion and detail (Beesley et al, 1989; Kiraz, 2000;
Habash and Rambow, 2006). The second type is typ-
ically implemented using hash-tables with a simple
search algorithm. Examples include the Buckwalter
Arabic Morphological Analyzer (BAMA) (Buck-
walter, 2004), its Standard Arabic Morphological
Analyzer (SAMA) (Graff et al, 2009) incarnation,
and their generation-oriented extension, ALMOR
(Habash, 2007). These systems do not represent the
morphemic, phonological and orthographic rules di-
rectly, and instead compile their effect into the lexi-
con itself, which consists of three tables for prefixes,
stems and suffixes and their compatibilities. A pre-
fix or suffix in this approach is a string consisting of
all the word?s prefixes and suffixes, respectively, as
a single unit (including null affix sequences). Dur-
ing analysis, all possible splits of a word into com-
patible prefix-stem-suffix combination are explored.
More details are discussed in Section 4.5. Numer-
ous intermediate points exist between these two ex-
tremes (e.g., ElixirFM (Smr?, 2007)). Altantawy et
al. (2011) describe a method for converting a lin-
guistically complex and abstract implementation of
Arabic verbs in finite-state machinery into a simple
precompiled tabular representation.
The approach we follow in this paper is closer
to the second type. We start with a lexicon of in-
flected forms and derive from it a tabular represen-
tation compatible with the SAMA system for MSA.
However, as we do this, we design the tables and ex-
tend them in ways that capture generalizations and
extend orthographic coverage.
3.2 Arabic Dialect Morphology
The majority of the work discussed above has fo-
cused on MSA, while only a few efforts have tar-
geted DA morphology (Kilany et al, 2002; Riesa
and Yarowsky, 2006; Habash and Rambow, 2006;
Abo Bakr et al, 2008; Salloum and Habash, 2011;
Mohamed et al, 2012). These efforts generally fall
in two camps. First are solutions that focus on ex-
tending MSA tools to cover DA phenomena. For
example, both Abo Bakr et al (2008) and Salloum
and Habash (2011) extended the BAMA/SAMA
databases (Buckwalter, 2004; Graff et al, 2009) to
accept DA prefixes and suffixes. Both of these ef-
forts were interested in mapping DA text to some
MSA-like form; as such they did not model DA lin-
3
guistic phenomena, e.g., the ADAM system (Sal-
loum and Habash, 2011) outputs only MSA diacrit-
ics that are discarded in later processing.
The second camp is interested in modeling DA di-
rectly. However, the attempts at doing so are lacking
in coverage in one dimension or another. The earli-
est effort on EGY that we know of is the Egyptian
Colloquial Arabic Lexicon (ECAL) (Kilany et al,
2002). It was developed as part of the CALLHOME
Egyptian Arabic (CHE) corpus (Gadalla et al, 1997)
which contains 140 telephone conversations and
their transcripts. The lexicon lists all of the words
appearing in the CHE corpus and provides phono-
logical, orthographic and morphological informa-
tion for them. This is an important resource; how-
ever, it is lacking in many ways: the orthographic
forms are undiacritized, no morpheme segmenta-
tions are provided, and the lexicon has only some
66K fully inflected forms and as such lacks general
morphological coverage. Another effort is the work
by Habash and Rambow (2006) which focuses on
modeling DAs together with MSA using a common
multi-tier finite-state-machine framework. Although
this approach has a lot of potential, in practice, it
is closer to the first camp in its results since they
used MSA lexicons as a base. Finally, two previ-
ous efforts focused on modeling shallow dialectal
segmentation using supervised methods (Riesa and
Yarowsky, 2006; Mohamed et al, 2012). Riesa and
Yarowsky (2006) presented a supervised algorithm
for online morpheme segmentation for Iraqi Arabic
that cut the out-of-vocabulary rates by half in the
context of machine translation into English. Mo-
hamed et al (2012) annotated a collection of EGY
for morpheme boundaries and used this data to de-
velop an EGY tokenizer. Although these efforts
model DA directly, they remain at a shallow level
of representation (undiacritized surface morph seg-
mentation).
We use the ECAL lexicon as a base for CAL-
IMA and extend it further. Some of the expansion
techniques we used are inspired by previous solu-
tions (Abo Bakr et al, 2008; Salloum and Habash,
2011). For the morphological representation, we
follow the Linguistic Data Consortium guidelines
which extend the MSA POS guidelines to multi-
ple dialects (Maamouri et al, 2006; Maamouri et
al., 2012b). To address the problem of orthographic
variations, we follow the proposal by Habash et al
(2012) who designed a conventional orthography for
DA (or CODA) for NLP applications in the CAL-
IMA databases. However, to handle input in a vari-
ety of spellings, we extend our analyzer to accept
non-CODA-compliant word forms but map them
only to CODA-compliant forms as part of the anal-
ysis.
4 Approach
We describe next the various steps for creating
CALIMA starting with ECAL. The details of the
approach are to some degree dependent on this
unique resource; however, some aspects of the ap-
proach may be generalizable to other resources, and
languages or dialects.
4.1 The Egyptian Colloquial Arabic Lexicon
ECAL has about 66K entries: 27K verbs, 36K
nouns and adjectives, 1.5K proper nouns and 1K
closed classes. For each entry, the lexicon pro-
vides a phonological form, an undiacritized Ara-
bic script orthography, a lemma (in phonological
form), and morphological features, among other
information. There are 36K unique lemmas and
1,464 unique morphological feature combinations.
The following is an example ECAL entry for the
word ?????J
J.? mbyklmw? ?he did not talk to him?.
3
We only show Arabic orthography, phonology, and
lemma+features:
mbyklmw?
mabiykallimU$4
kallim:verb+pres-3rd-masc-sg+DO-3rd-masc-sg+neg
Our goal for CALIMA is to have a much larger
coverage, a CODA-compliant diacritized orthogra-
phy, and a morpheme-based morphological analysis.
The next steps allow us to accomplish these goals.
4.2 Diacritic Insertion
First, we built a component to diacritize the ECAL
undiacritized Arabic script entries in a way that is
consistent with ECAL phonological form. This was
implemented using a finite-state transducer (FST)
that maps the phonological form to multiple possible
3The same orthographic form has another reading ?they did
not talk? which of course has different morphological features.
4The phonological form as used in ECAL. For transcrip-
tion details, see (Kilany et al, 2002).
4
diacritized Arabic script forms. The form that is the
same as the undiacritized ECAL orthography (ex-
cept for diacritics) is used as the diacritized orthog-
raphy for the rest of the process. The FST consists of
about 160 transformations that we created manually.
All except for 100 cases are generic mappings, e.g.,
two repeated b consonants are turned into H. b?,
5
or a short vowel u can be orthographically a short
vowel (just the diacritic u) or a long vowel uw which
shortened. The exceptional 100 cases were specified
by hand in the FST as complete string mappings.
These were mostly odd spellings of foreign words
or spelling errors. We did not attempt to correct or
change the ECAL letter spelling; we only added di-
acritics.
After diacritization, we modify the Arabic orthog-
raphy in the example above to: mabiykal~imuw?.
4.3 Morphological Tag Mapping
Next, we wrote rules to convert from ECAL
diacritized Arabic and morphology to CODA-
compliant diacritized Arabic and LDC EGY POS
compliant tags. The rules fall into three categories:
ignore rules specify which ECAL entries to ex-
clude due to errors; correction rules correct for some
ECAL entry errors; and prefix/suffix/stem rules are
used to identify specific pairs of prefix/suffix/stem
substrings and morphological features to map to
appropriate prefix/suffix/stem morphemes, respec-
tively. For stems, the majority of the rules also
identify roots and patterns. Since multiple root-
pattern combinations may be possible for a partic-
ular word, the appropriate root-pattern is chosen by
enforcing consistency across all the inflected forms
of the lemma of the word and minimizing the over-
all number of roots in the system. We do not use
or report on root-patterns in CALIMA in this paper
since this information is not required by the LDC
tags; however, we plan on using them in future ef-
forts exploiting templatic morphology.
At the time of writing this paper, the system in-
cluded 4,632 rules covering all POS. These include
1,248 ignore rules, 1,451 correction rules, 83 pre-
fix rules, and 441 suffixes rules. About 1,409 stem
rules are used to map core POS tags and iden-
tify templatic roots and patterns. Some rules were
5The ? diacritic or Shadda indicates the presence of conso-
nantal doubling.
semi-automatically created, but all were manually
checked. The rules are specified in a simple format
that is interpreted and applied by a separate rule pro-
cessing script. Developing the script and writing the
rules took about 3 person-months of effort.
As an example, the following three rules are used
to handle the circumfix ma++? ?not? and the pro-
gressing particle bi+.
PRE: ma,+neg => ? ,+neg >> mA/NEG_PART#
PRE: bi,+pres => ? ,+subj >> bi/PROG_PART+
SUF: ?,+neg => ?,? >> +?/NEG_PART
The input to the rule processor is a pair of surface
form and morphological features. Each rule matches
on a surface substring and a combination of mor-
phological features (first two comma-separated to-
kens in the rule) and rewrites the parts it matched
on (second two comma-separated tokens in the rule
after =>). The type of the rule, i.e. prefix or suf-
fix rule, determines how the matching is applied. In
addition, the rule generates a substring of the tar-
get tag (last token in the rule). The first and third
rules above handle a circumfix; the +neg feature is
not deleted in the first rule (which handles the pre-
fix) to allow the third rule (which handles the suffix)
to fire. The second rule rewrites the feature +pres
(present tense) as +subj (subjunctive) which is con-
sistent with the form of the verb after removing the
progressive particle bi+. After applying these rules
in addition to a few others, the above example is
turned into CODA and EGY POS compliant forms
(# means word boundary):6
mA#bi+yi+kal~im+huw+?
NEG_PART#PROG_PART+IV3MS+IV+IVSUFF_DO:3MS+NEG_PART
The stem rules, whose results are not shown here,
determine that the root is klm and the pattern is
1a22i3.
We extended the set of mapped ECAL entries
systematically. We copied entries and modified them
to include additional clitics that are not present with
all entries, e.g., the conjunction +
	
? fa+ ?then?, and
the definite article +?@ Al+.
4.4 Orthographic Lemma Identification
The ECAL lemmas are specified in a phonological
form, e.g., in the example above, it is kallim. To de-
termine the diacritized Arabic orthography spelling
6CODA guidelines state that the negative particle A? mA is
not to be cliticized except in a very small number of words
(Habash et al, 2012).
5
of the lemma, we relied on the existence of the
lemma itself as an entry and other ad hoc rules to
identify the appropriate form. Using this technique,
we successfully identified the orthographic lemma
form for 97% of the cases. The remainder were
manually corrected. We followed the guidelines for
lemma specification in SAMA, e.g., verbs are cited
using the third person masculine singular perfective
form. For our example, the CALIMA lemma is
kal?im.
4.5 Table Construction
We converted the mapped ECAL entries to a
SAMA-like representation (Graff et al, 2009). In
SAMA, morphological information is stored in six
tables. Three tables specify complex prefixes, com-
plex suffixes and stems. A complex prefix/suffix is
a set of prefix/suffix morphemes that are treated as a
single database entry, e.g., wi+Ha+yi is a complex
prefix made of three prefix morphemes. Each com-
plex prefix, complex suffix and stem has a class cat-
egory which abstract away from all similarly behav-
ing complex affixes and stems. The other three ta-
bles specify compatibility across the class categories
(prefix-stem, prefix-suffix and stem-suffix). We ex-
tracted triples of prefix-stem-suffix and used them to
build the six SAMA-like tables. The generated ta-
bles are usable by the sama-analyze engine provided
as part of SAMA3.1 (Graff et al, 2009). We also
added back off mode support for NOUN_PROP.
Prefix/stem/suffix class categories are generated
automatically. We identified specific features of the
word?s stem and affixes to generate specific affix
classes that allow for correct coverage expansion.
For example, in a complex suffix, the first morpheme
is the only one interacting with the stem. As such,
there is no need to give each complex suffix its own
class category, but rather assign the class category
based on the first morpheme. This allows us to auto-
matically extend the coverage of the analyzer com-
pared to that of the ECAL lexicon.
We also go further in terms of generalizations. For
instance, some of the pronoun clitics in EGY have
two forms that depend on whether the stem ends
with vowel-consonant or two consonants, e.g., A?E. A

J?
kitAb+hA ?her book? as opposed to A? 	DK. @ Aibn+ahA
?her son?. This information is used to give the suf-
fixes +hA and +ahA different class categories that
are generalizable to other similarly behaving clitics.
At this stage of our system, which we refer to as
CALIMA-core in Section 5.2, there are 252 unique
complex prefixes and 550 unique complex suffixes,
constructed from 43 and 86 unique simple prefixes
and suffixes, respectively. The total number of pre-
fix/suffix class categories is only 41 and 78, respec-
tively.
4.6 Various Table Extensions
We extended the CALIMA-core tables in a simi-
lar approach to the extension of SAMA tables done
by Salloum and Habash (2011). We distinguish two
types of extensions.
Additional Clitics and POS Tags We added a
number of clitics and POS tags that are not part of
ECAL, e.g., the prepositional clitic +? Ea+ ?on?
and multiple POS tags for the proclitic +
	
? fa+ (as
CONJ, SUB_CONJ and CONNEC_PART). Here we copied a
related entry and modified it but kept its category
class. For example, in the case of +? Ea+ ?on?, we
copied a prepositional clitic with similar distribution
and behavior: +H. bi+ ?with?.
Non-CODA Orthography Support We extended
the generated tables to include common non-CODA
orthographic variants. The following are some ex-
amples of the expansions. First, we added the vari-
ant ?+ +w for two suffixes: ?+ +uh ?his/him? and
@?+ +uwA ?they/you [plural]?. Second, we added
the form ha+ for the future particle Ha+. Third,
we introduced non-CODA-compliant Hamza forms
as variants for some stems. Finally, some of the
extensions target specific stems of frequently used
words, such as the adverb ? 	?QK. brDh ?also? which
can be written as ?XQK. brdh and ?
	
?QK. brDw among
other forms. The non-CODA forms are only used to
match on the input word, with the returned analysis
being a corrected analysis. For example, the word
?J.

J?J
? hyktbw returns the analysis @?J.

J?J
k Hyk-
tbwA Ha/FUT_PART+yi/IV3P+ktib/IV+uwA/3P ?they
will write? among other analyses. The orthographic
variations supported include 16 prefix cases, 41 stem
cases, and eight suffix cases.
After all the clitic, POS tag and orthographic ex-
tensions, the total number of complex prefix entries
6
substantially increases from 352 to 2,421, and the
number of complex suffix entries increases from 826
to 1,179. The number of stem entries increases from
around 60K to 100K. The total number of recogniz-
able word forms increases from 4M to 48M. We will
refer to the system with all the extensions as CAL-
IMA in Section 5.
5 Current Status
In this section, we present some statistics on the cur-
rent status of the CALIMA analyzer. As with all
work on morphological analyzers, there are always
ways to improve the quality and coverage.
5.1 System Statistics
CALIMA has 100K stems corresponding to 36K
lemmas. There are 2,421 complex prefixes and
1,179 complex suffixes (unique diacritized form and
POS tag combinations). The total number of ana-
lyzable words by CALIMA is 48M words (com-
pared to the 66K entries in ECAL). This is still lim-
ited compared to the SAMA3.1 analyzer (Graff et
al., 2009) whose coverage of MSA reaches 246M
words. See Table 1.
5.2 Coverage Evaluation
We tested CALIMA against a manually annotated
EGY corpus of 3,300 words (Maamouri et al,
2012a) which was not used as part of its develop-
ment, i.e., a completely blind test.7 This evaluation
is a POS recall evaluation. It is not about selecting
the correct POS answer in context. We do not con-
sider whether the diacritization or the lemma choice
are correct or not. We compare CALIMA coverage
with that of ECAL and a state-of-the-art MSA an-
alyzer, SAMA3.1 (Graff et al, 2009). For the pur-
pose of completeness, we also compare CALIMA-
core and an extended version of SAMA3.1. The
SAMA3.1 extensions include two EGY verbal pro-
clitics (Ha/FUT_PART and bi/PROG_PART), some alter-
native suffixes that have no case or mood, and all the
orthographic variations used inside CALIMA. We
7We ignore some specific choices made by the annotators,
most importantly the use of ".VN" to mark verbal nominals,
which is not even supported in SAMA3.1. We also ignore
some annotation choices that are not consistent with the latest
LDC guidelines (Maamouri et al, 2012b), such as using gender-
marked plurals in some contexts, e.g., 3MP instead of 3P.
also compare the performance of different merged
versions of SAMA3.1 and CALIMA. The results
are presented in Table 1.
The second column in Table 1, Correct Answer
indicates the percentage of the test words whose cor-
rect analysis in context appears among the analyses
returned by the analyzer. The third column, No Cor-
rect Answer, presents the percentage of time one or
more analyses are returned, but none matching the
correct answer. The fourth column, No Analysis, in-
dicates the percentage of words returning no anal-
yses. The last column presents the total number of
recognizable words in the system.
CALIMA provides among its results a correct an-
swer for POS tags over 84% of the time. This is al-
most 27% absolute over the original list of words
from ECAL and almost 21% absolute over the
SAMA3.1 system. The various extensions in CAL-
IMA give it about 10% absolute over CALIMA-
core (and increase its size 10-fold). The limited
extensions to SAMA3.1 reduce the difference be-
tween it and CALIMA-core by 50% relative. The
overall performance of CALIMA-core merged with
SAMA3.1 is comparable to CALIMA, although
CALIMA has three times the number of no-analysis
cases. Merging CALIMA and extended SAMA3.1
increases the performance to 92%, an 8% absolute
increase over CALIMA alone. The final rate of no-
analysis cases is only 1%.
5.3 Error Analysis
We analyzed a sample of 100 cases where no an-
swer was found (No Correct Answer + No Analy-
sis) for CALIMA+extended SAMA3.1. About a
third of the cases (30%) are due to gold tag errors.
Irrecoverable typographical errors occur 5% of the
time, e.g., 	?

	
? fyn instead of ?


	
? fy ?in?. Only 2%
of the cases involve a speech effect, e.g., ?J

J

J
?g.
jmyyyyyl ?beautiful!!!?. A fifth of the cases (22%) in-
volve a non-CODA orthographic choice which was
not extended, e.g., the shortened long vowel in HAm.k
HjAt instead of the CODA-compliant HAg. Ag HAjAt
?things?. Another fifth of the cases (20%) are due to
incomplete paradigms, i.e., the lemma exists but not
the specific inflected stem. Finally, 21% of the cases
receive a SAMA3.1 analysis that is almost correct,
except for the presence of some mood/case mark-
7
Correct Answer No Correct Answer No Analysis Words
ECAL 57.4% 14.7% 27.9% 66K
SAMA3.1 63.7% 27.1% 9.3% 246M
extended SAMA3.1 68.8% 24.9% 6.3% 511M
CALIMA-core 73.9% 10.8% 15.3% 4M
CALIMA 84.1% 8.0% 7.9% 48M
CALIMA-core + SAMA3.1 84.4% 12.8% 2.8% 287M
CALIMA + extended SAMA3.1 92.1% 7.0% 1.0% 543M
Table 1: Comparison of seven morphological analysis systems on a manually annotated test set. The second column
indicates the percentage of the test words whose correct analysis in context appears among the analyses returned by the
analyzer. The third column presents the percentage of time one or more analyses are returned, but none matching the
correct answer. The fourth column indicates the percentage of words returning no analyses. The last column presents
the total number of recognizable words in the system.
ers that are absent in EGY, and which we did not
handle. Overall, these are positive results that sug-
gest the next steps should involve additional ortho-
graphic and morphological extensions and paradigm
completion.
6 Outlook
We plan to continue improving the coverage of
CALIMA using a variety of methods. First, we are
investigating techniques to automatically fill in the
paradigm gaps using information from multiple en-
tries in ECAL belonging to different lemmas that
share similar characteristics, e.g., hollow verbs in
Form I. Another direction is to update our tables
with less common orthographic variations, perhaps
using information from the phonological forms in
ECAL. Manual addition of specific entries will also
be considered to fill in lexicon gaps. Furthermore,
we plan to add additional features which we did not
discuss such as the English and MSA glosses for all
the entries in CALIMA. We also plan to make this
tool public so it can be used by other people work-
ing on EGY NLP tasks, from annotating corpora to
building morphological disambiguation tools.
Acknowledgments
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-12-C-0014.
Any opinions, findings and conclusions or recom-
mendations expressed in this paper are those of
the authors and do not necessarily reflect the views
of DARPA. We thank Mohamed Maamouri, Owen
Rambow, Seth Kulick, Mona Diab and Mike Ciul,
for helpful discussions and feedback.
References
Ernest T. Abdel-Massih, Zaki N. Abdel-Malek, and El-
Said M. Badawi. 1979. A Reference Grammar of
Egyptian Arabic. Georgetown University Press.
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim Ziedan.
2008. A Hybrid Approach for Converting Written
Egyptian Colloquial Dialect into Diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008. Cairo University.
Imad Al-Sughaiyer and Ibrahim Al-Kharashi. 2004.
Arabic Morphological Analysis Techniques: A Com-
prehensive Survey. Journal of the American Society
for Information Science and Technology, 55(3):189?
213.
Mohamed Altantawy, Nizar Habash, and Owen Ram-
bow. 2011. Fast Yet Rich Morphological Analysis.
In Proceedings of the 9th International Workshop on
Finite-State Methods and Natural Language Process-
ing (FSMNLP 2011), Blois, France.
Kenneth Beesley, Tim Buckwalter, and Stuart Newton.
1989. Two-Level Finite-State Analysis of Arabic Mor-
phology. In Proceedings of the Seminar on Bilingual
Computing in Arabic and English, page n.p.
Tim Buckwalter. 2004. Buckwalter arabic morpho-
logical analyzer version 2.0. LDC catalog number
LDC2004L02, ISBN 1-58563-324-0.
Tim Buckwalter. 2007. Issues in Arabic Morphologi-
cal Analysis. In A. van den Bosch and A. Soudi, edi-
tors, Arabic Computational Morphology: Knowledge-
based and Empirical Methods. Springer.
Mark W. Cowell. 1964. A Reference Grammar of Syrian
Arabic. Georgetown University Press.
8
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky, 2007.
Arabic Computational Morphology: Knowledge-
based and Empirical Methods, chapter Automated
Methods for Processing Arabic Text: From Tokeniza-
tion to Base Phrase Chunking. Springer.
Wallace Erwin. 1963. A Short Reference Grammar of
Iraqi Arabic. Georgetown University Press.
Hassan Gadalla, Hanaa Kilany, Howaida Arram, Ashraf
Yacoub, Alaa El-Habashi, Amr Shalaby, Krisjanis
Karins, Everett Rowson, Robert MacIntyre, Paul
Kingsbury, David Graff, and Cynthia McLemore.
1997. CALLHOME Egyptian Arabic Transcripts. In
Linguistic Data Consortium, Philadelphia.
Hassan Gadalla. 2000. Comparative Morphology of
Standard and Egyptian Arabic. LINCOM EUROPA.
David Graff, Mohamed Maamouri, Basma Bouziri,
Sondos Krouna, Seth Kulick, and Tim Buckwal-
ter. 2009. Standard Arabic Morphological Analyzer
(SAMA) Version 3.1. Linguistic Data Consortium
LDC2009E73.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 573?580, Ann
Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
A Morphological Analyzer and Generator for the Ara-
bic Dialects. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 681?688, Sydney, Australia.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.
Nizar Habash, Mona Diab, and Owen Rabmow. 2012.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalua-
tion Conference (LREC), Istanbul.
Nizar Habash. 2007. Arabic Morphological Representa-
tions for Machine Translation. In Antal van den Bosch
and Abdelhadi Soudi, editors, Arabic Computational
Morphology: Knowledge-based and Empirical Meth-
ods. Kluwer/Springer.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Clive Holes. 2004. Modern Arabic: Structures, Func-
tions, and Varieties. Georgetown Classics in Ara-
bic Language and Linguistics. Georgetown University
Press.
H. Kilany, H. Gadalla, H. Arram, A. Yacoub, A. El-
Habashi, and C. McLemore. 2002. Egyptian
Colloquial Arabic Lexicon. LDC catalog number
LDC99L22.
George Anton Kiraz. 2000. Multi-Tiered Nonlinear
Morphology Using Multi-Tape Finite Automata: A
Case Study on Syriac and Arabic. Computational Lin-
guistics, 26(1):77?105.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, Mona
Diab, Nizar Habash, Owen Rambow, and Dalila
Tabessi. 2006. Developing and Using a Pilot Dialectal
Arabic Treebank. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC), Genoa,
Italy.
Mohamed Maamouri, Ann Bies, Seth Kulick, Dalila
Tabessi, and Sondos Krouna. 2012a. Egyptian Ara-
bic Treebank Pilot.
Mohamed Maamouri, Sondos Krouna, Dalila Tabessi,
Nadia Hamrouni, and Nizar Habash. 2012b. Egyptian
Arabic Morphological Annotation Guidelines.
Emad Mohamed, Behrang Mohit, and Kemal Oflazer.
2012. Annotating and Learning Morphological Seg-
mentation of Egyptian Colloquial Arabic. In Proceed-
ings of the Language Resources and Evaluation Con-
ference (LREC), Istanbul.
Jason Riesa and David Yarowsky. 2006. Minimally Su-
pervised Morphological Segmentation with Applica-
tions to Machine Translation. In Proceedings of the
7th Conference of the Association for Machine Trans-
lation in the Americas (AMTA06), pages 185?192,
Cambridge,MA.
Wael Salloum and Nizar Habash. 2011. Dialectal
to Standard Arabic Paraphrasing to Improve Arabic-
English Statistical Machine Translation. In Proceed-
ings of the First Workshop on Algorithms and Re-
sources for Modelling of Dialects and Language Va-
rieties, pages 10?21, Edinburgh, Scotland.
Otakar Smr?. 2007. Functional Arabic Morphology. For-
mal System and Implementation. Ph.D. thesis, Charles
University in Prague, Prague, Czech Republic.
9
Workshop on Computational Linguistics for Literature, pages 78?83,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
A Pilot PropBank Annotation for Quranic Arabic 
 
                  Wajdi Zaghouani 
            University of Pennsylvania 
                Philadelphia, PA USA 
     wajdiz@ldc.upenn.edu 
               Abdelati Hawwari and Mona Diab    
              Center for Computational Learning Systems 
            Columbia University, NYC, USA 
      {ah3019,mdiab}@ccls.columbia.edu 
 
 
 
  
Abstract 
The Quran is a significant religious text written in a 
unique literary style, close to very poetic language 
in nature. Accordingly it is significantly richer and 
more complex than the newswire style used in the 
previously released Arabic PropBank (Zaghouani 
et al, 2010; Diab et al, 2008). We present prelimi-
nary work on the creation of a unique Arabic prop-
osition repository for Quranic Arabic. We annotate 
the semantic roles for the 50 most frequent verbs in 
the Quranic Arabic Dependency Treebank (QATB) 
(Dukes and Buckwalter 2010). The Quranic Arabic 
PropBank (QAPB) will be a unique new resource 
of its kind for the Arabic NLP research community 
as it will allow for interesting insights into the 
semantic use of classical Arabic, poetic literary 
Arabic, as well as significant religious texts. More-
over, on a pragmatic level QAPB will add approx-
imately 810 new verbs to the existing Arabic 
PropBank (APB). In this pilot experiment, we 
leverage our knowledge and experience from our 
involvement in the APB project. All the QAPB 
annotations will be made freely available for re-
search purposes. 
1 Introduction 
Explicit characterization of the relation between 
verbs and their arguments has become an impor-
tant issue in sentence processing and natural lan-
guage understanding. Automatic Semantic role 
labeling [SRL] has become the correlate of this 
characterization in natural language processing 
literature (Gildea and Jurafsky 2002). In SRL, the 
system automatically identifies predicates and their 
arguments and tags the identified arguments with 
meaningful semantic information. SRL has been 
successfully used in machine translation, summari-
zation and information extraction. In order to build 
robust SRL systems there is a need for significant 
resources the most important of which are seman-
tically annotated resources such as proposition 
banks. Several such resources exist now for differ-
ent languages including FrameNet (Baker et al, 
1998), VerbNet (Kipper et al 2000) and PropBank 
(Palmer et al, 2005). These resources have marked 
a surge in efficient approaches to automatic SRL of 
the English language. Apart from English, there 
exist various PropBank projects in Chinese (Xue et 
al., 2009), Korean (Palmer et al 2006) and Hindi 
(Ashwini et al, 2011). These resources exist on a 
large scale spearheading the SRL research in the 
associated languages (Carreras and Marquez, 
2005), Surdeanu et al (2008). However, resources 
created for Arabic are significantly more modest. 
The only Arabic Propank [APB] project (Zaghoua-
ni et al, 2010; Diab et al, 2008) based on the 
phrase structure syntactic Arabic Treebank (Maa-
mouri et al 2010) comprises a little over 4.5K 
verbs of newswire modern standard Arabic. Apart 
from the modesty in size, the Arabic language 
genre used in the APB does not represent the full 
scope of the Arabic language. The Arabic culture 
has a long history of literary writing and a rich 
linguistic heritage in classical Arabic. In fact all 
historical religious non-religious texts are written 
in Classical Arabic. The ultimate source on clas-
sical Arabic language is the Quran. It is considered 
the Arabic language reference point for all learners 
of Arabic in the Arab and Muslim world. Hence 
understanding the semantic nuances of Quranic 
Arabic is of significant impact and value to a large 
population. This is apart from its significant differ-
ence from the newswire genre, being closer to 
poetic language and more creative linguistic ex-
78
pression. Accordingly, in this paper, we present a 
pilot annotation project on the creation a Quranic 
Arabic PropBank (QAPB) on layered above the 
Quranic Arabic Dependency Treebank (QATB) 
(Dukes and Buckwalter 2010).  
2 The PropBank model  
The PropBank model is a collection of annotated 
propositions where each verb predicate is anno-
tated with its semantic roles. An existing syntactic 
treebank is typically a prerequisite for this shallow 
semantic layer. For example consider the following 
English sentence: ?John likes apples?, the predicate 
is ?likes? and the first argument, the subject, is 
?John?, and the second argument, the object, is 
?apples?. ?John? would be semantically annotated 
as the agent and ?apples? would be the theme. Ac-
cording to PropBank, ?John? is labeled ARG0 and 
?apples? is labeled ARG1. Crucially, regardless of 
the adopted semantic annotation formalism (Prop-
Bank, FrameNet, etc), the labels do not vary in 
different syntactic constructions, which is why 
proposition annotation is different from Treebank 
annotation. For instance, if the example above was 
in the passive voice, ?Apples are liked by John?, 
John is still the agent ARG0, and Apples are still 
the theme ARG1.  
3 Motivation and Background 
The main goal behind this project is to extend cov-
erage of the existing Arabic PropBank (APB) to 
more verbs and genres (Zaghouani et al 2010; 
Diab et al 2008). APB is limited to the newswire 
domain in modern standard Arabic (MSA). It sig-
nificantly lags behind the English PropBank (EPB) 
in size. EPB consists of 5413 verbs corresponding 
to 7268 different verb senses, the APB only covers 
2127 verb types corresponding to 2657 different 
verb senses. According to El-Dahdah (2008) Arab-
ic Dictionary, there are more than 16,000 verbs in 
the Arabic language. The Quran corpus comprises 
a total of 1466 verb types including 810 not 
present in APB. Adding the 810 verbs to the APB 
is clearly a significant boost to the size of the APB 
(38% amounting to 2937 verb types).  
In the current paper however we address the 
annotation of the Quran as a stand alone resource 
while leveraging our experience in the APB anno-
tation process. The Quran consists of 1466 verb 
types corresponding to 19,356 verb token in-
stances. The language of the Quran is Classical 
Arabic (CA) of 77,430 words, sequenced in chap-
ters and verses, dating back to over 1431 years. It 
is considered a reference text on both religious as 
well as linguistic matters. The language is fully 
specified with vocalic and pronunciation markers 
to ensure faithful oration. The language is poetic 
and literary in many instances with subtle allusions 
(Zahri 1990). It is the source of many other reli-
gious and heritage writings and a book of great 
importance to muslims worldwide, including non 
speakers of Arabic.  
Dukes and Buckwalter (2010) started the Qu-
ranic Arabic Corpus, an annotated linguistic re-
source which marks the Arabic grammar, syntax 
and morphology for each word. The QATB pro-
vides two levels of analysis: morphological annota-
tion and syntactic representation. The syntax of 
traditional Arabic grammar is represented in the 
Quranic Treebank using hybrid dependency graphs 
as shown in Figure 1.1 To the best of our know-
ledge, this is the first PropBank annotation of a 
religious and literary style text. 
The new verbs added from the Quran are also 
common verbs widely used today in MSA but the 
Quranic context adds more possible senses to these 
verbs. Having a QAPB allows for a more semantic 
level of analysis to the Quran. Currently the Qu-
ranic Corpus Portal2 comprises morphological 
annotations, syntactic treebanks, and a semantic 
ontology. Adding the QAPB will render it a unique 
source for Arabic language scholars worldwide 
(more than 50,000 unique visitors per day).  
Linguistic studies of the Quranic verbs such as 
verbal alternations, verb valency, polysemy and 
verbal ambiguity are one of the possible research 
directions that could be studied with this new re-
source. On the other hand, the Arabic NLP re-
search community will benefit from the increased 
coverage of the APB verbs, and the new domain 
covered (religious) and the new writing style (Qu-
ranic Arabic). Furthermore, Quranic citations are 
commonly used today in MSA written texts 
(books, newspapers, etc.), as well as Arabic social 
media intertwined with dialectal writings. This 
                                                 
1This display is different from the other existing Arabic Tree-
bank, the Prague Arabic Dependency Treebank (PADT) (Smr? 
et al, 2008). 
2http://corpus.quran.com/ 
79
makes the annotation of a Quranic style a rare and 
relevant resource for the building of Arabic NLP 
applications.  
4 Methodology 
We leverage the approach used with the previous 
APB (Zaghouani et al 2010; Diab et al 2008). We 
pay special attention to the polysemic nature of 
predicates used in Quranic Arabic. An Arabic root 
meaning tool is used as a reference to help in iden-
tifying different senses of the verb. More effort is 
dedicated to revision of the final product since 
unlike the APB, the QAPB is based on a depen-
dency Treebank (QATB) not a phrase structure 
Treebank.3 
For this pilot annotation experiment, we on-
ly annotate the 50 most frequent verbs in the cor-
pus corresponding to 7227 verbal occurrences in 
the corpus out of 19,356 total verbal instances. In 
the future plans, the corpus will cover eventually 
all the 1466 verbs in the whole Quranic corpus. 
Ultimately, it is our plan to perform a merging 
between the new frame files of the QAPB and the 
existing 1955 Frame files of the Arabic PropBank 
 
4.1 The annotation process 
 
The PropBank annotation process is divided into 
two steps: a. creation of the frame files for verbs 
occurring in the data, and b. annotation of the ver-
bal instances with the frame file ids. During the 
creation of the Frame Files, the usages of the verbs 
in the data are examined by linguists (henceforth, 
?framers?). During the frameset creation process, 
verbs that share similar semantic and syntactic 
characteristics are usually framed similarly). Once 
a predicate (in this case a verb) is chosen, framer-
look at an average sample size of 60-70 instances 
per predicate found in the Quranic corpus in order 
to get an idea of its syntactic behavior. Based on 
these observations and their linguistic knowledge 
and native-speaker intuition, the framers create a 
Frame File for each verb containing one or more 
framesets, which correspond to coarse-grained 
senses of the predicate lemma. Each frameset spe-
cifies the PropBank core labels (i.e., ARG0, 
                                                 
3 The Propbank style of annotation are already used with other 
languages on top of dependency Treebank structures such as 
the Hindi  Treebank project (Ashwini et al, 2011).  
ARG1,?ARG4) corresponding to the argument 
structure of the verb. Additionally, illustrative ex-
amples are included for each frameset, which will 
later be referenced by the annotators. Note that in 
addition to these core, numbered roles, PropBank 
also includes annotations of a variety of modifier 
roles, prefixed by ARGM labels from a list of 15 
arguments (ARGM-ADV, ARGM-BNF, ARGM-
CAU,ARGM-CND, ARGM-DIR, ARGM-DIS, 
ARGM-EXT, ARGM-LOC, ARGM-MNR, 
ARGM-NEG, ARGM-PRD, ARGM-PRP, ARGM-
REC, ARGM-TMP, ARGM-PRD). Unlike the 
APB frame files creation, where no specific Arabic 
reference is used, for this project, an Arabic root 
meaning reference tool developed by Swalha 
(2011) is used by the framers to ensure that all 
possible meanings of the verbs in the corpus are 
covered and all various senses are taken into ac-
count. The Arabic root-meaning search tool is free-
ly available online.4 The search is done by root, the 
tool displays all possible meanings separated by a 
comma with citation examples from many sources 
including the Quran. Once the Frame files are 
created, the data that have the identified predicate 
occurrences are passed on to the annotators for a 
double-blind annotation process using the pre-
viously created framesets. Each PropBank entry 
represents a particular instance of a verb in a par-
ticular sentence in the Treebank and the mapping 
of numbered roles to precise meanings is given on 
a verb-by-verb basis in a set of frames files during 
the annotation procedure. To ensure consistency, 
the data is double annotated and finally adjudicated 
by a third annotator. The adjudicator resolves dif-
ferences between the two annotations if present to 
produce the gold annotation. A sample Frameset 
and a related annotation example from the QAPB 
are shown in Table 1. During the annotation 
process, the data is organized by verb such that 
each verb with all its instances is annotated at 
once. In doing so, we firstly ensure that the frame-
sets of similar verbs, and in turn, the annotation of 
the verbs, will both be consistent across the data. 
Secondly, by tackling annotation on a verb-by-verb 
basis, the annotators are able to concentrate on a 
single verb at a time, making the process easier and 
faster for the annotators. 
                                                 
4 Available at :<http://www.comp.leeds.ac.uk/cgi-
bin/scmss/arabic_roots.py> 
 
80
 
FrameSet Example Annotation Example 
Predicate: wajada?????   
 ?
Roleset id: f1, to find 
Arg0: the finder 
Arg1: thing found 
 
Rel: wajada, ?????? 
Arg0: -NONE- * 
Gloss: You 
Arg1: ?? 
Gloss: it 
ArgM-LOC: ??????? ???????? 
Gloss: with Allah 
 
Example in Arabic: 
 ?????? ??????????? ?????????????? ???? ?????? ????????? ?????
????????? 
Gloss: and whatever good you 
put forward for yourselves - 
you will find it with Allah 
Table 1. The frameset / Annotation of wajada 
 
4.2 Tools 
 
Frameset files are created in an XML format. We 
use tools used in the APB project. The Frame File 
editing is performed by the Cornerstone tool (Choi 
et al, 2010a), which is a PropBank frameset editor 
that allows creation and editing of PropBank fra-
mesets without requiring any prior knowledge of 
XML. Moreover, we use Jubilee5 as the annotation 
tool (Choi et al, 20010b). Jubilee is a recent anno-
tation tool which improves the annotation process 
of the APB by displaying several types of relevant 
syntactic and semantic information simultaneously. 
Having everything displayed helps the annotator 
quickly absorb and apply the necessary syntactic 
and semantic information pertinent to each predi-
cate for consistent and efficient annotation. Both 
tools are currently being modified in order to han-
dle the Dependency TreeBank structure, originally 
the tool was designed specifically to handle phrase 
structure Tree format. Moreover, since the file 
formats and the tree formats in the dependency 
Treebank are different from the previous APB 
effort, a revision in the Quranic Treebank output 
had to be done. This involves mainly a change in 
the annotated data format in order to add the role 
labels in the annotation file. For the moment, all of 
the 50 XML Frame files have been created and 
some manual annotation is performed to illustrate 
the feasibility of the experiment. 
                                                 
5 Cornerstone and Jubilee are available as Open Source tools 
on Google code. 
 
4.3 Impact of the dependency structure 
Treebank  
 
Having The Quran corpus annotated using a de-
pendency structure Treebank has some advantages. 
First, semantic arguments can be marked explicitly 
on the syntactic trees (such as the Arg0 Pron. In 
Figure 1), so annotations of the predicate argument 
structure can be more consistent with the depen-
dency structure as shown in Figure 1.  
 
 
Figure 1. Semantic role labels to the QATB 
 
Secondly, the Quranic Arabic Dependency 
Treebank (QATB) provides a rich set of dependen-
cy relations that capture the syntactic-semantic 
information. This facilitates possible mappings 
between syntactic dependents and semantic argu-
ments. A successful mapping would reduce the 
annotation effort.  
It is worth noting the APB comprises 1955 ver-
bal predicates corresponding to 2446 framesets 
with an ambiguity ratio of 1.25. This is in contrast 
to the QAPB where we found that the 50 verbal 
predicate types we annotated corresponded to 71 
framesets thereby an ambiguity ratio of 1.42. 
Hence these results suggest that the QAPB is more 
ambiguous than the newswire genre annotated in 
the APB. By way of contrast, the EPB comprises 
6089 verbal predicates corresponding to 7268 fra-
mesets with an ambiguity ratio of 1.19. 
21 verb types of the 50 verbs we annotated are 
present in both corpora corresponding to 31 frame-
sets in QAPB (a 1.47 ambiguity ratio) and 25 fra-
mesets in APB (1.19 ambiguity ratio).  The total 
verbal instances in the QAPB is 2974. 29 verb 
81
types with their corresponding 40 framesets occur 
only in the QAPB (58% of the list of 50 verbs). 
This translated to a 1.38 ambiguity ratio. 
In the common 21 verb types shared between 
APB and QAPB corpora we note that 12 predicates 
share the same exact frame sets indicating no 
change in meaning between the use of the predi-
cates in the Quran and MSA. However, 9 of the 
verbal predicates have more framesets in QAPB 
than APB. None of the verbal predicates have 
more framesets in APB than QAPB. Below is an 
example of a verbal predicate with two different 
framesets.  
 
FrameSet  
Example 
Annotation Example 
Predicate: >anozal ??????? 
Roleset id: f1, to reveal 
Arg0: revealer 
Arg1: thing revealed 
Arg2: start point 
Arg3: end point, recipient 
Rel: >anozal 
Arg0: ??? 
Gloss: we 
Arg1: ??????? ?????????  
Gloss:  clear verses 
Arg3: ????????  
Gloss: to you 
 
Example in Arabic: 
????????? ??????????? ???????? ?????? ?????????  
We have certainly revealed to you 
verses [which are] clear proofs 
Table 2. The frameset / Annotation of  >anozal 
(QAPB)  
 
FrameSet  
Example 
Annotation Example 
Predicate:  
>anozal ??????? 
Roleset id: f1, to 
release 
Arg0: agent re-
leasing 
Arg1:thing re-
leased 
Rel: >anozal 
Arg0: ????   
Gloss: Zyad 
Arg1:NONE-* 
Gloss: He 
ARGM-TMP: ????? ?????????? 
Gloss: the mid-eighties 
 
Example in Arabic: 
 ???? ????? ??? ????? ???? ??? ?? ???? ???? ?????
???? ????? ?????????? 
The songs of the Album I am not a disbe-
liever released by Ziad during the eighties 
are popular again.  
Table 3. The frameset / Annotation of  >anozal 
(APB)  
The two frames of verb ?? >anozal ? can clarify 
the meaning differences between MSA and QA as 
used in the Quran. Although  both APB and QAPB 
have this verb, they have different senses leading 
to different semantic frames. In the QAPB the 
sense of revealed is only associated with religious 
texts, while in MSA it has the senses of released or 
dropped.   
5 Conclusion 
We have presented a pilot Quranic Arabic 
PropBank experiment with the creation of frame 
files for 50 verb types. At this point, our initial 
study confirms that building a lexicon and tagging 
the Arabic Quranic Corpus with verbal sense and 
semantic information following the PropBank 
model is feasible. In general, the peculiarities of 
the Quranic Arabic language did not seem to cause 
problems for the PropBank annotation model. We 
plan to start the effective annotation of the resource 
in order to finalize the creation of a QAPB that 
covers all 1466 verbal predicates. Once released, 
the data will be freely available for research pur-
pose. 
References  
Vaidya Ashwini, Jinho Choi, Martha Palmer, and Bhu-
vana Narasimhan. 2011. Analysis of the Hindi Prop-
osition Bank using Dependency Structure. In 
Proceedings of the fifth Linguistic Annotation Work-
shop. ACL 2011, pages 21-29. 
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING-ACL ?98, the University of Mon-
treal, pages 86?90. 
Xavier Carreras and Llu?s Marquez. 2005. Introduction 
to the CoNLL-2005 shared task: Semantic role labe-
ling. In Proceedings of the Ninth (CoNLL-2005), 
pages 152?164. 
Jinho Choi, Claire Bonial, and Martha Palmer.2010a. 
PropBank Instance Annotation Guidelines Using a 
Dedicated Editor, Cornerstone.In Proceedings of the 
(LREC'10), pages 3650-3653. 
Jinho Choi, Claire Bonial, and Martha Palmer.2010b. 
PropBank Instance Annotation Guidelines Using a 
Dedicated Editor, Jubilee.In Proceedings of the 
(LREC'10), pages 1871-1875. 
Mona Diab, Aous Mansouri, Martha Palmer, Olga Bab-
ko-Malaya,Wajdi Zaghouani, Ann Bies, and Mo-
82
hammed Maamouri. 2008. A Pilot Arabic PropBank. 
In Proceedings of the (LREC'08), pages 3467-3472. 
Kais Dukes and Tim Buckwalter. 2010. A Dependency 
Treebank of the Quran using Traditional Arabic 
Grammar. In Proceedings of the 7th International 
Conference on Informatics and Systems (INFOS). 
Antoine El-Dahdah. 2008. A Dictionary of Arabic Verb 
Conjugation. Librairie du Liban, Beirut, Lebanon. 
Daniel Gildea, and Daniel Jurafsky. 2002. Automatic-
Labeling of Semantic Roles. Computational Linguis-
tics 28:3, 245-288 
Karin Kipper, HoaTrang Dang, and Martha Palmer. 
2000. Class-Based Construction of a Verb Lexicon. 
In Proceedings of the AAAI-2000 Seventeenth Na-
tional Conference on Artificial Intelligence, pages 
691-696. 
 Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma-
Gaddeche, Wigdan Mekki, Sondos Krouna, Basma-
Bouziri, and Wajdi Zaghouani. 2011. Arabic 
Treebank: Part 2 v 3.1. LDC Catalog 
No.:LDC2011T09 
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. 
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics Journal, 
31:1 
Martha Palmer, Shijong Ryu, Jinyoung Choi, Sinwon 
Yoon, and Yeongmi Jeon. 2006. LDC Catalog 
LDC2006T03. 
Otakar Smr?, Viktor Bielick?, IvetaKou?ilov?, Jakub 
Kr??mar, Jan Haji? and Petr Zem?nek. 2008. Prague 
Arabic Dependency Treebank: A Word on the Mil-
lion Words.In Proceedings of the Workshop on 
Arabic and Local Languages (LREC 2008). 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Llu?s Marquez, and Joakim Nivre. 2008. The 
CoNLL-2008 shared task on joint parsing on syntac-
tic and semantic dependencies. In Proceedings of 
CoNLL?08, pages 159?177. 
Majdi Swalha. 2011. Open-source Resources and Stan-
dards for Arabic Word Structure Analysis: Fine 
Grained Morphological Analysis of Arabic Text Cor-
pora. PhD thesis, Leeds University. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
and Dekai Wu, editors, Proceedings of EMNLP 
2004, pages 88?94. 
Wajdi Zaghouani , Mona Diab, Aous Mansouri, Sameer 
Pradhan, and Martha Palmer. 2010. The revised 
Arabic PropBank. In Proceedings of the Fourth Lin-
guistic Annotation Workshop (LAW IV '10), ACL, 
pages 222-226. 
Maysoon Zahri. 1990. Metaphor and translation. PhD 
thesis, University of Salford. 
 
 
83
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 24?29,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Building an Arabic Multiword Expressions Repository 
  Abdelati Hawwari, Kfir Bar, Mona Diab Center for Computational Learning Systems Columbia University {ah3019,kfir,mdiab}@ccls.columbia.edu   Abstract We introduce a list of Arabic multiword expressions (MWE) collected from various dictionaries. The MWEs are grouped based on their syntactic type. Every constituent word in the expressions is manually annotated with its full context-sensitive morphological analysis. Some of the expressions contain semantic variables as place holders for words that play the same semantic role. In addition, we have automatically annotated a large corpus of Arabic text using a pattern-matching algorithm that considers some morpho-syntactic features as expressed by a highly inflected language, such as Arabic. A sample part of the corpus is manually evaluated and the results are reported in this paper. 1 Introduction A multiword expression (MWE) refers to a multiword unit or a collocation of words that co-occur together statistically more than chance. A MWE is a cover term for different types of collocations, which vary in their transparency and fixedness. MWEs are pervasive in natural language, especially in web based texts and speech genres. Identifying MWEs and understanding their meaning is essential to language understanding, hence they are of crucial importance for any Natural Language Processing (NLP) applications that aim at handling robust language meaning and use. In fact, the seminal paper (Sag et al, 2002) refers to this problem as a key issue for the development of high-quality NLP applications. MWEs are classified based on their syntactic 
constructions. Among the various classes, one can find the Verb Noun Constructions (VNC), Noun Noun Construction (NNC) and others. A MWE typically has an idiosyncratic meaning that is more or different from the meaning of its component words. In this paper we focus on MWEs in Arabic. Like many other Semitic languages, Arabic is highly inflected; words are derived from a root and a pattern (template), combined with prefixes, suffixes and circumfixes. As opposed to English equivalents, Arabic MWEs can be expressed in a large number of forms, expressing various inflections and derivations of the words while maintaining the exact same meaning, for example, >gmD [flAn] Eynyh En [Al>mr] 1 , ?[one] disregarded/overlooked/ignored [the issue]?, literally, closed one?s eyes, vs. >gmDt [flAnp] EynyhA En [Al>mr], ?[one_fem] disregarded/overlooked/ignored_fem [the issue]?, where the predicate takes on the feminine inflection. However, in many cases, there are morphological features that cannot be changed in different contexts, for example, mkrh >xAk lA bTl, ?forced with no choice?, in this example, regardless of context, the words of the MWE do not agree in number and gender with the surrounding context. These are considered frozen expressions. One of the challenges in building MWE list for Arabic is to identify those features and document them in every MWE. Our resource is available for download.2  We have manually collected a large number of MWEs from various Arabic dictionaries, which are based on MSA corpora, and then filtered by Arabic                                                 1 We use the Buckwalter transliteration for rendering Arabic script in Romanization through out the paper (Buckwalter, 2002). 2 To get a direct access, please send a request to one of the authors 
24
native linguists. We then classified them based on their syntactic constructions, considering the relevant syntactic phenomena expressed in Arabic. The MWEs were manually annotated with the context-sensitive SAMA (Maamouri, 2010) morphological analysis for each word to assist an automated identification of MWEs in a large corpus of text. Part of the Arabic Gigaword 4.0 (Parker, 2009) is processed accordingly and the MWEs are annotated based on a deterministic algorithm considering different variants of every MWE in our list. There are diverse tasks that require a corpus with annotated MWEs, which have not been addressed in Arabic due to the lack of such a resource. However, a lot of attention is put on those tasks when implemented in English and other languages. Among those tasks, classifying MWEs in a running text is the most common one. Diab and Bhutada (2009) applied a supervised learning framework to the problem of classifying token level English MWEs in context. They used the annotated corpus provided by Cook (2008), a resource of almost 3000 English sentences annotated with VNC usage at the token level. Katz and Giesbrecht (2006) carried out a vector similarity comparison between the context of an English MWE and that of the constituent words using Latent Semantic Analysis to determine if the expression is idiomatic or not. In work by Hashimoto and Kawahara (2008), they addressed token classification into idiomatic versus literal for Japanese MWEs of all types. They annotated a corpus of 102K sentences, and used it to train a supervised classifier for MWEs. Using MWEs in machine translation is another application. Carpuat and Diab (2010) studied the effect of integrating English MWEs with a statistical translation system. They used the WordNet 3.0 lexical database (Fellbaum, 1998) as the main source for MWEs. Attia et al, in 2010, extracted Arabic MWEs from various resources. They focused only on nominal MWEs and used diverse techniques for automatic MWE extraction from cross-lingual parallel Wikipedia titles, machine-translated English MWEs taken from the English WordNet and the Arabic Gigaword 4.0 corpus. They found a large number of MWEs, however only a few of them were evaluated.  In this paper, we describe the process of manually creating a relatively comprehensive Arabic MWE list. We use the resulting list to tag 
MWE occurrences in context in a corpus.   The paper is organized as follows: In Section 2 we describe the process of creating the Arabic MWE list. Section 3 discusses the algorithm for automatic deterministic tagging of MWEs in running text, based on pattern matching. Sections 4 and 5 summarize the results of applying the pattern-matching algorithm on a corpus. Finally, we conclude in Section 6. 2 Arabic MWE List Our Arabic MWE list is created based on a collection of about 5,000 expressions, which is manually extracted from various Arabic dictionaries (Abou Saad, 1987; Seeny et al, 1996; Dawod, 2003; Fayed, 2007). Each MWE is preprocessed by the following steps: 1) cleaning punctuations and unnecessary characters, 2) breaking alternative expressions into individual entries, and 3) running MADA (Habash and Rambow, 2005; Roth et al 2008) on each MWE individually for finding the context-sensitive morphological analysis for every word. Some of the extracted MWEs are originally enriched with placeholder generic words that play the same semantic role in the context of the MWE. That set of generic words is manually normalized and reduced to a group of types, as shown in Table 2.  Generic Type Semantic Role Example flAn  ?so-and-so? a person Agent/Patient qr flAn EynA  ?pleased someone? k*A ?something? an object Goal ElY HsAb k*A  ?at the expense of that/this? <mr ?something? an issue Source <mr Abn ywmh ?something very new?  Table 1 ? Generic Types  Generic words are sometimes provided with or without additional clitics. For example, in the MWE lEbt [bflAn] AldnyA, literally, ?the world played-passive with so-and-so:?, which could be translated as ?life played havoc with so-and-so?,  the word bflan ?to so-and-so? has the preposition b ?with? cliticized to it.  Every word that substitutes a generic word (an instantiation) has to comply 
25
with the morphological features of the context surrounding it.  The automatic preprocessing steps we ran on the list are followed by a series of manual ones. We found that the short context we had for every MWE was not sufficient for MADA to return the correct analysis with reasonable precision.  Therefore, we had to go over the results and manually select the correct analysis for each word in every MWE. Generic words are also assigned with their correct analysis in context.  The class of each MWE is assigned manually. Arabic is highly inflected; therefore many MWE classes can be identified. However, in this paper, we focus only on the major ones. The following classes are used: Verb-Verb Construction (VVC) as in >xZ [flAn] w>ETY ?give and take?; Verb-Noun Construction (VNC), for example, md [flAn] Aljswr ?[someone] built bridges? as in extending the arms of peace; Verb-Particle Construction (VPC) as in mDY [flAn] fy ?[someone] continues working on?; Noun-Noun Construction (NNC) as in Enq {lzjAjp ?bottleneck?; Adjective Noun Construction (ANC) as in [flAn] wAsE {l&fq ?[someone] broad-minded?.  The final list comprises 4,209 MWE types. Table 2 presents the total number of MWE types for each category.   MWE Category Type Number VVC 41 VNC 1974 VPC 670 NNC 1239 ANC 285  Table 2 ? Arabic MWEs by category types 3 Deterministic Identification of Arabic MWEs We developed a pattern-matching algorithm for discovering MWEs in Arabic running text. The main goal of this algorithm is to deterministically identify instances of MWEs from the list in a large Arabic corpus, considering some morphological as well as syntactic phenomena. We use the Arabic Gigaword 4.0 (AGW). 3  To capture the large                                                 3 http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2009T30 
number of morpho-syntactic variations of the MWEs in context, the pattern-matching algorithm is designed to use some of the information available from the selected morphological analysis for every MWE word, as well as shallow-syntactic information that we automatically assigned for every word in the corpus.  One of our immediate intentions is to use the list of MWEs for learning how to statistically classify new ones in running text. Therefore, we begin here with annotating a large part of the AGW corpus  with all the occurrences a MWE given in the list. In order to make some shallow-syntactic features available for the pattern-matching algorithm, we pre-processed the AGW with AMIRAN, an updated version of the AMIRA tools (Diab et al, 2004, 2007). AMIRAN is a tool for finding the context-sensitive morpho-syntactic information. AMIRAN combines AMIRA output with morphological analyses provided by SAMA. AMIRAN is also enriched with Named-Entity-Recognition (NER) class tags provided by (Benajiba et al, 2008). For every word, AMIRAN is capable of identifying the clitics, diacritized lemma, stem, full part-of-speech tag excluding case and mood, base-phrase chunks and NER tags. Part of this information was used in previous work for processing English MWEs.  When looking for Arabic MWEs in the pre-processed corpus, there are two important issues that the pattern-matching algorithm is addressing: morphological variations and gaps. We now elaborate further on each one of them.  Morphological variations: As mentioned above, Arabic is highly inflected; clitics may be attached to reflect definiteness, conjunction, possessive pronouns and prepositions. This fact forces the pattern-matching algorithm to match words on a more abstract level then their surface form. The algorithm considers different levels of representation for each of the words. Those levels are matched based on the information provided by AMIRAN on corpus words, on the one hand, and the morphological analyses that are selected manually for every MWE word on the other hand. In the experiment reported here, we match words on the lemma level. The lemma provided by AMIRAN and the one manually chosen by MADA/SAMA analyses are taken from the same pool, hence matching is enabled. It is worth noting 
26
that in Arabic, the lemma is a generic name for a group of words that can be derived from one of its underlying stems, sharing the same meaning. For instance, the noun bnt  ?girl? and its plural form bnAt ?girls? are reduced to the same lemma form bnt. Obviously, perfect and imperfect forms of a verb are also assigned to the same lemma. A lemma form does not include the clitics; for every corpus word, this information is recorded by AMIRAN. Since clitics are in many cases important for matching MWES, the pattern-matching algorithm considers them. For example, in the MWE: <x* [flAn] bAlv<r, ?[so-and-so] requited?, the proclitic b ?with? expressed in the last word, is important for matching.   Gaps: Sometimes a MWE can be found with additional words such as modifiers that are not part of the original MWE expression words. For instance, the MWE: wDEt AlHrb <wzArhA, ?the war is over?, is found in the text: wDEt AlHrb AlEAlmyp AlvAnyp <wzArhA, ?the second world war is over?. The nominal modifiers AlEAlmyp AlvAnyp (?second world??) are not present in the original MWE taken from the list, and therefore considered as gap fillers.  To be able to identify gaps of MWEs in context, the pattern-matching algorithm uses the part-of-speech and base-phrase tags provided for every word by AMIRAN. In the reported experiment, we allowed an MWE to be matched over gaps of noun-phrases complementing MWE words. In other words, we allowed every MWE noun to be matched with a complete non-recursive noun-phrase that appears in the text. The matching is performed only on the first noun of the containing noun-phrase, restricting our approach using only noun-phrases expressing the head noun in the beginning of a phrase. For instance, in the previous example AlHrb AlEAlmyp AlvAnyp, ?the Second World War?, is a noun-phrase with a first noun word AlHrb ?the war?. This noun-phrase matches the word AlHrb ?the war? from the list MWE wDEt AlHrb <wzArhA ?the war is over?, hence allowing the entire MWE to be found. Obviously, allowing gaps of any types would have increased the recall but on the other hand a large number of false positive MWEs would have been identified. Currently, only noun-phrases are considered as potential gap fillers. Considering other phrase types is left for future work. We plan on 
identifying the types of potential gap fillers and correlating them with the various MWE types. One of the remaining problems with identifying MWEs deterministically in a running text is that the exact MWE words can be found in a text, however given the context, in some cases they are not idiomatic. This is the case for many VNCs for instance. Hence, they are not a unified concept ? a word with gaps -- as in our definition of a MWE usage.  Accordingly a token MWE classifier is required to identify such cases, teasing idiomatic from literal MWEs apart.  4 Building MWE Annotated Corpus We ran the pattern-matching algorithm on a large part of the AGW after we pre-processed the documents with AMIRAN. Overall, we had 250 million tokens and found 481,131 MWE instances. Table 3 summarizes the exact number of MWEs that we found, grouped by their class type. The matching was performed on the lemma level constraining the search with clitic matching. Gaps are restricted only to noun-phrases at this time, as mentioned above. The output of this process follows the Inside Outside Beginning (IOB) annotation scheme. In fact, the output files are based on the same input AMIRAN files, enriched with O, B/I-MWE tags as found by the pattern-matching algorithm. Figure 1 shows how a complete sentence, containing a MWE, is annotated by the pattern-matching algorithm.  MWE Category Type Number VVC 576 VNC 64,504 VPC 75,844 NNC 316,393 ANC 23,814  Table 3 ? Annotated MWEs by class 5 Evaluation The annotations are manually evaluated by a native speaker of Arabic. We sub sampled the corpus and examined each MWE instance that is identified by the pattern-matching algorithm. Table 4 shows our findings. Each row represents one category type. The middle column shows the number of instances evaluated, followed by the number of unique MWE types. In the last column, the number of correct instances as it was examined in context, is 
27
reported. The correctness of an instance is determined by its context. Remember that MWEs are not only matched statically; generic words, gaps and inflections may cause the pattern-matching algorithm to annotate expressions with an MWE type, incorrectly.  Word Lemma POS NER MWE swlAnA suwlAnA NN I-OR O : : PUNC O O AlAtHAd <it~iHAd NN B-GP O AlAwrwby Auwrub~iy NN I-GP O w+ wa+ CC O O wA$nTn wA$inoTuwn NNP B-GP O ysEyAn saEaY-a VBPMD3 O O l+ li+ IN O O AyjAd AiyjAd NN O O Alyp |liy~ap NNFS O O l+ li+ IN O O wqf waqof NN O O ATlAq Talaq NN O B-MW Al+ Al+ DET O I-MW nAr nAr NN O I-MW  Figure 1 ? Annotated sentence example   MWE Type Evaluated Instances Correct Instances VVC 111 (2 types) 2 VNC 157 (34 types) 154 VPC 161 (32 types) 125 NNC 155 (26 types) 154 
 Table 4 ? Evaluation Results  The evaluation set is relatively small. Nevertheless, one can see that in most cases the annotations are correct. For the VNC, the pattern matching algorithm achieves an accuracy of 98%, for VPC, we get an accuracy of 77.6%, and NNC we achieve an accuracy of 99%. It is worth noting that NNCs are the only category that employs the gapping. The VVC category contains only a few MWE types, in the sampled set we evaluated 111 instances of merely two different types from which, one was constantly identified incorrectly by the algorithm and it constitutes the majority of the instances (109 instances). 6 Conclusions In this paper we have introduced a list of MWEs in Arabic. The MWEs are enriched with morphological information that was carefully 
assigned to every word. A large part of the Arabic Gigaword 4.0 was deterministically annotated using a pattern-matching algorithm, considering morphological variations as expressed by Arabic and some potential gaps. A sample of the corpus was manually evaluated with encouraging results. Building both resources is a first step toward our research in the field of Arabic MWEs. Classifying the level of idiomaticity of the part of the MWE classes is one direction we are currently exploring. Acknowledgment This paper is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-12-C-0014. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA.  References  Attia, Mohammed, Antonio Toral, Lamia Tounsi, Pavel Pecina and Josef van Genabith. 2010. Automatic extraction of Arabic multiword expressions. In Proceedings of the 7th Conference on Language Resources and Evaluation, LREC-2010, Valletta, Malta. Abou Saad, Ahmed. 1987. A Dictionary of Arabic Idiomatic Expressions (mu?jm altrakib wala?barat alastlahiah ala?rbiah alkdim mnha walmould). Dar El Ilm Lilmalayin. Benajiba, Yassine, Mona Diab and Paolo Rosso. 2008. Arabic Named Entity Recognition: An SVM-based approach. In Proceedings of the Arab International Conference on Information Technology, ACIT-2008, Tunisia. Buckwalter, Tim. 2002. Buckwalter Arabic Morphological Analyzer Version 1.0. LDC catalog number LDC2002L49, ISBN 1-58563-257-0. Carpuat, Marine and Mona Diab. 2010. Task-based Evaluation of Multiword Expressions: a Pilot Study in Statistical Machine Translation. HLT-NAACL. Cook, Paul, Afsaneh Fazly, and Suzanne Stevenson. 2008. The VNC-Tokens Dataset. In Proceedings of the LREC Workshop on Towards a Shared Task for Multiword Expressions (MWE 2008), Marrakech, Morocco. 
28
Dawood, Mohammed. 2003. A Dictionary of Arabic Contemporary Idioms (mu?jm alta?bir alastlahiat). Dar Ghareeb. Diab, Mona and Pravin Bhutada. 2009. Verb noun construction MWE token supervised classification. In Workshop on Multiword Expressions (ACL-IJCNLP), pp. 17?22. Diab, Mona. 2009. Second Generation Tools (AMIRA 2.0): Fast and Robust Tokenization, POS tagging, and Base Phrase Chunking. MEDAR 2nd International Conference on Arabic Language Resources and Tools, Cairo, Egypt. Fayed, Wafaa Kamel. 2007. A Dictionary of Arabic Contemporary Idioms (mu?jm alta?bir alastlahiat). Abu Elhoul. Fellbaum, Christiane. 1998. WordNet: An Electronic Lexical Database. MIT Press. Habash, Nizar and Owen Rambow. 2005. Arabic Tokenization, Morphological Analysis, and Part-of-Speech Tagging in One Fell Swoop. In Proceedings of the Conference of American Association for Computational Linguistics, Ann Arbor, MI, 578-580. Hashimoto, Chikara and Daisuke Kawahara. 2008. Construction of an idiom corpus and its application to idiom identification based on WSD incorporating idiom-specific features. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Honolulu, Hawaii, pages 992?1001. Katz, Graham and Eugenie Giesbrecht. 2006. Automatic identification of non-compositional multiword expressions using latent semantic analysis. In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, Sydney, Australia, pages 12?19. Maamouri, Mohamed, et al 2010. Standard Arabic Morphological Analyzer (SAMA) Version 3.1. Linguistic Data Consortium, Philadelphia Parker, Robert, et al 2009. Arabic Gigaword Fourth Edition LDC2009T30, ISBN 1-58563-532-4. Linguistic Data Consortium (LDC), Philadelphia Roth, R., Rambow, O., Habash, N., Diab, M. and Rudin, C. 2008. Arabic Morphological Tagging, Diacritization, and Lemmatization Using Lexeme Models and Feature Ranking. In Proceedings of Association for Computational Linguistics (ACL), Columbus, Ohio. Sag, Ivan A. and Timothy Baldwin, Francis Bond, Ann A. Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for nlp. In 
Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, London, UK, pp. 1?15. Sieny, Mahmoud Esmail, Mokhtar A. Hussein and Sayyed A. Al-Doush. 1996. A contextual Dictionary of Idioms (almu?jm alsyaqi lelta?birat alastlahiah). Librairie du Liban Publishers. 
29
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 48?56,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
A Framework for the Classification and Annotation of Multiword  Expressions in Dialectal Arabic 
  Abdelati Hawwari, Mohammed Attia, Mona Diab Department of Computer Science The George Washington University {Abhawwari,mohattia,mtdiab}@gwu.edu 
 
    Abstract In this paper we describe a framework for classifying and annotating Egyptian Ara-bic Multiword Expressions (EMWE) in a specialized computational lexical re-source. The framework intends to en-compass comprehensive linguistic infor-mation for each MWE including: a. pho-nological and orthographic information; b. POS tags; c. structural information for the phrase structure of the expression; d. lexicographic classification; e. semantic classification covering semantic fields and semantic relations; f. degree of idio-maticity where we adopt a three-level rat-ing scale; g. pragmatic information in the form of usage labels; h. Modern Standard Arabic equivalents and English transla-tions, thereby rendering our resource a three-way ? Egyptian Arabic, Modern Standard Arabic and English ? repository for MWEs. 1 Introduction Multiword expressions (MWEs) comprise a wide range of diverse, arbitrary and yet linguistically related phenomena that share the characteristic of crossing word boundaries (Sag et al., 2002). MWEs are computationally challenging because the exact interpretation of an MWE is not direct-ly obtained from its component parts. MWEs are intrinsically single units on the deep conceptual and semantic levels, but on the surface (lexical and syntactic) levels they are expressed as multi-ple units. MWEs vary in their syntactic category, morphological behavior, and degree of semantic opaqueness. MWEs are pervasively present in natural texts, which makes it imperative to tackle them explicitly if we aspire to make large-scale, 
linguistically-motivated, and precise processing of a human language.  Integrating MWEs in NLP applications has evidently and consistently shown to improve the performance in tasks such as Information Re-trieval (Acosta et al. 2011; da Silva and Souza, 2012), Text Mining (SanJuan and Ibekwe-SanJuan, 2006), Syntactic Parsing (Eryi?it et al., 2011; Nivre and Nilsson, 2004; Attia, 2006; Korkontzelos and Manandhar, 2010), Machine Translation (Deksne, 2008; Carpuat and Diab, 2010; Ghoneim and Diab 2013; Bouamor et al., 2011), Question Answering, and Named-Entity extraction (Bu et al., 2011). In the current work, we propose guidelines for detailed linguistic annotation of an MWE lexicon for dialectal (Egyptian) Arabic that covers, among other types, expressions that are tradi-tionally classified as idioms (e.g. ??? ??????? EalaY Alriyq 1  ?on an empty stomach?), prepositional verbs (e.g. ???? ??? tawak~al EalaY ?rely on?), compound nouns (e.g. ???????? ?????? <i$Arap muruwr ?traffic light?), and collocations (e.g. ???? ???? >axad du$~ ?to take a shower?).  Creating a repository of annotated MWEs that is focused on dialects is essential for computa-tional linguistics research as it provides a crucial resource that is conducive to better analysis and understanding of the user-generated content rife in the social media (such as Facebook, Twitter, blogs, and forums). Moreover, it helps in under-standing he correspondences between different languages and their representation of the seman-tic space. We hope that the multilingual data in this repository will lead to a significant en-hancement in the processing of comparable and parallel corpora. We believe that our proposed framework will contribute to the sustainability of                                                 1 In this paper, we use the Buckwalter Transliteration Scheme for rendering Romanized Arabic as described in www.qamus.com. 
48
MWE research in general, and provide a blue print for research on MWEs in dialects, informal vernaculars, as well as morphologically rich lan-guages.  MWE are not only interesting from an NLP perspective but also from a linguistic perspec-tive, as MWE can help in understanding the link between lexicon, syntax and semantics. Until now, this is hampered by the lack of comprehen-sive resources for MWEs with fine-grained clas-sification on different dimensions related to se-mantic roles and syntactic functions. Arabic comprises numerous divergent dialects, and hav-ing an annotated MWE lexical resource in dia-lects and Modern Standard Arabic (MSA) will allow for studying transformation, change and development in this language. From a theoretical linguistic point of view, our work will be interesting particularly in studies related to Diglossia. Diglossia (Walters, 1996) is where two languages or dialects exist side by side within a community, where typically one is used in formal contexts while the other is used in informal communications and interactions. Stud-ying the MWE space for dialects and MSA as a continuum will lead to deeper insights into varia-tions as we note intersection and overlap be-tween the two. In many instances, we see that MSA MWEs and their dialectal equivalents are not necessarily shared as they occupy comple-mentary linguistic spaces. However, the nature of this complementarity and its cultural and social implications will need more exploration and in-vestigation, which will be possible once a com-plete resource becomes available. In the current work, we give detailed descrip-tion of our methodology and guidelines for anno-tating phonological, morphological, syntactic, semantic and pragmatic information of an Egyp-tian Multiword Expressions (EMWE) lexical resource. Our annotation scheme covers the fol-lowing areas. a) Phonological and orthographic information;  b) POS tag, based on the observation of how an MWE functions as a whole lexical unit; c) Syntactic variability and structural composi-tion;  d) Lexicographic types, which includes the classifications followed in the dictionary-writing domain (idioms, support verbs, com-pound nouns, etc.); e) Semantic information, where we cover se-mantic fields and relations; 
f) Idiomaticity Degree; we adopt a three level rating scale (Mel??uk, 1998) to measure the degree of semantic opaqueness; g) Degree of morphological, lexical and syntac-tic flexibility (Sag et al., 2002); h) Pragmatic information, which includes add-ing usage labels to MWEs where applicable; i) Translation, which includes the MSA and English equivalents, either as an MWE in MSA and English if available or as a para-phrase otherwise.   2 Previous Work There are four main areas of research on MWEs: extraction from structured and unstructured data, construction of lexicons for specific languages, integration in NLP applications, and the con-struction of guidelines and best practices. A sig-nificant amount of research has focused on the identification and extraction of MWEs (Ramisch et al., 2010; Dubremetz and Nivre, 2014; Attia et al., 2010; Weller and Heid, 2010; Schneider et al., 2014). Description and specifications of MWE lexical resources have been presented for Japanese (Shudo et al. 2011), Italian (Zaninello and Nissim, 2010), Dutch (Gr?goire, 2010; Odijk, 2013), and Modern Standard Arabic (Hawwari et al., 2012). Moreover, Calzolari et al. (2002) presented a project that attempted to in-troduce best practice recommendations for the treatment of MWE in mono- and multi-lingual computational lexicons that incorporate both syntactic and semantic information, but the limi-tation of their work is that they focus on only two types of MWEs, namely, support verbs and noun compounds. Apart from Schneider et al. (2014), who fo-cused on the language of the social web, none of these projects dealt with informal or dialectal languages, which are rampant in user-generated content (UGC). With the explosion of social me-dia, the language of Web 2.0 is undergoing fun-damental changes: English is no longer dominat-ing the web, and UGC is outpacing professional-ly edited content.  UGC is re-shaping the way people are con-suming and dealing with information, as the user is no longer a passive recipient, but has now turned into an active participant, and in many instances, a source or producer of information. Social media have empowered users to be more creative and interactive, and allowed them to 
49
voice their opinions on events and products and exert powerful influence on the behavior and opinion of others. Yet, the current overflow of UGC poses significant challenges in data gather-ing, annotation and presentation.  3 MWE Taxonomy Although the importance of the MWEs has been acknowledged by many researchers in the field of NLP as evident by the large number of research papers and dedicated workshops in the past decade, the theory of MWEs is still underdeveloped (Sag et al., 2002). There is critical need for studying MWEs both from the theoretical and practical point of views. MWEs have diverse categories, varying degrees of idiomaticity, different syntactic compositions, and different morphological, lexical and syntactic behavior, and dealing with them is complicated even further by the fact that there is no ?watertight criteria? for distinguishing them them (Atkins and Rundell, 2008). Moreover, there is no universally-agreed tax-onomy of MWEs (Ramisch, 2012), and different researchers proposed different typology for this phenomena. Fillmore et al. (1988) proposed three types based on lexical and syntactic familiarity: a) unfamiliar pieces familiarly combined, b) fa-miliar pieces unfamiliarly combined, and c) fa-miliar pieces familiarly combined. Mel'?uk (1989), on the other hand, introduced three different classes: a) complete phraseme, b) semi-phraseme, c) and quasi-phraseme. Sag et al. introduced two classes: institutionalized phrases and lexicalized phrases, with lexicalized phrases subdivided into fixed, semi-fixed and syntactically flexible expressions. Ramisch (2012) introduced yet another set of classes: nominal, verbal and adverbial expressions.  From the lexicographic point of view, the leg-acy three-way division of MWEs proved to be too coarse-grained to cater for the needs of lexi-cographers who need to identify the large array of sub-types that fall under the umbrella of ?MWEs?.  Atkins and Rundell (2008) empha-sized the need for lexicographers to be able to recognize MWE types such as fixed phrases, transparent collocations, similes, catch phrases, proverbs, quotations, greetings, phatic expres-sions, compounds, phrasal verbs, and support verbs. When we look deeply into the different classi-fications, we notice that each approach has 
looked at the phenomenon from a different angle, either focusing on its syntactic regularity, seman-tic and pragmatic properties, meaning composi-tionality, surface flexibility, POS (part of speech) category, or lexicographic relevance. What we propose is that it is not possible to come up with a hard and fast classification that cuts through all levels of representation. All afore-mentioned classifications are valid and can work parallel to each other, instead of substituting for each other. The assumption that we follow in this paper is that MWEs have different classifications at dif-ferent levels of representation from the very deep level of semantics and pragmatics to the very shallow level of morphology and phonetics.      The details of our annotation scheme are ex-plained in the following section. It is worth noting that in our current work, we move the focus away from edited text to the challenging and creative language found in UGC and by trying to close the language resource gap between edited and unedited text. We handle this gap by focusing on dialects, the language used in informal communications such as emails, chat rooms, and in social media in general. We cover the full range of MWEs (nominal, verbal, adver-bial, adjectival and prepositional expressions) in Egyptian Arabic, covering 7,331 MWEs (col-lected from corpora and paper dictionaries).   4 Annotation of Linguistic Features in MWE In this section, we provide a comprehensive specification of MWE types and the detailed lin-guistic information, including the phonological, orthographical, syntactic, semantic and pragmat-ic features.   4.1 Phonological Each MWE is provided in full diacritization to indicate its common pronunciation in Cairene Arabic accent, such as  ?????? ?? ??  ?? ? ?? ?? ?  EalaY kaf~ Eaforiyt ?at high risk?, ?lit. on the palm of a  de-mon?. We also list other phonological variants when available.  4.2 Orthography Since dialects do not have a standard orthogra-phy, we follow the CODA style (Habash et al., 2012), which is a devised standard for conven-tionalizing the orthography of dialectal Arabic. CODA takes canonical forms and etymological 
50
facts into consideration. For example, the Egyp-tian expression ???? ????? >axad bAluh ?to pay atten-tion? is rendered in CODA as ???? ????? >axa* bAluh. 4.3 POS At this level of annotation we consider the POS of the entire MWE when regarded as one unit from a functional perspective. We annotate each MWE with a POS tag from a predefined tagset. We define the POS tag based on the headword POS in the MWE. Our POS tagset includes verb, noun, adjective, adverb, interjection, proper noun, and preposition. The list of POS tags used along with examples is shown in Table 1.   POS Example 1 verb ???? ??? ?????????   jar~ EalaY AlHisAb  ?pay later? 2 noun ?????? ??????????  >akol AlEay$  ?making ends meet? [lit. eating bread] 3 adjective ??????? ??????????  >a$okAl wa->alowAn  ?various shapes and colors? 4 adverb ?????? ??????? >axorip Al-matam~ap ?at the end? 5 interjection ??? ???? ??????????  yA nAs yAhuwh  ?anybody there? 6 proper nouns ??????? ????   $ajarip Aldur~  ?Shajar al-Durr? 7 preposition  ?????? ?????   bi-gaD~ AlnaZar Eano  ?irrespective of? Table 1: MWE Examples with their POS Tags  4.4 Syntactic Annotation A syntactic variable is a slot that intervenes be-tween the component parts of an MWE, without being itself a part of it, but fills a syntactic gap. Syntactic variables are added, when needed, to MWEs to represent the syntactic behavior of an MWE and they exemplify how the MWE inter-acts with other elements within its scope. We create a tagset of syntactic variables reflecting the argument structure of an MWE. Examples are shown in Table 2.  
No Syntactic  Variable Example 1  ?????? somebody (masc_ nominative) 
??? (??????) ??????  jas~ (fulAn) AlnaboD ? (somebody) tested the waters? 2  ?????? somebody (fem_ accusative) 
 ?????) ?????????(????   >akal (fulAnap) bi-Eaynayh ?he devoured (some woman/girl) with his eyes? 3  ???????? people  (genitive) 
???? ???? (???????) ???????  daq~ bayn (Alqawom) <isofiyn ?he drove a wedge between (some people)?  4  ??????? some matter (accusative) ?? (??????) ?? ?????? Hat~ (Al>amora) fiy HisAbihi  ?he took (some matter) into consideration? 5  ??????? something (nominative) 
??????) ????? ??????) (Al$ayo') mitofaS~al Ealayh ?(something) fits him perfectly? Table 2: Syntactic variables and example usages  4.5 Lexicographic Annotation In the dictionary market there are specialized dictionaries for idioms, phrasal verbs, proverbs and quotations. However, general domain dic-tionaries try to avoid the use of too technical terms in the description of MWEs and use for the sake of simplicity a general term like ?phrase? to denote them to users. Yet, in the meta language of the dictionary compiling profession, lexicog-raphers make a more fine-grained distinction be-tween the various types of MWEs. Our lexico-graphic classification of MWEs is adapted from Atkins and Rundell (2008) and includes the fol-lowing tags. Examples are listed in Table 3.  1. Idiom: An idiom is an MWE whose mean-ing is fully or partially unpredictable from the meanings of its components (Nunberg et al., 1994); 2. Support verb, or ?light verbs?, may be defined as semantically empty verbs, which share their arguments with a noun (Meyers et al., 2004); 
51
3. Prepositional verb: These are verbs fol-lowed by prepositions with impact on the meaning; 4. Compound noun: A compound noun is a lexeme that consists of more than one noun; 5. Compound term: This is a technical com-pound noun used in a specific technical field; 6. Compound named entity: This is a multi-word proper name; 7. Phatic expression: an expression that is in-tended for performing a social function (such as greeting or well-wishing) rather than conveying information;  8. Proverb: We consider proverbs as multi-word expression if they are used as lexical units;  9. Quotation: We list only quotations that have gained currency in the language and have become familiar to the majority of the community. 10.  Classification Example 1 Idiom ??? ??????? ?? ?????   biyiEomil min AlHab~ap qub~ap  ?to make a mountain out of a molehill? 2 Support verb ?????? ???? >axad tAr  ?to take revenge? 3 Prepositional verb ??? ?????? DiHik Ealayh  ?to play a joke on? [lit. laugh on him]? 4 Compound noun ???? ???????? >abuw qirodAn  ?Cattle egret? 5 Compound term ???? ?????? Eiroq AlnisA  ?Sciatica? 6 Compound named entity ?????? ???????? >abuw Alhuwl  ?the Sphinx? 7 Phatic expres-sion ?????? ???? ?????  >a$uwf wu$~ak bi-xayr ?see you later? 8 Quotation ??? ?????? ??? ??????  yA mawolAyA kamA xa-laqotiniy ?penniless? 9 Proverb ?????? ??????  AlEaqol ziynap  ?wisdom is a blessing? Table 3: Examples of Lexical Types 4.6 Structural Classification We provide the syntactic phrase structure com-position of the expressions, giving the MWE pat-tern or the POS of its component elements. The purpose is to show the normal productive syntac-
tic patterns underlying the expressions. Table 4 shows the list of possible structural pattern in Egyptian MWEs.   Structure Example 1 adjective + conjunction + adjective 
?????????? ???? ?????   rayiq wa-fayiq  ?happy and relaxed? 
2 adjective + noun ?????? ?????????  tanaboliq Al-sulotAn ?couch potatoes? [lit. Sultan dependents]? 3 noun + noun  ??????? ???  kilomiq Haq~  ?word of truth? 4 adjective + preposition + noun ?????? ???????  garoqAn li-$uw$otuh  ?up to his ears? 5 adverb + noun ???? ???????  bayn nArayn  ?confused? [lit. between two fires] 6 adverb + verb  ????????? ?????????   HasobamA Ait~afaq ?haphazardly? [lit. as happens] 7 noun + adjec-tive ???? ???????  nafoxap kad~Abap  ?false pride/arrogance? [lit. false blow] 8  verb + con-junction + verb 
 ??????????? ???????????  yilit~ wa-yiEojin  ?to babble? [lit. knead and fold] 9  verb + verb  ????????? ??   Aimo$iy Ainojar~  ?get moving/get out? [lit. walk and drag] 10  verb + preposition + noun   
??????? ????? ??????  tawak~al EalaY Allah ?rely on Allah/go away? 11  preposition + noun  ??? ????????? EalaY AlTabo-TAb ?effortlessly? [lit. on ease] 12  verb + noun  ??? ???????  nafa$ riy$uh  ?show pride? [lit. stretched his feathers] 13  noun + verb  ???? ???????!  Allah yiroHamuh  ?Allah have mercy on him? Table 4: Examples Syntactic Classification  4.7 Semantic Fields The entries in the current lexical resource are classified into semantic fields based on their se-mantic contents. The objective is to assign one semantic field tag for each MWE in the lexicon. Organizing Lexical data in semantic field format brings many theoretical and practical benefits, one of those is to allow the current lexical re-source to function both as a lexicon and a thesau-
52
rus. In Table 5 we show a sample of our seman-tic field classification.   Semantic Field Example 1 Social Relation ??? ??? ???  samon EalaY Easal  ?getting on well? [lit. ghee on honey] 2 Oath and Em-phasis ?????? ????????  wa-Allah AlEaZiym  ?I swear by Allah? 3 Occasions ?????? ?? ????  yitrab~aY fiyEiz~ak ?congratulations on the new baby? [lit. may he grow up in your wealth] 4 Death ????? ????????  rab~inA Aifotakaruh ?he died?  [lit. the Lord remem-bered him] 5  wishing and cursing ?? ??? ???????  baEod Al$ar~ ?God forbid? [lit. may the evil be far away] 6 trickery ?????? ??????  lab~isuh AlEim~ap  ?to hoodwink? [lit. put the turban on him] 7 Occultism ???? ??????  Darab Alramol  ?to practice divination? [lit. to strike the sand]? Table 5: Semantic fields  4.8 Semantic Relations Aiming at presenting detailed lexical semantic information, we further classify our entries based on semantic relations like synonymy, antonymy and polysemy.  ? Synonymy: MWE synonyms are grouped together; as the following expressions which all mean ?to practice divination? ???? ????????? qarA AlfinojAn [lit. read the cup], ???? ???????? Darab AlwadaE [lit. hit the shells], ????? ????? qarA Alkaf~ [lit. read the hand palm]. ? Antonymy: MWE antonyms are two MWE having the opposite meaning to each other. For examples, ??????? ????? <iyduh nA$ofap ?ava-ricious? [lit. his hand is dry] is the antonym of ??????? ??????? <iyduh maxoruwmap ?wasteful? [lit. his hand has a hole in it]. 
? Polysemy. This is when an MWE has more than one meaning. For example, ???????? ??????? <iyduh Tawiylap [lit. his hand is long] can mean either a ?powerful person? or a ?thief?.  4.9 Idiomaticity Degree Mel??uk (1998) classified MWEs with regards to idiomaticity into three types: full phrasemes, quasi-phrasemes and semi-phrasemes.  ? Full phrasemes are when the meaning of the expression does not match the meaning of the component words, such as ???????? ???? Wa-halum~ jar~A ?and so on?.  ? Quasi-phrasemes are when the meaning of the expression matches the meaning of the component words in addition to an extra piece of meaning that is not directly derived from either components, such as ???? ?????? majolis Al$aEob ?people's assembly?.  ? Semi-phrasemes are when the meaning of the MWE is partially directly derived from one component and partially indirectly indi-cated by the other component, such as  ??????????????? dirAsAt EuloyA ?higher studies?.  4.10 Morpho-lexico-grammatical flexibility A scale of three levels is used to measure the de-gree of morphological, lexical and grammatical flexibility of a MWE, adopted from Sag et al. (2002). The three levels are as follows: ? Fixed MWE: An MWE is considered as a fixed expression if it does not have any de-gree of syntactic, morphological or lexical flexibility, and its meaning cannot be pre-dicted from its component elements, for ex-ample, ?????? ?????? sadAH madAH ?slapdash?. ? Semi-Fixed MWE: Semi-fixed expressions allow for a certain degree of morphological and lexical variation, but they are fixed in terms of the syntactic word order, for ex-ample, ????????\???????? ??? ?? ????????\??????  mA$oyap/mA$oyiyn EalaY Hal~ $aEo-rahA/$aEoruhum [lit. living by letting down her/their hair] ?whore/whores? or ?loose women?. ? Syntactically flexible MWE: A syntacti-cally flexible MWE is a frequent combina-tion of two words or more, characterized by high degree of morphological and syntactic flexibility. Example, ?????? (?????) ???? <id~aY 
53
(fulAn) du$~ ?to scold someone harshly? [lit. give someone a shower].   4.11 Pragmatic Annotation (Usage Labels) The reason we provide usage labels is inspired by the CALLHOME Egyptian Arabic corpus (Gadalla et al., 1997)), which is a collection of data gathered from spoken colloquial language. The usage labels present specifications on who uses an MWE and how it is used. The usage label tagset in our lexicon includes labels such vulgar, youth, aggressive or taboo, as exemplified in Table 6.  Who or how Example youth  ?????? ??????? ??? ???????  yisuwq Alhabal fiy Aljabal  ?to act foolishly? [lit. to act madly in the mountain]? women / girls ?????????? ????? ???? ?????  Al$ATrap tigozil birijol HumAr ?make do with what you have? [lit. a clever girl will knit with a donkey?s leg]? Aggressive ??????? ?? ???? >ad~iyk fiy wi$~ak  ?I shall slap you in the face? Table 6: Pragmatic annotation  5 Status of the current resource The Egyptian MWE lexical resource at the cur-rent stage contains 7,331 entries, and work is still on going in the linguistic annotation of the dic-tionary. Table 7 presents the current annotation progress statistics regarding the various classifi-cations and features.   Feature Completion 1 Diacritization 34.10% 2 Syntactic Variables 25.92% 3 MSA Equivalent 27.28% 4 POS  34.10% 5 Syntactic Classification 23.58% 6 English Equivalent 27.28% 7 Lexical Type 98.94% 8 Pragmatics Usage 4.09% 9 Synonymous 0.14% 10 Idiomaticity Degree 12.82% 11 Semantic-Field 2.29% Table 7: Annotation work progress 
6 Conclusion We have described the annotation guidelines for a lexical database of MWE for dialectal Arabic. We provide descriptive specifications of MWE at the phonological, orthographical, syntactic and semantic levels. The main contribution of this paper is that it is the first description of a classi-fication and annotation scheme of a lexical data-base for dialects, which can be extended for in-formal languages and with direct applicability on user-generated content. Acknowledgement This work was supported by the Defense Ad-vanced Research Projects Agency (DARPA) Contract No. HR0011-12-C-0014, BOLT pro-gram with subcontract from Raytheon BBN. References Acosta, Otavio Costa, Aline Villavicencio, Viviane P. Moreira. (2011) Identification and Treatment of Multiword Expressions applied to Information Re-trieval. Proceedings of the Workshop on Multi-word Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 101?109, Portland, Oregon, USA, 23 June 2011. Atkins, B. T. S. and M. Rundell. 2008. The Oxford Guide to Practical Lexicography. Oxford University Press. Attia, Mohammed, Antonio Toral, Lamia Tounsi, Pavel Pecina and Josef van Genabith. 2010. Auto-matic Extraction of Arabic Multiword Expressions. COLING 2010 Workshop on Multiword Expres-sions: from Theory to Applications. Beijing, China  Attia, Mohammed. (2006) Accommodating Multi-word Expressions in an Arabic LFG Grammar. In T. Salakoski et al. (Eds.): Advances in Natural Language Processing. FinTAL 2006, Lecture Notes in Computer Science. Vol. 4139, pp. 87 - 98, 2006. Springer-Verlag Berlin Heidelberg. Baldwin, T. (2005a). The deep lexical acquisition of English verb-particles. Computer Speech and Lan-guage, Special Issue on Multiword Expressions 19 (4), 398?414. Baldwin, T. (2005b). Looking for prepositional verbs in corpus data. In Proceedings of the 2nd ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications, Colches-ter, UK, pp. 115?126. Baldwin, Timothy and Su Nam Kim. 2009. Multi-word expressions. In Nitin Indurkhya and Fred J. Damerau, editors, Handbook of Natural Language 
54
Processing, pages 267?292. CRC Press, Boca Ra-ton, USA, 2nd edition. Bannard, C. 2007. A Measure of Syntactic Flexibil-ity for Automatically Identifying Multi Word Ex-pressions in Corpora. Proceedings of A Broader Perspective on Multiword Expressions, Workshop at the ACL 2007 Conference: 1?8. Benson, M. 1990. Collocations and general-purpose dictionaries. International Journal of Lexicogra-phy, 3(1):23--35. Bouamor, Dhouha, Nasredine Semmar and Pierre Zweigenbaum. (2011) Improved Statistical Ma-chine Translation Using MultiWord Expressions. International Workshop on Using Linguistic In-formation for Hybrid Machine Translation LIHMT. Barcelona, November 2011 Bu, Fan, Xiao-Yan Zhu, and Ming Li. (2011) A New Multiword Expression Metric and Its Applications. In Journal of Computer Science and Technology. 26(1): 3-13, Jan. 2011. Calzolari, Nicoletta, Charles J. Fillmore, Ralph Grishman, Nancy Ide, Alessandro Lenci, Catherine MacLeod, Antonio Zampolli. (2002) Towards Best Practice for Multiword Expressions in Computa-tional Lexicons. In Proceedings of the Third Inter-national Conference on Language Resources and Evaluation (LREC 2002), Las Palmas, Canary Is-lands, pp. 1934-1940 Carpuat, Marine and Mona Diab. (2010) Task-based evaluation of multiword expressions: a pilot study in statistical machine translation. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Los Angeles, CA. Pp. 242-245. Chafe, Wallace1968. Idiomaticity as an Anomaly in the Chomskyan Paradigm. Foundations of Lan-guage 4.109-127. da Silva, Edson Marchetti and Renato Rocha Souza. (2012) Information retrieval system using Multi-words Expressions (MWE) as descriptors. JISTEM - Journal of Information Systems and Technology Management. Vol.9 no.2 S?o Paulo May/Aug. 2012. Deksne, Daiga, Raivis Skadi??, and Inguna Skadi?a. a. 2008. Dictionary of Multiword Expressions for Translation into Highly Infliected Languages. In the 6th International Conference on Language Re-sources and Evaluation (LREC 2008), Marrakech, Morocco. Dubremetz, Marie and Joakim Nivre. (2014) Extrac-tion of Nominal Multiword Expressions in French. In proceedings of the 10thWorkshop on Multiword Expressions (MWE 2014), the 14th Conference of the European Chapter of the Association for Com-
putational Linguistics. 26-27 April 2014. Gothen-burg, Sweden Eryi?it, G?l?en, Tugay ?Ilbay Ozan and Arkan Can. (2011) Multiword Expressions in Statistical De-pendency Parsing. Proceedings of the Second Workshop on Statistical Parsing of Morphological-ly Rich Languages. Fillmore, C.J., P. Kay, M. O?Connor. (1988) Regu-larity and idiomaticity in grammatical construc-tions: the case of let alone. Language, 64, 3, 501?538. Gadalla, Hassan, Hanaa Kilany, Howaida Arram, Ashraf Yacoub, Alaa El-Habashi, Amr Shalaby, Krisjanis Karins, Everett Rowson, Robert Mac-Intyre, Paul Kingsbury, David Graff, Cynthia McLemore. (1997) CALLHOME Egyptian Arabic Transcripts. LDC catalog number LDC97T19, ISBN 1-58563-115-9. Ghoneim, Mahmoud and Mona Diab. (2013) Multi-word Expressions in the context of Statistical Ma-chine Translation. In the Proceedings of IJCNLP 2013, October, Nagoya, Japan. Gibbs, R. W. (1980). Spilling the beans on under-standing and memory for idioms. Memory & Cog-nition, 8, 449?456. Gr?goire, Nicole. (2010) DuELME: a Dutch electron-ic lexicon of multiword expressions. In Language Resources and Evaluation, 44(1-2):23-39 (2010) Gross, Maurice, 1986. Lexicon-Grammar. The Repre-sentation of Compound Words. In COLING-1986 Proceedings, Bonn, pp. 1-6. Habash, Nizar, Mona Diab, Owen Rambow (2012). CODA: A Conventional Orthography for Dialectal Arabic. Proceedings of LREC, Istanbul Turkey, May 2012. Hawwari, Abdelati, Kfir Bar, and Mona Diab (2012). Building an Arabic Multiword Expressions Reposi-tory. Proceedings of the NAACL-HLT 2012 Workshop on Computational Linguistics for Lit-erature, Montreal, Canada, June 2012.  Jackendoff, R. (1973). The base rules for preposition-al phrases. In A Festschrift for Morris Halle, pp. 345?356. New York, USA: Rinehart and Winston. Korkontzelos, Ioannis, and Suresh Manandhar. (2010) Can Recognising Multiword Expressions Improve Shallow Parsing? In proceedings of Human Lan-guage Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 636?644, Los Angeles, California, June 2010. Mel??uk, I. (1998) Collocations and Lexical Func-tions. In A.P. Cowie (ed.): Phraseology. Theory, Analysis, and Applications, Oxford: Clarendon Press, 23-53. 
55
Mel??uk, Igor (2004) Verbes supports sans peine. Lingvistic? Investigationes 27: 2, 203-217. Nivre, Joakim and Jens Nilsson. 2004. Multiword Units in Syntactic Parsing. InWorkshop on Meth-odologies and Evaluation of Multiword Units in Real-World Applications, the 4th International Conference on Language Resources and Evaluation (LREC 2004), pp. 39-46. Lisbon, Portugal. Odijk, Jan. (2013) Identification and Lexical Repre-sentation of Multiword Expressions. In P. Spyns and J. Odijk (eds.), Essential Speech and Language Technology for Dutch, Theory and Applications of Natural Language Processing Palmer, Martha, Dan Gildea, Paul Kingsbury. (2005) The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31:1., pp. 71-105.  Ramisch, Carlos, Aline Villavicencio, Christian Boi-tet, "mwetoolkit: a Framework for Multiword Ex-pression Identification", Proceedings of the Sev-enth International Conference on Language Re-sources and Evaluation (LREC 2010), Valetta, Malta, May, 2010. Sag, I. A., Baldwin, T., Bond, F., Copestake, A., and Flickinger, D. 2002. Multiword Expressions: A Pain in the Neck for NLP. Proceedings of the 3rd International Conference on Intelligent Text Pro-cessing and Computational Linguistics, CI-CLING2002: 1?15. SanJuan, Eric and Fidelia Ibekwe-SanJuan. 2006. Text mining without document context. In Infor-mation Processing and Management. Volume 42, Issue 6, pp. 1532-1552. Schneider, Nathan, Spencer Onuffer, Nora Kazour, Emily Danchik, Michael T. Mordowanec, Henriet-ta Conrad, and Noah A. Smith. (2014) Comprehen-sive Annotation of Multiword Expressions in a So-cial Web Corpus. In Proceedings of the Language Resources and Evaluation Conference (LREC 2014), Reykjavik, Iceland, May 2014. Shudo, Kosho, Akira Kurahone, and Toshifumi Tana-be. (2011) A Comprehensive Dictionary of Multi-word Expressions. Proceedings of HLT '11 Pro-ceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Language Technologies, Portland, Oregon. Volume 1, pp. 161-170 Walters, Keith. Diglossia, linguistic variation, and language change in Arabic. 1996. In Eid, Mushira, Perspectives on Arabic Linguistics VIII. John Ben-jamins. 1996 Weller, Marion, Ulrich Heid. (2010) Extraction of German Multiword Expressions from Parsed Cor-pora Using Context Features. In proceedings of the 
seventh international conference on Language Re-sources and Evaluation (LREC), Val-letta, Malta. Zaninello, Andrea, Malvina Nissim. (2010) Creation of lexical resources for a characterisation of multi-word expressions in Italian. In proceedings of the seventh international conference on Language Re-sources and Evaluation (LREC), Valletta, Malta  
56
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 62?72,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Overview for the First Shared Task on
Language Identification in Code-Switched Data
Thamar Solorio
Dept. of Computer Science
University of Houston
Houston, TX, 77004
solorio@cs.uh.edu
Elizabeth Blair, Suraj Maharjan, Steven Bethard
Dept. of Computer and Information Sciences
University of Alabama at Birmingham
Birmingham, AL, 35294
{eablair,suraj,bethard}@uab.edu
Mona Diab, Mahmoud Gohneim, Abdelati Hawwari, Fahad AlGhamdi
Dept. of Computer Science
George Washington University
Washington, DC 20052
{mtdiab,mghoneim,abhawwari,fghamdi}@gwu.edu
Julia Hirschberg and Alison Chang
Dept. of Computer Science
Columbia University
New York, NY 10027
julia@cs.columbia.edu
ayc2135@columbia.edu
Pascale Fung
Dept. of Electronic & Computer Engineering
Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong
pascale@ece.ust.hk
Abstract
We present an overview of the first shared
task on language identification on code-
switched data. The shared task in-
cluded code-switched data from four lan-
guage pairs: Modern Standard Arabic-
Dialectal Arabic (MSA-DA), Mandarin-
English (MAN-EN), Nepali-English (NEP-
EN), and Spanish-English (SPA-EN). A to-
tal of seven teams participated in the task
and submitted 42 system runs. The evalua-
tion showed that language identification at
the token level is more difficult when the
languages present are closely related, as in
the case of MSA-DA, where the prediction
performance was the lowest among all lan-
guage pairs. In contrast, the language pairs
with the higest F-measure where SPA-EN
and NEP-EN. The task made evident that
language identification in code-switched
data is still far from solved and warrants
further research.
1 Introduction
The main goal of this language identification shared
task is to increase awareness of the outstanding
challenges in the automated processing of Code-
Switched (CS) data and motivate more research in
this direction. We define CS broadly as a commu-
nication act, whether spoken or written, where two
or more languages are being used interchangeably.
In its spoken form, CS has probably been around
ever since different languages first came in contact.
Linguists have studied this phenomenon since the
mid 1900s. In contrast, the Natural Language Pro-
cessing (NLP) community has only recently started
to pay attention to CS, with the earliest work in
this area dating back to Joshi?s theoretical work
proposing an approach to parsing CS data (Joshi,
1982) based on the Matrix and Embedded language
framework. With the wide-spread use of social me-
dia, CS is now being used more and more in written
language and thus we are seeing an increase in pub-
lished papers dealing with CS. We are specifically
interested in intrasentential code switched phenom-
ena. As a result of this task, we have successfully
created the first set of annotated data for several
language pairs with a coherent set of labels across
the languages. As the shared task results show,
CS poses new research questions that warrant new
NLP approaches, and thus we expect to see a sig-
nificant increase in NLP work in the coming years
addressing CS phenomena in data.
The shared task covers four language pairs and
is focused on social media data. We provided par-
ticipants with annotated data from Twitter for the
62
language pairs: Modern Standard Arabic-Arabic
dialects (MSA-DA), Mandarin-English (MAN-
EN), NEP-EN (NEP-EN), and SPA-EN (SPA-EN).
These language pairs represent a good variety in
terms of language typology and relatedness among
pairs. They also cover languages with different rep-
resentation in terms of number of speakers world
wide. Participants were asked to make predictions
on unseen Twitter data for each language pair. We
also provided participants with test data from a
?surprise genre? with the objective of assessing the
robustness of language identification systems to
genre variation.
2 Task Description
The task consists of labeling each token/word in
the input file with one of six labels: lang1, lang2,
other, ambiguous, mixed, and named entities NE.
The lang1, lang2 labels refer to the two languages
addressed in the subtask, for example for the lan-
guage pair MSA-DA, lang1 would be an MSA and
lang2 is DA. The other category is a label used to
tag all punctuation marks, emoticons, numbers, and
similar tokens that do not represent actual words in
any of the given languages. The ambiguous label
is for instances where it is not possible to assign
a language with certainty, for example, a lexical
form that belongs to both languages, appearing in a
context that does not indicate one language over the
other. The mixed category is for words composed
of CS morphemes, such as the word snapchateando
?to chat? from SPA-EN, the word overai from NEP-
EN, or the word hayqwlwn
1
?they will say?, from
MSA-DA, where the ?ha? is a DA future morpheme
and the stem ?yqwlwn? is MSA.The NE label is
included in this task in an effort to allow for a more
focused analysis of CS data with the exclusion of
proper nouns. NEs have a very different behavior
than most other words in a language vocabulary
and thus from our perspective they need to be iden-
tified to be handled properly.
Table 1 shows Twitter examples taken from the
training data. The annotation guidelines are posted
on the workshop website
2
. We post the ones used
for SPA-EN as for the other language pairs the only
differences are the examples provided.
1
We use Buckwalter transliteration scheme http://
www.qamus.org/transliteration.htm
2
http://emnlp2014.org/workshops/
CodeSwitch/call.html
Language Pair Example
MSA-DA AlnhArdp AlsAEp 11 hAkwn Dyf >.
HAfZ AlmyrAzy ElY qnAp drym llHdyv
En >wlwyAt Alvwrp fy AlmrHlp Al-
HAlyp wqDyp tSHyH msAr Alvwrp
Al<ElAmy
(Today O?Clock 11 I will be
[a ]guest[ of] Mr. Hafez
AlMirazi on Channel Dream
to talk about [the ]priorities[ of]
the revolution in the stage the current
and [the ]issue[ of] correcting
[the ]path[ of] the revolution Media)
NEP-EN My car at the workshop for a much
needed repairs... ABA pocket khali
hune bho
(My car at the workshop for a much
needed repairs. . . now my pocket will
be empty)
SPA-EN Por primera vez veo a @username ac-
tually being hateful! it was beautiful:)
(For the first time I get to see @user-
name actually being hateful! it was
beautiful:)
Table 1: Examples of Twitter data used in the
shared task.
3 Related Work
In the past, most language identification research
has been done at the document level. Some re-
searchers, however, have developed methods to
identify languages within multilingual documents
(Singh and Gorla, 2007; Nguyen and Do?gru?oz,
2013; King and Abney, 2013). Their test data
comes from a variety of sources, including web
pages, bilingual forum posts, and jumbled data
from monolingual sources, but none of them are
trained on code-switched data, opting instead for a
monolingual training set per language. This could
prove to be a problem when working on code-
switched data, particularly in shorter samples such
as social media data, as the code-switching context
is not present in training material.
One system tackled both the problems of code-
switching and social media in language and code-
switched status identification (Lignos and Marcus,
2013). Lignos and Marcus gathered millions of
monolingual tweets in both English and Spanish in
order to model the two languages, and used crowd-
sourcing to annotate tens of thousands of Span-
ish tweets, approximately 11% of which contained
code-switched content. This system was able to
achieve 96.9% word-level accuracy and a 0.936
F-measure in identifying code-switched tweets.
The issue still stands that relatively little code-
switching data, such as that used in Lignos and
63
Marcus? research, is readily available. Even in
their data, the percentage of code-switched tweets
was barely over a tenth of the total test data. There
have been other corpora built, particularly for other
language pairs such as Mandarin-English (Li et
al., 2012; Lyu et al., 2010), but the amount of data
available and the percentage of code-switching data
within that data are not up to the standards of other
areas of the natural language processing field. With
this in mind, we sought to provide corpora for mul-
tiple language pairs, each with a better distribution
of code-switching phenomena.
4 Data Sets
Most of the data for the shared task comes form
Twitter. However, we also collected and annotated
data from other social media sources, including
Facebook, web forums, and blogs. These additional
sources of data were used as the surprise data. In
this section we describe briefly the corpora curated
for the shared task.
Language-pair Training Test Surprise
MAN-EN 1000 313 n/a
MSA-DA 5,838 2332, 1,777 12,017
NEP-EN 9,993 3,018 (2,874) 1,087
SPA-EN 11,400 3,060 (1,626) 1,102
Table 2: Statistics of the shared task data sets
per language pairs. The numbers are according to
what was actually annotated, numbers in parenthe-
sis show what the participating systems were able
to crawl from Twitter. The Surprise genre comes
from various sources, other than Twitter.
Table 2 shows some statistics about the differ-
ent datasets used in this task. We strive to provide
dataset sizes that would allow a robust analysis of
results. However, an unexpected challenge was
the rate at which tweets became unavailable. Dif-
ferent language pairs had different attrition rates
with SPA-EN being the most affected language and
MSA-DA and NEP-EN the least affected. Note
that we provided two test datasets for MSA-DA.
Since we separated the data on a per user basis, the
first test set had a highly skewed distribution. The
second test set was distributed to participants to
allow a comparison with a data set having a class
distribution more similar to the training set.
4.1 SPA-EN data
Developing the corpus involved two primary steps:
locating code-switching tweets and using crowd-
sourcing to annotate their tokens with language
tags. A small portion of the tweets were annotated
in-lab and this was used as the gold data for quality
control in the crowdsourcing annotation.
To avoid biasing the data used in this task, we
used a two step process to select the tweets: first we
identified CS tweets by doing a keyword search on
Twitter?s API. We selected a few frequently used
English words and restricted the search to tweets
identified by Twitter as Spanish from users in Cali-
fornia and Texas. An additional set of tweets was
then collected by using frequent Spanish words in
an all English tweet, from users in the same loca-
tions. We filtered these tweets to remove tweets
containing URLs, duplicates, spam tweets and
retweets.
In-lab annotators labeled the filtered tweets using
the guidelines referenced above. From this set of
labeled data we then ranked the users in this set by
the percentage of CS tweets. We selected the 12
most prolific CS users and then pulled all of their
available tweets. These 12 users contributed the
tweets used in the shared task. The tweets were
labeled using CrowdFlower
3
. After analyzing the
number and content distribution of the tweets, the
SPA-EN data was split into a 11,400 tweet training
set and a 3,014 tweet test set.
The SPA-EN Surprise Genre (SPA-EN-SG) in-
cluded Facebook comments from the Veteranas
community
4
and the Chicanas community
5
and
blog data from the Albino Bean
6
. Data was col-
lected using Python scripts that implemented the
Beautiful Soup library and the third-party Python
Facebook SDK (for Blogger and Facebook respec-
tively). Post and comment IDs were used to iden-
tify Facebook posts, and URLs were used to iden-
tify Blogger posts. The collected posts were format-
ted to match those collected from Twitter. In-lab
annotators were used to annotate approximately 1K
tokens. All the data we collected in this manner
was released as surprise data to all participants.
4.2 NEP-EN data
The collection of NEP-EN data followed a simi-
lar approach to that of SPA-EN. We first focused
on finding users that switched frequently between
3
http://www.crowdflower.com/
4
https://www.facebook.com/
VeteranaPinup
5
https://www.facebook.com/pages/
Chicanas/444483772293893
6
http://thealbinobean.blogspot.com/
64
Nepali and English. In addition, the users must
not be using Devnagari script as done by Nepalese
to write Nepali, but must have used its Roman-
ized form. We started by manually reading tweets
from some of our Nepali friends. We then crawled
their followers who corresponded with them using
code-switched tweets or replies. We found that
a lot of these users were regular code-switchers
themselves. We repeated the same process with the
followers and collected nearly 30 such users. We
then collected about 2,000 tweets each from these
users using the Twitter API. We filtered out all the
retweets and the tweets with URLs, following the
same process that was used for SPA-EN.
For the surprise test data, we crawled code-
switched data from Facebook comments and posts.
We found that most Nepalese comments had a rich
amount of code-switched data. However, we could
not crawl their data because of privacy issues. Nev-
ertheless, we could crawl data from public Face-
book pages. We identified some public Nepali Face-
book pages where anyone could comment. These
pages include FM, news and public figures? public
Facebook pages. We crawled the latest 10 feeds
from these public pages using the Facebook API
and gathered about 12,000 comments and posts for
the shared task.
Initially, we sought out help from Nepali gradu-
ate students at the University of Alabama at Birm-
ingham to annotate 100 tweets (1739 tokens). We
gave the same annotation file to two annotators to
do the annotation. We found that they agreed with
an accuracy of 95.34%. These tweets were then re-
viewed and used as initial gold data in Crowdflower
to annotate the first 1000 tweets. The annotation
job was enabled only in Nepal and Bhutan. We
disabled India, even though people living in some
regions of India (Darjeeling, Sikkim) also speak
and write in Nepali, as most spammers were com-
ing from India. We then ran two batches of 5000
tweets and one batch of 3000 tweets along with the
initial 1,000 tweets as the gold data. This NEP-EN
data was then split into a 9,993 tweet training set
and a 2,874 tweet test set. No Twitter user appeared
in both sets.
4.3 MAN-EN data
The MAN-EN tweets were collected from Twitter
with the Twitter API. Users were selected from
lists of most followed Twitter accounts in Taiwan
(where Mandarin Chinese is the official language).
These users? tweets were checked for Mandarin En-
glish bilingualism and added to our data collection
if they contained both languages.
The next round of usernames came from the
lists of users that our original top accounts were
following. The tweets written by this new set of
users were then examined for Mandarin English
code switching and stored as data if they matched
the criteria.
The jieba tokenizer
7
was used to segment the
Mandarin sections of the tweets and compute off-
sets of each segment. We format the code switch-
ing tweets into columns including language type,
labels, and offsets. Named entities were labeled
manually by a single annotator.
The data was split by user into 1000 tweets for
training and 313 for testing. No MAN-EN surprise
data for the current shared task.
4.4 MSA-DA data
For the MSA-DA language pair, we selected Egyp-
tian Arabic (EGY) as the Arabic dialect. We har-
vested data from two social media sources: Twitter
[TWT] and Blog commentaries [COM]. The TWT
data served as the main gold standard data for the
task where we provided fully annotated data for
Training/Tuning and Test. We provided two TWT
data sets for the test data that exemplified different
tag distributions. The COM data set comprised
only test data and it served as the Arabic surprise
data set.
To reduce the potential of TWT data attrition
from users deleting their accounts or tweets, we
selected tweets that are less prone to deletion and/or
change. Thereby we harvested tweets by a select
set of Egyptian Public Figures. The percentage
of deleted tweets and deactivated accounts among
those users is significantly lower if we compare it
to the tweets crawled from random Egyptian users.
We used the ?Tweepy? library to crawl the time-
lines of 12 Public Figures. Similar to other lan-
guage pairs, we excluded all re-tweets, tweets with
URLs, tweets mentioning other users, and tweets
containing Latin characters. We accepted 9,947
tweets, for each we extracted the tweet-id and user-
id. Using these IDs, we retrieved the tweets text,
tokenized it and assigned character offsets. To guar-
antee consistency and avoid any misalignment is-
sues, we compiled the full pipeline into the ?Arabic
Tweets Token Assigner? package which is made
7
https://github.com/fxsjy/jieba
65
available through the workshop website
8
.
For COM, we selected 6723 commentaries (half
MSA and half DA) from ?youm7?
9
commen-
taries provided by the Arabic Online Commentary
Dataset (Zaidan and Callison-Burch, 2011). The
COM data set was processed (12017 total tokens)
using the same pipeline created for the task. We
also provided the participants with the data format-
ted with character offsets to maintain consistency
across data sets in the Arabic subtask.
The annotation of MSA-DA language pair data
is based on two sets of guidelines. The first set
is a generic set of guidelines for code switching
in general across different language pairs. These
guidelines provide the overarching framework for
annotating code switched data on the morpholog-
ical, lexical, syntactic, and pragmatic levels. The
second set of guidelines is language pair specific.
We created the guidelines for the Arabic language
specifically. We enlisted the help of 3 annotators
in addition to a super annotator, hence resulting
in 4 annotators overall for the whole collection of
the data. All the annotators are native speakers
of Egyptian Arabic with excellent proficiency in
MSA. The super annotator only annotated 10% of
the overall data and served as the adjudicator. The
annotation process was iterative with several repe-
titions of the cycle of training, annotation, revision,
adjudication until we approached a stable Inter An-
notator Agreement (IAA) of over 90% pairwise
agreement.
5 Survey of Shared Task Systems
We received submissions from seven different
teams. Each participating system had the freedom
to submit responses to any of the language pairs
covered in the shared task. All seven participants
submitted system responses for SPA-EN, making
this language pair the most popular in this shared
task and MAN-EN the least popular.
All but one participating system used a machine
learning algorithm or language models, or even a
combination of both, as part of their configuration.
A couple of the participating systems used hand-
crafted rules of some sort, either at the intermediate
steps or as the final post-processing step. We also
observed a good number of systems using exter-
nal resources, in the form of labeled monolingual
8
http://emnlp2014.org/workshops/
CodeSwitch/call.html
9
An Egyptian newspaper, www.youm7.com
corpora, language specific gazetteers, off the shelf
tools (NE recognizers, language id systems, or mor-
phological analyzers) and even unsupervised data
crawled from the same users present in the data
sets provided. Affixes were also used in some form
by different systems.
The architecture of the different systems ranged
from a simple approach based on frequencies of
character n-grams combined in a rule-based system,
to more complex approaches using word embed-
dings, extended Markov Models, and CRF autoen-
coders. The majority of the systems that partici-
pated in more than one language pair did little to no
customization to account for the morphological dif-
ferences of the specific language pairs beyond lan-
guage specific parameter-tuning, which probably
reflects participants? goal to develop a multilingual
id system.
Due to the presence of the NE label, several
systems included a component for NE recognition
where there was one available for the specific lan-
guage. In addition, many systems also included
case information. One unexpected finding from
the shared task was that no participating system
tried to embed in their models some form of lin-
guistic theory or framework about CS. Only one
system made an explicit reference to CS theories
(Chittaranjan et al., 2014) in their motivation to use
contextual information, which can be considered
as a loose embedding of CS theory. While system
performance was competitive (see next section),
there is still room for improvement and perhaps
some of that improvement can come out of adding
this kind of knowledge into the models. Lastly, we
were surprised to see that not all systems made use
of character encoding information, even though for
Mandarin-English that would have been a strong
indicator. In Table 3 we present a summary high-
lighting some of the design choices of participating
systems.
6 Results
We used the following evaluation metrics: Accu-
racy, Precision, Recall, and F-measure. We use
F-measure to provide a ranking of systems. In the
evaluation at the tweet level we use the standard
f-measure. For the evaluation at the token level
we use instead the average weighted f-measure to
account for the highly imbalanced distribution of
classes.
To provide a fair evaluation, we only scored pre-
66
System
Machine
Learning
Rules Case
Character
Encoding
External Resources LM Affixes Context
(Chittaranjan et al., 2014)
CRF
4
4 dbpedia dumps, online sources
? 3
(Shrestha, 2014) 4
4 spell checker
(Jain and Bhat, 2014)
CRF
4
4 English dictionary
4 4 ? 2
(King et al., 2014)
eMM
ANERgazet, TwitterNLP, Stan-
ford NER
4 4 4
(Bar and Dershowitz, 2014)
SVM
4
Illocution Twitter Lexicon,
monolingual corpora (NE lists)
4 4 ? 2
(Lin et al., 2014)
CRF
4
4
Hindi-Nepali Wikipedia, JRC,
CoNLL 2003 shared task, lang
id predictors: cld2 and ldig
4 4
(Barman et al., 2014)
kNN, SVM
4 4
BNC, LexNorm
4 ? 1
Table 3: Comparison of shared task participating system algorithm choices. CRF stands for Conditional
Random Fields, SVM for Support Vector Machines and LM for Language Models.
dictions on tweets submitted by all teams. All
systems were compared to a simple lexicon-based
baseline. The lexicon was gathered from the train-
ing data for classes lang1 and lang2 only. Emoti-
cons, punctuation marks, usernames and URLs are
by default tagged as other. In the case of a tie or a
new token, the baseline system assigns the majority
class for that language pair.
Figure 1 shows prediction performance on the
Twitter test data for each language pair at the tweet
level. The system predictions for this task are taken
directly from the individual token predictions in
the following manner: if the system predictions for
the same tweet contain at least one tag from each
language (lang1 and lang2), the tweet is labeled
as code-switched, otherwise it is labeled as mono-
lingual. As illustrated, each language pair shows
different patterns. Comparing the systems that par-
ticipated in all language pairs, there is no clear
winner across the board. However, (Chittaranjan et
al., 2014) was in the top three places in at least one
test file for each language pair. Table 4 shows the
results at the token level by label. Here again the
figures show F-measure per class label and the last
column is the weighted average f-measure (Avg-F).
One of the few general trends on these results is
that most participating systems were not able to
correctly identify the minority classes ?ambiguous?
and ?other?. There are only few instances of these
labels in the training set and some test sets did not
have one of these classes present. The impact on
final system performance from these classes is not
significant. However, to study CS patterns we will
need to have these labels identified properly.
The MAN-EN pair received four system re-
sponses and all four of them reached an F-measure
>80% and outperformed the simple baseline by a
considerable margin. We expected this language
pair to be the easiest one for the shared task since
each language uses a different encoding script. A
very rough but accurate distinction between Man-
darin and English could be achieved by looking
at the character encoding. However, according to
the system descriptions provided, not all systems
used encoding information. The best performing
systems for MAN-EN are (King et al., 2014) and
(Chittaranjan et al., 2014). The former slightly
outperformed the latter at the Tweet level (see Fig-
ure 1a) task while the opposite was true at the token
level (see Table 4 rows 4 and 5).
In the case of SPA-EN, all seven systems out-
performed the simple baseline. The best perform-
ing system in all SPA-EN tasks was (Bar and
Dershowitz, 2014). This system achieved an F-
measure of 82.2%, 2.9 percentage points above the
second best system (Lin et al., 2014) on the tweet
level task (see Figure 1(d)). In the token level
evaluation, (Bar and Dershowitz, 2014) reached
an Avg. F-measure of 94%. This top performing
system uses a sequential classification approach
where the labels from the preceding words are used
as features in the model. Another design choice
that might have given the edge to this system is the
fact that their model combines character- and word-
based language models in what the authors call
?intra- and inter-word level? features. Both types
of language models are trained on large amounts
of monolingual data and NE lists, which again pro-
vides additional knowledge that other systems are
not exploiting. For instance, the NE lexicons might
account for the best results in the NE class in both
the Twitter data and the Surprise genre (see Table 4
last row for SPA-EN and second to last for SPA-
EN Surprise). Most systems showed considerable
67
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Chittaranjan
et al., 2014)
(King et
al., 2014)
0.6
0.7
0.8
0.9
1
0.838
0.888
0.892
0.894
F
-
m
e
a
s
u
r
e
Baseline
(a) MAN-EN
(Chittaranjan
et al., 2014)
(King et
al., 2014)
(Jain and
Bhat, 2014)
(Elfardy et
al., 2014)
(Lin et
al., 2014)
0.1
0.2
0.3
0.4
0.152
0.118
0.048
0.044
0.095
0.196
0.260
0.338
0.360
0.417
F
-
m
e
a
s
u
r
e
Baseline Test1
Baseline Test2
(b) MSA-DA. Dark gray bars show performance on Test1 and light gray bars show performance for Test2
(King et
al., 2014)
(Lin et
al., 2014)
(Jain and
Bhat, 2014)
(Shrestha,
2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
0.8
0.9
1
0.952
0.962
0.972
0.974
0.975
0.977
F
-
m
e
a
s
u
r
e
Baseline
(c) NEP-EN
(Shrestha,
2014)
(King et
al., 2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Bar and
Dershowitz,
2014)
0.6
0.7
0.8
0.9
1
0.634
0.703
0.753
0.754
0.783
0.793
0.822
F
-
m
e
a
s
u
r
e
Baseline
(d) SPA-EN
Figure 1: Prediction results on language identification at the tweet level. This is a binary task to distinguish
between a monolingual and a CS tweet. We show performance of participating systems using F-measure
as the evaluation metric. The solid line shows the lexicon baseline performance.
differences in prediction performance in both gen-
res. In all cases the Avg. F-measure was higher
on the Twitter test data than on the surprise genre.
Although the surprise genre is too small to draw
strong conclusions, all language pairs with surprise
genre test data showed a decrease in performance
of around 10%.
We analyzed system outputs and found some
consistent sources of error. Lexical forms that exist
in both languages were frequently mislabeled by
68
most systems. For example the word for ?he? was
frequently mislabeled by at least one system. In
most of the cases systems were predicting EN as
label when the target language was SPA. Cases like
this were even more prone to errors when these
words fell in the CS point, as in this tweet: ni el
header he hecho (I haven?t even done the header).
Tweets like this one, with just one token from the
other language, were difficult for most systems.
Named entities were also frequent sources of error,
especially when they were spelled with lower cases
letters.
By far the hardest language pair in this shared
task was MSA-DA, as anticipated. Especially when
considering the typological similarities between
MSA and DA. This is mainly due to the fact that
DA and MSA are close variants of one another and
hence they share considerable amount of lexical
items. The shared lexical items could be simple
cognates of one another, or faux amis where they
are homographs or homophones, but have com-
pletely different meaning. Both categories con-
stitute a significant challenge. Accordingly, the
baseline system had the lowest performance from
all language pairs in both test sets. We note chal-
lenges in this language pair on each linguistic level
where CS occurs especially for the shared lexical
items.
On the phonological level, DA writers tend to
mimic the MSA script for DA words even if they
are pronounced differently. For example: ?heart? is
pronounced in DA Alob and in MSA as qalob but
commonly written in MSA as ?qalob? in DA data.
Also many phonological differences are in short
vowels that are underspecified in written Arabic,
adding another layer of ambiguity.
On the morphological level, there is no avail-
able morphological analyzer able to recognize such
shared words and hence they are mostly misclassi-
fied. Language identification for MSA-DA CS text
highly depends on the context. Typically some Ara-
bic variety word serves as a marker for a context
switch such as mElh$ for DA or mn? for MSA. But
if shared lexical items are used, it is challenging
to identify the Arabic variant. An example from
the training data is qlb meaning either heart as a
noun or change as a verb in the phrase lw qlb mjrm,
corresponding to ?If the heart of a criminal? or ?if
he changes into a criminal?. These challenges ren-
der language identification for CS MSA-DA data
far from solved as evident by the fact that the high-
est scoring system reached an F-measure of only
41.7% in Test2 for CS identification. Moreover,
this is the only language pair where at least one
system was not able to outperform the baseline and
in the case of Test2 only one system (Lin et al.,
2014) outperformed the baseline.
Most teams did well for the NEP-EN shared task,
and all teams outperformed the baseline. The rea-
son for the high performance might be the high
number of codeswitched tweets in the training and
test data for NEP-EN (much higher than other lan-
guage pairs). This allowed systems to have more
samples of CS instances. The other reason for good
performance by most participants in both evalua-
tions might be that Nepali and English are two very
different languages. The structure of the words and
syntax of word formation are very different. We
suspect, for instance, that there is a much lower
overlap of character n-grams in this language pair
than in SPA-EN, which makes for an easier task. At
the Tweet level, system performance ranged over
a small set of values, the lowest F-measure was
95.2% while the highest was 97.7%. Looking at
the numbers in Table 4, we can see that even NE
recognition seemed to be a much easier task for this
language pair than for SPA-EN (compare results
for the NE category in both SPA-EN sets to those
of both NEP-EN data sets). The best performing
system for the Twitter test data is (Barman et al.,
2014) with an F-measure of 97.7%. The results
trend in the surprise genre is not consistent with
what we observed for the Twitter test data. The
top ranked system for Twitter sunk to the 4th place
with an F-measure or 59.6%, a considerable drop
of almost 40 percentage points. In this case, the
overall numbers indicate a much wider difference
in the genres than what we observed for other lan-
guages, such as SPA-EN, for example. We should
note that the class distribution in the surprise data is
considerably different from what the models used
for training, and from that of the test data as well.
In the Twitter data there was a larger number of CS
tweets than monolingual ones, while in the surprise
genre the majority class was monolingual. This
will account for a good portion of the differences
in performance. But here as well, the small number
of labeled instances makes it hard to draw strong
conclusions.
69
Test Set System lang1 lang2 NE other ambiguous mixed Avg-F
MAN-EN
Baseline 0.9 0.47 0 0.29 - 0 0.761
(Jain and Bhat, 2014) 0.97 0.66 0.52 0.33 - 0 0.871
(Lin et al., 2014) 0.98 0.73 0.62 0.34 - 0 0.886
(King et al., 2014) 0.98 0.74 0.58 0.30 - 0 0.884
(Chittaranjan et al., 2014) 0.98 0.76 0.66 0.34 - 0 0.892
MSA-DA Test 1
(King et al., 2014) 0.88 0.14 0.05 0 0 - 0.720
Baseline 0.92 0.06 0 0.89 0 - 0.819
(Chittaranjan et al., 2014) 0.94 0.15 0.57 0.91 0 - 0.898
(Jain and Bhat, 2014) 0.93 0.05 0.73 0.87 0 - 0.909
(Lin et al., 2014) 0.94 0.09 0.74 0.98 0 - 0.922
(Elfardy et al., 2014)* 0.94 0.05 0.85 0.99 0 - 0.936
MSA-DA Test 2
Baseline 0.54 0.27 0 0.94 0 0 0.385
(King et al., 2014) 0.59 0.59 0.13 0.01 0 0 0.477
(Chittaranjan et al., 2014) 0.58 0.50 0.42 0.43 0.01 0 0.513
(Jain and Bhat, 2014) 0.62 0.49 0.67 0.75 0 0 0.580
(Elfardy et al., 2014)* 0.73 0.73 0.91 0.98 0 0.01 0.777
(Lin et al., 2014) 0.76 0.81 0.73 0.98 0 0 0.799
MSA-DA Surprise
(King et al., 2014) 0.48 0.60 0.05 0.02 0 0 0.467
(Jain and Bhat, 2014) 0.53 0.61 0.62 0.96 0 0 0.626
(Chittaranjan et al., 2014) 0.56 0.69 0.33 0.96 0 0 0.654
(Lin et al., 2014) 0.68 0.82 0.61 0.97 0 0 0.778
(Elfardy et al., 2014)* 0.66 0.81 0.87 0.99 0 0 0.801
NEP-EN
Baseline 0.67 0.76 0 0.61 - 0 0.678
(King et al., 2014) 0.87 0.80 0.51 0.34 - 0.03 0.707
(Lin et al., 2014) 0.93 0.91 0.49 0.95 - 0.02 0.917
(Jain and Bhat, 2014) 0.94 0.96 0.52 0.94 - 0 0.942
(Shrestha, 2014) 0.94 0.96 0.57 0.95 - 0 0.944
(Chittaranjan et al., 2014) 0.94 0.96 0.45 0.97 - 0 0.948
(Barman et al., 2014) 0.96 0.97 0.58 0.97 - 0.06 0.959
NEP-EN Surprise
(Lin et al., 2014) 0.83 0.73 0.46 0.65 - - 0.712
(King et al., 2014) 0.82 0.88 0.43 0.12 - - 0.761
(Chittaranjan et al., 2014) 0.78 0.87 0.37 0.80 - - 0.796
(Jain and Bhat, 2014) 0.83 0.91 0.50 0.87 - - 0.850
(Barman et al., 2014) 0.87 0.90 0.61 0.74 - - 0.853
(Shrestha, 2014) 0.85 0.92 0.53 0.78 - - 0.855
SPA-EN
Baseline 0.72 0.56 0 0.75 0 0 0.704
(Shrestha, 2014) 0.88 0.85 0.35 0.92 0 0 0.873
(Jain and Bhat, 2014) 0.92 0.92 0.36 0.90 0 0 0.905
(Lin et al., 2014) 0.93 0.93 0.32 0.91 0.03 0 0.913
(Barman et al., 2014) 0.93 0.92 0.47 0.93 0.03 0 0.921
(King et al., 2014) 0.94 0.93 0.54 0.92 0 0 0.923
(Chittaranjan et al., 2014) 0.94 0.93 0.28 0.95 0 0 0.926
(Bar and Dershowitz, 2014) 0.95 0.95 0.56 0.94 0.04 0 0.940
SPA-EN Surprise
(Shrestha, 2014) 0.80 0.78 0.23 0.81 0 0 0.778
(Jain and Bhat, 2014) 0.83 0.84 0.22 0.79 0 0 0.811
(Lin et al., 2014) 0.83 0.86 0.19 0.80 0.03 0 0.816
(Barman et al., 2014) 0.84 0.85 0.31 0.82 0.03 0 0.823
(Chittaranjan et al., 2014) 0.94 0.86 0.14 0.83 0 0 0.824
(King et al., 2014) 0.84 0.85 0.35 0.81 0 0 0.828
(Bar and Dershowitz, 2014) 0.85 0.87 0.37 0.83 0.03 0 0.839
Table 4: Performance results on language identification at the token level. A ?-? indicates there were no
tokens of this class in the test set. We ranked systems using weighted averaged f-measure (Avg-F). The ?*?
marks the system by (Elfardy et al., 2014). This system was not considered in the ranking for the shared
task as it was developed by co-organizers of the task.
7 Lessons Learned
Among the things we want to improve for future
shared tasks is the issue of data loss due to re-
moval of tweets or users deleting their accounts.
We decided to use Twitter data to have a relevant
corpus. However, the trade-off is the lack of rights
to distribute the data ourselves. This is not just a
burden for the participants. It is an awful waste of
resources as the data that was expensive to gather
and label is not being used beyond the small group
of researchers involved in the creation of the cor-
pus. This will deter us from using Twitter data for
future shared tasks, at least until a better solution
is identified.
70
(Chittaranjan
et al., 2014)
(King et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Elfardy et
al., 2014)
0
0.1
0.2
0.3
0.170
0.194
0.222
0.276
0.277
F
-
m
e
a
s
u
r
e
(a) MSA-DA Surprise Genre Results
(Chittaranjan
et al., 2014)
(Jain and
Bhat, 2014)
(Barman et
al., 2014)
(King et
al., 2014)
(Shrestha,
2014)
(Lin et
al., 2014)
0.4
0.5
0.6
0.7
0.554
0.571
0.596
0.604
0.632
0.702
F
-
m
e
a
s
u
r
e
(b) NEP-EN Surprise Genre Results
(Shrestha,
2014)
(King et
al., 2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Bar and
Dershowitz,
2014)
0.4
0.5
0.6
0.7
0.8
0.633
0.640
0.704
0.710
0.725
0.727
0.753
F
-
m
e
a
s
u
r
e
(c) SPA-EN Surprise Genre Results
Figure 2: Prediction results on language identification at the document level for the surprise genre. This
is a binary task to distinguish between a monolingual and a code-switched text. We show performance of
participating systems using F-measure as the evaluation metric.
Using crowdsourcing for annotating the data is a
cheap and easy way for generating resources. But
we found out that even when following best prac-
tices for quality control, there was a substantial
amount of noise in the gold data. We plan to con-
tinue working on refining the annotation guidelines
and quality control processes to reduce the amount
of noise in gold annotations.
8 Conclusion
This is the first shared task on language identifica-
tion in CS data. Yet, the response was quite positive
as we received 42 system runs from seven different
teams, plus submissions for MSA-AD from a sub
group of the task organizers (Elfardy et al., 2014).
The systems presented are overall robust and with
interesting differences from one another. Although
we did not see a single system ranking in the top
places across all language pairs and tasks, we did
see systems showing robust performance indicat-
ing some level of language independence. But the
results are not consistent at the tweet/document
level. The language pair that proved to be the most
difficult for the task was MSA-DA, where the lexi-
con baseline system was hard to beat even with an
F-measure of 47.1%.
This shared task showed that language identifica-
tion in code-switched data is still an open problem
that warrants further investigation. Perhaps in the
near future we will see systems that embed some
form of linguistic theory about CS and maybe that
would result in more accurate predictions.
Our goal is to support new research addressing
CS data. Discussions about the challenge for the
next shared task are already underway. One pos-
sibility might be parsing. We plan to investigate
the challenges in parsing CS data and we will start
by exploring the hardships in manually annotating
CS with syntactic information. We would also like
to explore the possibility of classifying CS points
according to their socio-pragmatic role.
71
Acknowledgments
We would like to thank all shared task partici-
pants. We also thank Brian Hester and Mohamed
Elbadrashiny for their invaluable support in the
development of the gold standard data and analy-
sis of results. We also thank the in-lab annotators
and the CrowdFlower contributors. This work was
partly funded by NSF under awards 1205475 and
1205556.
References
Kfir Bar and Nachum Dershowitz. 2014. Tel Aviv Uni-
versity system description for the code-switching
workshop shared task. In Proceedings of the First
Workshop on Computational Approaches to Code-
Switching, Doha, Qatar, October. ACL.
Utsab Barman, Joachim Wagner, Grzegorz Chrupala,
and Jennifer Foster. 2014. DCU-UVT: Word-
level language classification with code-mixed data.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code-Switching, Doha, Qatar,
October. ACL.
Gokul Chittaranjan, Yogarshi Vyas, Kalika Bali, and
Monojit Choudhury. 2014. A framework to label
code-mixed sentences in social media. In Proceed-
ings of the First Workshop on Computational Ap-
proaches to Code-Switching, Doha, Qatar, October.
ACL.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2014. AIDA: Identifying code switching in
informal Arabic text. In Proceedings of the First
Workshop on Computational Approaches to Code-
Switching, Doha, Qatar, October. ACL.
Naman Jain and Riyaz Ahmad Bhat. 2014. Language
identification in codeswitching scenario. In Pro-
ceedings of the First Workshop on Computational
Approaches to Code-Switching, Doha, Qatar, Octo-
ber. ACL.
A. Joshi. 1982. Processing of sentences with in-
trasentential code-switching. In J?an Horeck?y, editor,
COLING-82, pages 145?150, Prague, July.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia, June. Association for Com-
putational Linguistics.
Levi King, Eric Baucom, Tim Gilmanov, Sandra
K?ubler, Dan Whyatt, Wolfgang Maier, and Paul Ro-
drigues. 2014. The IUCL+ system: Word-level
language identification via extended Markov models.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code-Switching, Doha, Qatar,
October. ACL.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
Mandarin-English code-switching corpus. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 2515?2519, Istanbul, Turkey, May. European
Language Resources Association (ELRA). ACL An-
thology Identifier: L12-1573.
Constantine Lignos and Mitch Marcus. 2013. Toward
web-scale analysis of codeswitching. In 87th An-
nual Meeting of the Linguistic Society of America.
Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori
Levin. 2014. The CMU submission for the shared
task on language identification in code-switched
data. In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching, Doha,
Qatar, October. ACL.
D.C. Lyu, T.P. Tan, E. Chng, and H. Li. 2010. SEAME:
a Mandarin-English code-switching speech corpus
in South-East Asia. In INTERSPEECH, volume 10,
pages 1986?1989.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing, pages 857?862, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Prajwol Shrestha. 2014. An incremental approach for
language identification in codeswitched text. In Pro-
ceedings of the First Workshop on Computational
Approaches to Code-Switching, Doha, Qatar, Octo-
ber. ACL.
Anil Kumar Singh and Jagadeesh Gorla. 2007. Identifi-
cation of languages and encodings in a multilingual
document. In Proceedings of ACL-SIGWAC?s Web
As Corpus3, Belgium.
Omar F. Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: An annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: Short Papers - Volume
2, HLT ?11, pages 37?41, Stroudsburg, PA, USA.
Association for Computational Linguistics.
72
