Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 45?50,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
An Ontology-Based View on Prepositional Senses
Tine Lassen
Roskilde University
Denmark
tlassen@ruc.dk
Abstract
This paper describes ongoing work, aimed
at producing a lexicon of prepositions, i.e.
relations denoted by prepositions, to be
used for information retrieval purposes.
The work is ontology based, which for this
project means that the ontological types of
the arguments of the preposition are con-
sidered, rather than the word forms. Thus,
sense distinctions are made based on onto-
logical constraints on the arguments.
1 Introduction
In traditional web search engines, information re-
trieval relies more or less exclusively on sim-
ple string match. In the OntoQuery project1,
ontology-based search in text databases is per-
formed based on a match between the concep-
tual content of the search phrase and the text seg-
ments in the database.(Andreasen et al, 2002; An-
dreasen et al, 2004). In short, concepts are identi-
fied through their corresponding surface form and
mapped into an ontology. The use of an ontology
makes it possible to introduce the notion of con-
ceptual distance and thereby ranking the search re-
sult by semantic similarity. E.g. ?pony? and ?ze-
bra? may be more similar concepts than ?pony?
and ?lion?, because the distance when traversing
a graph representation of the ontology is longer
going from ?pony? to ?lion? than from ?pony? to
?zebra?. See figure 1 for a simplified excerpt of
the ontology.
However, only relatively simple noun phrases
are currently recognized and mapped into the on-
tology, and we are thus investigating the possibil-
ities of expanding the scope of our concept-based
1http://www.ontoquery.dk
Figure 1: Excerpt from the ontology with the con-
cepts ?horse? and ?cat?
analysis by including the semantic relations that
hold between noun phrases. Our first experiments
in this have been an analysis of prepositions and
their surrounding noun phrases. The immediate
aim is to be able to include a lexicon of preposi-
tions, consisting of a lexicon entry for each sense
of a given preposition (a sense, in this context, is
a relation that it can denote). Each entry has an
argument structure and ontological constraints on
the arguments. Thus, a preposition in a given con-
text will be assigned a pertinent sense based on the
ontological types of its surrounding noun phrases.
If this can be achieved, we will be able to say, e.g.,
that the text segments ?she is riding her pony in the
morning? and ?he was riding a pony during the
war? are more closely related, i.e. the relational
distance is smaller, than any of the two to ?she
was riding her pony in the hall?. The relations that
hold between ?pony? and ?morning?, and ?pony?
and ?war?, denoted by in and during respectively,
are of a temporal nature, whereas the relation that
holds between ?pony? and ?hall?, denoted by in, is
of a locative nature. The notion of relational dis-
tance is similar to that of conceptual distance; the
distance when traversing a graph representation of
the relation ontology. A combined measure, that
takes into account both the conceptual and rela-
45
tional distance would have to be introduced in or-
der to express the combined distance between such
more complex structures (structures that are more
complex than the simple noun phrases, that is), but
this question is beyond the scope of this paper2.
Initially, we use a predefined set of relations that
was originally proposed in (Nilsson, 2001) (see
table 1). This set is hierarchically unstructured,
which means that per default, the conceptual dis-
tance between any given two pairs of relations is
the same. We will later introduce an ontology of
relations, which will make it possible to express
that the distance between e.g. the partitive and the
locative relation is smaller than the distance be-
tween the locative and the causal relation (see sec-
tion 4.1).
2 Approach
We are using a bottom-up approach, in which we
manually annotate a corpus3 with semantic rela-
tions for all occurences of prepositions that are
surrounded by noun phrases. Further we anno-
tate the heads of the surrounding noun phrases
with their ontological type and subsequently an-
alyze the result in order to produce ontological
constraint rules. The ontology that was used for
the ontological type annotation, is the SIMPLE top
ontology (Pedersen, 1999; Lenci et al, 2000).
Relations exist between the entities referred to in
discourse, and can exist at different syntactic lev-
els; across sentence boundaries as in Peter owns
a pony. It is stubborn , or whithin a sentence, a
phrase or a word. The relations can be denoted by
different parts of speech, such as a verb, a prepo-
sition or an adjective, or they can be implicitly
present in compounds and genitive constructions
as in Peter?s pony.
The following account is based on the work of
(Jensen and Nilsson, 2006): Semantic relations are
n-ary (where n?1): In the example Peter owns a
pony the verb ?owns? denotes a binary relation be-
tween Peter and a pony, and in example Peter gave
the pony a carrot, the verb ?give? denotes a ternary
relation between Peter, the pony and a carrot. In
the example The pony in the field the preposition
?in? denotes a binary relation between the pony
2For a discussion of a distance measure between noun
phrases, see e.g. (Bulskov and Andreasen, 2004) or (Knappe
and Andreasen, 2002)
3The corpus is a small corpus of approximately 18,500
running words has been compiled from texts from the domain
of nutrition.
and the field. In the framework of this project,
however, we will only consider binary relations
denoted by prepositions. Using the algebraic de-
scription language OntoLog (Nilsson, 2001), we
express binary relations as A[REL:B], where the
first argument of the relation, A, relates to the sec-
ond argument, B, in the manner REL.
A preposition, however, can be ambiguous in re-
gard to which relation it denotes (we assume a re-
stricted set of possible relations for prepositions,
see table 1). As an example, let us consider the
Danish preposition i (Eng: in): The surface form i
in ?A i B? can denote at least five different relations
between A and B:
1. A patient relation PNT; a relation where one of the ar-
guments? case role is patient, e.g. ??ndringer i stof-
skiftet? (changes in the metabolism).
2. A locational relation LOC; a relation that denotes the
location/position of one of the arguments compared to
the other argument, e.g. ?skader i hjertemuskulaturen?
(injuries in the heart muscle).
3. A temporal relation TMP; a relation that denotes the
placement in time of one of the arguments compared
to the other, e.g. ?generalforsamlingen i 1981? (the
general assembly in 1981).
4. A property ascription relation CHR; a relation that de-
notes a characterization relation between one of the ar-
guments and a property, e.g. ?antioxidanter i renfrem-
stillet form? (antioxidants in a pure form)
5. A ?with respect to? relation WRT; an underspecified re-
lation that denotes an ?aboutness? relation between the
arguments, e.g. ?forskelle i saltindtagelsen? (differ-
ences in the salt intake) .
Role Description
TMP Temporal aspects
LOC Location, position
PRP Purpose, function
WRT With respect to
CHR Characteristic (property ascription)
CUM Cum (i.e., with, accompanying)
CBY Caused by
CAU Causes
BMO By means of, instrument, via
CMP Comprising, has part
POF Part of
AGT Agent of act or process
PNT Patient of act or process
SRC Source of act or process
RST Result of act or process
DST Destination of moving process
Table 1: The set of possible relations used in the
annotation process (Nilsson, 2001)
46
3 Results
Following the initial annotation, we performed an
analysis of all occurences of the relations and the
ontological types of their arguments. Could we
identify patterns that could result in lexical rules
for the lexicon? The limited space here does not
allow us to show the full results of the analysis,
so we will focus on one preposition, the Danish
preposition i (Eng: in) and later focus on one
relation type denoted by that preposition, namely
the locative relation. There are 199 occurences of
the preposition i in the corpus, and the relations
that it denotes are distributed as follows:
LOC (137/199 : 68,8%)
WRT (25/199 : 12,5%)
TMP (17/199 : 8,5%)
PNT (11/199 : 5.5%)
CHR (9/199 : 4,5%)
If we look at the LOC relation, which is the
most frequent relation denoted by i in the corpus,
we get this distribution of ontological types for the
arguments: Of the 137 instances of i denoting a
locative relation, there are 57 different ontological
type-pairs, if we consider unique occurences of a
given onlogical type-pair (a pair, meaning the on-
tological types of the two arguments combined),
31 different first arguments, and 16 different sec-
ond arguments. The most significant ontological
type for arguments is the type ?body part? (BPA),
which occurs 10 times as first argument, and 66
times as second argument. However, in total, the
type occurs 119 times (13 times as first argument
and 106 times as second argument) in the corpus
as a whole. If we were to implement a rule that
would assign the relation LOC to any preposition
that has the ontological type ?body part? as any ar-
gument, we would get a precision4 score of 68.9.,
a higher score of 92.3 if we only consider the first
argument, and 66 if we only consider the second
argument.
However, if we limit the rule to assign the relation
LOC only to occcurences of the preposition i, with
arguments of the type BPA, then we get a precision
score of 100. This sounds promising, but it should
be noted that the coverage of the best rule ?IF any
argument is BPA AND preposition is ?i? THEN as-
4Precision = number of correct matches / number of
matches
sign LOC to preposition? is quite low: the recall5
score for the rule is 55.8, which means that we can
correctly assign a relation to 55.8% of the LOC
senses of the preposition i, and these only make
up 68.8% of the total number of i-occurences. In
fact, we can only correctly assign the LOC rela-
tion to 38.7% of the actual relations denoted by i
in the corpus using this rule. Only if we can pro-
duce more rules of this type with high precision
scores, we can be optimistic about the outcome of
the project.
Rule Precision Recall
IF any argument is BPA
THEN assign LOC to preposition 68.9 56.5
IF first argument is BPA
THEN assign LOC to preposition 92.3 8.3
IF second argument is BPA
THEN assign LOC to preposition 66 48.3
IF any argument is BPA
AND preposition is ?i?
THEN assign LOC to preposition 100 55.8
IF first argument is BPA
AND preposition is ?i?
THEN assign LOC to preposition 100 7.2
IF second argument is BPA
AND preposition is ?i?
THEN assign LOC to preposition 100 48.6
Table 2: Precision and recall scores for rules that
assign the LOC relation to prepositions with a
BPA constraint on the ontological type of argu-
ments
4 Suggestions to improve the results
In the following, we propose a way of improving
the results by introducing a relation ontology, and
further, by either generalizing or specializing the
ontological type level for the arguments. Our
hypothesis is that by doing this, we will end
up with rules that have broader coverage. By
coverage, we mean the number of occurences
that the rule applies to, compared to the number
of occurences that potentially could be covered
by the rule.If the relations are too general, then
we miss out on some of the semantic content of
the relation between the items that we consider,
and we want to capture as much semantics as we
can. On the other hand, in some cases it may be
that we have made distinctions in the relation set
that are not detectable when analyzing the data.
Also, if the ontological type distribution for the
5Recall = number of correct matches / ideal number of
matches
47
arguments is too coarse or too fine grained, the
patterns that appear when we analyze the data,
will not be general enough to produce rules from.
4.1 The relation ontology
The flat list of possible relations, as can be seen
in table 1, that we initially used, now has to be
transformed into a relation ontology. Our heuris-
tics for doing this, in short, is to group relations
that are more closely related than others in sub-
branches, such that the distance between them is
shorter than the distance to other less closely re-
lated relations. In figure 2, the intralocal and ex-
tralocal relations are more closely related than e.g.
the intralocal and dynamic relation, because the
distance when traversing the graph is two archs for
the former, and three arcs for the latter. One way
of deciding relatedness, is to say that if two rela-
tions have proven difficult to differentiate in the
initial annotation process, then they are probably
more closely related. This is the approach we have
chosen. Also, we have grouped together other re-
lations, such as the bidirective relations ?part of?
and ?has parts?, and ?causes? and ?caused by?, and
other relations that naturally group together, such
as the theta roles ?agent?-?patient? and ?source?-
?result?.
A possible next step is to specialize the rela-
tions that can be specialized. The relations that
intuitively make sense to specialize are the tem-
poral (as has been done in OWL-Time ontology
(formerly DAML-Time) (Hobbs and Pan, 2004)),
partitive (Winston and Hermann, 1987) and spa-
tial/local relations (e.g. DOLCE Spatialrel ontol-
ogy)6.
Our work with specialisations of the relation
ontology, particularly the local relations, is largely
inspired by Pustejovsky?s work on event struc-
tures (Pustejovsky, 1996). Pustejovsky suggests a
subdivision of complex events into subevents.
However, another way of expressing the dif-
ference between events with one or more
subevents is, as we will do in the following,
static and dynamic relations: static relations only
consist of one subevent, and dynamic relations
have more than one subevent.
? A dynamic locative relation is a com-
plex event, that consists of more than one
6http://www.loa-cnr.it/Files/DLPOnts/SpatialRel 397.owl
Figure 2: Illustration of static locative relations
subevent, and it denotes a source or a goal of
a process, or a place where a process unfolds.
? A static locative relation consists of just one
subevent, and it denotes ?being located at?.
Another type of specialisation of the locative re-
lation could be a subdivision into relations con-
cerning area, region, distance, etc. (as it has been
done in DOLCE), but the aforementioned static
and dynamic locative relations appears to be more
appropriate when the subject is relations denoted
by prepositions.
As a possible further specialisation of dynamic
and static relations we suggest:
? Intralocal: an intralocal relation denotes a
goal of a process (e.g. into the box), or a
location within a delimited area.
? Extralocal: an extralocal relation denotes a
point of departure of a process (e.g. out of
the box), or a location outside or touching
the outer limitation of a delimited area.
? Translocal: a translocal relation denotes a
location or a process through a delimited
area (e.g. through the box).
48
Figure 4: Excerpt of the relation ontology containing the locative relations
Figure 3: Illustration of dynamic locative relations
We now reannotate the locative relations ac-
cording to these new subtypes of the locative re-
lation; but conforming to the bottom-up approach,
we initially only subdivide into static and dynamic
locative relations. Can we observe a clearer pat-
tern with respect to the ontological types of the
arguments, if we do this? If so, then we try a more
fine grained subdivision.
The results of the LOC dynamic and LOC static
subdivision show that of the original 137 instances
of i that denote a locative relation, 33 denote a
LOC dynamic relation, and 104 denote a LOC
static relation. The patterns that we observe, are
actually clearer: for all, but one, of the dynamic
relations the first arguments denote some kind of
process or event, whereas the second arguments
are all more or less specialized types of concrete
entity. The most prevalent ontological type for the
static local relation is ?natural substance?, which
occurs 50 times. For the dynamic local relation,
the most prevalent relation for the first argument is
?change?, which occurs 17 times. The most preva-
lent second argument is again ?body part?, which
also occurs 17 times. If we now calculate preci-
sion and recall for rules that constrain arguments
of the static and dynamic locative relations to the
most prevalent ones, we get the results shown in
tables 3 and 4. However, we only calculate scores
for rules constraining the first argument, and only
for the preposition i, because only part of the cor-
pus has been annotated with these relations.
The precision score for the best rule is lower
than for the original LOC-rules (92.1 compared
to 100). However, considering that we capture
more semantics, and the fact that the arguments of
the static and dynamic locative relations are more
49
Rule Precision Recall
IF any argument is BPA
AND preposition is ?i?
THEN assign LOC>static to preposition 64.1 48.1
IF first argument is NSU
AND preposition is ?i?
THEN assign LOC>static to preposition 92.1 33.7
IF second argument is BPA
AND preposition is ?i?
THEN assign LOC>static to preposition 68.7 47.1
Table 3: Precision and recall scores for the rule
that assign the LOC>static relation to the preposi-
tion i with constraints on the ontological types the
arguments
Rule Precision Recall
IF first argument is CHA
AND preposition is ?i?
THEN assign LOC>dynamic to preposition 58.6 51.5
IF second argument is BPA
AND preposition is ?i?
THEN assign LOC>dynamic to preposition 25.4 51.5
Table 4: Precision and recall scores for the rule
that assign the LOC>dynamic relation to the
preposition i with constraints on the ontological
type of the first and second argument
uniform in their distribution, this indicates that a
generalisation of the ontological types of the ar-
guments will result in even better rules, i.e. rules
with a larger coverage.
5 Conclusion and further work
Our aim is to show that ontological types can be
used as constraints in a lexicon of semantic rela-
tions denoted by prepositions. In this paper we
have presented our preliminary results, that are
based on an analysis of a Danish corpus, com-
piled of texts from the domain of nutrition. We
have introduced an ontology of relations, which
will make it possible to measure relational dis-
tance between complex structures in addition to
the conceptual distance that we can measure be-
tween concepts. The results are promising: We
can produce rules that have good precision scores
for the locative relation, and we expect to improve
the rules by generalizing the ontological types of
the prepositional arguments. Also, we plan to ex-
pand our research to cover other relations than the
ones treated in this paper.
6 Acknowledgements
We would like to thank Troels Andreasen, Per
Anker Jensen and three anonymous reviewers for
fruitful comments and suggestions.
References
Troels Andreasen, Per Anker Jensen, J?rgen Fischer
Nilsson, Patrizia Paggio, Bolette Sandford Pedersen,
and Hanne Erdman Thomsen. 2002. Ontological
extraction of content for text querying. In Lecture
Notes in Computer Science, volume 2553, pages 123
? 136. Springer-Verlag.
Troels Andreasen, Per Anker Jensen, J?rgen Fischer
Nilsson, Patrizia Paggio, Bolette Sandford Pedersen,
and Hanne Erdman Thomsen. 2004. Content-based
text querying with ontological descriptors. Data &
Knowledge Engineering, 48(2):199?219.
R. Knappe H. Bulskov and T. Andreasen. 2004. Per-
spectives on ontology-based querying. Interna-
tional Journal of Intelligent Systems, to appear.
Jerry R. Hobbs and Feng Pan. 2004. An ontology of
time for the semantic web. ACM Trans. Asian Lang.
Inf. Process., 3(1):66?85.
Per Anker Jensen and J?rgen Fischer Nilsson, 2006.
Syntax and Semantics of Prepositions, volume 29
of Text, Speech and Language Technology, chap-
ter Ontology-Based Semantics for Prepositions.
Springer.
H. Bulskov R. Knappe and T. Andreasen, 2002. Flex-
ible Query Answering Systems, chapter On Measur-
ing Similarity for Conceptual Querying, pages pp.
100?111. Number 2522 in Lecture Notes in Artifi-
cial Intelligence.
Alessandro Lenci, Nuria Bel, Federica Busa, Nico-
letta Calzolari1, Elisabetta Gola, Monica Mona-
chini, Antoine Ogonowski, Ivonne Peters, Wim Pe-
ters, Nilda Ruimy, Marta Villegas, and Antonio
Zampolli. 2000. Simple: A general framework for
the development of multilingual lexicons. Interna-
tional Journal of Lexicography, 13(4):249?263.
J?rgen Fischer Nilsson. 2001. A logico-algebraic
framework for ontologies, ontolog. In Jensen and
Skadhauge, editors, Proceedings of the First Inter-
national OntoQuery Workshop Ontology-based in-
terpretation of NP?s. University of Southern Den-
mark, Kolding.
Bolette Sandford Pedersen. 1999. Den danske simple-
ordbog. en semantisk, ontologibaseret ordbog. In
C. Poulsen, editor, DALF 99, Datalingvistisk Foren-
ings a?rsm?de 1999. Center for sprogteknologi.
James Pustejovsky. 1996. The generative lexicon.
MIT Press, Cambridge, Mass.
Roger Winston, Morton E. Chaffin and Douglas Her-
mann. 1987. A taxonomy of part-whole relations.
Cognitive Science, 11:417?444.
50
An Ontology-Based Approach to Disambiguation of Semantic Relations
Tine Lassen and Thomas Vestskov Terney
Department of Computer Science, Roskilde University, Denmark
tlassen@ruc.dk, tvt@ruc.dk
Abstract
This paper describes experiments in using
machine learning for relation disambiguation.
There have been succesfuld experiments in
combining machine learning and ontologies,
or light-weight ontologies such as WordNet,
for word sense disambiguation. However,
what we are trying to do, is to disambiguate
complex concepts consisting of two simpler
concepts and the relation that holds between
them. The motivation behind the approach is
to expand existing methods for content based
information retrieval. The experiments have
been performed using an annotated extract of
a corpus, consisting of prepositions surroun-
ded by noun phrases, where the prepositions
denote the relation we are trying disambigu-
ate. The results show an unexploited opportu-
nity of including prepositions and the relations
they denote, e.g. in content based information
retrieval.
1 Introduction
What we describe in this paper, which we refer to as re-
lation disambiguation, is in some sense similar to word
sense disambiguation. In traditional word sense disam-
biguation the objective is to associate a distinguishable
sense with a given word (Ide and Ve?ronis, 1998). It
is not a novel idea to use machine learning in con-
nection with traditional word sense disambiguation,
and as such it is not a novel idea to include some kind
of generalization of the concept that a word expres-
ses in the learning task either (Yarowsky, 1992). Ot-
her projects have used light-weight ontologies such as
WordNet in this kind of learning task (Voorhees, 1993;
Agirre and Martinez, 2001). What we believe is our
contribution with this work is the fact that we attempt to
learn complex concepts that consist of two simpler con-
cepts, and the relation that holds between them. Thus,
we start out with the knowledge that some relation
holds between two concepts, which we could express
as REL(concept1,concept2), and what we aim at being
able to do is to fill in a more specific relation type than
the generic REL, and get e.g. POF(concept1,concept2)
in the case where a preposition expresses a partitive re-
lation. This makes it e.g. possible to determine from
the sentence ?France is in Europe? that France is a part
of Europe. As in word sense disambiguation we here
presuppose a finite and minimal set of relations, which
is described in greater detail in section 2.
The ability to identify these complex structures in text,
can facilitate a more content based information retri-
eval as opposed to more traditional search engines,
where the information retrieval relies more or less
exclusively on keyword recognition. In the OntoQuery
project1, pertinent text segments are retrieved based on
the conceptual content of the search phrase as well as
the text segments (Andreasen et al, 2002; Andreasen
et al, 2004). Concepts are here identified through their
corresponding surface form (noun phrases), and map-
ped into the ontology. As a result, we come from a flat
structure in a text to a graph structure, which describes
the concepts that are referred to in a given text segment,
in relation to each other.
However, at the moment the ontology is strictly a
subsumption-based hierarchy and, further, only relati-
vely simple noun phrases are recognized and mapped
into the ontology. The work presented here expands
this scope by including other semantic relations be-
tween noun phrases. Our first experiments in this di-
rection have been an analysis of prepositions with sur-
rounding noun phrases (NPs). Our aim is to show that
there is an affinity between the ontological types of the
NP-heads and the relation that the preposition denotes,
which can be used to represent the text as a complex
semantic structure, as opposed to simply running text.
The approach to showing this has been to annotate a
corpus and use standard machine learning methods on
this corpus.
2 Semantic relations
The following account is based on the work of (Jensen
and Nilsson, 2006): Relations exist between entities re-
ferred to in discourse. They can exist at different synta-
ctic levels; across sentence boundaries as in example 1,
or within a sentence, a phrase or a word. The relations
1http://www.ontoquery.dk
72
can be denoted by different parts of speech, such as a
verb, a preposition or an adjective, or they can be impli-
citly present in compounds and genitive constructions
as in example 2.
Semantic relations are n-ary: In example 1 below the
verb form ?owns? denotes a binary relation between Pe-
ter and a dog, and in example 3, the verb form ?gave?
denotes a ternary relation between Peter, the dog and a
bone. In example 4 the preposition ?in? denotes a bi-
nary relation between the dog and the yard.
(1) Peter owns a dog. It is a German shepherd.
(2) Peter?s dog.
(3) Peter gave the dog a bone.
(4) The dog in the yard.
In the framework of this machine learning project, we
will only consider binary relations denoted by prepo-
sitions. A preposition, however, can be ambiguous in
regard to which relation it denotes. As an example, let
us consider the Danish preposition i (Eng: in): The sur-
face form i in ?A i B? can denote at least five different
relations between A and B:
1. A patient relation PNT; a relation where one of the
arguments? case role is patient, e.g. ??ndringer i
stofskiftet? (changes in the metabolism).
2. A locational relation LOC; a relation that denotes
the location/position of one of the arguments com-
pared to the other argument, e.g. ?skader i hjer-
temuskulaturen? (injuries in the heart muscle).
3. A temporal relation TMP; a relation that denotes
the placement in time of one of the arguments
compared to the other, e.g. ?mikrobiologien i
1800-tallet? (microbiology in the 19th century).
4. A property ascription relation CHR; a relation that
denotes a characterization relation between one of
the arguments and a property, e.g. ?antioxidanter
i renfremstillet form? (antioxidants in a pure form)
5. A ?with respect to? relation WRT; an underspeci-
fied relation that denotes an ?aboutness? relation
between the arguments, e.g. ?forskelle i saltindta-
gelsen? (differences in the salt intake) .
As presented above, the idea is to perform supervised
machine learning, that will take into account the sur-
face form of the preposition and the ontological type
of the heads of the surrounding noun phrases, and on
this basis be able to determine the relation that holds
between noun phrases surrounding a preposition in un-
seen text.
3 The corpus
In order to establish a training set, a small corpus of ap-
proximately 18,500 running words has been compiled
from texts from the domain of nutrition and afterwards
annotated with the ontological type of the head of the
noun phrases, and the semantic relation denoted by the
preposition 2.
All the text samples in this corpus derive from ?The
Danish National Encyclopedia? (Gyldendal, 2004), and
are thus not only limited domain-wise, but also of a
very specific text type which can be classified as expert-
to-non-expert. Thus, we cannot be certain that our re-
sults can be directly transferred to a larger or more ge-
neral domain, or to a different text type. This aspect
would have to be empirically determined.
3.1 Annotation
For the purpose of learning relations, 952 excerpts of
the form:
NP ? P ?NP (5)
have been extracted from the corpus and annotated with
information about part of speech, ontological type and
relation type for NP heads and prepositions, respecti-
vely. An example of the analyzed text excerpts are gi-
ven in table 1 on the following page, where each row
indicates a level of the analysis.
The POS-tagging and head extraction have been done
automatically, the ontological type assignation partly
automatically (ontology look-up) and partly manually
(for words that do not exist as instantiations of concepts
in the ontology). The relation annotation has been done
manually.
The tags used in the annotation on the three levels are:
POS-tags. Our tagger uses a subset of the PAROLE
tag set, consisting of 43 tags, see (Hansen, 2000),
which means that it is a low level POS tagging
with little morphosyntactic information. We only
use the tags in order to extract NPs and preposi-
tions, and thus do not need a more fine-grained
information level.
SIMPLE-tags. The tags used for the ontological type
annotation consist of abbreviations of the types in
the SIMPLE top ontology. The tag set consists of
151 tags.
Relation-tags. The tags used for the relation anno-
tation derive from a minimal set of relations that
have been used in earlier OntoQuery related work.
The set can be seen in table 2
2Extraction, POS-tagging and initial ontological and re-
lation type annotation was done by Dorte Haltrup Hansen,
CST, University of Copenhagen
73
surface form blodprop (thrombosis) i (in) hjertet (the heart)
syntactic structure head of first NP preposition head of second NP
relation and ontological type disease location body part
Table 1: Example of the text excerpts analyzed in our experiments. Each row indicate a level of analysis
The manual relation annotation has been done by one
annotator for this initial project. The ideal situation
would be to have several annotators annotate the cor-
pus. If two or more people annotate the same corpus,
they are almost certain to disagree on some occasions.
This disagreement can have two sources: first it can be
due to cognitive differences. Two people subjected to
the same utterance are not guaranteed to perceive the
same content, or to perceive the content intended by
the producer of the utterance. Many factors are at play
here; cultural background, knowledge, memory, etc.
Secondly, it can be due to conceptual, lexical or syn-
tactic ambiguity in the utterance. We cannot remove
these sources of disagreement, but we can introduce
tools that make the annotation more consistent. By
using a finite and minimal realtion tag set and, further,
by introducing paraphrase tests, we hope to minimize
the risk of inter-annotator disagreement in a future an-
notation on a larger scale.
3.1.1 The ontological type annotation
As noted above, the ontological types used in the ex-
periments derive from the SIMPLE top ontology (Pe-
dersen, 1999; Lenci et al, 2000). The heads of the
phrases have been annotated with the lowest possible
node, i.e. ontological type, of the top ontology. In the
case of blodprop the annotation of ontological type is
?disease?, since ?disease? is the lowest node in the top
ontology in the path from thrombosis to the top. This is
illustrated in figure 1, which shows the path from blod-
prop (thrombosis) to the top level of SIMPLE.
Thus, for the purpose of this project, we only consi-
der one node for each concept: the lowest possible
node in the top ontology. Another approach would
be to consider the the full path to the top node, and
also including the path from the leaf node to the
lowest node in the top ontology. In the example depi-
cted in figure 1, the full path from trombosis to the
top node would be trombosis?cardiovascular disease?
disease?phenomenon?event?entity?top or trombosis?
cardiovascular disease?disease?agentive?top.
3.1.2 The set of relations
For the purpose of the manual relation annotation, we
needed to decide on a finite set of possible relations that
can be denoted by prepositions. This is a non-trivial
task, as it is almost impossible to foresee which rela-
tions prepositions can denote generally, and in the text
type at hand specifically, by introspection alone. The
method that we decided to use was the following: An
top
entity
eventagentive
phenomenon
disease
thrombosis
y
I
:
6
6
6
cardiovascular disease
6Top ontology
Domain ontology
.............................................................
Figure 1: An illustration of the path from blodprop
(thrombosis) to the top level of the SIMPLE ontology.
initial set of relations that have all been used in prior
OntoQuery-related work (Nilsson, 2001; Madsen et al,
2001; Madsen et al, 2000), were chosen as a point of
departure. The final set was found by annotating the
text segments using this set as the possible relation ty-
pes, and the relations that are actually manifested in
the data then form the final subset that was used as in-
put for a machine learning algorithm. The final subset
is shown in table 2.
Role Description
AGT Agent of act or process
BMO By means of, instrument, via
CBY Caused by
CHR Characteristic (property ascription)
CMP Comprising, has part
DST Destination of moving process
LOC Location, position
PNT Patient of act or process
SRC Source of act or process
TMP Temporal aspects
WRT With respect to
Table 2: The set of relations used in the annotation,
which is a subset of the set proposed in Nilsson, 2001.
74
3.2 Paraphrase tests
In order to ensure a consistent relation annotation, it
is necessary to develop a set of paraphrase tests that
can help the annotator determine which relation a given
preposition denotes in a given context. Some relations
are particularly difficult to intuitively keep apart from
closely related relations. One of these problematic re-
lation pairs is treated in some detail below.
For example locative and partitive relations can be diffi-
cult to keep apart, probably because they to some extent
are overlapping semantically. From a philosophical po-
int of view, an important question is ?when does an en-
tity become part of the entity it is located in??, but from
a practical point of view, we are interested in answe-
ring the question ?how can we decide if a given relation
a locative or partitive relation??.
In this paper we will only treat the latter question. A
tool that is useful for this purpose is the paraphrase test:
If we can paraphrase the text segment in question into
the phrasing the test prescribes, while preserving the
semantic content, we can conclude that the relation is a
possible relation for the given phrase.
3.2.1 Attribute Transportation Test
The two relations LOC and POF can be difficult to dif-
ferentiate, even when using paraphrase tests. There-
fore, an additional test that could be considered, is
Ruus? attribute transportation test (Ruus, 1995)3. In
the example ?The pages in the book?, the book gets
e.g. the attribute ?binding: {hardback | paperback}?
from cover, and the attribute ?paper grade:{bond | book
| bristol | newsprint}? from pages.
Figure 2: A graphical representation of the relation be-
tween book and pages
We cannot observe an attribute transport, neither from
the bird to the roof, nor the other way. This suggests
that it is possible to use the atrribute transportation test
in order to determine whether a given relation is a POF
or a LOC relation. Thus, we can now formulate the
following paraphrase test for POF:
POF: A consists e.g. of B and
A has the attribute X, from B.
3We will here ignore the question of direction of transport
4 Experiments
The annotation process generates af a feature space of
six dimensions, namely the lemmatized form of the two
heads of the noun phrases, the ontological types of the
heads, the preposition and the relation. In the corpus
there is a total of only 952 text segments. In general
the distribution of the data is highly skewed and spar-
seness is a serious problem. More than half of the in-
stances are of the relation type WRT or PNT, and the
rest of the instances are distributed among the remai-
ning 10 relations with only 14 instances scattered over
the tree smallest classes. This is illustrated in figure 3.
There are 332 different combinations of ontological ty-
pes where 197 are unique. There are 681 different he-
ads and 403 of them are unique, with all of them being
lemmatized.
Figure 3: An illustration of the distribution of the 12
possible relations.
Our assumption is that there is consistency in which
relations prepositions usually denote in particular con-
texts, and hence the learning algorithms should be able
to generalize well. We also assume that the addition
of the ontological types of the head of the NP, is the
most vital information in classifying the relation type,
at least in this case where data is sparse.
We have run the experiments with a Support Vector
Machine algorithm SMO (Keerthi et al, 2001) and
the prepositional rule learning algorithm JRip (Cohen,
1995). The former in order to get high precision, the
latter in order to get easily interpretable rules for later
analysis (see section 4.1). The experiments were run
using 10-fold-cross-validation, with a further partition
of the training set at each fold into a tuning and a trai-
ning set. The tuning set was used to optimize the pa-
rameter4 settings for each algorithm . The implemen-
tation of the algorithms that we used, was the WEKA
software package (Frank et al, 2005).
4For SMO the parameters where complexity, kernel used
and gamma for the RBF kernel. For JRip it was number of
folds used for growing and pruning, minimum number of in-
stances covered and number of optimization runs
75
The experiments were run on seven different combi-
nations of the feature space, ranging from using only
the heads to using both heads, preposition and ontolo-
gical types of the heads. This was done in order to get
insight into the importance of using ontological types
in the learning. The results of these experiments are
shown in table 3. The last column shows the precision
for a projected classifier (PC) in the cases where it out-
performs the trivial rejector. The projected classifier,
in this case, assigns the relation that is most common
for the corresponding input pair; e.g if the ontological
types are DIS/HUM, then the most common relation is
PNT. The trivial rejector, which assigns the most com-
mon relation, in this case WRT, to all the instances,
achieves a precision of 37.8%.
Feature space JRip SVM PC
1 Preposition 68.4 68.5 67.6
2 Ontological types 74.4 77.0 61.8
3 Lemma 66.8 73.3 ?
4 Lemma and Preposi-
tion
72.3 83.4 ?
5 Ontological types and
Lemma
74,7 81.7 ?
6 Ontological types and
Preposition
82.6 86.6 ?
7 Ontological types,
Preposition and
Lemma
84,0 88.3 ?
Table 3: The precision of SVM, JRip and a projected
classifier on the seven different combinations of input
features. ?Lemma? here is short for lemmatized NP
head.
The following conclusions can be drawn from table 3.
The support vector machine algorithm produces a re-
sult which in all cases is better than the baseline, i.e. we
are able to produce a model that generalizes well over
the training instances compared to the projected clas-
sifier or the trivial rejector. This difference is not sta-
tistically significant at a confidence level of 0.95 when
only training on the surface form of prepositions.
A comparison of line 1?3 shows that training on onto-
logical types seems to be superior to using lemmatized
NP heads or prepositions, though the superiority is not
statistically significant when comparing to the lemma-
tized NP heads. When comparing line 4?7 the diffe-
rence between the results are not statistically signifi-
cant. This fact may owe to the data sparseness. Howe-
ver, comparing line 1 to line 6 or 7, shows that the im-
provement of adding the preposition and the lemma-
tized NP heads to the ontological types is statistically
significant.
In general, the results reveal an unexplored opportu-
nity to include ontological types and the relations that
prepositions denote in information retrieval. In the next
section, we will look more into the rules created by the
JRip algorithm from a linguistic point of view.
4.1 Analyzing the rules
In this section we will take a deeper look into the rules
produced by JRip on the data set with only ontological
types, since they are the most interesting in this context.
The JRip algorithm produced on average 21 rules. The
most general rule covering almost half of the instan-
ces is the default rule, that assigns all instances to the
WRT relation if no other rules apply. At the other end
of the spectrum, there are ten rules covering no more
than 34 instances, but with a precision of 100%. It is
futile to analyse these rules, since they cover the most
infrequent relations and hence may be overfitting the
data set. However, this seems not be the case with a
rule like ?if the ontotype of the first head is DISEASE
and and the ontotype of the second head is HUMAN
then the relation is PATIENT? covering an instance as
e.g. ?iron deficiency in females?.
The rule with the second highest coverage, and a fairly
low precision of around 66%, is the rule: ?if the on-
totype of the second head is BODY PART then the
relation type is LOCATIVE?. The rule covers instan-
ces as e.g. ?. . . thrombosis in the heart? but also incor-
rectly classifies all instances as LOCATIVE where the
relation type should be SOURCE. E.g. the sentence
?. . . iron absorbtion from the intestine?, which is in fact
a SOURCE relation, but is classified as LOCATIVE by
the rule.
One of the least surprising and most precise rules is:
?if the ontotype of the second head is TIME then the
relation type is TEMPORAL? covering an instance as
e.g. ?. . . diet for many months?. We would expect a
similar rule to be produced, if we had performed the
learning task on a general language corpus.
5 Conclusion and future work
Even though the experiments are in an early phase, the
results indicate that it is possible to analyse the seman-
tic relation a preposition denotes between two noun
phrases, by using machine learning and an annotated
corpus ? at least within the domain covered by the on-
tology. Future work will therefore include annotation
and investigation of a general language corpus. Also, a
more thorough examination of the corpus, more specifi-
cally an investigation of which relations or prepositions
that are most difficult to analyse. Also, we will experi-
ment with the amount of information that we train on,
not as we have already done by in- or excluding types
of information, but rather the extension of the infor-
mation: Could we predict the ontological type of one
of the arguments by looking at the other? Finally, an
explicit inclusion of the whole ontology in the learning
process is on the agenda, as proposed in section 3.1.1
on page 3, in the anticipation that the learner will pro-
duce an even better model.
76
6 Acknowledgements
We would like to thank Troels Andreasen, Per Anker
Jensen and two anonymous reviewers for fruitful com-
ments. The latter especially for comments on the expe-
rimental part and inter-annotator agreement.
References
[Agirre and Martinez2001] E. Agirre and D. Martinez.
2001. Learning class-to-class selectional preferences.
[Andreasen et al2002] Troels Andreasen, Per Anker
Jensen, J?rgen Fischer Nilsson, Patrizia Paggio, Bo-
lette Sandford Pedersen, and Hanne Erdman Thomsen.
2002. Ontological extraction of content for text que-
rying. In Lecture Notes in Computer Science, volume
2553, pages 123 ? 136. Springer-Verlag.
[Andreasen et al2004] Troels Andreasen, Per Anker
Jensen, J&#248;rgen Fischer Nilsson, Patrizia Paggio,
Bolette Sandford Pedersen, and Hanne Erdman Thom-
sen. 2004. Content-based text querying with onto-
logical descriptors. Data & Knowledge Engineering,
48(2):199?219.
[Cohen1995] William W. Cohen. 1995. Fast effective
rule induction. In Armand Prieditis and Stuart Russell,
editors, Proceedings of the 12th International Confe-
rence on Machine Learning, pages 115?123, Tahoe
City, CA. Morgan Kaufmann.
[Frank et al2005] Eibe Frank, Mark Hall, and Len
Trigg. 2005. Weka. Publicly available, November.
[Gyldendal2004] Gyldendal. 2004. The danish natio-
nal encyclopedia. ISBN: 8702031051.
[Hansen2000] Dorte Haltrup Hansen. 2000. Tr?ning
og brug af brill-taggeren pa? danske tekster. Technical
report, CST.
[Ide and Ve?ronis1998] Nancy Ide and Jean Ve?ronis.
1998. Special issue on word sense disambiguation: In-
troduction to the special issue on word sense disambi-
guation: the state of the art. Computational Lingui-
stics, 24.
[Jensen and Nilsson2006] Per Anker Jensen and
J?rgen Fischer Nilsson, 2006. Syntax and Semantics of
Prepositions, volume 29 of Text, Speech and Language
Technology, chapter Ontology-Based Semantics for
Prepositions. Springer.
[Keerthi et al2001] S. Sathiya Keerthi, Shirish Krish-
naj Shevade, Chiranjib Bhattacharyya, and K. R. K.
Murthy. 2001. Improvements to platt?s smo algo-
rithm for svm classifier design. Neural Computation,
13(3):637?649.
[Lenci et al2000] Alessandro Lenci, Nuria Bel, Fede-
rica Busa, Nicoletta Calzolari1, Elisabetta Gola, Mo-
nica Monachini, Antoine Ogonowski, Ivonne Peters,
Wim Peters, Nilda Ruimy, Marta Villegas, and Anto-
nio Zampolli. 2000. Simple: A general framework for
the development of multilingual lexicons. Internatio-
nal Journal of Lexicography, 13(4):249?263.
[Madsen et al2000] Bodil Nistrup Madsen, Bo-
lette Sandford Pedersen, and Hanne Erdman Thomsen.
2000. Semantic relations in content-based querying
systems: a research presentation from the ontoquery
project. In K Simov and A Kiryakov, editors, Ontolo-
gyes and Lexical Knowledge Bases. Proceedings of the
1st International Workshop, OntoLex 2000. University
of Southern Denmark, Kolding.
[Madsen et al2001] Bodil Nistrup Madsen, Bo-
lette Sandford Pedersen, and Hanne Erdman Thomsen.
2001. Defining semantic relations for ontoquery. In
Per Anker Jensen and P Skadhauge, editors, Procee-
dings of the First International OntoQuery Workshop
Ontology-based interpretation of NP?s. University of
Southern Denmark, Kolding.
[Nilsson2001] J?rgen Fischer Nilsson. 2001. A logico-
algebraic framework for ontologies, ontolog. In Jensen
and Skadhauge, editors, Proceedings of the First In-
ternational OntoQuery Workshop Ontology-based in-
terpretation of NP?s. University of Southern Denmark,
Kolding.
[Pedersen1999] Bolette Sandford Pedersen. 1999. Den
danske simple-ordbog. en semantisk, ontologibaseret
ordbog. In C. Poulsen, editor, DALF 99, Datalingvi-
stisk Forenings a?rsm?de 1999. Center for sprogtekno-
logi.
[Ruus1995] Hanne Ruus. 1995. Danske kerneord.
Centrale dele af den danske leksikalske norm 1-2. Mu-
seum Tusculanums Forlag.
[Voorhees1993] EllenM. Voorhees. 1993. Using word-
net to disambiguate word senses for text retrieval. In
Robert Korfhage, Edie M. Rasmussen, and Peter Wil-
lett, editors, Proceedings of the 16th Annual Interna-
tional ACM-SIGIR Conference on Research and Deve-
lopment in Information Retrieval. Pittsburgh, PA, USA,
June 27 - July 1, 1993, pages 171?180. ACM.
[Yarowsky1992] David Yarowsky. 1992. Word-sense
disambiguation using statistical models of Roget?s ca-
tegories trained on large corpora. In Proceedings of
COLING-92, pages 454?460, Nantes, France, July.
77
