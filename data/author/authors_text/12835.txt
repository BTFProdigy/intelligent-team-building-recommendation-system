Proceedings of the 12th Conference of the European Chapter of the ACL, pages 94?102,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Incremental Parsing Models for Dialog Task Structure
Srinivas Bangalore and Amanda J. Stent
AT&T Labs ? Research, Inc., 180 Park Avenue,
Florham Park, NJ 07932, USA
{srini,stent}@research.att.com
Abstract
In this paper, we present an integrated
model of the two central tasks of dialog
management: interpreting user actions and
generating system actions. We model the
interpretation task as a classi?cation prob-
lem and the generation task as a predic-
tion problem. These two tasks are inter-
leaved in an incremental parsing-based di-
alog model. We compare three alterna-
tive parsing methods for this dialog model
using a corpus of human-human spoken
dialog from a catalog ordering domain
that has been annotated for dialog acts
and task/subtask information. We contrast
the amount of context provided by each
method and its impact on performance.
1 Introduction
Corpora of spoken dialog are now widely avail-
able, and frequently come with annotations for
tasks/games, dialog acts, named entities and ele-
ments of syntactic structure. These types of infor-
mation provide rich clues for building dialog mod-
els (Grosz and Sidner, 1986). Dialog models can
be built of?ine (for dialog mining and summariza-
tion), or online (for dialog management).
A dialog manager is the component of a dia-
log system that is responsible for interpreting user
actions in the dialog context, and for generating
system actions. Needless to say, a dialog manager
operates incrementally as the dialog progresses. In
typical commercial dialog systems, the interpre-
tation and generation processes operate indepen-
dently of each other, with only a small amount of
shared context. By contrast, in this paper we de-
scribe a dialog model that (1) tightly integrates in-
terpretation and generation, (2) makes explicit the
type and amount of shared context, (3) includes
the task structure of the dialog in the context, (4)
can be trained from dialog data, and (5) runs in-
crementally, parsing the dialog as it occurs and in-
terleaving generation and interpretation.
At the core of our model is a parser that in-
crementally builds the dialog task structure as the
dialog progresses. In this paper, we experiment
with three different incremental tree-based parsing
methods. We compare these methods using a cor-
pus of human-human spoken dialogs in a catalog
ordering domain that has been annotated for dialog
acts and task/subtask information. We show that
all these methods outperform a baseline method
for recovering the dialog structure.
The rest of this paper is structured as follows:
In Section 2, we review related work. In Sec-
tion 3, we present our view of the structure of task-
oriented human-human dialogs. In Section 4, we
present the parsing approaches included in our ex-
periments. In Section 5, we describe our data and
experiments. Finally, in Section 6, we present con-
clusions and describe our current and future work.
2 Related Work
There are two threads of research that are relevant
to our work: work on parsing (written and spoken)
discourse, and work on plan-based dialog models.
Discourse Parsing Discourse parsing is the pro-
cess of building a hierarchical model of a dis-
course from its basic elements (sentences or
clauses), as one would build a parse of a sen-
tence from its words. There has now been con-
siderable work on discourse parsing using statisti-
cal bottom-up parsing (Soricut and Marcu, 2003),
hierarchical agglomerative clustering (Sporleder
and Lascarides, 2004), parsing from lexicalized
tree-adjoining grammars (Cristea, 2000), and rule-
based approaches that use rhetorical relations and
discourse cues (Forbes et al, 2003; Polanyi et al,
2004; LeThanh et al, 2004). With the exception of
Cristea (2000), most of this research has been lim-
ited to non-incremental parsing of textual mono-
logues where, in contrast to incremental dialog
parsing, predicting a system action is not relevant.
The work on discourse parsing that is most
similar to ours is that of Baldridge and Las-
carides (2005). They used a probabilistic head-
driven parsing method (described in (Collins,
2003)) to construct rhetorical structure trees for a
spoken dialog corpus. However, their parser was
94
Dialog
Task
Topic/SubtaskTopic/Subtask
Task Task
Clause
UtteranceUtteranceUtterance
Topic/Subtask
DialogAct,Pred?Args DialogAct,Pred?Args DialogAct,Pred?Args
Figure 1: A schema of a shared plan tree for a
dialog.
not incremental; it used global features such as the
number of turn changes. Also, it focused strictly
in interpretation of input utterances; it could not
predict actions by either dialog partner.
In contrast to other work on discourse parsing,
we wish to use the parsing process directly for di-
alog management (rather than for information ex-
traction or summarization). This in?uences our
approach to dialog modeling in two ways. First,
the subtask tree we build represents the functional
task structure of the dialog (rather than the rhetor-
ical structure of the dialog). Second, our dialog
parser must be entirely incremental.
Plan-Based Dialog Models Plan-based ap-
proaches to dialog modeling, like ours, operate di-
rectly on the dialog?s task structure. The process
of task-oriented dialog is treated as a special case
of AI-style plan recognition (Sidner, 1985; Litman
and Allen, 1987; Rich and Sidner, 1997; Carberry,
2001; Bohus and Rudnicky, 2003; Lochbaum,
1998). Plan-based dialog models are used for both
interpretation of user utterances and prediction of
agent actions. In addition to the hand-crafted mod-
els listed above, researchers have built stochastic
plan recognition models for interaction, includ-
ing ones based on Hidden Markov Models (Bui,
2003; Blaylock and Allen, 2006) and on proba-
bilistic context-free grammars (Alexandersson and
Reithinger, 1997; Pynadath and Wellman, 2000).
In this area, the work most closely related to
ours is that of Barrett and Weld (Barrett and Weld,
1994), who build an incremental bottom-up parser
Opening
Order Placement
Contact Info
Delivery InfoShipping Info
ClosingSummaryPayment InfoOrder Item
Figure 2: Sample output (subtask tree) from a
parse-based model for the catalog ordering do-
main.
to parse plans. Their parser, however, was not
probabilistic or targeted at dialog processing.
3 Dialog Structure
We consider a task-oriented dialog to be the re-
sult of incremental creation of a shared plan by
the participants (Lochbaum, 1998). The shared
plan is represented as a single tree T that incorpo-
rates the task/subtask structure, dialog acts, syn-
tactic structure and lexical content of the dialog,
as shown in Figure 1. A task is a sequence of sub-
tasks ST ? S. A subtask is a sequence of dialog
acts DA ? D. Each dialog act corresponds to one
clause spoken by one speaker, customer (cu) or
agent (ca) (for which we may have acoustic, lexi-
cal, syntactic and semantic representations).
Figure 2 shows the subtask tree for a sample di-
alog in our domain (catalog ordering). An order
placement task is typically composed of the se-
quence of subtasks opening, contact-information,
order-item, related-offers, summary. Subtasks can
be nested; the nesting can be as deep as ?ve lev-
els in our data. Most often the nesting is at the
leftmost or rightmost frontier of the subtask tree.
As the dialog proceeds, an utterance from a par-
ticipant is accommodated into the subtask tree in
an incremental manner, much like an incremen-
tal syntactic parser accommodates the next word
into a partial parse tree (Alexandersson and Rei-
thinger, 1997). An illustration of the incremental
evolution of dialog structure is shown in Figure 4.
However, while a syntactic parser processes in-
put from a single source, our dialog parser parses
user-system exchanges: user utterances are inter-
preted, while system utterances are generated. So
the steps taken by our dialog parser to incorpo-
rate an utterance into the subtask tree depend on
whether the utterance was produced by the agent
or the user (as shown in Figure 3).
User utterances Each user turn is split into
clauses (utterances). Each clause is supertagged
95
Interpretation of a user?s utterance:
DAC : daui = argmax
du?D
P (du|cui , ST
i?1
i?k , DA
i?1
i?k, c
i?1
i?k)
(1)
STC : stui = argmax
su?S
P (su|daui , c
u
i , ST
i?1
i?k , DA
i?1
i?k, c
i?1
i?k)
(2)
Generation of an agent?s utterance:
STP : stai = argmax
sa?S
P (sa|ST i?1i?k , DA
i?1
i?k, c
i?1
i?k)
(3)
DAP : daai = argmax
da?D
P (da|stai , ST
i?1
i?k , DA
i?1
i?k, c
i?1
i?k)
(4)
Table 1: Equations used for modeling dialog act and sub-
task labeling of agent and user utterances. cui /c
a
i = the
words, syntactic information and named entities associated
with the ith utterance of the dialog, spoken by user/agent
u/a. daui /da
a
i = the dialog act of the i
th utterance, spoken
by user/agent u/a. stui /st
a
i = the subtask label of the i
th ut-
terance, spoken by user/agent u/a. DAi?1i?k represents the
dialog act tags for utterances i? 1 to i? k.
and labeled with named entities1. Interpretation of
the clause (cui ) involves assigning a dialog act la-
bel (daui ) and a subtask label (st
u
i ). We use ST
i?1
i?k ,
DAi?1i?k, and c
i?1
i?k to represent the sequence of pre-
ceeding k subtask labels, dialog act labels and
clauses respectively. The dialog act label daui is
determined from information about the clause and
(a kth order approximation of) the subtask tree so
far (Ti?1 = (ST i?1i?k , DAi?1i?k, ci?1i?k)), as shown in
Equation 1 (Table 1). The subtask label stui is de-
termined from information about the clause, its di-
alog act and the subtask tree so far, as shown in
Equation 2. Then, the clause is incorporated into
the subtask tree.
Agent utterances In contrast, a dialog sys-
tem starts planning an agent utterance by iden-
tifying the subtask to contribute to next, stai ,
based on the subtask tree so far (Ti?1 =
(ST i?1i?k , DAi?1i?k, ci?1i?k)), as shown in Equation 3
(Table 1) . Then, it chooses the dialog act of the
utterance, daai , based on the subtask tree so far and
the chosen subtask for the utterance, as shown in
Equation 4. Finally, it generates an utterance, cai ,
to realize its communicative intent (represented
as a subtask and dialog act pair, with associated
named entities)2.
Note that the current clause cui is used in the
1This results in a syntactic parse of the clause and could
be done incrementally as well.
2We do not address utterance realization in this paper.
Figure 3: Dialog management process
conditioning context of the interpretation model
(for user utterances), but the corresponding clause
for the agent utterance cai is to be predicted and
hence is not part of conditioning context in the
generation model.
4 Dialog Parsing
A dialog parser can produce a ?shallow? or ?deep?
tree structure. A shallow parse is one in which
utterances are grouped together into subtasks, but
the dominance relations among subtasks are not
tracked. We call this model a chunk-based dia-
log model (Bangalore et al, 2006). The chunk-
based model has limitations. For example, dom-
inance relations among subtasks are important
for dialog processes such as anaphora resolu-
tion (Grosz and Sidner, 1986). Also, the chunk-
based model is representationally inadequate for
center-embedded nestings of subtasks, which do
occur in our domain, although less frequently than
the more prevalent ?tail-recursive? structures.
We use the term parse-based dialog model to
refer to deep parsing models for dialog which
not only segment the dialog into chunks but also
predict dominance relations among chunks. For
this paper, we experimented with three alternative
methods for building parse-based models: shift-
reduce, start-complete and connection path.
Each of these operates on the subtask tree for
the dialog incrementally, from left-to-right, with
access only to the preceding dialog context, as
shown in Figure 4. They differ in the parsing ac-
tions and the data structures used by the parser;
this has implications for robustness to errors. The
instructions to reconstruct the parse are either en-
tirely encoded in the stack (in the shift-reduce
method), or entirely in the parsing actions (in the
start-complete and connection path methods). For
each of the four types of parsing action required
to build the parse tree (see Table 1), we construct
96
......
Order Item Task
Opening
Hello Request(MakeOrder) Ack
number with area codesecond
please
Ack
Contact?Info
can i have your
home telephone thank you 
please
Ack
Contact?Info
thank
youto place an order secondfor calling
XYZ catalog
this is mary
how may I
help you
yes one yes one thank you 
for calling
XYZ catalog
this is mary
how may I
help you
Ack
Order Item Task
Opening
Hello Request(MakeOrder)
yes i would like
Opening
Hello Request(MakeOrder) Ack
thank you 
for calling
Order Item Task
yes please
Shipping?Address
can i have your
home telephone 
number with area code
......XYZ catalog
Contact?Info
to place an order
yes i would like
you
thank
.........
Ack
this is mary
how may I
help you
yes one 
second
please ......
Request(MakeOrder) Ack
thank you 
for calling
XYZ catalog
this is mary
Hello
you
thank
to place an order
yes i would like
Order Item Task
Opening
how may I
you
thank
Closing.........
may we deliver this
order to your home
yes i would like
help you
yes one 
second
please
Ack
to place an order
yes i would like
you
thank
to place an order
help you
yes one 
second
please
Ack
Contact?Info
may we deliver this
order to your home
......
yes please
how may I
Shipping?Address
Request(MakeOrder) Ack
thank you 
for calling
XYZ catalog
this is mary
Hello
can i have your
home telephone 
number with area code
......
Order Item Task
Opening
Shipping?Address
Request(MakeOrder) Ack
thank you 
for calling
XYZ catalog
this is mary
Hello
can i have your
home telephone 
number with area code
......
Order Item Task
Opening
how may I
you
thankyes i would like
help you
yes one 
second
please
Ack
Contact?Info
to place an order
Figure 4: An illustration of incremental evolution of dialog structure
a feature vector containing contextual information
for the parsing action (see Section 5.1). These fea-
ture vectors and the associated parser actions are
used to train maximum entropy models (Berger et
al., 1996). These models are then used to incre-
mentally incorporate the utterances for a new di-
alog into that dialog?s subtask tree as the dialog
progresses, as shown in Figure 3.
4.1 Shift-Reduce Method
In this method, the subtask tree is recovered
through a right-branching shift-reduce parsing
process (Hall et al, 2006; Sagae and Lavie, 2006).
The parser shifts each utterance on to the stack. It
then inspects the stack and decides whether to do
one or more reduce actions that result in the cre-
ation of subtrees in the subtask tree. The parser
maintains two data structures ? a stack and a tree.
The actions of the parser change the contents of
the stack and create nodes in the dialog tree struc-
ture. The actions for the parser include unary-
reduce-X, binary-reduce-X and shift, where X is
each of the non-terminals (subtask labels) in the
tree. Shift pushes a token representing the utter-
ance onto the stack; binary-reduce-X pops two to-
kens off the stack and pushes the non-terminal X;
and unary-reduce-X pops one token off the stack
and pushes the non-terminal X. Each type of re-
duce action creates a constituent X in the dialog
tree and the tree(s) associated with the reduced el-
ements as subtree(s) of X. At the end of the dialog,
the output is a binary branching subtask tree.
Consider the example subdialog A: would you
like a free magazine? U: no. The process-
ing of this dialog using our shift-reduce dialog
parser would proceed as follows: the STP model
predicts shift for sta; the DAP model predicts
YNP(Promotions) for daa; the generator outputs
would you like a free magazine?; and the parser
shifts a token representing this utterance onto the
stack. Then, the customer says no. The DAC
model classi?es dau as No; the STC model clas-
si?es stu as shift and binary-reduce-special-offer;
and the parser shifts a token representing the ut-
terance onto the stack, before popping the top two
elements off the stack and adding the subtree for
special-order into the dialog?s subtask tree.
4.2 Start-Complete Method
In the shift-reduce method, the dialog tree is con-
structed as a side effect of the actions performed
on the stack: each reduce action on the stack in-
troduces a non-terminal in the tree. By contrast,
in the start-complete method the instructions to
build the tree are directly encoded in the parser ac-
tions. A stack is used to maintain the global parse
state. The actions the parser can take are similar
to those described in (Ratnaparkhi, 1997). The
parser must decide whether to join each new termi-
nal onto the existing left-hand edge of the tree, or
start a new subtree. The actions for the parser in-
clude start-X, n-start-X, complete-X, u-complete-
X and b-complete-X, where X is each of the non-
terminals (subtask labels) in the tree. Start-X
pushes a token representing the current utterance
onto the stack; n-start-X pushes non-terminal X
onto the stack; complete-X pushes a token repre-
senting the current utterance onto the stack, then
97
pops the top two tokens off the stack and pushes
the non-terminal X; u-complete-X pops the top to-
ken off the stack and pushes the non-terminal X;
and b-complete-X pops the top two tokens off the
stack and pushes the non-terminal X. This method
produces a dialog subtask tree directly, rather than
producing an equivalent binary-branching tree.
Consider the same subdialog as before, A:
would you like a free magazine? U: no. The
processing of this dialog using our start-complete
dialog parser would proceed as follows: the STP
model predicts start-special-offer for sta; the DAP
model predicts YNP(Promotions) for daa; the gen-
erator outputs would you like a free magazine?;
and the parser shifts a token representing this ut-
terance onto the stack. Then, the customer says
no. The DAC model classi?es dau as No; the STC
model classi?es stu as complete-special-offer; and
the parser shifts a token representing the utter-
ance onto the stack, before popping the top two
elements off the stack and adding the subtree for
special-order into the dialog?s subtask tree.
4.3 Connection Path Method
In contrast to the shift-reduce and the start-
complete methods described above, the connec-
tion path method does not use a stack to track the
global state of the parse. Instead, the parser di-
rectly predicts the connection path (path from the
root to the terminal) for each utterance. The col-
lection of connection paths for all the utterances in
a dialog de?nes the parse tree. This encoding was
previously used for incremental sentence parsing
by (Costa et al, 2001). With this method, there
are many more choices of decision for the parser
(195 decisions for our data) compared to the shift-
reduce (32) and start-complete (82) methods.
Consider the same subdialog as before, A:
would you like a free magazine? U: no. The pro-
cessing of this dialog using our connection path
dialog parser would proceed as follows. First, the
STP model predicts S-special-offer for sta; the
DAP model predicts YNP(Promotions) for daa;
the generator outputs would you like a free mag-
azine?; and the parser adds a subtree rooted at
special-offer, with one terminal for the current ut-
terance, into the top of the subtask tree. Then,
the customer says no. The DAC model classi-
?es dau as No and the STC model classi?es stu
as S-special-offer. Since the right frontier of the
subtask tree has a subtree matching this path, the
Type Task/subtask labels
Call-level call-forward, closing, misc-other, open-
ing, out-of-domain, sub-call
Task-level check-availability, contact-info,
delivery-info, discount, order-change,
order-item, order-problem, payment-
info, related-offer, shipping-address,
special-offer, summary
Table 2: Task/subtask labels in CHILD
Type Subtype
Ask Info
Explain Catalog, CC Related, Discount, Order Info
Order Problem, Payment Rel, Product Info
Promotions, Related Offer, Shipping
Convers- Ack, Goodbye, Hello, Help, Hold,
-ational YoureWelcome, Thanks, Yes, No, Ack,
Repeat, Not(Information)
Request Code, Order Problem, Address, Catalog,
CC Related, Change Order, Conf, Credit,
Customer Info, Info, Make Order, Name,
Order Info, Order Status, Payment Rel,
Phone Number, Product Info, Promotions,
Shipping, Store Info
YNQ Address, Email, Info, Order Info,
Order Status,Promotions, Related Offer
Table 3: Dialog act labels in CHILD
parser simply incorporates the current utterance as
a terminal of the special-offer subtree.
5 Data and Experiments
To evaluate our parse-based dialog model, we used
817 two-party dialogs from the CHILD corpus of
telephone-based dialogs in a catalog-purchasing
domain. Each dialog was transcribed by hand;
all numbers (telephone, credit card, etc.) were
removed for privacy reasons. The average di-
alog in this data set had 60 turns. The di-
alogs were automatically segmented into utter-
ances and automatically annotated with part-of-
speech tag and supertag information and named
entities. They were annotated by hand for dia-
log acts and tasks/subtasks. The dialog act and
task/subtask labels are given in Tables 2 and 3.
5.1 Features
In our experiments we used the following features
for each utterance: (a) the speaker ID; (b) uni-
grams, bigrams and trigrams of the words; (c) un-
igrams, bigrams and trigrams of the part of speech
tags; (d) unigrams, bigrams and trigrams of the su-
pertags; (e) binary features indicating the presence
or absence of particular types of named entity; (f)
the dialog act (determined by the parser); (g) the
task/subtask label (determined by the parser); and
(h) the parser stack at the current utterance (deter-
98
mined by the parser). Each input feature vector for
agent subtask prediction has these features for up
to three utterances of left-hand context (see Equa-
tion 3). Each input feature vector for dialog act
prediction has the same features as for agent sub-
task prediction, plus the actual or predicted sub-
task label (see Equation 4). Each input feature
vector for dialog act interpretation has features a-
h for up to three utterances of left-hand context,
plus the current utterance (see Equation 1). Each
input feature vector for user subtask classi?cation
has the same features as for user dialog act inter-
pretation, plus the actual or classi?ed dialog act
(see Equation 2).
The label for each input feature vector is the
parsing action (for subtask classi?cation and pre-
diction) or the dialog act label (for dialog act clas-
si?cation and prediction). If more than one pars-
ing action takes place on a particular utterance
(e.g. a shift and then a reduce), the feature vec-
tor is repeated twice with different stack contents.
5.2 Training Method
We randomly selected roughly 90% of the dialogs
for training, and used the remainder for testing.
We separately trained models for: user dia-
log act classi?cation (DAC, Equation 1); user
task/subtask classi?cation (STC, Equation 2);
agent task/subtask prediction (STP, Equation 3);
and agent dialog act prediction (DAP, Equation 4).
In order to estimate the conditional distributions
shown in Table 1, we use the general technique of
choosing the MaxEnt distribution that properly es-
timates the average of each feature over the train-
ing data (Berger et al, 1996). We use the machine
learning toolkit LLAMA (Haffner, 2006), which
encodes multiclass classi?cation problems using
binary MaxEnt classi?ers to increase the speed of
training and to scale the method to large data sets.
5.3 Decoding Method
The decoding process for the three parsing meth-
ods is illustrated in Figure 3 and has four stages:
STP, DAP, DAC, and STC. As already explained,
each of these steps in the decoding process is mod-
eled as either a prediction task or a classi?ca-
tion task. The decoder constructs an input feature
vector depending on the amount of context being
used. This feature vector is used to query the ap-
propriate classi?er model to obtain a vector of la-
bels with weights. The parser action labels (STP
and STC) are used to extend the subtask tree. For
example, in the shift-reduce method, shift results
in a push action on the stack, while reduce-X re-
sults in popping the top two elements off the stack
and pushing X on to the stack. The dialog act la-
bels (DAP and DAC) are used to label the leaves
of the subtask tree (the utterances).
The decoder can use n-best results from the
classi?er to enlarge the search space. In order
to manage the search space effectively, the de-
coder uses a beam pruning strategy. The decod-
ing process proceeds until the end of the dialog is
reached. In this paper, we assume that the end of
the dialog is given to the decoder3.
Given that the classi?ers are error-prone in their
assignment of labels, the parsing step of the de-
coder needs to be robust to these errors. We ex-
ploit the state of the stack in the different meth-
ods to rule out incompatible parser actions (e.g. a
reduce-X action when the stack has one element,
a shift action on an already shifted utterance). We
also use n-best results to alleviate the impact of
classi?cation errors. Finally, at the end of the di-
alog, if there are unattached constituents on the
stack, the decoder attaches them as sibling con-
stituents to produce a rooted tree structure. These
constraints contribute to robustness, but cannot be
used with the connection path method, since any
connection path (parsing action) suggested by the
classi?er can be incorporated into the incremental
parse tree. Consequently, in the connection path
method there are fewer opportunities to correct the
errors made by the classi?ers.
5.4 Evaluation Metrics
We evaluate dialog act classi?cation and predic-
tion by comparing the automatically assigned di-
alog act tags to the reference dialog act tags.
For these tasks we report accuracy. We evaluate
subtask classi?cation and prediction by compar-
ing the subtask trees output by the different pars-
ing methods to the reference subtask tree. We
use the labeled crossing bracket metric (typically
used in the syntactic parsing literature (Harrison et
al., 1991)), which computes recall, precision and
crossing brackets for the constituents (subtrees) in
a hypothesized parse tree given the reference parse
tree. We report F-measure, which is a combination
of recall and precision.
For each task, performance is reported for 1, 3,
3This is an unrealistic assumption if the decoder is to
serve as a dialog model. We expect to address this limitation
in future work.
99
5, and 10-best dynamic decoding as well as oracle
(Or) and for 0, 1 and 3 utterances of context.
5.5 Results
0 1
1
3 0 1
3
3 0 1
5
3 0 1
10
3 0 1
Or
3
0
20
40
60
80
100
Number utterances history
Nbest
F
start?complete
connection?paths
shift?reduce
Figure 5: Performance of parse-based methods for
subtask tree building
Figure 5 shows the performance of the different
methods for determining the subtask tree of the di-
alog. Wider beam widths do not lead to improved
performance for any method. One utterance of
context is best for shift-reduce and start-join; three
is best for the connection path method. The shift-
reduce method performs the best. With 1 utter-
ance of context, its 1-best f-score is 47.86, as com-
pared with 34.91 for start-complete, 25.13 for the
connection path method, and 21.32 for the chunk-
based baseline. These performance differences are
statistically signi?cant at p < .001. However, the
best performance for the shift-reduce method is
still signi?cantly worse than oracle.
All of the methods are subject to some ?stick-
iness?, a certain preference to stay within the
current subtask rather than starting a new one.
Also, all of the methods tended to perform poorly
on parsing subtasks that occur rarely (e.g. call-
forward, order-change) or that occur at many dif-
ferent locations in the dialog (e.g. out-of-domain,
order-problem, check-availability). For example,
the shift-reduce method did not make many shift
errors but did frequently b-reduce on an incor-
rect non-terminal (indicating trouble identifying
subtask boundaries). Some non-terminals most
likely to be labeled incorrectly by this method
(for both agent and user) are: call-forward, order-
change, summary, order-problem, opening and
out-of-domain.
Similarly, the start-complete method frequently
mislabeled a non-terminal in a complete action,
e.g. misc-other, check-availability, summary or
contact-info. It also quite frequently mislabeled
nonterminals in n-start actions, e.g. order-item,
contact-info or summary. Both of these errors in-
dicate trouble identifying subtask boundaries.
It is harder to analyze the output from the con-
nection path method. This method is more likely
to mislabel tree-internal nodes than those imme-
diately above the leaves. However, the same
non-terminals show up as error-prone for this
method as for the others: out-of-domain, check-
availability, order-problem and summary.
0 1
1
3 0 1
3
3 0 1
5
3 0 1
10
3 0 1
Or
3
0.0
0.2
0.4
0.6
0.8
1.0
Number utterances history
Nbest
A
cc
u
ra
cy
start?complete
connection?paths
shift?reduce
Figure 6: Performance of dialog act assignment to
user?s utterances.
Figure 6 shows accuracy for classi?cation of
user dialog acts. Wider beam widths do not
lead to sign?cantly improved performance for any
method. Zero utterances of context gives the high-
est accuracy for all methods. All methods per-
form fairly well, but no method signi?cantly out-
performs any other: with 0 utterances of context,
1-best accuracy is .681 for the connection path
method, .698 for the start-complete method and
.698 for the shift-reduce method. We note that
these results are competitive with those reported
in the literature (e.g. (Poesio and Mikheev, 1998;
Sera?n and Eugenio, 2004)), although the dialog
corpus and the label sets are different.
The most common errors in dialog act classi?-
cation occur with dialog acts that occur 40 times
or fewer in the testing data (out of 3610 testing
utterances), and with Not(Information).
Figure 7 shows accuracy for prediction of agent
dialog acts. Performance for this task is lower than
100
Speaker Utterance Shift-Reduce Start-Complete Connection Path
A This is Sally shift, Hello start-opening, Hello opening S, Hello
A How may I help you shift, binary-reduce-out-of-
domain, Hello
complete-opening,
Hello
opening S, Hello
B Yes Not(Information), shift,
binary-reduce-out-of-domain
Not(Information),
complete-opening
Not(Information), open-
ing S
B Um I would like to place
an order please
Rquest(Make-Order), shift,
binary-reduce-opening
Rquest(Make-Order),
complete-opening,
n-start-S
Rquest(Make-Order),
opening S
A May I have your tele-
phone number with the
area code
shift, Acknowledge start-contact-info, Ac-
knowledge
contact-info S,
Request(Phone-Number)
B Uh the phone number is
[number]
Explain(Phone-Number),
shift, binary-reduce-contact-
info
Explain(Phone-
Number), complete-
contact-info
Explain(Phone-Number),
contact-info S
Table 4: Dialog extract with subtask tree building actions for three parsing methods
0 1
1
3 0 1
3
3 0 1
5
3 0 1
10
3 0 1
Or
3
0.0
0.2
0.4
0.6
0.8
1.0
Number utterances history
Nbest
A
cc
u
ra
cy
start?complete
connection?paths
shift?reduce
Figure 7: Performance of dialog act prediction
used to generate agent utterances.
that for dialog act classi?cation because this is a
prediction task. Wider beam widths do not gener-
ally lead to improved performance for any method.
Three utterances of context generally gives the
best performance. The shift-reduce method per-
forms signi?cantly better than the connection path
method with a beam width of 1 (p < .01), but not
at larger beam widths; there are no other signi?-
cant performance differences between methods at
3 utterances of context. With 3 utterances of con-
text, 1-best accuracies are .286 for the connection
path method, .329 for the start-complete method
and .356 for the shift-reduce method.
The most common errors in dialog act predic-
tion occur with rare dialog acts, Not(Information),
and the prediction of Acknowledge at the start of a
turn (we did not remove grounding acts from the
data). With the shift-reduce method, some YNQ
acts are commonly mislabeled. With all methods,
dialog acts pertaining to Order-Info and Product-
Info acts are commonly mislabeled, which could
potentially indicate that these labels require a sub-
tle distinction between information pertaining to
an order and information pertaining to a product.
Table 4 shows the parsing actions performed by
each of our methods on the dialog snippet pre-
sented in Figure 4. For this example, the connec-
tion path method?s output is correct in all cases.
6 Conclusions and Future Work
In this paper, we present a parsing-based model
of task-oriented dialog that tightly integrates in-
terpretation and generation using a subtask tree
representation, can be trained from data, and runs
incrementally for use in dialog management. At
the core of this model is a parser that incremen-
tally builds the dialog task structure as it interprets
user actions and generates system actions. We ex-
periment with three different incremental parsing
methods for our dialog model. Our proposed shift-
reduce method is the best-performing so far, and
performance of this method for dialog act classi?-
cation and task/subtask modeling is good enough
to be usable. However, performance of all the
methods for dialog act prediction is too low to be
useful at the moment. In future work, we will ex-
plore improved models for this task that make use
of global information about the task (e.g. whether
each possible subtask has yet been completed;
whether required and optional task-related con-
cepts such as shipping address have been ?lled).
We will also separate grounding and task-related
behaviors in our model.
101
References
J. Alexandersson and N. Reithinger. 1997. Learning
dialogue structures from a corpus. In Proceedings
of Eurospeech.
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse. In Proceedings
of CoNLL.
S. Bangalore, G. Di Fabbrizio, and A. Stent. 2006.
Learning the structure of task-driven human-human
dialogs. In Proceedings of COLING/ACL.
A. Barrett and D. Weld. 1994. Task-decomposition via
plan parsing. In Proceedings of AAAI.
A. Berger, S.D. Pietra, and V.D. Pietra. 1996. A Max-
imum Entropy Approach to Natural Language Pro-
cessing. Computational Linguistics, 22(1):39?71.
N. Blaylock and J. F. Allen. 2006. Hierarchical instan-
tiated goal recognition. In Proceedings of the AAAI
Workshop on Modeling Others from Observations.
D. Bohus and A. Rudnicky. 2003. RavenClaw: Dialog
management using hierarchical task decomposition
and an expectation agenda. In Proceedings of Eu-
rospeech.
H.H. Bui. 2003. A general model for online probabal-
istic plan recognition. In Proceedings of IJCAI.
S. Carberry. 2001. Techniques for plan recogni-
tion. User Modeling and User-Adapted Interaction,
11(1?2):31?48.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589?638.
F. Costa, V. Lombardo, P. Frasconi, and G. Soda. 2001.
Wide coverage incremental parsing by learning at-
tachment preferences. In Proceedings of the Con-
ference of the Italian Association for Artificial Intel-
ligence (AIIA).
D. Cristea. 2000. An incremental discourse parser ar-
chitecture. In Proceedings of the 2nd International
Conference on Natural Language Processing.
K. Forbes, E. Miltsakaki, R. Prasad, A. Sarkar,
A. Joshi, and B. Webber. 2003. D-LTAG system:
Discourse parsing with a lexicalized tree-adjoining
grammar. Journal of Logic, Language and Informa-
tion, 12(3):261?279.
B.J. Grosz and C.L. Sidner. 1986. Attention, inten-
tions and the structure of discourse. Computational
Linguistics, 12(3):175?204.
P. Haffner. 2006. Scaling large margin classi?ers for
spoken language understanding. Speech Communi-
cation, 48(3?4):239?261.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discriminative
classi?ers for deterministic dependency parsing. In
Proceedings of COLING/ACL.
P. Harrison, S. Abney, D. Fleckenger, C. Gdaniec,
R. Grishman, D. Hindle, B. Ingria, M. Marcus,
B. Santorini, and T. Strzalkowski. 1991. Evaluating
syntax performance of parser/grammars of English.
In Proceedings of the Workshop on Evaluating Nat-
ural Language Processing Systems, ACL.
H. LeThanh, G. Abeysinghe, and C. Huyck. 2004.
Generating discourse structures for written texts. In
Proceedings of COLING.
D. Litman and J. Allen. 1987. A plan recognition
model for subdialogs in conversations. Cognitive
Science, 11(2):163?200.
K. Lochbaum. 1998. A collaborative planning model
of intentional structure. Computational Linguistics,
24(4):525?572.
M. Poesio and A. Mikheev. 1998. The predictive
power of game structure in dialogue act recognition:
experimental results using maximum entropy esti-
mation. In Proceedings of ICSLP.
L. Polanyi, C. Culy, M. van den Berg, G. L. Thione, and
D. Ahn. 2004. A rule based approach to discourse
parsing. In Proceedings of SIGdial.
D.V. Pynadath and M.P. Wellman. 2000. Probabilistic
state-dependent grammars for plan recognition. In
Proceedings of UAI.
A. Ratnaparkhi. 1997. A linear observed time statis-
tical parser based on maximum entropy models. In
Proceedings of EMNLP.
C. Rich and C.L. Sidner. 1997. COLLAGEN: When
agents collaborate with people. In Proceedings of
the First International Conference on Autonomous
Agents.
K. Sagae and A. Lavie. 2006. A best-?rst proba-
bilistic shift-reduce parser. In Proceedings of COL-
ING/ACL.
R. Sera?n and B. Di Eugenio. 2004. FLSA: Extending
latent semantic analysis with features for dialogue
act classi?cation. In Proceedings of ACL.
C.L. Sidner. 1985. Plan parsing for intended re-
sponse recognition in discourse. Computational In-
telligence, 1(1):1?10.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proceedings of NAACL/HLT.
C. Sporleder and A. Lascarides. 2004. Combining hi-
erarchical clustering and machine learning to pre-
dict high-level discourse structure. In Proceedings
of COLING.
102
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 389?396,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Geo-Centric Language Models for Local Business Voice Search
Amanda Stent, Ilija Zeljkovic?, Diamantino Caseiro and Jay Wilpon
AT&T Labs ? Research
180 Park Avenue Bldg. 103
Florham Park, NJ 07932, USA
stent, ilija, caseiro, jgw@research.att.com
Abstract
Voice search is increasingly popular, espe-
cially for local business directory assistance.
However, speech recognition accuracy on
business listing names is still low, leading to
user frustration. In this paper, we present a
new algorithm for geo-centric language model
generation for local business voice search for
mobile users. Our algorithm has several ad-
vantages: it provides a language model for
any user in any location; the geographic area
covered by the language model is adapted to
the local business density, giving high recog-
nition accuracy; and the language models can
be pre-compiled, giving fast recognition time.
In an experiment using spoken business list-
ing name queries from a business directory
assistance service, we achieve a 16.8% abso-
lute improvement in recognition accuracy and
a 3-fold speedup in recognition time with geo-
centric language models when compared with
a nationwide language model.
1 Introduction
Voice search is an increasingly popular application
of speech recognition to telephony. In particular,
in the last two years several companies have come
out with systems for local business voice search
(LBVS). In this type of application, the user pro-
vides a desired location (city/state) and a business
name, and the system returns one or more match-
ing business listings. The most traditional LBVS
applications are commercial 411 services, which are
implemented as a speech-only two-exchange dialog
such as the one in Figure 1. In this approach to
LBVS, the speech recognizer (ASR) uses one gram-
mar to recognize city/state, and then uses separate
grammars for recognizing listings in each local area.
This gives relatively high recognition accuracy.
Advancements in ASR and search technology
have made a more information retrieval-style LBVS
feasible. In this approach, the ASR typically uses
a large stochastic language model that permits the
user to specify location and listing name or cate-
gory together in a single utterance, and then sub-
mits recognition results to a search engine (Natara-
jan et al, 2002). This gives the user more flexibility
to ?say anything at any time?. However, in recent
evaluations of one-exchange LBVS we have found
that locations are recognized with much higher ac-
curacy than listing names1. This may mean that the
user has to repeat both location and listing several
times (while in a traditional two-exchange interac-
tion only one piece of information would have to be
repeated). In effect, system developers have traded
recognition accuracy for interaction flexibility, po-
tentially increasing user frustration.
Advances in mobile phone technology make it
possible for us to combine the advantages of two-
exchange and one-exchange LBVS. The newest
smart phones come with global positioning system
(GPS) receivers and/or with the ability to determine
location through cell tower triangulation or wi-fi. If
we know the location of a LBVS user, we can use
a geo-centric language model to achieve improved
speech recognition accuracy and speed. This ap-
proach unobtrusively exploits the benefits of two-
1The vocabulary size for listing names is larger than that for
cities and states in the USA.
389
S City and state?
U Glendale California
S What listing?
U pizza
Figure 1: Example 411-search dialog
exchange voice search applications, while maintain-
ing the flexibility of one-exchange systems.
In this paper, we present an efficient algorithm
for constructing geo-centric language models from
a business listing database and local business search
logs. Our algorithm has several advantages: it pro-
vides a language model for any user in any location;
the geographic area covered by the language model
is adapted to the local business density, giving high
recognition accuracy; and the language models can
be pre-compiled, giving fast recognition time. In
an experiment using LBVS queries, we achieve: a
16.8% absolute improvement in recognition accu-
racy and a 3-fold speedup in recognition time with
geo-centric language models when compared with a
nationwide language model (such as those used in
one-exchange LBVS); and a 4.4% absolute increase
in recognition accuracy and a 16% speedup in recog-
nition time with geo-centric language models when
compared with local area language models (such as
those used in two-exchange LBVS).
The rest of this paper is structured as follows: In
Section 2 we discuss related work on voice-driven
local search. In Section 3 we present the motivation
for and architecture of a LBVS application. In Sec-
tion 4 we present our algorithm for generating geo-
centric language models. In Section 5 we describe
an evaluation of the performance of our geo-centric
language models on business listing name queries
from a deployed voice-driven search application. In
Section 6 we conclude and present future work.
2 Related Work
LBVS is the most recent variation on automated di-
rectory assistance (Buntschuh et al, 1998). ASR
for directory assistance is difficult for several rea-
sons: the vocabulary is large and includes foreign
words; there may be multiple possible pronuncia-
tions for many words; and the frequency distribu-
tion of words in the vocabulary is unusual, with a
few words occurring very often and the rest, rarely.
These difficulties are compounded by directory size.
For example, Kamm et al (1995), in experiments
on personal name directories, showed that ASR ac-
curacy decreases from 82% for a 200 name directory
to 16.5% for a 1.5 million name directory.
One way to reduce the directory size is to cover a
smaller geographic area. For example, early LBVS
covered only one city (Seide and Kellner, 1997;
Collingham et al, 1997). Later, two-exchange, ap-
plications required the user to specify their desired
location in the first exchange. This information was
then used to select a local area grammar or language
model for recognition of the listing name (Acero et
al., 2008; Bacchiani et al, 2008; Yu et al, 2007;
Georgila et al, 2003). In our research, we have cre-
ated a novel method for constructing language mod-
els that cover a very small geographic area specific
to the user?s geo-location.
Another way to reduce the directory size is to drop
listings that are unlikely to be requested. For exam-
ple, Kamm et al (1995), in their analysis of 13,000
directory assistance calls, found that a mere 245 list-
ings covered 10% of the call volume, and 870 list-
ings covered 20%. Chang et al (2008) found that in
their data sets, 19-25% of the call volume was cov-
ered by the top 200 listings. We take a different ap-
proach: we add frequent nationwide listings to our
geo-centric language models to increase coverage.
Other work related to ASR in automated direc-
tory assistance has looked at ways in which users
refer to locations (Gupta et al, 1998) and listings
(Li et al, 2008; Scharenborg et al, 2001; Yu et al,
2007), confidence scoring for directory assistance
search results (Wang et al, 2007), and ways of han-
dling recognition errors through multimodal confir-
mation and correction (Acero et al, 2008; Chang et
al., 2008; Paek et al, 2008). We do not address these
issues here.
3 Local Business Voice Search
The current generation of smart phones contains
GPS and/or can run applications that can detect the
user?s geo-location using cell tower triangulation or
wi-fi. We hypothesize that this geo-location infor-
mation can be used in mobile LBVS to improve
recognition accuracy without sacrificing interaction
flexibility. Our analysis of a directory assistance
data set shows that in the majority of cases, users
390
Figure 2: Architecture of a voice-driven local search ap-
plication
request local listings. It is frustrating for the user of
a LBVS who cannot retrieve information for a busi-
ness right around the corner. So, a LBVS should
maximize accuracy for local listings2.
Figure 2 shows the architecture of a mobile
LBVS. It includes ASR (in a speech-only or multi-
modal interface), search, and presentation of results
(through speech, text and/or graphics). It also in-
cludes location information from GPS, cell tower tri-
angulation or wi-fi, or the user?s query history (from
previous dialogs, or previous turns in this dialog).
4 Using Location to Tailor Language
Models
There are two ways to use geo-location informa-
tion in ASR for LBVS. One way is to use the user?s
geo-location to automatically determine the nearest
city. City and state can then be used to select a lo-
cal area language model (LM) for recognizing list-
ing names. The advantages of this approach include:
human knowledge about location can be included in
the design of the local areas; and local areas can be
2Of course, a LBVS should also give the user the option of
specifying a different location, and/or should be able to recog-
nize listings users are most likely to ask for that may not exist
in their local area.
designed to produce a minimal number of local area
LMs. However, if the user is near the edge of the
pre-defined local area, the selected LM may exclude
businesses close to the user and include businesses
far away from the user. Also, some local area LMs
contain many more directory listings than others.
Another way is to construct a geo-centric LM
covering businesses in a given radius around the
user?s geo-location. This approach has the advan-
tage that listings included in the language model
will certainly be close to the user. However, on-
the-fly computation of geo-centric language models
for large numbers of users is too computationally
demanding given current database and processing
technology. It is equally impractical to pre-compile
all possible geo-centric language models, since com-
mercial GPS provides coordinates accurate to about
20 feet. Here we present an algorithm for approxi-
mating true geo-centric language modeling in a way
that is computationally feasible and user relevant.
4.1 Local Area Language Models
Telecommunications companies have long under-
stood that customers may not know the exact town
in which a desired listing is, or may be interested in
listings from several nearby towns. Considerable ef-
fort has been devoted to defining local service areas
(LSAs) for telephone directories. In the directory
service that provided the database we use, business
listings are organized into about 2000 LSAs, each
consisting either of several adjacent small towns or
of one big city. For example, the Morristown, NJ
LSA includes Morristown itself as well as 53 ad-
jacent localities and neighborhoods spanning from
Pine Brook in the north-east to Mendham in the
south-west. By contrast, the New York, NY LSA
contains only New York City, which includes sev-
eral hundred neighborhoods. The Morristown, NJ
LSA contains 50000 business listings while the New
York, NY LSA contains more than 200000 listings.
We construct one LM for each LSA, giving
roughly 2000 local area LMs for the whole of the
USA.
4.2 Geo-Centric Language Models
To construct a a geo-centric LM for a user, we need
geo-coordinates (for the center of the LM) and a
search radius (to determine the extent of the LM). It
391
Figure 3: Geo-centric areas in New York City
is computationally infeasible to either pre-compute
geo-centric LMs for each uniquely identifiable set
of geo-coordinates in the USA, or to compute them
on-the-fly for large numbers of users. Fortunately,
the number of business geo-coordinates in the USA
is much sparser than the number of possible user
geo-coordinates. There are about 17 million name-
address unique businesses in the USA; assuming 8-
digit geo-code accuracy they are located at about 8.5
million unique geo-coordinates3. So we build LMs
for business geo-coordinates rather than user geo-
coordinates, and at run-time we map a user?s geo-
coordinates to those of their closest business.
To determine the search radius, we need a work-
ing definition of ?local listing?. However, ?local?
varies depending on one?s location. In New York
City, a local listing may be one up to ten blocks
away (covering a smaller geographic area than the
LSA), while in Montana a local listing may be one
that one can drive to in 45 minutes (covering a larger
geographic area than the LSA). Compare Figures 3
and 4. ?Local? is clearly related to business den-
sity at a particular location. So we compute business
density and use this to determine the radius of our
geo-centric LMs.
We can do even better than this, however. Busi-
nesses are clustered geographically (in towns, shop-
ping malls, etc.). This means that the set of listings
local to one business is likely to be very similar to
the set of listings local to a nearby business. So we
do not need to build a separate LM for each business
listing; instead, we can pre-determine the number of
businesses we want to be different from one LM to
another. Then we can ?quantize? the business geo-
3The area of the USA with the highest business density is
New York, NY, where about 270000 businesses share about
43000 geo-coordinates.
Figure 4: Geo-centric area near Vaughn, Montana
coordinates so that those that have fewer than that
number of businesses different between their search
radii end up sharing a single LM.
Our algorithm for constructing geo-centric LMs
starts with LSAs. It proceeds in two stages: first, the
business centers for the LMs are found. Second, a
search radius is computed for each LM center; and
third, the data for the LM is extracted.
The LM center finding algorithm uses two param-
eters: r1 (radius within an LSA; should be a little
smaller than average LSA radius) and Nq (number
of businesses that should be different between two
different geo-centric LMs). For each LSA:
1. Find mean latitude and longitude for the LSA:
Compute mean and standard deviation for lati-
tude (?lb, ?lb) and longitude (?gb, ?gb) over all
businesses in the LSA.
2. Exclude national businesses which are listed in
the LSA with their out-of-LSA address and geo-
coordinates: Compute mean and standard de-
viation of latitude and longitude, (?l, ?l) and
(?g, ?g) respectively, using all geo-coordinates
(l, g) where: (l, g) is within a r1-mile radius of
(?lb, ?gb); l is within ?lb of ?lb; and g is within
?gb of ?gb.
3. Compute business density in the most business-
dense region in the LSA: find a minimum
and maximum longitude (gm, gM ) and lati-
tude (lm, lM ) for all businesses that are within
(?12?g) and (?12?l) of ?g and ?l respectively.Business density per square mile (d2) is equal
to the number of businesses in the rectangle de-
fined by the low-left (gm, lm) and upper-right
(gM , lM ) corner. Business density per mile is
d1 =
?
d2.
392
4. Compute geo-location quantization accuracy:
Choose a desired number of business listings
Nq that will fall to the same geo-coordinates
when the quantization is applied. This corre-
sponds roughly to the minimum desired num-
ber of different businesses in two adjacent
geo-centric LMs. Quantization accuracy, in
miles, ?qm, then follows from the business den-
sity d1: ?qm = Nq/d1. Quantization ac-
curacy for the longitude ?g satisfies equation
distance((?g, ?l), (?g+?g, ?l)) = ?qm. ?l sat-
isfies a similar equation.
5. Quantize geo-coordinates for each business in
the LSA: Compute quantized geo-coordinates
(lq, gq) for each business in the LSA. gq =
int(g/?g)??g; lq = int(l/?l)??l. Each unique
(lq, gq) is a LM center.
The LM radius finding algorithm also uses two
parameters: r2 (maximum search radius for an LM);
and Np (minimum number of businesses within a
geo-centric language model, should be smaller than
average number of businesses per LSA). For each
LM center:
1. Count the number of businesses at 1-mile ra-
dius increments of the LM center
2. Choose the smallest radius containing at least
Np listings (or the r2 radius if there is no
smaller radius containing at least Np listings)
3. Extract data for all listings within the radius.
Build LM from this data.
The number of geo-centric LMs can be arbitrar-
ily small, depending on the parameter values. We
believe that any number between 10K and 100K
achieves good accuracy while maintaining tractabil-
ity for LM building and selection. In the experi-
ments reported here we used r1 = 3.5, Nq = 50,
r2 = 3 and Np = 1000, giving about 15000 LMs
for the whole USA.
To summarize: we have described an algorithm
for building geo-centric language models for voice-
driven business search that: gives a local language
model for any user anywhere in the country; uses
business density determine ?local? for any location
in the country; can be pre-compiled; and can be
tuned (by modifying the parameters) to maximize
performance for a particular application
5 Experiments
In this section we report an evaluation of geo-centric
language models on spoken business listing queries
from an existing directory assistance application.
We compare the recognition accuracy and recogni-
tion speed for geo-centric LMs to those of local area
LMs, of a national LM, and of combined LMs.
5.1 Data
Our test data comes from an existing two-exchange
directory assistance application. It comprises 60,000
voice queries, each consisting of a city and state
in the first exchange, followed by a business listing
name in the second exchange.
We wanted to test using queries for which we
know there is a matching listing in the city/state pro-
vided by the caller. So we used only the 15000
queries for which there was a match in our nation-
wide business listing database4. We categorized
each query as nationwide or local by looking up
the listing name in our database. We considered any
listing name that occurred five or more times to be
nationwide; the remaining listings were considered
to be local. This method fails to distinguish between
national chains and different companies that happen
to have the same name. (However, from a recog-
nition point of view any listing name that occurs in
multiple locations across the country is in fact na-
tionwide, regardless of whether the businesses to
which it refers are separate businesses.) It is also
quite strict because we used string equality rather
than looser name matching heuristics. Example na-
tional queries include Wal-mart and Domino?s Pizza.
Example local queries include Sauz Taco (Glendale,
CA); Dempsey?s Restaurant (Adrian, MI); and Con-
cord Farmers Club (Saint Louis, MO). Some queries
contain street names, e.g. Conoco on South Divi-
sion; uh Wal-Mart on five thirty five; and Chuy?s
Mesquite Broiler off of Rosedale.
For each query in our data, we say that its local
area LM is the local area LM that comes from its
4A query matched an entry in our database if there was a
business listing in our database starting with the listing name
portion of the query, in the city/state from the location portion
of the query.
393
city and state, and that contains its listing name. Its
geo-centric LM is defined similarly.
5.2 Language Model Construction
We constructed two baseline LMs. The first is a Na-
tional LM. To take advantage of the non-uniform
distribution of queries to listings (see Section 2), we
also build a Top 2000 LM containing only informa-
tion about the top 2000 most frequently requested
listing names nationwide5 . We expected this LM to
perform poorly on its own but potentially quite well
in combination with local LMs.
For national, top 2000, local area and geo-
centric LMs, we build trigram Katz backoff lan-
guage models using AT&T?s Watson language mod-
eling toolkit (Riccardi et al, 1996). The models
are built using the listing names and categories in
our nationwide listing database. Listing names are
converted to sentences containing the listing name,
street address, neighborhood and city/state.
We predict that location-specific LMs will
achieve high accuracy on local listings but will not
be very robust to national listings. So we also exper-
iment with combination LMs: local area combined
with top 2000; geo-centric combined with top 2000;
local area combined with national; and geo-centric
combined with national. We use two combination
stategies: count merging and LM union.
5.2.1 Count Merging
The count merging approach can be viewed as an
instance of maximum a posteriori (MAP) adapta-
tion. Let hw be a n-gram ending in word w and with
a certain context h, and let cL(hw) and CT (hw)
be its counts in the geo-centric/local area corpus L
and top 2000 corpus T respectively. Then p(w|h) is
computed as:
p(w|h) = ?LcL(hw) + (1? ?L)cT (hw)?LcL(h) + (1? ?L)cT (h) (1)
where ?L is a constant that controls the contribution
of each corpus to the combined model. We applied
this combination strategy to local area/geo-centric
and top 2000 only, not to local area/geo-centric and
nationwide.
5We computed listing frequencies from query logs and used
listings from the left-hand side of the frequency distribution
curve before it flattens out; there were about 2000 of these.
5.2.2 LM Union
The LM union approach uses a union of language
models at runtime. Let W = w0w1 . . . w|W | be a
sentence, pL(W ) be the probability ofW in the geo-
centric/local area corpus L, and pT (W ) be the prob-
ability ofW in the top 2000/national corpus T . Then
p(W ) is computed as:
p(W ) = max(?LpL(W ), (1? ?L)pT (W )) (2)
?L is a constant that controls the contribution of each
corpus to the combined model. We applied this com-
bination strategy to local area/geo-centric and top
2000, and to local area/geo-centric and nationwide.
Given the small size of our test set relative to the
large number of local LMs it is unfeasible to train
?L on held-out data. Instead, we selected a value
for ?L such that the adjusted frequency of the top
business in the top 2000 corpus becomes similar to
the frequency of the top business in the local LM.
We anticipate that if we did have data for training ?L
more weight would be given to the local area/geo-
centric LM.
5.3 Experimental Method
In our experiments we use AT&T?s Watson speech
recognizer with a general-purpose acoustic model
trained on telephone speech produced by American
English speakers (Goffin et al, 2005). We ran all
tests on a research server using standard settings for
our speech recognizer for large vocabulary speech
recognition. For each LM we report recognition ac-
curacy (string accuracy and word accuracy) overall,
on nationwide listings only, on local listings only,
and on queries that contain street names only. We
also report recognition time (as a fraction of real
time speed).
5.4 Results
Results are given in Table 1. Comparing the base-
line (National LM) to our geo-centric LMs, we see
that we achieve a 16.8% absolute increase in overall
sentence accuracy with a 3-fold speedup. Most of
the improvement in sentence accuracy is due to bet-
ter performance on local queries; however, we also
achieve a 2.9% absolute increase in sentence accu-
racy on nationwide queries.
394
LM Recognition accuracy: String/Word [%] Real time speed
Overall Nationwide Local Queries with
queries queries street name
Nationwide language models
National 51.3/58.0 59.9/60.8 40.3/54.1 17.9/47.3 1.05
Top 2000 23.2/31.6 40.6/43.3 9.5/25.8 1.3/18.3 0.44
Local language models
Local area 63.7/69.7 60.8/63.2 69.5/77.2 22.4/53.4 0.42
Geo-centric 68.1/73.0 62.8/65.0 75.0/81.7 15.1/49.7 0.36
Combined language models, LM union
Local area, national 58.9/64.5 61.4/62.3 57.9/67.1 21.8/50.6 0.84
Geo-centric, national 64.7/69.1 63.6/64.5 67.2/74.5 23.2/52.1 0.78
Local area, top 2000 60.0/67.0 62.1/65.8 61.8/71.3 20.6/50.3 0.45
Geo-centric, top 2000 64.7/70.7 63.4/66.7 68.8/76.5 14.7/48.2 0.42
Combined language models, count merging
Local area, top 2000 66.7/72.2 69.2/71.5 67.8/75.7 22.5/54.0 0.50
Geo-centric, top 2000 67.7/72.6 68.3/70.5 70.4/77.7 13.2/46.9 0.44
Table 1: Results on mobile 411 data (total listings 14235; national listings 4679; local listings 2495; listings with street
addresses 1163)
Now we look at the performance of different ap-
proaches to nationwide and local language model-
ing. First we compare the two nationwide LMs. As
expected, we see that the overall sentence accuracy
for the National LM is more than twice as high as
that of the Top 2000 LM, but the recognition time is
more than twice as slow. Next we compare the two
local language modeling approaches. We see that
geo-centric LMs achieve a 4.4% absolute increase
in overall sentence accuracy compared to local area
LMs and a 5.5% increase in sentence accuracy on
local listings, while using less processing time.
Next we look at combination language models.
When we combine local and nationwide LMs us-
ing LM union, we get small increases in sentence
accuracy for nationwide queries compared to local
LMs alone. However, sentence accuracy for local
listings decreases. Also, these models use more pro-
cessing time than the local LMs. When we com-
bine local and national LMs using count merging,
we get larger increases in sentence accuracy for na-
tionwide queries over local LMs alone, and smaller
decreases for local queries, compared to using LM
union. LMs trained using count merging use more
processing time than those trained using LM union,
but still less than the National LM.
We conclude that: geo-centric language model-
ing leads to increased recognition accuracy and im-
provements in recognition time, compared to us-
ing a national language model; geo-centric language
modeling leads to increased recognition accuracy
and improvements in recognition time, compared to
using local area language models; and geo-centric
language models can be combined with a ?most fre-
quently asked-for? nationwide language model to
get increased recognition accuracy on nationwide
queries, at the cost of a small increase in recognition
time and a slight decrease in recognition accuracy
for local listings.
Further analysis of our results showed another
interesting phenomenon. While geo-centric LMs
achieve higher recognition accuracy than the Na-
tional LM and local area LMs on nationwide and
local queries, recognition accuracy on queries that
contain a street name decreases. The likely reason
is that small local LMs do not have rich street name
coverage and people often do not refer to a street ad-
dress precisely. A person might use a route number
instead of a street name; if a single road has dif-
ferent names at different points they might use the
wrong name; or they might use a variation on the
actual name. For example, the query ?Conoco on
South Divison? is correctly recognized by our na-
tional LM but not with a geo-centric LM. The clos-
est matching listing in our database for that loca-
tion is ?Conoco Convenience Store on South Boule-
vard?. We note that we did not make any attempt
to generalize over the street names in our LMs, sim-
395
ply pulling one street name for each listing from the
database. Slightly more robust handling of street
names may cause this phenomenon to disappear.
6 Conclusions and Future Work
Smart phones are able to give system developers
increasingly detailed information about their users.
This information can and should be exploited to give
improved robustness and performance in customer
services. In this paper, we explored the use of lo-
cation information (from GPS or cell tower triangu-
lation) to improve ASR accuracy in LBVS. We pre-
sented an algorithm for geo-centric language model
generation that: adapts to the local business density;
enables good local listing coverage; and requires
only a limited number of language models. We com-
pared the performance of our geo-centric language
modeling to an alternative ?local? language model-
ing approach and to a nationwide language model-
ing approach, and showed that we achieve signifi-
cant improvements in recognition accuracy (a 4.4%
absolute increase in sentence accuracy compared to
local area language modeling, and a 16.8% absolute
increase compared to the use of a national language
model) with significant speedup.
We are currently testing our geo-centric language
models in a LBVS prototype. In future work, we
will optimize the parameters in our algorithm for
geo-centric LM computation and merging. We also
plan to explore the impact of integrating language
modeling with search, and to examine the impact
of these different language modeling approaches on
performance of a trainable dialog manager that takes
n-best output from the speech recognizer.
References
A. Acero, N. Bernstein, R. Chambers, Y. C. Ju, X. Li,
J. Odell, P. Nguyen, O. Scholz, and G. Zweig. 2008.
Live search for mobile: web services by voice on the
cellphone. In Proceedings of ICASSP, pages 5256?
5259.
M. Bacchiani, F. Beaufays, J. Schalkwyk, M. Schuster,
and B. Strope. 2008. Deploying GOOG-411: Early
lessons in data, measurement, and testing. In Proceed-
ings of ICASSP, pages 5260?5263.
B. Buntschuh, C. Kamm, G. Di Fabbrizio, A. Abella,
M. Mohri, S. Narayanan, I. Zeljkovic, R. Sharp,
J. Wright, S. Marcus, J. Shaffer, R. Duncan, and
J. Wilpon. 1998. VPQ: a spoken language interface
to large scale directory information. In Proceedings of
ICSLP.
S. Chang, S. Boyce, K. Hayati, I. Alphonso, and
B. Buntschuh. 2008. Modalities and demographics
in voice search: Learnings from three case studies. In
Proceedings of ICASSP, pages 5252?5255.
R. Collingham, K. Johnson, D. Nettleton, G. Dempster,
and R. Garigliano. 1997. The Durham telephone en-
quiry system. International Journal of Speech Tech-
nology, 2(2):113?119.
K. Georgila, K. Sgarbas, A. Tsopanoglou, N. Fakotakis,
and G. Kokkinakis. 2003. A speech-based human-
computer interaction system for automating directory
assistance services. International Journal of Speech
Technology, 6:145?159.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur,
A. Ljolje, S. Parthasarathy, M. Rahim, G. Riccardi,
and M. Saraclar. 2005. The AT&T Watson speech
recognizer. In Proceedings ICASSP.
V. Gupta, S. Robillard, and C. Pelletier. 1998. Automa-
tion of locality recognition in ADAS plus. In Proceed-
ings of IVITA, pages 1?4.
C. Kamm, C. Shamieh, and S. Singhal. 1995. Speech
recognition issues for directory assistance applica-
tions. Speech Communication, 17(3?4):303?311.
X. Li, Y. C. Ju, G. Zweig, and A. Acero. 2008. Language
modeling for voice search: a machine translation ap-
proach. In Proceedings of ICASSP, pages 4913?4916.
P. Natarajan, R. Prasad, R. Schwartz, and J. Makhoul.
2002. A scalable architecture for directory assistance
automation. In Proceedings of ICASSP, pages 21?24.
T. Paek, B. Thiesson, Y. C. Ju, and B. Lee. 2008. Search
Vox: Leveraging multimodal refinement and partial
knowledge for mobile voice search. In Proceedings
of the 21st annual ACM symposium on User interface
software and technology, pages 141?150.
G. Riccardi, R. Pieraccini, and E. Bocchieri. 1996.
Stochastic automata for language modeling. Com-
puter Speech and Language, 10(4):265?293.
O. Scharenborg, J. Sturm, and L. Boves. 2001. Business
listings in automatic directory assistance. In Proceed-
ings of Eurospeech, pages 2381?2384.
F. Seide and A. Kellner. 1997. Towards an automated
directory information system. In Proceedings of Eu-
rospeech, pages 1327?1330.
Y. Y. Wang, D. Yu, Y. C. Ju, G. Zweig, and A. Acero.
2007. Confidence measures for voice search applica-
tions. In Proceedings of INTERSPEECH.
D. Yu, Y. C. Ju, Y. Y. Wang, G. Zweig, and A. Acero.
2007. Automated directory assistance system ? from
theory to practice. In Proceedings of INTERSPEECH,
pages 2709?2712.
396
MATCH: An Architecture for Multimodal Dialogue Systems
Michael Johnston, Srinivas Bangalore, Gunaranjan Vasireddy, Amanda Stent
Patrick Ehlen, Marilyn Walker, Steve Whittaker, Preetam Maloor
AT&T Labs - Research, 180 Park Ave, Florham Park, NJ 07932, USA
johnston,srini,guna,ehlen,walker,stevew,pmaloor@research.att.com
Now at SUNY Stonybrook, stent@cs.sunysb.edu
Abstract
Mobile interfaces need to allow the user
and system to adapt their choice of com-
munication modes according to user pref-
erences, the task at hand, and the physi-
cal and social environment. We describe a
multimodal application architecture which
combines finite-state multimodal language
processing, a speech-act based multimodal
dialogue manager, dynamic multimodal
output generation, and user-tailored text
planning to enable rapid prototyping of
multimodal interfaces with flexible input
and adaptive output. Our testbed appli-
cation MATCH (Multimodal Access To
City Help) provides a mobile multimodal
speech-pen interface to restaurant and sub-
way information for New York City.
1 Multimodal Mobile Information Access
In urban environments tourists and residents alike
need access to a complex and constantly changing
body of information regarding restaurants, theatre
schedules, transportation topology and timetables.
This information is most valuable if it can be de-
livered effectively while mobile, since places close
and plans change. Mobile information access devices
(PDAs, tablet PCs, next-generation phones) offer
limited screen real estate and no keyboard or mouse,
making complex graphical interfaces cumbersome.
Multimodal interfaces can address this problem by
enabling speech and pen input and output combining
speech and graphics (See (Andre?, 2002) for a detailed
overview of previous work on multimodal input and
output). Since mobile devices are used in different
physical and social environments, for different tasks,
by different users, they need to be both flexible in in-
put and adaptive in output. Users need to be able to
provide input in whichever mode or combination of
modes is most appropriate, and system output should
be dynamically tailored so that it is maximally effec-
tive given the situation and the user?s preferences.
We present our testbed multimodal application
MATCH (Multimodal Access To City Help) and the
general purpose multimodal architecture underlying
it, that: is designed for highly mobile applications;
enables flexible multimodal input; and provides flex-
ible user-tailored multimodal output.
Figure 1: MATCH running on Fujitsu PDA
Highly mobile MATCH is a working city guide
and navigation system that currently enables mobile
users to access restaurant and subway information for
New York City (NYC). MATCH runs standalone on
a Fujitsu pen computer (Figure 1), and can also run
in client-server mode across a wireless network.
Flexible multimodal input Users interact with a
graphical interface displaying restaurant listings and
a dynamic map showing locations and street infor-
mation. They are free to provide input using speech,
by drawing on the display with a stylus, or by us-
ing synchronous multimodal combinations of the two
modes. For example, a user might ask to see cheap
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 376-383.
                         Proceedings of the 40th Annual Meeting of the Association for
Italian restaurants in Chelsea by saying show cheap
italian restaurants in chelsea, by circling an area on
the map and saying show cheap italian restaurants
in this neighborhood; or, in a noisy or public envi-
ronment, by circling an area and writing cheap and
italian (Figure 2). The system will then zoom to the
appropriate map location and show the locations of
restaurants on the map. Users can ask for information
about restaurants, such as phone numbers, addresses,
and reviews. For example, a user might circle three
restaurants as in Figure 3 and say phone numbers for
these three restaurants (or write phone). Users can
also manipulate the map interface directly. For exam-
ple, a user might say show upper west side or circle
an area and write zoom.
Figure 2: Unimodal pen command
Flexible multimodal output MATCH provides
flexible, synchronized multimodal generation and
can take initiative to engage in information-seeking
subdialogues. If a user circles the three restaurants in
Figure 3 and writes phone, the system responds with
a graphical callout on the display, synchronized with
a text-to-speech (TTS) prompt of the phone number,
for each restaurant in turn (Figure 4).
Figure 3: Two area gestures
Figure 4: Phone query callouts
The system also provides subway directions. If the
user says How do I get to this place? and circles one
of the restaurants displayed on the map, the system
will ask Where do you want to go from? The user
can then respond with speech (e.g., 25th Street and
3rd Avenue), with pen by writing (e.g., 25th St & 3rd
Ave), or multimodally ( e.g, from here with a circle
gesture indicating location). The system then calcu-
lates the optimal subway route and dynamically gen-
erates a multimodal presentation of instructions. It
starts by zooming in on the first station and then grad-
ually zooms out, graphically presenting each stage of
the route along with a series of synchronized TTS
prompts. Figure 5 shows the final display of a sub-
way route heading downtown on the 6 train and trans-
ferring to the L train Brooklyn bound.
Figure 5: Multimodal subway route
User-tailored generation MATCH can also pro-
vide a user-tailored summary, comparison, or rec-
ommendation for an arbitrary set of restaurants, us-
ing a quantitative model of user preferences (Walker
et al, 2002). The system will only discuss restau-
rants that rank highly according to the user?s dining
preferences, and will only describe attributes of those
restaurants the user considers important. This per-
mits concise, targeted system responses. For exam-
ple, the user could say compare these restaurants and
circle a large set of restaurants (Figure 6). If the user
considers inexpensiveness and food quality to be the
most important attributes of a restaurant, the system
response might be:
Compare-A: Among the selected restaurants, the following
offer exceptional overall value. Uguale?s price is 33 dollars. It
has excellent food quality and good decor. Da Andrea?s price is
28 dollars. It has very good food quality and good decor. John?s
Pizzeria?s price is 20 dollars. It has very good food quality and
mediocre decor.
Figure 6: Comparing a large set of restaurants
2 Multimodal Application Architecture
The multimodal architecture supporting MATCH
consists of a series of agents which communicate
through a facilitator MCUBE (Figure 7).
Figure 7: Multimodal Architecture
MCUBE is a Java-based facilitator which enables
agents to pass messages either to single agents or
groups of agents. It serves a similar function to sys-
tems such as OAA (Martin et al, 1999), the use of
KQML for messaging in Allen et al(2000), and the
Communicator hub (Seneff et al, 1998). Agents may
reside either on the client device or elsewhere on the
network and can be implemented in multiple differ-
ent languages. MCUBE messages are encoded in
XML, providing a general mechanism for message
parsing and facilitating logging.
Multimodal User Interface Users interact with
the system through the Multimodal UI, which is
browser-based and runs in Internet Explorer. This
greatly facilitates rapid prototyping, authoring, and
reuse of the system for different applications since
anything that can appear on a webpage (dynamic
HTML, ActiveX controls, etc.) can be used in
the visual component of a multimodal user inter-
face. A TCP/IP control enables communication with
MCUBE.
MATCH uses a control that provides a dynamic
pan-able, zoomable map display. The control has ink
handling capability. This enables both pen-based in-
teraction (on the map) and normal GUI interaction
(on the rest of the page) without requiring the user to
overtly switch ?modes?. When the user draws on the
map their ink is captured and any objects potentially
selected, such as currently displayed restaurants, are
identified. The electronic ink is broken into a lat-
tice of strokes and sent to the gesture recognition
and handwriting recognition components which en-
rich this stroke lattice with possible classifications of
strokes and stroke combinations. The UI then trans-
lates this stroke lattice into an ink meaning lattice
representing all of the possible interpretations of the
user?s ink and sends it to MMFST.
In order to provide spoken input the user must tap
a click-to-speak button on the Multimodal UI. We
found that in an application such as MATCH which
provides extensive unimodal pen-based interaction, it
is preferable to use click-to-speak rather than pen-
to-speak or open-mike. With pen-to-speak, spurious
speech results received in noisy environments can
disrupt unimodal pen commands.
The Multimodal UI also provides graphical output
capabilities and performs synchronization of multi-
modal output. For example, it synchronizes the dis-
play actions and TTS prompts in the answer to the
route query mentioned in Section 1.
Speech Recognition MATCH uses AT&T?s Wat-
son speech recognition engine. A speech manager
running on the device gathers audio and communi-
cates with a recognition server running either on the
device or on the network. The recognition server pro-
vides word lattice output which is passed to MMFST.
Gesture and handwriting recognition Gesture
and handwriting recognition agents provide possible
classifications of electronic ink for the UI. Recogni-
tions are performed both on individual strokes and
combinations of strokes in the input ink lattice. The
handwriting recognizer supports a vocabulary of 285
words, including attributes of restaurants (e.g. ?chi-
nese?,?cheap?) and zones and points of interest (e.g.
?soho?,?empire?,?state?,?building?). The gesture rec-
ognizer recognizes a set of 10 basic gestures, includ-
ing lines, arrows, areas, points, and question marks.
It uses a variant of Rubine?s classic template-based
gesture recognition algorithm (Rubine, 1991) trained
on a corpus of sample gestures. In addition to classi-
fying gestures the gesture recognition agent also ex-
tracts features such as the base and head of arrows.
Combinations of this basic set of gestures and hand-
written words provide a rich visual vocabulary for
multimodal and pen-based commands.
Gestures are represented in the ink meaning lat-
tice as symbol complexes of the following form: G
FORM MEANING (NUMBER TYPE) SEM. FORM
indicates the physical form of the gesture and has val-
ues such as area, point, line, arrow. MEANING indi-
cates the meaning of that form; for example an area
can be either a loc(ation) or a sel(ection). NUMBER
and TYPE indicate the number of entities in a selec-
tion (1,2,3, many) and their type (rest(aurant), the-
atre). SEM is a place holder for the specific content
of the gesture, such as the points that make up an area
or the identifiers of objects in a selection.
When multiple selection gestures are present
an aggregation technique (Johnston and Bangalore,
2001) is employed to overcome the problems with
deictic plurals and numerals described in John-
ston (2000). Aggregation augments the ink meaning
lattice with aggregate gestures that result from com-
bining adjacent selection gestures. This allows a de-
ictic expression like these three restaurants to com-
bine with two area gestures, one which selects one
restaurant and the other two, as long as their sum is
three. For example, if the user makes two area ges-
tures, one around a single restaurant and the other
around two restaurants (Figure 3), the resulting ink
meaning lattice will be as in Figure 8. The first ges-
ture (node numbers 0-7) is either a reference to a
location (loc.) (0-3,7) or a reference to a restaurant
(sel.) (0-2,4-7). The second (nodes 7-13,16) is either
a reference to a location (7-10,16) or to a set of two
restaurants (7-9,11-13,16). The aggregation process
applies to the two adjacent selections and adds a se-
lection of three restaurants (0-2,4,14-16). If the user
says show chinese restaurants in this neighborhood
and this neighborhood, the path containing the two
locations (0-3,7-10,16) will be taken when this lat-
tice is combined with speech in MMFST. If the user
says tell me about this place and these places, then
the path with the adjacent selections is taken (0-2,4-
9,11-13,16). If the speech is tell me about these or
phone numbers for these three restaurants then the
aggregate path (0-2,4,14-16) will be chosen.
Multimodal Integrator (MMFST) MMFST re-
ceives the speech lattice (from the Speech Manager)
and the ink meaning lattice (from the UI) and builds
a multimodal meaning lattice which captures the po-
tential joint interpretations of the speech and ink in-
puts. MMFST is able to provide rapid response times
by making unimodal timeouts conditional on activity
in the other input mode. MMFST is notified when the
user has hit the click-to-speak button, when a speech
result arrives, and whether or not the user is inking on
the display. When a speech lattice arrives, if inking
is in progress MMFST waits for the ink meaning lat-
tice, otherwise it applies a short timeout (1 sec.) and
treats the speech as unimodal. When an ink meaning
lattice arrives, if the user has tapped click-to-speak
MMFST waits for the speech lattice to arrive, other-
wise it applies a short timeout (1 sec.) and treats the
ink as unimodal.
MMFST uses the finite-state approach to multi-
modal integration and understanding proposed by
Johnston and Bangalore (2000). Possibilities for
multimodal integration and understanding are cap-
tured in a three tape device in which the first tape
represents the speech stream (words), the second the
ink stream (gesture symbols) and the third their com-
bined meaning (meaning symbols). In essence, this
device takes the speech and ink meaning lattices as
inputs, consumes them using the first two tapes, and
writes out a multimodal meaning lattice using the
third tape. The three tape finite-state device is sim-
ulated using two transducers: G:W which is used to
align speech and ink and G W:M which takes a com-
posite alphabet of speech and gesture symbols as in-
put and outputs meaning. The ink meaning lattice
G and speech lattice W are composed with G:W and
the result is factored into an FSA G W which is com-
posed with G W:M to derive the meaning lattice M.
In order to capture multimodal integration using
finite-state methods, it is necessary to abstract over
specific aspects of gestural content (Johnston and
Bangalore, 2000). For example, all possible se-
quences of coordinates that could occur in an area
gesture cannot be encoded in the finite-state device.
We employ the approach proposed in (Johnston and
Bangalore, 2001) in which the ink meaning lattice is
converted to a transducer I:G, where G are gesture
symbols (including SEM) and I contains both gesture
symbols and the specific contents. I and G differ only
in cases where the gesture symbol on G is SEM, in
which case the corresponding I symbol is the specific
interpretation. After multimodal integration a pro-
jection G:M is taken from the result G W:M machine
and composed with the original I:G in order to rein-
corporate the specific contents that were left out of
the finite-state process (I:G o G:M = I:M).
The multimodal finite-state transducers used at
runtime are compiled from a declarative multimodal
context-free grammar which captures the structure
Figure 8: Ink Meaning Lattice
and interpretation of multimodal and unimodal com-
mands, approximated where necessary using stan-
dard approximation techniques (Nederhof, 1997).
This grammar captures not just multimodal integra-
tion patterns but also the parsing of speech and ges-
ture, and the assignment of meaning. In Figure 9 we
present a small simplified fragment capable of han-
dling MATCH commands such as phone numbers for
these three restaurants. A multimodal CFG differs
from a normal CFG in that the terminals are triples:
W:G:M, where W is the speech stream (words), G
the ink stream (gesture symbols) and M the meaning
stream (meaning symbols). An XML representation
for meaning is used to facilate parsing and logging
by other system components. The meaning tape sym-
bols concatenate to form coherent XML expressions.
The epsilon symbol (eps) indicates that a stream is
empty in a given terminal.
When the user says phone numbers for these
three restaurants and circles two groups of restau-
rants (Figure 3). The gesture lattice (Figure 8) is
turned into a transducer I:G with the same sym-
bol on each side except for the SEM arcs which are
split. For example, path 15-16 SEM([id1,id2,id3])
becomes [id1,id2,id3]:SEM. After G and the speech
W are integrated using G:W and G W:M. The G path
in the result is used to re-establish the connection
between SEM symbols and their specific contents
in I:G (I:G o G:M = I:M). The meaning read off
I:M is<cmd><phone><restaurant> [id1,id2,id3]
</restaurant> </phone> </cmd>. This is passed
to the multimodal dialog manager (MDM) and from
there to the Multimodal UI resulting in a display like
Figure 4 with coordinated TTS output. Since the
speech input is a lattice and there is also potential
for ambiguity in the multimodal grammar, the output
from MMFST to MDM is an N-best list of potential
multimodal interpretations.
Multimodal Dialog Manager (MDM) The MDM
is based on previous work on speech-act based mod-
els of dialog (Stent et al, 1999; Rich and Sidner,
1998). It uses a Java-based toolkit for writing dialog
managers that is similar in philosophy to TrindiKit
(Larsson et al, 1999). It includes several rule-based
S ! eps:eps:<cmd> CMD eps:eps:</cmd>
CMD ! phone:eps:<phone> numbers:eps:eps
for:eps:eps DEICTICNP
eps:eps:</phone>
DEICTICNP ! DDETPL eps:area:eps eps:selection:eps
NUM RESTPL eps:eps:<restaurant>
eps:SEM:SEM eps:eps:</restaurant>
DDETPL ! these:G:eps
RESTPL ! restaurants:restaurant:eps
NUM ! three:3:eps
Figure 9: Multimodal grammar fragment
processes that operate on a shared state. The state
includes system and user intentions and beliefs, a di-
alog history and focus space, and information about
the speaker, the domain and the available modalities.
The processes include interpretation, update, selec-
tion and generation processes.
The interpretation process takes as input an N-best
list of possible multimodal interpretations for a user
input from MMFST. It rescores them according to a
set of rules that encode the most likely next speech
act given the current dialogue context, and picks the
most likely interpretation from the result. The update
process updates the dialogue context according to the
system?s interpretation of user input. It augments the
dialogue history, focus space, models of user and sys-
tem beliefs, and model of user intentions. It also al-
ters the list of current modalities to reflect those most
recently used by the user.
The selection process determines the system?s next
move(s). In the case of a command, request or ques-
tion, it first checks that the input is fully specified
(using the domain ontology, which contains informa-
tion about required and optional roles for different
types of actions); if it is not, then the system?s next
move is to take the initiative and start an information-
gathering subdialogue. If the input is fully specified,
the system?s next move is to perform the command or
answer the question; to do this, MDM communicates
with the UI. Since MDM is aware of the current set
of preferred modalities, it can provide feedback and
responses tailored to the user?s modality preferences.
The generation process performs template-based
generation for simple responses and updates the sys-
tem?s model of the user?s intentions after generation.
The text planner is used for more complex genera-
tion, such as the generation of comparisons.
In the route query example in Section 1, MDM first
receives a route query in which only the destination
is specified How do I get to this place? In the se-
lection phase it consults the domain model and de-
termines that a source is also required for a route.
It adds a request to query the user for the source to
the system?s next moves. This move is selected and
the generation process selects a prompt and sends it
to the TTS component. The system asks Where do
you want to go from? If the user says or writes 25th
Street and 3rd Avenue then MMFST will assign this
input two possible interpretations. Either this is a re-
quest to zoom the display to the specified location or
it is an assertion of a location. Since the MDM dia-
logue state indicates that it is waiting for an answer
of the type location, MDM reranks the assertion as
the most likely interpretation. A generalized overlay
process (Alexandersson and Becker, 2001) is used to
take the content of the assertion (a location) and add
it into the partial route request. The result is deter-
mined to be complete. The UI resolves the location
to map coordinates and passes on a route request to
the SUBWAY component.
We found this traditional speech-act based dia-
logue manager worked well for our multimodal inter-
face. Critical in this was our use of a common seman-
tic representation across spoken, gestured, and multi-
modal commands. The majority of the dialogue rules
operate in a mode-independent fashion, giving users
flexibility in the mode they choose to advance the di-
alogue. On the other hand, mode sensitivity is also
important since user modality choice can be used to
determine system mode choice for confirmation and
other responses.
Subway Route Constraint Solver (SUBWAY)
This component has access to an exhaustive database
of the NYC subway system. When it receives a route
request with the desired source and destination points
from the Multimodal UI, it explores the search space
of possible routes to identify the optimal one, using a
cost function based on the number of transfers, over-
all number of stops, and the walking distance from
the station at each end. It builds a list of actions re-
quired to reach the destination and passes them to the
multimodal generator.
Multimodal Generator and Text-to-speech The
multimodal generator processes action lists from
SUBWAY and other components and assigns appro-
priate prompts for each action using a template-based
generator. The result is a ?score? of prompts and ac-
tions which is passed to the Multimodal UI. The Mul-
timodal UI plays this ?score? by coordinating changes
in the interface with the corresponding TTS prompts.
AT&T?s Natural Voices TTS engine is used to pro-
vide the spoken output. When the UI receives a mul-
timodal score, it builds a stack of graphical actions
such as zooming the display to a particular location
or putting up a graphical callout. It then sends the
prompts to be rendered by the TTS server. As each
prompt is synthesized the TTS server sends progress
notifications to the Multimodal UI, which pops the
next graphical action off the stack and executes it.
Text Planner and User Model The text plan-
ner receives instructions from MDM for execution
of ?compare?, ?summarize?, and ?recommend? com-
mands. It employs a user model based on multi-
attribute decision theory (Carenini and Moore, 2001).
For example, in order to make a comparison between
the set of restaurants shown in Figure 6, the text
planner first ranks the restaurants within the set ac-
cording to the predicted ranking of the user model.
Then, after selecting a small set of the highest ranked
restaurants, it utilizes the user model to decide which
restaurant attributes are important to mention. The
resulting text plan is converted to text and sent to TTS
(Walker et al, 2002). A user model for someone who
cares most highly about cost and secondly about food
quality and decor leads to a system response such as
that in Compare-A above. A user model for someone
whose selections are driven by food quality and food
type first, and cost only second, results in a system
response such as that shown in Compare-B.
Compare-B: Among the selected restaurants, the following of-
fer exceptional overall value. Babbo?s price is 60 dollars. It has
superb food quality. Il Mulino?s price is 65 dollars. It has superb
food quality. Uguale?s price is 33 dollars. It has excellent food.
Note that the restaurants selected for the user who
is not concerned about cost includes two rather more
expensive restaurants that are not selected by the text
planner for the cost-oriented user.
Multimodal Logger User studies, multimodal data
collection, and debugging were accomplished by in-
strumenting MATCH agents to send details of user
inputs, system processes, and system outputs to a log-
ger agent that maintains an XML log designed for
multimodal interactions. Our critical objective was
to collect data continually throughout system devel-
opment, and to be able to do so in mobile settings.
While this rendered the common practice of video-
taping user interactions impractical, we still required
high fidelity records of each multimodal interaction.
To address this problem, MATCH logs the state of
the UI and the user?s ink, along with detailed data
from other components. These components can in
turn dynamically replay the user?s speech and ink as
they were originally received, and show how the sys-
tem responded. The browser- and component-based
architecture of the Multimodal UI facilitated its reuse
in a Log Viewer that reads multimodal log files, re-
plays interactions between the user and system, and
allows analysis and annotation of the data. MATCH?s
logging system is similar in function to STAMP (Ovi-
att and Clow, 1998), but does not require multimodal
interactions to be videotaped and allows rapid re-
configuration for different annotation tasks since it
is browser-based. The ability of the system to log
data standalone is important, since it enables testing
and collection of multimodal data in realistic mobile
environments without relying on external equipment.
3 Experimental Evaluation
Our multimodal logging infrastructure enabled
MATCH to undergo continual user trials and evalu-
ation throughout development. Repeated evaluations
with small numbers of test users both in the lab and
in mobile settings (Figure 10) have guided the design
and iterative development of the system.
Figure 10: Testing MATCH in NYC
This iterative development approach highlighted
several important problems early on. For example,
while it was originally thought that users would for-
mulate queries and navigation commands primarily
by specifying the names of New York neighborhoods,
as in show italian restaurants in chelsea, early field
test studies in the city revealed that the need for
neighborhood names in the grammar was minimal
compared to the need for cross-streets and points of
interest; hence, cross-streets and a sizable list of land-
marks were added. Other early tests revealed the
need for easily accessible ?cancel? and ?undo? fea-
tures that allow users to make quick corrections. We
also discovered that speech recognition performance
was initially hindered by placement of the ?click-to-
speak? button and the recognition feedback box on
the bottom-right side of the device, leading many
users to speak ?to? this area, rather than toward the
microphone on the upper left side. This placement
also led left-handed users to block the microphone
with their arms when they spoke. Moving the but-
ton and the feedback box to the top-left of the device
resolved both of these problems.
After initial open-ended piloting trials, more struc-
tured user tests were conducted, for which we devel-
oped a set of six scenarios ordered by increasing level
of difficulty. These required the test user to solve
problems using the system. These scenarios were left
as open-ended as possible to elicit natural responses.
Sample scenario:You have plans to meet your aunt for dinner
later this evening at a Thai restaurant on the Upper West Side
near her apartment on 95th St. and Broadway. Unfortunately,
you forgot what time you?re supposed to meet her, and you can?t
reach her by phone. Use MATCH to find the restaurant and write
down the restaurant?s telephone number so you can check on the
reservation time.
Test users received a brief tutorial that was inten-
tionally vague and broad in scope so the users might
overestimate the system?s capabilities and approach
problems in new ways. Figure 11 summarizes re-
sults from our last scenario-based data collection for
a fixed version of the system. There were five sub-
jects (2 male, 3 female) none of whom had been in-
volved in system development. All of these five tests
were conducted indoors in offices.
exchanges 338 asr word accuracy 59.6%
speech only 171 51% asr sent. accuracy 36.1%
multimodal 93 28% handwritten sent. acc. 64%
pen only 66 19% task completion rate 85%
GUI actions 8 2% average time/scenario 6.25m
Figure 11: MATCH study
There were an average of 12.75 multimodal ex-
changes (pairs of user input and system response) per
scenario. The overall time per scenario varied from
1.5 to to 15 minutes. The longer completion times
resulted from poor ASR performance for some of the
users. Although ASR accuracy was low, overall task
completion was high, suggesting that the multimodal
aspects of the system helped users to complete tasks.
Unimodal pen commands were recognized more suc-
cessfully than spoken commands; however, only 19%
of commands were pen only. In ongoing work, we
are exploring strategies to increase users? adoption of
more robust pen-based and multimodal input.
MATCH has a very fast system response time.
Benchmarking a set of speech, pen, and multimodal
commands, the average response time is approxi-
mately 3 seconds (time from end of user input to sys-
tem response). We are currently completing a larger
scale scenario-based evaluation and an independent
evaluation of the functionality of the text planner.
In addition to MATCH, the same multimodal ar-
chitecture has been used for two other applications:
a multimodal interface to corporate directory infor-
mation and messaging and a medical application to
assist emergency room doctors. The medical proto-
type is the most recent and demonstrates the utility of
the architecture for rapid prototyping. System devel-
opment took under two days for two people.
4 Conclusion
The MATCH architecture enables rapid develop-
ment of mobile multimodal applications. Combin-
ing finite-state multimodal integration with a speech-
act based dialogue manager enables users to interact
flexibly using speech, pen, or synchronized combina-
tions of the two depending on their preferences, task,
and physical and social environment. The system
responds by generating coordinated multimodal pre-
sentations adapted to the multimodal dialog context
and user preferences. Features of the system such
as the browser-based UI and general purpose finite-
state architecture for multimodal integration facili-
tate rapid prototyping and reuse of the technology for
different applications. The lattice-based finite-state
approach to multimodal understanding enables both
multimodal integration and dialogue context to com-
pensate for recognition errors. The multimodal log-
ging infrastructure has enabled an iterative process
of pro-active evaluation and data collection through-
out system development. Since we can replay multi-
modal interactions without video we have been able
to log and annotate subjects both in the lab and in
NYC throughout the development process and use
their input to drive system development.
Acknowledgements
Thanks to AT&T Labs and DARPA (contract MDA972-99-3-
0003) for financial support. We would also like to thank Noemie
Elhadad, Candace Kamm, Elliot Pinson, Mazin Rahim, Owen
Rambow, and Nika Smith.
References
J. Alexandersson and T. Becker. 2001. Overlay as the ba-
sic operation for discourse processing in a multimodal
dialogue system. In 2nd IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems.
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2000. An architecture for
a generic dialogue shell. JNLE, 6(3).
E. Andre?. 2002. Natural language in multime-
dia/multimodal systems. In Ruslan Mitkov, editor,
Handbook of Computational Linguistics. OUP.
G. Carenini and J. D. Moore. 2001. An empirical study of
the influence of user tailoring on evaluative argument
effectiveness. In IJCAI, pages 1307?1314.
M. Johnston and S. Bangalore. 2000. Finite-state mul-
timodal parsing and understanding. In Proceedings of
COLING 2000, Saarbru?cken, Germany.
M. Johnston and S. Bangalore. 2001. Finite-state meth-
ods for multimodal parsing and integration. In ESSLLI
Workshop on Finite-state Methods, Helsinki, Finland.
M. Johnston. 2000. Deixis and conjunction in mul-
timodal systems. In Proceedings of COLING 2000,
Saarbru?cken, Germany.
S. Larsson, P. Bohlin, J. Bos, and D. Traum. 1999.
TrindiKit manual. Technical report, TRINDI Deliver-
able D2.2.
D. Martin, A. Cheyer, and D. Moran. 1999. The Open
Agent Architecture: A framework for building dis-
tributed software systems. Applied Artificial Intelli-
gence, 13(1?2):91?128.
M-J. Nederhof. 1997. Regular approximations of CFLs:
A grammatical view. In Proceedings of the Interna-
tional Workshop on Parsing Technology, Boston.
S. L. Oviatt and J. Clow. 1998. An automated tool for
analysis of multimodal system performance. In Pro-
ceedings of ICSLP.
C. Rich and C. Sidner. 1998. COLLAGEN: A collabora-
tion manager for software interface agents. User Mod-
eling and User-Adapted Interaction, 8(3?4):315?350.
D. Rubine. 1991. Specifying gestures by example. Com-
puter graphics, 25(4):329?337.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. Galaxy-II: A reference architecture for
conversational system development. In ICSLP-98.
A. Stent, J. Dowding, J. Gawron, E. Bratt, and R. Moore.
1999. The CommandTalk spoken dialogue system. In
Proceedings of ACL?99.
M. A. Walker, S. J. Whittaker, P. Maloor, J. D. Moore,
M. Johnston, and G. Vasireddy. 2002. Speech-Plans:
Generating evaluative responses in spoken dialogue. In
In Proceedings of INLG-02.
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 290?297,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Evaluating automatic extraction of rules for sentence plan construction
Amanda Stent
AT&T Labs ? Research
Florham Park, NJ, USA
stent@research.att.com
Martin Molina
Department of Artificial Intelligence
Universidad Polite?cnica de Madrid, Spain
martin.molina@upm.es
Abstract
The freely available SPaRKy sentence
planner uses hand-written weighted rules
for sentence plan construction, and a user-
or domain-specific second-stage ranker for
sentence plan selection. However, coming
up with sentence plan construction rules
for a new domain can be difficult. In this
paper, we automatically extract sentence
plan construction rules from the RST-DT
corpus. In our rules, we use only domain-
independent features that are available to a
sentence planner at runtime. We evaluate
these rules, and outline ways in which they
can be used for sentence planning. We
have integrated them into a revised version
of SPaRKy.
1 Introduction
Most natural language generation (NLG) systems
have a pipeline architecture consisting of four core
stages: content selection, discourse planning, sen-
tence planning, and surface realization (Reiter and
Dale, 2000; Rambow et al, 2001). A sentence
planner maps from an input discourse plan to an
output sentence plan. As part of this process it
performs several tasks, including sentence order-
ing, sentence aggregation, discourse cue insertion
and perhaps referring expression generation (Stent
et al, 2004; Walker et al, 2007; Williams and Re-
iter, 2003).
The developer of a sentence planner must typ-
ically write rules by hand (e.g. (Stent et al,
2004; Walker et al, 2007)) or learn a domain-
specific model from a corpus of training data (e.g.
(Williams and Reiter, 2003)). Unfortunately, there
are very few corpora annotated with discourse
plans, and it is hard to automatically label a cor-
pus for discourse structure. It is also hard to
hand-write sentence planning rules starting from
a ?blank slate?, as it were.
In this paper, we outline a method for ex-
tracting sentence plan construction rules from the
only publicly available corpus of discourse trees,
the RST Discourse Treebank (RST-DT) (Carl-
son et al, 2002). These rules use only domain-
independent information available to a sentence
planner at run-time. They have been integrated
into the freely-available SPaRKy sentence plan-
ner. They serve as a starting point for a user of
SPaRKy, who can add, remove or modify rules to
fit a particular domain.
We also describe a set of experiments in which
we look at each sentence plan construction task in
order, evaluating our rules for that task in terms
of coverage and discriminative power. We discuss
the implications of these experiments for sentence
planning.
The rest of this paper is structured as follows: In
Section 2 we describe the sentence planning pro-
cess using SPaRKy as an example. In Sections 3
through 5 we describe how we obtain sentence
plan construction rules. In Section 6, we evalu-
ate alternative rule sets. In Section 7, we describe
our modifications to the SPaRKy sentence planner
to use these rules. In Section 8 we conclude and
present future work.
2 Sentence Planning in SPaRKy
The only publicly available sentence planner for
data-to-text generation is SPaRKy (Stent et al,
2004). SPaRKy takes as input a discourse plan (a
tree with rhetorical relations on the internal nodes
and a proposition representing a text span on each
leaf), and outputs one or more sentence plans
290
(each a tree with discourse cues and/or punctua-
tion on the internal nodes). SPaRKy is a two-stage
sentence planner. First, possible sentence plans
are constructed through a sequence of decisions
made using only local information about single
nodes in the discourse plan. Second, the possible
sentence plans are ranked using a user- or domain-
specific sentence plan ranker that evaluates the
global quality of each sentence plan (Walker et al,
2007).
Sentence plan construction in SPaRKy involves
three tasks: span ordering, sentence aggregation
(deciding whether to realize a pair of propositions
as a single clause, a single sentence, or two sen-
tences), and discourse cue selection1. SPaRKy
uses a single set of hand-written weighted rules
to perform these tasks. In the current distributed
version of SPaRKy, there are 20 rules covering
9 discourse cues (and, because, but, however, on
the other hand, since, while, with, and the default,
period). Each rule operates on the children of
one rhetorical relation, and may impose an order-
ing, insert punctuation or merge two propositions,
and/or insert a discourse cue. During sentence
plan construction, SPaRKy walks over the input
discourse plan, at each node finding all matching
rules and applying one which it selects probabilis-
tically according to the rule weights (with some
randomness to permit variation).
While the developer of a NLG system will al-
ways have to adapt the sentence planner to his or
her domain, it is often hard to come up with sen-
tence planning rules ?from scratch?. As a result of
the work described here a SPaRKy user will have
a solid foundation for sentence plan construction.
3 Data
We use the Wall Street Journal Penn Treebank
corpus (Marcus et al, 1993), which is a corpus
of text annotated for syntactic structure. We also
use two additional annotations done on (parts of)
that corpus: PropBank (Kingsbury and Palmer,
2003), which consists of annotations for predicate-
argument structure; and the RST-DT (Carlson
et al, 2002), which consists of annotations for
rhetorical structure.
We had to process this data into a form suitable
for feature extraction. First, we produced a flat-
tened form of the syntactic annotations, in which
1SPaRKy also does some referring expression generation,
in a single pass over each completed sentence plan.
each word was labeled with its part-of-speech tag
and the path to the root of the parse tree. Each
word was also assigned indices in the sentence (so
we could apply the PropBank annotations) and in
the document (so we could apply the RST-DT an-
notations)2.
Second, we attach to each word one or more
labels from the PropBank annotations (each label
consists of a predicate index, and either a predicate
name or a semantic role type and index).
Third, we extract relation information from the
RST-DT. For each relation, we extract the rela-
tion name, the types of each child (?Nucleus? or
?Satellite?), and the start and end word indices for
each child. Finally, we extract from the word-
level annotations the marked-up words for each
text span in each rhetorical relation.
4 Features
Features are individual rule conditions. In the
standard NLG pipeline, no information about the
realized text is available to the sentence planner.
However, existing sentence planners use lexical
and word sequence information to improve per-
formance for a particular domain. Williams and
Reiter (2003) appear to do surface realization be-
fore sentence planning, while Walker et al (2007)
perform surface realization between sentence plan
construction and sentence plan ranking. We are
concerned with sentence plan construction only;
also, we want to produce sentence plan construc-
tion rules that are as domain-independent as pos-
sible. So we use no features that rely on having
realized text. However, we assume that the input
propositions have been fairly well fleshed-out, so
that one has information about predicate-argument
structure, tense, and the information status of enti-
ties to be realized.
A relation has a label as well as one or more
child text spans. The features we extract from our
data include both per-span and per-relation fea-
tures. In our experiments we use a subset of these
features which is fairly domain-independent and
does not overly partition our data. The complete
set of features (full) is as well as our reduced set
are given in Table 1.
2The Penn Treebank and the RST-DT segment words and
punctuation slightly differently, which makes it hard to align
the various annotations.
291
Feature type Full feature set Reduced feature set
Per-relation relation, relation is leaf, parent relation, span coref, com-
bined verb type class, combined verb type, identifier of
shortest span, temporal order of spans
relation, relation is leaf, parent rela-
tion, span coref, combined verb type
class, identifier of shortest span, tem-
poral order of spans
Per-span, span identifier span identifier span identifier
Per-span, span length number of NPs in span
Per-span, span verb verb type class, verb type, verb part of speech, verb is
negated, verb has modal
Per-span, arguments argument status for ARG0 to ARG5 plus ARGM-{EXT,
DIR, LOC, TMP, REC, PRD, ADV, MNR, CAU, PNC}
Table 1: Features used in evaluation
4.1 Per-Span Features
We extract per-span features from basic spans
(leaves of the RST tree) and from complex spans
(internal nodes of the RST tree). For each span we
compute: identifier, text, length, verb information,
span argument information, discourse cue infor-
mation, and span-final punctuation.
Identifier We need a way to refer to the child spans
in the rules. For relations having only one child
span of each type (Satellite or Nucleus), we order
the spans by type. Otherwise, we order the spans
alphabetically by span text. The span identifier for
each child span is the index of the span in the re-
sulting list.
TextWe extract the text of the span, and the indices
of its first and last words in the Penn Treebank. We
only use this information during data extraction.
However, in a system like that of Williams and Re-
iter (Williams and Reiter, 2003), where sentence
planning is done after or with surface realization,
these features could be used. They could also be
used to train a sentence plan ranker for SPaRKy
specific to the news domain.
Length We use the number of base NPs in the span
(as we cannot rely on having the complete realiza-
tion during sentence planning).
VerbWe extract verb type, which can beN/A (there
is no labeled predicate for the span), stat (the
span?s main verb is a form of ?to be?), a single
PropBank predicate (e.g. create.01), or mixed (the
span contains more than one predicate). We then
abstract to get the verb type class: N/A, pb (a Prop-
Bank predicate), stat, or mixed.
If the span contains a single predicate or multi-
ple predicates all having the same part-of-speech
tag, we extract that (as an indicator of tense).
We also extract information about negation and
modals (using the PropBank tags ARGM-NEG
and ARGM-MOD).
Arguments We extract the text of the arguments
of the predicate(s) in the span: ARG0 to ARG5,
as well as ARGM-{EXT, DIR, LOC, TMP, REC,
PRD, ADV, MNR, CAU, PNC}. We then abstract
to get an approximation of information status.
An argument status feature covers zero or more
instantiations of the argument and can have the
value N/A (no instantiations), proper (proper noun
phrase(s)), pro (pronoun(s)), def (definite noun
phrase(s)), indef (indefinite noun phrase(s)), quant
(noun phrase (s) containing quantifiers), other (we
cannot determine a value), or mixed (the argument
instantiations are not all of the same type).
Discourse Cues We extract discourse cue informa-
tion from basic spans and from the first basic span
in complex spans. We identify discourse cue(s)
appearing at the start of the span, inside the span,
and at the end of the span. PropBank includes
the argument label ARGM-DIS for discourse cues;
however, we adopt a more expansive notion of dis-
course cue. We say that a discourse cue can be ei-
ther: any sequence of words all labeled ARGM-
DIS and belonging to the same predicate, any-
where in the span; or any cue from a (slightly
expanded version of) the set of cues studied by
Marcu (Marcu, 1997), if it appears at the start of a
span, at the end of a span, or immediately before or
after a comma, and if its lowest containing phrase
tag is one of {ADJP, ADVP, CONJP, FRAG, NP-
ADV, PP, UCP, SBAR, WH} or its part of speech
tag is one of {CC, WDT}3.
Punctuation We extract punctuation (N/A or . or ?
or ! or ; or : or ,) at the end of the span.
3We constructed these rules by extracting from the WSJ
Penn Treebank all instances of the cues in Marcu?s list, and
then examining instances where the word sequence was not
actually a discourse cue. Some mistakes still occur in cue
extraction.
292
4.2 Per-Relation Features
For each relation we compute: name, the com-
bined verb type and verb class of the child spans,
whether any argument instantiations in the child
spans are coreferential, and which child span is
shortest (or the temporal order of the child spans).
Relation, Parent Relation The core relation label
for the relation and its parent relation (e.g. attri-
bution for attribution-e and attribution-n).
Relation is Leaf True if child spans of the relation
are leaf spans (not themselves relations).
Combined Verb The shared verb for the relation:
the child spans? verb type if there is only one non-
N/A verb type among the child spans; otherwise,
mixed. We then abstract from the shared verb type
to the shared verb type class.
Span Coreference We use the information Prop-
Bank gives about intra-sentential coreference. We
do not employ any algorithm or annotation to iden-
tify inter-sentential coreference.
Shortest Span The identifier of the child span with
the fewest base NPs.
Temporal Order of Spans For some relations (e.g.
sequence, temporal-before, temporal-after), the
temporal order is very important. For these rela-
tions we note the temporal order of the child spans
rather than the shortest span.
5 Rule Extraction
Each rule we extract consists of a set of per-
relation and per-span features (the conditions), and
a pattern (the effects). The conditions contain
either: the relation only, features from the re-
duced feature set, or features from the full fea-
ture set. The pattern can be an ordering of child
spans, a set of between-span punctuation markers,
a set of discourse cues, or an ordering of child
spans mixed with punctuation markers and dis-
course cues. Each extracted rule is stored as XML.
We only extract rules for relations having two or
more children. We also exclude RST-DT?s span
and same-unit relations because they are not im-
portant for our task. Finally, because the accu-
racy of low-level (just above the span) rhetorical
relation annotation is greater than that of high-
level relation annotation, we extract rules from
two data sets: one only containing first-level re-
lations (those whose children are all basic spans),
and one containing all relations regardless of level
in the RST tree. The output from the rule ex-
traction process is six alternative rule sets for each
Concession rule:
conditions:
type child=?0?: nucleus, type child=?1?: satellite, shortest: 0,
isCoref: 0, isLeaf: 1, isSamePredClass: mixed,
numChildren: 2, relation: concession, parentRel: antithesis
effects:
order: 1 0, punc child=?1?: comma, cues child=?1?: while
example:
(1) While some automotive programs have been delayed,
(0) they have n?t been canceled
Sequence rule:
conditions:
type child=?0?: nucleus, type child=?1?: nucleus,
type child=?2?: nucleus, type child=?3?: nucleus,
isCoref: 1, isLeaf: 1, isSamePredClass: mixed,
numChildren: 4, relation: sequence, parentRel: circumstance,
temporalOrder: 0 1 2 3
effects:
order: 0 1 2 3, punc child=?0?: comma, punc child=?1?:
comma, punc child=?2?: n/a, cues child=?3?: and
example:
(0) when you can get pension fund money, (1) buy a portfolio,
(2) sell off pieces off it (3) and play your own game
Purpose rule:
conditions:
type child=?0?: nucleus, type child=?1?: satellite, shortest: 0,
isCoref: 0, isLeaf: 0, isSamePredClass: shared,
numChildren: 2, relation: purpose, parentRel: list
effects:
order: 0 1, punc child=?0?: n/a, cues child=?1?: so
example:
(0) In a modern system the government ?s role is to give the
people as much choice as possible
(1) so they are capable of making a choice
Figure 1: Glosses of extracted sentence planning
rules for three relations (reduced feature set)
sentence plan construction task: first-level or all
data, with either the relation condition alone, the
reduced feature set, or the full feature set.
The maximum number of patterns we could
have is 7680 per relation, if we limit ourselves
to condition sets, relation instances with only two
child spans, and a maximum of one discourse
cue to each span (two possible orderings for child
spans * four possible choices for punctuation *
480 choices for discourse cue on each span). By
contrast, for our all data set there are 5810 unique
rules conditioned on the reduced feature set (109.6
per relation) and 292 conditioned on just the rela-
tion (5.5 per relation). Example rules are given in
Figure 1. Even though the data constrains sentence
planning choices considerably, we still have many
rules (most differing only in discourse cues).
293
6 Rule Evaluation
6.1 On Evaluation
There are two basic approaches to NLG, text-to-
text generation (in which a model learned from a
text corpus is applied to produce new texts from
text input) and data-to-text generation (in which
non-text input is converted into text output). In
text-to-text generation, there has been consider-
able work on sentence fusion and information or-
dering, which are partly sentence planning tasks.
For evaluation, researchers typically compare au-
tomatically produced text to the original human-
produced text, which is assumed to be ?correct?
(e.g. (Karamanis, 2007; Barzilay and McKeown,
2005; Marsi and Krahmer, 2005)). However, an
evaluation that considers the only ?correct? an-
swer for a sentence planning task to be the an-
swer in the original text is overly harsh. First, al-
though we assume that all the possibilities in the
human-produced text are ?reasonable?, some may
be awkward or incorrect for particular domains,
while other less frequent ones in the newspaper
domain may be more ?correct? in another domain.
Our purpose is to lay out sentence plan construc-
tion possibilities, not to reproduce the WSJ au-
thorial voice. Second, because SPaRKy is a two-
stage sentence planner and we are focusing here
on sentence plan construction, we can only evalu-
ate the local decisions made during that stage, not
the overall quality of SPaRKy?s output.
Evaluations of sentence planning tasks for data-
to-text generation have tended to focus solely
on discourse cues (e.g. (Eugenio et al, 1997;
Grote and Stede, 1998; Moser and Moore, 1995;
Nakatsu, 2008; Taboada, 2006)). By contrast, we
want good coverage for all core sentence planning
tasks. Although Walker et al performed an eval-
uation of SPaRKy (Stent et al, 2004; Walker et
al., 2007), they evaluated the output from the sen-
tence planner as a whole, rather than evaluating
each stage separately. Williams and Reiter, in the
work most similar to ours, examined a subset of
the RST-DT corpus to see if they could use it to
perform span ordering, punctuation selection, and
discourse cue selection and placement. However,
they assumed that surface realization was already
complete, so they used lexical features. Their sen-
tence planner is not publicly available.
In the following sections, we evaluate the infor-
mation in our sentence plan construction rules in
terms of coverage and discriminative power. The
first type of evaluation allows us to assess the de-
gree to which our rules are general and provide
system developers with an adequate number of
choices for sentence planning. The second type
of evaluation allows us to evaluate whether our re-
duced feature set helps us choose from the avail-
able possibilities better than a feature set consist-
ing simply of the relation (i.e. is the complicated
feature extraction necessary). Because we include
the full feature set in this evaluation, it can also
be seen as a text-to-text generation type of evalua-
tion for readers who would like to use the sentence
planning rules for news-style text generation.
6.2 Coverage
In our evaluation of coverage, we count the num-
ber of relations, discourse cues, and patterns we
have obtained, and compare against other data sets
described in the research literature.
6.2.1 Relation Coverage
There are 57 unique core relation labels in
the RST-DT. We exclude span and same-unit.
Two others, elaboration-process-step and topic-
comment, never occur with two or more child
spans. Our first-level and all rules cover all of
the remaining 53. The most frequently occurring
relations are elaboration-additional, list, attribu-
tion, elaboration-object-attribute, contrast, cir-
cumstance and explanation-argumentative.
By contrast, the current version of SPaRKy cov-
ers only 4 relations (justify, contrast, sequence,
and infer)4.
Mann and Thompson originally defined 24 re-
lations (Mann and Thompson, 1987), while Hovy
andMaier listed about 70 (Hovy andMaier, 1992).
6.2.2 Discourse Cue Coverage
Our first-level rules cover 92 discourse cues,
and our all rules cover 205 discourse cues. The
most commonly occurring discourse cues in both
cases are and, but, that, when, as, who and which.
By contrast, the current version of SPaRKy cov-
ers only about 9 discourse cues.
In his dissertation Marcu identified about 478
discourse cues. We used a modified version of
Marcu?s cue list to extract discourse cues from our
corpus, but some of Marcu?s discourse cues do not
occur in the RST-DT.
4Curiously, only two of these relations (contrast and se-
quence) appear in the RST-DT data (although infer may be
equivalent to span).
294
6.2.3 Sentence Plan Pattern Coverage
For the first-level data we have 140 unique sen-
tence plan patterns using the relation condition
alone, and 1767 conditioning on the reduced fea-
ture set. For the all data we have 292 unique pat-
terns with relation condition alone and 5810 with
the reduced feature set. Most patterns differ only
in choice of discourse cue(s).
No system developer will want to examine all
5810 rules. However, she or he may wish to look
at the patterns for a particular relation. In our use
of SPaRKy, for example, we have extended the
patterns for the sequence relation by hand to cover
temporal sequences of up to seven steps.
6.3 Discriminative Power
In this evaluation, we train decision tree classifiers
for each sentence plan construction task. We ex-
periment with both the first-level and all data sets
and with both the reduced and full feature sets.
For each experiment we perform ten-fold cross-
validation using the J48 decision tree implemen-
tation provided in Weka (Witten and Eibe, 2005)
with its default parameters. We also report perfor-
mance for a model that selects a pattern condition-
ing only on the relation. Finally, we report perfor-
mance of a baseline which always selects the most
frequent pattern.
We evaluate using 1-best classification accu-
racy, by comparing with the choice made in the
Penn Treebank for that task. We test for signifi-
cant differences between methods using Cochran?s
Q, followed by post-hoc McNemar tests if signif-
icant differences existed. We also report the fea-
tures with information gain greater than 0.1.
6.3.1 Span Ordering
We have one input feature vector for each rela-
tion instance that has two children5. In the feature
vector, child spans are ordered by their identifiers,
and the pattern is either 0 1 (first child, then sec-
ond child) or 1 0 (second child, then first child).
Classification accuracy for all methods is re-
ported in Table 2. All methods perform signifi-
cantly better than baseline (p < .001), and both
the reduced and full feature sets give results sig-
nificantly better than using the relation alone (p <
.001). The full feature set performs significantly
5The number of relation instances with three or more child
spans is less than 2% of the data. Removing these relations
made it feasible for us to train classifiers without crashing
Weka.
First-level All
Baseline 71.8144 71.4356
Per-relation 84.2707 82.3894
Reduced 89.6092 90.3147
Full 90.2129 91.9666
Table 2: Span ordering classification accuracy.
For first-level data, n = 3147. For all data, n =
10170. Labels = {0 1, 1 0}.
First-level All
Baseline 74.5154 50.4425
Per-relation 74.5154 64.2773
Reduced 77.8201 72.1731
Full 74.3883 66.1357
Table 3: Between-span punctuation classification
accuracy. For first-level data, n = 3147. For all
data, n = 10170. Labels = {semicolon, comma,
full, N/A}.
better than the reduced feature set for the all data
set (p < .001), but not for the first-level data set.
Most of the relations have a strong preference
for one ordering or the other. Most mistakes are
made on those that don?t (e.g. attribution, list).
6.3.2 Punctuation Insertion
We have one input feature vector for each re-
lation instance that has two children. We assume
that span ordering is performed prior to punctu-
ation insertion, so the child spans are ordered as
they appear in the data. The pattern is the punc-
tuation mark that should appear between the two
child spans (one of N/A or comma or semicolon
or full6), which indicates whether the two children
should be realized as separate sentences, as sepa-
rate clauses, or merged.
Classification accuracy for all methods is re-
ported in Table 3. For the all data set, all meth-
ods perform significantly better than baseline (p <
.001), and both the reduced and full feature sets
give results significantly better than using the re-
lation alone (p < .001). Furthermore, the re-
duced feature set performs significantly better than
the full feature set (p < .001). By contrast, for
the first-level data set, the reduced feature set per-
forms significantly better than all the other data
sets, while there are no statistically significant dif-
ferences in performance between the baseline, per-
relation and full feature sets.
The most common type of error was misclas-
sifying comma, semicolon or full as N/A: for the
6full indicates a sentence boundary (. or ? or !).
295
First-level All
Baseline 62.6629 68.4267
Per-relation 68.605 70.1377
Reduced 73.6257 73.9135
Full 74.3565 74.5919
Table 4: Discourse cue classification accuracy.
For first-level data, n = 3147 and no. labels = 92.
For all data, n = 10170 and no. labels = 203.
first-level data this is what the models trained on
the per-relation and full feature sets do most of the
time. The second most common type of error was
misclassifying comma, semicolon or N/A as full.
6.3.3 Discourse cue selection
We have one input feature vector for each re-
lation instance having two children. We use the
same features as in the previous experiment, and
as in the previous experiment, we order the child
spans as they appear in the data. The pattern is the
first discourse cue appearing in the ordered child
spans7.
Classification accuracy for all methods is re-
ported in Table 4. All methods perform signifi-
cantly better than baseline (p < .001), and both
the reduced and full feature sets give results sig-
nificantly better than using the relation alone (p <
.001). The performance differences between the
reduced and full feature sets are not statistically
significant for either data set.
For this task, 44 of the 92 labels in the first-level
data, and 97 of the 203 labels in the all data, oc-
curred only once. These cues were typically misla-
beled. Commonly occurring labels were typically
labeled correctly.
6.4 Discussion
Our methods for rule extraction are not general in
the sense that they rely on having access to particu-
lar types of annotation which are not widely avail-
able nor readily obtainable by automatic means.
However, our extracted rules have quite broad cov-
erage and will give NLG system developers a jump
start when using and adapting SPaRKy.
Our reduced feature set compares favorably in
discriminative power to both our full feature set
and the per-relation feature set. It achieves a very
7Some relations have multiple cues, either independent
cues such as but and also, or cues that depend on each other
such as on the one hand and on the other hand. Using all
cues is infeasible, and there are too few span-internal and
span-final cues to break up the cue classification for this eval-
uation.
good fit to the input data for the span ordering task
and a good fit to the input data for the punctua-
tion and discourse cue insertion tasks, especially
for the first-level data set. Factors affect perfor-
mance include: the punctuation insertion data is
highly imbalanced (by far the most common label
is N/A), while for the discourse cue insertion task
there is a problem of data sparsity.
7 Revised SPaRKy
One way to use these results would be to model the
sentence planning task as a cascade of classifiers,
but this method does not permit the system devel-
oper to add his or her own rules. So we continue to
use SPaRKy, which is rule-based. We have made
several changes to the Java version of SPaRKy to
support application of our sentence plan construc-
tion rules. We modified the classes for storing and
managing rules to read our XML rule format and
process rule conditions and patterns. We stripped
out the dependence on RealPro and added hooks
for SimpleNLG (Gatt and Reiter, 2009). We modi-
fied the rule application algorithm so that users can
choose to use a single rule set with patterns cov-
ering all three sentence planning tasks, or one rule
set for each sentence planning task. Also, since
there are now many rules, we give the user the
option to specify which relations jSPaRKy should
load rules for at each run.
Information about the revised jSparky, in-
cluding how to obtain it, is available at
http://www.research.att.com/?stent/sparky2.0/
or by contacting the first author.
8 Conclusions and Future Work
In this paper we described how we extracted
less domain-dependent sentence plan construction
rules from the RST-DT corpus. We presented eval-
uations of our extracted rule sets and described
how we integrated them into the freely-available
SPaRKy sentence planner.
In future work, we will experiment with dis-
course cue clustering. We are also looking at alter-
native ways of doing sentence planning that permit
a tighter interleaving of sentence planning and sur-
face realization for improved efficiency and output
quality.
296
References
R. Barzilay and K. McKeown. 2005. Sentence fusion
for multidocument news summarization. Computa-
tional Linguistics, 31(3):297?328.
L. Carlson, D. Marcu, and M. E. Okurowski. 2002.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. In Proceedings
of the SIGdial workshop on discourse and dialogue.
B. Di Eugenio, J. D. Moore, and M. Paolucci. 1997.
Learning features that predict cue usage. In Pro-
ceedings of the EACL.
A. Gatt and E. Reiter. 2009. SimpleNLG: A realisation
engine for practical applications. In Proceedings of
the European Workshop on Natural Language Gen-
eration.
B. Grote and M. Stede. 1998. Discourse marker choice
in sentence planning. In Proceedings of the 9th In-
ternational Workshop on Natural Language Gener-
ation.
E. Hovy and E. Maier. 1992. Parsimo-
nious or profligate: how many and which dis-
course structure relations? Available from
http://handle.dtic.mil/100.2/ADA278715.
N. Karamanis. 2007. Supplementing entity coherence
with local rhetorical relations for information order-
ing. Journal of Logic, Language and Information,
16(4):445?464.
P. Kingsbury and M. Palmer. 2003. PropBank: the
next level of TreeBank. In Proceedings of the Work-
shop on Treebanks and Lexical Theories.
B. Lavoi and O. Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings
of ANLP.
W. Mann and S. Thompson. 1987. Rhetorical structure
theory: A theory of text organization. Technical Re-
port ISI/RS-87-190, Information Sciences Institute,
Los Angeles, CA.
D. Marcu. 1997. The rhetorical parsing, summa-
rization, and generation of natural language texts.
Ph.D. thesis, Department of Computer Science, Uni-
versity of Toronto.
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
E. Marsi and E. Krahmer. 2005. Explorations in sen-
tence fusion. In Proceedings of the European Work-
shop on Natural Language Generation.
M. Moser and J. D. Moore. 1995. Using discourse
analysis and automatic text generation to study dis-
course cue usage. In Proceedings of the AAAI 1995
Spring Symposium on Empirical Methods in Dis-
course Interpretation and Generation.
C. Nakatsu. 2008. Learning contrastive connective in
sentence realization ranking. In Proceedings of SIG-
dial 2008.
O. Rambow, S. Bangalore, and M. A. Walker. 2001.
Natural language generation in dialog systems. In
Proceedings of HLT.
E. Reiter and R. Dale. 2000. Building natural lan-
guage generation systems. Cambridge University
Press, Cambridge, UK.
A. Stent, R. Prasad, andM. A.Walker. 2004. Trainable
sentence planning for complex information presen-
tations in spoken dialog systems. In Proceedings of
the ACL.
M. Taboada. 2006. Discourse markers as signals (or
not) of rhetorical relations. Journal of Pragmatics,
38(4):567?592.
M. A. Walker, A. Stent, F. Mairesse, and R. Prasad.
2007. Individual and domain adaptation in sentence
planning for dialogue. Journal of Artificial Intelli-
gence Research, 30:413?456.
S. Williams and E. Reiter. 2003. A corpus analysis of
discourse relations for natural language generation.
In Proceedings of Corpus Linguistics.
I. Witten and F. Eibe. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
297
Rhetorical structure in dialog* 
Amanda Stent  
Computer  Science Depar tment  
Un ivers i ty  of  Rochester  
Rochester ; :N 'Y  14~27 ...... 
s tent~cs ,  rochester ,  edu  
Abst rac t  
In this paper we report on several issues arising 
out of a first attempt o annotate task-oriented spo- 
ken dialog for rhetorical structure using Rhetorical 
Structure Theory. We discuss an annotation scheme 
we are developing to resolve the difficulties we have 
encountered. 
1 In t roduct ion  
In this paper we report on several issues arising out 
of a first attempt o annotate complex task-oriented 
spoken dialog for rhetorical structure using Rhetor- 
ical Structure Theory (RST): 
o Relations needed (section 3.1) 
o Identification of minimal units for annotation 
(section 3.2.2) 
? Dialog coverage (section 3.2.3) 
? Overlap due to the subject-matter/presenta- 
tional relation distinction (section 3.3) 
We discuss how we are dealing with these issues in an 
annotation scheme for argumentation acts in dialog 
that we are developing. 
2 P rev ious  work  
We are engaged in tim construction and inlplemen- 
ration of a theory of content-planning for complex, 
mixed-initiative task-oriented ialogs based on cor- 
pus analysis, for use in dialog systems uch as the 
TRIPS system (Allen et al, 2000) 1 . Our basic 
premise is that a conversational gent should be able 
to produce whatever a human can produce in simi- 
lar discourse situations, and that if we can explain 
why a human produced a particular contribution, 
" This work w~ supported by ONR research grant N00014- 
95-l-1088, U.S. Air Force/Rome Labs research contract no. 
F30602-95-1-0025, NSF research grant no. IRI-9623665 and 
Columbia University/NSF research grant no. OPG: 1307. We 
would like to thank the armuymous reviewers and l)r. Jason 
Eisner for their helpful comments on earlier drafts of this 
paper. 
IWe are using the Monroe corpus (Stent, 2000), with ref- 
erence t.o the TRAINS corpus (Heeman and Allen, 1995) and 
the HCRC Mapta~sk corpus (Anderson et al, 1991). 
247 
we can program a conversational gent to produce 
something similar. Therefore, in examining our di- 
alogs the question we must answer is "Why did this 
speaker produce this?". 
RST is a descriptive theory of hierarchical struc- 
ture in discourse that identifies functional relation- 
ships between discourse parts based on the inten- 
tions behind their production (Mann and Thomp- 
son, 1987). It has been used in content plan- 
ning systems for text (effectively text monolog) (e.g. 
(Cawsey, 1993), (How, 1993), (Moore and Paris, 
1993)). It has not yet been used much in content 
planning for spoken dialog. 
Because the dialogs we are examining are task- 
oriented, they are hierarchically structured and so 
provide a natural place to use RST. In fact, in or- 
der to uncover the full structure behind discourse 
contributions, it is necessary for us to use a model 
of rhetorical structure. Certain dialog contribu- 
tions are explained by the speaker's rhetorical goals, 
rather than by task goals. In example 1, utterance 3
is justification for utterance 1 but does not directly 
contribute to completing the task. 
Example  1 
A 1 They can't fix that power line at five 
ninety and East 
B 2 \Veil it 
A 3 Because you got to fix the tree first 
The details of how to apply RST to spoken dialog 
are unclear. If we mark rhetorical structure only 
within individual turns (as has generally been the 
case  in annotations of text dialog, e.g. (Moser et 
al., 1996),(Cawsey, 1993)), we miss the structure in 
contributions like example 1 or example 2. There 
is also tile question of how to handle dialog-specific 
behaviors: grounding utterances and back-channels 
(utterances that maintain the comnmnication), and 
al)andoned or interrupted utterances. 
Example  2 (simpli f ied) 
A 1 Bus C at irondequoit broke down. 
B 2 Before it. even got started? 
A 3 ~'eah, but we convinced some people to 
loan US sonic vans. 
Initial annotation 
Dialog-specific Subtypes of Elaboration Other 
Comment Particularize, Generalize Comparison 
Correction Instantiate Counter-expectation 
Cue i Exemplify Agent, Role 
Argumentation acts 
? Question~response.:: 
Proposal-accept 
Greeting-ack. 
New manual 
Subtypes of Elaboration Schemas 
~Set~member . . . . . . . . .  Joke, List 
Process-step Make-plan 
Object-attribute Describe-situation 
Figure 1: Examples of other relations 
In our first attempt to annotate, we removed 
abandoned utterances, back-channels, and simple 
acknowledgments such as "Okay". We used utter- 
ances as minimal units; utterances were segmented 
using prosodic and syntactic cues and speaker 
changes (see 3.2.2). We did occasionally split an ut- 
terance into two units if it consisted of two phrases or 
clauses eparated by a cue word such as "because". 
Two annotators, working separately, marked one 
complete dialog using Michael O'Donnell's RST an- 
notation tool (1997). They used the set of relations 
in (Mann and Thompson, 1987), and some addi- 
tional relations pecific to dialog or to our domain. 
Examples of the additional relations are given in fig- 
ure 1. When we compared the results, the tree struc- 
tures obtained were similar, but the relation labels 
were very different, and in neither case was the entire 
dialog covered. Also, the annotators found structure 
not covered by the relations given. As a result, we 
stopped the annotation project and started evelop- 
ing an annotation scheme that would retain rhetor- 
ical relations while dealing with the difficulties we 
had encountered. The rest of this paper describes 
this new annotation scheme. An example of the type 
of analysis we are looking for appears in figure 3. 
3 I ssues  and  proposa ls  
The issues we encountered fall into three areas, 
which we will examine in turn: issues related to in- 
dividual relations, dialog-specific issues, and issues 
related to the well-known presentational/subject- 
matter distinction in RST. 
3.1 Relat ions 
The key in any annotation project is to have a set 
of tags that are mutually exclusive, descriptive, and 
give a useful distinction between different behaviors. 
The set of relations we used failed this test with 
respect o our corpus. 
As in earlier work (Moore and Paris. 1992). our 
annotators found some of the relations ambiguous. 
In particular, the differences between the motivate 
and justify relations and between the elaboration and 
motivation relations were unclear (partly because 
248 
we did not distinguish between presentational nd 
subject-matter relations). 
Some of the relations we used overlapped. The 
elaboration relation is too broad; in some sections 
of our dialogs almost every utterance is an elabora- 
tion of the first one, but the utterances cover a wide 
variety of different ypes of elaborations. Anticipat- 
ing this, we had given the annotators several more 
specific relations (see figure 1), but we also allowed 
them to use the elaboration tag in case a type of elab- 
oration arose for which there was no subtype. As a 
result of the overlap, use of the elaboration tag was 
inconsistent. The joint relation is also too broad. 
Other relations were never used, although one an- 
notator went on to look at several more dialogs. In 
short, the set of relation-tags we used did not effec- 
tively partition the set of relations we saw. 
In our annotation scheme, we are taking several 
steps to define relations more clearly, reduce over- 
lap, and eliminate too-broad relations. Instead of 
giving annotators an semi-ordered set of relations 
with their definitions, we are giving them decision 
trees, with questions they can use to clarify the dis- 
tinctions between relations at each point (figure 2). 
The annotators did not find the relation definitions 
in (Mann and Thompson, 1987) particularly help- 
ful, but we are including simplified definitions, and 
annotators are instructed to test against he defini- 
tions before labeling any relation. We are including 
several examples with each definition, so that anno- 
tators can obtain an intuitive understanding of how 
the relations appear. Finally, we are providing any 
useful discourse cues that signal the existence of a 
relation. 
We are eliminating relations that overlap with 
others. Where a relation appears to cover a variety 
of different phenomena, s in the case of elaboration, 
we are using more specific relations instead. We are 
eliminating the joint relation, as it gives no help- 
ful information from a content-planning perspective 
and annotators are tempted to over-use it. 
One of the criticisms of RST is that there is an 
infinite set of relations (Grosz and Sidner, 1986). 
The goal is to arrive at a mutually-exclusive, clearly- 
defined set of relations with" discr iminatory power in 
each domain, so we expect that  for each new do- 
main, it may be necessary to start  with an initial 
set of high-level relations elected from different cat- 
egories, examine a small  set of texts or dialogs in that 
domain, and then revise the set of relations by mak- 
? ing relevant high-leve! .relations more.specific.._We.. 
used this process to develop our annotat ion scheme. 
In the manual  we include instructions for moving to 
new domains. Our examples come from a variety of 
domains and types of discourse, to add generality. 
3.2 D ia log -spec i f i c  i ssues  
3.2.1 Dia log-spec i f i c  re la t ions ,  schemas  and 
conversat iona l  games  
Task-oriented ialog is a complex behavior, involv- 
ing two part ic ipants,  each with their own beliefs 
and intentions, in a col laborative ffort to inter- 
act to solve some problem. There is a whole set 
of behaviors related to maintaining the col labora- 
tion and synchronizing beliefs that does not arise 
in monolog \[(Clark, 1996), (Traum and Hinkelman, 
1992)\]. These include answering questions, agree- 
ing to proposals, and simply acknowledging that  the 
othe r part ic ipant has spoken. 
In example 3, ut terance 3 provides motivation for 
utterance 1. However, A would not have produced 
utterance 3 without B's question. If we simply mark 
a motivation relation between utterances 1 and 3 we 
will be losing dialog coverage, the spans involved 
in the relation will not be adjacent, and we will be 
ignoring the important  relationship between utter- 
ances 2 and 3. A better  analysis would be to mark 
a question-answer relation between utterances 2 and 
3, and a motivation relation between utterance 1and 
the unit consisting of utterances 2 and 3. 
Example  3 
A 1 Then they're going to have to 
basically wait 
B 2 Why? 
A 3 Because the roads have to be fixed before 
electrical lines can be fixed 
The question-answer relation is not in Mann and 
Thompson's  original list of relations 2. It is an "ad- 
jacency pair  ''a, and is a type of conversational game 
(ClarM 1996). Adjacency pairs, like other relations, 
are functional relat ionships between parts of dis- 
course, but. they are specific to mult i -party discourse. 
In our annotat ion scheme, we include relations for 
different kinds of adjacency pairs (figure 1). We have 
2They do. however, include requests for information in the 
solutionhood relation 
aAn adjacency pair is a pair of utterances, the first of which 
imposes a cognitive preference for the second, e.g. question- 
answer, proposabaeeept. 
249 
1. In this set of spans, is the speaker attempting to 
affect the hearer's: 
o be l ie f -  go to question 2 
? a t t i tude  - go to question 3 
o abi l i ty  to perform an action - enablemen~ 
...... .2.. Is:t:he_speaker..tryi.ug..to.inccrease.the.hearer'.s be l ie f  
in some fact, or enable the hearer to better under -  
s tand  some fact? 
? Bel ief -  evidence 
? Understanding- background 
3 . . . .  
Figure 2: Par t ia l  decision tree for presentational re- 
lations, expressed as a list of questions 
tentat ively categorized adjacency pairs with subject-  
matter  relations, although they may eventual ly be- 
come a third category of relation. 
Some of these relations are bi-nuclear. For in- 
stance, a l though usually the answer is the only par t  
required for discourse coherence, at times both ques- 
tion and answer may be needed, as in example 4. 
Example  4 
A 1 And the last one was at the where 
on the loop? 
B 2 Four ninety. 
It would seem that these relations can only apply  
at the lowest levels of an RST analysis, with a dif- 
ferent speaker for each span. However, example 5, 
in which turns 2-7 are the answer to the question in 
utterance 1, shows that this is not the case. 
Example  5 ( s l ight ly  s impl i f ied)  
A 1 What's "close"? 
B 2 "Close". Um I don't know. I I'm pretty 
sure that 
A 3 So Mount Hope and Highland would be. 
B 4 Yeah. 
A 5 Well what about like 252 and 383'? 
B 6 It says "next". 
A 7 Oka~v. So I guess it has to be adjacent. 
It might seem that .the simplest approach would 
be to annotate  adjacency pairs between turns, and 
mark other rhetorical relations only within turns. 
However, we have found many instances of rhetori-  
cal relations, or even units (section 3.2.2), spanning 
turns. The two examples below i l lustrate a cross- 
speaker elaboration and a cross-speaker sequence re- 
lation. 
Example  6 
A i So that.takes care of the ill guy 
and the handicapped guy. 
B 2 " Okay 
B 3 And that takes two hours. 
A 1 
A 2 
B 3 
A 4 
B 5 
B 6 
Summary 
Make-fla~ \ (6) 
...... Object-attribute, Enablement 
, /  \ 
$olutionhood, Quesffon-answer (nun~er), 
Motivation , / ,~ 
, / \ (3) Assert-ack. 
(~) (2) , / \ 
(4} (5} 
We have to send buses to the Lake. 
There are people there to evacuate. 
How many are we sending? 
Two. 
Okay. 
So 1 ambulance to Pittsford and 2 
buses to the Lake. 
Figure 3: Sample analysis of part  of a constructed 
dialog. Nuclei are marked with *; non-RST relations 
are in italics. 
Example  7 
A 1 So they can ta- to- take out the power. 
B 2 And then we have to wait ... 
Wi th  a model of adjacency pairs,_we can-now han- 
dle grounding acts such as acknowledgments.  If an 
utterance is clearly a back-channel or abandoned,  
annotators  are instructed to so mark it and leave it 
out of further annotat ion.  
RST in its original formulat ion does not cover en- 
veloping or parallel structures or conventional forms. 
However, even in task-or iented ialogs speakers oc- 
casionally tell jokes. Furthermore,  there are fixed, 
structural  patterns in dialog, such as form-fill ing 
behaviors. These are frequently domain-specif ic, 
and resemble schemas \[(McKeown, 1985), (Cawsey, 
1993)\]. While it may be possible to give an RST 
analysis for some of these, it is more accurate to 
identify, what is actual ly going on. Our annotat ion 
scheme includes four of these, make-plan, describe- 
situation, list and joke. It also includes an adjacency 
pair for greetings, a conventional form. 
An annotated ialog extract  i l lustrat ing most of 
these issues is shownin  figure 3. 
3.2.2 Ident i fy ing  and  order ing  un i ts  
In spoken dialog, both part ic ipants often speak at 
once, or one speaker may complete what another 
speaker says, as in examples 8 and 9. 
Example  8 (+ 's  mark  over lapp ing  speech)  
:\ 1 And + he's done + with that at one thirty 
B 2 + Okay + 
Example  9 
A 1 So it'll take them 
B 2 Two nmre hours 
250 
Our  original use of utterances as minimal  units 
spl its a cross-turn completion from the utterance it 
completes (example 9) ,  and says nothing about  how 
to order  units when one overlaps with another.  We 
have altered our segmentat ion rules to take care of 
these difficulties. Our definition is that  a minimal 
~unit .must .be one~.~f tthe following,~.with:eadier pos- 
sibi l it ies taking precedence over later ones: 
1. A syntact ic phrase separated from the immedi- 
ate ly  prior phrase by a cue word such as "be- 
cause" or "since" 
2. A syntact ical ly  complete clause 
3. A stretch of continuous speech ended by a 
pause, a prosodic boundary  or a change of 
speaker 
One unit  will be considered to succeed another if 
it s tar ts  after the other. 
This  means that  the standard segmentat ion of a 
dialog into utterances may have to be modified for 
the purposes of an RST analysis, although a segmen- 
tat ion into utterances and one into minimal units 
will be very similar. Annotators  will start  with a 
dialog segmented into turns and utterances, and are 
encouraged to re-segment as needed. 
3 .2.3 D ia log  coverage  
When one gets higher in the tree resulting from an 
RST annotat ion,  the spans typical ly begin to fol- 
low the task structure or the exper imental  structure. 
In the Monroe corpus, usually one partner tells the 
other about  the task, then the two col laborate to 
solve it, and finally one partner  summarizes the so- 
lution (following the experimental  structure).  In the 
TRAINS corpus usually one subtask in the plan is 
discussed at a t ime (following the task structure).  
Given the length and complexity of a typical dia- 
log, it may not be possible to achieve complete cov- 
erage, even with our expanded relation set and the 
use of schemas. If we can identify useful sub-dialogs 
or can associate parts  of a dialog with parts of the 
task, f inding annotat ions for each part may suffice. 
For our domain,  we have establ ished heuristics about 
when an annotator  can stop trying to achieve cover- 
age. An annotator  can stop when: 
o The top level of the annotat ion tree has one 
re lat ion label covering the whole dialog. 
o The structure between the spans at the top level 
is identical to the task structure.  
* Tim structure between the spans at the top 
level is identical to a domain-dependent or
expe.r iment-dependent schema. 
o There is consensus between annotators  that no 
more relations can be marked. 
3.3 The sub jec t -mat ter /p resentat iona l  
relation d ist inct ion 
The relations in RST fall into two classes. Subject- 
matter relations uch as summary are intended to 
be recognized by the hearer. Presentational rela- 
tions such as motivation are supposed to "increase 
some inclination" in the hearex~ LtCh .as. the. inclina- 
tion to act (Mann and Thompson, 1987). As Moore 
and associates have explained in (1992) and (1993), 
while the intentions of the speaker are adequately 
represented in the case of presentational relations 
by the relations themselves, in the case of subject- 
matter relations the intentions of the speaker may 
vary. Furthermore, these two types of relations ac- 
tually come from different levels of relationship be- 
tween discourse elements: the informational level 
(subject-matter relations), and the intentional level 
(presentational relations). RST conflates these two 
levels. 
Mann and Thompson said that, in the case where 
a presentational relation and a subject-matter re- 
lation were both applicable, the subject-matter re- 
lation should take precedence. However, we would 
like to have information about both levels when pos- 
sible. In our annotation scheme the presentational 
relations are split from the subject-matter relations 
and annotators are instructed to consider for each 
set of spans whether there is a subject-matter rela- 
tion, and also whether there is a presentational rela- 
tion. If there are two relations, both are marked. If 
one covers a slightly different span than the other, 
at the next level of annotation the span that seems 
more appropriate is used. 
In the following example, utterance 3 is justifica- 
tion (presentational) for utterance 1, but it is also 
in a non-volitional cause (subject-matter) relation- 
ship with utterance 1. The annotator would be in- 
structed to label both relations. 
Example  10 (s l ight ly s impl i f ied)  
A 1 I can't find the Rochester airport 
B 2 + I- it's + 
A 3 + I think I have + a disability with maps 
We would also like more information, at times, 
about the subject matter in the spans of a relation. 
The relation between a "When" question and an- 
swer is question-answer, as is that between a "Why" 
question and answer; but the first question-answer 
forms part of an elaboration and the second forms 
part of a justification or motivation. In our ammta- 
tion scheme, we supply a list of content types, such 
as time. location and number. The annotator adds 
the content ype in I)arentheses after the relation tag 
when required. This means that the annotator may 
have to mark three items for a given set of spans: 'the 
presentational relation (if any), the subject-matter 
relation, and the content ype (if required). We find 
25t 
this approach preferable to expanding the set of re- 
lations to include, for instance, temporal-question- 
answer and spatial-question, answer. Cawsey used a 
similar method in (1993). 
4 Cur rent  and  fu ture  work  
? -"-We-:havean :amaotation ~manuat"that weare"refming "
using TRAINS-93 dialogs 4. Shortly, we will begin 
annotating the Monroe corpus with the new manual 
and different annotators. We will also annotate a 
few dialogs from a different corpus (e.g. Maptask) 
to ensure generality. We plan to use the results of 
our annotation in the construction (ongoing) of new 
generation components for the TRIPS system at the 
University of Rochester (Allen et al, 2000). 
5 Re la ted  Work  
In recent years there has been much research on 
annotation schemes for dialog. Traum and Hinkel- 
man outline four levels of "conversational cts" in 
(1992). "Argumentation acts", including rhetorical 
relations, form the top level, but this level is not de- 
scribed in detail. DAMSL (Core and Allen, 1997) in- 
cludes speech acts and some grounding acts, but not 
rhetorical relations. The HCRC Maptask project an- 
notation scheme includes adjacency pairs, but not 
rhetorical relations (Carletta et al, 1996). 
The COCONUT project annotation manual al- 
lows the annotator to mark individual utter- 
ances as elaboration, and segments as summary, 
act:condition, act:consequence or otherinfo (DiEu- 
genio et al, 1998). This annotation scheme does 
not treat rhetorical structure separately from other 
types of dialog behavior. We have observed enough 
structure in the corpora we have looked at to jus- 
tify treating rhetorical structure as a separate, im- 
portant phenomenon. For instance, in a DAMSL- 
tagged set of 8 dialogs in our corpus, 40% of the 
utterances were statements, and many of these ap- 
peared in sequences of statements. The relationships 
between many of these statements are unclear with- 
out a model of rhetorical structure. 
In (1999), Nakatani and Traum describe a hierar- 
chical annotation of dialog for I-units, based on the 
.. domination and satisfaction-precedence relations of 
(Grosz and Sidner, 1986). Other researchers have 
shown that Grosz and Sidner's model of discourse 
structure (GST) and RST are similar in many re- 
spects \[(Moser and Moore, 1996), (Marcu, 1999)\]. 
However, RST provides more specific relations than 
GST, and this is useful for content planning. As 
well as helping to specify generation goals, content 
and ordering constraints, the rhetorical information 
is needed in case the system has to explain what it. 
has said. 
4A rough draft is available from the author. 
RDA is an annotation scheme for identifying 
rhetorical structure in explanatory texts in the 
SHERLOCK domain (Moser et al, 1996). We follow 
RDA in requiring annotators to consider both in- 
tentional and informational relations. However, be- 
cause of the dialog issues previously described, RDA 
is not sufficient for dialog. 
Marcu uses discourse-cuesto"automa~ically un- 
cover rhetorical relations in text (1997). Much of 
this work is applicable to the problem of uncovering 
rhetorical relations in dialog; however, many cues 
in dialog are prosodic and it is not yet possible to 
obtain accurate information about prosodic ues au- 
tomatically. 
6 Conclusions 
We have examined several issues arising from a first 
attempt o annotate spoken dialog for rhetorical 
structure. We have proposed ways of dealing with 
each of these issues in an annotation scheme we are 
developing. Much future work is certainly needed 
in this area; we hope that the results of our annota- 
tion may form a quantitative baseline for comparison 
with future work. 
References 
J. Allen, D. Byron, M. Dzikovska, G. Ferguson, 
L. Galescu, and A. Stent. 2000. An architecture 
for a generic dialogue shell, upcoming in the Nat- 
ural Language Engineering Journal special issue 
on Best Practices in Spoken Language Dialogue 
Systems Engineering. 
A. Anderson, M. Bader, E. Bard, E. Boyle, G. Do- 
herty, S. Garrod, S. Isard, J. Kowtko, J. MeAl- 
lister, J. Miller, C. Sotillo, H. Thompson, and 
R. Weinert. 1991. The HCRC Maptask corpus. 
Language and Speech, 34:351-366. 
J. Carletta, A. Isard, S. Isard, J. Kowtko, 
and G. Doherty-Sneddon. 1996. HCRC dia- 
log structure coding manual. Technical Report 
HCRC/TR-82, HCRC, Edinburgh University. 
A. Cawsey. 1993. Planning interactive explanations. 
International Journal of Man-Machine Studies, 
38:169-199. 
H. Clark. 1996. Using Language. Cambridge Uni- 
versity Press. 
M. Core and J. Allen. 1997. Coding dialogs with the 
DAMSL annotation scheme. In AAAI Fall Sym- 
posium on Communicative Action in Humans and 
Machines, pages 28-35, November. 
B. DiEugenio, P. Jordan. and L. Pylkkiinen. 1998. 
The COCONUT project: Dialogue annotation 
manual. Technical Report 98-I, ISP, University 
of Pittsburgh. 
B. Gross- and C. Sidner. 1986. Attention, inten- 
tions, and the structure of discourse. Computa- 
tional Linguistics, 12(3). 
252 
P. Heeman and J. Allen. 1995. The TRAINS-93 
dialogs. Technical Report Trains TN 94-2, Com- 
puter Science Dept., U. Rochester, March. 
E. Hovy. 1993. Automated iscourse generation us- 
ing discourse structure relations. Artificial Intel- 
ligence, 63(1-2):341-385. 
W.. Mann_and S. Thompsom 19877. Rhetorical struc- 
ture theow: a theory of  text organisation. In 
L. Polanyi, editor, The Structure of Discourse. 
Ablex, Norwood, NJ. 
D. Marcu. 1997. The rhetorical parsing, sum- 
marization, and generation of natural anguage 
texts. Technical Report CSRG-371, Department 
of Computer Science, University of Toronto. 
D. Marcu. 1999. A formal and computational 
synthesis of Grosz and Sidner's and Mann and 
Thompson's theories. In The Workshop on Levels 
of Representation i  Discourse, Edinburgh, Scot- 
land. 
K. McKeown. 1985. Text Generation: Using Dis- 
course Strategies and Focus Constraints to Gener- 
ate Natural Language Text. Cambridge University 
Press, Cambridge. 
J. Moore and C. Paris. 1992. Exploiting user feed- 
back to compensate for the unreliability of user 
models. UMUAI, 2(4):331-365. 
J. D. Moore and C. L. Paris. 1993. Planning text 
for advisory dialogues: Capturing intentional nd 
rhetorical information. Computational Linguis- 
tics, 19(4):651-695. 
J. Moore and M. Pollack. 1992. A problem for RST: 
The need for multi-level discourse analysis. Com- 
putational Linguistics, 18(4):537-544. 
M. G. Moser and J. D. Moore. 1996. Toward a 
synthesis of two accounts of discourse structure. 
Computational Linguistics, 22(3):409-420. 
M. Moser, J. Moore, and E. Glendening. 1996. 
Instructions for coding explanations: Identifying 
segments, relations and minimal units. Technical 
Report 96-17, University of Pittsburgh. Depart- 
ment of Computer Science. 
C. Nakatani and D. Traum. 1999. Coding discourse 
structure in dialogue (version 1.0). Technical Re- 
port UMIACS-TR-99-03, University, of Maryland. 
Michael O'Donnell. 1997. RST-Tool: An RST 
analysis tool. In Proceedings of the 6th Eu- 
ropean Workshop on Natural Language Gener- 
ation, Gerhard-Mercator University, Duisburg, 
Germany. 
A. Stent. 2000. The Monroe corpus. Technical Re- 
port TR728/TN99-2, University of Rochester. 
D. Traum and E. Hinkehnan. 1992. Conversation 
acts in task-oriented spoken dialogue. Computa- 
tional Intelli.qenee, 8(3):575--599. 
NAACL HLT Demonstration Program, pages 15?16,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
RavenCalendar: A Multimodal Dialog System for Managing a
Personal Calendar?
Svetlana Stenchikova and Basia Mucha and Sarah Hoffman and Amanda Stent
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
sveta,basia,shoffman,stent@cs.sunysb.edu
1 Introduction
Dialog applications for managing calendars have
been developed for every generation of dialog
systems research (Heidorn, 1978; Yankelovich,
1994; Constantinides and others, 1998; Horvitz
and Paek, 2000; Vo and Wood, 1996; Huang
and others, 2001). Today, Web-based calendar
applications are widely used. A spoken dialog
interface to a Web-based calendar application
permits convenient use of the system on a hand-
held device or over the telephone.
In this demo, we present RavenCalendar,
a multimodal dialog system built around the
Google Calendar and Google Maps Web appli-
cations. RavenCalendar allows the user to cre-
ate, modify and remove calendar events, query
for events, and hear descriptions of events. In
our demonstration we will focus on two aspects
of RavenCalendar: its flexible approach to lan-
guage understanding and dialog management,
and its multimodal interface.
Flexible dialog management In RavenCal-
endar, during event management or event query-
ing a user can provide event information either
in a single utterance (e.g. ?Add a meeting for
tomorrow from two to three in the computer sci-
ence lounge? or by engaging in a subdialog with
the system (Table 1). This functionality is made
possible because we use the Ravenclaw dialog
manager (Bohus and Rudnicky, 2003).
?We would like to thank Dan Bohus, Antoine Raux,
and Thomas Harris for providing Olympus, and for an-
swering many questions about Ravenclaw. This research
is based on work supported by the National Science Foun-
dation under grant no. 0325188.
S: Welcome to RavenCalendar,
the automated dialog calendar system.
S: What can I do for you today?
U: add a meeting for Monday
S: For what time do you want to add the meeting?
U: from ten until eleven
S: The event was added type meeting
for tomorrow starting at ten a.m.
U: List all events for the next week
Table 1: Sample dialog with RavenCalendar
Multimodality A RavenCalendar user may
interact with the calendar directly using the
Google Calendar interface, or may interact
through RavenCalendar using text, speech, map
gestures or a combination of these media. A user
may use the Google Maps interface to specify
the location of an event; the system uses Google
Maps to display the locations of events.
2 System Description
RavenCalendar, whose architecture is shown in
Figure 1, is developed using Ravenclaw and
Olympus (Bohus and others, 2007). Olympus
is a dialog system shell; Ravenclaw is the Olym-
pus dialog manager. In developing RavenCal-
endar, we chose to use an existing dialog shell
to save time on system development. (We are
gradually replacing the Olympus components
for speech recognition, generation and TTS.)
RavenCalendar is one of the first dialog systems
based on Olympus to be developed outside of
CMU. Other Olympus-based systems developed
at CMU include the Let?s Go (Raux and oth-
ers, 2005), Room Line, and LARRI (Bohus and
Rudnicky, 2002) systems.
Flexible dialog management The Raven-
claw dialog manager (Bohus and Rudnicky,
2003) allows ?object-oriented? specification of a
15
Figure 1: RavenCalendar Design
dialog structure. In RavenCalendar, we define
the dialog as a graph. Each node in the graph
is a minimal dialog component that performs a
specific action and has pre- and post-conditions.
The dialog flow is determined by edges between
nodes. With this structure, we maximize the
reuse of minimal dialog components. Ravenclaw
gives a natural way to define a dialog, but fine-
tuning the dialog manager was the most chal-
lenging part of system development.
Multimodality In RavenCalendar, a back-
end server integrates with Google Calendar for
storing event data. Also, a maps front end server
integrates with Google Maps. In addition to the
locations recognized by Google Maps, an XML
file with pre-selected location-name mappings
helps the user specify locations.
3 Current and Future Work
We are currently modifying RavenCalendar
to use grammar-based speech recognition for
tighter integration of speech recognition and
parsing, to automatically modify its parsing
grammar to accommodate the words in the
user?s calendar, to permit trainable, adaptable
response generation, and to connect to addi-
tional Web services and Web-based data re-
sources. This last topic is particularly inter-
esting to us. RavenCalendar already uses sev-
eral Web-based applications, but there are many
other Web services of potential utility to mo-
bile users. We are now building a component
for RavenClaw that searches a list of URLs for
event types of interest to the user (e.g. sports
events, music events), and automatically notifies
the user of events of interest. In the future, we
plan to incorporate additional Web-based func-
tionality, with the ultimate goal of creating a
general-purpose dialog interface to Web appli-
cations and services.
References
D. Bohus et al 2007. Olympus: an open-source
framework for conversational spoken language in-
terface research. In Proceedings of the Workshop
?Bridging the Gap? at HLT/NAACL 2007.
D. Bohus and A. Rudnicky. 2002. LARRI: A
language-based maintenance and repair assistant.
In Proceedings of IDS.
D. Bohus and A. Rudnicky. 2003. Ravenclaw: Dia-
log management using hierarchical task decompo-
sition and an expectation agenda. In Proceedings
of Eurospeech.
P. Constantinides et al 1998. A schema based ap-
proach to dialog control. In Proceedings of ICSLP.
G. Heidorn. 1978. Natural language dialogue for
managing an on-line calendar. In Proceedings of
ACM/CSCER.
E. Horvitz and T. Paek. 2000. DeepListener: Har-
nessing expected utility to guide clarification dia-
log in spoken language systems. In Proceedings of
ICSLP.
X. Huang et al 2001. MIPAD: A next generation
PDA prototype. In Proceedings of ICSLP.
A. Raux et al 2005. Let?s go public! Taking a spo-
ken dialog system to the real world. In Proceedings
of Interspeech.
M. Tue Vo and C. Wood. 1996. Building an appli-
cation framework for speech and pen input inte-
gration in multimodal learning interfaces. In Pro-
ceedings of ICASSP.
N. Yankelovich. 1994. Talking vs taking: Speech ac-
cess to remote computers. In Proceedings of the
Conference on Human Factors in Computing Sys-
tems.
16
Proceedings of NAACL HLT 2009: Short Papers, pages 189?192,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Lexical and Syntactic Priming and Their Impact in Deployed Spoken Dialog
Systems
Svetlana Stoyanchev and Amanda Stent
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400, USA
svetastenchikova@gmail.com, amanda.stent@stonybrook.edu
Abstract
In this paper, we examine user adaptation to
the system?s lexical and syntactic choices in
the context of the deployed Let?s Go! dialog
system. We show that in deployed dialog sys-
tems with real users, as in laboratory experi-
ments, users adapt to the system?s lexical and
syntactic choices. We also show that the sys-
tem?s lexical and syntactic choices, and con-
sequent user adaptation, can have an impact
on recognition of task-related concepts. This
means that system prompt formulation, even
in flexible input dialog systems, can be used
to guide users into producing utterances con-
ducive to task success.
1 Introduction
Numerous studies have shown that people adapt
their syntactic and lexical choices in conversation to
those of their conversational partners, both human
(Brennan, 1996; Pickering et al, 2000; Lockridge
and Brennan, 2002; Reitter et al, 2006) and com-
puter (Branigan et al, 2003; Brennan, 1991; Bren-
nan, 1996; Gustafson et al, 1997; Ward and Litman,
2007). User adaptation to the system?s lexical and
syntactic choices can be particularly useful in flexi-
ble input dialog systems. Limited input dialog sys-
tems, including most commercial systems, require
the user to respond to each system prompt using
only the concept and words currently requested by
the system. Flexible input dialog systems allow the
user to respond to system prompts with concepts
and words in addition to or other than the ones cur-
rently requested, and may even allow the user to
take task initiative. Speech recognition (ASR) accu-
racy in limited input systems is better than in flexi-
ble input systems (Danieli and Gerbino, 1995; Smith
and Gordon, 1997). However, task completion rates
and times are better in flexible input systems (Chu-
Carroll and Nickerson, 2000; Smith and Gordon,
1997). With user adaptation, in flexible input dia-
log systems prompts can be formulated to maximize
ASR accuracy and reduce the number of ASR time-
outs (Sheeder and Balogh, 2003).
Previous research on user adaptation to dialog
systems was conducted in laboratory settings. How-
ever, the behavior of recruited subjects in a quiet
laboratory may differ from that of real users in the
noisy world (Ai et al, 2007). Here we present the
first study, to the best of our knowledge, that in-
vestigates the adaptive behavior of real users of a
live dialog system. We analyze dialogs from CMU?s
Let?s Go! dialog system (Raux et al, 2005). We
look at the effects of the system?s lexical and syn-
tactic choices on: 1) lexical and syntactic choices
in user responses; and 2) concept identification rates
for user responses. We confirm prior results showing
that users adapt to the system?s lexical and syntactic
choices. We also show that particular choices for
system prompts can lead to higher concept identifi-
cation rates.
2 Experimental Method
We conducted our experiment using the Let?s Go!
telephone-based spoken dialog system that provides
information about bus routes in Pittsburgh (Raux
et al, 2005). The users are naive callers from the
general population seeking information about bus
189
condition request departure confirm departure request arrival confirm arrival
location location location location
(1) Where are you leav-
ing from?
Leaving from X, is this
correct?
Where are you going
to?
Going to X, is this
correct
(2) Where are you leav-
ing from?
From X, is this cor-
rect?
Where are you going
to?
To X, is this correct
(3) What is the place of
your departure
X, is this correct? What is the place of
your arrival?
X, is this correct
(4) Where do you want to
leave from?
You want to leave from
X, is this correct?
Where do you want to
go to?
You want to go to X,
is this correct
Table 1: Experimental conditions
Spkr Task type Utterance
Sys Open Welcome to the CMU Let?s
Go bus information system.
What can I do for you?
Usr 61A schedule
Sys Request
Departure
Where do you wanna leave
from?
Usr Location From downtown
Sys Confirm
Departure
Leaving from downtown. Is
this correct?
Usr Location Yes
Sys Request
Arrival
Where are you going to?
Usr Location Oakland
Sys Confirm
Arrival
Going to Waterfront. Is this
correct?
Usr Location No, to Oakland
Figure 1: Dialog extract from Let?s Go! data
schedules. In order to provide the user with route
information, Let?s Go! elicits a departure location,
a destination, a departure time, and optionally a bus
route number. Each concept value provided by the
user is explicitly confirmed by the system. Figure 1
shows an example dialog with the system.
Let?s Go! is a flexible input dialog system. The
user can respond to a system prompt using a single
word or short phrase, e.g. Downtown, or a complete
sentence, e.g. I am leaving from downtown1.
We ran four experimental conditions for two
months. The conditions varied in the lexical choice
and syntax of system prompts for two system re-
quest location tasks and two system confirm loca-
tion tasks (see Table 1). System prompts differed
1The user response can also contain concepts not requested
in the prompt, e.g. specifying departure location and bus num-
ber in one response.
by presence of a verb (to leave, to go) or a preposi-
tion (to, from), and by the syntactic form of the verb.
The request location prompt contained both a verb
and a preposition in the experimental conditions (1,
3, and 4). The confirm location prompt contained
both a verb and a preposition in conditions 1 and 4,
only a preposition in condition 2, and neither verb
nor preposition in condition 3. In conditions 1 and
4, both request and confirmation prompts differed in
the verb form (leaving/leave, going/go).
2184 dialogs were used for this analysis. For each
experimental condition, we counted the percentages
of verbs, verb forms, prepositions, and locations in
the ASR output for user responses to system request
location and confirm location prompts. Although
the data contains recognition errors, the only differ-
ence in system functionality between the conditions
is the formulation of the system prompt, so any sta-
tistically significant difference in user responses be-
tween different conditions can be attributed to the
formulation of the prompt.
3 Syntactic Adaptation
We analyze whether users are more likely to use ac-
tion verbs (leave, leaving, go, or going) and prepo-
sitions (to, from) in response to system prompts that
use a verb or a preposition. This analysis is interest-
ing because ASR partially relies on context words,
words related to a particular concept type such as
place, time or bus route. For example, the likelihood
of correctly recognizing the location Oakland in the
utterance ?going to Oakland? is different from the
likelihood of correctly recognizing the single word
utterance ?Oakland?.
Table 2 shows the percentages of user responses
190
Cond. Sys uses Sys uses % with % with
verb prep verb prep
Responses to request location prompt
(1) yes yes 2.3% ? 5.6%
(2) yes yes 1.9% 4.3%
(3) no no 0.7% 4.5%
(4) yes yes 2.4%? 6.0%
Responses to confirm location prompt
(1) yes yes 15.7% ? ? 23.4%
(2) no yes 3.9% 16.9%
(3) no no 6.4% 12.7%
(4) yes yes 10.8% 22.0%
Table 2: Percentages of user utterances containing verbs
and prepositions. ? indicates a statistically significant dif-
ference (p<0.01) from the no action verb condition (3).
? indicates a statistically significant difference from the
no action verb in confirmation condition (2).
in each experimental condition that contain a verb
and/or a preposition. We observe adaptation to the
presence of a verb in user responses to request lo-
cation prompts. The prompts in conditions 1, 2 and
4 contain a verb, while those in condition 3 do not.
The differences between conditions 1 and 3, and be-
tween conditions 4 and 3, are statistically significant
(p<0.01)2. The difference between conditions 2 and
3 is not statistically significant, perhaps due to the
absence of a verb in a prior confirm location prompt.
A similar adaptation to the presence of a verb in
the system prompt is seen in user responses to con-
firm location prompts. The prompts in conditions
1 and 4 contain a verb while those in conditions 2
and 3 do not. The differences between conditions
1 and 2, and between conditions 1 and 3, are statis-
tically significant (p<.01), while the difference be-
tween conditions 4 and 2 exhibits a trend. We hy-
pothesize that the lack of the statistically significant
differences between conditions 4 and 2, and condi-
tions 4 and 3, is caused by the low relative frequency
in our data of dialogs in condition 4.
We do not find statistically significant differences
in the use of prepositions. However, we observe a
trend showing higher likelihood of a preposition in
user responses to confirm location in the conditions
where the system uses a preposition. Prepositions
are short closed-class context words that are more
likely to be misrecognized (Goldwater et al, 2008).
2All analyses in this section are t-tests with Bonferroni ad-
justment.
Condition/ LEAVING LEAVE total
User?s verb (progressive) (simple)
(1) Progressive 74.5% 25.5% 55
(3) Neutral 61.3% 38.7% 31
(4) Simple 43% 57% 42
Condition/ GOING GO total
User?s verb (progressive) (simple)
(1) Progressive 84.4% 15.6% 45
(3) Neutral 66.6% 33.4% 21
(4) Simple 46.5% 53.5% 43
Table 3: Usage of verb forms in user utterances
Hence, more data (or human transcription) may be
required to see a statistically significant effect.
4 Lexical Adaptation
We analyze whether system choice of a particular
verb form affects user choice of verb form. For
this analysis we only consider user utterances in
response to a request location or confirm location
prompt that contain a concept and at least one of the
verb forms leaving, going, leave, or go3.
Table 3 shows the total counts and percentages
of each verb form in the progressive form condition
(condition 1), and the neutral condition (condition
3), and the simple form condition (condition 4)4.
We find that the system?s choice of verb form has
a statistically significant impact on the user?s choice
(?2 test, p<0.01). In the neutral condition, users
are more likely to choose the progressive verb form.
In the progressive form condition, this preference in-
creases by 13.2% for the verb to leave, and by 17.8%
for the verb to go. By contrast, in the simple form
condition, this preference decreases by 18.3% for
the verb to leave and by 20.1% for the verb to go,
making users slightly more likely to choose the sim-
ple verb form than the progressive verb form.
5 Effect of Adaptation on Speech
Recognition Performance
The correct identification and recognition of task-
related concepts in user utterances is an essential
functionality of a dialog system. Table 4 shows
3Such utterances constitute 3% of all user responses to all
request and confirm place prompts in our data.
4We ignore condition 2 where the verb is used only in the
request prompt.
191
System
prompt
Arrival
request
Departure
request
(1) 72.2% ? 63.8%
(2) 77.4% 61.0%
(3) 74.5% ? 61.5%
(4) 82.0% 66.0%
Table 4: Concept identification rates following request
location prompts. ? indicates a statistically significant
difference (p<0.01 with Bonferroni adjustment) from
condition 4.
the percentage of user utterances following a re-
quest location prompt that contain an automatically-
recognized location concept. Condition 4, where the
system prompt uses the verb form to leave, achieves
the highest concept identification rates. The differ-
ences in concept identification rates between condi-
tions 1 and 4, and between conditions 3 and 4, are
statistically significant for request arrival location
(t-test, p<.01). Other differences are not statistically
significant, perhaps due to lack of data.
6 Conclusions and Future Work
In this paper, we showed that in deployed dialog sys-
tems with real users, as in laboratory experiments,
users adapt to the lexical and syntactic choices of the
system. We also showed that user adaptation to sys-
tem prompts can have an impact on recognition of
task-related concepts. This means that the formula-
tion of system prompts, even in flexible input dialog
systems, can be used to guide users into producing
utterances conducive to task success.
In future work, we plan to confirm these results
using transcribed data. We also plan additional ex-
periments on adaptation in Let?s Go!, including an
analysis of the time course of adaptation and further
analyses of the impact of adaptation on ASR perfor-
mance.
7 Acknowledgements
We would like to thank the Let?s Go! researchers at
CMU for making Let?s Go! available. This research
was supported by the NSF under grant no. 0325188.
References
H. Ai, A. Raux, D. Bohus, M. Eskenazi, and D. Lit-
man. 2007. Comparing spoken dialog corpora col-
lected with recruited subjects versus real users. In Pro-
ceedings of SIGDial.
H. Branigan, M. Pickering, J. Pearson, J. McLean, and
C. Nass. 2003. Syntactic alignment between comput-
ers and people: the role of belief about mental states.
In Proceedings of CogSci.
S. Brennan. 1991. Conversation with and through com-
puters. User Modeling and User-Adapted Interaction,
1(1):67?86.
S. Brennan. 1996. Lexical entrainment in spontaneous
dialog. In Proceedings of ISSD.
J. Chu-Carroll and J. Nickerson. 2000. Evaluating au-
tomatic dialogue strategy adaptation for a spoken dia-
logue system. In Proceedings of NAACL.
M. Danieli and E. Gerbino. 1995. Metrics for evaluat-
ing dialogue strategies in a spoken language system.
In Proceedings of the AAAI Spring Symposium on Em-
pirical Methods in Discourse Interpretation and Gen-
eration.
S. Goldwater, D. Jurafsky, and C. Manning. 2008.
Which words are hard to recognize? Lexical, prosodic,
and disfluency factors that increase asr error rates. In
Proceedings of ACL/HLT.
J. Gustafson, A. Larsson, R. Carlson, and K. Hellman.
1997. How do system questions influence lexical
choices in user answers? In Proceedings of Eu-
rospeech.
C. Lockridge and S. Brennan. 2002. Addressees? needs
influence speakers? early syntactic choices. Psycho-
nomics Bulletin and Review.
M. Pickering, H. Branigan, A. Cleland, and A. Stew-
art. 2000. Activation of syntactic priming during
language production. Journal of Psycholinguistic Re-
search, 29(2):205?216.
A. Raux, B. Langner, A. Black, and M Eskenazi. 2005.
Let?s Go public! taking a spoken dialog system to the
real world. In Proceedings of Eurospeech.
E. Reitter, J. Moore, and F. Keller. 2006. Priming of syn-
tactic rules in task-oriented dialogue and spontaneous
conversation. In Proceedings of CogSci.
T. Sheeder and J. Balogh. 2003. Say it like you mean
it: priming for structure in caller responses to a spoken
dialog system. International Journal of Speech Tech-
nology, 6(2):103?111.
R. Smith and S. Gordon. 1997. Effects of variable initia-
tive on linguistic behavior in human-computer spoken
natural language dialogue. Computational Linguistics,
23(1):141?168.
A. Ward and D. Litman. 2007. Automatically measuring
lexical and acoustic/prosodic convergence in tutorial
dialog corpora. In Proceedings of the SLaTE Work-
shop on Speech and Language Technology in Educa-
tion.
192
Proceedings of NAACL HLT 2009: Short Papers, pages 229?232,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Determining the Position of Adverbial Phrases in English
Huayan Zhong and Amanda Stent
Computer Science Department
Stony Brook University
Stony Brook, NY 11794, USA
zhong@cs.sunysb.edu, amanda.stent@stonybrook.edu
Abstract
In this paper we compare three approaches to
adverbial positioning using lexical, syntactic,
semantic and sentence-level features. We find
that: (a), one- and two-stage classification-
based approaches can achieve almost 86% ac-
curacy in determining the absolute position of
adverbials; (b) a classifier trained with only
syntactic features gives performance close to
that of a classifier trained with all features; and
(c) a surface realizer incorporating a two-stage
classifier for adverbial positioning as the sec-
ond stage gives improvements of at least 10%
in simple string accuracy over a baseline real-
izer for sentences containing adverbials.
1 Introduction
The job of a surface realizer is to transform an input
semantic/syntactic form into a sequence of words.
This task includes word choice, and word and con-
stituent ordering. In English, the positions of re-
quired elements of a sentence, verb phrase or noun
phrase are relatively fixed. However, many sen-
tences also include adverbials whose position is not
fixed (Figure 1). There may be several appropriate
positions for an adverbial in a particular context, but
other positions give output that is non-idiomatic or
disfluent, ambiguous, or incoherent.
Some computational research has included mod-
els for adjunct ordering (e.g. (Ringger et al, 2004;
Marciniak and Strube, 2004; Elhadad et al, 2001)).
However, this is the first computational study to look
specifically at adverbials. Adverbial positioning has
long been studied in linguistics (e.g. (Keyser, 1968;
Allen and Cruttenden, 1974; Ernst, 1984; Haider,
2000)). Most linguistic research focuses on whether
adverbial placement is functional or semantic in na-
ture. However, Costa (2004) takes a more flexi-
ble feature-based approach that uses: lexical fea-
tures (e.g. phonological shape, ambiguity of mean-
ing, categorical status); syntactic features (e.g. pos-
sible adjunction sites, directionality of adjunction,
domain of modification); and information structure
features (e.g. focus, contrast). We decided to evalu-
ate Costa?s approach computationally, using features
automatically extracted from an annotated corpus.
In this paper, we compare three approaches to ad-
verbial positioning: a simple baseline approach us-
ing lexical and syntactic features, and one- and two-
stage classification-based approaches using lexical,
syntactic, semantic and sentence-level features. We
apply these approaches in a hybrid surface realizer
that uses a probabilistic grammar to produce real-
ization alternatives, and a second-stage classifier to
select among alternatives. We find that: (a) One-
and two-stage classification-based approaches can
achieve almost 86% accuracy in determining the ab-
solute position of adverbials; (b) A classifier trained
with only syntactic features gives performance close
to that of a classifier trained with all features; and (c)
A surface realizer using a two-stage classifier for ad-
verbial positioning can get improvements of at least
10% in simple string accuracy over a baseline real-
izer for sentences containing adverbials.
As well as being useful for surface realization, a
model of adverbial ordering can be used in machine
translation (e.g. (Ogura et al, 1997)), language
learning software (e.g. (Leacock, 2007; Burstein et
al., 2004)), and automatic summarization (e.g. (El-
hadad et al, 2001; Clarke and Lapata, 2007; Mad-
nani et al, 2007)).
229
Figure 1: Example syntax tree for Then she cashed the
check at your bank on Tuesday with adverbials circled
and possible VP and S adverbial positions in squares.
2 Data and Features
From the sentences in the Wall Street Journal (WSJ)
and Switchboard (SWBD) sections of the Penn
Treebank III (Marcus et al, 1999), we extracted all
NP, PP and ADVP phrases labeled with the adver-
bial tags -BNF, -DIR, -EXT, -LOC, -MNR, -PRP,
-TMP or -ADV. These phrases mostly modify S con-
stituents (including RRC, S, SBAR, SBARQ, SINV,
SQ), VP constituents, or NP constituents (includ-
ing NP and WHNP), but also modify other adjuncts
(PP, ADJP or ADVP) and other phrase types (FRAG,
INTJ, LST, NAC, PRT, QP, TOP, UCP, X).
Corpus Number of adverbials of type:
PP-ADVP NP-ADVP ADVP
WSJ 36128 10587 13700
SWBD 12231 5405 17193
Table 1: Adverbials in the Penn Treebank III
For each adverbial, we automatically extracted
lexical, syntactic, semantic and discourse features.
We included features similar to those in (Costa,
2004) and from our own previous research on prepo-
sitional phrase ordering (Zhong and Stent, 2008).
Due to the size of our data set, we could only use
features that can be extracted automatically, so some
features were approximated. We dropped adverbials
for which we could not get features, such as empty
adverbials. Tables 1 and 2 summarize the resulting
data. A list of the features we used in our classifi-
cation experiment appears in Table 3. We withheld
10% of this data for our realization experiment.
3 Classification Experiment
Our goal is to determine the position of an adverbial
with respect to its siblings in the phrase of which it
Adverbial Data Set
Type WSJ SWBD
S 8196 5144
VP 29734 22845
NP 12985 2071
PP/ADJP/ADVP 1739 987
Other 297 686
Table 2: Adverbials in the Penn Treebank III
is a part. An adverbial may have non-adverbial sib-
lings, whose position is typically fixed. It may also
have other adverbial siblings. In the sentence in Fig-
ure 1, at your bank has one adverbial and two non-
adverbial siblings. If this adverbial were placed at
positions VP:0 or VP:1 the resulting sentence would
be disfluent but meaningful; placed at position VP:2
the resulting sentence is fluent, meaningful and id-
iomatic. (In this sentence, both orderings of the two
adverbials at position VP:2 are valid.)
3.1 Approaches
We experimented with three approaches to adverbial
positioning.
Baseline Our baseline approach has two stages. In
the first stage the position of each adverbial with
respect to its non-adverbial siblings is determined:
each adverbial is assigned the most likely position
given its lexical head and category (PP, NN, ADVP).
In the second stage, the relative ordering of adjacent
adverbials is determined in a pairwise fashion (cf.
(Marciniak and Strube, 2004)): the ordering of a pair
of adverbials is assigned to be the most frequent in
the training data, given the lexical head, adverbial
phrase type, and category of each adverbial.
One-stage For our one-stage classification-based
approach, we determine the position of all adver-
bials in a phrase at one step. There is one feature
vector for each phrase containing at least one adver-
bial. It contains features for all non-adverbial sib-
lings in realization order, and then for each adverbial
sibling in alphabetical order by lexical head. The la-
bel is the order of the siblings. For example, for the
S-modifying adverbial in Figure 1, the label would
be 2 0 1, where 0 = ?she?, 1 = ?cashed? and 2 =
?Then?. If there are n siblings, then there are n!
possible labels for each feature vector, so the perfor-
mance of this classifier by chance would be .167 if
each adverbial has on average three siblings.
230
Type Features
lexical preposition in this adverbial and in adverbial siblings 0-4; stems of lexical heads of this adverbial,
its parent, non-adverbial siblings 0-4, and adverbial siblings 0-4; number of phonemes in lexical
head of this adverbial and in lexical heads of adverbial siblings 0-4; number of words in this
adverbial and in adverbial siblings 0-4
syntactic syntactic categories of this adverbial, its parent, non-adverbial siblings 0-4, and adverbial sib-
lings 0-4; adverbial type of this adverbial and of adverbial siblings 0-4 (one of DIR, EXT, LOC,
MNR, PRP, TMP, ADV); numbers of siblings, non-adverbial siblings, and adverbial siblings
semantic hypernyms of heads of this adverbial, its parent, non-adverbial siblings 0-4, and adverbial sib-
lings 0-4; number of meanings for heads of this adverbial and adverbial siblings 0-4 (using
WordNet)
sentence sequence of children of S node (e.g. NP VP, VP); form of sentence (declarative, imperative,
interrogative, clause-other); presence of the following in the sentence: coordinating conjunc-
tion(s), subordinating conjunction(s), correlative conjunction(s), discourse cue(s) (e.g. ?how-
ever?, ?therefore?), pronoun(s), definite article(s)
Table 3: Features used for determining adverbial positions. We did not find phrases with more than 5 adverbial siblings
or more more than 5 non-adverbial siblings. If a phrase did not have 5 adverbial or non-adverbial siblings, NA values
were used in the features for those siblings.
Two-stage For our two-stage classification-based
approach, we first determine the position of each ad-
verbial in a phrase in relation to its non-adverbial
siblings, and then the relative positions of adjacent
adverbials. For the first stage we use a classifier.
There is one feature vector for each adverbial. It
contains features for all non-adverbial siblings in re-
alization order, then for each adverbial sibling in al-
phabetical order by lexical head, and finally for the
target adverbial itself. The label is the position of
the target adverbial with respect to the non-adverbial
siblings. For our example sentence in Figure 1, the
label for ?Then? would be 0; for ?at the bank? would
be 2, and for ?on Tuesday? would be 2. If there are n
non-adverbial siblings, then there are n+1 possible
labels for each feature vector, so the performance of
this classifier by chance would be .25 if each adver-
bial has on average three non-adverbial siblings.
For the second stage we use the same second stage
as the baseline approach.
3.2 Method
We use 10-fold cross-validation to compute perfor-
mance of each approach. For the classifiers, we used
the J4 decision tree classifier provided by Weka1.
We compute correctness for each approach as the
percentage of adverbials for which the approach out-
puts the same position as that found in the original
1We experimented with logistic regression and SVM classi-
fiers; the decision tree classifier gave the highest performance.
human-produced phrase. (In some cases, multiple
positions for the adverbial would be equally accept-
able, but we cannot evaluate this automatically.)
3.3 Results
Our classification results are shown in Table 4. The
one- and two-stage approaches both significantly
outperform baseline. Also, the two-stage approach
outperforms the one-stage approach for WSJ.
The decision trees using all features are quite
large. We tried dropping feature sets to see if we
could get smaller trees without large drops in per-
formance. We found that for all data sets, the
models containing only syntactic features perform
only about 1% worse for one-stage classification and
only about 3% worse for two-stage classification,
while in most cases giving much smaller trees (1015
[WSJ] and 972 [SWBD] nodes for the one-stage ap-
proach; 1008 [WSJ] and 877 [SWBD] for the two-
stage approach). This is somewhat surprising given
Costa?s arguments about the need for lexical and dis-
course features; it may be due to errors introduced
by approximating discourse features automatically,
as well as to data sparsity in the lexical features.
There are only small performance differences be-
tween the classifiers for speech and those for text.
4 Realization Experiment
To investigate how a model of adverbial position-
ing may improve an NLP application, we incorpo-
231
Approach Tree Classification SSA
size accuracy
WSJ
baseline n/a 45.98 75.1
one-stage 6519 84.43 82.2
two-stage 1053 86.27 85.1
SWBD
baseline n/a 41.48 61.3
one-stage 4486 85.13 74.5
two-stage 3707 85.01 73.1
Table 4: Performance of adverbial position determination
rated our best-performing models into a surface re-
alizer. We automatically extracted a probabilistic
lexicalized tree-adjoining grammar from the whole
WSJ and SWBD corpora minus our held-out data,
using the method described in (Zhong and Stent,
2005). We automatically re-realized all adverbial-
containing sentences in our held-out data (10%), af-
ter first automatically constructing input using the
method described in (Zhong and Stent, 2005).
We compute realization performance using sim-
ple string accuracy (SSA)2. Realization performance
is reported in Table 4. Both classification-based ap-
proaches outperform baseline, with the two-stage
approach performing best for WSJ with either met-
ric (for SWBD, the classification-based approaches
perform similarly).
5 Conclusions and Future Work
In this paper, we tested classification-based ap-
proaches to adverbial positioning. We showed that
we can achieve good results using syntactic features
alone, with small improvements from adding lexi-
cal, semantic and sentence-level features. We also
showed that use of a model for adverbial position-
ing leads to improved surface realization. In future
work, we plan a human evaluation of our results to
see if more features could lead to performance gains.
6 Acknowledgments
This research was partially supported by the NSF
under grant no. 0325188.
2Although in general we do not find SSA to be a reliable
metric for evaluating surface realizers, in this case it is valid
because lexical selection is done already; only the positions of
adverbials will generally be different.
References
D. Allen and A. Cruttenden. 1974. English sentence ad-
verbials: Their syntax and their intonation in British
English. Lingua, 34:1?30.
J. Burstein, M. Chodorow, and C. Leacock. 2004. Auto-
mated essay evaluation: The Criterion online writing
service. AI Magazine, 25(3):27?36.
J. Clarke and M. Lapata. 2007. Modelling compres-
sion with discourse constraints. In Proceedings of
EMNLP/CoNLL.
J. Costa. 2004. A multifactorial approach to adverb
placement: assumptions, facts, and problems. Lingua,
114:711?753.
M. Elhadad, Y. Netzer, R. Barzilay, and K. McKeown.
2001. Ordering circumstantials for multi-document
summarization. In Proceedings of BISFAI.
Thomas Ernst. 1984. Towards an integrated theory of
adverb position in English. Ph.D. thesis, Indiana Uni-
versity, Bloomington, Indiana.
H. Haider. 2000. Adverb placement. Theoretical lin-
guistics, 26:95?134.
J. Keyser. 1968. Adverbial positions in English. Lan-
guage, 44:357?374.
C. Leacock. 2007. Writing English as a second
language: A proofreading tool. In Proceedings of
the Workshop on optimizing the role of language in
technology-enhanced learning.
N. Madnani, D. Zajic, B. Dorr, N. F. Ayan, and J. Lin.
2007. Multiple alternative sentence compressions for
automatic text summarization. In Proceedings of the
Document Understanding Conference.
T. Marciniak and M. Strube. 2004. Classification-
based generation using TAG. In Lecture Notes
in Computer Science, volume 3123/2004. Springer
Berlin/Heidelberg.
M. Marcus, B. Santorini, M. Marcinkiewicz, and A. Tay-
lor. 1999. Treebank-3. Available from the Linguistic
Data Consortium, Catalog Number LDC99T42.
K. Ogura, S. Shirai, and F. Bond. 1997. English ad-
verb processing in Japanese-to-English machine trans-
lation. In Seventh International Conference on Theo-
retical and Methodological Issues in Machine Trans-
lation.
E. Ringger, M. Gamon, R. Moore, D. Rojas, M. Smets,
and S. Corston-Oliver. 2004. Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In Proceedings of COLING.
H. Zhong and A. Stent. 2005. Building surface realiz-
ers automatically from corpora using general-purpose
tools. Proceedings of UCNLG.
H. Zhong and A. Stent. 2008. A corpus-based compari-
son of models for predicting ordering of prepositional
phrases. In submission.
232
Trainable Sentence Planning for Complex Information
Presentation in Spoken Dialog Systems
Amanda Stent
Stony Brook University
Stony Brook, NY 11794
U.S.A.
stent@cs.sunysb.edu
Rashmi Prasad
University of Pennsylvania
Philadelphia, PA 19104
U.S.A.
rjprasad@linc.cis.upenn.edu
Marilyn Walker
University of Sheffield
Sheffield S1 4DP
U.K.
M.A.Walker@sheffield.ac.uk
Abstract
A challenging problem for spoken dialog sys-
tems is the design of utterance generation mod-
ules that are fast, flexible and general, yet pro-
duce high quality output in particular domains.
A promising approach is trainable generation,
which uses general-purpose linguistic knowledge
automatically adapted to the application do-
main. This paper presents a trainable sentence
planner for the MATCH dialog system. We
show that trainable sentence planning can pro-
duce output comparable to that of MATCH?s
template-based generator even for quite com-
plex information presentations.
1 Introduction
One very challenging problem for spoken dialog
systems is the design of the utterance genera-
tion module. This challenge arises partly from
the need for the generator to adapt to many
features of the dialog domain, user population,
and dialog context.
There are three possible approaches to gener-
ating system utterances. The first is template-
based generation, used in most dialog systems
today. Template-based generation enables a
programmer without linguistic training to pro-
gram a generator that can efficiently produce
high quality output specific to different dialog
situations. Its drawbacks include the need to
(1) create templates anew by hand for each ap-
plication; (2) design and maintain a set of tem-
plates that work well together in many dialog
contexts; and (3) repeatedly encode linguistic
constraints such as subject-verb agreement.
The second approach is natural language gen-
eration (NLG), which divides generation into:
(1) text (or content) planning, (2) sentence
planning, and (3) surface realization. NLG
promises portability across domains and dialog
contexts by using general rules for each genera-
tion module. However, the quality of the output
for a particular domain, or a particular dialog
context, may be inferior to that of a template-
based system unless domain-specific rules are
developed or general rules are tuned for the par-
ticular domain. Furthermore, full NLG may be
too slow for use in dialog systems.
A third, more recent, approach is trainable
generation: techniques for automatically train-
ing NLG modules, or hybrid techniques that
adapt NLG modules to particular domains or
user groups, e.g. (Langkilde, 2000; Mellish,
1998; Walker, Rambow and Rogati, 2002).
Open questions about the trainable approach
include (1) whether the output quality is high
enough, and (2) whether the techniques work
well across domains. For example, the training
method used in SPoT (Sentence Planner Train-
able), as described in (Walker, Rambow and Ro-
gati, 2002), was only shown to work in the travel
domain, for the information gathering phase of
the dialog, and with simple content plans in-
volving no rhetorical relations.
This paper describes trainable sentence
planning for information presentation in the
MATCH (Multimodal Access To City Help) di-
alog system (Johnston et al, 2002). We pro-
vide evidence that the trainable approach is
feasible by showing (1) that the training tech-
nique used for SPoT can be extended to a
new domain (restaurant information); (2) that
this technique, previously used for information-
gathering utterances, can be used for infor-
mation presentations, namely recommendations
and comparisons; and (3) that the quality
of the output is comparable to that of a
template-based generator previously developed
and experimentally evaluated with MATCH
users (Walker et al, 2002; Stent et al, 2002).
Section 2 describes SPaRKy (Sentence Plan-
ning with Rhetorical Knowledge), an extension
of SPoT that uses rhetorical relations. SPaRKy
consists of a randomized sentence plan gen-
erator (SPG) and a trainable sentence plan
ranker (SPR); these are described in Sections 3
strategy:recommend
items: Chanpen Thai
relations:justify(nuc:1;sat:2); justify(nuc:1;sat:3); jus-
tify(nuc:1;sat:4)
content: 1. assert(best(Chanpen Thai))
2. assert(has-att(Chanpen Thai, decor(decent)))
3. assert(has-att(Chanpen Thai, service(good))
4. assert(has-att(Chanpen Thai, cuisine(Thai)))
Figure 1: A content plan for a recommendation
for a restaurant in midtown Manhattan
strategy:compare3
items: Above, Carmine?s
relations:elaboration(1;2); elaboration(1;3); elabora-
tion(1,4); elaboration(1,5); elaboration(1,6);
elaboration(1,7); contrast(2;3); contrast(4;5);
contrast(6;7)
content: 1. assert(exceptional(Above, Carmine?s))
2. assert(has-att(Above, decor(good)))
3. assert(has-att(Carmine?s, decor(decent)))
4. assert(has-att(Above, service(good)))
5. assert(has-att(Carmine?s, service(good)))
6. assert(has-att(Above, cuisine(New Ameri-
can)))
7. assert(has-att(Carmine?s, cuisine(italian)))
Figure 2: A content plan for a comparison be-
tween restaurants in midtown Manhattan
and 4. Section 5 presents the results of two
experiments. The first experiment shows that
given a content plan such as that in Figure 1,
SPaRKy can select sentence plans that commu-
nicate the desired rhetorical relations, are sig-
nificantly better than a randomly selected sen-
tence plan, and are on average less than 10%
worse than a sentence plan ranked highest by
human judges. The second experiment shows
that the quality of SPaRKy?s output is compa-
rable to that of MATCH?s template-based gen-
erator. We sum up in Section 6.
2 SPaRKy Architecture
Information presentation in the MATCH sys-
tem focuses on user-tailored recommendations
and comparisons of restaurants (Walker et al,
2002). Following the bottom-up approach to
text-planning described in (Marcu, 1997; Mel-
lish, 1998), each presentation consists of a set of
assertions about a set of restaurants and a spec-
ification of the rhetorical relations that hold be-
tween them. Example content plans are shown
in Figures 1 and 2. The job of the sentence
planner is to choose linguistic resources to real-
ize a content plan and then rank the resulting
alternative realizations. Figures 3 and 4 show
alternative realizations for the content plans in
Figures 1 and 2.
Alt Realization H SPR
2 Chanpen Thai, which is a Thai restau-
rant, has decent decor. It has good
service. It has the best overall quality
among the selected restaurants.
3 .28
5 Since Chanpen Thai is a Thai restau-
rant, with good service, and it has de-
cent decor, it has the best overall qual-
ity among the selected restaurants.
2.5 .14
6 Chanpen Thai, which is a Thai restau-
rant, with decent decor and good ser-
vice, has the best overall quality among
the selected restaurants.
4 .70
Figure 3: Some alternative sentence plan real-
izations for the recommendation in Figure 1. H
= Humans? score. SPR = SPR?s score.
Alt Realization H SPR
11 Above and Carmine?s offer exceptional
value among the selected restaurants.
Above, which is a New American
restaurant, with good decor, has good
service. Carmine?s, which is an Italian
restaurant, with good service, has de-
cent decor.
2 .73
12 Above and Carmine?s offer exceptional
value among the selected restaurants.
Above has good decor, and Carmine?s
has decent decor. Above and Carmine?s
have good service. Above is a New
American restaurant. On the other
hand, Carmine?s is an Italian restau-
rant.
2.5 .50
13 Above and Carmine?s offer exceptional
value among the selected restaurants.
Above is a New American restaurant.
It has good decor. It has good service.
Carmine?s, which is an Italian restau-
rant, has decent decor and good service.
3 .67
20 Above and Carmine?s offer exceptional
value among the selected restaurants.
Carmine?s has decent decor but Above
has good decor, and Carmine?s and
Above have good service. Carmine?s is
an Italian restaurant. Above, however,
is a New American restaurant.
2.5 .49
25 Above and Carmine?s offer exceptional
value among the selected restaurants.
Above has good decor. Carmine?s is
an Italian restaurant. Above has good
service. Carmine?s has decent decor.
Above is a New American restaurant.
Carmine?s has good service.
NR NR
Figure 4: Some of the alternative sentence plan
realizations for the comparison in Figure 2. H
= Humans? score. SPR = SPR?s score. NR =
Not generated or ranked
The architecture of the spoken language gen-
eration module in MATCH is shown in Figure 5.
The dialog manager sends a high-level commu-
nicative goal to the SPUR text planner, which
selects the content to be communicated using a
user model and brevity constraints (see (Walker
Synthesizer
How to Say It
Realizer
Surface
Assigner
Prosody
Speech
  
UTTERANCE
SYSTEM
Sentence
SPUR
Planner
Communicative
DIALOGUE
MANAGER
Goals
Text
Planner
What to Say
Figure 5: A dialog system with a spoken lan-
guage generator
et al, 2002)). The output is a content plan for
a recommendation or comparison such as those
in Figures 1 and 2.
SPaRKy, the sentence planner, gets the con-
tent plan, and then a sentence plan generator
(SPG) generates one or more sentence plans
(Figure 7) and a sentence plan ranker (SPR)
ranks the generated plans. In order for the
SPG to avoid generating sentence plans that are
clearly bad, a content-structuring module first
finds one or more ways to linearly order the in-
put content plan using principles of entity-based
coherence based on rhetorical relations (Knott
et al, 2001). It outputs a set of text plan
trees (tp-trees), consisting of a set of speech
acts to be communicated and the rhetorical re-
lations that hold between them. For example,
the two tp-trees in Figure 6 are generated for
the content plan in Figure 2. Sentence plans
such as alternative 25 in Figure 4 are avoided;
it is clearly worse than alternatives 12, 13 and
20 since it neither combines information based
on a restaurant entity (e.g Babbo) nor on an
attribute (e.g. decor).
The top ranked sentence plan output by the
SPR is input to the RealPro surface realizer
which produces a surface linguistic utterance
(Lavoie and Rambow, 1997). A prosody as-
signment module uses the prior levels of linguis-
tic representation to determine the appropriate
prosody for the utterance, and passes a marked-
up string to the text-to-speech module.
3 Sentence Plan Generation
As in SPoT, the basis of the SPG is a set of
clause-combining operations that operate on tp-
trees and incrementally transform the elemen-
tary predicate-argument lexico-structural rep-
resentations (called DSyntS (Melcuk, 1988))
associated with the speech-acts on the leaves
of the tree. The operations are applied in a
bottom-up left-to-right fashion and the result-
ing representation may contain one or more sen-
tences. The application of the operations yields
two parallel structures: (1) a sentence plan
tree (sp-tree), a binary tree with leaves labeled
by the assertions from the input tp-tree, and in-
terior nodes labeled with clause-combining op-
erations; and (2) one or more DSyntS trees
(d-trees) which reflect the parallel operations
on the predicate-argument representations.
We generate a random sample of possible
sentence plans for each tp-tree, up to a pre-
specified number of sentence plans, by ran-
domly selecting among the operations accord-
ing to a probability distribution that favors pre-
ferred operations1. The choice of operation is
further constrained by the rhetorical relation
that relates the assertions to be combined, as
in other work e.g. (Scott and de Souza, 1990).
In the current work, three RST rhetorical rela-
tions (Mann and Thompson, 1987) are used in
the content planning phase to express the rela-
tions between assertions: the justify relation
for recommendations, and the contrast and
elaboration relations for comparisons. We
added another relation to be used during the
content-structuring phase, called infer, which
holds for combinations of speech acts for which
there is no rhetorical relation expressed in the
content plan, as in (Marcu, 1997). By explicitly
representing the discourse structure of the infor-
mation presentation, we can generate informa-
tion presentations with considerably more inter-
nal complexity than those generated in (Walker,
Rambow and Rogati, 2002) and eliminate those
that violate certain coherence principles, as de-
scribed in Section 2.
The clause-combining operations are general
operations similar to aggregation operations
used in other research (Rambow and Korelsky,
1992; Danlos, 2000). The operations and the
1Although the probability distribution here is hand-
crafted based on assumed preferences for operations such
as merge, relative-clause and with-reduction, it
might also be possible to learn this probability distribu-
tion from the data by training in two phases.
nucleus:<3>assert-com-decor
contrast
nucleus:<2>assert-com-decor nucleus:<6>assert-com-cuisine
nucleus:<7>assert-com-cuisine
contrast
nucleus:<4>assert-com-service
nucleus:<5>assert-com-service
contrast
elaboration
nucleus:<1>assert-com-list_exceptional infer
nucleus:<3>assert-com-decor
nucleus:<5>assert-com-service
nucleus:<7>assert-com-cuisine
inferinfer
nucleus:<2>assert-com-decor nucleus:<6>assert-com-cuisine
nucleus:<4>assert-com-service
elaboration
nucleus:<1>assert-com-list_exceptional contrast
Figure 6: Two tp-trees for alternative 13 in Figure 4.
constraints on their use are described below.
merge applies to two clauses with identical
matrix verbs and all but one identical argu-
ments. The clauses are combined and the non-
identical arguments coordinated. For example,
merge(Above has good service;Carmine?s has
good service) yields Above and Carmine?s have
good service. merge applies only for the rela-
tions infer and contrast.
with-reduction is treated as a kind of
?verbless? participial clause formation in which
the participial clause is interpreted with the
subject of the unreduced clause. For exam-
ple, with-reduction(Above is a New Amer-
ican restaurant;Above has good decor) yields
Above is a New American restaurant, with good
decor. with-reduction uses two syntactic
constraints: (a) the subjects of the clauses must
be identical, and (b) the clause that under-
goes the participial formation must have a have-
possession predicate. In the example above, for
instance, the Above is a New American restau-
rant clause cannot undergo participial forma-
tion since the predicate is not one of have-
possession. with-reduction applies only for
the relations infer and justify.
relative-clause combines two clauses with
identical subjects, using the second clause to
relativize the first clause?s subject. For ex-
ample, relative-clause(Chanpen Thai is a
Thai restaurant, with decent decor and good ser-
vice;Chanpen Thai has the best overall quality
among the selected restaurants) yields Chanpen
Thai, which is a Thai restaurant, with decent
decor and good service, has the best overall qual-
ity among the selected restaurants. relative-
clause also applies only for the relations infer
and justify.
cue-word inserts a discourse connective
(one of since, however, while, and, but, and on
the other hand), between the two clauses to be
combined. cue-word conjunction combines
two distinct clauses into a single sentence with a
coordinating or subordinating conjunction (e.g.
Above has decent decor BUT Carmine?s has
good decor), while cue-word insertion inserts
a cue word at the start of the second clause, pro-
ducing two separate sentences (e.g. Carmine?s
is an Italian restaurant. HOWEVER, Above
is a New American restaurant). The choice of
cue word is dependent on the rhetorical relation
holding between the clauses.
Finally, period applies to two clauses to be
treated as two independent sentences.
Note that a tp-tree can have very different
realizations, depending on the operations of the
SPG. For example, the second tp-tree in Fig-
ure 6 yields both Alt 11 and Alt 13 in Figure 4.
However, Alt 13 is more highly rated than Alt
11. The sp-tree and d-tree produced by the SPG
for Alt 13 are shown in Figures 7 and 8. The
composite labels on the interior nodes of the sp-
PERIOD_elaboration
PERIOD_contrast
RELATIVE_CLAUSE_inferPERIOD_infer
PERIOD_infer <4>assert-com-service <7>assert-com-cuisine MERGE_infer
<3>assert-come-decor <5>assert-com-service<2>assert-com-decor<6>assert-com-cuisine
<1>assert-com-list_exceptional
Figure 7: Sentence plan tree (sp-tree) for alternative 13 in Figure 4
offer
exceptional
among
restaurant
selected
Above_and_Carmine?s
Carmine?s
BE3
restaurantCarmine?s
Italian
decor
decent AND2
service
good
HAVE1
PERIOD
New_American
BE3
Above Above decor
good
HAVE1
restaurant
Above
good
HAVE1
service
PERIOD
PERIOD
value
PERIOD
Figure 8: Dependency tree (d-tree) for alternative 13 in Figure 4
tree indicate the clause-combining relation se-
lected to communicate the specified rhetorical
relation. The d-tree for Alt 13 in Figure 8 shows
that the SPG treats the period operation as
part of the lexico-structural representation for
the d-tree. After sentence planning, the d-tree
is split into multiple d-trees at period nodes;
these are sent to the RealPro surface realizer.
Separately, the SPG also handles referring ex-
pression generation by converting proper names
to pronouns when they appear in the previous
utterance. The rules are applied locally, across
adjacent sequences of utterances (Brennan et
al., 1987). Referring expressions are manipu-
lated in the d-trees, either intrasententially dur-
ing the creation of the sp-tree, or intersenten-
tially, if the full sp-tree contains any period op-
erations. The third and fourth sentences for Alt
13 in Figure 4 show the conversion of a named
restaurant (Carmine?s) to a pronoun.
4 Training the Sentence Plan
Ranker
The SPR takes as input a set of sp-trees gener-
ated by the SPG and ranks them. The SPR?s
rules for ranking sp-trees are learned from a la-
beled set of sentence-plan training examples us-
ing the RankBoost algorithm (Schapire, 1999).
Examples and Feedback: To apply Rank-
Boost, a set of human-rated sp-trees are en-
coded in terms of a set of features. We started
with a set of 30 representative content plans for
each strategy. The SPG produced as many as 20
distinct sp-trees for each content plan. The sen-
tences, realized by RealPro from these sp-trees,
were then rated by two expert judges on a scale
from 1 to 5, and the ratings averaged. Each sp-
tree was an example input for RankBoost, with
each corresponding rating its feedback.
Features used by RankBoost: RankBoost
requires each example to be encoded as a set of
real-valued features (binary features have val-
ues 0 and 1). A strength of RankBoost is that
the set of features can be very large. We used
7024 features for training the SPR. These fea-
tures count the number of occurrences of certain
structural configurations in the sp-trees and the
d-trees, in order to capture declaratively de-
cisions made by the randomized SPG, as in
(Walker, Rambow and Rogati, 2002). The fea-
tures were automatically generated using fea-
ture templates. For this experiment, we use
two classes of feature: (1) Rule-features: These
features are derived from the sp-trees and repre-
sent the ways in which merge, infer and cue-
word operations are applied to the tp-trees.
These feature names start with ?rule?. (2) Sent-
features: These features are derived from the
DSyntSs, and describe the deep-syntactic struc-
ture of the utterance, including the chosen lex-
emes. As a result, some may be domain specific.
These feature names are prefixed with ?sent?.
We now describe the feature templates used
in the discovery process. Three templates were
used for both sp-tree and d-tree features; two
were used only for sp-tree features. Local feature
templates record structural configurations local
to a particular node (its ancestors, daughters
etc.). Global feature templates, which are used
only for sp-tree features, record properties of the
entire sp-tree. We discard features that occur
fewer than 10 times to avoid those specific to
particular text plans.
Strategy System Min Max Mean S.D.
Recommend SPaRKy 2.0 5.0 3.6 .71
HUMAN 2.5 5.0 3.9 .55
RANDOM 1.5 5.0 2.9 .88
Compare2 SPaRKy 2.5 5.0 3.9 .71
HUMAN 2.5 5.0 4.4 .54
RANDOM 1.0 5.0 2.9 1.3
Compare3 SPaRKy 1.5 4.5 3.4 .63
HUMAN 3.0 5.0 4.0 .49
RANDOM 1.0 4.5 2.7 1.0
Table 1: Summary of Recommend, Compare2
and Compare3 results (N = 180)
There are four types of local feature
template: traversal features, sister features,
ancestor features and leaf features. Local
feature templates are applied to all nodes in a
sp-tree or d-tree (except that the leaf feature is
not used for d-trees); the value of the resulting
feature is the number of occurrences of the
described configuration in the tree. For each
node in the tree, traversal features record the
preorder traversal of the subtree rooted at
that node, for all subtrees of all depths. An
example is the feature ?rule traversal assert-
com-list exceptional? (with value 1) of the
tree in Figure 7. Sister features record all
consecutive sister nodes. An example is the fea-
ture ?rule sisters PERIOD infer RELATIVE
CLAUSE infer? (with value 1) of the
tree in Figure 7. For each node in the
tree, ancestor features record all the ini-
tial subpaths of the path from that node
to the root. An example is the feature
?rule ancestor PERIOD contrast*PERIOD
infer? (with value 1) of the tree in Figure 7.
Finally, leaf features record all initial substrings
of the frontier of the sp-tree. For example, the
sp-tree of Figure 7 has value 1 for the feature
?leaf #assert-com-list exceptional#assert-com-
cuisine?.
Global features apply only to the sp-
tree. They record, for each sp-tree and for
each clause-combining operation labeling a non-
frontier node, (1) the minimal number of leaves
dominated by a node labeled with that op-
eration in that tree (MIN); (2) the maximal
number of leaves dominated by a node la-
beled with that operation (MAX); and (3)
the average number of leaves dominated by
a node labeled with that operation (AVG).
For example, the sp-tree in Figure 7 has
value 3 for ?PERIOD infer max?, value 2 for
?PERIOD infer min? and value 2.5 for ?PE-
RIOD infer avg?.
5 Experimental Results
We report two sets of experiments. The first ex-
periment tests the ability of the SPR to select a
high quality sentence plan from a population of
sentence plans randomly generated by the SPG.
Because the discriminatory power of the SPR is
best tested by the largest possible population of
sentence plans, we use 2-fold cross validation for
this experiment. The second experiment com-
pares SPaRKy to template-based generation.
Cross Validation Experiment: We re-
peatedly tested SPaRKy on the half of the cor-
pus of 1756 sp-trees held out as test data for
each fold. The evaluation metric is the human-
assigned score for the variant that was rated
highest by SPaRKy for each text plan for each
task/user combination. We evaluated SPaRKy
on the test sets by comparing three data points
for each text plan: HUMAN (the score of the
top-ranked sentence plan); SPARKY (the score
of the SPR?s selected sentence); and RANDOM
(the score of a sentence plan randomly selected
from the alternate sentence plans).
We report results separately for comparisons
between two entities and among three or more
entities. These two types of comparison are gen-
erated using different strategies in the SPG, and
can produce text that is very different both in
terms of length and structure.
Table 1 summarizes the difference between
SPaRKy, HUMAN and RANDOM for recom-
mendations, comparisons between two entities
and comparisons between three or more enti-
ties. For all three presentation types, a paired
t-test comparing SPaRKy to HUMAN to RAN-
DOM showed that SPaRKy was significantly
better than RANDOM (df = 59, p < .001) and
significantly worse than HUMAN (df = 59, p
< .001). This demonstrates that the use of a
trainable sentence planner can lead to sentence
plans that are significantly better than baseline
(RANDOM), with less human effort than pro-
gramming templates.
Comparison with template generation:
For each content plan input to SPaRKy, the
judges also rated the output of a template-
based generator for MATCH. This template-
based generator performs text planning and sen-
tence planning (the focus of the current pa-
per), including some discourse cue insertion,
clause combining and referring expression gen-
eration; the templates themselves are described
in (Walker et al, 2002). Because the templates
are highly tailored to this domain, this genera-
tor can be expected to perform well. Example
template-based and SPaRKy outputs for a com-
parison between three or more items are shown
in Figure 9.
Strategy System Min Max Mean S.D.
Recommend Template 2.5 5.0 4.22 0.74
SPaRKy 2.5 4.5 3.57 0.59
HUMAN 4.0 5.0 4.37 0.37
Compare2 Template 2.0 5.0 3.62 0.75
SPaRKy 2.5 4.75 3.87 0.52
HUMAN 4.0 5.0 4.62 0.39
Compare3 Template 1.0 5.0 4.08 1.23
SPaRKy 2.5 4.25 3.375 0.38
HUMAN 4.0 5.0 4.63 0.35
Table 2: Summary of template-based genera-
tion results. N = 180
Table 2 shows the mean HUMAN scores for
the template-based sentence planning. A paired
t-test comparing HUMAN and template-based
scores showed that HUMAN was significantly
better than template-based sentence planning
only for compare2 (df = 29, t = 6.2, p < .001).
The judges evidently did not like the template
for comparisons between two items. A paired
t-test comparing SPaRKy and template-based
sentence planning showed that template-based
sentence planning was significantly better than
SPaRKy only for recommendations (df = 29, t
= 3.55, p < .01). These results demonstrate
that trainable sentence planning shows promise
for producing output comparable to that of a
template-based generator, with less program-
ming effort and more flexibility.
The standard deviation for all three template-
based strategies was wider than for HUMAN
or SPaRKy, indicating that there may be
content-specific aspects to the sentence plan-
ning done by SPaRKy that contribute to out-
put variation. The data show this to be cor-
rect; SPaRKy learned content-specific prefer-
ences about clause combining and discourse cue
insertion that a template-based generator can-
System Realization H
Template Among the selected restaurants, the fol-
lowing offer exceptional overall value.
Uguale?s price is 33 dollars. It has good
decor and very good service. It?s a
French, Italian restaurant. Da Andrea?s
price is 28 dollars. It has good decor and
very good service. It?s an Italian restau-
rant. John?s Pizzeria?s price is 20 dollars.
It has mediocre decor and decent service.
It?s an Italian, Pizza restaurant.
4.5
SPaRKy Da Andrea, Uguale, and John?s Pizze-
ria offer exceptional value among the se-
lected restaurants. Da Andrea is an Ital-
ian restaurant, with very good service, it
has good decor, and its price is 28 dol-
lars. John?s Pizzeria is an Italian , Pizza
restaurant. It has decent service. It has
mediocre decor. Its price is 20 dollars.
Uguale is a French, Italian restaurant,
with very good service. It has good decor,
and its price is 33 dollars.
4
Figure 9: Comparisons between 3 or more
items, H = Humans? score
not easily model, but that a trainable sentence
planner can. For example, Table 3 shows the
nine rules generated on the first test fold which
have the largest negative impact on the final
RankBoost score (above the double line) and
the largest positive impact on the final Rank-
Boost score (below the double line), for com-
parisons between three or more entities. The
rule with the largest positive impact shows that
SPaRKy learned to prefer that justifications in-
volving price be merged with other information
using a conjunction.
These rules are also specific to presentation
type. Averaging over both folds of the exper-
iment, the number of unique features appear-
ing in rules is 708, of which 66 appear in the
rule sets for two presentation types and 9 ap-
pear in the rule sets for all three presentation
types. There are on average 214 rule features,
428 sentence features and 26 leaf features. The
majority of the features are ancestor features
(319) followed by traversal features (264) and
sister features (60). The remainder of the fea-
tures (67) are for specific lexemes.
To sum up, this experiment shows that the
ability to model the interactions between do-
main content, task and presentation type is a
strength of the trainable approach to sentence
planning.
6 Conclusions
This paper shows that the training technique
used in SPoT can be easily extended to a new
N Condition ?s
1 sent anc PROPERNOUN RESTAURANT
*HAVE1 ? 16.5
-0.859
2 sent anc II Upper East Side*ATTR IN1*
locate ? 4.5
-0.852
3 sent anc PERIOD infer*PERIOD infer
*PERIOD elaboration ? -?
-0.542
4 rule anc assert-com-service*MERGE infer
? 1.5
-0.356
5 sent tvl depth 0 BE3 ? 4.5 -0.346
6 rule anc PERIOD infer*PERIOD infer
*PERIOD elaboration ? -?
-0.345
7 rule anc assert-com-decor*PERIOD infer
*PERIOD infer*PERIOD contrast *PE-
RIOD elaboration? -?
-0.342
8 rule anc assert-com-food quality*MERGE
infer ? 1.5
0.398
9 rule anc assert-com-price*CW
CONJUNCTION infer*PERIOD justify
? -?
0.527
Table 3: The nine rules generated on the first
test fold which have the largest negative impact
on the final RankBoost score (above the dou-
ble line) and the largest positive impact on the
final RankBoost score (below the double line),
for Compare3. ?s represents the increment or
decrement associated with satisfying the condi-
tion.
domain and used for information presentation
as well as information gathering. Previous work
on SPoT also compared trainable sentence plan-
ning to a template-based generator that had
previously been developed for the same appli-
cation (Rambow et al, 2001). The evalua-
tion results for SPaRKy (1) support the results
for SPoT, by showing that trainable sentence
generation can produce output comparable to
template-based generation, even for complex in-
formation presentations such as extended com-
parisons; (2) show that trainable sentence gen-
eration is sensitive to variations in domain ap-
plication, presentation type, and even human
preferences about the arrangement of particu-
lar types of information.
7 Acknowledgments
We thank AT&T for supporting this research,
and the anonymous reviewers for their helpful
comments on this paper.
References
I. Langkilde. Forest-based statistical sentence gen-
eration. In Proc. NAACL 2000, 2000.
S. E. Brennan, M. Walker Friedman, and C. J. Pol-
lard. A centering approach to pronouns. In Proc.
25th Annual Meeting of the ACL, Stanford, pages
155?162, 1987.
L. Danlos. 2000. G-TAG: A lexicalized formal-
ism for text generation inspired by tree ad-
joining grammar. In Tree Adjoining Grammars:
Formalisms, Linguistic Analysis, and Processing.
CSLI Publications.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker, and P. Mal-
oor. MATCH: An architecture for multimodal di-
alogue systems. In Annual Meeting of the ACL,
2002.
A. Knott, J. Oberlander, M. O?Donnell and C. Mel-
lish. Beyond Elaboration: the interaction of rela-
tions and focus in coherent text. In Text Repre-
sentation: linguistic and psycholinguistic aspects,
pages 181-196, 2001.
B. Lavoie and O. Rambow. A fast and portable re-
alizer for text generation systems. In Proc. of the
3rd Conference on Applied Natural Language Pro-
cessing, ANLP97, pages 265?268, 1997.
W.C. Mann and S.A. Thompson. Rhetorical struc-
ture theory: A framework for the analysis of texts.
Technical Report RS-87-190, USC/Information
Sciences Institute, 1987.
D. Marcu. From local to global coherence: a
bottom-up approach to text planning. In Proceed-
ings of the National Conference on Artificial In-
telligence (AAAI?97), 1997.
C. Mellish, A. Knott, J. Oberlander, and M.
O?Donnell. Experiments using stochastic search
for text planning. In Proceedings of INLG-98.
1998.
I. A. Melc?uk. Dependency Syntax: Theory and Prac-
tice. SUNY, Albany, New York, 1988.
O. Rambow and T. Korelsky. Applied text genera-
tion. In Proceedings of the Third Conference on
Applied Natural Language Processing, ANLP92,
pages 40?47, 1992.
O. Rambow, M. Rogati and M. A. Walker. Evalu-
ating a Trainable Sentence Planner for a Spoken
Dialogue Travel System In Meeting of the ACL,
2001.
R. E. Schapire. A brief introduction to boosting. In
Proc. of the 16th IJCAI, 1999.
D. R. Scott and C. Sieckenius de Souza. Getting
the message across in RST-based text generation.
In Current Research in Natural Language Gener-
ation, pages 47?73, 1990.
A. Stent, M. Walker, S. Whittaker, and P. Maloor.
User-tailored generation for spoken dialogue: An
experiment. In Proceedings of ICSLP 2002., 2002.
M. A. Walker, S. J. Whittaker, A. Stent, P. Mal-
oor, J. D. Moore, M. Johnston, and G. Vasireddy.
Speech-Plans: Generating evaluative responses
in spoken dialogue. In Proceedings of INLG-02.,
2002.
M. Walker, O. Rambow, and M. Rogati. Training a
sentence planner for spoken dialogue using boost-
ing. Computer Speech and Language: Special Is-
sue on Spoken Language Generation, 2002.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 85?88, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Two diverse systems built using
generic components for spoken dialogue
(Recent Progress on TRIPS)
James Allen, George Ferguson, Mary Swift, Amanda Stent, Scott Stoness, 
Lucian Galescu, Nathan Chambers, Ellen Campana, and Gregory Aist
University of Rochester
Computer Science Department
UR Comp Sci RC 270226
Rochester NY 14627 USA
{james, ferguson, swift, stoness,
campana, gaist}
@cs.rochester.edu
Institute for
Human and Machine Cognition
40 South Alcaniz St.
Pensacola FL 32502
{lgalescu,nchambers}@ihmc.us
State University of New York at
Stony Brook
1418 Computer Science
Stony Brook University
Stony Brook NY 11794 USA
stent@cs.sunysb.edu
Abstract
This  paper  describes  recent  progress  on  the
TRIPS architecture for developing spoken-lan-
guage dialogue systems.  The interactive poster
session will include demonstrations of two sys-
tems built using TRIPS: a computer purchas-
ing assistant, and an object placement (and ma-
nipulation) task.
1 Introduction
Building a robust spoken dialogue system for a new
task currently requires considerable effort,  includ-
ing  extensive  data  collection,  grammar  develop-
ment, and building a dialogue manager that drives
the  system using its  "back-end" application (e.g.
database query, planning and scheduling). We de-
scribe progress in an effort to build a generic dia-
logue system that  can be rapidly customized to a
wide range of different types of applications, pri-
marily  by  defining a  domain-specific  task  model
and the interfaces to the back-end systems. This is
achieved by  using generic  components  (i.e.,  ones
that apply in any practical domain) for all stages of
understanding  and developing techniques for rapid-
ly customizing the generic components to new do-
mains  (e.g.  Aist,  Allen,  and  Galescu  2004).  To
achieve this goal we have made several innovations,
including (1) developing domain independent mod-
els of  semantic and  contextual  interpretation,  (2)
developing generic  dialogue  management  compo-
nents based on an abstract  model of collaborative
problem solving, and (3) extensively using an ontol-
ogy-mapping system that connects the domain inde-
pendent representations to the representations/query
languages used by the back-end applications,  and
which is used to automatically optimize the perfor-
mance of the system in the specific domain.
2 Theoretical  Underpinnings:  The Prob-
lem-Solving Model of Dialogue
While many have observed that communication
is a specialized form of joint action that happens to
involve language and that dialogue can be viewed
as collaborative problem solving, very few imple-
mented systems have been explicitly based on these
ideas. Theories of speech act interpretation as inten-
tion recognition have been developed  (including ex-
tensive  prior  work  in  TRIPS'  predecessor,  the
TRAINS project), but have been generally consid-
ered impractical for actual systems.  Planning mod-
els  have been more successful  on the  generation
side, and some systems have used the notion of exe-
cuting explicit task models to track and drive the in-
teractions  (e.g.,  Sidner  and  Rich's  COLLAGEN
framework). But collaborative problem solving, and
dialogue in general, is much more general than exe-
cuting tasks. In our applications, in addition to exe-
cuting tasks, we see dialogue that is used to define
the task (i.e., collaborative planning), evaluate the
task (e.g., estimating how long it will take,  com-
paring options,  or  likely effects),    debug a  task
(e.g., identifying and discussing problems and how
to remedy them), learn new tasks (e.g., by demon-
stration and instruction).
85
In the remainder of the paper, we'll first discuss
the methods we've developed for building dialogue
systems using generic components.  We'll then de-
scribe two systems implemented using the TRIPS
architecture that we will demonstrate at the interac-
tive poster session.
3 Generic Methods:  Ontology Mappings
and Collaborative Problem Solving
The goal of our work is to develop generic spoken
dialogue technology that can be rapidly customized
to new applications, tasks and domains. To do this,
we have developed generic domain independent rep-
resentations not only of sentence meaning but also
of the collaborative actions that are performed by
the speech acts as one engages in dialogue. Further-
more, we need to be able to easily connect these
generic representations to a wide range of different
domain specific task models and applications, rang-
ing from data base query systems to state-of-the-art
planning and scheduling systems.  This  paper  de-
scribes  the  approach  we  have  developed  in  the
TRIPS system. TRIPS is now being used in a wide
range of diverse applications, from interactive plan-
ning (e.g., developing evacuation plans), advice giv-
ing  (e.g.,  a  medication  advisor  (Ferguson  et  al.
2002)),  controlling teams of robots,   collaborative
assistance (e.g., an assistant that can help you pur-
chase a computer, as described in this paper), sup-
porting human learning, and most recently having
the computer  learn (or  be  taught)  tasks,  such as
learning to perform tasks on the web.  Even though
the tasks and domains differ dramatically, these ap-
plications use the same set of core understanding
components. 
The key to supporting such a range of tasks and ap-
plications is the use of a general ontology-mapping
system. This allows the developer to express a set
of mapping rules that translate the generic knowl-
edge representation into the specific representations
used by the back-end applications (called the KR
representation).   In  order  to  support  generic dis-
course processing, we represent these mappings as
a chain of simpler transformations. These represen-
tations are thus transformed in several stages. The
first,  using the ontology mapping rules,  maps the
LF representation into an intermediary representa-
tion (AKRL - the abstract KR language) that has a
generic syntax  but  whose content is  expressed in
terms of the KR ontology. The second stage is a
syntactic transformation that occurs at the time that
calls to the back-end applications actually occur so
that  interactions  occur  in  the  representations  the
back-end expects.   In  addition to  using ontology
mapping to  deal  with the representational  issues,
TRIPS is unique in that it uses a generic model of
collaborative problem solving to drive the dialogue
itself  (e.g.  Allen,  Blaylock,  and  Ferguson 2002).
This model forms the basis of a generic component
(the collaboration manager) that supports both in-
tention recognition to identify the intended speech
acts and their content, planning the system's actions
to respond to the user (or that take initiative), and
providing utterance realization goals to the genera-
tion system. To develop this, we have been develop-
ing  a  generic  ontology  of  collaborative  problem
solving acts, which provide the framework for man-
aging  the  dialogue.  The  collaboration  manager
queries a domain-specific task component in order
to  make  decisions  about  interpretations  and  re-
sponses.
4 TRIPS  Spoken  Dialogue  Interface  to
the CALO Purchasing Assistant 
The CALO project is a large multisite effort which
aims  at  building  a  computerized  assistant  that
learns how to help you with day-to-day tasks. The
overarching goal of the CALO project is to 
... create cognitive software systems, that is,
systems that can reason, learn from experi-
ence, be told what to do, explain what they
are doing, reflect on their experience, and re-
spond robustly to surprise (Mark and Per-
rault 2004). 
Within this broad mandate, one of our current areas
of focus is user-system dialogue regarding the task
of purchasing - including eliciting user needs, de-
scribing possibilities, and reviewing & finalizing a
purchase  decision.  (Not  necessarily  as  discrete
stages; these elements may be interleaved as appro-
priate for the specific item(s) and setting.)  Within
the purchasing domain,  we began with computer
purchasing and have branched out to other equip-
ment such as projectors.
How to help with purchasing? The family of tasks
involving purchasing items online, regardless of the
type of item, have a  number of elements in com-
mon. The process of purchasing has some common
86
dialogue elements - reporting on the range of fea-
tures  available,  allowing the user  to specify con-
straints, and so forth.  Also, regarding the goal that
must be reached at the end of the task, the eventual
item must:
Meet requirements.  The item needs to meet some
sort of user expectations. This could be as arbitrary
as a specific part number, or as compositional - and
amenable to machine understanding -  as  a  set  of
physical  dimensions (length,  width,  height,  mass,
etc.) 
Be approved. Either the system will have the au-
thority to approve it (cf. Amazon's one-click order-
ing system), or more commonly the user will review
and confirm the purchase. In an office environment
the approval process may extend to include review
by a supervisor, such as might happen with an item
costing over (say) $1000. 
Be available. (At  one time a  certain  electronics
store in California had the habit of leaving out floor
models of laptops beyond the point where any were
actually available for sale.  (Perhaps to entice the
unwitting customer into an ?upsale?, that is, buying
a  similar  but  more  expensive  computer.))  On  a
more serious note, computer specifications change
rapidly, and so access to online information about
available  computers  (provided  by  other  research
within CALO) would be important in order to en-
sure that the user can actually order the machine he
or she has indicated a preference for.  
At  the interactive poster  session,  we will demon-
strate some of the current spoken dialogue capabili-
ty related to the CALO task of purchasing equip-
ment.  We will demonstrate a number of the aspects
of the system such as initiating a conversation, dis-
cussing specific requirements,  presenting possible
equipment to purchase,  system-initiated reminders
to ask for supervisor approval for large purchases,
and finalizing a decision to purchase. 
Figure 1. Fruit carts display.
87
5 TRIPS  Spoken  Dialogue  Interface  to
choosing,  placing,  painting,  rotating,
and filling (virtual) fruit carts
TRIPS is versatile in its applications, as we've said
previously.  We hope to also demonstrate an inter-
face to  a  system for  using spoken commands to
modifying, manipulating, and placing objects on a
computer-displayed map.  This  system (aka  ?fruit
carts?)  extends  the  TRIPS  architecture  into  the
realm of continuous understanding.  That is, when
state-of-the-art  dialogue systems listen,  they typi-
cally wait for the end of the utterance before decid-
ing what to do.  People on the other hand do not
wait in this way ? they can act on partial informa-
tion as  it  becomes available.   A classic example
comes  from  M.  Tanenhaus  and  colleagues  at
Rochester: when presented with several objects of
various colors and told to ?click on the yel-?, people
will already tend to be looking relatively more at the
yellow object(s) even before the word ?yellow? has
been completed.  To achieve this type of interactivi-
ty with a dialogue system ? at least at the level of
two or three words at a time, if not parts of words ?
imposes some interesting challenges. For example:
1. Information must flow asynchronously between
dialogue components, so that actions can be trig-
gered based on partial utterances even while the
understanding continues
2. There must be reasonable representations of in-
complete information ? not just ?incomplete sen-
tence?,  but  specifying what  is  present  already
and perhaps what may potentially follow
3. Speech  recognition,  utterance  segmentation,
parsing, interpretation, discourse reasoning, and
actions must all be able to happen in real time
The fruit carts system consists of two main compo-
nents:  first,  a  graphical  interface implemented on
Windows  2000  using  the  .NET  framework,  and
connected to  a  high-quality  eyetracker;  second,  a
TRIPS-driven spoken dialogue interface implement-
ed primarily in LISP.   The actions in this domain
are as follows:
1. Select an object (?take the large plain square?)
2. Move it (?move it to central park?)
3. Rotate  it  (?and then turn  it  left  a  bit  ?  that's
good?)
4. Paint it (?and that one needs to be purple?)
5. Fill it (?and there's a grapefruit inside it?)
Figure 1 shows an example screenshot from the
fruit carts visual display. The natural language in-
teraction  is  designed to  handle  various  ways  of
speaking,  including conventional  definite  descrip-
tions (?move the large square to central park?) and
more interactive language such as (?up towards the
flag pole ? right a bit ? more ? um- stop there.?)
6 Conclusion
In this brief paper,  we have described some of
the recent progress on the TRIPS platform.  In par-
ticular we have focused on two systems developed
in TRIPS: a spoken dialogue interface to a mixed-
initiative purchasing assistant, and a spoken inter-
face for exploring continuous understanding in an
object-placement task.  In  both  cases  the  systems
make use of reusable components ? for input and
output  such as  parsing and speech synthesis,  and
also for dialogue functionality such as mapping be-
tween language,  abstract  semantics,  and  specific
representations for each domain.
References 
Aist,  G.  2004.  Speech,  gaze,  and  mouse  data  from
choosing,  placing,  painting,  rotating,  and  filling
(virtual) vending carts. International Committee for
Co-ordination  and  Standardisation  of  Speech
Databases  (COCOSDA)  2004  Workshop,  Jeju  Is-
land, Korea, October 4, 2004. 
Aist, G.S., Allen, J., and Galescu, L. 2004. Expanding
the linguistic coverage of a spoken dialogue system
by mining human-human dialogue for new sentences
with familiar meanings. Member Abstract, 26th An-
nual  Meeting  of  the  Cognitive  Science  Society,
Chicago, August 5-7, 2004. 
James Allen, Nate Blaylock, and George Ferguson. A
problem-solving model for collaborative agents.  In
First International Joint Conference on Autonomous
Agents and Multiagent Systems, Bologna, Italy, July
15-19 2002. 
George  Ferguson,  James  F.  Allen,  Nate  J.  Blaylock,
Donna K. Byron, Nate W. Chambers, Myrsolava O.
Dzikovska, Lucian Galescu, Xipeng Shen, Robert S.
Swier, and Mary D. Swift.  The Medication Advisor
Project: Preliminary Report, Technical Report 776,
Computer  Science  Dept.,  University  of  Rochester,
May 2002. 
Mark,  B.,  and  Perrault,  R.  (principal  investigators).
2004.  Website for Cognitive Assistant  that  Learns
and Organizes. http://www.ai.sri.com/project/CALO
88
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 201?208,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning the Structure of Task-driven Human-Human Dialogs
Srinivas Bangalore
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
srini@research.att.com
Giuseppe Di Fabbrizio
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
pino@research.att.com
Amanda Stent
Dept of Computer Science
Stony Brook University
Stony Brook, NY
stent@cs.sunysb.edu
Abstract
Data-driven techniques have been used
for many computational linguistics tasks.
Models derived from data are generally
more robust than hand-crafted systems
since they better reflect the distribution
of the phenomena being modeled. With
the availability of large corpora of spo-
ken dialog, dialog management is now
reaping the benefits of data-driven tech-
niques. In this paper, we compare two ap-
proaches to modeling subtask structure in
dialog: a chunk-based model of subdialog
sequences, and a parse-based, or hierarchi-
cal, model. We evaluate these models us-
ing customer agent dialogs from a catalog
service domain.
1 Introduction
As large amounts of language data have become
available, approaches to sentence-level process-
ing tasks such as parsing, language modeling,
named-entity detection and machine translation
have become increasingly data-driven and empiri-
cal. Models for these tasks can be trained to cap-
ture the distributions of phenomena in the data
resulting in improved robustness and adaptabil-
ity. However, this trend has yet to significantly
impact approaches to dialog management in dia-
log systems. Dialog managers (both plan-based
and call-flow based, for example (Di Fabbrizio and
Lewis, 2004; Larsson et al, 1999)) have tradition-
ally been hand-crafted and consequently some-
what brittle and rigid. With the ability to record,
store and process large numbers of human-human
dialogs (e.g. from call centers), we anticipate
that data-driven methods will increasingly influ-
ence approaches to dialog management.
A successful dialog system relies on the syn-
ergistic working of several components: speech
recognition (ASR), spoken language understand-
ing (SLU), dialog management (DM), language
generation (LG) and text-to-speech synthesis
(TTS). While data-driven approaches to ASR and
SLU are prevalent, such approaches to DM, LG
and TTS are much less well-developed. In on-
going work, we are investigating data-driven ap-
proaches for building all components of spoken
dialog systems.
In this paper, we address one aspect of this prob-
lem ? inferring predictive models to structure task-
oriented dialogs. We view this problem as a first
step in predicting the system state of a dialog man-
ager and in predicting the system utterance during
an incremental execution of a dialog. In particular,
we learn models for predicting dialog acts of ut-
terances, and models for predicting subtask struc-
tures of dialogs. We use three different dialog act
tag sets for three different human-human dialog
corpora. We compare a flat chunk-based model
to a hierarchical parse-based model as models for
predicting the task structure of dialogs.
The outline of this paper is as follows: In Sec-
tion 2, we review current approaches to building
dialog systems. In Section 3, we review related
work in data-driven dialog modeling. In Section 4,
we present our view of analyzing the structure of
task-oriented human-human dialogs. In Section 5,
we discuss the problem of segmenting and label-
ing dialog structure and building models for pre-
dicting these labels. In Section 6, we report ex-
perimental results on Maptask, Switchboard and a
dialog data collection from a catalog ordering ser-
vice domain.
2 Current Methodology for Building
Dialog systems
Current approaches to building dialog systems
involve several manual steps and careful craft-
ing of different modules for a particular domain
or application. The process starts with a small
scale ?Wizard-of-Oz? data collection where sub-
jects talk to a machine driven by a human ?behind
the curtains?. A user experience (UE) engineer an-
alyzes the collected dialogs, subject matter expert
interviews, user testimonials and other evidences
(e.g. customer care history records). This hetero-
geneous set of information helps the UE engineer
to design some system functionalities, mainly: the
201
semantic scope (e.g. call-types in the case of call
routing systems), the LG model, and the DM strat-
egy. A larger automated data collection follows,
and the collected data is transcribed and labeled by
expert labelers following the UE engineer recom-
mendations. Finally, the transcribed and labeled
data is used to train both the ASR and the SLU.
This approach has proven itself in many com-
mercial dialog systems. However, the initial UE
requirements phase is an expensive and error-
prone process because it involves non-trivial de-
sign decisions that can only be evaluated after sys-
tem deployment. Moreover, scalability is compro-
mised by the time, cost and high level of UE know-
how needed to reach a consistent design.
The process of building speech-enabled auto-
mated contact center services has been formalized
and cast into a scalable commercial environment
in which dialog components developed for differ-
ent applications are reused and adapted (Gilbert
et al, 2005). However, we still believe that ex-
ploiting dialog data to train/adapt or complement
hand-crafted components will be vital for robust
and adaptable spoken dialog systems.
3 Related Work
In this paper, we discuss methods for automati-
cally creating models of dialog structure using di-
alog act and task/subtask information. Relevant
related work includes research on automatic dia-
log act tagging and stochastic dialog management,
and on building hierarchical models of plans using
task/subtask information.
There has been considerable research on statis-
tical dialog act tagging (Core, 1998; Jurafsky et
al., 1998; Poesio and Mikheev, 1998; Samuel et
al., 1998; Stolcke et al, 2000; Hastie et al, 2002).
Several disambiguation methods (n-gram models,
hidden Markov models, maximum entropy mod-
els) that include a variety of features (cue phrases,
speaker ID, word n-grams, prosodic features, syn-
tactic features, dialog history) have been used. In
this paper, we show that use of extended context
gives improved results for this task.
Approaches to dialog management include
AI-style plan recognition-based approaches (e.g.
(Sidner, 1985; Litman and Allen, 1987; Rich
and Sidner, 1997; Carberry, 2001; Bohus and
Rudnicky, 2003)) and information state-based ap-
proaches (e.g. (Larsson et al, 1999; Bos et al,
2003; Lemon and Gruenstein, 2004)). In recent
years, there has been considerable research on
how to automatically learn models of both types
from data. Researchers who treat dialog as a se-
quence of information states have used reinforce-
ment learning and/or Markov decision processes
to build stochastic models for dialog management
that are evaluated by means of dialog simulations
(Levin and Pieraccini, 1997; Scheffler and Young,
2002; Singh et al, 2002; Williams et al, 2005;
Henderson et al, 2005; Frampton and Lemon,
2005). Most recently, Henderson et al showed
that it is possible to automatically learn good dia-
log management strategies from automatically la-
beled data over a large potential space of dialog
states (Henderson et al, 2005); and Frampton and
Lemon showed that the use of context informa-
tion (the user?s last dialog act) can improve the
performance of learned strategies (Frampton and
Lemon, 2005). In this paper, we combine the use
of automatically labeled data and extended context
for automatic dialog modeling.
Other researchers have looked at probabilistic
models for plan recognition such as extensions of
Hidden Markov Models (Bui, 2003) and proba-
bilistic context-free grammars (Alexandersson and
Reithinger, 1997; Pynadath and Wellman, 2000).
In this paper, we compare hierarchical grammar-
style and flat chunking-style models of dialog.
In recent research, Hardy (2004) used a large
corpus of transcribed and annotated telephone
conversations to develop the Amities dialog sys-
tem. For their dialog manager, they trained sepa-
rate task and dialog act classifiers on this corpus.
For task identification they report an accuracy of
85% (true task is one of the top 2 results returned
by the classifier); for dialog act tagging they report
86% accuracy.
4 Structural Analysis of a Dialog
We consider a task-oriented dialog to be the re-
sult of incremental creation of a shared plan by
the participants (Lochbaum, 1998). The shared
plan is represented as a single tree that encap-
sulates the task structure (dominance and prece-
dence relations among tasks), dialog act structure
(sequences of dialog acts), and linguistic structure
of utterances (inter-clausal relations and predicate-
argument relations within a clause), as illustrated
in Figure 1. As the dialog proceeds, an utterance
from a participant is accommodated into the tree in
an incremental manner, much like an incremental
syntactic parser accommodates the next word into
a partial parse tree (Alexandersson and Reithinger,
1997). With this model, we can tightly couple
language understanding and dialog management
using a shared representation, which leads to im-
proved accuracy (Taylor et al, 1998).
In order to infer models for predicting the struc-
ture of task-oriented dialogs, we label human-
human dialogs with the hierarchical information
shown in Figure 1 in several stages: utterance
segmentation (Section 4.1), syntactic annotation
(Section 4.2), dialog act tagging (Section 4.3) and
202
subtask labeling (Section 5).
Dialog
Task
Topic/SubtaskTopic/Subtask
Task Task
Clause
UtteranceUtteranceUtterance
Topic/Subtask
DialogAct,Pred?Args DialogAct,Pred?Args DialogAct,Pred?Args
Figure 1: Structural analysis of a dialog
4.1 Utterance Segmentation
The task of ?cleaning up? spoken language utter-
ances by detecting and removing speech repairs
and dysfluencies and identifying sentence bound-
aries has been a focus of spoken language parsing
research for several years (e.g. (Bear et al, 1992;
Seneff, 1992; Shriberg et al, 2000; Charniak and
Johnson, 2001)). We use a system that segments
the ASR output of a user?s utterance into clauses.
The system annotates an utterance for sentence
boundaries, restarts and repairs, and identifies
coordinating conjunctions, filled pauses and dis-
course markers. These annotations are done using
a cascade of classifiers, details of which are de-
scribed in (Bangalore and Gupta, 2004).
4.2 Syntactic Annotation
We automatically annotate a user?s utterance with
supertags (Bangalore and Joshi, 1999). Supertags
encapsulate predicate-argument information in a
local structure. They are composed with each
other using the substitution and adjunction oper-
ations of Tree-Adjoining Grammars (Joshi, 1987)
to derive a dependency analysis of an utterance
and its predicate-argument structure.
4.3 Dialog Act Tagging
We use a domain-specific dialog act tag-
ging scheme based on an adapted version of
DAMSL (Core, 1998). The DAMSL scheme is
quite comprehensive, but as others have also found
(Jurafsky et al, 1998), the multi-dimensionality
of the scheme makes the building of models from
DAMSL-tagged data complex. Furthermore, the
generality of the DAMSL tags reduces their util-
ity for natural language generation. Other tagging
schemes, such as the Maptask scheme (Carletta et
al., 1997), are also too general for our purposes.
We were particularly concerned with obtaining
sufficient discriminatory power between different
types of statement (for generation), and to include
an out-of-domain tag (for interpretation). We pro-
vide a sample list of our dialog act tags in Table 2.
Our experiments in automatic dialog act tagging
are described in Section 6.3.
5 Modeling Subtask Structure
Figure 2 shows the task structure for a sample di-
alog in our domain (catalog ordering). An order
placement task is typically composed of the se-
quence of subtasks opening, contact-information,
order-item, related-offers, summary. Subtasks can
be nested; the nesting structure can be as deep as
five levels. Most often the nesting is at the left or
right frontier of the subtask tree.
Opening
Order Placement
Contact Info
Delivery InfoShipping Info
ClosingSummaryPayment InfoOrder Item
Figure 2: A sample task structure in our applica-
tion domain.
Contact Info Order Item Payment Info Summary Closing
Shipping Info Delivery Info
Opening
Figure 3: An example output of the chunk model?s
task structure
The goal of subtask segmentation is to predict if
the current utterance in the dialog is part of the cur-
rent subtask or starts a new subtask. We compare
two models for recovering the subtask structure
? a chunk-based model and a parse-based model.
In the chunk-based model, we recover the prece-
dence relations (sequence) of the subtasks but not
dominance relations (subtask structure) among the
subtasks. Figure 3 shows a sample output from the
chunk model. In the parse model, we recover the
complete task structure from the sequence of ut-
terances as shown in Figure 2. Here, we describe
our two models. We present our experiments on
subtask segmentation and labeling in Section 6.4.
5.1 Chunk-based model
This model is similar to the second one described
in (Poesio and Mikheev, 1998), except that we
use tasks and subtasks rather than dialog games.
We model the prediction problem as a classifica-
tion task as follows: given a sequence of utter-
ances   in a dialog   	
 			
  and a
203
subtask label vocabulary  ffProceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 42?49,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Predicting Concept Types in User Corrections in Dialog
Svetlana Stoyanchev and Amanda Stent
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400, USA
svetlana.stoyanchev@gmail.com, amanda.stent@stonybrook.edu
Abstract
Most dialog systems explicitly confirm
user-provided task-relevant concepts.
User responses to these system confirma-
tions (e.g. corrections, topic changes) may
be misrecognized because they contain
unrequested task-related concepts. In this
paper, we propose a concept-specific lan-
guage model adaptation strategy where
the language model (LM) is adapted to
the concept type(s) actually present in
the user?s post-confirmation utterance.
We evaluate concept type classification
and LM adaptation for post-confirmation
utterances in the Let?s Go! dialog system.
We achieve 93% accuracy on concept type
classification using acoustic, lexical and
dialog history features. We also show that
the use of concept type classification for
LM adaptation can lead to improvements
in speech recognition performance.
1 Introduction
In most dialog systems, the system explicitly con-
firms user-provided task-relevant concepts. The
user?s response to a confirmation prompt such as
?leaving from Waterfront?? may consist of a sim-
ple confirmation (e.g. ?yes?), a simple rejection
(e.g. ?no?), a correction (e.g. ?no, Oakland?) or a
topic change (e.g. ?no, leave at 7? or ?yes, and go
to Oakland?). Each type of utterance has implica-
tions for further processing. In particular, correc-
tions and topic changes are likely to contain un-
requested task-relevant concepts that are not well
represented in the recognizer?s post-confirmation
language model (LM)1. This means that they are
1The word error rate on post-confirmation Let?s Go! utter-
ances containing a concept is 10% higher than on utterances
likely to be misrecognized, frustrating the user and
leading to cascading errors. Correct determina-
tion of the content of post-confirmation utterances
can lead to improved speech recognition, fewer
and shorter sequences of speech recognition er-
rors, and improved dialog system performance.
In this paper, we look at user responses to sys-
tem confirmation prompts CMU?s deployed Let?s
Go! dialog system. We adopt a two-pass recogni-
tion architecture (Young, 1994). In the first pass,
the input utterance is processed using a general-
purpose LM (e.g. specific to the domain, or spe-
cific to the dialog state). Recognition may fail
on concept words such as ?Oakland? or ?61C? ,
but is likely to succeed on closed-class words (e.g.
?yes?, ?no?, ?and?, ?but?, ?leaving?). If the ut-
terance follows a system confirmation prompt, we
then use acoustic, lexical and dialog history fea-
tures to determine the task-related concept type(s)
likely to be present in the utterance. In the second
recognition pass, any utterance containing a con-
cept type is re-processed using a concept-specific
LM. We show that: (1) it is possible to achieve
high accuracy in determining presence or absence
of particular concept types in a post-confirmation
utterance; and (2) 2-pass speech recognition with
concept type classification and language model
adaptation can lead to improved speech recogni-
tion performance for post-confirmation utterances.
The rest of this paper is structured as follows: In
Section 2 we discuss related work. In Section 3 we
describe our data. In Section 4 we present our con-
cept type classification experiment. In Section 5
we present our LM adaptation experiment. In Sec-
tion 6 we conclude and discuss future work.
without a concept.
42
2 Related Work
When a dialog system requests a confirmation,
the user?s subsequent corrections and topic change
utterances are particularly likely to be misrecog-
nized. Considerable research has now been done
on the automatic detection of spoken corrections.
Linguistic cues to corrections include the num-
ber of words in the post-confirmation utterance
and the use of marked word order (Krahmer et
al., 2001). Prosodic cues include F0 max, RMS
max, RMS mean, duration, speech tempo, and
percentage of silent frames(Litman et al, 2006;
Hirschberg et al, 2004; Levow, 1998). Discourse
cues include the removal, repetition, addition or
modification of a concept, the system?s dialog act
type, and information about error rates in the dia-
log so far (Krahmer et al, 2001; et al, 2002; Lit-
man et al, 2006; Walker et al, 2000). In our ex-
periments, we use most of these features as well as
additional lexical features.
We can use knowledge of the type or content
of a user utterance to modify system behavior.
For example, in this paper we use the concept
type(s) in the user?s utterance to adapt the recog-
nizer?s LM. It is now common practice to adapt
the recognizer to the type, context or style of in-
put speech (Bellegarda, 2004). LM adaptation
has been used to improve automatic speech recog-
nition performance in automated meeting tran-
scription (Tur and Stolcke, 2007), speech-driven
question answering (Stenchikova et al, 2008),
broadcast news recognition (Gildea and Hofmann,
1999), and spoken dialog systems (Tur, 2005).
LMs in dialog systems can be adapted to the di-
alog state (e.g. (Riccardi and Gorin, 2000; Esteve
et al, 2001)), the topic (Iyer and Ostendorf, 1999;
Gildea and Hofmann, 1999), or the speaker (Tur,
2007).
3 Data
In this experiment we use annotated dialog tran-
scripts and speech from the Let?s Go! sys-
tem, a telephone-based spoken dialog system that
provides information about bus routes in Pitts-
burgh (Raux et al, 2005). The data we used comes
from the first two months of Let?s Go! system
operation in 2005 (2411 dialogs), and one month
in 2006 (1430 dialogs). This data has been tran-
scribed, and annotated by hand for concept types.
In order to provide the user with route in-
formation, Let?s Go! must elicit a departure
1 Sys Welcome to the CMU Let?s Go bus informa-
tion system. What can I do for you?
2 User I need to go from Oakland:p
ASR I need to go .from. can?t
3 Sys Where do you wanna leave from?
4 User to Waterfront:p
ASR told. .me. Waterfront
5 Sys Leaving from WATERFRONT . Did I get
that right?
6 User OAKLAND:p
ASR OAKLAND
7 Sys Leaving from OAKLAND . Did I get that
right?
Figure 1: Dialog extract from Let?s Go! data. User
utterances are annotated with concept types (e.g.
:p for place)
location, a destination, a departure time, and
optionally a bus route number. Each concept
value provided by the user is explicitly con-
firmed by the system (see Figure 1). In the
annotated transcripts, the following concepts are
labeled: neighborhood, place, time,
hour, minute, time-of-day, and bus.
For our experiments we collapsed these concepts
into three concept types: time , place and bus.
Let?s Go! has five dialog states corresponding
to the type of user utterance it expects: first-query,
next-query, yes-no, place and time. Its speech
recognizer uses dialog state-specific n-gram LMs
trained on user utterances from the 2005 data.
We focus on user utterances in response to sys-
tem confirmation prompts (the yes-no state). Ta-
ble 1 shows statistics about yes-no state utterances
in Let?s Go!. Table 2 shows a confusion matrix
for confirmation prompt concept type and post-
confirmation utterance concept type. This table
indicates the potential for misrecognition of post-
confirmation utterances. For example, in the 2006
dataset after a system confirmation prompt for a
bus, a bus concept is used in only 64% of concept-
containing user utterances.
In our experiments, we used the 2006 data to
train concept type classifiers and for testing. We
used the 2005 data to build LMs for our speech
recognition experiment.
4 Concept Classification
4.1 Method
Our goal is to classify each post-confirmation user
utterance by the concept type(s) it contains (place,
time, bus or none) for later language-model adap-
tation (see Section 5). From the post-confirmation
user utterances in the 2006 dataset described in
43
Event 2005 2006
num % num %
Total dialogs 2411 1430
Total yes-no confirms 9098 100 9028 100
Yes-no confirms with
a concept
2194 24 1635 18.1
Dialog State
Total confirm place
utts
5548 61 5347 59.2
Total confirm bus utts 1763 19.4 1589 17.6
Total confirm time
utts
1787 19.6 2011 22.3
Concept Type Features
Yes-no utts with place 1416 15.6 1007 11.2
Yes-no utts with time 296 3.2 305 3.4
Yes-no utts with bus 584 6.4 323 3.6
Lexical Features
Yes-no utts with ?yes? 4395 48.3 3693 40.9
Yes-no utts with ?no? 2076 22.8 1564 17.3
Yes-no utts with ?I? 203 2.2 129 1.4
Yes-no utts with
?from?
114 1.3 185 2.1
Yes-no utts with ?to? 204 2.2 237 2.6
Acoustic Features
feature mean stdev mean stdev
Duration (seconds) 1.341 1.097 1.365 1.242
RMS mean .037 .033 .055 .049
F0 mean 183.0 60.86 185.7 58.63
F0 max 289.8 148.5 296.9 146.5
Table 1: Statistics on post-confirmation utterances
place bus time
2005 dataset
confirm place 0.86 0.13 0.01
confirm bus 0.18 0.81 0.01
confirm time 0.07 0.01 0.92
2006 dataset
confirm place 0.87 0.10 0.03
confirm bus 0.34 0.64 0.02
confirm time 0.15 0.13 0.71
Table 2: Confirmation state vs. user concept type
Section 3, we extracted the features described in
Section 4.2 below. To identify the correct concept
type(s) for each utterance, we used the human an-
notations provided with the data.
We performed a series of 10-fold cross-
validation experiments to examine the impact of
different types of feature on concept type classifi-
cation. We trained three binary classifiers for each
experiment, one for each concept type, i.e. we sep-
arately classified each post-confirmation utterance
as place + or place -, time + or time -, and bus + or
bus -. We used Weka?s implementation of the J48
decision tree classifier (Witten and Frank, 2005)2.
For each experiment, we report precision (pre+)
and recall (rec+) for determining presence of each
concept type, and overall classification accuracy
2J48 gave the highest classification accuracy compared to
other machine learning algorithms we tried on this data.
for each concept type (place, bus and time)3. We
also report overall pre+, rec+, f-measure (f+), and
classification accuracy across the three concept
types. Finally, we report the percentage of switch+
errors and switch errors. Switch+ errors are utter-
ances containing bus classified as time/place, time
as bus/place, and place as bus/time; these are the
errors most likely to cause decreases in speech
recognition accuracy after language model adap-
tation. Switch errors include utterances with no
concept classified as place, bus or time.
Only utterances classified as containing one of
the three concept types are subject to second-
pass recognition using a concept-specific language
model. Therefore, these are the only utterances on
which speech recognition performance may im-
prove. This means that we want to maximize rec+
(proportion of utterances containing a concept that
are classified correctly). On the other hand, utter-
ances that are incorrectly classified as containing a
particular concept type will be subject to second-
pass recognition using a poorly-chosen language
model. This may cause speech recognition per-
formance to suffer. This means that we want to
minimize switch+ errors.
4.2 Features
We used the features summarized in Table 3. All
of these features are available at run-time and so
may be used in a live system. Below we give ad-
ditional information about the RAW and LEX fea-
tures; the other feature sets are self-explanatory.
4.2.1 Acoustic and Dialog History Features
The acoustic/prosodic and dialog history features
are adapted from those identified in previous work
on detecting speech recognition errors (particu-
larly (Litman et al, 2006)). We anticipated that
these features would help us distinguish correc-
tions and rejections from confirmations.
4.2.2 Lexical Features
We used lexical features from the user?s current ut-
terance. Words in the output of first-pass ASR are
highly indicative both of concept presence or ab-
sence, and of the presence of particular concept
types; for example, going to suggests the pres-
ence of a place. We selected the most salient lexi-
3We do not report precision or recall for determining ab-
sence of each concept type. In our data set 82.2% of the ut-
terances do not contain any concepts (see Table 1). Conse-
quently, precision and recall for determining absence of each
concept type are above .9 in each of the experiments.
44
Feature type Feature source Features
System confirmation type
(DIA)
system log System?s confirmation prompt concept type (confirm time,
confirm place, or confirm bus)
Acoustic (RAW) raw speech F0 max; RMS max; RMS mean; Duration; Difference be-
tween F0 max in first half and in second half
Lexical (LEX) transcripts/ASR output Presence of specific lexical items; Number of tokens in utter-
ance; [transcribed speech only] String edit distance between
current and previous user utterances
Dialog history (DH1, DH3) 1-3 previous utterances System?s dialog states of previous utterances(place, bus,
time, confirm time, confirm place, or confirm bus); [tran-
scribed speech only] Concept(s) that occurred in user?s ut-
terances (YES/NO for each of the concepts place, bus, time)
ASR confidence score (ASR) ASR output Speech recognizer confidence score
Concept type match (CTM) transcripts/ASR output Presence of concept-specific lexical items
Table 3: Features for concept type classifiers
cal features (unigrams and bigrams) for each con-
cept type by computing the mutual information be-
tween potential features and concept types (Man-
ning et al, 2008). For each lexical feature t and
each concept type class c ? { place +, place -,
time +, time -, bus +, bus -}, we computed I:
I = NtcN ? log2
N ? Ntc
Nt. ? N.c
+
N0c
N ? log2
N ? N0c
N0. ? N.c
+
Nt0
N ? log2
N ? Nt0
Nt. ? N.0
+
N00
N ? log2
N ? N00
N0. ? N.0
where Ntc= number of utterances where t co-
occurs with c, N0c= number of utterances with c
but without t, Nt0= number of utterances where t
occurs without c, N00= number of utterances with
neither t nor c, Nt.= total number of utterances
containing t, N.c= total number of utterances con-
taining c, and N = total number of utterances.
To identify the most relevant lexical features,
we extracted from the data all the transcribed user
utterances. We removed all words that realize con-
cepts (e.g. ?61C?, ?Squirrel Hill?), as these are
likely to be misrecognized in a post-confirmation
utterance. We then extracted all word unigrams
and bigrams. We computed the mutual informa-
tion between each potential lexical feature and
concept type. We then selected the 30 features
with the highest mutual information which oc-
curred at least 20 times in the training data4.
For transcribed speech only, we also compute
the string edit distance between the current and
previous user utterances. This gives some indica-
tion of whether the current utterance is a correc-
tion or topic change (vs. a confirmation). How-
4We aimed to select equal number of features for each
class with information measure in the top 25%. 30 was an
empirically derived threshold for the number of lexical fea-
tures to satisfy the desired condition.
ever, for recognized speech recognition errors re-
duce the effectiveness of this feature (and of the
concept features in the dialog history feature set).
4.3 Baseline
A simple baseline for this task, No-Concept, al-
ways predicts none in post-confirmation utter-
ances. This baseline achieves overall classifica-
tion accuracy of 82% but rec+ of 0. At the other
extreme, the Confirmation State baseline assigns
to each utterance the dialog system?s confirmation
prompt type (using the DIA feature). This base-
line achieves rec+ of .79, but overall classification
accuracy of only 14%. In all of the models used in
our experiments, we include the current confirma-
tion prompt type (DIA) feature.
4.4 Experiment Results
In this section we report the results of experiments
on concept type classification in which we exam-
ine the impact of the feature sets presented in Ta-
ble 3. We report performance separately for recog-
nized speech, which is available at runtime (Table
5); and for transcribed speech, which gives us an
idea of best possible performance (Table 4).
4.4.1 Features from the Current Utterance
We first look at lexical (LEX) and prosodic (RAW)
features from the current utterance. For both rec-
ognized and transcribed speech, the LEX model
achieves significantly higher rec+ and overall ac-
curacy than the RAW model (p < .001). For
recognized speech, however, the LEX model has
significantly more switch+ errors than the RAW
model (p < .001). This is not surprising since the
majority of errors made by the RAW model are
labeling an utterance with a concept as none. Ut-
terances misclassified in this way are not subject to
second-pass recognition and do not increase WER.
45
Features Place Time Bus Overall
pre+ rec+ acc pre+ rec+ acc pre+ rec+ acc pre+ rec+ f+ acc switch+ switch
No Concept 0 0 .86 0 0 0.81 0 0 .92 0 0 0 0.82 0 0
Confirmation State 0.87 0.85 0.86 0.64 0.54 0.58 0.71 0.87 0.78 0.14 0.79 0.24 0.14 17 72.3
RAW 0.65 0.53 0.92 0.25 0.01 0.96 0.38 0.07 0.96 0.67 0.34 0.45 0.85 6.43 4.03
LEX 0.81 0.88 0.96 0.77 0.48 0.98 0.83 0.59 0.98 0.87 0.72 0.79 0.93 7.32 3.22
LEX RAW 0.83 0.84 0.96 0.75 0.54 0.98 0.76 0.59 0.98 0.88 0.70 0.78 0.93 7.39 3.00
DH1 LEX 0.85 0.91 0.97 0.72 0.63 0.98 0.89 0.83 0.99 0.88 0.81 0.84 0.95 5.48 2.85
DH3 LEX 0.85 0.87 0.97 0.72 0.59 0.98 0.92 0.82 0.99 0.89 0.78 0.83 0.94 5.22 2.62
Table 4: Concept type classification results: transcribed speech (all models include feature DIA). Best
overall values in each group are highlighted in bold.
Features Place Time Bus Overall
pre+ rec+ acc pre+ rec+ acc pre+ rec+ acc pre+ rec+ f+ acc switch+ switch
No Concept 0 0 .86 0 0 0.81 0 0 .92 0 0 0 0.82 0 0
Confirmation State 0.87 0.85 0.86 0.64 0.54 0.58 0.71 0.87 0.78 0.14 0.79 0.24 0.14 17 72.3
RAW 0.65 0.53 0.92 0.25 0.01 0.96 0.38 0.07 0.96 0.67 0.34 0.45 0.85 6.43 4.03
LEX 0.70 0.70 0.93 0.67 0.15 0.97 0.65 0.62 0.98 0.75 0.56 0.64 0.89 9.94 4.93
LEX RAW 0.70 0.72 0.93 0.66 0.38 0.97 0.68 0.57 0.98 0.76 0.60 0.67 0.90 10.32 5.10
DH1 LEX RAW 0.71 0.68 0.93 0.68 0.38 0.97 0.78 0.63 0.98 0.77 0.60 0.67 0.90 8.15 4.55
DH3 LEX RAW 0.71 0.70 0.93 0.67 0.42 0.97 0.79 0.63 0.98 0.77 0.62 0.68 0.90 7.20 4.57
ASR DH3 LEX
RAW
0.71 0.70 0.93 0.69 0.42 0.97 0.79 0.63 0.98 0.77 0.62 0.68 0.90 7.20 4.54
CTM DH3 LEX
RAW
0.82 0.82 0.96 0.86 0.71 0.99 0.76 0.68 0.98 0.85 0.74 0.79 0.93 3.89 2.94
CTM ASR DH3
LEX RAW
0.82 0.81 0.96 0.86 0.69 0.99 0.76 0.68 0.98 0.85 0.74 0.79 0.93 4.27 3.01
Table 5: Concept type classification results: recognized speech (all models include feature DIA). Best
overall values in each group are highlighted in bold.
For transcribed speech, the LEX RAW model
does not perform significantly differently from the
LEX model in terms of overall accuracy, rec+, or
switch+ errors. However, for recognized speech,
LEX RAW achieves significantly higher rec+ and
overall accuracy than LEX (p < .001). Lexical
content from transcribed speech is a very good in-
dicator of concept type. However, lexical content
from recognized speech is noisy, so concept type
classification from ASR output can be improved
by using acoustic/prosodic features.
We note that models containing only features
from the current utterance perform significantly
worse than the confirmation state baseline in terms
of rec+ (p < .001). However, they have signif-
icantly better overall accuracy and fewer switch+
errors (p < .001) .
4.4.2 Features from the Dialog History
Next, we add features from the dialog history
to our best-performing models so far. For tran-
scribed speech, DH1 LEX performs significantly
better than LEX in terms of overall accuracy, rec+,
and switch+ errors (p < .001). DH3 LEX per-
forms significantly worse than DH1 LEX in terms
of rec+ (p < 0.05). For recognized speech,
neither DH1 LEX RAW nor DH3 LEX RAW is
significantly different from LEX RAW in terms
of rec+ or overall accuracy. However, both
DH1 LEX RAW and DH3 LEX RAW do per-
form significantly better than LEX RAW in terms
of switch+ errors (p < .05). There are
no significant performance differences between
DH1 LEX RAW and DH3 LEX RAW.
4.4.3 Features Specific to Recognized Speech
Finally, we add the ASR and CTM features to
models trained on recognized speech.
We hypothesized that the classifier can use the
recognizer?s confidence score to decide whether
an utterance is likely to have been misrecognized.
However, ASR DH3 LEX RAW is not signifi-
cantly different from DH3 LEX RAW in terms of
rec+, overall accuracy or switch+ errors.
We hypothesized that the CTM feature will im-
prove cases where a part of (but not the whole)
concept instance is recognized in first-pass recog-
nition5. The generic language model used in first-
pass recognition recognizes some concept-related
words. So, if in the utterance Madison avenue,
avenue (but not Madison), is recognized in the
first-pass recognition, the CTM feature can flag
the utterance with a partial match for place, help-
ing the classifier to correctly assign the place
5We do not try the CTM feature on transcribed speech be-
cause there is a one-to-one correspondence between presence
of the concept and the CTM feature, so it perfectly indicates
presence of a concept.
46
type to the utterance. Then, in the second-pass
recognition the utterance will be decoded with
a place concept-specific language model, poten-
tially improving speech recognition performance.
Adding the CTM feature to DH3 LEX RAW and
ASR DH3 LEX RAW leads to a large statistically
significant improvement in all measures: a 12%
absolute increase in rec+, a 3% absolute increase
in overall accuracy, and decreases in switch+ er-
rors (p < .001). There are no statistically signifi-
cant differences between these two models.
4.4.4 Summary and Discussion
In this section we evaluated different models for
concept type classification. The best perform-
ing transcribed speech model, DH1 LEX, signif-
icantly outperforms the Confirmation State base-
line on overall accuracy and on switch+ and switch
errors (p < .001), and is not significantly different
on rec+. The best performing recognized speech
model, CTM DH3 LEX RAW, significantly out-
performs the Confirmation State baseline on
overall accuracy and on switch+ and switch er-
rors, but is significantly worse on rec+ (p < .001).
The best transcribed speech model achieves signif-
icantly higher rec+ and overall accuracy than the
best recognized speech model (p < .01).
5 Speech Recognition Experiment
In this section we report the impact of concept type
prediction on recognition of post-confirmation ut-
terances in Let?s Go! system data. We hypothe-
sized that speech recognition performance for ut-
terances containing a concept can be improved
with the use of concept-specific LMs. We (1) com-
pare the existing dialog state-specific LM adap-
tation approach used in Let?s Go! with our pro-
posed concept-specific adaptation; (2) compare
two approaches to concept-specific adaptation (us-
ing the system?s confirmation prompt type and us-
ing our concept type classifiers); and (3) evaluate
the impact of different concept type classifiers on
concept-specific LM adaptation.
5.1 Method
We used the PocketSphinx speech recognition en-
gine (et al, 2006) with gender-specific telephone-
quality acoustic models built for Communica-
tor (et al, 2000). We trained trigram LMs us-
ing 0.5 ratio discounting with the CMU language
modeling toolkit (Xu and Rudnicky, 2000)6. We
built state- and concept-specific hierarchical LMs
from the Let?s Go! 2005 data. The LMs are built
with [place], [time] and [bus] submodels.
We evaluate speech recognition performance
on the post-confirmation user utterances from the
2006 testing dataset. Each experiment varies in 1)
the LM used for the final recognition pass and 2)
the method of selecting a LM for use in decoding.
5.1.1 Language models
We built seven LMs for these experiments. The
state-specific LM contains all utterances in the
training data that were produced in the yes-no di-
alog state. The confirm-place, confirm-bus and
confirm-time LMs contain all utterances produced
in the yes-no dialog state following confirm place,
confirm bus and confirm time system confirma-
tion prompts respectively. Finally, the concept-
place, concept-bus and concept-time LMs contain
all utterances produced in the yes-no dialog state
that contain a mention of a place, bus or time.
5.1.2 Decoders
In the baseline, 1-pass general condition, we
use the state-specific LM to recognize all post-
confirmation utterances. In the 1-pass state ex-
perimental condition we use the confirm-place,
confirm-bus and confirm-time LMs to recog-
nize testing utterances produced following a con-
firm place, confirm bus and confirm time prompt
respectively7 . In the 1-pass concept experimen-
tal condition we use the concept-place, concept-
bus and concept-time LMs to recognize testing ut-
terances produced following a confirm place, con-
firm bus and confirm time prompt respectively.
In the 2-pass conditions we perform first-pass
recognition using the general LM. Then, we clas-
sify the output of the first pass using a concept
type classifier. Finally, we perform second-pass
recognition using the concept-place, concept-bus
or concept-time LMs if the utterance was classi-
fied as place, bus or time respectively8 . We used
the three classification models with highest overall
rec+: DH3 LEX RAW, ASR DH3 LEX RAW,
6We chose the same speech recognizer, acoustic models,
language modeling toolkit, and LM building parameters that
are used in the live Let?s Go! system (Raux et al, 2005).
7As we showed in Table 2, most, but not all, utterances in
a confirmation state contain the corresponding concept.
8We treat utterances classified as containing more than
concept type as none. In the 2006 data, only 5.6% of ut-
terances with a concept contain more than one concept type.
47
Recognizer Concept type Language Overall Concept utterances
classifier model WER WER Concept recall
1-pass general state-specific 38.49% 49.12% 50.75%
1-pass confirm state confirm-{place,bus,time} 38.83% 48.96% 51.36%
1-pass confirm state concept-{place,bus,time},
state-specific
46.47% ? 50.73% ? 52.9% ?
2-pass DH3 LEX RAW concept-{place,bus,time},
state-specific
38.48% 47.56% ? 53.2% ?
2-pass ASR DH3 LEX
RAW
concept-{place,bus,time},
state-specific
38.51% 47.99% ? 52.7%
2-pass CTM ASR DH3
LEX RAW
concept-{place,bus,time},
state-specific
38.42% 47.86% ? 52.6%
2-pass oracle concept-{place,bus,time},
state-specific
37.85% ? 45.94% ? 54.91% ?
Table 6: Speech recognition results. ? indicates significant difference (p<.01). ? indicates significant
difference (p<.05). * indicates near-significant trend in difference (p<.07). Significance for WER is
computed as a paired t-test. Significance for concept recall is an inference on proportion.
and CTM ASR DH3 LEX RAW. To get an idea
of ?best possible? performance, we also report 2-
pass oracle recognition results, assuming an oracle
classifier that always outputs the correct concept
type for an utterance.
5.2 Results
In Table 6 we report average per-utterance word
error rate (WER) on post-confirmation utterances,
average per-utterance WER on post-confirmation
utterances containing a concept, and average con-
cept recall rate (percentage of correctly recog-
nized concepts) on post-confirmation utterances
containing a concept. In slot-filling dialog sys-
tems like Let?s Go!, the concept recall rate largely
determines the potential of the system to under-
stand user-provided information and continue the
dialog successfully. Our goal is to maximize con-
cept recall and minimize concept utterance WER,
without causing overall WER to decline.
As Table 6 shows, the 1-pass state and 1-pass
concept recognizers perform better than the 1-
pass general recognizer in terms of concept recall,
but worse in terms of overall WER. Most of these
differences are not statistically significant. How-
ever, the 1-pass concept recognizer has signifi-
cantly worse overall and concept utterance WER
than the 1-pass general recognizer (p < .01).
All of the 2-pass recognizers that use au-
tomatic concept prediction achieve significantly
lower concept utterance WER than the 1-pass
general recognizer (p < .05). Differences be-
tween these recognizers in overall WER and con-
cept recall are not significant.
The 2-pass oracle recognizer achieves signif-
icantly higher concept recall and significantly
lower overall and concept utterance WER than
the 1-pass general recognizer (p < .01). It
also achieves significantly lower concept utterance
WER than any of the 2-pass recognizers that use
automatic concept prediction (p < .01).
Our 2-pass concept results show that it is possi-
ble to use knowledge of the concepts in a user?s ut-
terance to improve speech recognition. Our 1-pass
concept results show that this cannot be effec-
tively done by assuming that the user will always
address the system?s question; instead, one must
consider the user?s actual utterance and the dis-
course history (as in our DH3 LEX RAW model).
6 Conclusions and Future Work
In this paper, we examined user responses to sys-
tem confirmation prompts in task-oriented spoken
dialog. We showed that these post-confirmation
utterances may contain unrequested task-relevant
concepts that are likely to be misrecognized. Us-
ing acoustic, lexical, dialog state and dialog his-
tory features, we were able to classify task-
relevant concepts in the ASR output for post-
confirmation utterances with 90% accuracy. We
showed that use of a concept type classifier can
lead to improvements in speech recognition per-
formance in terms of WER and concept recall.
Of course, any possible improvements in speech
recognition performance are dependent on (1) the
performance of concept type classification; (2)
the accuracy of the first-pass speech recognition;
and (3) the accuracy of the second-pass speech
recognition. For example, with our general lan-
guage model, we get a fairly high overall WER
of 38.49%. In future work, we will systematically
vary the WER of both the first- and second-pass
48
speech recognizers to further explore the interac-
tion between speech recognition performance and
concept type classification.
The improvements our two-pass recognizers
achieve have quite small local effects (up to 3.18%
absolute improvement in WER on utterances con-
taining a concept, and less than 1% on post-
confirmation utterances overall) but may have
larger impact on dialog completion times and task
completion rates, as they reduce the number of
cascading recognition errors in the dialog (et al,
2002). Furthermore, we could also use knowledge
of the concept type(s) contained in a user utterance
to improve dialog management and response plan-
ning (Bohus, 2007). In future work, we will look
at (1) extending the use of our concept-type clas-
sifiers to utterances following any system prompt;
and (2) the impact of these interventions on overall
metrics of dialog success.
7 Acknowledgements
We would like to thank the researchers at CMU
for providing the Let?s Go! data and additional
resources.
References
J. R. Bellegarda. 2004. Statistical language model
adaptation: Review and perspectives. Speech Com-
munication Special Issue on Adaptation Methods for
Speech Recognition, 42:93?108.
D. Bohus. 2007. Error awareness and recovery in
task-oriented spoken dialog systems. Ph.D. thesis,
Carnegie Mellon University.
Y. Esteve, F. Bechet, A. Nasr, and R. Mori. 2001.
Stochastic finite state automata language model trig-
gered by dialogue states. In Proceedings of Eu-
rospeech.
A. Rudnicky et al 2000. Task and domain specific
modelling in the Carnegie Mellon Communicator
system. In Proceedings of ICSLP.
J. Shin et al 2002. Analysis of user behavior under
error conditions in spoken dialogs. In Proceedings
of ICSLP.
D. Huggins-Daines et al 2006. Sphinx: A free, real-
time continuous speech recognition system for hand-
held devices. In Proceedings of ICASSP.
D. Gildea and T. Hofmann. 1999. Topic-based lan-
guage models using EM. In Proceedings of Eu-
rospeech.
J. Hirschberg, D. Litman, and M. Swerts. 2004.
Prosodic and other cues to speech recognition fail-
ures. Speech Communication, 43:155?175.
R. Iyer and M. Ostendorf. 1999. Modeling long dis-
tance dependencies in language: Topic mixtures ver-
sus dynamic cache model. IEEE Transactions on
Speech and Audio Processing, 7(1):30?39.
E. Krahmer, M. Swerts, M. Theune, and M. Weegels.
2001. Error detection in spoken human-machine in-
teraction. International Journal of Speech Technol-
ogy, 4(1).
G.-A. Levow. 1998. Characterizing and recognizing
spoken corrections in human-computer dialogue. In
Proceedings of COLING-ACL.
D. Litman, J.Hirschberg, and M. Swerts. 2006. Char-
acterizing and predicting corrections in spoken dia-
logue systems. Computational Linguistics, 32:417?
438.
C. D. Manning, P. Raghavan, and H. Schu?tze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.
A. Raux, B. Langner, A. Black, and M Eskenazi. 2005.
Let?s Go Public! Taking a spoken dialog system to
the real world. In Proceedings of Eurospeech.
G. Riccardi and A. L. Gorin. 2000. Stochastic lan-
guage adaptation over time and state in a natural spo-
ken dialog system. IEEE Transactions on Speech
and Audio Processing, 8(1):3?9.
S. Stenchikova, D. Hakkani-Tu?r, and G. Tur. 2008.
Name-aware speech recognition for interactive
question answering. In Proceedings of ICASSP.
G. Tur and A. Stolcke. 2007. Unsupervised language
model adaptation for meeting recognition. In Pro-
ceedings of ICASSP.
G. Tur. 2005. Model adaptation for spoken language
understanding. In Proceedings of ICASSP.
G. Tur. 2007. Extending boosting for large scale
spoken language understanding. Machine Learning,
69(1):55?74.
M. Walker, J. Wright, and I. Langkilde. 2000. Using
natural language processing and discourse features
to identify understanding errors in a spoken dialogue
system. In Proceedings of ICML.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
W. Xu and A. Rudnicky. 2000. Language modeling
for dialog system. In Proceedings of ICSLP.
S. Young. 1994. Detecting misrecognitions and out-
of-vocabulary words. In Proceedings of ICASSP.
49
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 144?147,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Concept Form Adaptation in Human-Computer Dialog
Svetlana Stoyanchev and Amanda Stent
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400, USA
svetastenchikova@gmail.com, amanda.stent@gmail.com
Abstract
In this work we examine user adaptation
to a dialog system?s choice of realiza-
tion of task-related concepts. We ana-
lyze forms of the time concept in the Let?s
Go! spoken dialog system. We find that
users adapt to the system?s choice of time
form. We also find that user adaptation
is affected by perceived system adapta-
tion. This means that dialog systems can
guide users? word choice and can adapt
their own recognition models to gain im-
proved ASR accuracy.
1 Introduction
Considerable research has now demonstrated that
human dialog partners exhibit lexical and syntac-
tic convergence; that is, that in a human-human
conversation the participants become more simi-
lar in their use of language over time (Brennan
and Clark, 1996; Lockridge and Brennan, 2002;
Pickering and others, 2000; Reitter et al, 2006).
Several Wizard-of-Oz studies have also shown ev-
idence of convergence in human-computer dia-
log (Branigan and others, 2003; Brennan, 1996;
Gustafson and others, 1997).
In recent work, we examined user adaptation1
to the system?s choice of verb and preposition us-
ing the deployed Let?s Go! spoken dialog system
(Stoyanchev and Stent, 2009a). This was the first
study to look at convergence with real users of a
real dialog system and examined user adaptation
to verbs and prepositions. The study described
in this paper is a follow-on to our previous study.
1In this paper, we use the term adaptation to indicate di-
rectional convergence, e.g. user adaptation to a system. We
make no claims about the psycholinguistic models underly-
ing this adaptation.
Here we look at user adaptation to the system?s
choice of realization of task-related concepts. In
this paper, we: (1) Confirm our previous results
showing that users adapt to the system?s choice of
words, using transcribed data rather than ASR out-
put; (2) Show that adaptation can persist over time;
and (3) Show that user adaptation is strengthened
by perceived system adaptation.
In addition to providing further evidence of con-
vergence in human-computer dialog, our results
have implications for dialog system design. Cur-
rently, much dialog systems research is devoted to
improving ASR accuracy, because this is a signifi-
cant contributor to task success rates and to dialog
length. One way to improve ASR accuracy is to
use targeted language models. Since users adapt to
the system?s choices of realization for task-related
concepts, we can predict the user?s choice of real-
ization and use this to adjust the ASR?s language
model, improving ASR accuracy specifically on
concept words. Another way to improve ASR ac-
curacy is to guide the user into using words that are
likely to be recognized correctly (Hockey and oth-
ers, 2003; Sheeder and Balogh, 2003; Tomko and
Rosenfeld, 2006). Our results imply that if the de-
signer of a dialog system wants to improve ASR
accuracy, system prompts should be designed to
use word choices that are more recognizable; and,
when, possible, to be adaptive to the user?s choice
of form for task-related concepts.
2 System
We conducted our experiment using the Let?s Go!
deployed telephone-based spoken dialog system
which provides information about bus routes in
Pittsburgh (Raux and others, 2005). Let?s Go! an-
swers the phones at the transit authority?s website
outside of normal business hours. Its users are
144
SYS: Welcome to the CMU Let?s Go! Bus Information
System. How may I help you?
USR: I?d like to go to Forbes and Murray.
SYS: To Forbes and Murray. Where are you leaving
from?
USR: Carnegie Mellon.
SYS: From Carnegie Mellon. What time would you like
to leave?
USR: Ten thirty p.m.
SYS: Leaving at ten thirty p. m.?
USR: Yes
SYS: There are five buses running between Carnegie
Mellon and Forbes and Murray. . .
Figure 1: Sample dialog with Let?s Go!
Time form Example realizations Frequency
TIME four, five, six thirty. . . 31.1%
APM four a.m., ten p.m., one
fifteen p. m.
43.5%
POD four in the morn-
ing/evening/afternoon/
4.6%
OCLOCK five o?clock 16%
OTHER four o?clock p. m., sixteen
hundred hours
4.8%
Table 1: Time forms in users? utterances and their
relative frequencies in one month of Let?s Go!
2006 data.
naive callers from the general population with a
real task to perform. In order to provide bus route
information, Let?s Go! elicits values for several
task-related concepts: an optional bus route num-
ber, a departure place, a destination and a desired
travel time. Each concept is explicitly confirmed.
Figure 1 shows a sample dialog with the system.
In this work we investigate adaptation to the
time concept because it has multiple different re-
alizations, as shown in Table 1. This variability
is not unique to time; however, it is the only task-
related concept in Let?s Go! that is not usually
realized using named entities (which exhibit less
variability).
3 Method
In order to study adaptation, we need to identify a
prime, a point in the conversation where one part-
ner introduces a realization. In Let?s Go! the sys-
tem always asks the user to specify a departure
time. The user then typically says a time, which
the system confirms (see Figure 1). We simulate
an ASR error on the user?s response to the sys-
tem?s time request, so that when the system con-
firms the departure time it confirms a time other
than that recognized in the user?s response. To
make the system?s error more realistic, the time
in the simulated error is a time that is phonetically
close to the time (hour and minute) recognized in
the user?s response. The system?s confirmation
prompt is our prime.
The system runs in one of the three condi-
tions: SYS TIME, SYS APM, or SYS POD. In
each condition it uses the corresponding time for-
mat (TIME, APM, or POD as shown in Table 1).
TIME is the most frequent form in the 2006 Let?s
Go! corpus, but it is potentially ambiguous as it
can mean either night or day. APM is the shortest
unambiguous form. POD is longer and has a very
low frequency in the 2006 Let?s Go! corpus.2
We collected approximately 2000 dialogs with
Let?s Go! using this setup. We used the ASR
output to identify dialogs where a time appears
in the ASR output at least twice3. We manually
transcribed 50 dialogs for each experimental con-
dition. Some of these turned out not to contain
mentions of time either before or after the system?s
time confirmation prompt, so we excluded them.
We examine whether the user adapts to the
system?s choice of form for realizing the time
concept, both in the first time-containing post-
confirmation utterance, and in the rest of the dialog
(until the user hangs up or says ?New query?).
4 Results
In this section we first examine user adaptation to
system?s choice of time expression, and then look
at how perceived system adaptation affects user
adaptation.
4.1 User adaptation to system time form
If the user adapts to the system?s time form, then
we would expect to see a greater proportion of the
system?s time form in user utterances following
the prime. We compare the proportion of three
time forms (APM, TIME, and POD) in each sys-
tem condition for 1) Unprimed, 2) First After, and
3) All After user?s utterances, as shown in Table 2.
Unprimed utterances are the user?s time specifica-
tion immediately prior to the prime (the system?s
confirmation prompt). First After utterances are
user utterances immediately following the prime.
All After utterances are all user utterances from the
prime until the user either hangs up or says ?New
2We would have liked to also include OCLOCK in the
experiment. However, due to resource limitations we had to
choose only three conditions.
3The most frequent user response to the system?s request
to specify a departure time is ?Now?; we exclude these from
our experiment.
145
Unprimed
system/user Usr:APM Usr:TIME Usr:POD
SYS APM 25% 42% 8%
SYS TIME 30% 52% 2%
SYS POD 24% 49% 4%
First After
system/user Usr:APM Usr:TIME Usr:POD
SYS APM 49% 29% ? 2%
SYS TIME 21% ? 58% 0%
SYS POD 29% 45% 5%
All After
system/user Usr:APM Usr:TIME Usr:POD
SYS APM 63% 19% ? 3%
SYS TIME 21% ? 50% 2%
SYS POD 37% ? 38% 4%
Table 2: Proportions of time forms in different
system prompt conditions. The highest propor-
tion among system conditions for each time form
is highlighted. Occurrences of time forms other
than the three examined time forms are excluded
from this table. ? indicates a statistically signif-
icant difference from the highlighted value in the
column (p < .05 with Bonferroni adjustment). ?
indicates a statistically significant difference from
the highlighted value in the column (p < .01 with
Bonferroni adjustment).
query?. To test the statistical significance of our
results we perform inference on proportions for a
large sample.
APM There are no statistically significant differ-
ences in the proportions of Usr:APM4 forms in
Unprimed utterances for the different system con-
ditions. The proportion of Usr:APM forms in
First After utterances is significantly higher in the
SYS APM condition than in the SYS TIME con-
dition (p < .01), although not significantly dif-
ferent than in the SYS POD condition. The pro-
portion of Usr:APM forms in the All After ut-
terances is significantly higher in the SYS APM
condition than in both the SYS TIME and the
SYS POD conditions (p < .01). We conclude that
there is user adaptation to system time form in the
SYS APM condition.
TIME There are no statistically significant dif-
ferences in the proportions of Usr:TIME forms in
Unprimed utterances for the different system con-
ditions. The proportions of Usr:TIME forms in the
First After utterances in the SYS TIME condition
is significantly higher than that in the SYS APM
condition (p < .01), but not significantly higher
than that in the SYS POD condition. The same
is true of Usr:TIME forms in the All After utter-
4Usr:time-form refers to the occurrence of the time-form
in a user?s utterance.
condition keep adapt switch total
adaptive 81.8% - 18.2% 33
non-adaptive 37.5% 29.1% 35.4% 48
Table 3: Proportions of user actions in First After
confirmation utterances
ances. We conclude that there is user adaptation to
system time form in the SYS TIME condition.
POD We did not find statistically significant dif-
ferences in Usr:POD forms for the different sys-
tem conditions in either the Unprimed, First After
or All After data. Because this is the long unam-
biguous form, users may have felt that it would
not be recognized or that it would be inefficient to
produce it.
Figures 2 illustrates the effect of user adaptation
on time form for the SYS APM and SYS TIME
conditions.
4.2 The effect of system adaptation on user
adaptation
Sometimes the user happens to use the same form
in their initial specification of time that the system
uses in its confirmation prompt. This gives the il-
lusion that the system is adapting its choice of time
form to the user. We examined whether users? per-
ception of system adaptation affected user adapta-
tion in First After confirmation utterances.
For this analysis we used only the dialogs in
the SYS APM and SYS TIME conditions since
the POD form is rare in the Unprimed utterances.
We distinguish between three possible user actions
following the system?s confirmation prompt: 1)
keep - use the same form as in the unprimed ut-
terance; 2) adapt ? switch to the same form as in
the system?s confirmation prompt; and 3) switch -
switch to a different form than the one used in the
system?s confirmation prompt or in the unprimed
utterance.
Table 3 shows the proportions for each possible
user action. In the adaptive condition users are
twice as likely to keep the time form than in the
non-adaptive condition (81.8% vs. 37.5%). This
difference is statistically significant (p < .001).
In the non-adaptive system condition users who
change time form are slightly more likely to switch
(35.4%) than to adapt (29.1%).
These results suggest that when the system does
not adapt to the user, the user?s choice is unpre-
dictable. However, if the system adapts to the
user, the user is likely to keep the same form. This
146
Figure 2: User Utterances with TIME APM and TIME ONLY.
means that if the system can adapt to the user when
the user chooses a form that is more likely to be
recognized correctly, that provides positive rein-
forcement, making the user more likely to use that
felicitous form in the future. Furthermore, if the
system does adapt to the user then it may be pos-
sible with high accuracy to predict the user?s form
for subsequent utterances, and to use this infor-
mation to improve ASR accuracy for subsequent
utterances (Stoyanchev and Stent, 2009b).
5 Conclusions and Future Work
In this paper, we analyzed user adaptation to a dia-
log system?s choice of task-related concept forms.
We showed that users do adapt to the system?s
word choices, and that users are more likely to
adapt when the system appears to adapt to them.
This information may help us guide users into
more felicitous word choices, and/or modify the
system to better recognize anticipated user word
choices. In future work we plan to analyze the
effect of ASR adaptation to user word choice on
speech recognition performance in spoken dialog.
References
H. Branigan et al 2003. Syntactic alignment between
computers and people: The role of belief about men-
tal states. In Proceedings of the 25th Annual Confer-
ence of the Cognitive Science Society.
S. Brennan and H. Clark. 1996. Conceptual pacts and
lexical choice in conversation. Journal of Experi-
mental Psychology, 22(6):1482?1493.
S. Brennan. 1996. Lexical entrainment in spontaneous
dialog. In Proceedings of ISSD, pages 41?44.
J. Gustafson et al 1997. How do system questions
influence lexical choices in user answers? In Pro-
ceedings of Eurospeech.
B. Hockey et al 2003. Targeted help for spoken dia-
logue systems: intelligent feedback improves naive
users performance. In Proceedings of EACL.
C. Lockridge and S. Brennan. 2002. Addressees?
needs influence speakers? early syntactic choices.
Psychonomics Bulletin and Review, 9:550?557.
M. Pickering et al 2000. Activation of syntactic prim-
ing during language production. Journal of Psy-
cholinguistic Research, 29(2):205?216.
A. Raux et al 2005. Let?s go public! taking a spoken
dialog system to the real world. In Proceedings of
Eurospeech.
E. Reitter, J. Moore, and F. Keller. 2006. Priming of
syntactic rules in task-oriented dialogue and sponta-
neous conversation. In Proceedings of CogSci.
T. Sheeder and J. Balogh. 2003. Say it like you mean
it: Priming for structure in caller responses to a spo-
ken dialog system. International Journal of Speech
and Technology, 6:103?111.
S. Stoyanchev and A. Stent. 2009a. Lexical and syn-
tactic priming and their impact in deployed spoken
dialog systems. In Proceedings of NAACL.
S. Stoyanchev and A. Stent. 2009b. Predicting concept
types in user corrections in dialog. In Proceedings of
the EACL Workshop on the Semantic Representation
of Spoken Language.
S. Tomko and R. Rosenfeld. 2006. Shaping user input
in speech graffiti: a first pass. In Proceedings of
CHI.
147
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 20?24, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
ATT1: Temporal Annotation Using Big Windows and Rich Syntactic and
Semantic Features
Hyuckchul Jung and Amanda Stent
AT&T Labs - Research
180 Park Ave
Florham Park, NJ 07932, USA
hjung, stent@research.att.com
Abstract
In this paper we present the results of exper-
iments comparing (a) rich syntactic and se-
mantic feature sets and (b) big context win-
dows, for the TempEval time expression and
event segmentation and classification tasks.
We show that it is possible for models using
only lexical features to approach the perfor-
mance of models using rich syntactic and se-
mantic feature sets.
1 Introduction
TempEval-3 Temporal Annotation Task (UzZaman
et al, 2012) has three subtasks:
A Time expression extraction and classification -
extract time expressions from input text, and de-
termine the type and normalised value for each
extracted time expression.
B Event extraction and classification - extract event
mentions from input text, and determine the class,
tense and aspect features for each extracted event.
C Temporal link identification - identify and cate-
gorise temporal links between events in the same
or consecutive sentences, events and time expres-
sions in the same sentence, and events and the
document creation time of the input text.
Here we report results for the first two tasks.
Previous TempEval competitions have shown that
rich syntactic and semantic feature sets can lead to
good performance on event and time expression ex-
traction and classification tasks (e.g. (Llorens et al,
Type Files EVENT TIMEX
AQUAINT gold 73 4431 579
TimeBank gold 183 6698 1243
TE3-Silver silver 2452 81329 12739
Table 1: Frequency of event and time expressions in the
text portions of the TempEval-3 data sets
2010; UzZaman and Allen, 2010)). In this work, we
show that with large windows of context, it is pos-
sible for models using only lexical features to ap-
proach the performance of models using rich syn-
tactic and semantic feature sets.
2 Data
Using the gold and silver data distributed by the
TempEval-3 task organizers (see Table 1), we pro-
cessed each input file with the Stanford CoreNLP
(Stanford Natural Language Processing Group,
2012) and SENNA (Collobert et al, 2011) open-
source NLP tools. From the Stanford CoreNLP
tools we obtained a tokenization of the input text,
the lemma and part of speech (POS) tag for each
token, and dependency and constituency parses for
each sentence. From SENNA, we obtained a seman-
tic role labelling for each sentence.
3 Approach
We were curious to explore the tradeoff between ad-
ditional context on the one hand, and additional lay-
ers of representation on the other, for the event and
time expression extraction tasks. Researchers have
investigated the impacts of different sets of features
(Adafre and de Rijke, 2005; Angeli et al, 2012;
20
Feature type Features Used in
Lexical 1 token ATT1,
ATT2, ATT3
Lexical 2 lemma ATT1, ATT2
Part of speech POS tag ATT1, ATT2
Dependency governing verb, governing verb POS, governing preposition,
phrase tag, path to root of parse tree, head word, head word lemma,
head word POS
ATT1, ATT2
Constituency
parse
governing verb, governing verb POS, governing preposition,
phrase tag, path to root of parse tree
ATT1, ATT2
Semantic role semantic role label, semantic role labels along path to root of parse
tree
ATT1
Table 2: Features used in our models
Tag type Tags
time expression extraction tags B DATE, B DURATION, B SET, B TIME, I DATE,
I DURATION, I SET, I TIME, O
Event expression extraction tags B ACTION, B ASPECTUAL, B ACTION, B OCCURRENCE,
B PERCEPTION, B REPORTING, B STATE, O
Event tense FUTURE, INFINITIVE, PAST, PASTPART, PRESENT, PRES-
PART, NONE, O
Event aspect PROGRESSIVE, PREFECTIVE PROGRESSIVE, PERFEC-
TIVE, NONE, O
Event polarity NEG, POS
Event modality ?D, CAN, CLOSE, COULD, DELETE, HAVE TO, HAVE TO,
LIKELIHOOD, MAY, MIGHT, MUST, NONE, O, POSSIBLE,
POTENTIAL, SHOULD, SHOULD HAVE TO, TO, UNLIKELY,
UNTIL, WOULD, WOULD HAVE TO
Table 3: Tags assigned by our classifiers for TempEval-3 tasks A and B
Rigo and Lavelli, 2011). In particular, (Rigo and
Lavelli, 2011) also examined performance based on
different sizes of n-grams in a small scale (n=1,3).
In this work, we intended to systematically inves-
tigate the performance of various models with differ-
ent layers of representation (based on much larger
sets of rich syntactic/semantic features) as well as
additional context. For each time expression/event
segmentation/classification task, we trained twelve
models exploring these two dimensions, three of
which we submitted for TempEval-3.
Additional layers of representation We
trained three types of model: (ATT1) STAN-
FORD+SENNA, (ATT2) STANFORD and (ATT3)
WORDS ONLY. The basic features used in each
type of model are given in Table 2: ATT1 models
include lexical, syntactic and semantic features,
ATT2 models include only lexical and syntactic
features, and ATT3 models include only lexical
features. For the ATT1 models we had 18 basic
features per token, for the ATT2 models we had 16
basic features per token, and for the ATT3 models
we had one basic feature per token.
Additional context We experimented with context
windows of 0, 1, 3, and 7 words preceding and fol-
lowing the token to be labeled (i.e. window sizes of
1, 3, 7, and 15). For each window size, we trained
ATT1, ATT2 and ATT3 models. The ATT1 mod-
els had 18 basic features per token in the context
window, for up to 15 tokens, so up to 270 basic fea-
tures for each token to be labeled. The ATT2 mod-
els had 16 basic features per token in the context
21
window, so up to 240 basic features for each token
to be labeled. The ATT3 models had 1 basic feature
per token in the context window, so up to 15 basic
features for each token to be labeled.
Model training For event extraction and classifica-
tion, time expression extraction and classification,
and event feature classification, we used the machine
learning toolkit LLAMA (Haffner, 2006). LLAMA
encodes multiclass classification problems using bi-
nary MaxEnt classifiers to increase the speed of
training and to scale the method to large data sets.
We also used a front-end to LLAMA that builds un-
igram, bigram and trigram extended features from
basic features; for example, from the basic feature
?go there today?, it would build the features ?go?,
?there?, ?today?, ?go there?, ?there today?, and ?go
there today?. We grouped our basic features (see Ta-
ble 2) by type rather than by token, and the LLAMA
front-end then produced ngram features. We chose
LLAMA primarily because of the proven power
of the ngram feature-extraction front-end for NLP
tasks.
4 Event and Time Expression Extraction
For event and time expression extraction, we trained
BIO classifiers. A BIO classifier tags each input to-
ken as either Beginning, In, or Out of an event/time
expression. Our classifier for events simultaneously
assigns a B, I or O to each token, and classifies the
class of the event for tokens that Begin or are In an
event. Our time expression classifier simultaneously
assigns a B, I, or O to each token, and classifies the
type of the time expression for tokens that Begin or
are In a time expression (see Table 3).
A BIO model may sometimes be inconsistent; for
example, a token may be labeled as Inside a segment
of a particular type, while the previous token may
be labeled as Out of any segment. We considered
the two most likely labels for each token (as long as
each had likelihood at least 0.9), choosing the one
most consistent with the context.
5 Event Feature Classification
We determined the event features for each extracted
event using four additional classifiers, one each for
tense, aspect, polarity and modality. These classi-
fiers were trained only on tokens identified as part of
event expressions. Since the event expressions were
single words for all but a few (erroneous) cases in the
silver data, for determining the event features, we
used the same features as before, with the single ad-
dition of the event class (during testing, we used the
dynamically assigned event class from the event seg-
mentation classifier). As before, we experimented
with ATT1, ATT2, and ATT3 models. TempEval-
3 only includes evaluation of tense and aspect fea-
tures, so we only report for those. The tags assigned
by each classifier are listed in Table 3.
6 Time Normalization
To compute TIMEX3 standard based values for
extracted time expressions, we used the TIMEN
(Llorens et al, 2012) and TRIOS (UzZaman and
Allen, 2010) time normalizers. Values from the
normalizers were validated in post-processing (e.g.
?T2445? is invalid) and, when the normalizers re-
turned different non-nil values, TIMEN?s values
were selected without further reasoning. Time nor-
malization was out of scope in our research for this
evaluation, but it remains as part of our future work.
7 Results and Discussion
Our results for event segmentation/classification on
the TempEval-3 test data are provided in Table 4.
The absence of semantic features causes only small
changes in F1. The absence of syntactic features
causes F1 to drop slightly (less than 2.5% for all
but the smallest window size), with recall decreasing
while precision improves somewhat. Attribute F1 is
also impacted minimally by the absence of semantic
features, and about 2-5% by the absence of syntactic
features for all but the smallest window size.1
Our results for time expression extraction and
classification on the TempEval-3 test data are pro-
vided in Table 5. Here, the performance drops more
in the absence of semantic and syntactic features;
however, there is an interaction between length of
time expression and performance drop which we
may be able to ameliorate in future work by han-
dling consistency issues in the BIO time expression
extraction model better.
1In Tables 4 and 5, we present results that are slightly dif-
ferent from our submission due to a minor fix in our models by
removing some redundant feature values used twice.
22
Features Window size F1 P R Class Tense Aspect
STANFORD+SENNA 15 (ATT1) 81.16 81.49 80.83 71.60 59.62 73.76
7 81.08 81.74 80.43 71.49 59.05 73.78
3 80.35 81.23 79.49 71.41 58.67 73.17
1 80.94 80.77 81.10 72.37 58.06 73.71
STANFORD 15 (ATT2) 80.86 81.02 80.70 71.05 59.10 73.34
7 81.30 81.90 80.70 71.57 59.01 74.14
3 80.87 81.58 80.16 71.94 58.96 73.70
1 80.78 80.72 80.83 71.80 57.47 73.41
WORDS ONLY 15 (ATT3) 78.58 81.95 75.47 69.5 55.27 70.76
7 78.40 82.21 74.93 69.14 55.54 70.27
3 78.14 82.44 74.26 69.39 52.75 70.38
1 73.55 79.78 68.23 66.33 44.94 63.15
Table 4: Event extraction results (F1, P and R, strict match); feature classification results (attribute F1)
Features Window size F1 P R Type Value
STANFORD+SENNA 15 (ATT1) 80.17 (85.95) 93.27 (100) 70.29 (75.36) 77.69 65.29
7 76.99 (83.68) 91.09 (99.01) 66.67 (72.46) 75.31 64.44
3 75.52 (83.82) 88.35 (98.06) 65.94 (73.19) 75.52 63.07
1 66.12 (83.27) 75.70 (95.33) 58.70 (73.91) 72.65 59.59
STANFORD 15 (ATT2) 78.69 (85.25) 90.57 (98.11) 69.57 (75.36) 76.23 65.57
7 78.51 (84.30) 91.35 (98.08) 68.84 (73.91) 76.03 63.64
3 78.19 (84.77) 90.48 (98.10) 68.84 (74.64) 75.72 64.20
1 67.48 (83.74) 76.85 (95.37) 60.14 (74.64) 73.17 59.35
WORDS ONLY 15 (ATT3) 72.34 (80.85) 87.63 (97.94) 61.59 (68.84) 74.04 60.43
7 72.34 (80.85) 87.63 (97.94) 61.59 (67.84) 74.04 59.57
3 74.48 (82.85) 88.12 (98.02) 64.49 (71.74) 75.31 61.09
1 44.62 (82.87) 49.56 (92.04) 40.58 (75.36) 70.92 39.84
Table 5: Time expression extraction results (F1, P and R, strict match with relaxed match in parentheses); attribute F1
for type and value features
A somewhat surprising finding is that both event
and time expression extraction are subject to rela-
tively tight constraints from the lexical context. We
were surprised by how well the ATT3 (WORDS
ONLY) models performed, especially in terms of
precision. We were also surprised that the words
only models with window sizes of 3 and 7 performed
as well as the models with a window size of 15. We
think these results are promising for ?big data? text
analytics, where there may not be time to do heavy
preprocessing of input text or to train large models.
8 Future Work
For us, participation in TempEval-3 is a first step
in developing a temporal understanding component
for text analytics and virtual agents. We now in-
tend to appy our best performing models to this task.
In future work, we plan to evaluate our initial re-
sults with larger data sets (e.g., cross validation on
the tempeval training data) and experiment with hy-
brid/ensemble methods for performing time expres-
sion and temporal link extraction.
Acknowledgments
We thank Srinivas Bangalore, Patrick Haffner, and
Sumit Chopra for helpful discussions and for sup-
plying LLAMA and its front-end for our use.
23
References
S. F. Adafre and M. de Rijke. 2005. Feature engineering
and post-processing for temporal expression recogni-
tion using conditional random fields. In Proceedings
of the ACL Workshop on Feature Engineering for Ma-
chine Learning in Natural Language Processing.
G. Angeli, C. D. Manning, and D. Jurafsky. 2012. Pars-
ing time: Learning to interpret time expressions. In
Proceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies (HLT-NAACL).
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12.
P. Haffner. 2006. Scaling large margin classifiers for spo-
ken language understanding. Speech Communication,
48(3?4).
H. Llorens, E. Saquete, and B. Navarro. 2010. TIPSem
(English and Spanish): Evaluating CRFs and semantic
roles in TempEval-2. In Proceedings of the Interna-
tional Workshop on Semantic Evaluation (SemEval).
H. Llorens, L. Derczynski, R. Gaizauskas, and E. Sa-
quete. 2012. Timen: An open temporal expression
normalisation resource. In Proceedings of the Interna-
tional Conference on Language Resources and Evalu-
ation (LREC).
S. Rigo and A. Lavelli. 2011. Multisex - a multi-
language timex sequential extractor. In Proceedings
of Temporal Representation and Reasoning (TIME).
Stanford Natural Language Processing Group. 2012.
Stanford CoreNLP. http://nlp.stanford.
edu/software/corenlp.shtml.
N. UzZaman and J. F. Allen. 2010. TRIPS and TRIOS
system for TempEval-2: Extracting temporal informa-
tion from text. In Proceedings of the International
Workshop on Semantic Evaluation (SemEval).
N. UzZaman, H. Llorens, J. Allen, L. Derczynski,
M. Verhagen, and J. Pustejovsky. 2012. Tempeval-
3: Evaluating events, time expressions, and tempo-
ral relations. http://arxiv.org/abs/1206.
5333v1.
24
gency dispatcher, cooperating with the system to 
dynamically allocate resources to and make 
plans for solving problems as they arise in the 
world. The setting, Monroe County, NY, is con- 
siderably more complex than our previous do- 
mains (e.g. Pacifica, TRAINS), and raises new 
issues in knowledge representation a d refer- 
ence. Emergencies include requests for medical 
assistance, car accidents, civil disorder, and 
larger problems such as flooding and snow 
storms. Resources at the user's disposal may 
include road crews, electric crews, ambulances, 
police units and helicopters. Some of the in- 
crease in mixed-initiative interaction comes 
from givi-n~ the_ system more knowledge of the 
tasks being solved. Some comes from the fact 
that the solution to one problem may conflict 
with the solution to another, either because of 
scheduling conflicts, scarce resources, or aspects 
of the physical world (e.g. an ambulance can't go 
down a road that has not been plowed). The 
range of tasks and complexity of the world allow 
for problem solving at different levels of granu- 
larity, making it possible for the system to take 
as much control over the task as the user per- 
mits. 
4. Important  Contr ibut ions  
While a number of robust dialogue systems have 
been built in recent years, they mostly have op- 
erated in domains that require little if any rea- 
soning. Rather, the task is hard-coded into the 
system operation. One of the major goals of the 
TRIPS project has been to develop dialogue 
models and system architectures that support 
conversational interaction in domains where 
complex reasoning systems are required. One 
goal has been to build a fairly generic model in 
which different domains can then be specified 
fairly easily. On this front, we are seeing some 
success as we have now constructed versions of 
TRIPS in three different domains, and TRIPS? 
911 will be the fourth. In developing the system 
for new domains, the bulk of the work by far has 
been in system enhancements rather than in 
developing the domain models. 
The TRIPS-911 domain has forced a rethinking 
of the relationship between dialogue- 
management, problem-solving, the system's 
Figure 1: Monroe County map used in TRIPS-911 
own goal-pursuit and generation. The new ar- 
chitecture is designed to support research into 
mixed-initiative interactions, incremental gen- 
eration of content (in which the user might in- 
tervene before the system completes all it has to 
say), rich reference resolution models, and the 
introduction of plan monitoring and plan repair 
into the suite of plan management operations 
supported. The domain also can support longer 
and richer dialogues than in previous domains. 
More complex domains mean even more com- 
plex dialogues. The complexity arises from 
many factors. First, more complex dialogues 
will involve topic progression, development and 
resumption, and more complex referential phe- 
nomena. On the problem solving front, there will 
be more complex corrections, elaborations and 
modifications--forcing us to develop richer 
discourse models. In addition, the complexity of 
the domain demonstrates a need for better 
grounding behavior and a need for incremental 
dialogue-based generation. 
We have by no means solved these problems. 
Rather we have built a rich testbed, designed and 
implemented a plausible architecture, and have 
constructed an initial system to demonstrate 
basic capabilities in each of the problem areas. 
34 
5. Limitations 
TRIPS-911 is a first attempt at handling a do- 
main of this complexity. As such there are many 
capabilities that people have in such situations 
that are beyond the system's current capabilities. 
Some of the most important are: 
? Scale - we can only handle small domains 
and the existing techniques would not ex- 
tend directly to a realistic size 911 operation. 
To scale up we must face some difficult 
problems including reasoning about quanti- 
ties and aggregates, planning in large-scale 
domains (i.e., the real domains are beyond 
the capabilities of current plan technology), 
and performing intention recognition as the 
number of options increases. In addition, for 
an effective dialogue system, all this must be 
done in real-time. 
? Meta-talk - when faced with complex prob- 
lems, people often first generally discuss the 
problem and possible strategies for solving 
it, and later may explicitly direct attention to 
specific subproblems. The current TRIPS 
system does not support such discussion. 
? Time - in the 911 domain there are at least 
two temporal contexts that can be "used" by 
the conversants: there is the actual time (i.e., 
when they are talking), but there also is the 
time relative to a point of focus in a plan, or 
even simply talking about the past or the 
future. TRIPS-911 can currently interpret 
expressions with respect to the actual time. 
? Interleaved generation - when people are 
discussing complex issues, they often have 
to plan to communicate heir content across 
several different utterances. There is no 
guarantee that the other conversant will not 
"interrupt" (e.g., to clarify, correct, suggest 
alternatives, etc) before the entire content is 
conveyed. This requires a rethinking of cur- 
rent practice in generation to make it incre- 
mental and interactive. 
? True interruptions - people may interrupt the 
system while it is talking. It is unclear at this 
stage what the system should assume was 
conveyed. The strategies of assuming noth- 
ing was conveyed, or that all was conveyed 
have obvious faults. We are pursuing alter- 
natives based on knowing when speech was 
interrupted, but using this ififormation suc- 
cessfully remains adifficult problem. 
References 
Allen, James et al An Architecture for a Generic 
Dialogue Shell, to appear, J. Natural Language 
Engineering, 2000. 
Ferguson, George and J. Allen,-TRIPS: An Integrated 
Intelligent Problem-Solving Assistant, Proc. Na- 
tional Conference on AI (AAAI-98), Madison, WI, 
1998. 
35 
Referring Expression Generation Using Speaker-based Attribute Selection
and Trainable Realization (ATTR)
Giuseppe Di Fabbrizio and Amanda J. Stent and Srinivas Bangalore
AT&T Labs - Research, Inc.
180 Park Avenue
Florham Park, NJ 07932, USA
{pino,stent,srini}@research.att.com
Abstract
In the first REG competition, researchers
proposed several general-purpose algorithms
for attribute selection for referring expression
generation. However, most of this work did
not take into account: a) stylistic differences
between speakers; or b) trainable surface re-
alization approaches that combine semantic
and word order information. In this paper we
describe and evaluate several end-to-end re-
ferring expression generation algorithms that
take into consideration speaker style and use
data-driven surface realization techniques.
1 Introduction
There now exist numerous general-purpose algo-
rithms for attribute selection used in referring ex-
pression generation (e.g., (Dale and Reiter, 1995;
Krahmer et al, 2003; Belz and Gatt, 2007)). How-
ever, these algorithms by-and-large focus on the al-
gorithmic aspects of referring expression generation
rather than on psycholinguistic factors that influence
language production. For example, we know that
humans exhibit individual style differences during
language production that can be quite pronounced
(e.g. (Belz, 2007)). We also know that the lan-
guage production process is subject to lexical prim-
ing, which means that words and concepts that have
been used recently are likely to appear again (Levelt,
1989).
In this paper, we first explore the impact of indi-
vidual style and priming on attribute selection for
referring expression generation. To get an idea
of the potential improvement when modeling these
factors, we implemented a version of full brevity
search (Dale, 1992) that uses speaker-specific con-
straints, and another version that also uses recency
constraints. We found that using speaker-specific
constraints led to big performance gains for both
TUNA domains, while the use of recency constraints
was not as effective for TUNA-style tasks. We then
modified Dale and Reiter?s classic attribute selection
algorithm (Dale and Reiter, 1995) to model speaker-
specific constraints, and found performance gains in
this more greedy approach as well.
Then we looked at surface realization for referring
expression generation. There are several approaches
to surface realization described in the literature (Re-
iter and Dale, 2000) ranging from hand-crafted
template-based realizers to data-driven syntax-based
realizers (Langkilde and Knight, 2000; Bangalore
and Rambow, 2000). Template-based realization
involves the insertion of attribute values into pre-
determined templates. Data-driven syntax-based
methods use syntactic relations between words (in-
cluding long-distance relations) for word ordering.
Other data-driven techniques exhaustively generate
possible realizations with recourse to syntax in as
much as it is reflected in local n-grams. Such tech-
niques have the advantage of being robust although
they are inadequate to capture long-range depen-
dencies. In this paper, we explore three techniques
for the task of referring expression generation that
are different hybrids of hand-crafted and data-driven
methods.
The remainder of this paper is organized as fol-
lows: In Section 2, we present the algorithms for
attribute selection. The different methods for sur-
face realizers are presented in Section 3. The exper-
iments concerning the attribute selection and surface
realization are presented in Section 4 and Section 5.
The final remarks are discussed in Section 6.
2 Attribute Selection Algorithms
Full Brevity (FB) We implemented a version of
full brevity search (Dale, 1992). It does the follow-
211
ing: first, it constructs AS, the set of attribute sets
that uniquely identify the referent given the distrac-
tors. Then, it selects an attribute set ASu ? AS
based on a selection criterion. The minimality (FB-
m) criterion selects from among the smallest ele-
ments of AS at random. The frequency (FB-f) cri-
terion selects from among the elements of AS the
one that occurred most often in the training data.
The speaker frequency (FB-sf) criterion selects
from among the elements of AS the one used most
often by this speaker in the training data, backing off
to FB-f if necessary. This criterion models speaker-
specific constraints. Finally, the speaker recency
(FB-sr) criterion selects from among the elements
of AS the one used most recently by this speaker in
the training data, backing off to FB-sf if necessary.
This criterion models priming and speaker-specific
constraints.
Dale and Reiter We implemented two variants of
the classic Dale & Reiter attribute selection (Dale
and Reiter, 1995) algorithm. For Dale & Reiter
basic (DR-b), we first build the preferred list of
attributes by sorting the most frequently used at-
tributes in the training set. We keep separate lists
based upon the ?+LOC? and ?-LOC? conditions
and backoff to a global preferred frequency list in
case the attributes are not covered in the current list
(merge and sort by frequency). Next, we iterate over
the list of preferred attributes and select the next one
that rules out at least one entity in the contrast set
until no distractors are left. The Dale & Reiter
speaker frequency (DR-sf) uses a speaker-specific
preferred list, backing off to the DR-b preferred list
if an attribute is not in the current speaker?s preferred
list. For this task, we ignored any further attribute
knowledge base or taxonomy abstraction.
3 Surface Realization Approaches
We summarize our approaches to surface realization
in this section. All three surface realizers have the
same four stages: (a) lexical choice of words and
phrases for the attribute values; (b) generation of a
space of surface realizations (T ); (c) ranking the set
of realizations using a language model (LM ); (d)
selecting the best scoring realization.
T ? = BestPath(Rank(T, LM)) (1)
Template-Based Realizer To construct our
template-based realizer, we extract the annotated
word string from each trial in the training data
and replace each annotated text segment with the
attribute type with which it is annotated. The key
for each template is the lexicographically sorted list
of attribute types it contains. Consequently, any
attribute lists not found in the training data cannot
be realized by the template-based realizer; however,
if there is a template for an input attribute list it is
quite likely to be coherent.
At generation time, we find all possible realiza-
tions of each attribute in the input attribute set, and
fill in each possible template with each combina-
tion of the attribute realizations. We report results
for two versions of this realizer: one with speaker-
specific lexicon and templates (Template-S), and
one without (Template).
Dependency-Based Realizer To construct our
dependency-based realizer, we first parse all the
word strings from the training data using the depen-
dency parser described in (Bangalore et al, 2005;
Nasr and Rambow, 2004). Then, for every pair
of words wi, wj that occur in the same referring
expression (RE) in the training data, we compute:
freq(i < j), the frequency with which wi pre-
cedes wj in any RE; freq(i = j ? 1), the fre-
quency with which wi immediately precedes wj in
any RE; freq(dep(wi, wj) ? i < j), the frequency
with which wi depends on and precedes wj in any
RE, and freq(dep(wi, wj) ? j < i), the frequency
with which wi depends on and follows wj in any RE.
At generation time, we find all possible realiza-
tions of each attribute in the input attribute set, and
for each combination of attribute realizations, we
find the most likely set of dependencies and prece-
dences given the training data.
Permute and Rank In this method, the lexical
items associated with each of the attribute value to
be realized are treated as a disjunctive set of tokens.
This disjunctive set is represented as a finite-state
automaton with two states and transitions between
them labeled with the tokens of the set. The transi-
tions are weighted by the negative logarithm of the
probability of the lexical token (w) being associated
with that attribute value (attr): (?log(P (w|attr))).
These sets are treated as unordered bags of tokens;
we create permutations of these bags of tokens to
represent the set of possible surface realizations. We
then use the language model to rank this set of possi-
ble realizations and recover the highest scoring RE.
212
DICE MASI Acc. Uniq. Min.
Furniture
FB-m .36 .16 0 1 1
FB-f .81 .58 .40 1 0
FB-sf .95 .87 .79 1 0
FB-sr .93 .81 .71 1 0
DR-b .81 .60 .45 1 0
DR-sf .86 .64 .45 1 .04
People
FB-m .26 .12 0 1 1
FB-f .58 .37 .28 1 0
FB-sf .94 .88 .84 1 .01
FB-sr .93 .85 .79 1 .01
DR-b .70 .45 .25 1 0
DR-sf .78 .55 .35 1 0
Overall
FB-m .32 .14 0 1 1
FB-f .70 .48 .34 1 0
FB-sf .95 .87 .81 1 .01
FB-sr .93 .83 .75 1 .01
DR-b .76 .53 .36 1 0
DR-sf .82 .60 .41 1 .02
Table 1: Results for attribute selection
Unfortunately, the number of states of the min-
imal permutation automaton of even a linear au-
tomata (finite-state machine representation of a
string) grows exponentially with the number of
words of the string. So, instead of creating a full
permutation automaton, we choose to constrain per-
mutations to be within a local window of adjustable
size (also see (Kanthak et al, 2005)).
4 Attribute Selection Experiments
Data Preparation The training data were used to
build the models outlined above. The development
data were then processed one-by-one. For our final
submissions, we use training and development data
to build our models.
Results Table 1 shows the results for variations of
full brevity. As we would expect, all approaches
achieve a perfect score on uniqueness. For both cor-
pora, we see a large performance jump when we
use speaker constraints. However, when we incor-
porate recency constraints as well performance de-
clines slightly. We think this is due to two factors:
first, the speakers are not in a conversation, and self-
priming may have less impact; and second, we do
not always have the most recent prior utterance for a
given speaker in the training data.
Table 1 also shows the results for variations of
Dale and Reiter?s algorithm. When we incorpo-
String-Edit Dist. Accuracy
Furniture
DEV FB-sf DR-sf DEV FB-sf DR-sf
Permute&Rank 4.39 4.60 4.74 0.07 0.04 0.03
Dependency 3.90 4.25 5.50 0.14 0.06 0.03
Template 4.36 4.33 5.39 0.07 0.05 0.03
Template-S 3.52 3.81 5.16 0.28 0.20 0.04
People
Permute&Rank 6.26 6.46 7.01 0.01 0.01 0.00
Dependency 3.96 4.32 7.03 0.06 0.06 0.00
Template 5.16 4.62 7.26 0.03 0.06 0.00
Template-S 4.25 4.31 7.04 0.18 0.13 0.00
Overall
Permute&Rank 5.25 5.45 5.78 0.05 0.03 0.01
Dependency 3.93 4.28 6.20 0.07 0.06 0.01
Template 4.73 4.46 6.25 0.05 0.05 0.01
Template-S 3.86 4.04 6.03 0.23 0.17 0.02
Table 2: Results for realization
rate speaker constraints, we again see a performance
jump, although compared to the best possible case
(full brevity) there is still room for improvement.
Discussion We have shown that by using speaker
and recency constraints in standard algorithms, it
is possible to achieve performance gains on the at-
tribute selection task.
The most relevant previous research is the work of
(Gupta and Stent, 2005), who modified Dale and Re-
iter?s algorithm to model speaker adaptation in dia-
log. However, this corpus does not involve dialog so
there are no cross-speaker constraints, only within-
speaker constraints (style and priming).
5 Surface Realization Experiments
Data Preparation We first normalize the training
data to correct misspellings and remove punctuation
and capitalization. We then extract a phrasal lexi-
con. For each attribute value we extract the count of
all realizations of that value in the training data. We
treat locations as a special case, storing separately
the realizations of x-y coordinate pairs and single
x- or y-coordinates. We add a small number of re-
alizations to the lexicon by hand to cover possible
attribute values not seen in the training data.
Results Table 2 shows the evaluation results for
string-edit distance and string accuracy on the devel-
opment set with three different attributes sets: DEV
? attributes selected by the human test; FB-sf ? at-
tributes generated by the full brevity algorithm with
speaker frequency; and DR-sf ? attributes selected
213
by the Dale & Reiter algorithm with speaker fre-
quency.
For the TUNA realization task (DEV attributes),
our approaches work better for the furniture domain,
where there are fewer attributes, than for the people
domain. For the furniture domain, the Template-S
approach achieves lowest string-edit distance, while
for the people domain, the Dependency approach
achieves lowest string-edit distance. The latter
method was submitted for human evaluation.
When we consider the ?end-to-end? referring
expression generation task (FB-sf and DR-sf at-
tributes), the best overall performing system is the
speaker-based template generator with full-brevity
and speaker frequency attribute selection. In terms
of generated sentence quality, a preliminary and
qualitative analysis shows that the combination Per-
mute & Rank and DR-sf produces more naturalistic
phrases.
Discussion Although the Template-S approach
achieves the best string edit distance scores over-
all, it is not very robust. If no examples were found
in the training data neither Template approach will
produce no output. (This happens twice for each of
the domains on the development data.) The Depen-
dency approach achieves good overall performance
with more robustness.
The biggest cause of errors for the Permute
and Reorder approach was missing determiners and
missing modifiers. The biggest cause of errors for
the Dependency approach was missing determiners
and reordered words. The Template approach some-
times had repeated words (e.g. ?middle?, where
?middle? referred to both x- and y-coordinates).
6 Conclusions
When building computational models of language,
knowledge about the factors that influence human
language production can prove very helpful. This
knowledge can be incorporated in frequentist and
heuristic approaches as constraints or features. In
the experiments described in this paper, we used
data-driven, speaker-aware approaches to attribute
selection and referring expression realization. We
showed that individual speaking style can be use-
fully modeled even for quite ?small? generation
tasks, and confirmed that data-driven approaches to
surface realization can work well using a range of
lexical, syntactic and semantic information.
In addition to individual style and priming, an-
other potentially fruitful area for exploration with
TUNA-style tasks is human visual search strategies
(Rayner, 1998). We leave this idea for future work.
Acknowledgments
We thank Anja Belz, Albert Gatt, and Eric Kow
for organizing the REG competition and providing
data, and Gregory Zelinsky for discussions about
visually-based constraints.
References
S. Bangalore and O. Rambow. 2000. Exploiting a prob-
abilistic hierarchical model for generation. In Proc.
COLING.
S. Bangalore, A. Emami, and P. Haffner. 2005. Factor-
ing global inference by enriching local representations.
Technical report, AT&T Labs-Research.
A. Belz and A. Gatt. 2007. The attribute selection for
GRE challenge: Overview and evaluation results. In
Proceedings of UCNLG+MT at MT Summit XI.
A. Belz. 2007. Probabilistic generation of weather fore-
cast texts. In Proceedings of NAACL/HLT.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(2).
Robert Dale. 1992. Generating Referring Expressions:
Constructing Descriptions in a Domain of Objects and
Processes. MIT Press, Cambridge, MA.
S. Gupta and A. Stent. 2005. Automatic evaluation of
referring expression generation using corpora. In Pro-
ceedings of UCNLG.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proc. ACL Work-
shop on Building and Using Parallel Texts.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Computa-
tional Linguistics, 29(1).
I. Langkilde and K. Knight. 2000. Forest-based statisti-
cal sentence generation. In Proc. NAACL.
W. Levelt, 1989. Speaking: From intention to articula-
tion, pages 222?226. MIT Press.
A. Nasr and O. Rambow. 2004. Supertagging and
full parsing. In Proc. 7th International Workshop
on Tree Adjoining Grammar and Related Formalisms
(TAG+7).
K. Rayner. 1998. Eye movements in reading and infor-
mation processing: 20 years of research. Psychologi-
cal Bulletin, 124(3).
E. Reiter and R. Dale. 2000. Building Natural Language
Generation Systems. Cambridge University Press.
214
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 151?158
Manchester, August 2008
Trainable Speaker-Based Referring Expression Generation
Giuseppe Di Fabbrizio and Amanda J. Stent and Srinivas Bangalore
AT&T Labs - Research, Inc.
180 Park Avenue
Florham Park, NJ 07932, USA
{pino,stent,srini}@research.att.com
Abstract
Previous work in referring expression gen-
eration has explored general purpose tech-
niques for attribute selection and surface
realization. However, most of this work
did not take into account: a) stylistic dif-
ferences between speakers; or b) trainable
surface realization approaches that com-
bine semantic and word order information.
In this paper we describe and evaluate sev-
eral end-to-end referring expression gener-
ation algorithms that take into considera-
tion speaker style and use data-driven sur-
face realization techniques.
1 Introduction
Natural language generation (NLG) systems have
typically decomposed the problem of generating
a linguistic expression from a conceptual specifi-
cation into three major steps: content planning,
text planning and surface realization (Reiter and
Dale, 2000). The task in content planning is to
select the information that is to be conveyed to
maximize communication efficiency. The task in
text planning and surface realization is to use the
available linguistic resources (words and syntax) to
convey the selected information using well-formed
linguistic expressions.
During a discourse (whether written or spoken,
monolog or dialog), a number of entities are in-
troduced into the discourse context shared by the
reader/hearer and the writer/speaker. Construct-
ing linguistic references to these entities efficiently
and effectively is a problem that touches on all
c? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
parts of an NLG system. Traditionally, this prob-
lem is split into two parts. The task of selecting
the attributes to use in referring to an entity is the
attribute selection task, performed during content
planning or sentence planning. The actual con-
struction of the referring expression is part of sur-
face realization.
There now exist numerous general-purpose al-
gorithms for attribute selection (e.g., (Dale and Re-
iter, 1995; Krahmer et al, 2003; Belz and Gatt,
2007; Siddharthan and Copestake, 2004)). How-
ever, these algorithms by-and-large focus on the
algorithmic aspects of referring expression gener-
ation rather than on psycholinguistic factors that
influence language production. For example, we
know that humans exhibit individual differences in
language production that can be quite pronounced
(e.g. (Belz, 2007)). We also know that the
language production process is subject to lexical
priming, which means that words and concepts that
have been used recently are likely to appear again
(Levelt, 1989).
In this paper, we look at attribute selection and
surface realization for referring expression gener-
ation using the TUNA corpus 1, an annotated cor-
pus of human-produced referring expressions that
describe furniture and people. We first explore
the impact of individual style and priming on at-
tribute selection for referring expression genera-
tion. To get an idea of the potential improvement
when modeling these factors, we implemented a
version of full brevity search that uses speaker-
specific constraints, and another version that also
uses recency constraints. We found that using
speaker-specific constraints led to big performance
gains for both TUNA domains, while the use of re-
1http://www.csd.abdn.ac.uk/research/tuna/
151
cency constraints was not as effective for TUNA-
style tasks. We then modified Dale and Reiter?s
classic attribute selection algorithm (Dale and Re-
iter, 1995) to model individual differences in style,
and found performance gains in this more greedy
approach as well.
Then, we look at surface realization for re-
ferring expression generation. There are sev-
eral approaches to surface realizations described
in the literature (Reiter and Dale, 2000) rang-
ing from hand-crafted template-based realizers to
data-driven syntax-based realizers (Langkilde and
Knight, 2000; Bangalore and Rambow, 2000).
Template-based realization provides a straightfor-
ward method to fill out pre-defined templates with
the current attribute values. Data-driven syntax-
based methods employ techniques that incorporate
the syntactic relations between words which can
potentially go beyond local adjacency relations.
Syntactic information also helps in eliminating un-
grammatical sentence realizations. At the other ex-
treme, there are techniques that exhaustively gen-
erate possible realizations with recourse to syntax
in as much as it is reflected in local n-grams. Such
techniques have the advantage of being robust al-
though they are inadequate to capture long-range
dependencies. We explore three techniques for
the task of referring expression generation that are
different hybrids of hand-crafted and data-driven
methods.
The layout of this paper is as follows: In Sec-
tion 2, we describe the TUNA data set and the task
of identifying target entities in the context of dis-
tractors. In Section 3, we present our algorithms
for attribute selection. Our algorithms for sur-
face realization are presented in Section 4. Our
evaluation of these methods for attribute selection
and surface realization are presented in Sections 5
and 6.
2 The TUNA Corpus
The TUNA corpus was constructed using a web-
based experiment. Participants were presented
with a sequence of web pages, on each of which
they saw displayed a selection of 7 pictures of ei-
ther furniture (e.g. Figure 1) or people (e.g. Fig-
ure 2) sparsely placed on a 3 row x 5 column
grid. One of the pictures (the target) was high-
lighted; the other 6 objects (the distractors) were
randomly selected from the object database. Par-
ticipants were told that they were interacting with a
computer system to remove all but the highlighted
picture from the screen. They entered a description
of the object using natural language to identify the
object to the computer system.
The section of the TUNA corpus we used was
that provided for the REG 2008 Challenge2. The
training data includes 319 referring expressions in
the furniture domain and 274 in the people domain.
The development data (which we used for testing)
includes 80 referring expressions in the furniture
domain and 68 in the people domain.
Figure 1: Example of data from the furniture do-
main (The red couch on top).
Figure 2: Example of data from the people domain
(The bald subject on the bottom with the white
beard).
3 Attribute Selection Algorithms
Given a set of entities with attributes appropriate
to a domain (e.g., cost of flights, author of a book,
2http://www.nltg.brighton.ac.uk/research/reg08/. Prelimi-
nary versions of these algorithms were used in this challenge
and presented at INLG 2008.
152
color of a car) that are in a discourse context, and a
target entity that needs to be identified, the task of
attribute selection is to select a subset of the at-
tributes that uniquely identifies the target entity.
(Note that there may be more than one such at-
tribute set.) The efficacy of attribute selection can
be measured based on the minimality of the se-
lected attribute set as well as its ability to deter-
mine the target entity uniquely. There are varia-
tions however in terms of what makes an attribute
set more preferable to a human. For example, in
a people identification task, attributes of faces are
generally more memorable than attributes pertain-
ing to outfits. In this paper, we demonstrate that
the attribute set is speaker dependent.
In this section, we present two different attribute
selection algorithms. The Full Brevity algorithm
selects the attribute set by exhaustively searching
through all possible attribute sets. In contrast, Dale
and Reiter algorithm orders the attributes based
on a heuristic (motivated by human preference)
and selects the attributes in that order until the tar-
get entity is uniquely determined. We elaborate on
these algorithms below.
Full Brevity (FB) We implemented a version of
full brevity search. It does the following: first,
it constructs AS, the set of attribute sets that
uniquely identify the referent given the distrac-
tors. Then, it selects an attribute set ASu ? AS
based on one of the following four criteria: 1) The
minimality (FB-m) criterion selects from among
the smallest elements of AS at random. 2) The
frequency (FB-f) criterion selects the element of
AS that occurred most often in the training data.
3) The speaker frequency (FB-sf) criterion se-
lects the element of AS used most often by this
speaker in the training data, backing off to FB-f if
necessary. This criterion models individual speak-
ing/writing style. 4) Finally, the speaker recency
(FB-sr) criterion selects the element of AS used
most recently by this speaker in the training data,
backing off to FB-sf if necessary. This criterion
models priming.
Dale and Reiter We implemented two variants
of the classic Dale & Reiter attribute selection
(Dale and Reiter, 1995) algorithm. For Dale &
Reiter basic (DR-b), we first build the preferred
list of attributes by sorting the attributes according
to frequency of use in the training data. We keep
separate lists based on the ?LOC? condition (if its
value was ?+LOC?, the participants were told that
they could refer to the target using its location on
the screen; if it was ?-LOC?, they were instructed
not to use location on the screen) and backoff to
a global preferred attribute list if necessary. Next,
we iterate over the list of preferred attributes and
select the next one that rules out at least one en-
tity in the contrast set until no distractors are left.
Dale & Reiter speaker frequency (DR-sf) uses
a different preferred attribute list for each speaker,
backing off to the DR-b preferred list if an attribute
has never been observed in the current speaker?s
preferred attribute list. For the purpose of this task,
we did not use any external knowledge (e.g. tax-
onomies).
4 Surface Realization Approaches
A surface realizer for referring expression genera-
tion transforms a set of attribute-value pairs into a
linguistically well-formed expression. Our surface
realizers, which are all data-driven, involve four
stages of processing: (a) lexical choice of words
and phrases to realize attribute values; (b) genera-
tion of a space of surface realizations (T ); (c) rank-
ing the set of realizations using a language model
(LM ); (d) selecting the best scoring realization.
In general, the best ranking realization (T?) is de-
scribed by equation 1:
T ? = Bestpath(Rank(T,LM)) (1)
We describe three different methods for creating
the search space of surface realizations ? Template-
based, Dependency-based and Permutation-based
methods. Although these techniques share the
same method for ranking, they differ in the meth-
ods used for generating the space of possible sur-
face realizations.
4.1 Generating possible surface realizations
In order to transform the set of attribute-value
pairs into a linguistically well-formed expression,
the appropriate words that realize each attribute
value need to be selected (lexical choice) and the
selected words need to be ordered according to
the syntax of the target language (lexical order).
We present different models for approximating the
syntax of the target language. All three models
tightly integrate the lexical choice and lexical re-
ordering steps.
153
4.1.1 Template-Based Realizer
In the template-based approach, surface realiza-
tions from our training data are used to infer a set
of templates. In the TUNA data, each attribute in
each referring expression is annotated with its at-
tribute type (e.g. in ?the large red sofa? the sec-
ond word is labeled ?size?, the third ?color? and
the fourth ?type?). We extract the annotated re-
ferring expressions from each trial in the training
data and replace each attribute value with its type
(e.g. ?the size color type?) to create a tem-
plate. Each template is indexed by the lexicograph-
ically sorted list of attribute types it contains (e.g.
color size type). If an attribute set is not
found in the training data (e.g. color size)
but a superset of that set is (e.g. color size
type), then the corresponding template(s) may be
used, with the un-filled attribute types deleted prior
to output.
At generation time, we find all possible realiza-
tions (l) (from the training data) of each attribute
value (a) in the input attribute set (AS), and fill in
each possible template (t) with each combination
of the attribute realizations. The space of possible
surface realizations is represented as a weighted
finite-state automaton. The weights are computed
from the prior probability of each template and
the prior probability of each lexical item realizing
an attribute (Equation 2). We have two versions
of this realizer: one with speaker-specific lexi-
cons and templates (Template-S), and one without
(Template). We report results for both.
P (T |AS) =
?
t
P (t|AS)?
?
a?t
?
l
P (l|a, t) (2)
4.1.2 Dependency-Based Realizer
To construct our dependency-based realizer, we
first parse all the word strings from the train-
ing data using the dependency parser described
in (Bangalore et al, 2005; Nasr and Rambow,
2004). Then, for every pair of words wi, wj that
occur in the same referring expression (RE) in the
training data, we compute: freq(i < j), the fre-
quency with which wi precedes wj in any RE;
freq(dep(wi, wj) ? i < j), the frequency with
which wi depends on and precedes wj in any RE,
and freq(dep(wi, wj)?j < i), the frequency with
which wi depends on and follows wj in any RE.
At generation time, we find all possible realiza-
tions of each attribute value in the input attribute
set, and for each combination of attribute realiza-
tions, we find the most likely set of dependencies
and precedences given the training data. In other
words, we bin the selected attribute realizations
according to whether they are most likely to pre-
cede, depend on and precede, depend on and fol-
low, or follow, the head word they are closest to.
The result is a set of weighted partial orderings on
the attribute realizations. As with the template-
based surface realizer, we implemented speaker-
specific and speaker-independent versions of the
dependency-based surface realizer. Once again,
we encode the space of possible surface realiza-
tions as a weighted finite-state automaton.
4.1.3 Permute and Rank Realizer
In this method, the lexical items associated with
each attribute value to be realized are treated as a
disjunctive set of tokens. This disjunctive set is
represented as a finite-state automaton with two
states and transitions between them labeled with
the tokens of the set. The transitions are weighted
by the negative logarithm of the probability of the
lexical token (l) being associated with that attribute
value (a): (?log(P (l|a))). These sets are treated
as bags of tokens; we create permutations of these
bags of tokens to represent the set of possible sur-
face realizations.
In general, the number of states of the minimal
permutation automaton of even a linear automaton
(finite-state representation of a string) grows expo-
nentially with the number of words of the string.
Although creating the full permutation automaton
for full natural language generation tasks could
be computationally prohibitive, most attribute sets
in our two domains contain no more than five at-
tributes. So we choose to explore the full permu-
tation space. A more general approach might con-
strain permutations to be within a local window of
adjustable size (also see (Kanthak et al, 2005)).
Figure 3 shows the minimal permutation au-
tomaton for an input sequence of 4 words and a
window size of 2. Each state of the automaton is
indexed by a bit vector of size equal to the number
of words/phrases of the target sentence. Each bit
of the bit vector is set to 1 if the word/phrase in
that bit position is used on any path from the initial
to the current state. The next word for permutation
from a given state is restricted to be within the win-
dow size (2 in our case) positions counting from
the first as-yet uncovered position in that state. For
example, the state indexed with vector ?1000? rep-
154
0000
10001
0100
2
1100
2
10103
1
1110
3
1101
4
1111
4
3
2
Figure 3: Locally constraint permutation automaton for a sentence with 4 positions and a window size
of 2.
resents the fact that the word/phrase at position 1
has been used. The next two (window=2) posi-
tions are the possible outgoing arcs from this state
with labels 2 and 3 connecting to state ?1100? and
?1010? respectively. The bit vectors of two states
connected by an arc differ only by a single bit.
Note that bit vectors elegantly solve the problem of
recombining paths in the automaton as states with
the same bit vectors can be merged. As a result, a
fully minimized permutation automaton has only a
single initial and final state.
4.2 Ranking and Recovering a Surface
Realization
These three methods for surface realization create
a space of possible linguistic expressions given the
set of attributes to be realized. These expressions
are encoded as finite-state automata and have to be
ranked based on their syntactic well-formedness.
We approximate the syntactic well-formedness of
an expression by the n-gram likelihood score of
that expression. We use a trigram model trained
on the realizations in the training corpus. This
language model is also represented as a weighted
finite-state automaton. The automaton represent-
ing the space of possible realizations and the one
representing the language model are composed.
The result is an automaton that ranks the possible
realizations according to their n-gram likelihood
scores. We then produce the best-scoring realiza-
tion as the target realization of the input attribute
set.
We introduce a parameter ? which allows us
to control the importance of the prior score rela-
tive to the language model scores. We weight the
finite-state automata according to this parameter as
shown in Equation 3.
T ? = Bestpath(? ? T ? (1 ? ?) ? LM) (3)
DICE MASI Acc. Uniq. Min.
Furniture
FB-m .36 .16 0 1 1
FB-f .81 .58 .40 1 0
FB-sf .95 .87 .79 1 0
FB-sr .93 .81 .71 1 0
DR-b .81 .60 .45 1 0
DR-sf .86 .64 .45 1 .04
People
FB-m .26 .12 0 1 1
FB-f .58 .37 .28 1 0
FB-sf .94 .88 .84 1 .01
FB-sr .93 .85 .79 1 .01
DR-b .70 .45 .25 1 0
DR-sf .78 .55 .35 1 0
Overall
FB-m .32 .14 0 1 1
FB-f .70 .48 .34 1 0
FB-sf .95 .87 .81 1 .01
FB-sr .93 .83 .75 1 .01
DR-b .76 .53 .36 1 0
DR-sf .82 .60 .41 1 .02
Table 1: Results for attribute selection
5 Attribute Selection Experiments
Data Preparation The training data were used
to build the models outlined above. The develop-
ment data were then processed one-by-one.
Metrics We report performance using the met-
rics used for the REG 2008 competition. The
MASI metric is a metric used in summarization
that measures agreement between two annotators
(or one annotator and one system) on set-valued
items (Nenkova et al, 2007). Values range from
0 to 1, with 1 representing perfect agreement.
The DICE metric is also a measure of association
whose value varies from 0 (no association) to 1 (to-
tal association) (Dice, 1945). The Accuracy met-
ric is binary-valued: 1 if the attribute set is iden-
tical to that selected by the human, 0 otherwise.
The Uniqueness metric is also binary-valued: 1 if
the attribute set uniquely identifies the target refer-
ent among the distractors, 0 otherwise. Finally, the
Minimality metric is 1 if the selected attribute set
is as small as possible (while still uniquely identi-
fying the target referent), and 0 otherwise. We note
155
that attribute selection algorithms such as Dale &
Reiter?s are based on the observation that humans
frequently do not produce minimal referring ex-
pressions.
Results Table 1 shows the results for variations
of full brevity. As we would expect, all approaches
achieve a perfect score on uniqueness. For both
corpora, we see a large performance jump when
we use speaker constraints for all metrics other
than minimality. However, when we incorporate
recency constraints as well performance declines
slightly. We think this is due to two factors: first,
the speakers are not in a conversation, and self-
priming may have less impact than other-priming;
and second, we do not always have the most recent
prior utterance for a given speaker in the training
data.
Table 1 also shows the results for variations of
Dale & Reiter?s algorithm. When we incorporate
speaker constraints, we again see a performance
jump for most metrics, although compared to the
best possible case (full brevity) there is still room
for improvement.
We conclude that speaker constraints can be suc-
cessfully used in standard attribute selection algo-
rithms to improve performance on this task.
The most relevant previous research is the work
of (Gupta and Stent, 2005), who modified Dale
and Reiter?s algorithm to model speaker adaptation
in dialog. However, this corpus does not involve
dialog so there are no cross-speaker constraints,
only within-speaker constraints (speaker style and
priming).
6 Surface Realization Experiments
Data Preparation We first normalized the train-
ing data to correct misspellings and remove punc-
tuation and capitalization. We then extracted a
phrasal lexicon. For each attribute value we ex-
tracted the count of all realizations of that value in
the training data. We treated locations as a spe-
cial case, storing separately the realizations of x-
y coordinate pairs and single x- or y-coordinates.
We added a small number of realizations by hand
to cover possible attribute values not seen in the
training data.
Realization We ran two realization experiments.
In the first experiment, we used the human-
selected attribute sets in the development data as
the input to realization. If we want to maxi-
? SED ACC Bleu NIST
Furniture
Permute&Rank 0.01 3.54 0.14 0.311 3.87
Dependency 0.90 4.51 0.09 0.206 3.29
Dependency-S 0.60 4.30 0.11 0.232 3.91
Template 0.10 3.59 0.13 0.328 3.93
Template-S 0.10 2.80 0.28 0.403 4.67
People
Permute&Rank 0.04 4.37 0.10 0.227 3.15
Dependency 0.70 6.10 0.00 0.072 2.35
Dependency-S 0.50 5.84 0.02 0.136 3.05
Template 0.80 3.87 0.07 0.250 3.18
Template-S 0.70 3.79 0.15 0.265 3.59
Overall
Permute&Rank .01/.04 3.92 0.12 0.271 4.02
Dependency 0.9/0.7 5.24 0.05 0.146 3.23
Dependency-S 0.6/0.5 5.01 0.07 0.187 3.98
Template 0.1/0.8 3.77 0.10 0.285 4.09
Template-S 0.1/0.7 3.26 0.22 0.335 4.77
Table 2: Results for realization using speakers? at-
tribute selection (SED: String Edit Distance, ACC:
String Accuracy)
mize humanlikeness, then using these attribute sets
should give us an idea of the best possible perfor-
mance of our realization methods. In the second
experiment, we used the attribute sets output by
our best-performing attribute selection algorithms
(FB-sf and DR-sf) as the input to realization.
Metrics We report performance of our surface
realizers using the metrics used for the REG 2008
shared challenge and standard metrics used in the
natural language generation and machine trans-
lation communities. String Edit Distance (SED)
is a measure of the number of words that would
have to be added, deleted, or replaced in order to
transform the generated referring expression into
the one produced by the human. As used in the
REG 2008 shared challenge, it is unnormalized, so
its values range from zero up. Accuracy (ACC)
is binary-valued: 1 if the generated referring ex-
pression is identical to that produced by the hu-
man (after spelling correction and normalization),
and 0 otherwise. Bleu is an n-gram based met-
ric that counts the number of 1, 2 and 3 grams
shared between the generated string and one or
more (preferably more) reference strings (Papenini
et al, 2001). Bleu values are normalized and range
from 0 (no match) to 1 (perfect match). Finally,
the NIST metric is a variation on the Bleu met-
ric that, among other things, weights rare n-grams
higher than frequently-occurring ones (Dodding-
ton, 2002). NIST values are unnormalized.
156
SED ACC Bleu NIST
Furniture
FB-sf DR-sf FB-sf DR-sf FB-sf DR-sf FB-sf DR-sf
Permute&Rank 3.97 4.22 0.09 0.06 .291 .242 3.82 3.32
Dependency 4.80 5.03 0.04 0.03 .193 .105 3.32 2.46
Dependency-S 4.71 4.88 0.06 0.04 .201 .157 3.74 3.26
Template 3.89 4.56 0.09 0.05 .283 .213 3.48 3.22
Template-S 3.26 3.90 0.19 0.12 .362 .294 4.41 4.07
People
Permute&Rank 4.75 5.82 0.09 0.03 .171 .110 2.70 2.31
Dependency 6.35 6.91 0.00 0.00 .068 .073 1.81 1.86
Dependency-S 5.94 6.18 0.01 0.00 .108 .113 2.73 2.41
Template 3.62 4.24 0.07 0.04 .231 .138 2.88 1.35
Template-S 3.76 4.38 0.12 0.06 .201 .153 2.76 1.88
Overall
Permute&Rank 4.33 4.96 0.09 0.05 .236 .235 3.73 3.72
Dependency 5.51 6.00 0.02 0.01 .136 .091 2.97 2.50
Dependency-S 5.36 5.67 0.04 0.02 .159 .136 3.77 3.25
Template 3.76 4.41 0.08 0.05 .258 .180 3.69 2.89
Template-S 3.48 4.12 0.16 0.09 .288 .229 4.15 3.58
Table 3: Results for realization with different attribute selection algorithms
Furniture People
FB-sf DR-sf FB-sf DR-sf
Permute&Rank .01 .05 .05 .04
Dependency .9 .9 .9 .1
Dependency-S .2 .2 .4 .4
Template .8 .8 .8 .8
Template-S .6 .8 .8 .8
Table 4: Optimal ? values with different attribute
selection algorithms
Results Our experimental results are shown in
Tables 2 and 3. (These results are the results
obtained with the language model weighting that
gives best performance; the weights are shown in
Tables 2 and 4.) Our approaches work better for
the furniture domain, where there are fewer at-
tributes, than for the people domain. For both
domains, for automatic and human attribute se-
lection, the speaker-dependent Template-based ap-
proach seems to perform the best, then the speaker-
independent Template-based approach, and then
the Permute&Rank approach. However, we find
automatic metrics for evaluating generation qual-
ity to be unreliable. We looked at the output of the
surface realizers for the two examples in Section 2.
The best output for the example in Figure 1 is from
the FB-sf template-based speaker-dependent algo-
rithm, which is the big red sofa. The worst out-
put is from the DR-sf dependency-based speaker-
dependent algorithm, which is on the left red chair
with three seats. The best output for the exam-
ple in Figure 2 is from the FB-sf template-based
speaker-independent algorithm, which is the man
with the white beard. The worst output is from the
FB-sf dependency-based speaker-dependent algo-
rithm, which is beard man white.
Discussion The Template-S approach achieves
the best string edit distance scores, but it is not very
robust. If no examples are found in the training
data that realize (a superset of) the input attribute
set, neither Template approach will produce any
output.
The biggest cause of errors for the Permute and
Reorder approach is missing determiners and miss-
ing modifiers. The biggest cause of errors for the
Dependency approach is missing determiners and
reordered words. The Template approach some-
times has repeated words (e.g. ?middle?, where
?middle? referred to both x- and y-coordinates).
Here we report performance using automatic
metrics, but we find these metrics to be unreliable
(particularly in the absence of multiple reference
texts). Also, we are not sure that people would ac-
cept from a computer system output that is very
human-like in this domain, as the human-like out-
put is often ungrammatical and telegraphic (e.g.
?grey frontal table?). We plan to do a human eval-
uation soon to better analyze our systems? perfor-
mance.
7 Conclusions
When building computational models of language,
knowledge about the factors that influence human
language production can prove very helpful. This
knowledge can be incorporated in frequentist and
heuristic approaches as constraints or features. In
the experiments described in this paper, we used
157
data-driven, speaker-aware approaches to attribute
selection and referring expression realization. We
showed that individual speaking style can be use-
fully modeled even for quite ?small? generation
tasks, and confirmed that data-driven approaches
to surface realization can work well using a range
of lexical, syntactic and semantic information.
We plan to explore the impact of human visual
search strategies (Rayner, 1998) on the referring
expression generation task. In addition, we are
planning a human evaluation of the generation sys-
tems? output. Finally, we plan to apply our algo-
rithms to a conversational task.
Acknowledgments
We thank Anja Belz, Albert Gatt, and Eric Kow
for organizing the REG competition and providing
data, and Gregory Zelinsky for discussions about
visually-based constraints.
References
Bangalore, S. and O. Rambow. 2000. Exploiting a
probabilistic hierarchical model for generation. In
Proc. COLING.
Bangalore, S., A. Emami, and P. Haffner. 2005. Fac-
toring global inference by enriching local represen-
tations. Technical report, AT&T Labs-Research.
Belz, A. and A. Gatt. 2007. The attribute selection for
GRE challenge: Overview and evaluation results. In
Proc. UCNLG+MT at MT Summit XI.
Belz, A. 2007. Probabilistic generation of weather
forecast texts. In Proc. NAACL/HLT.
Dale, R. and E. Reiter. 1995. Computational interpre-
tations of the Gricean maxims in the generation of
referring expressions. Cognitive Science, 19(2).
Dice, L. 1945. Measures of the amount of ecologic
association between species. Ecology, 26.
Doddington, G. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. HLT.
Gupta, S. and A. Stent. 2005. Automatic evaluation
of referring expression generation using corpora. In
Proc. UCNLG.
Kanthak, S., D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proc. ACL Work-
shop on Building and Using Parallel Texts.
Krahmer, E., S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Computa-
tional Linguistics, 29(1).
Langkilde, I. and K. Knight. 2000. Forest-based statis-
tical sentence generation. In Proc. NAACL.
Levelt, W., 1989. Speaking: From intention to articu-
lation, pages 222?226. MIT Press.
Nasr, A. and O. Rambow. 2004. Supertagging and
full parsing. In Proc. 7th International Workshop on
Tree Adjoining Grammar and Related Formalisms
(TAG+7).
Nenkova, A., R. Passonneau, and K. McKeown. 2007.
The Pyramid method: incorporating human con-
tent selection variation in summarization evaluation.
ACM Transactions on speech and language process-
ing, 4(2).
Papenini, K., S. Roukos, T. Ward, andW.-J. Zhu. 2001.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proc. ACL.
Rayner, K. 1998. Eye movements in reading and infor-
mation processing: 20 years of research. Psycholog-
ical Bulletin, 124(3).
Reiter, E. and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
Siddharthan, A. and A. Copestake. 2004. Generat-
ing referring expressions in open domains. In Proc.
ACL.
158
Finding Common Ground: Towards a Surface Realisation Shared Task
Anja Belz
Natural Language Technology Group
Computing, Mathematical and Information Sciences
University of Brighton, Brighton BN2 4GJ, UK
a.s.belz@brighton.ac.uk
Mike White
Department of Linguistics
The Ohio State University
Columbus, OH, USA
mwhite@ling.osu.edu
Josef van Genabith and Deirdre Hogan
National Centre for Language Technology
School of Computing
Dublin City University
Dublin 9, Ireland
{dhogan,josef}@computing.dcu.ie
Amanda Stent
AT&T Labs Research, Inc.,
180 Park Avenue
Florham Park, NJ 07932, USA
stent@research.att.com
Abstract
In many areas of NLP reuse of utility tools
such as parsers and POS taggers is now
common, but this is still rare in NLG. The
subfield of surface realisation has perhaps
come closest, but at present we still lack
a basis on which different surface realis-
ers could be compared, chiefly because of
the wide variety of different input repre-
sentations used by different realisers. This
paper outlines an idea for a shared task in
surface realisation, where inputs are pro-
vided in a common-ground representation
formalism which participants map to the
types of input required by their system.
These inputs are derived from existing an-
notated corpora developed for language
analysis (parsing etc.). Outputs (realisa-
tions) are evaluated by automatic compari-
son against the human-authored text in the
corpora as well as by human assessors.
1 Background
When reading a paper reporting a new NLP sys-
tem, it is common these days to find that the
authors have taken an NLP utility tool off the
shelf and reused it. Researchers frequently reuse
parsers, POS-taggers, named entity recognisers,
coreference resolvers, and many other tools. Not
only is there a real choice between a range of dif-
ferent systems performing the same task, there are
also evaluation methodologies to help determine
what the state of the art is.
Natural Language Generation (NLG) has not
so far developed generic tools and methods for
comparing them to the same extent as Natural
Language Analysis (NLA) has. The subfield of
NLG that has perhaps come closest to developing
generic tools is surface realisation. Wide-coverage
surface realisers such as PENMAN/NIGEL (Mann
and Mathiesen, 1983), FUF/SURGE (Elhadad and
Robin, 1996) and REALPRO (Lavoie and Ram-
bow, 1997) were intended to be more or less off-
the-shelf plug-and-play modules. But they tended
to require a significant amount of work to adapt
and integrate, and required highly specific inputs
incorporating up to several hundred features that
needed to be set.
With the advent of statistical techniques in NLG
surface realisers appeared for which it was far sim-
pler to supply inputs, as information not provided
in the inputs could be added on the basis of like-
lihood. An early example, the Japan-Gloss sys-
tem (Knight et al, 1995) replaced PENMAN?s de-
fault settings with statistical decisions. The Halo-
gen/Nitrogen developers (Langkilde and Knight,
1998a) allowed inputs to be arbitrarily underspec-
ified, and any decision not made before the realiser
was decided simply by highest likelihood accord-
ing to a language model, automatically trainable
from raw corpora.
The Halogen/Nitrogen work sparked an interest
in statistical NLG which led to a range of surface
realisation methods that used corpus frequencies
in one way or another (Varges and Mellish, 2001;
White, 2004; Velldal et al, 2004; Paiva and Evans,
2005). Some surface realisation work looked at
directly applying statistical models during a lin-
guistically informed generation process to prune
the search space (White, 2004; Carroll and Oepen,
2005).
While statistical techniques have led to realisers
that are more (re)usable, we currently still have
no way of determining what the state of the art
is. A significant subset of statistical realisation
work (Langkilde, 2002; Callaway, 2003; Nakan-
ishi et al, 2005; Zhong and Stent, 2005; Cahill and
van Genabith, 2006; White and Rajkumar, 2009)
has recently produced results for regenerating the
Penn Treebank. The basic approach in all this
work is to remove information from the Penn Tree-
bank parses (the word strings themselves as well
as some of the parse information), and then con-
vert and use these underspecified representations
as inputs to the surface realiser whose task it is to
reproduce the original treebank sentence. Results
are typically evaluated using BLEU, and, roughly
speaking, BLEU scores go down as more informa-
tion is removed.
While publications of work along these lines do
refer to each other and (tentatively) compare BLEU
scores, the results are not in fact directly compara-
ble, because of the differences in the input repre-
sentations automatically derived from Penn Tree-
bank annotations. In particular, the extent to which
they are underspecified varies from one system to
the next.
The idea we would like to put forward with
this short paper is to develop a shared task in sur-
face realisation based on common inputs and an-
notated corpora of paired inputs and outputs de-
rived from various resources from NLA that build
on the Penn Treebank. Inputs are provided in a
common-ground representation formalism which
participants map to the types of input required by
their system. These inputs are automatically de-
rived from the Penn Treebank and the various lay-
ers of annotation (syntactic, semantic, discourse)
that have been developed for the documents in it.
Outputs (realisations) are evaluated by automatic
comparison against the human-authored text in the
corpora as well as by by human assessors.
In the short term, such a shared task would
make existing and new approaches directly com-
parable by evaluation on the benchmark data asso-
ciated with the shared task. In the long term, the
common-ground input representation may lead to
a standardised level of representation that can act
as a link between surface realisers and preceding
modules, and can make it possible to use alterna-
tive surface realisers as drop-in replacements for
each other.
2 Towards Common Inputs
One hugely challenging aspect in developing a
Surface Realisation task is developing a common
input representation that all, or at least a major-
ity of, surface realisation researchers are happy to
work with. While many different formalisms have
been used for input representations to surface re-
alisers, one cannot simply use e.g. van Genabith
et al?s automatically generated LFG f-structures,
White et als CCG logical forms, Nivre?s depen-
dencies, Miyao et al?s HPSG predicate-argument
structures or Copestake?s MRSs etc., as each of
them would introduce a bias in favour of one type
of system.
One possible solution is to develop a meta-
representation which contains, perhaps on multi-
ple layers of representation, all the information
needed to map to any of a given set of realiser in-
put representations, a common-ground representa-
tion that acts as a kind of interlingua for translating
between different input representations.
An important issue in deriving input repre-
sentations from semantically, syntactically and
discourse-annotated corpora is deciding what in-
formation not to include. A concern is that mak-
ing such decisions by committee may be difficult.
One way to make it easier might be to define sev-
eral versions of the task, where each version uses
inputs of different levels of specificity.
Basing a common input representation on what
can feasibly be obtained from non-NLG resources
would put everyone on reasonably common foot-
ing. If, moreover, the common input representa-
tions can be automatically derived from annota-
tions in existing resources, then data can be pro-
duced in sufficient quantities to make it feasible
for participants to automatically learn mappings
from the system-neutral input to their own input.
The above could be achieved by doing some-
thing along the lines of the CoNLL?08 shared task
on Joint Parsing of Syntactic and Semantic De-
pendencies, for which the organisers combined the
Penn Treebank, Propbank, Nombank and the BBN
Named Entity corpus into a dependency represen-
tation. Brief descriptions of these resources and
more details on this idea are provided in Section 4
below.
3 Evaluation
As many NLG researchers have argued, there is
usually not a single right answer in NLG, but var-
ious answers, some better than others, and NLG
tasks should take this into account. If a surface
realisation task is focused on single-best realiza-
tions, then it will not encourage research on pro-
ducing all possible good realizations, or multiple
acceptable realizations in a ranked list, etc. It
may not be the best approach to encourage sys-
tems that try to make a single, safe choice; in-
stead, perhaps one should encourage approaches
that can tell when multiple choices would be ok,
and if some would be better than others.
In the long term we need to develop task defi-
nitions, data resources and evaluation methodolo-
gies that properly take into account the one-to-
many nature of NLG, but in the short term it may be
more realistic to reuse existing non-NLG resources
(which do not provide alternative realisations) and
to adapt existing evaluation methodologies includ-
ing intrinsic assessment of Fluency, Clarity and
Appropriateness by trained evaluators, and auto-
matic intrinsic methods such as BLEU and NIST.
One simple way of adapting the latter, for exam-
ple, could be to calculate scores for the n best re-
alisations produced by a realiser and then to com-
pute a weighted average where scores for reali-
sations are weighted in inverse proportion to the
ranks given to the realisations by the realiser.
4 Data
There is a wide variety of different annotated re-
sources that could be of use in a shared task in sur-
face realisation. Many of these include documents
originally included in the Penn Treebank, and thus
make it possible in principle to combine the var-
ious levels of annotation into a single common-
ground representation. The following is a (non-
exhaustive) list of such resources:
1. Penn Treebank-3 (Marcus et al, 1999): one
million words of hand-parsed 1989 Wall
Street Journal material annotated in Treebank
II style. The Treebank bracketing style al-
lows extraction of simple predicate/argument
structure. In addition to Treebank-1 mate-
rial, Treebank-3 contains documents from the
Switchboard and Brown corpora.
2. Propbank (Palmer et al, 2005): This is a se-
mantic annotation of the Wall Street Journal
section of Penn Treebank-2. More specifi-
cally, each verb occurring in the Treebank has
been treated as a semantic predicate and the
surrounding text has been annotated for ar-
guments and adjuncts of the predicate. The
verbs have also been tagged with coarse
grained senses and with inflectional informa-
tion.
3. NomBank 1.0 (Meyers et al, 2004): Nom-
Bank is an annotation project at New York
University that provides argument structure
for common nouns in the Penn Treebank.
NomBank marks the sets of arguments that
occur with nouns in PropBank I, just as the
latter records such information for verbs.
4. BBN Pronoun Coreference and Entity Type
Corpus (Weischedel and Brunstein, 2005):
supplements the Wall Street Journal corpus,
adding annotation of pronoun coreference,
and a variety of entity and numeric types.
5. FrameNet (Johnson et al, 2002): 150,000
sentences annotated for semantic roles and
possible syntactic realisations. The annotated
sentences come from a variety of sources, in-
cluding some PropBank texts.
6. OntoNotes 2.0 (Weischedel et al, 2008):
OntoNotes 1.0 contains 674k words of Chi-
nese and 500k words of English newswire
and broadcast news data. OntoNotes follows
the Penn Treebank for syntax and PropBank
for predicate-argument structure. Its seman-
tic representation will include word sense
disambiguation for nouns and verbs, with
each word sense connected to an ontology,
and coreference. The current goal is to anno-
tate over a million words each of English and
Chinese, and half a million words of Arabic
over five years.
There are other resources which may be use-
ful. Zettelmoyer and Collins (2009) have man-
ually converted the original SQL meaning an-
notations of the ATIS corpus (et al, 1994)?
some 4,637 sentences?into lambda-calculus ex-
pressions which were used for training and testing
their semantic parser. This resource might make a
good out-of-domain test set for generation systems
trained on WSJ data.
FrameNet, used for semantic parsing, see for
example Gildea and Jurafsky (2002), identifies a
sentence?s frame elements and assigns semantic
roles to the frame elements. FrameNet data (Baker
and Sato, 2003) was used for training and test sets
in one of the SensEval-3 shared tasks in 2004 (Au-
tomatic Labeling of Semantic Roles). There has
been some work combining FrameNet with other
lexical resources. For example, Shi and Mihal-
cea (2005) integrated FrameNet with VerbNet and
WordNet for the purpose of enabling more robust
semantic parsing.
The Semlink project (http://verbs.colorado.
edu/semlink/) aims to integrate Propbank,
FrameNet, WordNet and VerbNet.
Other relevant work includes Moldovan and
Rus (Moldovan and Rus, 2001; Rus, 2002) who
developed a technique for parsing into logical
forms and used this to transform WordNet concept
definitions into logical forms. The same method
(with additional manual correction) was used to
produce the test set for another SensEval-3 shared
task (Identification of Logic Forms in English).
4.1 CoNLL 2008 Shared Task Data
Perhaps the most immediately promising resource
is is the CoNLL shared task data from 2008 (Sur-
deanu et al, 2008) which has syntactic depen-
dency annotations, named-entity boundaries and
the semantic dependencies model roles of both
verbal and nominal predicates. The data consist
of excerpts from Penn Treebank-3, BBN Pronoun
Coreference and Entity Type Corpus, PropBank I
and NomBank 1.0. In CoNLL ?08, the data was
used to train and test systems for the task of pro-
ducing a joint semantic and syntactic dependency
analysis of English sentences (the 2009 CoNLL
Shared Task extended this to multi-lingual data).
It seems feasible that we could reuse the CoNLL
data for a prototype Surface Realisation task,
adapting it and inversing the direction of the task,
i.e. mapping from syntactic-semantic dependency
representations to word strings.
5 Developing the Task
The first step in developing a Surface Realisa-
tion task could be to get together a working
group of surface realisation researchers to develop
a common-ground input representation automati-
cally derivable from a set of existing resources.
As part of this task a prototype corpus exempli-
fying inputs/outputs and annotations could be de-
veloped. At the end of this stage it would be use-
ful to write a white paper and circulate it and the
prototype corpus among the NLG (and wider NLP)
community for feedback and input.
After a further stage of development, it may be
feasible to run a prototype surface realisation task
at Generation Challenges 2011, combined with a
session for discussion and roadmapping. Depend-
ing on the outcome of all of this, a full-blown task
might be feasible by 2012. Some of this work will
need funding to be feasible, and the authors of this
paper are in the process of applying for financial
support for these plans.
6 Concluding Remarks
In this paper we have provided an overview of ex-
isting resources that could potentially be used for
a surface realisation task, and have outlined ideas
for how such a task might work. The core idea
is to develop a common-ground input representa-
tion which participants map to the types of input
required by their system. These inputs are derived
from existing annotated corpora developed for lan-
guage analysis. Outputs (realisations) are evalu-
ated by automatic comparison against the human-
authored text in the corpora as well as by by hu-
man assessors. Evaluation methods are adapted to
take account of the one-to-many nature of the re-
alisation mapping.
The ideas outlined in this paper began as a pro-
longed email exchange, interspersed with discus-
sions at conferences, among the authors. This pa-
per summarises our ideas as they have evolved so
far, to enable feedback and input from other re-
searchers interested in this type of task.
References
Colin F. Baker and Hiroaki Sato. 2003. The framenet
data and software. In Proceedings of ACL?03.
A. Cahill and J. van Genabith. 2006. Robust PCFG-
based generation using automatically acquired LFG
approximations. In Proc. ACL?06, pages 1033?44.
Charles Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proceedings of the
18th International Joint Conference on Artificial In-
telligence (IJCAI 2003), pages 811?817.
J. Carroll and S. Oepen. 2005. High efficiency
realization for a wide-coverage unification gram-
mar. In Proceedings of the 2nd International Joint
Conference on Natural Language Processing (IJC-
NLP?05), volume 3651, pages 165?176. Springer
Lecture Notes in Artificial Intelligence.
M. Elhadad and J. Robin. 1996. An overview of
SURGE: A reusable comprehensive syntactic real-
ization component. Technical Report 96-03, Dept
of Mathematics and Computer Science, Ben Gurion
University, Beer Sheva, Israel.
Deborah Dahl et al 1994. Expanding the scope of the
ATIS task: the ATIS-3 corpus. In Proceedings of the
ARPA HLT Workshop.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
C. Johnson, C. Fillmore, M. Petruck, C. Baker,
M. Ellsworth, J. Ruppenhoper, and E.Wood. 2002.
Framenet theory and practice. Technical report.
K. Knight, I. Chander, M. Haines, V. Hatzivassiloglou,
E. Hovy, M. Iida, S. Luk, R. Whitney, and K. Ya-
mada. 1995. Filling knowledge gaps in a broad-
coverage MT system. In Proceedings of the Four-
teenth International Joint Conference on Artificial
Intelligence (IJCAI ?95), pages 1390?1397.
I. Langkilde and K. Knight. 1998a. Generation
that exploits corpus-based statistical knowledge. In
Proc. COLING-ACL. http://www.isi.edu/licensed-
sw/halogen/nitro98.ps.
I. Langkilde. 2002. An empirical verification of cover-
age and correctness for a general-purpose sentence
generator. In Proc. 2nd International Natural Lan-
guage Generation Conference (INLG ?02).
B. Lavoie and O. Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings
of the 5th Conference on Applied Natural Language
Processing (ANLP?97), pages 265?268.
W. Mann and C. Mathiesen. 1983. NIGEL: A sys-
temic grammar for text generation. Technical Re-
port ISI/RR-85-105, Information Sciences Institute.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3. Technical report, Linguistic Data Consortium,
Philadelphia.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2004. Np-external arguments a study of argument
sharing in english. In MWE ?04: Proceedings of
the Workshop on Multiword Expressions, pages 96?
103, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Dan I. Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL?01.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic models for disambiguation of an
hpsg-based chart generator. In Proceedings of the
9th International Workshop on Parsing Technology
(Parsing?05), pages 93?102. Association for Com-
putational Linguistics.
D. S. Paiva and R. Evans. 2005. Empirically-based
control of natural language generation. In Proceed-
ings ACL?05.
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Vasile Rus. 2002. Logic Form For WordNet Glosses
and Application to Question Answering. Ph.D. the-
sis.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining framenet, verbnet and wordnet
for robust semantic parsing. In Proceedings of CI-
CLing?05.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL ?08:
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 159?177.
S. Varges and C. Mellish. 2001. Instance-based natu-
ral language generation. In Proceedings of the 2nd
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL ?01),
pages 1?8.
E. Velldal, S. Oepen, and D. Flickinger. 2004. Para-
phrasing treebanks for stochastic realization rank-
ing. In Proceedings of the 3rd Workshop on Tree-
banks and Linguistic Theories (TLT ?04), Tuebin-
gen, Germany.
Ralph Weischedel and Ada Brunstein. 2005. Bbn pro-
noun coreference and entity type corpus. Technical
report, Linguistic Data Consortium.
Ralph Weischedel et al 2008. Ontonotes release 2.0.
Technical report, Linguistic Data Consortium.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for ccg realisation. In Pro-
ceedings of the 2009 Conference on Empririal Meth-
ods in Natural Language Processing (EMNLP?09),
pages 410?419.
M. White. 2004. Reining in CCG chart realization. In
A. Belz, R. Evans, and P. Piwek, editors, Proceed-
ings INLG?04, volume 3123 of LNAI, pages 182?
191. Springer.
Luke Zettlemoyer and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical forms. In Proceedings of ACL-IJCNLP?09.
H. Zhong and A. Stent. 2005. Building surface
realizers automatically from corpora. In A. Belz
and S. Varges, editors, Proceedings of UCNLG?05,
pages 49?54.
Proceedings of the 8th International Natural Language Generation Conference, pages 54?63,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
A Hybrid Approach to Multi-document Summarization of
Opinions in Reviews
Giuseppe Di Fabbrizio
Amazon.com?
Cambridge, MA - USA
pino@difabbrizio.com
Amanda J. Stent
Yahoo! Labs
New York, NY - USA
stent@labs.yahoo.com
Robert Gaizauskas
Department of Computer Science
University of Sheffield, Sheffield - UK
R.Gaizauskas@sheffield.ac.uk
Abstract
We present a hybrid method to gener-
ate summaries of product and services re-
views by combining natural language gen-
eration and salient sentence selection tech-
niques. Our system, STARLET-H, re-
ceives as input textual reviews with asso-
ciated rated topics, and produces as out-
put a natural language document summa-
rizing the opinions expressed in the re-
views. STARLET-H operates as a hybrid
abstractive/extractive summarizer: using
extractive summarization techniques, it se-
lects salient quotes from the input reviews
and embeds them into an automatically
generated abstractive summary to provide
evidence for, exemplify or justify posi-
tive or negative opinions. We demon-
strate that, compared to extractive meth-
ods, summaries generated with abstractive
and hybrid summarization approaches are
more readable and compact.
1 Introduction
Text summarization is a well-established area of
research. Many approaches are extractive, that
is, they select and stitch together pieces of text
from the input documents (Goldstein et al., 2000;
Radev et al., 2004). Other approaches are abstrac-
tive; they use natural language generation (NLG)
techniques to paraphrase and condense the con-
tent of the input documents (Radev and McKeown,
1998). Most summarization methods focus on dis-
tilling factual information by identifying the in-
put documents? main topics, removing redundan-
cies, and coherently ordering extracted phrases or
sentences. Summarization of sentiment-laden text
(e.g., product or service reviews) is substantially
different from the traditional text summarization
task: instead of presenting facts, the summarizer
must present the range of opinions and the con-
sensus opinion (if any), and instead of focusing
on one topic, the summarizer must present infor-
mation about multiple aspects of the target entity.
?This work was conducted when in AT&T Labs Research
In addition, traditional summarization techniques
discard redundancies, while for summarization of
sentiment-laden text, similar opinions mentioned
multiple times across documents are crucial indi-
cators of the overall strength of the sentiments ex-
pressed by the writers (Ku et al., 2006).
Extractive summaries are linguistically interest-
ing and can be both informative and concise. Ex-
tractive summarizers also require less engineer-
ing effort. On the other hand, abstractive sum-
maries tend to have better coverage for a particular
level of conciseness, and to be less redundant and
more coherent (Carenini et al., 2012). They also
can be constructed to target particular discourse
goals, such as summarization, comparison or rec-
ommendation. Although in theory, it is possible to
produce user-targeted extractive summaries, user-
specific review summarization has only been ex-
plored in the context of abstractive summarization
(Carenini et al., 2012).
Current systems for summarizing sentiment-
laden text use information about the attributes of
the target entity (or entities); the range, mean
and median of the ratings of each attribute; re-
lationships between the attributes; and links be-
tween ratings/attributes and text elements in the
input documents (Blair-Goldensohn et al., 2008).
However, there is other information that no sum-
marizer currently takes into account. This in-
cludes temporal features (in particular, depending
on how old the documents are, products and ser-
vices evaluated features may change over time)
and social features (in particular, social or demo-
graphic similarities or relationships between doc-
ument authors and the reader of the summary).
In addition, there is an essential contradiction at
the heart of current review summarization sys-
tems: the system is authoring the review, but the
opinions contained therein are really attributable
to one or more human authors, and those attribu-
tions are not retained in the review summary. For
example, consider the extractive summary gener-
ated with STARLET-E (Di Fabbrizio et al., 2013):
?Delicious. Can?t wait for my next trip to Buffalo.
GREAT WINGS. I have rearranged business trips
54
so that I could stop in and have a helping or two
of their wings?. We were seated promptly and the
staff was courteous.
The summary is generated by selecting sen-
tences from reviews to reflect topics and rating dis-
tributions contained in the input review set. Do the
two sentences about wings reflect one (repeated)
opinion from a single reviewer, or two opinions
from two separate reviewers? The ability to at-
tribute subjective statements to known sources can
make them more trustworthy; conversely, in the
absence of the ability to attribute, a reader may
become skeptical or confused about the content of
the review summary. We term this summarization
issue opinion holder attribution.
In this paper we present STARLET-H, a hybrid
review summarizer that combines the advantages
of the abstractive and extractive approaches to
summarization and implements a solution to the
opinion holder attribution problem. STARLET-H
takes as input a set of reviews, each review of
which is labeled with aspect ratings and author-
ship. It generates hybrid abstractive/extractive re-
views that: 1) are informative (achieve broad cov-
erage of the input opinions); 2) are concise and
avoid redundancy; 3) are readable and coherent (of
high linguistic quality); 4) can be targeted to the
reader; and 5) address the opinion holder attribu-
tion problem by directly referring to reviewers au-
thorship when embedding phrases from reviews.
We demonstrate through a comparative evalua-
tion of STARLET-H and other review summariz-
ers that hybrid review summarization is preferred
over extractive summarization for readability, cor-
rectness, completeness (achieving broad coverage
of the input opinions) and compactness.
2 Hybrid summarization
Most NLG research has converged around a ?con-
sensus architecture? (Reiter, 1994; Rambow and
Korelsky, 1992), a pipeline architecture including
the following modules: 1) text planning, which
determines how the presentation content is se-
lected, structured, and ordered; 2) sentence plan-
ning, which assigns content to sentences, inserts
discourse cues to communicate the structure of
the presentation, and performs sentence aggrega-
tion and optionally referring expression genera-
tion; and 3) surface realization, which performs
lexical selection, resolves syntactic issues such as
subject-verb and noun-determiner agreement, and
assigns morphological inflection to produce the fi-
nal grammatical sentence. An abstractive sum-
marizer requires the customization of these three
modules. Specifically, the text planner has to se-
lect and organize the information contained in the
input reviews to reflect the rating distributions over
the aspects discussed by the reviewers. The sen-
tence planner must perform aggregation in such a
way as to optimize summary length without con-
fusing the reader, and insert discourse cues that
reveal the discourse structure underlying the sum-
mary. And, finally, the surface realizer must select
the proper domain lexemes to express positive and
negative opinions.
Figure 1: STARLET-H hybrid review summarizer
architecture
Figure 1 shows the architecture we adopted for
our STARLET-H hybrid review summarizer. We
use a generate-and-select approach: the decisions
to be made at each stage of the NLG process just
outlined are complex, and because they are not
truly independent of each other, a generate-and-
rank approach may be best (allowing each com-
ponent to express alternative ?good? choices and
choosing the best combination of these choices
at the end). Our text planner is responsible for
analyzing the input text reviews, extracting per-
attribute rating distributions and other meta-data
from each review, and synthesizing this informa-
tion to produce one or more discourse plans. Our
sentence planner, JSPARKY ? a freely-available
toolkit (Stent and Molina, 2009) ? can produce
several candidate sentence plans and their corre-
sponding surface realizations through SimpleNLG
(Gatt and Reiter, 2009). The candidate summaries
are ranked by calculating their perplexity with a
language model trained over a large number of
sentences from additional restaurant reviews col-
lected over the Web.
2.1 Data
STARLET-H uses review data directly, as input
to summarization, and indirectly, as training data
for statistical models and for lexicons for various
stages of the summarization process.
For training data, we used two sets of la-
beled data: one for the restaurant domain and
the other for the hotel domain. Both corpora in-
clude manually created sentence-level annotations
55
that identify: 1) opinion targets ? phrases refer-
ring to domain-relevant aspects that are the tar-
gets of opinions expressed by the reviewer; 2)
opinion phrases ? phrases expressing an opinion
about an entity, and its polarity (positive or neg-
ative); and 3) opinion groups ? links between
opinion phrases and their opinion targets. Ad-
ditionally, sentences satisfying the properties of
quotable sentence mentioned in Section 3 were la-
beled as ?quotable?. Table 1 summarizes the over-
all statistics of the two corpora. The annotated cor-
pora included the following rated aspects: Atmo-
sphere, Food, Service, Value, and Overall for the
Restaurant domain, and Location, Rooms, Service,
Value, and Overall for the Hotel domain1.
Table 1: Quote-annotated dataset statistics
Dataset RQ4000 HQ4000
Domain Restaurant Hotel Total
Reviews 484 404 888
Sentences 4,007 4,013 8,020
Avg sentences / review 8.28 9.93 9.03
2.2 Text planning
Reviews present highly structured information:
each contains an (implicit or explicit) rating of one
or more aspects of a target entity, possibly with
justification or evidence in the form of examples.
The rich information represented in these ratings
? either directly expressed in reviews or extracted
by an automatic rating prediction model ? can be
exploited in several ways. Our text planner re-
ceives as input a set of text reviews with associated
per-aspect ratings, and for each review proceeds
through the following analysis steps:
Entity description Extracts basic information
to describe the reviewed entity, e.g., the name and
location of the business, number of total and recent
reviews, review dates and authors, etc.
Aspect distribution categorization Catego-
rizes the rating distribution for each aspect of the
reviewed entity as one of four types: 1) positive
? most of the ratings are positive; 2) negative ?
most of the ratings are negative; 3) bimodal ?
most of the ratings are equally distributed into
positive and negative values; 4) uniform ? ratings
are uniformly distributed across the rating scale.
1Some examples from the annotated corpus are avail-
able at the following address http://s286209735.
onlinehome.us/starlet/examples
Quote selection and attribution Classifies each
sentence from the reviews using a quote selec-
tion model (see Section 3), which assigns to
each sentence an aspect, a rating polarity (posi-
tive/negative) and a confidence score. The classi-
fied sentences are sorted by confidence score and
a candidate quote is selected for each aspect of the
target entity that is explicitly mentioned in the in-
put reviews. Each quote is stored with the name
of the reviewer for correct authorship attribution.
Note that when the quote selection module is ex-
cluded, the system is an abstractive summarizer,
which we call STARLET-A.
Lexical selection Selects a lexicon for each as-
pect based on its rating polarity and its assigned
rating distribution type. Lexicons are extracted
from the corpus of annotated opinion phrases de-
scribed in Di Fabbrizio et al. (2011).
Aspect ordering Assigns an order over aspects
using aspect ordering statistics from our training
data (see Section 2.4), and generates a discourse
plan, using a small set of rhetorical relations orga-
nized into summary templates (see below).
2.3 Sentence planning
The STARLET-H sentence planner relies on rhetor-
ical structure theory (RST) (Mann and Thomp-
son, 1989). RST is a linguistic framework that
describes the structure of natural language text
in terms of the rhetorical relationships organizing
textual units. Through a manual inspection of our
training data, we identified a subset of six RST re-
lations that are relevant to review summarization:
concession, contrast, example, justify, list, and
summary. We further identified four basic RST-
based summary templates, one for each per-aspect
rating distribution: mostly positive, mostly nega-
tive, uniform across all ratings, and bimodal (e.g.,
both positive and negative). These summary tem-
plates are composed by the text planner to build
summary discourse plans. The JSPARKY sen-
tence planner then converts input discourse plans
into sentence plans, performing sentence order-
ing, sentence aggregation, cross-sentence refer-
ence resolution, sentence tense and mode (passive
or active), discourse cue insertion, and the selec-
tion of some lexical forms from FrameNet (Baker
et al., 1998) relations.
Figure 2 illustrates a typical RST template rep-
resenting a positive review summary and corre-
sponding text output generated by JSPARKY. For
each aspect of the considered domain, the sentence
plan strategy covers a variety of opinion distribu-
56
Figure 2: Example of RST structure generated by the text planner for mostly positive restaurant reviews
tion conditions (e.g., positive, negative, bimodal,
and uniform), and provides alternative RST struc-
tures when the default relation is missing due to
lack of data (e.g., missing quotes for a specific as-
pect, missing information about review distribu-
tion over time, missing type of cuisine, and so on).
The sentence template can also manage lexical
variations by generating multiple options to qual-
ify a specific pair of aspect and opinion polarity.
For instance, in case of very positive reviews about
restaurant atmosphere, it can provide few alterna-
tive adjective phrases (e.g., great, wonderful, very
warm, terrific, etc.) that can be used to produce
more summary candidates (over-generate) during
the final surface realization stage.
2.4 Ordering aspects and polarities
The discourse structure of a typical review consists
of a summary opinion, followed by a sequence
of per-aspect ratings with supporting information
(e.g., evidence, justification, examples, and con-
cessions). The preferred sequence of aspects to
present in a summary depends on the specific re-
view domain, the overall polarity of the reviews,
and how opinion polarity is distributed across the
reviewed aspects. Looking at our training data, we
observed that when the review is overall positive,
positively-rated aspects are typically discussed at
the beginning, while negatively-rated aspects tend
to gather toward the end. The opposite order
seems predominant in the case of negative re-
views. When opinions are mixed, aspect ordering
strategies are unclear. To most accurately model
aspect ordering, we trained weighted finite state
transducers for the restaurant and hotel domains
using our training data. Weighted finite state
transducers (WFSTs) are an elegant approach to
search large feature spaces and find optimal paths
by using well-defined algebraic operations (Mohri
et al., 1996). To find the optimal ordering of rated
aspects in a domain, the text planner creates a
WFST with all the possible permutations of the
input sequence of aspects, and composes it with a
larger WFST trained from bigram sequences of as-
pects extracted from the relevant domain-specific
review corpus. The best path sequence is then de-
rived from the composed WFST by applying the
Viterbi decoding algorithm. For instance, the se-
quence of aspects and polarities represented by the
string: value-n service-p overall-n food-n
atmosphere-n2 is first permuted in all the dif-
ferent possible sequences and then converted into
a WFST. Then the permutation network is fully
composed with the larger, corpus-trained WFST.
The best path is extracted by dynamic program-
ming, producing the optimal sequence service-p
value-n overall-n atmosphere-n food-n.
2We postfix the aspect label with a ?-p? for positive and
with ?-n? for negative opinion
57
2.5 Lexical choice
It can be hard to choose the best opinion words,
especially when the summary must convey the
different nuances between ?good? and ?great? or
?bad? and ?terrible? for a particular aspect in a
particular domain. For our summarization task,
we adopted a simple approach. From our anno-
tated corpora, we mined both positive and negative
opinion phrases with their associated aspects and
rating polarities. We sorted the opinion phrases
by frequency and then manually selected from the
most likely phrases adjective phrases that may cor-
rectly express per-aspect polarities. We then split
positive and negative phrases into two levels of
polarity (i.e., strongly positive, weakly positive,
weakly negative, strongly negative) and use the
number of star ratings to select the right polarity
during content planning. For bimodal and uniform
polarity distributions, we manually defined a cus-
tomized set of terms. Sample lexical terms are re-
ported in Table 2.
3 Quote selection modeling
There are several techniques to extract salient
phrases from text, often related to summariza-
tion problems, but there is a relatively little work
on extracting quotable sentences from text (Sar-
mento and Nunes, 2009; De La Clergerie et al.,
2009) and none, to our knowledge, on extract-
ing quotes from sentiment-latent text. So, what
does make a phrase quotable? What is a proper
quote definition that applies to review summa-
rization? We define a sentiment-laden quotable
phrase as a text fragment with the following char-
acteristics: attributable ? clearly ascribable to the
author; compact and simple ? it is typically a
relatively short phrase (between two and twenty
words) which contains a statement with a simple
syntactic structure and independent clauses; self-
contained its meaning is clear and self-contained,
e.g., it does not include pronominal references to
entities outside its scope; on-topic ? it refers to
opinion targets (i.e., aspects) in a specific domain;
sentiment-laden ? it has one or two opinion tar-
gets and an unambiguous overall polarity. Exam-
ple quotable phrases are presented in Table 3.
To automatically detect quotes from reviews,
we adopted a supervised machine learning ap-
proach based on manually labeled data. The clas-
sification task consists of classifying both aspects
and polarity for the most frequent aspects defined
for each domain. Quotes for the aspect food, for
instance, are split into positive and negative classi-
Table 3: Example of quotes from restaurant and
hotel domains
?Everyone goes out of their way to make sure you
are happy with their service and food.?
?The stuffed mushrooms are the best I?ve ever had
as was the lasagna.?
?Service is friendly and attentive even during
the morning rush.?
?I?ve never slept so well away from home loved
the comfortable beds.?
?The price is high for substandard mattresses
when I pay this much for a room.?
fication labels: food-p and food-n, respectively.
We identify quotable phrases and associate them
with aspects and rating polarities all in one step,
but multi-step approaches could also be used (e.g.,
a configuration with binary classification to detect
quotable sentences followed by another classifica-
tion model for aspect and polarity detection).
3.1 Training quote selection models
We used the following features for automatic
quote selection: ngrams ? unigrams, bigrams, and
trigrams from the input phrases with frequency
higher than three; binned number of words ?
we assumed a maximum length of twenty words
per sentence and created six bins, five of them
uniformly distributed from one to twenty, and the
sixth including all the sentences of length greater
than twenty words; POS ? unigrams, bigrams, and
trigrams for part of speech tags; chunks ? uni-
grams, bigrams, and trigrams for shallow parsed
syntactic chunks; opinion phrases ? a binary fea-
ture to keep track of the presence of positive and
negative opinion phrases as defined in our anno-
tated review corpora. In our annotated data only
the most popular aspects are well represented.
For instance, food-p and overall-p are the most
popular positive aspects among the quotable sen-
tences for the restaurant domain, while quotes on
atmosphere-n and value-n are scarce. The dis-
tribution is even further skewed for the hotel do-
main; there are plenty of quotes for overall-p
and service-p and only 13 samples (0.43%) for
location-n. To compensate for the broad vari-
ation in the sample population, we used stratified
sampling methods to divide the data into more bal-
anced testing and training data We generated 10-
fold stratified training/test sets. We experimented
with three machine learning algorithms: MaxEnt,
SVMs with linear kernels, and SVMs with poly-
nomial kernels. The MaxEnt learning algorithm
produced statistically better classification results
than the other algorithms when used with uni-
58
Table 2: Summarizer lexicon for most frequent adjective phrases by aspect and polarity
Domain Restaurant Hotel
Aspect positive very positive negative very negative Aspect positive very positive negative very negative
atmosphere nice, good,
friendly, com-
fortable
great, wonder-
ful, very warm,
terrific
ordinary,
depressing
really bad location good, nice,
pleasant
amazing,
awesome,
excellent, great
bad, noisy,
gloomy
very bad, very
bleak, very
gloomy
food good, deli-
cious, pleasant,
nice, hearty,
enjoyable
great, ex-
cellent, very
good, to die
for, incredible
very basic, un-
original, unin-
teresting, unac-
ceptable, sub-
standard, poor
mediocre, ter-
rible, horrible,
absolutely hor-
rible
rooms comfortable,
decent, clean,
good
amazing,
awesome,
gorgeous
average, basic,
subpar
terrible, very
limited, very
average
overall good, quite en-
joyable, lovely
wonderful, ter-
rific, very nice
bad, unremark-
able, not so
good
absolutely ter-
rible, horrible,
pretty bad
overall great, nice,
welcoming
excellent,
superb, perfect
average, noth-
ing great, noisy
quite bad, aw-
ful, horrible
service attentive,
friendly, pleas-
ant, courteous
very atten-
tive, great,
excellent, very
friendly
inattentive,
poor, not
friendly, bad
extremely
poor, horrible,
so lousy, awful
service friendly, great,
nice, helpful,
good
very friendly,
great, ex-
cellent, very
nice
average, basic,
not that great
very bad,
dreadful
value reasonable,
fair, good
value
very reason-
able, great
not that good,
not worthy
terrible, outra-
geous
value great, nice,
good, decent
very good,
wonderful,
perfectly good
not good not very good
gram features. This confirmed a general trend we
have previously observed in other text classifica-
tion experiments: with relatively small and noisy
datasets, unigram features provide better discrimi-
native power than sparse bigrams or trigrams, and
MaxEnt methods are more robust when dealing
with noisy data.
3.2 Quote selection results
Table 4 reports precision, recall and F-measures
averaged across 10-fold cross-validated test sets
with relative standard deviation. The label nq
identifies non-quotable sentences, while the other
labels refer to the domain-specific aspects and
their polarities. For the quote selection task, pre-
cision is the most important metric: missing some
potential candidates is less important than incor-
rectly identifying the polarity of a quote or sub-
stituting one aspect with another. The text planner
in STARLET-H further prunes the quotable phrases
by considering only the quote candidates with the
highest scores.
4 Evaluation
Evaluating an abstractive review summarizer in-
volves measuring how accurately the opinion con-
tent present in the reviews is reflected in the sum-
mary and how understandable the generated con-
tent is to the reader. Traditional multi-document
summarization evaluation techniques utilize both
qualitative and quantitative metrics. The former
require human subjects to rate different evaluative
characteristics on a Likert-like scale, while the lat-
ter relies on automatic metrics such as ROUGE
(Lin, 2004), which is based on the common num-
ber of n-grams between a peer, and one or several
gold-standard reference summaries.
Table 4: Quote, aspect, and polarity classification
performances for the restaurant domain
Precision Recall F-measure
atmosphere-n 0.233 0.080 0.115
atmosphere-p 0.589 0.409 0.475
food-n 0.634 0.409 0.491
food-p 0.592 0.634 0.612
nq 0.672 0.822 0.740
overall-n 0.545 0.275 0.343
overall-p 0.555 0.491 0.518
service-n 0.699 0.393 0.498
service-p 0.716 0.563 0.626
value-n 0.100 0.033 0.050
value-p 0.437 0.225 0.286
Hotel Precision Recall F-measure
location-n - - -
location-p 0.572 0.410 0.465
nq 0.678 0.836 0.748
overall-n 0.517 0.233 0.305
overall-p 0.590 0.492 0.536
rooms-n 0.628 0.330 0.403
rooms-p 0.667 0.573 0.612
service-n 0.517 0.163 0.240
service-p 0.605 0.500 0.543
value-n - - -
value-p 0.743 0.300 0.401
4.1 Evaluation materials
To evaluate our abstractive summarizer, we used
a qualitative metric approach and compared four
review summarizers: 1) the open source MEAD
system, designed for extractive summarization of
general text (Radev et al., 2004); 2) STARLET-E,
an extractive summarizer based on KL-divergence
and language modeling features that is described
in Di Fabbrizio et al. (2011); 3) STARLET-A, the
abstractive summarizer presented in this paper,
without the quote selection module; and 4) the hy-
brid summarizer STARLET-H.
We used the Amazon Mechanical Turk3 crowd-
3http://www.mturk.com
59
sourcing system to post subjective evaluation
tasks, or HITs, for 20 restaurant summaries. Each
HIT consists of a set of ten randomly ordered re-
views for one restaurant, and four randomly or-
dered summaries of reviews for that restaurant,
each one accompanied by a set of evaluation wid-
gets for the different evaluation metrics described
below. To minimize reading order bias, both re-
views and summaries were shuffled each time a
task was presented.
4.2 Evaluation metrics
We chose to carry out a qualitative evaluation
in the first instance as n-gram metrics, such as
ROUGE, are not necessarily appropriate for as-
sessing abstractive summaries. We asked each par-
ticipant to evaluate each summary by rating (using
a Likert scale with the following rating values: 1)
Not at all; 2) Not very; 3) Somewhat; 4) Very; 5)
Absolutely) the following four summary criteria:
readability ? a summary is readable if it is easy to
read and understand; correctness ? a summary is
correct if it expresses the opinions in the reviews;
completeness ? a summary is complete if it cap-
tures the whole range of opinions in the reviews;
compactness ? a summary is compact if it does
not repeat information.
4.3 Evaluation procedure
We requested five evaluators for each HIT. To in-
crease the chances of getting accurate evaluations,
we required evaluators to be located in the USA
and have an approval rate of 90% or higher (i.e.,
have a history of 90% or more approved HITs).
Manual examinations of the evaluation responses
did not show evidence of tampered data, but statis-
tical analysis showed unusually widely spread rat-
ing ranges. We noticed that most evaluators only
evaluated one or two HITs; this may imply that
they tried a few HITs and then decided not to con-
tinue because they found the task too long or the
instructions unclear. We then re-opened the evalu-
ation and directly contacted three additional eval-
uators, explaining in detail the instructions and the
evaluation scales. For consistency, we asked these
evaluators to complete the evaluation for all HITs.
In our analysis, we only included the five evalu-
ators (two from the first round of evaluation, and
three from the second) who completed all HITs.
For each evaluation metric, the five workers eval-
uated each of the 20 summaries, for a total of 100
ratings. Table 5 shows an example output of the
four summarization methods for a single set of
restaurant review documents.
Table 5: Example of MEAD-based, extractive, ab-
stractive and hybrid summaries from the restaurant
domain
MEAD Summary
a truly fun resturant everyone who like spicy
food should try the rattoes and for a mixed drink
the worm burner really good food and a fun place
to meet your friends. We were attracted by the
great big frog on the exterior of the building
and the fun RAZZOO S logo during a trip to the
mall. it was great the waitress was excellent
very prompt and courteous and friendly to all a
real complement to razzoo ?s way of service her
name was Tabitha. The best spicy food restaurant
with great server and fast service.
Extractive summary
Eat there every chance i get. We ve been going
here for years. Their crawfish etoufee is the
BEST. And such an awesome value for under 10.
Excellent as always. Some of the best food in
the area. I use to work at Razzoo s. It was
hard to leave. The people are great and so is
the food. I still go in there and miss it more
everytime. I Love Loney. It was great. Our
server was great and very observant. Try the
Chicken Tchoupitoulas.
Abstractive summary
Razzoo?s Cajun Cafe in Concord, NC is an American
restaurant. It has nine reviews. It had three
very recent reviews. It is an awesome, American
restaurant. It has many very positive reviews.
It has an excellent atmosphere and and has always
exceptional service.
Hybrid summary
Razzoo?s Cajun Cafe in Concord, NC is an American
restaurant. It has nine reviews. It had three
very recent reviews. It is an awesome, American
restaurant. It has many very positive reviews.
First it has a great price. Angela Haithcock
says ??And such an awesome value for under 10??.
Second it has always exceptional service and for
instance Danny Benson says ??it was great the
waitress was excellent very prompt and courteous
and friendly to all a real complement to razzoo?s
way of service her name was Tabitha??. Third it
has an excellent atmosphere. Last it has amazing
food. Scott Kern says ??Some of the best food in
the area??.
4.4 Evaluation results and discussion
The evaluation results are presented in Table 6.
Each evaluation metric is considered separately.
Average values for STARLET-E, STARLET-A and
STARLET-H are better than for MEAD across the
board, suggesting a preference for summaries of
sentiment-laden text that take opinion into ac-
count. To validate this hypothesis, we first com-
puted the non-parametric Kruskal-Wallis statistic
for each evaluation metric, using a chi-square test
to establish significance. The results were not sig-
nificant for any of the metrics.
However, when we conducted pairwise
Wilcoxon signed-rank tests considering two
summarization methods at a time, we found some
significant differences (p < 0.05). As predicted,
60
Table 6: Qualitative evaluation results
MEAD Starlet-E Starlet-A Starlet-H
Readability 2.95 3.17 3.64 3.74
Completeness 2.88 3.29 3.290 3.58
Compactness 3.07 3.35 3.80 3.58
Correctness 3.26 3.48 3.59 3.72
MEAD perform substantially worse than both
STARLET-A and STARLET-H on readability,
correctness, completeness, and compactness.
STARLET-A and STARLET-H are also preferred
over STARLET-E for readability. While STARLET-
A is preferred over STARLET-E for compactness
(the average length of the abstractive reviews
was 45.05 words, and of the extractive,102.30),
STARLET-H is preferred over STARLET-E for
correctness, since the former better captures the
reviewers opinions by quoting them in the ap-
propriate context. STARLET-A and STARLET-H
achieve virtually indistinguishable performance
on all evaluation metrics. Our evaluation results
accord with those of Carenini et al. (2012); their
abstractive summarizer had superior performance
in terms of content precision and accuracy when
compared to summaries generated by an extractive
summarizer. Carenini et al. (2012) also found that
the differences between extractive and abstractive
approaches are even more significant in the case
of controversial content, where the abstractive
system is able to more effectively convey the full
range of opinions.
5 Related work
Ganesan et al. (2010) propose a method to extract
salient sentence fragments that are both highly fre-
quent and syntactically well-formed by using a
graph-based data structure to eliminate redundan-
cies. However, this approach assumes that the in-
put sentences are already selected in terms of as-
pect and with highly redundant opinion content.
Also, the generated summaries are very short and
cannot be compared to a full-length output of a
typical multi-document summarizer (e.g., 100-200
words). A similar approach is described in Gane-
san et al. (2012), where very short phrases (from
two to five words) are collated together to generate
what the authors call ultra-concise summaries.
The most complete contribution to evaluative
text summarization is described in Carenini et al.
(2012) and it closely relates to this work. Carenini
et al. (2012) compare an extractive summariza-
tion system, MEAD* ? a modified version of
the open source summarization system MEAD
(Radev et al., 2004) ? with SEA, an abstractive
summarization system, demonstrating that both
systems perform equally well. The SEA approach,
although better than traditional MEAD, has a few
drawbacks. Firstly, the sentence selection mecha-
nism only considers the most frequently discussed
aspects, leaving the decision about where to stop
the selection process to the maximum summary
length parameter. This could leave out interest-
ing opinions that do not appear with sufficient fre-
quency in the source documents. Ideally, all opin-
ions should be represented in the summary accord-
ing to the overall distribution of the input reviews.
Secondly, Carenini et al. (2012) use the absolute
value of the sum of positive and negative contri-
butions to determine the relevance of a sentence in
terms of opinion content. This flattens the aspect
distributions since sentences with very negative or
very positive polarity or with numerous opinions,
but with moderate polarity strengths, will get the
same score, regardless. Finally, it does not ad-
dress the opinion holder attribution problem leav-
ing the source of opinion undefined. In contrast,
STARLET-H follows reviews aspect rating distri-
butions both to select quotable sentences and to
summarize relevant aspects. Moreover, it explic-
itly mentions the opinion source in the embedded
quoted sentences.
6 Conclusions
In this paper, we present a hybrid summarizer for
sentiment-laden text that combines an overall ab-
stractive summarization method with an extrac-
tive summarization-based quote selection method.
This summarizer can provide the readability and
correctness of abstractive summarization, while
addressing the opinion holder attribution problem
that can lead readers to become confused or mis-
led about who is making claims that they read in
review summaries. We plan a more extensive eval-
uation of STARLET-H. Another potential area of
future research concerns the ability to personal-
ize summaries to the user?s needs. For instance,
the text planner can adapt its communicative goals
based on polarity orientation ? a user can be more
interested in exploring in detail negative reviews
? or it can focus more on specific (user-tailored)
aspects and change the order of the presentation
accordingly. Finally, it could be interesting to cus-
tomize the summarizer to provide an overview of
what is available in a specific geographic neigh-
borhood and compare and contrast the options.
61
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. The Berkeley FrameNet Project. In
Proceedings of the 17th International Con-
ference on Computational Linguistics - Vol-
ume 1, COLING ?98, pages 86?90, Strouds-
burg, PA, USA, 1998. Association for Com-
putational Linguistics. doi: 10.3115/980451.
980860. URL http://dx.doi.org/10.
3115/980451.980860.
Sasha Blair-Goldensohn, Kerry Hannan, Ryan
McDonald, Tyler Neylon, George Reis, and Jeff
Reynar. Building a Sentiment Summarizer for
Local Service Reviews. In NLP in the Informa-
tion Explosion Era, 2008.
Giuseppe Carenini, Jackie Chi Kit Cheung, and
Adam Pauls. Multi-Document Summarization
of Evaluative Text. Computational Intelligence,
2012.
E?ric De La Clergerie, Beno??t Sagot, Rosa Stern,
Pascal Denis, Gae?lle Recource?, and Victor
Mignot. Extracting and Visualizing Quotations
from News Wires. In Language and Technol-
ogy Conference, Poznan, Pologne, 2009. Projet
Scribo (po?le de compe?titivite? System@tic).
Giuseppe Di Fabbrizio, Ahmet Aker, and Robert
Gaizauskas. STARLET: Multi-document Sum-
marization of Service and Product Reviews with
Balanced Rating Distributions. In Proceedings
of the 2011 IEEE International Conference on
Data Mining (ICDM) Workshop on Sentiment
Elicitation from Natural Text for Information
Retrieval and Extraction (SENTIRE), Vancou-
ver, Canada, december 2011.
Giuseppe Di Fabbrizio, Ahmet Aker, and Robert
Gaizauskas. Summarizing On-line Product and
Service Reviews Using Aspect RatingDistribu-
tions and Language Modeling. Intelligent Sys-
tems, IEEE, 28(3):28?37, May 2013. ISSN
1541-1672. doi: 10.1109/MIS.2013.36.
Kavita Ganesan, ChengXiang Zhai, and Jiawei
Han. Opinosis: A Graph-Based Approach to
Abstractive Summarization of Highly Redun-
dant Opinions. In Proceedings of the 23rd Inter-
national Conference on Computational Linguis-
tics, COLING ?10, pages 340?348, Strouds-
burg, PA, USA, 2010. Association for Compu-
tational Linguistics.
Kavita Ganesan, ChengXiang Zhai, and Evelyne
Viegas. Micropinion Generation: An Unsu-
pervised Approach to Generating Ultra-concise
Summaries of Opinions. In Proceedings of the
21st international conference on World Wide
Web, WWW ?12, pages 869?878, New York,
NY, USA, 2012. ACM.
Albert Gatt and Ehud Reiter. SimpleNLG: A Re-
alisation Engine for Practical Applications. In
Proceedings of the 12th European Workshop
on Natural Language Generation, ENLG ?09,
pages 90?93, Stroudsburg, PA, USA, 2009. As-
sociation for Computational Linguistics.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell,
and Mark Kantrowitz. Multi-document Sum-
marization by Sentence Extraction. In Proceed-
ings of the 2000 NAACL-ANLP Workshop on
Automatic summarization - Volume 4, pages 40?
48, Stroudsburg, PA, USA, 2000. Association
for Computational Linguistics.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
Opinion Extraction, Summarization and Track-
ing in News and BlogCorpora. In Proceedings
of AAAI-2006 Spring Symposium on Computa-
tional Approaches to Analyzing Weblogs, 2006.
Chin-Yew Lin. ROUGE: A Package for Auto-
matic Evaluation of summaries. In Proc. ACL
workshop on Text Summarization Branches Out,
page 10, 2004.
William C. Mann and Sandra A. Thompson.
Rhetorical Structure Theory: A Theory of Text
Organization. In Livia Polanyi, editor, The
Structure of Discourse. Ablex, Norwood, NJ,
1989.
Mehryar Mohri, Fernando Pereira, and Michael
Riley. Weighted Automata in Text and Speech
Processing. In ECAI-96 Workshop, pages 46?
50. John Wiley and Sons, 1996.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Ho-
racio Saggion, Simone Teufel, Michael Top-
per, Adam Winkel, and Zhu Zhang. MEAD
? A Platform for Multidocument Multilingual
Text Summarization. In Conference on Lan-
guage Resources and Evaluation (LREC), Lis-
bon, Portugal, May 2004.
Dragomir R. Radev and Kathleen R. McKe-
own. Generating natural language summaries
from multiple on-line sources. Computational
Linguistiscs, 24(3):470?500, September 1998.
ISSN 0891-2017.
62
Owen Rambow and Tanya Korelsky. Applied Text
Generation. In Proceedings of the Third Confer-
ence on Applied Natural Language Processing,
pages 40?47, Trento, Italy, 1992. Association
for Computational Linguistics. 31 March - 3
April.
Ehud Reiter. Has a Consensus NL Generation Ar-
chitecture Appeared, and is it Psychologically
Plausible? In David McDonald and Marie
Meteer, editors, Proceedings of the 7th. Inter-
national Workshop on Natural Language gen-
eration (INLGW ?94), pages 163?170, Kenneb-
unkport, Maine, 1994.
Luis Sarmento and Se?rgio Nunes. Automatic Ex-
traction of Quotes and Topics from News Feeds.
In 4th Doctoral Symposium on Informatics En-
gineering (DSIE09), 2009.
Amanda Stent and Martin Molina. Evaluating Au-
tomatic Extraction of Rules for Sentence Plan
Construction. In Proceedings of the SIGDIAL
2009 Conference: The 10th Annual Meeting of
the Special Interest Group on Discourse and Di-
alogue, SIGDIAL ?09, pages 290?297, Strouds-
burg, PA, USA, 2009. Association for Compu-
tational Linguistics.
63
