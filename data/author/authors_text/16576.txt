Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 65?72,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
A Hybrid Stepwise Approach for De-identifying Person Names  in Clinical Documents   Oscar Ferr?ndez1,2, Brett R. South1,2, Shuying Shen1,2, St?phane M. Meystre1,2 1 Department of Biomedical Informatics, University of Utah, Salt Lake City, Utah, USA 2 IDEAS Center SLCVA Healthcare System, Salt Lake City, Utah, USA oscar.ferrandez@utah.edu, {brett.south,shuying.shen,stephane.meystre}@hsc.utah.edu   Abstract 
As Electronic Health Records are growing ex-ponentially along with large quantities of un-structured clinical information that could be used for research purposes, protecting patient privacy becomes a challenge that needs to be met. In this paper, we present a novel hybrid system designed to improve the current strate-gies used for person names de-identification. To overcome this task, our system comprises several components designed to accomplish two separate goals: 1) achieve the highest re-call (no patient data can be exposed); and 2) create methods to filter out false positives. As a result, our system reached 92.6% F2-measure when de-identifying person names in Veteran?s Health Administration clinical notes, and considerably outperformed other existing ?out-of-the-box? de-identification or named entity recognition systems.  
 1 Introduction Electronic Healthcare Records are invaluable re-sources for clinical research, however they contain highly sensitive Protected Health Information (PHI) that must remain confidential. In the United States, patient confidentiality is regulated by the Health Insurance Portability and Accountability Act (HIPAA). To share and use clinical documents for research purposes without patient consent, HIPAA requires prior removal of PHI. More spe-cifically, the HIPAA ?Safe Harbor?1 determines 18                                                 1 GPO US: 45 C.F.R. ? 164 Security and Privacy. http://www.access.gpo.gov/nara/cfr/waisidx_08/45cfr164_08.html Further details about the 18 HIPAA Safe Harbor PHI identifi-ers can be also found in (Meystre et al, 2010). 
PHI categories that have to be obscured in order to consider clinical data de-identified. An ideal de-identification system should recog-nize PHI accurately, but also preserve relevant non-PHI clinical data, so that clinical records can later be used for various clinical research tasks. Of the 18 categories of PHI listed by HIPAA, one of the most sensitive is patient names, and all person names in general. Failure to de-identify such PHI involves a high risk of re-identification, and jeopardizes patient privacy. In this paper, we describe our effort to satisfac-torily de-identify person names in Veteran?s Health Administration (VHA) clinical documents. We propose improvements in person names de-identification with a pipeline of processes tailored to the idiosyncrasies of clinical documents. This effort was realized in the context of the develop-ment of a best-of-breed clinical text de-identification system (nicknamed ?BoB?), which will be released as an open source software pack-age, and it started with the implementation and evaluation of several existing de-identification and Named Entity Recognition (NER) systems recog-nizing person names. We then devised a novel methodology to better tackle this task and improve performance.  2 Background and related work  In many aspects de-identification resembles tradi-tional NER tasks (Grishman and Sundheim, 1996). NER involves detecting entities such as person names, locations, and organizations. Consequently, given the similar entities targeted by both tasks, NER systems can be relevant to de-identify docu-ments. However, most named entity recognizers were developed for newswire articles, and not for clinical narratives. Clinical records are character-
65
ized by fragmented and incomplete utterances, lack of punctuation marks and formatting, as well as domain specific language. These complications, in addition to the fact that some entities can appear both as PHI and non-PHI in the same document (e.g., ?Mr. Epley? vs. ?the Epley maneuver?), make clinical text de-identification a challenging task. Therefore, although person names de-identification is essentially NER, the unique char-acteristics of clinical texts make it more interesting and challenging than recognizing names in news articles, which also enhance the motivation for this study. Several different approaches were proposed to deal with de-identification of clinical documents, and for named entity recognition of person names. These approaches are mainly focused on either pattern matching techniques, or statistical methods (Meystre et al, 2010), as exemplified below. Beckwith et al (2006) developed a de-identification system for pathology reports. This system implemented some patterns to detect dates, locations, and ID numbers, as well as a database of proper names and well-known markers such as ?Mr.? and ?PhD? to find person names. Friedlin and McDonald (2008) described the Medical De-identification System (MeDS). It used a combination of methods including heuristics, pattern matching, and dictionary lookups to identi-fy PHI. Pattern matching through regular expres-sions was used to detect numerical identifiers, dates, addresses, ages, etc.; while for names, MeDS used lists of proper names, common usage words and predictive markers, as well as a text string nearness algorithm to deal with typograph-ical errors.  Neamatullah et al (2008) proposed another rule-based de-identification approach focused on pat-tern matching via dictionary lookups, regular ex-pressions and context checks heuristics denoting PHI. Dictionaries made up of ambiguous names and locations that could also be non-PHI, as well as dictionaries of common words were used by this system to disambiguate PHI terms. Other de-identification systems such as (Aberdeen et al, 2010; Gardner and Xiong, 2009) use machine learning algorithms to train models and predict new annotations. The key aspect of these systems is the selection of the learning algo-rithm and features. Both (Aberdeen et al, 2010) and (Gardner and Xiong, 2009) use an implemen-
tation of Conditional Random Fields (CRF) and a set of learning features based on the morphology of the terms and their context. One disadvantage of these systems is the need for large amounts of an-notated training examples. As mentioned previously, for detecting person names, we could also use traditional newswire-trained NER systems. NER has long been studied by the research community and many different ap-proaches have been developed (Tjong Kim Sang and De Meulder, 2003; Doddington et al, 2004). One successful and freely available named entity recognizer is the Stanford NER system (Finkel et al, 2005), which provides an implementation of linear chain CRF sequence models, coupled with well-engineered feature extractors for NER, and trained with newswire documents.  3 Methods As already mentioned, we first selected and ran several existing de-identification and NER systems detecting person names in our clinical documents. Afterwards, we devised and present here a novel pipeline of processes designed to improve the PHI recognition task. 3.1 Existing de-identification and NER sys-tems Five available de-identification systems, as well as one newswire-trained named entity recognizer, were selected for an ?out-of-the-box? evaluation. The aim of this evaluation was to compare the per-formance of the various methods and resources when de-identifying person names in our clinical documents. We included three rule-based de-identification approaches:  ? HMS Scrubber (Beckwith et al, 2006); ? MeDS (Friedlin and McDonald, 2008); and ? MIT deid system (Neamatullah et al, 2008).  And two systems based on machine learning classifiers: ? The MITRE Identification Scrubber Toolkit (MIST) (Aberdeen et al, 2010); and  ? The Health Information DE-identification (HIDE) system (Gardner and Xiong, 2009).  
66
Regarding NER systems, we chose the Stanford NER system (Finkel et al, 2005), which has re-ported successful results when detecting person names. These systems were described in Section 2, when we presented related work. 3.2 Our best-of-breed approach Our names de-identification approach consists of a novel pipeline of processes designed to improve the current strategies for person names de-identification. This system is being developed as an Apache UIMA2 pipeline, with two main goals:   1) Obtain the highest recall (i.e., sensitivity), re-gardless of the impact on precision; and  2) Improve overall precision by filtering out the false positives produced previously.   These goals correspond to the implementation of the main components of our system. When we tested existing systems (we will present results for these systems in Table 1), we observed that recall was better addressed by rule-based approaches, while precision was higher applying machine learning-based algorithms. We therefore used this knowledge for the design of our system: goal#1 is then accomplished mainly using rule-based tech-niques, and goal#2 implementing machine learn-ing-based approaches. Moreover, recall is of paramount importance in de-identification (patient PHI cannot be disclosed). And this was also a reason that motivated us to first focus on achieving high recall, and filtering out false positives afterwards as a separate proce-dure.  Unlike other de-identification and NER systems that tackle the classification problem from one per-spective (i.e., rule-based or machine learning-based) or from a limited combined approach (e.g., learning features extracted using regular expres-sions), the design of our system allows us to take advantage of the strong points of both techniques separately. And more importantly, our classifiers for filtering out false-positives (goal#2) are trained using correct and incorrect annotations derived from previous modules implemented in goal#1. Thus, they do not predict if every token in the doc-ument is or belongs to a PHI identifier, they in-stead decide if an actual annotation is a false or                                                 2 http://uima.apache.org/ 
true positive. This design makes our classifiers better with less learning examples, which is a re-striction we have to deal with, and it also allows us to create methods that can be only focused on max-imizing recall regardless of the amount of false-positives introduced (goal#1). To the best of our knowledge, this perspective has not been exploited before, and as we will show in the evaluation sec-tion, it empirically demonstrates more robustness than previous approaches. 
The design of our system integrates different components described below. Figure 1 depicts an overview of our system?s architecture and work-flow.  
  Figure 1. System?s architecture.  3.2.1 NLP preprocessing steps This NLP preprocessing prepares the input for the main components of our system. It includes sen-tence segmentation, tokenization, part-of-speech tagging, chunking, and word normalization based 
67
on Lexical Variant Generation (LVG)3. The output of this preprocessing will be used by subsequent pattern matching techniques and features for ma-chine learning algorithms. For these processes, we adapted several cTAKES (Savova et al, 2010) components. 3.2.2 Rules and dictionary lookups We created a pattern matching component support-ed by contextual keyword searches (e.g., ?Dr.?, ?Mr.?, ?M.D.?, ?R.N.?, ?L.C.S.W.?), dictionaries of person names4, and a simple disambiguation procedure based on a list of common words and the capitalization of the entity. We adapted some of the techniques implemented in (Beckwith et al, 2006; Friedlin and McDonald, 2008; Neamatullah et al, 2008) to our documents, and developed new patterns. For dictionary lookups, we used Lucene5 indexing, experimenting with keyword and fuzzy dictionary searches. Each word token is compared with our indexed dictionary of names (last and first names from the 1990 US Census4), considering all matches as candidate name annotations. However, candidates that also match with an entry in our dic-tionary of common words6 and do not contain an initial capital letter are discarded from this set of candidate name annotations. With this component, we attempt to maximize recall, even if precision is altered. 3.2.3 CRF-based predictions To further enhance recall, we created another com-ponent based on CRF models. We incorporated this component in our system considering that ma-chine learning classifiers are more generalizable and can detect instances of names that are not sup-ported by our rules or dictionaries. Therefore, alt-hough we knew the individual results of a CRF classifier at this level were not enough for de-identification, at this point our main concern is to obtain the highest recall. Thus, adding a machine learning classifier into this level we could help the system predicting the PHI formats and instances                                                 3 http://lexsrv2.nlm.nih.gov/LexSysGroup/Projects/lvg/      current/web/index.html 4 Frequently Occurring Names from the 1990 Census. http://www.census.gov/genealogy/names. 5 http://lucene.apache.org/java/docs/index.html 6 We used the dictionary of common words from Neamatullah et al (2008). 
that could not be covered by our patterns and dic-tionaries. To develop this component, we used the CRF classifier implementation provided by the Stanford NLP group7. We carried out a feature selection procedure using greedy forward selection. It pro-vided us with the best learning feature set, which consisted of: the target word, 2-grams of letters, position in the document, part-of-speech tag, lem-ma, widely-used word-shape features (e.g., initial capitals, all capitals, digits inside, etc.), features from dictionaries of names and common words, a 2-word context window, and combinations of words, word-shapes and part-of-speech tags of the word and its local context. The learning features considered before and af-ter the selection procedure are shown in Table 1. 3.2.4 False-positive filtering The two previous components? objective is maxi-mal recall, producing numerous false positives. The last component of our pipeline was therefore designed to filter out these false positives and con-sequently increase overall precision. We built a machine learning classifier for this task, based on LIBSVM (Chang and Lin, 2001), a library for Support Vector Machines (SVM), with the RBF (Radial Basis Function) kernel. We then trained this classifier with reference standard text annota-tions, as well as the correct and incorrect annota-tions made by the previous components. We used our training document set (section 4.1) for this purpose. Features for the LIBSVM machine learning model were: the LVG normalized form of the target anno-tation, three words before and after, part-of-speech tags of the words within the annotation and the local context, number of tokens within the annota-tion, position in the document, 40 orthographic features (denoting capitals, digits, special charac-ters, etc.), features from dictionaries of names and common words, and the previous strategy used to make the annotation (i.e., rules, dictionary lookups or CRF-based predictions). 
                                                7 http://nlp.stanford.edu/software/corenlp.shtml 
68
Feature Description Selected* target word The word to classify as person name Yes 2-grams of letters Features from the 2-grams of letters from the word Yes 3-grams of letters Features from the 3-grams of letters from the word No 4-grams of letters Features from the 4-grams of letters from the word No lowercase n-grams Features from the n-grams of letters from the word in  lowercase (considering 2-, 3-, and 4-grams separately) No position Position of the word within a sentence Yes PoS Part-of-speech tag of the word Yes lemma Lemma of the word Yes 
word shape 
Initial capital 
Yes 
All capitals Mix of uppercase and lowercase letters Digits inside All digits Has dash End dash Alpha-numeric Numeric-alpha (starts with a number) Contains punctuation mark 
dictionaries Does the word match with an entry of the dictionary of names? Yes Does the word match with an entry of the dictionary of  common words? Yes 2-word window The two preceding and following words in the context Yes 3-,4-,5-word window The three, four and five preceding and following words in the context No 
word-pairs Combinations of the word and the next and previous words in the  context window, preserving direction but not position  (considering separate features for the different combinations  of the context and the target word) No titles Match the word against a list of name titles (Mr, Mrs, etc.) No lemma_context Lemma of the words inside the contextual window No PoS_context Individual features from the part-of-speech tags  of the contextual window Yes PoS_sequence Sequence of the part-of-speech tags of the 2-word contextual window and the target word Yes word_shape_context Word shape features of the contextual window Yes word-tag Combination of the word and part-of-speech No Table 1. Set of learning features for the CRF-based prediction module. (* = selected in the best learning features set)  4 Evaluation and discussion Our evaluation consists of: 1) ?out-of-the-box? evaluation of the systems presented in Section 3.1; and 2) evaluation of the performance of our person names de-identification pipeline.     
4.1 Data We manually annotated all person names (includ-ing patients, relatives, health care providers, and other persons) in a corpus of various types of Vet-eran?s Health Administration (VHA) clinical notes. These notes were selected using a stratified ran-dom sampling approach with documents longer than 500 words. Then, the 100 most frequent VHA note types were used as strata for sampling, and the 
69
same number of notes was randomly selected in each stratum. Two reviewers independently anno-tated each document, a third reviewer adjudicated their disagreements, and a fourth reviewer eventu-ally examined ambiguous and difficult adjudicated cases. The evaluation corpus presented here comprises a subset of 275 VHA clinical notes from the aforementioned corpus. For training, 225 notes were randomly selected (contained 748 person name annotations), and the remaining 50 notes (with 422 name annotations) were used for testing the systems. 4.2 Experiments and results We present results in terms of precision, recall and F-measure (harmonic mean of recall and preci-sion). We used a weight of 2 when calculating the F2-measure giving recall more (twice) importance than precision (Jurafsky and Martin, 2009). This reflects our emphasis on recall for de-identification. To our understanding, due to legal and privacy issues, a good de-identification system should be tailored to prioritize recall, and conse-quently patient confidentiality. It is not the scope of this paper to judge or modify the development design adopted by other de-identification systems. Moreover, we considered correct predictions at least overlapping with the entire PHI annotation in the reference standard (i.e., exact match with the reference annotation, or more than the exact match). We can therefore assure complete redac-tion of PHI. Table 2 illustrates ?out-of-the-box? evaluation results of the systems described in Section 3.1. For this evaluation, we trained MIST and HIDE with our 225 notes training corpus, while the Stanford NER was run using the trained models available with its distribution8. Testing was realized using our 50 notes testing corpus. Table 3 shows the performance of our names de-identification approach. We provide results for dif-ferent configurations of our pipeline:  ? Rules & Dictionaries. Results of the rules and dictionary lookups component de-scribed in Section 3.2.2, in this case using a                                                 8 Further details about these models can be found at http://nlp.stanford.edu/software/CRF-NER.shtml 
keyword-search strategy for dictionary lookups. ? R&D with fuzzy searches. Results from the rules and dictionary lookups component us-ing Lucene?s Fuzzy Query engine for dic-tionary searches. It implements a fuzzy search based on the Levenshtein (edit dis-tance) algorithm9 (Levenshtein, 1966), which has to surpass a similarity threshold in order to produce a match. We carried out a greedy search on the training corpus for the best similarity threshold. We found 0.74 to be the best threshold. ? CRF-based w/FS. The CRF-based predic-tions component results after selecting the best set of features (see Section 3.2.3). The CRF classifier was trained using our 225-document training corpus. ? R&D + CRF w/FS. The cumulative results from the rules and dictionary lookups (not implementing fuzzy dictionary searches) and the CRF-based predictions components. ? R&D + CRF w/FS + FP-filtering. Includes all components together, adding the false-positive filtering component (Section 3.2.4) at the end of the pipeline. The SVM model for this last component was created using our training corpus.  System Prec. Rec. F2 HMS Scrubber 0.150 0.675 0.397 MeDS 0.149 0.768 0.419 MIT deid 0.636 0.893 0.826 MIST 0.865 0.319 0.356 HIDE 0.975 0.376 0.429 Stanford NER 0.692 0.723 0.716 Table 2. ?Out-of-the-box? evaluation of existing de-identification and NER systems (Prec.=precision; Rec.=recall; F2= F2-measure).  System Prec. Rec. F2 Rules & Dictionaries 0.360 0.962 0.721 R&D + fuzzy 0.171 0.969 0.502 CRF-based w/FS 0.979 0.874 0.893 R&D + CRF w/FS 0.360 0.988 0.732 R&D + CRF w/FS + FP-filtering 0.774 0.974 0.926 Table 3. Cumulative results of our pipeline of processes.                                                 9 http://www.merriampark.com/ld.htm 
70
4.3 Analysis Our novel names de-identification pipeline signifi-cantly outperforms all other systems we evaluated ?out-of-the-box? or trained with our VHA notes corpus. Among the five existing systems we evaluated (Table 1), only one achieved noteworthy recall around 89%. However, none of them obtained any remarkable F2-measure.  Most errors produced by the pattern matching systems (i.e., HMS Scrubber, MeDS, and MIT deid system) were due to false positive annotations of medical eponyms (e.g., ?Achilles?, ?Guyon?, etc.), as well as acronyms denoting medical facilities (e.g., ?ER? and ?HCS?). The false negatives consisted of ambigu-ous person names (e.g., ?Bill? and ?Chase?), some formats not covered by the patterns (e.g., ?[Last-Name], [FirstName] [Initial]?), and a few names not found in the dictionaries. Among machine learning-based systems, the two de-identification applications (i.e., MIST and HIDE) obtained good precision, but quite low re-call. The size of our training corpus was somewhat limited, and these results probably indicate a need for more sophisticated learning features, as well as feature selection procedures (rather than using the ?out-of-the-box? feature specification that comes with these systems) for better performance. With improved learning features, we could mitigate the relative lack of training examples. Interestingly, the NER system, which was trained on newswire documents, performed even better than some de-identification systems, although a need for im-provement is still present. We acknowledge that the comparison with Stan-ford NER is not completely fair due to the different source of documents used for training. However, we considered it interesting information, and alt-hough clinical notes contain characteristics not present in newswire corpora, they also have simi-larities regarding person names (e.g., titles ?Mr.?, ?Dr.?, ?PhD?, part-of-speech, verb tenses). There-fore, we think that only for names recognition, a newswire trained NER can provide interesting re-sults, and this was actually what we observed. Table 2 points out that the combination of our components produces successful cumulative re-sults. Using the training corpus to create a simple component made up of rules, dictionary lookups, and few heuristics for disambiguation allowed for 
recall values of 0.96. This demonstrates the need to adapt these techniques to the target documents, instead of employing systems ?out-of-the-box?. Our experiments with fuzzy dictionary lookups did not allow for a significant increase in recall, but caused a decrease in precision (-19%). It sug-gests that there was no need for considering person name misspellings. The component based on CRF predictions alone achieved good performance, especially in preci-sion. It obtained the best F2-measure (0.89), clearly higher than the other ?out-of-the-box? systems based on CRF models. It proves that selecting suit-able learning features mitigates to some extent the scarcity of training examples.  Our next experiment combined the rules and dictionaries and CRF components. It improved the overall recall to about 0.99, which means that CRF-based predictions recognized some person names that were missed by our pattern matching components, but didn?t increase the precision. We reached here our first goal of high recall or sensi-tivity. Finally, we added the false-positive filtering component to our system. This component was able to filter out 622 (84%) false positives from a total of 742, improving the precision to 0.77 (+41%); but also causing a slight decrease in recall (-1.4%). This application of our pipeline was suc-cessful, reaching an F2-mesure of 0.93, and was an effective way of training the SVM model for false-positives filtering.  5 Conclusions We designed and evaluated a novel person names de-identification system with VHA clinical docu-ments. We also presented an ?out-of-the-box? evaluation of several available de-identification and NER systems; all of them were surpassed by our approach. With our proposal, we showed that it is possible to improve the recognition of person names in clin-ical records, even when the corpus for training ma-chine learning classifiers is limited. Furthermore, the workflow of our pipeline allowed us to tackle the de-identification task from an intuitive but powerful perspective, i.e. facing the achievement of high recall and precision as two separate goals implementing specific techniques and components. 
71
Packaging this two-step procedure as a boot-strapping learning or adding the rules to define learning features would not allow us to use the qualities of the R&D and CRF components (i.e., obtain the highest recall by any means). Moreover, considering the small size of our manually anno-tated examples, these approaches would not work much better than existing systems.  As future efforts, we plan to improve the preci-sion of the rules and dictionary lookups component by adding more sophisticated person names disam-biguation procedures. Such procedures should deal with the peculiar formatting of clinical records as well as integrate enriched knowledge from bio-medical resources. We also plan to evaluate the portability of our approach by using other sets of clinical documents, such as the 2006 i2b2 de-identification challenge corpus (Uzuner et al, 2007).  Acknowledgments Funding provided by the Department of Veterans Affairs Health Services Research & Development Services Consortium for Healthcare Informatics Research grant (HIR 08-374).  References  John Aberdeen, Samuel Bayer, Reyyan Yeniterzi, Ben Wellner, Cheryl Clark, David Hanauer, Bradley Ma-lin, and Lynette Hirschman. 2010. The MITRE Iden-tification Scrubber Toolkit: design, training, and assessment. International journal of medical infor-matics, 79 (12) (December): 849-59. Bruce A. Beckwith, Rajeshwarri Mahaadevan, Ulysses J. Balis, and Frank Kuo. 2006. Development and evaluation of an open source software tool for deidentification of pathology reports. BMC medical informatics and decision making, 6 (1) (January): 12. Chih-Chung Chang and Chih-Jen Lin. (2001). LIBSVM: a library for support vector machines. Computer, 1-30. George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The Automatic Content Extraction (ACE) Program - Tasks, Data, and Evaluation. Pro-ceedings of LREC 2004: 837-840. Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sam-
pling. Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics: 363-370. F. Jeff Friedlin and Clement J. McDonald. 2008. A software tool for removing patient identifying infor-mation from clinical documents. Journal of the American Medical Informatics Association??: JAMIA 15 (5) (January 1): 601-10. James Gardner and Li Xiong. 2009. An integrated framework for de-identifying unstructured medical data. Data & Knowledge Engineering 68 (12) (De-cember): 1441-1451. Ralph Grishman and Beth Sundheim. 1996. Message understanding conference-6: A brief history. Pro-ceedings of the 16th conference on Computational linguistics - Volume 1: 466-471. Association for Computational Linguistics, Copenhagen, Denmark. Daniel Jurafsky and James H. Martin. 2009. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. 2nd edition. Prentice-Hall. Upper Saddle River, NJ, USA. V.I. Levenshtein. 1966. Binary Codes Capable of Cor-recting Deletions, Insertions and Reversals. Soviet Physics - Doklady 10: 707?710. Stephane M. Meystre, F. Jeffrey Friedlin, Brett R. South, Shuying Shen, and Matthew H. Samore. 2010. Automatic de-identification of textual documents in the electronic health record: a review of recent re-search. BMC medical research methodology 10 (1) (January): 70. Ishna Neamatullah, Margaret M. Douglass, Li-wei H. Lehman, Andrew Reisner, Mauricio Villarroel, Wil-liam J. Long, Peter Szolovits, George B. Moody, Roger G. Mark, and Gari D. Clifford. 2008. Auto-mated de-identification of free-text medical records. BMC medical informatics and decision making 8 (1) (January): 32. Guergana K. Savova, James J. Masanz, Philip V. Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-Schuler, and Christopher G. Chute. 2010. Mayo clin-ical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications. Journal of the American Medical In-formatics Association??: JAMIA 17 (5): 507-13. Erik F. Tjong Kim Sang, and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Lan-guage-Independent Named Entity Recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 - Volume 4: 142-147. ?zlem Uzuner, Yuan Luo, and Peter Szolovits. 2007. Evaluating the State-of-the-Art in Automatic De-identification. Journal of the American Medical In-formatics Association??: JAMIA 14(5):550-563. 
72
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 130?139,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
A Prototype Tool Set to Support Machine-Assisted Annotation 
Brett R. South1,2, Shuying Shen1,2, Jianwei Leng2, Tyler B. Forbush4,  Scott L. DuVall3,4, Wendy W. Chapman5 Departments of 1Biomedical Informatics, 2Internal Medicine, and 3Radiology University of Utah, Salt Lake City, Utah, USA  4IDEAS Center SLCVA Healthcare System, Salt Lake City, Utah, USA  5University of California, San Diego, Division of Biomedical Informatics, La Jolla, California, USA brett.south@hsc.utah.edu,  shuying.shen@hsc.utah.edu, jianwei.leng@utah.edu,  tyler.forbush@utah.edu, scott.duvall@utah.edu,  wendy.w.chapman@gmail.com 
Abstract 
Manually annotating clinical document corpora to generate reference standards for Natural Language Processing (NLP) sys-tems or Machine Learning (ML) is a time-consuming and labor-intensive endeavor. Although a variety of open source annota-tion tools currently exist, there is a clear opportunity to develop new tools and assess functionalities that introduce efficiencies into the process of generating reference standards. These features include: man-agement of document corpora and batch as-signment, integration of machine-assisted verification functions, semi-automated cu-ration of annotated information, and sup-port of machine-assisted pre-annotation. The goals of reducing annotator workload and improving the quality of reference standards are important considerations for development of new tools. An infrastruc-ture is also needed that will support large-scale but secure annotation of sensitive clinical data as well as crowdsourcing which has proven successful for a variety of annotation tasks. We introduce the Ex-tensible Human Oracle Suite of Tools  (eHOST) http://code.google.com/p/ehost that provides such functionalities that when coupled with server integration offer an end-to-end solution to carry out small or large scale as well as crowd sourced anno-tation projects. 1 Introduction Supervised learning methods benefit from a ref-erence standard that is used to train and evaluate 
the performance of Natural Language Processing (NLP) or Machine Learning (ML) systems for information extraction and classification. Ideal-ly, generating a reference standard involves the review of more than one annotator with an ac-companying adjudication step to resolve dis-crepancies (Roberts et al, 2007; Roberts et al, 2009). However, manual annotation of clinical, texts is time-consuming, expensive, and requires considerable effort. Reducing the time and costs required for manual annotation could be achieved by developing new tools that integrate methods to more efficiently annotate clinical texts and integrate a management interface that allows administration of large or small scale an-notation projects. Such a tool could also inte-grate methods to pre-annotate entities such as noun phrases or clinical concepts mapped to a standard vocabulary. Efficiencies could be real-ized via reduction in human workload, modifica-tion of annotation tasks that could include crowd sourcing, and implementation of machine-assisted approaches.  Typically annotation of clinical texts requires human reviewers to identify information classes of interest called ?markables?. These tasks may also require reviewers to assign attributes to those information classes and build relations between spans of annotated text. For each anno-tation task there may be one or many types of markables and each markable class may be asso-ciated with one or more spans of text and may include single or even multiple tokens. These tasks may occur simultaneously, or may also be done in different steps and by multiple review-ers. Furthermore, these activities require written guidelines that clearly explicate what infor-
130
mation to annotate, specifics about each marka-ble class, such as how much information to in-clude in annotated spans, or syntactic rules to provide further guidance on annotated spans. Annotation tasks may benefit by incorporating rules or guidelines as part of the annotation task itself in the form of machine-assisted verifica-tion. There are many annotation tools available, and the majority of them were designed for lin-guistic or gene annotation. Linguistic annotation tools such as Callisto and WordFreak are stand-alone clients suitable for small to medium scale tasks where collaborative effort is not empha-sized. Functionality integrated with eHOST was inspired by existing features of these tools with the intent of providing a more efficient means of reference standard generation in a large collabo-rative environment. One annotation tool called Knowtator, a plug-in for Prot?g? (Musen, M.A., et al 1995) developed by Ogren (2006) has been widely used to annotate clinical texts and gener-ate reference standards. However, no stand-alone system exists that can provide end users with the ability to manually or semi-automatically edit, curate, and easily navigate annotated information. There are also specific functionalities that are missing from open source annotation tools in the clinical and biomedical domains that would introduce efficiencies into manual annotation tasks. These functionalities include: annotation of clinical texts along with database storage of stand-off annotations, the ability to interactively annotate texts in a way that allows users to react to either pre-annotations imported from NLP or ML systems or use exact string matching across an active corpus to identify similar spans of text to those already annotated. Additionally, these systems do not generally support crowd sourcing, ma-chine-assisted pre-annotation or verification ap-proaches integrated directly with the annotation tool. This paper discusses development of a proto-type open source system designed to provide functionality that supports these activities and offers an end-to-end solution when coupled with server integration to reduce both annotator and administrative workload associated with refer-ence standard. We introduce the Extensible Hu-
man Oracle Suite of Tools (eHOST) created with these expectations in mind.  2 Background Our goal for these development efforts was to build a prototype open source system that im-proves upon existing tools by including new functions and refining capabilities available in other annotation tools. The resulting GUI inter-face provides a means of visually representing annotated information, its attributes, and rela-tions between annotated mentions. These efforts also focused integrating various machine-assisted approaches that can be used to easily curate and navigate annotated information with-in a document corpus, pre-annotate information, and also verify annotations based on rules checks that correspond with annotation guide-lines or linguistic and syntactic cues.   The eHOST provides basic functionality in-cluding manual annotation of information repre-senting markable classes and assignment of information attributes and relationships between markable classes. Annotations exported from eHOST are written using the XML format as Knowtator thus allowing integration of inputs and outputs to and from Knowtator and indirect-ly to Prot?g? 3.3.1. Coupling eHOST with an integrated server package such as the one under development by the VA Informatics and Com-puting Infrastructure (VINCI) called the Chart Administration Server for Patient Review (CASPR) provides one method of increasing efficiencies for small or large-scale annotation efforts that could also include crowd sourcing.  2.1 System Features Development In the domains of computational linguistics and biomedical informatics various approaches that can be used to improve annotation efficiencies have been evaluated for a variety of tasks in-cluding information extraction and classifica-tion. While several methods may help reduce the time and costs required to create reference standards, one of the simplest approaches may include integrating machine-assisted methods to pre-annotate relevant spans of text allowing the annotator to add missing annotations, modify spans, or delete spurious annotations. Neveol (2011) evaluated use of automatic semantic pre-
131
annotation of PubMed queries. This study showed a significant reduction in the number of required annotations when using pre-annotations, reduction in annotation time with higher inter-annotator agreement. Pre-annotation using simple approaches such as regular expres-sions coupled with dictionaries (South et al, 2010a) based on the UMLS as a source of lexi-cal knowledge (Friedman, 2001) and  pre-annotation of information representing protected health information (South et al, 2010b). In both cases finding that annotators preferred particular types of pre-annotation over others, but im-provements in reference standard quality occur when pre-annotation was provided. Others have explored the use of third party tools for the pre-annotation task for UMLS concepts (Savova, 2008) and pre-annotation using an algorithmic approach (Chapman, et al, 2007) combined with domain expert annotations reused for temporal relation annotation (Mowery, 2008). Savova (2008) suggests limited utility when a third party tool is used for pre-annotation and Mowery (2008) suggest that even with domain expert pre-annotations, additional features are required to discern temporality. Finally, Fort and Sagot (2008) evaluated using pre-annotation for part-of-speech tagging on the Penn Tree bank corpus and demonstrate a gain in quality and annotation speed even with a not so accurate tagger. Semi-automated curation has been explored as a means to build custom dictionaries for in-formation extraction tasks (Riloff, 1993). More recently this approach was spurred on by the BioCreative II competition (Yeh et al, 2003). Alex et al, (2008), explored the use of NLP-assisted text mining to speed up curation of bi-omedical texts. Settles et al, (2008) estimates true labeling costs and provides a review of ac-tive and interactive learning approaches as a means of providing labels and reducing the cost of obtaining training data (Settles, 2010). Alt-hough eHOST does not yet include an active learning module it does provide one means of interactive annotation so these are important considerations for future development efforts.  In the biomedical informatics domain crowd sourcing has been evaluated as part of the 2009 i2b2 Medication Challenge (Uzuner, 2010). Nowak and Ruger (2010) provide estimates of annotation reliability from crowd sourcing of 
image annotation. Hsueh et al, (2009) provide estimates of the quality of crowd sourcing for sentiment classification using both experts and non-expert annotators. In all three cases the re-sulting annotation set was of comparable quality to that derived from expert annotators. Wang et al, (2008) make general recommendations for best approaches to crowd sourcing that include closer interactions between human and machine methods in ways that more efficiently connect domain expertise with the annotation task.  Subsequent sections in this paper walk the reader through the various basic and advanced features eHOST provides. These features have been developed in a way that provides flexibility to add additional modules that support im-provements in annotation workflow and effi-ciency for a variety of annotation scenarios applicable to computational linguistics and bio-medical informatics. Some of these features may be useful for crowd-sourced efforts whereas oth-ers may simply represent an improvement in the way annotation is visualized or how manual ef-fort can be reduced. Figures in this paper use a set of synthetic clinical documents and a demon-stration annotation project based on the 2010 and 2011 i2b2/VA annotation tasks as examples available from http://code.google.com/p/ehost. 2.2 Systems Architecture The eHOST is a client application that can run on most operating systems that supports Java including, most Microsoft Windows x86/x64 platforms, Apple Mac OS X, Sun Solaris, and Linux. The application uses standardized for-mats including a file folder system, and struc-tured XML inputs and outputs. These capabilities also support integration with other open source tools for annotation and knowledge management including Knowtator and Prot?g?. An Extract-Transform-Load process (ETL) is used by the system to import concept infor-mation from different sources, such as XML or Prot?g? PINS files. These inputs sources are normalized for loading into eHOST. All data that exists in the data pool can be transformed into various output formats. Raw input data doc-uments in a single text file or sequential text files in a file folder system. Information representing an annotation in-
132
cluding concept attributes such as the annotated span, attributes, and relationships between anno-tations are inserted into a common data pool us-ing a dynamic structured storage space. The data pool ensures that eHOST has capabilities to add new functions easily without making major changes to system architecture.  2.3 Annotation Project Workspace In eHOST each project has its own user assigned  workspace that includes an annotation schema and document corpus. Annotation schema can also be imported from an existing Prot?g? PINS file. Project settings can be inherited from exist-ing projects for similar annotations tasks using eHOST. Other workspace functions include quickly switching between up to five of the most recently used workspaces. A workspace can be assigned for each annotation layer or document batch. In these situations, an annotator would receive a pre-compiled project that specifies all settings including any text documents and the annotation schema. Defining a workspace is a particularly useful function in situations where annotations may be crowd sourced and there may be multiple layers of annotation that are potentially fielded to many annotators. 2.3.1 Corpus Management For any annotation task, the end user must man-age the document corpus, which can originate from a server or a file folder system that con-tains individual text files. Using the stand-alone eHOST client tool, corpus management is ac-complished via the current workspace (Figure 1). When the user initializes a new project, docu-ments are placed in a ?corpus? folder that is as-sociated with the newly created annotation project. All text files, are copied to the ?corpus? folder at the time of workspace assignment. Therefore, there is no risk of deleting the origi-nal documents associated with each new annota-tion project. This feature makes distribution of projects easier, because of the consistency be-tween the workspace, corpus assignment and annotation output folders. For crowd-sourced projects eHOST can be integrated with a backend server via web services using an admin-istrative module called CASPR.    
 Figure 1. eHOST corpus management  2.3.2 Viewer/Editor Panels Figure 2 shows an annotation for ?full body pain?, (shown with black bar above and below the active annotation) and information for that annotation including the annotated span, the class assignment and an assertion for the 2010 and 2011 i2b2/VA Challenge annotation tasks (Uzuner et al, 2011 and Uzuner et al, 2012). The result editor tab and its associated panels serve as the central place for basic annotation features. These functionalities include: assigning an annotator, creating new annotations or adjust-ing annotated spans of text and assignment of attributes or creating relationships between an-notated spans of text. Other functions in the re-sults editor tab include navigation between documents in the active corpus, resizing the text displayed in the document viewer, and ?save? and ?save as? functions that assigns a path for XML output files. The end user can easily re-move all annotations in a document or remove specific kinds of annotations by deleting a ?markable? class as well as remove attributes, and relationships between all annotations.  From the navigator screen in the stand-alone eHOST client tool a user can build annotation schema specifying markable classes, their asso-ciated attributes, and any allowed relationships. The navigator interface allows the user to review all annotated spans either within the current document or across the entire document corpus, toggle the view of each class on or off, see counts for all unique annotations and all annota-tions for each class, and choose a class for a fast annotate mode.  An annotation editor panel allows the user to view more detailed information for each selected 
133
annotation. This includes the time stamp of when the annotation was created, annotator as-signment, comments on the annotation and class, attribute and relationship information.  Annotations can be created using several ap-proaches from the result editor. In the normal mode, a class assignment window appears when the user selects a span of text, new annotations are generated by selecting any one of the marka-ble classes.  Activating a ?one click annotate? mode is possible by checking the box next to a class of markables. Under this mode, any text 
selected is automatically annotated as that mark-able class. This feature improves task efficien-cies when categories of markables are low or annotations of the same category cluster in small sections. Keyboard shortcuts have also been in-tegrated with eHOST to reduce annotator click burden and dependence on a mouse. These shortcuts are available for tasks such as modifi-cation of spans, deletion of annotations, and nav-igation between annotations.  
 
 Figure 2. Example annotations using the eHOST interface  2.3.3 Server Integration Annotation projects of any scale benefit from an automated means of building and distributing batches of texts to annotators, managing stand-off XML files generated from annotation tasks or written directly to a database and getting and submitting assignments with minimal user input. Coupling eHOST with server components that comply with the web services API defined for eHOST allows these functionalities. The CASPR module under development by VINCI provides a means to automate the administration of annotation efforts that could include crowd-sourced annotation projects.  Clicking on the sync assignments tab in the eHOST client (Figure 2) brings up a GUI that 
allows annotators to sync with a server location, enter credentials, see documents assigned, and designate documents as on hold, in process, or completed. When a user syncs and gets assign-ments from CASPR, a project folder is created that contains the annotation schema, text docu-ments, and annotations sent from the server.  The CASPR module allows an annotator to open the project and complete their task without need-ing to manage files or folders.  Once completed, annotations can be synced to the server, and the next assignment will be loaded.  The CASPR module allows iterative distribution of annota-tion batches without sending large sets of docu-ments to annotators that may contain sensitive data, decreasing the risk of breaches in privacy and data security. 
134
2.3.4 Additional Features The document viewer panel employs visual cues to display relationships between annotations us-ing color coding representing a parent and child node and line indicator between them showing the relationship. An ?annotation profiler? to the right of the scroll bar shows the density of anno-tations color-coded to their categories, as well as relative to their positions in the document. This type of data visualization is useful to see the rel-
ative location of annotations within a single document or across an en tire document corpus.  An adjudication mode is also included in the stand-alone eHOST client that allows difference matching and side-by-side comparison of anno-tations for efficient adjudication of discrepancies between annotations. Standard reporting metrics can be calculated including Inter-Annotator Agreement (IAA), Recall, Precision and F1-Measure.  
 Figure 3. eHOST adjudication mode showing discrepant annotations between annotators A7 and B4
In Adjudication mode discrepant annotations are shown using a wavy red underline in the editor window and by a red bolded outline in a side by side two panel view between the annotation edi-tor and comparator (Figure 3). These metrics and comparison tables between annotator results on the same documents can be output as HTML formatted reports that can be used by an adjudi-cator to quickly identify discrepancies between 
annotators (Figure 4). These reports and the edi-tor window display can also be used to quickly train annotators on new clinical domains using a reference standard created by domain experts for training purposes. Using these features error analysis can also be done by importing outputs from an NLP system that have been converted into the XML format used by eHOST. 
    
135
Figure 4. HTML Formatted report showing discrepant annotations between annotators A7 and B4  3 Advanced eHOST Features  There are also other more advanced features that have been integrated with eHOST. These in-clude an ?Oracle? mode that allows semi-automated annotation of similar spans of text across a document corpus, a means to easily and quickly curate annotated spans of text to create custom dictionaries, and machine-assisted pre-annotation integrated with the annotation tool itself.  3.1 Oracle Mode Also implemented with eHOST is an ?Oracle? mode which uses exact string matching allowing the user to annotate all spans of text that are 
identical to a new annotation. The oracle lists where these candidate annotations are found along with the surrounding context. The annota-tor can then accept or reject candidate spans an-notated with the same markable class. Oracle mode can run within the current document or across the entire document corpus. This type of functionality is useful for annotation tasks that may involve identifying and marking spans of text that are repetitive or follow the same format For example, the 2011 i2b2/VA annotation task in which annotation of pronominal information was required for co-reference resolution (Figure 5). 
 
 Figure 5. Example annotations generated using the eHOST ?Oracle? mode 
136
3.2 Semi-Automated Curation and    Dictionary Management Using the navigator window users can navigate to all annotations in either a single document or across an entire document corpus (Figure 6). The end user can curate annotations directly, create classes on the fly, or add attributes to an-notations found from the navigator pane. These functions also allow users to easily identify spu-rious annotations introduced from machine-assisted approaches correct misclassification errors, and quickly curate all annotations within a single document or across an entire document corpus. 
 Figure 6. Semi-Automated curation within the  document corpus  One task often associated with development of NLP systems involves manually creating or en-hancing some existing representation of lexical knowledge that can be used as a domain specific dictionary. Using eHOST users can export anno-tations to create a dictionary of terms, phrases, or individual tokens that have been identified by human annotators and assigned to markable in-formation classes. Once curated, annotated in-formation can be exported as a new dictionary. User created dictionaries can be integrated with 
a database or exported and used in the creation of some ontologic representation of information using Prot?g?. Output from a dictionary manager is in the form of a delimited text file and can therefore be modified to fit any standardized information model or used to pre-annotate sub-sequent document batches. 3.3 Machine-Assisted Pre-Annotation An interface is provided in eHOST that can be used for machine-assisted pre-annotation of documents in the active project corpus using either dictionaries or regular expressions based approaches. Users can import libraries of regular expressions or build their own regular expres-sions using a custom regular expression builder. Users can build and modify dictionaries created as part of annotation tasks that may include semi-automated curation steps. Dictionaries and regular expressions can also be coupled with the ConText algorithm (Chapman et al, 2007) to identify concept attributes such as negation, ex-periencer, and temporality. Pre-annotations de-rived from some external third party source such as an NLP system written as Knowtator XML outputs may also be imported into eHOST or passed to eHOST using CASPR. Computational speed required for pre-annotation can be improved by selecting an op-tion to use an internal statistical dictionary in-dexing function. This feature is particularly useful in situations where pre-annotation dic-tionaries are extremely large, such as where a subset of some standard vocabulary may be used to pre-annotate documents. Using the result edi-tor and its associated functions annotators can add missed annotations, modifying existing an-notations and delete spurious annotations. Han-dling pre-annotations in this way allows troubleshooting and error analysis of NLP sys-tem outputs imported into eHOST that can be shown to a reviewer in context and also facili-tates interactive annotator training.   3.4 Machine-Assisted Verification One of the more innovative features integrated with eHOST is the ability to verify and produce recommendations that help human annotators comply with syntactic and lexical rules that are specified by annotation task guidelines. Ma-
137
chine-Assisted verification is most useful when used on lexical or syntax rules to ensure that candidate phrases generated by automated sys-tems are similar to those marked by humans. These rules rely more on adherence to patterns than on decision-making, so the strengths of human review with machine approaches to semi-automated verification can be leveraged. When identifying medical concepts, it is common that noun phrases are marked as candidates. The de-termination of how much of a noun phrase to mark (inclusion of articles, adjectives, noun-modifiers, prepositional phrases) and at what granularity (simple nouns or complex noun phrases) may vary with each project. The verifier allows portions of an annotation guideline to be programmed into rules that check for consistency. Rules check whether a word appears within a user-defined window before and after an annotation. Each rule can be linked to text that describes why the annotation was flagged. Annotators are then provided sugges-tions on the correct span based on the rule. Us-ing the surrounding text, the guideline text, and the suggestion, the annotator can determine the final span for an annotation. These machine-assisted verifier functions help support reference standard generation by providing the context of annotations that seem to fail syntactic and lexi-cal rules while allowing human annotators to focus on domain expertise required to identify and classify information found in clinical texts.  Conclusion Our prototype system provides functionalities that have been created to more efficiently sup-port reference standard generation including ma-chine-assisted annotation approaches. It is our hope that these system features will serve as the basis for the further development efforts that will be part of an enterprise level system. Out-puts of such an annotation tool could be used as inputs for pipeline NLP systems or as one com-ponent of a common workbench of tools used for clinical NLP development tasks.  We have implemented and tested eHOST for the 2010 and 2011 i2b2/VA challenge annota-tion tasks and annotation projects for the Con-sortium for Healthcare Informatics Research (CHIR). The stand-alone eHOST client tool is 
available from http://code.google.com/p/ehost along with a demonstration project, a users guide, API documentation, and source code. The eHOST/CASPR interfaces will be used to sup-port a large-scale crowd sourced annotation task used for annotation of disorders, temporal ex-pressions, uncertainty, and negation along with data standardization. These efforts will include more rigorous analysis and usability assessment of eHOST/CASPR for crowd sourcing and other small and large-scale annotation projects.  Acknowledgments Support and funding was provided by the VA Salt Lake City HealthCare System and the VA Consortium for Healthcare Informatics Research (CHIR), VA HSR HIR 08-374, the VA Infor-matics and Computing Infrastructure (VINCI), VA HIR 08-204, and NIH Grant U54 HL 108460 for integrating Data for Analysis, An-nonymization and Sharing (iDASH), NIGMS 7R01GM090187.  References  Beatrice Alex, Claire Grover, Barry Haddow, Mijail Kabadjov, Ewan Klein, Michael Matthews, Stuart Roebuck, Richard Tobin, and Xinglong Wang. 2008. Assisted curation: does text mining really help? In: Proceedings of the Pacific Symposium on Biocomputing.  Wendy W. Chapman, David Chu, John N. Dowling. 2007. ConText: An Algorithm for Identifying Contextual Features from Clinical Text. In: ACL-07 2007.  Carol Friedman, Hongfang Liu, Lyudmila Shagina, Stephen Johnson, George Hripcsak. 2001. Evaluat-ing the UMLS as a source of lexical knowledge for medical language processing. In: Proc AMIA Symp, 2001: 189-93. Karen Fort and Saggot B. 2010. Influence of Pre-Annotation on POS-tagged Corpus Development. In: Proceedings of the Fourth Linguistic Annota-tion Workshop. ACL 2010: 56-63.  Pei-Yun Hsueh, Prem Melville, Vikas Sindhwani. 2009. Data Quality from Crowdsourcing: A study of Annotation Selection Criteria. In: Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing. June 2009: 27-35.  Danielle Mowery, Henk Harkema, Wendy W. Chap-man. 2008. Temporal Annotation of Clinical Text. In: ACL-08 2008. 
138
Mark A. Musen, John Gennari, Henrik Eriksson, Samson W. Tu, and Angel R. Puerta. 1995. PROTEGE-II: computer support for development of intelligent systems from libraries of compo-nents. In: Medinfo 1995.  Aur?lie N?v?ol, Rezarta Islamaj-Do?an, Zhiyong Lu. 2011. Semi-automatic semantic annotation of PubMed queries: a study on quality, efficiency, satisfaction. In: J Biomed Inform. 2011 Apr; 44(2):310-8.  Stefanie Nowak and Stefan Ruger. 2010. How Relia-ble are Annotations via Crowdsourcing? A Study about Inter-Annotator Agreement for Multi-label Image Annotation. In: MIR 10  2010. Philip V. Ogren. 2006. Knowtator a protege plug-in for annotated corpus construction. In: Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Lin-guistics on Human Language Technology, 2006: 273-5. Philip V. Ogren, Guergana K. Savova, Christopher G. Chute. 2008. Constructing Evaluation Corpora for Automated Clinical Named Entity Recogni-tion. In: Proceedings of the sixth international conference on Language Resources and Evalua-tion LREC 2008: 3143-3150. Angus Roberts, Robert Gaizauskas, Mark Hepple, Neil Davis, George Demetriou, Yikun Guo, Jay Kola, Ian Roberts, Andrea Setzer, Archana Tapuria, Bill Wheeldin. 2007. The CLEF corpus: semantic annotation of clinical text. In: AMIA An-nu Symp Proc, 625-9. Angus Roberts, Robert Gaizauskas, Mark Hepple, George Demetriou, Yikun Guo, Ian Roberts, An-drea Setzer. 2009. Building a semantically anno-tated corpus of clinical texts. In: J Biomed Inform, 42(5): 950-66.  Ellen Riloff. 1993. Automatically constructing a dic-tionary for information extraction tasks.  In: Pro-ceedings of the Eleventh National Conference on Artificial Intelligence, 811-816.  
Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active Learning with Real Annotation Costs. In: Proceedings of the NIPS Workshop on Cost-Sensitive Learning. 2008. Burr Settles. 2009. Active Learning Literature Sur-vey. In: Computer Sciences Technical Report 1648. University of Wisconsin-Madison. 2009. Brett R. South, Shuying Shen, F. Jeff Friedlin, Mat-thew H. Samore, and Stephane M. Meystre. 2010. Enhancing Annotation of Clinical Text using Pre-Annotation of Common PHI. In: AMIA Annu Symp Proc. 2010.  Brett R. South, Shuying Shen, Robyn Barrus, Scott L. DuVall, Ozlem Uzuner, and Charlene Weir. 2011. Qualitative analysis of workflow modifications used to generate the reference standard for the 2010 i2b2/VA challenge. In: AMIA Annu Symp Proc. 2011.  ?zlem Uzuner, Imre Solti, Fei Xia, and Eithon Cadag. 2010. Community annotation experiment for ground truth generation for the i2b2 medica-tion challenge. In: J Am Med Inform Assoc, 2010. 17(5):519-23. ?zlem Uzuner, Brett R. South, Shuying Shen, and Scott L. DuVall. 2011. 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text. In: JAMIA 18(5): 552-556. ?zlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler B. Forbush, John Pestian, and Brett R. South. 2012. Evaluating the state of the art in co-reference resolution for electronic medical records. In: JAMIA doi: 10.1136/amiajnl-2011-000784. Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan. 2010. Perspectives on Crowdsourcing Anno-tations for Natural Language Processing. In: CSIDM Project No. CSIDM-200805. Alexander S. Yeh, Lynette Hirschman, and Alexan-der A. Morgan. 2003. Evaluation of text data min-ing for database curation: Lessons learned from the KDD challenge cup. In: Bioinformatics, 19(Suppl 1): i331?339, 2003. 
 
139
