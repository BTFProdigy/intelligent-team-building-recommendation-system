 Non-Verbal Cues for Discourse Structure 
Justine Cassell?, Yukiko I. Nakano?, Timothy W. Bickmore?,  
Candace L. Sidner?, and Charles Rich? 
 
?MIT Media Laboratory 
20 Ames Street 
Cambridge, MA 02139 
{justine, yukiko, bickmore}@media.mit.edu 
 
?Mitsubishi Electric Research Laboratories  
201 Broadway 
Cambridge, MA 02139 
{sidner, rich}@merl.com
Abstract 
This paper addresses the issue of 
designing embodied conversational 
agents that exhibit appropriate posture 
shifts during dialogues with human 
users.  Previous research has noted the 
importance of hand gestures, eye gaze 
and head nods in conversations 
between embodied agents and humans. 
We present an analysis of human 
monologues and dialogues that 
suggests that postural shifts can be 
predicted as a function of discourse 
state in monologues, and discourse and 
conversation state in dialogues. On the 
basis of these findings, we have 
implemented an embodied 
conversational agent that uses 
Collagen in such a way as to generate 
postural shifts.  
1. Introduction 
This paper provides empirical support for the 
relationship between posture shifts and 
discourse structure, and then derives an 
algorithm for generating posture shifts in an 
animated embodied conversational agent from 
discourse states produced by the middleware 
architecture known as Collagen [18].  Other 
nonverbal behaviors have been shown to be 
correlated with the underlying conversational 
structure and information structure of discourse.  
For example, gaze shifts towards the listener 
correlate with a shift in conversational turn 
(from the conversational participants? 
perspective, they can be seen as a signal that the 
floor is available).  Gestures correlate with 
rhematic content in accompanying language 
(from the conversational participants? 
perspective, these behaviors can be seen as a 
signal that accompanying speech is of high 
interest).  A better understanding of the role of 
nonverbal behaviors in conveying discourse 
structures enables improvements in the 
naturalness of embodied dialogue systems, such 
as embodied conversational agents, as well as 
contributing to algorithms for recognizing 
discourse structure in speech-understanding 
systems.   Previous work, however, has not 
addressed major body shifts during discourse, 
nor has it addressed the nonverbal correlates of 
topic shifts. 
2. Background 
Only recently have computational linguists 
begun to examine the association of nonverbal 
behaviors and language.  In this section we 
review research by non-computational linguists 
and discuss how this research has been 
employed to formulate algorithms for natural 
language generation or understanding. 
About three-quarters of all clauses in descriptive 
discourse are accompanied by gestures [17], and 
within those clauses, the most effortful part of 
gestures tends to co-occur with or just before the 
phonologically most prominent syllable of the 
accompanying speech [13]. It has been shown 
that when speech is ambiguous or in a speech 
situation with some noise, listeners rely on 
gestural cues [22] (and, the higher the noise-to-
signal ratio, the more facilitation by gesture). 
Even when gestural content overlaps with 
speech (reported to be the case in roughly 50% 
of utterances, for descriptive discourse), gesture 
often emphasizes information that is also 
focused pragmatically by mechanisms like 
prosody in speech.  In fact, the semantic and 
pragmatic compatibility in the gesture-speech 
relationship recalls the interaction of words and 
graphics in multimodal presentations [11]. 
On the basis of results such as these, several 
researchers have built animated embodied 
conversational agents that ally synthesized 
speech with animated hand gestures.  For 
example, Lester et al [15] generate deictic 
gestures and choose referring expressions as a 
function of the potential ambiguity and 
proximity of objects referred to.  Rickel and 
Johnson [19]'s pedagogical agent produces a 
deictic gesture at the beginning of explanations 
about objects. Andr? et al [1] generate pointing 
gestures as a sub-action of the rhetorical action 
of labeling, in turn a sub-action of elaborating.   
Cassell and Stone [3] generate either speech, 
gesture, or a combination of the two, as a 
function of the information structure status and 
surprise value of the discourse entity. 
Head and eye movement has also been examined 
in the context of discourse and conversation.   
Looking away from one?s interlocutor has been 
correlated with the beginning of turns.  From the 
speaker?s point of view, this look away may 
prevent an overload of visual and linguistic 
information. On the other hand, during the 
execution phase of an utterance, speakers look 
more often at listeners. Head nods and eyebrow 
raises are correlated with emphasized linguistic 
items ? such as words accompanied by pitch 
accents [7].  Some eye movements occur 
primarily at the ends of utterances and at 
grammatical boundaries, and appear to function 
as synchronization signals. That is, one may 
request a response from a listener by looking at 
the listener, and suppress the listener?s response 
by looking away.  Likewise, in order to offer the 
floor, a speaker may gaze at the listener at the 
end of the utterance. When the listener wants the 
floor, s/he may look at and slightly up at the 
speaker [10].  It should be noted that turn taking 
only partially accounts for eye gaze behavior in 
discourse. A better explanation for gaze 
behavior integrates turn taking with the 
information structure of the propositional 
content of an utterance [5]. Specifically, the 
beginning of themes are frequently accompanied 
by a look-away from the hearer, and the 
beginning of rhemes are frequently accompanied 
by a look-toward the hearer. When these 
categories are co-temporaneous with turn 
construction, then they are strongly predictive of 
gaze behavior.  
Results such as these have led researchers to 
generate eye gaze and head movements in 
animated embodied conversational agents.  
Takeuchi and Nagao, for example, [21] generate 
gaze and head nod behaviors in a ?talking head.?  
Cassell et al [2] generate eye gaze and head 
nods as a function of turn taking behavior, head 
turns just before an utterance, and eyebrow 
raises as a function of emphasis.   
To our knowledge, research on posture shifts 
and other gross body movements, has not been 
used in the design or implementation of 
computational systems.  In fact, although a 
number of conversational analysts and 
ethnomethodologists have described posture 
shifts in conversation, their studies have been 
qualitative in nature, and difficult to reformulate 
as the basis of algorithms for the generation of 
language and posture.  Nevertheless, researchers 
in the non-computational fields have discussed 
posture shifts extensively.  Kendon [13] reports 
a hierarchy in the organization of movement 
such that the smaller limbs such as the fingers 
and hands engage in more frequent movements, 
while the trunk and lower limbs change 
relatively rarely.   
A number of researchers have noted that 
changes in physical distance during interaction 
seem to accompany changes in the topic or in 
the social relationship between speakers.  For 
example Condon and Osgton [9] have suggested 
that in a speaking individual the changes in 
these more slowly changing body parts occur at 
the boundaries of the larger units in the flow of 
speech.  Scheflen (1973) also reports that 
posture shifts and other general body 
movements appear to mark the points of change 
between one major unit of communicative 
activity and another.   Blom & Gumperz (1972) 
identify posture changes and changes in the 
spatial relationship between two speakers as 
indicators of what they term "situational  shifts" 
-- momentary changes in the mutual rights and 
obligations between  speakers accompanied by 
shifts in language style. Erickson (1975) 
concludes that proxemic shifts seem to be 
markers of 'important' segments. In his analysis 
of college counseling interviews, they occurred 
more frequently than any other coded indicator 
of segment changes, and were therefore the best 
predictor of new segments in the data.  
Unfortunately, in none of these studies are 
statistics provided, and their analyses rely on 
intuitive definitions of discourse segment or 
?major shift?.  For this reason, we carried out 
our own empirical study. 
3. Empirical Study 
Videotaped ?pseudo-monologues? and dialogues 
were used as the basis for the current study.  In 
?pseudo-monologues,? subjects were asked to 
describe each of the rooms in their home, then 
give directions between four pairs of locations 
they knew well (e.g., home and the grocery 
store). The experimenter acted as a listener, only 
providing backchannel feedback (head nods, 
smiles and paraverbals such as "uh-huh").  For 
dialogues, two subjects were asked to generate 
an idea for a class project that they would both 
like to work on, including: 1) what they would 
work on; 2) where they would work on it 
(including facilities, etc.), and 3) when they 
would work on it. Subjects stood in both 
conditions and were told to perform their tasks 
in 5-10 minutes.  The pseudo-monologue 
condition (pseudo- because there was in fact an 
interlocutor, although he gave backchannel 
feedback only and never took the turn) allowed 
us to investigate the relationship between 
discourse structure and posture shift 
independent of turn structure.  The two tasks 
were constructed to allow us to identify exactly 
where discourse segment boundaries would be 
placed.  
The video data was transcribed and coded for 
three features: discourse segment boundaries, 
turn boundaries, and posture shifts. A discourse 
segment is taken to be an aggregation of 
utterances and sub-segments that convey the 
discourse segment purpose, which is an 
intention that leads to the segment initiation 
[12].   In this study we chose initially to look at 
high-level discourse segmentation phenomena 
rather than those discourse segments embedded 
deeper in the discourse.  Thus, the time points at 
which the assigned task topics were started 
served as segmentation points.  Turn boundaries 
were coded (for dialogues only) as the point in 
time in which the start or end of an utterance co-
occurred with a change in speaker, but excluding 
backchannel feedback. Turn overlaps were 
coded as open-floor time. We defined a posture 
shift as a motion or a position shift for a part of 
the human body, excluding hands and eyes 
(which we have dealt with in other work).  
Posture shifts were coded with start and end 
time of occurrence (duration), body part in play 
(for this paper we divided the body at the 
waistline and compared upper body vs. lower 
body shifts), and an estimated energy level of 
the posture shift. Energy level was normalized 
for each subject by taking the largest posture 
shift observed for each subject as 100% and 
coding all other posture shift energies relative to 
the 100% case.  Posture shifts that occurred as 
part of gesture or were clearly intentionally 
generated (e.g., turning one's body while giving 
directions) were not coded.  
4. Results 
Data from seven monologues and five dialogues 
were transcribed, and then coded and analyzed 
independently by two raters. A total of 70.5 
minutes of data was analyzed (42.5 minutes of 
dialogue and 29.2 minutes of monologue). A 
total of 67 discourse segments were identified 
(25 in the dialogues and 42 in the monologues), 
which constituted 407 turns in the dialogue data.  
We used the instructions given to subjects 
concerning the topics to discuss as segmentation 
boundaries.  In future research, we will address 
the smaller discourse segmentation.  For posture 
shift coding, raters coded all posture shifts 
independently, and then calculated reliability on 
the transcripts of one monologue (5.2 minutes) 
and both speakers from one dialogue (8.5 
minutes).   Agreement on the presence of an 
upper body or lower body posture shift in a 
particular location (taking location to be a 1-
second window that contains all of or a part of a 
posture shift) for these three speakers was 89% 
(kappa = .64).  For interrater reliability of the 
coding of energy level, a Spearman?s rho 
revealed a correlation coefficient of .48 (p<.01).  
4.1 Analysis 
Posture shifts occurred regularly throughout the 
data (an average of 15 per speaker in both 
pseudo-monologues and dialogues). This, 
together with the fact that the majority of time 
was spent within discourse segments and within 
turns (rather than between segments), led us to 
normalize our posture shift data for comparison 
purposes. For relatively brief intervals (inter-
discourse-segment and inter-turn) normalization 
by number of inter-segment occurrences was 
sufficient (ps/int), however, for long intervals 
(intra-discourse segment and intra-turn) we 
needed to normalize by time to obtain 
meaningful comparisons. For this normalization 
metric we looked at posture-shifts-per-second 
(ps/s).  This gave us a mean average of .06 
posture shifts/second (ps/s) in the monologues 
(SD=.07), and .07 posture shifts/second in the 
dialogues (SD=.08). 
 
Table 4.1.1. Posture WRT Discourse Segments 
Our initial analysis compared posture shifts 
made by the current speaker within discourse 
segments (intra-dseg) to those produced at the 
boundaries of discourse segments (inter-dseg). It 
can be seen (in Table 4.1.1) that posture shifts 
occur an order of magnitude more frequently at 
discourse segment boundaries than within 
discourse segments in both monologues and 
dialogues. Posture shifts also tend to be more 
energetic at discourse segment boundaries 
(F(1,251)=10.4; p<0.001). 
Table 4.1.2 Posture Shifts WRT Turns 
 ps/s ps/int energy 
inter-turn 0.140 0.268 0.742 
intra-turn 0.022  0.738 
Initially, we classified data as being inter- or 
intra-turn. Table 4.1.2 shows that turn structure 
does have an influence on posture shifts; 
subjects were five times more likely to exhibit a 
shift at a boundary than within a turn. 
 
Table 4.1.3 Posture by Discourse and Turn Breakdown 
 ps/s ps/int 
inter-dseg/start-turn 0.562 0.542 
inter-dseg/mid-turn 0.000 0.000 
inter-dseg/end-turn 0.130 0.125 
intra-dseg/start-turn 0.067 0.135 
intra-dseg/mid-turn 0.041  
intra-dseg/end-turn 0.053 0.107 
An interaction exists between turns and 
discourse segments such that discourse segment 
boundaries are ten times more likely to co-occur 
with turn changes than within turns. Both turn 
and discourse structure exhibit an influence on 
posture shifts, with discourse having the most 
predictive value. Starting a turn while starting a 
new discourse segment is marked with a posture 
shift roughly 10 times more often than when 
starting a turn while staying within discourse 
segment.  We noticed, however, that posture 
shifts appeared to congregate at the beginnings 
or ends of turn boundaries, and so our 
subsequent analyses examined start-turns, mid-
turns and end-turns. It is clear from these results 
that posture is indeed correlated with discourse 
state, such that speakers generate a posture shift 
when initiating a new discourse segment, which 
is often at the boundary between turns. 
In addition to looking at the occurrence and 
energy of posture shifts we also analyzed the 
distributions of upper vs. lower body shifts and 
the duration of posture shifts.  Speaker upper 
body shifts were found to be used more 
frequently at the start of turns (48%) than at the 
middle of turns (36%) or end of turns (18%) 
(F(2,147)=5.39; p<0.005), with no significant 
 Monologues Dialogues 
 ps/s ps/int energy ps/s ps/int energy 
inter-
dseg 
0.340 0.837 0.832 0.332 0.533 0.844 
intra-
dseg 
0.039 0.701 0.053  0.723 
dependence on discourse structure. Finally, 
speaker posture shift duration was found to 
change significantly as a function of both turn 
and discourse structure (see Figure 4.1.3). At the 
start of turns, posture shift duration is 
approximately the same whether a new topic is 
introduced or not (2.5 seconds). However, when 
ending a turn, speakers move significantly 
longer (7.0 seconds) when finishing a topic than 
when the topic is continued by the other 
interlocutor (2.7 seconds) (F(1,148)=17.9; 
p<0.001). 
Figure 4.1.1 Posture Shift Duration by DSeg and Turn 
 
5. System  
In the following sections we discuss how the 
results of the empirical study were integrated 
along with Collagen into our existent embodied 
conversational agent, Rea. 
5.1 System Architecture 
Rea is an embodied conversational agent that 
interacts with a user in the real estate agent 
domain [2]. The system architecture of Rea is 
shown in Figure 5.1. Rea takes input from a 
microphone and two cameras in order to sense 
the user?s speech and gesture. The UM 
interprets and integrates this multimodal input 
and outputs a unified semantic representation. 
The Understanding Module then sends the 
output to Collagen as the Dialogue Manager. 
Collagen, as further discussed below, maintains 
the state of the dialogue as shared between Rea 
and a user. The Reaction Module decides Rea?s 
next action based on the discourse state 
maintained by Collagen. It also assigns 
information structure to output utterances so that 
gestures can be appropriately generated.  The 
semantic representation of the action, including 
verbal and non-verbal behaviors, is sent to the 
Generation Module which generates surface 
linguistic expressions and gestures, including a 
set of instructions to achieve synchronization 
between animation and speech. These 
instructions are executed by a 3D animation 
renderer and a text-to-speech system. Table 5.1 
shows the associations between discourse and 
conversational state that Rea is currently able to 
handle. In other work we have discussed how 
Rea deals with the association between 
information structure and gesture [6]. In the 
following sections, we focus on Rea?s 
generation of posture shifts. 
 
Table 5.1:
 Discourse functions & non-verbal 
behavior cues 
Discourse 
level info. 
Functions non-verbal 
behavior cues 
Discourse 
structure 
new segment Posture_shift 
turn giving eye_gaze & 
(stop_gesturing  
hand_gesture) 
turn keeping (look_away  
keep_gesture) 
Conversation 
structure 
turn taking eye_gaze & 
posture_shift 
Information 
structure 
emphasize 
information 
eye_gaze & 
beat_and 
other_hand_gsts 
 
 
DSEG 
mid 
end 
intra inter 
8
7
6
5
4
3
2
1
start 
Understanding 
Module
Dialogue 
Manager
(Collagen)
Reaction 
Module (RM)
Animation 
Renderer 
Text to 
Speech
Speech 
Recognition
Vision 
Processing
Microphone Camera
Animation Speech 
Generation Module
Sentence 
Realizer
Gesture  
Component
Figure5.1: System architecture 
5.2 The Collagen dialogue manager 
CollagenTM is JAVA middleware for building 
COLLAborative interface AGENts to work with 
users on interface applications.  Collagen is 
designed with the capability to participate in 
collaboration and conversation, based on [12], 
[16].  Collagen updates the focus stack and 
recipe tree using a combination of the discourse 
interpretation algorithm of [16] and plan 
recognition algorithms of [14].  It takes as input 
user and system utterances and interface actions, 
and accesses a library of recipes describing 
actions in the domain.  After updating the 
discourse state, Collagen makes three resources 
available to the interface agent: focus of 
attention (using the focus stack), segmented 
interaction history (of completed segments) and 
an agenda of next possible actions created from 
the focus stack and recipe tree.  
 
5.3 Output Generation 
The Reaction Module works as a content 
planner in the Rea architecture, and also plays 
the role of an interface agent in Collagen. It has 
access to the discourse state and the agenda 
using APIs provided by Collagen. Based on the 
results reported above, we describe here how 
Rea plans her next nonverbal actions using the 
resources that Collagen maintains.  
The empirical study revealed that posture shifts 
are distributed with respect to discourse segment 
and turn boundaries, and that the form of a 
posture shift differs according to these co-
determinants. Therefore, generation of posture 
shifts in Rea is determined according to these 
two factors, with Collagen contributing 
information about current discourse state.  
5.3.1 Discourse structure information 
Any posture shift that occurs between the end of 
one discourse segment and the beginning of the 
next is defined as an inter-discourse segment 
posture shift. In order to elaborate different 
generation rules for inter- vs. intra-discourse 
segments, Rea judges (D1) whether the next 
utterance starts a new topic, or contributes to the 
current discourse purpose, (D2) whether the 
next utterance is expected to finish a segment. 
First, (D1) is calculated by referring to the focus 
stack and agenda. In planning a next action, Rea 
accesses the goal agenda in Collagen and gets 
the content of her next utterance. She also 
accesses the focus stack and gets the current 
discourse purpose that is shared between her and 
the user. By comparing the current purpose and 
the purpose of her next utterance, Rea can judge 
whether the her next utterance contributes to the 
current discourse purpose or not. For example, if 
the current discourse purpose is to find a house 
to show the user (FindHouse), and the next 
utterance that Rea plans to say is as follows, 
(1) (Ask.What (agent Propose.What (user FindHouse 
<city ?>)))  
Rea says: "What kind of transportation access do you 
need?" 
then Rea uses Collagen APIs to compare the 
current discourse purpose (FindHouse) to the 
purpose of utterance (1). The purpose of this 
utterance is to ask the value of the transportation 
parameter of FindHouse. Thus, Rea judges that 
this utterance contributes to the current 
discourse purpose, and continues the same 
discourse segment (D1 = continue).  On the 
other hand, if Rea?s next utterance is about 
showing a house,  
(2) (Propose.Should (agent ShowHouse (joint 
123ElmStreet))   
Rea says: "Let's look at 123 Elm Street." 
then this utterance does not directly contribute 
to the current discourse purpose because it does 
not ask a parameter of FindHouse, and it 
introduces a new discourse purpose ShowHouse. 
In this case, Rea judges that there is a discourse 
segment boundary between the previous 
utterance and the next one (D1 = topic change).  
In order to calculate (D2), Rea looks at the plan 
tree in Collagen, and judges whether the next 
utterance addresses the last goal in the current 
discourse purpose. If it is the case, Rea expects 
to finish the current discourse segment by the 
next utterance (D1 = finish topic).  As for 
conversational structure, Rea needs to know; 
(T1) whether Rea is taking a new turn with the 
next utterance, or keeping her current turn for 
the next utterance, (T2) whether Rea?s next 
utterance requires that the user respond.  
First, (T1) is judged by referring to the dialogue 
history1. The dialogue history stores both system 
utterances and user utterances that occurred in 
the dialogue. In the history, each utterance is 
stored as a logical form based on an artificial 
discourse language [20]. As shown above in 
utterance (1), the first argument of the action 
indicates the speaker of the utterance; in this 
example, it is ?agent?. The turn boundary can be 
estimated by comparing the speaker of the 
previous utterance with the speaker of the next 
utterance. If the speaker of the previous 
utterance is not Rea, there is a turn boundary 
before the next utterance (T1 = take turn). If the 
speaker of the previous utterance is Rea, that 
means that Rea will keep the same turn for the 
next utterance (T1 = keep turn).  
Second, (T2) is judged by looking at the type of 
Rea?s next utterance. For example, when Rea 
asks a question, as in utterance (1), Rea expects 
the user to answer the question. In this case, Rea 
must convey to the user that the system gives up 
the turn (T2 = give up turn).  
5.3.2 Deciding and selecting a posture shift 
Combining information about discourse 
structure (D1, D2) and conversation structure 
(T1, T2), the system decides on posture shifts 
                                                                 
1
 We currently maintain a dialogue history in Rea even 
though Collagen has one as well. This is in order to store 
and manipulate the information to generate hand gestures 
and assign intonational accents. This information will be 
integrated into Collagen in the near future. 
for the beginning of the utterance and the end of 
the utterance. Rea decides to do or not to do a 
posture shift by calling a probabilistic function 
that looks up the probabilities in Table 5.3.1.  
A posture shift for the beginning of the utterance 
is decided based on the combination of (D1) and 
(T1). For example, if the combined factors 
match Case (a), the system decides to generate a 
posture shift with 54% probability for the 
beginning of the utterance.  Note that in Case 
(d), that is, Rea keeps the turn without changing 
a topic, we cannot calculate a per interval 
posture shift rate. Instead, we use a posture shift 
rate normalized for time. This rate is used in the 
GenerationModule, which calculates the 
utterance duration and generates a posture shift 
during the utterance based on this posture shift 
rate.   On the other hand, ending posture shifts 
are decided based on the combination of (D2) 
and (T2).  
For example, if the combined factors match 
Case (e), the system decides to generate a 
posture shift with 0.04% probability for the 
ending of the utterance. When Rea does decide 
to activate a posture shift, she then needs to 
choose which posture shift to perform. Our 
empirical data indicates that the energy level of 
the posture shift differs depending on whether 
there is a discourse segment boundary or not. 
Moreover the duration of a posture shift differs 
depending on the place in a turn: start-, mid-, or 
end-turn. 
Posture shift selection Place of a 
posture shift Case 
Discourse 
structure 
information 
Conversation 
structure 
information 
Posture shift 
decision 
probability energy duration body part 
a 
topic 
change  take turn 0.54/int high default 
upper & 
lower 
b topic 
change keep turn 0 - - - 
c continue take turn 0.13/int low default upper or lower 
beginning of 
the utterance 
d 
D1  
continue 
T1 
keep turn 0.14/sec low short lower 
e 
finish 
topic give turn 0.04/int high long lower End of the 
utterance 
f 
D2 
continue 
T2 
give turn 0.11/int low default lower 
Table 5.3.1:Posture Decision Probabilities for Dialogue 
Based on these results, we define posture shift 
selection rules for energy, duration, and body 
part. The correspondence with discourse 
information is shown in Table 5.3.1.  For 
example, in Case (a), the system selects a 
posture shift with high energy, using both upper 
and lower body. After deciding whether or not 
Rea should shift posture and (if so) choosing a 
kind of posture shift, Rea sends a command to 
the Generation Module to generate a specific 
kind of posture shift within a specific time 
duration. 
Posture shift 
selection 
 
Ca
se 
Discourse 
structure 
information 
Posture 
shift 
decision 
probability energy 
g change topic 0.84/int high 
h 
D1 
continue 0.04/sec low 
 
Posture shifts for pseudo-monologues can be 
decided using the same mechanism as that for 
dialogue, but omitting conversation structure 
information.   The probabilities are given in 
table Table 5.3.2. For example, if Rea changes 
the topic with her next utterance, a posture shift 
is generated 84% of the time with high-energy 
motion. In other cases, the system randomly 
generates low-energy posture shifts 0.04 times 
per second.  
 
6. Example 
Figure 6.1 shows a dialogue between Rea and 
the user, and shows how Rea decides to generate 
posture shifts. This dialogue consists of two 
major segments: finding a house (dialogue), and 
showing a house (pseudo-monologue). Based on 
this task structure, we defined plan recipes for 
Collagen. The first shared discourse purpose 
[goal: HaveConversation] is introduced by the 
user before the example. Then, in utterance (1), 
the user introduces the main part of the 
conversation [goal: FindHouse].  
The next goal in the agenda, [goal: 
IdentifyPreferredCity], should be 
accomplished to identify a parameter value for 
[goal: FindHouse]. This goal directly 
contributes to the current purpose, [goal: 
FindHouse].  This case is judged to be a turn 
boundary within a discourse segment (Case (c)), 
and Rea decides to generate a posture shift at the 
beginning of the utterance with 13% probability. 
If Rea decides to shift posture she selects a low 
energy posture shift using either upper or lower 
body. In addition to a posture shift at the 
beginning of the utterance, Rea may also choose 
to generate a posture shift to end the turn. As 
utterance (2) expects the user to take the turn, 
and continue to work on the same discourse 
purpose, this is Case (f). Thus, the system 
generates an end utterance posture shift 11% of 
the time. If generated, a low energy  posture 
shift is chosen. If a beginning and/or ending 
posture shifts are generated, they are sent to the 
GM, which calculates the schedule of these 
multimodal events and generates them.  
In utterance (25), Rea introduces a new 
discourse purpose [goal : ShowHouse]. Rea, 
using a default rule, decides to take the initiative 
on this goal.  At this point, Rea accesses the 
discourse state and confirms that a new goal is 
about to start.  Rea judges this case as a 
discourse segment boundary and also a turn 
boundary (Case (a)). Based on this information, 
Rea selects a high energy posture shift.  An 
example of Rea?s high energy posture shift is 
shown on the right in Figure 5.2. 
As a subdialogue of showing a house, in a 
discourse purpose [goal : DiscussFeature], Rea 
keeps the turn and continues to describe the 
house. We handle this type of interaction as a 
pseudo-monologue. Therefore, we can use table 
Table 5.3.2 for deciding on posture shifts here. 
In utterance (27), Rea starts the discussion about 
the house, and takes the initiative. This is judged 
as Case (g), and a high energy body motion is 
generated 84% of the time. 
Table 5.3.2: Posture Decision Probabilities: Monologue 
  
7. Conclusion and Further work 
We have demonstrated a clear relationship 
between nonverbal behavior and discourse state, 
and shown how this finding can be incorporated 
into the generation of language and nonverbal 
behaviors for an embodied conversational agent. 
Speakers produce posture shifts at 53% of 
discourse segment boundaries, more frequently 
than they produce those shifts discourse 
segment-internally, and with more motion 
energy.  Furthermore, there is a relationship 
between discourse structure and conversational 
structure such that when speakers initiate a new 
segment at the same time as starting a turn (the 
most frequent case by far), they are more likely 
to produce a posture shift; while when they end 
a discourse segment and a turn at the same time, 
their posture shifts last longer than when these 
categories do not co-occur. 
Although this paper reports results from a 
limited number of monologues and dialogues, 
the findings are promising.  In addition, they 
point the way to a number of future directions, 
both within the study of posture and discourse, 
and more generally within the study of non-
verbal behaviors in computational linguistics. 
 
Figure 6.2: Rea demonstrating a low and high energy 
posture shift 
First, given the relationship between 
conversational and information structure in [5], 
a natural next step is to examine the three-way 
relationship between discourse state, 
conversational structure (turns), and information 
structure (theme/rheme).  For the moment, we 
have demonstrated that posture shifts may signal 
boundaries of units; do they also signal the 
information content of units? Next, we need to 
look at finer segmentations of the discourse, to 
see whether larger and smaller discourse 
segments are distinguished through non-verbal 
means.  Third, the question of listener posture is 
an important one.  We found that a number of 
posture shifts were produced by the participant 
who was not speaking.  More than half of these 
shifts were produced at the same time as a 
speaker shift, suggesting a kind of mirroring.  In 
order to interpret these data, however, a more 
sensitive notion of turn structure is required, as 
one must be ready to define when exactly 
speakers and listeners shift roles. Also, of 
course, evaluation of the importance of such 
nonverbal behaviors to user interaction is 
essential.  In a user study of our earlier Gandalf 
system [4], users rated the agent's language 
skills significantly higher under test conditions 
in which Gandalf deployed conversational 
behaviors (gaze, head movement and limited 
gesture) than when these behaviors were 
disabled.  Such an evaluation is also necessary 
for the Rea-posture system.  But, more 
generally, we need to test whether generating 
posture shifts of this sort actually serves as a 
signal to listeners, for example to initiative 
 
[Finding a house] < dialogue> 
  
(1) 
  
U: I?m looking for a house. 
  
(2) 
  
R:  (c)   Where do you want to live? (f)  (3) 
  
U: I like Boston. 
  
(4) 
  
R:  (c) (d)  What kind of transportation  
access do you need? (f)   
(5) 
  
U: I need T access. 
  
 ?.  
(23) 
  
R:  (c) (d)  How much storage space do  
you need?  (f)  
(24) 
  
U: I need to have a storage place in the  
basement. 
  
(25) 
  
R:  (a) (d) 
  
Let?s look at 123 Elm Street. (f) 
  
(26) 
  
U: OK. 
  
[Discuss a feature of the house] 
  
(27) 
  
R:  (g)  Let's discuss a feature of this place. 
  
(28) 
  
R:  (h)  Notice the hardw ood flooring in the  
living room. 
  
(29) 
  
R:  (h)  Notice the jacuzzi. 
  
(30) 
  
R:  (h) Notice the remodeled kitchen 
  
[Showing a house] <Pseudo-monologue> 
  Figure 6.1: Example dialogue 
structure in task and dialogue [8]. These 
evaluations form part of our future research 
plans. 
8. Acknowledgements 
This research was supported by MERL, France 
Telecom, AT&T, and the other generous sponsors of 
the MIT Media Lab.  Thanks to the other members of 
the Gesture and Narrative Language Group, in 
particular Ian Gouldstone and Hannes Vilhj?lmsson. 
9. REFERENCES  
[1] Andre, E., Rist, T., & Muller, J., Employing AI 
methods to control the behavior of animated 
interface agents, Applied Artificial Intelligence, 
vol. 13, pp. 415-448, 1999. 
[2] Cassell, J., Bickmore, T., Billinghurst, M., 
Campbell, L., Chang, K., Vilhjalmsson, H., & 
Yan, H., Embodiment in Conversational 
Interfaces: Rea, Proc. of CHI 99, Pittsburgh, PA, 
ACM, 1999. 
[3] Cassell, J., Stone, M., & Yan, H., Coordination 
and context-dependence in the generation of 
embodied conversation, Proc. INLG 2000, 
Mitzpe Ramon, Israel, 2000. 
[4] Cassell, J. and Thorisson, K. R., The Power of a 
Nod and a Glance: Envelope vs. Emotional 
Feedback in Animated Conversational Agents, 
Applied Art. Intell., vol. 13, pp. 519-538, 1999. 
[5] Cassell, J., Torres, O., & Prevost, S., Turn 
Taking vs. Discourse Structure: How Best to 
Model Multimodal Conversation., in Machine 
Conversations, Y. Wilks, Ed. The Hague: 
Kluwer, 1999, pp. 143-154. 
[6] Cassell, J., Vilhj?lmsson, H., & Bickmore, T., 
BEAT: The Behavior Expression Animation 
Toolkit, Proc. of SIGGRAPH, ACM Press, 
2001. 
[7] Chovil, N., Discourse-Oriented Facial Displays 
in Conversation, Research on Language and 
Social Interaction, vol. 25, pp. 163-194, 1992. 
[8] Chu-Carroll, J. & Brown, M., Initiative in 
Collaborative Interactions - Its Cues and Effects, 
Proc. of AAAI Spring 1997 Symp. on 
Computational Models of Mixed Initiative, 
1997. 
[9] Condon, W. S. & Osgton, W. D., Speech and 
body motion synchrony of the speaker-hearer, in 
The perception of language, D. Horton & J. 
Jenkins, Eds. NY: Academic Press, 1971, pp. 
150-184. 
[10] Duncan, S., On the structure of speaker-auditor 
interaction during speaking turns, Language in 
Society, vol. 3, pp. 161-180, 1974. 
[11] Green, N., Carenini, G., Kerpedjiev, S., & Roth, 
S, A Media-Independent Content Language for 
Integrated Text and Graphics Generation, Proc. 
of Workshop on Content Visualization and 
Intermedia Representations at COLING and 
ACL '98, 1998. 
[12] Grosz, B. & Sidner, C., Attention, Intentions, 
and the Structure of Discourse, Computational 
Linguistics, vol. 12, pp. 175-204, 1986. 
[13] Kendon, A., Some Relationships between Body 
Motion and Speech, in Studies in Dyadic 
Communication, A. W. Siegman and B. Pope, 
Eds. Elmsford, NY: Pergamon Press, 1972, pp. 
177-210. 
[14] Lesh, N., Rich, C., & Sidner, C., Using Plan 
Recognition in Human-Computer Collaboration, 
Proc. of the Conference on User Modelling, 
Banff, Canada, NY: Springer Wien, 1999. 
[15] Lester, J., Towns, S., Callaway, C., Voerman, J., 
& FitzGerald, P., Deictic and Emotive 
Communication in Animated Pedagogical 
Agents, in Embodied Conversational Agents, J. 
Cassell, J. Sullivan, et. al, Eds. Cambridge: MIT 
Press, 2000. 
[16] Lochbaum, K., A Collaborative Planning Model 
of Intentional Structure, Computational 
Linguistics, vol. 24, pp. 525-572, 1998. 
[17] McNeill, D., Hand and Mind: What Gestures 
Reveal about Thought. Chicago, IL/London, 
UK: The University of Chicago Press, 1992. 
[18] Rich, C. & Sidner, C. L., COLLAGEN: A 
Collaboration Manager for Software Interface 
Agents, User Modeling and User-Adapted 
Interaction, vol. 8, pp. 315-350, 1998. 
 [19] Rickel, J. & Johnson, W. L., Task-Oriented 
Collaboration with Embodied Agents in Virtual 
Worlds, in Embodied Conversational Agents, J. 
Cassell, Ed. Cambridge, MA: MIT Press, 2000. 
[20] Sidner, C., An Artificial Discourse Language for 
Collaborative Negotiation, Proc. of 12th Intnl. 
Conf. on Artificial Intelligence (AAAI), Seattle, 
WA, MIT Press, 1994. 
[21] Takeuchi, A. & Nagao, K., Communicative 
facial displays as a new conversational modality, 
Proc. of InterCHI '93, Amsterdam, NL, ACM, 
1993. 
[22] Thompson, L. and Massaro, D., Evaluation and 
Integration of Speech and Pointing Gestures 
during Referential Understanding, Journal of 
Experimental Child Psychology, vol. 42, pp. 
144-168, 1986. 
Taking Account of the User's View in 3D Multimodal Instruction 
Dialogue 
Yuk iko  I. Nakano and Ken j i  hnamura  and Hisash i  Ohara  
1-1 Hikari-no-oka, Yokosuka, Kanagawa, 239-0847 Japan 
{yukiko, i lnamura, ohara}@ntl;nly.isl.ntt.co.jp 
Abst ract  
While recent advancements in virtual reality technology 
have created a rich communication interface linking hu- 
mans and computers, there has beefl little work on build- 
ing dialogue systems for 3D virtual worlds. This paper 
proposes a method for altering the instruction dialogue 
to match the user's view in a virtual enviromnent. -\~re 
illustrate the method with the system MID-aD, which in- 
teractively instructs the user on dismantling some parts 
of a car. First, in order to change the content of ~he 
instruction dialogue to match the user's view, we extend 
the refinement-driven plmming algorithm by using the 
user's view as a l)lan constraint. Second, to manage the 
dialogue smoothly, the systeln keeps track of the user's 
viewpoint as part of the dialogue skate and uses this 
information for coping with interruptive sul)dialogues. 
These mechanisms enable MID-3D to set instruction di- 
alogues in an incremental way; it takes account of the 
user's view even when it changes frequently. 
1 I n t roduct ion  
In a aD virtual enviromnent, we can freely walk 
through the virtual space and view three di- 
mensional objects from various angles. A inul- 
tilnodal dialogue system for such a virtual en- 
vironment should ainl to realize conversations 
which are performed in the real world. It would 
also be very useflll for education, where it is 
necessary to learn in near real-life situations. 
One of the most significant characteristics of
3D virtual environments is that the user can se- 
lect her/his own view from whidi to observe the 
virtual world. Thus, the nmltimodal instruc- 
tion dialogue system should be able to set the 
course of the dialogue by considering the user's 
current view. However, previous works on nml- 
tilnodal presentation generation and instruc- 
tion dialogue generation (Wahlster et al, 1993; 
Moore, 1995; Cawsey, 1992) do not achieve this 
goal because they were not designed to hail- 
(lie dialogues pertbrmed in 3D virtual environ- 
ments .  
This paper proposes a method that ensures 
that the course of the dialogue matches the 
user's view in the virtual environment. More 
specificall> we focus on (1) how to select the 
contents of the dialogue since it is essential 
that the instruction dialogue system form a se- 
quence of dialogue contents that is coherent 
and comprehensible, and (2) how to control 
mixed-initiative instruction dialogues nloothly, 
especially how to manage interruptive subdia- 
logues. These two problelns basically determine 
the course of the dialogue. 
First, in order to decide the appropriate con- 
tent, we propose a content selection mechanism 
based on plan-based multilnodal presentation 
generation (Andrd and Rist, 1993; Wahlster et 
al., 1993). We extend this algorithm by using 
the user's view as a constraint in expanding the 
plan. In addition, by employing tilt incremen- 
tal planning algorithm, the syst;em can adjust 
the content o match the user's view during on- 
going conversations. 
Second, ill order to nlanage interruptive sub- 
dialogues, we propose a dialogue management 
mechanism that takes account of the user's 
view. This mechanism maintains the user's 
viewpoint as a dialogue state in addition to in- 
tentional and linguistic context (Rich and Sid- 
her, 1998). It maintains the dialogue state as a 
focus stack of discourse segments and updates 
it at each turn. Tlms, it Call track the view- 
point information in an on-going dialogue. By 
using this viewpoint inibrlnation in restarting 
the dialogue after an interruptive subdialogue, 
the dialogue Inai~agement medmnism returns 
the user's viewpoint o that of the interrupted 
segment. 
These two mechanisms work as a core dia- 
logue engine in MID-3D (Multimodal Instruc- 
tion Dialogue system for 3D virtual environ- 
ments). They make it possible to set the in- 
struction dialogue in an increnlental ww while 
572 
Figure 1: Right angle 
Figure 2: l,efl; angle 
considering the user's view. They also (mal)h'~ 
MID-a1) to (:re~te coherent and mixe, d-initiative 
(liah)gues in virtual enviromuents. 
This paper is organized as lbllows. In Sec- 
ti(m 2, we define the 1)rol)h;ms spc(:ifi(: 1;o 313 
multimoda\] (tiah)gne genera.tion. Section 3 de- 
scribes rclat;ed works. \ ]n S('x:l;ion 4, we pro- 
pose the MID-a1) architecture. Sections 5 ;rod 
6 des(:ril)e the contenl; plmming meclm.nism a.nd 
the dialogue manngement meclm.nism, a.nd show 
they dynami(:ally decide coherent insl;rn(:t;ions, 
and control mixed-initial;ire diah)guc.s consider- 
ing the user's view. V~/e also show a smnt)le di- 
:dogue in Section 7. 
2 Prob lems 
In a virtual emdromnent, the user can freely 
move a.round the world and select her/his own 
view. r\['he systelll C&llllOt; predict where the user 
will stand and what; s/he observes in the vir- 
tual environment. This section describes two 
types of 1)roblems in generating instru(:tion dia- 
logues ibr such virtual enviromnents. They arc 
caused l)y mismatches b(~,twe(;ll tile llSel'~S vi0,w- 
l)oint ;m(1 the sta.te of th(; dialogue. 
First, the syStelll shouM check whether the 
user's view matches the tbcns of the next ex- 
change when the systen~ tries to ('hange COllllllll- 
ni('ative goals. \]if a mismatch occurs, the system 
shouhl choose the instru(:tion (li~dogue content 
according to the user's view. Figure 1 a,n(1 2 m:e 
examl)les of observing a car's front suspension 
from (liff(',r(mt, points of view. In Figm'(', 1, the 
right; side of the steering system can 1)e seen, 
while Figure 2 shows the left side. If the system 
is not aware of the user's view, I;he system may 
talk about the left; tie rod end even though the 
user's view remains the right side (Figure 1). 
In such n (:ase, the system shouM chang(: its d(> 
scril)tion or ask the user to change her/his view 
to |;11('. left; side. view (Figure 2) and r('.(-Olmnen(:e 
its instruction hi)out this part. Therefore, the 
system should be al)le to change the content 
of the dialogue according 1;o the user's view. 
In order to ac(:omplish this, the system shoul(1 
lmve ;1. content selection nlechan.ism whi(:h in- 
crementally (let:ides i;h('~ content while ('he(:king 
the llSef~s (;llrrellt vi(!w. 
Second, t;here could 1)e a case in which 1;21(; 
user chang(~s 1;he, topi(: as well as the vie\vl)oillt 
as interrupl;ing the. system's instru('t;ion, i n such 
a case, the (tia.h)gue~ system shouhl kee l) track of 
the user's viewpoint as ~ 1)art of the dialogue 
state nnd return to that viewpoint when resmn- 
ing the (lia.logu(? after the interrupl;ing sul)(li- 
alogue. Sul)l)ose that while the sys|;em is (',x- 
l)lnining tlm right; t)i(; rod end, th('. user initially 
looks a,t the right side, (l"igure 1) hut then shifts 
her/his view to the left (Figure 2) and asks 
about the \]eft knu(-kle arm. After finishing a 
sub(lialogue about this arm, the syst(;nl tries 
to return to the dialogue al)out the interrupted 
topic. At this time, if the sysl;em resumed the 
dialogue using the current view (Figure 2), the 
view and the instruction would \])e(;olne mis- 
matched. When resmning the interrupted i- 
alogue, it would be less (:onfllsing to the user 
if the system retm:ned to the user's prior view- 
l)oint rather than selecting n new o11o. '\].'he user 
may be (:onfilsed if the dialogue is resulned but 
the observed state looks different. 
\,Ve address the ~fl)ove problems. In order to 
(:ope wit;h the first; problem, we present a con- 
tent selection mechanism that incrementally ex- 
pands the content plan of a multimodal dialogue 
while checking the user's view. To solve the 
second 1)roblem, we present a. dialogue nmnage- 
merit me(:\]mnism l;hat keel)s t:ra(-k of the user's 
viewpoint as a part of the diah)gue context and 
573 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  -: 
~; '~ Operation l,uttons 
Figure 3: The system architecture 
uses this intbrmation in resuming the dialogue 
after interruptive subdialogues. 
3 Re la ted  work  
There are many multimodal systems, such as 
nmltimedia presentation systems and animated 
agents (Mwbury, 1993; Lester et al, 1997; 
Bares and Lester, 1997; Stone and Lester, 1996; 
Towns et al, 1998)~ all of which use 3D graph- 
ics and 3D animations. In some of them (May- 
bury, 1993; Wahlster et al, 1993; Towns et 
al., 1998), planning is used in generating mul- 
timodal presentations including graphics and 
animations. They are similar to MID-aD in 
that they use planning mechanisms in content 
planning. However, in presentation systems, 
unlike dialogue systems, the user just watches 
the presentation without changing her/his view. 
Therefore, these studies are not concerned with 
dlanging the content of the discourse to match 
the user's view. 
In some studies of dialogue management 
(Rich and Sidner, 1998; Stent et M., 1999), 
the state of the dialogue is represented using 
Grosz and Sidner's framework (Grosz and Sid- 
ner, 1986). We also adopt this theory in our di- 
alogue management mechanism. However, they 
do not keep track of the user's viewpoint infor- 
mation as a part of the dialogue state because 
they were not concerned with dialogue manage- 
ment in virtual environments. 
Studies on pedagogical agents have goals 
closer to ours. In (Rickel and .\]ohnson, 1999), 
a pedagogical agent demonstrates the sequen- 
tial operation of complex machiuery and an- 
swers some follow up questions fl'on~ the stu- 
dent. Lester et al (1999) proposes a life- 
like pedagogical agent that supports problem- 
solving activities. Although these studies are 
concerned with building interactive learning en- 
vironments using natural anguage, they do not 
discuss how to decide the course of on-going in- 
struction dialogues in an incremental nd coher- 
ent way. 
4 Overview of the System 
Arch i tec ture  
This section describes the architecture of MID- 
3D. This system instructs users how to disman- 
tle the steering system of a cal'. Tile system 
steps through the procedure and the user can 
interrupt he system's instructions at any time. 
Figme 3 shows the architecture and a snapshot 
of the system. The 3D virtual environment is
viewed through an application window. A 3D 
model of a part of the car is provided and a frog- 
574 
like character is used as the pedagogical agent 
(Johnson et al, 2000). The user herself/himself 
Call also al)l)ear in the virtual enviromn(mt as 
an avatar. The buttons to the right of the 3D 
scre(m are operation 1)uttons tbr changillg the 
viewpoint. By using these buttons, the user can 
freely change her/his viewt)oint at any time. 
This system consists of five main modules: 
hll)Ut Analyzer, Domain Plan Reasoner, Con- 
tent Planner (CP), Sentence Planner, Dialogue 
Manager (DM), and Virtual Environment Con- 
troller. 
First of all, the user's inputs are interpreted 
through the Input Analyzer. It receives trings 
of characters from the voice recognizer and 
the user's inputs ti'om the Virtual Environment 
Controller. It interl)rets these inputs, trans- 
forms them into a semantic reprcsentation~ and 
sends them to the DM. 
The DM, working as a dialogue management 
mechanism, keeI)s track of the dialogue (:ontext 
including the user:s view and decides the, next 
goal (or a(:tion) of the system. Ut)on receiv- 
ing an intmt from the user through the Input 
Analyzer, the DM sends it to the l)omaill Plan 
Reasoner (DPR) to get discourse goals for re- 
st)onding to the inlmt. For example, if th(: user 
requests ome instruction, the DI'I{ decides the 
sequence of steps that realizes the l)rocedure 1)y 
refi~rring to domain knowh~dge. Th(: 1354 (;hen 
adds (;he discourse goals to the goal agenda. 
If the user does not sulmlit a ~lew (;ot)ie , the 
DM (:ontilmes to expand the, instruction plan 
1)y sending a goal in the goal agenda to (:lie CP. 
Details of the I)M are given in Section 6. 
After the goal is sent to the CP, it decides the 
apl)ropriate contents of instruction dialogue by 
eml)loying a refinement-driven hierar(:hi(:al lin- 
ear 1)lamfing technique. When it; receives a goal 
fl'om the DM, it exl)ands the goal and returns 
its sul)goal to the DM. 13y ret)eating this pro- 
cess, the dialogue contents are, gradually spec- 
ified. Theretbre, the CP provides the scenario 
tbr the instruction 1)ased on the control 1)rovided 
by the DM. Details of the CP are provided in 
Section 5. 
The Sentence Plalmer generates urface, lin- 
guisti(: expressions coordinated with action 
(Kato et al, 1996). The linguistic exl)ressions 
arc. output through a voice synthesizer. Actions 
;/re realized through the Virtual Enviromnent 
Controller as 3D animation. 
For the Virtual Environment Controller, we 
use HyCLASS (Kawanol)e et al, 1998), which 
<Operator 1> 
(:tleader 
:Iiftbcl 
:Constraints 
:Main-Acts 
:Subskliary-Acts 
<Operator 2> 
(:lleader 
:Effect 
:Conslraints 
:Main-Acts 
:Subsidiary-Acts 
(Inshuct-act N l l ?act MM) 
(BMB S 11 (Goal II (Done 11 ?act))) 
((KB (Obj ?act ?object)) 
(Visible-p (Visible ?ol~iect t))) 
((Look S II) 
(Request S I I (Try It (action ?act)) NO-SYNC MM)) 
((Describe- act S II ?act MM) 
(Reset S (actioll ?act)))) 
(Instruct-act S 11 ?act MM) 
(BMB S 11 (Goal I1 (Done 11 '?act))) 
((KB (Obj ?act ?object)) 
(Visiblc-p (Visible ?object oil))) 
((Look S ll) 
(Make-recognize S 11 (Object ?object) MM) 
(Rcqucst S 11 (Try I1 (action ?act)) NO-SYNC M M)) 
((l)escribc-act S 11 ?act MM) 
(Reset S (action '?act)))) 
Figure 4: Exanlt)les of Content Plan Operators 
is a 3D simulation-1)ased nvironment tbr edu- 
(:ational activities. Several APls are provided 
tbr controlling HyCLASS. By using these in- 
terfaces, the CP and the DM can discern the 
liser~s view and issue an action command in ()l'- 
der to challge the virtual (;nvironnmllt. \?h(m 
HyCLASS receives an action command, it in- 
terprets the command and renders the 31) ani- 
mation corresponding to the action in real time. 
5 Se lec t ing  the  Conten(;  o f  
I ns t ruct ion  D ia logue  
Ill this section, we introduce the CP and show 
how the instruction dialogue is (leeided in all 
in(:renl(:ntal way to ma, tch the user's view. 
5.1 Content  P lanner  
In MID-3D, the CP is (:ailed by the DM. Wheal 
a goal is put to the CP fl'(nn the DM, it; selects a 
plan operator fi)r achieving the goal, applies the 
ol)erator to lind new subgoals, and returns them 
to l;he \])M. The sul)goals are then added to the 
goal agenda maintained by the DM. Theretbre, 
the CP provides the seenm:io tbr the instruc- 
tion dialogue to the DM and enables MID-3D 
to output coherent instructions. Moreover, the 
Content Planer emt)loys depth-first search with 
a retinement-drivell hierarchical linear plmming 
algorithm as in (Cmvsey, 1992). The advantage 
of this method is that the t)lan is de, veloped in- 
crenmntally, and can be changed while the con- 
versation is in progress. Thus, by aI)plying this 
algorithm to 3D dialogues, it be(-omes lmssible 
to set instruction dialogue strategies that are 
contingent on the user's view. 
575 
5.2 Considering the User's View in 
Content  Se lect ion 
In order to decide the dialogue content accord- 
ing to tile user's view, we extend the descrip- 
tion of the content plan operator (Andrd and 
Rist, 1993) by using the user's view as a con- 
straint in plan operator selection. We also mod- 
ify the constraint checking flmctions of |;lie pre- 
vious planning algorithm such that HyCLASS 
is queried about the state of the virtual envi- 
ronment. 
Figure 4 shows examples of content plan op- 
erators. Each operator consists of the name 
of the operator (Header), the etfcct resulting 
from plan execution (Effect), the constraints for 
executing the plan (Constraints), the essential 
subgoals (Main-acts), and the optional subgoals 
(Subsidiary-acts). As shown in {Operator 1.) 
in Figure 4, we use the constraint (gisible-p 
(Visible ?object t)) to check whether the 
object is visible fl'om tile user's viewpoint. 
Actually, the CP asks HyCLASS to examine 
whether the object is in the student's field of 
view. 
If an object is bound to the ?ob jec t  vari- 
able by rel~rring to the knowledge base, and 
the object is visible to the user, (Operator 1) 
is selected. As a result, two Main-Acts (look- 
ing at the, user and requesting to try to do 
the action) and two Subsidiary-Acts (showing 
how to do the action, then resetting the state) 
are set as subgoals and returned to the DM. 
In contrast, if l;he object is not visible to the 
user, {Operator 2} is selected. In this case, a 
goal for making the user i(tenti(y the object is 
added to the Main-Acts; (Hake-recognize S 
H (Object ?object) MM). 
As shown al)ove, the user's view is considered 
in deciding the instruction strategy. In addition 
to the above example, the distance between the 
target object and the user as well as three di- 
mensional overlapping of objects, can also be 
considered as constraims related to the user's 
view. 
Although the user's view is also considered in 
selecting locative expressions of objects in the 
Sentence Planner in MID-3D, we do not discuss 
this issue here becanse surface generation is not 
the tbcus of this paper. 
6 Manag ing  I n ter rupt ive  
Subd ia logue  
The DM controls the other components ofMID- 
3D based on a discourse model that represents 
the state of tile dialogue. This section describes 
the DM and shows how the user's view is used 
in managing the instruction dialogue. 
6.1 Maintaining the  D iscourse  Mode l  
The DM maintains a discourse model for track- 
ing the state of the dialogue. The discourse 
model consists of the discourse goal agenda 
(agenda), focus stack, and dialogue history. The 
agenda is a list of goals that should be achieved 
through a dialogue between the user and the 
system. If all the goals in the agenda re accom- 
plished, the instruction (tialogue finishes suc- 
cessflflly. The focus stack is a sta& of discourse 
segment frames (DSF). Each DSF is a frmne 
structure that stores the tbllowing inlbrmation 
as slot vMues: 
utterance content (UC): A list of utter- 
ance contents constructing a discourse segment. 
Physical actions are also regarded as uttcra.nce 
contents (D;rguson and Allen, 1998). 
discourse purpose (1)19: The purt)ose of a dis- 
course segment. 
- 9oal state (GS): A state (or states) whi('h 
shouhl 1)e accomplished to achieve the discourse 
lmrpose of the segment. 
In addition to these, we add the user's view- 
point slot to the DSF description in order to 
track the user's viewl)oint information: 
user's vic.'wpoint (UV): Current user's view- 
point, which is represented as the position and 
orientation of the camera. The position consists 
of x-, y-, and z-coordinates. The orientation 
consists of x-, y-, and z-angles of the ('amera. 
The basic algorithm of the DM is to repeat 
(a) th(; peribnning actions step and (1)) updat- 
ing the discourse model, until there is no un- 
satisfied goal in the agenda (~IYaum, 1994). In 
1)ertbrming actions step, the DM decides what 
to do next ill the current dialogue state, an(1 
then pertbnns the action. When continuing the 
system explanation, the DM posts the first goal 
in the agenda to the CP. If the user's response 
is needed in the current state, the 1)M waits tbr 
the nser's input. 
The other step in the DM algorith.m is to up- 
date the discourse model according to the state 
that results from the actions pertbrmed by the 
user as well as the actions peribrmed by the sys- 
tem. Although we do not detail this step here, 
the tbllowing operations could be executed e- 
pending on the case. if the current discourse 
purpose is accomplished, the top level DSF is 
popped and added to the dialogue history, q_/he 
576 
l I)SFI21 
DSFI2 
DSFI Jf J J 
UV: ((18, -20, -263) (0, 0.3 I, 0)) 
UC: ((IJseJ~act (Ask where heal_r)) 
I)P: (Response-to-user-act 
(Uscr-act (ask where bootr))) 
GS: ((Know 11 (About (l'lace_of boot_r)))...) 
UV: ((-38, -22, -259) (0, -0.33, 0)) 
UC: ((System-act (lnl'(~rm S 11 (Show S (Action 
rcmovc-tiemd end.I)) NO-SYNC I'R)) 
DI': (I)cscribe-acl S l I rcmove-licrod end I)) 
GS: ((Know 1I (llove-lo-do 11 
(action remove-tiered eml I)))...) 
Figure 5: Example of the state of a dialogue 
system then assunms that the user understands 
the instruction and adds the assumption to the 
user model. If a new discourse 1)urpose is in- 
troduced from the CP, the I)M creates a new 
DSF by setting the header of the selected plan 
operator in the discourse lmrpose slot mM the 
effi~ct of the operator in the goal state slot. The 
DSF is then trashed to the tbcus stack. If the 
current discourse purpose is contimmd, the DM 
updates the information of the top level DSF. 
6.2 Cons ider ing  the  User 's  V iew in 
Coping wi th  Interrupt ive 
Subd ia logues  
The main ditlbxence of the Dialogue Manager of 
our system from the i)revious one is to maintain 
the user's viewpoint information and use this in 
managing the dialogue. When the DM updates 
the information of the current DSt i', it observes 
the user's viewi~oint at  that petal; and renews 
the UV slot and it also adds the sema.nl;ic rep- 
resentation of utterance (or action) in the UC 
slot. As a result, it becomes possible to update 
the user's viewpoint information at each turn, 
and to track the user's viewl)oint in an on-going 
dialogue. 
By using this mechanism, the DM can cope 
with interruptive subdialognes. In resmning 
from a subdialogue, the user may become con- 
timed if the dialogue is resumed but the ob- 
served state differs from what the user relllelll- 
hers. In order to match the view to the re- 
sumed dialogtm, the I)M refers the UV slot of 
the top DSF and puts the users view ha& to 
that point. This ensures that the user experi- 
ences a smooth transition back to the previous 
topic. Figure 5 shows an example of the state of 
a dialogue. DSF12 represents a discourse seg- 
ment that describes how to remove the left tie 
rod end. DSF121 represents the user-initiated 
interrul)tive subdialogue about where the right 
\[14\]System: Theleft knuckle arm is removed like this. 
(with the anilnation showing the left knuckle 
arm coming off) 
\[ 15\]User: (After moving the viewpoint to Figure I and 
clicking the right knuckle ann) 
What is this? 
\[16\]System: This is the right knuckle arm. 
\[I 7\]Uscr: OK. 
\[18\]Systeln: Now, let's continue the explanation. 
(with changing the view to the one in utterance \[ 14\]) 
\[19\]System: Theleft knuckle arm is removed like this. 
(with the animation showing the left knuckle arm 
coming off) 
\[20\]User: Yes. 
\[21\]System: Now, try removing the left knuckle arm. 
\[22\]User: (After clicking the left knuckle arm) Remove it. 
\[23\]System: Youhave succcssfitlly completed thc operation. 
\[24\]System: Thenext step is to remove the right ie rod cad. 
\[25\]System: Let's move to the right ie rod end. 
1126\] User: Ycs. 
\[27\]System: (Moving the view to the right) 
The right ie rod end is in fiont of you. 
\[28\] User: Yes, 1 see. 
Figure 6: Example of a dialogue with MII)-3D 
boot is. hmnediately before starting DSF\]21, 
the user's viewpoint in l)SF12 is ((-38, -22, 
-259) (0, -0.33, 0)). After completing the 
subdialogue \])y answering the user's question, 
DSF121 is l)opped and the system resmnes 
DSF12. At this time, the \])M gets the view- 
point value of the top DSF (DSF12), alld (;Oltl- 
mands ItyCLASS to change the viewpoint to 
that view, which is in this case ((-as, -22, -2,59) 
(0, -0.a3, 0)) ' The systeln then restarts the 
interrupted ialogue. 
7 Exmnple  
In order to illustrate the behavior of MID-3D, 
an example is shown in Figure 6. This is a part 
of an instruction dialogue on how to dismantle 
the steering system of a car. The current topic 
is removing the left knuckle arm. In utterance 
\[14\], the system describes how to remove this 
part in conjunction with an animation created 
by HyCLASS. 
In \[15\], the user interrupted the system's in- 
struction and asked "What is this?" by clicking 
the right knuckle arm. At this point, the user's 
speech input was interpreted in the Input An- 
~In the current system, it; is not 1)ossible to move 
the camera to an arbitrary point because of the limi- 
tations of the virtual environment controller employed. 
Accordingly, this func|;ion is al)proximated by selecting 
the nearest of several predetined viewpoints. 
577 
alyzer and a user initiative subdialogue started 
by t)ushing another DSF onto the focus stack. 
In order to answer the question, the DM asked 
the Domain Plan Reasoner how to answer the 
user's question. As a result, a discourse goal was 
returned to the DM and added to the agenda. 
The DM then sent the goal (Describe-name S 
H (object  knuckle_arm_r)) to the CP. This 
goal generated utterance \[16\]. 
In system utterance \[18\], in order to resume 
the dialogue, a recta-comment, "Now let's con- 
tinue the explanation", was generated and the 
viewpoint returned to the previous one in \[14\] 
as noted in the DSF. After returning to the pre- 
vious view, the interrupted goal was re-planned. 
As a result, utterance \[19\] was generated. 
After completing this operation in \[23\], 
the next step, removing the right tie rod 
end, is started. At this time, if the 
user is viewing the left side (Figure 2) and 
the system has the goal ( Ins t ruct -ac t  S 
H remove-tierod_end_r MR), (Operator 2} in 
Figure 4 is applied because the target object, 
right tie rod end, is not visible fi'om the user's 
viewpoint. Thus a goal of making the user view 
the right tie rod end is added as a subgoal and 
utterances \[24\] and \[25\] are generated. 
8 Discuss ion  
This paper proposed a inethod tbr altering in- 
struction dialogues to match the user's view in 
a virtual enviromnent. We described the Con- 
tent Planner which can incrementally decide co- 
herent instruction dialogue content to match 
changes in the user's view. We also presented 
the Dialogue Manager, which can keep track 
of the user's viewpoint in an on-going dialogue 
and use this intbrmation i resuming from inter- 
ruptive subdialogues. These mechanisms allow 
to detect mismatches between the user's view- 
point and the topic at any point in the dialogue, 
and then to choose the instruction content and 
user's viewpoint appropriately. MID-3D, an ex- 
perimental system that uses these mechanisms, 
shows that the method we proposed is effective 
in realizing instruction dialogues that suit the 
user's view in virtual enviromnents. 
Re ferences  
Elisabeth Andr6 and Thmnas Rist. 1993. The design of 
il lustrated ocuments as a planning task. In Mark T. 
Maybury, editor, Intelligent Multimedia Interfaces, 
pages 94-116. AAAI Press / The MIT Press. 
Will iam H. Bares and James C. Lester. 1997. Real- 
time generation of customized 3D animated explana- 
tions for knowledge-based learning environments. In 
AAAI97, pages 347-354. 
Alison Cawsey. 1992. Explanation and Interaction: The 
Computer Generation of Expalanatory Dialogues. The. 
MIT Press. 
George Ferguson and James F. Allen. 1998. TRIPS: 
An integrated intelligent problem-solving assistant. 
In AAAI98, pages 567-572. 
Barbara J. Orosz and Candace L. Sidner. 1986. Atten- 
tion, intentions, and the structure of discourse. Com- 
putational Linguistics, 12(3):175-204. 
W. Lewis Johnson, Jeff W. Rickel, and James C. Lester. 
2000. Animated pedagogical agents: Face-to-face in- 
teraction in interactive learning environments. Inter- 
national Journal of Artificial InteUigencc in Educa- 
tion. 
Tsuneaki Kato, Ynkiko I. Nakano, Hideharu Nakajima, 
and Takaaki Hasegawa. 1996. Interactive mnltimodal 
explanations and their temporal coordination. In 
ECAI-96, pages 261-265. John Willey and Sons Lim- 
ited. 
Akihisa Kawanobe, Susumn Kakuta, Hirofumi Touhei, 
and Katsumi Hosoya. 1998. Preliminary report 
on HyCLASS anthoring tool. In ED-MEDIA/ED- 
TELECOM. 
James C. Lester, Jennifer L. Voerlnan, Stuart O. Towns, 
and Charles B. Callaway. 1997. Cosmo: A lih;-like 
animated pedagogical gent witl, deictie believability. 
In IJCAI-97 Workshop, Animated Interface Agent. 
Jmnes C. Lester, Brian A. Stone, and Gray D. Stelling. 
1999. Lifelike pedagogical gents for mixed-initiative 
problem solving in constructivist learning environ- 
ments. User Modeling and User-Adapted Interaction, 
9(1-2):1-44. 
Mark T. Maybury. 1993. Planning multimedia explana- 
tion using communicative acts. In Mark T. Maylmry, 
editor, Intelligent Multimedia Interfaces, pages 59 -74. 
AAAI Press / The MIT Press. 
Johamm D. Moore. 1995. Participating in Explanatory 
Dialogues: Interpreting and I~esponding to Questions 
in Context. MIT Press. 
Chm'les Rich and Candace L. Sidner. 1998. COLLA- 
GEN: A collaboration manager for software interfhce 
agents. User Modeling and User-Adapted Interaction, 
8:315-350. 
Jeff W. Rickel and W. Lewis Johnson. 1999. Animated 
agents for procedual training in virtual reality: Per- 
ception, cognition and motor control. Applied Artifi- 
cial Intellifence, 13:343-392. 
Amanda Stent, John Dowding, Jean Mark Gawron, Eliz- 
abeth Owen Brat, and Robert Moore. 1999. The 
CommandTalk spoken dialogue systeln. In AC'Lgg, 
pages 183-190. 
Brian A. Stone and James C. Lester. 1996. Dynami- 
cally sequencing an animated pedagogical agent. In 
AAAI96, pages 424-431. 
Stuart G. Towns, Charles B. Callaway, and 3anles C. 
Lester. 1998. Generating coordinated natural lan- 
guage and 3D animations for complex spatial expla- 
nations. In AAAI98, pages 112-119. 
David R. Traum. 1994. A Computational Theory of 
Grounding in Natural Language Conversation. Ph.D. 
thesis, University of Rochester. 
Wolfgang \Vahlster, Elisabcth Andr6, Wolfgang Fin- 
kler, Hans-Jiirgen Profitlieh, and Thomas Rist. 1993. 
Plan-based integration of natural anguage and graph- 
ics generation. Artificial Intelligence, 63:387-427. 
578 
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 121?124,
Prague, June 2007. c?2007 Association for Computational Linguistics
Predicting Evidence of Understanding by Monitoring User?s Task 
Manipulation in Multimodal Conversations 
Yukiko I. Nakano? 
Yoshiko Arimoto?? 
?Tokyo University of Agri-
culture and Technology 
2-24-16 Nakacho, Koganei-
shi, Tokyo 184-8588, Japan 
{nakano, kmurata, meno-
moto}@cc.tuat.ac.jp 
Kazuyoshi Murata? 
Yasuhiro Asa??? 
??Tokyo University of 
Technology 
1404-1 Katakura, Hachioji, 
Tokyo 192-0981, Japan 
ar@mf.teu.ac.jp 
Mika Enomoto? 
Hirohiko Sagawa??? 
???Central Research Laboratory, 
Hitachi, Ltd. 
1-280, Higashi-koigakubo Kokub-
unji-shi, Tokyo 185-8601, Japan 
{yasuhiro.asa.mk, hiro-
hiko.sagawa.cu}@hitachi.com 
 
 
 
Abstract 
The aim of this paper is to develop ani-
mated agents that can control multimodal 
instruction dialogues by monitoring user?s 
behaviors. First, this paper reports on our 
Wizard-of-Oz experiments, and then, using 
the collected corpus, proposes a probabilis-
tic model of fine-grained timing dependen-
cies among multimodal communication 
behaviors: speech, gestures, and mouse 
manipulations. A preliminary evaluation 
revealed that our model can predict a in-
structor?s grounding judgment and a lis-
tener?s successful mouse manipulation 
quite accurately, suggesting that the model 
is useful in estimating the user?s under-
standing, and can be applied to determining 
the agent?s next action.  
1 Introduction 
In face-to-face conversation, speakers adjust their 
utterances in progress according to the listener?s 
feedback expressed in multimodal manners, such 
as speech, facial expression, and eye-gaze. In task-
manipulation situations where the listener manipu-
lates objects by following the speaker?s instruc-
tions, correct task manipulation by the listener 
serves as more direct evidence of understanding 
(Brennan 2000, Clark and Krych 2004), and affects 
the speaker?s dialogue control strategies.  
Figure 1 shows an example of a software in-
struction dialogue in a video-mediated situation 
(originally in Japanese). While the learner says 
nothing, the instructor gives the instruction in 
small pieces, simultaneously modifying her ges-
tures and utterances according to the learner?s 
mouse movements. 
To accomplish such interaction between human 
users and animated help agents, and to assist the 
user through natural conversational interaction, this 
paper proposes a probabilistic model that computes 
timing dependencies among different types of be-
haviors in different modalities: speech, gestures, 
and mouse events. The model predicts (a) whether 
the instructor?s current utterance will be success-
fully understood by the learner and grounded 
(Clark and Schaefer 1989), and (b) whether the 
learner will successfully manipulate the object in 
the near future. These predictions can be used as 
constraints in determining agent actions. For ex-
ample, if the current utterance will not be grounded, 
then the help agent must add more information. 
In the following sections, first, we collect hu-
man-agent conversations by employing a Wizard-
of-Oz method, and annotate verbal and nonverbal 
behaviors. The annotated corpus is used to build a 
Bayesian network model for the multimodal in-
struction dialogues. Finally, we will evaluate how 
?That? (204ms pause)
Pointing gesture <preparation>
<stroke>
Mouse move
Instructor:
Learner:
?at the most? (395ms pause)
?left-hand side?
Instructor:
Learner:
Instructor:
Mouse move  
Figure 1: Example of task manipulation dialogue 
121
accurately the model can predict the events in (a) 
and (b) mentioned above. 
2 Related work 
In their psychological study, Clark and Krych 
(2004) showed that speakers alter their utterances 
midcourse while monitoring not only the listener?s 
vocal signals, but also the listener?s gestural sig-
nals as well as through other mutually visible 
events. Such a bilateral process functions as a joint 
activity to ground the presented information, and 
task manipulation as a mutually visible event con-
tributes to the grounding process (Brennan 2000, 
Whittaker 2003). Dillenbourg, Traum, et al (1996) 
also discussed cross-modality in grounding: ver-
bally presented information is grounded by an ac-
tion in the task environment.  
Studies on interface agents have presented com-
putational models of multimodal interaction 
(Cassell, Bickmore, et al 2000). Paek and Horvitz 
(1999) focused on uncertainty in speech-based in-
teraction, and employed a Bayesian network to 
understand the user?s speech input. For user moni-
toring, Nakano, Reinstein, et al (2003) used a head 
tracker to build a conversational agent which can 
monitor the user?s eye-gaze and head nods as non-
verbal signals in grounding. 
These previous studies provide psychological 
evidence about the speaker?s monitoring behaviors 
as well as conversation modeling techniques in 
computational linguistics. However, little has been 
studied about how systems (agents) should monitor 
the user?s task manipulation, which gives direct 
evidence of understanding to estimate the user?s 
understanding, and exploits the predicted evidence 
as constraints in selecting the agent?s next action. 
Based on these previous attempts, this study pro-
poses a multimodal interaction model by focusing 
on task manipulation, and predicts conversation 
states using probabilistic reasoning. 
3 Data collection 
A data collection experiment was conducted using 
a Wizard-of-Oz agent assisting a user in learning a 
PCTV application, a system for watching and re-
cording TV programs on a PC.  
The output of the PC operated by the user was 
displayed on a 23-inch monitor in front of the user, 
and also projected on a 120-inch big screen, in 
front of which a human instructor was standing 
(Figure 2 (a)). Therefore, the participants shared 
visual events output from the PC (Figure 2 (b)) 
while sitting in different rooms. In addition, a rab-
bit-like animated agent was controlled through the 
instructor?s motion data captured by motion sen-
sors. The instructor?s voice was changed through a 
voice transformation system to make it sound like 
a rabbit agent. 
4 Corpus  
We collected 20 conversations from 10 pairs, and 
annotated 11 conversations of 6 pairs using the 
Anvil video annotating tool (Kipp 2004).   
Agent?s verbal behaviors: The agent?s (actually, 
instructor?s) speech data was split by pauses longer 
than 200ms. For each inter pausal unit (IPU), utter-
ance content type defined as follows was assigned.  
? Identification (id): identification of a target 
object for the next operation 
? Operation (op): request to execute a mouse 
click or a similar primitive action on the target 
? Identification + operation (idop): referring to 
identification and operation in one IPU 
In addition to these main categories, we also 
used:  State (referring to a state before/after an op-
eration), Function (explaining a function of the 
system), Goal (referring to a task goal to be ac-
complished), and Acknowledgment. The inter-
coder agreement for this coding scheme is very 
high K=0.89 (Cohen?s Kappa), suggesting that the 
assigned tags are reliable.  
Agent?s nonverbal behaviors: As the most salient 
instructor?s nonverbal behaviors in the collected 
data, we annotated agent pointing gestures: 
? Agent movement: agent?s position  movement 
? Agent touching target (att): agent?s touching 
the target object as a stroke of a pointing ges-
ture  
          (a) Instructor                          (b) PC output 
Figure 2: Wizard-of-Oz agent controlled by instructor 
122
User?s nonverbal behaviors: We annotated three 
types of mouse manipulation for the user?s task 
manipulation as follows:   
? Mouse movement: movement of the mouse 
cursor 
? Mouse-on-target: the mouse cursor is on the 
target object  
? Click target: click on the target object 
4.1 Example of collected data 
 An example of an annotated corpus is shown in 
Figure 3. The upper two tracks illustrate the 
agent?s verbal and nonverbal behaviors, and the 
other two illustrate the user?s behaviors. The agent 
was pointing at the target (att) and giving a se-
quence of identification descriptions [a1-3]. Since 
the user?s mouse did not move at all, the agent 
added another identification IPU [a4] accompanied 
by another pointing gesture. Immediately after that, 
the user?s mouse cursor started moving towards the 
target object. After finishing the next IPU, the 
agent finally requested the user to click the object 
in [a6]. Note that the collected Wizard-of-Oz con-
versations are very similar to the human-human 
instruction dialogues shown in Figure 1. While 
carefully monitoring the user?s mouse actions, the 
Wizard-of-Oz agent provided information in small 
pieces. If it was uncertain that the user was follow-
ing the instruction, the agent added more explana-
tion without continuing. 
5 Probabilistic model of user-agent mul-
timodal interaction 
5.1 Building a Bayesian network model 
To consider multiple factors for verbal and non-
verbal behaviors in probabilistic reasoning, we 
employed a Bayesian network technique, which 
can infer the likelihood of the occurrence of a tar-
get event based on the dependencies among multi-
ple kinds of evidence. We extracted the conversa-
tional data from the beginning of an instructor's 
identification utterance for a new target object to 
the point that the user clicks on the object. Each 
IPU was split at 500ms intervals, and 1395 inter-
vals were obtained. As shown in Figure 4, the net-
work consists of 9 properties concerning verbal 
and nonverbal behaviors for past, current, and fu-
ture interval(s).   
5.2 Predicting evidence of understanding 
As a preliminary evaluation, we tested how ac-
curately our Bayesian network model can predict 
an instructor?s grounding judgment, and the user?s 
mouse click. The following five kinds of evidence 
were given to the network to predict future states. 
As evidence for the previous three intervals (1.5 
sec), we used (1) the percentage of time the agent 
touched the target (att), (2) the number of the 
user?s mouse movements. Evidence for the current 
interval is (3) current IPU?s content type, (4) 
whether the end of the current interval will be the 
end of the IPU (i.e. whether a pause will follow 
after the current interval), and (5) whether the 
mouse is on the target object. 
Well, 
Yes View 
the TV right of 
Yes 
Beside the DVD There is a button 
starts with ?V?
Ah, yes Er, yes 
Press it 
This 
User
Agent
Speech
Gesture
Mouser actions
id id id id id+op
Mouse move
click
att att att
Mouse on 
target
[a2] [a3] [a4] [a5] [a6][a1]
ack ack ack ack
Speech
Off
On
 
Figure 3: Example dialogue between Wizard-of-Oz agent and user 
 
Figure 4: Bayesian network model 
123
(a) Predicting grounding judgment: We tested 
how accurately the model can predict whether the 
instructor will go on to the next leg of the instruc-
tion or will give additional explanations using the 
same utterance content type (the current message 
will not be grounded). 
The results of 5-fold cross-validation are shown 
in Table 1. Since 83% of the data are ?same con-
tent? cases, prediction for ?same content? is very 
accurate (F-measure is 0.90). However, it is not 
very easy to find ?content change? case because of 
its less frequency (F-measure is 0.68). It would be 
better to test the model using more balanced data.  
(b) Predicting user?s mouse click: As a measure 
of the smoothness of task manipulation, the net-
work predicted whether the user?s mouse click 
would be successfully performed within the next 5 
intervals (2.5sec). If a mouse click is predicted, the 
agent should just wait without annoying the user 
by unnecessary explanation. Since randomized 
data is not appropriate to test mouse click predic-
tion, we used 299 sequences of utterances that w-
ere not used for training. Our model predicted 84% 
of the user?s mouse clicks: 80% of them were pre-
dicted 3-5 intervals before the actual occurrence of 
the mouse click, and 20% were predicted 1 interval 
before. However, the model frequently generates 
wrong predictions. Improving precision rate is 
necessary.  
6 Discussion and Future Work 
We employed a Bayesian network technique to our 
goal of developing conversational agents that can 
generate fine-grained multimodal instruction dia-
logues, and we proposed a probabilistic model for 
predicting grounding judgment and user?s success-
ful mouse click. The results of preliminary evalua-
tion suggest that separate models of each modality 
for each conversational participant cannot properly 
describe the complex process of on-going multi-
modal interaction, but modeling the interaction as 
dyadic activities with multiple tracks of modalities 
is a promising approach. 
The advantage of employing the Bayesian net-
work technique is that, by considering the cost of 
misclassification and the benefit of correct classifi-
cation, the model can be easily adjusted according 
to the purpose of the system or the user?s skill level. 
For example, we can make the model more cau-
tious or incautious. Thus, our next step is to im-
plement the proposed model into a conversational 
agent, and evaluate our model not only in its accu-
racy, but also in its effectiveness by testing the 
model with various utility values. 
References 
Brennan, S. 2000. Processes that shape conversation and 
their implications for computational linguistics. In 
Proceedings of 38th Annual Meeting of the ACL. 
Cassell, J., Bickmore, T., Campbell, L., Vilhjalmsson, H. 
and Yan, H. (2000). Human Conversation as a Sys-
tem Framework: Designing Embodied Conversa-
tional Agents. Embodied Conversational Agents. J. 
Cassell, J. Sullivan, S. Prevost and E. Churchill. 
Cambridge, MA, MIT Press: 29-63. 
Clark, H. H. and Schaefer, E. F. 1989. Contributing to 
discourse. Cognitive Science 13: 259-294. 
Clark, H. H. and Krych, M. A. 2004. Speaking while 
monitoring addressees for understanding. Journal of 
Memory and Language 50(1): 62-81. 
Dillenbourg, P., Traum, D. R. and Schneider, D. 1996. 
Grounding in Multi-modal Task Oriented Collabora-
tion. In Proceedings of EuroAI&Education Confer-
ence: 415-425. 
Kipp, M. 2004. Gesture Generation by Imitation - From 
Human Behavior to Computer Character Animation, 
Boca Raton, Florida: Dissertation.com. 
Nakano, Y. I., Reinstein, G., Stocky, T. and Cassell, J. 
2003. Towards a Model of Face-to-Face Grounding. 
In Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics: 553-561. 
Paek, T. and Horvitz, E. (1999). Uncertainty, Utility, 
and Misunderstanding: A Decision-Theoretic Per-
spective on Grounding in Conversational Systems. 
Working Papers of the AAAI Fall Symposium on 
Psychological Models of Communication in Collabo-
rative Systems. S. E. Brennan, A. Giboin and D. 
Traum: 85-92. 
Whittaker, S. (2003). Theories and Methods in Medi-
ated Communication. The Handbook of Discourse 
Processes. A. Graesser, MIT Press. 
 
Table 1: Preliminary evaluation results 
 Precision Recall F-measure
Content  
change  0.53 0.99 0.68 
Same  
content 1.00 0.81 0.90 
124
Converting Text into Agent Animations: Assigning Gestures to Text 
Yukiko I. Nakano?   Masashi Okamoto?    Daisuke Kawahara?   Qing Li?   Toyoaki Nishida?
?Japan Science and Technology Agency 
2-5-1 Atago, Minato-ku, Tokyo, 105-6218 Japan
?The University of Tokyo  
7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656 Japan
{nakano, okamoto, kawahara, liqing, nishida}@kc.t.u-tokyo.ac.jp 
Abstract 
This paper proposes a method for assigning 
gestures to text based on lexical and syntactic 
information. First, our empirical study identi-
fied lexical and syntactic information strongly 
correlated with gesture occurrence and sug-
gested that syntactic structure is more useful 
for judging gesture occurrence than local syn-
tactic cues. Based on the empirical results, we 
have implemented a system that converts text 
into an animated agent that gestures and 
speaks synchronously. 
1 Introduction  
The significant advances in computer graphics over the 
last decade have improved the expressiveness of ani-
mated characters and have promoted research on inter-
face agents, which serve as mediators of human-
computer interactions. As an interface agent has an em-
bodied figure, it can use its face and body to display 
nonverbal behaviors while speaking.  
Previous studies in human communication suggest 
that gestures in particular contribute to better under-
standing of speech. About 90% of all gestures by 
speakers occur when the speaker is actually uttering 
something (McNeill, 1992). Experimental studies have 
shown that spoken sentences are heard twice as accu-
rately when they are presented along with a gesture 
(Berger & Popelka, 1971). Comprehension of a descrip-
tion accompanied by gestures is better than that accom-
panied by only the speaker?s face and lip movements 
(Rogers, 1978). These previous studies suggest that 
generating appropriate gestures synchronized with 
speech is a promising approach to improving the per-
formance of interface agents. In previous studies of 
multimodal generation, gestures were determined ac-
cording to the instruction content (Andre, Rist, & Mul-
ler, 1999; Rickel & Johnson, 1999), the task situation in 
a learning environment (Lester, Stone, & Stelling, 
1999), or the agent?s communicative goal in conversa-
tion (Cassell et al, 1994; Cassell, Stone, & Yan, 2000). 
These approaches, however, require the contents devel-
oper (e.g., a school teacher designing teaching materi-
als) to be skilled at describing semantic and pragmatic 
relations in logical form. A different approach, (Cassell, 
Vilhjalmsson, & Bickmore, 2001) proposes a toolkit 
that takes plain text as input and automatically suggests 
a sequence of agent behaviors synchronized with the 
synthesized speech. However, there has been little work 
in computational linguistics on how to identify and ex-
tract linguistic information in text in order to generate 
gestures.  
Our study has addressed these issues by considering 
two questions. (1) Is the lexical and syntactic informa-
tion in text useful for generating meaningful gestures? 
(2) If so, how can the information be extracted from the 
text and exploited in a gesture decision mechanism in 
an interface agent? Our goal is to develop a media con-
version technique that generates agent animations syn-
chronized with speech from plain text.  
This paper is organized as follows. The next section 
reviews theoretical issues about the relationships be-
tween gestures and syntactic information. The empirical 
study we conducted based on these issues is described 
in Sec. 3. In Sec. 4 we describe the implementation of 
our presentation agent system, and in the last section we 
discuss future directions.  
2 Linguistic Theories and Gesture Studies 
In this section we review linguistic theories and discuss 
the relationship between gesture occurrence and syntac-
tic information.  
Linguistic quantity for reference: McNeill (McNeill, 
1992) used communicative dynamism (CD), which 
represents the extent to which the message at a given 
point is ?pushing the communication forward? (Firbas, 
1971), as a variable that correlates with gesture occur-
rence. The greater the CD, the more probable the occur-
rence of a gesture. As a measure of CD, McNeill chose 
the amount of linguistic material used to make the refer-
ence (Givon, 1985). Pronouns have less CD than full 
nominal phrases (NPs), which have less CD than modi-
fied full NPs. This implies that the CD can be estimated 
by looking at the syntactic structure of a sentence.  
Theme/Rheme: McNeill also asserted that the theme 
(Halliday, 1967) of a sentence usually has the least CD 
and is not normally accompanied by a gesture. Gestures 
usually accompany the rhemes, which are the elements 
of a sentence that plausibly contribute information 
about the theme, and thus have greater CD. In Japanese 
grammar there is a device for marking the theme explic-
itly. Topic marking postpositions (or ?topic markers?), 
typically ?wa,? mark a nominal phrase as the theme. 
This facilitates the use of syntactic analysis to identify 
the theme of a sentence. Another interesting aspect of 
information structure is that in English grammar, a wh-
interrogative (what, how, etc.) at the beginning of a 
sentence marks the theme and indicates that the content 
of the theme is the focus (Halliday, 1967). However, we 
do not know whether such a special type of theme is 
more likely to co-occur with a gesture or not.  
Given/New: Given and new information demonstrate 
an aspect of theme and rheme. Given information usu-
ally has a low degree of rhematicity, while new infor-
mation has a high degree. This implies that rhematicity 
can be estimated by determining whether the NP is the 
first mention (i.e., new information) or has already been 
mentioned (i.e., old or given information).  
Contrastive relationship: Prevost (1996) reported that 
intonational accent is often used to mark an explicit 
contrast among the salient discourse entities. On the 
basis of this finding and Kendon?s theory about the rela-
tionship between intonation phrases and gesture place-
ments (Kendon, 1972), Cassell & Prevost (1996) 
developed a method for generating contrastive gestures 
from a semantic representation. In syntactic analysis, a 
contrastive relation is usually expressed as a coordina-
tion, which is a syntactic structure including at least two 
conjuncts linked by a conjunction.  
Figure 1 shows an example of the correlation between 
gesture occurrence and the dependency structure of a 
Japanese sentence. Bunsetsu units (8)-(9) and (10)-(13) 
in the figure are conjuncts. A ?bunsetsu unit? in Japa-
nese corresponds to a phrase in English, such as a noun 
phrase or a prepositional phrase. Each conjunct is ac-
companied by a gesture. Bunsetsu (14) is a complement 
containing a verbal phrase; it depends on bunsetsu (15), 
which is an NP. Thus, bunsetsu (15) is a modified full 
NP and thus has large linguistic quantity.  
3 Empirical Study 
To identify linguistic features that might 
be useful for judging gesture occurrence, 
we videotaped seven presentation talks 
and transcribed three minutes for each of 
them. The collected data included 2124 
bunsetsu units and 343 gestures. 
Gesture Annotation: Three coders dis-
cussed how to code the half the data and reached a con-
sensus on gesture occurrence. After this consensus on 
the coding scheme was established1, one of the coders 
annotated the rest of the data. A gesture consists of 
preparation, stroke, and retraction (McNeill, 1992), and 
a stroke co-occurs with the most prominent syllable 
(Kendon, 1972). Thus, we annotated the stroke time as 
well as the start and end time of each gesture.  
Linguistic Analysis: Each bunsetsu unit was automati-
cally annotated with linguistic information using a Japa-
nese syntactic analyzer (Kurohashi & Nagao, 1994)2. 
The information was determined by asked the following 
questions for each bunsetsu unit. 
(a) If it is an NP, is it modified by a clause or a com-
plement? 
(b) If it is an NP, what type of postpositional particle 
marks its end (e.g., ?wa?, ?ga?, ?wo?)? 
(c) Is it a wh-interrogative? 
(d) Are all the content words in the bunsetsu unit have 
mentioned in a preceding sentence? 
(e) Is it a constituent of a coordination? 
Moreover, as we noticed that some lexical entities fre-
quently co-occurred with a gesture in our data, we used 
the syntactic analyzer to annotate additional lexical in-
formation based on the following questions.  
(f) Is the bunsetsu unit an emphatic adverbial phrase 
(e.g., very, extremely), or is it modified by a pre-
ceding emphatic adverb (e.g., very important is-
sue)? 
(g) Does it include a cue word (e.g., now, therefore)? 
(h) Does it include a numeral (e.g., thousands of people, 
99 times)? 
We then investigated the correlation between these 
lexical and syntactic features and the occurrence of ges-
ture strokes.  
Result: The results are summarized in Table 1. The 
baseline gesture occurrence frequency was 10.1% per 
bunsetsu unit (a gesture occurred once about every ten 
                                                          
1 Inter-coder reliability among the three coders in catego-
rizing the gestures (beat, iconic, etc.) was sufficiently high 
(Kappa = 0.81). Although we did not measure agreement on 
gesture occurrence itself, this result suggests that the coders 
had very similar schemes for recognizing gestures.  
2 To prevent the effects of parsing errors, errors in syntac-
tic dependency analysis were corrected manually for about 
13% of the data.  
shindo-[ga] atae-rareru-to-ka sore-[ni] kawaru kasokudo-[ga] atae-rareru-to iu-youna jyoukyou-de
<parallel>
<nominal>
<complement>
<verbal>
(8) (9) (10) (11) (12) (13) (14) (15)
?a situation where seismic intensity is given, or degree of acceleration is given?
Figure 1: Example analysis of syntactic dependency  
Underlined phrases are accompanied by gestures, and strokes occur at dou-
ble-underlined parts. Case markers are enclosed by square brackets [ ]. 
Table 1. Summary of results 
Case Syntactic/lexical information of a bunsetsu unit 
Gesture 
occurrence
C1 (a) NP modified by clause 0.382 
C2 
Quantity of 
modification Pronouns, other 
types of NPs 
(b) Case marker = ?wo? 
& (d) New information 
0.281 
C3 (c) WH-interrogative 0.414 
C4 (e) Coordination 0.477 
C5 (f) Emphatic adverb itself 0.244 
C6 
Emphatic 
adverbial phrase (f?) Following emphatic adverb 0.350 
C7 (g) Cue word 0.415 
C8 (h) Numeral 0.393 
C9 Other (baseline) 0.101 
 
bunsetsu units). A gesture stroke most frequently co-
occurred with a bunsetsu unit forming a coordination 
(47.7%). When an NP was modified by a full clause, it 
was accompanied by a gesture 38.2% of the time. For 
the other types of noun phrases, including pronouns, 
when an accusative case marked with case marker ?wo? 
was new information (i.e., it was not mentioned in a 
previous sentence), a gesture co-occurred with the 
phrase 28.1% of the time. Moreover, gesture strokes 
frequently co-occurred with wh-interrogatives (41.4%), 
cue words (41.5%), and numeral words (39.3%). Ges-
ture strokes frequently occurred right after emphatic 
adverbs (35%) rather than with the adverb (24.4%).  
These cases listed in Table 1 had a 3 to 5 times higher 
probability of gesture occurrence than the baseline and 
accounted for 75% of all the gestures observed in the 
data. Our results suggest that these types of lexical and 
syntactic information can be used to distinguish be-
tween where a gesture should be assigned and where 
one should not be assigned. They also indicate that the 
syntactic structure of a sentence more strongly affects 
gesture occurrence than theme or rheme and than given 
or new information specified by local grammatical cues, 
such as topic markers and case markers.  
4 System Implementation 
4.1 Overview 
We used our results to build a presentation agent system, 
SPOC (Stream-oriented Public Opinion Channel).? This 
system enables a user to embody a story (written text) 
as a multimodal presentation featuring video, graphics, 
speech, and character animation. A snapshot of the 
SPOC viewer is shown in Figure 2.  
In order to implement a storyteller in SPOC, we de-
veloped an agent behavior generation system we call 
?CAST (Conversational Agent System for neTwork 
applications).? Taking text input, CAST automatically 
selects agent gestures and other nonverbal behaviors, 
calculates an animation schedule, and produces synthe-
sized voice output for the agent. As shown in Figure 2, 
CAST consists of four main components: (1) the Agent 
Behavior Selection Module (ABS), (2) the Language 
Tagging Module (LTM), (3) the agent animation system, 
and (4) a text-to-speech engine (TTS). The received text 
input is first sent to the ABS. The ABS selects appro-
priate gestures and facial expressions based on the lin-
guistic information calculated by the LTM. It then 
obtains the timing information from the TTS and calcu-
lates a time schedule for the set of agent actions. The 
output from the ABS is a set of animation instructions 
that can be interpreted and executed by the agent anima-
tion system. 
4.2 Determining Agent Behaviors 
Tagging linguistic information: First, the LTM parses 
the input text and calculates the linguistic information 
described in Sec. 3. For example, bunsetsu (9) in Figure 
1 has the following feature set. 
{Text-ID: 1, Sentence-ID: 1, Bunsetsu-ID: 9, Govern: 8, De-
pend-on: 13, Phrase-type: VP, Linguistic-quantity: NA, Case-
marker: NA, WH-interrogative: false, Given/New: new, Coor-
dinate-with: 13, Emphatic-Adv: false, Cue-Word: false, Nu-
meral: false} 
The text ID of this bunsetsu unit is 1, the sentence ID 
is 1, the bunsetsu ID is 9. This bunsetsu governs bun-
setsu 8 and depends on bunsetsu 13. It conveys new 
information and, together with bunsetsu 13, forms a 
parallel phrase.  
Assigning gestures: Then, for each bunsetsu unit, the 
ABS decides whether to assign a gesture or not based 
on the empirical results shown in Table 1. For example, 
bunsetsu unit (9) shown above matches case C4 in Ta-
ble 1, where a bunsetsu unit is a constituent of coordina-
tion. In this case, the system assigns a gesture to the 
bunsetsu with 47.7 % probability. In the current imple-
mentation, if a specific gesture for an emphasized con-
cept is defined in the gesture animation library (e.g., a 
gesture animation expressing ?big?), it is preferred to a 
?beat gesture? (a simple flick of the hand or fingers up 
and down (McNeill, 1992)). If a specific gesture is not 
defined, a beat gesture is used as the default. 
Animation ID
Start/end time
Agent Behavior 
Selection Module 
(ABS) Language Tagging 
Module (LTM)
Text-to-Speech 
(TTS)
S-POC Viewer
Input 
text
CAST
Video
Graphics
Graphics + Camera work
Agent Animation 
System
This is 
our ?
This is 
our 
Figure 2: Overview of CAST and SPOC 
The output of the ABS is stored in XML format. The 
type of action and the start and end times of the action 
are indicated by XML tags. In the example shown in 
Figure 3, the agent first gazes towards the user. It then 
performs contrast gestures at the second and sixth bun-
setsu units and a beat gesture at the eighth bunsetsu unit.  
Finally, the ABS transforms the XML into a time 
schedule by accessing the TTS engine and estimating 
the phoneme and bunsetsu boundary timings. The 
scheduling technique is similar to that described by 
(Cassell et al, 2001). The ABS also assigns visemes for 
the lip-sync and the facial expressions, such as head 
movement, eye gaze, blink, and eyebrow movement.  
5 Discussion and Conclusion 
We have addressed the issues related to assigning ges-
tures to text and converting the text into agent anima-
tions synchronized with speech. First, our empirical 
study identified useful lexical and syntactic information 
for assigning gestures to plain text. Specifically, when a 
bunsetsu unit is a constituent of coordination, gestures 
occur almost half the time. Gestures also frequently co-
occur with nominal phrases modified by a clause. These 
findings suggest that syntactic structure is a stronger 
determinant of gesture occurrence than theme or rheme 
and given or new information specified by local gram-
matical cues.  
We plan to enhance our model by incorporating more 
general discourse level information, though the current 
system exploits cue words as a very partial kind of dis-
course information. For instance, gestures frequently 
occur at episode boundaries. Pushing and popping of a 
discourse segment (Grosz & Sidner, 1986) may also 
affect gesture occurrence. Therefore, by integrating a 
discourse analyzer into the LTM, more general struc-
tural discourse information can be used in the model. 
Another important direction is to evaluate the effective-
ness of agent gestures in actual human-agent interaction. 
We expect that if our model can generate gestures with 
appropriate timing for emphasizing important words 
and phrases, users can perceive agent presentations as 
being more alive and comprehensible. We plan to con-
duct a user study to examine this hypothesis.  
References 
Andre, E., Rist, T., & Muller, J. (1999). Employing AI meth-
ods to control the behavior of animated interface agents. Ap-
plied Artificial Intelligence, 13, 415-448. 
Berger, K. W., & Popelka, G. R. (1971). Extra-facial Gestures 
in Relation to Speech-reading. Journal of Communication 
Disorders, 3, 302-308. 
Cassell, J. et al (1994). Animated Conversation: Rule-Based 
Generation of Facial Expression, Gesture and Spoken Intona-
tion for Multiple Conversational Agents. Paper presented at 
the SIGGRAPH '94. 
Cassell, J., & Prevost, S. (1996). Distribution of Semantic 
Features Across Speech and Gesture by Humans and Com-
puters. Paper presented at the Workshop on the Integration of 
Gesture in Language and Speech. 
Cassell, J., Stone, M., & Yan, H. (2000). Coordination and 
Context-Dependence in the Generation of Embodied Conver-
sation. Paper presented at the INLG 2000. 
Cassell, J., Vilhjalmsson, H., & Bickmore, T. (2001). BEAT: 
The Behavior Expression Animation Toolkit. Paper presented 
at the SIGGRAPH 01. 
Firbas, J. (1971). On the Concept of Communicative Dyna-
mism in the Theory of Functional Sentence Perspective. Phi-
lologica Pragensia, 8, 135-144. 
Givon, T. (1985). Iconicity, Isomorphism and Non-arbitrary 
Coding in Syntax. In J. Haiman (Ed.), Iconicity in Syntax (pp. 
187-219): John Benjamins. 
Grosz, B., & Sidner, C. (1986). Attention, Intentions, and the 
Structure of Discourse. Computational Linguistics, 12(3), 175-
204. 
Halliday, M. A. K. (1967). Intonation and Grammar in British 
English. The Hague: Mouton. 
Kendon, A. (1972). Some Relationships between Body Mo-
tion and Speech. In A. W. Siegman & B. Pope (Eds.), Studies 
in Dyadic Communication (pp. 177-210). Elmsford, NY: Per-
gamon Press. 
Kurohashi, S., & Nagao, M. (1994). A Syntactic Analysis 
Method of Long Japanese Sentences Based on the Detection 
of Conjunctive Structures. Computational Linguistics, 20(4), 
507-534. 
Lester, J. C., Stone, B., & Stelling, G. (1999). Lifelike Peda-
gogical agents for Mixed-Initiative Problem Solving in Con-
structivist Learning Environments. User Modeling and User-
Adapted Interaction, 9(1-2), 1-44. 
McNeill, D. (1992). Hand and Mind: What Gestures Reveal 
about Thought. Chicago, IL/London, UK: The University of 
Chicago Press. 
Prevost, S. A. (1996). An Informational Structural Approach 
to Spoken Language Generation. Paper presented at the 34th 
Annual Meeting of the Association for Computational Lin-
guistics, Santa Cruz, CA. 
Rickel, J., & Johnson, W. L. (1999). Animated Agents for 
Procedural Training in Virtual Reality: Perception, Cognition 
and Motor Control. Applied Artificial Intelligence, 13(4-5), 
343-382. 
Rogers, W. (1978). The Contribution of Kinesic Illustrators 
towards the Comprehension of Verbal Behavior within Utter-
ances. Human Communication Research, 5, 54-62. 
 
<Gaze type="towards">
shindo-ga
<Gesture_right type="contrast" handshape_right="stroke1@2">
atae-rareru-to-ka
</Gesture_right> 
sore-ni
kawaru
kasokudo-ga
<Gesture_right type="contrast" handshape_right="stroke2@2">
atae-rareru-to
</Gesture_right> 
iu-youna
<Gesture_right type="best" handshape_right="stroke1">
jyoukyou-de
</Gesture_right>
?  
Figure 3: Example of CAST output
Towards a Model of Face-to-Face Grounding 
Yukiko I. Nakano?/??   Gabe Reinstein?   Tom Stocky?   Justine Cassell? 
?MIT Media Laboratory 
E15-315 
20 Ames Street 
Cambridge, MA 02139 USA 
{yukiko, gabe, tstocky, justine}@media.mit.edu 
 
??Research Institute of Science and 
Technology for Society (RISTEX) 
2-5-1 Atago Minato-ku, 
Tokyo 105-6218, Japan 
nakano@kc.t.u-tokyo.ac.jp
Abstract 
We investigate the verbal and nonverbal 
means for grounding, and propose a design 
for embodied conversational agents that re-
lies on both kinds of signals to establish 
common ground in human-computer inter-
action. We analyzed eye gaze, head nods 
and attentional focus in the context of a di-
rection-giving task. The distribution of 
nonverbal behaviors differed depending on 
the type of dialogue move being grounded, 
and the overall pattern reflected a monitor-
ing of lack of negative feedback. Based on 
these results, we present an ECA that uses 
verbal and nonverbal grounding acts to up-
date dialogue state. 
1 Introduction 
An essential part of conversation is to ensure that 
the other participants share an understanding of 
what has been said, and what is meant.  The proc-
ess of ensuring that understanding ? adding what 
has been said to the common ground ? is called 
grounding [1]. In face-to-face interaction, nonver-
bal signals as well as verbal participate in the 
grounding process, to indicate that an utterance is 
grounded, or that further work is needed to ground. 
Figure 1 shows an example of human face-to-face 
conversation. Even though no verbal feedback is 
provided, the speaker (S) continues to add to the 
directions. Intriguingly, the listener gives no ex-
plicit nonverbal feedback ? no nods or gaze to-
wards S. S, however, is clearly monitoring the 
listener?s behavior, as we see by the fact that S 
looks at her twice (continuous lines above the 
words). In fact, our analyses show that maintaining 
focus of attention on the task (dash-dot lines un-
derneath the words) is the listener?s public signal 
of understanding S?s utterance sufficiently for the 
task at hand.  Because S is manifestly attending to 
this signal, the signal allows the two jointly to rec-
ognize S?s contribution as grounded. This paper 
provides empirical support for an essential role for 
nonverbal behaviors in grounding, motivating an 
architecture for an embodied conversational agent 
that can establish common ground using eye gaze, 
head nods, and attentional focus.   
Although grounding has received significant at-
tention in the literature, previous work has not ad-
dressed the following questions: (1) what 
predictive factors account for how people use non-
verbal signals to ground information, (2) how can a 
model of the face-to-face grounding process be 
used to adapt dialogue management to face-to-face 
conversation with an embodied conversational 
agent. This paper addresses these issues, with the 
goal of contributing to the literature on discourse 
phenomena, and of building more advanced con-
versational humanoids that can engage in human 
conversational protocols.  
In the next section, we discuss relevant previous 
work, report results from our own empirical study 
and, based on our analysis of conversational data, 
propose a model of grounding using both verbal 
and nonverbal information, and present our im-
plementation of that model into an embodied con-
versational agent. As a preliminary evaluation, we 
compare a user interacting with the embodied con-
versational agent with and without grounding.  
Figure 1: Human face-to-face conversation 
[580] S: Go    to    the    fourth    floor,
[590] S: hang    a    left,
[600] S: hang    another    left. 
look at map gaze at listener
gaze at listener
look at map
look at map
look at map
look at map
speaker?s behavior
listener?s behavior
2 Related Work 
Conversation can be seen as a collaborative activ-
ity to accomplish information-sharing and to pur-
sue joint goals and tasks.  Under this view, 
agreeing on what has been said, and what is meant, 
is crucial to conversation.  The part of what has 
been said that the interlocutors understand to be 
mutually shared is called the common ground, and 
the process of establishing parts of the conversa-
tion as shared is called grounding [1]. As [2] point 
out, participants in a conversation attempt to 
minimize the effort expended in grounding.  Thus, 
interlocutors do not always convey all the informa-
tion at their disposal; sometimes it takes less effort 
to produce an incomplete utterance that can be re-
paired if needs be. 
[3] has proposed a computational approach to 
grounding where the status of contributions as 
provisional or shared is part of the dialogue 
system?s representation of the ?information state? 
of the conversation. Conversational actions can 
trigger updates that register provisional 
information as shared. These actions achieve 
grounding.  Acknowledgment acts are directly as-
sociated with grounding updates while other utter-
ances effect grounding updates indirectly, because 
they proceed with the task in a way that presup-
poses that prior utterances are uncontroversial. 
[4], on the other hand, suggest that actions in 
conversation give probabilistic evidence of under-
standing, which is represented on a par with other 
uncertainties in the dialogue system (e.g., speech 
recognizer unreliability).  The dialogue manager 
assumes that content is grounded as long as it 
judges the risk of misunderstanding as acceptable. 
[1, 5] mention that eye gaze is the most basic 
form of positive evidence that the addressee is at-
tending to the speaker, and that head nods have a 
similar function to verbal acknowledgements. They 
suggest that nonverbal behaviors mainly contribute 
to lower levels of grounding, to signify that inter-
locutors have access to each other?s communica-
tive actions, and are attending.  With a similar goal 
of broadening the notion of communicative action 
beyond the spoken word, [6] examine other kinds 
of multimodal grounding behaviors, such as post-
ing information on a whiteboard.  Although these 
and other researchers have suggested that nonver-
bal behaviors undoubtedly play a role in grounding, 
previous literature does not characterize their pre-
cise role with respect to dialogue state.  
On the other hand, a number of studies on these 
particular nonverbal behaviors do exist. An early 
study, [7], reported that conversation involves eye 
gaze about 60% of the time. Speakers look up at 
grammatical pauses for feedback on how utter-
ances are being received, and also look at the task. 
Listeners look at speakers to follow their direction 
of gaze. In fact, [8] claimed speakers will pause 
and restart until they obtain the listener?s gaze.  [9] 
found that during conversational difficulties, mu-
tual gaze was held longer at turn boundaries. 
Previous work on embodied conversational 
agents (ECAs) has demonstrated that it is possible 
to implement face-to-face conversational protocols 
in human-computer interaction, and that correct 
relationships among verbal and nonverbal signals 
enhances the naturalness and effectiveness of em-
bodied dialogue systems [10], [11]. [12] reported 
that users felt the agent to be more helpful, lifelike, 
and smooth in its interaction style when it demon-
strated nonverbal conversational behaviors.  
3 Empirical Study 
In order to get an empirical basis for modeling 
face-to-face grounding, and implementing an ECA, 
we analyzed conversational data in two conditions. 
3.1 Experiment Design 
Based on previous direction-giving tasks, students 
from two different universities gave directions to 
campus locations to one another.  Each pair had a 
conversation in a (1) Face-to-face condition 
(F2F): where two subjects sat with a map drawn 
by the direction-giver sitting between them, and in 
a (2) Shared Reference condition (SR): where an 
L-shaped screen between the subjects let them 
share a map drawn by the direction-giver, but not 
to see the other?s face or body. 
Interactions between the subjects were video-
recorded from four different angles, and combined 
by a video mixer into synchronized video clips.  
3.2 Data Coding 
10 experiment sessions resulted in 10 dialogues per 
condition (20 in total), transcribed as follows. 
Coding verbal behaviors: As grounding oc-
curs within a turn, which consists of consecutive 
utterances by a speaker, following [13] we token-
ized a turn into utterance units (UU), correspond-
ing to a single intonational phrase [14]. Each UU 
was categorized using the DAMSL coding scheme 
[15]. In the statistical analysis, we concentrated on 
the following four categories with regular occur-
rence in our data: Acknowledgement, Answer, In-
formation request (Info-req), and Assertion.  
Coding nonverbal behaviors: Based on previ-
ous studies, four types of behaviors were coded: 
Gaze At Partner (gP): Looking at the partner?s 
eyes, eye region, or face. 
Gaze At Map (gM): Looking at the map 
Gaze Elsewhere (gE): Looking away elsewhere 
Head nod (Nod): Head moves up and down in a 
single continuous movement on a vertical axis, 
but eyes do not go above the horizontal axis. 
By combining Gaze and Nod, six complex catego-
ries (ex. gP with nod, gP without nod, etc) are gen-
erated.  In what follows, however, we analyze only 
categories with more than 10 instances. In order to 
analyze dyadic behavior, 16 combinations of the 
nonverbal behaviors are defined, as shown in Table 
1. Thus, gP/gM stands for a combination of 
speaker gaze at partner and listener gaze at map. 
Results 
We examine differences between the F2F and SR 
conditions, correlate verbal and nonverbal behav-
iors within those conditions, and finally look at 
correlations between speaker and listener behavior. 
Basic Statistics: The analyzed corpus consists 
of 1088 UUs for F2F, and 1145 UUs for SR. The 
mean length of conversations in F2F is 3.24 min-
utes, and in SR is 3.78 minutes  (t(7)=-1.667 p<.07 
(one-tail)). The mean length of utterances in F2F 
(5.26 words per UU) is significantly longer than in 
SR (4.43 words per UU) (t(7)=3.389 p< .01 (one-
tail)). For the nonverbal behaviors, the number of 
shifts between the statuses in Table 1 was com-
pared (eg. NV status shifts from gP/gP to gM/gM 
is counted as one shift). There were 887 NV status 
shifts for F2F, and 425 shifts for SR. The number 
of NV status shifts in SR is less than half of that in 
F2F (t(7)=3.377 p< .01 (one-tail)).  
These results indicate that visual access to the 
interlocutor?s body affects the conversation, sug-
gesting that these nonverbal behaviors are used as 
communicative signals. In SR, where the mean 
length of UU is shorter, speakers present informa-
tion in smaller chunks than in F2F, leading to more 
chunks and a slightly longer conversation. In F2F, 
on the other hand, conversational participants con-
vey more information in each UU. 
Correlation between verbal and nonverbal 
behaviors: We analyzed NV status shifts with re-
spect to the type of verbal communicative action 
and the experimental condition (F2F/SR). To look 
at the continuity of NV status, we also analyzed the 
amount of time spent in each NV status. For gaze, 
transition and time spent gave similar results; since 
head nods are so brief, however, we discuss the 
data in terms of transitions. Table 2 shows the most 
frequent target NV status (shift to these statuses from 
others) for each speech act type in F2F. Numbers in 
parentheses indicates the proportion to the total num-
ber of transitions.  
<Acknowledgement> Within an UU, the 
dyad?s NV status most frequently shifts to 
gMwN/gM (eg. speaker utters ?OK? while nodding, 
and listener looks at the map). At pauses, a shift to 
gMgM is most frequent. The same results were 
found in SR where the listener could not see the 
speaker?s nod. These findings suggest that Ac-
knowledgement is likely to be accompanied by a 
head nod, and this behavior may function intro-
spectively, as well as communicatively. 
<Answer> In F2F, the most frequent shift 
within a UU is to gP/gP. This suggests that speak-
ers and listeners rely on mutual gaze (gP/gP) to 
ensure an answer is grounded, whereas they cannot 
use this strategy in SR. In addition, we found that 
Table 1: NV statuses 
Listener?s behavior Combinations of 
NVs gP gM gMwN gE 
gP gP/gP gP/gM gP/gMwN gP/gE 
gM gM/gP gM/gM gM/gMwN gM/gE 
gMwN gMwN/gP gMwN/gM gMwN/gMwN gMwN/gE
 
Speaker?s 
behavior 
gE gE/gP gE/gM gE/gMwN gE/gE 
 
Shift to  
within UU pause 
Acknowledgement gMwN/gM (0.495) gM/gM (0.888) 
Answer gP/gP (0.436) gM/gM (0.667) 
Info-req gP/gM (0.38) gP/gP (0.5) 
Assertion gP/gM (0.317) gM/gM (0.418) 
 Table 2: Salient transitions 
speakers frequently look away at the beginning of 
an answer, as they plan their reply [7]. 
<Info-req> In F2F, the most frequent shift 
within a UU is to gP/gM, while at pauses between 
UUs shift to gP/gP is the most frequent. This sug-
gests that speakers obtain mutual gaze after asking 
a question to ensure that the question is clear, be-
fore the turn is transferred to the listener to reply. 
In SR, however, rarely is there any NV status shift, 
and participants continue looking at the map. 
<Assertion> In both conditions, listeners look 
at the map most of the time, and sometimes nod. 
However, speakers? nonverbal behavior is very 
different across conditions. In SR, speakers either 
look at the map or elsewhere. By contrast, in F2F, 
they frequently look at the listener, so that a shift 
to gP/gM is the most frequent within an UU. This 
suggests that, in F2F, speakers check whether the 
listener is paying attention to the referent men-
tioned in the Assertion. This implies that not only 
listener?s gazing at the speaker, but also paying 
attention to a referent works as positive evidence 
of understanding in F2F. 
In summary, it is already known that eye gaze 
can signal a turn-taking request [16], but turn-
taking cannot account for all our results. Gaze di-
rection changes within as well as between UUs, 
and the usage of these nonverbal behaviors differs 
depending on the type of conversational action. 
Note that subjects rarely demonstrated communica-
tion failures, implying that these nonverbal behaviors 
represent positive evidence of grounding. 
Correlation between speaker and listener 
behavior: Thus far we have demonstrated a differ-
ence in distribution among nonverbal behaviors, 
with respect to conversational action, and visibility 
of interlocutor.  But, to uncover the function of 
these nonverbal signals, we must examine how 
listener?s nonverbal behavior affects the speaker?s 
following action. Thus, we looked at two consecu-
tive Assertion UUs by a direction-giver, and ana-
lyzed the relationship between the NV status of the 
first UU and the direction-giving strategy in the 
second UU. The giver?s second UU is classified as 
go-ahead if it gives the next leg of the directions, 
or as elaboration if it gives additional information 
about the first UU, as in the following example: 
[U1]S: And then, you?ll go  
 down this little corridor. 
[U2]S: It?s not very long. 
Results are shown in Figure 2. When the listener 
begins to gaze at the speaker somewhere within an 
UU, and maintains gaze until the pause after the 
UU, the speaker?s next UU is an elaboration of the 
previous UU 73% of the time. On the other hand, 
when the listener keeps looking at the map during 
an UU, only 30% of the next UU is an elaboration 
(z = 3.678, p<.01). Moreover, when a listener 
keeps looking at the speaker, the speaker?s next 
UU is go-ahead only 27% of the time. In contrast, 
when a listener keeps looking at the map, the 
speaker?s next UU is go-ahead 52% of the time (z 
= -2.049, p<.05)1. These results suggest that speak-
ers interpret listeners? continuous gaze as evidence 
of not-understanding, and they therefore add more 
information about the previous UU. Similar find-
ings were reported for a map task by [17] who 
suggested that, at times of communicative diffi-
culty, interlocutors are more likely to utilize all the 
channels available to them. In terms of floor man-
agement, gazing at the partner is a signal of giving 
up a turn, and here this indicates that listeners are 
trying to elicit more information from the speaker.  
In addition, listeners? continuous attention to the 
map is interpreted as evidence of understanding, 
and speakers go ahead to the next leg of the direc-
tion2. 
3.3 A Model of Face-to-Face Grounding 
Analyzing spoken dialogues, [18] reported that 
grounding behavior is more likely to occur at an 
                                                          
1 The percentage for map does not sum to 100% because some 
of the UUs are cue phrases or tag questions which are part of 
the next leg of the direction, but do not convey content. 
2 We also analyzed two consecutive Answer UUs from a giver, 
and found that when the listener looks at the speaker at a 
pause, the speaker elaborates the Answer 78% of the time. 
When the listener looks at the speaker during the UU and at 
the map after the UU (positive evidence), the speaker  elabo-
rates only 17% of the time. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
gaze map
elaboration
go-ahead
Figure 2: Relationship between receiver?s NV and 
giver?s next verbal behavior 
intonational boundary, which we use to identify 
UUs. This implies that multiple grounding behav-
iors can occur within a turn if it consists of multi-
ple UUs. However, in previous models, 
information is grounded only when a listener re-
turns verbal feedback, and acknowledgement 
marks the smallest scope of grounding.  If we ap-
ply this model to the example in Figure 1, none of 
the UU have been grounded because the listener 
has not returned any spoken grounding clues. 
In contrast, our results suggest that considering 
the role of nonverbal behavior, especially eye-gaze, 
allows a more fine-grained model of grounding, 
employing the UU as a unit of grounding.  
Our results also suggest that speakers are ac-
tively monitoring positive evidence of understand-
ing, and also the absence of negative evidence of 
understanding (that is, signs of miscommunication).  
When listeners continue to gaze at the task, speak-
ers continue on to the next leg of directions.   
Because of the incremental nature of grounding, 
we implement nonverbal grounding functionality 
into an embodied conversational agent using a 
process model that describes steps for a system to 
judge whether a user understands system contribu-
tion: (1) Preparing for the next UU: according to 
the speech act type of the next UU, nonverbal posi-
tive or negative evidence that the agent expects to 
receive are specified. (2) Monitoring: monitors and 
checks the user?s nonverbal status and signals dur-
ing the UU. After speaking, the agent continues 
monitoring until s/he gets enough evidence of un-
derstanding or not-understanding represented by 
user?s nonverbal status and signals.(3) Judging: 
once the agent gets enough evidence, s/he tries to 
judge groundedness as soon as possible. According 
to some previous studies, length of pause between 
UUs is in between 0.4 to 1 sec [18, 19]. Thus, time 
out for judgment is 1 sec after the end of the UU. If 
the agent does not have evidence then, the UU re-
mains ungrounded.  
This model is based on the information state 
approach [3], with update rules that revise the state 
of the conversation based on the inputs the system 
receives.  In our case, however, the inputs are sam-
pled continuously, include the nonverbal state, and 
only some require updates.  Other inputs indicate 
that the last utterance is still pending, and allow the 
agent to wait further.  In particular, task attention 
over an interval following the utterance triggers 
grounding.  Gaze in the interval means that the 
contribution stays provisional, and triggers an ob-
ligation to elaborate.  Likewise, if the system 
times-out without recognizing any user feedback, 
the segment remains ungrounded.  This process 
allows the system to keep talking across multiple 
utterance units without getting verbal feedback 
from the user. From the user?s perspective, explicit 
acknowledgement is not necessary, and minimal 
cost is involved in eliciting elaboration. 
4 Face-to-face Grounding with ECAs 
Based on our empirical results, we propose a dia-
logue manager that can handle nonverbal input to 
the grounding process, and we implement the 
mechanism in an embodied conversational agent.  
4.1 System 
MACK is an interactive public information ECA 
kiosk.  His current knowledgebase concerns the 
activities of the MIT Media Lab; he can answer 
questions about the lab?s research groups, projects, 
and demos, and give directions to each. 
On the input side, MACK recognizes three mo-
dalities: (1) speech, using IBM?s ViaVoice, (2) pen 
gesture via a paper map atop a table with an em-
bedded Wacom tablet, and (3) head nod and eye 
gaze via a stereo-camera-based 6-degree-of-
freedom head-pose tracker (based on [20]).  These 
inputs operate as parallel threads, allowing the Un-
derstanding Module (UM) to interpret the multiple 
modalities both individually and in combination. 
MACK produces multimodal output as well: (1) 
speech synthesis using the Microsoft Whistler 
Text-to-Speech (TTS) API, (2) a graphical figure 
with synchronized hand and arm gestures, and 
head and eye movements, and (3) LCD projector 
highlighting on the paper map, allowing MACK to 
reference it. 
The system architecture is shown in Figure 3.  
The UM interprets the input modalities and con-
verts them to dialogue moves which it then passes 
on to the Dialogue Manager (DM).  The DM con-
sists of two primary sub-modules, the Response 
Planner, which determines MACK?s next action(s) 
and creates a sequence of utterance units, and the 
Grounding Module (GrM), which updates the Dis-
course Model and decides when the Response 
Planner?s next UU should be passed on to the Gen-
eration module (GM).  The GM converts the UU 
into speech, gesture, and projector output, sending 
these synchronized modalities to the TTS engine, 
Animation Module (AM), and Projector Module.  
The Discourse Model maintains information 
about the state and history of the discourse.  This 
includes a list of grounded beliefs and ungrounded 
UUs; a history of previous UUs with timing infor-
mation; a history of nonverbal information (di-
vided into gaze states and head nods) organized by 
timestamp; and information about the state of the 
dialogue, such as the current UU under considera-
tion, and when it started and ended. 
4.2 Nonverbal Inputs 
Eye gaze and head nod inputs are recognized by a 
head tracker, which calculates rotations and trans-
lations in three dimensions based on visual and 
depth information taken from two cameras [20].  
The calculated head pose is translated into ?look at 
MACK,? ?look at map,? or ?look elsewhere.? The 
rotation of the head is translated into head nods, 
using a modified version of [21].  Head nod and 
eye gaze events are timestamped and logged within 
the nonverbal component of the Discourse History.  
The Grounding Module can thus look up the ap-
propriate nonverbal information to judge a UU. 
4.3 The Dialogue Manager 
In a kiosk ECA, the system needs to ensure that the 
user understands the information provided by the 
agent.  For this reason, we concentrated on imple-
menting a grounding mechanism for Assertion, 
when the agent gives the user directions, and An 
swer, when the agent answers the user?s questions 
Generating the Response 
The first job of the DM is to plan the response to a 
user?s query.  When a user asks for directions, the 
DM receives an event from the UM stating this 
intention.  The Response Planner in the DM, rec-
ognizing the user?s direction-request, calculates the 
directions, broken up into segments.  These seg-
ments are added to the DM?s Agenda, the stack of 
UUs to be processed. 
At this point, the GrM sends the first UU (a di-
rection segment) on the Agenda to the GM to be 
processed.  The GM converts the UU into speech 
and animation commands.  For MACK?s own non-
verbal grounding acts, the GM determines 
MACK?s gaze behavior according to the type of 
UU. For example, when MACK generates a direc-
tion segment (an Assertion), 66% of the time he 
keeps looking at the map.  When elaborating a 
previous UU, 47% of the time he gazes at the user.  
When the GM begins to process the UU, it logs 
the start time in the Discourse Model, and when it 
finishes processing (as it sends the final command 
to the animation module), it logs the end time.  The 
GrM waits for this speech and animation to end 
(by polling the Discourse Model until the end time 
is available), at which point it retrieves the timing 
data for the UU, in the form of timestamps for the 
UU start and finish.  This timing data is used to 
look up the nonverbal behavior co-occurring with 
the utterance in order to judge whether or not the 
UU was grounded. 
Judgment of grounding  
When MACK finishes uttering a UU, the Ground-
ing Module judges whether or not the UU is 
grounded, based on the user?s verbal and nonverbal 
behaviors during and after the UU.   
Using verbal evidence:  If the user returns an 
acknowledgement, such as ?OK?, the GrM judges 
the UU grounded.  If the user explicitly reports 
failure in perceiving MACK?s speech (ex. 
?what??), or not-understanding (ex. ?I don?t un-
derstand?), the UU remains ungrounded.  Note 
that, for the moment, verbal evidence is considered 
stronger than nonverbal evidence. 
Using nonverbal evidence:  The GrM looks up 
the nonverbal behavior occurring during the utter-
ance, and compares it to the model shown in Table 
3.  For each type of speech act, this model specifies 
the nonverbal behaviors that signal positive or ex-
plicit negative evidence.  First, the GrM compares 
the within-UU nonverbal behavior to the model.  
Then, it looks at the first nonverbal behavior oc-
curring during the pause after the UU.  If these two 
behaviors (?within? and ?pause?) match a pattern 
that signals positive evidence, the UU is grounded.  
If they match a pattern for negative evidence, the 
UU is not grounded.  If no pattern has yet been 
Figure 3: MACK system architecture
matched, the GrM waits for a tenth of a second and 
checks again.  If the required behavior has oc-
curred during this time, the UU is judged.  If not, 
the GrM continues looping in this manner until the 
UU is either grounded or ungrounded explicitly, or 
a 1 second threshold has been reached.  If the 
threshold is reached without a decision, the GrM 
times out and judges the UU ungrounded. 
Updating the Dialogue State 
 After judging grounding, the GrM updates the 
Discourse Model. The Discourse State maintained 
in the Discourse Model is similar to TRINDI kit 
[3], except that we store nonverbal information.  
There are three key fields: (1) a list of grounded 
UUs, (2) a list of pending (ungrounded) UUs, and 
(3) the current UU. If the current UU is judged 
grounded, its belief is added to (1).  If ungrounded, 
the UU is stored in (2). If an UU has subsequent 
contributions such as elaboration, these are stored 
in a single discourse unit, and grounded together 
when the last UU is grounded.  
Determining the Next Action 
After judging the UU?s grounding, the GrM de-
cides what MACK does next. (1) MACK can con-
tinue giving the directions as normal, by sending 
on the next segment in the Agenda to the GM.  As 
shown in Table 3, this happens 70% of the time 
when the UU is grounded, and only 27% of the 
time when it is not grounded.  Note, this happens 
100% of the time if verbal acknowledgement (e.g. 
?Uh huh?) is received for the UU.  
(2) MACK can elaborate on the most recent 
stage of the directions.  Elaborations are generated 
73% of the time when an Assertion is judged un-
grounded, and 78% of the time for an ungrounded 
Answer.  MACK elaborates by describing the most 
recent landmark in more detail.  For example, if 
the directions were ?Go down the hall and make a 
right at the door,? he might elaborate by saying 
?The big blue door.?  In this case, the GrM asks 
the Response Planner (RP) to provide an elabora-
tion for the current UU; the RP generates this 
elaboration (looking up the landmark in the data-
base) and adds it to the front of the Agenda; and 
the GrM sends this new UU on to the GM. 
Finally, if the user gives MACK explicit verbal 
evience of not understanding, MACK will simply 
repeat the last thing he said, by sending the UU 
back to the GM. 
4.4 Example 
Figure 4 shows an example of a user's interaction 
with MACK.  The user asks MACK for directions, 
and MACK replies using speech and pointing (us-
ing a projector) to the shared map. 
When the GrM sends the first segment in the 
Agenda to the GM,the starting time of the UU is 
noted and it is sent to the AM to be spoken and 
animated. During this time, the user?s nonverbal 
signals are logged in the Discourse Model. When 
the UU has finished, the GrM evaluates the log of 
the UU and of the very beginning of the pause (by 
waiting a tenth of a second and then checking the 
nonverbal history). In this case, MACK noted that 
the user looked at the map during the UU, and con-
tinued to do so just afterwards. This pattern 
matches the model for Assertion. The UU is 
judged as grounded, and the grounded belief is 
added to the Discourse Model. 
MACK then utters the second segment as be-
fore, but this time the GrM, finds that the user was 
looking up at MACK during most of the UU as 
well as after it, which signals that the UU is not 
grounded.  Therefore, the RP generates an elabora-
tion (line 4).  This utterance is judged to be 
Table 3: Grounding Model for MACK 
Target UU 
Type 
Evidence 
Type 
NV  
Pattern 
Judgment of 
ground Suggested next action
positive within: map 
pause: map /nod 
grounded 
go-ahead: 0.7 
elaboration: 0.30 Assertion 
negative within: gaze 
pause: gaze 
ungrounded 
go-ahead: 0.27 
elaboration:0.73 
positive within: gaze 
pause: map 
grounded 
go-ahead: 0.83 
elaboration: 0.17 Answer 
negative pause: gaze ungrounded 
go-ahead: 0.22 
elaboration: 0.78 
 
[1] U: How do I get to Room 309?
[2] M: To get to Room 309, go to that door and make a right. 
[3] M: Walk down the hall and make a left at the door
[4] M: It?s the glass door with red couches right outside.
[5] M: And that?s Room 309.
look at map
look at map
look at map
look at map
gaze at MACK
nod
Figure 4: Example of user (U) interacting with 
MACK (M).  User gives negative evidence of 
grounding in [3], so MACK elaborates [4].  
grounded both because the user continues looking 
at the map, and because the user nods, and so the 
final stage of the directions is spoken.  This is also 
grounded, leaving MACK ready for a new inquiry. 
5 Preliminary Evaluation 
Although we have shown an empirical basis for 
our implementation, it is important to ensure both 
that human users interact with MACK as we ex-
pect, and that their interaction is more effective 
than without nonverbal grounding.  The issue of 
effectiveness merits a full-scale study and thus we 
have chosen to concentrate here on whether 
MACK elicits the same behaviors from users as 
does interaction with other humans. 
Two subjects were therefore assigned to one of the 
following two conditions, both of which were run 
as Wizard of Oz (that is, ?speech recognition? was 
carried out by an experimenter): 
(a) MACK-with-grounding: MACK recognized 
user?s nonverbal signals for grounding, and dis-
played his nonverbal signals as a speaker. 
(b) MACK-without-grounding: MACK paid no 
attention to the user?s nonverbal behavior, and did 
not display nonverbal signals as a speaker. He gave 
the directions in one single turn. 
Subjects were instructed to ask for directions to 
two places, and were told that they would have to 
lead the experimenters to those locations to test 
their comprehension. We analyzed the second di-
rection-giving interaction, after subjects became 
accustomed to the system.  
Results: In neither condition, did users return ver-
bal feedback during MACK?s direction giving. As 
shown in Table 4, in MACK-with-grounding 7 
nonverbal status transitions were observed during 
his direction giving, which consisted of 5 Assertion 
UUs, one of them an elaboration. The transition 
patterns between MACK and the user when 
MACK used nonverbal grounding are strikingly 
similar to those in our empirical study of human-
to-human communication. There were three transi-
tions to gM/gM (both look at the map), which is a 
normal status in map task conversation, and two 
transitions to gP/gM (MACK looks at the user, and 
the user looks at the map), which is the most fre-
quent transition in Assertion as reported in Section 
3. Moreover, in MACK?s third UU, the user began 
looking at MACK at the middle of the UU and 
kept looking at him after the UU ended. This be-
havior successfully elicited MACK?s elaboration 
in the next UU.  
On the other hand, in the MACK-without-
grounding condition, the user never looked at 
MACK, and nodded only once, early on. As shown 
in Table 4, only three transitions were observed 
(shift to gMgM at the beginning of the interaction, 
shift to gMgMwN, then back to gMgM).  
While a larger scale evaluation with quantita-
tive data is one of the most important issues for 
future work, the results of this preliminary study 
strongly support our model, and show MACK?s 
potential for interacting with a human user using 
human-human conversational protocols. 
6 Discussion and Future Work 
We have reported how people use nonverbal sig-
nals in the process of grounding. We found that 
nonverbal signals that are recognized as positive 
evidence of understanding are different depending 
on the type of speech act. We also found that main-
taining gaze on the speaker is interpreted as evi-
dence of not-understanding, evoking an additional 
explanation from the speaker. Based on these em-
pirical results, we proposed a model of nonverbal 
grounding and implemented it in an embodied 
conversational agent.  
One of the most important future directions is 
to establish a more comprehensive model of face-
to-face grounding. Our study focused on eye gaze 
Figure 5: MACK with user 
Table 4: Preliminary evaluation 
 with-grounding w/o-grounding 
num of UUs 5 4
gMgM 3 2
gPgM 2 0
gMgP 1 0
gPgP 1 0
gMgMwN 0 1
Shift to
total 7 3
 
and head nods, which directly contribute to 
grounding. It is also important to analyze other 
types of nonverbal behaviors and investigate how 
they interact with eye gaze and head nods to 
achieve common ground, as well as contradictions 
between verbal and nonverbal evidence (eg. an 
interlocutor says, ?OK?, but looks at the partner).  
Finally, the implementation proposed here is a 
simple one, and it is clear that a more sophisticated 
dialogue management strategy is warranted, and 
will allow us to deal with back-grounding, and 
other aspects of miscommunication. For example, 
it would be useful to distinguish different levels of 
miscommunication: a sound that may or may not 
be speech, an out-of-grammar utterance, or an ut-
terance whose meaning is ambiguous.  In order to 
deal with such uncertainty in grounding, incorpo-
rating a probabilistic approach [4] into our model 
of face-to-face grounding is an elegant possibility.  
Acknowledgement 
Thanks to Candy Sidner, Matthew Stone, and 3 
anonymous reviewers for comments that improved 
the paper. Thanks to Prof. Nishida at Univ. of To-
kyo for his support of the research. 
References 
1.Clark, H.H. and E.F. Schaefer, Contributing to dis-
course. Cognitive Science, 1989. 13,: p. 259-294. 
2.Clark, H.H. and D. Wilkes-Gibbs, Referring as a col-
laborative process. Cognition, 1986. 22: p. 1-39. 
3.Matheson, C., M. Poesio, and D. Traum. Modelling 
Grounding and Discourse Obligations Using Update 
Rules. in 1st Annual Meeting of the North American 
Association for Computational Linguistics 
(NAACL2000). 2000. 
4.Paek, T. and E. Horvitz, Uncertainty, Utility, and 
Misunderstanding, in Working Papers of the AAAI Fall 
Symposium on Psychological Models of Communication 
in Collaborative Systems, S.E. Brennan, A. Giboin, and 
D. Traum, Editors. 1999, AAAI: Menlo Park, California. 
p. 85-92. 
5.Clark, H.H., Using Language. 1996, Cambridge: 
Cambridge University Press. 
6.Traum, D.R. and P. Dillenbourg. Miscommunication 
in Multimodal Collaboration. in AAAI Workshop on 
Detecting, Repairing, and Preventing Human-Machine 
Miscommunication. 1996. Portland, OR. 
7.Argyle, M. and M. Cook, Gaze and Mutual Gaze. 
1976, Cambridge: Cambridge University Press. 
8.Goodwin, C., Achieving Mutual Orientation at Turn 
Beginning, in Conversational Organization: Interaction 
between speakers and hearers. 1981, Academic Press: 
New York. p. 55-89. 
9.Novick, D.G., B. Hansen, and K. Ward. Coordinating 
turn-taking with gaze. in ICSLP-96. 1996. Philadelphia, 
PA. 
10.Cassell, J., et al More Than Just a Pretty Face: Af-
fordances of Embodiment. in IUI 2000. 2000. New Or-
leans, Louisiana. 
11.Traum, D. and J. Rickel. Embodied Agents for Multi-
party Dialogue in Immersive Virtual Worlds. in 
Autonomous Agents and Multi-Agent Systems. 2002. 
12.Cassell, J. and K.R. Thorisson, The Power of a Nod 
and a Glance: Envelope vs. Emotional Feedback in 
Animated Conversational Agents. Applied Artificial 
Intelligence, 1999. 13: p. 519-538. 
13.Nakatani, C. and D. Traum, Coding discourse struc-
ture in dialogue (version 1.0). 1999, University of 
Maryland. 
14.Pierrehumbert, J.B., The phonology and phonetics of 
english intonation. 1980, Massachusetts Institute of 
Technology. 
15.Allen, J. and M. Core, Draft of DMSL: Dialogue Act 
Markup in Several Layers. 1997, 
http://www.cs.rochester.edu/research/cisd/resources/da
msl/RevisedManual/RevisedManual.html. 
16.Duncan, S., On the structure of speaker-auditor in-
teraction during speaking turns. Language in Society, 
1974. 3: p. 161-180. 
17.Boyle, E., A. Anderson, and A. Newlands, The Ef-
fects of Visibility in a Cooperative Problem Solving 
Task. Language and Speech, 1994. 37(1): p. 1-20. 
18.Traum, D. and P. Heeman. Utterance Units and 
Grounding in Spoken Dialogue. in ICSLP. 1996. 
19.Nakajima, S.y. and J.F. Allen. Prosody as a cue for 
discourse structure. in ICSLP. 1992. 
20.Morency, L.P., A. Rahimi, and T. Darrell. A View-
Based Appearance Model for 6 DOF Tracking," Pro-
ceed-ings of. in IEEE conference on Computer Vision 
and Pattern Recognition. 2003. Madison, Wisconsin. 
21.Kapoor, A. and R.W. Picard. A Real-Time Head Nod 
and Shake Detector. in Workshop on Perceptive User 
Interfaces. 2001. Orlando FL. 
 
