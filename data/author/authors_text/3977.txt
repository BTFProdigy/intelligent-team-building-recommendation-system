Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 75?80,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 15: TempEval Temporal Relation Identification
Marc Verhagen?, Robert Gaizauskas?, Frank Schilder?, Mark Hepple?,
Graham Katz? and James Pustejovsky?
? Brandeis University, {marc,jamesp}@cs.brandeis.edu
? University of Sheffield, {r.gaizauskas,m.hepple}@dcs.shef.ac.uk
? Thomson Legal & Regulatory, frank.schilder@thomson.com,
? Stanford University, egkatz@stanford.edu
Abstract
The TempEval task proposes a simple way
to evaluate automatic extraction of temporal
relations. It avoids the pitfalls of evaluat-
ing a graph of inter-related labels by defin-
ing three sub tasks that allow pairwise eval-
uation of temporal relations. The task not
only allows straightforward evaluation, it
also avoids the complexities of full tempo-
ral parsing.
1 Introduction
Newspaper texts, narratives and other texts describe
events that occur in time and specify the temporal
location and order of these events. Text comprehen-
sion, amongst other capabilities, clearly requires the
capability to identify the events described in a text
and locate these in time. This capability is crucial to
a wide range of NLP applications, from document
summarization and question answering to machine
translation.
Recent work on the annotation of events and tem-
poral relations has resulted in both a de-facto stan-
dard for expressing these relations and a hand-built
gold standard of annotated texts. TimeML (Puste-
jovsky et al, 2003a) is an emerging ISO standard
for annotation of events, temporal expressions and
the anchoring and ordering relations between them.
TimeBank (Pustejovsky et al, 2003b; Boguraev et
al., forthcoming) was originally conceived of as a
proof of concept that illustrates the TimeML lan-
guage, but has since gone through several rounds of
revisions and can now be considered a gold standard
for temporal information. TimeML and TimeBank
have already been used as the basis for automatic
time, event and temporal relation annotation tasks in
a number of research projects in recent years (Mani
et al, 2006; Boguraev et al, forthcoming).
An open evaluation challenge in the area of tem-
poral annotation should serve to drive research for-
ward, as it has in other areas of NLP. The auto-
matic identification of all temporal referring expres-
sions, events and temporal relations within a text is
the ultimate aim of research in this area. However,
addressing this aim in a first evaluation challenge
was judged to be too difficult, both for organizers
and participants, and a staged approach was deemed
more effective. Thus we here present an initial eval-
uation exercise based on three limited tasks that we
believe are realistic both from the perspective of as-
sembling resources for development and testing and
from the perspective of developing systems capable
of addressing the tasks. They are also tasks, which
should they be performable automatically, have ap-
plication potential.
2 Task Description
The tasks as originally proposed were modified
slightly during the course of resource development
for the evaluation exercise due to constraints on data
and annotator availability. In the following we de-
scribe the tasks as they were ultimately realized in
the evaluation.
There were three tasks ? A, B and C. For all
three tasks the data provided for testing and train-
ing includes annotations identifying: (1) sentence
boundaries; (2) all temporal referring expression as
75
specified by TIMEX3; (3) all events as specified
in TimeML; (4) selected instances of temporal re-
lations, as relevant to the given task. For tasks A and
B a restricted set of event terms were identified ?
those whose stems occurred twenty times or more in
TimeBank. This set is referred to as the Event Target
List or ETL.
TASK A This task addresses only the temporal re-
lations holding between time and event expressions
that occur within the same sentence. Furthermore
only event expressions that occur within the ETL are
considered. In the training and test data, TLINK an-
notations for these temporal relations are provided,
the difference being that in the test data the relation
type is withheld. The task is to supply this label.
TASK B This task addresses only the temporal
relations holding between the Document Creation
Time (DCT) and event expressions. Again only
event expressions that occur within the ETL are con-
sidered. As in Task A, TLINK annotations for these
temporal relations are provided in both training and
test data, and again the relation type is withheld in
the test data and the task is to supply this label.
TASK C Task C relies upon the idea of their being
a main event within a sentence, typically the syn-
tactically dominant verb. The aim is to assign the
temporal relation between the main events of adja-
cent sentences. In both training and test data the
main events are identified (via an attribute in the
event annotation) and TLINKs between these main
events are supplied. As for Tasks A and B, the task
here is to supply the correct relation label for these
TLINKs.
3 Data Description and Data Preparation
The TempEval annotation language is a simplified
version of TimeML 1. For TempEval, we use the fol-
lowing five tags: TempEval, s, TIMEX3, EVENT,
and TLINK. TempEval is the document root and s
marks sentence boundaries. All sentence tags in the
TempEval data are automatically created using the
Alembic Natural Language processing tools. The
other three tags are discussed here in more detail:
1See http://www.timeml.org for language specifica-
tions and annotation guidelines
? TIMEX3. Tags the time expressions in the text.
It is identical to the TIMEX3 tag in TimeML.
See the TimeML specifications and guidelines
for further details on this tag and its attributes.
Each document has one special TIMEX3 tag,
the Document Creation Time, which is inter-
preted as an interval that spans a whole day.
? EVENT. Tags the event expressions in the text.
The interpretation of what an event is is taken
from TimeML where an event is a cover term
for predicates describing situations that happen
or occur as well as some, but not all, stative
predicates. Events can be denoted by verbs,
nouns or adjectives. The TempEval event an-
notation scheme is somewhat simpler than that
used in TimeML, whose complexity was de-
signed to handle event expressions that intro-
duced multiple event instances (consider, e.g.
He taught on Wednesday and Friday). This
complication was not necessary for the Tem-
pEval data. The most salient attributes encode
tense, aspect, modality and polarity informa-
tion. For TempEval task C, one extra attribute
is added: mainevent, with possible values
YES and NO.
? TLINK. This is a simplified version of the
TimeML TLINK tag. The relation types for the
TimeML version form a fine-grained set based
on James Allen?s interval logic (Allen, 1983).
For TempEval, we use only six relation types
including the three core relations BEFORE, AF-
TER, and OVERLAP, the two less specific re-
lations BEFORE-OR-OVERLAP and OVERLAP-
OR-AFTER for ambiguous cases, and finally the
relation VAGUE for those cases where no partic-
ular relation can be established.
As stated above the TLINKs of concern for each
task are explicitly included in the training and in the
test data. However, in the latter the relType at-
tribute of each TLINK is set to UNKNOWN. For each
task the system must replace the UNKNOWN values
with one of the six allowed values listed above.
The EVENT and TIMEX3 annotations were taken
verbatim from TimeBank version 1.2.2 The annota-
2TimeBank 1.2 is available for free through the Linguistic
Data Consortium, see http://www.timeml.org for more
76
tion procedure for TLINK tags involved dual anno-
tation by seven annotators using a web-based anno-
tation interface. After this phase, three experienced
annotators looked at all occurrences where two an-
notators differed as to what relation type to select
and decided on the best option. For task C, there
was an extra annotation phase where the main events
were marked up. Main events are those events that
are syntactically dominant in the sentences.
It should be noted that annotation of temporal re-
lations is not an easy task for humans due to ram-
pant temporal vagueness in natural language. As a
result, inter-annotator agreement scores are well be-
low the often kicked-around threshold of 90%, both
for the TimeML relation set as well as the TempEval
relation set. For TimeML temporal links, an inter-
annotator agreement of 0.77 was reported, where
agreement was measured by the average of preci-
sion and recall. The numbers for TempEval are even
lower, with an agreement of 0.72 for anchorings of
events to times (tasks A and B) and an agreement of
0.65 for event orderings (task C). Obviously, num-
bers like this temper the expectations for automatic
temporal linking.
The lower number for TempEval came a bit as
a surprise because, after all, there were fewer rela-
tions to choose form. However, the TempEval an-
notation task is different in the sense that it did not
give the annotator the option to ignore certain pairs
of events and made it therefore impossible to skip
hard-to-classify temporal relations.
4 Evaluating Temporal Relations
In full temporal annotation, evaluation of temporal
annotation runs into the same issues as evaluation of
anaphora chains: simple pairwise comparisons may
not be the best way to evaluate. In temporal annota-
tion, for example, one may wonder how the response
in (1) should be evaluated given the key in (2).
(1) {A before B, A before C, B equals C}
(2) {A after B, A after C, B equals C}
Scoring (1) at 0.33 precision misses the interde-
pendence between the temporal relations. What we
need to compare is not individual judgements but
two partial orders.
details.
For TempEval however, the tasks are defined in
a such a way that a simple pairwise comparison is
possible since we do not aim to create a full temporal
graph and judgements are made in isolation.
Recall that there are three basic temporal relations
(BEFORE, OVERLAP, and AFTER) as well as three
disjunctions over this set (BEFORE-OR-OVERLAP,
OVERLAP-OR-AFTER and VAGUE). The addition
of these disjunctions raises the question of how to
score a response of, for example, BEFORE given a
key of BEFORE-OR-OVERLAP. We use two scor-
ing schemes: strict and relaxed. The strict scoring
scheme only counts exact matches as success. For
example, if the key is OVERLAP and the response
BEFORE-OR-OVERLAP than this is counted as fail-
ure. We can use standard definitions of precision
and recall
Precision = Rc/R
Recall = Rc/K
where Rc is number of correct answers in the re-
sponse, R the total number of answers in the re-
sponse, and K the total number of answers in the
key. For the relaxed scoring scheme, precision and
recall are defined as
Precision = Rcw/R
Recall = Rcw/K
where Rcw reflects the weighted number of correct
answers. A response is not simply counted as 1 (cor-
rect) or 0 (incorrect), but is assigned one of the val-
ues in table 1.
B O A B-O O-A V
B 1 0 0 0.5 0 0.33
O 0 1 0 0.5 0.5 0.33
A 0 0 1 0 0.5 0.33
B-O 0.5 0.5 0 1 0.5 0.67
O-A 0 0.5 0.5 0.5 1 0.67
V 0.33 0.33 0.33 0.67 0.67 1
Table 1: Evaluation weights
This scheme gives partial credit for disjunctions,
but not so much that non-commitment edges out pre-
cise assignments. For example, assigning VAGUE as
the relation type for every temporal relation results
in a precision of 0.33.
77
5 Participants
Six teams participated in the TempEval tasks. Three
of the teams used statistics exclusively, one used a
rule-based system and the other two employed a hy-
brid approach. This section gives a short description
of the participating systems.
CU-TMP trained three support vector machine
(SVM) models, one for each task. All models used
the gold-standard TimeBank features for events and
times as well as syntactic features derived from the
text. Additionally, the relation types obtained by
running the task B system on the training data for
Task A and Task C, were added as a feature to the
two latter systems. A subset of features was selected
using cross-validations on the training data, dis-
carding features whose removal improved the cross-
validation F-score. When applied to the test data,
the Task B system was run first in order to supply
the necessary features to the Task A and Task C sys-
tems.
LCC-TE automatically identifies temporal refer-
ring expressions, events and temporal relations in
text using a hybrid approach, leveraging various
NLP tools and linguistic resources at LCC. For tem-
poral expression labeling and normalization, they
used a syntactic pattern matching tool that deploys a
large set of hand-crafted finite state rules. For event
detection, they used a small set of heuristics as well
as a lexicon to determine whether or not a token is
an event, based on the lemma, part of speech and
WordNet senses. For temporal relation discovery,
LCC-TE used a large set of syntactic and semantic
features as input to a machine learning components.
NAIST-japan defined the temporal relation iden-
tification task as a sequence labeling model, in
which the target pairs ? a TIMEX3 and an EVENT
? are linearly ordered in the document. For analyz-
ing the relative positions, they used features from
dependency trees which are obtained from a depen-
dency parser. The relative position between the tar-
get EVENT and a word in the target TIMEX3 is used
as a feature for a machine learning based relation
identifier. The relative positions between a word in
the target entities and another word are also intro-
duced.
The USFD system uses an off-the-shelf Machine
Learning suite(WEKA), treating the assignment of
temporal relations as a simple classification task.
The features used were the ones provided in the
TempEval data annotation together with a few fea-
tures straightforwardly computed from the docu-
ment without any deeper NLP analysis.
WVALI?s approach for discovering intra-
sentence temporal relations relies on sentence-level
syntactic tree generation, bottom-up propaga-
tion of the temporal relations between syntactic
constituents, a temporal reasoning mechanism
that relates the two targeted temporal entities to
their closest ancestor and then to each other, and
on conflict resolution heuristics. In establishing
the temporal relation between an event and the
Document Creation Time (DCT), the temporal ex-
pressions directly or indirectly linked to that event
are first analyzed and, if no relation is detected,
the temporal relation with the DCT is propagated
top-down in the syntactic tree. Inter-sentence tem-
poral relations are discovered by applying several
heuristics and by using statistical data extracted
from the training corpus.
XRCE-T used a rule-based system that relies on
a deep syntactic analyzer that was extended to treat
temporal expressions. Temporal processing is inte-
grated into a more generic tool, a general purpose
linguistic analyzer, and is thus a complement for a
better general purpose text understanding system.
Temporal analysis is intertwined with syntactico-
semantic text processing like deep syntactic analy-
sis and determination of thematic roles. TempEval-
specific treatment is performed in a post-processing
stage.
6 Results
The results for the six teams are presented in tables
2, 3, and 4.
team strict relaxed
P R F P R F
CU-TMP 0.61 0.61 0.61 0.63 0.63 0.63
LCC-TE 0.59 0.57 0.58 0.61 0.60 0.60
NAIST 0.61 0.61 0.61 0.63 0.63 0.63
USFD* 0.59 0.59 0.59 0.60 0.60 0.60
WVALI 0.62 0.62 0.62 0.64 0.64 0.64
XRCE-T 0.53 0.25 0.34 0.63 0.30 0.41
average 0.59 0.54 0.56 0.62 0.57 0.59
stddev 0.03 0.13 0.10 0.01 0.12 0.08
Table 2: Results for Task A
78
team strict relaxed
P R F P R F
CU-TMP 0.75 0.75 0.75 0.76 0.76 0.76
LCC-TE 0.75 0.71 0.73 0.76 0.72 0.74
NAIST 0.75 0.75 0.75 0.76 0.76 0.76
USFD* 0.73 0.73 0.73 0.74 0.74 0.74
WVALI 0.80 0.80 0.80 0.81 0.81 0.81
XRCE-T 0.78 0.57 0.66 0.84 0.62 0.71
average 0.76 0.72 0.74 0.78 0.74 0.75
stddev 0.03 0.08 0.05 0.03 0.06 0.03
Table 3: Results for Task B
team strict relaxed
P R F P R F
CU-TMP 0.54 0.54 0.54 0.58 0.58 0.58
LCC-TE 0.55 0.55 0.55 0.58 0.58 0.58
NAIST 0.49 0.49 0.49 0.53 0.53 0.53
USFD* 0.54 0.54 0.54 0.57 0.57 0.57
WVALI 0.54 0.54 0.54 0.64 0.64 0.64
XRCE-T 0.42 0.42 0.42 0.58 0.58 0.58
average 0.51 0.51 0.51 0.58 0.58 0.58
stddev 0.05 0.05 0.05 0.04 0.04 0.04
Table 4: Results for Task C
All tables give precision, recall and f-measure for
both the strict and the relaxed scoring scheme, as
well as averages and standard deviation on the pre-
cision, recall and f-measure numbers. The entry for
USFD is starred because the system developers are
co-organizers of the TempEval task.3
For task A, the f-measure scores range from 0.34
to 0.62 for the strict scheme and from 0.41 to 0.63
for the relaxed scheme. For task B, the scores range
from 0.66 to 0.80 (strict) and 0.71 to 0.81 (relaxed).
Finally, task C scores range from 0.42 to 0.55 (strict)
and from 0.56 to 0.66 (relaxed).
The differences between the systems is not spec-
tacular. WVALI?s hybrid approach outperforms the
other systems in task B and, using relaxed scoring,
in task C as well. But for task A, the winners barely
edge out the rest of the field. Similarly, for task C
using strict scoring, there is no system that clearly
separates itself from the field.
It should be noted that for task A, and in lesser ex-
tent for task B, the XRCE-T system has recall scores
that are far below all other systems. This seems
mostly due to a choice by the developers to not as-
sign a temporal relation if the syntactic analyzer did
not find a clear syntactic relation between the two
3There was a strict separation between people assisting in
the annotation of the evaluation corpus and people involved in
system development.
elements that needed to be linked for the TempEval
task.
7 Conclusion: the Future of Temporal
Evaluation
The evaluation approach of TempEval avoids the in-
terdependencies that are inherent to a network of
temporal relations, where relations in one part of the
network may constrain relations in any other part of
the network. To accomplish that, TempEval delib-
erately focused on subtasks of the larger problem of
automatic temporal annotation.
One thing we may want to change to the present
TempEval is the definition of task A. Currently, it
instructs to temporally link all events in a sentence
to all time expressions in the same sentence. In the
future we may consider splitting this into two tasks,
where one subtask focuses on those anchorings that
are very local, like ?...White House spokesman Mar-
lin Fitzwater [said] [late yesterday] that...?. We ex-
pect both inter-annotator agreement and system per-
formance to be higher on this subtask.
There are two research avenues that loom beyond
the current TempEval: (1) definition of other sub-
tasks with the ultimate goal of establishing a hierar-
chy of subtasks ranked on performance of automatic
taggers, and (2) an approach to evaluate entire time-
lines.
Some other temporal linking tasks that can be
considered are ordering of consecutive events in a
sentence, ordering of events that occur in syntactic
subordination relations, ordering events in coordi-
nations, and temporal linking of reporting events to
the document creation time. Once enough temporal
links from all these subtasks are added to the en-
tire temporal graph, it becomes possible to let confi-
dence scores from the separate subtasks drive a con-
straint propagation algorithm as proposed in (Allen,
1983), in effect using high-precision relations to
constrain lower-precision relations elsewhere in the
graph.
With this more complete temporal annotation it
is no longer possible to simply evaluate the entire
graph by scoring pairwise comparisons. Instead
the entire timeline must be evaluated. Initial ideas
regarding this focus on transforming the temporal
graph of a document into a set of partial orders built
79
around precedence and inclusion relations and then
evaluating each of these partial orders using some
kind of edit distance measure.4
We hope to have taken the first baby steps with
the three TempEval tasks.
8 Acknowledgements
We would like to thank all the people who helped
prepare the data for TempEval, listed here in no
particular order: Amber Stubbs, Jessica Littman,
Hongyuan Qiu, Emin Mimaroglu, Emma Barker,
Catherine Havasi, Yonit Boussany, Roser Saur??, and
Anna Rumshisky.
Thanks also to all participants to this new task:
Steven Bethard and James Martin (University of
Colorado at Boulder), Congmin Min, Munirath-
nam Srikanth and Abraham Fowler (Language Com-
puter Corporation), Yuchang Cheng, Masayuki Asa-
hara and Yuji Matsumoto (Nara Institute of Science
and Technology), Mark Hepple, Andrea Setzer and
Rob Gaizauskas (University of Sheffield), Caroline
Hageg`e and Xavier Tannier (XEROX Research Cen-
tre Europe), and Georgiana Pus?cas?u (University of
Wolverhampton and University of Alicante).
Part of the work in this paper was funded by
the DTO/AQUAINT program under grant num-
ber N61339-06-C-0140 and part funded by the EU
VIKEF project (IST- 2002-507173).
References
James Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Bran Boguraev, James Pustejovsky, Rie Ando, and Marc
Verhagen. forthcoming. Timebank evolution as a
community resource for timeml parsing. Language
Resources and Evaluation.
Inderjeet Mani, BenWellner, Marc Verhagen, ChongMin
Lee, and James Pustejovsky. 2006. Machine learn-
ing of temporal relations. In Proceedings of the 44th
Annual Meeting of the Association for Computational
Linguistics, Sydney, Australia. ACL.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003a. TimeML: Robust specification of
4Edit distance was proposed by Ben Wellner as a way to
evaluate partial orders of precedence relations (personal com-
munication).
event and temporal expressions in text. In Proceedings
of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, January.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003b. The TIMEBANK corpus. In
Proceedings of Corpus Linguistics 2003, pages 647?
656, Lancaster, March.
80
The Annotation of Temporal Information
in Natural Language Sentences
Graham Katz and Fabrizio Arosio
SFB 411
Univertit?t T?bingen
Nauklerstr. 35
D-72074 T?bingen, Germany
graham.katz@uni-tuebingen.de
fabrizio.arosio@uni-tuebingen.de
.
Abstract
The aim of this paper is to present
a language-neutral, theory-neutral
method for annotating sentence-
internal temporal relations. The
annotation method is simple and
can be applied without special
training. The annotations are
provided with a well-defined
model-theoretic interpretation for
use in the content-based
comparison of annotations.
Temporally annotated corpora
have a number of applications, in
lexicon/induction, translation and
linguistic investigation. A
searchable multi-language
database has already been created.
1 Introduction
In interpreting narratives the most essential
information to be extracted is who did what
where, when and why, the classic journalistic
imperatives. The ?who? and ?what? information
is usually expressed overtly, and this has made it
possible to apply empirical techniques to
problems in this domain (such as word-sense
classification and argument structure mapping).
The ?when? and ?where? information is,
however, often left implicit, or, at least, only
partially specified, making it difficlut to apply
such techniques to this domain.
Formal semantic theories of temporal
interpretation (e.g. Kamp & Reyle 1993;
Ogihara 1996; Abusch 1997) have been quite
successful at specifying the contribution that
such overt markers as tenses and temporal
adverbials make to the meaning of a sentence or
discourse. Investigations into the interpretation
of narrative discourse (Lascarides & Asher
1993, Reyle & Rossdeutscher 2000) have,
however, shown that very specific lexical
information plays an important role in
determining temporal interpretation. As of yet it
is not clear how this kind of lexical information
could be automatically acquired. The most
promising avenue for acquiring lexical
information appears to be automatic induction
from very large annotated corpora (Rooth, et. al.
1998). In order to apply these techniques to the
temporal domain it is necessary that the
temporal information be made explicit.  Our task
here is to provide a system of temporal
annotation that fulfills this requirement.
 The systems for temporal annotation we are
familiar with have been concerned either with
absolute temporal information (Wiebe, et. al.
1998, Androutsopoulos, Rithie & Thanisch
1997), or with the annotation of overt markers
(Setzer & Gaizauskas 2000). Much temporal
information, however, is not absolute but
relative and not overtly marked but implicit. We
are frequently only interested (and only have
information about) the order events occurred in.
And while there are sometimes overt markers
for these temporal relations, the conjunctions
before, after and when being the most obvious,
usually this kind of relational information is
implicit. The examples in (1) illustrate the
phenomenon.
(1) a. John kissed the girl he met at the party.
b. Leaving the party, John walked home.
c. He remembered talking to her and asking
her for her name.
Although there are no obvious markers for the
temporal ordering of the events described in
these sentences, native speakers have clear
intuitions about what happened when: we know
that the kissing took place after the meeting and
that the asking was part of the talking. But how
do we know this? And ? more importantly ?
how could this information be automatically
extracted from these sentences? These are the
questions that motivate the development of our
annotation system.
We believe that the creation of a large scale
treebank annotated with relational temporal
information as well as standard morphological
and syntactic information will make it possible
to investigate these issues productively. The
annotated treebank must be large scale for the
obvious reason that the application of stochastic
methods requires this. It should be syntactically
as well as semantically annotated because we
consider it quite likely that syntactic as well as
lexical information plays a role in temporal
interpretation. We don?t know a priori whether
in (1a) it is the lexical relationship between kiss
and meet that is crucial to determining the
temporal interpretation, or whether the fact that
meet is in a subordinate clause ? the syntactic
relation ? also plays a role. To answer these
kinds of questions it is necessary to encode the
temporal information conveyed by a sentence in
a way which makes addressing such questions
possible.
What we describe below is a practical system
for encoding relational temporal information
that is suited to large-scale hand annotation of
texts. This system has a number of applications
beyond this, both in the domain of cross-
linguistic investigation and in empirical NLP.
2 Temporal annotation
The idea of developing a treebank enriched with
semantic information is not new. In particular
such semantically annotated corpora have been
used in research on word sense disambiguation
(wordNet, Eagles, Simple) and semantics role
interpretation (Eagles). The public availability
of large syntactically annotated treebanks (Penn,
Verbmobil, Negra) makes such work attractive,
particularly in light of the success that empirical
methods have had (Kilgarriff & Rosenzweig
2000). Traditional semantic representational
formalisms such as DRT (Kamp & Reyle 1993)
are ill suited to semantic annotation. Since these
formalisms are developed in the service of
theories of natural language interpretation, they
are ? rightly ? both highly articulated and highly
constrained. In short, they are often too complex
and sometimes not expressive enough for the
purposes at hand (as the experience of Poesio et.
al. (1999) makes clear). Our proposal here is to
adopt a radically simplified semantic formalism
which, by virtue of its simplicity, is suited the
temporal-annotation application.
The temporal interpretation of a sentence, for
our purposes, can simply be taken to be the set
of temporal relations that a speaker naturally
takes to hold among the states and events
described by the verbs of the sentence. To put it
more formally, we associate with each verb a
temporal interval, and concern ourselves with
relations among these intervals. Of the interval
relations discussed by Allen (1984), we will be
concerned with only two: precedence and
inclusion. Taking ttalk to be the time of talking
task to be the time of asking and tremember to be the
time of remembering, the temporal
interpretation (1c), for example, can be given by
the following table:
ttalk task tremember
ttalk <
task ? <
tremember
Such a table, in effect, stores the native
speaker?s judgement about the most natural
temporal interpretation of the sentence.
Since our goal was to annotate a large number of
sentences with their temporal interpretations,
with the goal of examining the interaction
between the lexical and syntactic structure, it
was imperative that the interpretation be closely
tied to its syntactic context. We needed to keep
track of both the semantic relations among times
referred to by the words in a sentence and the
syntactic relations among the words in the
sentences that refer to these times, but not much
more. By adopting existing technology for
syntactic annotation, we were able do this quite
directly, by essentially building the information
in this table into the syntax.
2.1 The annotation system
To carry out our temporal annotation, we made
use of the Annotate tool for syntactic annotation
developed in Saarbr?cken by Brants and Plaehn
(2000). We exploited an aspect of the system
originally designed for the annotation of
anaphoric relations: the ability to link two
arbitrary nodes in a syntactic structure by means
of labeled ?secondary edges.? This allowed us to
add a layer of semantic annotation directly to
that of syntactic annotation.
A sentence was temporally annotated by linking
the verbs in the sentence via secondary edges
labeled with the appropriate temporal relation.
As we were initially only concerned with the
relations of precedence and inclusion, we only
had four labels: ?<? , ???, and their duals.
Sentence (1a), then, is annotated as in (2).
(2)     John kissed the girl he met at the party
The natural ordering relation between the
kissing and the meeting is indicated by the
labeled edge. Note that the edge goes from the
verb associated with the event that fills the first
argument of the relation to the verb associated
with the event that fills the second argument of
the relation.
The annotation of (1c), which was somewhat
more complex, indicates the two relations that
hold among the events described by the
sentence.
(3) He remembered talking and asking her name
In addition to encoding the relations among the
events described in a sentence, we anticipated
that it would be useful to encode also the
relationship between these events and the time at
which the sentence is produced. This is, after all,
what tenses usually convey. To encode this
temporal indexical information, we introduce
into the annotation an explicit representation of
the speech time. This is indicated by the ? ? ?
symbol, which is automatically prefaced to all
sentences prior to annotation.
The complete annotation for sentence (1a), then,
is (4).
(4)    ?   John kissed the girl he met at the party
As we see in (5), this coding scheme enables us
to represent the different interpretations that past
tensed and present tensed clauses have.
(5)    ?   John kissed the girl who is at the party
Notice that we do not annotate the tenses
themselves directly.
Note that in the case of reported speech, the time
associated with the embedding verb plays, for
the embedded sentence, much the same role that
the speech time plays for the main clause.
Formally, in fact, the relational analysis implicit
in our notation makes it possible to avoid many
of the problems associated with the treatment of
these constructions (such as those discussed at
length by von Stechow (1995)).  We set these
issues aside here.
It should be clear that we are not concerned with
giving a semantics for temporal markers, but
rather with providing a language within which
we can describe the temporal information
conveyed by natural language sentences. With
the addition of temporal indexical annotation,
our annotation system gains enough expressive
power to account for most of the relational
information conveyed by natural language
sentences. Left out at this point is temporal-
metrical information such as that conveyed by
the adverbial ?two hours later.?
2.2 Annotation procedure
The annotation procedure itself is quite
straightforward. We begin with a syntactically
annotated treebank and add the speech time
marker to each of the sentences.  The annotator
then simply marks the temporal relations among
verbs and the speech time for each sentence in
the corpus. This is accomplished in accordance
with the following conventions:
(i) temporal relations are encoded with
directed ?secondary edges?;
>
> >
> ?
> ?
(ii) the edge goes from the element that fills
the first argument of the relation to the
element that fills the second;
(iii) edge labels indicate the temporal relation
that holds;
(iv) edge labels can be ?>?, ?<?, ??? and ???
Annotators are instructed to annotate the
sentences as they naturally understand them.
When the treebank is made up of a sequence of
connected text, the annotators are encouraged to
make use of contextual information.
The annotation scheme is simple, explicit and
theory neutral. The annotator needs only to
exercise his native competence in his language
and he doesn?t need any special training in
temporal semantics or in any specific formal
language; in pilot studies we have assembled
small temporal annotated databases in few
hours. Our current database consists of 300
sentences from six languages.
2.3 Comparing annotations
It is well known that hand-annotated corpora are
prone to inconsistency (Marcus, Santorini &
Marcinkiewicz, 1993) and to that end it is
desirable that the corpus be multiply annotated
by different annotators and that these
annotations be compared. The kind of semantic
annotation we are proposing here introduces an
additional complexity to inter-annotation
comparison, in that the consistency of an
annotation is best defined not in formal terms
but in semantic terms. Two annotations should
be taken to be equivalent, for example, if they
express the same meanings, even if they use
different sets of labeled edges.
To make explicit what semantic identity is, we
provide our annotations with a model theoretic
interpretation. The annotations are interpreted
with respect to a structure D,<,? , where D is
the domain (here the set of verbs tokens in the
corpus) and < and ? are binary relations on D.
Models for this structure are assignments of
pairs of entities in D to < and ? satisfying the
following axioms:
- ?x,y,z.  x<y & y<z ? x<z
- ?x,y,z.  x?y & y?z ? x?z
- ?w,x,y,z. x<y & z?x & w?y ? z<w
- ?w,x,y,z. x<y & y<z & x ?w & z?w ? y?w
Thus < and ? have the properties one would
expect for the precedence and inclusion relation.
We are assuming that in the cases of interest
verbs refer to simply convex events.  Intuitively,
the set of verb tokens in the corpus corresponds
the set of times at which an event or state of the
type indicated by the verb takes place or holds.
In our corpus the number of sentences that
involved quantified or generic event reference
was quite low.
An annotated relation of the following form
X1  X2
is satisfied in a model iff the model assigns
<X1,X2> to R if R is < or ?, or <X2,X1> to R if
R is > or ?.  A sentence is satisfied by a model
iff all relations associated with the sentence are
satisfied by the model.  Intuitively an annotated
is satisfied by a model if the model assigns the
appropriate relation to the verbs occurring in the
sentence.
There are four semantic relations that can hold
among between annotations. These can be
defined in model-theoretic terms:
? Annotation A and B are equivalent if all
models satisfying A satisfy B and all models
satisfying B satisfy A.
? Annotation A subsumes annotation B iff all
models satisfying B satisfy A.
? Annotations A and B are consistent iff there
are models satisfying both A and B.
? Annotations A and B are inconsistent if
there are no models satisfying both A and B.
We can also define the minimal model satisfying
an annotation in the usual way. We can then
compute a distance measure between two
annotations by comparing set of models
satisfying the annotations. Let MA be the models
satisfying A and MB be those satisfying B and
MAB be those satisfying both (simply shorthand
for the intersection of MA and MB). Then the
distance between A and B can be defined as:
     d(A,B) = (|MA- MAB| + |MB-MAB|)/ |MAB|
In other words, the distance is the number of
relation pairs that are not shared by the
annotations normalized by the number that they
R
do share. We can use this metric to quantify the
?goodness? of both annotations and annotators.
Consider again (1c). We gave one annotation for
this in (3). In (6) and (7) there are two
alternative annotations.
(6) He remembered talking and asking her name
(7) He remembered talking and asking her name
As we can compute on the basis of the semantics
for the annotations (6) is equivalent with (3) ?
they are no distance apart, while (7) is
inconsistent with (3) ? they are infinitely far
apart. The annotation (8) is compatible (7) and is
a distance of 1 away from it.
(8) He remembered talking and asking her name
As in the case of structural annotation, there are
a number of ways of resolving inter-annotator
variation. We can chose the most informative
annotation as the correct one, or the most
general. Or we can combine annotations. The
intersection of two compatible annotations gives
an equally compatible annotation which contains
more information than either of the two alone.
We do not, as of yet, have enough data to
determine which of these strategies is most
effective.
In preliminary work, we had two annotators
annotate 50 complex sentences extracted
randomly from the BNC.  The results were quite
encouraging.  Although the annotations were
identical in only 70% of the cases, the
annotations were semantically consistent in 85%
of the cases.
3 Applications of temporal
annotation
There are any number of applications for a
temporally annotated corpus such as that we
have been outlining. Lexicon induction is the
most interesting, but, as we indicated at the
outset, this is a long-term project, as it requires a
significant investment in hand annotation. We
hope to get around this problem. But even still,
there are a number of other applications which
require less extensive corpora, but which are of
significant interest. One of these has formed the
initial focus of our research, and this is the
development of a searchable multilingual
database.
3.1 Multilingual database
Our annotation method has been applied to
sentences from a variety of languages, creating a
searchable multi-language treebank. This
database allows us to search for sentences that
express a given temporal relation in a language.
We have already developed a pilot multilingual
database with sentences from the Verbmobil
database (see an example in fig. 1) and we have
developed a query procedure in order to extract
relevant information.
Fig.1 A temporally annotated sentence from the Verbmobil
English treebank as displayed by @nnotate.
As can be seen, the temporal annotation is
entirely independent of the syntactic annotation.
In the context of the Annotate environment a
number of tools have been developed (and are
under development) for the querying of
structural relations. Since each sentence is stored
in the relational database with both syntactic and
temporal semantic annotations, it is possible to
make use of these querying tools to query on
structures, on meanings, and on structures and
meanings together. For example a query such as:
?Find the sentences containing a relative clause
which is interpreted as temporally overlapping
the main clause? can be processed. This query is
> >
>
> ?
>
encoded as a partially specified tree, as indicated
below:
In this structure, both the syntactic configuration
of the relative clause and the temporal relations
between the matrix verb and the speech time and
between the matrix verb and the verb occurring
in the relative clause are represented. Querying
our temporally annotated treebank with this
request yields the following result:
The application to cross-linguistic research
should be clear. It is now possible to use the
annotated tree-bank as an informant by storing
the linguistically relevant aspects of the
temporal system of a language in a compact
searchable database.
3.2 Aid for translation technology
Another potential application of the annotation
system is as an aid to automatic translation
systems. That the behaviour of tenses differ
from language to language makes the translation
of tenses difficult. In particular, the application
of example-based techniques faces serious
difficulties (Arnold, et. al. 1994). Adding the
intended temporal relation to the database of
source sentences makes it possible to moderate
this problem.
For example in Japanese (9a) is properly
translated as (10a) on one reading, where the
embedded past tense is translated as a present
tense, but as (10b) on the other, where the verb
is translated as a past tense.
(9) a. Bernard said that Junko was sick
(10)a. Bernard-wa Junko ga byookida to it-ta
    lit: Bernard said Junko is sick
b. Bernard-wa Junko-ga byookidata to it-ta.
          lit: Bernard said Junko was sick.
Only the intended reading can distinguish these
two translations. If this is encoded as part of the
input, we can hope to achieve much more
reasonable output.
3.3 Extracting cues for temporal
interpretation
While we see this sort of cross-linguistic
investigation as of intrinsic interest, our real
goal is the investigation of the lexical and
grammatical cues for temporal interpretation. As
already mentioned, the biggest problem is one of
scale. Generating a temporally annotated
treebank of the size needed is a serious
undertaking.
It would, of course, be of great help to be able to
partially automate this task. To that end we are
currently engaged in research attempting to use
overt cues such as perfect marking and temporal
conjunctions such as before and after to
bootstrap our way towards a temporally
annotated corpus. Briefly, the idea is to use
these overt markers to tag a corpus directly and
to use this to generate a table of lexical
preferences. So, for example, the sentence (11)
can be tagged automatically, because of the
presence of the perfect marking.
(11)    ?   John kissed the girl he had met
This automatic tagging will allow us to assemble
an initial data set of lexical preferences, such as
that that would appear to hold between kiss and
meet. If this initial data is confirmed by
comparison with hand-tagged data, we can use
S
VB
S
HD
VB
?
HD
[ST]
>
ADJ
> >
this information to automatically annotate a
much larger corpus based on these lexical
preferences. It may then be possible to begin to
carry out the investigation of cues to temporal
interpretation before we have constructed a large
hand-coded temporally annotated treebank.
4 Conclusions
We have described a simple and general
technique for the annotation of relational
temporal information in naturally occurring
sentences.  This annotation system is intended
for use in the large-scale annotation of corpora.
The annotations are provided with a model
theoretic semantics.  This semantics provides the
basis for a content-based system for comparing
and evaluating inter-annotator agreement.
Temporally annotated corpora such as those we
are developing have a number of applications in
both corpus-based theoretical work, and in
practical applications. Of particular interest to us
is the promise such annotated databases bring to
the problem of extracting automatically lexical
information about stereotypical ordering
relations among events that are used by native
speakers to interpret sentences temporally.  We
see this application as the main future research
to be carried out.
References
Abusch, Dorit. 1997. Sequence of Tense and
Temporal De Re. Linguistics and
Philosophy, 20: 1-50.
Allen, James F. 1984. A General Model of
Action and Time. Artificial Intelligence
23, 2.
Androutsopoulos, Ion, Graeme D. Ritchie and
Peter Thanisch. 1998. Time, Tense and
Aspect in Natural Language Database
Interfaces. Natural Language Engineering,
vol. 4, part 3: 229-276, Cambridge
University Press.
Arnold, Doug, Lorna Balkan, R. Lee
Humphreys, Siety Meijer and Louisa Sadler.
1994. Machine Translation: an introductory
guide. Blackwells/NCC, London.
Brants, Thorsten and Oliver Plaehn. 2000.
Interactive Corpus Annotation. In Second
International Conference on Language
Resources and Evaluation (LREC-2000),
Athens, Greece.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic. Kluwer Academic
Publishers, Dordrecht, Holland.
Kilgarriff, Adam and Joseph Rosenzweig. 2000.
Framework and Results for English
SENSEVAL. Special Issue on SENSEVAL:
Computers and the Humanities, 34 (1-2):
15-48.
Lascarides, Alex and Nicholas Asher. 1993. A
Semantics and Pragmatics for the Pluperfect.
In Proceedings of the Sixth European
Chapter of the Association of Computational
Linguistics, Utrecht.
Marcus, Mitchell P., Beatrice Santorini and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: the Penn
Treebank. Computational linguistics, 19:
313-330.
Ogihara, Toshiyuki. 1996. Tense, Scope and
Attitude Ascription. Kluwer, Dordrecht,
Holland.
Poesio, Massimo, Renate Henschel, Janet
Hitzeman, Rodger Kibble, Shane Montague,
and Kees van Deemter. 1999. ?Towards An
Annotation Scheme For Noun Phrase
Generation?, Proc. of the EACL Workshop
on Linguistically Interpreted Corpora.
Bergen.
Reyle, Uwe and Antje Rossdeutscher. 2000.
Understanding very short stories, ms. Institut
f?r Maschinelle Sprachverarbeitung,
Universit?t Stuttgart.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Sabine Schulte im Walde, Glenn Carroll and
Franz Beil. 1998. Inducing Lexicons with the
EM Algorithm. AIMS Report 4(3). Institut f?r
Maschinelle Sprachverarbeitung, Universit?t
Stuttgart.
Setzer, Andrea and Robert Gaizauskas. 2000.
Annotating Events and Temporal
Information in Newswire Texts. In Second
International Conference on Language
Resources and Evaluation (LREC-2000),
Athens, Greece.
von Stechow, Arnim. 1995. The Proper
Treatment of Tense. In Proceedings of
Semantics and Linguistic Theory V. 362-386.
CLC Publications. Cornell University.
Ithaca, NY.
Vazov, Nikolay and Guy Lapalme. 2000.
Identification of Temporal Structure in
French. Proceeding of the Workshop of the
7th International Conference on Principles of
Knowledge Representation and Reasoning,
Breckenridge, Colorado.
Wiebe, Janyce, Tom O?Hara, Kenneth
McKeever, and Thorsten ?hrstr?m-
Sandgren. 1998. An Empirical Approach to
Temporal Reference Resolution. Journal of
Artificial Intelligence Research, 9: 247-293.
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 12?19,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Identification of Non-Compositional Multi-Word Expressions
using Latent Semantic Analysis
Graham Katz
Institute of Cognitive Science
University of Osnabru?ck
gkatz@uos.de
Eugenie Giesbrecht
Institute of Cognitive Science
University of Osnabru?ck
egiesbre@uos.de
Abstract
Making use of latent semantic analy-
sis, we explore the hypothesis that lo-
cal linguistic context can serve to iden-
tify multi-word expressions that have non-
compositional meanings. We propose that
vector-similarity between distribution vec-
tors associated with an MWE as a whole
and those associated with its constitutent
parts can serve as a good measure of the
degree to which the MWE is composi-
tional. We present experiments that show
that low (cosine) similarity does, in fact,
correlate with non-compositionality.
1 Introduction
Identifying non-compositional (or idiomatic)
multi-word expressions (MWEs) is an important
subtask for any computational system (Sag et al,
2002), and significant attention has been paid
to practical methods for solving this problem in
recent years (Lin, 1999; Baldwin et al, 2003;
Villada Moiro?n and Tiedemann, 2006). While
corpus-based techniques for identifying collo-
cational multi-word expressions by exploiting
statistical properties of the co-occurrence of the
component words have become increasingly
sophisticated (Evert and Krenn, 2001; Evert,
2004), it is well known that mere co-occurrence
does not well distinguish compositional from
non-compositional expressions (Manning and
Schu?tze, 1999, Ch. 5).
While expressions which may potentially have
idiomatic meanings can be identified using various
lexical association measures (Evert and Krenn,
2001; Evert and Kermes, 2003), other techniques
must be used to determining whether or not a par-
ticular MWE does, in fact, have an idiomatic use.
In this paper we explore the hypothesis that the
local linguistic context can provide adequate cues
for making this determination and propose one
method for doing this.
We characterize our task on analogy with word-
sense disambiguation (Schu?tze, 1998; Ide and
Ve?ronis, 1998). As noted by Schu?tze, WSD
involves two related tasks: the general task of
sense discrimination?determining what senses
a given word has?and the more specific task
of sense selection?determining for a particular
use of the word in context which sense was in-
tended. For us the discrimination task involves
determining for a given expression whether it has
a non-compositional interpretation in addition to
its compositional interpretation, and the selec-
tion task involves determining in a given context,
whether a given expression is being used compo-
sitionally or non-compostionally. The German ex-
pression ins Wasser fallen, for example, has a non-
compositional interpretation on which it means ?to
fail to happen? (as in (1)) and a compositional in-
terpretation on which it means ?to fall into water
(as in (2)).1
(1) Das Kind war beim Baden von einer Luftma-
tratze ins Wasser gefallen.
?The child had fallen into the water from an a
air matress while swimming?
(2) Die Ero?fnung des Skateparks ist ins Wasser
gefallen.
?The opening of the skatepark was cancelled?
The discrimination task, then, is to identify ins
Wasser fallen as an MWE that has an idiomatic
meaning and the selection task is to determine that
1Examples taken from a newspaper corpus of the German
Su?ddeutsche Zeitung (1994-2000)
12
in (1) it is the compositional meaning that is in-
tended, while in (2) it is the non-compositional
meaning.
Following Schu?tze (1998) and Landauer & Du-
mais (1997) our general assumption is that the
meaning of an expression can be modelled in
terms of the words that it co-occurs with: its
co-occurrence signature. To determine whether
a phrase has a non-compositional meaning we
compute whether the co-occurrence signature of
the phrase is systematically related to the co-
occurrence signatures of its parts. Our hypoth-
esis is that a systematic relationship is indica-
tive of compositional interpretation and lack of
a systematic relationship is symptomatic of non-
compositionality. In other words, we expect com-
positional MWEs to appear in contexts more sim-
ilar to those in which their component words ap-
pear than do non-compositional MWEs.
In this paper we describe two experiments that
test this hypothesis. In the first experiment we
seek to confirm that the local context of a known
idiom can reliably distinguish idiomatic uses from
non-idiomatic uses. In the second experiment we
attempt to determine whether the difference be-
tween the contexts in which an MWE appears and
the contexts in which its component words appear
can indeed serve to tell us whether the MWE has
an idiomatic use.
In our experiments we make use of lexical se-
mantic analysis (LSA) as a model of context-
similarity (Deerwester et al, 1990). Since this
technique is often used to model meaning, we will
speak in terms of ?meaning? similiarity. It should
be clear, however, that we are only using the LSA
vectors?derived from context of occurrence in a
corpus?to model meaning and meaning composi-
tion in a very rough way. Our hope is simply that
this rough model is sufficient to the task of identi-
fying non-compositional MWEs.
2 Previous work
Recent work which attempts to discriminate
between compositional and non-compositional
MWEs include Lin (1999), who used mutual-
information measures identify such phrases, Bald-
win et al (2003), who compare the distribution
of the head of the MWE with the distribution of
the entire MWE, and Vallada Moiro?n & Tiede-
mann (2006), who use a word-alignment strat-
egy to identify non-compositional MWEs making
use of parallel texts. Schone & Jurafsky (2001)
applied LSA to MWE identification, althought
they did not focus on distinguishing compositional
from non-compositional MWEs.
Lin?s goal, like ours, was to discriminate non-
compositional MWEs from compositional MWEs.
His method was to compare the mutual informa-
tion measure of the constituents parts of an MWE
with the mutual information of similar expressions
obtained by substituting one of the constituents
with a related word obtained by thesaurus lookup.
The hope was that a significant difference between
these measures, as in the case of red tape (mutual
information: 5.87) compared to yellow tape (3.75)
or orange tape (2.64), would be characteristic of
non-compositional MWEs. Although intuitively
appealing, Lin?s algorithm only achieves precision
and recall of 15.7% and 13.7%, respectively (as
compared to a gold standard generate from an id-
iom dictionary?but see below for discussion).
Schone & Jurafsky (2001) evaluated a num-
ber of co-occurrence-based metrics for identify-
ing MWEs, showing that, as suggested by Lin?s
results, there was need for improvement in this
area. Since LSA has been used in a number
of meaning-related language tasks to good ef-
fect (Landauer and Dumais, 1997; Landauer and
Psotka, 2000; Cederberg and Widdows, 2003),
they had hoped to improve their results by identify
non-compositional expressions using a method
similar to that which we are exploring here. Al-
though they do not demonstrate that this method
actually identifies non-compositional expressions,
they do show that the LSA similarity technique
only improves MWE identification minimally.
Baldwin et al, (2003) focus more narrowly
on distinguishing English noun-noun compounds
and verb-particle constructions which are com-
positional from those which are not composi-
tional. Their approach is methodologically similar
to ours, in that they compute similarity on the ba-
sis of contexts of occurrance, making use of LSA.
Their hypothesis is that high LSA-based similar-
ity between the MWE and each of its constituent
parts is indicative of compositionality. They evalu-
ate their technique by assessing the correlation be-
tween high semantic similarity of the constituents
of an MWE to the MWE as a whole with the like-
lihood that the MWE appears in WordNet as a hy-
ponym of one of the constituents. While the ex-
pected correlation was not attested, we suspect this
13
to be more an indication of the inappropriateness
of the evaluation used than of the faultiness of the
general approach.
Lin, Baldwin et al, and Schone & Jurafsky, all
use as their gold standard either idiom dictionaries
or WordNet (Fellbaum, 1998). While Schone &
Jurafsky show that WordNet is as good a standard
as any of a number of machine readable dictionar-
ies, none of these authors shows that the MWEs
that appear in WordNet (or in the MRDs) are gen-
erally non-compositional, in the relevant sense. As
noted by Sag et al (2002) many MWEs are sim-
ply ?institutionalized phrases? whose meanings
are perfectly compositional, but whose frequency
of use (or other non-linguistic factors) make them
highly salient. It is certainly clear that many
MWEs that appear in WordNet?examples being
law student, medical student, college man?are
perfectly compositional semantically.
Zhai (1997), in an early attempt to apply
statistical methods to the extraction of non-
compositional MWEs, made use of what we take
to be a more appropriate evaluation metric. In his
comparison among a number of different heuris-
tics for identifying non-compositional noun-noun
compounds, Zhai did his evaluation by applying
each heuristic to a corpus of items hand-classified
as to their compositionality. Although Zhai?s clas-
sification appears to be problematic, we take this
to be the appropirate paradigm for evaluation in
this domain, and we adopt it here.
3 Proceedure
In our work we made use of the Word Space
model of (semantic) similiarty (Schu?tze, 1998)
and extended it slightly to MWEs. In this frame-
work, ?meaning? is modeled as an n-dimensional
vector, derived via singular value decomposition
(Deerwester et al, 1990) from word co-occurrence
counts for the expression in question, a technique
frequently referred to as Latent Semantic Analysis
(LSA). This kind of dimensionality reduction has
been shown to improve performance in a number
of text-based domains (Berry et al, 1999).
For our experiments we used a local German
newspaper corpus.2 We built our LSA model
with the Infomap Software package.3, using the
1000 most frequent words not on the 102-word
2Su?ddeutsche Zeitung (SZ) corpus for 2003 with about 42
million words.
3Available from infomap.stanford.edu.
Figure 1: Two dimensional Word Space
hand-generated stop list as the content-bearing di-
mension words (the columns of the matrix). The
20,000 most frequent content words were assigned
row values by counting occurrences within a 30-
word window. SVD was used to reduce the di-
mensionality from 1000 to 100, resulting in 100
dimensional ?meaning?-vectors for each word. In
our experiments, MWEs were assigned meaning-
vectors as a whole, using the same proceedure.
For meaning similarity we adopt the standard mea-
sure of cosine of the angle between two vectors
(the normalized correlation coefficient) as a met-
ric (Schu?tze, 1998; Baeza-Yates and Ribeiro-Neto,
1999). On this metric, two expressions are taken
to be unrelated if their meaning vectors are orthog-
onal (the cosine is 0) and synonymous if their vec-
tors are parallel (the cosine is 1).
Figure 1 illustrates such a vector space in two
dimensions. Note that the meaning vector for
Lo?ffel ?spoon? is quite similar to that for es-
sen ?to eat? but distant from sterben ?to die?,
while the meaning vector for the MWE den Lo?ffel
abgeben is close to that for sterben. Indeed den
Lo?ffel abgeben, like to kick the bucket, is a non-
compositional idiom meaning ?to die?.
While den Lo?ffel abgeben is used almost ex-
clusively in its idiomatic sense (all four occur-
rences in our corpus), many MWEs are used reg-
ularly in both their idiomatic and in their literal
senses. About two thirds of the uses of the MWE
ins Wasser fallen in our corpus are idiomatic uses,
and the remaing one third are literal uses. In
our first experiment we tested the hypothesis that
these uses could reliably be distinguished using
distribution-based models of their meaning.
14
3.1 Experiment I
For this experiment we manually annotated the
67 occurrences of ins Wasser fallen in our cor-
pus as to whether the expression was used com-
positionally (literally) or non-compositionally (id-
iomatically).4 Marking this distinction we gen-
erate an LSA meaning vectors for the composi-
tional uses and an LSA meaning vector for the
non-compositional uses of ins Wasser fallen. The
vectors turned out, as expected, to be almost or-
thogonal, with a cosine of the angle between them
of 0.02. This result confirms that the linguis-
tic contexts in which the literal and the idiomatic
use of ins Wasser fallen appear are very differ-
ent, indicating?not surprisingly?that the seman-
tic difference between the literal meaning and the
idiomatic meaning is reflected in the way these
these phrases are used.
Our next task was to investigate whether this
difference could be used in particular cases to de-
termine what the intended use of an MWE in a
particular context was. To evaluate this, we did a
10-fold cross-validation study, calculating the lit-
eral and idiomatic vectors for ins Wasser fallen on
the basis of the training data and doing a simple
nearest neighbor classification of each memember
of the test set on the basis of the meaning vectors
computed from its local context (the 30 word win-
dow). Our result of an average accurace of 72%
for our LSA-based classifier far exceeds the sim-
ple maximum-likelihood baseline of 58%.
In the final part of this experiment we compared
the meaning vector that was computed by sum-
ming over all uses of ins Wasser fallen with the
literal and idiomatic vectors from above. Since id-
iomatic uses of ins Wasser fallen prevail in the cor-
pus (2/3 vs. 1/3), it is not surprisingly that the sim-
ilarity to the literal vector (0.0946) is much than
similarity to the idiomatic vector (0.3712).
To summarize Experiment I, which is a vari-
ant of a supervised phrase sense disambiguation
task, demonstrates that we can use LSA to distin-
guish between literal and the idiomatic usage of an
MWE by using local linguistic context.
4This was a straightforward task; two annotators anno-
tated independently, with very high agreement?kappa score
of over 0.95 (Carletta, 1996). Occurrences on which the an-
notators disagreed were thrown out. Of the 64 occurrences
we used, 37 were idiomatic and 27 were literal.
3.2 Experiment II
In our second experiment we sought to make
use of the fact that there are typically clear
distributional difference between compositional
and non-compositional uses of MWEs to deter-
mine whether a given MWE indeed has non-
compositional uses at all. In this experi-
ment we made use of a test set of German
Preposition-Noun-Verb ?collocation candidate?
database whose extraction is described by Krenn
(2000) and which has been made available elec-
tronically.5 From this database only word com-
binations with frequency of occurrence more than
30 in our test corpus were considered. Our task
was to classify these 81 potential MWEs accord-
ing whether or not thay have an idiomatic mean-
ing.
To accomplish this task we took the following
approach. We computed on the basis of the dis-
tribution of the components of the MWE an esti-
mate for the compositional meaning vector for the
MWE. We then compared this to the actual vec-
tor for the MWE as a whole, with the expecta-
tion MWEs which indeed have non-compositinoal
uses will be distinguished by a relatively low vec-
tor similarity between the estimated compositional
meaning vector and the actual meaning vector.
In other words small similarity values should be
diagnostic for the presense of non-compositinoal
uses of the MWE.
We calculated the estimated compositional
meaning vector by taking it to be the sum of the
meaning vector of the parts, i.e., the compositional
meaning of an expression w1w2 consisting of two
words is taken to be sum of the meaning vectors
for the constituent words.6 In order to maximize
the independent contribution of the constituent
words, the meaning vectors for these words were
always computed from contexts in which they ap-
pear alone (that is, not in the local context of the
other constituent). We call the estimated composi-
tional meaning vector the ?composed? vector.7
The comparisons we made are illustrated in Fig-
ure 2, where vectors for the MWE auf die Strecke
bleiben ?to fall by the wayside? and the words
Strecke ?route? and bleiben ?to stay? are mapped
5Available as an example data collection in UCS-Toolkit
5 from www.collocations.de.
6For all our experiments we consider only two-word com-
binations.
7Schone & Jurafsky (2001) explore a few modest varia-
tions of this estimate.
15
Figure 2: Composed versus Multi-Word
into two dimensions8. (the words Autobahn ?high-
way? and eigensta?ndig ?independent? are given for
comparison). Here we see that the linear com-
bination of the component words of the MWE is
clearly distinct from that of the MWE as a whole.
As a further illustration of the difference be-
tween the composed vector and the MWE vector,
in Table 2 we list the words whose meaning vector
is most similar to that of the MWE auf dis Strecke
bleiben along with their similarity values, and in
Table 3 we list those words whose meaning vec-
tor is most similar to the composed vector. The
semantic differences among these two classes are
readily apparent.
folgerung ?consequence? 0.769663
eigensta?ndig ?independent? 0.732372
langfristiger ?long-term? 0.731411
herbeifu?hren ?to effect? 0.717294
ausnahmefa?lle ?exceptions? 0.704939
Table 1: auf die Strecke bleiben
strecken ?to lengthen? 0.743309
fahren ?to drive? 0.741059
laufen ?to run? 0.726631
fahrt ?drives? 0.712352
schlie?en ?to close? 0.704364
Table 2: Strecke+bleiben
We recognize that the composed vector is
clearly nowhere near a perfect model of compo-
sitional meaning in the general case. This can be
illustrated by considering, for example, the MWE
fire breathing. This expression is clearly com-
positional, as it denotes the process of producing
8The preposition auf and the article die are on the stop list
combusting exhalation, exactly what the seman-
tic combination rules of the English would pre-
dict. Nevertheless the distribution of fire breath-
ing is quite unrelated to that of its constituents
fire and breathing ( the former appears frequently
with dragon and circus while the later appear fre-
quently with blaze and lungs, respectively). De-
spite these principled objections, the composed
vector provides a useful baseline for our investiga-
tion. We should note that a number of researchers
in the LSA tradition have attempted to provide
more compelling combinatory functions to cap-
ture the non-linearity of linguistic compositional
interpretation (Kintsch, 2001; Widdows and Pe-
ters, 2003).
As a check we chose, at random, a number of
simple clearly-compositional word combinations
(not from the candidate MWE list). We expected
that on the whole these would evidence a very high
similarity measure when compared with their as-
sociated composed vector, and this is indeed the
case, as shown in Table 1. We also compared
vor Gericht verantworten 0.80735103
?to appear in court?
im Bett liegen 0.76056000
?to lie in bed?
aus Gefa?ngnis entlassen 0.66532673
?dismiss from prison?
Table 3: Non-idiomatic phrases
the literal and non-literal vectors for ins Wasser
fallen from the first experiment with the composed
vector, computed out of the meaning vectors for
Wasser and for fallen.9 The difference isn?t large,
but nevertheless the composed vector is more sim-
ilar to the literal vector (cosine of 0.2937) than to
the non-literal vector (cosine of 0.1733).
Extending to the general case, our task was to
compare the composed vector to the actual vec-
tor for all the MWEs in our test set. The result-
ing cosine similarity values range from 0.01 to
0.80. Our hope was that there would be a similar-
ity threshold for distinguishing MWEs that have
non-compositional interpretations from those that
do not. Indeed of the MWEs with a similarity val-
ues of under 0.1, just over half are MWEs which
were hand-annotated to have non-literal uses.10 It
9The preposition ins is on the stop list and plays no role
in the computation.
10The similarity scores for the entire test set are given in
16
is clear then that the technique described is, prima
facie, capable of detecting idiomatic MWEs.
3.3 Evaluation and Discussion
To evaluate the method, we used the careful man-
ual annotation of the PNV database described by
Krenn (2000) as our gold standard. By adopt-
ing different threshholds for the classification de-
cision, we obtained a range of results (trading off
precision and recall). Table 4 illustrates this range.
The F-score measure is maximized in our ex-
periments by adopting a similarity threshold of
0.2. This means that MWEs which have a mean-
ing vector whose cosine is under this value when
compared with with the combined vector should
be classified as having a non-literal meaning.
To compare our method with that proposed by
Baldwin et al (2003), we applied their method
to our materials, generating LSA vectors for the
component content words in our candidate MWEs
and comparing their semantic similarity to the
MWEs LSA vector as a whole, with the expecta-
tion being that low similarity between the MWE as
a whole and its component words is indication of
the non-compositionality of the MWE. The results
are given in Table 5.
It is clear that while Baldwin et al?s expectation
is borne out in the case of the constituent noun
(the non-head), it is not in the case of the con-
stituent verb (the head). Even in the case of the
nouns, however, the results are, for the most part,
markedly inferior to the results we achieved using
the composed vectors.
There are a number of issues that complicate
the workability of the unsupervised technique de-
scribed here. We rely on there being enough
non-compositional uses of an idiomatic MWE in
the corpus that the overall meaning vector for the
MWE reflects this usage. If the literal meaning
is overwhelmingly frequent, this will reduce the
effectivity of the method significantly. A second
problem concerns the relationship between the lit-
eral and the non-literal meaning. Our technique
relies on these meaning being highly distinct. If
the meanings are similar, it is likely that local con-
text will be inadequate to distinguish a composi-
tional from a non-compositional use of the expres-
sion. In our investigation it became apparent, in
fact, that in the newspaper genre, highly idiomatic
expressions such as ins Wasser fallen were often
Appendix I.
used in their idiomatic sense (apparently for hu-
morous effect) particularly frequently in contexts
in which elements of the literal meaning were also
present.11
4 Conclusion
To summarize, in order to classify an MWE as
non-compositional, we compute an approximation
of its compositional meaning and compare this
with the meaning of the expression as it is used
on the whole. One of the obvious improvements
to the algorithm could come from better mod-
els for simulating compositional meaning. A fur-
ther issue that can be explored is whether linguis-
tic preprocessing would influence the results. We
worked only on raw text data. There is some ev-
idence (Baldwin et al, 2003) that part of speech
tagging might improve results in this kind of task.
We also only considered local word sequences.
Certainly some recognition of the syntactic struc-
ture would improve results. These are, however,
more general issues associated with MWE pro-
cessing.
Rather promising results were attained using
only local context, however. Our study shows
that the F-score measure is maximized by taking
as threshold for distinguishing non-compositional
phrases from compositional ones a cosine simi-
larity value somewhere between 0.1-0.2. An im-
portant point to be explored is that compositional-
ity appears to come in degrees. As Bannard and
Lascarides (2003) have noted, MWEs ?do not fall
cleanly into the binary classes of compositional
and non-compositional expressions, but populate
a continuum between the two extremes.? While
our experiment was designed to classify MWEs,
the technique described here, of course, provides
a means, if rather a blunt one, for quantifying the
degreee of compositonality of an expression.
References
Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto.
1999. Modern Information Retrieval. ACM Press /
Addison-Wesley.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
11One such example from the SZ corpus:
Der Auftakt wa?re allerdings fast ins Wasser gefallen, weil ein
geplatzter Hydrant eine fu?nfzehn Meter hohe Wasserfonta?ne
in die Luft schleuderte.
?The prelude almost didn?t occur, because a burst hydrant
shot a fifteen-meter high fountain into the sky.?
17
cos < 0.1 cos < 0.2 cos < 0.3 cos < 0.4 cos < 0.5
Precision 0.53 0.39 0.29 0.22 0.21
Recall 0.42 0.63 0.84 0.89 0.95
F-measure 0.47 0.48 0.43 0.35 0.34
Table 4: Evaluation of Various Similarity Thresholds
cos < 0.1 cos < 0.2 cos < 0.3 cos < 0.4 cos < 0.5
Verb F-measure 0.21 0.16 0.29 0.26 0.27
Noun F-measure 0.28 0.51 0.43 0.39 0.33
Table 5: Evaluation of Method of Baldwin et al (2003)
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions: Analysis, Acquisition and Treatment,
pages 89?96, Sapporo, Japan.
Colin Bannard, Timothy Baldwin, and Alex Las-
carides. 2003. A statistical approach to the seman-
tics of verb-particles. In Proceedings of the ACL-
2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment, pages 65?72, Sap-
poro, Japan.
Michael W. Berry, Zlatko Drmavc, and Elisabeth R.
Jessup. 1999. Matrices, vector spaces, and infor-
mation retrieval. SIAM Review, 41(2):335?362.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Scott Cederberg and Dominic Widdows. 2003. Using
LSA and noun coordination information to improve
the precision and recall of automatic hyponymy ex-
traction. In In Seventh Conference on Computa-
tional Natural Language Learning, pages 111?118,
Edmonton, Canada, June.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Stefan Evert and Hannah Kermes. 2003. Experi-
ments on candidate data for collocation extraction.
In Companion Volume to the Proceedings of the 10th
Conference of The European Chapter of the Associ-
ation for Computational Linguistics, pages 83?86,
Budapest, Hungary.
Stefan Evert and Brigitte Krenn. 2001. Methods for
the qualitative evaluation of lexical association mea-
sures. In Proceedings of the 39th Annual Meeting
of the Association for Computational Linguistics,
pages 188?195, Toulouse, France.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
University of Stuttgart.
Christiane Fellbaum. 1998. WordNet, an electronic
lexical database. MIT Press, Cambridge, MA.
Nancy Ide and Jean Ve?ronis. 1998. Word sense dis-
ambiguation: The state of the art. Computational
Linguistics, 14(1).
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
Brigitte Krenn. 2000. The Usual Suspects: Data-
Oriented Models for Identification and Representa-
tion of Lexical Collocations. Dissertations in Com-
putational Linguistics and Language Technology.
German Research Center for Artificial Intelligence
and Saarland University, Saarbru?cken, Germany.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato?s problem: The latent seman-
tic analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Review,
104:211?240.
Thomas K. Landauer and Joseph Psotka. 2000. Sim-
ulating text understanding for educational applica-
tions with latent semantic analysis: Introduction to
LSA. Interactive Learning Environments, 8(2):73?
86.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 317?324, College Park,
MD.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical NaturalLanguage Pro-
cessing. The MIT Press, Cambridge, MA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conferences on
Intelligent Text Processing and Computational Lin-
guistics, pages 1?15.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictio-
nary headwords a solved problem? In Proceedings
18
of Empirical Methods in Natural Language Process-
ing, Pittsburgh, PA.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Begon?a Villada Moiro?n and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL 2006
Workshop on Multiword Expressions in a Multilin-
gual Context, Trento, Italy.
Dominic Widdows and Stanley Peters. 2003. Word
vectors and quantum logic: Experiments with nega-
tion and disjunction. In Eighth Mathematics of Lan-
guage Conference, pages 141?150, Bloomington,
Indiana.
Chengxiang Zhai. 1997. Exploiting context to iden-
tify lexical atoms ? a statistical view of linguistic
context. In Proceedings of the International and In-
terdisciplinary Conference on Modelling and Using
Context (CONTEXT-97), pages 119?129.
APPENDIX
Similarity (cosine) values for the combined and
the MWE vector. Uppercase entries are those
hand-annotated as being MWEs which have an id-
iomatic interpretation.
Word Combinations Cosines
(vor) gericht verantworten 0.80735103
(in) bett liegen 0.76056000
(aus) gefa?ngnis entlassen 0.66532673
(zu) verfu?ung stellen 0.60310321
(aus) haft entlassen 0.59105617
(um) prozent steigern 0.55889772
(ZU) KASSE BITTEN 0.526331
(auf) prozent sinken 0.51281725
(IN) TASCHE GREIFEN 0.49350031
(zu) verfu?gung stehen 0.49236563
(auf) prozent steigen 0.47422122
(um) prozent zulegen 0.47329672
(in) betrieb gehen 0.47262171
(unter) druck geraten 0.44377297
(in) deutschland leben 0.44226071
(um) prozent steigen 0.41498688
(in) rechnung stellen 0.40985534
(von) prozent erreichen 0.39407666
(auf) markt kommen 0.38740534
(unter) druck setzen 0.37822936
(in) vergessenheit geraten 0.36654168
(um) prozent sinken 0.36600216
(in) rente gehen 0.36272313
(zu) einsatz kommen 0.3562527
(zu) schule gehen 0.35595884
(in) frage stellen 0.35406327
(in) frage kommen 0.34714701
(in) luft sprengen 0.34241143
(ZU) GESICHT BEKOMMEN 0.34160325
(vor) gericht ziehen 0.33405685
(in) gang setzen 0.33231573
(in) anspruch nehmen 0.32217044
(auf) prozent erho?hen 0.31574088
(um) prozent wachsen 0.3151615
(in) empfang nehmen 0.31420746
(fu?r) sicherheit sorgen 0.30230156
(zu) ausdruck bringen 0.30001438
(IM) MITTELPUNKT STEHEN 0.29770654
(zu) ruhe kommen 0.29753093
(IM) AUGE BEHALTEN 0.2969367
(in) urlaub fahren 0.29627064
(in) kauf nehmen 0.2947628
(in) pflicht nehmen 0.29470704
(in) ho?he treiben 0.29450525
(in) kraft treten 0.29311349
(zu) kenntnis nehmen 0.28969961
(an) start gehen 0.28315812
(auf) markt bringen 0.2800427
(in) ruhe standgehen 0.27575604
(bei) prozent liegen 0.27287073
(um) prozent senken 0.26506203
(UNTER) LUPE NEHMEN 0.2607078
(zu) zug kommen 0.25663165
(zu) ende bringen 0.25210009
(in) brand geraten 0.24819525
( ?UBER) B ?UHNE GEHEN 0.24644366
(um) prozent erho?hen 0.24058016
(auf) tisch legen 0.23264335
(auf) bu?hne stehen 0.23136641
(auf) idee kommen 0.23097735
(zu) ende gehen 0.20237252
(auf) spiel setzen 0.20112171
(IM) VORDERGRUND STEHEN 0.18957473
(IN) LEERE LAUFEN 0.18390151
(zu) opfer fallen 0.17724105
(in) gefahr geraten 0.17454816
(in) angriff nehmen 0.1643926
(auer) kontrolle geraten 0.16212899
(IN) HAND NEHMEN 0.15916243
(in) szene setzen 0.15766861
(ZU) SEITE STEHEN 0.14135151
(zu) geltung kommen 0.13119923
(in) geschichte eingehen 0.12458956
(aus) ruhe bringen 0.10973377
(zu) fall bringen 0.10900036
(zu) wehr setzen 0.10652383
(in) griff bekommen 0.10359659
(auf) tisch liegen 0.10011075
(IN) LICHTER SCHEINEN 0.08507655
(zu) sprache kommen 0.08503791
(IM) STICH LASSEN 0.0735844
(unter) beweis stellen 0.06064519
(IM) WEG STEHEN 0.05174435
(AUS) FUGEN GERATEN 0.05103952
(in) erinnerung bleiben 0.04339438
(ZU) WORT KOMMEN 0.03808749
(AUF) STRA?E GEHEN 0.03492515
(AUF) STRECKE BLEIBEN 0.03463844
(auer) kraft setzen 0.0338813
(AUF) WEG BRINGEN 0.03122951
(zu) erfolg fu?hren 0.02882997
(in) sicherheit bringen 0.02862914
(in) erfu?hlung gehen 0.01515792
(in) zeitung lesen 0.00354598
19
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 138?145,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Error Analysis of the TempEval Temporal Relation Identification Task
Chong Min Lee
Linguistics Department
Georgetown University
Washington, DC 20057, USA
cml54@georgetown.edu
Graham Katz
Linguistics Department
Georgetown University
Washington, DC 20057, USA
egk7@georgetown.edu
Abstract
The task to classify a temporal relation be-
tween temporal entities has proven to be dif-
ficult with unsatisfactory results of previous
research. In TempEval07 that was a first at-
tempt to standardize the task, six teams com-
peted with each other for three simple relation-
identification tasks and their results were com-
parably poor. In this paper we provide an anal-
ysis of the TempEval07 competition results,
identifying aspects of the tasks which pre-
sented the systems with particular challenges
and those that were accomplished with relative
ease.
1 Introduction
The automatic temporal interpretation of a text has
long been an important area computational linguis-
tics research (Bennett and Partee, 1972; Kamp and
Reyle, 1993). In recent years, with the advent of
the TimeML markup language (Pustejovsky et al,
2003) and the creation of the TimeBank resource
(Pustejovsky et al, 2003) interest has focussed on
the application of a variety of automatic techniques
to this task (Boguraev and Ando, 2005; Mani et al,
2006; Bramsen et al, 2006; Chambers et al, 2007;
Lee and Katz, 2008). The task of identifying the
events and times described in a text and classifying
the relations that hold among them has proven to be
difficult, however, with reported results for relation
classification tasks ranging in F-score from 0.52 to
0.60.
Variation in the specifics has made comparison
among research methods difficult, however. A first
attempt to standardize this task was the 2007 Tem-
pEval competition(Verhagen et al, 2007). This
competition provided a standardized training and
evaluation scheme for automatic temporal interpre-
tation systems. Systems were pitted against one an-
other on three simple relation-identification tasks.
The competing systems made use of a variety of
techniques but their results were comparable, but
poor, with average system performance on the tasks
ranging in F-score from 0.74 on the easiest task to
0.51 on the most difficult. In this paper we provide
an analysis of the TempEval 07 competition, identi-
fying aspects of the tasks which presented the sys-
tems with particular challenges and those that were
accomplished with relative ease.
2 TempEval
The TempEval competition consisted of three tasks,
each attempting to model an important subpart of the
task of general temporal interpretation of texts. Each
of these tasks involved identifying in running text
the temporal relationships that hold among events
and times referred to in the text.
? Task A was to identify the temporal relation
holding between an event expressions and a
temporal expression occurring in the same sen-
tence.
? Task B was to identify the temporal relations
holding between an event expressions and the
Document Creation Time (DCT) for the text.
? Task C was to identify which temporal relation
held between main events of described by sen-
138
tences adjacent in text.
For the competition, training and development
data?newswire files from the TimeBank corpus
(Pustejovsky et al, 2003) ?was made available in
which the events and temporal expressions of in-
terest were identified, and the gold-standard tempo-
ral relation was specified (a simplified set of tem-
poral relations was used: BEFORE, AFTER, OVER-
LAP, OVERLAP-OR-BEFORE,AFTER-OR-OVERLAP
and VAGUE.1). For evaluation, a set of newswire
texts was provided in which the event and temporal
expressions to be related were identified (with full
and annotated in TimeML markup) but the temporal
relations holding among them withheld. The task in
was to identify these relations.
The text below allows illustrates the features of
the TimeML markup that were made available as
part of the training texts and which will serve as the
basis for our analysis below:
<TIMEX3 tid="t13" type="DATE"
value="1989-11-02"
temporalFunction="false"
functionInDocument="CREATION TIME">11/02/89
</TIMEX3> <s> Italian chemical giant
Montedison S.p.A. <TIMEX3 tid="t19"
type="DATE" value="1989-11-01"
temporalFunction="true"
functionInDocument="NONE"
anchorTimeID="t13">yesterday</TIMEX3
<EVENT eid="e2" class="OCCURRENCE"
stem="offer" aspect="NONE"
tense="PAST" polarity="POS"
pos="NOUN">offered</EVENT>
$37-a-share for all the common shares
outstanding of Erbamont N.V.</s>
<s>Montedison <TIMEX3 tid="t17"
type="DATE" value="PRESENT REF"
temporalFunction="true"
functionInDocument="NONE"
anchorTimeID="t13">currently</TIMEX3>
<EVENT eid="e20" class="STATE"
stem="own" aspect="NONE"
tense="PRESENT" polarity="POS"
pos="VERB">owns</EVENT> about
72%of Erbamont?s common shares
outstanding.</s>
TimeML annotation associates with temporal ex-
pression and event expression identifiers (tid and
eid, respectively). Task A was to identify the tem-
poral relationships holding between time t19 and
event e2 and between t17 and e20 (OVERLAP was
1This contrasts with the 13 temporal relations supported by
TimeML. The full TimeML markup of event and temporal ex-
pressions was maintained.
Task A Task B Task C
CU-TMP 60.9 75.2 53.5
LCC-TE 57.4 71.3 54.7
NAIST 60.9 74.9 49.2
TimeBandits 58.6 72.5 54.3
WVALI 61.5 79.5 53.9
XRCE-T 24.9 57.4 42.2
average 54.0 71.8 51.3
Table 2: TempEval Accuracy (%)
the gold-standard answer for both). Task B was to
identify the relationship between the events and the
document creation time t13 (BEFORE for e2 and
OVERLAP for e20). Task C was to identify the
relationship between e2 and e20 (OVERLAP-OR-
BEFORE). The TempEval07 training data consisted
of a total of 162 document. This amounted to a total
of 1490 total relations for Task A, 2556 for task B,
and 1744 for Task C. The 20 documents of testing
data had 169 Task A relations, 337 Task B relations,
and 258 Task C relations. The distribution of items
by relation type in the training and test data is given
in Table 1.
Six teams participated in the TempEval compe-
tition. They made use of a variety of techniques,
from the application of off-the shelf machine learn-
ing tools to ?deep? NLP. As indicated in Table 22,
while the tasks varied in difficulty, within each task
the results of the teams were, for the most part, com-
parable.3
The systems (other than XRCE-T) did somewhat
to quite a bit better than baseline on the tasks.
Our focus here is on identifying features of the
task that gave rise to difficult, using overall per-
formance of the different systems as a metric. Of
the 764 test items, a large portion were either
?easy??meaning that all the systems provided cor-
rect output?or ?hard??meaning none did.
Task A Task B Task C
All systems correct 24 (14%) 160 (45%) 35 (14%)
No systems correct 33 (20%) 36 (11%) 40 (16%)
In task A, the cases (24/14%) that all participants
make correct prediction are when the target relation
is overlap. And, the part-of-speeches of most events
2TempEval was scored in a number of ways; we report accu-
racy of relation identification here as we will use this measure,
and ones related to it below
3The XRCE-T team, which made use of the deep analysis
engine XIP lightly modified for the competition, was a clear
outlier.
139
Task A Task B Task C
BEFORE 276(19%)/21(12%) 1588(62%)/186(56%) 434(25%)/59(23%)
AFTER 369(25%)/30(18%) 360(14%)/48(15%) 306(18%)/42(16%)
OVERLAP 742(50%)/97(57%) 487(19%)/81(25%) 732(42%)/122(47%)
BEFORE-OR-OVERLAP 32(2%)/2(1%) 47(2%)/8(2%) 66(4%)/12(5%)
OVERLAP-OR-AFTER 35(2%)/5(3%) 35(1%)/2(1%) 54(3%)/7(3%)
VAGUE 36(2%)/14(8%) 39(2%)/5(2%) 152(9%)/16(6%)
Table 1: Relation distribution of training/test sets
in the cases are verbs (19 cases), and their tenses are
past (13 cases). In task B, among 160 cases for that
every participant predicts correct temporal relation,
159 cases are verbs, 122 cases have before as target
relation, and 112 cases are simple past tenses. In
task C, we find that 22 cases among 35 cases are
reporting:reporting with overlap as target relation.
In what follows we will identify aspects of the tasks
that make some items difficult and some not so much
so.
3 Analysis
In order to make fine-grained distinctions and to
compare arbitrary classes of items, our analysis will
be stated in terms of a summary statistic: the success
measure (SM).
(1) Success measure
?
k=0 6kCk
6(
?
k=0 6Ck )
where Ck is the number of items k systems got
correct. This simply the proportion of total correct
responses to items in a class (for all systems) divided
by the total number of items in that class (a success
measure of 1.0 is easy and of 0.0 is hard). For exam-
ple, let?s suppose before relation have 10 instances.
Among the instances, three cases are correct by all
teams, four by three teams, two by two teams, and
one by no teams. Then, SM of before relation is
0.567 ( (3?6)+(4?3)+(2?2)+(1?0)6?(1+2+4+3) ).
In addition, we would like to keep track of how
important each class of errors is to the total evalu-
ation. To indicate this, we compute the error pro-
portion (ER) for each class: the proportion of total
errors attributable to that class.
(2) Error proportion
?
k=0 6 (6? k)Ck
AllErrorsInTask ?NumberOfTeams
TaskA TaskB TaskC
BEFORE 0.26/21% 0.89/23% 0.47/25%
AFTER 0.42/24% 0.56/23% 0.48/17%
OVERLAP 0.75/33% 0.56/39% 0.68/31%
BEFORE-OR-OVERLAP 0.08/9% 0/3% 0.06/9%
OVERLAP-OR-AFTER 0.03/2% 0/1% 0.10/5%
VAGUE 0/19% 0/5% 0.02/12%
Table 3: Overall performance by relation type (SM/ER)
When a case shows high SM and high ER, we can
guess that the case has lots of instances. With low
SM and low ER, it says there is little instances. With
high SM and low ER, we don?t need to focus on the
case because the case show very good performance.
Of particular interest are classes in which the SM is
low and the ER is high because it has a room for the
improvement.
3.1 Overall analysis
Table 3 provides the overall analysis by relation
type. This shows that (as might be expected) the
systems did best on the relations that were the ma-
jority class for each task: overlap in Task A, before
in Task B, and overlap in Task C.
Furthermore systems do poorly on all of the dis-
junctive classes, with this accounting for between
1% and 9% of the task error. In what follows we will
ignore the disjunctive relations. Performance on the
before relation is low for Task A but very good for
Task B and moderate for Task C. For more detailed
analysis we treat each task separately.
3.2 Task A
For Task A we analyze the results with respect to the
attribute information of the EVENT and TIMEX3
TimeML tags. These are the event class (aspectual,
i action, i state, occurrence, perception, reporting,
and state)4 part-of-speech (basically noun and verb),
4The detailed explanations on the event classes
can be found in the TimeML annotation guideline at
140
NOUN VERB
BEFORE 0/5% 0.324/15%
AFTER 0.119/8% 0.507/15%
OVERLAP 0.771/7% 0.747/24%
VAGUE 0/8% 0/10%
Table 4: POS of EVENT in Task A
and tense&aspect marking for event expressions. In-
formation about the temporal expression turned out
not to be a relevant dimension of analysis.
As we seen in Table 4, verbal event expressions
make for easier classification for before and after
(there is a 75%/25% verb/noun split in the data).
When the target relation is overlap, nouns and verbs
have similar SMs.
One reason for this difference, of course, is
that verbal event expressions have tense and aspect
marking (the tense and aspect marking for nouns is
simply none).
In Table 5 we show the detailed error analy-
sis with respect to tense and aspect values of the
event expression. The combination of tense and
aspect values of verbs generates 10 possible val-
ues: future, infinitive, past, past-perfective, past-
progressive (pastprog), past-participle (pastpart),
present, present-perfective (presperf), present-
progressive (presprog), and present-participle (pres-
part). Among them, only five cases (infinitive, past,
present, presperf, and prespart) have more than 2
examples in test data. Past takes the biggest por-
tions (40%) in test data and in errors (33%). Over-
lap seems less influenced with the values of tense
and aspect than before and after when the five cases
are considered. Before and after show 0.444 and
0.278 differences between infinitive and present and
between infinitive and present. But, overlap scores
0.136 differences between present and past. And a
problem case is before with past tense that shows
0.317 SM and 9% EP.
When we consider simultaneously SM and EP of
the semantic class of events in Table 6, we can find
three noticeable cases: occurrence and reporting of
before, and occurrence of after. All of them have
over 5% EP and under 0.4 SM. In case of reporting
of after, its SM is over 0.5 but its EP shows some
room for the improvement.
http://www.timeml.org/.
BEFORE AFTER OVERLAP VAGUE
FUTURE 0/0% 0.333/1% 0.833/0% 0/0%
INFINITIVE 0/3% 0.333/3% 0.667/2% 0/1%
NONE 0/5% 0.119/8% 0.765/7% 0/8%
PAST 0.317/9% 0.544/9% 0.782/10% 0/5%
PASTPERF 0/0% 0.333/1% 0.833/0% 0/0%
PASTPROG 0/0% 0/0% 0.500/1% 0/0%
PRESENT 0.444/2% 0.611/2% 0.646/4% 0/1%
PRESPERF 0.833/0% 0/0% 0.690/3% 0/0%
PRESPROG 0/0% 0/0% 0.833/0% 0/0%
PRESPART 0/0% 0/0% 0.774/4% 0/1%
Table 5: Tense & Aspect of EVENT in Task A
? 4 ? 16 > 16
BEFORE 0/1% 0.322/13% 0.133/6%
AFTER 0.306/5% 0.422/13% 0.500/5%
OVERLAP 0.846/10% 0.654/17% 0.619/3%
VAGUE 0/0% 0/5% 0/13%
Table 7: Distance in Task A
Boguraev and Ando (2005) report a slight in-
crease in performance in relation identification
based on proximity of the event expression to the
temporal expression. We investigated this in Table 7,
looking at the distance in word tokens.
We can see noticeable cases in before and after of
? 16 row. Both cases show over 13% EP and under
0.5 SM. The participants show good SM in overlap
of ? 4. Overlap of ? 16 has the biggest EP (17%).
When its less satisfactory SM (0.654) is considered,
it seems to have a room for the improvement. One of
the cases that have 13% EP is vague of ? 16. It says
that it is difficult even for humans to make a decision
on a temporal relation when the distance between an
event and a temporal expression is greater than and
equal to 16 words.
3.3 Task B
Task B is to identify a temporal relation between an
EVENT and DCT. We analyze the participants per-
formance with part-of-speech. This analysis shows
how poor the participants are on after and overlap of
nouns (0.167 and 0.115 SM). And the EM of over-
lap of verbs (26%) shows that the improvement is
needed on it.
In test data, occurrence and reporting have simi-
lar number of examples: 135 (41%) and 106 (32%)
in 330 examples. In spite of the similar distribu-
tion, their error rates show difference. It suggests
that reporting is easier than occurrence. Moreover,
141
ASPECTUAL I ACTION I STATE OCCURRENCE PERCEPTION REPORTING STATE
BEFORE 0.167/1% 0/0% 0.333/3% 0.067/6% 0/0% 0.364/9% 0/1%
AFTER 0.111/3% 0/0% 0/0% 0.317/9% 0/0% 0.578/8% 0.167/2%
OVERLAP 0.917/0% 0.778/1% 0.583/3% 0.787/15% 0.750/1% 0.667/9% 0.815/2%
VAGUE 0/1% 0/1% 0/0% 0/9% 0/0% 0/6% 0/0%
Table 6: EVENT Class in Task A
ASPECTUAL I ACTION I STATE OCCURRENCE PERCEPTION REPORTING STATE
BEFORE 1/0% 0.905/1% 0.875/1% 0.818/13% 0.556/1% 0.949/5% 0.750/1%
AFTER 0.500/3% 0.500/1% 0/0% 0.578/15% 0.778/1% 0.333/1% 0.444/2%
OVERLAP 0.625/2% 0.405/5% 0.927/1% 0.367/17% 0.500/1% 0.542/6% 0.567/7%
VAGUE 0/1% 0/0% 0/0% 0/4% 0/0% 0/0% 0/0%
Table 9: EVENT Class in Task B
NOUN VERB
BEFORE 0.735/6% 0.908/16%
AFTER 0.167/8% 0.667/14%
OVERLAP 0.115/13% 0.645/26%
VAGUE 0/4% 0/1%
Table 8: POS of EVENT in Task B
Table 9 shows most errors in after occur with oc-
currence class 65% (15%/23%) when we consider
23% EP in Table 3. Occurrence and reporting of be-
fore show noticeably good performance (0.818 and
0.949). And occurrence of overlap has the biggest
error rate (17%) with 0.367 of SM.
In case of state, it has 22 examples (7%) but takes
10% of errors. And it is interesting that the most
errors are concentrated in state. In our intuition, it
is not a difficult task to identify overlap relation of
state class.
Table 9 does not clearly show what causes the
poor performance of nouns in after and overlap.
In the additional analysis of nouns with class in-
formation, occurrence shows poor performance in
after and overlap: 0.111/6% and 0.083/8%. And
other noticeable case in nouns is state of overlap:
0.125/4%. We can see the low performance of nouns
in overlap is due to the poor performance of state
and occurrence, but only occurrence is a cause of
the poor performance in after.
DCT can be considered as speech time. Then,
tense and aspect of verb events can be a cue in pre-
dicting temporal relations between verb events and
DCT. The better performance of the participants in
verbs can be an indirect evidence. The analysis with
tense & aspect can tell us which tense & aspect in-
formation is more useful. A problem with the in-
formation is sparsity. Most cases appear less than
3 times. The cases that have more than or equal
to three instances are 13 cases among the possible
combinations of 7 tenses and 4 aspects in TimeML.
Moreover, only two cases are over 5% of the whole
data: past with before (45%) and present with over-
lap (15%). In Table 10, tense and aspect information
seems valuable in judging a relation between a verb
event and DCT. The participants show good perfor-
mances in the cases that seem easy intuitively: past
with before, future with after, and present with over-
lap. Among intuitively obvious cases that are past,
present, or future tense, present tense makes large
errors (20% of verb errors). And present shows 7%
EP in before.
When events has no cue to infer a relation like
infinitive, none, pastpart, and prespart, their SMs
are lower than 0.500 except infinitive and none of
after. infinitive of overlap shows poor performance
with the biggest error rate (0.125/12%).
3.4 Task C
The task is to identify the relation between consec-
utive main events. There are four part-of-speeches
in Task C: adjective, noun, other, and verb. Among
eight possible pairs of part-of-speeches, only three
pairs have over 1% in 258 TLINKs: noun and verb
(4%), verb and noun (4%), and verb and verb (85%).
When we see the distribution of verb and verb by
three relations (before, after, and overlap), the rela-
tions show 19%, 14%, and 41% distribution each.
In Table 11, the best SM is verb:verb of overlap
(0.690). And verb:verb shows around 0.5 SM in be-
fore and after.
Tense & aspect pairs of main event pairs show
142
BEFORE AFTER OVERLAP VAGUE
FUTURE 0/0% 0.963/1% 0.333/2% 0/0%
FUTURE-PROGRESSIVE 0/0% 0/0% 0.167/1% 0/0%
INFINITIVE 0.367/5% 0.621/7% 0.125/12% 0/2%
NONE 0/0% 0.653/7% 0/2% 0/0%
PAST 0.984/3% 0.333/1% 0.083/3% 0/0%
PASTPERF 1.000/0% 0/0% 0/0% 0/0%
PASTPROG 1.000/0% 0/0% 0/0% 0/0%
PASTPART 0.583/1% 0/0% 0/0% 0/0%
PRESENT 0.429/7% 0.167/3% 0.850/10% 0/0%
PRESPERP 0.861/3% 0/0% 0/2% 0/0%
PRESENT-PROGRESIVE 0/0% 0/0% 0.967/0% 0/0%
PRESPART 0/0% 0.444/3% 0.310/8% 0/0%
Table 10: Tense & Aspect of EVENT in Task B
BEFORE AFTER OVERLAP VAGUE
NOUN:VERB 0.250/2% 0/0% 0.625/1% 0/0%
VERB:NOUN 0.583/1% 0.500/2% 0.333/1% 0/1%
VERB:VERB 0.500/20% 0.491/15% 0.690/26% 0.220/12%
Table 11: POS pairs in Task C
skewed distribution, too. The cases that have
over 1% data are eight: past:none, past:past,
past:present, present:past, present:present,
present:past, presperf:present, and pres-
perf:presperf. Among them, past tense pairs
show the biggest portion (40%). The performance
of the eight cases is reported in Table 12. As we can
guess with the distribution of tense&aspect, most
errors are from past:past (40%). When the target re-
lation of past:past is overlap, the participants show
reasonable SM (0.723). But, their performances are
unsatisfactory in before and after.
When we consider cases over 1% of test data in
main event class pairs, we can see eleven cases as
Table 13. Among the eleven cases, four pairs have
over 5% data: occurrence:occurrence (13%), occur-
rence:reporting (14%), reporting:occurrence (9%),
and reporting:reporting (17%). Reporting:reporting
shows the best performance (0.934/2%) in over-
lap. Two class pairs have over 10% EP: occur-
rence:occurrence (15%), and occurrence:reporting
(14%). In addition, occurrence pairs seem difficult
tasks when target relations are before and after be-
cause they show low SMs (0.317 and 0.200) with
5% and 3% error rates.
4 Discussion and Conclusion
Our analysis shows that the participants have the dif-
ficulty in predicting a relation of a noun event when
its target relation is before and after in Task A, and
after and overlap in Task B. When the distance is in
the range from 5 to 16 in Task A, more effort seems
to be needed.
In Task B, tense and aspect information seems
valuable. Six teams show good performance when
simple tenses such as past, present, and future ap-
pear with intuitively relevant target relations such as
before, overlap, and after. Their poor performance
with none and infinitive tenses, and nouns can be an-
other indirect evidence.
A difficulty in analyzing Task C is sparsity. So,
this analysis is focused on verb:verb pair. When we
can see in (12), past pairs still show the margin for
the improvement. But, a lot of reporting events are
used as main events. When we consider that im-
portant events in news paper are cited, the current
TempEval task can miss useful information.
Six participants make very little correct predic-
tions on before-or-overlap, overlap-or-after, and
vague. A reason on the poor prediction can be small
distribution in the training data as we can see in Ta-
ble 1. Data sparsity problem is a bottleneck in nat-
ural language processing. The addition of the dis-
junctive relations and vague to the target labels can
make the sparsity problem worse. When we con-
sider the participants? poor performance on the la-
bels, we suggest to use three labels (before, overlap,
and after) as the target labels.
143
BEFORE AFTER OVERLAP VAGUE
PAST:NONE 0.750/1% 0.167/1% 0.167/3% 0/0%
PAST:PAST 0.451/12% 0.429/10% 0.723/11% 0.037/7%
PAST:PRESENT 0.667/1% 0/0% 0.708/2% 0/0%
PRESENT:PAST 0/0% 0.292/2% 0.619/2% 0/1%
PRESENT:PRESENT 0.056/2% 0/0% 0.939/1% 0/1%
PRESPERF:PAST 0.500/0% 0/0% 0.542/1% 0/0%
PRESPERF:PRESENT 0/1% 0/0% 0.583/1% 0/0%
PRESPERF:PRESPERF 0/0% 0/0% 0.600/2% 0/0%
Table 12: Tense&Aspect Performance in Task C
BEFORE AFTER OVERLAP VAGUE
I ACTION:OCCURRENCE 0.524/1% 0.400/2% 0.500/1% 0/0%
I STATE:OCCURRENCE 0.250/1% 0.500/1% 0.833/0% 0/0%
I STATE:ASPECTUAL 0/0% 0.333/1% 0.500/0% 0/0%
OCCURRENCE:I ACTION 0.583/1% 0.417/1% 0.300/3% 0/0%
OCCURRENCE:OCCURRENCE 0.317/5% 0.200/3% 0.600/5% 0/2%
OCCURRENCE:REPORTING 0.569/4% 0.367/3% 0.594/5% 0.111/2%
OCCURRENCE:STATE 0.333/1% 0/0% 0.583/1% 0/0%
REPORTING:I STATE 0.167/1% 0.583/1% 0.867/1% 0/0%
REPORTING:OCCURRENCE 0.625/1% 0.611/3% 0.542/3 0/2%
REPORTING:REPORTING 0.167/1% 0.167/2% 0.934/2% 0/4%
Table 13: Event class in Task C
Our analysis can be used as a cue in adding an
additional module for weak points. When a pair of
a noun event and a temporal expression appears in a
sentence, a module can be added based on our study.
References
Branimir Boguraev and Rie Kubota Ando. 2005.
TimeML-Compliant Text Analysis for Temporal Rea-
soning. Preceedings of IJCAI-05, 997?1003.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew
See, David Day, Lisa Ferro, Robert Gaizauska, Marcia
Lazo, Andrea Setzer, and Beth Sundheim. 2003. The
TIMEBANK corpus. Proceedings of Corpus Linguis-
tics 2003, 647?656.
Michael Bennett and Barbara Partee. 1972. Toward the
logic of tense and aspect in English. Technical report,
System Development Corporation. Santa Monica, CA
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Inducing Temporal
Graphs Proceedings of EMNLP 2006, 189?198.
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying Temporal Relations Between
Events Proceedings of ACL 2007, 173?176.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine Learn-
ing of Temporal Relations. Proceedings of ACL-2006,
753?760.
Chong Min Lee and Graham Katz. 2008. Toward an
Automated Time-Event Anchoring System. The Fifth
Midwest Computational Linguistics Colloquium.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic: Introduction to modeltheoretic semantics of
natural language. Kluwer Academic, Boston.
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2003. TimeML: Robust Specification of Event
and Temporal Expressions in Text. IWCS-5, Fifth In-
ternational Workshop on Computational Semantics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, James Pustejovsky.
2007. SemEval-2007 Task 15: TempEval Tempo-
ral Relation Identification. Proceedings of SemEval-
2007, 75?80.
Caroline Hage`ge and Xavier Tannier. 2007 XRCE-T:
XIP Temporal Module for TempEval campaign. Pro-
ceedings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), 492?495.
Steven Bethard and James H. Martin. 2007. CU-TMP:
Temporal Relation Classification Using Syntactic and
Semantic Features. Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), 129?132.
Congmin Min, Munirathnam Srikanth, and Abraham
Fowler. 2007. LCC-TE: A Hybrid Approach to Tem-
poral Relation Identification in News Text. Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), 219?222.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. NAIST.Japan: Temporal Relation
144
Identification Using Dependency Parsed Tree. Pro-
ceedings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), 245?248.
Georgiana Pus?cas?u. 2007. WVALI: Temporal Relation
Identification by Syntactico-Semantic Analysis Pro-
ceedings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), 484?487.
Mark Hepple, Andrea Setzer, and Robert Gaizauskas.
2007. USFD: Preliminary Exploration of Features
and Classifiers for the TempEval-2007 Task. Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), 438?441.
145
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 223?227,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Extracting and modeling durations for habits and events from Twitter
Jennifer Williams Graham Katz
Department of Linguistics Department of Linguistics
Georgetown University Georgetown University
Washington, D.C., USA Washington, D.C., USA
jaw97@georgetown.edu egk7@georgetown.edu
Abstract
We seek to automatically estimate typical 
durations for  events  and  habits  described 
in Twitter tweets.  A corpus of more than 
14 million tweets containing temporal du-
ration  information  was  collected.  These 
tweets were classified as to their habituality 
status  using a bootstrapped, decision tree. 
For each verb lemma,  associated duration 
information was collected for episodic and 
habitual uses of the verb. Summary statis-
tics for  483 verb lemmas and their typical 
habit and episode durations has been com-
piled and made available.  This automati-
cally  generated  duration  information  is 
broadly comparable to hand-annotation.
1 Introduction
Implicit  information  about  temporal  durations  is 
crucial to any natural language processing task in-
volving  temporal  understanding  and  reasoning. 
This  information  comes  in  many  forms,  among 
them knowledge about typical durations for events 
and  knowledge  about  typical  times  at  which  an 
event occurs. We know that lunch lasts for half an 
hour  to  an  hour  and takes  place  around noon,  a 
game of chess lasts from a few minutes to a few 
hours and can occur any time, and so when we in-
terpret a text such as ?After they ate lunch, they 
played a game of chess and then went to the zoo? 
we can infer that the zoo visit probably took place 
in the early afternoon. In this paper we focus on 
duration. Hand-annotation of event durations is ex-
pensive slow (Pan et al, 2011), so it is desirable to 
automatically determine typical durations. This pa-
per describes a method for automatically extracting 
information about typical durations for events from 
tweets posted to the Twitter microblogging site.
Twitter is a rich resource for information about 
everyday events ? people post their tweets to Twit-
ter publicly in real-time as they conduct their activ-
ities throughout the day, resulting in a significant 
amount  of  mundane  information  about  common 
events. For example, (1) and (2) were used to pro-
vide information about how long a work event can 
last:
(1) Had  work for an hour and 30 mins now 
going to disneyland with my cousins :)
(2) I play in a loud rock band, I  worked at a 
night  club for  two years.  My ears  have  
never  hurt  so  much  @melaniemarnie  
@giorossi88 @CharlieHi11 
In this paper, we sought to use this kind informa-
tion to  determine likely durations  for  events  and 
habits  of  a  variety  of  verbs.  This  involved  two 
steps: extracting a wide range of tweets such as (1) 
and (2) and classifying these as to whether they re-
ferred to specific event (as in (1)) or a general habit 
(as in (2)), then summarizing the duration informa-
tion associated with each kind of use of a given 
verb.
This paper answers two investigative questions:
? How  well  can  we  automatically  extract 
fine-grain  duration information  for events 
and habits from Twitter?
? Can we effectively distinguish episode and 
habit duration distributions ?
The results presented here show that Twitter can be 
mined  for  fine-grain  event  duration  information 
223
with high precision using regular expressions. Ad-
ditionally, verb uses can be effectively categorized 
as  to  their  habituality,  and  duration  information 
plays an important role in this categorization. 
2 Prior Work
Past research on typical durations has made use of 
standard  corpora  with  texts  from  literature  ex-
cerpts, news stories, and full-length weblogs (Pan 
et al 2006;  2007;  2011; Kozareva & Hovy, 2011; 
Gusev et al, 2011). For example, Pan et al (2011) 
hand-annotated of  a  portion  of  the  TIMEBANK 
corpus that consisted of Wall Street  Journal  arti-
cles. For 58 non-financial articles, they annotated 
over 2,200 events with typical temporal duration, 
specifying the upper and lower bounds for the du-
ration of  each event.  In  addition they  used their 
corpus to automatically determine event durations 
with machine learning,  predicting features  of  the 
duration on the basis of the verb lemma, local tex-
tual  context.  and  other  information.  Their best 
(SVM) classifier  achieved  precision of 78.2% on 
the course-grained task of determining whether an 
event's duration was longer or shorter than one day 
(compared with 87.7% human agreement). For de-
termining the fine-grained task of determining the 
most  likely  temporal  unit?second,  minute,  hour, 
day,  week,  etc.?achieved  67.9%  (human  agree-
ment: 79.8%). This shows that lexical information 
can be effectively  leveraged for  duration predic-
tion.
To compile temporal duration information for a 
wider range of verbs, Gusev et al (2011) explored 
an automatic Web-based query method for harvest-
ing typical durations of events. Their data consist-
ed of search engine ?hit-counts? and they analyzed 
the distribution of durations associated with  1000 
frequent verbs in terms of whether the  event lasts 
for more or less than a day (course-grain task) or 
whether it lasts for seconds, minutes, hours, days, 
weeks,  months,  or  years  (fine-grain  task).  They 
note that many verbs have a two-peaked distribu-
tion and they suggest that the two-peaked distribu-
tion could be a result  of the usage referring to a 
habit or a single episode. (When used with a dura-
tion marker,  run,  for example, is used about 15% 
of the time with hour-scale and 38% with year-s-
cale duration markers). Rather than making a dis-
tinction between habits and episodes in their data, 
they apply a heuristic to focus on episodes only. 
Kozareva and Hovy (2011) also collected typi-
cal durations of events using Web query patterns. 
They proposed a six-way classification of ways in 
which events are related to time, but provided only 
programmatic analyses of a few verbs using We-
b-based  query  patterns.  They  have  proposed  a 
compilation  of  the  5,000  most  common  verbs 
along with their typical temporal durations. In each 
of  these  efforts,  automatically  collecting  a  large 
amount of reliable to cover a wide range of verbs 
has been noted as a difficulty. It is this task that we 
seek to take up.
3 Corpus Methodology
Our goal was to discover the duration distribution 
as well as typical habit and typical episode dura-
tions for each verb lemma that we found in our col-
lection.  A wide range of factors influence typical 
event durations. Among these are the character of a 
verb's arguments, the presence of negation and oth-
er embedding features. For this preliminary work, 
we ignored the effects of arguments, and focused 
only on generating duration information for verb 
lemmas. Also, tweets that were negated, condition-
al tweets, and tweets in the future tense  were put 
aside.
3.1 Data Collection
A corpus of tweets was collected from the Twitter 
web  service  API  using  an  open-source module 
called  Tweetstream  (Halvorsen  &  Schierkolk, 
2010). Tweets were collected that contained refer-
ence to  a  temporal  duration.  The data  collection 
task began on February 1, 2011 and ended on Sep-
tember 28, 2011.  Duplicate tweets were identified 
by their unique tweet ID provided by Twitter, and 
were  removed from the data set. Also tweets that 
were marked by Twitter as 'retweets' (tweets that 
have been reposted to Twitter) were removed. The 
following query terms (denoting temporal duration 
measure) were used to filter the Twitter stream for 
tweets containing temporal duration:
second,  seconds,  minute,  minutes,  hour,  
hours,  day,  days,  week,  weeks,  month,  
months, year, years, decade, decades, cen-
tury,  centuries,  sec,  secs,  min,  mins,  hr,  
hrs, wk, wks, yr, yrs
The number of tweets in  the  resulting  corpus was 
14,801,607 and the total number of words in the 
224
corpus was 224,623,447. Tweets were normalized, 
tokenized,  and  then  tagged  for  POS,  using  the 
NLTK Treebank Tagger (Bird & Loper, 2004). 
3.2 Extraction Frames
To associate each temporal duration with its event, 
events and durations were identified and extracted 
using  four  types  of  regular  expression  extraction 
frames.  The  patterns  applied  a heuristic  to  asso-
ciate each verb with a temporal expression, similar 
to  the  extraction  frames used  in  Gusev  et  al. 
(2011). The four types of extraction frames were:
? verb for duration
? verb in duration
? spend duration verbing 
? takes duration to verb 
where verb is the target verb and duration is a du-
ration-measure term. In (3), for example,  the verb 
work is associated with the temporal duration term 
44 years.
(3) Retired watchmaker worked for 44 years 
without a telephone, to avoid unnecessary  
interruptions, http://t.co/ox3mB6g
These four extraction frame types were also varied 
to  include different  tenses,  different  grammatical 
aspects,  and  optional  verb  arguments to  reach  a 
wide  range  of  event  mentions  and  ordering  be-
tween the verb and the duration clause. For each 
matched tweet  a  feature  vector  was created with 
the  following  features:  verb  lemma,  temporal 
bucket  (seconds,  minutes,  hours,  weeks,  days, 
months or years), tense (past or present), grammat-
ical aspect (simple, progressive, or perfect), dura-
tion in seconds, and the extraction frame type (for, 
in, spend, or take). For example, the features ex-
tracted from (3) were: 
[work, years, past, simple, 1387584000, FOR]
Tweets with verbal lemmas that occur fewer than 
100 times in the extracted corpus were filtered out. 
The  resulting  data  set contained  390,562 feature 
vectors covering 483 verb lemmas.
3.3 Extraction Precision
Extraction frame performance was estimated using 
precision on a random sample of 400 hand-labeled 
tweets. Each instance in the sample was labeled as 
correct if the extracted feature vector was correct 
in its entirety. The overall precision for extraction 
frames was estimated as 90.25%, calculated using 
a  two-tailed t-test  for  sample size  of  proportions 
with 95% confidence (p=0.05, n=400). 
3.4 Duration Results 
In order to summarize information about dura-
tion for each of the 483 verb lemmas, we calculat-
ed the frequency distribution of tweets by duration 
in seconds. This distribution can be represented in 
histogram form, as in Figure 1 for the verb lemma 
search,  with with bins corresponding to temporal 
units of measure (seconds, minutes, etc.). 
Figure 1: Frequency distribution for search 
This  histogram  shows the  characteristic  bi-
modal-distributions noted  by Pan et al, (2011) and 
Gusev et. al., (2011), an issue taken up in the next 
section.
4 Episodic/Habitual Classification 
Most verbs have both episodic and habitual uses, 
which clearly correspond to different typical dura-
tions. In order to draw this distinction we built a 
system to automatically classify our tweets  as to 
their  habituality.  The  extracted  feature  vectors 
were used in a machine learning task to label each 
tweet  in the collection as denoting a habit  or  an 
episode, broadly following Mathew & Katz (2009). 
This classification was done with bootstrapping, in 
a partially supervised manner.
4.1 Bootstrapping Classifier
First, a random sample of 1000 tweets from the ex-
tracted  corpus  was  hand-labeled  as  being  either 
225
habit  or  episode (236 habits;  764 episodes).  The 
extracted  feature  vectors  for  these  tweets  were 
used to train a C4.5 decision tree classifier (Hall et 
al., 2009). This classifier achieved an accuracy of 
83.6% during training. We used this classifier and 
the hand-labeled set to seed the generic Yarowsky 
Algorithm  (Abney,  2004), iteratively  inducing  a 
habit or episode label for all the tweets in the col-
lection,  using  the  WEKA output  for  confidence 
scoring and a confidence threshold of 0.96. 
The extracted corpus was classified into 94,643 
habitual tweets and  295,918 episodic tweets.  To 
estimate  the  accuracy  of  the  classifier,  400  ran-
domly  chosen  tweets  from  the  extracted  corpus 
were hand-labeled, giving an estimated accuracy of 
85% accuracy with 95% confidence, using the two-
tailed t-test for sample size of proportions (p=0.05, 
n=400).
4.2 Results 
Clearly the data in Figure 1 represents two com-
bined distributions: one for episodes and one for 
habits, as we illustrate in Figure 2. We see that the 
verb search describes episodes that most often last 
minutes or hours, while it describes habits that go 
on for years. 
Figure 2: Duration distribution for search
These two different uses are illustrated in (4) and 
(5). 
(4) Obviously I'm the one who found the tiny  
lost black Lego in 30 seconds after the 3 of  
them searched for 5 minutes. 
(5) @jaynecheeseman they've been searching  
for you for 11 years now. I'd look out if I  
were you.
In Table  1  we provide  summary information for 
several verb  lemmas, indicating the average dura-
tion  for  each  verb  and  the  temporal  unit  corre-
sponding to the largest bin for each verb.
Verb 
 Episodic Use  Habitual Use
Modal 
bin Mean
Modal 
bin Mean
snooze minutes 1.6 hrs decades 7.5 yrs
coach hours 10 days years 8.5 yrs
approve minutes 1.7 mon. years 1.4 yrs
eat minutes 5.3 wks days 5.7 yrs
kiss seconds 4.5 days weeks 1.8 yrs
visit weeks 7.2 wks. years 4.9 yrs
Table 1. Mean duration and mode for 6 of the verbs 
It is clear that the methodology  overestimates the 
duration of episodes somewhat ?  our estimates of 
typical durations are 2-3 times as long as those that 
come from the annotation in  Pan,  et.  al.  (2009). 
Nevertheless, the modal bin corresponds approxi-
mately to that the hand annotation in Pan, et. al., 
(2011) for nearly half (45%) of the verbs lemmas.
5 Conclusion
We have presented a hybrid approach for extract-
ing typical  durations  of  habits  and episodes.  We 
are able to extract high-quality information about 
temporal  durations  and  to  effectively  classify 
tweets as to their habituality. It is clear that Twitter 
tweets contain a lot of unique data about different 
kinds of events and habits, and mining this data for 
temporal duration information has turned out to be 
a fruitful avenue for collecting the kind of world-
knowledge that we need for robust temporal lan-
guage processing. Our verb lexicon is available at: 
https://sites.google.com/site/relinguistics/.
226
References 
Steven Abney. 2004. ?Understanding the Yarowsky Al-
gorithm?. Computational Linguistics 30(3): 365-395.
Steven Bird and Edward Loper. 2004. NLTK: The natu-
ral language toolkit.  In Proceedings of 42nd Annual  
Meeting  of  the  Association for  Computational  Lin-
guistics (ACL-04).
Andrey  Gusev,  Nathaniel  Chambers,  Pranav  Khaitan, 
Divye Khilnani, Steven Bethard, and Dan Jurafsky. 
2011. ?Using query patterns to learn the durations of 
events?. IEEE IWCS-2011, 9th International Confer-
ence on Web Services. Oxford, UK 2011.
Mark  Hall,  Eibe  Frank,  Geoffrey  Holmes,  Bernhard 
Pfahringer,  Peter  Reutemann,  and  Ian  H.  Witten. 
2009.  The WEKA  Data  Mining Software:  An Up-
date; SIGKDD Explorations, Volume 11, Issue 1.
Rune  Halvorsen,  and Christopher  Schierkolk.  2010. 
Tweetstream:  Simple  Twitter  Streaming  API  (Ver-
sion  0.3.5)  [Software].  Available  from  https://bit-
bucket.org/runeh/tweetstream/src/.
Jerry Hobbs and James Pustejovsky. 2003. ?Annotating 
and reasoning about time and events?.  In Proceed-
ings of the AAAI Spring Symposium on Logical For-
mulation of Commonsense Reasoning. Stanford Uni-
versity, CA 2003.
Zornitsa  Kozareva  and Eduard  Hovy. 2011. ?Learning 
Temporal  Information  for  States  and  Events?.  In 
Proceedings of  the Workshop on Semantic Annota-
tion for Computational Linguistic Resources (ICSC  
2011), Stanford.
Thomas  Mathew and  Graham  Katz. 2009. ?Supervised 
Categorization of Habitual and Episodic Sentences?. 
Sixth  Midwest  Computational  Linguistics  Colloqui-
um. Bloomington, Indiana: Indiana University. 
Marc Moens and Mark Steedman. 1988. ?Temporal On-
tology  and  Temporal  Reference?.  Computational  
Linguistics 14(2):15-28.
Feng  Pan,  Rutu  Mulkar-Mehta,  and  Jerry  R.  Hobbs. 
2006. ?An Annotated Corpus of Typical Durations of 
Events?.  In Proceedings  of  the  Fifth  International  
Conference on Language Resources and Evaluation 
(LREC), 77-82. Genoa, Italy.
Feng  Pan,  Rutu  Mulkar-Mehta,  and  Jerry  R.  Hobbs. 
2011. "Annotating and Learning Event Durations in 
Text." Computational Linguistics 37(4):727-752.
227
The Exploitation of Spatial Information in Narrative Discourse
Blake Stephen Howald
Georgetown University
bsh25@georgetown.edu
E. Graham Katz
Georgetown University
egk7@georgetown.edu
Abstract
We present the results of several machine learning tasks that exploit explicit spatial language
to classify rhetorical relations and the spatial information of narrative events. Three corpora are
annotated with figure and ground (granularity) relationships, mereotopologically classified verbs
and prepositions, and frames of reference. For rhetorical relations, Na??ve Bayesian models achieve
84.90% and 57.87% accuracy in classifying NARRATION and BACKGROUND / ELABORATION re-
lations respectively (16% and 23% above baseline). For the spatial information of narrative events,
K* models achieve 55.68% average accuracy (12% above baseline) for all spatial information types.
This result is boosted to 71.85% (28% above baseline) when inertial spatial reference and text se-
quence information are considered. Overall, spatial information is shown to be central to narrative
discourse structure and prediction tasks.
1 Introduction
Clauses in discourse are related to one another in a number of semantic and pragmatic ways. Some of the
most prominent are temporal relations that hold among the times of events and states described (Partee,
1984; Pustejovsky et al, 2003) and the rhetorical relations that hold between a pair of clauses (Mann
and Thompson, 1987; Asher and Lascarides, 2003). For example, (1) illustrates the NARRATION relation
which obtains between (1a-b) and between (1b-c).
(1) a. Klose was sitting with his teammates.
b. He walked to the sidelines.
c. Then he entered the game.
Because of the temporal properties of NARRATION (Asher and Lascarides 2003, p. 462), the event
described in (1a) is taken to precede that described in (1b) and (1b)?s event to precede (1c)?s. As Asher
and Lascarides show, there is a close tie between the rhetorical structure of a discourse and its temporal
structure. In (2), for example, the fact that the clauses are related by ELABORATION entails that the
temporal relation between (2a) and (2b) is inclusion.
(2) a. Klose scored a goal.
b. He headed the ball into the upper corner.
We observe that the spatial relations among the locations of the events described in these discourses
are also highly determined by the rhetorical relations between the clauses used to describe them. In
the NARRATION-related discourse (1), there is a spatial progression: Klose is located relative to his
teammates (1a), he then moves from the bench to the sidelines (1b), and then he moves from the sidelines
into the game (1c). In the ELABORATION-related discourse (2), there is no such progression.
In this paper, we investigate the degree to which the spatial structure of discourse and its rhetorical
structure are co-determined. Using supervised machine learning techniques (Witten and Frank, 2002),
we evaluate two hypotheses: (a) spatial information encoded in adjacent clauses is highly predictive of
the rhetorical relations that hold between them and (b) spatial information is highly predictable based on
associated spatial information within narrative event clauses. To do this, we build a corpus of narrative
texts which are annotated both for spatial information (figure and ground (granularity) relationships,
175
mereotopologically classified verbs and prepositions, and frames of reference) and rhetorical relations (a
binary NARRATION vs. ELABORATION/BACKGROUND distinction discussed in Section 3.2). This corpus
is then used to train two types of classifiers - one type that classifies the rhetorical relations holding
between clauses on the basis of spatial information, and another type that classifies spatial relationships
within clauses where the NARRATION relation holds. The results support both hypotheses and indicate
the centrality of spatial information to narrative discourse structure and associated classification tasks.
2 Background and Related Research
2.1 Rhetorical Relations
Rhetorical relations describe the role that one clause plays with respect to another in a text and contributes
to a text?s coherence (Hobbs, 1985). As such, these relations are pragmatic features of a text. In NLP
generally, classifying rhetorical relations has been an important area of research (Marcu, 2000; Sporleder
and Lascarides, 2005) and has been shown to be useful for tasks such as text summarization (Marcu,
1998). The inventory of rhetorical relations in Segmented Discourse Representation Theory (SDRT)
(Asher and Lascarides, 2003) is widely used in these applications. This inventory includes the following
relations, illustrated by example: NARRATION: Klose got up. He entered the game. ELABORATION:
Klose pushed the Serbian midfielder. He knew him from school. BACKGROUND: Klose entered the game.
The pitch was very wet. EXPLANATION: Klose received a red card. He pushed the Serbian midfielder.
CONSEQUENCE: If Klose received a red card, then he pushed the Serbian midfielder. RESULT: Klose
pushed the Serbian midfielder. He received a red card. ALTERNATION: Klose received a red card or he
received a yellow card. CONTINUATION: Klose received a red card. Ronaldo received a yellow card.
In previous work, rhetorical relations have been predicted based on a range of features including
discourse connectives, relation location, clause length, part-of-speech, content and function words, and
syntactic features (Marcu and Echihabi, 2002; Lapata and Lascarides, 2004). These systems have a wide
range of average accuracies for all relations sought to be predicted - e.g. 33.96% (Marcu and Echihabi,
2002) to 70.70% (Lapata and Lascarides, 2004) - and individual relations - e.g. RESULT - 16.21% and
EXPLANATION - 75.39% (Marcu and Echihabi, 2002) and CONTRAST - 43.64% and CONTINUATION -
83.35% (Sporleder and Lascarides, 2005). Our focus is on the NARRATION, BACKGROUND and ELAB-
ORATION relations, which account for over 90% of the discourses in our corpus.
2.2 Spatial Language and Discourse
Spatial language has been discussed in a number of NLP contexts. For example, linking natural language
with physical locations via semantic mark-up (e.g. SpatialML (MITRE, 2009)); spatial description and
wayfinding tasks (e.g. Anderson et al, 1991); and dialogue systems (e.g. Coventry et al, 2009), just
to name a very few. Perspectives on spatial language are similarly varied in terms of their focus and
theoretical background (e.g. cognitive, semantic and syntactic); however, common threads do emerge.
First, all physical spatial references are reducible to figure and ground relationships (Talmy, 2000). In
English, these are triggered by a deictic verb or adverb (e.g. went, here) (3a); a spatial preposition (e.g.
in, at) (3b); a particle verb (e.g. put on, got out) (3c); or a motion verb (e.g. drive, follow) (3d).
(3) a. [Ronaldo]figure is [here]ground.
b. [Ronaldo]figure is in [the park]ground.
c. [Ronaldo]figure rolled over [?]ground.
d. [Ronaldo]figure ran to [the park]ground.
Second, figure and ground relationships qualitatively vary by the type of verb and preposition cre-
ating the relationship. These differences can be modeled in mereotopology, which defines spatial re-
lationships in terms of regions and connections (e.g. RCC-8 (Randell et al, 1992)). We follow Asher
and Sablayrolles (1995) who classify prepositions based on the position (Position - at, Initial Direction
- from, Medial Position - through, Final Position - to) and contact (Inner - in, Contact - against, Outer
176
- along, and Outer-Most - beyond) of two regions (figure and ground). For verbs, Muller (2002) pro-
poses six mereotopological classes: Reach, Leave, Internal, External, Hit, and Cross. Pustejovsky and
Moszkowicz (2008) mapped Muller?s classes to FrameNet and VerbNet and propose ten general classes
of motion (Move, Move-External, Move-Internal, Leave, Reach, Detach, Hit, Follow, Deviate, Stay).
Third, figure and ground relationships vary by the perspective used to describe the relationship.
For this discussion, perspective takes two forms, granularity of spatial description (following Montello
(1993)) and frames of reference (following Levinson (1996)). Granularity refers to the level of detail
in a given spatial description. Montello (1993, p. 315) indicates four spatial granularities based on the
cognitive organization of spatial knowledge (summarized in (4)).
(4) a. Ronaldo jumped on the ball.
b. Ronaldo is in the corner.
c. Ronaldo is running around the field.
d. Ronaldo is in Cape Town.
(4a) is a Figural granularity which describes space smaller than the human body. (4b) is a Vista gran-
ularity which describes space from a single point of view. (4c) is an Environmental granularity which
describes space larger than the body with multiple (scanning) point(s) of view. (4d) is a Geographic
granularity which describes space even larger than the body and is learned by symbolic representation.
Frames of reference provide different ways of describing the same spatial relationships. For example,
given a static scene of Ronaldo sitting on a bench next to his coach, each utterance in (5) would be an
accurate spatial description.
(5) a. Deictic: Ronaldo is there.
b. Contiguity: Ronaldo is on the bench.
c. Named Location: Ronaldo is at the sideline.
d. Relative: Ronaldo is in front of me.
e. Intrinsic: Ronaldo is behind his coach.
f. Absolute: Ronaldo is north of his coach.
(5a-c) are non-coordinated as they relate just the figure and ground. Coordinated information, relating
the figure to an additional entity within the ground, occurs in (5d-f). Frames of reference apply to both
static and dynamic relationships (Levinson, 1996, p. 360).
In terms of attending to spatial information in discourse, Herman (2001) argues that spatial informa-
tion patterns in narrative discourse carve out spatially defined domains that group narrative actions. In
particular, the emergence and change in different types of spatial reference to physical location (discourse
cues) create maps of the narrative actions. These discourse cues include figure, ground and path (motion)
relationships (3); frames of reference (5); and deictic shifts - here vs. there. Herman?s demonstration is
based on ghost story narratives that are rich in spatial reference.
Howald (2010) showed in a corpus of serial killer first person narratives, also rich in spatial reference,
that these spatial narrative domains, in the form of abstract Pre-Crime, Crime and Post-Crime events,
were predicted to a 90% accuracy from three spatial features (figure, ground, and spatial verb) and
discourse sequence. Overall, research by Herman (2001) and Howald (2010) demonstrates some level
of dependency between spatial information and discourse structure. The present research addresses the
specific question of whether there is a systematic relationship between spatial information and temporal
information via rhetorical relations and the spatial architecture of narrative events.
3 Data and Annotation
3.1 Data
Three corpora of narrative discourse were annotated with rhetorical and spatial information. These cor-
pora were then used to train and test machine learning systems. Summarized in Table 1, the three dif-
ferent narrative corpora selected for analysis were: (1) narratives from serial criminals (CRI) - oral and
177
written confession statements and guilty pleas; (2) American National Corpus Charlotte Narrative and
Conversation Collection (Ide and Suderman, 2007) (ANC) - oral narratives in conversations collected in
a sociolinguistic interview format; and (3) The Degree Confluence Project (DEG) - this project, which
seeks to map all possible latitude-longitude intersections on Earth, requires that participants who visit
these intersections provide written narratives of the visit for inclusion on the project?s website.
Table 1: Relation and Spatial Clause Distribution
Corpus ANC (n=20) DEG (n=20) CRI (n=20) Total (N=60)
Total Clauses 588 611 1,710 2,909
Spatial Clauses 260 354 932 1,546
Average 44.21 57.93 54.50 53.14
Total Rhetorical 568 591 1,690 2,848
Spatial Rhetorical 259 345 929 1,533
Average 45.59 58.37 55.00 53.82
20 narratives from each corpus were selected. There was a total of 2,909 (independent) clauses with
1,546 of those clauses containing spatial information - spatial clauses (53.14% on average). There was a
total of 2,848 relations with 1,533 of those relations where both clauses contained spatial information -
spatial rhetorical (53.82% on average).
3.2 Spatial Information and Rhetorical Relation Annotation
We developed a coding scheme for spatial information that consolidates the insights on spatial langauge
discussed in Section 2.2.
? FIGURE is an indication of grammatical person or a non-person entity (1 = I, my; 2 = you, your;
3 = he, she, it, his, her; 4 = we, our; 5 = you, your; 6 = they, their; NP = the purse, a bench, three
cars);
? VERB is one of the four mereotopological classes - a consolidation of Pustejovsky and Moszkow-
icz?s (2008) ten classifications (State = was, stay, was sitting; Move = run, go, jump; Outside =
follow, pass, track; Hit = attach, detach, strike);
? PREPOSITION is one of four mereotopological classes based on Asher and Sablayrolles (1995)
(Positional = in, on; Initial = from ; Medial = through; Final = to);
? GROUND is one of four granularities (Figural, Environmental, Vista, Geographic) (see (4)
above);
? FRAME is one of six frames of reference (Deictic, Contiguity, Named Location, Relative, In-
trinsic, Absolute) (see (5) above).
The three corpora were annotated by one of the authors. Annotation occurred one narrative at a
time and any information from that narrative could be used to resolve rhetorical relations and spatial
information. A reference sheet including several examples of each coding element was available to
the annotator. The annotation happened in two phases. First, each pair of clauses was annotated with
an SDRT relation. Second, each clause that contained a physical figure and ground relationship was
identified. The figure, ground, preposition and verb were annotated with a Figure, Verb, Preposition,
Ground, and Frame. We illustrate with (6) where the NARRATION relation obtains between (6a-b).
(6) a. Kaka kicked the ball into the goal.
b. Then he ran to the left side of the bench.
178
The spatial annotation of (6a) is: FIGURE = NP, the ball; VERB = Hit (H), kicked; PREPOSITION =
Final (F), into; GROUND = Environmental (E), the goal; and FRAME = Contiguity (C). The spatial
annotation of (6b) is: FIGURE = 3, he; VERB = Move (M), ran; PREPOSITION = Final (F), to the left
side of; GROUND = Environmental (E), the bench; and FRAME = Intrinsic (INT). The distribution of
spatial rhetorical relations is summarized in Table 2.
Table 2: Spatial Rhetorical Relation Distribution per Corpus
Relation ANC DEG CRI Total
NARRATION 133 124 654 911
BACKGROUND 74 87 238 399
ELABORATION 34 63 17 114
CONTINUATION 14 27 10 51
RESULT 3 22 0 25
EXPLANATION 0 16 1 17
ALTERNATION 0 0 9 9
CONSEQUENCE 1 6 0 7
Total 259 345 929 1,533
An additional individual was queried for inter-rater reliability against the author annotation. The rater
was given roughly one-third of the data (10 narratives (4 ANC, 4 DEG, 2 CRI) accounting for 510 spatial
clause pairs), the same example sheet used by the author, and as much time as needed to complete the
task. Average agreement and Cohen?s kappa statistics (Cohen, 1960) were computed between the inter-
rater and the author for the spatial annotations and NARRATION, BACKGROUND, and ELABORATION
codings. Individually, BACKGROUND and ELABORATION have low interannotator agreement (? = 32.92
and 54.20 respectively), but these two relations were often confused (26% of BACKGROUND relations
coded as ELABORATION and 12% of ELABORATION relations coded as BACKGROUND). As illustrated
in (7-8), both BACKGROUND and ELABORATION add information to the surrounding state of affairs.
(7) a. Klose entered the game.
b. The pitch was very wet.
(8) a. Klose pushed the Serbian midfielder.
b. He knew him from school.
As evidenced by the annotation confusions, the difference between these relations is difficult to distin-
guish and the distinction made by Asher and Lascarides (2003) is subtle - BACKGROUND?s temporal
consequence is one of overlap and ELABORATION, a subordinating relation, is one of part-of. However
collapsing these relations resulted in a fairly reliably distinguished category. Average agreement and
kappa statistics are summarized in Table 3.
Table 3: Agreement and Kappa Statistics for Relation and Spatial Codings
Coding Agreement (%) Kappa (?)
All Rhetorical Relations 71.97 60.27
NARRATION 86.32 74.36
BACKGROUND / ELABORATION 73.40 62.20
Figure 94.91 89.92
Verb 90.90 81.80
Preposition 78.35 56.70
Granularity 87.87 75.74
Frame 69.38 38.76
179
For rhetorical relations, the average agreement and kappa statistic are consistent with previously re-
ported performances (e.g. Agreement = 71.25 / ? = 61.00 (Sporleder and Lascarides, 2005)). We have
not been able to find previously reported performance accuracies for NARRATION, ELABORATION and
BACKGROUND relations specifically. However, ? statistics from 60.00 to 75.00 and above are considered
acceptable (e.g. Landis and Koch, 1977). For the spatial codings, the average agreements are relatively
high with Preposition and Frame falling lowest. There is no basis for direct comparison of these num-
bers to other research as the coding scheme is novel.
4 Machine Learning Experiments
We constructed two machine learning tasks to exploit the annotated spatial information to determine what
contributions the information is making to narrative structure. The first task evaluates the prediction of
NARRATION and BACKGROUND/ ELABORATION relations based on pairs of spatial clauses. The second
task evaluates the prediction of spatial information types, based on the other spatial information types in
that clause, in individual clauses where the NARRATION relation holds.
4.1 Rhetorical Relation Prediction
4.1.1 Methods and Results
Task 1 builds a 2-way classifier for the NARRATION and BACKGROUND/ ELABORATION relations.
Clause pairs were coded as vectors (n = 1,424) - for example, the vector for (6) is NP3, HM, FF,
EE, CINT. These vectors were used to train and test (10-fold cross-validation) a number of classifiers.
The Na??ve Bayes classifier performed the best. Results are reported in Table 4.
Table 4: Na??ve Bayes Classification Accuracy and F-Measures for Task 1
NARRATION Accuracy (% / baseline) Precision Recall F-Score
ANC 63.29 / 58 .676 .633 .654
DEG 75.71 / 61 .803 .757 .779
CRI 90.12 / 73 .822 .901 .860
TOTAL 84.90 / 68 .808 .841 .824
BACK/ ELAB Accuracy (% / baseline) Precision Recall F-Score
ANC 57.89 / 41 .532 .579 .555
DEG 70.11 / 38 .642 .701 .670
CRI 45.63 / 26 .624 .456 .527
TOTAL 57.87 / 35 .622 .567 .593
For all corpora combined, the majority class (?baseline?) for NARRATION is 68% and 26% for BACK-
GROUND / ELABORATION; the classifier performs 16% and 22% above baseline respectively. The differ-
ence between the NARRATION and BACKGROUND / ELABORATION relations and baselines is statistically
significant for each corpus and all corpora combined - ANC: ?2 = 25.64, d.f. = 1, p ? .001; DEG: ?2 =
33.86, d.f. = 1, p ? .001; CRI: ?2 = 22.69, d.f. = 1, p ? .001; and TOTAL:?2 = 34.09, d.f. = 1, p ? .001.
4.1.2 Discussion
Again, we have not been able to find reported results for a direct comparison of NARRATION and BACK-
GROUND/ ELABORATION. However, the 84.90% and 57.87% (at 16% and 22% over baseline) perfor-
mance of our Na??ve Bayesian model is consistent with results reported in similar tasks. For example,
Marcu and Echihabi (2002) report an average accuracy of 33.96% (5-way classifier) and 49.70% (6-way
classifier) based on training with very large data sets. Sporleder and Lascarides (2005) report a 57.55%
average accuracy, based on training with large data sets, which is 20% over Marcu and Echihabi?s 5-way
180
classifier and almost 40% over a random 20% baseline. Lapata and Lascarides (2004) report an average
accuracy of 70.70% for inferring temporal relations based on training.
We ran an additional set of experiments to determine the relative contribution of spatial features to
predict NARRATION and BACKGROUND / ELABORATION relations. As shown in Table 5, Figure and
Verb outperform Ground, Preposition and Frame in accuracy. Figure performs at a 71% average
accuracy (85% for NARRATION and 40% for BACKGROUND/ ELABORATION) and Verb performs at a
74% average accuracy (84% for NARRATION and 54% for BACKGROUND/ ELABORATION). Figure and
Verb appear to be most discriminating. Note that we are not suggesting that subject and verb generally
are similarly discriminatory - Figure and Verb in this task are overtly spatial. Despite the performance
of Figure and Verb, different subsets of spatial information worked better (we ran all permutations of
spatial features - the top five are listed in Table 5). However, the difference in performance is negligible.
For example, the best subset of Figure, Verb and Ground (85% and 58%) only performed 1% above
NARRATION and BACKGROUND/ ELABORATION prediction based on all five features combined.
Table 5: Single and Combined Spatial Feature Performance
Feature NARRATION BACK/ ELAB Features NARRATION BACK/ ELAB
Figure (F) 85.58 40.33 FVG 85.24 58.33
Verb (V) 84.59 54.97 VGP 84.34 58.33
Prepostion (P) 97.34 1.00 FVGR 86.33 56.45
Ground (G) 97.33 1.00 FV 86.56 56.90
Frame (R) 98.02 2.00 VG 85.37 57.33
These results tell us several things about the relationship between spatial information and rhetorical
structure as it applies to narrative discourse. First, spatial information predicts rhetorical structure as
good as non-spatial types of linguistic information reported in other investigations and with many fewer
features. For example, Sporleder and Lascarides (2005) rely on 72 different features falling into nine
classes whereas we rely on 14 features in five classes. This suggests that spatial information is not only
central to rhetorical stucture, like temporal components, but central to the task of prediction. Second,
while the type of spatial information that predicts rhetorical structure is based on the primary figure and
ground relationship, it is the qualitative semantic variations within these elements that is providing the
discrimination. It is the organization of spatial relationships - (Verb and Preposition) and the perspective
provided by the narrator (Figure, Ground and Frame) combined - rather than any individual elements.
4.2 Spatial Information Prediction
4.2.1 Methods and Results
Task 2 is a series of five experiments. Each experiment builds a classifier for each type of spatial infor-
mation: a 6-way classifier for Frame; a 5-way classifier for Figure (Figure types 2 and 5 did not occur
in our corpus); and 4-way classifiers for Ground, Preposition and Verb. Single clauses that contribute
to the NARRATION relation were coded as vectors (n = 911) - for example, the single vectors for (6a)
and (6b) are NP, H, F, E, C and 3, M, F, E, INT. These vectors were used to train and test (10-fold
cross-validation) a number of classifiers to predict one of the five spatial features given the remaining
four. The K* classifier performed the best. Results are reported in Table 6. For all corpora combined, the
K* classifier performs above baseline for all spatial information (Figure = 9%, Verb = 17%, Preposition
= 9%, Ground = 19%, Frame = 8%) (?2 = 20.95, d.f. = 4, p ? .001).
4.2.2 Discussion
Even though the accuracies of predicting spatial information are significantly above baseline, we sought
ways to boost performance by considering implicit spatial information. For those clauses without explicit
spatial information, we extended the annotation of the previous clause?s coding based on the inertia of
181
Table 6: K* Classification Accuracy and F-Measures for Task 2
Spatial Information Accuracy (% / baseline) Precision Recall F-Score
Figure 47.97 / 38 .464 .480 .428
Verb 67.32 / 50 .635 .673 .640
Preposition 53.69 / 46 .492 .537 .499
Ground 53.59 / 34 .530 .536 .519
Frame 55.67 / 47 .507 .557 .511
narrative texts. Rapaport, et al (1994) discuss the temporal inertia of narrative texts - time moves forward
through narrative events. In the absence of updating, information is maintained. We suggest that inertia
applies to spatial information as well. For example, given the clauses - John entered the room. He sat
down. - we make the assumption that John sat down in the room that he entered. We illustrate with (9).
(9) a. Kaka kicked the ball into the goal.
NP, H, F, E, C, .33
b. The goaltender yelled in frustration.
NP, H, F, E, C, .66
c. Then Kaka ran to the left side of the bench.
3, M, F, E, INT, 1
No explicit spatial information exists in (9b). We took the coding from the explicit spatial information
in (9a) and maintained it for (9b). New explicit spatial information occurs in (9c) and the coding is
updated. Further, we included explicit sequence information as a measure of a given clause?s proportional
position within the text (.33, .66 and 1). In the absence of overt temporal specification (occuring in only
10% of the clauses in our corpus), the sequence information, a textual feature, parallels the temporal
progression (and inertia) of narrative events. This added 560 additional vectors (n = 1,471). The K*
classifier still performed the best. The results are summarized in Table 7.
Table 7: K* Classification Accuracy and F-Measures for Task 2 Boosted Vectors
SPATIAL INERTIA Accuracy (% / baseline) Precision Recall F-Score
Figure 51.73 / 41 .509 .517 .473
Verb 70.22 / 48 .673 .700 .679
Preposition 57.30 / 47 .571 .573 .540
Ground 62.61 / 35 .636 .626 .611
Frame 59.82 / 44 .574 .598 .564
SPATIAL INERTIA + SEQUENCE Accuracy (% / baseline) Precision Recall F-Score
Figure 70.56 / 41 .702 .706 .699
Verb 79.33 / 48 .789 .793 .790
Preposition 67.91 / 47 .676 .679 .674
Ground 72.39 / 35 .721 .724 .721
Frame 69.06 / 44 .678 .691 .681
Inclusion of the spatial inertia values improves performance of the K* classifier in all cases (?2 =
40.59, d.f. = 4, p ? .001). Inclusion of sequence information improves performance even further (?2
= 102.36, d.f. = 4, p ? .001). Note that, despite the increase in performance, sequencing information
alone does not do as well, indicating that spatial information still plays a discriminatory role. Using
sequence information alone as a baseline (Figure = 47%, Verb = 52%, Preposition = 47%, Ground =
44%, Frame = 48%;), the normalized performance values above sequence baseline become Figure =
23%, Verb = 27%, Preposition = 28%, Ground = 20%, and Frame = 21%.
The ability to predict spatial features appears to be dependent both on a patterned distribution of
182
the per-clause spatial information (increased by spatial inertia) and on the textual feature of sequence
(temporal inertia). This seems to hold despite the specific subject matter or spatial characteristics of a
given narrative. Considering the complete spatiotemporal picture for narrative clauses yields the best
prediction results and suggests that the spatial information structure of narrative discourse represents
some type of organization akin to what Herman (2001) and Howald (2010) have evaluated in spatially-
rich narratives. Based on the tasks presented here, this organization appears to be fundamental and
relative to formal temporally-informed discourse structure.
5 Conclusion
Exploration of the spatial dimension in narrative discourse provides interesting and robust possibilities
for computational discourse analysis. We have described two machine learning tasks which exploit
spatial linguistic features. In addition to improving on existing prediction systems, both tasks empirically
demonstrate that, when available, certain types of spatial information are predictors of the rhetorical
structure of narrative discourse and the spatial information of narrative event sequences. Based on these
results, we indicate that spatial structure is related to temporal structure in narrative discourse.
The coding scheme proposed here models complex and interrelated properties of spatial relationships
and perspectives and should be generalizeable to other non-narrative discourses. Future research will fo-
cus on different discourse corpora to determine how spatial information is related to rhetorical structure.
Additional future research will also focus on automation of the annotation process. The ambiguity of
spatial language makes automatic extraction of spatial features infeasible at the current state of the art.
Fortunately, average agreement and kappa statistics for coding of the spatial information and rhetorical
relations are within acceptable ranges. The annotated spatial features are semantically deep and useful
for not only computational discourse systems, but tasks that involve the semantic modeling of spatial
relations and spatial reasoning.
Acknowledgments
Thank you to David Herman and James Pustejovsky for productive comments and discussion and to
Jerry Hobbs for suggesting the Degree Confluence Project as a source of spatially rich narratives. Thank
you also to four anonymous reviewers for very helpful insights.
References
[1] Anne Anderson, Miles Bader, Ellen Bard, Elizabeth Boyle, Gwyneth Doherty, Simon Garrod, Stephen
Isard, Jacqueline Kowtko, Jan McAllister, Jim Miller, Catherine Sotillo, Henry Thompson, and Regina
Weinert. 1991. The HCRC Map Task Corpus. Language and Speech, 34:351?366.
[2] Nicholas Asher and Alex Lascarides. 2003. Logics of Conversation. Cambridge University Press,
Cambridge, UK.
[3] Nicholas Asher and Pierre Sablayrolles. 1995. A Typology and Discourse Semantics for Motion
Verbs and Spatial PPs in French. Journal of Semantics, 12(2):163?209.
[4] Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
[5] Kenny Coventry, Thora Tenbrink, and John Bateman. 2009. Spatial Language and Dialogue. Oxford
University Press, Oxford, UK.
[6] David Herman. 2001. Spatial Reference in Narrative Domains. Text, 21(4):515?541.
[7] Jerry R. Hobbs. 1985. On The Coherence and Structure of Discourse. CSLI Technical Report, 85-37.
183
[8] Blake Howald. 2010. Linguistic Spatial Classifications of Event Domains in Narratives of Crime.
Journal of Spatial Information Science, 1.75?93.
[9] Nancy Ide and Keith Suderman. 2007. The Open American National Corpus (OANC), available at
http://www.AmericanNationalCorpus.org/OANC.
[10] Richard Landis and Gary Koch. 1977. The Measurement of Observer Agreement for Categorical
Data. Biometrics, 33(1):159?174.
[11] Mirella Lapata and Alex Lascarides. 2004. Inferring sentence internal temporal relations. In Pro-
ceedings of NAACL-04, 153?160.
[12] Stephen C. Levinson. 1996. Language and Space. Annual Review of Anthropology, 25(1):353?382.
[13] William Mann and Sandra Thompson. 1987. Rhetorical Structure Theory: A Framework for The
Analysis of Texts. International Pragmatics Association Papers in Pragmatics, 1:79?105.
[14] Daniel Marcu. 1998. Improving Summarization Through Rhetorical Parsing Tuning. In The 6th
Workshop on Very Large Corpora, 206?215.
[15] Daniel Marcu. 2000. The Rhetorical Parsing of Unrestricted Texts: A Surface-Based Approach.
Computational Linguistics, 26(3):395?448.
[16] Daniel Marcu and Abdessamad Echihabi. 2002. An Unsupervised Approach to Recognizing Dis-
course Relations. In Proceedings of ACL-02, 368?375.
[17] MITRE. 2009. SpatialML: Annotation Scheme for Marking Spatial Expressions in Natural Lan-
guage, Version 3.0. April 3, 2009.
[18] Daniel R. Montello. 1993. Scale and Multiple Psychologies of Space. In A. Frank and I. Campari
(eds.), Spatial Information Theory: A Theoretical Basis for GIS (LNCS 716), 312?321. Springer-Verlag,
Berlin.
[19] Philippe Muller. 2002. Topological Spatio-temporal Reasoning and Representation. Computational
Intelligence, 18(3):420?450.
[20] Barbara Partee. 1984. Nominal and Temporal Anaphora. Linguistics and Philosophy, 7(3):243?286.
[21] James Pustejovsky and Jessica Moszkowicz. 2008. Integrating motion predicate classes with spatial
and temporal annotations. COLING 2008:95?98.
[22] James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser Saur, Robert Gaizauskas, Andrea Setzer,
and Graham Katz. 2003. TimeML: Robust Specification of Event and Temporal Expressions in Text. In
Proceedings of the IWCS-5, Fifth International Workshop on Computational Semantics.
[23] David Randell, Zhan Cui, and Anthony Cohn. 1992. A Spatial Logic Based on Regions and
Connection. Proceedings of KR92, 394?398. Los Altos, CA: Morgan Kaufmann.
[24] William Rapaport, Erwin Segal, Stuart Shapiro, David Zubin, Gail Bruder, Judith Duchan, Michael
Almeida, Joyce Daniels, Mary Galbraith, Janyce Wiebe and Albert Yuhan. 1994. Deictic Centers and
the Cognitive Structure of Narrative Comprehension. Technical Report No. 89-01. Buffalo, NY: SUNY
Buffalo Department of Computer Science.
[25] Caroline Sporleder and Alex Lascarides. 2005. Exploiting Linguistic Cues to Classify Rhetorical
Relations. Proceedings of Recent Advances in Natural Language Processing (RANLP-05), 532?539.
[26] Leonard Talmy. 2000. Toward a Cognitive Semantics, Volume 2. The MIT Press, Cambridge, MA.
[27] Ian Witten and Eibe Frank. 2002. Data Mining Practical Machine Learning Tools and Techniques
with Java Implementation. Morgan Kaufmann.
184
