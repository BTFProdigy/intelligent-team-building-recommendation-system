Chinese Classifier Assignment Using SVMs
Hui Guo and Huayan Zhong
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400, USA
{huguo, huayan}@cs.sunysb.edu
Abstract
In Chinese, nouns need numeral clas-
sifiers to express quantity. In this pa-
per, we explore the relationship be-
tween classifiers and nouns. We ex-
tract a set of lexical, syntactic and onto-
logical features and the corresponding
noun-classifier pairs from a corpus and
then train SVMs to assign classifers to
nouns. We analyse which features are
most important for this task.
1 Introduction
In English, numbers directly modify count nouns,
as in ?two apples? and ?five computers?. Num-
bers cannot directly modify mass nouns; instead,
an embedded noun phrase must be formed, e.g.
?five slices of bread?. However, in Chinese all
nouns need numeral classifiers to express quan-
tity1. When translating from English to Chinese,
we may need to choose Chinese classifiers to form
noun phrases. We can see the difference between
the two languages in the following two examples:
?[liang] ?[ge]?*[pingguo] (Chinese)
two apples (English)
and
?[wu]?[pian]??[mianbao] (Chinese)
five slices of bread (English)
Noun classifer combinations appear with high
frequency in Chinese. There are more than 500
classifiers although fewer than 200 of them are
frequently used. Each classifier can only be
1Proper nouns and bare noun phrases do not need classi-
fiers.
used with certain classes of noun. Nouns in a
class usually have similar properties. For exam-
ple, nouns that can be used with the classifier
??[gen]? are: ?uz?(straw), ?M?(chopstick),
??(pipe), etc. All these objects are long and
thin. However, sometimes nouns with similar
properties are in different classes. For example,
?:?(cow), ?j?(horse) and ???(lamb) are all live-
stock, but they associate with different classifiers.
This means that classifier assignment is not totally
rule-based but partly idiomatic.
In this paper, we explore the relationship be-
tween classifiers and nouns. We extract a set of
features and the corresponding noun-classifier at-
tachments from a corpus and then train SVMs to
assign classifers to nouns. In Section 4 we de-
scribe our data set. In Section 5 we describe our
experiments. In Section 6 we present our results.
2 Related Work
Many Asian languages (e.g. Chinese, Korean,
Japanese and Thai) have numeral classifier sys-
tems. Previous work on noun-classifier match-
ing has been done in these languages. (Sorn-
lertlamvanich et al, 1994) present an algorithm
for selecting an appropriate classifier for a noun
in Thai. The general idea is to extract noun-
classifier collocations from a corpus, and output a
list of noun-classifier pairs with frequency infor-
mation. During noun phrase generation, the most
frequently co-occurring classifier for a given noun
is selected. However, no evaluation is reported for
this algorithm.
The algorithm described in (Paik and Bond,
2001) generates Japanese and Korean numeral
25
classifiers using semantic classes from an ontol-
ogy. The authors assigned classifiers to each
of the 2,710 semantic classes in the ontology
by hand. During generation, nouns in each se-
mantic class are assigned the associated classi-
fier. The classifier assignment accuracy is 81%
for Japanese classifiers and 62% for Korean clas-
sifiers. However, the evaluation set contains only
90 noun phrases, which is pretty small. Further-
more, it is hard work to attach classifiers to an on-
tology by hand, and with this approach it is hard
to deal with cases like the cattle example men-
tioned earlier.
(Paul et al, 2002) present a method for ex-
tracting classifier information from a bilingual
(Japanese-English) corpus based on phrasal cor-
respondences in the sentential context. Bilin-
gual sentence pairs are compared to find noun-
classifier collocations. The evaluation was done
by a human. The precision is high (84.2%) but
the recall is only about 40% because the algorithm
does not give output for half of the nouns.
In contrast to these algorithms, our approach: is
based on a large data set; uses machine learning;
and does not require the attachment of classifiers
to an ontology by hand.
3 Support Vector Machines
Support Vector Machines (SVMs) are a type of
classifier first introduced in (Boser et al, 1992).
In the last few years SVMs have become an im-
portant and active field in machine learning re-
search. The SVM algorithm detects and exploits
complex patterns in data.
A binary SVM is a maximum margin classifier.
Given a set of training data {x1, x2, ..., xk}, with
corresponding labels y1, y2, ..., yk ? {+1,?1}, a
binary SVM divides the input space into two re-
gions at a decision boundary, which is a separat-
ing hyperplane ?w, x? + b = 0 (Figure 1). The
decision boundary should classify all points cor-
rectly, that is:
yi(?w, xi? + b) > 0,?i
Also, the decision boundary should have the
maximum separating margin with respect to the
two classes. If we rescale w and b to make
the closest point(s) to the hyperplane satisfy
b
w
x
x
x
x
x
x
x
x
<w, x> + b = 0
Figure 1: The input space and hyperplane
|?w, xi? + b| = 1, then the margin equals 1/||w||
and the problem can be formulated as:
minimize 12 ||w||
2
subject to yi(?w, xi? + b) ? 1,?i
The generalized Lagrange Function is:
L(w, b, ?) = 12?w,w??
l
?
i=1
?i[yi(?w, xi?+b)?1]
So we can transform the problem to its dual:
maximize
W (?) =
n
?
i=1
?i ?
1
2
n
?
i=1,j=1
?i?jyiyj?xi, xj?
subject to ?i ? 0,
n
?
i=1
?iyi = 0
This is a quadratic programming (QP) problem
and we can always find the global maximum of
?i. We can recover w and b for the hyperplane
by:
w =
n
?
i=1
?iyixi
b = ?maxyi=?1(?w, xi?) + minyi=+1(?w, xi?)2
If the points in the input space are not linearly
separable, we allow ?slack variables? ?i in the
classification. We need to find a soft margin hy-
perplane, e.g.:
minimize 12 ||w||
2 + C
n
?
i=1
?i
26
subject to yi(?w, xi? + b) ? 1 ? ?i,?i
Once again, a QP solver can be used to find the
solution.
For our task we need multi-class SVMs. To get
multi-class SVMs, we can construct and combine
several binary SVMs (one-against-one), or we can
directly consider all data in one optimization for-
mula (one-against-all).
Many SVM implementations are available on
the web. We chose LIBSVM (Chang and Lin,
2001), which is an efficient multi-class imple-
mentation. LIBSVM uses the ?one-against-one?
approach in which k(k? 1)/2 classifiers are con-
structed and each one trains on data from two dif-
ferent classes (Hsu and Lin, 2002).
4 Data and Resources
We use the Penn Chinese Treebank (Xue et al,
2002) as our corpus and the ontology/lexicon
HowNet (Dong and Dong, 2000) to get ontologi-
cal features for nouns. We train SVMs on differ-
ent feature sets to see which set(s) of features are
important for noun-classifier matching.
4.1 Penn Chinese Treebank
The Penn Chinese Treebank is a 500,000 word
Chinese corpus annotated with both part-of-
speech (POS) tags and syntactic brackets.
We automatically extract noun phrases that
contain classifiers from the corpus. An example
noun phrase (translation: ?a major commercial
waterway?) is:
(IP
....
(NP (QP (CD) (CLP (M)))
(NP (NNy?)) (ADJP (JJL))
(NP (NN?s)))
...
)
The word in (CLP (M[tiao])) is the classifier
and the head noun of the noun phrase is (NN ?
s). In Section 5.3 we describe a set of features
we obtain from each noun phrase and the sentence
in which it is embedded.
In our corpus, there are 61587 noun occur-
rences (12225 unique nouns) and 3940 classifier-
noun co-occurrences (212 unique classifiers).
However, there is a trival rule determining
whether a noun needs a classifier. If a noun is
preceded by a quantifier or a determiner, then a
classifier is needed, otherwise it is not. Hence,
we only focus on noun-classifier pairs. The most
frequently occurring classifier in this corpus is
??[ge]?, which occurs with 497 unique nouns. In
this corpus, 87 classifiers occur in only one noun-
classifier pair.
4.2 HowNet
We get ontological features of nouns from
HowNet. HowNet is a bilingual Chinese-English
lexicon and ontology. Each word sense is as-
signed to a concept containing ontological fea-
tures. HowNet uses basic meaning units named
sememes to construct concepts.
Table 1 shows an example entry in HowNet.
The entry in Table 1 is for the word ?*
?(writer). The sememe at the first position, ?hu-
man(|)?, is the categorical attribute, which de-
scribes the general category of the concept. The
sememes following the first sememe are addi-
tional attributes, which give additional specific
features. There are two types of pointer, ?#? and
?*?, in the definition. ?#? means ?related?, so
?#occupation? shows that the concept has a re-
lationship with ?occupation?. ?*? means ?agent?,
so ?*compile? shows that ?writer? is the agent of
?compile?. The sememes ?#readings? and ?litera-
ture? show that the job of ?writer? is to compile
?readings? about ?literature?.
We use HowNet 2000, which contains 120,496
entries for about 65,000 Chinese words defined
with a set of 1503 sememes. It is big enough for
our task and we can get ontological features for
94.71% of the nouns from the Penn Chinese Tree-
bank. For the nouns that are not in HowNet, we
just leave the ontological features blank.
5 Experiments
We use six different feature sets to assign classi-
fiers to nouns. To evaluate each feature set, we
perform 10-fold cross validation. We report our
results in Section 6.
5.1 Baseline Algorithm
In the training data, we count the number of times
each classifier appears with a given noun. We as-
sign to each noun in the testing data its most fre-
27
No.: 114303
W C (word in Chinese): *
E C (example in Chinese):
G C (POS tag in Chinese): N
W E (word in English): writer
E E (example in English):
G E (POS tag in English): N
DEF (concept definition): human(|),#occupation(??),*compile(?),
#readings(??),literature(?)
Table 1: An entry in HowNet
Lexical Features Syntactic Features
noun POS of noun
first premod POS of first premod
second premod POS of second premod
main verb POS of main verb
total number of premodifiers sentType
embedded in vp or pp
quoted or not
Table 2: Features extracted from training data
quently co-occurring classifier (c.f. (Sornlertlam-
vanich et al, 1994)). If a noun does not appear in
the training data, we assign the classifier ??[ge]?,
the classifier which appears most frequently over-
all in the corpus.
5.2 Noun Features
Since classifiers are assigned mostly based on the
noun, the most important features for classifier
prediction should be features of the nouns. We
ran four different experiments for noun features:
? (1) The feature set includes only the noun it-
self.
? (2) The feature set includes ontological fea-
tures of the noun only. If classifiers are as-
sociated with semantic categories (c.f. (Paik
and Bond, 2001)), we should be able to as-
sign classifiers based on the ontological fea-
tures of nouns.
? (3) The feature set includes the noun and on-
tological features.
? (4) Two SVMs are trained: one on the noun
only, and one on ontological features only.
During testing, nouns in the training set
are assigned classifiers using the first SVM;
other nouns are assigned classifiers using the
second SVM.
5.3 Context Features
In this set of experiments, we used features from
both the noun and the context. The features we
used can be categorized into two groups: lexical
features and syntactic features. They are shown
in Table 2.
We ran two experiments using this set of fea-
tures:
? (5) The feature set includes the noun, lexical
and syntactic features only.
? (6) The feature set includes the noun, lexical,
syntactic and ontological features.
6 Results and Discussion
We built SVMs using all the feature sets described
in Section 5 and tested using 10-fold cross valida-
tion. We tried the four types of kernel function in
LIBSVM: linear, polynomial, radial basis func-
tion (RBF) and sigmoid, then selected the RBF
kernal K(x, y) = e??||x?y||2 , which gives the
28
Algorithm All nouns Nouns occuring 2+ times
Baseline 50.76% 50.69%
(1) noun only 57.81% (c = 4, ? = 0.5) 59.34% (c = 16, ? = 0.125)
(2) ontology only 58.69% (c = 4, ? = 0.5) 60.68% (c = 256, ? = 0.125)
(3) noun and ontology 57.81% (c = 16, ? = 0.5) 59.46% (c = 16, ? = 0.125)
(4) noun or ontology 58.71% 60.55%
(5) noun, syntactic and
lexical features 52.14% (c = 1024, ? = 0.5) 53.51% (c = 16, ? = 0.5)
(6) all features 52.06% (c = 1024, ? = 0.075) 53.55% (c = 16, ? = 0.5)
Table 3: Accuracy of different algorithms
Most common
noun
?[wei] '[ci] ?[ge] ?[ming] ?[jie] 1[xiang]
?[wei] ? (official) 24.1 (57.1) 14.7 (34.7)
'[ci] L? (conven-
tion)
22.3 (53.3) 1.1 (2.6) 7.6 (18.2)
?[ge] 1? (project) 1.0 (7.0) 0.7 (5.2) 0.2 (1.7) 3.3 (24.4)
?[ming] |? (person) 31.7 (55.2) 23.8 (41.4)
?[jie] ??? (sports
tournament)
1.9 (2.1) 29.6 (34.0) 31.5 (36.2)
1[xiang] ?* (achieve-
ment)
6.6 (11.3) 35.2 (60.4) 1.1 (1.9)
Table 4: Most commonly misclassified classifiers; Cell shows percentage of total occurrences of row
value misclassified as column value and (percentage of total misclassifications of row value misclassi-
fied as column value)
highest accuracy. For each feature set, we sys-
tematically varied the values for the parameters C
(range from 2?5 to 215) and ? (range from 23 to
2?15); we report the best results with correspond-
ing values for C and ?. Finally, for each feature
set, we ran once on all nouns and once only on
nouns occurring twice or more in the corpus.
Classifier assignment accuracy is reported in
Table 3. The performance of all the SVMs is sig-
nificantly better than baseline (paired t-test, p <
0.005). There is no significant difference between
the performance with the 1st, 2nd, 3rd and 4th
feature sets. But the performance of the SVMs us-
ing lexical and syntactic features (experiments 5
and 6) is significantly worse than the performance
on feature sets 1-4 (df = 17.426, p < 0.05).
These results show that lexical and syntactic
contextual features do not have a positive effect
on the assignment of classifiers. They confirm the
intuition that the noun is the single most important
predictor of the classifier; however, the semantic
class of the noun works as well as the noun itself.
In addition, a combination approach that uses se-
mantic class information when the noun is previ-
ously unseen does not perform better.
We also computed the confusion matrix for the
most commonly misclassified classifiers. The re-
sults are reported in Table 4.
For these experiments we used automatic eval-
uation (cf. (Paul et al, 2002)). A classifier is only
judged to be correct if it is exactly the same as that
in the original test set. For some noun phrases,
there are multiple valid classifiers. For example,
we can say
?[yi]L[kuai]?][jinpai]?
or
?[yi]?[mei]?][jinpai]?
(a golden medal).
We did a subjective evaluation on part of our
data to evaluate how many automatically gener-
ated classifiers are acceptable to human readers.
We randomly selected 241 noun-classifier pairs
from our data. We presented the sentence con-
taining each pair to a human judge who is a na-
tive speaker of Mandarin Chinese. We asked the
judge to rate all the classifiers generated by our
29
Algorithm Number rated 1
or higher
Percent rated 1 or
higher
Average rating
Baseline 209 86.7% 1.59
(1) noun only 224 92.9% 1.76
(2) ontology only 226 93.8% 1.78
(3) noun and ontology 226 93.8% 1.77
(4) noun or ontology 227 94.2% 1.80
(5) noun, syntactic and
lexical features 218 90.5% 1.67
(6) all features 218 90.5% 1.67
Original 241 100% 1.95
Table 5: Human evaluation: Ratings of classifiers
algorithms as well as the original classifier by in-
dicating whether each is good (2), acceptable (1)
or bad (0) in that sentence context. The classifiers
were presented in random order; the judge was
blind to the source of the classifiers.
The results for our human evaluation are re-
ported in Table 5. Although our automatic eval-
uation indicates relatively poor accuracy, 94.2%
of generated classifiers using feature set 4) are
rated acceptable or good in our subjective evalua-
tion. Also, the performance of SVMs with the 1st,
2nd, 3rd and 4th feature sets is significantly bet-
ter than baseline (paired t-test, p < 0.005). There
is no significant difference between the perfor-
mance with the 1st, 2nd, 3rd and 4th feature sets.
But the performance of the SVMs using lexical
and syntactic features (experiments 5 and 6) is
significantly worse than those without (p < 0.05).
The ratings of the classifiers generated by all our
algorithms are significantly worse than the origi-
nal classifiers in the corpus. In future work, we
plan to extend this evaluation using more judges.
Which classifier to select also depends on the
emotional background of the discourse (Fang,
2003). For example, we can use different class-
fiers to express different affect for the same noun
(e.g. if a government official is in favor or dis-
grace). However, we cannot get this kind of in-
formation from our corpus.
7 Conclusions and Future Work
Our machine learning approach to classifier as-
signment in Chinese performs better than previ-
ously published rule-based approaches and works
for bigger data sets. The noun is clearly the most
important feature (experiment 1). However, we
still think ontological features may be useful in
classifier assignment, for example for previously
unseen nouns, and our experimental results show
a trend in this direction, although not a statisti-
cally significant one (experiments 2 and 4).
We used the Chinese Treebank for these ex-
periments because it is the only available corpus
of parsed Chinese text. Now that we have iso-
lated the relevant features for this task, we plan to
conduct further experiments using larger corpora,
such as the Chinese Gigaword (Graf and Chen,
2003).
Our use of ontological features could be im-
proved in several ways. First, the ontological fea-
tures we get from HowNet do not fit our pur-
pose well. For example, the definitions of ???
(cat) and ?:? (cow) are both ?livestock?; how-
ever, they should use different classifiers. In or-
der to improve the performance of our approach,
we need an ontology that correctly groups nouns
into classes according to their semantic properties
(e.g. type, shape, color, size).
For another knowledge-rich approach, we
could use a complex ontology plus a Chinese
classifier dictionary that describes the properties
of the objects each classifier can modify. By
comparing noun properties and classifier char-
acteristics, classifier assignment could be im-
proved as long as the nouns are in the ontol-
ogy. However, there are many idiomatic noun-
classifier matchings that can not be categorised
by dictionaries. Therefore, a combination of rule-
30
based and machine-learning approaches seems
most promising.
Third, we can classify Chinese classifers into
groups and focus on those that modify single ob-
jects. Certain Chinese classifiers can be used
before all plural nouns. Some classifiers spec-
ify the container of the objects, for example,
?[yi] {[lanzi] ?*[pingguo]? (a basket of
apples). The classifier changes when the con-
tainer changes. These can be treated differently
from sortal and anaphoric classifiers.
Acknowledgements
We thank the anonymous reviewers for their help-
ful feedback, HowNet for giving us permission to
download their knowledge base, and Chih-Chung
Chang and Chih-Jen Lin for the implementation
of LIBSVM.
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA), through the Department of the Interior,
NBC, Acquisition Services Division, under Con-
tract No. NBCHD030010.
References
B. Boser, I. Guyon, and V. Vapnik. 1992. A training
algorithm for optimal margin classifiers. In Pro-
ceedings of the Fifth Annual ACM Conference on
Computational Learning Theory (COLT 1992).
Chih-Chung Chang and Chih-Jen Lin,
2001. LIBSVM: a library for support
vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Z. Dong and Q. Dong. 2000. Introduction
to HowNet - Chinese message structure base.
http://www.keenage.com.
L. Fang. 2003. Research of Chinese lexicon teaching
- quantities. Journal of Secondary Education.
D. Graf and K. Chen. 2003. Chinese gigaword. LDC
Catalog Number LDC2003T09.
C. Hsu and C. Lin. 2002. A comparison of methods
for multi-class support vector machines. In IEEE
Transactions on Neural Networks.
K. Paik and F. Bond. 2001. Multilingual generation
of numeral classifiers using a common ontology. In
Proceedings of the 19th International Conference
on Computer Processing of Oriental Languages.
M. Paul, E. Sumita, and S. Yamamoto. 2002. Corpus-
based generation of numeral classifier using phrase
alignment. In Proceedings of the 19th International
Conference on Computational Linguistics.
V. Sornlertlamvanich, W. Pantachat, and S. Meknavin.
1994. Classifier assignment by corpus-based ap-
proach. In Proceedings of the 15th Conference on
Computational Linguistics.
N. Xue, F. Chiou, and M. Palmer. 2002. Building a
large-scale annotated Chinese corpus. In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics.
31
Proceedings of NAACL HLT 2009: Short Papers, pages 229?232,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Determining the Position of Adverbial Phrases in English
Huayan Zhong and Amanda Stent
Computer Science Department
Stony Brook University
Stony Brook, NY 11794, USA
zhong@cs.sunysb.edu, amanda.stent@stonybrook.edu
Abstract
In this paper we compare three approaches to
adverbial positioning using lexical, syntactic,
semantic and sentence-level features. We find
that: (a), one- and two-stage classification-
based approaches can achieve almost 86% ac-
curacy in determining the absolute position of
adverbials; (b) a classifier trained with only
syntactic features gives performance close to
that of a classifier trained with all features; and
(c) a surface realizer incorporating a two-stage
classifier for adverbial positioning as the sec-
ond stage gives improvements of at least 10%
in simple string accuracy over a baseline real-
izer for sentences containing adverbials.
1 Introduction
The job of a surface realizer is to transform an input
semantic/syntactic form into a sequence of words.
This task includes word choice, and word and con-
stituent ordering. In English, the positions of re-
quired elements of a sentence, verb phrase or noun
phrase are relatively fixed. However, many sen-
tences also include adverbials whose position is not
fixed (Figure 1). There may be several appropriate
positions for an adverbial in a particular context, but
other positions give output that is non-idiomatic or
disfluent, ambiguous, or incoherent.
Some computational research has included mod-
els for adjunct ordering (e.g. (Ringger et al, 2004;
Marciniak and Strube, 2004; Elhadad et al, 2001)).
However, this is the first computational study to look
specifically at adverbials. Adverbial positioning has
long been studied in linguistics (e.g. (Keyser, 1968;
Allen and Cruttenden, 1974; Ernst, 1984; Haider,
2000)). Most linguistic research focuses on whether
adverbial placement is functional or semantic in na-
ture. However, Costa (2004) takes a more flexi-
ble feature-based approach that uses: lexical fea-
tures (e.g. phonological shape, ambiguity of mean-
ing, categorical status); syntactic features (e.g. pos-
sible adjunction sites, directionality of adjunction,
domain of modification); and information structure
features (e.g. focus, contrast). We decided to evalu-
ate Costa?s approach computationally, using features
automatically extracted from an annotated corpus.
In this paper, we compare three approaches to ad-
verbial positioning: a simple baseline approach us-
ing lexical and syntactic features, and one- and two-
stage classification-based approaches using lexical,
syntactic, semantic and sentence-level features. We
apply these approaches in a hybrid surface realizer
that uses a probabilistic grammar to produce real-
ization alternatives, and a second-stage classifier to
select among alternatives. We find that: (a) One-
and two-stage classification-based approaches can
achieve almost 86% accuracy in determining the ab-
solute position of adverbials; (b) A classifier trained
with only syntactic features gives performance close
to that of a classifier trained with all features; and (c)
A surface realizer using a two-stage classifier for ad-
verbial positioning can get improvements of at least
10% in simple string accuracy over a baseline real-
izer for sentences containing adverbials.
As well as being useful for surface realization, a
model of adverbial ordering can be used in machine
translation (e.g. (Ogura et al, 1997)), language
learning software (e.g. (Leacock, 2007; Burstein et
al., 2004)), and automatic summarization (e.g. (El-
hadad et al, 2001; Clarke and Lapata, 2007; Mad-
nani et al, 2007)).
229
Figure 1: Example syntax tree for Then she cashed the
check at your bank on Tuesday with adverbials circled
and possible VP and S adverbial positions in squares.
2 Data and Features
From the sentences in the Wall Street Journal (WSJ)
and Switchboard (SWBD) sections of the Penn
Treebank III (Marcus et al, 1999), we extracted all
NP, PP and ADVP phrases labeled with the adver-
bial tags -BNF, -DIR, -EXT, -LOC, -MNR, -PRP,
-TMP or -ADV. These phrases mostly modify S con-
stituents (including RRC, S, SBAR, SBARQ, SINV,
SQ), VP constituents, or NP constituents (includ-
ing NP and WHNP), but also modify other adjuncts
(PP, ADJP or ADVP) and other phrase types (FRAG,
INTJ, LST, NAC, PRT, QP, TOP, UCP, X).
Corpus Number of adverbials of type:
PP-ADVP NP-ADVP ADVP
WSJ 36128 10587 13700
SWBD 12231 5405 17193
Table 1: Adverbials in the Penn Treebank III
For each adverbial, we automatically extracted
lexical, syntactic, semantic and discourse features.
We included features similar to those in (Costa,
2004) and from our own previous research on prepo-
sitional phrase ordering (Zhong and Stent, 2008).
Due to the size of our data set, we could only use
features that can be extracted automatically, so some
features were approximated. We dropped adverbials
for which we could not get features, such as empty
adverbials. Tables 1 and 2 summarize the resulting
data. A list of the features we used in our classifi-
cation experiment appears in Table 3. We withheld
10% of this data for our realization experiment.
3 Classification Experiment
Our goal is to determine the position of an adverbial
with respect to its siblings in the phrase of which it
Adverbial Data Set
Type WSJ SWBD
S 8196 5144
VP 29734 22845
NP 12985 2071
PP/ADJP/ADVP 1739 987
Other 297 686
Table 2: Adverbials in the Penn Treebank III
is a part. An adverbial may have non-adverbial sib-
lings, whose position is typically fixed. It may also
have other adverbial siblings. In the sentence in Fig-
ure 1, at your bank has one adverbial and two non-
adverbial siblings. If this adverbial were placed at
positions VP:0 or VP:1 the resulting sentence would
be disfluent but meaningful; placed at position VP:2
the resulting sentence is fluent, meaningful and id-
iomatic. (In this sentence, both orderings of the two
adverbials at position VP:2 are valid.)
3.1 Approaches
We experimented with three approaches to adverbial
positioning.
Baseline Our baseline approach has two stages. In
the first stage the position of each adverbial with
respect to its non-adverbial siblings is determined:
each adverbial is assigned the most likely position
given its lexical head and category (PP, NN, ADVP).
In the second stage, the relative ordering of adjacent
adverbials is determined in a pairwise fashion (cf.
(Marciniak and Strube, 2004)): the ordering of a pair
of adverbials is assigned to be the most frequent in
the training data, given the lexical head, adverbial
phrase type, and category of each adverbial.
One-stage For our one-stage classification-based
approach, we determine the position of all adver-
bials in a phrase at one step. There is one feature
vector for each phrase containing at least one adver-
bial. It contains features for all non-adverbial sib-
lings in realization order, and then for each adverbial
sibling in alphabetical order by lexical head. The la-
bel is the order of the siblings. For example, for the
S-modifying adverbial in Figure 1, the label would
be 2 0 1, where 0 = ?she?, 1 = ?cashed? and 2 =
?Then?. If there are n siblings, then there are n!
possible labels for each feature vector, so the perfor-
mance of this classifier by chance would be .167 if
each adverbial has on average three siblings.
230
Type Features
lexical preposition in this adverbial and in adverbial siblings 0-4; stems of lexical heads of this adverbial,
its parent, non-adverbial siblings 0-4, and adverbial siblings 0-4; number of phonemes in lexical
head of this adverbial and in lexical heads of adverbial siblings 0-4; number of words in this
adverbial and in adverbial siblings 0-4
syntactic syntactic categories of this adverbial, its parent, non-adverbial siblings 0-4, and adverbial sib-
lings 0-4; adverbial type of this adverbial and of adverbial siblings 0-4 (one of DIR, EXT, LOC,
MNR, PRP, TMP, ADV); numbers of siblings, non-adverbial siblings, and adverbial siblings
semantic hypernyms of heads of this adverbial, its parent, non-adverbial siblings 0-4, and adverbial sib-
lings 0-4; number of meanings for heads of this adverbial and adverbial siblings 0-4 (using
WordNet)
sentence sequence of children of S node (e.g. NP VP, VP); form of sentence (declarative, imperative,
interrogative, clause-other); presence of the following in the sentence: coordinating conjunc-
tion(s), subordinating conjunction(s), correlative conjunction(s), discourse cue(s) (e.g. ?how-
ever?, ?therefore?), pronoun(s), definite article(s)
Table 3: Features used for determining adverbial positions. We did not find phrases with more than 5 adverbial siblings
or more more than 5 non-adverbial siblings. If a phrase did not have 5 adverbial or non-adverbial siblings, NA values
were used in the features for those siblings.
Two-stage For our two-stage classification-based
approach, we first determine the position of each ad-
verbial in a phrase in relation to its non-adverbial
siblings, and then the relative positions of adjacent
adverbials. For the first stage we use a classifier.
There is one feature vector for each adverbial. It
contains features for all non-adverbial siblings in re-
alization order, then for each adverbial sibling in al-
phabetical order by lexical head, and finally for the
target adverbial itself. The label is the position of
the target adverbial with respect to the non-adverbial
siblings. For our example sentence in Figure 1, the
label for ?Then? would be 0; for ?at the bank? would
be 2, and for ?on Tuesday? would be 2. If there are n
non-adverbial siblings, then there are n+1 possible
labels for each feature vector, so the performance of
this classifier by chance would be .25 if each adver-
bial has on average three non-adverbial siblings.
For the second stage we use the same second stage
as the baseline approach.
3.2 Method
We use 10-fold cross-validation to compute perfor-
mance of each approach. For the classifiers, we used
the J4 decision tree classifier provided by Weka1.
We compute correctness for each approach as the
percentage of adverbials for which the approach out-
puts the same position as that found in the original
1We experimented with logistic regression and SVM classi-
fiers; the decision tree classifier gave the highest performance.
human-produced phrase. (In some cases, multiple
positions for the adverbial would be equally accept-
able, but we cannot evaluate this automatically.)
3.3 Results
Our classification results are shown in Table 4. The
one- and two-stage approaches both significantly
outperform baseline. Also, the two-stage approach
outperforms the one-stage approach for WSJ.
The decision trees using all features are quite
large. We tried dropping feature sets to see if we
could get smaller trees without large drops in per-
formance. We found that for all data sets, the
models containing only syntactic features perform
only about 1% worse for one-stage classification and
only about 3% worse for two-stage classification,
while in most cases giving much smaller trees (1015
[WSJ] and 972 [SWBD] nodes for the one-stage ap-
proach; 1008 [WSJ] and 877 [SWBD] for the two-
stage approach). This is somewhat surprising given
Costa?s arguments about the need for lexical and dis-
course features; it may be due to errors introduced
by approximating discourse features automatically,
as well as to data sparsity in the lexical features.
There are only small performance differences be-
tween the classifiers for speech and those for text.
4 Realization Experiment
To investigate how a model of adverbial position-
ing may improve an NLP application, we incorpo-
231
Approach Tree Classification SSA
size accuracy
WSJ
baseline n/a 45.98 75.1
one-stage 6519 84.43 82.2
two-stage 1053 86.27 85.1
SWBD
baseline n/a 41.48 61.3
one-stage 4486 85.13 74.5
two-stage 3707 85.01 73.1
Table 4: Performance of adverbial position determination
rated our best-performing models into a surface re-
alizer. We automatically extracted a probabilistic
lexicalized tree-adjoining grammar from the whole
WSJ and SWBD corpora minus our held-out data,
using the method described in (Zhong and Stent,
2005). We automatically re-realized all adverbial-
containing sentences in our held-out data (10%), af-
ter first automatically constructing input using the
method described in (Zhong and Stent, 2005).
We compute realization performance using sim-
ple string accuracy (SSA)2. Realization performance
is reported in Table 4. Both classification-based ap-
proaches outperform baseline, with the two-stage
approach performing best for WSJ with either met-
ric (for SWBD, the classification-based approaches
perform similarly).
5 Conclusions and Future Work
In this paper, we tested classification-based ap-
proaches to adverbial positioning. We showed that
we can achieve good results using syntactic features
alone, with small improvements from adding lexi-
cal, semantic and sentence-level features. We also
showed that use of a model for adverbial position-
ing leads to improved surface realization. In future
work, we plan a human evaluation of our results to
see if more features could lead to performance gains.
6 Acknowledgments
This research was partially supported by the NSF
under grant no. 0325188.
2Although in general we do not find SSA to be a reliable
metric for evaluating surface realizers, in this case it is valid
because lexical selection is done already; only the positions of
adverbials will generally be different.
References
D. Allen and A. Cruttenden. 1974. English sentence ad-
verbials: Their syntax and their intonation in British
English. Lingua, 34:1?30.
J. Burstein, M. Chodorow, and C. Leacock. 2004. Auto-
mated essay evaluation: The Criterion online writing
service. AI Magazine, 25(3):27?36.
J. Clarke and M. Lapata. 2007. Modelling compres-
sion with discourse constraints. In Proceedings of
EMNLP/CoNLL.
J. Costa. 2004. A multifactorial approach to adverb
placement: assumptions, facts, and problems. Lingua,
114:711?753.
M. Elhadad, Y. Netzer, R. Barzilay, and K. McKeown.
2001. Ordering circumstantials for multi-document
summarization. In Proceedings of BISFAI.
Thomas Ernst. 1984. Towards an integrated theory of
adverb position in English. Ph.D. thesis, Indiana Uni-
versity, Bloomington, Indiana.
H. Haider. 2000. Adverb placement. Theoretical lin-
guistics, 26:95?134.
J. Keyser. 1968. Adverbial positions in English. Lan-
guage, 44:357?374.
C. Leacock. 2007. Writing English as a second
language: A proofreading tool. In Proceedings of
the Workshop on optimizing the role of language in
technology-enhanced learning.
N. Madnani, D. Zajic, B. Dorr, N. F. Ayan, and J. Lin.
2007. Multiple alternative sentence compressions for
automatic text summarization. In Proceedings of the
Document Understanding Conference.
T. Marciniak and M. Strube. 2004. Classification-
based generation using TAG. In Lecture Notes
in Computer Science, volume 3123/2004. Springer
Berlin/Heidelberg.
M. Marcus, B. Santorini, M. Marcinkiewicz, and A. Tay-
lor. 1999. Treebank-3. Available from the Linguistic
Data Consortium, Catalog Number LDC99T42.
K. Ogura, S. Shirai, and F. Bond. 1997. English ad-
verb processing in Japanese-to-English machine trans-
lation. In Seventh International Conference on Theo-
retical and Methodological Issues in Machine Trans-
lation.
E. Ringger, M. Gamon, R. Moore, D. Rojas, M. Smets,
and S. Corston-Oliver. 2004. Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In Proceedings of COLING.
H. Zhong and A. Stent. 2005. Building surface realiz-
ers automatically from corpora using general-purpose
tools. Proceedings of UCNLG.
H. Zhong and A. Stent. 2008. A corpus-based compari-
son of models for predicting ordering of prepositional
phrases. In submission.
232
