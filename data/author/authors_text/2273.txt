Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 667?674,
Sydney, July 2006. c?2006 Association for Computational Linguistics
URES : an Unsupervised Web Relation Extraction System 
Benjamin Rosenfeld 
Computer Science Department 
Bar-Ilan University 
Ramat-Gan, ISRAEL 
grurgrur@gmail.com 
Ronen Feldman 
Computer Science Department 
Bar-Ilan University 
Ramat-Gan, ISRAEL 
feldman@cs.biu.ac.il 
 
 
Abstract 
Most information extraction systems ei-
ther use hand written extraction patterns 
or use a machine learning algorithm that 
is trained on a manually annotated cor-
pus. Both of these approaches require 
massive human effort and hence prevent 
information extraction from becoming 
more widely applicable. In this paper we 
present URES (Unsupervised Relation 
Extraction System), which extracts rela-
tions from the Web in a totally unsuper-
vised way. It takes as input the 
descriptions of the target relations, which 
include the names of the predicates, the 
types of their attributes, and several seed 
instances of the relations. Then the sys-
tem downloads from the Web a large col-
lection of pages that are likely to contain 
instances of the target relations. From 
those pages, utilizing the known seed in-
stances, the system learns the relation 
patterns, which are then used for extrac-
tion. We present several experiments in 
which we learn patterns and extract in-
stances of a set of several common IE re-
lations, comparing several pattern 
learning and filtering setups. We demon-
strate that using simple noun phrase tag-
ger is sufficient as a base for accurate 
patterns. However, having a named en-
tity recognizer, which is able to recog-
nize the types of the relation attributes 
significantly, enhances the extraction 
performance. We also compare our ap-
proach with KnowItAll?s fixed generic 
patterns. 
1 Introduction 
The most common preprocessing technique for 
text mining is information extraction (IE). It is 
defined as the task of extracting knowledge out 
of textual documents. In general, IE is divided 
into two main types of extraction tasks ? Entity 
tagging and Relation extraction. 
The main approaches used by most informa-
tion extraction systems are the knowledge engi-
neering approach and the machine learning 
approach. The knowledge engineering (mostly 
rule based) systems traditionally were the top 
performers in most IE benchmarks, such as 
MUC (Chinchor, Hirschman et al 1994), ACE 
and the KDD CUP (Yeh and Hirschman 2002). 
Recently though, the machine learning systems 
became state-of-the-art, especially for simpler 
tagging problems, such as named entity recogni-
tion (Bikel, Miller et al 1997), or field extrac-
tion (McCallum, Freitag et al 2000). The 
general idea is that a domain expert labels the 
target concepts in a set of documents. The sys-
tem then learns a model of the extraction task, 
which can be applied to new documents auto-
matically. 
Both of these approaches require massive hu-
man effort and hence prevent information extrac-
tion from becoming more widely applicable. In 
order to minimize the huge manual effort in-
volved with building information extraction sys-
tems, we have designed and developed URES 
(Unsupervised Relation Extraction System) 
which learns a set of patterns to extract relations 
from the web in a totally unsupervised way. The 
system takes as input the names of the target re-
lations, the types of its arguments, and a small 
set of seed instances of the relations. It then uses 
a large set of unlabeled documents downloaded 
from the Web in order to build extraction pat-
terns. URES patterns currently have two modes 
of operation. One is based upon a generic shal-
low parser, able to extract noun phrases and their 
667
heads. Another mode builds patterns for use by 
TEG (Rosenfeld, Feldman et al 2004). TEG is a 
hybrid rule-based and statistical IE system. It 
utilizes a trained labeled corpus in order to com-
plement and enhance the performance of a rela-
tively small set of manually-built extraction 
rules. When it is used with URES, the relation 
extraction rules and training data are not built 
manually but are created automatically from the 
URES-learned patterns. However, URES does 
not built rules and training data for entity extrac-
tion. For those, we use the grammar and training 
data we developed separately. 
It is important to note that URES is not a clas-
sic IE system. Its purpose is to extract as many 
as possible different instances of the given rela-
tions while maintaining a high precision. Since 
the goal is to extract instances and not mentions, 
we are quite willing to miss a particular sentence 
containing an instance of a target relation ? if the 
instance can be found elsewhere. In contrast, the 
classical IE systems extract mentions of entities 
and relations from the input documents. This 
difference in goals leads to different ways of 
measuring the performance of the systems. 
The rest of the paper is organized as follows: 
in Section 2 we present the related work. In Sec-
tion 3 we outline the general design principles of 
URES and the architecture of the system and 
then describe the different components of URES 
in details while giving examples to the input and 
output of each component. In Section 4 we pre-
sent our experimental evaluation and then wrap 
up with conclusions and suggestions for future 
work. 
2 Related Work 
Information Extraction (IE) is a sub-field of 
NLP, aims at aiding people to sift through large 
volume of documents by automatically identify-
ing and tagging key entities, facts and events 
mentioned in the text.  
Over the years, much effort has been invested 
in developing accurate and efficient IE systems. 
Some of the systems are rule-based (Fisher, So-
derland et al 1995; Soderland 1999), some are 
statistical (Bikel, Miller et al 1997; Collins and 
Miller 1998; Manning and Schutze 1999; Miller, 
Schwartz et al 1999) and some are based on in-
ductive-logic-based (Zelle and Mooney. 1996; 
Califf and Mooney 1998). Recent IE research 
with bootstrap learning  (Brin 1998; Riloff and 
Jones 1999; Phillips and Riloff 2002; Thelen and 
Riloff 2002) or learning from documents tagged 
as relevant (Riloff 1996; Sudo, Sekine et al 
2001) has decreased, but not eliminated hand-
tagged training. 
Snowball (Agichtein and Gravano 2000) is an 
unsupervised system for learning relations from 
document collections. The system takes as input 
a set of seed examples for each relation, and uses 
a clustering technique to learn patterns from the 
seed examples. It does rely on a full fledges 
Named Entity Recognition system. Snowball 
achieved fairly low precision figures (30-50%) 
on relations such as merger and acquisition on 
the same dataset used in our experiments. 
KnowItAll system is a direct predecessor of 
URES. It is developed at University of Washing-
ton by Oren Etzioni and colleagues (Etzioni, 
Cafarella et al 2005). KnowItAll is an autono-
mous, domain-independent system that extracts 
facts from the Web.  The primary focus of the 
system is on extracting entities (unary predi-
cates).  The input to KnowItAll is a set of entity 
classes to be extracted, such as ?city?, ?scien-
tist?, ?movie?, etc., and the output is a list of 
entities extracted from the Web. KnowItAll uses 
a set of manually-built generic rules, which are 
instantiated with the target predicate names, pro-
ducing queries, patterns and discriminator 
phrases. The queries are passed to a search en-
gine, the suggested pages are downloaded and 
processed with patterns. Every time a pattern is 
matched, the extraction is generated and evalu-
ated using Web statistics ? the number of search 
engine hits of the extraction alone and the ex-
traction together with discriminator phrases. 
KnowItAll has also a pattern learning module 
(PL) that is able to learn patterns for extracting 
entities. However, it is unsuitable for learning 
patterns for relations. Hence, for extracting rela-
tions KnowItAll currently uses only the generic 
hand written patterns. 
3 Description of URES 
The goal of URES is extracting instances of rela-
tions from the Web without human supervision. 
Accordingly, the input of the system is limited to 
(reasonably short) definition of the target rela-
tions. The output of the system is a large list of 
relation instances, ordered by confidence. The 
system consists of several largely independent 
components. The Sentence Gatherer generates 
(e.g., downloads from the Web) a large set of 
sentences that may contain target instances. The 
Pattern Learner uses a small number of known 
seed instances to learn likely patterns of relation 
668
occurrences. The Sentence Classifier filters the 
set of sentences, removing those that are unlikely 
to contain instances of the target relations. The 
Instance Extractor extracts the attributes of the 
instances from the sentences, and generates the 
output of the system.  
3.1 Sentence Gatherer 
The Sentence Gatherer is currently implemented 
in a very simple way. It gets a set of keywords as 
input, and proceeds to download all documents 
that contain one of those keywords. From the 
documents, it extracts all sentences that contain 
at least one of the keywords. 
The keywords for a relation are the words that 
are indicative of instances of the relation. The 
keywords are given to the system as part of the 
relation definition. Their number is usually 
small. For instance, the set of keywords for Ac-
quisition in our experiments contains two words 
? ?acquired? and ?acquisition?. Additional key-
words (such as ?acquire?, ?purchased?, and 
?hostile takeover?) can be added automatically 
by using WordNet (Miller 1995).  
3.2 Pattern Learner 
The task of the Pattern Learner is to learn the 
patterns of occurrence of relation instances. This 
is an inherently supervised task, because at least 
some occurrences must be known in order to be 
able to find patterns among them. Consequently, 
the input to the Pattern Learner includes a small 
set (10-15 instances) of known instances for 
each target relation. Our system assumes that the 
seeds are a part of the target relation definition. 
However, the seeds need not be created manu-
ally. Instead, they can be taken from the top-
scoring results of a high-precision low-recall 
unsupervised extraction system, such as 
KnowItAll. The seeds for our experiments were 
produced in exactly this way. 
The Pattern Learner proceeds as follows: first, 
the gathered sentences that contain the seed in-
stances are used to generate the positive and 
negative sets. From those sets the pattern are 
learned. Then, the patterns are post-processed 
and filtered. We shall now describe those steps 
in detail. 
Preparing the positive and negative sets 
The positive set of a predicate (the terms predi-
cate and relation are interchangeable in our 
work) consists of sentences that contain a known 
instance of the predicate, with the instance at-
tributes changed to ?<AttrN>?, where N is the 
attribute index. For example, assuming there is a 
seed instance Acquisition(Oracle, PeopleSoft), 
the sentence 
The Antitrust Division of the U.S. De-
partment of Justice evaluated the likely 
competitive effects of Oracle's proposed 
acquisition of PeopleSoft. 
will be changed to 
The Antitrust Division? ?of <Attr1>'s 
proposed acquisition of <Attr2>. 
The positive set of a predicate P is generated 
straightforwardly, using substring search. 
The negative set of a predicate consists of 
similarly modified sentences with known false 
instances of the predicate. We build the negative 
set as a union of two subsets. The first subset is 
generated from the sentences in the positive set 
by changing the assignment of one or both at-
tributes to some other suitable entity. In the first 
mode of operation, when only a shallow parser is 
available, any suitable noun phrase can be as-
signed to an attribute. Continuing the example 
above, the following sentences will be included 
in the negative set:  
<Attr1> of <Attr2> evaluated the likely? 
<Attr2> of the U.S. ? ?acquisition of 
<Attr1>. 
etc. 
In the second mode of operation, when the 
NER is available, only entities of the correct 
type get assigned to an attribute. 
The other subset of the negative set contains 
all sentences produced in a similar way from the 
positive sentences of all other target predicates. 
We assume without loss of generality that the 
predicates that are being extracted simultane-
ously are all disjoint. In addition, the definition 
of each predicate indicates whether the predicate 
is symmetric (like ?merger?) or antisymmetric 
(like ?acquisition?). In the former case, the sen-
tences produced by exchanging the attributes in 
positive sentences are placed into the positive 
set, and in the later case ? into the negative set of 
the predicate. 
The following pseudo code shows the process 
of generating the positive and negative sets in 
detail: 
669
Let S be the set of gathered sentences.  
For each predicate P 
    For each s?S containing a word from Keywords(P) 
        For each known seed P(A1, A2) of the predicate P 
            If A1 and A2 are each found exactly once inside s 
                For all entities e1, e2 ? s, such that e2 ? e1, and 
                            Type(e1) = type of Attr1 of P, and 
                            Type(e2) = type of Attr2 of P 
                    Let s' := s  with eN changed to ?<AttrN>?. 
                    If e1 = A1 and e2 = A2 
                        Add  s'  to the PositiveSet(P). 
                    Elseif e1 = A2 and e2 = A1 and symmetric(P) 
                        Add s' to the PositiveSet(P). 
                    Else 
                        Add s' to the NegativeSet(P). 
For each predicate P 
    For each predicate P2 ? P 
        For each sentence s ? PositiveSet(P2) 
            Put s into the NegativeSet(P). 
Generating the patterns 
The patterns for predicate P are generalizations 
of pairs of sentences from the positive set of P. 
The function Generalize(S1, S2)  is applied to 
each pair of sentences S1 and S2 from the positive 
set of the predicate. The function generates a 
pattern that is the best (according to the objective 
function defined below) generalization of its two 
arguments. The following pseudo code shows 
the process of generating the patterns: 
For each predicate P 
    For each pair S1, S2 from PositiveSet(P) 
        Let Pattern := Generalize(S1, S2). 
        Add Pattern to PatternsSet(P). 
The patterns are sequences of tokens, skips 
(denoted *), limited skips (denoted *?) and slots. 
The tokens can match only themselves, the skips 
match zero or more arbitrary tokens, and slots 
match instance attributes.  The limited skips 
match zero or more arbitrary tokens, which must 
not belong to entities of the types equal to the 
types of the predicate attributes. The General-
ize(s1, s2) function takes two patterns (note, that 
sentences in the positive and negative sets are 
patterns without skips) and generates the least 
(most specific) common generalization of both. 
The function does a dynamical programming 
search for the best match between the two pat-
terns (Optimal String Alignment algorithm), 
with the cost of the match defined as the sum of 
costs of matches for all elements. We use the 
following numbers:  two identical elements 
match at cost 0, a token matches a skip or an 
empty space at cost 10, a skip matches an empty 
space at cost 2, and different kinds of skip match 
at cost 3. All other combinations have infinite 
cost. After the best match is found, it is con-
verted into a pattern by copying matched identi-
cal elements and adding skips where non-
identical elements are matched. For example, 
assume the sentences are 
Toward this end, <Attr1> in July acquired 
<Attr2> 
Earlier this year, <Attr1> acquired <Attr2> 
from X 
After the dynamical programming-based 
search, the following match will be found: 
 
Table 1 - Best Match between Sentences 
Toward (cost 10)
Earlier   (cost 10)
this this (cost 0)
end (cost 10)
year (cost 10)
, , (cost 0)
<Attr1 > <Attr1 > (cost 0)
in  July (cost 20)
acquired acquired (cost 0)
<Attr2 > <Attr2 > (cost 0)
from (cost 10)
X (cost 10)  
 
at total cost = 80. The match will be converted to 
the pattern (assuming the NER mode, so the only 
entity belonging to the same type as one of the 
attributes is ?X?): 
*? *? this *? *? , <Attr1> *? acquired <Attr2> *? * 
which becomes, after combining adjacent skips, 
*?  this  *?  ,  <Attr1>  *?  acquired  <Attr2>   * 
Note, that the generalization algorithm allows 
patterns with any kind of elements beside skips, 
such as CapitalWord, Number, CapitalizedSe-
quence, etc. As long as the costs and results of 
matches are properly defined, the Generalize 
function is able to find the best generalization of 
any two patterns. However, in the present work 
we stick with the simplest pattern definition as 
described above. 
Post-processing, filtering, and scoring 
The number of patterns generated at the previous 
step is very large. Post-processing and filtering 
tries to reduce this number, keeping the most 
useful patterns and removing the too specific and 
irrelevant ones. 
First, we remove from patterns all ?stop 
words? surrounded by skips from both sides, 
670
such as the word ?this? in the last pattern in the 
previous subsection. Such words do not add to 
the discriminative power of patterns, and only 
needlessly reduce the pattern recall. The list of 
stop words includes all functional and very 
common English words, as well as puncuation 
marks. Note, that the stop words are removed 
only if they are surrounded by skips, because 
when they are adjacent to slots or non-stop 
words they often convey valuable information. 
After this step, the pattern above becomes 
       *?  ,  <Attr1>  *?  acquired  <Attr2>   * 
In the next step of filtering, we remove all pat-
terns that do not contain relevant words. For 
each predicate, the list of relevant words is 
automatically generated from WordNet by fol-
lowing all links to depth at most 2 starting from 
the predicate keywords. For example, the pattern 
       *   <Attr1>  *  by  <Attr2>   *   
will be removed, while the pattern 
       *   <Attr1>  *  purchased  <Attr2>  *   
will be kept, because the word ?purchased? can 
be reached from ?acquisition? via synonym and 
derivation links. 
The final (optional) filtering step removes all 
patterns, that contain slots surrounded by skips 
on both sides, keeping only the patterns, whose 
slots are adjacent to tokens or to sentence 
boundaries. Since both the shallow parser and 
the NER system that we use are far from perfect, 
they often place the entity boundaries incor-
rectly. Using only patterns with anchored slots 
significantly improves the precision of the whole 
system. In our experiments we compare the per-
formance of anchored and unanchored patterns. 
The filtered patterns are then scored by their 
performance on the positive and negative sets.  
Currently we use a simple scoring method ? the 
score of a pattern is the number of positive 
matches divided by the number of negative 
matches plus one: 
| { : matches } |
( )
|{ : matches } | 1
S PositiveSet Pattern S
Score Pattern
S NegativeSet Pattern S
?= ? +  
This formula is purely empirical and produces 
reasonable results. The threshold is applied to 
the set of patterns, and all patterns scoring less 
than the threshold (currently, it is set to 6) are 
discarded. 
3.3 Sentence Classifier 
The task of the Sentence Classifier is to filter out 
from the large pool of sentences produced by the 
Sentence Gatherer the sentences that do not con-
tain the target predicate instances. In the current 
version of our system, this is only done in order 
to reduce the number of sentences that need to 
be processed by the Slot Extractor. Therefore, in 
this stage we just remove the sentences that do 
not match any of the regular expressions gener-
ated from the patterns. Regular expressions are 
generated from patterns by replacing slots with 
skips. 
3.4 Instance Extractor 
The task of the Instance Extractor is to use the 
patterns generated by the Pattern Learner on the 
sentences that were passed through by the Sen-
tence Classifier. However, the patterns cannot be 
directly matched to the sentences, because the 
patterns only define the placeholders for instance 
attributes and cannot by themselves extract the 
values of the attributes. 
We currently have two different ways to solve 
this problem ? using a general-purpose shallow 
parser, which is able to recognize noun phrases 
and their heads, and using an information extrac-
tion system called TEG (Rosenfeld, Feldman et 
al. 2004), together with a trained grammar able 
to recognize the entities of the types of the 
predicates? attributes. We shall briefly describe 
the two modes of operation. 
Shallow Parser mode 
In the first mode of operation, the predicates 
may define attributes of two different types: 
ProperName and CommonNP. We assume that 
the values of the ProperName type are always 
heads of proper noun phrases. And the values of 
the 
CommonNP type are simple common noun 
phrases (with possible proper noun modifiers, 
e.g. ?the Kodak camera?). 
We use a Java-written shallow parser from the 
OpenNLP (http://opennlp.sourceforge.net/) 
package. Each sentence is tokenized, tagged with 
part-of-speech, and tagged with noun phrase 
boundaries. The pattern matching and extraction 
is straightforward. 
TEG mode 
TEG (Trainable Extraction Grammars) 
(Rosenfeld, Feldman et al 2004) is general-
671
purpose hybrid rule-based and statistical IE sys-
tem, able to extract entities and relations at the 
sentence level. It is adapted to any domain by 
writing a suitable set of rules, and training them 
using an annotated corpus. The TEG rule lan-
guage is a straightforward extension of a con-
text-free grammar syntax. A complete set of 
rules is compiled into a PCFG (Probabilistic 
Context Free Grammar), which is then trained 
upon the training corpus. 
Some of the nonterminals inside the TEG 
grammar can be marked as target concepts. 
Wherever such nonterminal occurs in a final 
parse of a sentence, TEG generates an output 
label. The target concept rules may specify some 
of their parts as attributes. Then the concept is 
considered to be a relation, with the values of the 
attributes determined by the concept parse. Con-
cepts without attributes are entities.  
For the TEG-based instance extractor we util-
ize the NER ruleset of TEG and an internal train-
ing corpus called INC, as described in 
(Rosenfeld, Feldman et al 2004). The ruleset 
defines a grammar with a set of concepts for 
Person, Location, and Organization entities. In 
addition, the grammar defines a generic Noun-
Phrase concept, which can be used for capturing 
the entities that do not belong to any of the entity 
types above. 
 
In order to do the extraction, the patterns gener-
ated by the Pattern Learner are converted to the 
TEG syntax and added to the pre-built NER 
grammar. This produces a grammar, which is 
able to extract relations. This grammar is trained 
upon the automatically labeled positive set from 
the Pattern Learning. The resulting trained 
model is applied to the sets of sentences pro-
duced by the Sentence Classifier. 
 
4 Experimental Evaluation 
In order to evaluate URES, we used five predi-
cates 
Acquisition(BuyerCompany, BoughtCom-
pany), 
Merger(Company1, Company2), 
CEO_Of(Company, Name), 
MayorOf(City, Name), 
InventorOf(InventorName, Invention). 
Merger is symmetric predicate, in the sense that 
the order of its attributes does not matter. Acqui-
sition is antisymmetric, and the other three are 
tested as bound in the first attribute. For the 
bound predicates, we are only interested in the 
instances with particular prespecified values of 
the first attribute. 
 We test both modes of operation ? using shal-
low parser and using TEG. In the shallow parser 
mode, the Invention attribute of the InventorOf 
predicate is of type CommonNP, and all other 
attributes are of type ProperName. In the TEG 
mode, the ?Company? attributes are of type Or-
ganization, the ?Name? attributes are of type 
Person, the ?City? attribute is of type Location, 
and the ?Invention? attribute is of type Noun-
Phrase. 
We evaluate our system by running it over a 
large set of sentences, counting the number of 
extracted instances, and manually checking a 
random sample of the instances to estimate pre-
cision. In order to be able to compare our results 
with KnowItAll-produced results, we used the 
set of sentences collected by the KnowItAll?s 
crawler as if they were produced by the Sentence 
Gatherer.  
The set of sentences for the Acquisition and 
Merger predicates contained around 900,000 
sentences each. For the other three predicates, 
each of the sentences contained one of the 100 
predefined values for the first attribute. The val-
ues (100 companies for CEO_Of, 100 cities for 
MayorOf, and 100 inventors for InventorOf) are 
entities collected by KnowItAll, half of them are 
frequent entities (>100,000 hits), and another 
half are rare (<10,000 hits). 
In all of the experiments, we use ten top 
predicate instances extracted by KnowItAll for 
the relation seeds needed by the Pattern Learner. 
The results of our experiments are summa-
rized in the Table 2. The table displays the num-
ber of extracted instances and estimated 
precision for three different URES setups, and 
for the KnowItAll manually built patterns. Three 
results are shown for each setup and each rela-
tion ? extractions supported by at least one, at 
least two, and at least three different sentences, 
respectively. 
Several conclusions can be drawn from the re-
sults. First, both modes of URES significantly 
outperform KnowItAll in recall (number of ex-
tractions), while maintaining the same level of 
precision or improving it. This demonstrates util-
ity of our pattern learning component. Second, it 
is immediately apparent, that using only an-
chored patterns significantly improves precision 
of NP Tagger-based URES, though at a high cost 
in recall. The NP tagger-based URES with an-
chored patterns performs somewhat worse than 
672
 
Table 2 - Experimental results. 
 
   Acquisition CEO_Of InventorOf MayorOf Merger 
  support Count Prec Count Prec Count Prec Count Prec Count Prec
? 1 10587 0.74 545 0.7 1233 0.84 2815 0.6 25071 0.71
? 2 815 0.87 221 0.92 333 0.92 717 0.74 2981 0.8 
NP Tagger 
All patterns 
 ? 3 234 0.9 133 0.94 185 0.96 442 0.84 1245 0.88
? 1 5803 0.84 447 0.8 1035 0.86 2462 0.65 17107 0.8 
? 2 465 0.96 186 0.94 284 0.92 652 0.78 2481 0.83
NP Tagger 
Anchored  
patterns ? 3 148 0.98 123 0.96 159 0.96 411 0.88 1084 0.9 
? 1 8926 0.82 618 0.83 2322 0.65 2434 0.85 15002 0.8 
? 2 1261 0.94 244 0.94 592 0.85 779 0.93 2932 0.86
TEG 
All patterns 
 ? 3 467 0.98 158 0.98 334 0.88 482 0.98 1443 0.9 
? 1 2235 0.84 421 0.81 604 0.8 725 0.76 3233 0.82KnowItAll 
 ? 2 257 0.98 190 0.98 168 0.92 308 0.92 352 0.92
 
 
 
 
TEG-based URES on all predicates except In-
ventorOf, as expected. For the InventorOf, TEG 
performs worse, because of overly simplistic 
implementation of the NounPhrase concept in-
side the TEG grammar ? it is defined as a se-
quence of zero or more adjectives followed by a 
sequence of nouns. Such definition often leads to 
only part of a correct invention name being ex-
tracted. 
5 Conclusions and Future Work 
We have presented the URES system for autono-
mously extracting relations from the Web. 
URES bypasses the bottleneck created by classic 
information extraction systems that either relies 
on manually developed extraction patterns or on 
manually tagged training corpus. Instead, the 
system relies upon learning patterns from a large 
unlabeled set of sentences downloaded from 
Web. 
One of the topics we would like to further ex-
plore is the complexity of the patterns that we 
learn. Currently we use a very simple pattern 
language that just has 4 types of elements, slots, 
constants and two types of skips. We want to see 
if we can achieve higher precision with more 
complex patterns. In addition we would like to 
test URES on n-ary predicates, and to extend the 
system to handle predicates that are allowed to 
lack some of the attributes. 
 
 
References  
Agichtein, E. and L. Gravano (2000). Snowball: Ex-
tracting Relations from Large Plain-Text Collec-
tions. Proceedings of the 5th ACM International 
Conference on Digital Libraries (DL). 
Bikel, D. M., S. Miller, et al (1997). Nymble: a high-
performance learning name-finder. Proceedings of 
ANLP-97: 194-201. 
Brin, S. (1998). Extracting Patterns and Relations 
from the World Wide Web. WebDB Workshop, 
EDBT '98. 
Califf, M. E. and R. J. Mooney (1998). Relational 
Learning of Pattern-Match Rules for Information 
Extraction. Working Notes of AAAI Spring Sym-
posium on Applying Machine Learning to    Dis-
course Processing. Menlo Park, CA, AAAI Press: 
6-11. 
Chinchor, N., L. Hirschman, et al (1994). "Evaluat-
ing Message Understanding Systems: An Analysis 
of the Third Message Understanding Conference 
(MUC-3)." Computational Linguistics 3(19): 409-
449. 
Collins, M. and S. Miller (1998). Semantic Tagging 
using a Probabilistic Context Free Grammar. Pro-
ceedings of the Sixth Workshop on Very Large 
Corpora. 
Etzioni, O., M. Cafarella, et al (2005). "Unsupervised 
named-entity extraction from the Web: An ex-
perimental study." Artificial Intelligence. 
Fisher, D., S. Soderland, et al (1995). Description of 
the UMass Systems as Used for MUC-6. 6th Mes-
sage Understanding Conference: 127-140. 
673
Manning, C. and H. Schutze (1999). Foundations of 
Statistical Natural Language Processing. Cam-
bridge, US, The MIT Press. 
McCallum, A., D. Freitag, et al (2000). Maximum 
Entropy Markov Models for Information Extrac-
tion and    Segmentation. Proc. 17th International 
Conf. on Machine Learning, Morgan Kaufmann, 
San Francisco, CA: 591-598. 
Miller, D., R. Schwartz, et al (1999). Named entity 
extraction from broadcast news. Proceedings of 
DARPA Broadcast News Workshop. Herndon, 
VA. 
Miller, G. A. (1995). "WordNet: A lexical database 
for English." CACM 38(11): 39-41. 
Phillips, W. and E. Riloff (2002). Exploiting Strong 
Syntactic Heuristics and Co-Training to Learn 
Semantic Lexicons. Conference on Empirical 
Methods in Natural Language Processing 
(EMNLP 2002). 
Riloff, E. (1996). Automatically Generating Extrac-
tion Patterns from Untagged Text. AAAI/IAAI, 
Vol. 2: 1044-1049. 
Riloff, E. and R. Jones (1999). Learning Dictionaries 
for Information Extraction by Multi-level Boot-
strapping. Proceedings of the Sixteenth National 
Conference on Artificial    Intelligence, The AAAI 
Press/MIT Press: 1044-1049. 
Rosenfeld, B., R. Feldman, et al (2004). TEG: a hy-
brid approach to information extraction. CIKM 
2004, Arlington, VA. 
Soderland, S. (1999). "Learning Information Extrac-
tion Rules for Semi-Structured and Free Text." 
Machine Learning 34(1-3): 233-272. 
Sudo, K., S. Sekine, et al (2001). Automatic pattern 
acquisition for Japanese information extraction. 
Human Language Technology Conference 
(HTL2001). 
Thelen, M. and E. Riloff (2002). A Bootstrapping 
Method for Learning Semantic Lexicons using 
Extraction Pattern Contexts. Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP 2002). 
Yeh, A. and L. Hirschman (2002). "Background and 
overview for kdd cup 2002 task 1: Information ex-
traction from biomedical articles." KDD Ex-
plorarions 4(2): 87-89. 
Zelle, J. M. and R. J. Mooney. (1996). Learning to 
parse database queries using inductive logic pro-
gramming. 13th National Conference on Artificial 
Intelligence (AAAI-96). 
 
 
674
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 473?481,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Boosting Unsupervised Relation Extraction by Using NER 
Ronen Feldman 
Computer Science Department 
Bar-Ilan University 
Ramat-Gan, ISRAEL 
feldman@cs.biu.ac.il 
Benjamin Rosenfeld 
Computer Science Department 
Bar-Ilan University 
Ramat-Gan, ISRAEL 
grurgrur@gmail.com 
 
Abstract 
Web extraction systems attempt to use 
the immense amount of unlabeled text 
in the Web in order to create large lists 
of entities and relations. Unlike 
traditional IE methods, the Web 
extraction systems do not label every 
mention of the target entity or relation, 
instead focusing on extracting as many 
different instances as possible while 
keeping the precision of the resulting 
list reasonably high. URES is a Web 
relation extraction system that learns 
powerful extraction patterns from 
unlabeled text, using short descriptions 
of the target relations and their 
attributes. The performance of URES is 
further enhanced by classifying its 
output instances using the properties of 
the extracted patterns. The features we 
use for classification and the trained 
classification model are independent 
from the target relation, which we 
demonstrate in a series of experiments. 
In this paper we show how the 
introduction of a simple rule based 
NER can boost the performance of 
URES on a variety of relations. We 
also compare the performance of 
URES to the performance of the state-
of-the-art KnowItAll system, and to the 
performance of its pattern learning 
component, which uses a simpler and 
less powerful pattern language than 
URES. 
1   Introduction 
Information Extraction (IE) (Riloff 1993; 
Cowie and Lehnert 1996; Grishman 1996; 
Grishman 1997; Kushmerick, Weld et al 1997; 
Freitag 1998; Freitag and McCallum 1999; 
Soderland 1999)  is the task of extracting 
factual assertions from text. 
Most IE systems rely on knowledge 
engineering or on machine learning to generate 
extraction patterns ? the mechanism that 
extracts entities and relation instances from 
text. In the machine learning approach, a 
domain expert labels instances of the target 
relations in a set of documents. The system 
then learns extraction patterns, which can be 
applied to new documents automatically. 
Both approaches require substantial human 
effort, particularly when applied to the broad 
range of documents, entities, and relations on 
the Web.   In order to minimize the manual 
effort necessary to build Web IE systems, we 
have designed and implemented URES 
(Unsupervised Relation Extraction System). 
URES takes as input the names of the target 
relations and the types of their arguments. It 
then uses a large set of unlabeled documents 
downloaded from the Web in order to learn the 
extraction patterns. 
URES is most closely related to the 
KnowItAll system developed at University of 
Washington by Oren Etzioni and colleagues 
(Etzioni, Cafarella et al 2005), since both are 
unsupervised and both leverage relation-
independent extraction patterns to 
automatically generate seeds, which are then 
fed into a pattern-learning component.  
KnowItAll is based on the observation that the 
Web corpus is highly redundant. Thus, its 
selective, high-precision extraction patterns 
readily ignore most sentences, and focus on 
sentences that indicate the presence of relation 
instances with very high probability. 
 In contrast, URES is based on the 
observation that, for many relations, the Web 
corpus has limited redundancy, particularly 
when one is concerned with less prominent 
instances of these relations (e.g., the 
acquisition of Austria Tabak).  Thus, URES 
utilizes a more expressive extraction pattern 
language, which enables it to extract 
information from a broader set of sentences.  
URES relies on a sophisticated mechanism to 
473
assess its confidence in each extraction, 
enabling it to sort extracted instances, thereby 
improving its recall without sacrificing 
precision. 
 
Our main contributions are as follows: 
 
? We introduce the first domain-
independent system to extract relation 
instances from the Web with both high 
precision and high recall. 
? We show how to minimize the human 
effort necessary to deploy URES for 
an arbitrary set of relations, including 
automatically generating and labeling 
positive and negative examples of the 
relation.  
? We show how we can integrate a 
simple NER component into the 
classification scheme of URES in 
order to boost recall between 5-15% 
for similar precision levels. 
? We report on an experimental 
comparison between URES, URES-
NER and the state-of-the-art 
KnowItAll system, and show that 
URES can double or even triple the 
recall achieved by KnowItAll for 
relatively rare relation instances. 
 
The rest of the paper is organized as 
follows:  Section 2 describes previous work.  
Section 3 outlines the general design principles 
of URES, its architecture, and then describes 
each URES component in detail.  Section 4 
presents our experimental evaluation.  Section 
5 contains conclusions and directions for future 
work. 
2   Related Work 
The IE systems most similar to URES are 
based on bootstrap learning: Mutual 
Bootstrapping (Riloff and Jones 1999), the 
DIPRE system (Brin 1998), and the Snowball 
system (Agichtein and Gravano 2000 ). 
(Ravichandran and Hovy 2002) also use 
bootstrapping, and learn simple surface 
patterns for extracting binary relations from the 
Web. 
Unlike those unsupervised IE systems, 
URES patterns allow gaps that can be matched 
by any sequences of tokens. This makes URES 
patterns much more general, and allows to 
recognize instances in sentences inaccessible 
to the simple surface patterns of systems such 
as (Brin 1998; Riloff and Jones 1999; 
Ravichandran and Hovy 2002). The greater 
power of URES requires different and more 
complex methods for learning, scoring, and 
filtering of patterns. 
Another direction for unsupervised relation 
learning was taken in (Hasegawa, Sekine et al 
2004; Chen, Ji et al 2005). These systems use 
a NER system to identify pairs of entities and 
then cluster them based on the types of the 
entities and the words appearing between the 
entities. Only pairs that appear at least 30 times 
were considered. The main benefit of this 
approach is that all relations between two 
entity types can be discovered simultaneously 
and there is no need for the user to supply the 
relations definitions. Such a system could have 
been used as a preliminary step to URES, 
however its relatively low precision makes it 
unfeasible. Unlike URES, the evaluations 
performed in these papers ignored errors that 
were introduced by the underlying NER 
component. The precision reported by these 
systems (77% breakeven for the COM-COM 
domain) is inferior to that of URES. 
We compared our results directly to two 
other unsupervised extraction systems, the 
Snowball (Agichtein and Gravano 2000 ) and 
KnowItAll. Snowball is an unsupervised 
system for learning relations from document 
collections. The system takes as input a set of 
seed examples for each relation, and uses a 
clustering technique to learn patterns from the 
seed examples. It does rely on a full fledged 
Named Entity Recognition system. Snowball 
achieved fairly low precision figures (30-50%) 
on relations such as Merger and Acquisition on 
the same dataset we used in our experiments. 
KnowItAll is a system developed at 
University of Washington by Oren Etzioni and 
colleagues (Etzioni, Cafarella et al 2005). We 
shall now briefly describe it and its pattern 
learning component. 
Brief description of KnowItAll 
KnowItAll uses a set of generic extraction 
patterns, and automatically instantiates rules by 
combining those patterns with user supplied 
relation labels. For example, KnowItAll has 
patterns for a generic ?of? relation: 
NP1  <relation>  NP2 
NP1  's  <relation>  ,  NP2 
NP2  ,  <relation>  of  NP1 
474
where NP1 and NP2 are simple noun phrases 
that extract values of attribute1 and attribute2 
of a relation, and <relation> is a user-supplied 
string associated with the relation. The rules 
may also constrain NP1 and NP2 to be proper 
nouns. 
The rules have alternating context strings 
(exact string match) and extraction slots 
(typically an NP or head of an NP).  Each rule 
has an associated query used to automatically 
find candidate sentences from a Web search 
engine. 
KnowItAll also includes mechanisms to 
control the amount of search, to merge 
redundant extractions, and to assign a 
probability to each extraction based on 
frequency of extraction or on Web statistics 
(Downey, Etzioni et al 2004). 
KnowItAll-PL.  While those generic rules 
lead to high precision extraction, they tend to 
have low recall, due to the wide variety of 
contexts describing a relation. KnowItAll 
includes a simple pattern learning scheme 
(KnowItAll-PL) that builds on the generic 
extraction mechanism (KnowItAll-baseline). 
Like URES, this is a self-supervised method 
that bootstraps from seeds that are 
automatically extracted by the baseline system. 
KnowItAll-PL creates a set of positive 
training sentences by downloading sentences 
that contain both argument values of a seed 
tuple and also the relation label. Negative 
training is created by downloading sentences 
with only one of the seed argument values, and 
considering a nearby NP as the other argument 
value. This does not guarantee that the 
negative example will actually be false, but 
works well in practice. 
Rule induction tabulates the occurrence of 
context tokens surrounding the argument 
values of the positive training sentences. Each 
candidate extraction pattern has a left context 
of zero to k tokens immediately to the left of 
the first argument, a middle context of all 
tokens between the two arguments, and a right 
context of zero to k tokens immediately to the 
right of the second argument.  A pattern can be 
generalized by dropping the furthest terms 
from the left or right context. KnowItAll-PL 
retains the most general version of each pattern 
that has training frequency over a threshold 
and training precision over a threshold. 
 
 
3   Description of URES 
The goal of URES is extracting instances of 
relations from the Web without human 
supervision. Accordingly, the input of the 
system is limited to (reasonably short) 
definition of the target relations (composed of 
the relation's schema and a few keywords that 
enable gathering relevant sentences). For 
example, this is the description of the 
acquisition relation: 
 
     Acquisition(ProperNP, ProperNP) ordered 
          keywords={"acquired" "acquisition"} 
 
The word ordered indicates that Acquisition 
is not a symmetric relation and the order of its 
arguments matters. The ProperNP tokens 
indicate the types of the attributes. In the 
regular mode, there are only two possible 
attribute types ? ProperNP and CommonNP, 
meaning proper and common noun phrases, 
respectively. When using the NER Filter 
component described in the section 4.1 we 
allow further subtypes of ProperNP, and the 
predicate definition becomes: 
          acquisition(Company, Company) ? 
The keywords are used for gathering 
sentences from the Web and for instantiating 
the generic patterns for seeds generation. 
Additional keywords (such as ?acquire?, 
?purchased?, ?hostile takeover?, etc), which 
can be used for gathering more sentences, are 
added automatically by using WordNet [18]. 
URES consists of several largely 
independent components; their layout is shown 
on the Figure 1. The Sentence Gatherer 
generates (e.g., downloads from the Web) a 
large set of sentences that may contain target 
instances. The Seeds Generator, which is 
essentially equal to the KnowItAll-baseline 
system, uses a small set of generic patterns 
instantiated with the predicate keywords to 
extract a small set of high-confidence instances 
of the target relations. The Pattern Learner uses 
the seeds to learn likely patterns of relation 
occurrences. Then, the Instance Extractor uses 
the patterns to extracts the instances from the 
sentences. Those instances can be filtered by a 
NER Filter, which is an optional part of the 
system. Finally, the Classifier assigns the 
confidence score to each extraction. 
 
 
475
 Sentence 
Gatherer 
Input: 
Target Relations 
Definitions 
Web 
Sentences 
keywords 
Pattern 
Learner 
Instance 
Extractor 
Output: 
Extractions 
Seeds 
Generator 
seeds 
patterns 
NER Filter 
(optional) 
instances 
Classifier 
 
Figure 1. The architecture of URES 
3.1  Pattern Learner 
The task of the Pattern Learner is to learn the 
patterns of occurrence of relation instances. 
This is an inherently supervised task, because 
at least some occurrences must be known in 
order to be able to find patterns among them. 
Consequently, the input to the Pattern Learner 
includes a small set (10 instances in our 
experiments) of known instances for each 
target relation. Our system assumes that the 
seeds are a part of the target relation definition. 
However, the set of seeds need not be created 
manually. Instead, the seeds can be taken 
automatically from the top-scoring results of a 
high-precision low-recall unsupervised 
extraction system, such as KnowItAll. The 
seeds for our experiments were produced in 
exactly this way: we used two generic patterns 
instantiated with the relation name and 
keywords. Those patterns have a relatively 
high precision (although low recall), and the 
top-confidence results, which are the ones 
extracted many times from different sentences, 
have close to 100% probability of being 
correct. 
The Pattern Learner proceeds as follows: 
first, the gathered sentences that contain the 
seed instances are used to generate the positive 
and negative sets. From those sets the patterns 
are learned. Finally, the patterns are post-
processed and filtered. We shall now describe 
those steps in detail. 
 
PREPARING THE POSITIVE AND NEGATIVE 
SETS 
The positive set of a predicate (the terms 
predicate and relation are interchangeable in 
our work) consists of sentences that contain a 
known instance of the predicate, with the 
instance attributes changed to ?<AttrN>?, 
where N is the attribute index. For example, 
assuming there is a seed instance 
Acquisition(Oracle, PeopleSoft), the sentence 
The Antitrust Division of the U.S. Department of 
Justice evaluated the likely competitive effects of 
Oracle's proposed acquisition of PeopleSoft. 
will be changed to 
The Antitrust Division? ?of <Attr1>'s proposed 
acquisition of <Attr2>. 
The positive set of a predicate P is generated 
straightforwardly, using substring search. The 
negative set of a predicate consists of 
sentences with known false instances of the 
predicate similarly marked (with <AttrN> 
substituted for attributes). The negative set is 
used by the pattern learner during the scoring 
and filtering step, to filter out the patterns that 
are overly general. We generate the negative 
set from the sentences in the positive set by 
476
changing the assignment of one or both 
attributes to other suitable entities in the 
sentence. In the shallow parser based mode of 
operation, any suitable noun phrase can be 
assigned to an attribute. 
 
GENERATING THE PATTERNS 
The patterns for the predicate P are 
generalizations of pairs of sentences from the 
positive set of P. The function Generalize(s1, 
s2)  is applied to each pair of sentences s1 and 
s2 from the positive set of the predicate.  The 
function generates a pattern that is the best 
(according to the objective function defined 
below) generalization of its two arguments. 
The following pseudocode shows the 
process of generating the patterns for the 
predicate P: 
 
For each pair s1, s2 from PositiveSet(P) 
    Let Pattern = Generalize(s1, s2). 
    Add Pattern to PatternsSet(P). 
The patterns are sequences of tokens, skips 
(denoted *), limited skips (denoted *?) and 
slots. The tokens can match only themselves, 
the skips match zero or more arbitrary tokens, 
and slots match instance attributes.  The 
limited skips match zero or more arbitrary 
tokens, which must not belong to entities of the 
types equal to the types of the predicate 
attributes. In the shallow parser based mode, 
there are only two different entity types ? 
ProperNP and CommonNP, standing for 
proper and common noun phrases. 
The Generalize(s1, s2) function takes two 
sentences and generates the least (most 
specific) common generalization of both.  The 
function does a dynamical programming 
search for the best match between the two 
patterns (Optimal String Alignment algorithm), 
with the cost of the match defined as the sum 
of costs of matches for all elements. The exact 
costs of matching elements are not important 
as long as their relative order is maintained. 
We use the following numbers:  two identical 
elements match at cost 0, a token matches a 
skip or an empty space at cost 10, a skip 
matches an empty space at cost 2, and different 
kinds of skip match at cost 3. All other 
combinations have infinite cost. After the best 
match is found, it is converted into a pattern by 
copying matched identical elements and 
adding skips where non-identical elements are 
matched. For example, assume the sentences 
are 
   Toward this end, <Attr1> in July acquired 
<Attr2> 
   Earlier this year, <Attr1> acquired <Attr2> from 
X 
After the dynamic programming-based 
search, the following match will be found: 
Toward (cost 10)
Earlier   (cost 10)
this this (cost 0)
end (cost 10)
year (cost 10)
, , (cost 0)
<Attr1 > <Attr1 > (cost 0)
in   July (cost 20)
acquired acquired (cost 0)
<Attr2 > <Attr2 > (cost 0)
from (cost 10)
X (cost 10)  
 
at total cost = 80. Assuming that ?X? 
belongs to the same type as at least one of the 
attributes while the other tokens are not 
entities, the match will be converted to the 
pattern 
     *?  this  *?  ,  <Attr1>  *?  acquired  <Attr2>   
* 
3.2  Classifying the Extractions 
The goal of the final classification stage is to 
filter the list of all extracted instances, keeping 
the correct extractions and removing mistakes 
that would always occur regardless of the 
quality of the patterns. It is of course 
impossible to know which extractions are 
correct, but there exist properties of patterns 
and pattern matches that increase or decrease 
the confidence in the extractions that they 
produce. Thus, instead of a binary classifier, 
we seek a real-valued confidence function c, 
mapping the set of extracted instances into the 
[0, 1] segment. 
Since confidence value depends on the 
properties of particular sentences and patterns, 
it is more properly defined over the set of 
single pattern matches. Then, the overall 
confidence of an instance is the maximum of 
the confidence values of the matches that 
produce the instance. 
Assume that an instance E was extracted 
from a match of a pattern P at a sentence S. 
477
The following set of binary features may 
influence the confidence c(E, P, S): 
f1(E, P, S) = 1,  if the number of sentences  
                     producing E  is greater than one. 
f2(E, P, S) = 1,  if the number of sentences  
                     producing E is greater than two. 
f3(E, P, S) = 1,  if at least one slot of the pattern P is 
                      adjacent to a non-stop-word token. 
f4(E, P, S) = 1,  if both slots of the pattern P are 
                       adjacent to non-stop-word tokens. 
f5?f9(E, P, S)  = 1,  if the number of nonstop                    
                       words in P is 0 (f5), 1 or greater (f6),  
                       2 or greater (f7), 3 or greater (f8), and  
                       4 or greater (f9). 
f10?f15(E, P, S)  = 1, if the number of words 
                       between the slots of the match M                                  
                       that were matched to skips of the 
                       pattern P is 0 (f10), 1 or less (f11), 2  
                       or less (f12) , 3 or less(f13),  5 or less 
                       (f14), and 10 or less (f15). 
 
Utilizing the NER 
In the URES-NER version the entities of each 
candidate instance are passed through a simple 
rule-based NER filter, which attaches a score 
(?yes?, ?maybe?, or ?no?) to the argument(s) 
and optionally fixes the arguments boundaries. 
The NER is capable of identifying entities of 
type PERSON and COMPANY (and can be 
extended to identify additional types).   
 
The scores mean: 
   ?yes? ? the argument is of the correct 
entity type. 
   ?no? ? the argument is not of the right 
entity type, and hence 
              the candidate instance should be 
removed. 
   ?maybe? ? the argument type is uncertain, 
can be either 
                    correct or no.  
 
If ?no? is returned for one of the arguments, 
the instance is removed. Otherwise, an 
additional binary feature is added to the 
instance's vector: 
     f16 = 1 iff the score for both arguments is 
?yes?. 
For bound predicates, only the second 
argument is analyzed, naturally. 
As can be seen, the set of features above is 
small, and is not specific to any particular 
predicate. This allows us to train a model using 
a small amount of labeled data for one 
predicate, and then use the model for all other 
predicates: 
Training: The patterns for a single model 
predicate are run over a relatively small set of 
sentences (3,000-10,000 sentences in our 
experiments), producing a set of extractions 
(between 150-300 extractions in our 
experiments). 
The extractions are manually labeled 
according to whether they are correct or not. 
For each pattern match Mk = (Ek, Pk, Sk), the 
value of the feature vector fk = (f1(Mk), ?, 
f15(Mk)) is calculated, and the label Lk = ?1  
is set according to whether the extraction Ek is 
correct or no. 
A regression model estimating the function 
L(f) is built from the training data {(fk, Lk)}. 
For our classifier we used the BBR (Genkin, 
Lewis et al 2004), but other models, such as 
SVM or NaiveBayes are of course also 
possible. 
Confidence estimation: For each pattern 
match M, its score L(f(M)) is calculated by the 
trained regression model. Note that we do not 
threshold the value of L, instead using the raw 
probability value between zero and one. 
The final confidence estimates c(E) for the 
extraction E is set to the maximum of L(f(M)) 
over all matches M that produced E. 
4   Experimental Evaluation 
Our experiments aim to answer three 
questions: 
  
1. Can we train URES?s classifier once, and 
then use the results on all other relations?  
2. What boost will we get by introducing a 
simple NER into the classification scheme of 
URES?   
3. How does URES?s performance compare 
with KnowItAll and KnowItAll-PL? 
 
Our experiments utilized five relations: 
Acquisition(BuyerCompany,AcquiredCompan
y), 
Merger(Company1, Company2), 
CEO_Of(Company, Person), 
MayorOf(City, Person), 
InventorOf(Person, Invention). 
 
Merger is a symmetric predicate, in the 
sense that the order of its attributes does not 
matter. Acquisition is antisymmetric, and the 
other three are tested as bound in the first 
478
attribute. For the bound predicates, we are only 
interested in the instances with particular 
prespecified values of the first attribute. The 
Invention attribute of the InventorOf predicate 
is of type CommonNP. All other attributes are 
of type ProperName. 
The data for the experiments were collected 
by the KnowItAll crawler. The data for the 
Acquisition and Merger predicates consist of 
about 900,000 sentences for each of the two 
predicates, where each sentence contains at 
least one predicate keyword. The data for the 
bounded predicates consist of sentences that 
contain a predicate keyword and one of a 
hundred values of the first (bound) attribute. 
Half of the hundred are frequent entities 
(>100,000 search engine hits), and another half 
are rare (<10,000 hits). 
The pattern learning for each of the 
predicates was performed using the whole 
corpus of sentences for the predicate. For 
testing the precision of each of the predicates 
in each of the systems we manually evaluated 
sets of 200 instances that were randomly 
selected out of the full set of instances 
extracted from the whole corpus. 
In the first experiment, we test the 
performance of the classification component 
using different predicates for building the 
model. In the second experiment we evaluate 
the full system over the whole dataset. 
 
4.1  Cross-Predicate Classification 
Performance 
In this experiment we test whether the choice 
of the model predicate for training the 
classifier is significant. 
The pattern learning for each of the 
predicates was performed using the whole 
corpus of sentences for the predicate. For 
testing we used a small random selection of 
sentences, run the Instance Extractor over 
them, and manually evaluated each extracted 
instance. The results of the evaluation for 
Acquisition, CEO_Of, and Merger are 
summarized in Figure 2. As can be seen, using 
any of the predicates as the model produces 
similar results. The graphs for the other two 
predicates are similar. We have used only the 
first 15 features, as the NER-based feature (f16) 
is predicate-dependent.  
 
 
Acquisition
0.7
0.75
0.8
0.85
0.9
0.95
1
0 50 100 150
P
re
ci
si
on
CEO_Of
0 50 100 150 200 250
Extractions count
Merger
0 50 100 150 200 250
Acq.
CEO
Inventor
Mayor
Merger
 
Figure 2.  Cross-predicate classification performance results. Each graph shows the five precision-recall curves produced by 
using the five different model predicates. As can be seen, the curves on each graph are very similar. 
 
479
CeoOf
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0 50 100 150 200 250 300
Correct Extractions
P
re
ci
si
o
n
KIA KIA-PL URES U_NER
 
InventorOf
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0 200 400 600 800 1,000 1,200
Correct Extractions
P
re
ci
si
o
n
KIA KIA-PL URES
  
Acquisition
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0 2,000 4,000 6,000 8,000 10,000
Correct Extractions
P
re
ci
si
on
KIA KIA-PL URES U_NER
 
Merger
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0 2,000 4,000 6,000 8,000 10,000 12,000 14,000
Correct Extractions
P
re
ci
si
on
KIA KIA-PL URES U_NER
 
Figure 3.  Comparision between URES, URES-NER, KnowItAll-baseline, and KnowItAll-PL. 
 
4.2  Performance of the whole system 
In this experiment we compare the 
performance of URES with classification to the 
performance of KnowItAll. To carry out the 
experiments, we used extraction data kindly 
provided by the KnowItAll group. They 
provided us with the extractions obtained by 
the KnowItAll system and by its pattern 
learning component (KnowItAll-PL). Both are 
sketched in Section 2.1 and are described in 
detail in (Etzioni, Cafarella et al 2005). 
In this experiment we used Acquisition as 
the model predicate for testing all other 
predicates except itself.  For testing 
Acquisition we used CEO_Of as the model 
predicate.  The results are summarized in the 
five graphs in the Figure 3. 
For three relations (Acquisition, Merger, and 
InventorOf) URES clearly outperforms 
KnowItAll. Yet for the other two (CEO_Of 
and MayorOf), the simpler method of 
KnowItAll-PL or even the KnowItAll-baseline 
do as well as URES. Close inspection reveals 
that the key difference is the amount of 
redundancy of instances of those relations in 
the data. Instances of CEO_Of and MayorOf 
are mentioned frequently in a wide variety of 
sentences whereas instances of the other 
relations are relatively infrequent. 
KnowItAll extraction works well when 
redundancy is high and most instances have a 
good chance of appearing in simple forms that 
KnowItAll is able to recognize. The additional 
machinery in URES is necessary when 
redundancy is low. Specifically, URES is more 
effective in identifying low-frequency 
instances, due to its more expressive rule 
representation, and its classifier that inhibits 
those rules from overgeneralizing. 
In the same graphs we can see that URES-
NER outperforms URES by 5-15% in recall 
for similar precision levels. We can also see 
that for Person-based predicates the 
improvement is much more pronounced, 
because Person is a much simpler entity to 
recognize.  Since in the InventorOf predicate 
the 2nd attribute is of type CommonNP, the 
NER component adds no value and URES-
NER and URES results are identical for this 
predicate. 
 
 
480
5   Conclusions 
We have presented the URES system for 
autonomously extracting relations from the 
Web. We showed how to improve the 
precision of the system by classifying the 
extracted instances using the properties of the 
patterns and sentences that generated the 
instances and how to utilize a simple NER 
component. The cross-predicate tests showed 
that classifier that performs well for all 
relations can be built using a small amount of 
labeled data for any particular relation. We 
performed an experimental comparison 
between URES, URES-NER and the state-of-
the-art KnowItAll system, and showed that 
URES can double or even triple the recall 
achieved by KnowItAll for relatively rare 
relation instances, and get an additional 5-15% 
boost in recall by utilizing a simple NER. In 
particular we have shown that URES is more 
effective in identifying low-frequency 
instances, due to its more expressive rule 
representation, and its classifier (augmented by 
NER) that inhibits those rules from 
overgeneralizing. 
References 
Agichtein, E. and L. Gravano (2000 ). Snowball: 
Extracting Relations from Large Plain-Text 
Collections. Proceedings of the 5th ACM 
International Conference on Digital Libraries 
(DL). 
Brin, S. (1998). Extracting Patterns and Relations 
from the World Wide Web. WebDB Workshop at 
6th International Conference on Extending 
Database Technology, EDBT?98, Valencia, 
Spain. 
Chen, J., D. Ji, et al (2005). Unsupervised Feature 
Selection for Relation Extraction IJCNLP-05, Jeju 
Island, Korea. 
Cowie, J. and W. Lehnert (1996). "Information 
Extraction." Communications of the Association 
of Computing Machinery 39(1): 80-91. 
Downey, D., O. Etzioni, et al (2004). Learning 
Text Patterns for Web Information Extraction and 
Assessment (Extended Version). Technical 
Report UW-CSE-04-05-01. 
Etzioni, O., M. Cafarella, et al (2005). 
"Unsupervised named-entity extraction from the 
Web: An experimental study." Artificial 
Intelligence 165(1): 91-134. 
Freitag, D. (1998). Machine Learning for 
Information Extraction in Informal Domains. 
Computer Science Department. Pittsburgh, PA, 
Carnegie Mellon University: 188. 
Freitag, D. and A. K. McCallum (1999). 
Information extraction with HMMs and 
shrinkage. Proceedings of the AAAI-99 
Workshop on Machine Learning for Information 
Extraction. 
Genkin, A., D. D. Lewis, et al (2004). Large-Scale 
Bayesian Logistic Regression for Text 
Categorization. New Brunswick, NJ, DIMACS: 
1-41. 
Grishman, R. (1996). The role of syntax in 
Information Extraction. Advances in Text 
Processing: Tipster Program Phase II, Morgan 
Kaufmann. 
Grishman, R. (1997). Information Extraction: 
Techniques and Challenges. SCIE: 10-27. 
Hasegawa, T., S. Sekine, et al (2004). Discovering 
Relations among Named Entities from Large 
Corpora. ACL 2004. 
Kushmerick, N., D. S. Weld, et al (1997). Wrapper 
Induction for Information Extraction. IJCAI-97: 
729-737. 
Ravichandran, D. and E. Hovy (2002). Learning 
Surface Text Patterns for a Question Answering 
System. 40th ACL Conference. 
Riloff, E. (1993). Automatically Constructing a 
Dictionary for Information Extraction Tasks. 
AAAI-93. 
Riloff, E. and R. Jones (1999). Learning 
Dictionaries for Information Extraction by Multi-
level Boot-strapping. AAAI-99. 
Soderland, S. (1999). "Learning Information 
Extraction Rules for Semi-Structured and Free 
Text." Machine Learning 34(1-3): 233-272. 
 
 
481
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 600?607,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Using Corpus Statistics on Entities to Improve Semi-supervised  
Relation Extraction from the Web 
Benjamin Rosenfeld 
Information Systems 
HU School of Business, 
Hebrew University, Jerusalem, Israel 
grurgrur@gmail.com 
Ronen Feldman 
Information Systems 
HU School of Business, 
Hebrew University, Jerusalem, Israel 
ronen.feldman@huji.ac.il 
 
 
Abstract 
Many errors produced by unsupervised and 
semi-supervised relation extraction (RE) 
systems occur because of wrong recogni-
tion of entities that participate in the rela-
tions. This is especially true for systems 
that do not use separate named-entity rec-
ognition components, instead relying on 
general-purpose shallow parsing. Such sys-
tems have greater applicability, because 
they are able to extract relations that 
contain attributes of unknown types. 
However, this generality comes with the 
cost in accuracy. In this paper we show 
how to use corpus statistics to validate and 
correct the arguments of extracted relation 
instances, improving the overall RE 
performance. We test the methods on 
SRES ? a self-supervised Web relation 
extraction system. We also compare the 
performance of corpus-based methods to 
the performance of validation and correc-
tion methods based on supervised NER 
components.  
 
1 Introduction 
Information Extraction (IE) is the task of extract-
ing factual assertions from text. Most IE systems 
rely on knowledge engineering or on machine 
learning to generate the ?task model? that is subse-
quently used for extracting instances of entities and 
relations from new text. In the knowledge engi-
neering approach the model (usually in the form of 
extraction rules) is created manually, and in the 
machine learning approach the model is learned 
automatically from a manually labeled training set 
of documents. Both approaches require substantial 
human effort, particularly when applied to the 
broad range of documents, entities, and relations 
on the Web.  In order to minimize the manual ef-
fort necessary to build Web IE systems, semi-
supervised and completely unsupervised systems 
are being developed by many researchers.  
The task of extracting facts from the Web has 
significantly different aims than the regular infor-
mation extraction. The goal of regular IE is to 
identify and label all mentions of all instances of 
the given relation type inside a document or inside 
a collection of documents. Whereas, in the Web 
Extraction (WE) tasks we are only interested in 
extracting relation instances and not interested in 
particular mentions. 
This difference in goals leads to a difference in 
the methods of performance evaluation. The usual 
measures of performance of regular IE systems are 
precision, recall, and their combinations ? the 
breakeven point and F-measure. Unfortunately, the 
true recall usually cannot be known for WE tasks. 
Consequently, for evaluating the performance of 
WE systems, the recall is substituted by the num-
ber of extracted instances. 
WE systems usually order the extracted in-
stances by the system?s confidence in their cor-
rectness. The precision of top-confidence extrac-
tions is usually very high, but it gets progressively 
lower when lower-confidence candidates are con-
sidered. The curve that plots the number of extrac-
tions against precision level is the best indicator of 
system?s quality. Naturally, for a comparision be-
600
tween different systems to be meaningful, the 
evaluations must be performed on the same corpus. 
In this paper we are concerned with Web RE 
systems that extract binary relations between 
named entities. Most of such systems utilize sepa-
rate named entity recognition (NER) components, 
which are usualy trained in a supervised way on a 
separate set of manually labeled documents. The 
NER components recognize and extract the values 
of relation attributes (also called arguments, or 
slots), while the RE systems are concerned with 
patterns of contexts in which the slots appear. 
However, good NER components only exist for 
common and very general entity types, such as 
Person, Organization, and Location. For some re-
lations, the types of attributes are less common, 
and no ready NER components (or ready labeled 
training sets) exist for them. Also, some Web RE 
systems (e.g., KnowItAll (Etzioni, Cafarella et al 
2005)) do not use separate NER components even 
for known entity types, because such components 
are usually domain-specific and may perform 
poorly on cross-domain text collections extracted 
from the Web. 
In such cases, the values for relation attributes 
must be extracted by generic methods ? shallow 
parsing (extracting noun phrases), or even simple 
substring extraction. Such methods are naturally 
much less precise and produce many entity-
recognition errors (Feldman and Rosenfeld 2006). 
In this paper we propose several methods of us-
ing corpus statistics to improve Web RE precision 
by validating and correcting the entities extracted 
by generic methods. The task of Web Extraction is 
particularly suited for the corpus statistics-based 
methods because of very large size of the corpora 
involved, and because the system is not required to 
identify individual mentions of the relations. 
Our methods of entity validation and correction 
are based on the following two observations: 
First, the entities that appear in target relations 
will often also appear in many other contexts, 
some of which may strongly discriminate in favor 
of entities of specific type. For example, assume 
the system encounters a sentence ?Oracle bought 
PeopleSoft.? If the system works without a NER 
component, it only knows that ?Oracle? and ?Peo-
pleSoft? are proper noun phrases, and its confi-
dence in correctness of a candidate relation in-
stance  Acquisition(Oracle, PeopleSoft)  cannot be 
very high. However, both entities occur many 
times elsewhere in the corpus, sometimes in 
strongly discriminating contexts, such as ?Oracle 
is a company that?? or ?PeopleSoft Inc.? If the 
system somehow learned that such contexts indi-
cate entities of the correct type for the Acquisition 
relation (i.e., companies), then the system would 
be able to boost its confidence in both entities 
(?Oracle? and ?PeopleSoft?) being of correct types 
and, consequently, in (Oracle, PeopleSoft) being a 
correct instance of the Acquisition relation. 
Another observation that we can use is the fact 
that the entities, in which we are interested, usually 
have sufficient frequency in the corpus for statisti-
cal term extraction methods to perform reasonably 
well. These methods may often correct a wrongly 
placed entity boundary, which is a common mis-
take of general-purpose shallow parsers. 
In this paper we show how to use these observa-
tions to supplement a Web RE system with an en-
tity validation and correction component, which is 
able to significantly improve the system?s accu-
racy. We evaluate the methods using SRES 
(Feldman and Rosenfeld 2006) ? a Web RE sys-
tem, designed to extend and improve KnowItAll 
(Etzioni, Cafarella et al 2005). The contributions 
of this paper are as follows: 
? We show how to automatically generate 
the validating patterns for the target relation 
arguments, and how to integrate the results 
produced by the validating patterns into the 
whole relation extraction system. 
? We show how to use corpus statistics and 
term extraction methods to correct the 
boundaries of relation arguments. 
? We experimentally compare the improve-
ment produced by the corpus-based entity 
validation and correction methods with the 
improvements produced by two alternative 
validators ? a CRF-based NER system 
trained on a separate labeled corpus, and a 
small manually-built rule-based NER com-
ponent. 
The rest of the paper is organized as follows:  
Section 2 describes previous work.  Section 3 out-
lines the general design principles of SRES and 
briefly describes its components. Section 4 de-
scribes in detail the different entity validation and 
correction methods, and Section 5 presents their 
601
experimental evaluation. Section 6 contains con-
clusions and directions for future work. 
2 Related Work 
We are not aware of any work that deals specifi-
cally with validation and/or correction of entity 
recognition for the purposes of improving relation 
extraction accuracy. However, the background 
techniques of our methods are relatively simple 
and known. The validation is based on the same 
ideas that underlie semi-supervised entity extrac-
tion (Etzioni, Cafarella et al 2005), and uses a 
simplified SRES code. The boundary correction 
process utilizes well-known term extraction meth-
ods, e.g., (Su, Wu et al 1994). 
We also recently became aware of the work by 
Downey, Broadhead and Etzioni (2007) that deals 
with locating entities of arbitrary types in large 
corpora using corpus statistics. 
The IE systems most similar to SRES are based 
on bootstrap learning: Mutual Bootstrapping 
(Riloff and Jones 1999), the DIPRE system (Brin 
1998), and the Snowball system (Agichtein and 
Gravano 2000). Ravichandran and Hovy 
(Ravichandran and Hovy 2002) also use bootstrap-
ping, and learn simple surface patterns for extract-
ing binary relations from the Web. 
Unlike these systems, SRES surface patterns al-
low gaps that can be matched by any sequences of 
tokens. This makes SRES patterns more general, 
and allows to recognize instances in sentences in-
accessible to the simple surface patterns of systems 
such as (Brin 1998; Riloff and Jones 1999; Ravi-
chandran and Hovy 2002). 
Another direction for unsupervised relation 
learning was taken in (Hasegawa, Sekine et al 
2004; Chen, Ji et al 2005). These systems use a 
NER system to identify frequent pairs of entities 
and then cluster the pairs based on the types of the 
entities and the words appearing between the enti-
ties. The main benefit of this approach is that all 
relations between two entity types can be discov-
ered simultaneously and there is no need for the 
user to supply the relations definitions. 
3 Description of SRES 
The goal of SRES is extracting instances of speci-
fied relations from the Web without human super-
vision. Accordingly, the supervised input to the 
system is limited to the specifications of the target 
relations. A specification for a given relation con-
sists of the relation schema and a small set of seeds 
? known true instances of the relation. In the full-
scale SRES, the seeds are also generated automati-
cally, by using a set of generic patterns instantiated 
with the relation schema. However, the seed gen-
eration is not relevant to this paper. 
A relation schema specifies the name of the rela-
tion, the names and types of its arguments, and the 
arguments ordering. For example, the schema of 
the Acquisition relation 
Acquisition(Buyer=ProperNP, 
                   Acquired=ProperNP)  ordered  
specifies that Acquisition has two slots, named 
Buyer and Acquired, which must be filled with en-
tities of type ProperNP. The order of the slots is 
important (as signified by the word ?ordered?, and 
as opposed to relations like Merger, which are 
?unordered? or, in binary case, ?symmetric?). 
The baseline SRES does not utilize a named en-
tity recognizer, instead using a shallow parser for 
exracting the relation slots. Thus, the only allowed 
entity types are ProperNP, CommonNP, and 
AnyNP, which mean the heads of, respectively, 
proper, common, and arbitrary noun phrases. In the 
experimental section we compare the baseline 
SRES to its extensions containing additional NER 
components. When using those components we 
allow further subtypes of ProperNP, and the rela-
tion schema above becomes 
? (Buyer=Company, Acquired=Company) ? 
The main components of SRES are the Pattern 
Learner, the Instance Extractor, and the Classifier. 
The Pattern Learner uses the seeds to learn likely 
patterns of relation occurrences. Then, the Instance 
Extractor uses the patterns to extract the candidate 
instances from the sentences. Finally, the Classifier 
assigns the confidence score to each extraction. We 
shall now briefly describe these components. 
3.1 Pattern Learner 
The Pattern Learner receives a relation schema 
and a set of seeds. Then it finds the occurences of 
seeds inside a large (unlabeled) text corpus, ana-
lyzes their contexts, and extracts common patterns 
among these contexts. The details of the patterns 
language and the process of pattern learning are 
not significant for this paper, and are described 
fully in (Feldman and Rosenfeld 2006). 
602
3.2 Instance Extractor 
The Instance Extractor applies the patterns gener-
ated by the Pattern Learner to the text corpus. In 
order to be able to match the slots of the patterns, 
the Instance Extractor utilizes an external shallow 
parser from the OpenNLP package 
(http://opennlp.sourceforge.net/), which is able to 
find all proper and common noun phrases in a sen-
tence. These phrases are matched to the slots of the 
patterns. In other respects, the pattern matching 
and extraction process is straightforward. 
3.3 Classifier 
The goal of the final classification stage is to filter 
the list of all extracted instances, keeping the cor-
rect extractions, and removing mistakes that would 
always occur regardless of the quality of the pat-
terns. It is of course impossible to know which ex-
tractions are correct, but there exist properties of 
patterns and pattern matches that increase or de-
crease the confidence in the extractions that they 
produce. 
These properties are turned into a set of binary 
features, which are processed by a linear feature-
rich classifier. The classifier receives a feature vec-
tor for a candidate, and produces a confidence 
score between 0 and 1. 
 The set of features is small and is not specific to 
any particular relation. This allows to train a model 
using a small amount of labeled data for one rela-
tion, and then use the model for scoring the candi-
dates of all other relations. Since the supervised 
training stage needs to be run only once, it is a part 
of the system development, and the complete sys-
tem remains unsupervised, as demonstrated in 
(Feldman and Rosenfeld 2006). 
4 Entity Validation and Correction 
In this paper we describe three different methods 
of validation and correction of relation arguments 
in the extracted instances. Two of them are ?classi-
cal? and are based, respectively, on the knowledge-
engineering, and on the statistical supervised ap-
proaches to the named entity recognition problems. 
The third is our novel approach, based on redun-
dancy and corpus statistics. 
The methods are implemented as components 
for SRES, called Entity Validators, inserted be-
tween the Instance Extractor and the Classifier. 
The result of applying Entity Validator to a candi-
date instance is an (optionally) fixed instance, with 
validity values attached to all slots. There are three 
validity values: valid, invalid, and uncertain. 
The Classifier uses the validity values by con-
verting them into two additional binary features, 
which are then able to influence the confidence of 
extractions. 
We shall now describe the three different valida-
tors in details. 
4.1 Small Rule-based NER validator 
This validator is a small Perl script that checks 
whether a character string conforms to a set of 
simple regular expression patterns, and whether it 
appears inside lists of known named entities. There 
are two sets of regular expression patterns ? for 
Person and for Company entity types, and three 
large lists ? for known personal names, known 
companies, and ?other known named entities?, cur-
rently including locations, universities, and gov-
ernment agencies. 
The manually written regular expression repre-
sent simple regularities in the internal structure of 
the entity types. For example, the patterns for Per-
son include: 
Person = KnownFirstName  [Initial]  LastName 
Person = Honorific [FirstName] [Initial] LastName 
Honorific = (?Mr? | ?Ms? | ?Dr? |?) [?.?] 
Initial = CapitalLetter [?.?] 
KnownFirstName = member of  
                                      KnownPersonalNamesList 
FirstName = CapitalizedWord  
LastName = CapitalizedWord 
LastName = CapitalizedWord [???CapitalizedWord] 
LastName = (?o? | ?de? | ?) ?`?CapitalizedWord 
          ? 
while the patterns for Company include: 
Company = KnownCompanyName 
Company = CompanyName CompanyDesignator 
Company = CompanyName FrequentCompanySfx 
KnownCompanyName = member of 
                                              KnownCompaniesList 
CompanyName = CapitalizedWord + 
CompanyDesignator = ?inc? | ?corp? | ?co? | ? 
FrequentCompanySfx = ?systems? | ?software? | ? 
          ? 
The validator works in the following way: it re-
ceives a sentence with a labeled candidate entity of 
a specified entity type (which can be either Person 
or Company). It then applies all of the regular ex-
pression patterns to the labeled text and to its en-
603
closing context. It also checks for membership in 
the lists of known entities. If a boundary is incor-
rectly placed according to the patterns or to the 
lists, it is fixed. Then, the following result is re-
turned: 
Valid, if some pattern/list of the right entity type 
matched the candidate entity, while there 
were no matches for patterns/lists of other 
entity types. 
Invalid, if no pattern/list of the right entity type 
matched the candidate entity, while there 
were matches for patterns/lists of other entity 
types. 
Uncertain, otherwise, that is either if there were 
no matches at all, or if both correct and in-
correct entity types matched. 
The number of patterns is relatively small, and 
the whole component consists of about 300 lines in 
Perl and costs several person-days of knowledge 
engineering work. Despite its simplicity, we will 
show in the experimental section that it is quite 
effective, and even often outperforms the CRF-
based NER component, described below. 
4.2 CRF-based NER validator 
This validator is built using a feature-rich CRF-
based sequence classifier, trained upon an English 
dataset of the CoNLL 2003 shared task (Rosenfeld, 
Fresko et al 2005). For the gazetteer lists it uses 
the same large lists as the rule-based component 
described above. 
The validator receives a sentence with a labeled 
candidate entity of a specified entity type (which 
can be either Person or Company). It then sends 
the sentence to the CRF-based classifier, which 
labels all named entities it knows ? Dates, Times, 
Percents, Persons, Organizations, and Locations. 
If the CRF classifier places the entity boundaries 
differently, they are fixed. Then, the following re-
sult is returned: 
Valid, if CRF classification of the entity accords 
with the expected argument type. 
Invalid, if CRF classification of the entity is dif-
ferent from the expected argument type. 
Uncertain, otherwise, that is if the CRF classi-
fier didn?t recognize the entity at all. 
4.3 Corpus-based NER validator 
The goal of building the corpus-based NER valida-
tor is to provide the same level of performance as 
the supervised NER components, while requiring 
neither additional human supervision nor addi-
tional labeled corpora or other resources. There are 
several important facts that help achieve this goal. 
First, the relation instances that are used as seeds 
for the pattern learning are known to contain cor-
rect instances of the right entity type. These in-
stances can be used as seeds in their own right, for 
learning the patterns of occurrence of the corre-
sponding entity types. Second, the entities in which 
we are interested usually appear in the corpus with 
a sufficient frequency. The validation is based on 
the first observation, while the boundary fixing on 
the second. 
Corpus-based entity validation 
There is a preparation stage, during which the 
information required for validation is extracted 
from the corpus. This information is the lists of all 
entities of every type that appears in the target rela-
tions. In order to extract these lists we use a simpli-
fied SRES. The entities are considered to be unary 
relations, and the seeds for them are taken from the 
slots of the target binary relations seeds. We don?t 
use the Classifier on the extracted entity instances. 
Instead, for every extracted instance we record the 
number of different sentences the entity was ex-
tracted from. 
During the validation process, the validator?s 
task is to evaluate a given candidate entity in-
stance. The validator compares the number of 
times the instance was extracted (during the prepa-
ration stage) by the patterns for the correct entity 
type, and by the patterns for all other entity types. 
The validator then returns 
Valid, if the number of times the entity was ex-
tracted for the specified entity type is at least 
5, and at least two times bigger than the 
number of times it was extracted for all other 
entity types. 
Invalid, if the number of times the instance was 
extracted for the specified entity type is less 
than 5, and at least 2 times smaller than the 
number of times it was extracted for all other 
entity types. 
604
Uncertain, otherwise, that is if it was never ex-
tracted at all, or extracted with similar fre-
quency for both correct and wrong entity 
types. 
Corpus-based correction of entity boundaries 
Our entity boundaries correction mechanism is 
similar to the known statistical term extraction 
techniques (Su, Wu et al 1994). It is based on the 
assumption that the component words of a term (an 
entity in our case) are more tightly bound to each 
other than to the context. In the statistical sense, 
this fact is expressed by a high mutual information 
between the adjacent words belonging to the same 
term. 
There are two possible boundary fixes: remov-
ing words from the candidate entity, or adding 
words from the context to the entity. There is a 
significant practical difference between the two 
cases. 
Assume that an entity boundary was placed too 
broadly, and included extra words. If this was a 
chance occurrence (and only such cases can be 
found by statistical methods), then the resulting 
sequence of tokens will be very infrequent, while 
its parts will have relatively high frequency. For 
example, consider a sequence ?Formerly Microsoft 
Corp.?, which is produced by mistakenly labeling 
?Formerly? as a proper noun by the PoS tagger. 
While it is easy to know from the frequencies that 
a boundary mistake was made, it is unclear (to the 
system) which part is the correct entity. But since 
the entity (one of the parts of the candidate) has a 
high frequency, there is a chance that the relation 
instance, in which the entity appears, will be re-
peated elsewhere in the corpus and will be ex-
tracted correctly there. Therefore, in such case, the 
simplest recourse is to simply label the entity as 
Invalid, and not to try fixing the boundaries. 
On the other hand, if a word was missed from an 
entity (e.g., ?Beverly O?, instead of ?Beverly O ' 
Neill?), the resulting sequence will be frequent. 
Moreover, it is quite probable that the same 
boundary mistake is made in many places, because 
the same sequence of tokens is being analyzed in 
all those places. Therefore, it makes sense to try to 
fix the bounary in this case, especially since it can 
be done simply and  reliably: a word (or several 
words) is attached to the entity string if both their 
frequencies and their mutual information are above 
a threshold. 
5 Experimental Evaluation 
The experiments described in this paper aim to 
confirm the effectiveness of the proposed corpus-
based relation argument validation and correction 
method, and to compare its performance with the 
classical knowledge-engineering-based and super-
vised-training-based methods. The experiments 
were performed with five relations: 
Acquisition(BuyerCompany, AcquiredCompany), 
Merger(Company1, Company2), 
CEO_Of(Company, Person), 
MayorOf(City, Person), 
InventorOf(Person, Invention). 
The data for the experiments were collected by the 
KnowItAll crawler. The data for the Acquisition 
and Merger consist of about 900,000 sentences for 
each of the two relations. The data for the bound 
relations consist of sentences, such that each con-
tains one of a hundred values of the first (bound) 
attribute. Half of the hundred are frequent entities 
(>100,000 search engine hits), and another half are 
rare (<10,000 hits). 
For evaluating the validators we randomly se-
lected a set of 10000 sentences from the corpora 
for each of the relations, and manually evaluated 
the SRES results generated from these sentences. 
Four sets of results were evaluated: the baseline 
results produced without any NER validator, and 
three sets of results produced using three different 
NER validators. For the InventorOf relation, only 
the corpus-based validator results can be produced, 
since the other two NER components cannot be 
adapted to validate/correct entities of type Inven-
tion. 
The results for the five relations are shown in 
the Figure 1. Several conclusions can be drawn 
from the graphs. First, all of the NER validators 
improve over the baseline SRES, sometimes as 
much as doubling the recall at the same level of 
precision. In most cases the three validators show 
roughly similar levels of performance. A notable 
difference is the CEO_Of relation, where the sim-
ple rule-based component performs much better 
than CRF, which performs yet better than the cor-
pus-based component. The CEO_Of relation is 
tested as bound, which means that only the second 
relation argument, of type Person, is validated. The 
Person entities have much more rigid internal 
structure than the other entities ? Companies and 
Inventions. Consequently, the best performing of 
605
Acquisition
0.50
0.60
0.70
0.80
0.90
1.00
0 50 100 150
Correct Extractions
P
re
c
is
io
n
Baseline RB-NER CRF Corpus
Merger
0.50
0.60
0.70
0.80
0.90
1.00
0 50 100 150
Correct Extractions
P
re
c
is
io
n
Baseline RB-NER CRF Corpus
 
CeoOf
0.50
0.60
0.70
0.80
0.90
1.00
0 20 40 60 80 100 120
Correct Extractions
P
re
c
is
io
n
Baseline RB-NER CRF Corpus
 
InventorOf
0.50
0.60
0.70
0.80
0.90
1.00
0 20 40 60 80 100 120
Correct Extractions
P
re
c
is
io
n
Baseline Corpus
 
 
Figure 1. Comparison between Baseline-SRES and its extensions with three different NER validators:  a 
simple Rule-Based one, a CRF-based statistical one, and a Corpus-based one. 
 
 
the three validators is the rule-based, which di-
rectly tests this internal structure. The CRF-based 
validator is also able to take advantage of the struc-
ture, although in a weaker manner. The Corpus-
based validator, however, works purely on the ba-
sis of context, entirely disregarding the internal 
structure of entities, and thus performs worst of all 
in this case. On the other hand, the Corpus-based 
validator is able to improve the results for the In-
ventor relation, which the other two validators are 
completely unable to do. 
It is also of interest to compare the performance 
of CRF-based and the rule-based NER components 
in other cases. As can be seen, in most cases the 
rule-based component, despite its simplicity, out-
performs the CRF-based one. The possible reason 
for this is that relation extraction setting is signifi-
cantly different from the classical named entity 
recognition setting. A classical NER system is set 
to maximize  the F1 measure of all mentions of all 
entities in the corpus. A relation argument extrac-
tor, on the other hand, should maximize its per-
formance on relation arguments, and apparently 
their statistical properties are often significantly 
different. 
6 Conclusions 
We have presented a novel method for validation 
and correction of relation arguments for the state-
of-the-art unsupervised Web relation extraction 
system SRES. The method is based on corpus sta-
tistics and requires no human supervision and no 
additional corpus resources beyond the corpus that 
is used for relation extraction. 
We showed experimentally the effectiveness of 
our method, which performed comparably to both 
simple rule-based NER and a statistical CRF-based 
NER in the task of validating Companies, and 
somewhat worse in the task of validating Persons, 
606
due to its complete disregard of internal structure 
of entities. The ways to learn and use this structure 
in an unsupervised way are left for future research. 
Our method also successfully validated the 
Invention entities, which are inaccessible to the 
other methods due to the lack of training data. 
In our experiments we made use of a unique fea-
ture of SRES system ? a feature-rich classifier that 
assigns confidence score to the candidate in-
stances, basing its decisions on various features of 
the patterns and of the contexts from which the 
candidates were extracted. This architecture allows 
easy integration of the entity validation compo-
nents as additional feature generators. We believe, 
however, that our results have greater applicability, 
and that the corpus statistics-based components can 
be added to RE systems with other architectures as 
well. 
References 
Agichtein, E. and L. Gravano (2000). Snowball: Ex-
tracting Relations from Large Plain-Text Collections. 
Proceedings of the 5th ACM International Confer-
ence on Digital Libraries (DL). 
Brin, S. (1998). Extracting Patterns and Relations from 
the World Wide Web. WebDB Workshop at 6th In-
ternational Conference on Extending Database Tech-
nology, EDBT?98, Valencia, Spain. 
Chen, J., D. Ji, C. L. Tan and Z. Niu (2005). Unsuper-
vised Feature Selection for Relation Extraction. 
IJCNLP-05, Jeju Island, Korea. 
Downey, D., M. Broadhead and O. Etzioni (2007). Lo-
cating Complex Named Entities in Web Text. IJCAI-
07. 
Etzioni, O., M. Cafarella, D. Downey, A. Popescu, T. 
Shaked, S. Soderland, D. Weld and A. Yates (2005). 
Unsupervised named-entity extraction from the Web: 
An experimental study. Artificial Intelligence 165(1): 
91-134. 
Feldman, R. and B. Rosenfeld (2006). Boosting Unsu-
pervised Relation Extraction by Using NER. 
EMNLP-06, Sydney, Australia. 
Feldman, R. and B. Rosenfeld (2006). Self-Supervised 
Relation Extraction from the Web. ISMIS-2006, Bari, 
Italy. 
Hasegawa, T., S. Sekine and R. Grishman (2004). Dis-
covering Relations among Named Entities from 
Large Corpora. ACL 2004. 
Ravichandran, D. and E. Hovy (2002). Learning Sur-
face Text Patterns for a Question Answering System. 
40th ACL Conference. 
Riloff, E. and R. Jones (1999). Learning Dictionaries 
for Information Extraction by Multi-level Boot-
strapping. AAAI-99. 
Rosenfeld, B., M. Fresko and R. Feldman (2005). A 
Systematic Comparison of Feature-Rich Probabilis-
tic Classifiers for NER Tasks. PKDD. 
Su, K.-Y., M.-W. Wu and J.-S. Chang (1994). A Cor-
pus-based Approach to Automatic Compound Ex-
traction. Meeting of the Association for Computa-
tional Linguistics: 242-247. 
 
 
607
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1310?1319,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Identifying and Following Expert Investors in Stock Microblogs
1Roy Bar-Haim, 1Elad Dinur, 1,2Ronen Feldman, 1Moshe Fresko and 1Guy Goldstein
1Digital Trowel, Airport City, Israel
2School of Business Administration, The Hebrew University of Jerusalem, Jerusalem, Israel
{roy, moshe}@digitaltrowel.com, ronen.feldman@huji.ac.il
Abstract
Information published in online stock invest-
ment message boards, and more recently in
stock microblogs, is considered highly valu-
able by many investors. Previous work fo-
cused on aggregation of sentiment from all
users. However, in this work we show that it
is beneficial to distinguish expert users from
non-experts. We propose a general framework
for identifying expert investors, and use it as a
basis for several models that predict stock rise
from stock microblogging messages (stock
tweets). In particular, we present two methods
that combine expert identification and per-user
unsupervised learning. These methods were
shown to achieve relatively high precision in
predicting stock rise, and significantly outper-
form our baseline. In addition, our work pro-
vides an in-depth analysis of the content and
potential usefulness of stock tweets.
1 Introduction
Online investment message boards such as Yahoo!
Finance and Raging Bull allow investors to share
trading ideas, advice and opinions on public com-
panies. Recently, stock microblogging services such
as StockTwits (which started as a filtering service
over the Twitter platform) have become very popu-
lar. These forums are considered by many investors
as highly valuable sources for making their trading
decisions.
This work aims to mine useful investment in-
formation from messages published in stock mi-
croblogs. We shall henceforth refer to these mes-
sages as stock tweets. Ultimately, we would like to
transform those tweets into buy and sell decisions.
Given a set of stock-related messages, this process
typically comprises two steps:
1. Classify each message as ?bullish? (having a
positive outlook on the stock), ?bearish? (hav-
ing a negative outlook on the stock), or neutral.
2. Make trading decisions based on these message
classifications.
Previous work on stock investment forums and
microblogs usually regarded the first step (message
classification) as a sentiment analysis problem, and
aligned bullish with positive sentiment and bearish
with negative sentiment. Messages were classified
by matching positive and negative terms from sen-
timent lexicons, learning from a hand-labeled set of
messages, or some combination of the two (Das and
Chen, 2007; Antweiler and Frank, 2004; Chua et al,
2009; Zhang and Skiena, 2010; Sprenger andWelpe,
2010). Trading decisions were made by aggregating
the sentiment for a given stock over all the tweets,
and picking stocks with strongest sentiment signal
(buying the most bullish stocks and short-selling the
most bearish ones).
Sentiment aggregation reflects the opinion of the
investors community as a whole, but overlooks the
variability in user expertise. Clearly, not all investors
are born equal, and if we could tell experts from non-
experts, we would reduce the noise in these forums
and obtain high-quality signals to follow. This pa-
per presents a framework for identifying experts in
stock microblogs by monitoring their performance
in a training period. We show that following the ex-
perts results in more precise predictions.
1310
Based on the expert identification framework, we
experiment with different methods for deriving pre-
dictions from stock tweets. While previous work
largely aligned bullishness with message sentiment,
our in-depth content analysis of stock tweets (to be
presented in section 2.2) suggests that this view is
too simplistic. To start with, one important dif-
ference between bullishness/bearishness and posi-
tive/negative sentiment is that while the former rep-
resents belief about the future, the latter may also
refer to the past or present. For example, a user re-
porting on making profit from a buying stock yester-
day and selling it today is clearly positive about the
stock, but does not express any prediction about its
future performance. Furthermore, messages that do
refer to the future differ considerably in their signif-
icance. A tweet reporting on buying a stock by the
user conveys a much stronger bullishness signal than
a tweet that merely expresses an opinion. Overall, it
would seem that judging bullishness is far more elu-
sive than judging sentiment.
We therefore propose and compare two alterna-
tive approaches that sidestep the complexities of as-
sessing tweets bullishness. These two approaches
can be viewed as representing two extremes. The
first approach restricts our attention to the most ex-
plicit signals of bullishness and bearishness, namely,
tweets that report actual buy and sell transactions
performed by the user. In the second approach we
learn directly the relation between tweets content
and stock prices, following previous work on pre-
dicting stock price movement from factual sources
such as news articles (Lavrenko et al, 2000; Koppel
and Shtrimberg, 2004; Schumaker and Chen, 2010).
This approach poses no restrictions on the tweets
content and avoids any stipulated tweet classifica-
tion. However, user-generated messages are largely
subjective, and their correlation with the stock prices
depends on user?s expertise. This introduces much
noise into the learning process. We show that by
making the learning user-sensitive we can improve
the results substantially. Overall, our work illus-
trates the feasibility of finding expert investors, and
the utility of following them.
2 Stock Tweets
2.1 Stock Tweets Language
Stock tweets, as Twitter messages in general, are
short textual messages of up to 140 characters. They
are distinguished by having one or more references
to stock symbols (tickers), prefixed by a dollar sign.
For instance, the stock of Apple, Inc. is referenced
as $AAPL. Two other noteworthy Twitter conven-
tions that are also found in stock tweets are hashtags,
user-defined labels starting with ?#?, and references
to other users, starting with ?@?. Table 1 lists some
examples of stock tweets.
As common with Twitter messages, stock tweets
are typically abbreviated and ungrammatical utter-
ances. The language is informal and includes many
slang expressions, many of which are unique to the
stock tweets community. Thus, many positive and
negative expressions common to stock tweets are not
found in standard sentiment lexicons. Their unique
language and terminology often make stock tweets
hard to understand for an outsider. Many words
are abbreviated and appear in several non-standard
forms. For example, the word bought may also ap-
pear as bot or bght, and today may appear as 2day.
Stock tweets also contain many sentiment expres-
sions which may appear in many variations, e.g.
wow, woooow, woooooooow and so on. These char-
acteristics make the analysis of stock tweets a par-
ticularly challenging task.
2.2 Content Analysis
A preliminary step of this research was an exten-
sive data analysis, aimed to gain better understand-
ing of the major types of content conveyed in stock
tweets. First, we developed a taxonomy of tweet
categories while reading a few thousands of tweets.
Based on this taxonomy we then tagged a sample
of 350 tweets to obtain statistics on the frequency
of each category. The sample contained only tweets
that mention exactly one ticker. The following types
of tweets were considered irrelevant:
? Tweets that express question. These tweets
were labeled as Question.
? Obscure tweets, e.g. ?$AAPL fat?, tweets
that contain insufficient information (e.g.
?http://url.com $AAPL?) and tweets that seem
1311
Example %
Fact
News $KFRC: Deutsche Bank starts at Buy 14.3%
Chart Pattern $C (Citigroup Inc) $3.81 crossed its 2nd Pivot Point Support
http://empirasign.com/s/x4c
10.9%
Trade bot back some $AXP this morning 12.9%
Trade Outcome Sold $CELG at 55.80 for day-trade, +0.90 (+1.6%)X 2.9%
Opinion
Speculation thinking of hedging my shorts by buying some oil. thinking of
buying as much $goog as i can in my IRA. but i need more doing,
less thinking.
4.0%
Chart Prediction http://chart.ly/wsy5ny $GS - not looking good for this one -
breaks this support line on volume will nibble a few short
12.9%
Recommendation $WFC if you have to own financials, WFC would be my choice.
http://fsc.bz/448 #WORDEN
1.7%
Sentiment $ivn is rocking 8.6%
Question $aapl breaking out but in this mkt should wait till close? 7.1%
Irrelevant $CLNE follow Mr. Clean $$ 24.9%
Table 1: Tweets categories and their relative frequencies
to contain no useful information (e.g ?Even
Steve Jobs is wrong sometimes... $AAPL
http://ow.ly/1Tw0Z?). These tweets were la-
beled Irrelevant.
The rest of the tweets were classified into two major
categories: Facts and Opinions.
Facts can be divided into four main subcategories:
1. News: such tweets are generally in the form of
a tweeted headline describing news or a current
event generally drawn from mass media. As
such they are reliable but, since the information
is available in far greater detail elsewhere, their
added value is limited.
2. Chart Pattern: technical analysis aims to pro-
vide insight into trends and emerging patterns
in a stock?s price. These tweets describe pat-
terns in the stock?s chart without the inclusion
of any predicted or projected movement, an im-
portant contrast to Chart Prediction, which is
an opinion tweet described below. Chart pat-
tern tweets, like news, are a condensed form of
information already available through more in-
depth sources and as such their added value is
limited.
3. Trade: reports an actual purchase or sale of a
stock by the user. We consider this as the most
valuable form of tweet.
4. Trade Outcome: provides details of an ?inverse
trade?, the secondary trade to exit the initial
position along with the outcome of the over-
all trade (profit/loss). The value of these tweets
is debatable since although they provide details
of a trade, they generally describe the ?exit?
transaction. This creates a dilemma for ana-
lysts since traders will often exit not because
of a perceived change in the stock?s potential
but as a result of many short-term trading ac-
tivities. For this reason trade outcome provides
a moderate insight into a user?s position which
should be viewed with some degree of caution.
Opinions can also be divided into four main subcat-
egories:
1. Speculation: provides individual predictions of
future events relating to a company or actions
of the company. These are amongst the least
reliable categories, as the individual user is typ-
ically unable to justify his or her insight into the
predicted action.
2. Chart Prediction: describes a user?s prediction
of a future chart movement based on technical
analysis of the stock?s chart.
3. Recommendation: As with analyst recommen-
dations, this category represents users who
summarize their understanding and insight into
1312
a stock with a simple and effective recommen-
dation to take a certain course of action with
regard to a particular share. Recommendation
is the less determinate counterpart to Trade.
4. Sentiment: These tweets express pure senti-
ment toward the stock, rather than any factual
content.
Table 1 shows examples for each of the tweet cate-
gories, as well as their relative frequency in the ana-
lyzed sample.
3 An Expert Finding Framework
In this section we present a general procedure for
finding experts in stock microblogs. Based on this
procedure, we will develop in the next sections sev-
eral models for extracting reliable trading signals
from tweets.
We assume that a stock tweet refers to exactly one
stock, and therefore there is a one-to-one mapping
between tweets and stocks. Other tweets are dis-
carded. We define expertise as the ability to pre-
dict stock rise with high precision. Thus, a user is
an expert if a high percentage of his or her bullish
tweets is followed by a stock rise. In principle, we
could analogously follow bearish tweets, and see if
they are followed by a stock fall. However, bearish
tweets are somewhat more difficult to interpret: for
example, selling a share may indicate a negative out-
look on the stock, but it may also result from other
considerations, e.g. following a trading strategy that
holds the stock for a fixed period (cf. the discussion
on Trade Outcome tweets in the previous section).
We now describe a procedure that determines
whether a user u is an expert. The procedure re-
ceives a training set T of tweets posted by u, where
each tweet is annotated with its posting time. It is
also given a classifier C, which classifies each tweet
as bullish or not bullish (either bearish or neutral).
The procedure first applies the classifier C to iden-
tify the bullish tweets in T . It then determines the
correctness of each bullish tweet. Given a tweet t,
we observe the price change of the stock referenced
by t over a one day period starting at the next trading
day. The exact definition of mapping tweets to stock
prices is given in section 5.1. A one-day holding
period was chosen as it was found to perform well
in previous works on tweet-based trading (Zhang
and Skiena, 2010; Sprenger and Welpe, 2010), in
particular for long positions (buy transactions). A
bullish tweet is considered correct if it is followed
by a stock rise, and as incorrect otherwise1. Given a
set of tweets, we define its precision as the percent-
age of correct tweets in the set. Let Cu, Iu denote
the number of correct and incorrect bullish tweets
of user u, respectively. The precision of u?s bullish
tweets is therefore:
Pu =
Cu
Cu + Iu
Let Pbl be the baseline precision. In this work we
chose the baseline precision to be the proportion of
tweets that are followed by a stock rise in the whole
training set (including all the users). This represents
the expected precision when picking tweets at ran-
dom. Clearly, if Pu ? Pbl then u is not an expert.
If Pu > Pbl, we apply the following statistical test
to assess whether the difference is statistically sig-
nificant. First, we compute the expected number of
correct and incorrect transactions Cbl, Ibl according
to the baseline:
Cbl = Pbl ? (Cu + Iu)
Ibl = (1? Pbl)? (Cu + Iu)
We then compare the observed counts (Cu, Iu) to
the expected counts (Cbl, Ibl), using Pearson?s Chi-
square test. Since it is required for this test that
Cbl and Ibl are at least 5, cases that do not meet
this requirement are discarded. If the resulting p-
value satisfies the required significance level ?, then
u is considered an expert. In this work we take
? = 0.05. Note that since the statistical test takes
into account the number of observations, it will re-
ject cases where the number of the observations is
very small, even if the precision is very high. The
output of the procedure is a classification of u as
expert/non-expert, as well as the p-value (for ex-
perts). The expert finding procedure is summarized
in Algorithm 1.
In the next two sections we propose several alter-
natives for the classifier C.
1For about 1% of the tweets the stock price did not change
in the next trading day. These tweets are also considered correct
throughout this work.
1313
Algorithm 1 Determine if a user u is an expert
Input: set of tweets T posted by u, bullishness
classifier C, baseline probability Pbl, significance
level ?
Output: NON-EXPERT/(EXPERT, p-value)
Tbullish ? tweets in T classified by C as bullish
Cu ? 0 ; Iu ? 0
for each t ? Tbullish do
if t is followed by a stock rise then
Cu++
else
Iu++
end if
end for
Pu = CuCu+Iuif Pu ? Pbl then
return NON-EXPERT
else
Cbl ? Pbl ? (Cu + Iu)
Ibl ? (1? Pbl)? (Cu + Iu)
p? ChiSquareTest(Cu, Iu, Cbl, Ibl)
if p > ? then
return NON-EXPERT
else
return (EXPERT, p)
end if
end if
4 Following Explicit Transactions
The first approach we attempt for classifying bullish
(and bearish) tweets aims to identify only tweets that
report buy and sell transactions (that is, tweets in
the Trade category). According to our data analysis
(reported in section 2.2), about 13% of the tweets
belong to this category. There are two reasons to
focus on these tweets. First, as we already noted,
actual transactions are clearly the strongest signal
of bulishness/bearishness. Second, the buy and sell
actions are usually reported using a closed set of
expressions, making these tweets relatively easy to
identify. A few examples for buy and sell tweets are
shown in Table 2.
While buy and sell transactions can be captured
reasonably well by a relatively small set of patterns,
the examples in Table 2 show that stock tweets have
sell sold sum $OMNI 2.14 +12%
buy bot $MSPD for earnings testing
new indicator as well.
sell Out 1/2 $RIMM calls @ 1.84
(+0.81)
buy added to $joez 2.56
buy I picked up some $X JUL 50 Puts@
3.20 for gap fill play about an hour
ago.
buy long $BIDU 74.01
buy $$ Anxiously sitting at the bid on
$CWCO @ 11.85 It seems the ask
and I are at an impasse. 20 min of
this so far. Who will budge? (not
me)
buy In 300 $GOOG @ 471.15.
sell sold $THOR 41.84 for $400 the
FreeFactory is rocking
sell That was quick stopped out $ICE
sell Initiated a short position in $NEM.
Table 2: Buy and sell tweets
their unique language for reporting these transac-
tions, which must be investigated in order to come
by these patterns. Thus, in order to develop a clas-
sifier for these tweets, we created a training and test
corpora as follows. Based on our preliminary anal-
ysis of several thousand tweets, we composed a vo-
cabulary of keywords which trade tweets must in-
clude2. This vocabulary contained words such as in,
out, bot, bght, sld and so on. Filtering out tweets that
match none of the keywords removed two thirds of
the tweets. Out of the remaining tweets, about 5700
tweets were tagged. The training set contains about
3700 tweets, 700 of which are transactions. The test
set contains about 2000 tweets, 350 of which are
transactions.
Since the transaction tweets can be characterized
by a closed set of recurring patterns, we developed
a classifier that is based on a few dozens of man-
ually composed pattern matching rules, formulated
as regular expressions. The classifier works in three
stages:
1. Normalization: The tweet is transformed into
a canonical form. For example, user name
2That is, we did not come across any trade tweet that does
not include at least one of the keywords in the large sample we
analyzed, so we assume that such tweets are negligible.
1314
Dataset Transaction P R F1
Train Buy 94.0% 84.0% 0.89Sell 96.0% 83.0% 0.89
Test Buy 85.0% 70.0% 0.77Sell 88.5% 79.0% 0.84
Table 3: Results for buy/sell transactition classifier. Pre-
cision (P), Recall (R), and F-measure (F1) are reported.
is transformed into USERNAME; ticker name
is transformed into TICKER; buy, buying,
bought, bot, bght are transformed into BUY,
and so on.
2. Matching: Trying to match one of the buy/sell
patterns in the normalized tweet.
3. Filtering: Filtering out tweets that match ?dis-
qualifying? patterns. The simplest examples
are a tweet starting with an ?if? or a tweet con-
taining a question mark.
The results of the classifier on the train and test set
are summarized in Table 3. The results show that
our classifier identifies buy/sell transactions with a
good precision and a reasonable recall.
5 Unsupervised Learning from Stock
Prices
The drawback of the method presented in the pre-
vious section is that it only considers a small part
of the available tweets. In this section we propose
an alternative method, which considers all the avail-
able tweets, and does not require any tagged corpus
of tweets. Instead, we use actual stock price move-
ments as our labels.
5.1 Associating Tweets with Stock Prices
We used stock prices to label tweets as follows. Each
tweet message has a time stamp (eastern time), indi-
cating when it was published. Our policy is to buy
in the opening price of the next trading day (PB),
and sell on the opening price of the following trad-
ing day (PS). Tweets that are posted until 9:25 in the
morning (market hours begin at 9:30) are associated
with the same day, while those are posted after that
time are associated with the next trading date.
5.2 Training
Given the buy and sell prices associated with each
tweet, we construct positive and negative training
examples as follows: positive examples are tweets
where PS?PBPB ? 3%, and negative examples are
tweets where PS?PBPB ? ?3%.We used the SVM-light package (Joachims,
1999), with the following features:
? The existence of the following elements in the
message text:
? Reference to a ticker
? Reference to a user
? URL
? Number
? Hashtag
? Question mark
? The case-insensitive words in the message after
dropping the above elements.
? The 3, 4, 5 letter prefixes of each word.
? The name of the user who authored the tweet,
if it is a frequent user (at least 50 messages in
the training data). Otherwise, the user name is
taken to be ?anonymous?.
? Whether the stock price was up or down 1% or
more in the previous trading day.
? 2, 3, 4-word expressions which are typical to
tweets (that is, their relative frequency in tweets
is much higher than in general news text).
6 Empirical Evaluation
In this section we focus on the empirical task of
tweet ranking: ordering the tweets in the test set ac-
cording to their likelihood to be followed by a stock
rise. This is similar to the common IR task of rank-
ing documents according to their relevance. A per-
fect ranking would place all the correct tweets before
all the incorrect ones.
We present several ranking models that use the
expert finding framework and the bullishness classi-
fication methods discussed in the previous sections
as building blocks. The performance of these mod-
els is evaluated on the test set. By considering the
1315
precision at various points along the list of ranked
tweets, we can compare the precision-recall trade-
offs achieved by each model.
Before we discuss the ranking models and the em-
pirical results, we describe the datasets used to train
and test these models.
6.1 Datasets
Stock tweets were downloaded from the StockTwits
website3, during two periods: from April 25, 2010
to November 1, 2011, and from December 14, 2010
to February 3, 2011. A total of 700K tweets mes-
sages were downloaded. Tweets that do not contain
exactly one stock ticker (traded in NYSE or NAS-
DAQ) were filtered out. The remaining 340K tweets
were divided as follows:
? Development set: April 25, 2010 to August 31,
2010: 124K messages
? Held out set: September 1, 2010 to November
1, 2010: 110K messages
? Test set: December 14, 2010 to February 3,
2011: 106K messages
We consider the union of the development and held
out sets as our training set.
6.2 Ranking Models
6.2.1 Joint-All Model
This is our baseline model, as it does not attempt
to identify experts. It learns a single SVM model
as described in Section 5 from all the tweets in the
training set. It then applies the SVM model to each
tweet in the test set, and ranks them according to the
SVM classification score.
6.2.2 Transaction Model
This model finds expert users in the training set
(Algorithm 1), using the buy/sell classifier described
in Section 4. Tweets classified as buy are considered
bullish, and the rest are considered non-bullish. Ex-
pert users are ranked according to their p value (in
ascending order). The same classifier is then applied
to the tweets of the expert users in the test set. The
tweets classified as bullish are ordered according to
the ranking of their author (first all the bullish tweets
3stocktwits.com
of the highest-ranked expert user, then all the bullish
tweets of the expert ranked second, and so on).
6.2.3 Per-User Model
The joint all model suffers from the tweets of
non-experts twice: at training time, these tweets in-
troduce much noise into the training of the SVM
model. At test time, we follow these unreliable
tweets along with the more reliable tweets of the ex-
perts. The per-user model addresses both problems.
This model learns from the development set a sep-
arate SVM model Cu for each user u, based solely
on the user?s tweets. We then optimize the clas-
sification threshold of the learnt SVM model Cu
as follows. Setting the threshold to ? results in a
new classifier Cu,?. Algorithm 1 is applied to u?s
tweets in the held-out set (denoted Hu), using the
classifier Cu,?. For the ease of presentation, we de-
fine ExpertPValue(Hu, Cu,?,Pbl,?) as a function that
calls Algorithm 1 with the given parameters, and re-
turns the obtained p-value if u is an expert and 1
otherwise. We search exhaustively for the thresh-
old ?? for which this function is minimized (in other
words, the threshold that results in the best p-value).
The threshold of Cu is then set to ??, and the user?s
p-value is set to the best p-value found. If u is a
non-expert for all of the attempted ? values then u is
discarded. Otherwise, u is identified as an expert.
The rest of the process is similar to the transac-
tion model: the tweets of each expert u in the test
set are classified using the optimized per-user clas-
sifier Cu. The final ranking is obtained by sorting
the tweets that were classified as bullish according
to the p-value of their author. The per-user ranking
procedure is summarized in Algorithm 2.
6.2.4 Joint-Experts Model
The joint experts model makes use of the experts
identified by the per-user model, and builds a sin-
gle joint SVM model from the tweets of these users.
This results in a model that is trained on more exam-
ples than in the previous per-user method, but unlike
the joint all method, it learns only from high-quality
users. As with the joint all model, test tweets are
ranked according to the SVM?s score. However, the
model considers only the tweets of expert users in
the test set.
1316








 	

	

	

       




	

	
Figure 1: Empirical model comparison
Algorithm 2 Per-user ranking model
Input: dev. set D, held-out set H, test set S , base-
line probability Pbl, significance level ?
Output: A ranked listR of tweets in S
// Learning from the training set
E ? ? // set of expert users
for each user u do
Du ? u?s tweets in D
Cu ? SVM classifier learnt from Du
Hu ? u?s tweets inH
?? = argmin? ExpertPValue(Hu, Cu,?,Pbl,?)
Cu ? Cu,??
pu ?ExpertPValue(Hu, Cu,??,Pbl,?)if pu ? ? then
add u to E
end if
end for
// Classifying and ranking the test set
for each user u ? E do
Sbullish,u ? u?s tweets in S that were classified
as bullish by Cu
end for
R ? tweets in ?u Sbullish,u sorted by pureturn R
6.3 Results
Figure 1 summarizes the results obtained for the
various models. Each model was used to rank the
tweets according to the confidence that they predict
a positive stock price movement. Each data point
corresponds to the precision obtained for the first k
tweets ranked by the model, and the results for vary-
ing k values illustrate the precision/recall tradeoff of
the model. These data points were obtained as fol-
lows:
? For methods that learn a single SVM model
(joint all and joint experts), the graph was ob-
tained by decreasing the threshold of the SVM
classifier, at fixed intervals of 0.05. For each
threshold value, k is the number of tweets clas-
sified as bullish by the model.
? For methods that rank the users by their p value
and order the tweets accordingly (transaction
and per user), the i-th data point corresponds
to the cumulative precision for the tweets clas-
sified as bullish by the first i users. For the per
user method we show the cumulative results for
the first 20 users. For the transaction method
we show all the users that were identified as ex-
perts.
The random line is our baseline. It shows the ex-
pected results for randomly ordering the tweets in
the test set. The expected precision at any point is
equal to the percentage of tweets in the test set that
were followed by a stock rise, which was found to
be 51.4%.
We first consider the joint all method, which
learns a single model from all the tweets. The only
1317
Correct Incorrect P p
87 46 65.4 0.001
142 86 62.3 0.001
162 103 61.1 0.002
220 158 58.2 0.008
232 168 58.0 0.008
244 176 58.1 0.006
299 229 56.6 0.016
335 255 56.8 0.009
338 268 55.8 0.031
344 269 56.1 0.019
419 346 54.8 0.062
452 387 53.9 0.152
455 389 53.9 0.145
479 428 52.8 0.395
481 430 52.8 0.398
487 435 52.8 0.388
675 564 54.5 0.030
683 569 54.6 0.026
690 573 54.6 0.022
720 591 54.9 0.011
Table 4: Per user model: cumulative results for first 20
users. The table lists the number of correct and incorrect
tweets, the precision P and the significance level p.
per-user information available to this model is a fea-
ture fed to the SVM classifier, which, as we found,
does not contribute to the results. Except for the
first 58 tweets, which achieved precision of 55%,
the precision quickly dropped to a level of around
52%, which is just a little better than the random
baseline. Next, we consider the transaction configu-
ration, which is based on detecting buy transactions.
Only 10 users were found to be experts according to
this method, and in the test period these users had a
total of 173 tweets. These 173 tweets achieve good
precision (57.1% for the first 161 tweets, and 54.9%
for the first 173 tweets). However this method re-
sulted in a low number of transactions. This happens
because it is able to utilize only a small fraction of
the tweets (explicit buy transactions).
Remarkably, per user and joint experts, the two
methods which rely on identifying the experts via
unsupervised learning are by far the best methods.
Both models seem to have comparable performance,
where the results of the join experts model are some-
what smoother, as expected. Table 4 shows cumu-
lative results for the first 20 users in the per-user
model. The results show that this model achieves
good precision for a relatively large number of
tweets, and for most of the data points reported in the
table the results significantly outperform the base-
line (as indicated by the p value). Overall, these re-
sults show the effectiveness of our methods for find-
ing experts through unsupervised learning.
7 Related Work
A growing body of work aims at extracting senti-
ment and opinions from tweets, and exploit this in-
formation in a variety of application domains. Davi-
dov et al (2010) propose utilizing twitter hash-
tag and smileys to learn enhanced sentiment types.
O?Connor et al (2010) propose a sentiment detec-
tor based on Twitter data that may be used as a re-
placement for public opinion polls. Bollen et al
(2011) measure six different dimensions of public
mood from a very large tweet collection, and show
that some of these dimensions improve the predica-
tion of changes in the Dow Jones Industrial Average
(DJIA).
Sentiment analysis of news articles and financial
blogs and their application for stock prediction were
the subject of several studies in recent years. Some
of these works focus on document-level sentiment
classification (Devitt and Ahmad, 2007; O?Hare et
al., 2009). Other works also aimed at predicting
stock movement (Lavrenko et al, 2000; Koppel
and Shtrimberg, 2004; Schumaker and Chen, 2010).
All these methods rely on predefined sentiment lex-
icons, manually classified training texts, or their
combination. Lavrenko et al (2000), Koppel and
Shtrimberg (2004), and Schumaker and Chen (2010)
exploit stock prices for training, and thus save the
need in supervised learning.
Previous work on stock message boards include
(Das and Chen, 2007; Antweiler and Frank, 2004;
Chua et al, 2009). (Sprenger andWelpe, 2010) is, to
the best of our knowledge, the first work to address
specifically stock microblogs. All these works take
a similar approach for classifying message bullish-
ness: they train a classifier (Na??ve Bayes, which Das
and Chen combined with additional classifiers and
a sentiment lexicon, and Chua et al presented im-
provement for) on a collection of manually labeled
messages (classified into Buy, Sell, Hold). Interest-
ingly, Chua et al made use of an Australian mes-
1318
sage board (HotCopper), where, unlike most of the
stock message boards, these labels are added by the
message author. Another related work is (Zhang and
Skiena, 2010), who apply lexicon-based sentiment
analysis to several sources of news and blogs, in-
cluding tweets. However, their data set does not in-
clude stock microblogs, but tweets mentioning the
official company name.
Our work differs from previous work on stock
messages in two vital aspects. Firstly, these works
did not attempt to distinguish between experts and
non-expert users, but aggregated the sentiment over
all the users when studying the relation between sen-
timent and the stock market. Secondly, unlike these
works, our best-performing methods are completely
unsupervised, and require no manually tagged train-
ing data or sentiment lexicons.
8 Conclusion
This paper investigated the novel task of finding ex-
pert investors in online stock forums. In particular,
we focused on stock microblogs. We proposed a
framework for finding expert investors, and exper-
imented with several methods for tweet classifica-
tion using this framework. We found that combin-
ing our framework with user-specific unsupervised
learning allows us to predict stock price movement
with high precision, and the results were shown to be
statistically significant. Our results illustrate the im-
portance of distinguishing experts from non-experts.
An additional contribution of this work is an in-
depth analysis of stock tweets, which sheds light on
their content and its potential utility.
In future work we plan to improve the features of
the SVM classifier, and further investigate the use-
fulness of our approach for trading.
References
Werner Antweiler and Murray Z. Frank. 2004. Is all
that talk just noise? the information content of in-
ternet stock message boards. Journal of Finance,
59(3):1259?1294.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1 ? 8.
Christopher Chua, Maria Milosavljevic, and James R.
Curran. 2009. A sentiment detection engine for in-
ternet stock message boards. In Proceedings of the
Australasian Language Technology Association Work-
shop 2009.
Sanjiv R. Das and Mike Y. Chen. 2007. Yahoo! for
Amazon: Sentiment extraction from small talk on the
Web. Management Science, 53(9):1375?1388.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10. Association for Computational Linguis-
tics.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 984?991.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scholkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT-Press.
Moshe Koppel and Itai Shtrimberg. 2004. Good news
or bad news? Let the market decide. In Proceedings
of the AAAI Spring Symposium on Exploring Attitude
and Affect in Text: Theories and Applications.
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000. Min-
ing of concurrent text and time series. In Proceedings
of the 6th ACM SIGKDD Int?l Conference on Knowl-
edge Discovery and Data Mining Workshop on Text
Mining.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
tweets to polls: Linking text sentiment to public opin-
ion time series. In Proceedings of the International
AAAI Conference on Weblogs and Social Media.
Neil O?Hare, Michael Davy, Adam Bermingham, Paul
Ferguson, Pvraic Sheridan, Cathal Gurrin, and Alan F
Smeaton. 2009. Topic-dependent sentiment analysis
of financial blogs. In TSA?09 - 1st International CIKM
Workshop on Topic-Sentiment Analysis for Mass Opin-
ion Measurement.
Robert P. Schumaker and Hsinchun Chen. 2010. A dis-
crete stock price prediction engine based on financial
news. Computer, 43:51?56.
Timm O. Sprenger and Isabell M. Welpe. 2010. Tweets
and trades: The information content of stock mi-
croblogs. Technical report, TUM School of Manage-
ment, December. working paper.
Wenbin Zhang and Steven Skiena. 2010. Trading
strategies to exploit blog and news sentiment. In
ICWSM?10.
1319
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 87?92,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Entities' Sentiment Relevance  
 
 
Zvi Ben-Ami 
The Hebrew University 
Jerusalem, ISRAEL 
zvi.benami@mail.huji.a
c.il 
Ronen Feldman 
The Hebrew University 
Jerusalem, ISRAEL 
ronen.feldman@huji.ac
.il 
Binyamin Rosenfeld 
Digital Trowel  
New York, USA 
grurgrur@gmail.
com 
 
  
 
Abstract 
Sentiment relevance detection problems oc-
cur when there is a sentiment expression in a 
text, and there is the question of whether or 
not the expression is related to a given entity 
or, more generally, to a given situation. The 
paper discusses variants of the problem, and 
shows that it is distinct from other somewhat 
similar problems occurring in the field of sen-
timent analysis and opinion mining. We ex-
perimentally demonstrate that using the in-
formation about relevancy significantly af-
fects the final sentiment evaluation of the en-
tities. We then compare a set of different al-
gorithms for solving the relevance detection 
problem. The most accurate results are 
achieved by algorithms that use certain doc-
ument-level information about the target enti-
ties. We show that this information can be 
accurately extracted using supervised classi-
fication methods. 
1 Introduction 
Sentiment extraction by modern sentiment analy-
sis (SA) systems is usually based on searching 
the input text for sentiment-bearing words and 
expressions, either general (language-wide) or 
domain-specific. In most common SA approach-
es, each such expression carries a polarity value 
("positive" or "negative") which is possibly 
weighted. The sum of all polarity values from all 
expressions found in a text becomes the senti-
ment score for the whole text.  
People are, however, usually interested in sen-
timents regarding some entity or situation, and 
not in sentiments of a particular document. A 
natural way to make the SA more focused is to 
explicitly bind each sentiment expression to a 
specific entity, or to a small set of entities from 
among all entities mentioned in the document. 
The choice of which entity to bind a sentiment 
expression to, can be made according to the 
proximity (physical, syntactical, and/or semantic) 
and/or salience of the entities. 
In this paper, we argue that all of these meth-
ods can be useful in different contexts, and so the 
best single algorithm should use all available 
proximity information, of all kinds, together with 
additional context information ?position in the 
document, section, or paragraph; proximity of 
other entities; lexical contents; etc. One of the 
most important context information is the type of 
relation between the target entity and the docu-
ment ? whether the entity is the main topic of the 
document, or one of several main topics, or men-
tioned in passing, etc. 
Another layer that we'd like to add concerns 
the interaction of different entity types during 
SA. In a typical situation, there is only one entity 
type which is the target for SA. In such cases, 
clearly distinguishing between the relevancy of 
target and non-target entities types is not essen-
tial. For example, when the general topic is a 
COMPANY, and there is a sentiment expression 
referring to a PERSON or a PRODUCT, this 
sentiment expression is still relevant to the com-
pany and can be regarded as such. In other situa-
tions, SA users may be specifically interested in 
an interaction between entities of different types. 
For example, in a medical forum setting, it may 
be interesting to know the users' sentiments re-
garding a given DRUG in the context of a given 
DISEASE. We will show that such situations are 
modeled well enough using intersections of re-
gions of relevance of the participating entity 
types, with the relevance region for each type 
calculated separately. 
We purposefully exclude possible interactions 
between entities of the same type, because they 
behave in a different way. The precise analysis 
of such interactions is a different topic from rele-
87
vance detection, and so it is mostly ignored in 
this paper. 
2 Related Work 
The task of SA has drawn the attention of many 
researchers worldwide (Connor et al, 2010; Liu, 
2012; Loughran and Mcdonald, 2010; Pang and 
Lee, 2004; Turney, 2002).  While most SA re-
search is focused on discovering and classifying 
the expressions, some are also concerned with 
the targets of the expressions and explicitly iden-
tify the syntactic targets of sentiment expressions 
(Pang and Lee, 2004).  
Other related works belong to the Passage Re-
trieval field, since the relevance detection prob-
lem can be construed as a specific form of pas-
sage retrieval problem (Liu and Croft, 2002; 
Tiedemann and Mur, 2008). Different approach-
es were suggested for passage retrieval (Buscaldi 
et al, 2010; Comas et al, 2012; Hearst, 1997; 
Lafferty et al, 2001; Lin et al, 2012; Liu and 
Croft, 2002; Lloret et al, 2012; O?Connor et al, 
2013; Otterbacher et al, 2009; Salton et al, 
1993; Wachsmuth, 2013), some are more sophis-
ticated than others.  
The closest approach to ours is the one of 
Scheible and Sch?tze (2013), but in contrast to 
them, we strive to discover sentiments' relevance 
for all entities (of a given type) mentioned in the 
document, not necessarily topical. 
3 Entity Relevance 
An instance of the sentiment relevance detection 
problem for a single entity consists of a text doc-
ument, a sentiment expression within the docu-
ment, and a target entity. The task is a binary 
decision: 'relevant' vs. 'irrelevant'. To solve this 
task, we can use any information that can be 
found by analyzing the document. Thus, we can 
assume that we know the parse trees of all sen-
tences and the locations of all references of all 
entities in the document, including co-references. 
In addition, we make use of an extra piece of 
information for each target entity ? its "status 
within the document", or "document type with 
respect to the entity". We distinguish between 
several types which are intuitively clearly differ-
ent: 
? 'Target' ? the entity is the main topic of the 
document; 
? 'Accidental' ? the entity is not the main topic 
of the document, and is mentioned in passing; 
? 'RelationTarget' ? the main topic of the doc-
ument is a relation between the entity and 
some other entities of the same type; 
? 'ListTarget' ? the entity is one of a few equal-
ly important topics, dealt with sequentially. 
In the datasets we use for experiments, each 
entity is manually annotated with its status with-
in the document, which allows us to directly ob-
serve the influence of this data on the accuracy 
of relevance discernment. We also show that this 
data can be automatically extracted using super-
vised classification. 
Since this paper is primarily a study of senti-
ment relevance, the actual sentiment expressions 
are not always labeled in our datasets. Instead, 
relevance ranges are annotated for each entity, in 
the style of passage retrieval problems, with the 
expectation that sentiment expressions relevant 
to an entity only appear in the parts of the docu-
ment that are labeled as "relevant", and converse-
ly, that all expressions appearing in parts labeled 
"irrelevant" are irrelevant. This way of annotat-
ing allows the comparing of different relevance 
detection strategies independently of the main 
sentiment extraction tool.  
All of the algorithms discussed in this paper 
use the same document processing methods, thus 
allowing us to compare the algorithms them-
selves independent of the quality and specifics of 
the underlying NLP. 
The multiple-entity relevance problem is dis-
tinguished from the single-entity relevance prob-
lem by the requirement for the sentiment expres-
sion to be relevant to several entities of different 
types. The problem is close to Relation Extrac-
tion in this sense. The examples we are interested 
in are in the medical domain and deal with three 
main entity types: PERSON, DRUG, and 
DISEASE, where PERSON is restricted to 
known physicians. While each of the entity types 
can be the target of a sentiment expression, the 
more interesting questions in this domain involve 
multiple entities, specifically, DRUG + 
DISEASE ("how effective is this drug for this 
disease?"), and PERSON + DRUG + DISEASE 
("what does this physician say about using this 
drug to cure this disease?"). 
We solve the multiple-entity relevance prob-
lem by intersecting the relevance ranges of dif-
ferent-type entities, thus reducing the problem to 
the single-entity relevance detection. As such, 
the experiments regarding the multiple-entity 
relevance need only check the accuracy of this 
reduction. In the medical domain, at least, this 
accuracy appears to be adequate. 
88
4 Relevance Algorithms 
Each algorithm receives, as input, the text of the 
document, with labeled reference of the target 
entity and other entities of the same type. The 
labeled references also include all coreferential 
references, extracted automatically by an NLP 
system. The input text also includes labeled can-
didate sentiment expressions, either manually 
labeled or automatically extracted by a rele-
vance-ignoring SA system1. The task of the algo-
rithms is to label each candidate expression as 
relevant or irrelevant to the target entity. The 
algorithms are evaluated according to the accura-
cy (recall, precision, and F1) of this labeling of 
individual sentiment expressions.  
This method produces a reasonably well-
understandable quality measure (the percentage 
of expressions that the algorithms get right or 
wrong), and also allows us to compare algo-
rithms focused on individual expressions and 
algorithms working on text ranges. The algo-
rithms we evaluate are as follows: 
? Baseline - Every expression is declared rele-
vant. This is the standard mode of operation of 
document-level SA tools, although it is usually 
only applied to the 'Target' entities ? the main 
topic(s) of the document. 
? Physical-proximity-based - A text-range fo-
cused algorithm, which labels pieces of text as 
relevant or irrelevant according to their place-
ment relative to the references of the target en-
tity and other entities of the same type, as well 
as some other contextual clues, such as para-
graph boundaries. Generally, the mentioning of 
an entity starts its relevance range (and stops 
the relevance range of the previously men-
tioned entity). For the first entity reference in a 
paragraph, the range also extends backward to 
the beginning of the sentence. There are three 
flavors of the algorithm, specifically adapted 
for different document-types-with-respect-to-
the-target-entity: 
o 'Proximity-Accidental' - stops relevance 
ranges at paragraph boundaries, 
o 'Proximity-Targeted' - restarts relevance 
ranges at paragraph boundaries (every para-
                                                 
1In our experiments, we also use a standalone automatic 
Financial SA system from Feldman et al (2010), working 
in the 'ignore relevance' mode, which (1) finds and labels 
all entities of the target type(s); (2) resolves all corefer-
ences for the target entity type(s); (3) finds and labels all 
sentiment expressions, regardless of their relevance; and 
(4) provides dependency parses for all sentences in the 
corpus.  
graph is assumed relevant at the start, unless 
another entity is mentioned). 
o 'Proximity-List' - interpolates relevance 
ranges over intermission paragraphs, unless 
they are explicitly irrelevant (e.g., contain-
ing references of other entities of the same 
type). 
? Syntactic-proximity-based - An expression-
focused algorithm, which labels expressions as 
relevant or irrelevant according to their dis-
tance to various entity references in the de-
pendency parse graph. There are two flavors of 
the algorithm: direct and reverse. The former 
considers an expression relevant only if it is 
closest to the target entity from among all enti-
ties of the same type, and the distance is suffi-
ciently close. The latter considers an expres-
sion irrelevant only if it has the above-
described relation to some non-target entity of 
the same type. The rationale for the two flavors 
is the distinction between 'Targeted' and 'Acci-
dental' document types regarding the target en-
tity. For the 'Accidental' entities, a sentiment 
expression is assumed to be relevant only if it 
is explicitly connected to the entity. For 'Tar-
geted' entities, an expression is irrelevant only 
if it is explicitly connected to some other entity 
of the same type. 
? Classification-based - This algorithm consid-
ers each candidate sentiment expression as an 
instance of a binary classification problem, to 
be solved using supervised classification. For 
evaluating this algorithm, some part of the test 
corpus is used for training, and the other for 
testing, with N-fold cross-validation. The fea-
tures for classification may use any infor-
mation present in the input.  
In the current experiments, we use refer-
ences of target and non-target entities, appear-
ances of paragraph and document boundaries, 
length of syntactic connections to target and 
non-target entities, when available, and explicit 
entity status within documents, when available. 
The (binary) classification features are built 
from sequences of up to 5 occurrences of the 
above-described pieces, with the pieces ap-
pearing before and after the sentiment expres-
sion tracked separately. For classification, we 
use a linear classifier with Large Margin train-
ing (regularized perceptron, as discussed in 
Scheible and Sch?tze, (2013)). 
? Sequence-classification-based - The algo-
rithm uses exactly the same features as the di-
rect classification-based above, but instead of 
considering each expression separately, it con-
89
siders them as a sequence, one per document. 
So, instead of a Large Margin binary classifier, 
a probabilistic sequence classifier is used 
(CRF, as discussed in Lafferty et al (2001)). 
5 Experiments 
For the experiments, we use two manually-
annotated corpora 2 , a financial corpus 3  and a 
medical4 corpus. In the Financial corpus, COM-
PANIEs are used as target entities and in the 
medical corpus, DISEASEs, DRUGs and PER-
SONs are the entity types that are used as target 
entities. For the purpose of the experiments, we 
are interested only in single-entity sentiments 
about DRUGs, and multiple-entity sentiments 
about DRUGs + DISEASEs, and DRUGs + 
DISEASEs + PERSONs. 
The evaluation metrics in all of the experi-
ments are precision, recall, and F1. For the clas-
sification-based algorithms, unless stated other-
wise, we use 10-fold cross-validation.  
5.1 Experiment: Importance of relevance 
In the first experiment, we demonstrate the im-
portance of using relevance when calculating the 
consolidated sentiment score of an entity within 
a set of documents. For each entity, we set the 
'correct' consolidated sentiment score to the av-
erage of polarities of all sentiments in a corpus 
which are labeled as relevant to the entity. Then, 
we compare the correct value to the two scores 
calculated without considering relevance: 
? 'Baseline' - the average of polarities of all sen-
timents in all documents where the entity is 
mentioned, and 
? 'TargetedOnly' - the average of polarities of 
all sentiments in the documents where the enti-
ty is labeled as target (main topic of the docu-
ment). This case models the typical state of a 
relevance-agnostic SA system. 
For this evaluation, we only compare the sign 
of the final sentiment scores, without considering 
their magnitudes (unless it is close to zero, in 
                                                 
2 Fully annotating texts for semantic relevance is an arduous 
task, thus the used annotated corpora are relatively small. 
Sample can be found at http://goo.gl/6HONHP. 
3 A corpus of 160 financial news documents on at least one 
entity of interest, of average size ~5Kb, downloaded from 
various financial news websites. The dataset mentions 424 
different companies.  
4 A corpus of 160 documents, of average size ~7Kb, down-
loaded following Google queries on a set of a few com-
mon drugs and diseases. The dataset mentions 722 differ-
ent people, 46 diseases, and 175 drugs. 
which it is considered 'neutral'). The errors at this 
level indicate definite SA errors ? miscalculating 
entity's sentiment into its opposite.  
The results of the evaluation are as follows: 
The 'Baseline' scores show a large difference 
from the correct scores, with 33% and 38% of 
entities having wrong final polarity in the finan-
cial (COMPANY) and medical (DRUG) do-
mains, respectively. The 'TargetedOnly' scores 
are somewhat closer to correct, with 12% and 
28% of entities with incorrect final polarities. 
However, the 'TargetedOnly' method naturally 
suffers from a very low recall, with only 19% 
and 38% of entities covered in the financial and 
medical domains, respectively. 
5.2 Experiment: Influence of entity status 
In this experiment, we compare the performance 
of various algorithms while either providing or 
withholding the information about the document-
type-with-respect-to-the-target-entity. 
The performance of the physical proximity al-
gorithms on the financial corpus is shown at the 
top left hand side of Table 1. The set of all in-
stances of relevance detection problems in the 
corpus (an instance consists of a sentiment ex-
pression within a text, together with a target enti-
ty) is divided into three subsets, according to the 
status of the target entity within the document. 
As expected, the three flavors of the physical 
proximity algorithm perform much better on the 
corpus subsets they are adapted to. At the bottom 
left hand side of Table 1, we similarly show the 
performance of the two flavors of the syntax-
proximity-based algorithm on the medical do-
main (DRUG entities). Same as above, there is a 
large difference in the performance of the two 
flavors of the algorithm on different subsets of 
the problem set. Finally, at the top of Table 2, we 
compare the performance of the two classifica-
tion-based algorithms on the two (whole) prob-
lem sets, while either keeping or withholding the 
entity status information from the classifier. The 
difference in results is less pronounced here, but 
is still noticeable. The reason for the smaller dif-
ference, we hypothesize, is the ability of the clas-
sifiers to partially infer the entity status from the 
various context clues that are used as classifica-
tion features (see the experiment 5.3). 
5.3 Experiment: Automatic identification of 
entity status using classification. 
In this experiment, we confirm that it is possible 
to identify the entity status within documents 
using supervised classification. 
90
 
Table 1. Performance of different algorithms on three subsets of the corpus with a different status of 
the target entity within the document. 
 
Experiment Algorithm  Financial Medical 
Experiment 5.2  
(Prec./ Rec,/F1). 
Classification (with entity 
status info) 
90/86/88 84/88/86 
Classification  (without 
entity status info) 
89/85/87 87/81/84 
Sequence Classification  
(with entity status info) 
96/84/90 99/84/91 
Sequence Classification  
(without entity status info) 
96/83/89 95/85/90 
Experiment 5.3 
(F1, (diff. in F1 
from exp. 5.2)) 
Classification 86.7  (-0.9) 83.9 (-2.0) 
Sequence Classification 89.7 (+0.1) 90.9 (-0.3) 
Experiment 5.5  
(F1) 
Baseline 37.2 28.6 
Physical Proximity 84.1 79.5 
Syntactic-Proximity 43.8 54.6 
Classification 87.6 85.9 
Sequence-Classification 91.2 89.6 
 
Table 2. Performance of different algorithms 
on the different domains. 
 
The results of direct evaluation show that the 
accuracies of the Medical and Financial corpora 
(using 10-fold X-validation) are 87.8% and 
82.2% respectively, and the accuracy when using 
the Medical corpus for training the Financial 
corpus for testing and vice versa, are 78.2% and 
86.1% , respectively.  
The results of relevance detection using the 
automatically extracted entity status values are 
shown at the right hand side of Table 1 and in the 
middle of Table 2, which utilize the same da-
tasets and algorithms as at the left hand side of 
Table 1 and at the top of Table 2.  As can be seen 
from the tables, the drop in performance is small, 
demonstrating the success of classification-based 
extraction of entity status information. 
5.4 Experiment: Cross-domain applicability 
In this experiment, we test how well the classifi-
ers trained on data from one domain work on 
input from a different domain.  
The classification results using different types 
of training data are shown in Table 3.  
 
  Classification Sequence classification 
Medical 2-fold/10-fold 84.6/85.9 85.7/89.6 
Train on Fin, test on Med 83.5 86.8 
Financial 2-fold/10-fold 86.1/87.6 90.3/91.2 
Train on Med, test on Fin 85.4 91.0 
 
Table 3. Performance of classification-based  
algorithms using different training data (F1). 
The table confirms general independence of 
the classification performance on the domain. 
Comparing the 2-fold and 10-fold cross-
validation results (the difference is equivalent to 
doubling the amount of training data), shows that 
the amount of training data is sufficient. 
5.5 Experiment: Overall performance of 
algorithms 
In this experiment, we simply compare the over-
all accuracy of various algorithms for relevance 
discernment, operating at their best parameters. 
The results are shown at the bottom of Table 2. 
Overall, classification-based algorithms perform 
better than the deterministic ones, with sequence-
classification performing significantly better than 
direct classification. Syntactic proximity-based is 
precise, but has relatively low recall, reducing its 
overall performance. Physical proximity-based is 
simplest, and produce reasonably high overall 
results, although worse than the best-performing 
classification-based methods. 
6 Conclusion 
The results are mostly intuitively understood and 
confirm the expectations. We confirmed that 
relevance detection is essential for producing 
correct consolidated SA results. We found that 
the entity status within the document is one of 
the important clues for solving the relevance 
detection problem, and showed that this infor-
mation can be effectively automatically extracted 
using supervised classification. We also com-
pared several algorithms for relevance detection, 
with the results that classification-based algo-
rithms generally outperform simpler ones based 
on the same clues, although a very simple prox-
imity-based algorithm performs reasonably well 
if allowed to use the entity status information.  
 
Acknowledgments 
 
This work is supported by the Israel Ministry of 
Science and Technology Center of Knowledge in 
Machine Learning and Artificial Intelligence and 
the Israel Ministry of Defense. 
 Experiment 5.2  (Precision/Recall/F1) Experiment 5.3 ( F1, (diff. in F1 from exp. 5.2) 
 Accidental Targeted List Whole Accidental Targeted List Whole 
Proximity-Accidental 84/43/57 93/76/84 92/74/82 92/72/81 60 (+2.6) 79 (-5.5) 83 (+1.1)  
Proximity-Targeted 31/50/38 90/84/87 55/89/68 63/83/72 38 (-0.4) 82 (-5.2) 73 (+4.3)  
Proximity-List 58/44/50 90/83/87 88/83/86 85/80/82 52 (+2.1) 81 (-5.9) 87 (+1.6)  
Proximity-Combined    89/80/84    83 (-1.2) 
Syntactic-Prox.-Direct 93/48/64 99/42/60   65 (+0.8) 59 (-0.2)   
Syntactic-Prox.-Inverse 04/72/08 70/66/68   8 (-0.2) 76 (+6.4)   
91
References 
Buscaldi, D., Rosso, P., G?mez-Soriano, J., Sanchis, 
E., 2010. Answering questions with an n-gram 
based passage retrieval engine. J. Intell. Inf. Syst. 
34, 113?134. doi:10.1007/s10844-009-0082-y 
Comas, P.R., Turmo, J., M?rquez, L., 2012. Sibyl, a 
factoid question-answering system for spoken 
documents. ACM Trans. Inf. Syst. 30, 19:1?19:40. 
doi:10.1145/2328967.2328972 
Connor, B.O., Balasubramanyan, R., Routledge, B.R., 
Smith, N.A., 2010. From Tweets to Polls?: Linking 
Text Sentiment to Public Opinion Time Series, in: 
Proceedings of the Fourth International AAAI 
Conference on Weblogs and Social Media. pp. 
122?129. 
Feldman, R., Rosenfeld, B., Bar-haim, R., Fresko, M., 
2010. The Stock Sonar ? Sentiment Analysis of 
Stocks Based on a Hybrid Approach, in: 
Proceedings of the Twenty-Third Innovative 
Applications of Artificial Intelligence Conference. 
pp. 1642?1647. 
Hearst, M.A., 1997. TextTiling: segmenting text into 
multi-paragraph subtopic passages. Comput. 
Linguist. 23, 33?64. 
Lafferty, J., McCallum, A., Pereira, F., 2001. 
Conditional Random Fields: Probabilistic Models 
for Segmenting and Labeling Sequence Data., in: 
Proceedings of the Eighteenth International 
Conference on Machine Learning (ICML-2001). 
Lin, H.-T., Chi, N.-W., Hsieh, S.-H., 2012. A 
concept-based information retrieval approach for 
engineering domain-specific technical documents. 
Adv. Eng. Informatics 26, 349?360. 
doi:http://dx.doi.org/10.1016/j.aei.2011.12.003 
Liu, B., 2012. Sentiment Analysis and Opinion 
Mining Synthesis Lectures on Human Language 
Technologies. Morgan & Claypool Publishers. 
Liu, X., Croft, W.B., 2002. Passage retrieval based on 
language models, in: Proceedings of the Eleventh 
International Conference on Information and 
Knowledge Management, CIKM ?02. ACM, New 
York, NY, USA, pp. 375?382. 
doi:10.1145/584792.584854 
Lloret, E., Balahur, A., G?mez, J., Montoyo, A., 
Palomar, M., 2012. Towards a unified framework 
for opinion retrieval, mining and summarization. J. 
Intell. Inf. Syst. 39, 711?747. doi:10.1007/s10844-
012-0209-4 
Loughran, T.I.M., Mcdonald, B., 2010. When is a 
Liability not a Liability?? Textual Analysis , 
Dictionaries , and 10-Ks Journal of Finance , 
forthcoming. J. Finance 66, 35?65. 
O?Connor, B., Stewart, B.M., Smith, N.A., 2013. 
Learning to Extract International Relations from 
Political Context, in: Proceedings of the 51st 
Annual Meeting of the Association for 
Computational Linguistics (Volume 1: Long 
Papers). Association for Computational 
Linguistics, Sofia, Bulgaria, pp. 1094?1104. 
Otterbacher, J., Erkan, G., Radev, D.R., 2009. Biased 
LexRank: Passage retrieval using random walks 
with question-based priors. Inf. Process. Manag. 
45, 42?54. 
doi:http://dx.doi.org/10.1016/j.ipm.2008.06.004 
Pang, B., Lee, L., 2004. A Sentimental Education?: 
Sentiment Analysis Using Subjectivity 
Summarization Based on Minimum Cuts. 
Salton, G., Allan, J., Buckley, C., 1993. Approaches 
to passage retrieval in full text information 
systems, in: Proceedings of the 16th Annual 
International ACM SIGIR Conference on Research 
and Development in Information Retrieval, SIGIR 
?93. ACM, New York, NY, USA, pp. 49?58. 
doi:10.1145/160688.160693 
Scheible, C., Sch?tze, H., 2013. Sentiment Relevance, 
in: Proceedings of the 51st Annual Meeting of the 
Association for Computational Linguistics 
(Volume 1: Long Papers). Association for 
Computational Linguistics, Sofia, Bulgaria, pp. 
954?963. 
Tiedemann, J., Mur, J., 2008. Simple is best: 
experiments with different document segmentation 
strategies for passage retrieval, in: Coling 2008: 
Proceedings of the 2nd Workshop on Information 
Retrieval for Question Answering, IRQA ?08. 
Association for Computational Linguistics, 
Stroudsburg, PA, USA, pp. 17?25. 
Turney, P., 2002. Thumbs Up or Thumbs Down?? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews, in: Proceedings of the 
Association for Computational Linguistics (ACL). 
pp. 417?424. 
Wachsmuth, H., 2013. Information Extraction as a 
Filtering Task Categories and Subject Descriptors, 
in: To Appear in Proc. of the 22th ACM CIKM. 
 
92
