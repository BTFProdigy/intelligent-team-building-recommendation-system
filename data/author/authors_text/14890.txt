Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1356?1364,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Decomposition of a Document into Authorial Components 
 
 
Moshe Koppel      Navot Akiva Idan Dershowitz Nachum Dershowitz 
Dept. of Computer Science  Dept. of Bible School of Computer Science 
Bar-Ilan University Hebrew University Tel Aviv University 
Ramat Gan, Israel Jerusalem, Israel Ramat Aviv, Israel 
{moishk,navot.akiva}@gmail.com dershowitz@gmail.com nachumd@tau.ac.il 
 
 
 
 
Abstract 
We propose a novel unsupervised method 
for separating out distinct authorial compo-
nents of a document. In particular, we show 
that, given a book artificially ?munged? 
from two thematically similar biblical 
books, we can separate out the two consti-
tuent books almost perfectly. This allows 
us to automatically recapitulate many con-
clusions reached by Bible scholars over 
centuries of research. One of the key ele-
ments of our method is exploitation of dif-
ferences in synonym choice by different 
authors. 
1 Introduction  
We propose a novel unsupervised method for 
separating out distinct authorial components of a 
document.  
There are many instances in which one is faced 
with a multi-author document and wishes to deli-
neate the contributions of each author. Perhaps the 
most salient example is that of documents of his-
torical significance that appear to be composites of 
multiple earlier texts. The challenge for literary 
scholars is to tease apart the document?s various 
components. More contemporary examples include 
analysis of collaborative online works in which 
one might wish to identify the contribution of a 
particular author for commercial or forensic pur-
poses.  
We treat two versions of the problem. In the 
first, easier, version, the document to be decom-
posed is given to us segmented into units, each of 
which is the work of a single author. The challenge 
is only to cluster the units according to author. In 
the second version, we are given an unsegmented 
document and the challenge includes segmenting 
the document as well as clustering the resulting 
units. 
We assume here that no information about the 
authors of the document is available and that in 
particular we are not supplied with any identified 
samples of any author?s writing. Thus, our me-
thods must be entirely unsupervised.  
There is surprisingly little literature on this 
problem, despite its importance. Some work in this 
direction has been done on intrinsic plagiarism de-
tection (e.g., Meyer zu Eisen 2006) and document 
outlier detection (e.g., Guthrie et al 2008), but this 
work makes the simplifying assumption that there 
is a single dominant author, so that outlier units 
can be identified as those that deviate from the 
document as a whole. We don?t make this simpli-
fying assumption. Some work on a problem that is 
more similar to ours was done by Graham et al 
(2005). However, they assume that examples of 
pairs of paragraphs labeled as same-
author/different-author are available for use as the 
basis of supervised learning. We make no such 
assumption. 
The obvious approach to our unsupervised ver-
sion of the problem would be to segment the text 
(if necessary), represent each of the resulting units 
of text as a bag-of-words, and then use clustering 
algorithms to find natural clusters. We will see, 
however, that this na?ve method is quite inade-
quate. Instead, we exploit a method favored by the 
literary scholar, namely, the use of synonym 
choice. Synonym choice proves to be far more use-
ful for authorial decomposition than ordinary lexi-
cal features. However, synonyms are relatively 
1356
sparse and hence, though reliable, they are not 
comprehensive; that is, they are useful for separat-
ing out some units but not all. Thus, we use a two-
stage process: first find a reliable partial clustering 
based on synonym usage and then use these as the 
basis for supervised learning using a different fea-
ture set, such as bag-of-words. 
We use biblical books as our testbed. We do 
this for two reasons. First, this testbed is well mo-
tivated, since scholars have been doing authorial 
analysis of biblical literature for centuries. Second, 
precisely because it is of great interest, the Bible 
has been manually tagged in a variety of ways that 
are extremely useful for our method. 
Our main result is that given artificial books 
constructed by randomly ?munging? together ac-
tual biblical books, we are able to separate out au-
thorial components with extremely high accuracy, 
even when the components are thematically simi-
lar. Moreover, our automated methods recapitulate 
many of the results of extensive manual research in 
authorial analysis of biblical literature. 
The structure of the paper is as follows. In the 
next section, we briefly review essential informa-
tion regarding our biblical testbed. In Section 3, we 
introduce a na?ve method for separating compo-
nents and demonstrate its inadequacy. In Section 4, 
we introduce the synonym method, in Section 5 we 
extend it to the two-stage method, and in Section 6, 
we offer systematic empirical results to validate 
the method. In Section 7, we extend our method to 
handle documents that have not been pre-
segmented and present more empirical results. In 
Section 8, we suggest conclusions, including some 
implications for Bible scholarship. 
2 The Bible as Testbed 
While the biblical canon differs across religions 
and denominations, the common denominator con-
sists of twenty-odd books and several shorter 
works, ranging in length from tens to thousands of 
verses. These works vary significantly in genre, 
and include historical narrative, law, prophecy, and 
wisdom literature. Some of these books are re-
garded by scholars as largely the product of a sin-
gle author?s work, while others are thought to be 
composites in which multiple authors are well-
represented ? authors who in some cases lived in 
widely disparate periods. In this paper, we will 
focus exclusively on the Hebrew books of the Bi-
ble, and we will work with the original untran-
slated texts. 
The first five books of the Bible, collectively 
known as the Pentateuch, are the subject of much 
controversy. According to the predominant Jewish 
and Christian traditions, the five books were writ-
ten by a single author ? Moses. Nevertheless, scho-
lars have found in the Pentateuch what they believe 
are distinct narrative and stylistic threads corres-
ponding to multiple authors.  
Until now, the work of analyzing composite 
texts has been done in mostly impressionistic fa-
shion, whereby each scholar attempts to detect the 
telltale signs of multiple authorship and compila-
tion. Some work on biblical authorship problems 
within a computational framework has been at-
tempted, but does not handle our problem. Much 
earlier work (for example, Radday 1970; Bee 
1971; Holmes 1994) uses multivariate analysis to 
test whether the clusters in a given clustering of 
some biblical text are sufficiently distinct to be 
regarded as probably a composite text. By contrast, 
our aim is to find the optimal clustering of a docu-
ment, given that it is composite. Crucially, unlike 
that earlier work, we empirically prove the efficacy 
of our methods by testing it against known ground 
truth. Other computational work on biblical au-
thorship problems (Mealand 1995; Berryman et al 
2003) involves supervised learning problems 
where some disputed text is to be attributed to one 
of a set of known authors. The supervised author-
ship attribution problem has been well-researched 
(for surveys, see Juola (2008), Koppel et al (2009) 
and Stamatatos (2009)), but it is quite distinct from 
the unsupervised problem we consider here.  
Since our problem has been dealt with almost 
exclusively using heuristic methods, the subjective 
nature of such research has left much room for de-
bate. We propose to set this work on a firm algo-
rithmic basis by identifying an optimal stylistic 
subdivision of the text. We do not concern our-
selves with how or why such distinct threads exist. 
Those for whom it is a matter of faith that the Pen-
tateuch is not a composition of multiple writers can 
view the distinction investigated here as that of 
multiple styles. 
3 A Na?ve Algorithm 
For expository purposes, we will use a canoni-
cal example to motivate and illustrate each of a 
1357
sequence of increasingly sophisticated algorithms 
for solving the decomposition problem. Jeremiah 
and Ezekiel are two roughly contemporaneous 
books belonging to the same biblical sub-genre 
(prophetic works), and each is widely thought to 
consist primarily of the work of a single distinct 
author. Jeremiah consists of 52 chapters and Eze-
kiel consists of 48 chapters. For our first challenge, 
we are given all 100 unlabeled chapters and our 
task is to separate them out into the two constituent 
books. (For simplicity, let?s assume that it is 
known that there are exactly two natural clusters.) 
Note that this is a pre-segmented version of the 
problem since we know that each chapter belongs 
to only one of the books. 
As a first try, the basics of which will serve as a 
foundation for more sophisticated attempts, we do 
the following: 
1. Represent each chapter as a bag-of-words (us-
ing all words that appear at least k times in the 
corpus). 
2. Compute the similarity of every pair of chapters 
in the corpus. 
3. Use a clustering algorithm to cluster the chap-
ters into two clusters. 
We use k=2, cosine similarity and ncut cluster-
ing (Dhillon et al 2004). Comparing the Jeremiah-
Ezekiel split to the clusters thus obtained, we have 
the following matrix: 
Book Cluster I Cluster II 
Jer 
Eze 
29 
28 
23 
20 
 
As can be seen, the clusters are essentially or-
thogonal to the Jeremiah-Ezekiel split. Ideally, 
100% of the chapters would lie on the majority 
diagonal, but in fact only 51% do. Formally, our 
measure of correspondence between the desired 
clustering and the actual one is computed by first 
normalizing rows and then computing the weight 
of the majority diagonal relative to the whole. This 
measure, which we call normalized majority di-
agonal (NMD), runs from 50% (when the clusters 
are completely orthogonal to the desired split) to 
100% (where the clusters are identical with the 
desired split). NMD is equivalent to maximal ma-
cro-averaged recall where the maximum is taken 
over the (two) possible assignments of books to 
clusters. In this case, we obtain an NMD of 51.5%, 
barely above the theoretical minimum. 
This negative result is not especially surprising 
since there are many ways for the chapters to split 
(e.g., according to thematic elements, sub-genre, 
etc.) and we can?t expect an unsupervised method 
to read our minds. Thus, to guide the method in the 
direction of stylistic elements that might distin-
guish between Jeremiah and Ezekiel, we define a 
class of generic biblical words consisting of all 223 
words that appear at least five times in each of ten 
different books of the Bible. 
Repeating our experiment of above, though li-
miting our feature set to generic biblical words, we 
obtain the following matrix: 
Book Cluster I Cluster II 
Jer 
Eze 
32 
28 
20 
20 
 
As can be seen, using generic words yields 
NMD of 51.3%, which does not improve matters at 
all. Thus, we need to try a different approach. 
4 Exploiting Synonym Usage 
One of the key features used by Bible scholars 
to classify different components of biblical litera-
ture is synonym choice. The underlying hypothesis 
is that different authorial components are likely to 
differ in the proportions with which alternative 
words from a set of synonyms (synset) are used. 
This hypothesis played a part in the pioneering 
work of Astruc (1753) on the book of Genesis ?
using a single synset: divine names ? and has been 
refined by many others using broader feature sets, 
such as that of Carpenter and Hartford-Battersby 
(1900). More recently, the synonym hypothesis has 
been used in computational work on authorship 
attribution of English texts in the work of Clark 
and Hannon (2007) and Koppel et al (2006). 
This approach presents several technical chal-
lenges. First, ideally ? in the absence of a suffi-
ciently comprehensive thesaurus ? we would wish 
to identify synonyms in an automated fashion. 
Second, we need to adapt our similarity measure 
for reasons that will be made clear below. 
4.1 (Almost) Automatic Synset Identification 
One of the advantages of using biblical litera-
ture is the availability of a great deal of manual 
annotation. In particular, we are able to identify 
synsets by exploiting the availability of the stan-
dard King James translation of the Bible into Eng-
1358
lish (KJV). Conveniently, and unlike most modern 
translations, KJV almost invariably translates syn-
onyms identically. Thus, we can generally identify 
synonyms by considering the translated version of 
the text. There are two points we need to be precise 
about. First, it is not actually words that we regard 
as synonymous, but rather word roots. Second, to 
be even more precise, it is not quite roots that are 
synonymous, but rather senses of roots. Conve-
niently, Strong?s (1890 [2010]) Concordance lists 
every occurrence of each sense of each root that 
appears in the Bible separately (where senses are 
distinguished in accordance with the KJV transla-
tion). Thus, we can exploit KJV and the concor-
dance to automatically identify synsets as well as 
occurrences of the respective synonyms in a syn-
set.1 (The above notwithstanding, there is still a 
need for a bit of manual intervention: due to poly-
semy in English, false synsets are occasionally 
created when two non-synonymous Hebrew words 
are translated into two senses of the same English 
word. Although this could probably be handled 
automatically, we found it more convenient to do a 
manual pass over the raw synsets and eliminate the 
problems.)  
The above procedure yields a set of 529 synsets 
including a total of 1595 individual synonyms. 
Most synsets consist of only two synonyms, but 
some include many more. For example, there are 7 
Hebrew synonyms corresponding to ?fear?. 
4.2 Adapting the Similarity Measure 
Let?s now represent a unit of text as a vector in 
the following way. Each entry represents a syn-
onym in one of the synsets. If none of the syn-
onyms in a synset appear in the unit, all their cor-
responding entries are 0. If j different synonyms in 
a synset appear in the unit, then each correspond-
ing entry is 1/j and the rest are 0. Thus, in the typi-
cal case where exactly one of the synonyms in a 
synset appears, its corresponding entry in the vec-
tor is 1 and the rest are 0. 
Now we wish to measure the similarity of two 
such vectors. The usual cosine measure doesn?t 
capture what we want for the following reason. If 
the two units use different members of a synset, 
cosine is diminished; if they use the same members 
of a synset, cosine is increased. So far, so good. 
But suppose one unit uses a particular synonym 
                                                           
1
 Thanks to Avi Shmidman for his assistance with this. 
and the other doesn?t use any member of that syn-
set. This should teach us nothing about the similar-
ity of the two units, since it reflects only on the 
relevance of the synset to the content of that unit; it 
says nothing about which synonym is chosen when 
the synset is relevant. Nevertheless, in this case, 
cosine would be diminished. 
The required adaptation is as follows: we first 
eliminate from the representation any synsets that 
do not appear in both units (where a synset is said 
to appear in a unit if any of its constituent syn-
onyms appear in the unit). We then compute cosine 
of the truncated vectors. Formally, for a unit x 
represented in terms of synonyms, our new similar-
ity measure is cos'(x,y) = cos(x|S(x ?y),y|S(x ?y)), 
where x|S(x ?y) is the projection of x onto the syn-
sets that appear in both x and y.  
4.3  Clustering Jeremiah-Ezekiel Using Syn-
onyms 
We now apply ncut clustering to the similarity 
matrix computed as described above. We obtain 
the following split: 
Book Cluster I Cluster II 
Jer 
Eze 
48 
5 
4 
43 
 
Clearly, this is quite a bit better than results ob-
tained using simple lexical features as described 
above. Intuition for why this works can be pur-
chased by considering concrete examples. There 
are two Hebrew synonyms ? p???h and miq??a? 
corresponding to the word ?corner?, two (min??h 
and t?r?m?h) corresponding to the word ?obla-
tion?, and two (n??a? and ???al) corresponding to 
the word ?planted?. We find that p???h, min??h 
and n??a? tend to be located in the same units and, 
concomitantly, miq??a?, t?r?m?h and ???al are lo-
cated in the same units. Conveniently, the former 
are all Jeremiah and the latter are all Ezekiel.  
While the above result is far better than those 
obtained using more na?ve feature sets, it is, never-
theless, far from perfect. We have, however, one 
more trick at our disposal that will improve these 
results further. 
5 Combining Partial Clustering and Su-
pervised Learning 
Analysis of the above clustering results leads to 
two observations. First, some of the units belong 
1359
firmly to one cluster or the other. The rest have to 
be assigned to one cluster or the other because 
that?s the nature of the clustering algorithm, but in 
fact are not part of what we might think of as the 
core of either cluster. Informally, we say that a unit 
is in the core of its cluster if it is sufficiently simi-
lar to the centroid of its cluster and it is sufficiently 
more similar to the centroid of its cluster than to 
any other centroid. Formally, let S be a set of syn-
sets, let B be a set of units, and let C be a cluster-
ing of B where the units in B are represented in 
terms of the synsets in S. For a unit x in cluster 
C(x) with centroid c(x), we say that x is in the core 
of C(x) if cos'(x,c(x))>?1 and cos'(x,c(x))-cos'(x,c)>?2 
for every centroid c?c(x). In our experiments be-
low, we use ?1=1/?2 (corresponding to an angle of 
less than 45 degrees between x and the centroid of 
its cluster) and ?2=0.1. 
Second, the clusters that we obtain are based on 
a subset of the full collection of synsets that does 
the heavy lifting. Formally, we say that a synonym 
n in synset s is over-represented in cluster C if 
p(x?C|n?x) > p(x?C|s?x) and p(x?C|n?x) > p(x?C). 
That is, n is over-represented in C if knowing that 
n appears in a unit increases the likelihood that the 
unit is in C, relative to knowing only that some 
member of n?s synset appears in the unit and rela-
tive to knowing nothing. We say that a synset s is a 
separating synset for a clustering {C1,C2} if some 
synonym in s is over-represented in C1 and a dif-
ferent synonym in s is over-represented in C2. 
5.1 Defining the Core of a Cluster 
We leverage these two observations to formally 
define the cores of the respective clusters using the 
following iterative algorithm. 
1. Initially, let S be the collection of all synsets, let 
B be the set of all units in the corpus 
represented in terms of S, and let {C1,C2} be 
an initial clustering of the units in B. 
2. Reduce B to the cores of C1 and C2. 
3. Reduce S to the separating synsets for {C1,C2}. 
4. Redefine C1 and C2 to be the clusters obtained 
from clustering the units in the reduced B 
represented in terms of the synsets in reduced S. 
5. Repeat Steps 2-4 until convergence (no further 
changes to the retained units and synsets). 
At the end of this process, we are left with two 
well-separated cluster cores and a set of separating 
synsets. When we compute cores of clusters in our 
Jeremiah-Ezekiel experiment, 26 of the initial 100 
units are eliminated. Of the 154 synsets that appear 
in the Jeremiah-Ezekiel corpus, 118 are separating 
synsets for the resulting clustering. The resulting 
cluster cores split with Jeremiah and Ezekiel as 
follows:  
Book Cluster I Cluster II 
Jer 
Eze 
36 
2 
0 
36 
 
We find that all but two of the misplaced units 
are not part of the core. Thus, we have a better 
clustering but it is only a partial one. 
5.2 Using Cores for Supervised Learning 
Now that we have what we believe are strong 
representatives of each cluster, we can use them in 
a supervised way to classify the remaining unclus-
tered units. The interesting question is which fea-
ture set we should use. Using synonyms would just 
get us back to where we began. Instead we use the 
set of generic Bible words introduced earlier. The 
point to recall is that while this feature set proved 
inadequate in an unsupervised setting, this does not 
mean that it is inadequate for separating Jeremiah 
and Ezekiel, given a few good training examples. 
Thus, we use a bag-of-words representation re-
stricted to generic Bible words for the 74 units in 
our cluster cores and label them according to the 
cluster to which they were assigned. We now apply 
SVM to learn a classifier for the two clusters. We 
assign each unit, including those in the training set, 
to the class assigned to it by the SVM classifier. 
The resulting split is as follows: 
Book Cluster I Cluster II 
Jer 
Eze 
51 
0 
1 
48 
 
Remarkably, even the two Ezekiel chapters that 
were in the Jeremiah cluster (and hence were es-
sentially misleading training examples) end up on 
the Ezekiel side of the SVM boundary.  
It should be noted that our two-stage approach 
to clustering is a generic method not specific to our 
particular application. The point is that there are 
some feature sets that are very well suited to a par-
ticular unsupervised problem but are sparse, so 
they give only a partial clustering. At the same 
time, there are other feature sets that are denser 
and, possibly for that reason, adequate for super-
1360
vised separation of the intended classes but inade-
quate for unsupervised separation of the intended 
classes. This suggests an obvious two-stage me-
thod for clustering, which we use here to good ad-
vantage. 
This method is somewhat reminiscent of semi-
supervised methods sometimes used in text catego-
rization where few training examples are available 
(Nigam et al 2000). However, those methods typi-
cally begin with some information, either in the 
form of a small number of labeled documents or in 
the form of keywords, while we are not supplied 
with these. Furthermore, the semi-supervised work 
bootstraps iteratively, at each stage using features 
drawn from within the same feature set, while we 
use exactly two stages, the second of which uses a 
different type of feature set than the first.  
For the reader?s convenience, we summarize the 
entire two-stage method: 
1. Represent units in terms of synonyms. 
2. Compute similarities of pairs of units using 
cos'. 
3. Use ncut to obtain an initial clustering. 
4. Use the iterative method to find cluster cores. 
5. Represent units in cluster cores in terms of ge-
neric words. 
6. Use units in cluster cores as training for learn-
ing an SVM classifier. 
7. Classify all units according to the learned SVM 
classifier. 
6 Empirical Results 
We now test our method on other pairs of bibli-
cal books to see if we obtain comparable results to 
those seen above. We need, therefore, to identify a 
set of biblical books such that (i) each book is suf-
ficiently long (say, at least 20 chapters), (ii) each is 
written by one primary author, and (iii) the authors 
are distinct. Since we wish to use these books as a 
gold standard, it is important that there be a broad 
consensus regarding the latter two, potentially con-
troversial, criteria. Our choice is thus limited to the 
following five books that belong to two biblical 
sub-genres: Isaiah, Jeremiah, Ezekiel (prophetic 
literature), Job and Proverbs (wisdom literature). 
(Due to controversies regarding authorship (Pope 
1952, 1965), we include only Chapters 1-33 of 
Isaiah and only Chapters 3-41 of Job.) 
Recall that our experiment is as follows: For 
each pair of books, we are given all the chapters in 
the union of the two books and are given no infor-
mation regarding labels. The object is to sort out 
the chapters belonging to the respective two books. 
(The fact that there are precisely two constituent 
books is given.) 
We will use the three algorithms seen above: 
1. generic biblical words representation and ncut 
clustering; 
2. synonym representation and ncut clustering; 
3. our two-stage algorithm. 
We display the results in two separate figures. 
In Figure 1, we see results for the six pairs of 
books that belong to different sub-genres. In Figure 
2, we see results for the four pairs of books that are 
in the same genre. (For completeness, we include 
Jeremiah-Ezekiel, although it served above as a 
development corpus.) All results are normalized 
majority diagonal. 
 
 
Figure 1. Results of three clustering methods for differ-
ent-genre pairs 
 
 
Figure 2. Results of three clustering methods for same-
genre pairs 
    
As is evident, for different-genre pairs, even the 
simplest method works quite well, though not as 
well as the two-stage method, which is perfect for 
five of six such pairs. The real advantage of the 
two-stage method is for same-genre pairs. For 
1361
these the simple method is quite erratic, while the 
two-stage method is near perfect. We note that the 
synonym method without the second stage is 
slightly worse than generic words for different-
genre pairs (probably because these pairs share 
relatively few synsets) but is much more consistent 
for same-genre pairs, giving results in the area of 
90% for each such pair. The second stage reduces 
the errors considerably over the synonym method 
for both same-genre and different-genre pairs. 
7  Decomposing Unsegmented Documents 
Up to now, we have considered the case where 
we are given text that has been pre-segmented into 
pure authorial units. This does not capture the kind 
of decomposition problems we face in real life. For 
example, in the Pentateuch problem, the text is 
divided up according to chapter, but there is no 
indication that the chapter breaks are correlated 
with crossovers between authorial units. Thus, we 
wish now to generalize our two-stage method to 
handle unsegmented text. 
7.1 Generating Composite Documents 
To make the problem precise, let?s consider 
how we might create the kind of document that we 
wish to decompose. For concreteness, let?s think 
about Jeremiah and Ezekiel. We create a composite 
document, called Jer-iel, as follows: 
1. Choose the first k1 available verses of Jeremiah, 
where k1 is a random integer drawn from the 
uniform distribution over the integers 1 to m. 
2. Choose the first k2 available verses of Ezekiel, 
where k2 is a new random integer drawn from 
the above distribution. 
3. Repeat until one of the books is exhausted; then 
choose the remaining verses of the other book. 
For the experiments discussed below, we use 
m=100 (though further experiments, omitted for 
lack of space, show that results shown are essen-
tially unchanged for any m?60). Furthermore, to 
simulate the Pentateuch problem, we break Jer-iel 
into initial units by beginning a new unit whenever 
we reach the first verse of one of the original chap-
ters of Jeremiah or Ezekiel. (This does not leak any 
information since there is no inherent connection 
between these verses and actual crossover points.) 
7.2 Applying the Two-Stage Method 
Our method works as follows. First, we refine 
the initial units (each of which might be a mix of 
verses from Jeremiah and Ezekiel) by splitting 
them into smaller units that we hope will be pure 
(wholly from Jeremiah or from Ezekiel). We say 
that a synset is doubly-represented in a unit if the 
unit includes two different synonyms of that syn-
set. Doubly-represented synsets are an indication 
that the unit might include verses from two differ-
ent books. Our object is thus to split the unit in a 
way that minimizes doubly-represented synonyms. 
Formally, let M(x) represent the number of synsets 
for which more than one synonym appear in x. Call 
?x1,x2? a split of x if x=x1x2. A split ?x1',x2'? is optim-
al if ?x1',x2'?= argmax M(x)-max(M(x1),M(x2)) where 
the maximum is taken over all splits of x. If for an 
initial unit, there is some split for which M(x)-
max(M(x1),M(x2)) is greater than 0, we split the unit 
optimally; if there is more than one optimal split, 
we choose the one closest to the middle verse of 
the unit. (In principle, we could apply this proce-
dure iteratively; in the experiments reported here, 
we split only the initial units but not split units.) 
Next, we run the first six steps of the two-stage 
method on the units of Jer-iel obtained from the 
splitting process, as described above, until the 
point where the SVM classifier has been learned. 
Now, instead of classifying chapters as in Step 7 of 
the algorithm, we classify individual verses. 
The problem with classifying individual verses 
is that verses are short and may contain few or no 
relevant features. In order to remedy this, and also 
to take advantage of the stickiness of classes across 
consecutive verses (if a given verse is from a cer-
tain book, there is a good chance that the next 
verse is from the same book), we use two smooth-
ing tactics. 
Initially, each verse is assigned a raw score by 
the SVM classifier, representing its signed distance 
from the SVM boundary. We smooth these scores 
by computing for each verse a refined score that is 
a weighted average of the verse?s raw score and 
the raw scores of the two verses preceding and 
succeeding it. (In our scheme, the verse itself is 
given 1.5 times as much weight as its immediate 
neighbors and three times as much weight as sec-
ondary neighbors.) 
Moreover, if the refined score is less than 1.0 
(the width of the SVM margin), we do not initially 
1362
assign the verse to either class. Rather, we check 
the class of the last assigned verse before it and the 
first assigned verse after it. If these are the same, 
the verse is assigned to that class (an operation we 
call ?filling the gaps?). If they are not, the verse 
remains unassigned.  
To illustrate on the case of Jer-iel, our original 
?munged? book has 96 units. After pre-splitting, 
we have 143 units. Of these, 105 are pure units. 
Our two cluster cores, include 33 and 39 units, re-
spectively; 27 of the former are pure Jeremiah and 
30 of the latter are pure Ezekiel; no pure units are 
in the ?wrong? cluster core. Applying the SVM 
classifier learned on the cluster cores to individual 
verses, 992 of the 2637 verses in Jer-iel lie outside 
the SVM margin and are assigned to some class. 
All but four of these are assigned correctly. Filling 
the gaps assigns a class to 1186 more verses, all 
but ten of them correctly. Of the remaining 459 
unassigned verses, most lie along transition points 
(where smoothing tends to flatten scores and where 
preceding and succeeding assigned verses tend to 
belong to opposite classes). 
7.3 Empirical Results 
We randomly generated composite books for 
each of the book pairs considered above. In Fig-
ures 3 and 4, we show for each book pair the per-
centage of all verses in the munged document that 
are ?correctly? classed (that is, in the majority di-
agonal), the percentage incorrectly classed (minori-
ty diagonal) and the percentage not assigned to 
either class. As is evident, in each case the vast 
majority of verses are correctly assigned and only a 
small fraction are incorrectly assigned. That is, we 
can tease apart the components almost perfectly.  
 
 
Figure 3. Percentage of verses in each munged differ-
ent-genre pair of books that are correctly and incorrectly 
assigned or remain unassigned. 
 
Figure 4. Percentage of verses in each munged same-
genre pair of books that are correctly and incorrectly 
assigned or remain unassigned. 
8 Conclusions and Future Work 
We have shown that documents can be decom-
posed into authorial components with very high 
accuracy by using a two-stage process. First, we 
establish a reliable partial clustering of units by 
using synonym choice and then we use these par-
tial clusters as training texts for supervised learn-
ing using generic words as features. 
We have considered only decompositions into 
two components, although our method generalizes 
trivially to more than two components, for example 
by applying it iteratively. The real challenge is to 
determine the correct number of components, 
where this information is not given. We leave this 
for future work. 
Despite this limitation, our success on munged 
biblical books suggests that our method can be 
fruitfully applied to the Pentateuch, since the broad 
consensus in the field is that the Pentateuch can be 
divided into two main authorial categories: Priestly 
(P) and non-Priestly (Driver 1909). (Both catego-
ries are often divided further, but these subdivi-
sions are more controversial.) We find that our 
split corresponds to the expert consensus regarding 
P and non-P for over 90% of the verses in the Pen-
tateuch for which such consensus exists. We have 
thus been able to largely recapitulate several centu-
ries of painstaking manual labor with our auto-
mated method. We offer those instances in which 
we disagree with the consensus for the considera-
tion of scholars in the field. 
In this work, we have exploited the availability 
of tools for identifying synonyms in biblical litera-
ture. In future work, we intend to extend our me-
thods to texts for which such tools are unavailable. 
1363
References  
J. Astruc. 1753. Conjectures sur les m?moires originaux 
dont il paroit que Moyse s?est servi pour composer le 
livre de la Gen?se. Brussels. 
R. E. Bee. 1971. Statistical methods in the study of the 
Masoretic text of the Old Testament. J. of the Royal 
Statistical Society, 134(1):611-622. 
M. J. Berryman, A. Allison, and D. Abbott. 2003. Sta-
tistical techniques for text classification based on 
word recurrence intervals. Fluctuation and Noise Let-
ters, 3(1):L1-L10. 
J. E. Carpenter, G. Hartford-Battersby. 1900. The Hex-
ateuch: According to the Revised Version. London. 
J. Clark and C. Hannon. 2007. A classifier system for 
author recognition using synonym-based features. 
Proc. Sixth Mexican International Conference on Ar-
tificial Intelligence, Lecture Notes in Artificial Intel-
ligence, vol. 4827, pp. 839-849. 
I. S. Dhillon, Y. Guan, and B. Kulis. 2004. Kernel k-
means: spectral clustering and normalized cuts. Proc. 
ACM International Conference on Knowledge Dis-
covery and Data Mining (KDD), pp. 551-556. 
S. R. Driver. 1909. An Introduction to the Literature of 
the Old Testament (8th ed.). Clark, Edinburgh. 
N. Graham, G. Hirst, and B. Marthi. 2005. Segmenting 
documents by stylistic character. Natural Language 
Engineering, 11(4):397-415. 
D. Guthrie, L. Guthrie, and Y. Wilks. 2008. An unsu-
pervised probabilistic approach for the detection of 
outliers in corpora. Proc. Sixth International Lan-
guage Resources and Evaluation (LREC'08), pp. 28-
30. 
D. Holmes. 1994. Authorship attribution, Computers 
and the Humanities, 28(2):87-106.  
P. Juola. 2008. Author Attribution. Series title: 
Foundations and Trends in Information Retriev-
al. Now Publishing, Delft. 
M. Koppel, N. Akiva, and I. Dagan. 2006. Feature in-
stability as a criterion for selecting potential style 
markers. J. of the American Society for Information 
Science and Technology, 57(11):1519-1525. 
M. Koppel, J.  Schler, and S. Argamon. 2009.  Compu-
tational methods in authorship attribution. J. of the 
American Society for Information Science and Tech-
nology, 60(1):9-26. 
D. L. Mealand. 1995. Correspondence analysis of Luke. 
Lit. Linguist Computing, 10(3):171-182. 
S. Meyer zu Eisen and B. Stein. 2006. Intrinsic plagiar-
ism detection. Proc. European Conference on Infor-
mation Retrieval (ECIR 2006), Lecture Notes in 
Computer Science, vol. 3936, pp. 565?569. 
K. Nigam, A. K. McCallum, S. Thrun, and T. M. Mit-
chell. 2000. Text classification from labeled and un-
labeled documents using EM, Machine Learning, 
39(2/3):103-134. 
M. H. Pope. 1965. Job (The Anchor Bible, Vol. XV). 
Doubleday, New York, NY. 
M. H. Pope. 1952. Isaiah 34 in relation to Isaiah 35, 40-
66. Journal of Biblical Literature, 71(4):235-243. 
Y. Radday. 1970. Isaiah and the computer: A prelimi-
nary report, Computers and the Humanities, 5(2):65-
73. 
E. Stamatatos. 2009. A survey of modern authorship 
attribution methods. J. of the American Society for 
Information Science and Technology, 60(3):538-556. 
J. Strong. 1890. The Exhaustive Concordance of the 
Bible. Nashville, TN. (Online edition: 
http://www.htmlbible.com/sacrednamebiblecom/kjvs
trongs/STRINDEX.htm; accessed 14 November 
2010.) 
 
 
 
 
 
 
 
1364
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 112?117,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
 
Language Classification and Segmentation of                                   Noisy Documents in Hebrew Scripts 
  Nachum Dershowitz Alex Zhicharevich School of Computer Science School of Computer Science Tel Aviv University Tel Aviv University Ramat Aviv, Israel Ramat Aviv, Israel nachumd@tau.ac.il alex.zhicharevich@gmail.com     Abstract 
Language classification is a preliminary step for most natural-language related processes. The significant quantity of multilingual doc-uments poses a problem for traditional lan-guage-classification schemes and requires segmentation of the document to monolingual sections. This phenomenon is characteristic of classical and medieval Jewish literature, which frequently mixes Hebrew, Aramaic, Judeo-Arabic and other Hebrew-script lan-guages. We propose a method for classifica-tion and segmentation of multi-lingual texts in the Hebrew character set, using bigram sta-tistics. For texts, such as the manuscripts found in the Cairo Genizah, we are also forced to deal with a significant level of noise in OCR-processed text. 1.     Introduction  The identification of the language in which a given test is written is a basic problem in natural-language processing and one of the more studied ones. For some tasks, such as automatic cata-loguing, it may be used stand-alone, but, more often than not, it is just a preprocessing step for some other language-related task. In some cases, even English and French, the identification of the language is trivial, due to non-identical character sets. But this is not always the case. When look-ing at Jewish religious documents, we often find a mixture of several languages, all with the same Hebrew character set. Besides Hebrew, these include Aramaic, which was once the lingua franca in the Middle East, and Judeo-Arabic, which was used by Jews living all over the Arab world in medieval times.     Language classification has well-established methods with high success rates. In particular, character n-grams, which we dub n-chars, work 
well. However, when we looked at recently digitized documents from the Cairo Genizah, we found that a large fraction contains segments in different languages, so a single language class is rather useless. Instead, we need to identify mono-lingual segments and classify them. Moreover, all that is available is the output of mediocre OCR of handwritten manuscripts that are them-selves of poor quality and often seriously de-graded. This raises the additional challenge of dealing with significant noise in the text to be segmented and classified.    We describe a method for segmenting docu-ments into monolingual sections using statistical analysis of the distribution of n-grams for each language. In particular, we use cosine distance between character unigram and bigram distribu-tions to classify each section and perform smoothing operations to increase accuracy.    The algorithms were tested on artificially produced multilingual documents. We also artificially introduced noise to simulate mistakes made in OCR. These test documents are similar in length and language shifts to real Genizah texts, so similar results are expected for actual manuscripts. 2      Related Work Language classification is well-studied, and is usually approached by character-distribution methods (Hakkinen and Tian, 2001) or diction-ary-based ones. Due to the lack of appropriate dictionaries for the languages in question and their complex morphology, the dictionary-based approach is not feasible. The poor quality of the results of OCR also precludes using word lists.    Most work on text segmentation is in the area of topic segmentation, which involves semantic features of the text. The problem is a simple case of structured prediction (Bakir, 2007). Text tiling (Hearst, 1993) uses a sliding-window approach. 
112
 
Similarities between adjacent blocks within the text are computed using vocabularies, counting new words introduced in each segment. These are smoothed and used to identify topic bounda-ries via a cutoff function. This method is not suitable for language segmentation, since each topic is assumed to appear once, while languages in documents tend to switch repeatedly. Choi (2000) uses clustering methods for boundary identification.  3.    Language Classification Obviously, different languages, even when sharing the same character set, have different distribution of character occurrences. Therefore, gathering statistics on the typical distribution of letters may enable us to uncover the language of a manuscript by comparing its distribution to the known ones. A simple distribution of letters may not suffice, so it is common to employ n-chars (Hakkinen and Tian, 2001).     Classification entails the following steps: (1) Collect n-char statistics for relevant languages. (2) Determine n-char distribution for the input manuscript. (3) Compute the distance between the manuscript and each language using some distance measure. (4) Classify the manuscript as being in the language with the minimal distance.    The characters we work with all belong to the Hebrew alphabet, including its final variants (at the end of words). The only punctuation we take into account is inter-word space, because differ-ent languages can have different average word lengths (shorter words mean more frequent spaces), and different languages tend to have different letters at the beginnings and ends of words. For instance, a human might look for a prevalence of words ending in alef to determine that the language is Aramaic. After testing, bigrams were found to be significantly superior to unigrams and usually superior to trigrams, so bigrams were used throughout the classification process. Moreover, in the segmentation phase, we deal with very short texts on which trigram probabilities will be too sparse.     We represent the distribution function as a vector of probabilities. The language with small-est cosine distance between vectors is chosen, as this measure works well in practice. 4.    Language Segmentation  For the splitting task, we use only n-char statis-tics, not presuming the availability of useful wordlists. We want the algorithm to work even if 
the languages shift frequently, so we do not assume anything about the minimal or maximal length of segments. We do not, of course, con-sider a few words in another language to consti-tute a language shift. The algorithm comprises four major steps: (1) Split text into arbitrary segments. (2) Calculate characteristics of each segment. (3) Classify each. (4) Refine classifica-tions and output final results. 4.1     Splitting the Text  Documents are not always punctuated into sen-tences or paragraphs. So, splitting is done in the na?ve way of breaking the text into fixed-size segments. As language does not shift mid-word (except for certain prefixes), we break the text between words. If sentences are delineated and one ignores possible transitions mid-sentence, then the breaks should be between sentences. The selection of segment size should depend on the language shift frequency. Nonetheless, each segment is classified using statistical prop-erties, so it has to be long enough to have some statistical significance. But if it is too long, the language transitions will be less accurate, and if a segment contains two shifts, it will miss the inner one. Because the post-processing phase is com-putationally more expensive, and grows propor-tionally with segment length, we opt for relative-ly short initial segments.  4.2   Feature Extraction The core of the algorithm is the initial classifica-tion of segments. Textual classification is usually reduced to vector classification, so there each segment is represented as a vector of features. Naturally, the selection of features is critical for successful classification, regardless of classifica-tion algorithm. Several other features were tried such as hierarchical clustering of segments and classification of the clusters (Choi, 2000) but did not yield significant improvement.  N-char distance ? The first and most obvious feature is the classification of the segment using the methods described in Section 3.  However, the segments are significantly smaller than the usual documents, so we expect lower accuracy than usual for language classification. The fea-tures are the cosine distance from each language model. This is rather natural, since we want to preserve the distances from each language model in order to combine it with other features later on. For each segment f and language l, we com-
113
 
pute ????????? = ???? ?, ? , the cosine distance of their bigram distributions. Neighboring segments language ? We expect that languages in a document do not shift too frequently, since paragraphs tend to be monolin-gual and at least several sentences in a row will be in the same language to convey some idea. Therefore, if we are sure about one segment, there is a high chance that the next segment will be in the same language. One way to express such dependency is by post-processing the re-sults to reduce noise. Another way is by combin-ing the classification results of neighboring segments as features in the classification of the segment. Of course, not only neighboring seg-ments can be considered, but all segments within some distance can help. Some parameter should be estimated to be the threshold for the distance between segments under which they will be considered neighbors. We denote by (negative or positive) Neighbor(f,i) the i-th segment be-fore/after f. If i=0, Neighbor(f,i) = f. For each segment f and language l, we compute ??????,? ? = ???? ?, ???????? ?, ? . Whole document language - Another feature is the cosine distance of the whole document from each language model. This tends to smooth and reduce noise from classification, especially when the proportion of languages is uneven. For a monolingual document, the algorithm is ex-pected to output the whole document as one correctly-classified segment.  4.3    Post-processing We refine the segmentation procedure as fol-lows: We look at the results of the splitting procedure and recognize all language shifts. For each shift, we try to find the place where the shift takes place (at word granularity). We unify the two segments and then try to split the segment at N different points. For every point, we look at the cosine distance of the text before the point from the class of the first segment, and at the distance of the text after the point to the language of the second segment. For example, suppose a segment A1?An was classified as Hebrew and segment B1?Bm, which appeared immediately after was classified Aramaic. We try to split A1?An,B1?Bm at any of N points, N=2, say. First, we try F1=A1?A(n+m)/3 and F2=A(n+m)/3+1?Bm (supposing (n+m)/3<n). We look at cosine distance of F1 to Hebrew and of F2 to Aramaic. Then, we look at F1 = A1?A2(n+m)/3 and F2 = A2(n+m)/3+1?Bm. We choose the split 
with the best product of the two cosine distances. The value of N is a tradeoff between accuracy and efficiency. When N is larger, we check more transition points, but for large segments it can be computationally expensive.  4.4    Noise Reduction OCR-processed documents display a significant error rate and classification precision should be expected to drop. Relying only on n-char statis-tics, we propose probabilistic approaches to the reduction of noise. Several methods (Kukich, 1992) have been proposed for error correction using n-chars, using letter-transition probabili-ties.  Here, we are not interested in error correc-tion, but rather in adjusting the segmentation to handle noisy texts.  To account for noise, we introduce a $ sign, meaning ?unknown? character, imagining a conservative OCR system that only outputs characters with high probability. There is also no guarantee that word boundaries will not be missed, so $ can occur instead of a space.  Ignoring unrecognized n-chars ? We simply ignore n-chars containing $ in the similarity measures. We assume there are enough bigrams left in each segment to successfully identify its language. Error correction ? Given an unknown char-acter, we could try correcting it using trigrams, looking for the most common trigram of the form a$b.  This seems reasonable and enhances the statistical power of the n-char distribution, but does not scale well for high noise levels, since there is no solution for consecutive unknowns. Averaging n-char probabilities ? When en-countering the $, we can use averaging to esti-mate the probability of the n-char containing it. For instance, the probability of the bigram $x will be the average probability of all bigrams starting with x in a certain language. This can of course scale to longer n-chars and integrates the noisy into the computation. Top n-chars ? When looking at noisy text, we can place more weight on corpus statistics, since they are error free. Therefore, we can look only at the N most common n-chars in the corpus for edit distance computing. Higher n-char space ? So far we used bi-grams, which showed superior performance. But when the error rate rises, trigrams may show a higher success rate. 
114
 
5.   Experimental Results We want to test the algorithm with well-defined parameters and evaluation factors. So, we created artificially mixed documents, containing seg-ments from pairs of different languages (He-brew/Aramaic, which is hard, Hebrew/Judeo-Arabic, where classification is easy and segmen-tation is the main challenge, or a mix of all three). The segments are produced using two parameters: The desired document length d and the average monolingual segment length k. Obviously, ? < ?. We iteratively take a random number in the range [k?20:k+20] and take a substring of that length from a corpus, rounded to whole words. We cycle through the languages until the text is of size d. The smaller k, the harder to segment. 5.2   Evaluation Measures Obviously splitting will not be perfect and we cannot expect to precisely split a document. Given that, we want to establish some measures for the quality of the splitting result. We would like the measure to produce some kind of score to the algorithm output, using which we can indicate whether a certain feature or parameter in the algorithm improves it or not. However, the result quality is not well defined since it is not clear what is more important: detecting the segment's boundaries accurately, classifying each segment correctly or even split the document to the exact number of segments.  For example, given a long document in Hebrew with a small segment in Aramaic, is it better to return that it actually is a long document in Hebrew with Aramaic segment but misidentify the segment's location or rather recognize the Aramaic segment perfectly but classify it as Judeo-Arabic. There are several measures for evaluating text segmen-tation (Lamprier et al, 2007). Correct word percentage ? The most intui-tive measure is simply measuring the percentage of the words classified correctly. Since the ?atomic? block of the text is words (or sentences in some cases described further), which are certainly monolingual, this measure will resem-ble the algorithm accuracy pretty good for most cases. It is however not enough, since in some cases it does not reflect the quality of the split-ting. Assume a long Hebrew document with several short sentences in Aramaic. If the He-brew is 95% of the text, a result that classifies the whole text as Hebrew will get 95% but is pretty 
useless and we may prefer a result that identifies the Aramaic segments but errs on more words. Segmentation error (SE) estimates the algo-rithm?s sensitivity to language shifts. It is the difference between the correct number and that returned by the algorithm, divided by correct number. Obviously, SE is in the range [?1:1]. It will indeed resemble the problem previously described, since, if the entire document is classi-fied as Hebrew, the SE score will be very low, as the actual number is much greater than 1.  5.3 Experiments  Neighboring segments ? The first thing we tested is the way a segment?s classification is affected by neighboring segments. We begin by checking if adding the distance of the closest segments enhances performance. Define  ???????,? = ???? ?, ? +  ?? ??????,? 1 +  ? ???????,? ?1 . For the test we set a=0.4.    From Figures 1 and 2, one can see that neigh-boring segments improve classification of short segments, while on shorter ones classification without the neighbors was superior. It is not surprising that when using neighbors the splitting procedure tends to split the text to longer seg-ments, which has good effect only if segments actually are longer. We can also see from Figure 3 that the SE measure is now positive with k=100, which means the algorithm underesti-mates the number of segments even when each segment is 100 characters long. By further exper-iments, we can see that the a parameter is insig-nificant, and fix it at 0.3. As expected, looking at neighboring segments can often improve results. The next question is if farther neighbors also do. Let: ??????,? = ???? ?, ? + ? ?? ??????,? ?1 +  ? ???????,? ?????? . Parameter N stands for the longest distance of neighbors to consider in the score. Parameter a is set to 0.3.    We see that increasing N does not have a significant impact on algorithm performance, and on shorter segment lengths performance drops with N. We conclude that there is no advantage at looking at distant neighbors. Post-processing ? Another thing we test is the post-processing of the splitting results to refine the initial segment choice. We try to move the transition point from the original position to a more accurate position using the technique described above. We note is cannot affect the SE measure, since we only move the transition points without changing the classification. As 
115
 
shown in Figure 4, it does improve the perfor-mance for all values of l.     Noise reduction ? To test noise reduction, we artificially added noise, randomly replacing some letters with $. Let P denote the desired noise rate and replace each letter independently with $ with probability P. Since the replacements of charac-ter is mutually independent, we can expect a normal distribution of error positions, and the correction phase described above does not as-sume anything about the error creation process. Error creation does not assign different probabili-ties for different characters in the text unlike natural OCR systems or other noisy processing.     Not surprisingly, Figure 5 illustrates that the accuracy reduces as the error rate rises. However, it does not significantly drop even for a very high error rate, and obviously we cannot expect that the error reducing process will perform better then the algorithm performs on errorless text. Figure 6 illustrates the performance of each method. It looks like looking at most common n-chars does not help, nor trying to correct the unrecognized character. Ignoring the unrecog-nized character, using either bigrams or trigrams, or estimating the missing unrecognized bigram probability show the best and pretty similar results.  6.    Conclusion We have described methods for classifying texts, all using the same character set, into several languages. Furthermore, we considered segment-ed multilingual texts into monolingual compo-nents. In both cases, we made allowance for corrupted texts, such as that obtained by OCR from handwritten manuscripts. The results are encouraging and will be used in the Friedberg Genizah digitization project (www.genizah.org).  
 Figure 1:  Correct word percentage considering neighbors and not, as a function of segment length k (document length was 1500). 
 
Figure 2: Segmentation error considering neighbors or not (k =1500). 
Figure 3:  Correct word percentage for various resplit-ting values N as a function of k. 
 Figure 4: Correct word percentage with and without post-processing. 
Figure 5: Word accuracy as a function of noise. 
 Figure 6: The performance of suggested correction methods for each error rate.  
0.5	 ?
0.7	 ?
0.9	 ?
50	 ?100	 ?150	 ?200	 ?250	 ?
N=1	 ?
N=2	 ?
N=3	 ?
N=4	 ?
0	 ?
0.5	 ?
1	 ?
50	 ?100	 ?150	 ?200	 ?250	 ?
No	 ?
Neighbours	 ?
With	 ?
Neighbours	 ?
0	 ?
0.5	 ?
1	 ?
50	 ?100	 ?150	 ?200	 ?250	 ?
No	 ?
Neighbours	 ?
With	 ?
Neighbours	 ?
0.7	 ?
0.75	 ?
0.8	 ?
0.85	 ?
0.9	 ?
0	 ? 0.1	 ? 0.2	 ? 0.3	 ? 0.4	 ? 0.5	 ?
trigrams	 ? Ignoring	 ?
error	 ?correc?on	 ? avearge	 ?bigram	 ?
0.7	 ?
0.8	 ?
0.9	 ?
1	 ?
0.1	 ? 0.2	 ? 0.3	 ? 0.4	 ? 0.5	 ?
100	 ?
150	 ?
200	 ?
250	 ?
0.5	 ?
0.7	 ?
0.9	 ?
50	 ?100	 ?150	 ?200	 ?250	 ?
Without	 ?Post	 ?
Processing	 ?
with	 ?
PostProceesing	 ?
Without Post-processing 
With Post-processing 
116
 
Acknowledgement We thank the Friedberg Genizah Project for supplying data to work with and for their support. References  G?khan Bak?r. 2007. Predicting Structured Data. MIT Press, Cambridge, MA. Freddy Y. Y. Choi. 2000. Advances in domain inde-pendent linear text segmentation. Proc. 1st North American Chapter of the Association for Compu-tational Linguistics Conference, pp. 26-33. Juha H?kkinen and Jilei Tian. 2001. N-gram and decision tree based language identification for written words. Proc. Automatic Speech Recogni-tion and Understanding (ASRU '01), Italy, pp. 335-338. Marti A. Hearst. 1993. TextTiling: A quantitative approach to discourse segmentation. Technical Report, Sequoia 93/24, Computer Science Divi-sion. Sylvain Lamprier, Tassadit Amghar, Bernard Levrat, and Fr?d?ric Saubion. 2007. On evaluation meth-odologies for text segmentation algorithms. Proc. 9th IEEE International Conference on Tools with Artificial Intelligence (ICTAI), volume 2, pp. 19-26. Karen Kukich. 1992. Techniques for automatically correcting words in text. ACM Computing Surveys 24(4): 377-439. 
117
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 139?143,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The Tel Aviv University System
for the Code-Switching Workshop Shared Task
Kfir Bar
School of Computer Science
Tel Aviv University
Ramat Aviv, Israel
kfirbar@post.tau.ac.i
Nachum Dershowitz
School of Computer Science
Tel Aviv University
Ramat Aviv, Israel
nachumd@tau.ac.il
Abstract
We describe our entry in the EMNLP 2014
code-switching shared task. Our system
is based on a sequential classifier, trained
on the shared training set using various
character- and word-level features, some
calculated using a large monolingual cor-
pora. We participated in the Twitter-genre
Spanish-English track, obtaining an accu-
racy of 0.868 when measured on the tweet
level and 0.858 on the word level.
1 Introduction
Code switching is the act of changing language
while speaking or writing, as often done by bilin-
guals (Winford, 2003). Identifying the transition
points is a necessary first step before applying
other linguistic algorithms, which usually target a
single language. A switching point may occur be-
tween sentences, phrases, words, or even between
certain morphological components. Code switch-
ing happens frequently in informal ways of com-
munication, such as verbal conversations, blogs
and microblogs; however, there are many exam-
ples in which languages are switched in formal
settings. For example, alternating between Collo-
quial Egyptian Arabic and Modern Standard Ara-
bic in modern Egyptian prose is prevalent (Rosen-
baum, 2000).
This shared task (Solorio et al., 2014),
1
the first
of its kind, challenges participants with identify-
ing those switching points in blogs as well as in
microblog posts. Given posts with a mix of a
specific pair of languages, each participating sys-
tem is required to identify the language of ev-
ery word. Four language-pair tracks were offered
by the task organizers: Spanish-English, Nepali-
English, Modern Standard Arabic and Colloquial
1
http://emnlp2014.org/workshops/
CodeSwitch/call.html
Arabic, and Mandarin-English. For each language
pair, a training set of Twitter
2
statuses was pro-
vided, which was manually annotated with a label
for every word, indicating its language. In addi-
tion to the two language labels, a few additional
labels were used. Altogether there were six labels:
(1) lang1?the first language; (2) lang2?the sec-
ond language; (3) ne?named entity; (4) ambigu-
ous?for ambiguous words belonging to both lan-
guages; (5) mixed?for words composed of mor-
phemes in each language; and (6) other?for cases
where it is impossible to determine the language.
For most of the language pairs, the organizers sup-
plied three different evaluation sets. The first set
was composed of a set of unseen Twitter statuses,
provided with no manual annotation. The other
two sets contained data from a ?surprise genre?,
mainly composed of blog posts.
We took part only in the Spanish-English track.
Both English and Spanish are written in Latin
script. The Spanish alphabet contains some addi-
tional letters, such as those indicating stress (vow-
els with acute accents: ?a, ?e, ??, ?o, ?u), a u adorned
with a diaeresis (?u), the additional letter ?n (e?ne),
and inverted question and exclamation punctua-
tion marks ? and ? (used at the beginning of ques-
tions and exclamatory phrases, respectively). Al-
though social-media users are not generally con-
sistent in their use of accents, their appearance
in a word may disclose its language. By and
large, algorithms for code switching have used
the character-based k-mer feature, introduced by
(Cavnar and Trenkle, 1994).
3
Our system is an implementation of a multi-
class classifier that works on the word level, con-
sidering features that we calculate using large
Spanish as well as English monolingual corpora.
Working with a sequential classifier, the predicted
2
http://www.twitter.com
3
We propose the term ?k-mer? for character k-grams, in
contradistinction to word n-grams.
139
labels of the previous words are used as features in
predicting the current word.
In Section 2, we describe our system and the
features we use for classification. Section 3 con-
tains the evaluation results, as published by the or-
ganizers of this shared task. We conclude with a
brief discussion.
2 System Description
We use a supervised framework to train a classifier
that predicts the label of every word in the order
written. The words were originally tokenized by
the organizers, preserving punctuation, emoticons,
user mentions (e.g., @emnlp2014), and hashtags
(e.g., #emnlp2014) as individual tokens. The in-
formal language, as used in social media, intro-
duces an additional challenge in predicting the lan-
guage of every word. Spelling mistakes as well
as grammatical errors are very common. Hence,
we believe that predicting the language of a given
word merely using dictionaries for the two lan-
guages is likely to be insufficient.
Our classifier is trained on a learning set, as pro-
vided by the organizers, enriched with some addi-
tional features. Every word in the order written is
treated as a single instance for the classifier, each
including features from a limited window of pre-
ceding and successive words, enriched with the
predicted label of some of the preceding words.
We ran a few experiments with different window
sizes, based on 10-fold cross validation, and found
that the best token-level accuracy is obtained us-
ing a window of size 2 for all features, that is, two
words before the focus word and two words after.
The features that we use may be grouped in
three main categories, as described next.
2.1 Features
We use three main groups of features:
Word level: The specific word in focus, as well
as the two previous words and the two following
ones are considered as features. To reduce the
sparsity, we convert words into lowercase. In ad-
dition, we use a monolingual lexicon for English
words that are typically used in Twitter. For this
purpose, we employ a sample of the Twitter Gen-
eral English lexicon, released by Illocution, Inc.,
4
containing the top 10K words and bigrams from
a relatively large corpus of public English tweets
4
http://www.illocutioninc.com
they collected over a period of time, along with
frequency information. We bin the frequency rates
into 5 integer values (with an additional value for
words that do not exist in the lexicon), which are
used as the feature value for every word in focus,
and for the other four words in its window. This
feature seems to be quite noisy, as some common
Spanish words appear in the lexicon (e.g., de, no,
a, me); on the other hand, it may capture typi-
cal English misspellings and acronyms (e.g., oomf,
noww, lmao). We could not find a similar resource
for Spanish, unfortunately.
To help identify named entities, we created a list
of English as well Spanish names of various en-
tity types (e.g., locations, family and given names)
and used it to generate an additional boolean fea-
ture, indicating whether the word in focus is an en-
tity name. The list was compiled out of all words
beginning with a capital letter in relatively large
monolingual corpora, one for English and another
for Spanish. To avoid words that were capitalized
because they occur at the beginning of a sentence,
regardless of whether they are proper names, we
first processed the text with a true-casing tool, pro-
vided as part of Moses (Koehn et al., 2007)?
the open source implementation for phrase-based
statistical machine translation. Our list contains
about 146K entries.
Intra-word level: Spanish, as opposed to En-
glish, is a morphologically rich language, demon-
strating a complicated suffix-based derivational
morphology. Therefore, in order to capture re-
peating suffixes and prefixes that may character-
ize the languages, we consider as features sub-
strings of 1?3 prefix and suffix characters of the
word in focus and the other four words in its win-
dow. Although it is presumed that capitalization
is not used consistently in social media, we con-
sider a boolean feature indicating whether the first
letter of each word in the window was capitalized
in the original text or not. At this level, we use
two additional features that capture the level of un-
certainty of seeing the sequence of characters that
form the specific word in each language. This is
done by employing a 3-mer character-based lan-
guage model, trained over a large corpus in each
language. Then, the two language models, one for
each language, are applied on the word in focus
to calculate two log-probability values. These are
binned into ten discrete values that are used as the
features? values. We add a boolean feature, indi-
140
cating which of the two models returned a lower
log probability.
Inter-word level: We capture the level of un-
certainty of seeing specific sequences of words in
each language. We used 3-gram word-level lan-
guage models, trained over large corpora in each
of the languages. We apply the models to the fo-
cus word, considering it to be the last in a sequence
of three words (with the two previous words) and
calculate log probabilities. Like before, we bin the
values into ten discrete values, which are then used
as the features? values. An additional boolean fea-
ture is used, indicating which of the two models
returned a lower log probability.
2.2 Supervised Framework
We designed a sequential classifier running on top
of the Weka platform (Frank et al., 2010) that is
capable of processing instances sequentially, sim-
ilar to YamCha (Kudo and Matsumoto, 2003).
We use LibSVM (Chang and Lin, 2011), an im-
plementation of Support Vector Machines (SVM)
(Cortes and Vapnik, 1995), as the underlying tech-
nology, with a degree 2 polynomial kernel. Since
we work on a multi-class classification problem,
we take the one-versus-one approach. As men-
tioned above, we use features from a window of
?2 words before and after the word of interest. In
addition, for every word, we consider as features
the predicted labels of the two prior words.
3 Evaluation Results
We report on the results obtained on the unseen
task evaluation sets, which were provided by the
workshop organizers.
5
There are three evaluation
sets. The first is composed of a set of unseen Twit-
ter statuses and the other two contain data from a
?surprise genre?. The results are available online
at the time of writing only for the first and second
sets. The results of the third set will be published
during the upcoming workshop meeting.
The training set contains 11,400 statuses, com-
prising 140,706 words. Table 1 shows the distri-
bution of labels.
The first evaluation set contains 3,060 tweets.
However, we were asked to download the statuses
directly from Twitter, and some of the statuses
were missing. Therefore, we ended up with only
1,661 available statuses, corresponding to 17,723
5
http://emnlp2014.org/workshops/
CodeSwitch/results.php
Label Number
lang1 77,101
lang2 33,099
ne 2,918
ambiguous 344
mixed 51
other 27,194
Table 1: Label distribution in the training set.
Accuracy 0.868
Recall 0.720
Precision 0.803
F1-Score 0.759
Table 2: Results for the first evaluation set, mea-
sured on tweet level.
words. According to the organizers, the evaluation
was performed only on the 1,626 tweets that were
available for all the participating groups. Out of
the 1,626, there are 1,155 monolingual tweets and
471 code-switched tweets. Table 2 shows the eval-
uation results for the Tel Aviv University (TAU)
system on the first set, reported on the tweet level.
In addition, the organizers provide evaluation
results, calculated on the word level. Table 3
shows the label distribution among the words in
the first evaluation set, and Table 4 shows the ac-
tual results. The overall accuracy on the word level
is 0.858.
The second evaluation set contains 1,103 words
of a ?surprise? (unseen) genre, mainly blog posts.
Out of the 49 posts, 27 are monolingual and 22 are
code-switched posts. Table 5 shows the results for
the surprise set, calculated on the post level.
As for the first set, Table 6 shows the distribu-
tion of the labels among the words in the surprise
set, and in Table 7 we present the results as mea-
sured on the word level. The overall accuracy on
the surprise set is 0.941.
4 Discussion
We believe that we have demonstrated the po-
tential of using sequential classification for code-
switching, enriched with three types of features,
some calculated using large monolingual corpora.
Compared to the other participating systems as
published by the workshop organizers, our system
obtained encouraging results. In particular, we ob-
serve relatively good results in relating words to
141
Label Count
lang1 (English) 7,040
lang2 (Spanish) 5,549
ne 464
mixed 12
ambiguous 43
other 4,311
Table 3: Label distribution in the first evaluation
set.
Label Recall Precision F1-Score
lang1 (English) 0.900 0.830 0.864
lang2 (Spanish) 0.869 0.914 0.891
ne 0.313 0.541 0.396
mixed 0.000 1.000 0.000
ambiguous 0.023 0.200 0.042
other 0.845 0.860 0.853
Table 4: Results for the first evaluation set, mea-
sured on word level.
their language; however, identifying named enti-
ties did not work as well. We plan to further in-
vestigate this issue. The results on the surprise
genre are similar to that for the genre the system
was trained on. However, since the surprise set
is relatively small in size, we refrain from draw-
ing conclusions about this. Trying the same code-
switching techniques on other pairs of languages
is part of our planned future research.
References
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of
the 3rd Annual Symposium on Document Analysis
and Information Retrieval (SDAIR-94), pages 161?
175.
Chih C. Chang and Chih J. Lin. 2011. LIBSVM:
A Library for Support Vector Machines. ACM
Transactions on Intelligent Systems and Technology,
2(27):1?27, May.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20:273?297.
Eibe Frank, Mark Hall, Geoffrey Holmes, Richard
Kirkby, Bernhard Pfahringer, Ian H. Witten, and Len
Trigg. 2010. Weka?A machine learning work-
bench for data mining. In Oded Maimon and Lior
Rokach, editors, Data Mining and Knowledge Dis-
covery Handbook, chapter 66, pages 1269?1277.
Springer US, Boston, MA.
Accuracy 0.864
Recall 0.708
Precision 0.803
F1-Score 0.753
Table 5: Results for the second, ?surprise? evalua-
tion set, measured on the post level.
Label Count
lang1 (English) 636
lang2 (Spanish) 306
ne 38
mixed 1
ambiguous 1
other 120
Table 6: Label distribution in the ?surprise? eval-
uation set.
Label Recall Precision F1-Score
lang1 (English) 0.883 0.824 0.853
lang2 (Spanish) 0.864 0.887 0.876
ne 0.293 0.537 0.379
mixed 0.000 1.000 0.000
ambiguous 0.022 0.200 0.039
other 0.824 0.843 0.833
Table 7: Results for the ?surprise? evaluation set,
measured on the word level.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Interactive Poster and Demon-
stration Sessions of the 45th Annual Meeting of the
ACL (ACL ?07), pages 177?180, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods
for kernel-based text analysis. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 24?31, Sapporo,
Japan.
Gabriel M. Rosenbaum. 2000. Fushammiyya: Alter-
nating style in Egyptian prose. Journal of Arabic
Linguistics (ZAL), 38:68?87.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
142
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar.
Donald Winford, 2003. Code Switching: Linguistic
Aspects, chapter 5, pages 126?167. Blackwell Pub-
lishing, Malden, MA.
143
