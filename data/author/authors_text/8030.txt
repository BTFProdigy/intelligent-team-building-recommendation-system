Word Selection for EBMT based on Monolingual Similarity and Translation
Confidence
Eiji Aramaki, Sadao Kurohashi, Hideki Kashioka and Hideki Tanaka
 Graduate School of Information Science and Tech. University of Tokyo
Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
aramaki, kuro@kc.t.u-tokyo.ac.jp
 ATR Spoken Language Translation Research Laboratories
2-2 Hikaridai, Seika, Soraku, Kyoto 619-0288, Japan
hideki.kashioka, hideki.tanaka@atr.co.jp
Abstract
We propose a method of constructing an
example-based machine translation (EBMT)
system that exploits a content-aligned bilingual
corpus. First, the sentences and phrases in the
corpus are aligned across the two languages,
and the pairs with high translation confidence
are selected and stored in the translation mem-
ory. Then, for a given input sentences, the
system searches for fitting examples based on
both the monolingual similarity and the transla-
tion confidence of the pair, and the obtained re-
sults are then combined to generate the transla-
tion. Our experiments on translation selection
showed the accuracy of 85% demonstrating the
basic feasibility of our approach.
1 Introduction
The basic idea of example-based machine translation, or
EBMT, is that translation examples similar to a part of
an input sentence are retrieved and combined to produce
a translation(Nagao, 1984). In order to make a practi-
cal MT system based on this approach, a large number
of translation examples with structural correspondences
are required. This naturally presupposes high-accuracy
parsers and well-aligned large bilingual corpora.
Over the last decade, the accuracy of the parsers im-
proved significantly. The availability of well-aligned
bilingual corpora, however, has not increased despite our
expectations. In reality, the number of bilingual cor-
pora that share the same content, such as newspapers and
broadcast news, has increased steadily. We call this type
of corpus a content-aligned corpus. With these observa-
tions, we started a research project that covered all as-
pects of constructing EBMT systems starting from using
Figure 1: Translation Example (TE).
a content-aligned corpus, i.e., a bilingual broadcast news
corpus.
First, the sentences and phrases in the corpus are
aligned across the two languages, and the pairs with high
translation confidence are selected and stored in the trans-
lation memory. Then, translation examples are retrieved
based on both the monolingual similarity and the trans-
lation confidence of the pair. Finally, these examples are
combined to generate the translation.
This paper is organized as follows. The next sec-
tion presents how to build the translation memory from
a content-aligned corpus. Section 3 describes our EBMT
system, paying special attention to the selection of trans-
lation examples. Section 4 reports experimental results
of word selection, Section 5 describes related works, and
Section 6 gives our conclusions.
* Underlined phrases and sentences have no parallel expressions in the other language.
Figure 2: NHK News Corpus.
2 Building Translation Memory
In EBMT, an input sentence can hardly be translated by
a single translation example, except when an input is ex-
tremely short or is a typical domain-dependent sentence.
Therefore, two or more translation examples are used to
translate parts of the input and are then combined to gen-
erate a whole translation. Syntactic information is useful
for composing example fragments.
In this paper, we call a structurally aligned bilingual
sentence pair a translation example or TE (Figure 1).
This section presents our method for building TEs from a
content-aligned corpus.
Since the bilingual corpus used in our project does not
contain literal translations, automatic parsing and align-
ment inevitably contain errors. Therefore, we selected
highly likely TEs to make a translation memory.
2.1 NHK News Corpus
We used a bilingual news corpus compiled by the NHK
broadcasting service (NHK News Corpus), which con-
sists of about 40,000 Japanese-English article pairs cov-
ering a five-year period. The average number of Japanese
sentences in an article is 5.2, and that of English sentence
is 7.4. Table 2 shows an example of an article pair.
As shown in Table 2, an English article is not a literal
translation of a Japanese article, although their contents
are almost parallel.
2.2 Sentence Alignment
We used a DP matching for bilingual sentence alignment,
where we allow the matching of 1-to-1, 1-to-2, 1-to-3, 2-
to-1 and 2-to-2 Japanese and English sentence pairs. This
matching covered 84% of the following evaluation set.
We selected 96 article pairs for the evaluation of sentence
and phrase alignment, and we call this the evaluation set.
We use the following score for matching, which is based
on a ratio of corresponding content words (WCR: content
Word Corresponding Ratio).
WCR  




 (1)
where 

is the number of Japanese content words in a
unit, 

is the number of English content words, and 

is the number of content words whose translation is also
in the unit, which is found by translation dictionaries?
We used the EDR electronic dictionary, EDICT,
ENAMDICT, the ANCHOR translation dictionary, and
Figure 3: Handling of Remaining Phrases.
Figure 4: WCR and Precision.
the EIJIRO translation dictionary. These dictionaries
have about two million entries in total.
On the evaluation data, the precision of the sentence
alignment (defined as follows) was 60.7%.
precision  # of correct system outputs
# of system outputs
(2)
Among types of a corresponding unit, the precision of
1-to-1 correspondence was the best, at 77.5%. Since a 1-
to-1 correspondence is suitable for the following phrase
alignment, we decided to use only the 1-to-1 correspon-
dence results.
2.3 Phrase Alignment
The 1-to-1 sentence pairs obtained in the previous sec-
tion are then aligned at phrase level by the method based
on (Aramaki et al, 2001). The method consists of the
following pre-process and two aligning steps.
Pre-process: Conversion to phrasal dependency struc-
tures.
First, the phrasal dependency structures of the sen-
tence pair are estimated. The English parser returns
a word-based phrase structure, which is merged into
a phrase sequence by the following rules and con-
verted into a dependency structure by lifting up head
phrases.
Table 1: Number of TEs.
Corpus WCR # of TEs
0.3?0.4 18290
NHK News 0.4?0.5 6975
0.5? 2314
White Paper ? 2225
SENSEVAL ? 6920
1. Function words are grouped with the following
content word.
2. Adjoining nouns are grouped into one phrase.
3. Auxiliary verbs are grouped with the following
verb.
The Japanese parser outputs the phrasal dependency
structure of an input, and that is used as is. We used
The Japanese parser KNP (Kurohashi and Nagao,
1994) and The English nl-parser (Charniak, 2000).
Step 1: Estimation of basic phrasal correspondences.
We started with the word-level alignment to get the
basic phrasal alignment. We used translation dictio-
naries for this process. The word sense ambiguity
in the dictionaries is resolved with a heuristics that
the most plausible correspondence is near other cor-
respondences.
Step 2: Expansion of phrasal correspondences.
Finally, the remaining phrases, which were not han-
dled in the step 1, are merged into a neighboring
phrase correspondence or are used to establish a new
correspondence, depending on the surrounding ex-
isting correspondences. Figure 3 shows an example
of a new correspondence established by a structural
pattern.
These procedures can detect the phrasal alignments in
a pair of sentences as shown in Figure 1.
For phrase alignment evaluation, we selected all of the
145 sentence pairs that had 1-to-1 correspondences form
the evaluation set and gave correct content word corre-
spondences to these pairs. The phrase correspondences
detected by the system were judged correct when the cor-
respondences include the manually given content word
correspondences.
Based on this criterion, the precision of phrase align-
ment was 50%. Then, we found a correlation between
the phrase alignment precision and WCR of parallel sen-
tences as shown in Figure 4. Furthermore, the precision
of sentence alignment and WCR also have a correlation.
Since their performances nearly reaches their limits when
WCR is 0.3, we decided to use parallel sentences whose
WCR is 0.3 or greater as TEs.
Figure 5: Example of Translation.
2.4 Building Translation Memory
As explained in the preceding sections, among sentence-
aligned and phrase-aligned NHK News articles, TEs with
a 1-to-1 sentence correspondence and whose WCR is 0.3
or greater are registered in the translation memory. Table
1 shows the number of TEs for each WCR range.
In addition, the Bilingual White Paper and Translation
Memory of SENSEVAL2 (Kurohashi, 2001) were also
phrase-aligned and registered in the translation memory.
Sentence alignments are already given for these corpora.
Since their parallelism are fairly high and the accuracies
of their phrase alignments are more than 70%, we utilized
all phrase-aligned sentence pairs as TEs (Table 1).
3 EBMT System
Our EBMT system translates a Japanese sentence into
English. A Japanese input sentence is parsed and trans-
formed into a phrase-based dependency structure. Then,
for each phrase, an appropriate TE is retrieved from the
translation memory that is most suitable for translating
Figure 6: Selection of a TE.
the phrase (and its neighboring phrases). Finally, the En-
glish expressions of the TEs are combined to produce the
final English translation (Figure 5).
This section describes our EBMT system, mainly the
TE selection part.
3.1 Basic Idea of TE Selection
The basic idea of TE selection is shown in Figure 6.
When a part of the input sentence and a part of the TE
source language sentence have an equal expression, the
part of the input sentence is called I and the part of the
TE source language sentence is called S. A part of the TE
target language corresponding to S is called T. The pair S
and T is called fragment of TE (FTE).
I, S and T have to meet the following conditions, as a
natural consequence of the fact that S-T is used for trans-
lating I.
1. I, S and T are each structurally connected phrases.
2. I is equal to S except for function words at the
boundaries.
3. S corresponds to T completely, that is, all phrases in
S and T are aligned.
It might be the case that for an I, two or more FTEs that
meet the above conditions exist in the translation mem-
ory. Our method takes into account the following rela-
tions among I-S-T to select the best FTE:
1. The largest pair of I and S.
2. The similarity between the surroundings of I and
these of S.
3. The confidence of alignment between S and T.
The following sections concretely present how to cal-
culate these criteria. For simplicity of explanation, we
call a set of phrasal correspondences between S and T,
EQ; that neighboring EQ, CONTEXT; that between S and
T, ALIGN (Figure 6).
3.2 Monolingual Similarity between Japanese
Expressions
The equality between I and S is a sum of the equality
score of each phrase correspondence in EQ, which is cal-
culated as follows:
EQUAL 



 





	
 

	
 (3)
where

is the number of content words in the phrase
correspondence, 
	
is the number of function words,


is the equality between content words, and 
	
is the equality between function words. 

and 
	
are given in Table 2.1
Usually, the equality score between I and S is equal to
the number of phrases in I (the number of phrase corre-
spondences in EQ), but sometimes these are slightly dif-
ferent, depending on the conjugation type and function
words.
1All constant values in Table 2 and formulas were decided
based on preliminary experiments.
On the other hand, the similarity between the surround-
ings of I and those of S is a sum of the similarity score of
each phrase correspondence in CONTEXT, which is cal-
culated as follows:
SIM 




 






 






(4)
Basically the calculation of SIM and EQUAL is the
same, except that SIM considers the relation type between
the phrase in I and its outer phrase by 

. When
the relation is the same, the influence of the surrounding
phrases must be large, so 

is set to 1.0; when the
relation is not the same, 

is set to 0.5. The rela-
tions between phases are estimated by the function word
or conjugation type of the dependent phrase.
The monolingual similarity between Japanese expres-
sions I and S is calculated as follows:



EQUAL 



SIM (5)
3.3 Translation Confidence of Japanese-to-English
Alignment
The translation confidence of phrase alignment between
S and T is the sum of the confidence score of each phrase
correspondence in ALIGN, CONF() in Table 2, and it is
weighted by the WCR of the parallel sentences.
As a final measure, the score of I-S-T is calculated as
follows:




EQUAL 



SIM






CONF

WCR (6)
3.4 Search Algorithm of FTE
For each phrase (P) in an input sentence, the most plausi-
ble FTE is retrieved by the following algorithm:
1. FTEs are retrieved from the translation memory, in
which a Japanese phrase matches P, and it is aligned
to an English phrase. (that is, these are FTEs that
meet the basic conditions for translation in Section
3.1).
2. For each FTE obtained in the previous step, it is
checked whether the surrounding phrase of P and
that of FTE are the same or similar, phrase by
phrase, and the largest I-S-T that meets the basic
conditions is detected.
Table 2: Parameters for Similarity and Confidence Calculation.
1.1 exact match
1.0 stem match


0.5  

+ 0.3 thesaurus match
0.3 POS match
0 otherwise
* 

is a similarity calculated based on NTT thesaurus(Ikehara et al, 1997) (max = 1).
1.1 exact match


1.0 stem match
0 otherwise
1.0 all content words in alignment  correspond to each other in dic
CONF() 0.8 some content words in alignment  correspond to each other in dic
0.5 otherwise
3. The score of each I-S-T is calculated, and the best
I-S-T (S-T is the FTE) is selected as the FTE for P.
As a result of detecting FTEs for phrases in the input,
two FTEs starting from the different phrase might over-
lap each other. In such a case, we employed a greedy
search algorithm that adopts the higher score FTE one
by one; therefore, each previously adopted FTE is only
partly used for translation.
On the other hand, when no FTE is obtained for an in-
put phrase, a translation dictionary is utilized (when the
phrase contains two or more content words, the longest
matching strategy is used for dictionary look-up). When
two or more possible translations are given from the dic-
tionary, the most frequent phrase/word in the NHK News
Corpus is adopted.
Figure 5 shows examples of FTEs detected by our
method.2
3.5 Generating a Target Sentence
The English expressions in the selected FTEs are com-
bined, and the English dependency structure is con-
structed. The dependency relations in FTEs are pre-
served, and the relation between the two FTEs is esti-
mated based on the relation of the input sentences. Figure
5 shows an example of a combined English dependency
structure.
When a surface expression is generated from its depen-
dency structure, its word order must be selected properly.
This can be done by preserving the word order in FTEs
and by ordering FTEs by a set of rules governing both the
dependency relation and the word-order.
The module for controlling conjugation, determiner,
and singular/plural is not yet implemented in our current
MT system.
2As the bottom example in Figure 5 shows, EBMT can eas-
ily handle head-switching translation by using an FTE that con-
tains all of the head-switching phenomena in it.
4 Experiments
For evaluation, we selected 50 sentence pairs from the
NHK News Corpus that were not used for the translation
memory. Their source (Japanese) sentences were trans-
lated by our EBMT system, and the selected FTEs were
evaluated by hand, referring to the target (English) sen-
tences.
A phrase by phrase evaluation was done to judge
whether the English expression of the selected FTE was
good or bad. The accuracy was 85.0%.
In order to investigate the effectiveness of each com-
ponent of FTE selection, we compared the following four
methods:
1. EQCONTEXTALIGN: The proposed method.
2. EQALIGN: FTE score is calculated as follows, with-
out the CONTEXT similarity:



EQUAL()



CONF()WCR (7)
3. EQCONTEXT: FTE score is calculated as follows,
without the ALIGN confidence:



EQUAL() 



SIM() (8)
4. DICONLY: Word selection is based only on dictio-
naries and frequency in the corpus.
The accuracy of each method is shown in Table 3,
and the results indicate that the proposed method, EQ-
CONTEXTALIGN, is the best, that is, using context sim-
ilarity and align confidence works effectively. Figure 7
Figure 7: Word Selection by EQCONTEXTALIGN and DICONLY.
Table 3: Experimental Results.
Good Bad Accuracy
EQCONTEXTALIGN 268 47 85.0%
(246) (35) (87.5%)
EQALIGN 254 61 80.6 %
(233) (48) (82.9%)
EQCONTEXT 234 80 74.2%
(213) (68) (75.8%)
DICONLY 232 83 73.6%
* Values in brackets indicate the accuracy only for FTEs,
excluding cases in which the dictionary was used as a
backup.
shows examples of EQCONTEXTALIGN and DICONLY.
EQCONTEXTALIGN usually selects appropriate words,
compared to DICONLY.
When there are no plausible translation examples in the
translationmemory, the system selects a low-similarity or
low-confidence FTE. However we believe this problem
will be resolved as the number of translation examples
increases, since the News Corpus is increasing day by
day.
5 Related Work
The idea of example based machine translation systems
was first proposed by (Nagao, 1984), and preliminary
systems that appeared about ten years (Sato and Na-
gao, 1990; Sadler and Vendelmans, 1990; Maruyama and
Watanabe, 1992; Furuse and Iida, 1994) showed the basic
feasibility of the idea.
Recent studies have focused on the practical aspects
of EBMT, and this technology has even been applied
to some restricted domains. The work in (Richardson
et al, 2001; Menezes and Richardson, 2001) addressed
the problem of technical manual translation in several
languages, and the work of (Imamura, 2002) dealt with
dialogues translation in the travel arrangement domain.
These works select the translation example pairs based
solely on the source language similarity. We believe this
is partly due to the high parallelism found in their cor-
pora.
Our work targets a more general corpus of wider cover-
age, i.e., the broadcast news collection. Generally avail-
able corpora like the one we use tend to be more freely
translated and suffer from lower parallelism. This com-
pelled us to use the criterion of translation confidence,
together with the criterion of monolingual similarity used
in the previous works. As we showed in this paper, this
metric succeeded in meeting our expectations.
6 Conclusion
In this paper, we described operations of the entire EBMT
process while using a content-aligned corpus, i.e., the
NHK Broadcast Corpus. In this process, one of the key
problems is how to select plausible translation examples.
We proposed a new method to select translation exam-
ples based on source language similarity and translation
confidence. In the word selection task, the performance
is highly accurate.
Acknowledgements
This work was supported in part by the 21st Century COE
program ?Information Science and Technology Strate-
gic Core? at University of Tokyo and by a contract with
the Telecommunications Advancement Organization of
Japan, entitled ?A study of speech dialogue translation
technology based on a large corpus?.
References
Eiji Aramaki, Sadao Kurohashi, Satoshi Sato, and Hideo
Watanabe. 2001. Finding translation correspondences
from parallel parsed corpus for example-based transla-
tion. In Proceedings of MT Summit VIII, pages 27?32.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In In Proceedings of NAACL 2000, pages 132?
139.
Osamu Furuse and Hitoshi Iida. 1994. Constituent
boundary parsing for example-based machine transla-
tion. In Proceedings of the 15th COLING, pages 105?
111.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentarou Ogura, and Yoshi-
fumi Oyama Yoshihiko Hayashi, editors. 1997.
Japanese Lexicon. Iwanami Publishing.
Kenji Imamura. 2002. Application of translation knowl-
edgeacquired by hierarchical phrase alignment for
pattern-based mt. In Proceedings of TMI-2002, pages
74?84.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4).
Sadao Kurohashi. 2001. Senseval2 Japanese translation
task. In Proceedings of SENSEVAL2, pages 37?40.
Hiroshi Maruyama and Hideo Watanabe. 1992. The
cover search algorithm for example-based translation.
In Proceedings of TMI-1992, pages 173?184.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the ACL 2001 Workshop on Data-Driven Meth-
ods in Machine Translation, pages 39?46.
Makoto Nagao. 1984. A framework of a mechanical
translation between Japanese and english by analogy
principle. In In Artificial and Human Intelligence,
pages 173?180.
Stephen D. Richardson, William B. Dolan, Arul
Menezes, and Monica Corston-Oliver. 2001. Over-
coming the customization bottleneck using example-
based mt. In Proceedings of the ACL 2001 Work-
shop onData-DrivenMethods in Machine Translation,
pages 9?16.
V. Sadler and R. Vendelmans. 1990. Pilot implementa-
tion of a bilingual knowledge bank. In Proeedings of
the 13th COLING, pages 449?451.
Satoshi Sato andMakoto Nagao. 1990. Toward memory-
based translation. InProceedings of the 13th COLING,
pages 247?252.
Construction and Analysis of Japanese-English Broadcast News Corpus
with Named Entity Tags
Tadashi Kumano, Hideki Kashioka and Hideki Tanaka
ATR Spoken Language Translation Research Laboratories
2?2?2, Hikaridai, Keihanna Science City, Kyoto 619?0288, Japan
{tadashi.kumano, hideki.kashioka, hideki.tanaka}@atr.co.jp
Takahiro Fukusima
Otemon Gakuin University
1?15, Nishiai 2-chome, Ibaraki, Osaka 567?8502, Japan
fukusima@res.otemon.ac.jp
Abstract
We are aiming to acquire named entity
(NE) translation knowledge from non-
parallel, content-aligned corpora, by uti-
lizing NE extraction techniques. For this
research, we are constructing a Japanese-
English broadcast news corpus with NE
tags. The tags represent not only NE
class information but also coreference in-
formation within the same monolingual
document and between corresponding
Japanese-English document pairs. Anal-
ysis of about 1,100 annotated article pairs
has shown that if NE occurrence informa-
tion, such as classes, number of occur-
rence and occurrence order, is given for
each language, it may provide a good clue
for corresponding NEs across languages.
1 Introduction
Studies on named entity (NE) extraction are mak-
ing progress for various languages, such as En-
glish and Japanese. A number of evaluation work-
shops have been held, including the Message Under-
standing Conference (MUC)1 for English and other
languages, and the Information Retrieval and Ex-
traction Exercise (IREX)2 for Japanese. Extraction
accuracy for English has reached a nearly practi-
cal level (Marsh and Perzanowski, 1998). As for
Japanese, it is more difficult to find NE bound-
1http://www.itl.nist.gov/iaui/894.02/
related_projects/muc/
2http://nlp.cs.nyu.edu/irex/
aries, however, NE extraction is relatively accurate
(Sekine and Isahara, 2000).
Most of the past research on NE extraction used
monolingual corpora, but the application of NE ex-
traction techniques to bilingual (or multilingual) cor-
pora is expected to obtain NE translation pairs. We
are developing a Japanese-English machine trans-
lation system for documents including many NEs,
such as news articles or documents about current
topics. Translating NE correctly is indispensable for
conveying information correctly. NE translations,
however, are not listed in conventional dictionaries.
It is necessary to retrieve NE translation knowledge
from the latest bilingual documents.
When extracting translation knowledge from
bilingual corpora, using literally translated parallel
corpora, such as official documents written in sev-
eral languages makes it easier to get the desired in-
formation. However, not many of such corpora con-
tain the latest NEs. There are few Japanese-English
corpora which are translated literally. Therefore,
we decided to extract NE translation pairs from
content-aligned corpora, such as multilingual broad-
cast news articles including new NEs daily, which
are not literally translated.
Sentential alignment (Brown et al, 1991; Gale
and Church, 1993; Kay and Ro?scheisen, 1993; Ut-
suro et al, 1994; Haruno and Yamazaki, 1996) is
commonly used as a starting point for finding the
translations of words or expressions from bilingual
corpora. However, it is not always possible to cor-
respond non-parallel corpora in sentences. Past sta-
tistical methods for non-parallel corpora (Fung and
Yee, 1998) are not valid for finding translations of
words or expressions with low frequency. These
methods have a problem in covering NEs because
there are many NEs that appear only once in a cor-
pus. So we need a specialized method for extract-
ing NE translation pairs. Transliteration is used for
finding the translations of NE in the source language
from texts in the target language (Stalls and Knight,
1998; Goto et al, 2001; Al-Onazian and Knight,
2002). Transliteration is useful for the names of per-
sons and places; however, it is not applicable to all
sorts of NEs.
Content-aligned documents, such as a bilingual
news corpus, are made to convey the same top-
ics. Since NEs are the essential element of docu-
ment contents, content-aligned documents are likely
to share NEs pointing to the same objects. Con-
sequently, when extracting all NEs with NE class
information from each of a pair of bilingual docu-
ments separately by applying monolingual NE ex-
traction techniques, the distribution of the NEs in
each document may be similar enough to recognize
correspondences between the NE translation pairs.
A technique for finding bilingual NE correspon-
dences will have a wide range of applications other
than NE translation-pair extraction. For example,
? Bilingual NE correspondences have clues for
identifying corresponding parts in a pair of
noisy bilingual documents.
? The similarity of any two documents in dif-
ferent languages can be estimated by NE
translation-pair correspondence.
For this research, we obtained a Japanese-English
broadcast news corpus (Kumano et al, 2002) by
the Japanese broadcast company NHK3, and we are
manually tagging NEs in the corpus to analyze it
and to conduct NE translation-pair extraction exper-
iments.
The tag specifications are based on the IREX
NE task (Sekine and Isahara, 1999), the evaluation
workshop of Japanese NE extraction. We extended
the specifications to English NEs. In addition, coref-
erence information between NEs, within the same
monolingual document and between the correspond-
ing Japanese-English document pairs (henceforth,
3Nippon Hoso Kyokai (Japan Broadcasting Corporation)
(http://www.nhk.or.jp/englishtop/)
we call these in a language and across languages,
respectively), is added to each of the tagged NEs,
for NE translation-pair extraction studies.
In Section 2, we will introduce the bilingual cor-
pus used in this study and describe its characteris-
tics. Then, we will discuss tag design for NE extrac-
tion studies, and explain the tag specifications and
existing problems. The current status of corpus an-
notation under these specifications will also be in-
troduced. We analyzed an annotated part of the cor-
pus in terms of NE occurrence and translation. This
analysis will be shown in Section 3. In Section 4,
we will mention future plans for the extraction of
NE translation-pairs.
2 Constructing a Japanese-English
broadcast news corpus with NE tags
2.1 Characteristics of the NHK
Japanese-English broadcast news corpus
We are annotating an NHK broadcast news corpus
with NE tags. The corpus is composed of Japanese
news articles for domestic programs and English
news articles translated for international broadcast-
ing4 and domestic bilingual programs5.
Figure 1 shows an example of a Japanese news
article and its translation in English. The original
Japanese article and the translated English article
deal with the same topic, but they differ much in de-
tails. The difference arises from the following rea-
sons (Kumano et al, 2002).
Audience Content might be added or deleted, ac-
cording to the audience, especially for interna-
tional broadcasting.
Broadcasting date The broadcasting of English
news is often delayed compared to the origi-
nal Japanese news. The time expressions might
be changed sometimes or new facts might be
added to the articles.
News styles / languages Comparing news articles
of two languages reveals that they have differ-
ent presentation styles, for example, facts are
sometimes introduced in a different order. The
4NHK WORLD (http://www.nhk.or.jp/
nhkworld/)
5http://www.nhk.or.jp/englishtop/
program_list/
Original article in Japanese (and its literal translation in English by authors):
1: ????????????????????????????????????
????????????
(There was a strong earthquake at 6:42 this morning in Izu Islands, the site of recent
numerous earthquakes. An earthquake of a little less than five in seismic intensity was
observed at Shikine Island.)
2: ????????????????????????????????????
????????????????????
(In addition, an event of seismic intensity four was observed for Niijima and Kozu Is-
land, events seismic intensity three for Toshima Island and Miyake Island, and events of
seismic intensity two and one for various parts of Kanto Area and Shizuoka Prefecture.)
3: ???????????????????
(There is no risk of tsunamis resulting from this earthquake.)
4: ?????????????????????????????????????
????????????????????????????
(According to observations by the Meteorological Agency, the earthquake epicenter
was located in the sea at a depth of ten kilometers near Niijima and Kozu Island. The
magnitude of the earthquakes was estimated to be five point one.)
5: ????????????????????????????????????
????????????????????????????????????
??????????????????
(In Izu Islands, where seismic activity has been observed from the end of June, repeated
cycles of seismic activity and dormancy have been observed. On the 30th of the previ-
ous month, a single strong earthquake having seismic intensity of a little less than six
was observed at Miyake Island, while two earthquakes having seismic intensity of five
were also observed there.)
6: ????????????????????????????????????
????????????????????????????????????
????
(In a series of seismic events, seventeen earthquakes having seismic intensity over five
have been observed up to this point, including strong tremors with a seismic intensity
of a little less than six observed four times at Kozu Island, Niijima, and Miyake Island.)
Translated article in English:
1: A strong earthquake jolted
Shikine Island, one of the Izu
islands south of Tokyo, early
on Thursday morning.
2: The Meteorological Agency
says the quake measured five-
minus on the Japanese scale of
seven.
3: The quake affected other is-
lands nearby.
4: Seismic activity began in the
area in late July, and 17 quakes
of similar or stronger intensity
have occurred.
5: Officials are warning of more
similar or stronger earthquakes
around Niijima and Kozu Is-
lands.
6: Tokyo police say there have
been no reports of damage
from the latest quake.
Figure 1: An article pair in an NHK broadcast news corpus
difference is due to language and socio-cultural
backgrounds.
2.2 NE tag design
We designed NE tags for NE translation-pair extrac-
tion research and working efficiency for manual an-
notation. The specifications are shown below.
? It is desirable that NE recognition guidelines
be consistent with NE tags of existing corpora.
Past guidelines of MUC and IREX should be
respected because they were configured as a re-
sult of many discussions. Consistent guidelines
enable us to utilize existing annotated corpora
and systems designated for the corpora.
? Within each bilingual document pair, corefer-
ence between NEs in a language and across lan-
guages will be specified. When several NEs
exist for the same referent in a document, it
is not always possible to determine the actual
translation for each instance of the NEs from
the counterpart document, because our corpus
is not composed of literal translations. There-
fore, coreference between NEs in a language
should be marked so that the coreference across
languages can be assigned between NE groups
that have the same referent. Coreference be-
tween NE groups is sufficient for our purpose.
? Assignment of coreference in a language is lim-
ited between NEs only. Although NEs may
have the same referent with pronouns or non-
NE expressions, these elements are ignored to
avoid complicating the annotation work.
2.3 Tag specifications
1. The tag specifications conform to IREX NE
tag specifications (IREX Committee, 1999) (an
English description in (Sekine and Isahara,
1999)) as regards the markup form, NE classes,
and NE recognition guidelines.
Japanese:
????????<LOCATION ID=?1? COR=?2?>
(Izu Islands)
????</LOCATION>
?<DATE ID=?2? COR=?4?>
(today)
???</DATE><TIME ID=?3? COR=?5?>
(a.m.)
??
(6:42)
??????</TIME>????????<LOCATION ID=?4? COR=?1?>
(Shikine Island)
???</LOCATION>????????????? ? ? ?
English:
A strong earthquake jolted <LOCATION ID=?1? COR=?4?>
Shikine Island</LOCATION>, one of the <LOCATION ID=?2?
COR=?1?>Izu islands</LOCATION> south of <LOCATION ID=?3?>
Tokyo</LOCATION>, early on <DATE ID=?4? COR=?2?>Thursday
</DATE> <TIME ID=?5? COR=?3?>morning</TIME>. ? ? ?
Figure 2: An annotation example
NE Class Example
Named entities (in the narrow sense):
ORGANIZATION The Diet; IREX Committee
PERSON (Mr.) Obuchi; Wakanohana
LOCATION Japan; Tokyo; Mt. Fuji
ARTIFACT Pentium Processor; Nobel Prize
Temporal expressions:
DATE September 2, 1999; Yesterday
TIME 11 PM; midnight
Number expressions:
MONEY 100 yen; $12,345
PERCENT 10%; a half
Table 1: NE Classes
Eight NE classes were defined at the IREX NE
task ? the same 7 classes as MUC-7 (3 types
of named entities in the narrow sense, 2 types
of temporal expressions, and 2 types of number
expressions), and ARTIFACT (concrete objects
like commercial products and abstract objects
such as laws or intellectual properties). Table 1
shows a list of these.
2. IREX?s NE classes and NE recognition guide-
lines are applied to English for consistency be-
tween Japanese and English NEs. For English-
specific annotation, such as prepositions or de-
terminers in NE, the MUC-7 Named Entity
Task Definition (Chinchor, 1997) is consulted6.
3. The SGML markup form of the IREX tag is
extended by adding the following two tag at-
tributes, which represent coreference informa-
tion in a language, and across languages.
ID=?NE group ID? (mandatory)
Each NE is assigned an attribute ID and
an ID number as its value. All corefer-
ent NEs in each language document are
6The tag specifications of IREX NE and those of MUC-7 do
not differ radically, because IREX NE tags are designed based
on the discussions of MUC.
given the same ID number7. The same
ID number is assigned to NEs that have
different forms, such as the full name and
the first name or the official name and the
abbreviated form, in addition to NEs with
the same form. Basically, NE are assigned
the same ID number when they belong to
an NE class and have the identical surface
form8.
COR=?ID for corresponding NE groups in
the other language? (optional)
When there exists a corresponding NE
(group) belonging to the same NE class
in the other language, an attribute COR
is given to each NE (group) in both lan-
guages, and the ID number for the coun-
terpart is assigned as a value to each other.
Annotations by the specifications are illustrated
in Figure 2.
2.4 Current status of the corpus annotation
Annotators who have experience in translation work
and in the production of linguistic data are engaging
in the tag annotation. Plans call for a total of 2,000
article pairs to be annotated, and about 1,100 pairs
have been finished up to the present.
2.5 Problems
Some problems became obvious in the course of
discussions of tag specifications and tag annotation
work. They confuse annotators and make the result
inaccurate. Typical cases are shown below.
2.5.1 The granularity difference between
Japanese and English
In Japanese, a unit smaller than a morpheme may
be accepted as an NE according to IREX guidelines.
7ID numbers do not maintain uniqueness across the docu-
ments.
8There are some exceptions. See Section 2.5.3.
(last Sunday and this Sunday)
sensyuu-no nichiyou -to konsyuu-no nichiyou
J: ???<DATE ID=?1?>??</DATE>????<DATE ID=?2?>??</DATE>
E: <DATE COR=?1?>last Sunday</DATE> and <DATE COR=?2?>this Sunday</DATE>
Figure 3: Assignment of different group IDs with NEs having the same surface form
On the other hand, English does not accept any unit
smaller than a word by MUC-7 guidelines. Some
Japanese NEs cannot have a counterpart English
NE, even if they have a corresponding English ex-
pression because of the difference in the segmenta-
tion granularity. For example, ????? (amerika;
America)? in the Japanese morpheme ??????
(amerika-jin; America-people)? is treated as an NE,
while no NE can be tagged to ?American?, the En-
glish counterpart of ??????.?
2.5.2 Translation problems
NEs have the same problem that translation in
general has: What is the exact translation word(s)
for an expression?
? Semantically corresponding expressions may
not be assigned corresponding NE relations,
because they belong to different NE classes or
an expression in a language is not recognized
as an NE. For example, a non-NE word ???
(seifu; government)? which means Japanese
government in Japanese articles is often trans-
lated as the English NE: ?Japan.?
? A non-literal translation of an NE may cause
difficulty in recognizing corresponding rela-
tions. Correspondences for some expressions
cannot be decided with the information repre-
sented in documents: Relative temporal expres-
sions in Japanese are often translated as ab-
solute expressions in English and those corre-
spondences cannot be identified without con-
sulting the calendar; Money expressions are
generally converted to dollars and the ex-
change rate at the relative time is needed to
confirm correspondences. For example, we
found a translation pair of money expressions
????? (sanzen-oku-en; three hundred bil-
lion yen)? and ?three billion U-S dollars? in our
corpus, which constitutes a rough conversion
from yen into dollars when the articles were
produced.
2.5.3 Assigning NE group IDs
We defined NEs that have the identical surface
form and the same NE class to be coreferent and
assigned the same NE group ID, in order to make
coreference judgment easier. There are some cases
where we cannot apply this rule, especially to tem-
poral expressions or number expressions.
The example in Figure 3 shows the translation
pair ???????????? (last Sunday and this
Sunday)? and ?last Sunday and this Sunday? anno-
tated with NE tags. Japanese temporal expressions
?????? (last Sunday)? and ?????? (this
Sunday)? are translated into English as ?last Sun-
day? and ?this Sunday? respectively. When anno-
tating NE tags for this translation pair, only ???
(Sunday)? in those temporal expressions in Japanese
is regarded as an NE according to the IREX?s NE
specifications. This causes a problem in which the
two NEs of the same surface form that are assigned
the same NE class have different referents. Each of
them should assign correspondence to different NEs
in the counterpart: the former to ?last Sunday? and
the latter to ?this Sunday.?
Tentatively, we allowed a different NE group ID to
be assigned to an NE with the identical surface form
in an NE class, as shown in Figure 3. It would be
better reexamine the consistency of the NE tag spec-
ification between Japanese and English, and the ne-
cessity of coreference information for temporal ex-
pressions and number expressions.
3 Analysis
We conducted an elementary investigation into
1,096 pairs of annotated Japanese and English ar-
ticles.
3.1 Corpus size
Table 2 shows the content size of our corpus by
the number of sentences and the morphemes/words.
The content decreases significantly when translating
from Japanese to English. This fact points out that
NE class
Japanese English
tokens avr. per types avr. per tokens avr. per types avr. per( art. / sent.) ( art. / sent.) ( art. / sent.) ( art. / sent.)
Total 24,147 (22.03 / 4.13) 12,809 (11.69 / 2.19) 15,844 (14.46 / 2.03) 10,353 ( 9.45 / 1.32)
ORGANIZATION 5,160 ( 4.71 / 0.88) 2,558 ( 2.33 / 0.44) 2,882 ( 2.63 / 0.37) 1,863 ( 1.70 / 0.24)
PERSON 3,525 ( 3.22 / 0.60) 1,628 ( 1.49 / 0.28) 2,800 ( 2.55 / 0.36) 1,410 ( 1.29 / 0.18)
LOCATION 8,737 ( 7.97 / 1.49) 3,752 ( 3.42 / 0.64) 5,792 ( 5.28 / 0.74) 3,302 ( 3.01 / 0.42)
ARTIFACT 455 ( 0.42 / 0.08) 282 ( 0.26 / 0.05) 241 ( 0.22 / 0.03) 193 ( 0.18 / 0.02)
DATE 4,342 ( 3.96 / 0.74) 2,959 ( 2.70 / 0.51) 2,990 ( 2.73 / 0.38) 2,620 ( 2.39 / 0.34)
TIME 854 ( 0.78 / 0.15) 740 ( 0.68 / 0.13) 245 ( 0.22 / 0.03) 232 ( 0.21 / 0.03)
MONEY 577 ( 0.53 / 0.10) 462 ( 0.42 / 0.08) 517 ( 0.47 / 0.07) 375 ( 0.34 / 0.05)
PERCENT 497 ( 0.45 / 0.08) 428 ( 0.39 / 0.07) 377 ( 0.34 / 0.05) 358 ( 0.33 / 0.05)
Table 3: NE frequency
articles sentences morphemes/words(avr. per article) (avr. per sent.)
J 1,096 5,851 (5.34) 321,204 (54.90)E 7,815 (7.13) 181,180 (23.18)
Table 2: Corpus size
the content tends to be lost through the translation
process.
3.2 In-language characteristics of NE
occurrences
3.2.1 Frequency
The number of occurrences for each NE class is
listed in Table 3. The distribution of NE classes is
almost the same as that in the data for MUC-7 or
IREX.
By comparing the decrease in content (cf. Ta-
ble 2), the number of NE tokens also decreases for
translations. However, the degree of the NE de-
crease is less than that of the morphemes/words. It
is also remarkable that the number of NE types is
fairly well preserved. Notice that only a small num-
ber of tokens in the NE class TIME appear in En-
glish. The reason may be that detailed time infor-
mation may become less important for English ar-
ticles, which are intended for audiences outside of
Japan and broadcast later than the original Japanese
articles.
3.2.2 NE characteristics within NE groups
To examine the surface form distribution in the
same NE groups, we counted the number of mem-
bers ( freq) and sorts of surface form (sort) for each
NE group in each article. The probability that a
given member has a unique surface form in a group
NE class
Japanese English
freq sort uniq freq sort uniq
Average 1.89 1.10 0.131 1.53 1.14 0.332
ORG. 2.02 1.12 0.144 1.55 1.16 0.345
PERSON 2.17 1.12 0.121 1.99 1.49 0.655
LOCATION 2.33 1.14 0.114 1.75 1.07 0.105
ARTIFACT 1.61 1.05 0.072 1.25 1.05 0.216
DATE 1.47 1.08 0.175 1.14 1.03 0.200
TIME 1.15 1.02 0.098 1.06 1.01 0.182
MONEY 1.25 1.03 0.109 1.38 1.35 0.936
PERCENT 1.16 1.00 0.008 1.05 1.06 0.278
Table 4: Surface form distribution in the same NE
groups
that has two or more members (uniq) has also been
calculated as follows:
uniq = freq? 2Csort? 2freq? 1Csort? 1 =
sort ? 1
freq ? 1 ( freq ? 2).
Table 4 shows the values averaged for all the NE
groups that appeared in all articles.
In English, a repetition of the same expression is
not conventionally desirable. Therefore, pronouns
or paraphrases are used frequently. On the other
hand, Japanese does not have such a convention.
This difference is considered to be the reason for the
result shown in Table 4: freq in English is smaller
than that in Japanese, and sort in English is larger
than that in Japanese. As a result, uniq in English is
higher than that in Japanese. These tendencies differ
slightly according to the NE classes.
? The sort of English PERSON is notably large. In
English, the name of a person is usually first ex-
pressed in full, and after that, it tends to be ex-
pressed only by the family name. In Japanese,
only the family name is generally used from the
beginning, especially for well-known persons.
NE class
J? E J? E
token type token type
Average 0.742 0.639 0.842 0.786
ORGANIZATION 0.684 0.612 0.877 0.837
PERSON 0.881 0.777 0.938 0.898
LOCATION 0.799 0.673 0.833 0.753
ARTIFACT 0.701 0.628 0.925 0.912
DATE 0.717 0.656 0.761 0.742
TIME 0.207 0.184 0.596 0.591
MONEY 0.593 0.595 0.781 0.733
PERCENT 0.712 0.692 0.830 0.827
Table 5: Cross-language corresponding rate
NE class
Japanese English
freq sort uniq freq sort uniq
Average 2.19 1.14 0.134 1.64 1.17 0.342
ORG. 2.25 1.17 0.164 1.62 1.19 0.364
PERSON 2.45 1.14 0.110 2.07 1.53 0.645
LOCATION 2.77 1.19 0.117 1.94 1.10 0.112
ARTIFACT 1.80 1.06 0.075 1.27 1.05 0.222
DATE 1.60 1.10 0.167 1.17 1.04 0.211
TIME 1.30 1.04 0.106 1.07 1.01 0.250
MONEY 1.24 1.04 0.138 1.47 1.43 0.934
PERCENT 1.20 1.00 0.010 1.06 1.01 0.250
Table 6: Surface form distribution in the same NE
groups (only for those having cross-language corre-
spondences)
? The uniq of English MONEY is quite high. A
money expression in Japanese tends to be trans-
lated into English as both the original currency
(usually yen) and dollars.
? The freq of temporal and number expressions
are smaller than those of named entities in the
narrow sense.
3.3 Cross-language characteristics of NE
occurrences
3.3.1 Correspondence across languages
We calculated the rates for a given NE in a doc-
ument to have a corresponding NE in the counter-
part language. The units of NE correspondences we
used for these calculations are both NE token and
NE group (type). The results, shown in Table 5,
show that an NE that appeared in English will have
a Japanese NE correspondent with a high rate.
We also conducted the same survey as we did in
Table 4 for only NEs having cross-language corefer-
ences, whose results are shown in Table 6. A com-
parison of both results shows that the freq for only
NEs having cross-language coreferences is larger,
NE class
J? E J? E
All Corr. only All Corr. only
All NEs 0.291 0.774 0.483 0.774
Average 0.304 0.790 0.494 0.790
ORG. 0.269 0.808 0.568 0.809
PERSON 0.403 0.877 0.671 0.875
LOCATION 0.318 0.746 0.461 0.745
ARTIFACT 0.410 0.725 0.662 0.710
DATE 0.307 0.805 0.428 0.805
TIME 0.033 0.815 0.227 0.815
MONEY 0.170 0.829 0.407 0.829
PERCENT 0.509 0.903 0.658 0.903
Table 7: Preservation ratio of NE order
especially in Japanese. An NE occurring more times
in an article may have more important information
and is more likely to appear in the translation.
3.3.2 Preservation of NE order
We investigated how well the order of NEs oc-
curring in an article is preserved in the counterpart
language as follows:
1. In every article, we eliminated all NEs except
the first occurrence of every NE group.
2. We calculated the ratio between all of the pos-
sible NE pairs in the source language and those
translated into the target language with the
same order of occurrence.
Table 7 lists the average preservation ratios of the
NE order for all NEs (?All?) and for NEs having
corresponding NEs in the counterpart (?Corr. only?).
The scores labeled ?All NEs? express ratios for the
order of all NEs. The preservation ratio for each
NE class is listed below in the table. The NE or-
ders are preserved so well even for all NEs that they
can be used for determining cross-language corre-
spondences.
4 Conclusion
In this paper, in which we aimed to acquire NE trans-
lation knowledge, we described our construction of
a Japanese-English broadcast news corpus with NE
tags for NE translation-pair extraction. The tags rep-
resent NE characteristics and coreference informa-
tion in a language and across languages. Analysis
of the annotated 1,097 article pairs has shown that
if NE occurrence information, such as classes, num-
ber of occurrences and occurrence order, is given for
each language side, it may provide a good clue for
determining NE correspondence across languages.
Our future plans are listed below.
? The problems in Section 2.5 need to be reex-
amined from the point of view of what infor-
mation bilingual corpora should have for NE
translation-pair extraction research.
? The proposed analysis in Section 3 pointed out
that identifying coreferences in a language is
very important for achieving NE translation-
pair extraction. Richer coreference information
should be annotated in our corpus for coref-
erence identification studies. We are planning
to annotate coreference information for pro-
nouns and some other non-NE expressions, re-
ferring to the MUC-7 coreference task defini-
tion (Hirschman and Chinchor, 1997).
? Corpora with different characteristics, such as a
bilingual newspaper corpus, will be annotated
and analyzed.
Acknowledgments This research was supported
in part by the Telecommunications Advancement
Organization of Japan.
References
Yaser Al-Onazian and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL-
02), pages 400?408.
Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In Pro-
ceedings of the 29th Annual Meeting of the Association
for Computational Linguistics (ACL-91), pages 169?
176.
Nancy Chinchor. 1997. MUC-7 named entity task
definition. http://www.itl.nist.gov/iaui/
894.02/related_projects/muc/proceedings/
ne_task.html.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the 36th Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics
(COLING-ACL ?98), volume I, pages 414?420.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Isao Goto, Noriyoshi Uratani, and Terumasa Ehara.
2001. Cross-language information retrieval of proper
nouns using context information. In Proceedings of
the 6th Natural Language Processing Pacific Rim Sym-
posium (NLPRS 2001), pages 571?578.
Masahiko Haruno and Takefumi Yamazaki. 1996. High-
performance bilingual text alignment using statistical
and dictionary information. In Proceedings of the 34th
International Conference on Computational Linguis-
tics (ACL ?96), pages 131?138.
Lynette Hirschman and Nancy Chinchor. 1997.
MUC-7 coreference task definition. http:
//www.itl.nist.gov/iaui/894.02/related_
projects/muc/proceedings/co_task.html.
IREX Committee. 1999. Named entity extraction task
definition (version 990214). http://nlp.cs.nyu.
edu/irex/NE/df990214.txt. (In Japanese).
Martin Kay and Martin Ro?scheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121?142.
Tadashi Kumano, Isao Goto, Hideki Tanaka, Noriyoshi
Uratani, and Terumasa Ehara. 2002. A translation
aid system by retrieving bilingual news database. Sys-
tems and Computers in Japan, 33(8):19?29. (Original
written in Japanese is in Transactions of the Institute
of Electronics, Information and Communication Engi-
neers, J85-D-II(6):1175?1184. 2001).
Elaine Marsh and Dennis Perzanowski. 1998. MUC-7
evaluation of IE technology: Overview and results.
http://www.itl.nist.gov/iaui/894.02/
related_projects/muc/proceedings/muc_7_
proceedings/marsh_slides.pdf.
Satoshi Sekine and Hitoshi Isahara. 1999. IREX
project overview. http://nlp.cs.nyu.edu/
irex/Paper/irex-e.ps. (Original written in
Japanese is in Proceedings of the IREX Workshop,
pages 1?5).
Satoshi Sekine and Hitoshi Isahara. 2000. IREX: IR
and IE evaluation project in Japanese. In Proceed-
ings of the 2nd International Conference on Language
Resources and Evaluation (LREC-2000), pages 1475?
1480.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating names and technical terms in Arabic text. In
Proceedings of the Workshop on Computational Ap-
proaches of the Semitic Languages, pages 34?41.
Takehito Utsuro, Hiroshi Ikeda, Masaya Yamane, Yuji
Matsumoto, and Makoto Nagao. 1994. Bilingual
text matching using bilingual dictionary and statistics.
In Proceedings of the 32th International Conference
on Computational Linguistics (ACL-94), pages 1076?
1082.
Back Transliteration from Japanese to English 
Using Target English Context  
Isao Goto?, Naoto Kato??, Terumasa Ehara???, and Hideki Tanaka? 
?NHK Science and Technical 
Research Laboratories  
1-11-10 Kinuta, Setagaya,  
Tokyo, 157-8510, Japan 
goto.i-es@nhk.or.jp 
tanaka.h-ja@nhk.or.jp 
??ATR Spoken Language Trans-
lation Research Laboratories 
2-2-2 Hikaridai, Keihanna  
Science City, Kyoto, 619-0288, 
Japan 
naoto.kato@atr.jp 
???Tokyo University of  
Science, Suwa  
5000-1, Toyohira, Chino,  
Nagano, 391-0292, Japan 
eharate@rs.suwa.tus.
ac.jp 
 
Abstract 
This paper proposes a method of automatic 
back transliteration of proper nouns, in which 
a Japanese transliterated-word is restored to 
the original English word. The English words 
are created from a sequence of letters; thus 
our method can create new English words that 
are not registered in dictionaries or English 
word lists. When a katakana character is con-
verted into English letters, there are various 
candidates of alphabetic characters. To ensure 
adequate conversion, the proposed method 
uses a target English context to calculate the 
probability of an English character or string 
corresponding to a Japanese katakana charac-
ter or string. We confirmed the effectiveness 
of using the target English context by an ex-
periment of personal-name back translitera-
tion.  
1 Introduction 
In transliteration, a word in one language is con-
verted into a character string of another language 
expressing how it is pronounced. In the case of 
transliteration into Japanese, special characters 
called katakana are used to show how a word is 
pronounced. For example, a personal name and 
its transliterated word are shown below.  
Cunningham         ?????
(ka ni n ga mu)
[Transliteration]
 
Here, the italic alphabets are romanized Japanese 
katakana characters.  
New transliterated words such as personal 
names or technical terms in katakana are not al-
ways listed in dictionaries. It would be useful for 
cross-language information retrieval if these 
words could be automatically restored to the 
original English words.  
Back transliteration is the process of restoring 
transliterated words to the original English words. 
Here is a problem of back transliteration.  
?                ?????????
(English word) (ku ra cchi fi ? ru do)
[Back transliteration]
 
There are many ambiguities to restoring a 
transliterated katakana word to its original Eng-
lish word. For example, should "a" in "ku ra cchi 
fi ? ru do" be converted into the English letter of 
"a" or "u" or some other letter or string? Trying 
to resolve the ambiguity is a difficult problem, 
which means that back transliteration to the cor-
rect English word is also difficult.  
Using the pronunciation of a dictionary or lim-
iting output English words to a particular English 
word list prepared in advance can simplify the 
problem of back transliteration. However, these 
methods cannot produce a new English word that 
is not registered in a dictionary or an English 
word list. Transliterated words are mainly proper 
nouns and technical terms, and such words are 
often not registered. Thus, a back transliteration 
framework for creating new words would be very 
useful.  
A number of back transliteration methods for 
selecting English words from an English pronun-
ciation dictionary have been proposed. They in-
clude Japanese-to-English (Knight and Graehl, 
1998) 1 , Arabic-to-English (Stalls and Knight, 
                                                          
1 Their English letter-to-sound WFST does not convert Eng-
lish words that are not registered in a pronunciation diction-
ary.  
1998), and Korean-to-English (Lin and Chen, 
2002).  
There are also methods that select English 
words from an English word list, e.g., Japanese-
to-English (Fujii and Ishikawa, 2001) and Chi-
nese-to-English (Chen et al, 1998).  
Moreover, there are back transliteration meth-
ods capable of generating new words, there are 
some methods for back transliteration from Ko-
rean to English (Jeong et al, 1999; Kang and 
Choi, 2000).  
These previous works did not take the target 
English context into account for calculating the 
plausibility of matching target characters with the 
source characters.  
This paper presents a method of taking the tar-
get English context into account to generate an 
English word from a Japanese katakana word. 
Our character-based method can produce new 
English words that are not listed in the learning 
corpus.  
This paper is organized as follows. Section 2 
describes our method. Section 3 describes the 
experimental set-up and results. Section 4 dis-
cusses the performance of our method based on 
the experimental results. Section 5 concludes our 
research. 
2 Proposed Method 
2.1 Advantage of using English context 
First we explain the difficulty of back translitera-
tion without a pronunciation dictionary. Next, we 
clarify the reason for the difficulty. Finally, we 
clarify the effect using English context in back 
transliteration.  
In back transliteration, an English letter or 
string is chosen to correspond to a katakana char-
acter or string. However, this decision is difficult. 
For example, there are cases that an English letter 
"u" corresponds to "a" of katakana, and there are 
cases that the same English letter "u" does not 
correspond to the same "a" of katakana. "u" in 
Cunningham corresponds to "a" in katakana and 
"u" in Bush does not correspond to "a" in kata-
kana. It is difficult to resolve this ambiguity 
without the pronunciation registered in a diction-
ary.  
The difference in correspondence mainly 
comes from the difference of the letters around 
the English letter "u." The correspondence of an 
English letter or string to a katakana character or 
string varies depending on the surrounding char-
acters, i.e., on its English context.  
Thus, our back transliteration method uses the 
target English context to calculate the probability 
of English letters corresponding to a katakana 
character or string.  
2.2 Notation and conversion-candidate 
lattice  
We formulate the word conversion process as a 
unit conversion process for treating new words. 
Here, the unit is one or more characters that form 
a part of characters of the word.  
A katakana word, K, is expressed by equation 
2.1 with "^" and "$" added to its start and end, 
respectively.  
1
0 0 1 1...
m
mk k k k
+
+= =K  (2.1) 
0 ^k = , 1 $mk + =  (2.2) 
where jk  is the j-th character in the katakana 
word, and m is the number of characters except 
for "^" and "$" and 10
mk +  is a character string 
from 0k  to 1mk + .  
We use katakana units constructed of one or 
more katakana characters. We denote a katakana 
unit as ku. For any ku, many English units, eu, 
could be corresponded as conversion-candidates. 
The ku's and eu's are generated using a learning 
corpus in which bilingual words are separated 
into units and every ku unit is related an eu unit.  
{ }EL  denotes the lattice of all eu's correspond-
ing to ku's covering a Japanese word. Every eu is 
a node of the lattice and each node is connected 
with next nodes. { }EL  has a lattice structure start-
ing from "^" and ending at "$." Figure 1 shows an 
example of { }EL  corresponding to a katakana 
word "?????????(ki ru shu shu ta i 
n)." In the figure, each circle represents one eu. 
A character string linking individual character 
units in the paths 1 2( , ,.., )d qp p p p?  between "^" 
and "$" in { }EL  becomes a conversion candidate, 
where q is the number of paths between "^" and 
"$" in { }EL . 
We get English word candidates by joining eu's 
from "^" to "$" in { }EL . We select a certain path, 
pd, in { }EL . The number of character units 
cchi
?(ki)
chi
ci
cki
cy
k
ke
khi
ki
kie
kii
ky
qui
ch
che
chou
chu
s
sc
sch
schu
sh
?(ru) ??(shu) ??(shu) ?(ta) ?(i) ?(n)
ta
tad
tag
te
ter
tha
ti
to
tta
tu
e
hi
i
ji
y
yeh
yi
m
mon
mp
n
ne
ng
ngh
nin
nn
nne
nt
nw
t
??(tai)
l
ld
le
les
lew
ll
lle
llu
lou
lu
r
rc
rd
re
rg
roo
rou
rr
rre
rt
lu
she
shu
su
sy
sz
ch
che
chou
chu
s
sc
sch
schu
sh
she
shu
su
sy
sz
taj
tay
tey
ti
tie
ty
tye? ? ?
? ? ?
? ? ? ? ? ?
? ? ?
? ? ?
? ? ?
? ? ?
^ $
 
Figure 1: Example of lattice { }EL  of conversion candidates units. 
 
except for "^" and "$" in pd is expressed as ( )dn p . 
The character units in pd are numbered from start 
to end.  
The English word, E, resulting from the con-
version of a katakana word, K, for pd is expressed 
as follows: 
1
0 0 1 1..
m
mk k k k
+
+= =K  
( ) 1
0 0 1 ( ) 1..
d
d
n p
n p
+
+= ku = ku ku ku , (2.3) 
( ) 1
0 0 1 ( ) 1..
d
d
l p
l pe e e e
+
+= =E  
( ) 1
0 0 1 ( ) 1..
d
d
n p
n p
+
+= eu = eu eu eu , (2.4) 
0 0 0 0 ^k e= = = =ku eu ,  
1 ( ) 1 ( ) 1 ( ) 1 $d d dm l p n p n pk e+ + + += = = =ku eu ,  (2.5) 
where ej is the j-th character in the English word. 
( )dl p  is the number of characters except for "^" 
and "$" in the English word. ( ) 10 d
n p +eu  for each pd 
in { }EL  in equation 2.4 becomes the candidate 
English word. ( ) 10 d
n p +ku  in equation 2.3 shows the 
sequence of katakana units. 
2.3 Probability models using target Eng-
lish context 
To determine the corresponding English word for 
a katakana word, the following equation 2.6 must 
be calculated: 
? arg max ( | )P
E
E = E K . (2.6) 
Here, E?  represents an output result. 
To use the English context for calculating the 
matching of an English unit with a katakana unit, 
the above equation is transformed into Equation 
2.7 by using Bayes? theorem. 
? arg max ( ) ( | )P P=
E
E E K E  (2.7) 
Equation 2.7 contains a translation model in 
which an English word is a condition and kata-
kana is a result.  
The word in the translation model ( | )P K E  in 
Equation 2.7 is broken down into character units 
by using equations 2.3 and 2.4. 
{
}
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1
0 0
( ) 1 ( ) 1 ( ) 1
0 0 0
? arg max ( )
( , , | )
arg max ( )
( | , , )
( | , ) ( | )
d d
n p n pd d
d d
n p n pd d
d d d
n p n p
n p n p
n p n p n p
P
P
P
P
P P
+ +
+ +
+ +
+ +
+ + +
=
?
=
?
?
? ?
? ?
E
eu ku
E
eu ku
E E
K ku eu E
E
K ku eu E
ku eu E eu E
 (2.8) 
( ) 1
0
dn p +eu  includes information of E. K is only 
affected by ( ) 1
0
dn p +ku . Thus equation 2.8 can be 
rewritten as follows:  
( ) 1 ( ) 1
0 0
( ) 1
0
( ) 1 ( ) 1 ( ) 1
0 0 0 .
? argmax ( )
( | )
( | ) ( | )
d
n p n pd d
d d d
n p
n p n p n p
P
P
P P
+ +
+
+ + +
???
???
=
?
?
? ?
E
eu ku
E E
K ku
ku eu eu E
 (2.9) 
( ) 1
0( | )
dn pP +K ku  is 1 when the string of K and 
( ) 1
0
dn p +ku  is the same, and the strings of the 
( ) 1
0
dn p +ku  of all paths in the lattice and the string 
of the K is the same. Thus, ( ) 1
0( | )
dn pP +K ku  is al-
ways 1.  
We approximate the sum of paths by selecting 
the maximum path.  
( ) 1 ( ) 1
0 0
( ) 1
0
? arg max ( ) ( | )
( | )
d d
d
n p n p
n p
P P
P
+ +
+
?
?
E
E E ku eu
eu E
 
 (2.10) 
We show an instance of each probability 
model with a concrete value as follows: 
 
( )
(^Crutchfield$)
P
P
E , 
 
( ) 1
0( | )
(^ | ^ / )
( ) ( / / / / / )
dn pP
P
ku ra cchi fi ru do ku ra cchi fi ru do
+
? ?
K ku
?????????$ ?/?/??/???/?/?/$ , 
 
( ) 1 ( ) 1
0 0( | )
(^ / | ^ / C/ru/tch/fie/l/d / $)
( / / / / / )
d dn p n pP
P
ku ra cchi fi ru do
+ +
?
ku eu
?/?/??/???/?/?/$ , 
 
( ) 1
0( | )
(^ / C/ru/tch/fie/ld / $ | ^Crutchfield$)
dn pP
P
+eu E . 
 
We broke down the language model ( )P E  in 
equation 2.10 into letters.  
( ) 1
1
1
( | )( )
dl p
j
j j a
j
P e eP
+
?
?
=
? ?E  (2.11) 
Here, a is a constant. Equation 2.11 is an (a+1)-
gram model of English letters.  
Next, we approximate the translation model 
( ) 1 ( ) 1
0 0( | )
d dn p n pP + +ku eu  and the chunking model 
( ) 1
0( | )
dn pP +eu E . For this, we use our previously 
proposed approximation technique (Goto et al, 
2003). The outline of the technique is shown as 
follows.  
( ) 1 ( ) 1
0 0( | )
d dn p n pP + +ku eu  is approximated by reduc-
ing the condition.  
( ) 1 ( ) 1
0 0
( ) 1
( ) 11
0 0
1
( | )
( | , )
d d
d
d
n p n p
n p
n pi
i
i
P
P
+ +
+
+?
=
= ?
ku eu
ku ku eu
 
( ) 1
( ) 1
( ) ( ) 1
1
( | , , )
dn p
start i
i start i b i end i
i
P e e
+
?
? +
=
? ? ku eu
 (2.12)
 
where start(i) is the first position of the i-th char-
acter unit eui, while end(i) is the last position of 
the i-th character unit eui; and b is a constant. 
Equation 2.12 takes English context ( ) 1( )
start i
start i be
?
?  and 
( ) 1end ie +  into account.  
Next, the chunking model ( ) 10( | )d
n pP +eu E  is 
transformed. All chunking patterns of ( ) 10 d
l pe +=E  
into ( ) 10 d
n p +eu  are denoted by each l(pd)+1 point 
between l(pd)+2 characters that serve or do not 
serve as delimiters. eu0 and ( ) 1dn p +eu  are deter-
mined in advance. l(pd)-1 points remain ambigu-
ous. We represent the value that is delimiter or is 
non-delimiter between ej and ej+1 by zj. We call 
the zj delimiter distinction.  { delimiternon-delimiterjz =  (2.13) 
Here, we show an example of English units us-
ing zj.  
(e1 e2 e3 e4  e5  e6 e7 e8 e9  e10 e11)
C r u t c h f i e l d
(z1  z2  z3  z4  z5  z6 z7  z8  z9  z10)
/ / / /
1  0  1  0  0  1  0  0  1  1
English:
Values of zj:
/
 
 
In this example, a delimiter of zj is represented by 
1 and a non-delimiter is represented by 0.  
The chunking model is transformed into a 
processing per character by using zj. And we re-
duce the condition.  
( ) 1
0
( ) 1 ( ) 1
0 0
( ) 1
( ) 11
0 0
1
( | )
( | )
( | , )
d
d d
d
d
n p
l p l p
l p
l pj
j
j
P
P z e
P z z e
+
? +
?
+?
=
=
= ?
eu E
 
( ) 1
1 1
1
1
( | , )
dl p
j j
j j c j c
j
P z z e
?
? +
? ? ?
=
? ?  (2.14) 
The conditional information of the English 
1j
j ce
+
?  is as many as c characters and 1 character 
before and after zj, respectively. The conditional 
information of 1 1
j
j cz
?
? ?  is as many as c+1 delimiter 
distinctions before zj.  
By using equation 2.11, 2.12, and 2.14, equa-
tion 2.10 becomes as follows:  
( ) 1
1
1
( )
( ) 1
( ) ( ) 1
1
( ) 1
1 1
1
1
? arg max ( | )
( | , , )
( | , ).
d
d
d
l p
j
j j a
j
n p
start i
i start i b i end i
i
l p
j j
j j c j c
j
P e e
P e e
P z z e
+
?
?
=
?
? +
=
?
? +
? ? ?
=
?
?
?
?
?
?
E
E
ku eu
 (2.15) 
Equation 2.15 is the equation of our back 
transliteration method.  
2.4 Beam search solution for context 
sensitive grammar 
Equation 2.15 includes context-sensitive gram-
mar. As such, it can not be carried out efficiently. 
In decoding from the head of a word to the tail, 
eend(i)+1 in equation 2.15 becomes context-
sensitive. Thus we try to get approximate results 
by using a beam search solution. To get the re-
sults, we use dynamic programming. Every node 
of eu in the lattice keeps the N-best results evalu-
ated by using a letter of eend(i)+1 that gives the 
maximum probability in the next letters. When 
the results of next node are evaluated for select-
ing the N-best, the accurate probabilities from the 
previous nodes are used.  
2.5 Learning probability models based 
on the maximum entropy method 
The probability models are learned based on the 
maximum entropy method. This makes it possi-
ble to prevent data sparseness relating to the 
model as well as to efficiently utilize many con-
ditions, such as context, simultaneously. We use 
the Gaussian Prior (Chen and Rosenfeld, 1999) 
smoothing method for the language model. We 
use one Gaussian variance. We use the value of 
the Gaussian variance that minimizes the test 
set's perplexity.  
The feature functions of the models based on 
the maximum entropy method are defined as 
combinations of letters. In addition, we use 
vowel, consonant, and semi-vowel classes for the 
translation model. We manually define the com-
binations of the letter positions such as ej and ej-1. 
The feature functions consist of the letter combi-
nations that meet the combinations of the letter 
positions and are observed at least once in the 
learning data.  
2.6 Corpus for learning 
A Japanese-English word list aligned by unit was 
used for learning the translation model and the 
chunking model and for generating the lattice of 
conversion candidates. The alignment was done 
by semi-automatically. A romanized katakana 
character usually corresponds to one or several 
English letters or strings. For example, a roman-
ized katakana character "k" usually corresponds 
to an English letter "c," "k," "ch," or "q." With 
such heuristic rules, the Japanese-English word 
corpus could be aligned by unit and the align-
ment errors were corrected manually.  
3 Experiment 
3.1 Learning data and test data 
We conducted an experiment on back translitera-
tion using English personal names. The learning 
data used in the experiment are described below. 
The Dictionary of Western Names of 80,000 
People2 was used as the source of the Japanese-
English word corpus. We chose the names in al-
phabet from A to Z and their corresponding kata-
kana. The number of distinct words was 39,830 
for English words and 39,562 for katakana words. 
The number of English-katakana pairs was 
83,0573. We related the alphabet and katakana 
character units in those words by using the 
method described in section 2.6. We then used 
the corpus to make the translation and the chunk-
ing models and to generate a lattice of conversion 
candidates. 
The learning of the language model was car-
ried out using a word list that was created by 
merging two word lists: an American personal-
                                                          
2 Published by Nichigai Associates in Japan in 1994.  
3  This corpus includes many identical English-katakana 
word pairs.  
name list4, and English head words of the Dic-
tionary of Western Names of 80,000 people. The 
American name list contains frequency informa-
tion for each name; we also used the frequency 
data for the learning of the language model. A 
test set for evaluating the value of the Gaussian 
variance was created using the American name 
list. The list was split 9:1, and we used the larger 
data for learning and the smaller data for evaluat-
ing the parameter value.  
The test data is as follows. The test data con-
tained 333 katakana name words of American 
Cabinet officials, and other high-ranking officials, 
as well as high-ranking governmental officials of 
Canada, the United Kingdom, Australia, and 
New Zealand (listed in the World Yearbook 2002 
published by Kyodo News in Japan). The English 
name words that were listed along with the corre-
sponding katakana names were used as answer 
words. Words that included characters other than 
the letters A to Z were excluded from the test 
data. Family names and First names were not 
distinguished.  
3.2 Experimental models 
We used the following methods to test the indi-
vidual effects of each factor of our method.  
? Method A 
Used a model that did not take English context 
into account. The plausibility is expressed as fol-
lows: 
( )
1
? arg max ( | )
dn p
i i
i
P
=
= ?
E
E eu ku . (3.1) 
? Method B 
Used our language model and a translation model 
that did not consider English context. The con-
stant a = 3 in the language model. The plausibil-
ity is expressed as follows: 
( ) 1 ( )
1
3
1 1
? arg max ( | ) ( | )
d dl p n p
j
j j i i
j i
P e e P
+
?
?
= =
= ? ?
E
E ku eu . 
 (3.2) 
? Method C 
Applied our chunking model to method B, with c 
= 3 in the chunking model. The plausibility is 
expressed as follows: 
                                                          
4  Prepared from the 1990 Census conducted by the U.S. 
Department of Commerce. Available at 
http://www.census.gov/genealogy/names/ . The list includes 
91,910 distinct words.  
( ) 1 ( )
1
3
1 1
? arg max ( | ) ( | )
d dl p n p
j
j j i i
j i
P e e P
+
?
?
= =
= ? ?
E
E ku eu  
( ) 1
1 4
4 3
1
( | , ).
dl p
j j
j j j
j
P z z e
?
? +
? ?
=
? ?  (3.3) 
? Method D 
Used our translation model that considered Eng-
lish context, but not the chunking model. b = 3 in 
the translation model. The plausibility is ex-
pressed as follows: 
( )
( ) 1
1
3
1
( )
( ) 1
( ) 3 ( ) 1
1
? arg max ( | )
| , , .
d
d
l p
j
j j
j
n p
start i
i start i i end i
i
P e e
P e e
+
?
?
=
?
? +
=
=
?
?
?
E
E
ku eu
 (3.4) 
? Method E 
Used our language model, translation model, and 
chunking model. The plausibility is expressed as 
follows: 
( )
( ) 1
1
3
1
( )
( ) 1
( ) 3 ( ) 1
1
( ) 1
1 4
4 3
1
? arg max ( | )
| , ,
( | , ).
d
d
d
l p
j
j j
j
n p
start i
i start i i end i
i
l p
j j
j j j
j
P e e
P e e
P z z e
+
?
?
=
?
? +
=
?
? +
? ?
=
=
?
?
?
?
?
E
E
ku eu  
 (3.5) 
3.3 Results 
Table 1 shows the results of the experiment5 on 
back transliteration from Japanese katakana to 
English. The conversion was determined to be 
successful if the generated English word agreed 
perfectly with the English word in the test data. 
Table 2 shows examples of back transliterated 
words.  
Method A B C D E 
Top 1 23.7 57.4 61.6 63.1 66.4
Top 2 34.8 69.1 72.4 71.8 74.2
Top 3 42.9 73.6 76.6 75.4 79.3
Top 5 54.1 77.5 79.9 80.8 83.5
Top 10 63.4 82.0 85.3 86.5 87.7
Table 1: Ratio (%) of including the answer word 
in high-ranking words.  
                                                          
5 For model D and E, we used N=50 for the beam search 
solution. In addition, we kept paths that represented parts of 
words existing in the learning data.  
Japanese katakana 
(romanized katakana) 
Created English 
??????? 
(a shu ku ro fu to) 
Ashcroft 
????????? 
(ki ru shu shu ta i n) 
Kirschstein 
????? 
(su pe n sa -) 
Spencer 
 
???? 
(pa u e ru) 
Powell 
 
????? 
(pu ri n shi pi) 
Principi 
 
Table 2: Example of English words produced.  
4 Discussion 
The correct-match ratio of the method E for the 
first-ranked words was 66%. Its correct-match 
ratio for words up to the 10th rank was 87%.  
Regarding the top 1 ranked words, method B 
that used a language model increase the ratio 33-
points from method A that did not use a language 
model. This demonstrates the effectiveness of the 
language model. 
Also for the top 1 ranked words, method C 
which adopted the chunking model increase the 
ratio 4-points from method B that did not adopt 
the chunking model in the top 1 ranked words. 
This indicates the effectiveness of the chunking 
model. 
Method D that used a translation model taking 
English context into account had a ratio 5-points 
higher in top 1 ranked words than that of method 
B that used a translation model not taking Eng-
lish context into account. This demonstrates the 
effectiveness of the language model.  
Method E gave the best ratio. Its ratio for the 
top 1 ranked word was 42-points higher than 
method A's.  
These results demonstrate the effectiveness of 
using English context for back transliteration.  
5 Conclusion 
This paper described a method for Japanese to 
English back transliteration. Unlike conventional 
methods, our method uses a target English con-
text to calculate the plausibility of matching be-
tween English and katakana. Our method can 
treat English words that do not exist in learning 
data. We confirmed the effectiveness of our 
method in an experiment using personal names. 
We will apply this technique to cross-language 
information retrieval.  
References  
Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding, 
and Shih-Chung Tsai. 1998. Proper Name Transla-
tion in Cross-Language Information Retrieval. 36th 
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference 
on Computational Linguistics, pp.232-236. 
Stanley F. Chen, Ronald Rosenfeld. 1999. A Gaussian 
Prior for Smoothing Maximum Entropy Models. 
Technical Report CMU-CS-99-108, Carnegie Mel-
lon University.  
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating Names and Technical Terms in Arabic Text. 
COLING/ACL Workshop on Computational Ap-
proaches to Semitic Languages. 
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Teru-
masa Ehara. 2003. Transliteration Considering 
Context Information based on the Maximum En-
tropy Method. Machine Translation Summit IX, 
pp.125-132. 
Kil Soon Jeong, Sung Hyun Myaeng, Jae Sung Lee, 
and Key-Sun Choi. 1999. Automatic Identification 
and Back-Transliteration of Foreign Words for In-
formation Retrieval. Information Processing and 
Management, Vol.35, No.4, pp.523-540. 
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic 
Transliteration and Back-Transliteration by Deci-
sion Tree Learning. International Conference on 
Language Resources and Evaluation. pp.1135-1411. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics, Vol.24, 
No.4, pp.599-612. 
Wei-Hao Lin and Hsin-Hsi Chen. 2002. Backward 
Machine Transliteration by Learning Phonetic 
Similarity. 6th Conference on Natural Language 
Learning, pp.139-145.  
Atsushi Fujii and Tetsuya Ishikawa. 2001. Japa-
nese/English Cross-Language Information Re-
trieval: Exploration of Query Translation and 
Transliteration. Computers and the Humanities, 
Vol.35, No.4, pp.389-420. 
Analysis and modeling of manual summarization of 
 Japanese broadcast news 
 Hideki Tanaka, Tadashi Kumano, Masamichi Nishiwaki and Takayuki Itoh 
Science and Techinical Research Laboratories of NHK 
1-10-11, Kinuta, Setagaya-ku 
Tokyo, 157-8510, Japan 
{tanaka.h-ja,kumano.t-eq,nishiwaki.m-hk,itou.t-gq}@nhk.or.jp 
Abstract 
We describe our analysis and modeling 
of the summarization process of Japa-
nese broadcast news. We have studied 
the entire manual summarization proc-
ess of the Japan Broadcasting Corpora-
tion (NHK). The staff of NHK has been 
making manual summarizations of 
news text on a daily basis since De-
cember 2000. We interviewed these 
professional abstractors and obtained a 
considerable amount of news summa-
ries. We matched the summary with the 
original text, investigated the news text 
structure, and thereby analyzed the 
manual summarization process. We 
then developed a summarization model 
on which we intend to build a summa-
rization system. 
1  Introduction 
Automatic text summarization research has a 
long history that dates back to the late 50?s 
(Mani and Maybury, 1999). It started mainly 
with the purpose of information gathering or 
assimilation, and most of the research has dealt 
with extracting the important parts of the texts. 
The summaries obtained with these techniques, 
so called extracts, have been used for judging 
the importance of the texts. 
We have started research on automatic sum-
marization for the purpose of information 
dissemination, namely summarization of news 
texts for broadcast news. Recently, we have 
studied the entire manual summarization process 
of the Japan Broadcasting Corporation (NHK). 
NHK has been making manual summariza-
tions of news text on a daily basis since Decem-
ber 2000, when it started satellite digital 
broadcasting. The summarized text has been 
used for the data service of the digital broadcast-
ing and on Web pages accessible by mobile 
phones.  
We interviewed NHK?s professional abstrac-
tors and analyzed a considerable amount of 
news summaries. We matched these summaries 
with the original news and studied the summari-
zation process based on the results of our analy-
sis and interviews.  
In this paper, we report on what we found 
during the interviews with the abstractors and 
the results of the automatic text alignment be-
tween summaries and the original news together 
with the word position matching. We also pro-
pose a summarization model for an automatic or 
semi-automatic summarization system. 
2 The manual summarization process 
Most of the radio and TV news services of 
NHK are based on a ?general news manuscript.? 
We call such manuscripts the original news in 
this paper. The original news is manually sum-
marized into summary news that are made avail-
able to the public through Web pages and digital 
broadcasting, as mentioned in section 1.  
We asked professional abstractors about the 
summarization environment and process and in 
so doing discovered the following. 
? Abstractor 
The original news is written by NHK report-
ers, and the text is summarized by different 
writers, i.e., professional abstractors. Most pro-
fessional abstractors are retired reporters who 
have expertise in writing news. 
? Compression rate and time allowance 
The original news is compressed to a maxi-
mum length of 105 Japanese characters. We will 
49
show in section 4 that the average compression 
rate is about 22.5%. The upper bound is decided 
from the display design of the data service of 
digital TV broadcasting. The abstractors must 
work quickly because the summary news must 
be broadcast promptly. 
? Techniques 
The abstractors use only information con-
tained in the original news. They scan the origi-
nal news quickly and repeatedly, not to 
understand the full content, but to select the 
parts to be used in the summary news. The ab-
stractors? special reading tendency has been re-
ported in (Mani, 2001), and we can say the same 
tendency was observed in our Japanese abstrac-
tors. The abstractors focus on the lead (the open-
ing part) of the original news. They sometimes 
use the end part of the original news. 
3    Corpus construction 
We planned the summary news corpus as a 
resource to investigate the manual summariza-
tion process and to look into the possibility of an 
automatic summarization system for broadcast 
news. We obtained 18,777 pieces of summary 
news from NHK. Although each piece is a 
summary of a particular original news text, the 
link between the summary and the original news 
is not available.  
We matched the summary and original news 
and constructed a corpus. There have been sev-
eral attempts to construct <summary text, origi-
nal text> corpora (Marcu, 1999; Jing and 
McKeown, 1999). We decided to use the 
method proposed by Jing and McKeown (1999) 
for the reasons given below. 
As our abstractors mentioned that they used 
only information available in the original news, 
we hypothesize that the summary and the origi-
nal news share many surface words. This indi-
cates that the surface-word-based matching 
methods such as (Marcu, 1999; Jing and McKe-
own, 1999) will be effective. 
In particular, the word position matching re-
alized in (Jing and McKeown, 1999) seems es-
pecially useful. We thought that we might be 
able to observe the summarization process pre-
cisely by tracing the word position links, and we 
employed their work with a little modification.  
As a result, our corpus takes the form of the 
triple: <summary, original, word position corre-
spondence>. 
3.1 Matching algorithm 
Jing and McKeown (1999) treated a word 
matching problem between a summary and its 
text, which they called the summary decomposi-
tion problem. They employed a statistical model 
(briefly described below) and obtained good 
results when they tested their method with the 
Ziff-Davis corpus. In the following explanation, 
we use the notion of summary and text instead 
of summary news and original news for simplic-
ity. 
(1) The word position in a summary is repre-
sented by <I>. 
(2) The word position in the text is repre-
sented by a pair of the sentence position (S) 
and the word position in a sentence (W) as in 
<S, W>. 
(3) Each summary word is checked as to 
whether it appears in the text. If it appears, 
all of the positions in the text are stored in 
the form of <S,W> to form a position trellis. 
(4) Scan the n summary words from left to 
right and find the path on the trellis that 
maximizes the score of formula (1). 
??
=
+ ===
1
1
11221 )),(|),((
n
i
ii WSIWSIPP  (1) 
This formula is the repeated product of the 
probability that the two adjacent words in a 
summary (Ii and Ii+1) appear at positions (S1, W1) 
and (S2, W2) in the text, respectively. This quan-
tity represents the goodness of the summary and 
the text word matching. As a result, the path on 
the trellis with the maximum probability gives 
the overall most likely word position match. 
Jing and McKeown (1999) assigned six-
grade heuristic values to the probability. The 
highest probability of 1.0 was given when two 
adjacent words in a summary appear at adjacent 
positions in the same sentence of the text. The 
lowest probability of 0.5 was given when two 
adjacent words in a summary appear in different 
sentences in the text with a certain distance or 
greater. We fixed the distance at two sentences, 
considering the average sentence count of the 
original news texts. 
50
Original news text 
??????????????????????????????????????????
??????????????????????????????????????????
????????????????????? 
?????????????????????????????????????????
?????????????????????????
?????????????????????????????????????????
?????????????????????????????????????????
??????????????????????? 
?????????????????????????????????????????
?????????????????????????????????????????
?????? 
body
lead
Summary news text
Figure 1. Summary and original news text matching.  
Jing and McKeown?s algorithm (1999) is de-
signed to treat a fixed summary and text pair and 
needs some modification to be applied to our 
two-fold problem of finding the original news of 
a given summary news from a large collection 
of news together with the word position match-
ing. 
Their method has a special treatment for a 
summary word that does not appear in the text. 
It assumes that such a word does not exist in the 
summary and therefore skips the trellis at this 
word with a probability of 1. This unfavorably 
biases news text that contains fewer matching 
words. To alleviate this problem, we experimen-
tally found that the probability score of 0.55 
works well for such a case (This score was the 
second smallest of the original six-grade score). 
We developed a word match browser to pre-
cisely check the words of the summary and 
original news. 
3.2 Summary and original news matching 
We matched 18,777 summary news texts 
from November 2003 to June 2004 against the 
news database, which mostly covers the original 
news of the period. We followed the procedures 
below. 
? Numerical expression normalization 
Numerical expressions in the original news 
are written in Chinese numerals (Kanji) and 
those of the summary news are written in 
Arabic numerals. We normalized the Chinese 
numerals into Arabic numerals. 
? Morphological analysis 
The summary and original news were mor-
phologically analyzed. We used morphemes 
as a matching unit. In this paper, we will use 
morphemes and words interchangeably. 
? Search span 
Each summary news was matched against the 
news written in the three-day period before 
the summary was written. This period was 
chosen experimentally. 
4  Results and observation 
We randomly checked the news matching re-
sults and found more than 90% were correct. 
Some of the summaries were exceptionally long, 
and we consider that such noisy data was the 
main reason for incorrect matching. Figure 1 
shows a matching example. The underlined (line 
and broken line) sentences show the word posi-
tion match.  
The word matching is not easy to evaluate 
because we do not have the correct matching 
answer. Although there are some problems in 
the matching, most of the results seem to be 
good enough for approximate analysis. The fol-
lowing discussion assumes that the word match-
ing is correct. 
4.1 Compression rate 
Table 1 shows the basic statistics of the 
summary and its corresponding original news. 
51
We can see that the average compression rate is 
22.5% in terms of characters. The average sum-
mary news length (109.9 characters per news 
text) was longer than what we were told (105, 
see section 2). 
We then checked the length of the typical 
summary texts. We found that the cumulative 
relative frequency of the summary text with the 
sentence count from 1 to 4 was 0.99 and was 
quite dominant. We checked the average length 
of these summaries and obtained 105.4, which is 
close to what we were told. We guess that noisy 
?long summaries? skewed the figure.  
 
 Original Summary
text counts 18,777 
Ave. sent. count/text 5.13 1.63
Ave. text length (char.) 487.7 109.9
Ave. first line length (char.)  94.9 81.3
0
10
20
30
40
50
60
70
80
%
1 2 3 4 5 6 7 8Sent. No.
Figure 2. Summary word employment
ratio of original news
4 sent. 5 sent 6 sent 7 sent 8 sent
Table 1. Basic statistics of summary and original 
news 
4.2 Word match ratio 
We measured how many of the summary 
words came from original news. As our match-
ing result contains word-to-word correspon-
dence, we calculated the ratio of the matched 
words in a summary text. Table 2 shows a part 
of the result. It shows the relative frequency of 
the summary news in which 100% of the words 
came from the original news reached 0.265 and 
those that had more than 90% reached 0.970.  
 
Word match ratio Rel. summary  freq. 
100? 0.265 
More than 90? 0.970 (cumulative) 
Table 2. Word match ratio 
 
This strongly suggests that most of the sum-
mary news is the ?extract? (Mani, 2001), which 
is written using only vocabulary appearing in the 
original news. This result is in accord with what 
the abstractors told us. 
4.3 Summary word employment in the 
original news sentences 
The previous section indicated that our sum-
mary likely belongs to the extract type. Where in 
the original news do these words come from? 
We next measured the word employment ratio 
of each sentence in the original news and the 
result is presented in Figure 2. 
In this graph, the original news is categorized 
into five cases according to its sentence count 
from 4 to 81 and the average word employment 
ratio is shown for each sentence.  
Of this figure, the following observations can 
be made: 
?  Bias toward the first sentence 
In all five cases, the first sentence recorded 
the highest word employment ratio. The per-
centages of the second and third sentences in-
crease when the news contains many sentences. 
The opening part of the news text is called the 
lead. We will discuss its role in the next section. 
? No clear favorite for the final sentence 
There was no employment ratio rise for the 
closing sentences in any case even though our 
abstractors indicated they often use information 
in the last sentence. This inconsistency may be 
due to the word match error. Final sentences 
actually have an important role in news, as we 
will see in the next section. 
5  Summarization model 
In the previous section, we found a quite 
high word overlap between a summary and the 
opening part of the original news text. We 
checked with our word match browser the simi-
larity of the summary news and lead sentences, 
and found that most of the summary sentences 
                                                          
1 These news texts cover the 88 % of the total news texts.  
52
take exactly the same syntactic pattern of the 
opening sentence. Based on this observation and 
what we found in the interviews, we devised a 
news text summarization model. The model can 
explain our abstractors? behavior, and we are 
planning to develop an automatic or semi-
automatic summarization system with it. We 
will explain the typical news text structure and 
present our model. 
5.1 News text structure 
Most of our news texts are written with a 
three-part structure, i.e., lead, body and supple-
ment. Figure 1 shows the two-fold structure of 
the lead and the body. Each part has the follow-
ing characteristics.   
? Lead 
The most important information is briefly de-
scribed in the opening part of a news text. This 
part is called the lead. Proper nouns are often 
avoided in favor of more abstract expressions 
such as ?a young man? or ?a big insurance com-
pany.? The lead is usually written in one or two 
sentences. 
? Body 
The lead is detailed in the body. The 5W1H 
information is mainly elaborated, and the proper 
names that were vaguely mentioned in the lead 
appear here. The statements of people involved 
in the news sometimes appear here. The repeti-
tive structure of the lead and the body is rooted 
in the nature of radio news; listeners cannot go 
back to the previous part if they missed the in-
formation. 
? Supplement 
Necessary information that has not been cov-
ered in the lead and the body is placed here. 
Take for an example of weather news about a 
typhoon. A caution from the Meteorological 
agency is sometimes added after the typhoon?s 
movement has been described.  
5.2 Model 
We found that most of the summary news is 
written based on the lead sentences. They are 
then shortened or partly modified with the ex-
pressions in the body to make them more infor-
mative and self-contained.  
The essential operation, we consider, lies in 
the editing of the lead sentences under the sum-
mary length constraint. Based on the observation, 
we have proposed a two-step summarization 
model of reading and editing. The summary in 
Figure 1 is constructed with the lead sentence 
with the insertion of a phrase in the body. 
? Reading phase 
(1) Identify the lead, the body and the sup-
plement sentences in the original news. 
(2) Analysis 
Find the correspondences between the parts 
in the lead and those in the body. We can re-
gard this process as a co-reference resolution.  
?  Summary editing phase 
(3) Set the lead sentence as the base sentence 
of the summary. 
(4) Apply the following operations until the 
base sentence length is close enough to the 
predefined length N. 
(4-1) Delete parts in the base sentence. 
(4-2) Substitute parts in the base sentence 
with the corresponding parts in the body with 
the results of (2). 
(4-2?) Add a body part to the base sentence. 
We may view this as a null part substituted 
by a body part. 
(4-3) Add supplement sentences. 
The supplement is often included in a sum-
mary; this part contains different information 
from the other parts.  
5.3 Related works and discussion 
Our two-step model essentially belongs to 
the same category as the works of (Mani et al, 
1999) and (Jing and McKeown, 2000). Mani et 
al. (1999) proposed a summarization system 
based on the ?draft and revision.? Jing and 
McKeown (2000) proposed a system based on 
?extraction and cut-and-paste generation.? Our 
abstractors performed the same cut-and-paste 
operations that Jing and McKeown noted in their 
work, and we think that our two-step model will 
be a reasonable starting point for our subsequent 
research. Below are some of our observations. 
53
The lead sentences play a central role in our 
model since they serve as the base of the final 
summary. Their identification can be achieved 
with the same techniques as used for the impor-
tant sentence extraction. In our case, the sen-
tence position information plays an important 
role as was shown by Kato and Uratani (2000). 
We consider the identification of the body and 
the supplement part together with the lead will 
be beneficial for the co-reference resolution. 
The co-reference resolution problem between 
the lead and the body should be treated in a 
more general way than usual. We found that our 
problem ranges from the word level, the corre-
spondence between named entities and their ab-
stract paraphrases, to the sentence level, an 
entire statement of a person and its short para-
phrase. We are now investigating the types of 
co-reference that we have to cover. 
We found that the deletion of lead parts did 
not occur very often in our summary, unlike the 
case of Jing and McKeown (2000). One reason 
is that most of our leads were short enough2 to 
be included in the summary and therefore the 
substitution operation became conspicuous. This 
usually increased the length of summary but 
contributed to making it more lively and infor-
mative. 
A supplement part was often included in the 
summary. We consider that this feature corre-
sponds to the abstractors? comments on em-
ployment of the final sentence, which was not 
clearly detected in our statistical investigation 
described in section 4.3. We are now investigat-
ing the conditions for including the supplement. 
We have so far listed the basic operations of 
editing through the manual checking of samples, 
and we are currently analyzing the operations 
with more examples. We will then study auto-
matic selection of the optimum operation se-
quence to achieve the most informative and 
natural summary. 
6  Conclusions 
We have described the manual summary 
process of NHK?s broadcast news and experi-
ments on automatic text alignment between 
news summaries and the original news together 
                                                          
2 The present summary length constraint is 105 characters. 
Meanwhile, the average length of the first sentence (typi-
cally the lead) of a  news text is 94.5 as is shown in table 1.  
with the word position matching. Through a sta-
tistical analysis of the results and interviews 
with abstractors, we found that the abstractors 
summarize news by taking advantage of its 
structure. Based on this observation, we pro-
posed a summarization model that consists of a 
reading and editing phase. We are now design-
ing an automatic or semi automatic summariza-
tion system employing the model. 
Acknowledgement  
The authors would like to thank Mr. Isao 
Goto and Dr. Naoto Kato of ATR for valuable 
discussion and Mr. Riuzo Waki of Eugene 
Software Inc. for implementing our ideas. 
References 
Jing, Hongyan and Kathleen R. McKeown. 1999. 
The Decomposition of Human-Written Summary 
Sentences. The 22nd Annual International ACM 
SIGIR Conference, pages 129-136, Berkeley. 
Jing, Hongyan and Kathleen R. McKeown. 2000. Cut 
and Paste Based Text Summarization. The 1st 
Meeting of the North American Chapter of the As-
sociation for Computational Linguistics, pages 
178-185, Seattle. 
Kato, Naoto and Noriyoshi Uratani. 2000. Important 
Sentence Selection for Broadcast News (in Japa-
nese), The 6th Annual convention of the Associa-
tion for Natural Language Processing, pages 237-
240, Kanazawa, Japan 
Mani, Inderjeet and Mark T. Maybury. 1999. Ad-
vances in Automatic Summarization, The MIT 
press, Cambridge, Massachusetts 
Mani, Inderjeet, Barbara Gates and Eric Bloedorn. 
1999. Improving Summaries by Revising them, 
The 37th Annual Meeting of the Association for 
Computational Linguisics, pages 558-565, Mary-
land. 
Mani, Inderjeet. 2001. Automatic Summarization. 
John Benjamins, Amsterdam/Philadelphia. 
Marcu, Daniel. 1999. The automatic construction of 
large-scale corpora for summarization research.  
The 22nd Annual International ACM SIGIR Con-
ference, pages 137-144, Berkeley. 
54
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 39?47,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Syntax-Driven Sentence Revision for Broadcast News Summarization 
 
Hideki Tanaka, Akinori Kinoshita, Takeshi Kobayakawa, 
Tadashi Kumano and Naoto Kato 
NHK Science and Technology Research Labs. 
1-10-11, Kinuta, Setagaya-ku, Tokyo, Japan 
{tanaka.h-ja,kinoshita.a-ek,kobayakawa-t.ko,kumano.t-eq,kato.n-ga}@nhk.or.jp 
 
Abstract 
We propose a method of revising lead sentences in 
a news broadcast. Unlike many other methods pro-
posed so far, this method does not use the corefer-
ence relation of noun phrases (NPs) but rather, 
insertion and substitution of the phrases modifying 
the same head chunk in lead and other sentences. 
The method borrows an idea from the sentence 
fusion methods and is more general than those 
using NP coreferencing as ours includes them. We 
show in experiments the method was able to find 
semantically appropriate revisions thus demon-
strating its basic feasibility. We also show that that 
parsing errors mainly degraded the sentential com-
pleteness such as grammaticality and redundancy.  
1 Introduction 
We address the problem of revising the lead sen-
tence in a broadcast news text to increase the 
amount of background information in the lead. 
This is one of the draft and revision approaches 
to summarization, which has received keen atten-
tion in the research community. Unlike many 
other methods that directly utilize noun phrase 
(NP) coreference (Nenkova 2008; Mani et al 
1999), we propose a method that employs inser-
tion and substitution of phrases that modify the 
same chunk in the lead and other sentences. We 
also show its effectiveness in a revision experi-
ment.  
As is well known, the extractive summary that 
has been extensively studied from the early days 
of summarization history (Luhn, 1958) suffers 
from various drawbacks. These include the prob-
lems of a break in cohesion in the summary text 
such as dangling anaphora and a sudden shift in 
topic.  
To ameliorate these problems, the idea of revis-
ing the extracted sentences was proposed in a 
single document summarization study. Jing and 
McKeown (1999; 2000) found that human sum-
marization can be traced back to six cut-and-
paste operations of a text and proposed a revision 
method consisting of sentence reduction and 
combination modules with a sentence extraction 
part. Mani and colleagues (1999) proposed a 
summarization system based on ?draft and revi-
sion? together with sentence extraction. The re-
vision part is achieved with the sentence aggre-
gation and smoothing modules. 
The cohesion break problem becomes particu-
larly conspicuous in multi-document summariza-
tion. To ameliorate this, revision of the extracted 
sentences is also thought to be effective, and 
many ideas and methods have been proposed so 
far. For example, Otterbacher and colleagues 
(2002) analyzed manually revised extracts and 
factored out cohesion problems. Nenkova (2008) 
proposed a revision idea that utilizes noun 
coreference with linguistic quality improvements 
in mind.  
Other than the break in cohesion, multi-
document summarization faces the problem of 
information overlap particularly when the docu-
ment set consists of similar sentences. Barzilay 
and McKeown (2005) proposed an idea called 
sentence fusion that integrates information in 
overlapping sentences to produce a non-
overlapping summary sentence. Their algorithm 
firstly analyzes the sentences to obtain the de-
pendency trees and sets a basis tree by finding 
the centroid of the dependency trees. It next 
augments the basis tree with the sub-trees in oth-
er sentences and finally prunes the predefined 
constituents. Their algorithm was further modi-
fied and applied to the German biographies by 
Filippova and Strube (2008).  
Like the work of Jing and McKeown (2000) and 
Mani et al (1999), our work was inspired by the 
summarization method used by human abstrac-
tors. Actually, our abstractors first extract impor-
tant sentences, which is called lead identification, 
and then revise them, which is referred to as 
phrase elaboration or specification. In this paper, 
we concentrate on the revision part.   
Our work can be viewed as an application of the 
sentence fusion method to the draft and revision 
39
approach to a single Japanese news document 
summarization. Actually, our dependency struc-
ture alignment is almost the same as that of 
Filippova and Strube (2008), and our lead sen-
tence plays the role of a basis tree in the Barzilay 
and McKeown approach (2005). Though the idea 
of sentence fusion was developed mainly for 
suppressing the overlap in multi-document sum-
marization, we consider this effective in aug-
menting the extracts in a single-document sum-
marization task where we face less overlap 
among sentences. 
Before explaining the method in detail, we will 
briefly introduce the Japanese dependency 1  
structure on which our idea is based. The de-
pendency structure is constructed based on the 
bunsetsu chunk, which we call ?chunk? for sim-
plicity. The chunk usually consists of one con-
tent-bearing word and a series of function words. 
All the chunks in a sentence except for the last 
one modify a chunk in the right direction. We 
call the modifying chunk the modifier and the 
modified chunk the head. We usually span a di-
rected edge from a modifier chunk to the head 
chunk 2 . Our dependency tree has no syntactic 
information such as subject or object. 
                                                
2 Broadcast news summarization 
Tanaka et al (2005) showed that most Japanese 
broadcast news texts are written with a three-part 
structure, i.e., the lead, body, and supplement. 
The most important information is succinctly 
mentioned in the lead, which is the opening sen-
tence(s) of a news story, referred to as an ?arti-
cle? here. Proper names and details are some-
times avoided in favor of more abstract expres-
sions such as ?big insurance company.? The lead 
is then detailed in the body by answering who, 
what, when, where, why, and how, and proper 
names only alluded to in the lead appear here. 
Necessary information that was not covered in 
the lead or the body is placed in the supplement.  
The research also reports that professional news 
abstractors who are hired for digital text services 
summarize articles in a two-step approach. First, 
they identify the lead sentences and set it (them) 
as the starting point of the summary. As the av-
erage lead length is 95 characters and the al-
 
1 This is the kakari-uke (modifier-modifiee) relation of 
Japanese, which differs from the conventional dependency 
relation. We use the term dependency for convenience in 
this paper. 
2 This is the other way around compared to the English de-
pendency such as in Barzilay and McKeown (2005).  
lowed summary length is about 115 characters 
(or 150 characters depending on the screen de-
sign), they revise the lead sentences using ex-
pressions from the remainder of the story.   
We see here that the extraction and revision 
strategy that has been extensively studied by 
many researchers for various reasons was actu-
ally applied by human abstractors, and therefore, 
the strategy can be used as a real summarization 
model. Inspired by this, we decided to study a 
news summarization system based on the above 
approach. To develop a complete summarization 
system, we have to solve three problems: 1) 
identifying the lead, body, and supplement struc-
ture in each article, 2) finding the lead revision 
candidates, and 3) generating a final summary by 
selecting and combining the candidates. 
We have already studied problem 1) and showed 
that automatic recognition of three tags with a 
decision tree algorithm reached a precision over 
92% (Tanaka et al 2007). We then moved to 
problem 2), which we discuss extensively in the 
rest of this paper.  
3 Manual lead revision experiment 
To see how problem 2) in the previous section 
could be solved, we conducted a manual lead-
revision experiment. We asked a native Japanese 
speaker to revise the lead sentences of 15 news 
articles using expressions from the body section 
of each article with cut-and-paste operations (in-
sertion and substitution) of bunsetsu chunk se-
quences. We refer to chunk sequences as phrases. 
We also asked the reviser to find as many revi-
sions as possible.  
In the interview with her, we found that she took 
advantage of the syntactic structure to revise the 
lead sentences. Actually, she first searched for 
the ?same? chunks in the lead and the body and 
checked whether the modifier phrases to these 
chunks could be used for revision. To see what 
makes these chunks the ?same,? we compared 
the syntactic head chunk of the lead and body 
phrases used for substitution and insertion. 
Table 1 summarizes the results of the compari-
son in three categories: perfect match, partial 
match (content word match), and different. 
The table indicates that nearly half of the head 
chunks were exactly the same, and the rest con-
tained some differences. The second row shows 
the number where the syntactic heads had the 
same content words but not the same function 
words. The pair ??? kaidan-shi ?talked? and
?????? kaidan-shi-mashi-ta ?talked? is an 
40
  Ins. Sub. Total 
1) Perfect 9 6 15
2) Partial 6 6 12
3) Different 1 6 7
 Total 16 18 34
Lead
IAEA? 
of the IAEA
???? 
the team
??? 
at Korea 
??????
arrived 
Table 1. Degree of syntactic head agreement 
example. These are the  syntactic and aspectual 
variants of the same verb ???? kaidan-suru 
?talk.?  
The third row represents cases where the syntac-
tic heads had no common surface words. We 
found that even in this case, though, the syntactic 
heads were close in some way. In one example, 
there was accordance in the distant heads, for 
instance, in the pair ?????  mitsuka-tta  
?found? and ??? ichibu-no ?part of.? In this 
case, we can find the chunk ????? mit-
suka-tta ?found? at a short edge distance from ?
?? ichibu-no ?part of.?  Based on the findings, 
we devised a lead sentence revision algorithm. 
4 Revision algorithm 
4.1 Concept 
We explain here the concept of our algorithm 
and show an example in Figure 1. We have a 
lead sentence and a body sentence, both of which 
have the ?same? syntactic head chunk, ????
??, touchaku-shima-shi-ta, ?arrived.?  
The head chunk of the lead has two phrases (un-
derlined with thick lines in Figure 1) that directly 
modify the head. We call such a phrase a maxi-
mum phrase of a head3. Like the lead sentence, 
the body sentence also has two maximum phras-
es. In the following part, we use the term phrase 
to refer to a maximum phrase for simplicity. 
By comparing the phrases in Figure 1, we notice 
that the following operations can add useful in-
formation to the lead sentence; 1) inserting the 
first phrase of the body will supply the fact the 
visit was on the 4th, 2) substituting the first 
phrase of the lead with the second one in the 
body adds the detail of the IAEA team. This re-
vision strategy was employed by the human re-
viser mentioned in section 2, and we consider 
this to be effective because our target document 
has a so-called inverse pyramid structure (Robin 
and McKeown 1996), in which the first sentence 
is elaborated by the following sentences. 
                                                 
3 To be more precise, a maximum phrase is defined as the 
maximum chunk sequence on a dependency path of a head. 
 
 
Figure 1. Concept of revision algorithm 
Further analyzing the above fact, we devised the 
lead sentence revision algorithm below. We pre-
sent the outline here and discuss the details in the 
next section. We suppose an input pair of a lead 
and a body sentence that are syntactically ana-
lyzed. 
1) Trigger search 
We search for the ?same? chunks in the lead 
and body sentences. We call the ?same? 
chunks triggers as they give the starting point 
to the revision. 
2) Phrase alignment 
We identify the maximum phrases of each 
trigger, and these phrases are aligned according 
to a similarity metric. 
3) Substitution 
If a body phrase has a corresponding phrase in 
the lead, and the body phrase is richer in in-
formation, we substitute the body phrase for 
the lead phrase.  
4) Insertion 
If a body phrase has no counterpart in the lead, 
that is, the phrase is floating, we insert it into 
the lead sentence. 
Our method inserts and substitutes any type of 
phrase that modifies the trigger and therefore has 
no limitation in syntactic type. Although NP 
elaboration such as in (Nenkova 2008) is of great 
importance, there are other useful syntactic types 
for revision. An example is the adverbial phrase 
insertion of time and location. The insertion of 
the phrase 4? yokka ?on the 4th? in figure 1 in-
deed adds useful information to the lead sentence.     
4.2 Algorithm 
The overall flow of the revision algorithm is 
shown in Algorithm 1. The inputs are a lead and 
a body sentence that are syntactically parsed, 
which are denoted by L and B respectively. 
The whole algorithm starts with the all-trigger 
search in step 1. Revision candidates are then 
found for each trigger pair in the main loop from 
steps 2 to 6.  The revision for each trigger pair is  
IAEA? 
of the IAEA 
??? 
inspectors 
??????
arrived
5?? 
five 
Body
4?? 
on the 4th
insertion substitution
maximum phrase 
41
Algorithm 14 (Left figures are the step numbers.) 
1: find all trigger pairs between L and B and  
              store them in T.  
 T={(l, b) ; l b, l?L and b?B } ?
2: for all (l, b) ? T do 
  find l?s max phrases and store in Pl. 
  Pl={pl ; pl ? max phrase of l} 
3:  do the same for trigger b 
  Pb={pb ; pb ? max phrase of b} 
4:  align phrases in Pl and Pb and store  
   result in A 
   A={( pl, pb) ; pl  pb ,  ?
     pl ? Pl, pb ? Pb } 
5:  for all (pl, pb) ? A do 
   follow Table 2 
  end for 
6: end for 
 
Body  
pb =?  pb? ?  
pl =?  4: no op. 1: insertion Lead 
pl  ? ? 3: no op. 2: substitution 
Table 2. Operations for step 5 
found based on the idea in the previous section in 
steps 4 and 5. Now we explain the main parts. 
? Step 1: trigger chunk pair search  
We first detect the trigger pairs in step 1 that are 
the base of the revision process. What then can 
be a trigger pair that yields correct revisions? We 
roughly define trigger pairs as the ?coreferential? 
chunk pairs of all parts of speech, i.e., the parts 
of speech that point to the same entity, event, 
action, change, and so on.  
Notice that the term coreferential is used in an 
extended way as it is usually used to describe the 
phenomena in noun group pairs (Mitkov, 2002).  
The chunk ?????? touchaku-shimashita 
?arrived? and IAEA? IAEA-no ?of the IAEA? 
in Figure 1 are examples.  
Identifying our coreferential chunks is even 
harder than the conventional coreference resolu-
tion, and we made a simplifying assumption as in 
Nenkova (2008) with some additional conditions 
that were obtained through our preliminary ex-
periments.  
(1) Assumption: Two chunks having the same 
surface forms are coreferential. 
(2) Conditions for light verb (noun) chunks: 
Agreement of modifying verbal nous is fur-
                                                 ??
4 The sign a b means the chunk ?a? and ?b? are triggers.  
The sign p q means the phrases ?p? and ?q? are aligned. 
ther required for chunks whose content 
words consist only of light verbs such as ?
? aru ?be? and ?? naru ?become?: these 
chunks themselves have little lexical mean-
ing. The agreement is checked with the 
hand-crafted rules. Similar checks are ap-
plied to chunks whose content words consist 
only of light nouns such as ?? koto (?koto? 
makes the previous verb a noun) . 
(3) Conditions for verb inflections: a chunk that 
contains a verb usually ends with a function 
word series that indicates a variety of infor-
mation such as inflection type, dependency 
type, tense, aspect, and modality. Some in-
formation such as tense and aspect is vital to 
decide the coreference relation (exchanging 
the modifier phrases ?arrive? and ?will ar-
rive? will likely bring about inconsistency in 
meaning), although some is not. We are in 
the process of categorizing function words 
that do not affect the coreference relation and 
temporally adopted the empirically obtained 
rule: the difference in verb inflection be-
tween the te-form (predicate modifying 
form) and dictionary form (sentence end 
form) can be ignored.     
? Step 4: phrase alignment 
We used the surface form agreement for similar-
ity evaluation. We applied several metrics and 
explain them one by one. 
1) Chunk similarity t, s 
t, s : x, y? chunk [0, 1]. ?
Function t is the Dice coefficient between the 
set of content words in x and those in y. The 
same coefficient calculated with all words 
(function and content words) is denoted as s. 
2) Phrase absorption ratio 
a : px, py? phrases  [0, 1] ?
This is the function that indicates how many 
chunks in phrase px is represented in py and is 
calculated with t as in, 
?
? ?
=
x
ypx
py
x
yx yxtp
ppa )),((max
1
:),( . 
3) Alignment  quality 
With the above two functions, the alignment qual-
lity is evaluated by the function 
g : px, py ? phrases ?  [0, 1] 
],1,0[
),,()1(),(:),(
?
?+=
?
?? yxsppappg yxyx  
where the shorter phrase is set to px so that 
yx pp < . The variables x and y are the last 
42
chunks in px and py, respectively. Intuitively, 
the function evaluates how many chunks in the 
shorter phrase px are represented in py and how 
similar the last chunks are. The last chunk in a 
phrase, especially the function words in the 
chunk, determines the syntactic character of 
the phrase, and we measured this value with 
the second term of the alignment quality. The 
parameter ? is decided empirically, which was 
set at 0.375 in this paper. 
In alignment, we calculated the score for all 
possible phrase combinations and then greed-
ily selected the pair with the highest score. We 
set the minimum alignment score at 0.185; 
those pairs with scores lower than this value 
were not aligned. 
? Step 5 (Table 2, case 1): insertion 
Step 5 starts either an insertion or substitution 
process, as in Table 2. If pb  (body phrase is 
not null) and pl =  (lead phrase is null) in Table 
2, the insertion process starts.  
? ?
?
In this process, we check the following.  
1) Redundancy check 
Insertion may cause redundancy in informa-
tion. As a matter of fact, redundancy often 
happens when there is an error in syntactic 
analysis. Suppose there are the same lead and 
body phrases that modify the same chunks in 
the lead and body sentences. If the lead phrase 
fails to modify the correct chunk because of an 
error, the body phrase loses the chance to be 
aligned to the lead phrase since they belong to 
different trigger chunks. As a result, the body 
phrase becomes a floating phrase and is in-
serted into the lead chunk, which duplicates 
the same phrase.  
To prevent this, we evaluate the degree of du-
plication with the phrase absorption ratio a 
and allow phrase insertion when the score is 
below a predefined threshold ? : we allow in-
sertion when 
? ),( bb pLpa < ,? phrase, L : lead sentence, 
is satisfied. 
2) Discourse coherence check 
Blind phrase insertion may invite a break in 
cohesion in a lead sentence.  This frequently 
happens when the inserted phrase has words 
that require an antecedent. We then prepared a 
list of words that contain such context-
requiring words and forbid phrase insertions 
that contain words that are on the list.  This list 
contains the pronoun family such as ?? ko-
kono ?this? and special adjectives such as ?? 
chigau ?different.?  
3) Insertion point decision 
The body phrase should be inserted at the 
proper position in the lead sentence to main-
tain the syntactic consistency. Because we 
dealt with single-phrase insertion here, we 
employed a simple heuristics.  
Since the Japanese dependency edge spans 
from left to right as we mentioned in section 1, 
we considered that the right phrase of the in-
serted phrase is important to keep the new de-
pendency from the inserted phrase to the trig-
ger chunk. Because we already know the 
phrase alignment status at this stage, we fol-
low the next steps to determine the insertion 
position in the lead of the insertion phrase. 
A) In the body sentence, find the nearest right 
substitution phrase pr of the insertion 
phrase. 
B) Find the pr?s aligned phrase in the lead prL. 
C) Insert the phrase to the left of the prL. 
D) If there is no pr, insert the phrase to the left 
to the trigger. 
? Step 5 (Table 2, case 2): substitution 
If pb ?  ?  and pl ? ?  in Table 2, the substitu-
tion process starts. This process first checks if 
each aligned phrase pair contains the same chunk 
other than the present trigger. If there is such a 
chunk, the substitution phrase is reduced to the 
subtree from the present trigger to the identical 
chunk. The newly found identical chunks are in 
trigger table T, and the remaining part will be 
evaluated later in the main loop. Owing to the 
phrase partitioning, we can avoid phrase substi-
tutions which are in an inclusive relation.  
The substitution candidate goes through three 
checks: information increase, redundancy, and 
discourse cohesion. As the latter two are almost 
the same as those in the insertion, we explain 
here the information increase. This involves 
checking whether the number of chunks in the 
body phrase is greater than that in the aligned 
lead phrase. This is based on the simple assump-
tion that elaboration requires more words. 
5 Revision experiments 
5.1 Data and evaluation steps 
? Purpose 
We conducted a lead revision experiment with 
three purposes. The first one was to empirically 
evaluate the validity of our simplified assump-
43
tions: trigger identification and concreteness in-
crease evaluation. For trigger identification, we 
basically viewed the identical chunks as triggers 
and added some amendments for light verbs 
(nouns) and verb inflections. For the check of an 
increase in concreteness, we assumed that 
phrases with more chunks were more concrete. 
However, these simplifications should be veri-
fied in experiments. 
The second purpose was to check the validity of 
using the revision phrases only in body sentences 
and not in the supplemental sentences. 
The last one was to determine how ineffective 
the result is if the syntactic parsing fails. With 
these purposes in mind, we designed our experi-
ment as follows.  
? Data  
A total of 257 articles from news programs 
broadcast on 20 Jan., 20 Apr., and 20 July in 
2004 were tagged with lead, body, and supple-
ment tags by a native Japanese evaluator. The 
articles were morphologically analyzed by Me-
cab (Kudo et al, 2003) and syntactically parsed 
by Cabocha (Kudo and Matsumoto, 2002). 
? Evaluator and evaluation detail 
We prepared an evaluation interface that presents 
a lead with one revision point (insertion or sub-
stitution) that was obtained using the body and 
supplemental sentences to an evaluator. 
A Japanese native speaker evaluated the results 
one by one with the above interface. We planned 
a linguistic evaluation like DUC2005 (Hoa Trang, 
2005). Since their five-type evaluation is in-
tended for multi-document summarization, 
whereas our task is single-document summariza-
tion, and we are interested in evaluating our 
questions mentioned above, we carried out the 
evaluation as follows. In future, we plan to in-
crease the number of evaluation items and the 
number of evaluators.  
Concreteness Score 
Decreased 0 
Unchanged 1 
Increased 2 
Table 3. Evaluation of increased concreteness 
Completeness Required operations Score
Poor More than 2 0
Acceptable One 1
Perfect None 2
Table 4. Sentential completeness 
 
E1) The evaluator judged if the revision was ob-
tained from the lead and body sentences with 
or without parsing errors. Here, errors that did 
not affect the revision were not considered.  
E2) Second, she checked whether the revision 
was semantically correct or revised informa-
tion matching the fact described in the lead 
sentence. Here, she did not care about the 
grammaticality or the improvements in con-
creteness of the revision; if the revision was 
problematic but manually correctable, it was 
judged as OK. This step evaluated the correct-
ness of the trigger selection; wrong triggers, 
i.e., those referring to different facts produce 
semantically inconsistent revisions as they mix 
up different facts. 
The following evaluation was done for those 
judged correct in evaluation step E2, as we found 
that revisions that were semantically inconsistent 
with the lead?s facts were often too difficult to 
evaluate further.  
E3) Third, she evaluated the change in concrete-
ness after revision with the revisions that 
passed evaluation E2. She judged whether or 
not the revision increased the concreteness of 
the lead in three categories (Table 3). 
Notice that original lead sentences are sup-
posed to have an average score of 1. 
E4) Last, she checked the sentential complete-
ness of the revision result that passed evalua-
tion E2. They still contained problems such as 
grammatical errors and improper insertion po-
sition. Rather than evaluating these items sepa-
rately, we measured them together for senten-
tial completeness. At this time, we measured in 
terms of the number of operations (insertion, 
deletion, substitution) needed to make the sen-
tence complete5.  
As shown in Table 4, revisions requiring more 
than two operations are categorized as ?poor,? 
those requiring one operation are ?acceptable,? 
and those requiring no operations are ?perfect.? 
We employed this measure because we found 
that grading detailed items such as grammatical-
ity and insertion positions at fine levels was 
rather difficult. We also found that native Japa-
nese speakers can correct errors easily. Notice 
the lead sentences are perfect and are supposed 
                                                 
5 This was not an automatic process and may not be perfect. 
The evaluator simulated the correction in mind and judged 
whether it was done with one action. 
44
to have an average score of 2 in sentential com-
pleteness. Since the revision does not improve 
the completeness further but elicits defects such 
as grammatical errors, it usually produces a score 
below 2. Some examples of the results with their 
scores are shown below. The underlined parts are 
the inserted body chunk phrases, and the paren-
thesized parts are the deleted lead chunks. 
1) Concreteness 2, Completeness 2 
?????????????????
?????????????????
????????????? 
minkan-dantai-no ?private 
organization?, korea-
society-nado-ga ?Korea Soci-
ety and others?, shusai-suru  
?sponsored?, chousen-hantou-
heiwa-forumu-ni  ?Peace Fo-
rum in Korean Peninsula?, 
(moyooshi-ni ?event?), 
shusseki-suru ?attend? 
2) Concreteness 1, Completeness 2 
?????????????????
???? 
buhin-ni ?to the parts? ki-
retsu-ga ?cracks?, haitte-
iru-no-ga ?being there? (), 
mitsuka-tta ?found? 
3) Concreteness 2, Completeness 0 
?????????????????
???????????????? 
Herikoputa-kara ?from a hel-
icopter?, chijou-niju-
metoru-no-takasa-kara ?from 
20 meters high? (), rakka-
shi ?fell and?, shibou-
shima-shita ?killed? 
Example 1 is the perfect substitution and had 
scores of 2 for both concreteness increase and 
completeness. Actually, the originally vaguely 
mentioned term ?event? was replaced by a more 
concrete phrase with proper names, ?Korean Pen-
insula Peace Forum sponsored by Korea Society 
and others.? Notice that this can be achieved by 
NP coreference based methods if they can iden-
tify that these two different phrases are corefer-
ential. Our method does this through the depend-
ency on the same trigger ???? shusseki-suru 
?attend.? 
Example 2 is a perfect sentence, but its concrete-
ness stayed at the same level. As a result, the 
scores were 1 for concreteness increase and 2 for 
completeness. 
 Incorrect Correct Cor. Ratio 
Succ. 70 353 0.83Parse
Fail. 31 149 0.83
Body 50 464 0.90Sent.
Supp. 51 38 0.43
Table 5. Results of semantic correctness 
Score 0 1 2 Ave.
Succ. 0 55 298 1.84Parse
Fail. 1 19 129 1.86
Body 1 61 402 1.86Sent.
Supp. 0 13 25 1.66
Table 6. Results of concreteness increase 
Score 0 1 2 Ave.
Succ. 78 60 215 1.39Parse
Fail. 66 55 28 0.74
Body 120 110 234 1.25Sent.
Supp. 24 5 9 0.61
Table 7. Results of sentential completeness 
Actually, the original sentence that meant ?They 
found a crack in the parts? was revised to ?They 
found there was a crack in the parts,? which did 
not add useful information. Example 3 has a 
grammatical problem although the revision sup-
plied useful information.As a result, it had scores 
of 2 for concreteness increase and 0 for com-
pleteness. The added kara-case phrase (from 
phrase) ????????????? chijou-
niju-metoru-no-takasa-kara ?from 20 meters 
high? is useful, but since the original sentence 
already has the kara-case ???????? 
herikoputa-kara ?from helicopter,? the insertion 
invited a double kara-case, which is forbidden in 
Japanese. To correct the error, we need at least 
two operations, and thus, a completeness score of 
0 was assigned. 
5.2 Results of experiments 
Table 5 presents the results of evaluation E2, the 
semantic correctness with the parsing status of 
evaluation E1 and the source sentence category 
from which the phrases for revision were ob-
tained. Columns 2 and 3 list the number of revi-
sions (insertions and substitutions) that were cor-
rect and incorrect and column 4 shows the cor-
rectness ratio. We obtained a total of 603 revi-
sions and found that 30% (180/603) of them 
were derived with syntactic errors. 
The semantic correctness ratio was unchanged 
regardless of the parsing success. On the contrary, 
it was affected by the source sentence type. The 
correctness ratio with the supplemental sentence 
45
was significantly6 lower than that with the body 
sentence. Table 6 lists the results of the con-
creteness improvements with the parsing status 
and the source sentence type. Columns 2, 3 and 4 
list the number of revisions that fell in the scores 
(0-2) listed in the first row. The average score in 
this table again was not affected by the parsing 
failure but was significantly affected by the 
source sentence category. The result with the 
supplement sentences was significantly worse 
than that with body sentences. 
Table 7 lists the results of the sentential com-
pleteness in the same fashion as Table 6. The 
sentential completeness was significantly wors-
ened by both the parsing failure and source sen-
tence category.  
These results indicate that the answers to the 
questions posed at the beginning of this section 
are as follows. From the semantic correctness 
evaluation, we infer that our trigger selection 
strategy worked well especially when the source 
sentence category was limited to the body.  
From the concreteness-increase evaluation, the 
assumption that we made also worked reasonably 
well when the source sentence category was lim-
ited to the body.  
The effect of parsing was much more limited 
than we had anticipated in that it did not degrade 
either the semantic correctness or the concrete-
ness improvements. Parsing failure, however, 
degraded the sentential completeness of the re-
vised sentences. This seems quite reasonable: 
parsing errors elicit problems such as wrong 
phrase attachment and wrong maximum phrase 
identification. The revisions with these errors 
invite incomplete sentences that need corrections.  
It is worth noting that cases sometimes occurred 
where a parsing error did not cause any problem 
in the revision. We found that the phrases gov-
erned by a trigger pair in many cases were quite 
similar, and therefore, the parser makes the same 
error. In that case, the errors are often offset and 
cause no problems superficially. 
We consider that the sentential completeness 
needs further improvements to make an auto-
matic summarization system, although the se-
mantic correctness and concreteness increase are 
at an almost satisfactory level. Our dependency-
based revision is expected to be potentially use-
ful to develop a summarization system. 
                                                 
6 In this section, the ?significance? was tested with the 
Mann-Whitney U test with Fisher?s exact probability. We 
set the significance level at 5%.  
6 Future work  
Several problems remain to be solved, which will 
be addressed in future work. Obviously, we need 
to improve the parsing accuracy that degraded 
the sentential completeness in our experiments. 
Although we did not quantitatively evaluate the 
errors in phrase insertion position and redun-
dancy, we could see these happening in the re-
vised sentences because of the inaccurate parsing. 
Apart from this, we need to further refine the 
following problems.  
Regarding the trigger selection, one particular 
problem we faced was the mixture of statements 
of different politicians in a news article. The 
statements were often included as direct quota-
tions that end with the chunk ????? nobe-
mashi-ta ?said.? Our system takes the chunk as 
the trigger and does not care whose statements 
they are; thus, it ended up mixing them up. A 
similar problem happened when we had two dif-
ferent female victims of an incident in an article. 
Since our system has no means to distinguish 
them, the modifier phrases about these women 
were mixed up. 
We think that we can improve our method by 
applying more general language generation tech-
niques. An example is the kara-case collision that 
we explained in example 3 in section 5.1. The 
essence of the problem is that the added content 
is useful, but there is a grammatical problem. In 
other words, ?what to say? is ok but ?how to 
say? needs refinement. This particular problem 
can be solved by doing the case-collision check, 
and by synthesizing the colliding phrases into 
one. These can be better treated in the generation 
framework.  
7 Conclusion 
We proposed a lead sentence revision method 
based on the operations of phrases that have the 
same head in the lead and other sentences.  This 
method is a type of sentence fusion and is more 
general than methods that use noun phrase 
coreferencing in that it can add phrases of any 
syntactic type. We described the algorithm and 
the rules extensively, conducted a lead revision 
experiment, and showed that the algorithm was 
able to find semantically appropriate revisions. 
We also showed that parsing errors mainly de-
grade the sentential completeness such as gram-
maticality and repetition. 
 
46
Reference 
Regina Barzilay and Kathleen R. McKeown. 2005. 
Sentence Fusion for Multidocument News Summa-
rization. Computational Linguistics. 31(3): 298-
327. 
Katja Filippova and Michael Strube. 2008. Sentence 
Fusion via Dependency Graph Compression.  proc. 
of the EMNLP 2008: 177-185 
Hongyan Jing and Kathleen R. McKeown. 1999. The 
Decomposition of Human-Written Summary Sen-
tences. proc. of the 22nd International Conference 
on Research and Development in Information Re-
trieval  SIGIR 99: 129-136. 
Hongyan Jing and Kathleen R. McKeown. 2000. Cut 
and Paste Based Text Summarization, proc. of the 
1st meeting of the North American Chapter of the 
Association for Computational Linguistics: 178-
185. 
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. Proc. 
of the 6th Conference on Natural Language Learn-
ing 2002: 63-69. 
Taku Kudo, Kaoru Yamamoto and Yuji Matsumoto. 
2004. Applying Conditional Random Fields to Jap-
anese Morphological Analysis, proc. of the 
EMNLP 2004: 230-237. 
H. P. Luhn. 1958. The Automatic Creation of Litera-
ture Abstracts. Advances in Automatic Text Sum-
marization. The MIT Press: 15-21. 
Inderjeet Mani, Barbara Gates, and Eric Bloedorn.  
1999. Improving Summaries by Revising Them. 
Proc. of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics.: 558-565. 
Ruslan Mitkov 2002, Anaphora Resolution, Pearson 
Education. 
Ani Nenkova. 2008. Entity-driven Rewrite for Multi-
document Summarization, proc. of the 3rd Interna-
tional Joint Conference on Natural Language Gen-
eration: 118-125. 
Jahna C. Otterbacher, Dragomir R. Radev, and Airong 
Luo 2002, Revisions that Improve Cohesion in 
Multi-document Summaries: A Preliminary Study. 
Proc. of the ACL-02 Workshop on Automatic 
Summarization: 27-36. 
Jacques Robin and Kathleen McKeown. 1996. Em-
pirically designing and evaluating a new revision-
based model for summary generation. Artificial In-
telligence.  85: 135-179. 
 
47
 
	