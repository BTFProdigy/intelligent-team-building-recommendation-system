Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 54?61,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Participant Subjectivity and Involvement as a Basis for Discourse
Segmentation
John Niekrasz and Johanna Moore
Human Communication Research Centre
School of Informatics
University of Edinburgh
{jniekras,jmoore}@inf.ed.ac.uk
Abstract
We propose a framework for analyzing
episodic conversational activities in terms
of expressed relationships between the
participants and utterance content. We
test the hypothesis that linguistic features
which express such properties, e.g. tense,
aspect, and person deixis, are a useful ba-
sis for automatic intentional discourse seg-
mentation. We present a novel algorithm
and test our hypothesis on a set of inten-
tionally segmented conversational mono-
logues. Our algorithm performs better
than a simple baseline and as well as or
better than well-known lexical-semantic
segmentation methods.
1 Introduction
This paper concerns the analysis of conversations
in terms of communicative activities. Examples of
the kinds of activities we are interested in include
relating a personal experience, making a group de-
cision, committing to future action, and giving in-
structions. The reason we are interested in these
kinds of events is that they are part of partici-
pants? common-sense notion of the goals and ac-
complishments of a dialogue. They are part of par-
ticipants? subjective experience of what happened
and show up in summaries of conversations such
as meeting minutes. We therefore consider them
an ideal target for the practical, common-sense de-
scription of conversations.
Activities like these commonly occur as cohe-
sive episodes of multiple turns within a conver-
sation (Korolija, 1998). They represent an inter-
mediate level of dialogue structure ? greater than
a single speech act but still small enough to have
a potentially well-defined singular purpose. They
have a temporal granularity of anywhere from a
few seconds to several minutes.
Ultimately, it would be useful to use descrip-
tions of such activities in automatic summariza-
tion technologies for conversational genres. This
would provide an activity-oriented summary de-
scribing what ?happened? that would complement
one based on information content or what the con-
versation was ?about?. Part of our research goal is
thus to identify a set of discourse features for seg-
menting, classifying, and describing conversations
in this way.
1.1 Participant subjectivity and involvement
The approach we take to this problem is founded
upon two basic ideas. The first is that the activities
we are interested in represent a coarse level of the
intentional structure of dialogue (Grosz and Sid-
ner, 1986). In other words, each activity is unified
by a common purpose that is shared between the
participants. This suggests there may be linguis-
tic properties which are shared amongst the utter-
ances of a given activity episode.
The second idea concerns the properties which
distinguish different activity types. We propose
that activity types may be usefully distinguished
according to two complex properties of utterances,
both of which concern relationships between the
participants and the utterance: participant sub-
jectivity and participant involvement. Participant
subjectivity concerns attitudinal and perspectival
relationships toward the dialogue content. This
includes properties such as whether the utterance
expresses the private mental state of the speaker,
or the participants? temporal relationship to a de-
scribed event. Participant involvement concerns
the roles participants play within the dialogue con-
54
tent, e.g., as the agent of a described event.
1.2 Intentional segmentation
The hypothesis we test in this paper is that the
linguistic phenomena which express participant-
relational properties may be used as an effective
means of intentional discourse segmentation. This
is based on the idea that if adjacent discourse seg-
ments have different activity types, then they are
distinguishable by participant-relational features.
If we can reliably extract such features, then this
would allow segmentation of the dialogue accord-
ingly.
We test our hypothesis by constructing an algo-
rithm and examining its performance on an exist-
ing set of intentionally segmented conversational
monologues (i.e., one person speaks while another
listens) (Passonneau and Litman, 1997, henceforth
P&L). While our long term goal is to apply our
techniques to multi-party conversations (and to
a somewhat coarser-grained analysis), using this
dataset is a stepping-stone toward that end which
allows us to compare our results with existing in-
tentional segmentation algorithms.
An example dialogue extract from the dataset
is shown in Dialogue 1. Two horizontal lines in-
dicate a segment boundary which was identified
by at least 3 of 7 annotators. A single horizon-
tal line indicates a segment boundary which was
identified by 2 or fewer annotators. In the exam-
PearStories-09 (Chafe, 1980)
21.2 okay.
22.1 Meanwhile,
22.2 there are three little boys,
22.3 up on the road a little bit,
22.4 and they see this little accident.
23.1 And u-h they come over,
23.2 and they help him,
23.3 and you know,
23.4 help him pick up the pears and everything.
24.1 A-nd the one thing that struck me about the- three
little boys that were there,
24.2 is that one had ay uh I don?t know what you call
them,
24.3 but it?s a paddle,
24.4 and a ball-,
24.5 is attached to the paddle,
24.6 and you know you bounce it?
25.1 And that sound was really prominent.
26.1 Well anyway,
26.2 so- u-m tsk all the pears are picked up,
26.3 and he?s on his way again,
Dialogue 1: An example dialogue extract showing
intentional segment boundaries.
ple, there are three basic types of discourse activity
distinguishable according to the properties of par-
ticipant subjectivity and participant involvement.
The segments beginning at 22.1 and 26.2 share the
use of the historical present tense ? a type of par-
ticipant subjectivity ? in a narrative activity type.
Utterances 24.1 and 25.1, on the other hand, are
about the prior perceptions of the speaker, a type
of participant involvement in a past event. The
segment beginning at 24.2 is a type of generic de-
scription activity, exhibiting its own distinct con-
figuration of participant relational features, such
as the generic you and present tense.
We structure the rest of the paper as follows.
First, we begin by describing related and support-
ing theoretical work. This is followed by a test of
our main hypothesis. We then follow this with a
similar experiment which contextualizes our work
both theoretically and in practical terms with re-
spect to the most commonly studied segmentation
task: topic segmentation. We finish with a general
discussion of the implications of our experiments.
2 Background and Related Work
The influential work of Grosz and Sidner (1986)
provides a helpful starting point for understand-
ing our approach. Their theory suggests that in-
tentions (which equate to the goals and purposes
of a dialogue) are a foundation for the structure of
discourse. The individual discourse purposes that
emerge in a dialogue relate directly to the natural
aggregation of utterances into discourse segments.
The attentional state of the dialogue, which con-
tains salient objects and relations and allows for
the efficient generation and interpretation of utter-
ances, is then dependent upon this interrelated in-
tentional and linguistic structure in the emerging
dialogue.
Grosz and Sidner?s theory suggests that atten-
tional state is parasitic upon the underlying inten-
tional structure. This implication has informed
many approaches which relate referring expres-
sions (an attentional phenomenon) to discourse
structure. One example is Centering theory (Grosz
et al, 1995), which concerns the relationship of
referring expressions to discourse coherence. An-
other is P&L, who demonstrated that co-reference
and inferred relations between noun phrases are
a useful basis for automatic intentional segmen-
tation.
Our approach expands on this by highlighting
55
the fact that objects that are in focus within the
attentional state have an important quality which
may be exploited: they are focused upon by the
participants from particular points of view. In ad-
dition, the objects may in fact be the participants
themselves. We would expect the linguistic fea-
tures which express such relationships (e.g., as-
pect, subjectivity, modality, and person deixis) to
therefore correlate with intentional structure, and
to do so in a way which is important to partici-
pants? subjective experience of the dialogue.
This approach is supported by a theory put forth
by Chafe (1994), who describes how speakers can
express ideas from alternative perspectives. For
example, a subject who is recounting the events of
a movie of a man picking pears might say ?the man
was picking pears?, ?the man picks some pears?,
or ?you see a man picking pears.? Each variant is
an expression of the same idea but reflects a dif-
ferent perspective toward, or manner of participa-
tion in, the described event. The linguistic vari-
ation one sees in this example is in the proper-
ties of tense and aspect in the main clause (and in
the last variant, a perspectival superordinate clause
which uses the generic you). We have observed
that discourse coheres in these perspectival terms,
with shifts of perspective usually occurring at in-
tentional boundaries.
Wiebe (1994; 1995) has investigated a phe-
nomenon closely related to this: point-of-view
and subjectivity in fictional narrative. She notes
that paragraph-level blocks of text often share a
common objective or subjective context. That
is, sentences may or may not be conveyed from
the point-of-view of individuals, e.g., the author
or the characters within the narrative. Sentences
continue, resume, or initiate such contexts, and
she develops automatic methods for determining
when the contexts shift and whose point-of-view
is being taken. Her algorithm provides a de-
tailed method for analyzing written fiction, but
has not been developed for conversational or non-
narrative genres.
Smith?s (2003) analysis of texts, however,
draws a more general set of connections between
the content of sentences and types of discourse
segments. She does this by analyzing texts at
the level of short passages and determines a non-
exhaustive list of five basic ?discourse modes? oc-
curring at that level: narrative, description, report,
information, and argument. The mode of a pas-
sage is determined by the type of situations de-
scribed in the text (e.g., event, state, general sta-
tive, etc.) and the temporal progression of the sit-
uations in the discourse. Situation types are in
turn organized according to the perspectival prop-
erties of aspect and temporal location. A narrative
passage, for example, relates principally specific
events and states, with dynamic temporal advance-
ment of narrative time between sentences. On the
other hand, an information passage relates primar-
ily general statives with atemporal progression.
3 Automatic Segmentation Experiment
The analysis described in the previous sections
suggests that participant-relational features corre-
late with the intentional structure of discourse. In
this section we describe an experiment which tests
the hypothesis that a small set of such features, i.e.,
tense, aspect, and first- and second-person pro-
nouns, are a useful basis for intentional segmen-
tation.
3.1 Data
Our experiment uses the same dataset as P&L, a
corpus of 20 spoken narrative monologues known
as the Pear Stories (Chafe, 1980). Chafe asked
subjects to view a silent movie and then sum-
marize it for a second person. Their speech
was then manually transcribed and segmented into
prosodic phrases. This resulted in a mean 100
phrases per narrative and a mean 6.7 words per
phrase. P&L later had each narrative segmented
by seven annotators according to an informal defi-
nition of communicative intention. Each prosodic
phrase boundary was a possible discourse segment
boundary. Using Cochran?s Q test, they concluded
that an appropriate gold standard could be pro-
duced by using the set of boundaries assigned by
at least three of the seven annotators. This is the
gold standard we use in this paper. It assigns a
boundary at a mean 16.9% (? = 4.5%) of the pos-
sible boundary sites in each narrative. The result is
a mean discourse segment length of 5.9 prosodic
phrases, (? = 1.4 across the means of each narra-
tive).
3.2 Algorithm
The basic idea behind our algorithm is to distin-
guish utterances according to the type of activ-
ity in which they occur. To do this, we iden-
tify a set of utterance properties relating to par-
56
ticipant subjectivity and participant involvement,
according to which activity types may be distin-
guished. We then develop a routine for automati-
cally extracting the linguistic features which indi-
cate such properties. Finally, the dialogue is seg-
mented at locations of high discontinuity in that
feature space. The algorithm works in four phases:
pre-processing, feature extraction, similarity mea-
surement, and boundary assignment.
3.2.1 Pre-processing
For pre-processing, disfluencies are removed by
deleting repeated strings of words and incomplete
words. The transcript is then parsed (Klein and
Manning, 2002), and a collection of typed gram-
matical dependencies are generated (de Marneffe
et al, 2006). The TTT2 chunker (Grover and To-
bin, 2006) is then used to perform tense and aspect
tagging.
3.2.2 Feature extraction
Feature extraction is the most important and
novel part of our algorithm. Each prosodic phrase
(the corpus uses prosodic phrases as sentence-like
units, see Data section) is assigned values for five
binary features. The extracted features correspond
to a set of utterance properties which were iden-
tified manually through corpus analysis. The first
four relate directly to individual activity types and
are therefore mutually exclusive properties.
first-person participation [1P] ? helps to distin-
guish meta-discussion between the speaker
and hearer (e.g., ?Did I tell you that??)
generic second-person [2P-GEN] ? helps to dis-
tinguish narration told from the perspective
of a generic participant (e.g., ?You see a man
picking pears?)
third-person stative/progressive [3P-STAT]
? helps to distinguish narrative activities
related to ?setting the scene? (e.g., ?[There is
a man | a man is] picking pears?)
third-person event [3P-EVENT] ? helps to dis-
tinguish event-driven third-person narrative
activities (e.g. ?The man drops the pears?)
past/non-past [PAST] ? helps to distinguish nar-
rative activities by temporal orientation (e.g.
?The man drops the pears? vs. ?The man
dropped the pears?)
Feature extraction works by identifying the lin-
guistic elements that indicate each utterance prop-
erty. First, prosodic phrases containing a first- or
second-person pronoun in grammatical subject or
object relation to any clause are identified (com-
mon fillers like you know, I think, and I don?t know
are ignored). Of the identified phrases, those with
first-person pronouns are marked for 1P, while the
others are marked for 2P-GEN. For the remain-
ing prosodic phrases, those with a matrix clause
are identified. Of those identified, if either its
head verb is be or have, it is tagged by TTT2 as
having progressive aspect, or the prosodic phrase
contains an existential there, then it is marked for
3P-STAT. The others are marked for 3P-EVENT.
Finally, if the matrix clause was tagged as past
tense, the phrase is marked for PAST. In cases
where no participant-relational features are iden-
tified (e.g., no matrix clause, no pronouns), the
prosodic phrase is assigned the same features as
the preceding one, effectively marking a continua-
tion of the current activity type.
3.2.3 Similarity measurement
Similarity measurement is calculated according
to the cosine similarity cos(vi, ci) between the fea-
ture vector vi of each prosodic phrase i and a
weighted sum ci of the feature vectors in the pre-
ceding context. The algorithm requires a parame-
ter l to be set for the desired mean segment length.
This determines the window w = floor(l/2) of
preceding utterances to be used. The weighted
sum representing the preceding context is com-
puted as ci =
?w
j=1((1 + w ? j)/w)vi?j , which
gives increasingly greater weight to more recent
phrases.
3.2.4 Boundary assignment
In the final step, the algorithm assigns bound-
aries where the similarity score is lowest, namely
prior to prosodic phrases where cos is less than the
first 1/l quantile for that discourse.
3.3 Experimental Method and Evaluation
Our experiment compares the performance of
our novel algorithm (which we call NM09) with
a naive baseline and a well-known alternative
method ? P&L?s co-reference based NP algorithm.
To our knowledge, P&L is the only existing publi-
cation describing algorithms designed specifically
for intentional segmentation of dialogue. Their
NP algorithm exploits annotations of direct and
57
inferred relations between noun phrases in adja-
cent units. Inspired by Centering theory (Grosz
et al, 1995), these annotations are used in a com-
putational account of discourse focus to measure
coherence. Although adding pause-based features
improved results slightly, the NP method was the
clear winner amongst those using a single feature
type and produced very good results.
The NP algorithm requires co-reference anno-
tations as input, so to create a fully-automatic
version (NP-AUTO) we have employed a state-of-
the-art co-reference resolution system (Poesio and
Kabadjov, 2004) to generate the required input.
We also include results based on P&L?s original
human co-reference annotations (NP-HUMAN).
For reference, we include a baseline that ran-
domly assigns boundaries at the same mean fre-
quency as the gold-standard annotations, i.e., a se-
quence drawn from the Bernoulli distribution with
success probability p = 0.169 (this probability de-
termines the value of the target segment length pa-
rameter l in our own algorithm). As a top-line ref-
erence, we calculate the mean of the seven anno-
tators? scores with respect to the three-annotator
gold standard.
For evaluation we employ two types of mea-
sure. On one hand, we use P (k) (Beeferman et al,
1999) as an error measure designed to accommo-
date near-miss boundary assignments. It is useful
because it estimates the probability that two ran-
domly drawn points will be assigned incorrectly
to either the same or different segments. On the
other hand, we use Cohen?s Kappa (?) to evalu-
ate the precise placement of boundaries such that
each potential boundary site is considered a binary
classification. While ? is typically used to evalu-
ate inter-annotator agreement, it is a useful mea-
sure of classification accuracy in our experiment
for two reasons. First, it accounts for the strong
class bias in our data. Second, it allows a direct
and intuitive comparison with our inter-annotator
top-line reference. We also provide results for the
commonly-used IR measures F1, recall, and pre-
cision. These are useful for comparing with pre-
vious results in the literature and provide a more
widely-understood measure of the accuracy of the
results. Precision and recall are also helpful in re-
vealing the effects of any classification bias the al-
gorithms may have.
The results are calculated for 18 of the 20 narra-
tives, as manual feature development involved the
Table 1: Mean results for the 18 test narratives.
P (k) ? F1 Rec. Prec.
Human .21 .58 .65 .64 .69
NP-HUMAN .35 .38 .40 .52 .46
NM09 .44 .11 .24 .23 .28
NP-AUTO .52 .03 .27 .71 .17
Random .50 .00 .15 .14 .17
use of two randomly selected narratives as devel-
opment data. The one exception is NP-HUMAN,
which is evaluated on the 10 narratives for which
there are manual co-reference annotations.
3.4 Results
The mean results for the 18 narratives, calculated
in comparison to the three-annotator gold stan-
dard, are shown in Table 1. NP-HUMAN and NM09
are both superior to the random baseline for all
measures (p?0.05). NP-AUTO, however, is only
superior in terms of recall and F1 (p?0.05).
3.5 Discussion
The results indicate that the simple set of features
we have chosen can be used for intentional seg-
mentation. While the results are not near human
performance, it is encouraging that such a simple
set of easily extractable features achieves results
that are 19% (?), 24% (P (k)), and 18% (F1) of
human performance, relative to the random base-
line.
The other notable result is the very high recall
score of NP-AUTO, which helps to produce a re-
spectable F1 score. However, a low ? reveals that
when accounting for class bias, this system is ac-
tually not far from the performance of a high recall
random classifier.
Error analysis showed that the reason for the
problems with NP-AUTO was the lack of reference
chains produced by the automatic co-reference
system. While the system seems to have per-
formed well for direct co-reference, it did not do
well with bridging reference. Inferred relations
were an important part of the reference chains pro-
duced by P&L, and it is now clear that these play
a significant role in the performance of the NP al-
gorithm. Our algorithm is not dependent on this
difficult processing problem, which typically re-
quires world knowledge in the form of training on
large datasets or the use of large lexical resources.
58
4 Topic vs. Intentional Segmentation
It is important to place our experiment on inten-
tional segmentation in context with the most com-
monly studied automatic segmentation task: topic-
based segmentation. While the two tasks are dis-
tinct, the literature has drawn connections between
them which can at times be confusing. In this sec-
tion, we attempt to clarify those connections by
pointing out some of their differences and similar-
ities. We also conduct an experiment comparing
our algorithm to well-known topic-segmentation
algorithms and discuss the results.
4.1 Automatic segmentation in the literature
One of the most widely-cited discourse segmen-
tation algorithms is TextTiling (Hearst, 1997).
Designed to segment texts into multi-paragraph
subtopics, it works by operationalizing the notion
of lexical cohesion (Halliday and Hasan, 1976).
TextTiling and related algorithms exploit the col-
location of semantically related lexemes to mea-
sure coherence. Recent improvements to this
method include the use of alternative lexical sim-
ilarity metrics like LSA (Choi et al, 2001) and
alternative segmentation methods like the mini-
mum cut model (Malioutov and Barzilay, 2006)
and ranking and clustering (Choi, 2000). Re-
cently, Bayesian approaches which model top-
ics as a lexical generative process have been em-
ployed (Purver et al, 2006; Eisenstein and Barzi-
lay, 2008). What these algorithms all share is a
focus on the semantic content of the discourse.
Passonneau and Litman (1997) is another of the
most widely-cited articles on discourse segmenta-
tion. Their overall approach combines an investi-
gation of prosodic features, cue words, and entity
reference. As described above, their approach to
using entity reference is motivated by Centering
theory (Grosz et al, 1995) and the hypothesis that
intentional structure is exhibited in the attentional
relationships between discourse referents.
Hearst and P&L try to achieve different goals,
but their tasks are nonetheless related. One might
reasonably hypothesize, for example, that either
lexical similarity or co-reference could be use-
ful to either type of segmentation on the grounds
that the two phenomena are clearly related. How-
ever, there are also clear differences of intent be-
tween the two studies. While there is an ob-
vious difference in the dataset (written exposi-
tory text vs. spoken narrative monologue), the an-
notation instructions reflect the difference most
clearly. Hearst instructed naive annotators to mark
paragraph boundaries ?where the topics seem to
change,? whereas P&L asked naive annotators to
mark prosodic phrases where the speaker had be-
gun a new communicative task.
The results indicate that there is a difference
in granularity between the two tasks, with inten-
tional segmentation relating to finer-grained struc-
ture. Hearst?s segments have a mean of about 200
words to P&L?s 40. Also, two hierarchical topic
segmentations of meetings (Hsueh, 2008; Gruen-
stein et al, 2008) have averages above 400 words
for the smallest level of segment.
To our knowledge, P&L is the only existing
study of automatic intention-based segmentation.
However, their work has been frequently cited as a
study of topic-oriented segmentation, e.g., (Galley
et al, 2003; Eisenstein and Barzilay, 2008). Also,
recent research in conversational genres (Galley et
al., 2003; Hsueh and Moore, 2007) analyze events
like discussing an agenda or giving a presentation,
which resemble more intentional categories. Inter-
estingly, these algorithms demonstrate the bene-
fit of including non-lexical, non-semantic features.
The results imply that further analysis is needed to
understand the links between different types of co-
herence and different types of segmentation.
4.2 Experiment 2
We have extended the above experiment to com-
pare the results of our novel algorithm with ex-
isting topic segmentation methods. We employ
Choi?s implementations of C99 (Choi, 2000) and
TEXTTILING (Hearst, 1997) as examples of well-
known topic-oriented methods. While we ac-
knowledge that there are newer algorithms which
improve upon this work, these were selected for
being well studied and easy to apply out-of-the-
box. Our method and evaluation is the same as in
the previous experiment.
The mean results for the 18 narratives are shown
in Table 2, with the human and baseline score re-
produced from the previous table. All three auto-
matic algorithms are superior to the random base-
line in terms of P (k), ?, and F1 (p?0.05). The
only statistically significant difference (p?0.05)
between the three automatic methods is between
NM09 and TEXTTILING in terms of F1. The ob-
served difference between NM09 and TEXTTIL-
ING in terms of ? is only moderately significant
59
Table 2: Results comparing our method to topic-
oriented segmentation methods.
NP-auto P (k) ? F1 Rec. Prec.
Human .21 .58 .65 .64 .69
NM09 .44 .11 .24 .24 .28
C99 .44 .08 .22 .20 .24
TEXTTILING .41 .05 .18 .16 .21
Random .50 .00 .15 .14 .17
(p?0.08). The observed differences between be-
tween NM09 and C99 are minimally significant
(p?0.24) .
4.3 Discussion
The comparable performance achieved by our
simple perspective-based approach in comparison
to lexical-semantic approaches suggests two main
points. First, it validates our novel approach in
practical applied terms. It shows that perspective-
oriented features, being simple to extract and ap-
plicable to a variety of genres, are potentially very
useful for automatic discourse segmentation sys-
tems.
Second, the results show that the teasing apart
of topic-oriented and intentional structure may be
quite difficult. Studies of coherence at the level of
short passages or episodes (Korolija, 1998) sug-
gest that coherence is established through a com-
plex interaction of topical, intentional, and other
contextual factors. In this experiment, the major
portion of the dialogues are oriented toward the
basic narrative activity which is the premise of the
Pear Stories dataset. This means that there are
many times when the activity type does not change
at intentional boundaries. At other times, the ac-
tivity type changes but neither the topic nor the set
of referents is significantly changed. The differ-
ent types of algorithms we have tried (i.e., topical,
referential, and perspectival) seem to be operating
on somewhat orthogonal bases, though it is dif-
ficult to say quantitatively how this relates to the
types of ?communicative task? transitions occur-
ring at the boundaries. In a sense, we have pro-
posed an algorithm for performing ?activity type
cohesion? which mimics the methods of lexical
cohesion but is based upon a different dimension
of the discourse. The results indicate that these are
both related to intentional structure.
5 General Discussion and Future Work
Future work in intentional segmentation is needed.
Our ultimate goal is to extend this work to more
conversational domains (e.g., multi-party planning
meetings) and to define the richer set of perspec-
tives and related deictic features that would be
needed for them. For example, we hypothesize
that the different uses of second-person pronouns
in conversations (Gupta et al, 2007) are likely to
reflect alternative activity types. Our feature set
and extraction methods will therefore need to be
further developed to capture this complexity.
The other question we would like to address is
the relationship between various types of coher-
ence (e.g., topical, referential, perspectival, etc.)
and different types (and levels) of discourse struc-
ture. Our current approach uses a feature space
that is orthogonal to most existing segmentation
methods. This has allowed us to gain a deeper
understanding of the relationship between certain
linguistic features and the underlying intentional
structure, but more work is needed.
In terms of practical motivations, we also plan
to address the open question of how to effectively
combine our feature set with other feature sets
which have also been demonstrated to contribute
to discourse structuring and segmentation.
References
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177?210.
Wallace L. Chafe, editor. 1980. The Pear Stories:
Cognitive, Cultural, and Linguistic Aspects of Nar-
rative Production, volume 3 of Advances in Dis-
course Processes. Ablex, Norwood, NJ.
Wallace L. Chafe. 1994. Discourse, Consciousness,
and Time: The Flow and Displacement of Conscious
Experience in Speaking and Writing. University of
Chicago Press, Chicago.
Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Jo-
hanna Moore. 2001. Latent semantic analysis for
text segmentation. In Proc. EMNLP, pages 109?
117.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proc. NAACL,
pages 26?33.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. LREC, pages 562?569.
60
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proc. EMNLP,
pages 334?343.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proc.
ACL, pages 562?569.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modelling the
local coherence of discourse. Computational Lin-
guistics, 21(2):203?225.
Claire Grover and Richard Tobin. 2006. Rule-based
chunking and reusability. In Proc. LREC.
Alexander Gruenstein, John Niekrasz, and Matthew
Purver. 2008. Meeting structure annotation: Anno-
tations collected with a general purpose toolkit. In
L. Dybkjaer and W. Minker, editors, Recent Trends
in Discourse and Dialogue, pages 247?274.
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007. Resolving ?you? in multi-
party dialog. In Proc. SIGdial, pages 227?230.
M. A. K. Halliday and Ruqayia Hasan. 1976. Cohe-
sion in English. Longman, New York.
Marti Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Pei-Yun Hsueh and Johanna D. Moore. 2007. Com-
bining multiple knowledge sources for dialogue seg-
mentation in multimedia archives. In Proc. ACL,
pages 1016?1023.
Pei-Yun Hsueh. 2008. Meeting Decision Detection:
Multimodal Information Fusion for Multi-Party Di-
alogue Understanding. Ph.D. thesis, School of In-
formatics, University of Edinburgh.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In NIPS 15.
Natascha Korolija. 1998. Episodes in talk: Construct-
ing coherence in multiparty conversation. Ph.D. the-
sis, Link?ping University, The Tema Institute, De-
partment of Communications Studies.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Proc.
COLING-ACL, pages 25?32.
Rebecca J. Passonneau and Diane J. Litman. 1997.
Discourse segmentation by human and automated
means. Computational Linguistics, 23(1):103?139.
Massimo Poesio and Mijail A. Kabadjov. 2004. A
general-purpose, off-the-shelf anaphora resolution
module: Implementation and preliminary evalua-
tion. In Proc. LREC.
Matthew Purver, Konrad K?rding, Thomas Griffiths,
and Joshua Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In
Proc. COLING-ACL, pages 17?24.
Carlota S. Smith. 2003. Modes of Discourse. Camb-
drige University Press, Cambridge.
Janyce M. Wiebe. 1994. Tracking point of view in nar-
rative. Computational Linguistics, 20(2):233?287.
Janyce M. Wiebe. 1995. References in narrative text.
In Judy Duchan, Gail Bruder, and Lynne Hewitt, ed-
itors, Deixis in Narrative: A Cognitive Science Per-
spective, pages 263?286.
61
Multi-Human Dialogue Understanding for Assisting
Artifact-Producing Meetings
John Niekrasz and Alexander Gruenstein and Lawrence Cavedon
Center for the Study of Language and Information (CSLI)
Stanford University
Cordura Hall, Stanford, CA, 94305-4115, USA
http://www-csli.stanford.edu/semlab/
{niekrasz, alexgru, lcavedon}@csli.stanford.edu
Abstract
In this paper we present the dialogue-
understanding components of an architec-
ture for assisting multi-human conversa-
tions in artifact-producing meetings: meet-
ings in which tangible products such as
project planning charts are created. Novel
aspects of our system include multimodal
ambiguity resolution, modular ontology-
driven artifact manipulation, and a meeting
browser for use during and after meetings.
We describe the software architecture and
demonstrate the system using an example
multimodal dialogue.
1 Introduction
Recently, much attention has been focused on
the domain of multi-person meeting under-
standing. Meeting dialogue presents a wide
range of challenges including continuous multi-
speaker automatic speech recognition (ASR),
2D whiteboard gesture and handwriting recog-
nition, 3D body and eye tracking, and multi-
modal multi-human dialogue management and
understanding. A significant amount of re-
search has gone toward understanding the prob-
lems facing the collection, organization, and
visualization of meeting data (Moore, 2002;
Waibel et al, 2001), and meeting corpora like
the ICSI Meeting Corpus (Janin et al, 2003) are
being made available. Continuing research in
the multimodal meeting domain has since blos-
somed, including ongoing work from projects
such as AMI1 and M42, and efforts from sev-
eral institutions.
Previous work on automatic meeting un-
derstanding has mostly focused on surface-
level recognition, such as speech segmentation,
for obvious reasons: understanding free multi-
human speech at any level is an extremely diffi-
cult problem for which best performance is cur-
rently poor. In addition, the primary focus for
1http://www.amiproject.org/
2http://www.m4project.org/
applications has been on off-line tools such as
post-meeting multimodal information browsing.
In parallel to such efforts we are applying
dialogue-management techniques to attempt to
understand and monitor meeting dialogues as
they occur, and to supplement multimodal
meeting records with information relating to the
structure and purpose of the meeting.
Our efforts are focused on assisting artifact-
producing meetings, i.e. meetings for which the
intended outcome is a tangible product such as
a project management plan or a budget. The
dialogue-understanding system helps to create
and manipulate the artifact, delivering a final
product at the end of the meeting, while the
state of the artifact is used as part of the dia-
logue context under which interpretation of fu-
ture utterances is performed, serving a num-
ber of useful roles in the dialogue-understanding
process:
? The dialogue manager employs generic di-
alogue moves with plugin points to be de-
fined by specific artifact types, e.g. project
plan, budget;
? The artifact state helps resolve ambiguity
by providing evidence for multimodal fu-
sion and constraining topic-recognition;
? The artifact type can be used to bias ASR
language-models;
? The constructed artifact provides a inter-
face for a meeting browser that supports
directed queries about discussion that took
place in the meeting, e.g. ?Why did we
decide on that date??
In addition, we focus our attention on the
handling of ambiguities produced on many
levels, including those produced during au-
tomatic speech recognition, multimodal com-
munication, and artifact manipulation. The
present dialogue manager uses several tech-
niques to do this, including the maintenance of
Multimodal
Integrator
3-D
Gesture
Recognizer
2-D
Drawing
Recognizer
ASR
CIA
NL
Parser
Information State
Ontology
KB DMT,
Active Node,
Salience List,
etc.
Dialogue
Manager
Meeting Browser
Hypothesis
Repository (CMU)
(OGI)
(OGI)
(MIT)
Figure 1: The meeting assistant architecture,
highlighting the dialogue-management compo-
nents.
multiple dialogue-move hypotheses, fusion with
multimodal gestures, and the incorporation of
artifact-specific plug-ins.
The software architecture we use for manag-
ing multi-human dialogue is an enhancement of
a dialogue-management toolkit previously used
at CSLI in a range of applications, including
command-and-control of autonomous systems
(Lemon et al, 2002) and intelligent tutoring
(Clark et al, 2002). In this paper, we detail the
dialogue-management components (Section 3),
which support a larger project involving mul-
tiple collaborating institutions (Section 2) to
build a multimodal meeting-understanding sys-
tem capable of integrating speech, drawing and
writing on a whiteboard, and physical gesture
recognition.
We also describe our toolkit for on-line and
off-line meeting browsing (Section 4), which al-
lows a meeting participant, observer, or devel-
oper to visually and interactively answer ques-
tions about the history of a meeting, the pro-
cesses performed to understand it, and the
causal relationships between dialogue and ar-
tifact manipulation.
2 Meeting Assistant Architecture
The complete meeting assistant architecture is
a highly collaborative effort from several insti-
tutions. Its overall architecture, focusing on our
contributions to the system is illustrated in Fig-
ure 1.
The components for drawing and writing
recognition and multimodal integration (Kaiser
et al, 2003) were developed at The Oregon
Graduate Institute (OGI) Center for Human-
Computer Communication3; the component for
physical gesture recognition (Ko et al, 2003)
was developed at The Massachusetts Institute
of Technology (MIT) AI Lab4. Integration be-
tween all components was performed by project
members at those sites and at SRI Interna-
tional5, and integration between our CSLI Con-
versational Intelligence Architecture and OGI?s
Multimodal Integrator (MI) was performed by
members of both teams. ASR is done using
CMU Sphinx6, from which the n-best list of re-
sults are passed to SRI?s Gemini parser (Dowd-
ing et al, 1993). Gemini incorporates a suite
of techniques for handling noisy input, includ-
ing fragment detection, and its dynamic gram-
mar capabilities are used to register new lexical
items, such as names of tasks that may be out-
of-grammar.
An example of a multimodal meeting conver-
sation that the meeting assistant currently sup-
ports can be found in Figure 2.7 There are two
meeting participants in a conference room with
an electronic whiteboard which can record their
pen strokes and a video camera that tracks their
body movements; A is standing at the white-
board and drawing while B is sitting at the
table. A gloss of how the system behaves in
response to each utterance and gesture follows
each utterance; these glosses will be explained
in greater detail throughout the rest of the pa-
per. The drawing made on the whiteboard is
in Figure 3(a), and the chart artifact as it was
constructed by the system is displayed in Figure
3(b).
3 Conversational Intelligence
Architecture
To meet the challenges presented by multi-
person meeting dialogue, we have extended
and enhanced our previously used Conversa-
tional Intelligence Architecture (CIA). The CIA
is a modular and highly configurable multi-
application system: a separation is made be-
tween generic dialogue processes and those spe-
cific to a particular domain. Creating a new
application may involve writing new dialogue
moves and configuring the CIA to use these. We
3http://www.cse.ogi.edu/CHCC/
4http://www.ai.mit.edu/
5http://www.sri.com/
6http://www.speech.cs.cmu.edu/sphinx/
7A video demonstration will be available soon at
http://www-csli.stanford.edu/semlab/calo/
A: So, lets uh figure out what uh needs uh needs to be done. Let?s
look at the schedule. [draws a chart axes] utterance and gesture
information fused, a new milestone chart artifact is created
B: So, if all goes well, we?ve got funding for five years. system sets
unit on axis to ?years?
A: Yeah. Let?s see one, two ... [draws five tick marks on the x-axis]
system assumes tick marks are years
B: Well, the way I see it, uh we?ve got three tasks. dialogue man-
ager hypothesizes three tasks should be added, waits for multimodal
confirmation
A: Yeah right [draws three task lines horizontally on the axis] multi-
modal confirmation is given, information about task start and end
dates is fused from the drawing
A: Let?s call this task line demo [touches the top line with the
pen], call this task line signoff [touches the middle line with the
pen], and call this task line system [touches the bottom line with
the pen]. each utterance causes the dialogue manager to hypoth-
esize three distinct hypotheses, in each task a different hypothesis
is named, the gestures disambiguate these in the multimodal inte-
grator
B: So we have two demos to get done.
A: uh huh
B: Darpatech is at the end of month fifteen [A draws a diamond
at month fifteen on the demo task line] dialogue manager hy-
pothesizes a milestone called ?darpatech? at month fifteen; gesture
confirms this and pinpoints appropriate task line
B: And the final demonstrations are at the end of year five [A draws
a diamond at year five on the demo task line] same processing as
previous
A: Hmm, so when do the signoffs need to happen do you think?
dialogue manager expects next utterance to be an answer
B: Six months before the demos [A draws two diamonds on the sig-
noff task line, each one about 6 months before the demo mile-
stones drawn above] answer arrives; dialogue manager hypothe-
sizes two new milestones which are confirmed by gesture
A: And we?ll need the systems by then too [A draws two diamonds
on the system task line] dialogue manager hypothesizes two more
milestones, confirmed by gesture
B: That?s a bit aggressive I think. Let?s move the system milestone
back six months. [B points finger at rightmost system milestone.
A crosses it out and draws another one six months earlier] di-
alogue manager hypothesizes a move of the milestone, 3D gesture
and drawing confirm this
Figure 2: Example conversation understood by
the system.
(a) The whiteboard in-
put captured by OGI?s
Charter gesture recog-
nizer
(b) The artifact as
maintained in the
dialogue system
Figure 3: Ink-captured vs ?idealized? artifact
output.
have successfully used this ?toolkit? approach
in our previous applications at CSLI to inter-
face novel devices without modifying the core
dialogue manager.
The present application is however very dif-
ferent to our previous applications, and those
commonly encountered in the literature, which
typically involve a single human user interact-
ing with a dialogue-enabled artificial agent. In
the meeting environment, the dialogue manager
should at most very rarely interpose itself into
the discussion?to do so would be disruptive
to the interaction between the human partic-
ipants. This requirement prohibits ambiguity
and uncertainty from being resolved with, say,
a clarification question, which is the usual strat-
egy in conversational interfaces. Instead, uncer-
tainty must be maintained in the system until
it can be resolved by leveraging context, using
evidence from another modality, or by a future
utterance.
The meeting-understanding domain has thus
prompted several extensions to our existing
CIA, many of which we expect will be applied
in other conversational domains. These include:
? Support for handling multiple competing
speech parses; (Section 3.2)
? A generic artifact ontology which enables
designing generically useful artifact-savvy
dialogue applications; (Section 3.3)
? Support for the generation and subsequent
confirmation of dialogue-move hypotheses
in a multimodal integration framework;
(Section 3.4)
? The acceptance of non-verbal unimodal
gestures into the dialogue-move repertoire.
(Section 3.5)
? A preliminary mechanism for supporting
uncertainty across multiple conversational
moves; (Section 3.6)
Before discussing these new features in detail,
the following section introduces the CIA and
its persisting core dialogue-management com-
ponents.
3.1 Core Components: Information
State and Context
The core dialogue management components of
the CIA maintain dialogue context using the
information-state and dialogue-move approach
(Larsson and Traum, 2000) where each con-
tributed utterance modifies the current context,
or information state, of the dialogue. Each new
utterance is then interpreted within the current
context (see (Lemon et al, 2002) for a detailed
description).
A number of data structures are employed
in this process. The central dialogue state-
maintaining structure is the Dialogue Move Tree
(DMT). The DMT represents the historical con-
text of a dialogue. An incoming utterance, clas-
sified by dialogue move, is interpreted in con-
text by attaching itself to an appropriate active
node on the DMT; e.g., an answer attaches to
an active corresponding question node. Cur-
rently, active nodes are kept on an Active Node
List , which is ordered so that those most likely
to be relevant to the current conversation are
at the front of the list. Incoming utterances
are displayed to each node in turn, and at-
tach to the first appropriate node (determined
by information-state-update functions). Other
structures include the context-specific Salience
List , which maintains recently used terms for
performing anaphora resolution, and a Knowl-
edge Base containing application specific infor-
mation, which may be leveraged to interpret in-
coming utterances.8
We now present the various enhancements
made to the CIA for use in the meeting domain.
3.2 ASR and Robust Parsing
The first step in understanding any dialogue is
recognizing and interpreting spoken utterances.
In the meeting domain, we are presented with
the particularly difficult task of doing this for
spontaneous human-human speech. We there-
fore chose to perform ASR using a statisti-
cal language model (LM) and employ CMU?s
Sphinx to generate an n-best list of recogni-
tion results. The recognition engine uses a tri-
gram LM trained on the complete set of pos-
sible utterances expected given a small hand-
crafted scenario like that in the example dia-
logue. Despite the task?s limited domain, the re-
alized speech is very disfluent, generating an ex-
tremely broad range of possible utterances that
the system must handle. The resulting n-best
list is therefore often extremely varied.
To handle the ASR results of disfluent utter-
ances, we employ SRI?s Gemini robust language
parser (Dowding et al, 1993). In particular,
we use Gemini to retrieve the longest strings
of valid S and NP fragments in each ASR re-
sult. Currently, we reject all but the parsed S
fragments?and NP fragments when expected
8Command-and-control applications have also made
use of an Activity Tree, which represents activities being
carried out by the dialogue-enabled device (Gruenstein,
2002); however, this application currently makes no use
of this.
by the system (e.g. an answer to a question
containing an NP gap). The parser uses generic
syntactic rules, but is constrained semantically
by sorts specific to the domain. In Section 3.4,
we describe how the dialogue manager handles
the multiple parses for a single utterance and
how it uses the uncertainty they represent.
3.3 Artifact Knowledge Base and
Ontology
In the present version of the CIA, all static do-
main knowledge about meeting artifacts is de-
fined in a modularized class-based ontology. In
conjunction with the ontology, we also maintain
a dynamic knowledge base (KB) which holds
the current state of any artifacts. This is stored
as a collection of instances of the ontological
classes, and both components are maintained
together using the Prote?ge?-20009 ontology and
knowledge-base toolkit (Grosso et al, 1999).
The principal base classes in the artifact on-
tology are designed to be both architecturally
elegant and intuitive. To this end, we charac-
terize the world of artifacts as being made up
of three essential classes: entities which repre-
sent the tangible objects themselves, relations
which represent how the entities relate to one
another, and events which change the state of
entities or relations. Events are the most im-
portant tool aiding the dialogue management
algorithm. They comprehensively characterize
the set of actions which can change the current
state of an artifact. They may be classified into
three categories: insert changes which insert a
new entity or relation instance into the KB, re-
move changes which remove an instance, and
value changes which modify the value of a slot
in an instance. All changes to the KB can be
characterized as one of these three atomic events
or a combination of them.
3.4 Hypothesizers: A plugin
architecture for artifact-driven
multimodal integration
Abmiguities and uncertainties are both ram-
pant in multimodal meeting dialogues, and in
artifact-producing meetings, the majority per-
tain to artifacts and the utterances performed
to change them. In this section we explain how
the CIA?s dialogue manager uses the artifact on-
tology, and the repertoire of event classes in it,
to formulate sets of artifact-changing dialogue-
move hypotheses from single utterances. We
9http://protege.stanford.edu/
also demonstrate how it uses the current state
of the artifact in the KB to constrain the in-
terpretation of utterances in context, and how
multimodal gestures help to resolve ambiguous
interpretations.
To begin, each dialogue-move hypothesis con-
sists of the following elements: (1) the DMT
node associated with this hypothesis, (2) the
parse that gave rise to the hypothesis, (3) the
probability of the hypothesis, (4) an isUnimodal
flag indicating whether or not the dialogue move
requires confirmation from other modalities, (5)
a list of artifact-change events to be made to
the KB, and (6) the information state update
function to be invoked if this hypothesis is con-
firmed by the multimodal integrator. Each of
these elements participate in the generation and
confirmation process as detailed below.
First, consider the utterance Darpatech is at
the end of month fifteen. from the example dia-
logue. This utterance is much more likely to
indicate the creation of a new milestone if a
task line is pertinent to the current dialogue
context, e.g. the user has just created a new
task line. In our system, the ambiguous or un-
certain utterance, the current dialogue context,
and the current state of the chart is delegated to
artifact-type specific components called hypoth-
esizers. Hypothesizers take the above as input,
and using the set of events available to its cor-
responding artifact in the ontology, they pro-
grammatically generate a list of dialogue-move
hypotheses appropriate in the given context?
or they can return the empty list to indicate
that there is no reasonable interpretation of the
utterance given the current context.
Hypothesizers work directly with the DMT
architecture: as an incoming utterance is se-
quentially presented to each active node in the
DMT, the dialogue context and the proposed
active node are passed into a hypothesizer cor-
responding to the particular artifact associated
with that node. If the hypothesizer can create
one or more valid hypotheses, then the utter-
ance is attached to the DMT as a child of that
active node.10
In a multimodal domain, some hypotheses re-
quire confirmation in other modalities before
the dialogue manager can confidently update
10There are, in fact, other rules as well which allow for
attachment. For example, questions?which don?t im-
mediately generate hypotheses?can also be attached to
various nodes depending on the dialogue context. While
the emphasis here is on hypothesizers, these are just one
part of the dialogue processing toolkit
the information state. In this particular system,
in fact, the dialogue manager does not directly
update the KB?s current artifact state; rather, it
hypothesizes a set of dialogue-move hypotheses
and assigns each a confidence derived from ASR
confidence, the fragmentedness of the parse, and
confidence in the proposed attachment to a con-
versational thread. Each conversational move is
then provided a Hypothesis Repository for stor-
ing the hypotheses associated with it. When
dialogue processing is completed for a partic-
ular conversational move, i.e. when all pos-
sible attachments of all possible parses on the
n-best list have been made, the set of hypothe-
ses is sent to the Multimodal Integrator (MI)
for potential fusion with gesture. Depending on
the information from other modalities, the MI
confirms or rejects the hypotheses?moreover, a
confirmed hypothesis might be augmented with
information provided by other modalities. Such
an augmentation occurs for the utterance We
have three tasks from the example dialogue. In
this situation, the dialogue manager hypothe-
sizes that the user may be creating three new
task lines on the chart. When the user actually
draws the three task lines, the MI infers the
start and stop date based on where the lines
start and stop on the axis. In this case, it not
only confirms the dialogue manager?s hypoth-
esis, but augments it to reflect the additional
date information yielded from the whiteboard
input.
3.5 Unimodal Gestures
In addition to the Information State updates
based on both speech and gesture, multimodal
meeting dialogue can often include gestures in
which a participant makes a change to an ar-
tifact using a unimodal gesture not associated
with an utterance. For example, a user may
draw a diamond on a task line but say nothing.
Even in the absence of speech, this can be unam-
biguously understood as the creation of a mile-
stone at a particular point on the line. These
unimodally produced changes to the chart must
be noted by the dialogue manager, as they are
potential targets for later conversation. To ac-
commodate this, we introduce a new DMT node
of type Unimodal Gesture, thus implicitly in-
cluding gesture as a communicative act that can
stand on its own in a conversation
3.6 Uncertain DMT Node Attachment
Since hypotheses are not always immediately
confirmed, uncertainty must be maintained
Figure 4: A snapshot from the meeting browser.
across multiple dialogue moves. The system ac-
complishes this by extending the CIA to main-
tain multiple competing Information States. In
particular, the DMT has been extended to al-
low for the same parse to attach in multiple
locations?these multiple attachments are even-
tually pruned as more evidence is accumulated
in the form of further speech or gestures?that
is, as hypotheses are confirmed or rejected over
time.
4 Meeting Viewer Toolkit
Throughout an artifact-producing meeting, the
dialogue system processes a complex chronolog-
ical sequence of events and information states
that form structures rich in information useful
to dialogue researchers and the dialogue partic-
ipants themselves. To harness the power of this
information, we have constructed a toolkit for
visualizing and investigating the meeting infor-
mation state and its history.
Central to the toolkit is our meeting history
browser, which can be seen in Figure 4, dis-
playing a portion of the example dialogue, with
the results of a search for ?demo? highlighted.
This record of the meeting is available both dur-
ing the meeting and afterwards to assist users
in answering questions they might have about
the meeting. Many kinds of questions can be
answered in the browser, like those a manager
might ask the day after a meeting: ?Why did we
move the deadline on that task 6 months later??,
?Did I approve setting that deadline so early??,
and ?What were we thinking when we put that
milestone at month fifteen??. A meeting partic-
ipant might have questions as the meeting oc-
curs, like ?What did the chart look like 5 min-
utes ago??, ?What did we say to make the sys-
tem move that milestone??, and ?What did Mr.
Smith say at the beginning of the meeting??.
To help answer these questions, the browser
performs many of the functions found in current
multimodal meeting browsers. For example,
it provides concise display of a meeting tran-
scription, advanced searching capabilities, sav-
ing and loading of meeting sessions, and person-
alization of its own display characteristics. As
a novel addition to these basic behaviors, the
browser is also designed to display artifacts and
the causal relationships between artifacts and
the utterances that cause them to change.
To effectively convey this information, the
record of components monitored by the history
toolkit is presented to the user through a win-
dow which chronologically displays the visual
embodiment of those components. Recognized
utterances are shown as text, parses are shown
as grouped string fragments, and artifacts and
their sub-components are shown in their pro-
totypical graphical form. The window orga-
nizes these visual representations of the meet-
ing?s events and states into chronological tracks,
each of which monitors a unified conceptual part
of the meeting. The user is then able to link the
elements causally.
Beyond the history browser, the toolkit also
displays the current state of all artifacts in an
artifact-state window (e.g. Figure 3(b)). In the
window, the user not only confirms the state of
the artifact but can also gain insight into the
currently interpreted dialogue context by mon-
itoring how the artifact is highlighted. In the
figure, the third task is highlighted because it is
the most recently talked-about task. A meeting
participant can therefore see that subsequent
anaphoric references to an unknown task will
be resolved to the third one.
Another GUI component of the toolkit is
a small hypothesis window which shows the
current set of unresolved artifact-changing hy-
potheses. It does this by displaying an artifact
for each hypothesis, reflecting the artifact?s fu-
ture state given confirmation of the hypothe-
sis. The hypothesis? probability and associated
parse is displayed under the artifact. The user
may even directly click a hypothesis to confirm
it. The hypothesized future states are however
not displayed in the artifact-state window or
artifact-history browser, which show only the
results of confirmed actions.
In addition to being a GUI front-end, the
toolkit maintains a fully generic architecture for
recording the history of any object in the sys-
tem software. These objects can be anything
from the utterances of a participant, to the state
history of an artifact component, or the record
of hypotheses formulated by the dialogue man-
ager. This generic functionality provides the
toolkit the ability to answer a wide variety of
questions for the user about absolutely any as-
pect of the dialogue context history.
5 Future Work
Work is currently proceeding in a number of
directions. Firstly, we plan to incorporate fur-
ther techniques for robust language understand-
ing, including word-spotting and other topic-
recognition techniques, within the context of
the constructed artifact. We also plan to in-
vestigate using the current state of the artifact
to further bias the ASR language model. We
also plan on generalizing the uncertainty man-
agement within the dialogue manager, allowing
multiple competing hypotheses to be supported
over multiple dialogue moves. Topic and other
ambiguity management techniques will be used
to statistically filter and bias hypotheses, based
on artifact state.
We are currently expanding the meeting
browser to categorize utterances by dialogue
act, and to recognize and categorize aggrega-
tions as multi-move strategies, such as negoti-
ations. This will allow at-a-glance detection of
where disagreements took place, and where is-
sues may have been left unresolved. A longer-
term aim of the project is to provide further
support to the participants in the meeting, e.g.
by detecting opportunities to provide useful in-
formation (e.g. schedules, when discussing who
to allocate to a task; documents pertinent to a
topic under discussion) to meeting participants
automatically. Evaluation criteria are currently
being designed that include both standard mea-
sures, such as word error rate, and measures in-
volving recognition of meeting-level phenomena,
such as detecting agreement on action-items.
Evaluation will be performed using both corpus-
based approaches (e.g. for evaluating recog-
nition of meeting phenomena) and real (con-
trolled) meetings with human subjects.
6 Acknowledgements
We would like to gratefully acknowledge Phil
Cohen?s group at OGI, especially Ed Kaiser,
Xiaoguang Li, and Matt Wesson, and David
Demirdjian at MIT. This work was funded by
DARPA grant NBCH-D-03-0010(1).
References
B. Clark, E. Owen Bratt, O. Lemon, S. Pe-
ters, H. Pon-Barry, Z. Thomsen-Gray, and
P. Treeratpituk. 2002. A general purpose ar-
chitecture for intelligent tutoring systems. In
International CLASS Workshop on Natural,
Intelligent and Effective Interaction in Multi-
modal Dialogue Systems.
J. Dowding, J.M. Gawron, D. Appelt, J. Bear,
L. Cherny, R. Moore, and D. Moran. 1993.
Gemini: a natural language system for
spoken-language understanding. In Proc.
ACL 93.
W. E. Grosso, H. Eriksson, R. W. Fergerson,
J. H. Gennari, S. W. Tu, and M. A. Musen.
1999. Knowledge modeling at the millen-
nium: (the design and evolution of Prote?ge?-
2000). In Proc. KAW 99.
A. Gruenstein. 2002. Conversational interfaces:
A domain-independent architecture for task-
oriented dialogues. Master?s thesis, Stanford
University.
A. Janin, D. Baron, J. Edwards, D. Ellis,
D. Gelbart, N. Morgan, B. Peskin, T. Pfau,
E. Shriberg, A. Stolcke, and C. Wooters.
2003. The ICSI meeting corpus. In Proc.
ICASSP 2003.
E. Kaiser, A. Olwal, D. McGee, H. Benko,
A. Corradini, X. Li, P. Cohen, and S. Feiner.
2003. Mutual disambiguation of 3D multi-
modal interaction in augmented and virtual
reality. In Proc. ICMI 2003.
T. Ko, D. Demirdjian, and T. Darrell. 2003.
Untethered gesture acquisition and recogni-
tion for a multimodal conversational system.
In Proc. ICMI 2003.
S. Larsson and D. Traum. 2000. Informa-
tion state and dialogue management in the
TRINDI dialogue move engine toolkit. Natu-
ral Language Engineering, 6.
O. Lemon, A. Gruenstein, and S. Peters. 2002.
Collaborative activities and multi-tasking in
dialogue systems. Traitment Automatique
des Langues, 43(2).
D. Moore. 2002. The IDIAP smart meeting
room. Technical Report IDIAP Communica-
tion 02-07.
A. Waibel, M. Bett, F. Metze, K. Ries,
T. Schaaf, T. Schultz, H. Soltau, H. Yu, and
K. Zechner. 2001. Advances in automatic
meeting record creation and access. In Proc.
ICASSP 2001.
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 31?34,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Shallow Discourse Structure for Action Item Detection
Matthew Purver, Patrick Ehlen, and John Niekrasz
Center for the Study of Language and Information
Stanford University
Stanford, CA 94305
{mpurver,ehlen,niekrasz}@stanford.edu
Abstract
We investigated automatic action item
detection from transcripts of multi-party
meetings. Unlike previous work (Gruen-
stein et al, 2005), we use a new hierarchi-
cal annotation scheme based on the roles
utterances play in the action item assign-
ment process, and propose an approach
to automatic detection that promises im-
proved classification accuracy while en-
abling the extraction of useful information
for summarization and reporting.
1 Introduction
Action items are specific kinds of decisions common
in multi-party meetings, characterized by the con-
crete assignment of tasks together with certain prop-
erties such as an associated timeframe and reponsi-
ble party. Our aims are firstly to automatically de-
tect the regions of discourse which establish action
items, so their surface form can be used for a tar-
geted report or summary; and secondly, to identify
the important properties of the action items (such as
the associated tasks and deadlines) that would fos-
ter concise and informative semantically-based re-
porting (for example, adding task specifications to a
user?s to-do list). We believe both of these aims are
facilitated by taking into account the roles different
utterances play in the decision-making process ? in
short, a shallow notion of discourse structure.
2 Background
Related Work Corston-Oliver et al (2004) at-
tempted to identify action items in e-mails, using
classifiers trained on annotations of individual sen-
tences within each e-mail. Sentences were anno-
tated with one of a set of ?dialogue? act classes; one
class Task included any sentence containing items
that seemed appropriate to add to an ongoing to-
do list. They report good inter-annotator agreement
over their general tagging exercise (? > 0.8), al-
though individual figures for the Task class are not
given. They then concentrated on Task sentences,
establishing a set of predictive features (in which
word n-grams emerged as ?highly predictive?) and
achieved reasonable per-sentence classification per-
formance (with f-scores around 0.6).
While there are related tags for dialogue act tag-
ging schema ? like DAMSL (Core and Allen, 1997),
which includes tags such as Action-Directive
and Commit, and the ICSI MRDA schema
(Shriberg et al, 2004) which includes a commit
tag ? these classes are too general to allow iden-
tification of action items specifically. One compa-
rable attempt in spoken discourse took a flat ap-
proach, annotating utterances as action-item-related
or not (Gruenstein et al, 2005) over the ICSI and
ISL meeting corpora (Janin et al, 2003; Burger et
al., 2002). Their inter-annotator agreement was low
(? = .36). While this may have been partly due
to their methods, it is notable that (Core and Allen,
1997) reported even lower agreement (? = .15) on
their Commit dialogue acts. Morgan et al (forth-
coming) then used these annotations to attempt auto-
31
matic classification, but achieved poor performance
(with f-scores around 0.3 at best).
Action Items Action items typically embody the
transfer of group responsibility to an individual.
This need not be the person who actually performs
the action (they might delegate the task to a subor-
dinate), but publicly commits to seeing that the ac-
tion is carried out; we call this person the owner of
the action item. Because this action is a social ac-
tion that is coordinated by more than one person,
its initiation is reinforced by agreement and uptake
among the owner and other participants that the ac-
tion should and will be done. And to distinguish
this action from immediate actions that occur during
the meeting and from more vague future actions that
are still in the planning stage, an action item will be
specified as expected to be carried out within a time-
frame that begins at some point after the meeting and
extends no further than the not-too-distant future. So
an action item, as a type of social action, often com-
prises four components: a task description, a time-
frame, an owner, and a round of agreement among
the owner and others. The related discourse tends to
reflect this, and we attempt to exploit this fact here.
3 Baseline Experiments
We applied Gruenstein et al (2005)?s flat annotation
schema to transcripts from a sequence of 5 short re-
lated meetings with 3 participants recorded as part
of the CALO project. Each meeting was simulated
in that its participants were given a scenario, but
was not scripted. In order to avoid entirely data-
or scenario-specific results (and also to provide an
acceptable amount of training data), we then added
a random selection of 6 ICSI and 1 ISL meetings
from Gruenstein et al (2005)?s annotations. Like
(Corston-Oliver et al, 2004) we used support vec-
tor machines (Vapnik, 1995) via the classifier SVM-
light (Joachims, 1999). Their full set of features are
not available to us, but we experimented with com-
binations of words and n-grams and assessed classi-
fication performance via a 5-fold validation on each
of the CALO meetings. In each case, we trained
classifiers on the other 4 meetings in the CALO se-
quence, plus the fixed ICSI/ISL training selection.
Performance (per utterance, on the binary classifica-
tion problem) is shown in Table 1; overall f-score
figures are poor even on these short meetings. These
figures were obtained using words (unigrams, after
text normalization and stemming) as features ? we
investigated other discriminative classifier methods,
and the use of 2- and 3-grams as features, but no
improvements were gained.
Mtg. Utts AI Utts. Precision Recall F-Score
1 191 22 0.31 0.50 0.38
2 156 27 0.36 0.33 0.35
3 196 18 0.28 0.55 0.37
4 212 15 0.20 0.60 0.30
5 198 9 0.19 0.67 0.30
Table 1: Baseline Classification Performance
4 Hierarchical Annotations
Two problems are apparent: firstly, accuracy is
lower than desired; secondly, identifying utterances
related to action items does not allow us to ac-
tually identify those action items and extract their
properties (deadline, owner etc.). But if the ut-
terances related to these properties form distinct
sub-classes which have their own distinct features,
treating them separately and combining the results
(along the lines of (Klein et al, 2002)) might al-
low better performance, while also identifying the
utterances where each property?s value is extracted.
Thus, we produced an annotation schema which
distinguishes among these four classes. The first
three correspond to the discussion and assignment
of the individual properties of the action item (task
description, timeframe and owner); the fi-
nal agreement class covers utterances which ex-
plicitly show that the action item is agreed upon.
Since the task description subclass ex-
tracts a description of the task, it must include any
utterances that specify the action to be performed,
including those that provide required antecedents for
anaphoric references. The owner subclass includes
any utterances that explicitly specify the responsible
party (e.g. ?I?ll take care of that?, or ?John, we?ll
leave that to you?), but not those whose function
might be taken to do so implicitly (such as agree-
ments by the responsible party). The timeframe
subclass includes any utterances that explicitly refer
to when a task may start or when it is expected to
be finished; note that this is often not specified with
32
a date or temporal expression, but rather e.g. ?by
the end of next week,? or ?before the trip to Aruba?.
Finally, the agreement subclass includes any ut-
terances in which people agree that the action should
and will be done; not only acknowledgements by the
owner themselves, but also when other people ex-
press their agreement.
A single utterance may be assigned to more than
one class: ?John, you need to do that by next
Monday? might count as owner and timeframe.
Likewise, there may be more than one utterance of
each class for a single action item: John?s response
?OK, I?ll do that? would also be classed as owner
(as well as agreement). While we do not require
all of these subclasses to be present for a set of ut-
terances to qualify as denoting an action item, we
expect any action item to include most of them.
We applied this annotation schema to the same
12 meetings. Initial reliability between two anno-
tators on the single ISL meeting (chosen as it pre-
sented a significantly more complex set of action
items than others in this set) was encouraging. The
best agreement was achieved on timeframe utter-
ances (? = .86), with owner utterances slightly
less good (between ? = .77), and agreement and
description utterances worse but still accept-
able (? = .73). Further annotation is in progress.
5 Experiments
We trained individual classifiers for each of the utter-
ance sub-classes, and cross-validated as before. For
agreement utterances, we used a naive n-gram
classifier similar to that of (Webb et al, 2005) for di-
alogue act detection, scoring utterances via a set of
most predictive n-grams of length 1?3 and making a
classification decision by comparing the maximum
score to a threshold (where the n-grams, their scores
and the threshold are automatically extracted from
the training data). For owner, timeframe and
task description utterances, we used SVMs
as before, using word unigrams as features (2- and
3-grams gave no improvement ? probably due to the
small amount of training data). Performance var-
ied greatly by sub-class (see Table 2), with some
(e.g. agreement) achieving higher accuracy than the
baseline flat classifications, but others being worse.
As there is now significantly less training data avail-
able to each sub-class than there was for all utter-
ances grouped together in the baseline experiment,
worse performance might be expected; yet some
sub-classes perform better. The worst performing
class is owner. Examination of the data shows
that owner utterances are more likely than other
classes to be assigned to more than one category;
they may therefore have more feature overlap with
other classes, leading to less accurate classification.
Use of relevant sub-strings for training (rather than
full utterances) may help; as may part-of-speech in-
formation ? while proper names may be useful fea-
tures, the name tokens themselves are sparse and
may be better substituted with a generic tag.
Class Precision Recall F-Score
description 0.23 0.41 0.29
owner 0.12 0.28 0.17
timeframe 0.19 0.38 0.26
agreement 0.48 0.44 0.40
Table 2: Sub-class Classification Performance
Even with poor performance for some of the sub-
classifiers, we should still be able to combine them
to get a benefit as long as their true positives cor-
relate better than their false positives (intuitively, if
they make mistakes in different places). So far we
have only conducted an initial naive experiment, in
which we combine the individual classifier decisions
in a weighted sum over a window (currently set to
5 utterances). If the sum over the window reaches
a given threshold, we hypothesize an action item,
and take the highest-confidence utterance given by
each sub-classifier in that window to provide the
corresponding property. As shown in Table 3, this
gives reasonable performance on most meetings, al-
though it does badly on meeting 5 (apparently be-
cause no explicit agreement takes place, while our
manual weights emphasized agreement).1 Most en-
couragingly, the correct examples provide some use-
ful ?best? sub-class utterances, from which the rele-
vant properties could be extracted.
These results can probably be significantly im-
proved: rather than sum over the binary classifica-
tion outputs of each classifier, we can use their con-
fidence scores or posterior probabilities, and learn
1Accuracy here is currently assessed only over correct de-
tection of an action item in a window, not correct assignment of
all sub-classes.
33
Mtg. AIs Correct False+ False- F-Score
1 3 2 1 1 0.67
2 4 1 0 3 0.40
3 5 2 1 3 0.50
4 4 4 0 0 1.00
5 3 0 1 3 0.00
Table 3: Combined Classification Performance
the combination weights to give a more robust ap-
proach. There is still a long way to go to evaluate
this approach over more data, including the accu-
racy and utility of the resulting sub-class utterance
hypotheses.
6 Discussion and Future Work
So accounting for the structure of action items ap-
pears essential to detecting them in spoken dis-
course. Otherwise, classification accuracy is lim-
ited. We believe that accuracy can be improved, and
the detected utterances can be used to provide the
properties of the action item itself. An interesting
question is how and whether the structure we use
here relates to discourse structure in more general
use. If a relation exists, this would shed light on the
decision-making process we are attempting to (be-
gin to) model, and might allow us to use other (more
plentiful) annotated data.
Our future efforts focus on annotating more meet-
ings to obtain large training and testing sets. We also
wish to examine performance when working from
speech recognition hypotheses (as opposed to the
human transcripts used here), and the best way to in-
corporate multiple hypotheses (either as n-best lists
or word confusion networks). We are actively inves-
tigating alternative approaches to sub-classifier com-
bination: better performance (and a more robust and
trainable overall system) might be obtained by using
a Bayesian network, or a maximum entropy classi-
fier as used by (Klein et al, 2002). Finally, we are
developing an interface to a new large-vocabulary
version of the Gemini parser (Dowding et al, 1993)
which will allow us to use semantic parse informa-
tion as features in the individual sub-class classifiers,
and also to extract entity and event representations
from the classified utterances for automatic addition
of entries to calendars and to-do lists.
References
S. Burger, V. MacLaren, and H. Yu. 2002. The ISLMeet-
ing Corpus: The impact of meeting type on speech
style. In Proceedings of the 6th International Confer-
ence on Spoken Language Processing (ICSLP 2002).
M. Core and J. Allen. 1997. Coding dialogues with
the DAMSL annotation scheme. In D. Traum, edi-
tor, AAAI Fall Symposium on Communicative Action
in Humans and Machines.
S. Corston-Oliver, E. Ringger, M. Gamon, and R. Camp-
bell. 2004. Task-focused summarization of email. In
Proceedings of the Text Summarization Branches Out
ACL Workshop.
J. Dowding, J. M. Gawron, D. Appelt, J. Bear, L. Cherny,
R. Moore, and D. Moran. 1993. Gemini: A natural
language system for spoken language understanding.
In Proc. 31st Annual Meeting of the Association for
Computational Linguistics.
A. Gruenstein, J. Niekrasz, and M. Purver. 2005. Meet-
ing structure annotation: Data and tools. In Proceed-
ings of the 6th SIGdial Workshop on Discourse and
Dialogue.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C.Wooters. 2003. The ICSI Meeting Corpus.
In Proc. IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP 2003).
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods ? Support Vector
Learning. MIT Press.
D. Klein, K. Toutanova, H. T. Ilhan, S. D. Kamvar, and
C. D.Manning. 2002. Combining heterogeneous clas-
sifiers for word-sense disambiguation. In Proceedings
of the ACL Workshop on Word Sense Disambiguation:
Recent Successes and Future Directions.
W. Morgan, S. Gupta, and P.-C. Chang. forthcoming.
Automatically detecting action items in audio meeting
recordings. Ms., under review.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Car-
vey. 2004. The ICSI Meeting Recorder Dialog Act
Corpus. In Proceedings of the 5th SIGdial Workshop
on Discourse and Dialogue.
S. Siegel and J. N. J. Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
N. Webb, M. Hepple, and Y. Wilks. 2005. Dialogue act
classification using intra-utterance features. In Proc.
AAAI Workshop on Spoken Language Understanding.
34
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 256?264,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Annotating Participant Reference in English Spoken Conversation
John Niekrasz and Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
{jniekras,jmoore}@inf.ed.ac.uk
Abstract
In conversational language, references to
people (especially to the conversation par-
ticipants, e.g., I, you, and we) are an es-
sential part of many expressed meanings.
In most conversational settings, however,
many such expressions have numerous po-
tential meanings, are frequently vague,
and are highly dependent on social and sit-
uational context. This is a significant chal-
lenge to conversational language under-
standing systems ? one which has seen
little attention in annotation studies. In this
paper, we present a method for annotat-
ing verbal reference to people in conver-
sational speech, with a focus on reference
to conversation participants. Our goal is
to provide a resource that tackles the is-
sues of vagueness, ambiguity, and contex-
tual dependency in a nuanced yet reliable
way, with the ultimate aim of supporting
work on summarization and information
extraction for conversation.
1 Introduction
Spoken conversation ? the face-to-face verbal in-
teraction we have every day with colleagues, fam-
ily, and friends ? is the most natural setting for
language use. It is how we learn to use language
and is universal to the world?s societies. This
makes it an ideal subject for research on the ba-
sic nature of language and an essential subject for
the development of technologies supporting natu-
ral communication. In this paper, we describe our
research on designing and applying an annotation
procedure for a problem of particular relevance to
conversational language ? person reference.
The procedure is a coreference annotation of all
references to people, and the focus of our scheme
is on distinguishing different types of participant
reference (references to the conversation?s partic-
ipants), the predominant type of person reference
in face-to-face multi-party conversation. Partici-
pant reference is exemplified by the use of proper
names such as James or most commonly by the
pronouns I, you, and we.
Participant reference plays an essential role in
many of the most important types of expressed
meanings and actions in conversation, including
subjective language, inter-personal agreements,
commitments, narrative story-telling, establishing
social relationships, and meta-discourse. In fact,
some person-referring words are the most frequent
words in conversation.1
Perhaps contrary to intuition, however, in-
terpreting person-referring expressions can be
rather complex. Person-reference interpretation
is strongly dependent on social, situational, and
discourse context. The words you and we
are especially problematic. Either can be used
for generic, plural, or singular reference, as
addressee-inclusive or addressee-exclusive, in ref-
erence to hypothetical individuals or non-human
entities, or even metonymically in reference to ob-
jects connected to individuals (Mu?hlha?usler and
Harre?, 1990; Wales, 1996). In addition, these and
many other issues are not simply occasional prob-
lems but arise regularly.
Consider the following utterance from the AMI
corpus of remote control design meetings, which
is typical of the corpus in terms of complexity of
person-reference.
1The words I and you are the most frequently used nom-
inals in several conversational corpora, including Switch-
board (Godfrey et al, 1992) and the AMI Meeting Cor-
pus (McCowan et al, 2005). In the British National Corpus
they are the two most common of any words in the demo-
graphic (i.e., conversational) subcorpus (Burnard, 2007), and
Google?s Web 1T 5-gram statistics (Brants and Franz, 2006)
list I and you as more frequent even than the word it. The
word we falls within the top 10 most frequent words in all of
these corpora.
256
?Current remote controls do not match well with
the operating behaviour of the user overall. For
example, you can see below there, seventy five
percent of users zap a lot, so you?ve got your
person sunk back in the sofa channel-hopping.?
As this example demonstrates, person-referring
expressions have many potential meanings and are
often vague or non-specific. In this case, ?the
user? refers to a non-specific representative of a
hypothetical group, which is referred to itself as
?users.? The first use of ?you? refers to the ad-
dressees, but the second use has a more ?generic?
meaning whilst retaining an addressee-oriented
meaning as well. The phrase ?your person? refers
to a specific hypothetical example of the ?users?
referred to previously.
1.1 Purpose of the Annotations
The annotation research we describe here aims at
addressing the fact that if conversational language
applications are to be useful and effective (our
interest is primarily with abstractive summariza-
tion), then accurate interpretation of reference to
the conversation?s participants is of critical impor-
tance. Our work looks at language as a means for
action (Clark, 1996), and our focus is on those ac-
tions that the participants themselves consider as
relevant and salient, such as the events occurring
in a meeting that might appear in the minutes of
the meeting. For our system to identify, distin-
guish, or describe such events, it is essential for
it to understand the participants? roles and rela-
tionships to those events through interpreting their
linguistic expression within the dialogue. This in-
cludes understanding direct reference to partici-
pants and recognizing discourse structure through
evidence of referential coherence.
Another aim of our research is to increase un-
derstanding of the nature of participant reference
through presenting a nuanced yet reliable set of
type and property distinctions. We propose novel
distinctions concerning three main issues. The
first distinction concerns vagueness and indetermi-
nacy, which is often exploited by speakers when
using words such as you, they, and we. Our aim
is to provide a reliable basis for making an ex-
plicit distinction between specific and vague uses,
motivated by usefulness to the aforementioned ap-
plications. The second distinction concerns an
issue faced frequently in informal conversation,
where words typically used to do person-referring
are also commonly used in non-person-referring
ways. A principal goal is thus establishing reliable
person/non-person and referential/non-referential
distinctions for these words. The third issue con-
cerns addressing roles (i.e., speaker, addressee,
and non-addressee), which we propose can be a
useful means for further distinguishing between
different types of underspecified and generic refer-
ences, beyond the specific/underspecified/generic
distinctions made in schemes such as ACE (Lin-
guistic Data Consortium, 2008).
1.2 Summary and Scope of Contributions
The work described in this paper includes the de-
sign of an annotation procedure and a statistical
analysis of a corpus of annotations and their re-
liability. The procedure we propose (Section 3)
is based on a simple non-anaphoric coreference-
like scheme, modest in comparison to much pre-
vious work. The produced dataset (Section 4) in-
cludes annotations of 11,000 occasions of person-
referring in recorded workplace meetings. Our
analysis of the dataset includes a statistical sum-
mary of interesting results (Section 4.1) and an
analysis of inter-coder agreement (with discussion
of specific disagreements) for the introduced dis-
tinctions (Section 4.2).
Though our annotation procedure is designed
primarily for multi-party spoken conversation,
some of the central issues that concern us, such
as addressee inclusion and vagueness, arise in
textual and non-conversational settings as well.
Our scheme therefore has relevance to general
work on reference annotation, though principally
to settings where social relationships between
the participants (i.e., speakers/authors and ad-
dressees/readers) are important.
2 Related Annotation Schemes
Previous work on reference annotation has cov-
ered a wide range of issues surrounding reference
generally. It is useful to categorize this work ac-
cording to the natural language processing tasks
the annotations are designed to support.
2.1 Schemes for anaphora and generation
Several schemes have been designed with the goal
of testing linguistic theoretical models of dis-
course structure or for use in the study of discourse
processing problems like anaphora resolution and
reference generation. These schemes have been
applied to both text and dialogue and label dis-
257
course references with a rich set of syntactic, se-
mantic, and pragmatic properties. For example,
the DRAMA scheme (Passonneau, 1997) and the
GNOME scheme (Poesio, 2000; Poesio, 2004) in-
clude labels for features such as bridging relation
type and NP type in addition to a rich representa-
tion of referent semantics. Other schemes label an-
imacy, prosody, and information structure to study
their relationship to the organization and salience
of discourse reference (Nissim et al, 2004; Cal-
houn et al, 2005). Recent developments include
the explicit handling of anaphoric ambiguity and
discourse deixis (Poesio and Artstein, 2008).
Despite the depth and detail of these schemes,
participant reference has not been their main con-
cern. The annotations by Poesio et al (2000;
2004) include dialogue source material, but the
rather constrained interactional situations do not
elicit a rich set of references to participants. The
scheme thus employs simple default labels for
words like I and you. The work by Nissim et
al., (2004) is an annotation of the Switchboard cor-
pus (Godfrey et al, 1992), which contains only
two participants who are neither co-present nor
socially connected. Participant reference is thus
rather constrained. Other than labeling corefer-
entiality, the Nissim scheme includes only a sin-
gle distinction between referential and generic in-
stances of the word you.
2.2 Schemes for information extraction
In contrast to the schemes described above, which
are mainly driven toward investigating linguistic
theories of discourse processing, some reference
annotation projects are motivated instead by infor-
mation extraction applications. For these projects
(which includes our own), a priority is placed on
entity semantics and coreference to known entities
in the world. For example, the objective of the Au-
tomatic Content Extraction (ACE) program (Dod-
dington et al, 2004) is to recognize and extract
entities, events, and relations between them, di-
rectly from written and spoken sources, mostly
from broadcast news. The schemes thus focus
on identifying and labeling the properties of en-
tities in the real world, and then marking expres-
sions as referring to these entities. Recent work
in the ACE project has expanded the scope of
this task to include cross-document recognition
and resolution (Strassel et al, 2008). In the ACE
scheme (Linguistic Data Consortium, 2008), per-
son reference is a central component, and in the
broadcast conversation component of the corpus
there is an extensive inventory of participant refer-
ences. The annotation scheme contains a distinc-
tion between specific, underspecified, and general
entities, as well as a distinction between persons
and organizations.
Another closely related set of studies are four
recent investigations of second-person reference
resolution (Gupta et al, 2007a; Gupta et al,
2007b; Frampton et al, 2009; Purver et al,
2009). These studies are based upon a common
set of annotations of the word you in source mate-
rial from the Switchboard and ICSI Meeting cor-
pora. The purpose for the annotations was to
support learning of classifiers for two main prob-
lems: disambiguation of the generic/referential
distinction, and reference resolution for referential
cases. In addition to the generic/referential dis-
tinction and an addressing-based reference anno-
tation, the scheme employed special classes for re-
ported speech and fillers and allowed annotators to
indicate vague or difficult cases. Our work builds
directly upon this work by extending the annota-
tion scheme to all person-referring expressions.
3 Annotation Method
Our person-reference annotation method consists
of two main phases: a preliminary phase where
the first names of the conversation participants are
identified, and a subsequent person reference la-
beling process. The first phase is not of central
concern in this paper, though we provide a brief
summary below (Section 3.2). The primary focus
of this paper is the second phase (Section 3.3), dur-
ing which every instance of person-referring oc-
curring in a given meeting is labelled. We pro-
vide more detail concerning the most novel and
challenging aspects of the person-referring label-
ing process in Section 3.4 and present a brief sum-
mary of the annotation tool in Section 3.5.
3.1 Source Material
The source material is drawn from two source
corpora: the AMI corpus (McCowan et al,
2005), which contains experimentally-controlled
scenario-driven design meetings, and the ICSI cor-
pus (Janin et al, 2003), which contains naturally
occurring workplace meetings. All the meetings
have at least four participants and have an average
duration of about 45 minutes. In the AMI corpus,
258
the participants are experimental subjects who are
assigned institutional roles, e.g. project manager
and industrial designer. This helps to establish
controlled social relationships within the group,
but generally limits the types of person referring.
The ICSI meetings are naturally occurring and ex-
hibit complex pre-existing social relationships be-
tween the participants. Person referring in this cor-
pus is quite complex and often includes other in-
dividuals from the larger institution and beyond.
3.2 Labeling Participant Names
The first phase of annotation consists of identify-
ing the names of the participants. We perform this
task for every participant in every meeting in the
AMI and ICSI source corpora, which totals 275
unique participants in 246 meetings. Despite the
fact that the participants? are given anonymized
identifiers by the corpus creators, determining par-
ticipants? names is possible because name men-
tions are not excised from the speech transcript.
This allows identification of the names of any par-
ticipants who are referred to by name in the dia-
logue, as long as the referent is disambiguated by
contextual clues such as addressing.
To extract name information, the list of capi-
talized words in the speech transcript is scanned
manually for likely person names. This was done
manually due to the difficulty of training a suffi-
ciently robust named-entity recognizer for these
corpora. Proceeding through each meeting for
which any participant names are yet unidentified,
and taking each potential name token in order
of frequency of occurrence in that meeting, short
segments of the recording surrounding the occur-
rences were replayed. In most cases, the name was
used in reference to a participant and it was clear
from discourse context which participant was the
intended referent. In the AMI meetings, 158 of
223 (71%) of the participants? first names were
identified. In the ICSI meetings, 36 of 52 (69%)
were identified. While these numbers may seem
low, failure to determine a name was generally as-
sociated with a low level of participation of the
individual either in terms of amount of speech or
number of meetings attended. As such, the propor-
tion of utterances across both corpora for which
the speaker?s name is identified is actually 91%.
3.3 Person-reference Annotation
The second, principal phase of annotation con-
sists of annotating person-referring ? instances
of verbal reference to people. The recognition
of person-referring requires the annotator to si-
multaneously identify whether a referring event
has occurred, and whether the referent is a per-
son. In practice, this is divided into four an-
notation steps: markable identification, referent
identification, functional category labeling, and
co-reference linking. For non-specific references,
there is an additional step of labeling addressing
properties. For each meeting, annotators label ev-
ery instance of person-referring in every utterance
in the meeting, performing the steps in sequence
for each utterance. Section 4 describes the set of
meetings annotated. The UML diagram in Fig-
ure 1 depicts the formal data structure produced
by the procedure.2
The first step is markable identification, which
involves recognizing person-referring expres-
sions in the transcript. Only expressions that are
noun phrases are considered, and only the head
noun is actually labeled by the annotator ? the
extent of the expression is not labeled. These iden-
tified head nouns are called markables. Note,
however, that before human annotation begins, an
automatic process identifies occurrences of words
that are likely to be head nouns in person-referring
expressions. The list of words includes all per-
sonal pronouns except it, them, and they (these
are more likely to be non-person-referring in our
dataset) and the wh-pronouns (not labeled in our
scheme). It also includes any occurrences of
the previously identified proper names. Some of
the automatically identified words might not be
person-referring. Also, there may be instances of
person-referring that are not automatically iden-
tified. Annotators do not unmark any of the au-
tomatically identified words, even if they are not
person-referring. The resulting set of manually
and automatically identified words, which may or
may not be person-referring, constitute the com-
plete set of markables.
The second step is the labeling of person refer-
ents. Any people or groups of people that are re-
ferred to specifically and unambiguously (see Sec-
tion 3.4.3 for details) are added by the annotator
to a conversation referent list. The list is auto-
matically populated with each of the conversation
participants.
2The diagram may also be viewed informally as loosely
reflecting a decision tree for the main annotation steps. A
complete coding manual is available from the author?s web
site.
259
ATTR-QUANTIFIED-SUPERSET: Boolean
category: Enum: {
  FUNC-PREF-VOCATIVE,
  FUNC-PREF-INTRODUCTION, 
  FUNC-PREF-TROUBLE, 
  FUNC-PREF-DEFAULT }
Person-Referring Markable
1..*
1
Referent List
*
1
Transcript
*
1
category: Enum: {
  FUNC-FILLER, 
  FUNC-NON-PREF }
Non-Referring Markable
Person Referent
ATTR-SPEAKER-INCL: Boolean
ATTR-ADDRESSEE-INCL: Boolean
ATTR-OTHER-INCL: Boolean
Underspecified Referent (PERSON-OTHER)
id: String
NICKNAME: String
Specific Real Referent
PERSON-SINGLE PERSON-MULTIPLE
2..*
0..*
members
speaker: Participant
word: String
startTime: Double
Word
Conversation
Markable Word Non-Markable Word
Figure 1: A UML diagram depicting the data structure used to represent and store the annotations.
The third step consists of labeling markables
with a functional category (FUNC-*). The func-
tional categories serve two main purposes. They
are used to distinguish person-referring markables
from all others (corresponding to the two main
boxes in the diagram), and they are used to distin-
guish between specific dialogue purposes (the cat-
egories listed within the boxes, see Section 3.4.4).
The final step is to link the markables that were
labeled as person-referring to the appropriate ref-
erent in the referent list. This is only done for
specific and unambiguous referring. Otherwise,
the referent is said to be underspecified, and in-
stead of linking the markable to a referent, it is la-
beled with three binary addressing inclusion at-
tributes. Inclusion attributes label whether the
speaker, addressee, or any other individuals are in-
cluded in the set of people being referred to, given
the social, situational, and discourse context (de-
tails in Section 3.4.5).
3.4 Special Issues
3.4.1 Defining ?person? and ?referring?
To be person-referring, an expression must sat-
isfy two conditions. First, the expression?s pri-
mary contribution to the speaker?s intended mean-
ing or purpose must be either to identify, label,
describe, specify, or address. These are the ba-
sic types of referring. Second, the referent being
identified, labeled, etc., must be a person, which
we define to include any of the following: a dis-
tinct person in the real world; a fictitious or hypo-
thetical person; a human agent, perceiver, or par-
ticipant in a described event, scene, or fact; a class,
type, or kind of person, or representative thereof;
a specification or description of a person or set of
people; a (possibly vaguely defined) group or col-
lection of any of the above; the human race as a
whole, or a representative thereof.
If a noun phrase is used to do person-referring
as defined, the associated markable is labeled with
one of the four person-referring functional cat-
egories (FUNC-PREF-*). If a markable is not
person-referring (either non-referring or referring
to a non-person referent), it is labeled with the
functional category FUNC-NON-PREF. The one
exception to this is the use of a pre-defined list of
common discourse fillers such as you know and I
mean. When used as fillers, these are labeled with
the non-referential FUNC-FILLER category.
260
3.4.2 Joint action and referring ?trouble?
Annotators are asked to consider occasions of re-
ferring to be joint actions between the speaker and
the addressee(s) of the utterance. The annotator
assumes the role of an overhearer and considers
as referring any case where the speaker?s intended
purpose is to refer. If the instance of referring
is not successfully negotiated between the partic-
ipants (i.e., common ground is not achieved), but
the speaker?s intended purpose is to refer, then the
annotator marks this as FUNC-PREF-TROUBLE.
This is used to identify problematic cases for fu-
ture study.
3.4.3 Specific, Unambiguous Referring
Only the referents of specific, unambiguous re-
ferring to a person in the real world (PERSON-
SINGLE) are included in the conversation referent
list and made the subject of coreference annota-
tion. References to more than one such individual
can qualify (PERSON-MULTIPLE), but only if the
members are precisely enumerable and qualify in-
dividually. The motivation for this distinction is to
distinguish references that would be directly use-
ful to applications. Coreference for underspecified
references is not labeled.
3.4.4 Special Functional Categories
Two functional categories are used to distinguish
special uses of person-referring for subsequent
use in speaker name induction (the task of auto-
matically learning participants? names). The two
categories are FUNC-PREF-INTRODUCTION and
FUNC-PREF-VOCATIVE, which specify personal
introductions such as ?Hi, I?m John,? and vocative
addressing such as ?What do you think, Jane??
These categories are used only for proper names.
3.4.5 Addressing-based Inclusion Attributes
A major novelty in our annotation scheme is the
use of addressing-based distinctions for under-
specified referents. Rather than using the labels
?generic? or ?indeterminate?, we employ three bi-
nary attributes (ATTR-*-INCL) that label whether
the speaker, addressee or any other real individuals
are members of the set of people referred to.
The use of this distinction is informed by the no-
tion that addressing distinctions are of central im-
portance to the recognition of joint activity type,
structure, and participation roles. A generic pro-
noun, for example, will often have all three cat-
egories labeled positively. But as an example
of where this scheme creates a novel distinction,
consider the phrase ?You really take a beating
out there on the pitch!?, where the speaker is a
football player describing the nature of play to
someone who has never played the game. This
?generic? use of you, used in an activity of autobi-
ographical description, is intuitively interpreted as
not including the addressee (ATTR-ADDRESSEE-
INCL=FALSE) but including the speaker and others
(ATTR-{SPEAKER,OTHER}-INCL=TRUE). These
distinctions are hard to motivate linguistically yet
critical to identifying useful properties relating to
participation in the communicative activity.
3.4.6 Special or Difficult Cases
In some cases, an annotator can determine that a
reference is specific and unambiguous for the par-
ticipants but the annotator himself is unable to de-
termine the identity of the referent. This is gener-
ally due to a lack of contextual awareness such as
not having adequate video. In such cases, the an-
notator assigns a special REF-UNKNOWN referent.
Other difficult aspects of our annotation proce-
dure are covered in the annotation manual, includ-
ing handling of disfluencies, quantification, and
identifying lexical heads.
3.5 Annotation Tool
The annotations were collected using a software
tool we have designed for discrete event-based an-
notation of multi-modal corpora. The tool uses a
simple, low-latency text-based interface that dis-
plays multiple streams of discrete events in tempo-
ral order across the screen. In our case, the events
are time-synchronized words that are distributed
to different rows according to speaker. The inter-
face allows keyboard input only and is synchro-
nized with the MPlayer playback engine.
4 Results and Analysis
4.1 Statistical summary
The dataset consists of approximately 11,000 in-
dividually annotated referring expressions in 16
experimentally-controlled, scenario-driven design
meetings from the AMI corpus (McCowan et al,
2005) and 3 natural workplace meetings from
the ICSI corpus (Janin et al, 2003). Figure 2
shows, for each grammatical type of referring ex-
pression, the frequency of occurrence of the five
principal markable types, which are defined to
consist of the two non-person-referring functional
261
OTHER
QUANT
3PP
3PS
2P
1PP
1PS
FUNC?PREF / PERSON?SINGLE
FUNC?PREF / PERSON?MULTIPLE
FUNC?PREF / PERSON?OTHER
FUNC?NON?PREF
FUNC?FILLER
0 500 1000 2000 3000
Figure 2: Frequency of occurrence of referring
types for the whole corpus, by grammatical type
of the referring expression.
categories (FUNC-NON-PREF and FUNC-FILLER),
and a breakdown of person-referring according
to the type of person referent: a specific indi-
vidual (PERSON-SINGLE), multiple specific indi-
viduals (PERSON-MULTIPLE), or underspecified
(PERSON-OTHER). The grammatical types in-
clude a grouping of the personal pronouns by
grammatical person and number (1PS, 1PP, 2P,
3PS, 3PP), the quantified pronouns (QUANT), and
a group including all other expressions (OTHER).
Table 1 shows the relative frequency for the gram-
matical types and the most frequent expressions.
As is usually found in conversation, first-
person and second-person pronouns are the most
frequent, collectively comprising 82.0% of all
person-referring expressions. Of particular inter-
est, due to their high frequency and multiple possi-
ble referential meanings, are the 1PP and 2P cate-
gories (e.g., we and you), comprising respectively
24.6% and 23.7% of all person-referring expres-
Gram. Freq. Ent. Freq. words
(%) (bits)
1PS 33.7 .57 I, my, me
1PP 24.6 .67 we, our, us
2P 23.7 1.78 you, your, yours
3PS .9 .66 he, his, she
3PP 7.2 1.25 they, them, their
QUANT 1.0 1.14 everyone, everybody
OTHER 8.9 1.57 people, guys, user
Table 1: A statistical summary of all the mark-
ables in the dataset by grammatical type (gram.),
showing their frequency relative to all markables
(freq.), the entropy of the referring type given the
grammatical type (ent.), and a list of the most fre-
quent examples (freq. words).
sions. In Table 1, we show the information en-
tropy of the referring type, given the grammati-
cal category. This measures the uncertainty one
has about the type, given knowledge of only the
grammatical type of the expression. The analysis
reveals that second-person pronouns are a partic-
ularly challenging reference resolution problem,
with a broad and relatively even distribution across
referring types.
4.2 Reliability and Error Analysis
To show that our annotations are credible and suit-
able for empirical testing, we must establish that
the subjective distinctions defined in our scheme
may be applied by individuals other than the
scheme developers. To do this, we assess inter-
coder agreement between two independent anno-
tators on four meetings from the AMI corpus, us-
ing Cohen?s Kappa (Cohen, 1960). Each of the
decisions in the annotation procedure are assessed
separately: markable identification, labeling ref-
erentiality, labeling specificity of person refer-
ents, and labeling addressing inclusion attributes.
Because each decision depends on the previous,
we employ a hierarchical assessment procedure
that considers only instances where the annota-
tors have agreed on previous decisions. This kind
of multi-level assessment corresponds to that de-
scribed and used in Carletta et al, (1997).
Markables The first annotation decision of in-
terest is the identification of markables. Markables
are either automatically identified occurrences of
a pre-defined list of pronouns, or they are identi-
262
fied manually by the annotators. Agreement on
this task, assessed only for manually identified
words, was very good (?=.94). Error analysis
shows that the main issue with this decision was
not determining lexical heads, but rather deter-
mining whether phrases such as ?all age groups,?
?the older generation,? and ?the business market?
should be considered as referring to people or not.
Person referentiality The next annotation deci-
sion is between person-referring and non-person-
referring markables. For assessment of this
choice, we measure agreement on a three-way
categorization of the agreed markables as either
FUNC-NON-PREF, FUNC-FILLER, or one of the
FUNC-PREF-* categories. Agreement on this task
was good (?=.77). The only errors occurred on
first- and second-person pronouns and between the
FUNC-NON-PREF and FUNC-PREF-* categories.
Error analysis suggests confusion tends to occur
when pronouns are used with semantically light
verbs like go, get, and have, for example in phrases
such as ?there we go? and ?you?ve got the main
things on the front.? As in the latter example,
some of the difficult choices appear to involve de-
scriptions of states, which the speaker can choose
to express either from various participants? points
of view, as above, or alternatively without ex-
plicit subjectivity, e.g., ?the main things are on the
front.?
Specificity and cardinality The next choice we
assess is the decision between referring specif-
ically to a single person (PERSON-SINGLE), to
multiple people (PERSON-MULTIPLE), or as un-
derspecified (also referred to as PERSON-OTHER).
Agreement on this choice was very good (?=.91),
though considering only the difficult 1PP and 2P
grammatical categories (e.g., we and you), agree-
ment was less strong (?=.75). Note that due to the
hierarchical nature of the scheme, evaluation con-
sidered only cases where both annotators labeled
a word as person-referring. Errors on this decision
often involved ambiguities in addressing, where
one annotator believed a particular individual was
being addressed by you and the other thought the
whole group was being addressed. Another com-
mon disagreement was on cases such as ?we want
it to be original,? where we was interpreted by one
annotator as referring to the present group of par-
ticipants, but by the other as (presumably) refer-
ring to the organization to which the participants
belong.
Addressing inclusion attributes For the three
inclusion attributes for underspecified referents
(ATTR-*-INCL), agreement is calculated three
times, once for each of the binary attributes.
Agreement was good, though slightly problematic
for addressee inclusion (speaker ?=.72; addressee
?=.50; other ?=.66). Disagreements were mainly
for occurrences of you like the example of autobi-
ography in Section 3.4.5. For example, ?it?s your
best friend? was used to explain why a dog is the
speaker?s favorite animal, and the annotators dis-
agreed on whether the addressee was included.
5 Conclusion
We have presented an annotation scheme and a
set of annotations that address participant refer-
ence ? a conversational language problem that
has seen little previous annotation work. Our fo-
cus has been on eliciting novel distinctions that we
hypothesize will help us to distinguish, label, and
summarize conversational activities. We also ad-
dress the issues of vagueness, ambiguity, and con-
textual dependency in participant referring.
Based on analysis of inter-annotator agreement,
the major distinctions proposed by the scheme ap-
pear to be reliably codable. In addition, our sta-
tistical analysis shows that our dataset contains a
wide variety of participant references and should
be a useful resource for several reference resolu-
tion problems for conversation. Our novel method
for distinguishing specific reference to real indi-
viduals appears to be very reliably codable. Our
novel addressing-based distinctions for underspec-
ified reference are less reliable but adequate as a
resource for some dialogue structuring tasks.
Further work proposed for this task includes
labeling a variety of conversational and non-
conversation genres. Our immediate concern is to
apply our annotations in the training and/or test-
ing of machine learning approaches to discourse
segmentation and abstractive summarization.
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram, Version 1. Linguistic Data Consortium. Cat-
alog ID: LDC2006T13.
Lou Burnard, 2007. Reference Guide for the British
National Corpus (XML Edition). Research Tech-
nologies Service at Oxford University Computing
Services.
263
Sasha Calhoun, Malvina Nissim, Mark Steedman, and
Jason Brenier. 2005. A framework for annotating
information structure in discourse. In Proceedings
of the ACL Workshop on Frontiers in Corpus Anno-
tation II: Pie in the Sky.
Jean Carletta, Stephen Isard, Anne H. Anderson,
Gwyneth Doherty-Sneddon, Amy Isard, and Jacque-
line C. Kowtko. 1997. The reliability of a dialogue
structure coding scheme. Computational Linguis-
tics, 23(1):13?31.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37?46.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program: Tasks, data, and evaluation. In
Proc. LREC.
Matthew Frampton, Raquel Fernndez, Patrick Ehlen,
Mario Christoudias, Trevor Darrell, and Stanley Pe-
ters. 2009. Who is ?you?? Combining linguis-
tic and gaze features to resolve second-person ref-
erences in dialogue. In Proc. EACL.
John J. Godfrey, Edward Holliman, and J. McDaniel.
1992. SWITCHBOARD: Telephone speech corpus
for research and development. In Proc. ICASSP,
pages 517?520, San Francisco, CA.
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007a. Resolving ?you? in multi-
party dialog. In Proc. SIGdial, pages 227?230.
Surabhi Gupta, Matthew Purver, and Daniel Jurafsky.
2007b. Disambiguating between generic and refer-
ential ?you? in dialog. In Proc. ACL.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting cor-
pus. In Proc. ICASSP, volume 1, pages 364?367.
Linguistic Data Consortium, 2008. ACE (Au-
tomatic Content Extraction) English Annotation
Guidelines for Entities, Version 6.5. Down-
loaded from http://projects.ldc.upenn.
edu/ace/annotation/.
I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-
ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lin-
coln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Cor-
pus. In Proceedings of Measuring Behavior 2005,
the 5th International Conference on Methods and
Techniques in Behavioral Research, Wageningen,
Netherlands.
Peter Mu?hlha?usler and Rom Harre?. 1990. Pronouns
and People: The Linguistic Construction of Social
and Personal Identity. Blackwell, Oxford.
Malvina Nissim, Shipra Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for
information status in dialogue. In Proc. LREC.
R. Passonneau, 1997. Instructions for applying dis-
course reference annotation for multiple applica-
tions (DRAMA).
Massimo Poesio and Ron Artstein. 2008. Anaphoric
annotation in the ARRAU corpus. In Proc. LREC.
Massimo Poesio, 2000. The GNOME Annotation
Scheme Manual, Version 4. University of Edin-
burgh, HCRC and Informatics.
Massimo Poesio. 2004. Discourse annotation and se-
mantic annotation in the GNOME corpus. In Pro-
ceedings of the ACL 2004 Workshop on Discourse
Annotation, pages 72?79.
Matthew Purver, Raquel Fernndez, Matthew Framp-
ton, and Stanley Peters. 2009. Cascaded lexicalised
classifiers for second-person reference resolution.
In Proc. SIGdial, pages 306?309.
Stephanie Strassel, Mark Przybocki, Kay Peterson,
Zhiyi Song, and Kazuaki Maeda1. 2008. Linguistic
resources and evaluation techniques for evaluation
of cross-document automatic content extraction. In
Proc. LREC.
Katie Wales. 1996. Personal pronouns in present-day
English. Cambridge University Press, Cambridge.
264
