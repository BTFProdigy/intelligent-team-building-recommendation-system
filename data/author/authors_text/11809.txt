Proceedings of NAACL HLT 2007, pages 444?451,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Randomized Decoding for Selection-and-Ordering Problems
Pawan Deshpande, Regina Barzilay and David R. Karger
Computer Science and Articial Intelligence Laboratory
Massachusetts Institute of Technology
{pawand,regina,karger}@csail.mit.edu
Abstract
The task of selecting and ordering infor-
mation appears in multiple contexts in text
generation and summarization. For in-
stance, methods for title generation con-
struct a headline by selecting and order-
ing words from the input text. In this pa-
per, we investigate decoding methods that
simultaneously optimize selection and or-
dering preferences. We formalize decod-
ing as a task of finding an acyclic path
in a directed weighted graph. Since the
problem is NP-hard, finding an exact so-
lution is challenging. We describe a novel
decoding method based on a randomized
color-coding algorithm. We prove bounds
on the number of color-coding iterations
necessary to guarantee any desired likeli-
hood of finding the correct solution. Our
experiments show that the randomized de-
coder is an appealing alternative to a range
of decoding algorithms for selection-and-
ordering problems, including beam search
and Integer Linear Programming.
1 Introduction
The task of selecting and ordering information ap-
pears in multiple contexts in text generation and
summarization. For instance, a typical multidocu-
ment summarization system creates a summary by
selecting a subset of input sentences and ordering
them into a coherent text. Selection and ordering at
the word level is commonly employed in lexical re-
alization. For instance, in the task of title generation,
the headline is constructed by selecting and ordering
words from the input text.
Decoding is an essential component of the
selection-and-ordering process. Given selection and
ordering preferences, the task is to find a sequence of
elements that maximally satisfies these preferences.
One possible approach for finding such a solution
is to decompose it into two tasks: first, select a set
of words based on individual selection preferences,
and then order the selected units into a well-formed
sequence. Although the modularity of this approach
is appealing, the decisions made in the selection step
cannot be retracted. Therefore, we cannot guarantee
that selected units can be ordered in a meaningful
way, and we may end up with a suboptimal output.
In this paper, we investigate decoding methods
that simultaneously optimize selection and order-
ing preferences. We formalize decoding as find-
ing a path in a directed weighted graph.1 The
vertices in the graph represent units with associ-
ated selection scores, and the edges represent pair-
wise ordering preferences. The desired solution is
the highest-weighted acyclic path of a prespecified
length. The requirement for acyclicity is essential
because in a typical selection-and-ordering problem,
a well-formed output does not include any repeated
units. For instance, a summary of multiple docu-
ments should not contain any repeated sentences.
1We assume that the scoring function is local; that is, it is
computed by combining pairwise scores. In fact, the majority
of models that are used to guide ordering (i.e., bigrams) are local
scoring functions.
444
Since the problem is NP-hard, finding an exact
solution is challenging. We introduce a novel ran-
domized decoding algorithm2 based on the idea of
color-coding (Alon et al, 1995). Although the algo-
rithm is not guaranteed to find the optimal solution
on any single run, by increasing the number of runs
the algorithm can guarantee an arbitrarily high prob-
ability of success. The paper provides a theoretical
analysis that establishes the connection between the
required number of runs and the likelihood of find-
ing the correct solution.
Next, we show how to find an exact solution using
an integer linear programming (ILP) formulation.
Although ILP is NP-hard, this method is guaranteed
to compute the optimal solution. This allows us to
experimentally investigate the trade-off between the
accuracy and the efficiency of decoding algorithms
considered in the paper.
We evaluate the accuracy of the decoding algo-
rithms on the task of title generation. The decod-
ing algorithms introduced in the paper are compared
against beam search, a heuristic search algorithm
commonly used for selection-and-ordering and other
natural language processing tasks. Our experiments
show that the randomized decoder is an appealing al-
ternative to both beam search and ILP when applied
to selection-and-ordering problems.
2 Problem Formulation
In this section, we formally define the decoding task
for selection-and-ordering problems. First, we intro-
duce our graph representation and show an example
of its construction for multidocument summariza-
tion. (An additional example of graph construction
for title generation is given in Section 6.) Then, we
discuss the complexity of this task and its connec-
tion to classical NP-hard problems.
2.1 Graph Representation
We represent the set of selection units as the set of
vertices V in a weighted directed graph G. The
set of edges E represents pairwise ordering scores
between all pairs of vertices in V . We also add a
special source vertex s and sink vertex t. For each
vertex v in V , we add an edge from s to v and an
2The code is available at
http://people.csail.mit.edu/pawand/rand/
edge from v to t. We then define the set of all ver-
tices as V ? = V ? {s, t}, and the set of all edges as
E? = E ? {(s, v) ? v ? V } ? {(v, t) ? v ? V }.
To simplify the representation, we remove all ver-
tex weights in our graph structure and instead shift
the weight for each vertex onto its incoming edges.
For each pair of distinct vertices (v, u) ? V , we set
the weight of edge ev,u to be the sum of the loga-
rithms of the selection score for u and the pairwise
ordering score of (v, u).
We also enhance our graph representation by
grouping sets of vertices into equivalence classes.
We introduce these classes to control for redundancy
as required in many selection-and-ordering prob-
lems.3 For instance, in title generation, an equiva-
lence class may consist of morphological variants of
the same stem (i.e., examine and examination). Be-
cause a typical title is unlikely to contain more than
one word with the same stem, we can only select a
single representative from each class.
Our task is now to find the highest weighted
acyclic path starting at s and ending at t with k ver-
tices in between, such that no two vertices belong to
the same equivalence class.
2.2 Example: Decoding for Multidocument
Summarization
In multidocument summarization, the vertices in the
decoding graph represent sentences from input doc-
uments. The vertices may be organized into equiva-
lence classes that correspond to clusters of sentences
conveying similar information. The edges in the
graph represent the combination of the selection and
the ordering scores. The selection scores encode the
likelihood of a sentence to be extracted, while pair-
wise ordering scores capture coherence-based prece-
dence likelihood. The goal of the decoder is to find
the sequence of k non-redundant sentences that op-
timize both the selection and the ordering scores.
Finding an acyclic path with the highest weight will
achieve this goal.
3An alternative approach for redundancy control would be
to represent all the members of an equivalence class as a sin-
gle vertex in the graph. However, such an approach does not
allow us to select the best representative from the class. For in-
stance, one element in the equivalence class may have a highly
weighted incoming edge, while another may have a highly
weighted outgoing edge.
445
2.3 Relation to Classical Problems
Our path-finding problem may seem to be simi-
lar to the tractable shortest paths problem. How-
ever, the requirement that the path be long makes it
more similar to the the Traveling Salesman Problem
(TSP). More precisely, our problem is an instance of
the prize collecting traveling salesman problem, in
which the salesman is required to visit k vertices at
best cost (Balas, 1989; Awerbuch et al, 1995).
Since our problem is NP-hard, we might be pes-
simistic about finding an exact solution. But our
problem has an important feature: the length k of
the path we want to find is small relative to the num-
ber of vertices n. This feature distinguishes our task
from other decoding problems, such as decoding in
machine translation (Germann et al, 2001), that are
modeled using a standard TSP formulation. In gen-
eral, the connection between n and k opens up a new
range of solutions. For example, if we wanted to
find the best length-2 path, we could simply try all
subsets of 2 vertices in the graph, in all 2 possible
orders. This is a set of only O(n2) possibilities, so
we can check all to identify the best in polynomial
time.
This approach is very limited, however: in gen-
eral, its runtime of O(nk) for paths of length k
makes it prohibitive for all but the smallest values
of k. We cannot really hope to avoid the exponential
dependence on k, because doing so would give us
a fast solution to an NP-hard problem, but there is
hope of making the dependence ?less exponential.?
This is captured by the definition of xed parameter
tractability (Downey and Fellows, 1995). A prob-
lem is fixed parameter tractable if we can make the
exponential dependence on the parameter k indepen-
dent of the polynomial dependence on the problem
size n. This is the case for our problem: as we will
describe below, an algorithm of Alon et al can be
used to achieve a running time of roughly O(2kn2).
In other words, the path length k only exponentiates
a small constant, instead of the problem size n, while
the dependence on n is in fact quadratic.
3 Related Work
Decoding for selection-and-ordering problems is
commonly implemented using beam search (Banko
et al, 2000; Corston-Oliver et al, 2002; Jin and
Hauptmann, 2001). Being heuristic in nature, this
algorithm is not guaranteed to find an optimal so-
lution. However, its simplicity and time efficiency
make it a decoding algorithm of choice for a wide
range of NLP applications. In applications where
beam decoding does not yield sufficient accuracy,
researchers employ an alternative heuristic search,
A* (Jelinek, 1969; Germann et al, 2001). While in
some cases A* is quite effective, in other cases its
running time and memory requirements may equal
that of an exhaustive search. Time- and memory-
bounded modifications of A* (i.e., IDA-A*) do not
suffer from this limitation, but they are not guaran-
teed to find the exact solution. Nor do they pro-
vide bounds on the likelihood of finding the exact
solution. Newly introduced methods based on lo-
cal search can effectively examine large areas of a
search space (Eisner and Tromble, 2006), but they
still suffer from the same limitations.
As an alternative to heuristic search algorithms,
researchers also employ exact methods from com-
binatorial optimization, in particular integer linear
programming (Germann et al, 2001; Roth and Yih,
2004). While existing ILP solvers find the exact so-
lution eventually, the running time may be too slow
for practical applications.
Our randomized decoder represents an impor-
tant departure from previous approaches to decod-
ing selection-and-ordering problems. The theoreti-
cally established bounds on the performance of this
algorithm enable us to explicitly control the trade-
off between the quality and the efficiency of the de-
coding process. This property of our decoder sets it
apart from existing heuristic algorithms that cannot
guarantee an arbitrarily high probability of success.
4 Randomized Decoding with
Color-Coding
One might hope to solve decoding with a dynamic
program (like that for shortest paths) that grows an
optimal path one vertex at a time. The problem is
that this dynamic program may grow to include a
vertex already on the path, creating a cycle. One way
to prevent this is to remember the vertices used on
each partial path, but this creates a dynamic program
with too many states to compute efficiently.
Instead, we apply a color coding technique of
446
Alon et al(1995). The basic step of the algo-
rithm consists of randomly coloring the graph ver-
tices with a set of colors of size r, and using dy-
namic programming to find the optimum length-k
path without repeated colors. (Later, we describe
how to determine the number of colors r.) Forbid-
ding repeated colors excludes cycles as required, but
remembering only colors on the path requires less
state than remembering precisely which vertices are
on the path. Since we color randomly, any single it-
eration is not guaranteed to find the optimal path; in
a given coloring, two vertices along the optimal path
may be assigned the same color, in which case the
optimal path will never be selected. Therefore, the
whole process is repeated multiple times, increasing
the likelihood of finding an optimal path.
Our algorithm is a variant of the original color-
coding algorithm (Alon et al, 1995), which was de-
veloped to detect the existence of paths of length k
in an unweighted graph. We modify the original al-
gorithm to find the highest weighted path and also
to handle equivalence classes of vertices. In addi-
tion, we provide a method for determining the opti-
mal number of colors to use for finding the highest
weighted path of length k.
We first describe the dynamic programming algo-
rithm. Next, we provide a probabilistic bound on
the likelihood of finding the optimal solution, and
present a method for determining the optimal num-
ber of colors for a given value of k.
Dynamic Programming Recall that we began
with a weighted directed graphG to which we added
artificial start and end vertices s and t. We now posit
a coloring of that graph that assigns a color cv to
each vertex v aside from s and t. Our dynamic pro-
gram returns the maximum score path of length k+2
(including the artificial vertices s and t) from s to t
with no repeated colors.
Our dynamic program grows colorful paths?
paths with at most one vertex of each color. For
a given colorful path, we define the spectrum of
a path to be the set of colors (each used exactly
once) of nodes on the interior of the path?we ex-
clude the starting vertex (which will always be s)
and the ending vertex. To implement the dynamic
program, we maintain a table q[v, S] indexed by a
path-ending vertex v and a spectrum S. For vertex
v and spectrum S, entry q[v, S] contains the value
of the maximum-score colorful path that starts at s,
terminates at v, and has spectrum S in its interior.
We initialize the table with length-one paths:
q[v, ?] represents the path from s to v, whose spec-
trum is the empty set since there are no interior ver-
tices. Its value is set to the score of edge (s, v). We
then iterate the dynamic program k times in order
to build paths of length k + 1 starting at s. We ob-
serve that the optimum colorful path of length ` and
spectrum S from s to v must consist of an optimum
path from s to u (which will already have been found
by the dynamic program) concatenated to the edge
(u, v). When we concatenate (u, v), vertex u be-
comes an interior vertex of the path, and so its color
must not be in the preexisting path?s spectrum, but
joins the spectrum of the path we build. It follows
that
q[v, S] = max
(u,v)?G,cu?S,cv /?S
q[u, S?{cu}] +w(u, v)
After k iterations, for each vertex v we will have
a list of optimal paths from s to v of length k + 1
with all possible spectra. The optimum length-k+ 2
colorful path from s to t must follow the optimum
length-k + 1 path of some spectrum to some penul-
timate vertex v and then proceed to vertex t; we find
it by iterating over all such possible spectra and all
vertices v to determine argmaxv,Sq[v, S]+w(v, t).
Amplification The algorithm of Alon et al, and
the variant we describe, are somewhat peculiar in
that the probability of finding the optimal solu-
tion in one coloring iteration is quite small. But
this can easily be dealt with using a standard tech-
nique called amplication (Motwani and Raghavan,
1995). Suppose that the algorithm succeeds with
small probability p, but that we would like it to suc-
ceed with probability 1 ? ? where ? is very small.
We run the algorithm t = (1/p) ln 1/? times. The
probability that the algorithm fails every single run
is then (1 ? p)t ? e?pt = ?. But if the algorithm
succeeds on even one run, then we will find the op-
timum answer (by taking the best of the answers we
see).
No matter how many times we run the algo-
rithm, we cannot absolutely guarantee an optimal
answer. However, the chance of failure can easily be
driven to negligible levels?achieving, say, a one-in-
a-billion chance of failure requires only 20/p itera-
447
tions by the previous analysis.
Determining the number of colors Suppose that
we use r random colors and want to achieve a given
failure probability ?. The probability that the opti-
mal path has no repeated colors is:
1 ? r ? 1r ?
r ? 2
r ? ? ?
r ? (k ? 1)
r .
By the amplification analysis, the number of trials
needed to drive the failure probability to the desired
level will be inversely proportional to this quantity.
At the same time, the dynamic programming table
at each vertex will have size 2r (indexing on a bit
vector of colors used per path), and the runtime of
each trial will be proportional to this. Thus, the run-
ning time for the necessary number of trials Tr will
be proportional to
1 ? rr ? 1 ?
r
r ? 2 ? ? ?
r
r ? (k ? 1) ? 2
r
What r ? k should we choose to minimize this
quantity? To answer, let us consider the ratio:
Tr+1
Tr
=
(r + 1
r
)k
? r ? (k ? 1)r + 1 ? 2
= 2(1 + 1/r)k(1? k/(r + 1))
If this ratio is less than 1, then using r + 1 col-
ors will be faster than using r; otherwise it will be
slower. When r is very close to k, the above equa-
tion is tiny, indicating that one should increase r.
When r  k, the above equation is huge, indicating
one should decrease r. Somewhere in between, the
ratio passes through 1, indicating the optimum point
where neither increasing nor decreasing r will help.
If we write ? = k/r, and consider large k, then Tr+1Trconverges to 2e?(1??). Solving numerically to find
where this is equal to 1, we find ? ? .76804, which
yields a running time proportional to approximately
(4.5)k.
In practice, rather than using an (approximate)
formula for the optimum r, one should simply plug
all values of r in the range [k, 2k] into the running-
time formula in order to determine the best; doing
so takes negligible time.
5 Decoding with Integer Linear
Programming
In this section, we show how to formulate the
selection-and-ordering problem in the ILP frame-
work. We represent each edge (i, j) from vertex i
to vertex j with an indicator variable Ii,j that is set
to 1 if the edge is selected for the optimal path and 0
otherwise. In addition, the associated weight of the
edge is represented by a constant wi,j .
The objective is then to maximize the following
sum:
max
I
?
i?V
?
j?V
wi,jIi,j (1)
This sum combines the weights of edges selected to
be on the optimal path.
To ensure that the selected edges form a valid
acyclic path starting at s and ending at t, we intro-
duce the following constraints:
Source-Sink Constraints Exactly one edge orig-
inating at source s is selected:
?
j?V
Is,j = 1 (2)
Exactly one edge ending at sink t is selected:
?
i?V
Ii,t = 1 (3)
Length Constraint Exactly k + 1 edges are se-
lected: ?
i?V
?
j?V
Ii,j = k + 1 (4)
The k + 1 selected edges connect k + 2 vertices in-
cluding s and t.
Balance Constraints Every vertex v ? V has in-
degree equal to its out-degree:
?
i?V
Ii,v =
?
i?V
Iv,j ? v ? V ? (5)
Note that with this constraint, a vertex can have at
most one outgoing and one incoming edge.
Redundancy Constraints To control for redun-
dancy, we require that at most one representative
from each equivalence class is selected. Let Z be
a set of vertices that belong to the same equivalence
class. For every equivalence class Z, we force the
total out-degree of all vertices in Z to be at most 1.
448
s t
Figure 1: A subgraph that contains a cycle, while
satisfying constraints 2 through 5.
?
i?Z
?
j?V
Ii,j ? 1 ? Z ? V (6)
Acyclicity Constraints The constraints intro-
duced above do not fully prohibit the presence of
cycles in the selected subgraph. Figure 1 shows an
example of a selected subgraph that contains a cycle
while satisfying all the above constraints.
We force acyclicity with an additional set of vari-
ables. The variables fi,j are intended to number the
edges on the path from 1 to k+ 1, with the first edge
getting number fi,j = k + 1, and the last getting
number fi,j = 1. All other edges will get fi,j = 0.
To enforce this, we start by ensuring that only the
edges selected for the path (Ii,j = 1) get nonzero
f -values:
0 ? fi,j ? (k + 1) Ii,j ? i, j ? V (7)
When Ii,j = 0, this constraint forces fi,j = 0.
When Ii,j = 1, this allows 0 ? fi,j ? k+1. Now we
introduce additional variables and constraints. We
constrain demand variables dv by:
dv =
?
i?V
Ii,v ? v ? V ? ? {s} (8)
The right hand side sums the number of selected
edges entering v, and will therefore be either 0 or 1.
Next we add variables av and bv constrained by the
equations:
av =
?
i?V
fi,v (9)
bv =
?
i?V
fv,i (10)
Note that av sums over f values on all edges enter-
ing v. However, by the previous constraints those
f -values can only be nonzero on the (at most one)
selected edge entering v. So, av is simply the f -
value on the selected edge entering v, if one exists,
and 0 otherwise. Similarly, bv is the f -value on the
(at most one) selected edge leaving v.
Finally, we add the constraints
av ? bv = dv v 6= s (11)
bs = k + 1 (12)
at = 1 (13)
These last constraints let us argue, by induction, that
a path of length exactly k + 1 must run from s to t,
as follows. The previous constraints forced exactly
one edge leaving s, to some vertex v, to be selected.
The constraint bs = k+ 1 means that the f -value on
this edge must be k + 1. The balance constraint on
v means some edge must be selected leaving v. The
constraint av ? bv = dv means this edge must have
f -value k. The argument continues the same way,
building up a path. The balance constraints mean
that the path must terminate at t, and the constraint
that at = 1 forces that termination to happen after
exactly k + 1 edges.4
For those familiar with max-flow, our program
can be understood as follows. The variables I force
a flow, of value 1, from s to t. The variables f rep-
resent a flow with supply k + 1 at s and demand dv
at v, being forced to obey ?capacity constraints? that
let the flow travel only along edges with I = 1.
6 Experimental Set-Up
Task We applied our decoding algorithm to the task
of title generation. This task has been extensively
studied over the last six years (Banko et al, 2000; Jin
and Hauptmann, 2001). Title generation is a classic
selection-and-ordering problem: during title realiza-
tion, an algorithm has to take into account both the
likelihood of words appearing in the title and their
ordering preferences. In the previous approaches,
beam search has been used for decoding. Therefore,
it is natural to explore more sophisticated decoding
techniques like the ones described in this paper.
Our method for estimation of selection-and-
ordering preferences is based on the technique de-
scribed in (Banko et al, 2000). We compute the
4The network flow constraints allow us to remove the previ-
ously placed length constraint.
449
likelihood of a word in the document appearing in
the title using a maximum entropy classifier. Every
stem is represented by commonly used positional
and distributional features, such as location of the
first sentence that contains the stem and its TF*IDF.
We estimate the ordering preferences using a bigram
language model with Good-Turing smoothing.
In previous systems, the title length is either pro-
vided to a decoder as a parameter, or heuristics are
used to determine it. Since exploration of these
heuristics is not the focus of our paper, we provide
the decoder with the actual title length (as measured
by the number of content words).
Graph Construction We construct a decoding
graph in the following fashion. Every unique con-
tent word comprises a vertex in the graph. All the
morphological variants of a stem belong to the same
equivalence class. An edge (v, u) in the graph en-
codes the selection preference of u and the likeli-
hood of the transition from v to u.
Note that the graph does not contain any auxiliary
words in its vertices. We handle the insertion of aux-
iliary words by inserting additional edges. For every
auxiliary word x, we add one edge representing the
transition from v to u via x, and the selection pref-
erence of u. The auxiliary word set consists of 24
prepositions and articles extracted from the corpus.
Corpus Our corpus consists of 547 sections of a
commonly used undergraduate algorithms textbook.
The average section contains 609.2 words. A title,
on average, contains 3.7 words, among which 3.0 are
content words; the shortest and longest titles have 1
and 13 words respectively. Our training set consists
of the first 382 sections, the remaining 165 sections
are used for testing. The bigram language model is
estimated from the body text of all sections in the
corpus, consisting of 461,351 tokens.
To assess the importance of the acyclicity con-
straint, we compute the number of titles that have
repeated content words. The empirical findings sup-
port our assumption: 97.9% of the titles do not con-
tain repeated words.
Decoding Algorithms We consider three decod-
ing algorithms: our color-coding algorithm, ILP, and
beam search.5 The beam search algorithm can only
5The combination of the acyclicity and path length con-
straints require an exponential number of states for A* since
each state has to preserve the history information. This prevents
consider vertices which are not already in the path.6
To solve the ILP formulations, we employ a
Mixed Integer Programming solver lp solve which
implements the Branch-and-Bound algorithm. We
implemented the rest of the decoders in Python with
the Psyco speed-up module. We put substantial ef-
fort to optimize the performance of all of the al-
gorithms. The color-coding algorithm is imple-
mented using parallelized computation of coloring
iterations.
7 Results
Table 1 shows the performance of various decoding
algorithms considered in the paper. We first evalu-
ate each algorithm by the running times it requires
to find all the optimal solutions on the test set. Since
ILP is guaranteed to find the optimal solution, we
can use its output as a point of comparison. Table 1
lists both the average and the median running times.
For some of the decoding algorithms, the difference
between the two measurements is striking ? 6,536
seconds versus 57.3 seconds for ILP. This gap can be
explained by outliers which greatly increase the av-
erage running time. For instance, in the worst case,
ILP takes an astounding 136 hours to find the opti-
mal solution. Therefore, we base our comparison on
the median running time.
The color-coding algorithm requires a median
time of 9.7 seconds to find an optimal solution com-
pared to the 57.3 seconds taken by ILP. Furthermore,
as Figure 2 shows, the algorithm converges quickly:
just eleven iterations are required to find an optimal
solution in 90% of the titles, and within 35 itera-
tions all of the solutions are found. An alternative
method for finding optimal solutions is to employ a
beam search with a large beam size. We found that
for our test set, the smallest beam size that satisfies
this condition is 1345, making it twenty-three times
slower than the randomized decoder with respect to
the median running time.
Does the decoding accuracy impact the quality of
the generated titles? We can always trade speed for
accuracy in heuristic search algorithms. As an ex-
treme, consider a beam search with a beam of size
1: while it is very fast with a median running time
us from applying A* to this problem.
6Similarly, we avoid redundancy by disallowing two vertices
from the same equivalence class to belong to the same path.
450
Average (s) Median (s) ROUGE-L Optimal Solutions (%)
Beam 1 0.6 0.4 0.0234 0.0
Beam 80 28.4 19.3 0.2373 64.8
Beam 1345 368.6 224.4 0.2556 100.0
ILP 6,536.2 57.3 0.2556 100.0
Color-coding 73.8 9.7 0.2556 100.0
Table 1: Running times in seconds, ROUGE scores, and percentage of optimal solutions found for each of
the decoding algorithms.
 0
 20
 40
 60
 80
 100
 0  5  10  15  20  25  30  35
Ex
ac
t S
ol
ut
io
ns
 (%
)
Iterations
Figure 2: The proportion of exact solutions found
for each iteration of the color coding algorithm.
of less than one second, it is unable to find any of
the optimal solutions. The titles generated by this
method have substantially lower scores than those
produced by the optimal decoder, yielding a 0.2322
point difference in ROUGE scores. Even a larger
beam size such as 80 (as used by Banko et al (2000))
does not match the title quality of the optimal de-
coder.
8 Conclusions
In this paper, we formalized the decoding task for
selection-and-ordering as a problem of finding the
highest-weighted acyclic path in a directed graph.
The presented decoding algorithm employs random-
ized color-coding, and can closely approximate the
ILP performance, without blowing up the running
time. The algorithm has been tested on title genera-
tion, but the decoder is not specific to this task and
can be applied to other generation and summariza-
tion applications.
9 Acknowledgements
The authors acknowledge the support of the Na-
tional Science Foundation (CAREER grant IIS-
0448168 and grant IIS-0415865). We also would
like to acknowledge the MIT NLP group and the
anonymous reviewers for valuable comments.
References
N. Alon, R. Yuster, U. Zwick. 1995. Color-coding. Jour-
nal of the ACM (JACM), 42(4):844?856.
B. Awerbuch, Y. Azar, A. Blum, S. Vempala. 1995.
Improved approximation guarantees for minimum-
weight k-trees and prize-collecting salesmen. In Pro-
ceedings of the STOC, 277?283.
E. Balas. 1989. The prize collecting traveling salesman
problem. Networks, 19:621?636.
M. Banko, V. O. Mittal, M. J. Witbrock. 2000. Headline
generation based on statistical translation. In Proceed-
ings of the ACL, 318?325.
S. Corston-Oliver, M. Gamon, E. Ringger, R. Moore.
2002. An overview of amalgam: A machine-learned
generation module. In Proceedings of INLG, 33?40.
R. G. Downey, M. R. Fellows. 1995. Fixed-parameter
tractability and completeness II: On completeness for
W [1]. Theoretical Computer Science, 141(1?2):109?
131.
J. Eisner, R. W. Tromble. 2006. Local search with very
large-scale neighborhoods for optimal permutations
in machine translation. In Proceedings of the HLT-
NAACL Workshop on Computationally Hard Problems
and Joint Inference in Speech and Language Process-
ing.
U. Germann, M. Jahr, K. Knight, D. Marcu, K. Yamada.
2001. Fast decoding and optimal decoding for ma-
chine translation. In Proceedings of the EACL/ACL,
228?235.
F. Jelinek. 1969. A fast sequential decoding algorithm
using a stack. IBM Research Journal of Research and
Development.
R. Jin, A. G. Hauptmann. 2001. Automatic title genera-
tion for spoken broadcast news. In Proceedings of the
HLT, 1?3.
R. Motwani, P. Raghavan. 1995. Randomized Algo-
rithms. Cambridge University Press, New York, NY.
D. Roth, W. Yih. 2004. A linear programming formula-
tion for global inference in natural language tasks. In
Proceedings of the CONLL, 1?8.
451
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 371?379,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Global Models of Document Structure Using Latent Permutations
Harr Chen, S.R.K. Branavan, Regina Barzilay, David R. Karger
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{harr, branavan, regina, karger}@csail.mit.edu
Abstract
We present a novel Bayesian topic model for
learning discourse-level document structure.
Our model leverages insights from discourse
theory to constrain latent topic assignments in
a way that reflects the underlying organiza-
tion of document topics. We propose a global
model in which both topic selection and order-
ing are biased to be similar across a collection
of related documents. We show that this space
of orderings can be elegantly represented us-
ing a distribution over permutations called the
generalized Mallows model. Our structure-
aware approach substantially outperforms al-
ternative approaches for cross-document com-
parison and single-document segmentation.1
1 Introduction
In this paper, we introduce a novel latent topic model
for the unsupervised learning of document structure.
Traditional topic models assume that topics are ran-
domly spread throughout a document, or that the
succession of topics in a document is Markovian.
In contrast, our approach takes advantage of two
important discourse-level properties of text in de-
termining topic assignments: first, that each docu-
ment follows a progression of nonrecurring coher-
ent topics (Halliday and Hasan, 1976); and sec-
ond, that documents from the same domain tend
to present similar topics, in similar orders (Wray,
2002). We show that a topic model incorporat-
ing these long-range dependencies outperforms al-
1Code, data, and annotations used in this work are available
at http://groups.csail.mit.edu/rbg/code/mallows/
ternative approaches for segmentation and cross-
document comparison.
For example, consider a collection of encyclope-
dia articles about cities. The first constraint captures
the notion that a single topic, such as Architecture,
is expressed in a contiguous block within the docu-
ment, rather than spread over disconnected sections.
The second constraint reflects our intuition that all
of these related articles will generally mention some
major topics associated with cities, such as History
and Culture, and will often exhibit similar topic or-
derings, such as placing History before Culture.
We present a Bayesian latent topic model over re-
lated documents that encodes these discourse con-
straints by positing a single distribution over a doc-
ument?s entire topic structure. This global view on
ordering is able to elegantly encode discourse-level
properties that would be difficult to represent using
local dependencies, such as those induced by hid-
den Markov models. Our model enforces that the
same topic does not appear in disconnected portions
of the topic sequence. Furthermore, our approach
biases toward selecting sequences with similar topic
ordering, by modeling a distribution over the space
of topic permutations.
Learning this ordering distribution is a key tech-
nical challenge in our proposed approach. For this
purpose, we employ the generalized Mallows model,
a permutation distribution that concentrates proba-
bility mass on a small set of similar permutations.
It directly captures the intuition of the second con-
straint, and uses a small parameter set to control how
likely individual topics are to be reordered.
We evaluate our model on two challenging
371
document-level tasks. In the alignment task, we aim
to discover paragraphs across different documents
that share the same topic. We also consider the seg-
mentation task, where the goal is to partition each
document into a sequence of topically coherent seg-
ments. We find that our structure modeling approach
substantially outperforms state-of-the-art baselines
for both tasks. Furthermore, we demonstrate the im-
portance of explicitly modeling a distribution over
topic permutations; our model yields significantly
better results than variants that either use a fixed or-
dering, or are order-agnostic.
2 Related Work
Topic and ContentModels Our work is grounded
in topic modeling approaches, which posit that la-
tent state variables control the generation of words.
In earlier topic modeling work such as latent Dirich-
let alocation (LDA) (Blei et al, 2003; Griffiths and
Steyvers, 2004), documents are treated as bags of
words, where each word receives a separate topic
assignment; the topic assignments are auxiliary vari-
ables to the main task of language modeling.
More recent work has attempted to adapt the con-
cepts of topic modeling to more sophisticated repre-
sentations than a bag of words; they use these rep-
resentations to impose stronger constraints on topic
assignments (Griffiths et al, 2005; Wallach, 2006;
Purver et al, 2006; Gruber et al, 2007). These
approaches, however, generally model Markovian
topic or state transitions, which only capture lo-
cal dependencies between adjacent words or blocks
within a document. For instance, content mod-
els (Barzilay and Lee, 2004; Elsner et al, 2007)
are implemented as HMMs, where the states cor-
respond to topics of domain-specific information,
and transitions reflect pairwise ordering prefer-
ences. Even approaches that break text into con-
tiguous chunks (Titov and McDonald, 2008) as-
sign topics based on local context. While these
locally constrained models can implicitly reflect
some discourse-level constraints, they cannot cap-
ture long-range dependencies without an explosion
of the parameter space. In contrast, our model cap-
tures the entire sequence of topics using a compact
representation. As a result, we can explicitly and
tractably model global discourse-level constraints.
Modeling Ordering Constraints Sentence order-
ing has been extensively studied in the context of
probabilistic text modeling for summarization and
generation (Barzilay et al, 2002; Lapata, 2003;
Karamanis et al, 2004). The emphasis of that body
of work is on learning ordering constraints from
data, with the goal of reordering new text from the
same domain. Our emphasis, however, is on ap-
plications where ordering is already observed, and
how that ordering can improve text analysis. From
the methodological side, that body of prior work is
largely driven by local pairwise constraints, while
we aim to encode global constraints.
3 Problem Formulation
Our document structure learning problem can be for-
malized as follows. We are given a corpus of D
related documents. Each document expresses some
subset of a common set of K topics. We assign a
single topic to each paragraph,2 incorporating the
notion that paragraphs are internally topically con-
sistent (Halliday and Hasan, 1976). To capture the
discourse constraint on topic progression described
in Section 1, we require that topic assignments be
contiguous within each document.3 Furthermore,
we assume that the underlying topic sequences ex-
hibit similarity across documents. Our goal is to re-
cover a topic assignment for each paragraph in the
corpus, subject to these constraints.
Our formulation shares some similarity with the
standard LDA setup, in that a common set of topics
is assigned across a collection of documents. How-
ever, in LDA each word?s topic assignment is con-
ditionally independent, following the bag of words
view of documents. In contrast, our constraints on
how topics are assigned let us connect word distri-
butional patterns to document-level topic structure.
4 Model
We propose a generative Bayesian model that ex-
plains how a corpus of D documents, given as se-
quences of paragraphs, can be produced from a set
of hidden topic variables. Topic assignments to each
2Note that our analysis applies equally to other levels of tex-
tual granularity, such as sentences.
3That is, if paragraphs i and j are assigned the same topic,
every paragraph between them must have that topic.
372
paragraph, ranging from 1 to K, are the model?s
final output, implicitly grouping topically similar
paragraphs. At a high level, the process first selects
the bag of topics to be expressed in the document,
and how they are ordered; these topics then deter-
mine the selection of words for each paragraph.
For each document dwithNd paragraphs, we sep-
arately generate a bag of topics td and a topic order-
ing pid. The unordered bag of topics, which contains
Nd elements, expresses how many paragraphs of the
document are assigned to each of theK topics. Note
that some topics may not appear at all. Variable td
is constructed by taking Nd samples from a distri-
bution over topics ? , a multinomial representing the
probability of each topic being expressed. Sharing
? between documents captures the intuition that cer-
tain topics are more likely across the entire corpus.
The topic ordering variable pid is a permutation
over the numbers 1 through K that defines the order
in which topics appear in the document. We draw pid
from the generalized Mallows model, a distribution
over permutations that we explain in Section 4.1. As
we will see, this particular distribution biases the
permutation selection to be close to a single cen-
troid, reflecting the discourse constraint of prefer-
ring similar topic structures across documents.
Together, a document?s bag of topics td and or-
dering pid determine the topic assignment zd,p for
each of its paragraphs. For example, in a corpus
with K = 4, a seven-paragraph document d with
td = {1, 1, 1, 1, 2, 4, 4} and pid = (2 4 3 1) would
induce the topic sequence zd = (2 4 4 1 1 1 1). The
induced topic sequence zd can never assign the same
topic to two unconnected portions of a document,
thus satisfying the constraint of topic contiguity.
As with LDA, we assume that each topic k is as-
sociated with a language model ?k. The words of a
paragraph assigned to topic k are then drawn from
that topic?s language model ?k.
Before turning to a more formal discussion of the
generative process, we first provide background on
the permutation model for topic ordering.
4.1 The Generalized Mallows Model
A central challenge of the approach we take is mod-
eling the distribution over possible topic permuta-
tions. For this purpose we use the generalized Mal-
lows model (GMM) (Fligner and Verducci, 1986;
Lebanon and Lafferty, 2002; Meila? et al, 2007),
which exhibits two appealing properties in the con-
text of this task. First, the model concentrates proba-
bility mass on some ?canonical? ordering and small
perturbations of that ordering. This characteris-
tic matches our constraint that documents from the
same domain exhibit structural similarity. Second,
its parameter set scales linearly with the permuta-
tion length, making it sufficiently constrained and
tractable for inference. In general, this distribution
could potentially be applied to other NLP applica-
tions where ordering is important.
Permutation Representation Typically, permuta-
tions are represented directly as an ordered sequence
of elements. The GMM utilizes an alternative rep-
resentation defined as a vector (v1, . . . , vK?1) of in-
version counts with respect to the identity permuta-
tion (1, . . . ,K). Term vj counts the number of times
a value greater than j appears before j in the permu-
tation.4 For instance, given the standard-form per-
mutation (3 1 5 2 4), v2 = 2 because 3 and 5 appear
before 2; the entire inversion count vector would be
(1 2 0 1). Every vector of inversion counts uniquely
identifies a single permutation.
The Distribution The GMM assigns proba-
bility mass according to the distance of a
given permutation from the identity permutation
{1, . . . ,K}, based on K ? 1 real-valued parameters
(?1, . . . ?K?1).5 Using the inversion count represen-
tation of a permutation, the GMM?s probability mass
function is expressed as an independent product of
probabilities for each vj :
GMM(v | ?) = e
??j ?jvj
?(?)
=
n?1?
j=1
e??jvj
?j(?j) , (1)
where ?j(?j) is a normalization factor with value:
?j(?j) = 1? e
?(K?j+1)?j
1? e??j .
4The sum of a vector of inversion counts is simply that per-
mutation?s Kendall?s ? distance to the identity permutation.
5In our work we take the identity permutation to be the fixed
centroid, which is a parameter in the full GMM. As we explain
later, our model is not hampered by this apparent restriction.
373
Due to the exponential form of the distribution, re-
quiring that ?j > 0 constrains the GMM to assign
highest probability mass to each vj being zero, cor-
responding to the identity permutation. A higher
value for ?j assigns more probability mass to vj be-
ing close to zero, biasing j to have fewer inversions.
The GMM elegantly captures our earlier require-
ment for a probability distribution that concentrates
mass around a global ordering, and uses few param-
eters to do so. Because the topic numbers in our
task are completely symmetric and not linked to any
extrinsic observations, fixing the identity permuta-
tion to be that global ordering does not sacrifice any
representational power. Another major benefit of
the GMM is its membership in the exponential fam-
ily of distributions; this means that it is particularly
amenable to a Bayesian representation, as it admits
a natural conjugate prior:
GMM0(?j | vj,0, ?0) ? e(??jvj,0?log?j(?j))?0 . (2)
Intuitively, this prior states that over ?0 prior trials,
the total number of inversions was ?0vj,0. This dis-
tribution can be easily updated with the observed vj
to derive a posterior distribution.6
4.2 Formal Generative Process
We now fully specify the details of our model. We
observe a corpus of D documents, each an ordered
sequence of paragraphs, and a specification of a
number of topics K. Each paragraph is represented
as a bag of words. The model induces a set of hid-
den variables that probabilistically explain how the
words of the corpus were produced. Our final de-
sired output is the distributions over the paragraphs?
hidden topic assignment variables. In the following,
variables subscripted with 0 are fixed prior hyperpa-
rameters.
1. For each topic k, draw a language model ?k ?
Dirichlet(?0). As with LDA, these are topic-
specific word distributions.
2. Draw a topic distribution ? ? Dirichlet(?0),
which expresses how likely each topic is to ap-
pear regardless of position.
6Because each vj has a different range, it is inconvenient
to set the prior hyperparameters vj,0 directly. In our work, we
instead fix the mode of the prior distribution to a value ?0, which
works out to setting vj,0 = 1exp(?0)?1 ? K?j+1exp((K?j+1)?0)?1 .
3. Draw the topic ordering distribution parame-
ters ?j ? GMM0(?0, ?0) for j = 1 to K ? 1.
These parameters control how rapidly probabil-
ity mass decays for having more inversions for
each topic. A separate ?j for every topic allows
us to learn that some topics are more likely to
be reordered than others.
4. For each document d with Nd paragraphs:
(a) Draw a bag of topics td by sampling Nd
times from Multinomial(?).
(b) Draw a topic ordering pid by sampling a
vector of inversion counts vd ? GMM(?).
(c) Compute the vector of topic assignments
zd for document d?s paragraphs, by sorting
td according to pid.7
(d) For each paragraph p in document d:
i. Sample each word wd,p,j according to
the language model of p: wd,p,j ?
Multinomial(?zd,p).
5 Inference
The variables that we aim to infer are the topic as-
signments z of each paragraph, which are deter-
mined by the bag of topics t and ordering pi for each
document. Thus, our goal is to estimate the marginal
distributions of t and pi given the document text.
We accomplish this inference task through Gibbs
sampling (Bishop, 2006). A Gibbs sampler builds
a Markov chain over the hidden variable state space
whose stationary distribution is the actual posterior
of the joint distribution. Each new sample is drawn
from the distribution of a single variable conditioned
on previous samples of the other variables. We can
?collapse? the sampler by integrating over some of
the hidden variables in the model, in effect reducing
the state space of the Markov chain. Collapsed sam-
pling has been previously demonstrated to be effec-
tive for LDA and its variants (Griffiths and Steyvers,
2004; Porteous et al, 2008; Titov and McDonald,
2008). Our sampler integrates over all but three sets
7Multiple permutations can contribute to the probability of a
single document?s topic assignments zd, if there are topics that
do not appear in td. As a result, our current formulation is bi-
ased toward assignments with fewer topics per document. In
practice, we do not find this to negatively impact model perfor-
mance.
374
of hidden variables: bags of topics t, orderings pi,
and permutation inversion parameters ?. After a
burn-in period, we treat the last samples of t and
pi as a draw from the true posterior.
Document Probability As a preliminary step,
consider how to calculate the probability of a single
document?s words wd given the document?s para-
graph topic assignments zd, and other documents
and their topic assignments. Note that this proba-
bility is decomposable into a product of probabil-
ities over individual paragraphs, where paragraphs
with different topics have conditionally independent
word probabilities. Let w?d and z?d indicate the
words and topic assignments to documents other
than d, and W be the vocabulary size. The proba-
bility of the words in d is then:
P (wd | z,w?d, ?0)
=
K?
k=1
?
?k
P (wd | zd, ?k)P (?k | z,w?d, ?0)d?k
=
K?
k=1
DCM({wd,i : zd,i = k}
| {w?d,i : z?d,i = k}, ?0), (3)
where DCM(?) refers to the Dirichlet compound
multinomial distribution, the result of integrat-
ing over multinomial parameters with a Dirichlet
prior (Bernardo and Smith, 2000). For a Dirichlet
prior with parameters ? = (?1, . . . , ?W ), the DCM
assigns the following probability to a series of ob-
servations x = {x1, . . . , xn}:
DCM(x | ?) = ?(
?
j ?j)?
j ?(?j)
W?
i=1
?(N(x, i) + ?i)
?(|x|+?j ?j)
,
where N(x, i) refers to the number of times word
i appears in x. Here, ?(?) is the Gamma function,
a generalization of the factorial for real numbers.
Some algebra shows that the DCM?s posterior prob-
ability density function conditioned on a series of
observations y = {y1, . . . , yn} can be computed by
updating each ?i with counts of how often word i
appears in y:
DCM(x | y, ?)
= DCM(x | ?1 +N(y, 1), . . . , ?W +N(y,W )).
(4)
Equation 3 and 4 will be used again to compute the
conditional distributions of the hidden variables.
We now turn to a discussion of how each individ-
ual random variable is resampled.
Bag of Topics First we consider how to resample
td,i, the ith topic draw for document d conditioned
on all other parameters being fixed (note this is not
the topic of the ith paragraph, as we reorder topics
using pid):
P (td,i = t | . . .)
? P (td,i = t | t?(d,i), ?0)P (wd | td, pid,w?d, z?d, ?0)
?
N(t?(d,i), t) + ?0
|t?(d,i)|+K?0 P (wd | z,w?d, ?0),
where td is updated to reflect td,i = t, and zd is de-
terministically computed by mapping td and pid to
actual paragraph topic assignments. The first step
reflects an application of Bayes rule to factor out the
term for wd. In the second step, the first term arises
out of the DCM, by updating the parameters ?0 with
observations t?(d,i) as in equation 4 and dropping
constants. The document probability term is com-
puted using equation 3. The new td,i is selected
by sampling from this probability computed over all
possible topic assignments.
Ordering The parameterization of a permutation
pi as a series of inversion values vj reveals a natural
way to decompose the search space for Gibbs sam-
pling. For a single ordering, each vj can be sampled
independently, according to:
P (vj = v | . . .)
? P (vj = v | ?j)P (wd | td, pid,w?d, z?d, ?0)
= GMMj(v | ?j)P (wd | zd,w?d, z?d, ?0),
where pid is updated to reflect vj = v, and zd is com-
puted according to td and pid. The first term refers
to the jth multiplicand of equation 1; the second is
computed using equation 3. Term vj is sampled ac-
cording to the resulting probabilities.
GMM Parameters For each j = 1 to K ? 1, we
resample ?j from its posterior distribution:
P (?j | . . .)
= GMM0
(
?j
????
?
i vj,i + vj,0?0
N + ?0 , N + ?0
)
,
375
where GMM0 is evaluated according to equation 2.
The normalization constant of this distribution is un-
known, meaning that we cannot directly compute
and invert the cumulative distribution function to
sample from this distribution. However, the distri-
bution itself is univariate and unimodal, so we can
expect that an MCMC technique such as slice sam-
pling (Neal, 2003) should perform well. In practice,
the MATLAB black-box slice sampler provides a ro-
bust draw from this distribution.
6 Experimental Setup
Data Sets We evaluate our model on two data sets
drawn from the English Wikipedia. The first set
is 100 articles about large cities, with topics such
as History, Culture, and Demographics. The sec-
ond is 118 articles about chemical elements in the
periodic table, including topics such as Biological
Role, Occurrence, and Isotopes. Within each cor-
pus, articles often exhibit similar section orderings,
but many have idiosyncratic inversions. This struc-
tural variability arises out of the collaborative nature
of Wikipedia, which allows articles to evolve inde-
pendently. Corpus statistics are summarized below.
Corpus Docs Paragraphs Vocab Words
Cities 100 6,670 41,978 492,402
Elements 118 2,810 18,008 191,762
In each data set, the articles? noisy section head-
ings induce a reference structure to compare against.
This reference structure assumes that two para-
graphs are aligned if and only if their section head-
ings are identical, and that section boundaries pro-
vide the correct segmentation of each document.
These headings are only used for evaluation, and are
not provided to any of the systems.
Using the section headings to build the reference
structure can be problematic, as the same topic may
be referred to using different titles across different
documents, and sections may be divided at differing
levels of granularity. Thus, for the Cities data set, we
manually annotated each article?s paragraphs with a
consistent set of section headings, providing us an
additional reference structure to evaluate against. In
this clean section headings set, we found approxi-
mately 18 topics that were expressed in more than
one document.
Tasks and Metrics We study performance on the
tasks of alignment and segmentation. In the former
task, we measure whether paragraphs identified to
be the same topic by our model have the same sec-
tion headings, and vice versa. First, we identify the
?closest? topic to each section heading, by finding
the topic that is most commonly assigned to para-
graphs under that section heading. We compute the
proportion of paragraphs where the model?s topic as-
signment matches the section heading?s topic, giv-
ing us a recall score. High recall indicates that
paragraphs of the same section headings are always
being assigned to the same topic. Conversely, we
can find the closest section heading to each topic,
by finding the section heading that is most com-
mon for the paragraphs assigned to a single topic.
We then compute the proportion of paragraphs from
that topic whose section heading is the same as the
reference heading for that topic, yielding a preci-
sion score. High precision means that paragraphs
assigned to a single topic usually correspond to the
same section heading. The harmonic mean of recall
and precision is the summary F-score.
Statistical significance in this setup is measured
with approximate randomization (Noreen, 1989), a
nonparametric test that can be directly applied to
nonlinear metrics such as F-score. This test has been
used in prior evaluations for information extraction
and machine translation (Chinchor, 1995; Riezler
and Maxwell, 2005).
For the second task, we take the boundaries at
which topics change within a document to be a
segmentation of that document. We evaluate us-
ing the standard penalty metrics Pk and WindowD-
iff (Beeferman et al, 1999; Pevzner and Hearst,
2002). Both pass a sliding window over the doc-
uments and compute the probability of the words
at the ends of the windows being improperly seg-
mented with respect to each other. WindowDiff re-
quires that the number of segmentation boundaries
between the endpoints be correct as well.8
Our model takes a parameter K which controls
the upper bound on the number of latent topics. Note
that our algorithm can select fewer thanK topics for
each document, soK does not determine the number
8Statistical significance testing is not standardized and usu-
ally not reported for the segmentation task, so we omit these
tests in our results.
376
of segments in each document. We report results
using both K = 10 and 20 (recall that the cleanly
annotated Cities data set had 18 topics).
Baselines andModel Variants We consider base-
lines from the literature that perform either align-
ment or segmentation. For the first task, we
compare against the hidden topic Markov model
(HTMM) (Gruber et al, 2007), which represents
topic transitions between adjacent paragraphs in a
Markovian fashion, similar to the approach taken in
content modeling work. Note that HTMM can only
capture local constraints, so it would allow topics to
recur noncontiguously throughout a document.
We also compare against the structure-agnostic
approach of clustering the paragraphs using the
CLUTO toolkit,9 which uses repeated bisection to
maximize a cosine similarity-based objective.
For the segmentation task, we compare to
BayesSeg (Eisenstein and Barzilay, 2008),10
a Bayesian topic-based segmentation model
that outperforms previous segmentation ap-
proaches (Utiyama and Isahara, 2001; Galley et al,
2003; Purver et al, 2006; Malioutov and Barzilay,
2006). BayesSeg enforces the topic contiguity
constraint that motivated our model. We provide
this baseline with the benefit of knowing the correct
number of segments for each document, which is
not provided to our system. Note that BayesSeg
processes each document individually, so it cannot
capture structural relatedness across documents.
To investigate the importance of our ordering
model, we consider two variants of our model that
alternately relax and tighten ordering constraints. In
the constrained model, we require all documents to
follow the same canonical ordering of topics. This
is equivalent to forcing the topic permutation distri-
bution to give all its probability to one ordering, and
can be implemented by fixing all inversion counts v
to zero during inference. At the other extreme, we
consider the uniform model, which assumes a uni-
form distribution over all topic permutations instead
of biasing toward a small related set. In our im-
plementation, this can be simulated by forcing the
9http://glaros.dtc.umn.edu/gkhome/views/cluto/
10We do not evaluate on the corpora used in their work, since
our model relies on content similarity across documents in the
corpus.
GMM parameters ? to always be zero. Both variants
still enforce topic contiguity, and allow segments
across documents to be aligned by topic assignment.
Evaluation Procedures For each evaluation of
our model and its variants, we run the Gibbs sampler
from five random seed states, and take the 10,000th
iteration of each chain as a sample. Results shown
are the average over these five samples. All Dirich-
let prior hyperparameters are set to 0.1, encouraging
sparse distributions. For the GMM, we set the prior
decay parameter ?0 to 1, and the sample size prior
?0 to be 0.1 times the number of documents.
For the baselines, we use implementations pub-
licly released by their authors. We set HTMM?s pri-
ors according to values recommended in the authors?
original work. For BayesSeg, we use its built-in hy-
perparameter re-estimation mechanism.
7 Results
Alignment Table 1 presents the results of the
alignment evaluation. In every case, the best per-
formance is achieved using our full model, by a sta-
tistically significant and usually substantial margin.
In both domains, the baseline clustering method
performs competitively, indicating that word cues
alone are a good indicator of topic. While the sim-
pler variations of our model achieve reasonable per-
formance, adding the richer GMM distribution con-
sistently yields superior results.
Across each of our evaluations, HTMM greatly
underperforms the other approaches. Manual ex-
amination of the actual topic assignments reveals
that HTMM often selects the same topic for discon-
nected paragraphs of the same document, violating
the topic contiguity constraint, and demonstrating
the importance of modeling global constraints for
document structure tasks.
We also compare performance measured on the
manually annotated section headings against the ac-
tual noisy headings. The ranking of methods by per-
formance remains mostly unchanged between these
two evaluations, indicating that the noisy headings
are sufficient for gaining insight into the compara-
tive performance of the different approaches.
Segmentation Table 2 presents the segmentation
experiment results. On both data sets, our model
377
Cities: clean headings Cities: noisy headings Elements: noisy headings
Recall Prec F-score Recall Prec F-score Recall Prec F-score
K
=
10
Clustering 0.578 0.439 ? 0.499 0.611 0.331 ? 0.429 0.524 0.361 ? 0.428
HTMM 0.446 0.232 ? 0.305 0.480 0.183 ? 0.265 0.430 0.190 ? 0.264
Constrained 0.579 0.471 ? 0.520 0.667 0.382 ? 0.485 0.603 0.408 ? 0.487
Uniform 0.520 0.440 ? 0.477 0.599 0.343 ? 0.436 0.591 0.403 ? 0.479
Our model 0.639 0.509 0.566 0.705 0.399 0.510 0.685 0.460 0.551
K
=
20
Clustering 0.486 0.541 ? 0.512 0.527 0.414 ? 0.464 0.477 0.402 ? 0.436
HTMM 0.260 0.217 ? 0.237 0.304 0.187 ? 0.232 0.248 0.243 ? 0.246
Constrained 0.458 0.519 ? 0.486 0.553 0.415 ? 0.474 0.510 0.421 ? 0.461
Uniform 0.499 0.551 ? 0.524 0.571 0.423 ? 0.486 0.550 0.479  0.512
Our model 0.578 0.636 0.606 0.648 0.489 0.557 0.569 0.498 0.531
Table 1: Comparison of the alignments produced by our model and a series of baselines and model variations, for both
10 and 20 topics, evaluated against clean and noisy sets of section headings. Higher scores are better. Within the same
K, the methods which our model significantly outperforms are indicated with ? for p < 0.001 and  for p < 0.01.
Cities: clean headings Cities: noisy headings Elements: noisy headings
Pk WD # Segs Pk WD # Segs Pk WD # Segs
BayesSeg 0.321 0.376 ? 12.3 0.317 0.376 ? 13.2 0.279 0.316 ? 7.7
K
=
10 Constrained 0.260 0.281 7.7 0.267 0.288 7.7 0.227 0.244 5.4Uniform 0.268 0.300 8.8 0.273 0.304 8.8 0.226 0.250 6.6
Our model 0.253 0.283 9.0 0.257 0.286 9.0 0.201 0.226 6.7
K
=
20 Constrained 0.274 0.314 10.9 0.274 0.313 10.9 0.231 0.257 6.6Uniform 0.234 0.294 14.0 0.234 0.290 14.0 0.209 0.248 8.7
Our model 0.221 0.278 14.2 0.222 0.278 14.2 0.203 0.243 8.6
Table 2: Comparison of the segmentations produced by our model and a series of baselines and model variations, for
both 10 and 20 topics, evaluated against clean and noisy sets of section headings. Lower scores are better. ?BayesSeg
is given the true number of segments, so its segments count reflects the reference structure?s segmentation.
outperforms the BayesSeg baseline by a substantial
margin regardless of K. This result provides strong
evidence that learning connected topic models over
related documents leads to improved segmentation
performance. In effect, our model can take advan-
tage of shared structure across related documents.
In all but one case, the best performance is ob-
tained by the full version of our model. This result
indicates that enforcing discourse-motivated struc-
tural constraints allows for better segmentation in-
duction. Encoding global discourse-level constraints
leads to better language models, resulting in more
accurate predictions of segment boundaries.
8 Conclusions
In this paper, we have shown how an unsupervised
topic-based approach can capture document struc-
ture. Our resulting model constrains topic assign-
ments in a way that requires global modeling of en-
tire topic sequences. We showed that the generalized
Mallows model is a theoretically and empirically ap-
pealing way of capturing the ordering component
of this topic sequence. Our results demonstrate the
importance of augmenting statistical models of text
analysis with structural constraints motivated by dis-
course theory.
Acknowledgments
The authors acknowledge the funding support of
NSF CAREER grant IIS-0448168, the NSF Grad-
uate Fellowship, the Office of Naval Research,
Quanta, Nokia, and the Microsoft Faculty Fellow-
ship. We thank the members of the NLP group at
MIT and numerous others who offered suggestions
and comments on this work. We are especially grate-
ful to Marina Meila? for introducing us to the Mal-
lows model. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
378
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL/HLT.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring strategies for sentence ordering
in multidocument news summarization. Journal of Ar-
tificial Intelligence Research, 17:35?55.
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34:177?210.
Jose? M. Bernardo and Adrian F.M. Smith. 2000.
Bayesian Theory. Wiley Series in Probability and
Statistics.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. Journal of Machine Learn-
ing Research, 3:993?1022.
Nancy Chinchor. 1995. Statistical significance of MUC-
6 results. In Proceedings of the 6th Conference on
Message Understanding.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for discourse
coherence. In Proceedings of NAACL/HLT.
M.A. Fligner and J.S. Verducci. 1986. Distance based
ranking models. Journal of the Royal Statistical Soci-
ety, Series B, 48(3):359?369.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse segmen-
tation of multi-party conversation. In Proceedings of
ACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228?5235.
Thomas L. Griffiths, Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and
syntax. In Advances in NIPS.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic markov models. In Proceedings of AIS-
TATS.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish,
and Jon Oberlander. 2004. Evaluating centering-
based metrics of coherence for text structuring using
a reliably annotated corpus. In Proceedings of ACL.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
ACL.
Guy Lebanon and John Lafferty. 2002. Cranking: com-
bining rankings using conditional probability models
on permutations. In Proceedings of ICML.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL.
Marina Meila?, Kapil Phadnis, Arthur Patterson, and Jeff
Bilmes. 2007. Consensus ranking under the exponen-
tial model. In Proceedings of UAI.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705?767.
Eric W. Noreen. 1989. Computer Intensive Methods for
Testing Hypotheses. An Introduction. Wiley.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36.
Ian Porteous, David Newman, Alexander Ihler, Arthur
Asuncion, Padhraic Smyth, and Max Welling. 2008.
Fast collapsed gibbs sampling for latent dirichlet alo-
cation. In Proceedings of SIGKDD.
Matthew Purver, Konrad Ko?rding, Thomas L. Griffiths,
and Joshua B. Tenenbaum. 2006. Unsupervised topic
modelling for multi-party spoken discourse. In Pro-
ceedings of ACL/COLING.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceedings
of WWW.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of ACL.
Hanna M. Wallach. 2006. Topic modeling: beyond bag
of words. In Proceedings of ICML.
Alison Wray. 2002. Formulaic Language and the Lexi-
con. Cambridge University Press, Cambridge.
379
