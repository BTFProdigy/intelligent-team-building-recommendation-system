Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 93?96,
Prague, June 2007. c?2007 Association for Computational Linguistics
Generating Usable Formats for Metadata and
Annotations in a Large Meeting Corpus
Andrei Popescu-Belis and Paula Estrella
ISSCO/TIM/ETI, University of Geneva
40, bd. du Pont-d?Arve
1211 Geneva 4 - Switzerland
{andrei.popescu-belis, paula.estrella}@issco.unige.ch
Abstract
The AMI Meeting Corpus is now publicly
available, including manual annotation files
generated in the NXT XML format, but
lacking explicit metadata for the 171 meet-
ings of the corpus. To increase the usability
of this important resource, a representation
format based on relational databases is pro-
posed, which maximizes informativeness,
simplicity and reusability of the metadata
and annotations. The annotation files are
converted to a tabular format using an eas-
ily adaptable XSLT-based mechanism, and
their consistency is verified in the process.
Metadata files are generated directly in the
IMDI XML format from implicit informa-
tion, and converted to tabular format using
a similar procedure. The results and tools
will be freely available with the AMI Cor-
pus. Sharing the metadata using the Open
Archives network will contribute to increase
the visibility of the AMI Corpus.
1 Introduction
The AMI Meeting Corpus (Carletta and al., 2006)
is one of the largest and most extensively annotated
data sets of multimodal recordings of human interac-
tion. The corpus contains 171 meetings, in English,
for a total duration of ca. 100 hours. The meetings
either follow the remote control design scenario, or
are naturally occurring meetings. In both cases, they
have between 3 and 5 participants.
Perhaps the most valuable resources in this cor-
pus are the high quality annotations, which can be
used to train and test NLP tools. The existing anno-
tation dimensions include, beside transcripts, forced
temporal alignment, named entities, topic segmen-
tation, dialogue acts, abstractive and extractive sum-
maries, as well as hand and head movement and pos-
ture. However, these dimensions as well as the im-
plicit metadata for the corpus are difficult to exploit
by NLP tools due to their particular coding schemes.
This paper describes work on the generation of
annotation and metadata databases in order to in-
crease the usability of these components of the AMI
Corpus. In the following sections we describe the
problem, present the current solutions and give fu-
ture directions.
2 Description of the Problem
The AMI Meeting Corpus is publicly available at
http://corpus.amiproject.org and con-
tains the following media files: audio (headset mikes
plus lapel, array and mix), video (close up, wide
angle), slides capture, whiteboard and paper notes.
In addition, all annotations described in Section 1
are available in one large bundle. Annotators fol-
lowed dimension-specific guidelines and used the
NITE XML Toolkit (NXT) to support their task,
generating annotations in NXT format (Carletta and
al., 2003; Carletta and Kilgour, 2005). Using the
NXT/XML schema makes the annotations consis-
tent along the corpus but more difficult to use with-
out the NITE toolkit. A less developed aspect of
the corpus is the metadata encoding all auxiliary in-
formation about meetings in a more structured and
informative manner. At the moment, metadata is
spread implicitly along the corpus data, for example
93
it is encoded in the file or folder names or appears to
be split in several resource files.
We define here annotations as the time-dependent
information which is abstracted from the input me-
dia, i.e. ?higher-level? phenomena derived from
low-level mono- or multi-modal features. Con-
versely, metadata is defined as the static information
about a meeting that is not directly related to its con-
tent (see examples in Section 4). Therefore, though
not necessarily time-dependent, structural informa-
tion derived from meeting-related documents would
constitute an annotation and not metadata. These
definitions are not universally accepted, but they al-
low us to separate the two types of information.
The main goal of the present work is to facilitate
the use of the AMI Corpus metadata and annota-
tions as part of the larger objective of automating
the generation of annotation and metadata databases
to enhance search and browsing of meeting record-
ings. This goal can be achieved by providing plug-
and-play databases, which are much easier to ac-
cess than NXT files and provide declarative rather
than implicit metadata. One of the challenges in
the NXT-to-database conversion is the extraction of
relevant information, which is done here by solving
NXT pointers and discarding NXT-specific markup
to group all information for a phenomenon in only
one structure or table.
The following criteria were important when defin-
ing the conversion procedure and database tables:
? Simplicity: the structure of the tables should
be easy to understand, and should be close to
the annotation dimensions?ideally one table
per annotation. Some information can be du-
plicated in several tables to make them more
intelligible. This makes the update of this in-
formation more difficult, but as this concerns a
recorded corpus, changes are less likely to oc-
cur; if such changes do occur, they would first
be input in the annotation files, from which a
new set of tables can easily be generated.
? Reusability: the tools allow anyone to recreate
the tables from the official distribution of the
annotation files. Therefore, if the format of the
annotation files or folders changes, or if a dif-
ferent format is desired for the tables, it is quite
easy to change the tools to generate a new ver-
sion of the database tables.
? Applicability: the tables are ready to be loaded
into any SQL database, so that they can be im-
mediately used by a meeting browser plugged
into the database.
Although we report one solution here, there are
other approaches to the same problem relying, for
example, on different database structures using more
or fewer tables to represent this information.
3 Annotations: Generation of Tables
The first goal is to convert the NXT files from the
AMI Corpus into a compact tabular representation
(tab-separated text files), using a simple, declarative
and easily updatable conversion procedure.
The conversion principle is the following: for
each type of annotation, which is generally stored
in a specific folder of the data distribution, an XSLT
stylesheet converts the NXT XML file into a tab-
separated text file, possibly using information from
one or more annotations. The stylesheets resolve
most of the NXT pointers, by including redundant
information into the tables, in order to speed up
queries by avoiding frequent joins. A Perl script
applies the respective XSLT stylesheet to each an-
notation file according to its type, and generates the
global tab-separated files for each annotation. The
script also generates an SQL script that creates a re-
lational annotation database and populates it with
data from the tab-separated files. The Perl script
also summarizes the results into a log file named
<timestamp>.log.
The conversion process can be summarized as fol-
lows and can be repeated at will, in particular if the
NXT source files are updated:
1. Start with the official NXT release (or other
XML-based format) of the AMI annotations as
a reference version.
2. Apply the table generation mechanism to
XML annotation files, using XSLT stylesheets
called by the script, in order to generate tab-
ular files (TSV) and a table-creation script
(db loader.sql).
3. Create and populate the annotation database.
4. Adapt the XSLT stylesheets as needed for vari-
ous annotations and/or table formats.
94
4 Metadata: Generation of Explicit Files
and Conversion to Tabular Format
As mentioned in Section 2, metadata denotes here
any static information about a meeting, not di-
rectly related to its content. The main metadata
items are: date, time, location, scenario, partic-
ipants, participant-related information (codename,
age, gender, knowledge of English and other lan-
guages), relations to media-files (participants vs. au-
dio channels vs. files), and relations to other docu-
ments produced during the meeting (slides, individ-
ual and whiteboard notes).
This important information is spread in many
places, and can be found as attributes of a meeting
in the annotation files (e.g. start time) or obtained
by parsing file names (e.g. audio channel, camera).
The relations to media files are gathered from differ-
ent resource files: mainly the meetings.xml and
participants.xml files. An additional prob-
lem in reconstructing such relations (e.g. files gen-
erated by a specific participant) is that information
about the media resources must be obtained directly
from the AMI Corpus distribution web site, since
the media resources are not listed explicitly in the
annotation files. This implies using different strate-
gies to extract the metadata: for example, stylesheets
are the best option to deal with the above-mentioned
XML files, while a crawler script is used for HTTP
access to the distribution site. However, the solution
adopted for annotations in Section 3 can be reused
with one major extension and applied to the con-
struction of the metadata database.
The standard chosen for the explicit meta-
data files is the IMDI format, proposed by
the ISLE Meta Data Initiative (Wittenburg
et al, 2002; Broeder et al, 2004a) (see
http://www.mpi.nl/IMDI/tools), which
is precisely intended to describe multimedia
recordings of dialogues. This standard provides a
flexible and extensive schema to store the defined
metadata either in specific IMDI elements or as
additional key/value pairs. The metadata generated
for the AMI Corpus can be explored with the IMDI
BC-Browser (Broeder et al, 2004b), a tool that
is freely available and has useful features such as
search or metadata editing.
The process of extracting, structuring and storing
the metadata is as follows:
1. Crawl the AMI Corpus website and store re-
sulting metadata (related to media files) into an
XML auxiliary file.
2. Apply an XSLT stylesheet to the aux-
iliary XML file, using also the dis-
tribution files meetings.xml and
participants.xml, to obtain one IMDI
file per meeting.
3. Apply the table generation mechanism to each
IMDI file in order to generate tabular files
(TSV) and a table-creation script.
4. Create and populate metadata tables within
database.
5. Adapt the XSLT stylesheet as needed for vari-
ous table formats.
5 Results: Current State and Distribution
The 16 annotation dimensions from the public AMI
Corpus were processed following the procedure
described in Section 3. The main Perl script,
anno-xml2db.pl, applied the 16 stylesheets cor-
responding to each annotation dimension, which
generated one large tab-separated file each. The
script also generated the table-creation SQL script
db loader.sql. The number of lines of each ta-
ble, hence the number of ?elementary annotations?,
is shown in Table 1.
The application of the metadata extraction tools
described in Section 4 generated a first version of
the explicit metadata for the AMI Corpus, consist-
ing of 171 automatically generated IMDI files (one
per meeting). In addition, 85 manual files were
created in order to organize the metadata files into
IMDI corpus nodes, which form the skeleton of the
corpus metadata and allow its browsing with the
BC-Browser. The resources and tools for annota-
tion/metadata processing will be made soon avail-
able on the AMI Corpus website, along with a demo
access to the BC-Browser.
6 Discussion and Perspectives
The proposed solution for annotation conversion is
easy to understand, as it can be summarized as ?one
table per annotation dimension?. The tables pre-
serve only the relevant information from the NXT
95
Annotation dimension Nb. of entries
words (transcript) 1,207,769
named entities 14,230
speech segments 69,258
topics 1,879
dialogue acts 117,043
adjacency pairs 26,825
abstractive summaries 2,578
extractive summaries 19,216
abs/ext links 22,101
participant summaries 3,409
focus 31,271
hand gesture 1,453
head gesture 36,257
argument structures 6,920
argumentation relations 4,759
discussions 8,637
Table 1: Results of annotation conversion; dimen-
sions are grouped by conceptual similarity.
annotation files, and search is accelerated by avoid-
ing repeated joins between tables.
The process of metadata extraction and genera-
tion is very flexible and the obtained data can be eas-
ily stored in different file formats (e.g. tab-separated,
IMDI, XML, etc.) with no need to repeatedly parse
file names or analyse folders. Moreover, the ad-
vantage of creating IMDI files is that the metadata
is compliant with a widely used standard accompa-
nied by freely available tools such as the metadata
browser. These results will also help disseminating
the AMI Corpus.
As a by-product of the development of annotation
and metadata conversion tools, we performed a con-
sistency checking and reported a number of to the
corpus administrators. The automatic processing of
the entire annotation and metadata set enabled us to
test initial hypotheses about annotation structure.
In the future we plan to include the AMI Cor-
pus metadata in public catalogues, through the Open
(Language) Archives Initiatives network (Bird and
Simons, 2001), as well as through the IMDI network
(Wittenburg et al, 2004). The metadata repository
will be harvested by answering the OAI-PMH pro-
tocol, and the AMI Corpus website could become
itself a metadata provider.
Acknowledgments
The work presented here has been supported by
the Swiss National Science Foundation through the
NCCR IM2 on Interactive Multimodal Information
Management (http://www.im2.ch). The au-
thors would like to thank Jean Carletta, Jonathan
Kilgour and Mae?l Guillemot for their help in access-
ing the AMI Corpus.
References
Steven Bird and Gary Simons. 2001. Extending Dublin
Core metadata to support the description and discovery
of language resources. Computers and the Humani-
ties, 37(4):375?388.
Daan Broeder, Thierry Declerck, Laurent Romary,
Markus Uneson, Sven Stro?mqvist, and Peter Witten-
burg. 2004a. A large metadata domain of language
resources. In LREC 2004 (4th Int. Conf. on Language
Resources and Evaluation), pages 369?372, Lisbon.
Daan Broeder, Peter Wittenburg, and Onno Crasborn.
2004b. Using profiles for IMDI metadata creation. In
LREC 2004 (4th Int. Conf. on Language Resources and
Evaluation), pages 1317?1320, Lisbon.
Jean Carletta and al. 2006. The AMI Meeting Corpus:
A pre-announcement. In Steve Renals and Samy Ben-
gio, editors, Machine Learning for Multimodal Inter-
action II, LNCS 3869, pages 28?39. Springer-Verlag,
Berlin/Heidelberg.
Jean Carletta and Jonathan Kilgour. 2005. The NITE
XML Toolkit meets the ICSI Meeting Corpus: Import,
annotation, and browsing. In Samy Bengio and Herve?
Bourlard, editors, Machine Learning for Multimodal
Interaction, LNCS 3361, pages 111?121. Springer-
Verlag, Berlin/Heidelberg.
Jean Carletta, Stefan Evert, Ulrich Heid, Jonathan Kil-
gour, Judy Robertson, and Holger Voormann. 2003.
The NITE XML Toolkit: flexible annotation for multi-
modal language data. In Behavior Research Methods,
Instruments, and Computers, special issue on Measur-
ing Behavior, 35(3), pages 353?363.
Peter Wittenburg, Wim Peters, and Daan Broeder. 2002.
Metadata proposals for corpora and lexica. In LREC
2002 (3rd Int. Conf. on Language Resources and Eval-
uation), pages 1321?1326, Las Palmas.
Peter Wittenburg, Daan Broeder, and Paul Buitelaar.
2004. Towards metadata interoperability. In NLPXML
2004 (4th Workshop on NLP and XML at ACL 2004),
pages 9?16, Barcelona.
96
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 54?62,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Using Artiially Generated Data
to Evaluate Statistial Mahine Translation
Manny Rayner, Paula Estrella, Pierrette Bouillon
University of Geneva, TIM/ISSCO
40 bvd du Pont-d'Arve, CH-1211 Geneva 4, Switzerland
fEmmanuel.Rayner,Paula.Estrella,Pierrette.Bouillongunige.h
Beth Ann Hokey
Mail Stop 19-26, UCSC UARC
NASA Ames Researh Center, Moffett Field, CA 94035?1000
bahokeyus.edu
Yukie Nakao
LINA, Nantes University, 2, rue de la Houssiniere, BP 92208 44322 Nantes Cedex 03
yukie.nakaouniv-nantes.fr
Abstrat
Although Statistial Mahine Translation
(SMT) is now the dominant paradigm
within Mahine Translation, we argue that
it is far from lear that it an outperform
Rule-Based Mahine Translation (RBMT)
on small- to medium-voabulary applia-
tions where high preision is more impor-
tant than reall. A partiularly important
pratial example is medial speeh trans-
lation. We report the results of exper-
iments where we ongured the various
grammars and rule-sets in an Open Soure
medium-voabulary multi-lingual medial
speeh translation system to generate large
aligned bilingual orpora for English !
Frenh and English ! Japanese, whih
were then used to train SMT models based
on the ommon ombination of Giza++,
Moses and SRILM. The resulting SMTs
were unable fully to reprodue the per-
formane of the RBMT, with performane
topping out, even for English ! Frenh,
with less than 70% of the SMT translations
of previously unseen sentenes agreeing
with RBMT translations. When the out-
puts of the two systems differed, human
judges reported the SMT result as fre-
quently being worse than the RBMT re-
sult, and hardly ever better; moreover, the
added robustness of the SMT only yielded
a small improvement in reall, with a large
penalty in preision.
1 Introdution
When Statistial Mahine Translation (SMT) was
rst introdued in the early 90s, it enountered a
hostile reeption, and many people in the researh
ommunity were unwilling to believe it ould ever
be a serious ompetitor to symboli approahes
(f. for example (Arnold et al, 1994)). The pendu-
lum has now swung all the way to the other end of
the sale; right now, the prevailing wisdom within
the researh ommunity is that SMT is the only
truly viable arhiteture, and that rule-based ma-
hine translation (RBMT) is ultimately doomed to
failure. In this paper, one of our initial onerns
will be to argue for a ompromise position. In our
opinion, the initial septiism about SMT was not
groundless; the arguments presented against it of-
ten took the form of examples involving deep lin-
guisti reasoning, whih, it was laimed, would be
hard to address using surfae methods. Proponents
of RBMT had, however, greatly underestimated
the extent to whih SMT would be able to takle
the problem of robustness, where it appears to be
far more powerful than RBMT. For most mahine
translation appliations, robustness is the entral
issue, so SMT's urrent preeminene is hardly sur-
prising.
Even for the large-voabulary tasks where SMT
does best, the situation is by no means as lear as
one might imagine: aording to (Wilks, 2007),
purely statistial systems are still unable to out-
perform SYSTRAN. In this paper, we will how-
ever be more onerned with limited-domain MT
tasks, where robustness is not the key requirement,
and auray is paramount. An immediate exam-
54
ple is medial speeh translation, whih is estab-
lishing itself as an an appliation area of some sig-
niane (Bouillon et al, 2006; Bouillon et al,
2008a). Translation in medial appliations needs
to be extremely aurate, sine mistranslations
an have serious or even fatal onsequenes. At
the panel disussion at the 2008 COLING work-
shop on safety-ritial speeh translation (Rayner
et al, 2008), the onsensus opinion, based on in-
put from pratising physiians, was that an appro-
priate evaluation metri for medial appliations
would be heavily slanted towards auray, as op-
posed to robustness. If the metri is normalised so
as to award 0 points for no translation, and 1 point
for a orret translation, the estimate was that a
suitable sore for an inorret translation would
be something between ?25 and ?100 points. With
these requirements, it seems unlikely that a robust,
broad-overage arhiteture has muh hane of
suess. The obvious strategy is to build a limited-
domain ontrolled-language system, and tune it to
the point where auray reahes the desired level.
For systems of this kind, it is at least oneiv-
able that RBMT may be able to outperform SMT.
The next question is how to investigate the issues
in a methodologially even-handed way. A few
studies, notably (Seneff et al, 2006), suggest that
rule-based translation may in fat be preferable in
these ases. (Another related experiment is de-
sribed in (Dugast et al, 2008), though this was
arried out in a large-voabulary system). These
studies, however, have not been widely ited. One
possible explanation is suspiion about method-
ologial issues. Seneff and her olleagues trained
their SMT system on 20 000 sentene pairs, a
small number by the standards of SMT. It is a pri-
ori not implausible that more training data would
have enabled them to reate an SMT system that
was as good as, or better than, the rule-based sys-
tem.
In this paper, our primary goal is to take this
kind of objetion seriously, and develop a method-
ology designed to enable a tight omparison be-
tween rule-based and statistial arhitetures. In
partiular, we wish to examine the widely be-
lieved laim that SMT is now inherently better
than RBMT. In order to do this, we start with a
limited-domain RBMT system; we use it to auto-
matially generate a large orpus of aligned pairs,
whih is used to train a orresponding SMT sys-
tem. We then ompare the performane of the two
systems.
Our argument will be that this situation essen-
tially represents an upper bound for what is possi-
ble using the SMT approah in a limited domain.
It has been widely remarked that quality, as well
as quantity, of training data is important for good
SMT; in many projets, signiant effort is ex-
pended to lean the original training data. Here,
sine the data is automatially generated by a rule-
based system, we an be sure that it is already
ompletely lean (in the sense of being internally
onsistent), and we an generate as large a quan-
tity of it as we require. The appliation, more-
over, uses only a smallish voabulary and a fairly
onstrained syntax. If the derived SMT system is
unable to math the original RBMT system's per-
formane, it seems reasonable to laim that this
shows that there are types of appliations where
RBMT arhitetures are superior.
The experiments desribed have been arried
out using MedSLT, an Open Soure interlingua-
based limited-domain medial speeh translation
system. The rest of the paper is organised as fol-
lows. Setion 2 provides bakground on the Med-
SLT system. Setion 3 desribes the experimen-
tal framework, and Setion 4 the results obtained.
Setion 5 onludes.
2 The MedSLT System
MedSLT (Bouillon et al, 2005; Bouillon et al,
2008b) is a medium-voabulary interlingua-based
Open Soure speeh translation system for dotor-
patient medial examination questions, whih
provides any-language-to-any-language transla-
tion apabilities for all languages in the set En-
glish, Frenh, Japanese, Arabi, Catalan. Both
speeh reognition and translation are rule-based.
Speeh reognition runs on the Nuane 8.5 reog-
nition platform, with grammar-based language
models built using the Open Soure Regulus om-
piler. As desribed in (Rayner et al, 2006),
eah domain-spei language model is extrated
from a general resoure grammar using orpus-
based methods driven by a seed orpus of domain-
spei examples. The seed orpus, whih typi-
ally ontains between 500 and 1500 utteranes,
is then used a seond time to add probabilisti
weights to the grammar rules; this substantially
improves reognition performane (Rayner et al,
2006, x11.5). Voabulary sizes and performane
measures for speeh reognition in the three lan-
55
guages where serious evaluations have been ar-
ried out are shown in Figure 1.
Language Voab WER SemER
English 447 6% 11%
Frenh 1025 8% 10%
Japanese 422 3% 4%
Table 1: Reognition performane for English,
Frenh and Japanese MedSLT reognisers. ?Vo-
ab? = number of surfae words in soure lan-
guage reogniser voabulary; ?WER? = Word Er-
ror Rate for soure language reogniser, on in-
overage material; ?SemER? = semanti error rate
(proportion of utteranes failing to produe orret
interlingua) for soure language reogniser, on in-
overage material.
At run-time, the reogniser produes a soure-
langage semanti representation. This is rst
translated by one set of rules into an interlingual
form, and then by a seond set into a target lan-
guage representation. A target-language Regu-
lus grammar, ompiled into generation form, turns
this into one or more possible surfae strings, af-
ter whih a set of generation preferenes piks
one out. Finally, the seleted string is realised in
spoken form. Robustness issues are addressed by
means of a bak-up statistial reogniser, whih
drives a robust embedded help system. The pur-
pose of the help system (Chatzihrisas et al,
2006) is to guide the user towards supported ov-
erage; it performs approximate mathing of out-
put from the statistial reogniser again a library
of sentenes whih have been marked as orretly
proessed during system development, and then
presents the losest mathes to the user.
Examples of typial English domain sentenes
and their translations into Frenh and Japanese are
shown in Figure 2.
3 Experimental framework
In the literature on language modelling, there is
a known tehnique for bootstrapping a statisti-
al language model (SLM) from a grammar-based
language model (GLM). The grammar whih
forms the basis of the GLM is sampled randomly
in order to reate an arbitrarily large orpus of ex-
amples; these examples are then used as a train-
ing orpus to build the SLM (Jurafsky et al, 1995;
Jonson, 2005). We adapt this proess in a straight-
forward way to onstrut an SMT for a given
language pair, using the soure language gram-
mar, the soure-to-interlingua translation rules, the
interlingua-to-target-language rules, and the tar-
get language generation grammar. We start in the
same way, using the soure language grammar to
build a randomly generated soure language or-
pus; as shown in (Hokey et al, 2008), it is im-
portant to have a probabilisti grammar. We then
use the omposition of the other omponents to
attempt to translate eah soure language sentene
into a target language equivalent, disarding the
examples for whih no translation is produed.
The result is an aligned bilingual orpus of ar-
bitrary size, whih an be used to train an SMT
model.
We used this method to generate aligned or-
pora for the two MedSLT language pairs English
! Frenh and English ! Japanese. For eah lan-
guage pair, we rst generated one million soure-
language utteranes; we next ltered them to keep
only examples whih were full sentenes, as op-
posed to elliptial phrases, and nally used the
translation rules and target-language generators to
attempt to translate eah sentene. This reated
approximately 305K aligned sentene-pairs for
English ! Frenh (1901K words English, 1993K
words Frenh), and 311K aligned sentene-pairs
for English ! Japanese (1941K words English,
2214K words Japanese). We held out 2.5% of
eah set as development data, and 2.5% as test
data. Using Giza++, Moses and SRILM (Oh and
Ney, 2000; Koehn et al, 2007; Stolke, 2002), we
trained SMT models from inreasingly large sub-
sets of the training portion, using the development
portion in the usual way to optimize parameter val-
ues. Finally, we used the resulting models to trans-
late the test portion.
Our primary goal was to measure the extent to
whih the derived versions of the SMT were able
to approximate the original RBMT on data whih
was within the RBMT's overage. There is a sim-
ple and natural way to perform this measurement:
we apply the BLEU metri (Papineni et al, 2001),
with the RBMT's translation taken as the refer-
ene. This means that perfet orrespondene be-
tween the two translations would yield a BLEU
sore of 1.0.
This raises an important point. The BLEU
sores we are using here are non-standard; they
measure the extent to whih the SMT approxi-
mates the RBMT, rather than, as usual, measuring
56
English Is the pain above your eye?
Frenh Avez-vous mal au dessus des yeux?
Japanese Itami wa me no ue no atari desu ka?
English Have you had the pain for more than a month?
Frenh Avez-vous mal depuis plus d'un mois?
Japanese Ikkagetsu ijou itami wa tsuzuki mashita ka?
English Is the pain assoiated with nausea?
Frenh Avez-vous des nause?es quand vous avez la douleur?
Japanese Itamu to hakike wa okori masu ka?
English Does bright light make the pain worse?
Frenh La douleur est-elle aggrave?e par une lumiere forte?
Japanese Akarui hikari wo miru to zutsu wa hidoku nari masu ka?
Table 2: Examples of English domain sentenes, and the system's translations into Frenh and Japanese.
the extent to whih it approximates human trans-
lations. It is important to bring in human judge-
ment, to evaluate the ases where the SMT and
RBMT differ. If, in these ases, it transpired that
human judges typially thought that the SMT was
as good as the RBMT, then the differene would
be purely aademi. We need to satisfy ourselves
that human judges typially asribe differenes be-
tween SMT and RBMT to shortomings in the
SMT rather than in the RBMT.
Conretely, we olleted all the different
hSoure, SMT-translation, RBMT-translationi
triples produed during the ourse of the ex-
periments, and extrated those where the two
translations were different. We randomly seleted
a set of examples for eah language pair, and
asked human judges to lassify them into one of
the following ategories:
 RBMT better: The RBMT translation was
better, in terms of preserving meaning and/or
being grammatially orret;
 SMT better: The SMT translation was bet-
ter, in terms of preserving meaning and/or be-
ing grammatially orret;
 Similar: Both translations were about
equally good OR the soure sentene was
meaningless in the domain.
In order to show that our metris are intuitively
meaningful, it is sufient to demonstrate that the
frequeny of ourrene of RBMT better is both
large in omparison to that of SMT better, and
aounts for a substantial proportion of the total
population.
Finally, we onsider the question of whether
the SMT, whih is apable of translating out-of-
grammar sentenes, an add useful robustness to
the base system. We olleted, from the set used in
the experiments desribed in (Rayner et al, 2005),
all the English sentenes whih failed to be trans-
lated into Frenh. We used the best version of
the English ! Frenh SMT to translate eah of
these sentenes, and asked human judges to eval-
uate the translations as being learly aeptable,
learly unaeptable, or borderline.
In the next setion, we present the results of the
various experiments we have just desribed.
4 Results
We begin with Figure 1, whih shows non-
standard BLEU sores for versions of the English
! Frenh SMT system trained on quantities of
data inreasing from 14 287 to 285 740 pairs. As
an be seen, translation performane improves up
to about 175 000 pairs. After this, it levels out
at around BLEU = 0.90, well below that of the
RBMT system with whih it is being ompared.
A more diret way to report the result is simply to
ount the proportion of test sentenes that are not
in the training data, whih are translated similarly
by the SMT and the RBMT. This gure tops out at
around 68%.
The results strongly suggest that the SMT is
unable to repliate the RBMT's performane at
all losely even in an easy language-pair, irre-
spetive of the amount of training data available.
Out of uriosity, and to reassure ourselves that the
automati generation proedure was doing some-
thing useful, we also tried training the English !
Frenh SMT on pairs derived from the 669 ut-
57
Figure 1: Non-standard BLEU sores against
number of pairs of training sentenes for English
! Frenh; training and test data both indepen-
dently generated, hene overlapping.
terane ?seed orpus? used to generate the gram-
mar (f. Setion 2). This produed utterly dis-
mal performane, with BLEU = 0.52. The result is
more interesting than it may rst appear, sine, in
speeh reognition, the differene in performane
between the SLMs trained from seed orpora and
large generated orpora is fairly small (Hokey et
al., 2008).
It seemed possible that the improvement in per-
formane with inreased quantities of training data
might, in effet, only be due to the SMT fun-
tioning as a translation memory; sine training
and test data are independently generated by the
same random proess, they overlap, with the de-
gree of overlap inreasing as the training set gets
larger. In order to investigate this hypothesis,
we repeated the experiments with data whih had
been uniqued, so that the training and test sets
were ompletely disjoint, and neither ontained
any dupliate sentenes
1
. In fat, Figure 2 show
that the graph for uniqued English ! Frenh data
are fairly similar to the one for the original non-
uniqued data shown in Figures 1. The main differ-
ene is that the non-standard BLEU sore for the
1
Our opinion is that this is not a realisti way to evaluate
the performane of a small-voabulary system; for example,
in MedSLT, one expets that at least some training sentenes,
e.g. ?Where is the pain??, will also our frequently in test
data.
Figure 2: Non-standard BLEU sores against
number of pairs of training sentenes for English
! Frenh; training and test data both indepen-
dently generated, then uniqued to remove dupli-
ates and overlapping items.
uniqued data, unsurprisingly, tops out at a lower
level, reeting the fat that a ?translation mem-
ory? effet does indeed our to some extent.
Results for English ! Japanese showed the
same trends as English ! Frenh, but were more
pronouned. Table 3 ompares the performane
of the best versions of the SMTs for the two
language-pairs, using both plain and artiially
uniqued data. We see that, with plain data, the
English ! Japanese SMT falls even further short
of repliating the performane of the RBMT than
was the ase for English ! Frenh; BLEU is
only 0.76. The differene between the plain and
uniqued versions is also more extreme. BLEU
(0.64) is onsiderably lower for the version trained
on uniqued data, suggesting that the SMT for this
language pair is nding it harder to generalise,
and is in effet loser to funtioning as a trans-
lation memory. This is onrmed by ounting
the sentenes in test data and not in training data
whih were translated similarly by the SMT and
the RBMT; we nd that the gure tops out at the
very low value of 26%.
As noted in our disussion of the experimental
framework, the non-standard BLEU sores only
address the question of whether the performane
of the SMT and RBMT systems is the same. It is
58
Training data Test data BLEU
English ! Frenh
Generated Generated 0.90
Gen/uniqued Gen/uniqued 0.85
English ! Japanese
Generated Generated 0.76
Gen/uniqued Gen/uniqued 0.64
Table 3: Translation performane, in terms of non-
standard BLEU metri, for different ongura-
tions, training on all available data of the spe-
ied type. ?Generated? = data randomly gener-
ated; ?Gen/uniqued? = data randomly generated,
then uniqued so that dupliates are removed and
test and training pairs do not overlap.
neessary to establish what the differenes mean
in terms of human judgements. We onsequently
turn to evaluation of the pairs for whih the SMT
and the RBMT systems produed different trans-
lation results.
Table 4 shows the ategorisation, aording to
the riteria outlined at the end of Setion 3, for 500
English ! Frenh pairs randomly seleted from
the set of examples where RBMT and SMT gave
different results; we asked three judges to evalu-
ate them independently, and ombined their judg-
ments by majority deision where appropriate. We
observed a very heavy bias towards the RBMT,
with unanimous agreement among the judges that
the RBMT translation was better in 201/500 ases,
and 2-1 agreement in a further 127. In ontrast,
there were only 4/500 ases where the judges
unanimously thought that the SMT translation was
preferable, with a further 12 supported by a ma-
jority deision. The rest of the table gives the
ases where the RBMT and SMT translations were
judged the same or ases in whih the judges dis-
agreed; there were only 41/500 ases where no
majority deision was reahed. Our overall on-
lusion is that we are justied in evaluating the
SMT by using the BLEU sores with the RBMT as
the referene. Of the ases where the two systems
differ, only a tiny fration, at most 16/500, indi-
ate a better translation from the SMT, and well
over half are translated better by the RBMT. Ta-
ble 5 presents typial examples of bad SMT trans-
lations in the English ! Frenh pair, ontrasted
with the translations produed by the RBMT. The
rst two are grammatial errors (a superuous ex-
tra verb in the rst, and agreement errors in the
seond). The third is an bad hoie of tense and
preposition; although grammatial, the target lan-
guage sentene fails to preserve the meaning, and,
rather than referring to a 20 day period ending
now, instead refers to a 20 day period some time
in the past.
Result Agreement Count
RBMT better all judges 201
RBMT better majority 127
SMT better all judges 4
SMT better majority 12
Similar all judges 34
Similar majority 81
Unlear disagree 41
Total 500
Table 4: Comparison of RBMT and SMT perfor-
mane on 500 randomly hosen English ! Frenh
translation examples, evaluated independently by
three judges.
Table 6 shows a similar evaluation for the En-
glish ! Japanese. Here, the differene between
the SMT and RBMT versions was so pronouned
that we felt justied in taking a smaller sample, of
only 150 sentenes. This time, 92/150 ases were
unanimously judged as having a better RBMT
translation, and there was not a single ase where
even a majority found that the SMT was better.
Agreement was good here too, with only 8/150
ases not yielding at least a majority deision.
Result Agreement Count
RBMT better all judges 92
RBMT better majority 32
SMT better all judges 0
SMT better majority 0
Similar all judges 2
Similar majority 16
Unlear disagree 8
Total 150
Table 6: Comparison of RBMT and SMT per-
formane on 150 randomly hosen English !
Japanese translation examples, evaluated indepen-
dently by three judges.
Finally, we look at the performane of the SMT
on material whih the RBMT is not able to trans-
late. This would seem to be a situation where
59
English does a temperature hange ause the headahe
RBMT Frenh vos maux de t?ete sont-ils ause?s par des hangements de tempe?rature
(your headahes are-they aused by hanges of temperature)
SMT Frenh avez-vous vos maux de t?ete sont-ils ause?s par des hangements de tempe?rature
(have-you your headahes are-they aused by hanges of temperature)
English are headahes relieved in the afternoon
RBMT Frenh vos maux de t?ete diminuent-ils l'apres-midi
(your headahes (MASC-PLUR) derease-MASC-PLUR the afternoon)
SMT Frenh vos maux de t?ete diminue-t-elle l'apres-midi
(your headahes (MASC-PLUR) derease-FEM-SING the afternoon)
English have you had them for twenty days
RBMT Frenh avez-vous vos maux de t?ete depuis vingt jours
(have-you your headahes sine twenty days)
SMT Frenh avez-vous eu vos maux de t?ete pendant vingt jours
(have-you had your headahes during twenty days)
Table 5: Examples of inorret SMT translations from English into Frenh. Errors are highlighted in
bold.
the SMT ould have an advantage; robustness is
generally a strength of statistial approahes. We
return to English ! Frenh in Table 7, whih
presents the result of running the best SMT model
on the 357 examples from the test set in (Rayner
et al, 2005) whih failed to be translated by the
RBMT. We divide the set into ategories based on
the reason for failure of the RBMT.
In the most populous group, translations that
failed due to out of voabulary items, the SMT
was, more or less by onstrution, also unable
to produe a translation. For the 110 items that
were out of grammar overage for the RBMT, the
SMT produed 38 good translations, and another 4
borderline translations. There were 50 items that
were within the soure grammar overage of the
RBMT, but failed somewhere in transfer and gen-
eration proessing. Of those, the majority (32)
represented ?bad? soure sentenes, onsidered as
ill-formed for the purposes of this experiment. Out
of the remaining items that were within RBMT
grammar overage, the SMT managed to produe
5 good translations and 1 borderline translation. In
total, on the most lenient interpretation, the SMT
produed 48 additional translations out of 357.
While this improvement in reall is arguably worth
having, it would ome at the prie of a substantial
deline in preision.
5 Disussion and Conlusions
We have presented a novel methodology for om-
paring RBMT and SMT, and tested it on a spe-
Result Count
Out of voabulary
Bad translation 187
Out of soure grammar overage
Good translation 38
Bad translation 44
Borderline translation 4
Bad soure sentene 34
In soure grammar overage
Good translation 5
Bad translation 12
Borderline translation 1
Bad soure sentene 32
Total 357
Table 7: English ! Frenh SMT performane on
examples from the test set whih failed to be trans-
lated by the RBMT, evaluated by one judge.
i pair of RBMT and SMT arhitetures. Our
laim is that these results show that the version
of SMT used here is not in fat apable of repro-
duing the output of the RBMT system. Although
there has been some interest in attempting to train
SMT systems from RBMT output, the evaluation
issues that arise when omparing SMT and RBMT
versions of a high-preision limited-domain sys-
tem are different from those arising in most MT
tasks, and neessitate a orrespondingly different
methodology. It is easy to gain the impression that
it is unsound, and that the experiment has been set
60
up in suh a way that only one result is possible.
This is not, in fat, true.
When we have disussed the methodology with
people who work primarily with SMT, we have
heard two main objetions. The rst is that the
SMT is being trained on RBMT output, and hene
an only be worse; a ommon suggestion is that
a system trained on human-produed translations
ould yield better results. It is not at all implau-
sible that an SMT trained on this kind of data
might perform better on material whih is outside
the overage of the RBMT system. In this do-
main, however, the important issue is preision,
not reall; what is ritial is the ability to trans-
late aurately on material that is within the on-
strained language dened by the RBMT overage.
The RBMT engine gives very good performane
on in-overage data, as has been shown in other
evaluations of the MedSLT system, e.g. (Rayner et
al., 2005); over 97% of all in-overage sentenes
are orretly translated. Human-generated transla-
tions would often, no doubt, be more natural than
those produed by the RBMT, and there would be
slightly fewer outright mistranslations. But the
primary reason why the SMT is doing badly is
not that the training material ontains bad trans-
lations, but rather that the SMT is inapable of
orretly reproduing the translations it sees in the
training data. Even in the easy English ! Frenh
language-pair, the SMT often produes a different
translation from the RBMT. It ould a priori have
been oneivable that the differenes were unin-
teresting, in the sense that SMT outputs different
from RBMT outputs were as good, or even better.
In fat, Table 4 show that this is not true; when the
two translations differ, although the SMT transla-
tion an oasionally be better, it is usually worse.
Table 6 shows that this problem is onsiderably
more aute in English ! Japanese. Thus the
SMT system's inability to model the RBMT sys-
tem points to a real limitation.
If the SMT had instead been trained on human-
generated data, its performane on in-overage
material ould only have improved substantially if
the SMT for some reason found it easier to learn to
reprodue patterns in human-generated data than
in RBMT-generated data. This seems unlikely.
The SMT is being trained from a set of translation
pairs whih are guaranteed to be ompletely on-
sistent, sine they have been automatially gener-
ated by the RBMT; the fat that the RBMT system
only has a small voabulary should also work in
its favour. If the SMT is unable to reprodue the
RBMT's output, it is reasonable to assume it will
have even greater difulty reproduing transla-
tions present in normal human-generated training
data, whih is always far from onsistent, and will
have a larger voabulary.
The seond objetion we have heard is that the
non-standard BLEU sores whih we have used to
measure performane use the RBMT translations
as a referene. People are quik to point out that,
if real human translations were sored in this way,
they would do less well on the non-standard met-
ris than the RBMT translations. This is, indeed,
absolutely true, and explains why it was essential
to arry out the omparison judging shown in Ta-
bles 4 and 6. If we had ompared human transla-
tions with RBMT translations in the same way, we
would have found that human translations whih
differed from RBMT translations were sometimes
better, and hardly ever worse. This would have
shown that the non-standard metris were inap-
propriate for the task of evaluating human trans-
lations. In the atual ase onsidered in this paper,
we nd a ompletely different pattern: the differ-
enes are one-sided in the opposite diretion, in-
diating that the non-standard metris do in fat
agree with human judgements here.
A general objetion to all these experiments is
that there may be more powerful SMT arhite-
tures. We used the Giza++/Moses/SRILM om-
binination beause it is the de fato standard. We
have posted the data we used at http://www.
bahr.net/geaf2009; this will allow other
groups to experiment with alternate arhitetures,
and determine whether they do in fat yield sig-
niant improvements. For the moment, however,
we think it is reasonable to laim that, in domains
where high auray is required, it remains to be
shown that SMT approahes are apable of ahiev-
ing the levels of performane that rule-based sys-
tems an deliver.
61
Referenes
D. Arnold, L. Balkan, S. Meijer, R.L. Humphreys, and
L. Sadler. 1994. Mahine Translation: An Introdu-
tory Guide. Blakwell, Oxford.
P. Bouillon, M. Rayner, N. Chatzihrisas, B.A.
Hokey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generi multi-
lingual open soure platform for limited-domain
medial speeh translation. In Proeedings of the
10th Conferene of the European Assoiation for
Mahine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
P. Bouillon, F. Ehsani, R. Frederking, and M. Rayner,
editors. 2006. Proeedings of the HLT-NAACL In-
ternational Workshop on Medial Speeh Transla-
tion, New York.
P. Bouillon, F. Ehsani, R. Frederking, M. MTear,
and M. Rayner, editors. 2008a. Proeedings of
the COLING Workshop on Speeh Proessing for
Safety Critial Translation and Pervasive Applia-
tions, Manhester.
P. Bouillon, G. Flores, M. Georgesul, S. Halimi,
B.A. Hokey, H. Isahara, K. Kanzaki, Y. Nakao,
M. Rayner, M. Santaholma, M. Starlander, and
N. Tsourakis. 2008b. Many-to-many multilingual
medial speeh translation on a PDA. In Proeed-
ings of The Eighth Conferene of the Assoiation
for Mahine Translation in the Amerias, Waikiki,
Hawaii.
N. Chatzihrisas, P. Bouillon, M. Rayner, M. San-
taholma, M. Starlander, and B.A. Hokey. 2006.
Evaluating task performane for a unidiretional
ontrolled language medial speeh translation sys-
tem. In Proeedings of the HLT-NAACL Interna-
tional Workshop on Medial Speeh Translation,
pages 9?16, New York.
L. Dugast, J. Senellart, and P. Koehn. 2008. Can we
relearn an RBMT system? In Proeedings of the
Third Workshop on Statistial Mahine Translation,
pages 175?178, Columbus, Ohio.
B.A. Hokey, M. Rayner, and G. Christian. 2008.
Training statistial language models from grammar-
generated data: A omparative ase-study. In Pro-
eedings of the 6th International Conferene on Nat-
ural Language Proessing, Gothenburg, Sweden.
R. Jonson. 2005. Generating statistial language mod-
els from interpretation grammars in dialogue sys-
tems. In Proeedings of the 11th EACL, Trento,
Italy.
A. Jurafsky, C. Wooters, J. Segal, A. Stolke, E. Fos-
ler, G. Tajhman, and N. Morgan. 1995. Us-
ing a stohasti ontext-free grammar as a language
model for speeh reognition. In Proeedings of
the IEEE International Conferene on Aoustis,
Speeh and Signal Proessing, pages 189?192.
P. Koehn, H. Hoang, A. Birh, C. Callison-Burh,
M. Federio, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open soure
toolkit for statistial mahine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 2.
F.J. Oh and H. Ney. 2000. Improved statistial align-
ment models. In Proeedings of the 38th Annual
Meeting of the Assoiation for Computational Lin-
guistis, Hong Kong.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
BLEU: a method for automati evaluation of ma-
hine translation. Researh Report, Computer Si-
ene RC22176 (W0109-022), IBM Researh Divi-
sion, T.J.Watson Researh Center.
M. Rayner, P. Bouillon, N. Chatzihrisas, B.A.
Hokey, M. Santaholma, M. Starlander, H. Isahara,
K. Kanzaki, and Y. Nakao. 2005. A methodol-
ogy for omparing grammar-based and robust ap-
proahes to speeh understanding. In Proeedings
of the 9th International Conferene on Spoken Lan-
guage Proessing (ICSLP), pages 1103?1107, Lis-
boa, Portugal.
M. Rayner, B.A. Hokey, and P. Bouillon. 2006.
Putting Linguistis into Speeh Reognition: The
Regulus Grammar Compiler. CSLI Press, Chiago.
M. Rayner, P. Bouillon, G. Flores, F. Ehsani, M. Star-
lander, B. A. Hokey, J. Brotanek, and L. Biewald.
2008. A small-voabulary shared task for medial
speeh translation. In Proeedings of the COLING
Workshop on Speeh Proessing for Safety Criti-
al Translation and Pervasive Appliations, Manh-
ester.
S. Seneff, C. Wang, and J. Lee. 2006. Combining lin-
guisti and statistial methods for bi-diretional En-
glish Chinese translation in the ight domain. In
Proeedings of AMTA 2006.
A. Stolke. 2002. SRILM - an extensible language
modeling toolkit. In Seventh International Confer-
ene on Spoken Language Proessing. ISCA.
Y. Wilks. 2007. Stone soup and the Frenh room. In
K. Ahmad, C. Brewster, and M. Stevenson, editors,
Words and Intelligene I: Seleted Papers by Yorik
Wilks, pages 255?265.
62
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 667?672,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
 SAGAN: An approach to Semantic Textual Similarity  
based on Textual Entailment 
 
Julio Castillo??      Paula Estrella? 
                  
?
FaMAF, UNC, Argentina  
                      
?
UTN-FRC, Argentina 
                        jotacastillo@gmail.com 
                     pestrella@famaf.unc.edu.ar 
 
 
 
 
 
 
 
 
Abstract 
In this paper we report the results obtained 
in the Semantic Textual Similarity (STS) 
task, with a system primarily developed for 
textual entailment. Our results are quite 
promising, getting a run ranked 39 in the 
official results with overall Pearson, and 
ranking 29 with the Mean metric. 
 
1 Introduction 
For the last couple of years the research com-
munity has focused on a deeper analysis of natural 
languages, seeking to capture the meaning of the 
text in different contexts: in machine translation 
preserving the meaning of the translations is cru-
cial to determine whether a translation is useful or 
not, in question-answering understanding the ques-
tion leads to the desired answers (while the oppo-
site case makes a system rather frustrating to the 
user) and the examples could continue. In this 
newly defined task, Semantic Textual Similarity, 
there is hope that efforts in different areas will be 
shared and united towards the goal of identifying 
meaning and recognizing equivalent, similar or 
unrelated texts. Our contribution to the task, is 
from a textual entailment point of view, as will be 
described below. 
The paper is organized as follows: Section 2 de-
scribes the relevant tasks, Section 3 describes the 
architecture of the system, then Section 4 shows 
the experiments carried out and the results ob-
tained, and Section 5 presents some conclusions 
and future work. 
2 Related work  
In this section we briefly describe two different 
tasks that are closely related and in which our sys-
tem has participated with very promising results. 
 
2.1 Textual Entailment 
 
Textual Entailment (TE) is defined as a generic 
framework for applied semantic inference, where 
the core task is to determine whether the meaning 
of a target textual assertion (hypothesis, H) can be 
inferred from a given text (T). For example, given 
the pair (T,H): 
T: Fire bombs were thrown at the Tunisian embas-
sy in Bern 
H: The Tunisian embassy in Switzerland was at-
tacked 
we can conclude that T entails H. 
 
The recently created challenge ?Recognising 
Textual Entailment? (RTE) started in 2005 with 
the goal of providing a binary answer for each pair 
(H,T), namely whether there is entailment or not 
(Dagan et al, 2006). The RTE challenge has mu-
tated over the years, aiming at accomplishing more 
667
accurate and specific solutions; for example, in 
2008 a three-way decision was proposed (instead 
of the original binary decision) consisting of ?en-
tailment?, ?contradiction? and ?unknown?; in 2009 
the organizers proposed a pilot task, the Textual 
Entailment Search (Bentivogli et al 2009), consist-
ing in finding all the sentences in a set of docu-
ments that entail a given Hypothesis and since 
2010 there is a Novelty Detection Task, which 
means that RTE systems are required to judge 
whether the information contained in each H is 
novel with respect to (i.e., not entailed by) the in-
formation contained in the corpus. 
2.2 Semantic Textual Similarity 
The pilot task STS was recently defined in 
Semeval 2012 (Aguirre et al, 2012) and has as 
main objective measuring the degree of semantic 
equivalence between two text fragments. STS is 
related to both Recognizing Textual Entailment 
(RTE) and Paraphrase Recognition, but has the 
advantage of being a more suitable model for mul-
tiple NLP applications.  
As mentioned before, the goal of the RTE task 
(Bentivogli et al 2009) is determining whether the 
meaning of a hypothesis H can be inferred from a 
text T. Thus, TE is a directional task and we say 
that T entails H, if a person reading T would infer 
that H is most likely true.  The difference with STS 
is that STS consists in determining how similar 
two text fragments are, in a range from 5 (total 
semantic equivalence) to 0 (no relation). Thus, 
STS mainly differs from TE in that the classifica-
tion is graded instead of binary. In this manner, 
STS is filling the gap between several tasks. 
3 System architecture  
Sagan is a RTE system (Castillo and Cardenas, 
2010) which has taken part of several challenges, 
including the Textual Analysis Conference 2009 
and TAC 2010, and the Semantic Textual Similari-
ty and Cross Lingual Textual Entailment for con-
tent synchronization as part of the Semeval 2012. 
 The system is based on a machine learning ap-
proach and it utilizes eight WordNet-based 
(Fellbaum, 1998) similarity measures, as explained 
in (Castillo, 2011), with the purpose of obtaining 
the maximum similarity between two WordNet 
concepts. A concept is a cluster of synonymous 
terms that is called a synset in WordNet. These 
text-to-text similarity measures are based on the 
following word-to-word similarity metrics: 
(Resnik, 1995), (Lin, 1997), (Jiang and Conrath, 
1997), (Pirr? and Seco, 2008), (Wu & Palmer, 
1994), Path Metric, (Leacock & Chodorow, 1998), 
and a semantic similarity to sentence level named 
SemSim (Castillo and Cardenas,2010).  
 
Pre-Processing
Similarity Score
MSR
Word Level Semantic Metrics
Extraction Features 
SVM with 
Regression
Test Set:  MSR, 
MSRvid,Europarl, 
SMT-news, WN
RUN 1 
Normalizer Stemmer Parser
Resnik SemSimW&PLin ...
Sentence Level Semantic Metric
MSR+MSRvid
RUN 2
RUN 3 
MSR+MSRvid
+Europarl
Training sets:
...
Fig.1. System architecture  
 
The system construct a model of the semantic 
similarity of two texts (T,H) as a function of the 
semantic similarity of the constituent words of 
both phrases. In order to reach this objective, we 
used a text to text similarity measure which is 
based on word to word similarity. Thus, we expect 
that combining word to word similarity metrics to 
text level would be a good indicator of text to text 
similarity.  
Additional information about how to produce 
feature vectors as well as each word- and sentence-
level metric can be found in (Castillo, 2011). The 
architecture of the system is shown in Figure 1. 
The training set used for the submitted runs are 
those provided by the organizers of the STS. How-
ever we also experimented with RTE datasets as 
described in the next Section.  
668
4 Experiments and Results 
For preliminary experiments before the STS Chal-
lenge, we used the training set provided by the 
organizers, denoted with "_train", and consisting of 
750 pairs of sentences from the MSR Paraphrase 
Corpus (MSRpar), 750 pairs of sentences from the 
MSRvid Corpus (MSRvid), 459 pairs of sentences 
of the Europarl WMT2008 development set (SMT-
eur). We also used the RTE datasets from Pascal 
RTE Challenge (Dagan et al, 2006) as part of our 
training sets. Additionally, at the testing stage, we 
used the 399 pairs of  news conversation (SMT-
news) and 750 pairs of sentences where the first 
one comes from Ontonotes and the second one 
from a WordNet definition (On-WN).   
In STS Challenge it was required that participat-
ing systems do not use the test set of MSR-
Paraphrase, the text of the videos in MSR-Video, 
and the data from the evaluation tasks at any WMT 
to develop or train their systems. Additionally, we 
also assumed that the dataset to be processed was 
unknown in the testing phase, in order to avoid any 
kind of tuning of the system. 
4.1 Preliminary Experiments 
In a preliminary study performed before the final 
submission, we experimented with three machine 
learning algorithms Support Vector Machine 
(SVM) with regression and polynomial kernel, 
Multilayer perceptron (MLP), and Linear Regres-
sion (LR). Table 1 shows the results obtained with 
10-fold cross validation technique and Table 2 
shows the results of testing them with two datasets 
and 3 classifiers over MSR_train. 
 
Classifier Pearson c.c 
SVM with regression 0.54 
MLP 0.51 
LinearRegression 0.54 
Table 1. Results obtained using MSR training set 
(MSRpar + MSRvid) with 10 fold-cross validation. 
 
Training set & ML algorithm Pearson c.c 
Europarl + SVM w/ regression 0.61 
Europarl + MLP 0.44 
Europarl + linear regression 0.61 
MSRvid + SVM w/ regression 0.70 
MSRvid + MLP 0.52 
MSRvid + linear regression 0.69 
Table 2. Results obtained using MSR training set  
Results reported in Table 1 show that we 
achieved the best performance with SVM with 
regression and Linear Regression classifiers and 
using MLP we obtained the worst results to predict 
each dataset. To our surprise, a linear regression 
classifier reports better accuracy that MLP, it may 
be mainly due to the correlation coefficient used, 
namely Pearson, which is a measure of a linear 
dependence between two variables and linear re-
gression builds a model assuming linear influence 
of independent features. We believe that using 
Spearman correlation should be better than using 
the Pearson coefficient given that Spearman as-
sumes non-linear correlation among variables. 
However, it is not clear how it behaves when sev-
eral dataset are combined to obtain a global score. 
Indeed, further discussion is needed in order to 
find the best metric to the STS pilot task. Given 
these results, in our submission for the STS pilot 
task we used a combination of STS datasets as 
training set and the SVM with regression classifier. 
Because our approach is mainly based on ma-
chine learning the quality and quantity of dataset is 
a key factor to determine the performance of the 
system, thus we decided to experiment with RTE 
datasets too (Bentivogli et el., 2009) with the aim 
of increasing the size of the training set.  
To achieve this goal, first we chose the RTE3 
dataset because it is simpler than subsequent da-
tasets and it was proved to provide a high accuracy 
predicting other datasets (Castillo, 2011). Second, 
taking into account that RTE datasets are binary 
classified as YES or NO entailment, we assumed 
that a non entailment can be treated as a value of 
2.0 in the STS pilot task and an entailment can be 
thought of as a value of 3.0 in STS. Of course, 
many pairs classified as 3.0 could be mostly equiv-
alent (4.0) or completely equivalent (5.0) but we 
ignored this fact in the following experiment.  
 
Training set Test set Pearson 
c.c. 
RTE3 MSR_train 0.4817 
RTE3 MSRvid_train 0.5738 
RTE3 Europarl_train 0.4746 
MSR_train+RTE3 MSRvid_train 0.5652 
MSR_train+RTE3 Europarl_train 0.5498 
MSRvid_train+RTE3 MSR_train 0.4559 
MSRvid_train+RTE3 Europarl_train 0.4964 
Table 3. Results obtained using RTE in the training sets 
and SVM w/regression as classifier 
 
669
From these experiments we conclude that RTE3 
alone is not enough to adequately predict neither of 
the STS datasets, and it is understandable if we 
note that only one pair with 2.0 and 3.0 scores are 
present in this dataset.  
On the other hand, by combining RTE3 with a 
STS corpus we always obtain a slight decrease in 
performance in comparison to using STS alone. It 
is likely due to an unbalanced set and possible 
contradictory pairs (e.g: a par in RTE3 classified as 
3.0 when it should be classified 4.3). Thus, we 
conclude that in order to use the RTE datasets our 
system needs a manual annotation of the degree of 
semantic similarity of every pair <T,H> of RTE 
dataset. 
Having into account that in our training phase 
we obtained a decrease in performance using RTE 
datasets we decided not to submit any run using 
the RTE datasets. 
4.2 Submission to the STS shared task  
Our participation in the shared task consisted of 
three different runs using a SVM classifier with 
regression; the runs were set up as follows: 
- Run 1: system trained on a subset of the Mi-
crosoft Research Paraphrase Corpus (Dolan and 
Brockett, 2005), named MSR and consisting of 
750 pairs of sentences marked with a degree of 
similarity from 5 to 0.  
- Run 2: in addition to the MSR corpus we incor-
porated another 750 sentences extracted from the 
Microsoft Research Video Description Corpus 
(MSRvid), annotated in the same way as MSR. 
- Run 3: to the 1500 sentences from the MSR and 
MSRvid corpus we incorporated 734 pairs of sen-
tences from the Europarl corpus used as develop-
ment set in the WMT 2008; all sentences are 
annotated with the degree of similarity from 5 to 0. 
It is very interesting to note that we used the 
same system configurations for every dataset of 
each RUN. In this manner, we did not perform any 
kind of tuning to a particular dataset before our 
submission. We decided to ignore the "name" of 
each dataset and apply our system regardless of the 
particular dataset. Surely, if we take into account 
where each dataset came from we can develop a 
particular strategy for every one of them, but we 
assumed that this kind of information is unknown 
to our system.  
The official scores of the STS pilot task is the 
Pearson correlation coefficient, and other varia-
tions of Pearson which were proposed by the or-
ganizers with the aim of better understanding the 
behavior of the competing systems among the dif-
ferent scenarios. 
These metric are named ALL (overall Pearson), 
ALLnrm (normalized Pearson) and Mean 
(weighted mean), briefly described below: 
- ALL: To compute this metric, first a new dataset 
with the union of the five gold datasets is created 
and then the Pearson correlation is calculated over 
this new dataset. 
- ALLnrm: In this metric, the Pearson correlation 
is computed after the system outputs for each da-
taset are fitted to the gold standard using least 
squares. 
- Mean: This metric is a weighted mean across the 
five datasets, where the weight is given by the 
quantity of pairs in each dataset. 
Table 5 report the results achieved with these 
metrics followed by an individual Pearson correla-
tion for each dataset. 
Interestingly, if we analyze the size of data sets, 
we see that the larger the training set used, the 
greater the efficiency gains with ALL metric. In 
effect, RUN3 used 2234 pairs, RUN2 used 1500 
pairs and RUN1 was composed by 750 pairs. This 
highlights the need for larger datasets for the pur-
pose of building more accurate models.  
With ALLnrm our system achieved better re-
sults but since this metric is based on normalized 
Pearson correlation which assumes a linear correla-
tion, we believe that this metric is not representa-
tive of the underlying phenomenon. For example, 
conducting manual observation we can see that 
pairs from SMT-news are much harder to classify 
than MSRvid pairs. This results can also be evi-
denced from others participating teams who almost 
always achieved better results with MSRvid than 
SMT-news dataset.  
The last metric proposed is the Mean and we are 
ranked 29 among participating teams. It is proba-
bly due to the weight of SMT-news (399 pairs) is 
smaller than MSR or MSRvid. 
Mean metrics seems to be more suitable for this 
task but lack an important issue, do not have into 
account the different "complexity" of the datasets. 
It is also a issue for all metrics proposed. We be-
lieve that incorporating to Mean metric a complex-
ity factor weighting for each dataset based on a 
670
human judge assignment could be more suitable 
for the STS evaluation. We think in complexity as 
an underlying concept referring to the difficulty of 
determine how semantically related two sentences 
are to one another. Thus, two sentences with high 
lexical overlap should have a low complexity and 
instead two sentences that requires deep inference 
to determine similarity should have a high com-
plexity. This should be heighted by human annota-
tors and could be a method for a more precise 
evaluation of STS systems. 
 
Finally, we suggested measuring this new chal-
lenging task using a weighted Mean of the 
Spearman's rho correlation coefficient by incorpo-
rating a factor to weigh the difficulty of each da-
taset. 
 
 
 
 
 
 
Run ALL Rank ALLnrm 
Rank
Nrm 
Mean 
Rank
Mean 
MSR
par 
MSR
vid 
SMT-
eur 
On-
WN 
SMT-
news 
Best Run ,8239 1 ,8579 2 ,6773 1 ,6830 ,8739 ,5280 ,6641 ,4937 
Worst Run -,0260 89 ,5933 89 ,1016 89 ,1109 ,0057 ,0348 ,1788 ,1964 
Sagan-RUN1 ,5522 57 ,7904 47 ,5906 29 ,5659 ,7113 ,4739 ,6542 ,4253 
Sagan-RUN2 ,6272 42 ,8032 37 ,5838 34 ,5538 ,7706 ,4480 ,6135 ,3894 
Sagan-RUN3 ,6311 39 ,7943 45 ,5649 46 ,5394 ,7560 ,4181 ,5904 ,3746 
Table 5. Official results of the STS challenge 
 
5 Conclusions and future work 
In this paper we present Sagan, an RTE system 
applied to the task of Semantic Textual Similarity. 
After a preliminary study of the classifiers perfor-
mance for the task, we decided to use a combina-
tion of STS datasets for training and the classifier 
SVM with regression. With this setup the system 
was ranked 39 in the best run with overall Pearson, 
and ranked 29 with Mean metric. However, both 
rankings are based on the Pearson correlation coef-
ficient and we believe that this coefficient is not 
the best suited for this task, thus we proposed a 
Mean Spearman's rho correlation coefficient 
weighted by complexity, instead. Therefore, fur-
ther application of other metrics should be one in 
order to find the most representative and fair eval-
uation metric for this task. Finally, while promis-
ing results were obtained with our system, it still 
needs to be tested on a diversity of settings. This is 
work in progress, as the system is being tested as a 
metric for the evaluation of machine translation, as 
reported in (Castillo and Estrella, 2012). 
References  
Christoph Tillmann, Stephan Vogel, Hermann Ney, 
Arkaitz Zubiaga, and Hassan Sawaf. 1997. Acceler-
ated DP Based Search For Statistical Translation. In 
Proceedings of the 5th European Conference on 
Speech Communication and Technology  
(EUROSPEECH-97). 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of the 40th AnnualMeeting of the Association for 
Computational Linguistics(ACL-02), pages 311?318. 
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and 
Hermann Ney. 2000. A Evaluation Tool for Machine 
Translation:Fast Evaluation for MT Research. In 
Proceedings of the 2nd International Conference on 
Language Resources and Evaluation (LREC-2000). 
G. Doddington. 2002. Automatic Evaluation of Machine 
Translation Quality using N-gram Co-occurrence 
Statistics.  In Proceedings of the 2nd International 
Conference on Human Language Technology Re-
search (HLT-02), pages 138?145, San Francisco, 
CA, USA. 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the 43th Annual Meeting of the 
Association of Computational Linguistics (ACL-05), 
pages 65?72. 
Michael Denkowski and Alon Lavie. 2011. METEOR-
NEXT and the METEOR Paraphrase Tables: Im-
proved Evaluation Support For Five Target Lan-
guages. Proceedings of the ACL 2010 Joint 
Workshop on Statistical Machine Translation and 
Metrics MATR. 
671
He Yifan, Du Jinhua, Way Andy, and Van Josef . 2010. 
The DCU dependency-based metric in WMT-
MetricsMATR 2010. In: WMT 2010 - Joint Fifth 
Workshop on Statistical Machine Translation and 
Metrics MATR, ACL, Uppsala, Sweden. 
Chi-kiu Lo and Dekai Wu. 2011. MEANT: inexpensive, 
high-accuracy, semi-automatic metric for evaluating 
translation utility based on semantic roles. 49th An-
nual Meeting of the Association for Computational 
Linguistic (ACL-2011). Portland, Oregon, US. 
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A Study 
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the 
Association for Machine Translation in the Americas 
(AMTA-06), pages 223?231. 
Ido Dagan, Oren Glickman and Bernardo Magnini. 
2006. The PASCAL Recognising Textual Entailment 
Challenge. In Qui?onero-Candela, J.; Dagan, I.; 
Magnini, B.; d'Alch?-Buc, F. (Eds.) Machine Learn-
ing Challenges. Lecture Notes in Computer Science , 
Vol. 3944, pp. 177-190, Springer. 
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido 
Dagan, Marc Dymetman, and Idan Szpektor. 2009. 
Source-language entailment modeling for translating 
unknown terms. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL. 
Stroudsburg, PA, USA, 791-799. 
Wilker Aziz and Marc Dymetmany and Shachar Mirkin 
and Lucia Specia and Nicola Cancedda and Ido Da-
gan. 2010. Learning an Expert from Human Annota-
tions in Statistical Machine Translation: the Case of 
Out-of-VocabularyWords. In: Proceedings of the 
14th annual meeting of the European Association for 
Machine Translation (EAMT), Saint-Rapha, France. 
Dahlmeier, Daniel  and  Liu, Chang  and  Ng, Hwee 
Tou. 2011.TESLA at WMT 2011: Translation Evalu-
ation and Tunable Metric.In: Proceedings of the 
Sixth Workshop on Statistical Machine Translation. 
ACL,  pages 78-84, Edinburgh, Scotland. 
S. Pado, D. Cer, M. Galley, D. Jurafsky and C. Man-
ning. 2009. Measuring Machine Translation Quality 
as Semantic Equivalence: A Metric Based on Entail-
ment Features. Journal of MT 23(2-3), 181-193.  
S. Pado, M. Galley, D. Jurafsky and C. Manning. 2009a. 
Robust Machine Translation Evaluation with Entail-
ment Features. Proceedings of ACL 2009. 
Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-
lez-Agirre.    2012. SemEval-2012 Task 6: A Pilot on 
Semantic Textual Similarity.    In Proceedings of the 
6th International Workshop on Semantic    Evalua-
tion (SemEval 2012), in conjunction with the First 
Joint    Conference on Lexical and Computational 
Semantics (*SEM 2012). 
Bentivogli, Luisa, Dagan Ido, Dang Hoa, Giampiccolo, 
Danilo, Magnini Bernardo.2009.The Fifth PASCAL 
RTE Challenge. In: Proceedings of the Text Analysis 
Conference.  
Fellbaum C. 1998. WordNet: An Electronic Lexical 
Database, volume 1. MIT Press. 
Castillo Julio. 2011. A WordNet-based semantic ap-
proach to textual entailment and cross-lingual textu-
al entailment. International Journal of Machine 
Learning and Cybernetics - Springer, Volume 2, 
Number 3. 
Castillo Julio and Cardenas Marina. 2010. Using sen-
tence semantic similarity based onWordNet in recog-
nizing textual entailment. Iberamia 2010. In LNCS, 
vol 6433. Springer, Heidelberg, pp 366?375. 
Castillo Julio. 2010. A semantic oriented approach to 
textual entailment using WordNet-based measures. 
MICAI 2010. LNCS, vol 6437. Springer, Heidelberg, 
pp 44?55. 
Castillo Julio. 2010. Using machine translation systems 
to expand a corpus in textual entailment. In: Proceed-
ings of the Icetal 2010. LNCS, vol 6233, pp 97?102. 
Resnik P. 1995. Information content to evaluate seman-
tic similarity in a taxonomy. In: Proceedings of IJCAI 
1995, pp 448?453 907. 
Castillo Julio, Cardenas Marina. 2011. An Approach to 
Cross-Lingual Textual Entailment using Online Ma-
chine Translation Systems. Polibits Journal. Vol 44. 
Castillo Julio and Estrella Paula. 2012. Semantic Textu-
al Similarity for MT evaluation. NAACL 2012  
Seventh Workshop on Statistical Machine Transla-
tion. WMT 2012, Montreal, Canada. 
Lin D. 1997. An information-theoretic definition of 
similarity. In: Proceedings of Conference on Machine 
Learning, pp 296?304 909. 
Jiang J, Conrath D.1997. Semantic similarity based on 
corpus statistics and lexical taxonomy. In: Proceed-
ings of theROCLINGX 911 
Pirro G., Seco N. 2008. Design, implementation and 
evaluation of a new similarity metric combining fea-
ture and intrinsic information content. In: ODBASE 
2008, Springer LNCS. 
Wu Z, Palmer M. 1994. Verb semantics and lexical 
selection. In: Proceedings of the 32nd ACL 916. 
Leacock C, Chodorow M. 1998. Combining local con-
text and WordNet similarity for word sense identifi-
cation. MIT Press, pp 265?283 919 
Hirst G, St-Onge D . 1998. Lexical chains as represen-
tations of context for the detection and correction of 
malapropisms. MIT Press, pp 305?332 922 
Banerjee S, Pedersen T. 2002. An adapted lesk algo-
rithm for word sense disambiguation using WordNet. 
In: Proceeding of CICLING-02 
William B. Dolan and Chris Brockett.2005. Automati-
cally Constructing a Corpus of Sentential Para-
phrases. Third International Workshop on 
Paraphrasing (IWP2005). Asia Federation of Natural 
Language Processing. 
672
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 132?140, Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Dialogue Systems for Virtual Environments
Luciana Benotti, Paula Estrella, Carlos Areces
Grupo de Procesamiento de Lenguaje Natural (PLN)
Seccio?n de Ciencias de la Computacio?n
Facultad de Matema?tica, Astronom??a y F??sica (FaMAF)
Universidad Nacional de Co?rdoba, Argentina
Abstract
We present an on-going research project car-
ried out at the Universidad Nacional de Co?rdo-
ba in Argentina. This project investigates the-
oretical and practical research questions re-
lated to the development of a dialogue system
situated in a virtual environment. We describe
the PLN research group in which this project
is being developed and, in particular, we spell
out the areas of expertise of the authors. More-
over, we discuss relevant past, current and fu-
ture collaborations of the research group.
1 Introduction
The goal of this project is to implement a dialogue
system which automatically generates instructions
in order to help a user to fulfill a given task in a 3D
virtual environment. In this context, we will investi-
gate fundamental issues about human-computer in-
teraction. The expected results of the project can be
classified in three areas: pragmatics of interaction;
information representation and inference; and eval-
uation of dialogue systems. Once a working proto-
type is finished, we will adapt it to the specific task
of language learning, using the system as a virtual
language teacher. Our prototype will teach English
to native Spanish speakers. Hence, it will need to
understand and produce both languages.
Initially, we will investigate a model of unidirec-
tional linguistic interaction (i.e., linguistic informa-
tion flows only from the system to the user). In sub-
sequent stages, the model will be extended to allow
bidirectional language exchange. For example, the
user may ask clarifications to the system or redefine
the goal of the interaction.
The architecture of the envisioned dialogue sys-
tem presents both theoretical and practical chal-
lenges. On the theoretical side, heuristics are needed
in order to govern decisions such as what to say,
when, and how (given the current context). In addi-
tion, the system should implement inference meth-
ods in order to figure out how to modify the cur-
rent situation and reach the task goal. The complex-
ity of the theoretical issues is reflected, in practice,
in a system of multiple components: a natural lan-
guage generator, a planner, a 3D interactive envi-
ronment, to mention a few. Designing and imple-
menting all these components from scratch would
require a prohibitive effort. Instead we will adapt
tools already implemented and freely available for
prototyping this kind of systems, such as the plat-
form GIVE1, Generating Instructions in Virtual En-
vironments (Byron et al, 2009).
The quality of each of the components of the sys-
tem affects the perception users have of it. It is im-
perative to carry out extensive evaluation. We plan
to adapt and apply different evaluation techniques
and metrics from the area of Machine Translation to
assess the performance of the system.
The plan of the paper is as follows. Section 2
describes the project in detail. Section 3 spells out
the expected results as well as their foreseen impact
in the Argentinean socio-economic landscape. Sec-
tion 4 presents the PLN research group including its
lines of research. Section 5 discuss past, current and
future collaborations that are relevant to the project.
1http://www.give-challenge.org
132
2 Description of the Project
This section first introduces the virtual environ-
ment in which our dialogue system will be situated,
namely the GIVE platform, which is the basic ar-
chitecture of our dialogue system. Then we explain
in detail the tasks that our situated dialogue system
will implement, and we spell out the evaluation chal-
lenges that such a system poses. We close the sec-
tion discussing the application of our dialogue sys-
tem for the task of second language learning.
2.1 The Virtual Environment
In the scenario proposed by GIVE (Byron et al,
2009), a human user carries out a ?treasure hunt? in
a 3D virtual environment and the task of the genera-
tion system is to provide real-time, natural language
instructions that help the user find the hidden trea-
sure.
In the GIVE setup, the instruction giving system
must guide the user through interconnected rooms.
The final goal is to get a trophy which is hidden in
a safe. In order to achieve this goal, the system in-
structs the user to perform several subtasks such as
deactivating alarms and opening the safe combina-
tion by pressing a sequence of buttons on the walls
of the rooms.
Figure 1: The user?s view of the 3D world
Figure 1 shows a screen-shot of the user?s view on
the 3D world. On the top of the picture, the current
instruction generated by the dialogue system is dis-
played. The picture shows a closed door and an open
door that has an activated alarm (that looks like a red
tile) in the doorway. There are five visible buttons in
this room (two yellow, two red and one green) and
the instruction giver is instructing the user to press
a red button. Pressing a button can have different
effects such as opening a door, moving an object,
deactivating an alarm, etc.
The characteristics of the world, including the
functions of the buttons, are described in the world
specification by the world designers. The user can
move freely around the world (using the direction
keys as indicated in the bottom of the screen) but
she can loose the game if she triggers an alarm. The
user can also ask for help pressing ?H? if she did not
manage to read or understand the last instruction.
For the correct definition of the interaction poli-
cies of our prototype we need a corpus that pro-
vides examples of typical interactions in the domain.
GIVE provides tools for collecting such a corpus in
the form of a Wizard of Oz platform that records all
details of the interaction, thus allowing to easily ob-
tain a corpus of interaction in virtual environments
annotated automatically.
2.2 The Dialogue System Tasks
From the collected corpus we will begin the design,
implementation and testing of our dialogue system.
The main components that we will have to design
and implement can be organized using the tradi-
tional four tasks that a dialogue system should ad-
dress: (1) content planning, (2) generation of refer-
ring expressions, (3) management of the interaction
context, and (4) interpretation of user responses.
(1) Content Planning: Given the envisioned setup
we described before, the first task of the system is to
obtain a plan to reach the desired goal, from the cur-
rent state. The plan will contain physical actions to
be performed in the virtual environment. The second
step is to decide how to transmit this sequence of ac-
tions to the user. E.g, to decide how many actions
to communicate per instruction, and how to aggre-
gate them coherently. The result of the action ag-
gregation process can be represented as a tree de-
scribing the task structure at different levels of ab-
straction. The third and final step is to decide how
to navigate the tree of actions to verbalize the in-
structions (for example, post or preorder as explored
in (Foster et al, 2009)). We will investigate different
133
aggregation policies (e.g., aggregating actions that
manipulate similar objects) and innovative ways in
which to navigate the task tree (e.g., moving to a
lower level of abstraction in case of misunderstand-
ings). Plan computation can be solved using clas-
sical planners (Kautz and Selman, 1999; Hoffmann
and Nebel, 2001; Nau et al, 2004). However, while
there are planners that work well when optimized for
certain applications, none provides services such as
the generation of alternative plans, or the generation
of incomplete plans in case of the absence of plan.
One of the goals of the project is to design and im-
plement these extensions to classical planning algo-
rithms. We will also study the theoretical behavior
(e.g., complexity) of these new algorithms.
(2) Generation of Referring Expressions: Once
content planning is complete, the next step is to gen-
eration adequate referring expressions. This task
involves producing a phrase that describes a refer-
able entity so that the user can identify it (e.g., ?the
vase on the table?). To be acceptable, these expres-
sions should be ?natural:? they should be at the same
time sufficiently but not overly constrained, and they
should not impose on the user a heavier cognitive
load than necessary. For example, producing the
expression ?the vase that is not above the chair or
sofa or under the table? would probably not be ac-
ceptable. Areces et al (2008b) propose to use sym-
bolic minimization of the model that represents the
state of the world, in order to obtain a logical repre-
sentation that describe each object uniquely. In our
project we will implement this method and evaluate
it within the dialogue system.
(3) Management of the Interaction Context: To
manage the use of the interaction context we will use
existing knowledge maintenance systems such as
RACER2 or Pellet3, which support inference tasks
such as definition, maintenance and querying of on-
tologies. These systems have been used as infer-
ence engines in numerous applications in the area
and, in particular, in dialogue systems for text ad-
ventures (Benotti, 2009b). Once we have studied
the behavior of these inference engines on the task,
we will analyze its limitations and investigate the re-
quired extensions.
2http://www.racer-systems.com
3http://clarkparsia.com/pellet
(4) Interpretation of User Responses: The inter-
pretation of user responses in the unidirectional sys-
tem is relatively simple: it amounts to discretizing
the continuous flow of user behavior in the 3D world
into actions meaningful for the domain task. In a
first stage, we will use the discretizer provided by
GIVE. After evaluating it we can determine whether
or not this module meets the requirements of our
task and what are its limitations. In the bidirectional
system, however, the interpretation of user responses
is the task that will require more attention. To start
with, the bidirectional system should be expanded
with capabilities for processing statements coming
from the user (namely, parsing, semantic construc-
tion, resolution of references, etc.). We will study,
in particular, two types of user contributions: re-
quests for clarification of the instruction given (what
we call ?short-term repairs?), and for redefinition of
goals (what we call ?long-term repairs?). We will
implement short-term repairs using the approach de-
scribed in (Purver, 2006). For long-term repairs we
will use the guidelines of (Blaylock, 2005).
A sample interaction with the unidirectional sys-
tem guiding the player in the identification of a par-
ticular blue button is as follows:
(1) System says: Push a blue button.
The user focuses a blue button.
System says: Not this one.
Look for another one.
The user turns and focuses another blue button.
System says: Yes this one!
The user pushes the button.
This interaction illustrates the tasks described
above. To begin with, the verbalization of the in-
struction ?Push a blue button? is making explicit one
of the steps of the plan that needs to be performed in
order to achieve the task goal. As we can see, the
system implements in this case a referring strategy
which does not uniquely identify the referent (the
system generates ?a blue button? when there is more
than one blue button in the domain). But it is ca-
pable of producing further details about the referent
if the user focus in the wrong object. Finally, this
example makes evident that the interpretation of the
user responses is crucial even in a linguistically uni-
directional system. The user cannot make linguistic
134
contributions but can change the context by perform-
ing physical acts, the correct interpretation of such
acts is essential if the system is to react coherently.
2.3 Evaluation
To determine the quality of the obtained prototypes
we propose to create a quality model following the
ISO/IEC 9126 and 14528 standards for the evalua-
tion of software products (ISO/IEC, 2001; ISO/IEC,
1999). These standards were successfully applied
to the Machine Translation domain, resulting in the
FEMTI4, Framework for the Evaluation of Machine
Translation (Estrella et al, 2005). FEMTI guides
evaluators towards creating parameterized evalua-
tion plans that include various aspects of the to-be-
evaluated system and offer a relevant set of met-
rics. The identification of relevant metrics can be
performed using various methods, e.g., based on
previous experience (Hajdinjak and Mihelic, 2006;
Litman and Pan, 2002), conducting surveys or re-
quirement specifications (Lecoeuche et al, 1998), or
collecting such data through Wizard of Oz experi-
ments (Dahlba?ck et al, 1998). After developing a
quality model, several methodologies to assess vari-
ous aspects of the system can be applied: automatic
metrics, subjective metrics or metrics based on the
task (to evaluate both the contribution of each com-
ponent and the quality of the whole system).
The GIVE platform is used every year as a uni-
fied framework for evaluating generation systems.
Systems have to generate natural language instruc-
tions and be able to participate in a real-time interac-
tion situated in a 3D environment. The GIVE Chal-
lenge is one of the shared tasks endorsed by ACL?s
special interests groups in generation, dialogue and
semantics. We plan to participate in the challenge,
which will serve as an additional source of informa-
tion about aspects of the system that need improve-
ment. The evaluation metrics used in the Challenge
(such as average reference identification time) are
described in (Byron et al, 2009). In (Amoia et al,
2010) we extended such metrics in order to measure
alingment between system and user. Once the pro-
totype is evaluated and improved using the results of
the challenge, we will investigate its use as a virtual
language tutor as described in the next section.
4http://www.issco.unige.ch/femti/
2.4 An Application: A Virtual Tutor
The project outcome will be a system capable of giv-
ing natural language instructions situated in a virtual
3D environment. The technology and theoretical ad-
vances of the project could be used in various appli-
cations, but one of the most interesting character-
istics we plan to investigate is that, a priori, by just
changing the linguistic resources, the language of in-
teraction with the system (input and output) can be
changed as desired. After obtaining a first prototype
of an instruction giving dialogue system, we will in-
vestigate its use for distance learning, adapting the
system to operate as a foreign language tutor (Wik
and Hjalmarsson, 2009).
A one-way system that generates instructions in
English can be used to test the user understanding
of a foreign language. The correct interpretation of
the instructions can be evaluated from the proper ex-
ecution of the instructions. The two-way system
will allow the user to formulate clarifications (ei-
ther in their native language or in the foreign lan-
guage). The user may also redefine the objective to
be achieved during the interaction, and thus select
the type of vocabulary he wants to practice.
Virtual worlds (like Second Life) are being
rapidly incorporated into education, both initial
and superior (Doswell, 2005; Molka-Danielsen and
Deutschmann, 2009). The use of a virtual tutor
has certain advantages over a human tutor. Eng-
wall (2004) mentioned the following. (1) Amount
of practice: the chance to practice the new language
is essential for learning, and a virtual tutor provides
opportunities only limited by the technological re-
sources. (2) Prestige: a student may feel embar-
rassed about making mistakes with a human tutor,
and this might limit his willingness to speak in the
foreign language. (3) Augmented Reality: a virtual
tutor can provide additional material (e.g., examples
in context, explanatory images, etc.) with less effort
than a human tutor.
Such a virtual tutor can be used in distance learn-
ing. To develop distance learning systems, it is es-
sential to model the user?s learning progress. This
requires a system aware of the evolution of the user,
and that takes into account their achievements and
their problems. The system must be able to interpret
requirements, and generate appropriate responses,
135
for non-experts uses whose knowledge evolves dur-
ing the interaction. Moreover, the system must be
able to properly represent both the information con-
cerning the course material, and information about
the evolution of the user. For example, the system
must be able to diagnose what part of the course ma-
terial should be reviewed from the wrong answers of
the user. Finally, the system must be able to evaluate
the user interaction in order to decide which learning
objectives have been achieved. The theoretical and
practical results of the project contribute to solving
these difficult problems.
3 Impact of the Project
This project aims to achieve a balance between a
system which is sufficiently generic to be applica-
ble in different areas, and specific enough to ben-
efit from the efficient use of existing techniques
for knowledge management, planning and natural
language processing. Designing and implementing
such a system is a multidisciplinary effort leading to
research in diverse scientific areas:
Pragmatics is an interdisciplinary field which inte-
grates insights from linguistics (e.g., conversational
implicatures (Grice, 1975)), sociology (e.g., conver-
sational analysis (Schegloff, 1987)) and philosophy
(e.g., theory of speech acts (Austin, 1962)). It aims
to explore how the context (in which a conversation
is situated) contributes to the meaning (of everything
that is said during that conversation). The meaning
conveyed during a conversation depends not only on
linguistic information (entities in focus, grammati-
cal and morphological rules, etc.) but also on extra-
linguistic information (physical situation of conver-
sation, previous experiences of speakers, etc.). As a
result, the same sentence may mean different things
in different contexts. The area of pragmatics studies
the process by which a sentence is disambiguated
using its context. A dialogue system needs to have
pragmatic capabilities in order to interact in a nat-
ural way with its users. In particular, it must define
what kind of contextual information should be repre-
sented; and what inference tasks on a sentence and
context are necessary in order to interpret an utter-
ance. In such a system it is important that sentences
makes explicit the right amount of information: too
much information will delay and bore the user, but if
the information is not enough the user will not know
how to perform the task and make mistakes.
One of the major contributions of the project in
this area will be a virtual laboratory for pragmatic
theories: a controlled environment for studying in-
teraction set in a world where physical actions and
language intermingle. The prototype will let us in-
vestigate the impact that different instruction giving
policies (e.g., post order on the tree structure of the
task) have on successful achievement of the goal.
Similar studies have been done before (e.g., (Fos-
ter et al, 2009)) but they usually assume a prede-
termined task. Since our prototype allows for the
specification of the virtual world, the available ac-
tions, and the goal, we will be able to determine
when the impact associated to a particular policy is
dependent on the task or not. We will also investi-
gate short and long term repairs. Repairs are usu-
ally caused by conversational implicatures (Benotti,
2009a). Modeling these implicatures in a generic di-
alogue system is difficult because they are too open
ended. However, since the present prototype pro-
vides a situated interaction, restricted to the virtual
world, it will be possible to test the relationship be-
tween implicatures, the type of repairs they give rise
to, and the inference tasks needed to predict them.
Inference can be understood as any operation that
transforms implicit information in explicit informa-
tion. This definition is general enough to cover tasks
ranging from logical inference (i.e., deduction in
a formal language) to inference tasks common in
AI (e.g., planning and non-monotonic inference), as
well as statistical operations (e.g. obtaining estima-
tors on a data set). A dialogue system has to contin-
ually perform inference operations. E.g., inference
is needed to interpret information received from the
user, incorporate it to the system?s data repository,
and then decide what should be conveyed back to
the user. The very problem of deciding what kind
of logical representation and what type of inference
to use in a given situation is complex (propositional
logic vs. first-order logic, validity vs. model check-
ing, logical inference vs. statistical inference). Inde-
pendently of which type of inference is used, they
are usually computationally expensive. The chal-
lenge here is to find the appropriate balance between
the expressivity of the representation formalism and
136
the cost of the required inference methods.
The main contribution of the project in this area
is in the design, development and study of planning
algorithms. A typical planning system takes three
inputs ?initial state, possible actions and expected
goal? and returns a sequence of actions (a plan) that
when sequentially applied to the initial state, ends
in a state that satisfies the goal. Different methods
to obtain a plan have been studied (forward chain-
ing, backward chaining, coding in terms of proposi-
tional satisfiability, etc.), and they are currently im-
plemented in systems that can solve many planning
tasks efficiently. However, most of these systems
make assumptions that simplify the problem (deter-
ministic atomic time, complete information, absence
of a background theory, etc.). And most of them re-
turn a single plan. We will investigate algorithms
that eliminate some of these simplifications (in par-
ticular, we will study planning with incomplete in-
formation and based on a background theory). We
will also provide extended planning services: alter-
native plans, minimal plans, conditional plans, in-
complete plans, affordability of a given state, etc.
Evaluation of natural language generation systems
is one of the most difficult tasks in the area of NLP.
A given concept can be expressed in many different
ways, all of them correct. Hence, it is not possible
to determining the quality of a generated sentence
simply by, for example, comparing the result with a
gold standard. The problem of absence of gold stan-
dards is shared with another area of the NLP, namely
Machine Translation, for which various evaluation
methodologies, both direct and indirect, have been
proposed. Direct methods applies a metric to the
text generated by the system, while indirect meth-
ods evaluates the performance of the system through
the use of the generated text to perform some task.
But none of these methods is a standard and gener-
ally accepted methodology, which has been proven
to be effective in all cases. Since what is being eval-
uated in this project is a system that interacts via the
generation of natural language instructions, we can
determine its performance through quantitative met-
rics (e.g., average task completion time), qualitative
metrics (e.g., general user satisfaction) and metrics
based on the context (e.g., how well the system ad-
dressed the user needs in particular situations). We
will study the portability of evaluation techniques
from the domain of machine translation and multi-
modal human-computer interaction to the evaluation
of the system proposed in this project.
One of the main contributions of our project at
this respect is the integration of assessment tech-
niques from different areas into a methodology for
evaluating dialog systems for virtual environments,
aiming to estimate their usability and effectiveness.
This methodology could be used both to determine
whether a system is suitable for a task type and
user, and to compare the performance of different
systems of the same type. Another contribution
will be the study and application of software eval-
uation standards to the developed systems, creat-
ing a standardized quality model and proposing a
set of appropriate metrics to assess each of the as-
pects of the model. Finally, the annotated corpus
of human-human interaction, together with the cor-
pus of human-machine interaction collected during
the project will be made public. Such corpora will
serve, for example, to design more general platforms
for evaluating dialog systems, going beyond the as-
pects evaluated by existing platforms like GIVE.
Impact in the Argentinean Landscape: Natural
language processing, and in particular the field of
dialogue systems is a rapidly growing area in devel-
oped countries. The automatic processing of natu-
ral language has become a strategic capability for
companies and the wider community. However, this
area is extremely underdeveloped in Argentina. This
can be attributed to several factors. (a) The relative
youth of the area of NLP, which implies a relative
dearth of trained professionals throughout the world.
(b) The underdevelopment of the area of research in
Artificial Intelligence and Formal Linguistics in Ar-
gentina, for historical reasons and lack of industry
demand. (c) Poor interaction between the few re-
searchers in NLP that are in the region.
NLP is a strategic research area for Argentina
which can achieve academic excellence and indus-
try relevance. We believe in supporting the devel-
opment of this area by promoting the following.
(a) Training of human resources through doctoral
programs and courses taught in Argentina by in-
ternationally renowned professionals. (b) Incorpo-
ration of trained human resources to contribute to
137
the growth and diversification of the critical mass in
the area. (c) Improving interaction between various
groups and individual researchers in NLP, through
the organization of workshops, courses, visits, co-
tutoring, coordinated specialization programs, etc.
The particular topics investigated in the frame-
work of this project are of relevance in the current
Argentinean landscape for at least two reasons. On
the one hand, the project integrates and develops var-
ious key aspects of the area of computational lin-
guistics (syntax, semantics, pragmatics, representa-
tion, inference, evaluation); an area which, as we
mentioned, is today almost nonexistent in Argentina.
This project will be a step towards reversing this sit-
uation. On the other hand, the ultimate goal of the
project is to investigate the use of the developed plat-
form for distance education (specifically, as a tool
for language learning). Distance education is a valu-
able resource to overcome the problem of centraliza-
tion of educational resources in the country.
4 Introducing the Research Group
The PLN5 research group, in which the describe
scientific project will be carried out, was funded
in 2005. Te group is developing an important
role in human resource training, delivering courses
to undergraduate and postgraduate student at the
Universidad de Co?rdoba and other universities. It
also works in the development of various research
projects and integration with other groups in the re-
gion, both within Argentina and with neighboring
countries (Chile, Brazil and Uruguay).
The current project pools together many of the
key areas of expertise of the members of the group.
To begin with, some members of the group special-
ize in computational logic, particularly in the theo-
retical and applied study of languages for knowledge
representation (e.g., modal, hybrid and description
logics). They have also developed automated theo-
rem provers for these languages6. In relation with
the study of knowledge representation, they have
also investigated and developed algorithms for gen-
erating referring expressions (Areces et al, 2008b).
The second line of research of the PLN group that
is relevant for this project is context-based evalua-
5http://www.cs.famaf.unc.edu.ar/?pln
6http://www.glyc.dc.uba.ar/intohylo/
tion. Members of the group have proposed an eval-
uation model for machine translation systems which
relates the context of use to potentially important
quality characteristics (Estrella et al, 2008; Estrella
et al, 2009). This model is general enough to be
applied to other systems that produce natural lan-
guage like the ones proposed in this paper. Thanks to
the background on machine translation systems the
team has experience evaluating and comparing natu-
ral language output produced in different languages
(Spanish and English in particular), which will be
relevant for the development of the language tutor
described in Section 2.4. Finally, the team has ex-
perience developing and evaluating multimodal cor-
pora like those described in Section 2 (Estrella and
Popescu-Belis, 2008).
The third line of research that is relevant for this
project is pragmatics. In this area the team has im-
plemented a conversational agent which is able to
infer and negotiate conversational implicatures us-
ing inference tasks such as classical planning and
planning under incomplete information (Benotti,
2009b). We have also investigated how to infer
conversational implicatures triggered by compara-
tive utterances (Benotti and Traum, 2009). Recently
we have done corpus-based work, which shows what
kinds of implicatures are inferred and negotiated by
human dialogue participants during a task situated
in a 3D virtual environment (Benotti, 2009a).
Other lines of research in the PLN group are not
directly related to the project at this stage, but might
become relevant in the future. They include gram-
mar induction, text mining, statistical syntactic anal-
ysis and ontology population from raw text.
5 Ongoing and Future Collaborations
The members of the PLN in general and the authors
of this paper in particular have several collaborations
with national and international research groups in
computational linguistics and related fields that are
relevant for this project.
At the international level, we have ongoing col-
laboration with the TIM/ISSCO7 Multilingual In-
formation Processing Department at the University
of Geneva, with the Idiap Research Institute8 and
7http://www.issco.unige.ch/en
8http://www.idiap.ch
138
with some members of the PAI9, Pervasive Artifi-
cial Intelligence group of the University of Fribourg.
These collaborations include the evaluation of NLP
systems and the development of multilingual and
multimodal human language technology systems.
Members of the group have a long standing col-
laboration with the TALARIS10 group of the Labo-
ratoire Lorrain de Recherche en Informatique et ses
Applications (LORIA). The main research topic at
TALARIS is computational linguistics with strong
emphasis on semantics and inference. In the frame-
work of this collaboration we are participating in the
2010 edition of the GIVE Challenge. In the pro-
cess of designing the systems that will participate in
the challenge we jointly investigated the use of dif-
ferent referring strategies in situated instruction giv-
ing (Amoia et al, 2010).
We have also collaborated with the Virtual Hu-
mans group of the Institute for Creative Technolo-
gies11 from the University of Southern California.
In particular we computationally modeled the in-
ference of conversational implicatures triggered by
comparative utterances (Benotti and Traum, 2009).
The Institute for Creative Technologies offers In-
ternship programs every year that we plan to use in
order to strengthen our collaboration.
All these collaborations are directly related to the
main theme of the project described in this arti-
cle. The PLN group has also research collaborations
with other international research teams in the frame-
work of other scientific programs. For example, the
PLN group has being part of a recently finished in-
ternational project MICROBIO12 on ontology popu-
lation from raw text. The project was funded by the
Stic-Amsud13 program, a scientific-technological
cooperation program integrated by France, Argen-
tine, Brazil, Chile, Paraguay, Peru and Uruguay. The
expertise obtained during this project might be use-
ful in the future when trying to extend our GIVE on-
tologies to new domains. Similarly, the team main-
tain scientific relations with the University of Texas
at Austin (mainly with Dr. J. Moore in projects re-
9http://diuf.unifr.ch/pai/wiki
10http://talaris.loria.fr
11http://ict.usc.edu/projects/virtual_humans
12http://www.microbioamsud.net
13http://www.sticamsud.org
lated to the development of the ACL214 prover); and
with the Research team Symbiose15 of the Institut de
Recherche en Informatique et Syste?mes Ale?atoires
(working on the use of linguistic techniques for the
modelisation of genomic sequences).
At the national level, the group has inten-
sively collaborated with GLyC16, Grupo de Lo?gica,
Lenguaje y Computabilidad on knowledge represen-
tation and inference (see, e.g. (Areces and Gor??n,
2005; Areces et al, 2008a)). GLyC is part of the
Computer Science Department of the Universidad
de Buenos Aires. During 2010, teams PLN and
GLyC will join forces and collaborate in the organi-
zation of ELiC17, the First School in Computational
Linguistics in Argentina, which will take place in
July at the Universidad de Buenos Aires. ELiC 2010
will be co-located with the ECI18, Escuela de Cien-
cias Informa?ticas which has a long standing repu-
tation as a high-quality winter school in Computer
Science in Argentina, and is being organized yearly
since 1987. With ELiC we aim at creating, for the
first time, a space to introduce the field of computa-
tional linguistics to graduate students in Argentina.
Thanks to the support of the North American Chap-
ter of the Association for Computational Linguis-
tics (NAACL) and of the Universidad de Buenos
Aires, ELiC is offering student travel grants and fee
waivers to encourage participation.
The PLN group is also contacting other groups
working in computational linguistics in Argentina
like the research group in Artificial Intelligence from
the Universidad Nacional del Comahue19. Taking
advantage of previous co-participation in different
project we plan to organize exchange programs in
the framework of a research network.
Finally, the PLN group is planning to orga-
nize a workshop on Computational Linguistics as
a satellite event of IBERAMIA 201020, the Ibero-
American Conference on Artificial Intelligence, that
will be organized by the Universidad del Sur, in the
city of Bah??a Blanca, Argentina.
14http://www.cs.utexas.edu/users/moore/acl2
15http://www.irisa.fr/symbiose
16http://www.glyc.dc.uba.ar
17http://www.glyc.dc.uba.ar/elic2010
18http://www.dc.uba.ar/events/eci/2009/eci2009
19http://www.uncoma.edu.ar/
20http://cs.uns.edu.ar/iberamia2010
139
References
M. Amoia, A. Denis, L. Benotti, and C. Gardent. 2010.
Evaluating referring strategies in situated instruction
giving. Topics in Cognitive Science. Submitted.
C. Areces and D. Gor??n. 2005. Ordered resolution with
selection for H(@). In F. Baader and A. Voronkov,
editors, Proc. of LPAR 2004, volume 3452 of LNCS,
pages 125?141. Springer.
C. Areces, D. Figueira, S. Figueira, and S. Mera. 2008a.
Expressive power and decidability for memory log-
ics. In Logic, Language, Information and Computa-
tion, volume 5110 of LNCSs, pages 56?68. Springer.
C. Areces, A. Koller, and K. Striegnitz. 2008b. Referring
expressions as formulas of description logic. In Proc.
of INLG-08.
J. Austin. 1962. How to do Things with Words. Oxford
University Press.
L. Benotti and D. Traum. 2009. A computational ac-
count of comparative implicatures for a spoken dia-
logue agent. In Proc. of IWCS-8.
L. Benotti. 2009a. Clarification potential of instructions.
In SIGDIAL-09.
L. Benotti. 2009b. Frolog: An accommodating text-
adventure game. In Proc. of EACL-09.
N. Blaylock. 2005. Towards tractable agent-based dia-
logue. Ph.D. thesis, University of Rochester, Depart-
ment of Computer Science.
D. Byron, A. Koller, K. Striegnitz, J. Cassell, R. Dale,
J. Moore, and J. Oberlander. 2009. Report on the 1st
GIVE challenge. In Proc. of ENLG, pages 165?173.
N. Dahlba?ck, A. Jo?nsson, and L. Ahrenberg. 1998. Wiz-
ard of Oz studies?why and how. In Readings in intel-
ligent user interfaces, pages 610?619. Morgan Kauf-
mann Publishers Inc.
J. Doswell. 2005. It?s virtually pedagogical: pedagogi-
cal agents in mixed reality learning environments. In
Proc. of SIGGRAPH-05, page 25. ACM.
O. Engwall, P. Wik, J. Beskow, and G. Granstro?m. 2004.
Design strategies for a virtual language tutor. In
S. Kim and D. Young, editors, Proc. of ICSLP-04, vol-
ume 3, pages 1693?1696.
P. Estrella and A. Popescu-Belis. 2008. Multi-eval: an
evaluation framework for multimodal dialogue anno-
tations. Poster at the Joint IM2 and ASSI.
P. Estrella, A. Popescu-Belis, and N. Underwood. 2005.
Finding the system that suits you best: Towards the
normalization of MT evaluation. In Proc. of ASLIB-
05, pages 23?34.
P. Estrella, A. Popescu-Belis, and M. King. 2008. Im-
proving contextual quality models for MT evaluation
based on evaluators? feedback. In Proc. of LREC-08.
P. Estrella, A. Popescu-Belis, and M. King. 2009. The
femti guidelines for contextual mt evaluation: princi-
ples and tools. In W. Daelemans and V. Hoste, ed-
itors, Evaluation of Translation Technology. Linguis-
tica Antverpiensia.
M. Foster, M. Giuliani, A. Isard, C. Matheson, J. Ober-
lander, and A. Knoll. 2009. Evaluating description
and reference strategies in a cooperative human-robot
dialogue system. In Proc. of IJCAI-09.
P. Grice. 1975. Logic and conversation. In P. Cole
and J. Morgan, editors, Syntax and Semantics: Vol. 3:
Speech Acts, pages 41?58. Academic Press.
M. Hajdinjak and F. Mihelic. 2006. The paradise evalu-
ation framework: Issues and findings. Computational
Linguistics, 32(2):263?272.
J. Hoffmann and B. Nebel. 2001. The FF planning
system: Fast plan generation through heuristic search.
JAIR, 14:253?302.
ISO/IEC. 1999. 14598-1:1999 (E) ? Information Tech-
nology ? Software Product Evaluation ? Part 1: Gen-
eral Overview.
ISO/IEC. 2001. 9126-1:2001 (E) ? Software Engineer-
ing ? Product Quality ? Part 1:Quality Model.
H. Kautz and B. Selman. 1999. Unifying SAT-based
and graph-based planning. In Proc of the IJCAI, pages
318?325.
R. Lecoeuche, C. Mellish, and D. Robertson. 1998. A
framework for requirements elicitation through mixed-
initiative dialogue. In Proc. ICRE-98. IEEE.
D. Litman and S. Pan. 2002. Designing and evaluating
an adaptive spoken dialogue system. User Modeling
and User-Adapted Interaction, 12(2-3):111?137.
J. Molka-Danielsen and M. Deutschmann, editors. 2009.
Learning and Teaching in the Virtual World of Second
Life. Tapir Academic Press.
D. Nau, M. Ghallab, and P. Traverso. 2004. Automated
Planning: Theory & Practice. Morgan Kaufmann
Publishers Inc.
M. Purver. 2006. CLARIE: Handling clarification re-
quests in a dialogue system. Research on Language
and Computation, 4(2-3):259?288.
E. Schegloff. 1987. Some sources of misunderstanding
in talk-in-interaction. Linguistics, 8:201?218.
P. Wik and A. Hjalmarsson. 2009. Embodied conversa-
tional agents in computer assisted language learning.
Speech Commun., 51(10):1024?1037.
140
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 52?58,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
 Semantic Textual Similarity for MT evaluation  Julio Castillo??               Paula Estrella? ?FaMAF, UNC, Argentina ??UTN-FRC, Argentina jotacastillo@gmail.com  pestrella@famaf.unc.edu.ar      Abstract 
This paper describes the system used for our participation in the WMT12 Machine Transla-tion evaluation shared task.  We also present a new approach to Machine Translation evaluation based on the recently defined task Semantic Textual Similarity. This problem is addressed using a textual entail-ment engine entirely based on WordNet se-mantic features. We described results for the Spanish-English, Czech-English and German-English language pairs according to our submission on the Eight Workshop on Statistical Machine Translation. Our first experiments reports a competitive score to system level. 1 Introduction The evaluation of Machine Translation (MT) has become as important as MT itself over the last few years. This is evidenced by the fact that there are now specific forums to present and test new met-rics, such as the Workshop for Statistical MT (WMT) or the NIST MetricsMatr. Every year a vast number of MT metrics are created, the majori-ty being automatic, and seeking to find an efficient, low labor-intensive and reliable evaluation method as an alternative to human-based evaluation.  Automatic metrics employ different evaluation strategies: classical MT automatic metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington. 2002), WER (Tillmann et al, 1997), PER (Nie?en et al, 2000) are language-independent based on n-gram matching (considering or not the ordering of words in a sentence); other use some kind of lan-guage-specific knowledge, for example METEOR (Banerjee et al, 2005), which uses WordNet to 
match synonyms if exact matchings do not occur, and METEOR-NEXT (Denkowski et al, 2010) that, in addition to METEOR?s features, incorpo-rates paraphrases; and more sophisticated metrics use deeper linguistic information, as for example the DCU-LFG metric (Yifan et al, 2010).  However, relatively few attempts have been made to use semantic information for MT evalua-tion. Moreover, only one work has been published about using semantic equivalence (known as Tex-tual Entailment) of texts for MT evaluation. In this work we propose an improved metric, based on TE features, that indicates to what extent a candidate sentence is equivalent to a reference. The paper is organized as follows: Section 2 de-scribes the relevant work done on semantic orient-ed MT evaluation, Section 3 describes the architecture of the system to compute our metric, then Section 4 relates TE and semantic textual similarity to MT, and Section 5 presents some results obtained with our TE-based metric; and finally Section 6 summarize some conclusions and future work. 2 Related work  Given the vast literature in the field of MT evalua-tion, in this section we briefly mention a few at-tempts to evaluate MT based on semantic features, which we deem most recent and important. 2.1 Semantics for MT evaluation Gim?nez and M?rquez (2007) present a set of met-rics operating over shallow semantic structures, which they call linguistic elements, with the idea that a sentence can be seen as a ?bag? of LEs. Pos-sible LEs are word forms, part-of-speech tags, dependency relationships, syntactic phrases, named 
52
entities, semantic roles, etc. The metrics calculate the similarity of a candidate to one or more refer-ences by calculating the overlap and matches of LEs, and the resulting score is the highest obtained from the individual comparisons to each reference. The shallow-semantic evaluation is performed by computing the matching and overlap of named entities and semantic roles, after automatically annotating the sentences.  Following this work, Gim?nez and M?rquez (2009) propose the family of metrics discourse representation structure (DRS) based on the Dis-course Representation Theory of Kamp (1981), where a discourse is represented in structure that is essentially a variation of first-order predicate cal-culus. These sets of metrics are then used to evalu-ate poor quality MT, concluding that semantic oriented metrics are more stable at the system lev-el, while at the sentence level their performance decreases (probably due to external factors, for example if a parse tree of the sentence is not avail-able, the metric cannot be computed). More recently, Lo and Wu (2011) present a new semi-automated metric, MEANT, that assesses translation utility by matching semantic role fillers. Their hypothesis is that a good translation is one that lets a reader get the central information of the sentence. Conceptually, MEANT is defined in terms of f-score, calculated by averaging the trans-lation accuracy for all frames in the MT output across the number of frames in the MT out-put/reference translations. To determine the trans-lation accuracy for each semantic role filler in the reference and machine translations, they ask hu-mans to indicate if a role filler translation is cor-rect, incorrect or partially correct, hence being a semi-automatic metric. According to Lo and Wu (2011) MEANT can be run using inexpensive un-trained monolingual human judges and yet it corre-lates with human judgments on adequacy as well as other labor-intensive metrics, such as HTER (Snover et al, 2006), which needs to train humans to find the closest right translation.  2.2 Textual Entailment in MT  Textual Entailment (TE) is defined as a generic framework for applied semantic inference, where the core task is to determine whether the meaning of a target textual assertion (hypothesis, H) can be inferred from a given text (T). For example, given the pair (H,T): 
H: The Tunisian embassy in Switzerland was at-tacked T: Fire bombs were thrown at the Tunisian embas-sy in Bern we can conclude that T entails H.  The recently created challenge ?Recognising Textual Entailment? (RTE) started in 2005 with goal of providing a binary answer for each pair (H,T), namely whether there is entailment holds or not (Dagan et al, 2006). The RTE challenge has mutated over the years, aiming at accomplishing more accurate and specific solutions; for example, 2008 a three-way decision was proposed (instead of the original binary decision) consisting of ?en-tailment?, ?contradiction? and ?unknown?; in 2009 the organizers proposed a pilot task, the Textual Entailment Search (Bentivogli et al, 2009), con-sisting in finding all the sentences in a set of doc-uments that entail a given Hypothesis and since 2010 there is a Novelty Detection Task, which means that RTE systems are required to judge whether the information contained in each H is novel with respect to (i.e., not entailed by) the in-formation contained in the corpus. This task is quite close to the goal of MT and MT evaluation given that a correct translation should be semantically equivalent to its reference, and thus both translations should entail each other. Despite this close relation, at present there are only two works using TE in MT, namely Mirkin et al (2009) proposes to handle OOV(Out-of-vocabulary words) terms by generating alternative source sentences for translation but instead of simply using paraphrases they use entailed texts; the other contribution is by Aziz et al (2010), in which TE features are integrated into standard SMT workflow (i.e. they dynamically generate alternative entailed words to replace OOVs). More directly related to our work, is that of Pad? et al, (2009) that uses TE to evaluate MT. The main idea is to find out if the translation para-phrases (entails) the reference using entailment features. This is implementing by checking for entailment both from the candidate to the reference and from the reference to the candidate; best can-didates are thus assumed to be those that both en-tail and are entailed by the references and worst candidates are assumed to be those that neither entail the references nor are entailed by these ref-erences. Pad? et al (2009a) found that entailment-
53
based features extracted from partially ill-formed translations are sufficiently robust to be predictive for translation quality. Our approach differs from that of Pad? et al (2009) in that we do not have a binary entailment relation; instead we try to state in a scale of 0 ? 5 the degree of similarity between a candidate and a reference. This approach has very recently been proposed as a new task of the Semantic Evaluation Exercises 2012, called Semantic Textual Similarity (STS) by Aguirre et al (2012) and is explained in more detail in Section 4.  3 System architecture  Sagan is a RTE textual entailment system which has taken part of several challenges, including the Textual Analysis Conference 2009 and TAC 2010, and the Semantic Textual Similarity (Castillo and Estrella, 2012) and Cross Lingual Textual Entail-ment for content synchronization (Castillo and Cardenas, 2012) as part of the *SEM 2012 Task8 (Negri et al, 2012). The system is based on a machine learning ap-proach for STS. We adapted this system to produce feature vectors for all MT outputs for all language pairs ES-EN, DE-EN, FR-EN and CS-EN. It is worth noting that we work on all pairs into English because the system was run in a  monolingual set-ting to take advantage of all the resources available for EN. This Semantic Textual Similarity engine utilizes eight WordNet-based similarity measures, as ex-plained in (Castillo, 2011), with the purpose of obtaining the maximum similarity between two concepts. These text-to-text similarity measures are based on the following word-to-word similarity metrics: (Resnik, 1995), (Lin, 1997), (Jiang and Conrath, 1997), (Pirr? and Seco, 2008), (Wu & Palmer, 1994), Path Metric, (Leacock & Chodor-ow, 1998), and a semantic similarity to sentence level named SemSim (Castillo and Cardenas, 2010).  Additional information about how to produce feature vector and metric to word and sentence level can be found in (Castillo, 2011). The output of the system as modified for this workshop, is a similarity score between 5 and 0, where 5 means a perfect semantic similarity (ap-plied to MT it means that a candidate is indeed a good translation) and 0 means that there is no se-
mantic similarity between the pair, i.e. in MT terms, the candidate is not a translation. The architecture of the system is shown in Fig-ure 1.  
Pre-Processing
Result
Similarity Score
Testset: 
Lenguage 
Pairs XX->EN
Word Level Semantic Metrics
Feature Extraction
SVM with 
Regression
Training Set:  
MSRPC_STS
RUN 1 
Normalizer Stemming Parser
Resnik SemSimW&PLin ...
Gold 
Reference-
EN
MLP
Sentence Level Semantic Metric
 Fig.1. STS system architecture for MT evaluation  The system computes the semantic similarity of two texts (T,H) as a function of the semantic simi-larity of the constituent words of both phrases. A graph matching algorithm is used to determine the overall similarity between two text fragments. As a result, a text to text similarity measure is built based on word to word similarity. It is as-sumed that combining word to word similarity metrics to text level would be a good indicator of text to text similarity. 4 Sagan for MT evaluation Sagan for MT evaluation is based on a core devel-opment to approach the Semantic Textual Similari-ty task(STS). The pilot task STS was recently defined in Semeval 2012 (Aguirre et al, 2012) and has as main objective measuring the degree of semantic equivalence between two text fragments. STS is related to both Recognizing Textual En-tailment (RTE) and Paraphrase Recognition, but 
54
has the advantage of being a more suitable model for multiple NLP applications.  As mentioned before, the goal of the RTE task (Bentivogli et al, 2009) is determining whether the meaning of a hypothesis H can be inferred from a text T. Thus, TE is a directional task and we say that T entails H, if a person reading T would infer that H is most likely true.  The difference with STS is that STS consists in determining how similar two text fragments are, in a range from 5 (total semantic equivalence) to 0 (no relation). Thus, STS mainly differs from TE and Paraphrasing in that the classification is graded instead of binary. In this manner, STS is filling the gap between TE and Paraphrase. In view of this, our claim is that the output of MT systems will be more strongly correlated with humans if we have a higher STS score between MT system output and the reference translation.  To apply Sagan to MT evaluation, we first, pre-process the pairs from Microsoft Research Para-phrase Corpus (Dolan and Brockett, 2005) with dates and time normalization, and then optional modules are applied depending on the metric we want to calculate. Second, we compute 8 sentence level semantic features, and, finally, for every segment generated by systems participating at WMT 2012, we determine the semantic similarity score between that output and the given reference translation. The scores are then normalized to a value in the range 0 ? 1. 5 Experiments and results For the WMT 2012 we participated in the Czech-English and Spanish-English evaluation task but we did not have enough time to extensively test our metric on a diverse range of settings (i.e. dif-ferent corpora and language pairs), given that it was developed for the STS task, which released the data and results only a couple of months ago. However, we are now running experiments to get a better picture of the metric's ability to rate translation quality. In this section we report results obtained by training the system on the WMT 2011 data and testing on the news test portion, only for the Spanish-English pair. Although the system handles both SVM with regression and MLP clas-sifiers, well known to have good performance on natural language applications, we only submit the results obtained using SVM with regression due to 
previous experiments that consistently showed higher accuracy using SVM instead of MLP. At the system level, we calculated the Spearman Rank Correlation Coefficient (?) to compare our metric's behavior with respect to the human based metric applied in WMT 2011. The result is ? = 0.96 indicating a strong positive correlation. More-over, we successfully reproduce the systems rank-ing given by humans regarding the best and worst systems.   System Id Human score Sagan score online-B 0.72 0.71 online-A 0.72 0.71 systran 0.66 0.7 koc 0.67 0.69 alacant 0.66 0.69 rbmt-1 0.63 0.69 rbmt-4 0.6 0.69 rbmt-3 0.61 0.69 uedin 0.51 0.68 rbmt-2 0.6 0.68 upm 0.5 0.68 rbmt-5 0.51 0.68 ufal-um 0.47 0.67 cu-zeman 0.16 0.59 hyderabad 0.17 0.58 Table 1.  Sagan's score for ES-EN WMT 2011 news test set.  When correlating our metric to other automatic metrics, we find that it better correlates with Mete-or-Rank and Adq (Denkowski and Lavie, 2011), Tesla-b (Dahlmaier et al, 2011) and MPF (Popo-vic, 2011), with a correlation coefficient of 0.96. On the other hand, the worst correlations are found against Tesla-f, F15 (Bicici and Yuret, 2011) and the TER baseline (Snover et al, 2006).  We also performed experiments to segment lev-el with the language pair ES-EN. We used the MSR_STS as training set and the newstest2011 from WMT 2011 as test set. MSR_STS1 is com-posed by 750 sentence pairs with a graded seman-tic relationship ranging from 5 (equivalence) to 0 (no-equivalence). As result, we obtained a Kendall-tau correlation coefficient of 0.29 to segment-level for translations                                                             1 http://www.cs.york.ac.uk/semeval-2012/task6/ 
55
into English. These preliminary results, although low, shows that STS and Textual Entailment could be used to address the problem of MT evaluation. Clearly, further improvements are needed and we suspect that higher score can be reached using bigger training data. We also remark the necessity of larger corpus of STS providing a graded score among sentences. At the segment level, we show in Table 2 some examples found by manually inspecting the results.  Example Number MT out-put Texts Sagan score 2397 Reference Adelaida, 4 years old, wants a doll or a bicycle, while her sister Isabel, 3 years old, would like a Barbie doll. 
0.95 
Online-A Adelaide, of 4 years, want a doll or a bicy-cle, while his sister Isabel, 3 years, would like a Barbie doll. 2417 Reference "I strongly rely on the Charter."  
0.18 
Online-A "Me I based mainly on the letter." 45 Reference But there is a snag in that.  
0.105 
Alacant However, there is a fly in the ointment. 1510 Reference Unfortunately, even Scarlett Johansson might struggle to raise China's subterranean regard for these city squads. 
0.5206 
cu-zeman Lamentablemente, until scarlett johans-son should fight to increase the ?nfimo respect of china for with these es-cuadrones the city. Table 2. Sagan's score for some illustrative ES-EN WMT 2011 example pairs showing the score between MT outputs and the reference translation. The example number 2397 shows a sentence that achieves a high score (0.95) but that has an 
agreement error (marked in bold), that prevented Sagan from assigning the highest score. Otherwise, the instance number 2417 has a score of 0.18 showing that Sagan correctly penalizes ill-formed or meaningless sentences. Similarly, the example number 45 has a very low score which quantifies the dissimilarity with the reference translation. Finally, the last example provided shows that the translation remains words in the original Span-ish language (marked in bold). This manual inspection will be complemented with a deeper study of the correlations at the sen-tence level. 6 Conclusions and future work In this paper we introduced a new metric for MT evaluation based on Semantic Textual Similarity computed over textual entailment features. The metric's goal is to provide an indicative score of the extent to which two texts (a candidate transla-tion and a reference) are equivalent. This goal is more complex than classical binary decisions in the field of TE and is a new approach to bring to-gether the knowledge from different areas that a similar ambitions. While promising results were found at the sys-tem level, the metric still needs to be tested on a diversity of settings and at the segment level; this is work in progress and results will be reported in due time. References  Jes?s Gim?nez and Llu?s M?rquez. 2007. Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems. In Proceedings of the ACL Workshop on Statistical Machine Translation, pages 256?264. Christoph Tillmann, Stephan Vogel, Hermann Ney, Arkaitz Zubiaga, and Hassan Sawaf. 1997. Acceler-ated DP Based Search For Statistical Translation. In Proceedings of the 5th European Conference on Speech Communication and Technology  (EUROSPEECH-97). Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguistics(ACL-02), pages 311?318. Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Hermann Ney. 2000. A Evaluation Tool for Machine Translation:Fast Evaluation for MT Research. In 
56
Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC-2000). G. Doddington. 2002. Automatic Evaluation of Machine Translation Quality using N-gram Co-occurrence Statistics.  In Proceedings of the 2nd International Conference on HLT, pp. 138?145, San Francisco, CA, USA. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments. In Pro-ceedings of the 43th ACL, pages 65?72. Michael Denkowski and Alon Lavie. 2010. METEOR-NEXT and the METEOR Paraphrase Tables: Im-proved Evaluation Support For Five Target Lan-guages. Proceedings of the ACL 2010 Joint Workshop on Statistical Machine Translation and Metrics MATR. He Yifan, Du Jinhua, Way Andy, and Van Josef . 2010. The DCU dependency-based metric in WMT-MetricsMATR 2010. In: WMT 2010 - Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, ACL, Uppsala, Sweden. Kamp H. 1981. A theory of truth and semantic repre-sentation. In Groenendijk, J., Janssen, T., & Stokhof, M. (Eds.), Formal methods in the study of language, No. 135, pp. 277?322. Mathematical Centre, Am-sterdam. Chi-kiu Lo and Dekai Wu. 2011. MEANT: inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles. 49th An-nual Meeting of the Association for Computational Linguistic (ACL-2011). Portland, Oregon, US. Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Anno-tation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-06), pages 223?231. Ido Dagan, Oren Glickman and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In Qui?onero-Candela, J.; Dagan, I.; Magnini, B.; d'Alch?-Buc, F. (Eds.) Machine Learn-ing Challenges. LNCS, Vol. 3944, pp. 177-190. Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido Dagan, Marc Dymetman, and Idan Szpektor. 2009. Source-language entailment modeling for translating unknown terms. ACL 2009. Vol. 2. Stroudsburg, PA, USA, 791-799. Wilker Aziz and Marc Dymetmany and Shachar Mirkin and Lucia Specia and Nicola Cancedda and Ido Da-gan. 2010. Learning an Expert from Human Annota-tions in Statistical Machine Translation: the Case of Out-of-VocabularyWords. In: Proceedings of the 14th annual meeting of the European Association for Machine Translation (EAMT), Saint-Rapha, France. 
Dahlmeier, Daniel  and  Liu, Chang  and  Ng, Hwee Tou. 2011.TESLA at WMT 2011: Translation Evalu-ation and Tunable Metric.In: Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational Linguistics, pages 78-84, Edinburgh, Scotland. S. Pado, D. Cer, M. Galley, D. Jurafsky and C. Man-ning. 2009. Measuring Machine Translation Quality as Semantic Equivalence: A Metric Based on Entail-ment Features. Journal of MT 23(2-3), 181-193.  S. Pado, M. Galley, D. Jurafsky and C. Manning. 2009a. Robust Machine Translation Evaluation with Entail-ment Features. Proceedings of ACL 2009. Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonzalez-Agirre.    2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity.    In Proceed-ings of the 6th International Workshop on Semantic    Evaluation (SemEval 2012), in conjunction with the First Joint    Conference on Lexical and Computa-tional Semantics (*SEM 2012). Bentivogli, Luisa, Dagan Ido, Dang Hoa, Giampiccolo, Danilo, Magnini Bernardo.2009.The Fifth PASCAL RTE Challenge. In: Proceedings of the TAC.  Castillo Julio. 2011. A WordNet-based semantic ap-proach to textual entailment and cross-lingual textu-al entailment. International Journal of Machine Learning and Cybernetics - Springer, Volume 2, Number 3. Estrella Paula, Popescu-Belis A. and King M. 2007. A New Method for the Study of Correlations between MT Evaluation Metrics and Some Surprising Results. In: Proceedings of TMI-07- 11th Conference on The-oretical and Methodological Issues in Machine Translation -, Skvvde, Sweden. Castillo Julio and Cardenas Marina. 2010. Using sen-tence semantic similarity based onWordNet in recog-nizing textual entailment. Iberamia 2010. In LNCS, vol 6433. Springer, Heidelberg, pp 366?375. Castillo Julio. 2010. A semantic oriented approach to textual entailment using WordNet-based measures. MICAI 2010. LNCS, vol 6437. Springer, Heidelberg, pp 44?55. Castillo Julio. 2010. Using machine translation systems to expand a corpus in textual entailment. In: Proceed-ings of the Icetal 2010. LNCS, vol 6233, pp 97?102. Resnik P. 1995. Information content to evaluate seman-tic similarity in a taxonomy. In: Proceedings of IJCAI 1995, pp 448?453 907 Lin D. 1997.An information-theoretic definition of simi-larity. In: Proceedings of Conference on Machine Learning, pp 296?304 909 Jiang J, Conrath D.1997. Semantic similarity based on corpus statistics and lexical taxonomy. In: Proceed-ings of theROCLINGX 911 Pirro G., Seco N. 2008. Design, implementation and evaluation of a new similarity metric combining fea-
57
ture and intrinsic information content. In: ODBASE 2008, Springer LNCS. Wu Z, Palmer M. 1994. Verb semantics and lexical selection. In: Proceedings of the 32nd ACL 916. Leacock C, Chodorow M. 1998. Combining local con-text and WordNet similarity for word sense identifi-cation. MIT Press, pp 265?283 919 Hirst G, St-Onge D . 1998. Lexical chains as represen-tations of context for the detection and correction of malapropisms. MIT Press, pp 305?332 922 Banerjee S, Pedersen T. 2002. An adapted lesk algo-rithm for word sense disambiguation using WordNet. In: Proceeding of CICLING-02 Castillo Julio and Estrella Paula. 2012.  SAGAN: An approach to Semantic Textual Similarity based on Textual Entailment. In Proceedings of the 6th Inter-national Workshop on Semantic    Evaluation (SemEval 2012), in conjunction with the First Joint    Conference on Lexical and Computational Semantics (*SEM 2012). Castillo Julio and Cardenas Marina. 2012. SAGAN: A Machine Translation Approach for Cross-Lingual Textual Entailment. In Proceedings of the 6th Inter-national Workshop on Semantic    Evaluation (SemEval 2012), in conjunction with the First Joint    Conference on Lexical and Computational Semantics (*SEM 2012).  Chris Callison-Burch, Philipp Koehn, Christof Monz, Omar Zaidan. 2011.Findings of the 2011Workshop on Statistical Machine Translation. WMT 2011. M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and D. Giampiccolo. 2012. Semeval-2012. Task 8: Cross-lingual Textual Entailment for Content Syn-chronization. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012). William B. Dolan and Chris Brockett.2005. Automati-cally Constructing a Corpus of Sentential Para-phrases. Third International Workshop on Paraphras-ing (IWP2005). Asia Federation of Natural Language Processing.    
   
58
