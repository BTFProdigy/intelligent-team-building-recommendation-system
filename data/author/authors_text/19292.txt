Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 586?591,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Cross-language and Cross-encyclopedia Article Linking Using
Mixed-language Topic Model and Hypernym Translation
Yu-Chun Wang
Department of CSIE
National Taiwan University
Taipei, Taiwan
d97023@csie.ntu.edu.tw
Chun-Kai Wu
Department of CSIE
National Tsinghua University
Hsinchu, Taiwan
s102065512@m102.
nthu.edu.tw
Richard Tzong-Han Tsai
?
Department of CSIE
National Central University
Chungli, Taiwan
thtsai@csie.ncu.edu.tw
Abstract
Creating cross-language article links
among different online encyclopedias is
now an important task in the unification
of multilingual knowledge bases. In this
paper, we propose a cross-language article
linking method using a mixed-language
topic model and hypernym translation
features based on an SVM model to link
English Wikipedia and Chinese Baidu
Baike, the most widely used Wiki-like
encyclopedia in China. To evaluate our
approach, we compile a data set from the
top 500 Baidu Baike articles and their
corresponding English Wiki articles. The
evaluation results show that our approach
achieves 80.95% in MRR and 87.46%
in recall. Our method does not heavily
depend on linguistic characteristics and
can be easily extended to generate cross-
language article links among different
online encyclopedias in other languages.
1 Introduction
Online encyclopedias are among the most fre-
quently used Internet services today. One of
the largest and best known online encyclopedias
is Wikipedia. Wikipedia has many language ver-
sions, and articles in one language contain hyper-
links to corresponding pages in other languages.
However, the coverage of different language ver-
sions of Wikipedia is very inconsistent. Table 1
shows the statistics of inter-language link pages
in the English and Chinese editions in February
2014. The total number of Chinese articles is
about one-quarter of English ones, and only 2.3%
of English articles have inter-language links to
their Chinese versions.
?
corresponding author
Articles Inter-language Links Ratio
zh 755,628 zh2en 486,086 64.3%
en 4,470,246 en2zh 106,729 2.3%
Table 1: Inter-Language Links in Wikipedia
However, there are alternatives to Wikipedia for
some languages. In China, for example Baidu
Baike and Hudong are the largest encyclopedia
sites, containing more than 6.2 and 7 million Chi-
nese articles respectively. Similarly, in Korea,
Naver Knowledge Encyclopedia has a large pres-
ence.
Since alternative encyclopedias like Baidu
Baike are larger (by article count) and growing
faster than the Chinese Wikipedia, it is worth-
while to investigate creating cross-language links
among different online encyclopedias. Several
works have focused on creating cross-language
links between Wikipedia language versions (Oh
et al, 2008; Sorg and Cimiano, 2008) or find-
ing a cross-language link for each entity mention
in a Wikipedia article, namely Cross-Language
Link Discovery (CLLD) (Tang et al, 2013; Mc-
Namee et al, 2011). These works were able to
exploit the link structure and metadata common
to all Wikipedia language versions. However,
when linking between different online encyclope-
dia platforms this is more difficult as many of these
structural features are different or not shared. To
date, little research has been done into linking be-
tween encyclopedias on different platforms.
Title translation is an effective and widely used
method of creating cross-language links between
encyclopedia articles. (Wang et al, 2012; Adafre
and de Rijke, 2005) However, title translation
alone is not always sufficient. In some cases, for
example, the titles of corresponding articles in dif-
ferent languages do not even match. Other meth-
ods must be used along with title translation to cre-
ate a more robust linking tool.
586
In this paper, we propose a method compris-
ing title and hypernym translation and mixed-
language topic model methods to select and link
related articles between the English Wikipedia and
Baidu Baike online encyclopedias. We also com-
pile a suitable dataset from the above two ency-
clopedias to evaluate the linking accuracy of our
method.
2 Method
Cross-language article linking between different
encyclopedias can be formulated as follows: For
each encyclopedia K, a collection of human-
written articles, can be defined as K = {a
i
}
n
i=1
,
where a
i
is an article in K and n is the size of
K. Article linking can then be defined as fol-
lows: Given two encyclopedia K
1
and K
2
, cross-
language article linking is the task of finding the
corresponding equivalent article a
j
from encyclo-
pedia K
2
for each article a
i
from encyclopedia
K
1
. Equivalent articles are articles that describe
the same topic in different languages.
Our approach to cross-language article linking
comprises two stages: candidate selection, which
produces a list of candidate articles, and candidate
ranking, which ranks that list.
2.1 Candidate Selection
Since knowledge bases (KB) may contain millions
of articles, comparison between all possible pairs
in two knowledge bases is time-consuming and
sometimes impractical. To avoid brute-force com-
parison, we first select plausible candidate articles
on which to focus our efforts. To extract possible
candidates, two similarity calculation methods are
carried out: title matching and title similarity.
2.1.1 Title Matching
In our title matching method, we formulate can-
didate selection as an English-Chinese cross-
language information retrieval (CLIR) problem
(Sch?onhofen et al, 2008), in which every English
article?s title is treated as a query and all the arti-
cles in the Chinese encyclopedia are treated as the
documents. We employ the two main CLIR meth-
ods: query translation and document translation.
In query translation, we translate the title of ev-
ery English article into Chinese and then use these
translated titles as queries to retrieve articles from
the Chinese encyclopedia. In document transla-
tion, we translate the contents of the entire Chinese
encyclopedia into English and then search them
using the original English titles. The top 100 re-
sults for the query-translation and the top 100 re-
sults for document-translation steps are unionized.
The resulting list contains our title-matching can-
didates.
For the query- and document-translation steps,
we use the Lucene search engine with similar-
ity scores calculated by the Okapi BM25 ranking
function (Beaulieu et al, 1997). We separate all
words in the translated and original English article
titles with the ?OR? operator before submission to
the search engine. For all E-C and C-E translation
tasks, we use Google Translate.
2.1.2 Title Similarity
In the title similarity method, every Chinese arti-
cle title is represented as a vector, and each dis-
tinct character in all these titles is a dimension of
all vectors. The title of each English article is
translated into Chinese and represented as a vec-
tor. Then, cosine similarity between this vector
and the vector of each Chinese title is measured as
title similarity.
2.2 Candidate Ranking
The second stage of our approach is to score
each viable candidate using a supervised learning
method, and then sort all candidates in order of
score from high to low as final output.
Each article x
i
in KB K
1
can be
represented by a feature vector x
i
=
(f
1
(x
i
), f
2
(x
i
), . . . , f
n
(x
i
)). Also, we have
y
j
= (f
1
(y
j
), f
2
(y
j
), . . . , f
n
(y
j
)) for a candidate
article y
j
in KB K
2
. Then, individual feature
functions F
k
(x
i
, y
j
) are based on the feature
properties of both article a
i
and a
j
. The top pre-
dicted corresponding article y
j
in the knowledge
base K
2
for an input article x
i
in K
1
should
receive a higher score than any other entity in
K
2
, a
m
? K
2
,m 6= j. We use the support
vector machine (SVM) approach to determine the
probability of each pair (x
i
,y
j
) being equivalent.
Our SVM model?s features are described below.
Title Matching and Title Similarity Feature
(Baseline)
We use the results of title matching and title sim-
ilarity from the candidate selection stage as two
features for the candidate ranking stage. The sim-
ilarity values generated by title matching and title
similarity are used directly as real value features
in the SVM model.
587
Mixed-language Topic Model Feature (MTM)
For a linked English-Chinese article pair, the dis-
tribution of words used in each usually shows
some convergence. The two semantically corre-
sponding articles often have many related terms,
which results in clusters of specific words. If two
articles do not describe the same topic, the distri-
bution of terms is often scattered. (Misra et al,
2008) Thus, the distribution of terms is good mea-
surement of article similarity.
Because the number of all possible words is too
large, we adopt a topic model to gather the words
into some latent topics. For this feature, we use
the Latent Dirichlet Allocation (LDA) (Blei et al,
2003). LDA can be seen as a typical probabilistic
approach to latent topic computation. Each topic
is represented by a distribution of words, and each
word has a probability score used to measure its
contribution to the topic. To train the LDA model,
the pair English and Chinese articles are concate-
nated into a single document. English and Chinese
terms are all regarded as terms of the same lan-
guage and the LDA topic model, namely mixed-
language topic model, generates both English and
Chinese terms for each latent topic. Then, for each
English article and Chinese candidate pair in test-
ing, the LDA model provides the distribution of
the latent topics. Next, we can use entropy to mea-
sure the distribution of topics. The entropy of the
estimated topic distribution of a related article is
expected to be lower than that of an unrelated ar-
ticle. We can calculate the entropy of the distribu-
tion as a value for SVM. The entropy is defined as
follows:
H = ?
T
?
j=1
~
?
dj
log
~
?
dj
where T is the number of latent topics, ?
dj
is the
topic distribution of a given topic j.
Hypernym Translation Feature (HT)
The first sentence of an encyclopedia article usu-
ally contains the title of the article. It may also
contain a hypernym that defines the category of
the article. For example, the first sentence of the
?iPad? article in the English Wikipedia begins,
?iPad is a line of tablet computers designed and
marketed by Apple Inc. . .? In this sentence, the
term ?tablet computers? is the hypernym of iPad.
These extracted hypernyms can be treated as arti-
cle categories. Therefore, articles containing the
same hypernym are likely to belong to the same
category.
In this study, we only carry out title hypernym
extraction on the first sentences of English articles
due to the looser syntactic structure of Chinese. To
generate dependency parse trees for the sentences,
we adopt the Stanford Dependency Parser. Then,
we manually designed seven patterns to extract hy-
pernyms from the parse tree structures. To demon-
strate this idea, let us take the English article ?The
Hunger Games? for example. The first sentence of
this article is ?The Hunger Games is a 2008 young
adult novel by American writer Suzanne Collins.?
Since article titles may be named entities or com-
pound nouns, the dependency parser may mislabel
them and thus output an incorrect parse tree. To
avoid this problem, we first replace all instances of
an article?s title in the first sentence with pronouns.
For example, the previous sentence is rewritten as
?It is a 2008 young adult novel by American writer
Suzanne Collins.? Then, the dependency parser
generates the following parse tree:
novel
It is a 2008 young adult collinsnsubj cop det num amod nn prep_by
suzanne writer Americannn nnamod
Next, we apply our predefined syntactic patterns
to extract the hypernym. (Hearst, 1992) If any pat-
tern matches the structure of the dependency parse
tree, the hypernym can be extracted. In the above
example, the following pattern is matched:
NN
It is NNnsubj cop nn
[target]
In this pattern, the rightmost leaf is the hyper-
nym target. Thus, we can extract the hypernym
?novel? from the previous example. The term
?novel? is the extracted hypernym of the English
article ?The Hunger Games?.
After extracting the hypernym of the English ar-
ticle, the hypernym is translated into Chinese. The
value of this feature in the SVM model is calcu-
lated as follows:
F
hypernym
(h) = log count(translated(h))
where h is the hypernym, translated(h) is the
Chinese translation of the term h.
English Title Occurrence Feature (ETO)
In a Baidu Baike article, the first sentence may
contain a parenthetical translation of the main ti-
tle. For example, the first sentence of the Chinese
588
article on San Francisco is ?????San Fran-
cisco????????????????????.
We regard the appearance of the English title in
the first sentence of a Baidu Baike article as a bi-
nary feature: If the English title appears in the first
sentence, the value of this feature is 1; otherwise,
the value is 0.
3 Evalutaion
3.1 Evaluation Dataset
In order to evaluate the performance of cross-
language article linking between English Wikiep-
dia and Chinese Baidu Baike, we compile
an English-Chinese evaluation dataset from
Wikipedia and Baidu Baike online encyclopedias.
First, our spider crawls the entire contents of En-
glish Wikipedia and Chinese Baidu Baike. Since
the two encyclopedias? article formats differ, we
copy the information in each article (title, content,
category, etc.) into a standardized XML structure.
In order to generate the gold standard evalua-
tion sets of correct English and Chinese article
pairs, we automatically collect English-Chinese
inter-language links from Wikipedia. For pairs
that have both English and Chinese articles, the
Chinese article title is regarded as the translation
of the English one. Next, we check if there is a
Chinese article in Baidu Baike with exactly the
same title as the one in Chinese Wikipedia. If
so, the corresponding English Wikipedia article
and the Baidu Baike article are paired in the gold
standard.
To evaluate the performance of our method on
linking different types of encyclopedia articles, we
compile a set containing the most popular articles.
We select the top 500 English-Chinese article pairs
with the highest page view counts in Baidu Baike.
This set represents the articles people in China are
most interested in.
Because our approach uses an SVM model, the
data set should be split into training and test sets.
For statistical generality, each data set is randomly
split 4:1 (training:test) 30 times. The final evalua-
tion results are calculated as the mean of the aver-
age of these 30 evaluation sets.
3.2 Evaluation Metrics
To measure the quality of cross-language entity
linking, we use the following three metrics. For
each English article queries, ten output Baidu
Baike candidates are generated in a ranked list. To
define the metrics, we use following notations: N
is the number of English query; r
i,j
is j-th correct
Chinese article for i-th English query; c
i,k
is k-th
candiate the system output for i-th English query.
Top-k Accuracy (ACC)
ACC measures the correctness of the first candi-
date in the candidate list. ACC = 1 means that all
top candidates are correctly linked (i.e. they match
one of the references), and ACC = 0 means that
none of the top candidates is correct.
ACC =
1
N
N
?
i=1
{
1 if ?r
i,j
: r
i,j
= c
i,k
0 otherwise
}
Mean Reciprocal Rank (MRR)
Traditional MRR measures any correct answer
produced by the system from among the candi-
dates. 1/MRR approximates the average rank of
the correct transliteration. An MRR closer to 1 im-
plies that the correct answer usually appears close
to the top of the n-best lists.
RR
i
=
{
min
j
1
j
if ?r
i,j
, c
i,k
: r
i,j
= c
i,k
0 otherwise
}
MRR =
1
N
?
N
i=1
RR
i
Recall
Recall is the fraction of the retrieved articles that
are relevant to the given query. Recall is used to
measure the performance of the candidate selec-
tion method. If the candidate selection method can
actually select the correct Chinese candidate, the
recall will be high.
Recall =
|relevant articles| ? |retrieved articles|
|relevant articles|
3.3 Evaluation Results
The overall results of our method achieves 80.95%
in MRR and 87.46% in recall. Figure 1 shows the
top-k ACC from the top 1 to 5. These results show
that our method is very effective in linking articles
in English Wikipedia to those in Baidu Baike.
In order to show the benefits of each feature
used in the SVM model, we conduct a experiment
to test the performance of different feature combi-
nations. Because title similarity of the articles is a
widely used method, we choose English and Chi-
nese title similarity as the baseline. Then, another
feature is added to each configuration until all the
features have been added. Table 2 shows the final
results of different feature combinations.
589
0.76	 ?
0.839	 ?
0.858	 ?
0.869	 ? 0.87	 ?
0.7	 ?
0.72	 ?
0.74	 ?
0.76	 ?
0.78	 ?
0.8	 ?
0.82	 ?
0.84	 ?
0.86	 ?
0.88	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
TopK	 ?
Figure 1: Top-k Accuracy
Level Configuration MRR
0 Baseline (BL) 0.6559
1
BL + MTM
?1
0.6967
?
BL + HT
?2
0.6975
?
BL + ETO
?3
0.6981
?
2
BL + MTM + HT 0.7703
?
BL + MTM + ETO 0.7558
?
BL + HT + ETO 0.7682
?
3 BL + MTM + HT + ETO 0.8095
?
?1
MTM: mix-language topic model
?2
HT: hypernym translation
?3
ETO: English title occurrence
?
This config. outperforms the best config. in last level with
statistically significant difference.
Table 2: MRRs of Feature Combinations
In the results, we can observe that mix-language
topic model, hypernym, and English title oc-
curence features all noticeably improve the perfor-
mance. Combining two of these three feature has
more improvement and the combination of all the
features achieves the best.
4 Discussion
Although our method can effectively generate
cross-language links with high accuracy, some
correct candidates are not ranked number one. Af-
ter examining the results, we can divide errors into
several categories:
The first kind of error is due to large literal dif-
ferences between the English and Chinese titles.
For example, for the English article ?Nero?, our
approach ranks the Chinese candidate ?????
(?King Nero?) as number one, instead of the cor-
rect answer ?????????????????
????? (the number two candidate). The title
of the correct Chinese article is the full name of
the Roman Emperor Nero (Nero Claudius Drusus
Germanicus). The false positive ????? is a his-
torical novel about the life of the Emperor Nero.
Because of the large difference in title lengths, the
value of the title similarity feature between the En-
glish article ?Nero? and the corresponding Chi-
nese article is low. Such length differences may
cause the SVM model to rank the correct answer
lower when the difference of other features are not
so significant because the contents of the Chinese
candidates are similar.
The second error type is caused by articles that
have duplicates in Baidu Baike. For example, for
the English article ?Jensen Ackles?, our approach
generates a link to the Chinese article ?Jensen?
in Baidu Baike. However, there is another Baidu
article ???????? (?Jensen Ackles?). These
two articles both describe the actor Jensen Ackles.
In this case, our approach still generates a correct
link, although it is not the one in the gold standard.
The third error type is translation errors. For ex-
ample, the English article ?Raccoon? is linked to
the Baidu article ??? (raccoon dog), though the
correct one is ???? (raccoon). The reason is that
Google Translate provides the translation ??? in-
stead of ????.
5 Conclusion
Cross-language article linking is the task of creat-
ing links between online encyclopedia articles in
different languages that describe the same content.
We propose a method based on article hypernym
and topic model to link English Wikipedia articles
to corresponding Chinese Baidu Baike articles.
Our method comprises two stages: candidate se-
lection and candidate ranking. We formulate can-
didate selection as a cross-language information
retrieval task based on the title similarity between
English and Chinese articles. In candidate rank-
ing, we employ several features of the articles in
our SVM model. To evaluate our method, we com-
pile a dataset from English Wikipedia and Baidu
Baike, containing the 500 most popular Baidu ar-
ticles. Evaluation results of our method show an
MRR of up to 80.95% and a recall of 87.46%. This
shows that our method is effective in generating
cross-language links between English Wikipedia
and Baidu Baike with high accuracy. Our method
does not heavily depend on linguistic characteris-
tics and can be easily extended to generate cross-
language article links among different encyclope-
dias in other languages.
590
References
Sisay Fissaha Adafre and Maarten de Rijke. 2005.
Discovering missing links in wikipedia. In Proceed-
ings of the 3rd international workshop on Link dis-
covery (LinkKDD ?05).
M. Beaulieu, M. Gatford, X. Huang, S. Robertson,
S. Walker, and P. Williams. 1997. Okapi at TREC-
5. In Proceedings of the fifth Text REtrieval Confer-
ence (TREC-5), pages 143?166.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3(4-5):993?1022.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics,
volume 2.
Paul McNamee, James Mayfield, Dawn Lawrie, Dou-
glas W Oard, and David S Doermann. 2011. Cross-
language entity linking. In Proceedings of Interna-
tional Joint Con-ference on Natural Language Pro-
cessing (IJCNLP), pages 255?263.
Hemant Misra, Olivier Cappe, and Franc?ois Yvon.
2008. Using lda to detect semantically incoherent
documents. In Proceedings of the Twelfth Confer-
ence on Computational Natural Language Learning
(CoNLL ?08).
Jong-Hoon Oh, Daisuke Kawahara, Kiyotaka Uchi-
moto, Jun?ichi Kazama, and Kentaro Torisawa.
2008. Enriching multilingual language re-
sources by discovering missing cross-language
links in wikipedia. In Proceedings of the 2008
IEEE/WIC/ACM International Conference on Web
Intelligence and Intelligent Agent Technology, vol-
ume 1, pages 322?328.
P`eter Sch?onhofen, Andr`as Bencz`ur, Istv`an B`?r`o, and
K`aroly Csalog`any. 2008. Cross-language retrieval
with wikipedia. Advances in Multilingual and
Multimodal Information Retrieval, Lecture Notes in
Computer Science, 5152:72?79.
Philipp Sorg and Philipp Cimiano. 2008. Enrich-
ing the crosslingual link structure of wikipedia-a
classification-based approach. In Proceedings of the
AAAI 2008 Workshop on Wikipedia and Artifical In-
telligence, pages 49?54.
Ling-Xiang Tang, In-Su Kang, Fuminori Kimura, Yi-
Hsun Lee, Andrew Trotman, Shlomo Geva, and Yue
Xu. 2013. Overview of the ntcir-10 cross-lingual
link discovery task. In Proceedings of the Tenth NT-
CIR Workshop Meeting.
Zhichun Wang, Juanzi Li, Zhigang Wang, and Jie Tang.
2012. Cross-lingual knowledge linking across wiki
knowledge bases. In Proceedings of the 21st in-
ternational conference on World Wide Web (WWW
?12).
591
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 57?60,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
English-Korean Named Entity Transliteration Using Substring
Alignment and Re-ranking Methods
Chun-Kai Wu? Yu-Chun Wang? Richard Tzong-Han Tsai?
?Department of Computer Science and Engineering,
Yuan Ze University, Taiwan
?Department of Computer Science and Information Engineering,
National Taiwan University, Taiwan
s983301@mail.yzu.edu.tw d97023@csie.ntu.edu.tw
thtsai@saturn.yzu.edu.tw
Abstract
In this paper, we describe our approach
to English-to-Korean transliteration task in
NEWS 2012. Our system mainly consists
of two components: an letter-to-phoneme
alignment with m2m-aligner,and translitera-
tion training model DirecTL-p. We construct
different parameter settings to train several
transliteration models. Then, we use two re-
ranking methods to select the best transliter-
ation among the prediction results from the
different models. One re-ranking method is
based on the co-occurrence of the translitera-
tion pair in the web corpora. The other one is
the JLIS-Reranking method which is based on
the features from the alignment results. Our
standard and non-standard runs achieves 0.398
and 0.458 in top-1 accuracy in the generation
task.
1 Introduction
Named entity translation is a key problem in many
NLP research fields such as machine translation,
cross-language information retrieval, and question
answering. Most name entity translation is based on
transliteration, which is a method to map phonemes
or graphemes from source language into target lan-
guage. Therefore, named entity transliteration sys-
tem is important for translation.
In the shared task, we focus on English-Korean
transliteration. We consider to transform the translit-
eration task into a sequential labeling problem. We
adopt m2m-aligner and DirecTL-p (Jiampojamarn et
al., 2010) to do substring mapping and translitera-
tion predicting, respectively. With this approach (Ji-
ampojamarn et al, 2010) achieved promising results
on NEWS 2010 transliteration tasks. In order to im-
prove the transliteration performance, we also apply
several ranking techniques to select the best Korean
transliteration.
This paper is organized as following. In section
2 we describe the main approach we use including
how we deal with the data, the alignment and train-
ing methods and our re-ranking techniques. In sec-
tion 3, we show and discuss our results on English-
Korean transliteration task. And finally the conclu-
sion is in section 4.
2 Our Approach
In this section, we describe our approach for
English-Korean transliteration which comprises the
following steps:
1. Pre-processing
2. Letter-to-phoneme alignment
3. DirecTL-p training
4. Re-ranking results
2.1 Pre-processing
Korean writing system, namely Hangul, is alphabet-
ical. However, unlike western writing system with
Latin alphabets, Korean alphabet is composed into
syllabic blocks. Each Korean syllabic block repre-
sent a syllable which has three components: initial
consonant, medial vowel and optionally final con-
sonant. Korean has 14 initial consonants, 10 medial
vowels, and 7 final consonants. For instance, the syl-
labic block ??? (sin) is composed with three letters:
57
a initial consonant ??? (s), a medial vowel ??? (i),
and a final consonant ??? (n).
For transliteration from English to Korean , we
have to break each Korean syllabic blocks into two
or three Korean letters. Then, we convert these Ko-
rean letters into Roman letters according to Revised
Romanization of Korean for convenient processing.
2.2 Letter-to-phoneme Alignment
After obtaining English and Romanized Korean
name entity pair, we generate the alignment between
each pair by using m2m-aligner.
Since English orthography might not reflect its ac-
tual phonological forms, it makes one-to-one char-
acter alignment between English and Korean not
practical.
Compared with traditional one-to-one alignment,
the m2m-aligner overcomes two problems: One is
double letters where two letters are mapped to one
phoneme. English may use several characters for
one phoneme which is presented in one letter in Ko-
rean, such as ?ch? to ??? and ?oo? to ???. How-
ever, one-to-one alignment only allows one letter to
be mapped to one phoneme, so it must have to add
an null phoneme to achieve one-to-one alignment.
It may interfere with the transliteration prediction
model.
The other problem is double phonemes problem
where one letter is mapped to two phonemes. For
example, the letter ?x? in the English name entity
?Texas? corresponds to two letters ??? and ???
in Korean. Besides, some English letters in the
word might not be pronounced, like ?k? in the En-
glish word ?knight?. We can eliminate this by pre-
processing the data to find out double phonemes and
merge them into single phoneme. Or we can add
an null letter to it, but this may also disturb the pre-
diction model. While performing alignments, m2m
aligner allows us to set up the maximum length sub-
string in source language (with the parameter x) and
in target language (with the parameter y). Thus,
when aligning, we set both parameter x and y to two
because we think there are at most 2 English letters
mapped to 2 Korean letters. To capture more double
phonemes, we also have another parameter set with
x = 1 and y = 2.
As mentioned in previous section, Korean syl-
labic block is composed of three or two letters. In
order to cover more possible alignments, we con-
struct another alignment configurations to take null
consonant into consideration. Consequently, for any
Korean syllabic block containing two Korean letters
will be converted into three Roman letters with the
third one being a predefined Roman letter represent-
ing null consonant. We also have two set of param-
eters for this change, that is x = 2, y = 3 and x = 1
,y = 3. The reason we increase both y by one is that
there are three Korean letters for each word.
2.3 DirecTL-p Training
With aligned English-Korean pairs, we can train
our transliteration model. We apply DirecTL-p (Ji-
ampojamarn et al, 2008) for our training and testing
task. We train the transliteration models with differ-
ent alignment parameter settings individually men-
tioned in section 2.2.
2.4 Re-ranking Results
Because we train several transliteration models with
different alignment parameters, we have to combine
the results from different models. Therefore, the
re-ranking method is necessary to select the best
transliteration result. For re-ranking, we propose
two approaches.
1. Web-based re-ranking
2. JLIS-Reranking
2.4.1 Web-based re-ranking
The first re-ranking method is based on the oc-
currence of transliterations in the web corpora. We
send each English-Korean transliteration pair gen-
erated by our transliteration models to Google web
search engine to get the co-occurrence count of the
pair in the retrieval results. But the result number
may vary a lot, most of them will get millions of
results while some will only get a few hundred.
2.4.2 JLIS-Reranking
In addition to web-based re-ranking approach, we
also adopt JLIS-Reranking (Chang et al, 2010) to
re-rank our results for the standard run. For an
English-Korean transliteration pair, we can mea-
sure if they are actual transliteration of each other
by observing the alignment between them. Since
58
Table 1: Results on development data.
Run Accuracy Mean F-score MRR MAPref
1 (x = 2, y = 2) 0.488 0.727 0.488 0.488
2 (x = 1, y = 2) 0.494 0.730 0.494 0.494
3 (x = 1, y = 3, with null consonant) 0.452 0.713 0.452 0.452
4 (x = 2, y = 3, with null consonant) 0.474 0.720 0.474 0.473
Web-based Reranking 0.536 0.754 0.563 0.536
JLIS-Reranking 0.500 0.737 0.500 0.500
Table 2: Results on test data
Run Accuracy Mean F-score MRR MAPref
Standard (JLIS-Reranking) 0.398 0.731 0.398 0.397
Non-standard (Web-based reranking) 0.458 0.757 0.484 0.458
DirecTL-p model outputs a file containing the align-
ment of each result, there are some features in the
results that we can use for re-ranking. In our re-
ranking approach, there are three features used in
the process: source grapheme chain feature, target
grapheme chain feature and syllable consistent fea-
ture. These three feature are proposed in (Song et
al., 2010).
Source grapheme chain feature: This feature
can tell us that how the source characters are aligned.
Take ?A|D|A|M? for example, we will get three
chains which are A|D, D|A and A|M. With this fea-
ture we may know the alignment in the source lan-
guage.
Target grapheme chain feature: Similar to the
above feature, it tell us how the target characters are
aligned. Take ?NG:A:n|D|A|M? for example, which
is the Korean transliteration of ADAM, we will get
three chains which are n|D, D|A and A|M. With this
feature we may know the alignment in the target lan-
guage. ?n? is the predefined null consonant.
Syllable consistent feature: We use this feature
to measure syllable counts in both English and Ko-
rean. For English, we apply an Perl module1 to mea-
sure the syllable counts. And for Korean, we simply
count the number of syllabic blocks. This feature
may guard our results, since a wrong prediction may
not have the same number of syllable.
1http://search.cpan.org/?gregfast/
Lingua-EN-Syllable-0.251/Syllable.pm
Other than the feature vectors created by above
features, there is one important field when training
the re-ranker, performance measure. For this field,
we give it 1 when we predict a correct result other-
wise we give it 0 since we think it is useless to get a
partially correct result.
3 Result
To measure the transliteration models with different
alignment parameters and the re-ranking methods,
we construct several runs for experiments as follows.
? Run 1: m2m-aligner with parameters x = 2
and y = 2.
? Run 2: m2m-aligner with parameters x = 1
and y = 2.
? Run 3: m2m-aligner with parameters x = 1
and y = 3 and add null consonants in the Ko-
rean romanized representation.
? Run 4: m2m-aligner with parameters x = 2
and y = 3 and add null consonants in the Ko-
rean romanized representation.
? Web-based reranking: re-rank the results from
run 1 to 4 based on Google search results.
? JLIS-Reranking: re-rank the results from run 1
to 4 based on JLIS-rerakning features.
Table 1 shows our results on the development
data. As we can see in this table, Run 2 is better than
Run 1 by 6 NEs. It may be that the data in develop
59
set are double phonemes. And we also observe that
both Run 1 and Run 2 is better than Run 3 and Run
4, the reason may be that the extra null consonant
distract the performance of the prediction model.
From the results, it shows that our re-ranking
methods can actually improve transliteration.
Reranking based on web corpora can achieve better
accuracy compared with web-based reranking.
The JLIS-Reranking method slightly improve the
accuracy. It could be that the features we use
are not enough to capture the alignment between
English-Korean NE pair.
Because the runs with re-ranking achieving bet-
ter results, we submit the result on the test data with
JLIS-Reranking as the standard run, and the result
with the web-based re-ranking as the non-standard
run for our final results. The results on the test data
set are shown in table 2. The results also shows that
the web-based re-ranking can achieve the best accu-
racy up to 0.458.
4 Conclusion
In this paper, we describe our approach to English-
Korean named entity transliteration task for NEWS
2012. First, we decompose Korean word into Ko-
rean letters and then romanize them into sequential
Roman letters. Since a Korean word may not contain
the final consonant, we also create some alignment
results with the null consonant in romanized Korean
representations. After preprocessing the training
data, we use m2m-aligner to get the alignments from
English to Korean. Next, we train several translitera-
tion models based on DirecTL-p with the alignments
from the m2m-aligner. Finally, we propose two
re-ranking methods. One is web-based re-ranking
with Google search engine. We send the English
NE and its Korean transliteration pair our model
generates to Google to get the co-occurrence count
to re-rank the results. The other method is JLIS-
reranking based on three features from the alignment
results, including source grapheme chain feature,
target grapheme chain feature, and syllable consis-
tent feature. In the experiment results, our method
achieves the good accuracy up to 0.398 in the stan-
dard run and 0.458 in non-standard run. Our results
show that the transliteration model with a web-based
re-ranking method can achieve better accuracy in
English-Korean transliteration.
References
Ming-Wei Chang, Vivek Srikumar, Dan Goldwas-ser, and
Dan Roth. 2010. Structured output learning with indi-
rect supervision. Proceeding of the International Con-
ference on Machine Learning (ICML).
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme con-
version. Association for Computational Linguistics,
pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. Association
for Computational Linguistics, pages 905?912.
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma,
Aditya Bhargava, Qing Dou, Mi-Young Kim, and
Grzegorz Kondrak. 2010. Transliteration generation
and mining with limited training resources. Proceed-
ings of the 2010 Named Entities Workshop, ACL 2010,
pages 39?47.
Yan Song, Chunyu Kit, and Hai Zhao. 2010. Reranking
with multiple features for better transliteration. Pro-
ceedings of the 2010 Named Entities Work-shop, ACL
2010, pages 62?65.
60
