Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 7?12,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Solving the ?Who?s Mark Johnson? Puzzle:
Information Extraction Based Cross Document Coreference
Jian Huang?? Sarah M. Taylor? Jonathan L. Smith? Konstantinos A. Fotiadis? C. Lee Giles?
?Information Sciences and Technology
Pennsylvania State University, University Park, PA 16802, USA
??Advanced Technology Office, Lockheed Martin IS&GS
?4350 N. Fairfax Drive, Suite 470, Arlington, VA 22203, USA
?230 Mall Blvd, King of Prussia, PA 19406, USA
Abstract
Cross Document Coreference (CDC) is the
problem of resolving the underlying identity
of entities across multiple documents and is a
major step for document understanding.
We develop a framework to efficiently
determine the identity of a person based on
extracted information, which includes unary
properties such as gender and title, as well as
binary relationships with other named entities
such as co-occurrence and geo-locations.
At the heart of our approach is a suite of
similarity functions (specialists) for matching
relationships and a relational density-based
clustering algorithm that delineates name
clusters based on pairwise similarity. We
demonstrate the effectiveness of our methods
on the WePS benchmark datasets and point
out future research directions.
1 Introduction
The explosive growth of web data offers users both
the opportunity and the challenge to discover and
integrate information from disparate sources. As
alluded to in the title, a search query of the common
name ?Mark Johnson? refers to as many as 70
namesakes in the top 100 search results from the
Yahoo! search engine, only one of whom is the
Brown University professor and co-author of an
ACL 2006 paper (see experiments). Cross document
coreference (CDC) (Bagga and Baldwin, 1998) is a
distinct technology that consolidates named entities
across documents according to their real referents.
Despite the variety of styles and content in different
text, CDC can break the boundaries of documents
and cluster those mentions referring to the same
?Contact author: jhuang@ist.psu.edu
Mark Johnson. As unambiguous person references
are key to many tasks, e.g. social network analysis,
this work focuses on person named entities. The
method can be later extended to organizations.
We highlight the key differences between our
proposed CDC system with past person name
search systems. First, we seek to transcend the
simple bag of words approaches in earlier CDC
work by leveraging state-of-the-art information
extraction (IE) tools for disambiguation. The
main advantage is that our IE based approach has
access to accurate information such as a person?s
work titles, geo-locations, relationships and other
attributes. Traditional IR approaches, on the other
hand, may naively use the terms in a document
which can significantly hamper accuracy. For
instance, an article about Hillary Clinton may
contain references to journalists, politicians who
make comments about her. Even with careful word
selection, such textual features may still confuse the
disambiguation system about the true identity of the
person. The information extraction process in our
work can thus be regarded as an intelligent feature
selection step for disambiguation. Second, after
coreferencing, our system not only yields clusters
of documents, but also structured information
which is highly useful for automated document
understanding and data mining.
We review related work on CDC next and
describe our approach in Section 3. The methods
are evaluated on benchmark datasets in Section 4.
We discuss directions for future improvement in
Section 5 and conclude in Section 6.
2 Related Work
There is a long tradition of work on the within
document coreference (WDC) problem in NLP,
7
which links named entities with the same referent
within a document into a WDC chain. State-of-
the-art WDC systems, e.g. (Ng and Cardie, 2001),
leverage rich lexical features and use supervised
and unsupervised machine learning methods.
Research on cross document coreference began
more recently. (Bagga and Baldwin, 1998) proposed
a CDC system to merge the WDC chains using the
Vector Space Model on the summary sentences.
(Gooi and Allan, 2004) simplified this approach by
eliminating the WDC module without significant
deterioration in performance. Clustering approaches
(e.g. hierarchical agglomerative clustering (Mann
and Yarowsky, 2003)) have been commonly used
for CDC due to the variety of data distributions
of different names. Our work goes beyond the
simple co-occurrence features (Bagga and Baldwin,
1998) and the limited extracted information (e.g.
biographical information in (Mann and Yarowsky,
2003) that is relatively scarce in web data) using
the broad range of relational information with the
support of information extraction tools. There
are also other related research problems. (Li et
al., 2004) solved the robust reading problem by
adopting a probabilistic view on how documents are
generated and how names are sprinkled into them.
Our previous work (Huang et al, 2006) resolved
the author name ambiguity problem based on the
metadata records extracted from academic papers.
3 Methods
The overall framework of our CDC system works
as follows. Given a document, the information
extraction tool first extracts named entities and
constructs WDC chains. It also creates linkages
(relationships) between entities. The similarity
between a pair of relationships in WDC chains
is measured by an awakened similarity specialist
and the similarity between two WDC chains is
determined by the mixture of awakened specialists?
predictions. Finally, a density-based clustering
method generates clusters corresponding to real
world entities. We describe these steps in detail.
3.1 Entity and Relationship Extraction
Given a document, an information extraction
tool is first used to extract named entities and
perform within document coreference. Hence,
named entities in each document are divided into
a set of WDC chains, each chain corresponding
to one real world entity. In addition, state-of-
the-art IE tools are capable of creating relational
information between named entities. We use an
IE tool AeroText1 (Taylor, 2004) for this purpose.
Besides the attribute information about the person
named entity (first/middle/last names, gender,
mention, etc), AeroText also extracts relationship
information between named entities, such as
Family, List, Employment, Ownership, Citizen-
Resident-Religion-Ethnicity, etc, as specified in the
Automatic Content Extraction (ACE) evaluation.
The input to the CDC system is a set of WDC chains
(with relationship information stored in them) and
the CDC task is to merge these WDC chains2.
3.2 Similarity Features
We design a suite of similarity functions to
determine whether the relationships in a pair of
WDC chains match, divided into three groups:
Text similarity. To decide whether two names
in the co-occurrence or family relationship match,
we use SoftTFIDF (Cohen et al, 2003), which has
shown best performance among various similarity
schemes tested for name matching. SoftTFIDF is
a hybrid matching scheme that combines the token-
based TFIDF with the Jaro-Winkler string distance
metric. This permits inexact matching of named
entities due to name variations, spelling errors, etc.
Semantic similarity. Text or syntactic similarity is
not always sufficient for matching relationships. For
instance, although the mentions ?U.S. President?
and ?Commander-in-chief? have no textual overlap,
they are semantically highly related as they can be
synonyms. We use WordNet and the information
theory based JC semantic distance (Jiang and
Conrath, 1997) to measure the semantic similarity
between concepts in relationships such as mention,
employment, ownership and so on.
1AeroText is a text mining application for content
analysis, with main focus on information extraction
including entity extraction and intrasource link analysis
(see http://en.wikipedia.org/wiki/AeroText).
2We make no distinctions whether WDC chains are
extracted from the same document. Indeed, the CDC system
can correct the WDC errors due to lack of information for
merging named entities within a document.
8
Other rule-based similarity. Several other
cases require special treatment. For example, the
employment relationships of Senator and D-N.Y.
should match based on domain knowledge. Also,
we design rule-based similarity functions to handle
nicknames (Bill and William), acronyms (COLING
for International Conference on Computational
Linguistics), and geographical locations3.
3.3 Learning a Similarity Matrix
After the similarity features between a pair of
WDC chains are computed, we need to compute
the pairwise distance metric for clustering. (Cohen
et al, 2003) trained a binary SVM model and
interpreted its confidence in predicting the negative
class as the distance metric. In our case of using
information extraction results for disambiguation,
however, only some of the similarity features are
present based on the availability of relationships
in two WDC chains. Therefore, we treat each
similarity function as a subordinate predicting
algorithm (called specialist) and utilize the
specialist learning framework (Freund et al, 1997)
to combine the predictions. Here, a specialist is
awake only when the same relationships are present
in two WDC chains. Also, a specialist can refrain
from making a prediction for an instance if it is
not confident enough. In addition to the similarity
scores, specialists have different weights, e.g. a
match in a family relationship is considered more
important than in a co-occurrence relationship.
The Specialist Exponentiated Gradient (SEG)
(Freund et al, 1997) algorithm is adopted to learn
to mix the specialists? prediction. Given a set
of T training instances {xt} (xt,i denotes the
i-th specialist?s prediction), the SEG algorithm
minimizes the square loss of the outcome y? in an
online manner (Algorithm 1). In each learning
iteration, SEG first predict y?t using the set of awake
experts Et with respect to instance xt. The true
outcome yt (1 for coreference and 0 otherwise) is
then revealed and square loss L is incurred. SEG
then updates the weight distribution p accordingly.
To sum up, the similarity between a pair of
3Though a rich set of similarity features has been built for
matching the relationships, they may not encompass all possible
cases in real world documents. The goal of this work, however,
is to focus on the algorithms instead of knowledge engineering.
Algorithm 1 SEG (Freund et al, 1997)
Input: Initial weight distribution p1;
learning rate ? > 0; training set {xt}
1: for t=1 to T do
2: Predict using:
y?t =
?
i?Et ptixt,i?
i?Et pti
(1)
3: Observe true label yt and incur square loss
L(y?t, yt) = (y?t ? yt)2
4: Update weight distribution: for i ? Et
pt+1i = ptie?2?xt,i(y?t?yt)
?
j?Et ptj?
j?Et ptje?2?xt,i(y?t?yt)
pt+1i = pti, otherwise
5: end for
Output: Model p
WDC chains wi and wj can be represented in a
similarity matrix R, with ri,j computed by the SEG
prediction step using the learned weight distribution
p (Equation 1). A relational clustering algorithm
then clusters entities using R, as we introduce next.
3.4 Relational Clustering
The set of WDC chains to be clustered are
represented by a relational similarity matrix. Most
of the work in clustering, however, is only capable
of clustering numerical object data (e.g. K-means).
Relational clustering algorithms, on the other hand,
cluster objects based on the less direct measurement
of similarity between object pairs. We choose to
use a density based clustering algorithm DBSCAN
(Ester et al, 1996) mainly for two reasons.
First, most clustering algorithm require the
number of clusters K as an input parameter. The
optimal K can apparently vary greatly for names
with different frequency and thus is a sensitive
parameter. Even if a cluster validity index is used
to determine K, it usually requires running the
underlying clustering algorithm multiple times
and hence is inefficient for large scale data.
DBSCAN, as a density based clustering method,
only requires density parameters such as the
radius of the neighborhood ? that are universal for
different datasets. As we show in the experiment,
9
density parameters are relatively insensitive for
disambiguation performance.
Second, the distance metric in relational space
may be non-Euclidean, rendering many clustering
algorithms ineffective (e.g. single linkage clustering
algorithm is known to generate chain-shaped
clusters). Density-based clustering, on the other
hand, can generate clusters of arbitrary shapes since
only objects in dense areas are placed in a cluster.
DBSCAN induces a density-based cluster by
the core objects, i.e. objects having more than
a specified number of other data objects in their
neighborhood of size ?. In each clustering step, a
seed object is checked to determine whether it?s a
core object and if so, it induces other points of the
same cluster using breadth first search (otherwise
it?s considered as a noise point). In interest of
space, we refer readers to (Ester et al, 1996) for
algorithmic details of DBSCAN and now turn
our attention to evaluating the disambiguation
performance of our methods.
4 Experiments
We first formally define the evaluation metrics,
followed by the introduction to the benchmark test
sets and the system?s performance.
4.1 Evaluation Measures
We evaluate the performance of our method using
the standard purity and inverse purity clustering
metrics. Let a set of clusters C = {C1, ..., Cs}
denote the system?s output and a set of categories
D = {D1, ..., Dt} be the gold standard. Both C and
D are partitions of the WDC chains {w1, ..., wn}
(n = ?i |Ci| = ?j |Dj |). First, the precision of
a cluster Ci w.r.t. a category Dj is defined as,
Precision(Ci, Dj) = |Ci ?Dj ||Ci|
Purity is defined as the weighted average of the
maximum precision achieved by the clusters on one
of the categories,
Purity(C,D) =
s?
i=1
|Ci|
n maxj Precision(Ci, Dj)
Hence purity penalizes putting noise WDC chains in
a cluster. Trivially, the maximum purity (i.e. 1) can
be achieved by making one cluster per WDC chain
(referred to as the one-in-one baseline).
Reversing the role of clusters and categories,
Inverse purity(C,D) def= Purity(D, C). Inverse
Purity penalizes splitting WDC chains belonging
to the same category into different clusters. The
maximum inverse purity can be achieved by putting
all chain in one cluster (all-in-one baseline).
Purity and inverse purity are similar to the
precision and recall measures commonly used
in information retrieval. There is a tradeoff
relationship between the two and their harmonic
mean F0.5 is used for performance evaluation.
4.2 Datasets
We evaluate our methods using the benchmark
test collection from the ACL SemEval-2007 web
person search task (WePS hereafter) (Artiles et al,
2007). The test collection consists of three sets of
documents for 10 different names, sampled from
the English Wikipedia (famous people), participants
of the ACL 2006 conference (computer scientists)
and common names from the US Census data,
respectively. For each ambiguous name, the top 100
documents retrieved from the Yahoo! Search API
were annotated by human annotators according to
the actual entity of the name. This yields on average
45 different real world entities per set and about 3k
documents in total.
We note that the annotation in WePS makes the
simplifying assumption that each document refers to
only one real world person among the namesakes
in question. The CDC task in the perspective of
this paper, however, is to merge the WDC chains
rather than documents. Hence in our evaluation,
we adopt the document label to annotate the WDC
chain from the document that corresponds to the
person name search query. Despite the difference,
the results of the one-in-one and all-in-one baselines
are almost identical to those reported in the WePS
evaluation (F0.5 = 0.61, 0.40 respectively). Hence
the performance reported here is comparable to the
official evaluation results (Artiles et al, 2007).
4.3 Experiment Results
We computed the similarity features from the WDC
chains extracted from the WePS training data and
subsampled the non-coreferent pairs to generate a
10
Table 1: Cross document coreference performance
(macro-averaged scores, I-Pur denotes inverse purity).
Test set Method Purity I-Pur F0.5
Wikipedia AT-CDC 0.684 0.725 0.687
ACL-06 AT-CDC 0.792 0.657 0.712
US Census AT-CDC 0.772 0.700 0.722
Global
Average
AT-CDC 0.749 0.695 0.708
One-in-one 1.000 0.482 0.618
All-in-one 0.279 1.000 0.389
training set of around 32k pairwise instances. We
then used the SEG algorithm to learn the weight
distribution model. The macro-averaged cross
document coreference results on the WePS test
sets are reported in Table 1. The F0.5 score of our
CDC system (AT-CDC) is 0.708, comparable to the
test results of the first tier systems in the official
evaluation. The two baselines are also included.
Because the test set is very ambiguous (on average
only two documents per real world entity), the
one-in-one baseline has relatively high F0.5 score.
The Wikipedia, ACL06 and US Census sets
have on average 56, 31 and 50 entities per name
respectively. We notice that as the data set becomes
more ambiguous, purity decreases implying
it?s harder for the system to discard irrelevant
documents from a cluster. The other case is true
for inverse purity. In particular, we are interested in
how the coreference performance changes with the
number of entities per name (which can be viewed
as the ambiguity level of a data set). This is shown
in Figure 1. We observe that in general the harmonic
mean of the purity is fairly stable across different
number of entities per dataset (generally within
the band between 0.6 and 0.8). This is important
because the system?s performance does not vary
greatly with the underlying data characteristics.
There is a particular name (with only one underlying
referent) that appears to be an outlier in performance
in Figure 1. After examining the extraction results,
we notice that the extracted relationships refer to
the same person?s employment, coauthors and geo-
locations. The generated CDC clusters correctly
reflect the different aspects of the person but the
system is unable to link them together due to the
lack of information for merging. This motivates us
to further improve performance in future work.
Figure 2 shows how the coreference performance
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50  60  70  80  90  100
F(0
.5)
Number of entities per name 
Figure 1: Coreference performance for names with
different number of real world entities.
changes with different density parameter ?. We
observe that as we increase the size of the ?
neighborhood, inverse purity increases indicating
that more correct coreference decisions are made.
On the other hand, purity decreases as more noise
WDC chains appear in clusters. Due to this tradeoff
relationship, the F score is fairly stable with a wide
range of ? values and hence the density parameter is
rather insensitive (compared to, say, the number of
clusters K).
5 Future Work
We see several opportunities to improve the
coreference performance of the proposed methods.
First, though the system?s performance compares
favorably with the WePS submissions, we observe
that purity is higher than inverse purity, indicating
that the system finds it more difficult to link
coreferent documents than to discard noise from
0.3 0.35 0.4 0.45 0.5 0.550
0.2
0.4
0.6
0.8
1
?
DBSCAN with different parameter settings
Purity
Inverse Purity
F(0.5)
Figure 2: Coreference performance with different ?.
11
clusters. Thus coreferencing based solely on the
information generated by an information extraction
tool may not always be sufficient. For one, it
remains a huge challenge to develop a general
purpose information extraction tool capable of
applying to web documents with widely different
formats, styles, content, etc. Also, even if the
extraction results are perfect, relationships extracted
from different documents may be of different
types (family memberships vs. geo-locations) and
cannot be directly matched against one another. We
are exploring several methods to complement the
extracted relationships using other information:
? Context-aided CDC. The context where an named
entity is extracted can be leveraged for coreference.
The bag of words in the context tend to be less noisy
than that from the entire document. Moreover, we
can use noun phrase chunkers to extract base noun
phrases from the context. These word or phrase level
features can serve as a safenet when the IE tool fails.
? Topic-based CDC. Similar to (Li et al, 2004),
document topics can be used to ameliorate the
sparsity problem. For example, the topics Sport
and Education are important cues for differentiating
mentions of ?Michael Jordan?, which may refer to a
basketball player, a computer science professor, etc.
Second, as noted in the top WePS run (Chen and
Martin, 2007), feature development is important in
achieving good coreference performance. We aim
to improve the set of similarity specialists in our
system by leveraging large knowledge bases.
Moreover, although the CDC system is developed
in the web person search context, the methods are
also applicable to other scenarios. For instance,
there is tremendous interest in building structured
databases from unstructured text such as enterprise
documents and news articles for data mining, where
CDC is a key step for ?understanding? documents
from disparate sources. We plan to continue our
investigations along these lines.
6 Conclusions
We have presented and implemented an information
extraction based Cross Document Coreference
(CDC) system that employs supervised and
unsupervised learning methods. We evaluated
the proposed methods with experiments on a
large benchmark disambiguation collection, which
demonstrate that the proposed methods compare
favorably with the top runs in the SemEval
evaluation. We believe that by incorporating
information such as context and topic, besides the
extracted relationships, the performance of the CDC
can be further improved. We have outlined research
plans to address this and several other issues.
References
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The SemEval-2007 WePS evaluation: Establishing a
benchmark for the web people search task. In Proc 4th
Int?l Workshop on Semantic Evaluations (SemEval).
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proc. of 36th ACL and 17th COLING.
Ying Chen and James Martin. 2007. Towards robust
unsupervised personal name disambiguation. In
Proceedings of EMNLP and CoNLL, pages 190?198.
William W. Cohen, Pradeep Ravikumar, and Stephen E.
Fienberg. 2003. A comparison of string distance
metrics for name-matching tasks. In Proc. of IJCAI
Workshop on Information Integration on the Web.
Martin Ester, Hans-Peter Kriegel, Jorg Sander, and
Xiaowei Xu. 1996. A density-based algorithm for
discovering clusters in large spatial databases with
noise. In Proceedings of 2nd KDD, pages 226 ? 231.
Yoav Freund, Robert E. Schapire, Yoram Singer, and
Manfred K. Warmuth. 1997. Using and combining
predictors that specialize. In Proceedings of 29th ACM
symposium on Theory of computing (STOC).
Chung H. Gooi and James Allan. 2004. Cross-document
coreference on a large scale corpus. In HLT-NAACL.
Jian Huang, Seyda Ertekin, and C. Lee Giles.
2006. Efficient name disambiguation for large scale
databases. In Proc. of 10th PKDD, pages 536 ? 544.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical
taxonomy. In Proceedings of ROCLING X.
Xin Li, Paul Morie, and Dan Roth. 2004. Robust
reading: Identification and tracing of ambiguous
names. In Proceedings of HLT-NAACL, pages 17?24.
Gideon S. Mann and David Yarowsky. 2003.
Unsupervised personal name disambiguation. In
Proceedings of HLT-NAACL, pages 33?40.
Vincent Ng and Claire Cardie. 2001. Improving machine
learning approaches to coreference resolution. In
Proceedings of the 40th ACL, pages 104?111.
Sarah M. Taylor. 2004. Information extraction
tools: Deciphering human language. IT Professional,
6(6):28 ? 34.
12
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 414?422,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Profile Based Cross-Document Coreference
Using Kernelized Fuzzy Relational Clustering
Jian Huang? Sarah M. Taylor? Jonathan L. Smith? Konstantinos A. Fotiadis? C. Lee Giles?
?College of Information Sciences and Technology
Pennsylvania State University, University Park, PA 16802, USA
{jhuang, giles}@ist.psu.edu
?Advanced Technology Office, Lockheed Martin IS&GS, Arlington, VA 22203, USA
{sarah.m.taylor, jonathan.l.smith, konstantinos.a.fotiadis}@lmco.com
Abstract
Coreferencing entities across documents
in a large corpus enables advanced
document understanding tasks such as
question answering. This paper presents
a novel cross document coreference
approach that leverages the profiles
of entities which are constructed by
using information extraction tools and
reconciled by using a within-document
coreference module. We propose to
match the profiles by using a learned
ensemble distance function comprised
of a suite of similarity specialists. We
develop a kernelized soft relational
clustering algorithm that makes use of
the learned distance function to partition
the entities into fuzzy sets of identities.
We compare the kernelized clustering
method with a popular fuzzy relation
clustering algorithm (FRC) and show 5%
improvement in coreference performance.
Evaluation of our proposed methods
on a large benchmark disambiguation
collection shows that they compare
favorably with the top runs in the
SemEval evaluation.
1 Introduction
A named entity that represents a person, an or-
ganization or a geo-location may appear within
and across documents in different forms. Cross
document coreference (CDC) is the task of con-
solidating named entities that appear in multiple
documents according to their real referents. CDC
is a stepping stone for achieving intelligent in-
formation access to vast and heterogeneous text
corpora, which includes advanced NLP techniques
such as document summarization and question an-
swering. A related and well studied task is within
document coreference (WDC), which limits the
scope of disambiguation to within the boundary of
a document. When namesakes appear in an article,
the author can explicitly help to disambiguate, us-
ing titles and suffixes (as in the example, ?George
Bush Sr. ... the younger Bush?) besides other
means. Cross document coreference, on the other
hand, is a more challenging task because these
linguistics cues and sentence structures no longer
apply, given the wide variety of context and styles
in different documents.
Cross document coreference research has re-
cently become more popular due to the increasing
interests in the web person search task (Artiles
et al, 2007). Here, a search query for a person
name is entered into a search engine and the
desired outputs are documents clustered according
to the identities of the entities in question. In
our work, we propose to drill down to the sub-
document mention level and construct an entity
profile with the support of information extraction
tools and reconciled with WDC methods. Hence
our IE based approach has access to accurate
information such as a person?s mentions and geo-
locations for disambiguation. Simple IR based
CDC approaches (e.g. (Gooi and Allan, 2004)), on
the other hand, may simply use all the terms and
this can be detrimental to accuracy. For example, a
biography of John F. Kennedy is likely to mention
members of his family with related positions,
besides references to other political figures. Even
with careful word selection, these textual features
can still confuse the disambiguation system about
the true identity of the person.
We propose to handle the CDC task using a
novel kernelized fuzzy relational clustering algo-
rithm, which allows probabilistic cluster mem-
bership assignment. This not only addresses the
intrinsic uncertainty nature of the CDC problem,
but also yields additional performance improve-
ment. We propose to use a specialist ensemble
414
learning approach to aggregate the diverse set of
similarities in comparing attributes and relation-
ships in entity profiles. Our approach is first fully
described in Section 2. The effectiveness of the
proposed method is demonstrated using real world
benchmark test sets in Section 3. We review
related work in cross document coreference and
conclude in Section 5.
2 Methods
2.1 Document Level and Profile Based CDC
We make distinctions between document level and
profile based cross document coreference. Docu-
ment level CDC makes a simplifying assumption
that a named entity (and its variants) in a document
has one underlying real identity. The assump-
tion is generally acceptable but may be violated
when a document refers to namesakes at the same
time (e.g. George W. Bush and George H. W.
Bush referred to as George or President Bush).
Furthermore, the context surrounding the person
NE President Clinton can be counterproductive
for disambiguating the NE Senator Clinton, with
both entities likely to appear in a document at the
same time. The simplified document level CDC
has nevertheless been used in the WePS evaluation
(Artiles et al, 2007), called the web people task.
In this work, we advocate profile based disam-
biguation that aims to leverage the advances in
NLP techniques. Rather than treating a document
as simply a bag of words, an information extrac-
tion tool first extracts NE?s and their relationships.
For the NE?s of interest (i.e. persons in this work),
a within-document coreference (WDC) module
then links the entities deemed as referring to
the same underlying identity into a WDC chain.
This process includes both anaphora resolution
(resolving ?He? and its antecedent ?President Clin-
ton?) and entity tracking (resolving ?Bill? and
?President Clinton?). Let E = {e1, ..., eN} denote
the set of N chained entities (each corresponding
to a WDC chain), provided as input to the CDC
system. We intentionally do not distinguish which
document each ej belongs to, as profile based
CDC can potentially rectify WDC errors by lever-
aging information across document boundaries.
Each ei is represented as a profile which contains
the NE, its attributes and associated relationships,
i.e. ej =< ej,1, ..., ej,L > (ej,l can be a textual
attribute or a pointer to another entity). The profile
based CDC method generates a partition of E ,
represented by a partition matrix U (where uij
denotes the membership of an entity ej to the i-
th identity cluster). Therefore, the chained entities
placed in a name cluster are deemed as coreferent.
Profile based CDC addresses a finer grained
coreference problem in the mention level, enabled
by the recent advances in IE and WDC techniques.
In addition, profile based CDC facilitates user
information consumption with structured informa-
tion and short summary passages. Next, we focus
on the relational clustering algorithm that lies at
the core of the profile based CDC system. We then
turn our attention to the specialist learning algo-
rithm for the distance function used in clustering,
capable of leveraging the available training data.
2.2 CDC Using Fuzzy Relational Clustering
2.2.1 Preliminaries
Traditionally, hard clustering algorithms (where
uij ? {0, 1}) such as complete linkage hierarchi-
cal agglomerative clustering (Mann and Yarowsky,
2003) have been applied to the disambiguation
problem. In this work, we propose to use fuzzy
clustering methods (relaxing the membership con-
dition to uij ? [0, 1]) as a better way of handling
uncertainty in cross document coreference. First,
consider the following motivating example,
Example. The named entity President Bush is
extracted from the sentence ?President Bush ad-
dressed the nation from the Oval Office Monday.?
? Without additional cues, a hard clustering
algorithm has to arbitrarily assign the
mention ?President Bush? to either the NE
?George W. Bush? or ?George H. W. Bush?.
? A soft clustering algorithm, on the other
hand, can assign equal probability to the two
identities, indicating low entropy or high
uncertainty in the solution. Additionally, the
soft clustering algorithm can assign lower
probability to the identity ?Governor Jeb
Bush?, reflecting a less likely (though not
impossible) coreference decision.
We first formalize the cross document corefer-
ence problem as a soft clustering problem, which
minimizes the following objective function:
JC(E) =
C?
i=1
N?
j=1
umijd2(ej ,vi) (1)
s.t.
C?
i=1
uij = 1 and
N?
j=1
uij > 0, uij ? [0, 1]
415
where vi is a virtual (implicit) prototype of the i-th
cluster (ej ,vi ? D) and m controls the fuzziness
of the solution (m > 1; the solution approaches
hard clustering as m approaches 1). We will
further explain the generic distance function d :
D ? D ? R in the next subsection. The goal
of the optimization is to minimize the sum of
deviations of patterns to the cluster prototypes.
The clustering solution is a fuzzy partition P? =
{Ci}, where ej ? Ci if and only if uij > ?.
We note from the outset that the optimization
functional has the same form as the classical
Fuzzy C-Means (FCM) algorithm (Bezdek, 1981),
but major differences exist. FCM, as most ob-
ject clustering algorithms, deals with object data
represented in a vectorial form. In our case, the
data is purely relational and only the mutual rela-
tionships between entities can be determined. To
be exact, we can define the similarity/dissimilarity
between a pair of attributes or relationships of
the same type l between entities ej and ek as
s(l)(ej , ek). For instance, the similarity between
the occupations ?President? and ?Commander in
Chief? can be computed using the JC semantic
distance (Jiang and Conrath, 1997) with WordNet;
the similarity of co-occurrence with other people
can be measured by the Jaccard coefficient. In the
next section, we propose to compute the relation
strength r(?, ?) from the component similarities
using aggregation weights learned from training
data. Hence the N chained entities to be clustered
can be represented as relational data using an n?n
matrix R, where rj,k = r(ej , ek). The Any Rela-
tion Clustering Algorithm (ARCA) (Corsini et al,
2005; Cimino et al, 2006) represents relational
data as object data using their mutual relation
strength and uses FCM for clustering. We adopt
this approach to transform (objectify) a relational
pattern ej into an N dimensional vector rj (i.e.
the j-th row in the matrix R) using a mapping
? : D ? RN . In other words, each chained entity
is represented as a vector of its relation strengths
with all the entities. Fuzzy clusters can then
be obtained by grouping closely related patterns
using object clustering algorithm.
Furthermore, it is well known that FCM
is a spherical clustering algorithm and thus
is not generally applicable to relational data
which may yield relational clusters of arbitrary
and complicated shapes. Also, the distance in
the transformed space may be non-Euclidean,
rendering many clustering algorithms ineffective
(many FCM extensions theoretically require
the underlying distance to satisfy certain metric
properties). In this work, we propose kernelized
ARCA (called KARC) which uses a kernel-
induced metric to handle the objectified relational
data, as we introduce next.
2.2.2 Kernelized Fuzzy Clustering
Kernelization (Scho?lkopf and Smola, 2002) is a
machine learning technique to transform patterns
in the data space to a high-dimensional feature
space so that the structure of the data can be more
easily and adequately discovered. Specifically, a
nonlinear transformation ? maps data in RN to
H of possibly infinite dimensions (Hilbert space).
The key idea is the kernel trick ? without explicitly
specifying ? and H, the inner product in H can
be computed by evaluating a kernel function K in
the data space, i.e. < ?(ri),?(rj) >= K(ri, rj)
(one of the most frequently used kernel func-
tions is the Gaussian RBF kernel: K(rj , rk) =
exp(???rj ? rk?2)). This technique has been
successfully applied to SVMs to classify non-
linearly separable data (Vapnik, 1995). Kerneliza-
tion preserves the simplicity in the formalism of
the underlying clustering algorithm, meanwhile it
yields highly nonlinear boundaries so that spheri-
cal clustering algorithms can apply (e.g. (Zhang
and Chen, 2003) developed a kernelized object
clustering algorithm based on FCM).
Let wi denote the objectified virtual cluster vi,
i.e. wi = ?(vi). Using the kernel trick, the
squared distance between ?(rj) and ?(wi) in the
feature space H can be computed as:
??(rj)? ?(wi)?2H (2)
= < ?(rj)? ?(wi),?(rj)? ?(wi) >
= < ?(rj),?(rj) > ?2 < ?(rj),?(wi) >
+ < ?(wi),?(wi) >
= 2? 2K(rj ,wi) (3)
assuming K(r, r) = 1. The KARC algorithm
defines the generic distance d as d2(ej ,vi) def=
??(rj)??(wi)?2H = ??(?(ej))??(?(vi))?2H
(we also use d2ji as a notational shorthand).
Using Lagrange Multiplier as in FCM, the opti-
mal solution for Equation (1) is:
uij =
?
??
??
[
C?
h=1
(
d2ji
d2jh
)1/(m?1)]?1
, (d2ji 6= 0)
1 , (d2ji = 0)
(4)
416
?(wi) =
N?
k=1
umik?(rk)
N?
k=1
umik
(5)
Since ? is an implicit mapping, Eq. (5) can
not be explicitly evaluated. On the other hand,
plugging Eq. (5) into Eq. (3), d2ji can be explicitly
represented by using the kernel matrix,
d2ji = 2? 2 ?
N?
k=1
umikK(rj , rk)
N?
k=1
umik
(6)
With the derivation, the kernelized fuzzy clus-
tering algorithm KARC works as follows. The
chained entities E are first objectified into the
relation strength matrix R using SEG, the details
of which are described in the following section.
The Gram matrix K is then computed based on
the relation strength vectors using the kernel func-
tion. For a given number of clusters C, the
initialization step is done by randomly picking C
patterns as cluster centers, equivalently, C indices
{n1, .., nC} are randomly picked from {1, .., N}.
D0 is initialized by setting d2ji = 2? 2K(rj , rni).
KARC alternately updates the membership matrix
U and the kernel distance matrix D until conver-
gence or running more than maxIter iterations
(Algorithm 1). Finally, the soft partition is gen-
erated based on the membership matrix U , which
is the desired cross document coreference result.
Algorithm 1 KARC Alternating Optimization
Input: Gram matrix K; #Clusters C; threshold ?
initialize D0
t ? 0
repeat
t ? t+ 1
// 1? Update membership matrix U t:
uij = (d
2
ji)
? 1m?1
?C
h=1 (d2jh)
? 1m?1
// 2? Update kernel distance matrix Dt:
d2ji = 2? 2 ?
N?
k=1
umikKjk
N?
k=1
umik
until (t > maxIter) or
(t > 1 and |U t ? U t?1| < ?)
P? ? Generate soft partition(U t, ?)
Output: Fuzzy partition P?
2.2.3 Cluster Validation
In the CDC setting, the number of true underlying
identities may vary depending on the entities? level
of ambiguity (e.g. name frequency). Selecting the
optimal number of clusters is in general a hard
research question in clustering1. We adopt the
Xie-Beni Index (XBI) (Xie and Beni, 1991) as in
ARCA, which is one of the most popular cluster
validities for fuzzy clustering algorithms. Xie-
Beni Index (XBI) measures the goodness of clus-
tering using the ratio of the intra-cluster variation
and the inter-cluster separation. We measure the
kernelized XBI (KXBI) in the feature space as,
KXBI =
C?
i=1
N?
j=1
umij ??(rj)? ?(wi)?2H
N ? min
1?i<j?C
??(wi)? ?(wj)?2H
where the nominator is readily computed using D
and the inter-cluster separation in the denominator
can be evaluated using the similar kernel trick
above (details omitted). Note that KXBI is only
defined for C > 1. Thus we pick the C that
corresponds to the first minimum of KXBI, and
then compare its objective function value JC with
the cluster variance (J1 for C = 1). The optimal
C is chosen from the minimum of the two2.
2.3 Specialist Ensemble Learning of Relation
Strengths between Entities
One remaining element in the overall CDC ap-
proach is how the relation strength rj,k between
two entities is computed. In (Cohen et al, 2003),
a binary SVM model is trained and its confidence
in predicting the non-coreferent class is used as
the distance metric. In our case of using in-
formation extraction results for disambiguation,
however, only some of the similarity features are
present based on the available relationships in two
profiles. In this work, we propose to treat each
similarity function as a specialist that specializes
in computing the similarity of a particular type
of relationship. Indeed, the similarity function
between a pair of attributes or relationships may in
itself be a sophisticated component algorithm. We
utilize the specialist ensemble learning framework
(Freund et al, 1997) to combine these component
1In particular, clustering algorithms that regularize the
optimization with cluster size are not applicable in our case.
2In practice, the entities to be disambiguated tend to be
dominated by several major identities. Hence performance
generally does not vary much in the range of large C values.
417
similarities into the relation strength for clustering.
Here, a specialist is awakened for prediction only
when the same type of relationships are present in
both chained entities. A specialist can choose not
to make a prediction if it is not confident enough
for an instance. These aspects contrast with the
traditional insomniac ensemble learning methods,
where each component learner is always available
for prediction (Freund et al, 1997). Also, spe-
cialists have different weights (in addition to their
prediction) on the final relation strength, e.g. a
match in a family relationship is considered more
important than in a co-occurrence relationship.
Algorithm 2 SEG (Freund et al, 1997)
Input: Initial weight distribution p1;
learning rate ? > 0; training set {< st, yt >}
1: for t=1 to T do
2: Predict using:
y?t =
?
i?Et ptisti?
i?Et pti
(7)
3: Observe the true label yt and incur square
loss L(y?t, yt) = (y?t ? yt)2
4: Update weight distribution: for i ? Et
pt+1i =
ptie?2?x
t
i(y?t?yt)
?
j?Et
ptje?2?x
t
i(y?t?yt)
?
?
j?Et
ptj (8)
Otherwise: pt+1i = pti
5: end for
Output: Model p
The ensemble relation strength model is learned
as follows. Given training data, the set of chained
entities Etrain is extracted as described earlier. For
a pair of entities ej and ek, a similarity vector
s is computed using the component similarity
functions for the respective attributes and rela-
tionships, and the true label is defined as y =
I{ej and ek are coreferent}. The instances are
subsampled to yield a balanced pairwise train-
ing set {< st, yt >}. We adopt the Special-
ist Exponentiated Gradient (SEG) (Freund et al,
1997) algorithm to learn the mixing weights of the
specialists? prediction (Algorithm 2) in an online
manner. In each training iteration, an instance
< st, yt > is presented to the learner (with Et
denoting the set of indices of awake specialists in
st). The SEG algorithm first predicts the value y?t
based on the awake specialists? decisions. The true
value yt is then revealed and the learner incurs a
square loss between the predicted and the true val-
ues. The current weight distribution p is updated
to minimize square loss: awake specialists are
promoted or demoted in their weights according to
the difference between the predicted and the true
value. The learning iterations can run a few passes
till convergence, and the model is learned in linear
time with respect to T and is thus very efficient. In
prediction time, let E(jk) denote the set of active
specialists for the pair of entities ej and ek, and
s(jk) denote the computed similarity vector. The
predicted relation strength rj,k is,
rj,k =
?
i?E(jk) pis(jk)i?
i?E(jk) pi
(9)
2.4 Remarks
Before we conclude this section, we make several
comments on using fuzzy clustering for cross
document coreference. First, instead of conduct-
ing CDC for all entities concurrently (which can
be computationally intensive with a large cor-
pus), chained entities are first distributed into non-
overlapping blocks. Clustering is performed for
each block which is a drastically smaller problem
space, while entities from different blocks are
unlikely to be coreferent. Our CDC system uses
phonetic blocking on the full name, so that name
variations arising from translation, transliteration
and abbreviation can be accommodated. Ad-
ditional link constraints checking is also imple-
mented to improve scalability though these are not
the main focus of the paper.
There are several additional benefits in using
a fuzzy clustering method besides the capabil-
ity of probabilistic membership assignments in
the CDC solution. In the clustered web search
context, splitting a true identity into two clusters
is perceived as a more severe error than putting
irrelevant records in a cluster, as it is more difficult
for the user to collect records in different clusters
(to reconstruct the real underlying identity) than
to prune away noisy records. While there is no
universal way to handle this with hard clustering,
soft clustering algorithms can more easily avoid
the false negatives by allowing records to prob-
abilistically appear in different clusters (subject
to the sum of 1) using a more lenient threshold.
Also, while there is no real prototypical elements
in relational clustering, soft relational clustering
418
methods can naturally rank the profiles within
a cluster according to their membership levels,
which is an additional advantage for enhancing
user consumption of the disambiguation results.
3 Experiments
In this section, we first formally define the evalu-
ation metrics, followed by the introduction to the
benchmark test sets and the system?s performance.
3.1 Evaluation Metrics
We benchmarked our method using the standard
purity and inverse purity clustering metrics as in
the WePS evaluation. Let a set of clusters P =
{Ci} denote the system?s partition as aforemen-
tioned and a set of categories Q = {Dj} be the
gold standard. The precision of a cluster Ci with
respect to a category Dj is defined as,
Precision(Ci,Dj) = |Ci ? Dj ||Ci|
Purity is in turn defined as the weighted average
of the maximum precision achieved by the clusters
on one of the categories,
Purity(P,Q) =
C?
i=1
|Ci|
n maxj Precision(Ci,Dj)
where n = ? |Ci|. Hence purity penalizes putting
noise chained entities in a cluster. Trivially, the
maximum purity (i.e. 1) can be achieved by
making one cluster per chained entity (referred to
as the one-in-one baseline). Reversing the role of
clusters and categories, Inverse purity(P,Q) def=
Purity(Q,P). Inverse Purity penalizes splitting
chained entities belonging to the same category
into different clusters. The maximum inverse
purity can be similarly achieved by putting all
entities into one cluster (all-in-one baseline).
Purity and inverse purity are similar to the
precision and recall measures commonly used in
IR. The F score, F = 1/(? 1Purity + (1 ?
?) 1InversePurity ), is used in performance evalua-
tion. ? = 0.2 is used to give more weight to
inverse purity, with the justification for the web
person search mentioned earlier.
3.2 Dataset
We evaluate our methods using the benchmark
test collection from the ACL SemEval-2007 web
person search task (WePS) (Artiles et al, 2007).
The test collection consists of three sets of 10
different names, sampled from ambiguous names
from English Wikipedia (famous people), partici-
pants of the ACL 2006 conference (computer sci-
entists) and common names from the US Census
data, respectively. For each name, the top 100
documents retrieved from the Yahoo! Search API
were annotated, yielding on average 45 real world
identities per set and about 3k documents in total.
As we note in the beginning of Section 2, the
human markup for the entities corresponding to
the search queries is on the document level. The
profile-based CDC approach, however, is to merge
the mention-level entities. In our evaluation, we
adopt the document label (and the person search
query) to annotate the entity profiles that corre-
sponds to the person name search query. Despite
the difference, the results of the one-in-one and
all-in-one baselines are almost identical to those
reported in the WePS evaluation (F = 0.52, 0.58
respectively). Hence the performance reported
here is comparable to the official evaluation results
(Artiles et al, 2007).
3.3 Information Extraction and Similarities
We use an information extraction tool AeroText
(Taylor, 2004) to construct the entity profiles.
AeroText extracts two types of information for
an entity. First, the attribute information about
the person named entity includes first/middle/last
names, gender, mention, etc. In addition,
AeroText extracts relationship information
between named entities, such as Family, List,
Employment, Ownership, Citizen-Resident-
Religion-Ethnicity and so on, as specified in the
ACE evaluation. AeroText resolves the references
of entities within a document and produces the
entity profiles, used as input to the CDC system.
Note that alternative IE or WDC tools, as well
as additional attributes or relationships, can be
readily used in the CDC methods we proposed.
A suite of similarity functions is designed to
determine if the attributes relationships in a pair
of entity profiles match or not:
Text similarity. To decide whether two names
in the co-occurrence or family relationship match,
we use the SoftTFIDF measure (Cohen et al,
2003), which is a hybrid matching scheme that
combines the token-based TFIDF with the Jaro-
Winkler string distance metric. This permits in-
exact matching of named entities due to name
419
variations, typos, etc.
Semantic similarity. Text or syntactic similarity
is not always sufficient for matching relationships.
WordNet and the information theoretic semantic
distance (Jiang and Conrath, 1997) are used to
measure the semantic similarity between concepts
in relationships such as mention, employment,
ownership, etc.
Other rule-based similarity. Several other
cases require special treatment. For example,
the employment relationships of Senator and
D-N.Y. should match based on domain knowledge.
Also, we design dictionary-based similarity
functions to handle nicknames (Bill and William),
acronyms (COLING for International Conference
on Computational Linguistics), and geo-locations.
3.4 Evaluation Results
From the WePS training data, we generated a
training set of around 32k pairwise instances as
previously stated in Section 2.3. We then used
the SEG algorithm to learn the weight distribution
model. We tuned the parameters in the KARC
algorithm using the training set with discrete grid
search and chose m = 1.6 and ? = 0.3. The RBF
kernel (Gaussian) is used with ? = 0.015.
Table 1: Cross document coreference performance
(I. Purity denotes inverse purity).
Method Purity I. Purity F
KARC-S 0.657 0.795 0.740
KARC-H 0.662 0.762 0.710
FRC 0.484 0.840 0.697
One-in-one 1.000 0.482 0.524
All-in-one 0.279 1.000 0.571
The macro-averaged cross document corefer-
ence on the WePS test sets are reported in Table
1. The F score of our CDC system (KARC-
S) is 0.740, comparable to the test results of the
first tier systems in the official evaluation. The
two baselines are also included. Since different
feature sets, NLP tools, etc are used in different
benchmarked systems, we are also interested in
comparing the proposed algorithm with differ-
ent soft relational clustering variants. First, we
?harden? the fuzzy partition produced by KARC
by allowing an entity to appear in the cluster
with highest membership value (KARC-H). Purity
improves because of the removal of noise entities,
though at the sacrifice of inverse purity and the
Table 2: Cross document coreference performance
on subsets (I. Purity denotes inverse purity).
Test set Identity Purity I. Purity F
Wikipedia 56.5 0.666 0.752 0.717
ACL-06 31.0 0.783 0.771 0.773
US Census 50.3 0.554 0.889 0.754
F score deteriorates. We also implement a pop-
ular fuzzy relational clustering algorithm called
FRC (Dave and Sen, 2002), whose optimization
functional directly minimizes with respect to the
relation matrix. With the same feature sets and
distance function, KARC-S outperforms FRC in F
score by about 5%. Because the test set is very am-
biguous (on average only two documents per real
world entity), the baselines have relatively high F
score as observed in the WePS evaluation (Artiles
et al, 2007). Table 2 further analyzes KARC-
S?s result on the three subsets Wikipedia, ACL06
and US Census. The F score is higher in the
less ambiguous (the average number of identities)
dataset and lower in the more ambiguous one, with
a spread of 6%.
We study how the cross document coreference
performance changes as we vary the fuzziness in
the solution (controlled by m). In Figure 1, as
m increases from 1.4 to 1.9, purity improves by
10% to 0.67, which indicates that more correct
coreference decisions (true positives) can be made
in a softer configuration. The complimentary is
true for inverse purity, though to a lesser extent.
In this case, more false negatives, corresponding
to the entities of different coreferents incorrectly
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 1.4  1.5  1.6  1.7  1.8  1.9
m
KARC performance with different m
purityinverse purityF
Figure 1: Purity, inverse purity and F score with
different fuzzifiers m.
420
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.1  0.2  0.3  0.4  0.5  0.6
?
KARC performance with different ?
purityinverse purityF
Figure 2: CDC performance with different ?.
linked, are made in a softer partition. The F
score peaks at 0.74 (m = 1.6) and then slightly
decreases, as the gain in purity is outweighed by
the loss in inverse purity.
Figure 2 evaluates the impact of the different
settings of ? (the threshold of including a chained
entity in the fuzzy cluster) on the coreference
performance. We observe that as we increase
?, purity improves indicating less ?noise? entities
are included in the solution. On the other hand,
inverse purity decreases meaning more coreferent
entities are not linked due to the stricter threshold.
Overall, the changes in the two metrics offset each
other and the F score is relatively stable across a
broad range of ? settings.
4 Related Work
The original work in (Bagga and Baldwin, 1998)
proposed a CDC system by first performing WDC
and then disambiguating based on the summary
sentences of the chains. This is similar to ours in
that mentions rather than documents are clustered,
leveraging the advances in state-of-the-art WDC
methods developed in NLP, e.g. (Ng and Cardie,
2001; Yang et al, 2008). On the other hand, our
work goes beyond the simple bag-of-word features
and vector space model in (Bagga and Baldwin,
1998; Gooi and Allan, 2004) with IE results. (Wan
et al, 2005) describes a person resolution system
WebHawk that clusters web pages using some
extracted personal information including person
name, title, organization, email and phone number,
besides lexical features. (Mann and Yarowsky,
2003) extracts biographical information, which is
relatively scarce in web data, for disambiguation.
With the support of state-of-the-art information
extraction tools, the profiles of entities in this work
covers a broader range of relational information.
(Niu et al, 2004) also leveraged IE support, but
their approach was evaluated on a small artificial
corpus. Also, the pairwise distance model is
insomniac (i.e. all similarity specialists are awake
for prediction) and our work extends this with a
specialist learning framework.
Prior work has largely relied on using hier-
archical clustering methods for CDC, with the
threshold for stopping the merging set using the
training data, e.g. (Mann and Yarowsky, 2003;
Chen and Martin, 2007; Baron and Freedman,
2008). The fuzzy relational clustering method
proposed in this paper we believe better addresses
the uncertainty aspect of the CDC problem.
There are also orthogonal research directions
for the CDC problem. (Li et al, 2004) solved the
CDC problem by adopting a probabilistic view on
how documents are generated and how names are
sprinkled into them. (Bunescu and Pasca, 2006)
showed that external information from Wikipedia
can improve the disambiguation performance.
5 Conclusions
We have presented a profile-based Cross Docu-
ment Coreference (CDC) approach based on a
novel fuzzy relational clustering algorithm KARC.
In contrast to traditional hard clustering methods,
KARC produces fuzzy sets of identities which
better reflect the intrinsic uncertainty of the CDC
problem. Kernelization, as used in KARC, enables
the optimization of clustering that is spherical
in nature to apply to relational data that tend to
have complicated shapes. KARC partitions named
entities based on their profiles constructed by an
information extraction tool. To match the pro-
files, a specialist ensemble algorithm predicts the
pairwise distance by aggregating the similarities of
the attributes and relationships in the profiles. We
evaluated the proposed methods with experiments
on a large benchmark collection and demonstrate
that the proposed methods compare favorably with
the top runs in the SemEval evaluation.
The focus of this work is on the novel learning
and clustering methods for coreference. Future
research directions include developing rich feature
sets and using corpus level or external informa-
tion. We believe that such efforts can further im-
prove cross document coreference performance.
421
References
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2007. The SemEval-2007 WePS evaluation:
Establishing a benchmark for the web people search
task. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-
2007), pages 64?69.
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector
space model. In Proceedings of 36th International
Conference On Computational Linguistics (ACL)
and 17th international conference on Computational
linguistics (COLING), pages 79?85.
Alex Baron and Marjorie Freedman. 2008. Who
is who and what is what: Experiments in cross-
document co-reference. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 274?283.
J. C. Bezdek. 1981. Pattern Recognition with Fuzzy
Objective Function Algoritms. Plenum Press, NY.
Razvan Bunescu and Marius Pasca. 2006. Using
encyclopedic knowledge for named entity disam-
biguation. In Proceedings of the 11th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL), pages 9?16.
Ying Chen and James Martin. 2007. Towards
robust unsupervised personal name disambiguation.
In Proc. of 2007 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning.
Mario G. C. A. Cimino, Beatrice Lazzerini, and
Francesco Marcelloni. 2006. A novel approach
to fuzzy clustering based on a dissimilarity relation
extracted from data using a TS system. Pattern
Recognition, 39(11):2077?2091.
William W. Cohen, Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of
string distance metrics for name-matching tasks.
In Proceedings of IJCAI Workshop on Information
Integration on the Web.
Paolo Corsini, Beatrice Lazzerini, and Francesco
Marcelloni. 2005. A new fuzzy relational clustering
algorithm based on the fuzzy c-means algorithm.
Soft Computing, 9(6):439 ? 447.
Rajesh N. Dave and Sumit Sen. 2002. Robust fuzzy
clustering of relational data. IEEE Transactions on
Fuzzy Systems, 10(6):713?727.
Yoav Freund, Robert E. Schapire, Yoram Singer, and
Manfred K. Warmuth. 1997. Using and combining
predictors that specialize. In Proceedings of the
twenty-ninth annual ACM symposium on Theory of
computing (STOC), pages 334?343.
Chung H. Gooi and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics
(NAACL), pages 9?16.
Jay J. Jiang and David W. Conrath. 1997.
Semantic similarity based on corpus statistics and
lexical taxonomy. In Proceedings of International
Conference Research on Computational Linguistics.
Xin Li, Paul Morie, and Dan Roth. 2004. Robust
reading: Identification and tracing of ambiguous
names. In Proceedings of the Human Language
Technology Conference and the North American
Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 17?24.
Gideon S. Mann and David Yarowsky. 2003.
Unsupervised personal name disambiguation. In
Conference on Computational Natural Language
Learning (CoNLL), pages 33?40.
Vincent Ng and Claire Cardie. 2001. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 104?111.
Cheng Niu, Wei Li, and Rohini K. Srihari. 2004.
Weakly supervised learning for cross-document
person name disambiguation supported by infor-
mation extraction. In Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics (ACL), pages 597?604.
Bernhard Scho?lkopf and Alex Smola. 2002. Learning
with Kernels. MIT Press, Cambridge, MA.
Sarah M. Taylor. 2004. Information extraction tools:
Deciphering human language. IT Professional,
6(6):28 ? 34.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag New York.
Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong
Ding. 2005. Person resolution in person search
results: WebHawk. In Proceedings of the 14th
ACM international conference on Information and
knowledge management (CIKM), pages 163?170.
Xuanli Lisa Xie and Gerardo Beni. 1991. A validity
measure for fuzzy clustering. IEEE Transactions
on Pattern Analysis and Machine Intelligence,
13(8):841 ? 847.
Xiaofeng Yang, Jian Su, Jun Lang, Chew L. Tan,
Ting Liu, and Sheng Li. 2008. An entity-
mention model for coreference resolution with
inductive logic programming. In Proceedings of
the 46th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 843?851.
Dao-Qiang Zhang and Song-Can Chen. 2003.
Clustering incomplete data using kernel-based fuzzy
c-means algorithm. Neural Processing Letters,
18(3):155 ? 162.
422
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 483?491,
Beijing, August 2010
Enhancing Cross Document Coreference of Web Documents
with Context Similarity and Very Large Scale Text Categorization
Jian Huang
Information Sciences and Technology
Pennsylvania State University
jhuang@ist.psu.edu
Pucktada Treeratpituk
Information Sciences and Technology
Pennsylvania State University
pxt162@ist.psu.edu
Sarah M. Taylor
Lockheed Martin IS&GS
sarah.m.taylor@lmco.com
C. Lee Giles
Information Sciences and Technology
Pennsylvania State University
giles@ist.psu.edu
Abstract
Cross Document Coreference (CDC) is
the task of constructing the coreference
chain for mentions of a person across a set
of documents. This work offers a holistic
view of using document-level categories,
sub-document level context and extracted
entities and relations for the CDC task.
We train a categorization component with
an efficient flat algorithm using thousands
of ODP categories and over a million web
documents. We propose to use ranked cat-
egories as coreference information, partic-
ularly suitable for web documents that are
widely different in style and content. An
ensemble composite coreference function,
amenable to inactive features, combines
these three levels of evidence for disam-
biguation.
A thorough feature importance study is
conducted to analyze how these three
components contribute to the coreference
results. The overall solution is evaluated
using the WePS benchmark data and
demonstrate superior performance.
1 Introduction
Cross Document Coreference (CDC) is the task
to determine whether Named Entities (NE) from
different documents refer to the same underlying
identity. CDC enables a range of advanced NLP
applications such as automated text summariza-
tion and question answering (e.g. list-type ques-
tions). CDC has mainly been developed from two
perspectives.
First, in the Message Understanding Confer-
ence (MUC-6), CDC was viewed as an advanced
task performed based on a set of Information
Extraction (IE) artifacts. IE has been one of the
central topics in NLP since the 1970s and gained
much success in transforming natural language
text to structured text. IE on the Web, however,
is inherently very challenging. For one, the Web
is comprised of such heterogenous content that
IE systems, many of which are developed on
tidy and domain-specific corpora, may achieve
relatively limited coverage. Also, the content of
web documents may not even be in the natural
language form. Hence, though IE based features
are quite precise, it is rather difficult to achieve
good coverage that?s necessary to disambiguate
person entities on the Web.
Recently, there is significant research interest in
a related task called Web Person Search (WePS)
(Artiles et al, 2007), which seeks to determine
whether two documents refer to the same person
given a person name search query. Many systems
employed the simple vector space model and word
co-occurrence features for this task. Though more
robust with better coverage, these methods are
more susceptible to irrelevant words with regard
to the entity of interest.
Rather than relying solely on IE based or word
co-occurrence features, this work adopts a holistic
view of the different types of features useful for
cross document coreference. Specifically, the
main features of our proposed CDC approach are:
483
? The proposed approach covers the entire
spectrum of document level, sub-document
context level and entity/relation level
disambiguation evidence. In particular,
we propose to use document categories
as robust document level evidence. This
comprehensive design naturally combines
state-of-the-art categorization, information
extraction and IE-driven IR methods and
compensates the limitation of each of them.
? The features used in this work are domain in-
dependent and thus are particularly suitable
for coreferencing web documents.
? The composite pairwise coreference function
in this work can readily incorporate a set
of heterogenous features that are not always
active or are in different ranges, making
it easily extensible to additional features.
Moreover, we thoroughly study the contri-
bution of each component and its features
to gain insight on improving cross document
coreference performance.
In this work, three components specialize in
generating the aforementioned three levels of fea-
tures as coreference decisions. Thus we refer to
them as experts. After reviewing prior work on
CDC, we describe the methods of each of these
components in detail and present empirical results
where appropriate. We then show how these
components (and its features) are aggregated to
predict pairwise coreference using an ensemble
method. We evaluate the contribution of each
component and the overall CDC results on a
benchmark dataset. Finally, we conclude and
discuss future work.
2 Related Work
Compared to the traditional (within-document)
coreference resolution problem, cross document
coreference is a much harder problem due to the
divergence of contents and the lack of consistent
discourse information across documents.
(Bagga and Baldwin, 1998b) presented one of
the first CDC systems, which relied solely on the
contextual words of the named entities. (Gooi
and Allan, 2004) used a 55-word window as
the context without significant accuracy penalty.
As these approaches only considered word co-
occurrence, they were more susceptible to genre
differences. Recent CDC work has sought Infor-
mation Extraction (IE) support. Extracted NEs
and relationships were considered in (Niu et al,
2004) for improved CDC performance.
Many of these earlier CDC methods were
evaluated on small and tidy news articles. CDC
for Web documents is even more challenging.
(Wan et al, 2005) proposed a web person
resolution system called WebHawk, which
extracted several attributes such as title,
organization, email and phone number using
patterns. These features however only covered
small amount of disambiguation evidence and
certain types of web pages (such as personal
home pages). The more recent Web Person
Search (WePS) task (Artiles et al, 2007) has
created a benchmark dataset which is also used
in this work. Different from CDC which aims to
resolve mention level NEs, WePS distinguishes
documents retrieved by a name search query
according to the underlying identity. The top-
performing system (Chen and Martin, 2007)
in this task extracted phrasal contextual and
document-level entities as rich features for
coreference. Similar IR features are also used by
other WePS systems as they are more robust to
the variety of web pages (Artiles et al, 2007).
Instead of focusing on local information, (Li
et al, 2004) proposed a generative model of
entity co-occurrence to capture global document
level information. However, inference in gen-
erative models is expensive for large scale web
data. Our work instead considers document cat-
egories/topics that can be efficiently predicted
and easily interpretable by users. Hand-tuned
weights were used in (Baron and Freedman, 2008)
and a linear classifier was used in (Li et al,
2004) to combine the extracted features. Our
composite pairwise coreference function is based
on an ensemble classifier and is more robust and
capable of handling inactive features.
3 Text Categorization Aided CDC
Consider the following scenario for motivation.
When a user searches for ?Michael Jordan?,
the official web page of the basketball player
484
?Michael Jordan?1 contains mostly his career
statistics, whereas the homepage of ?Michael
I. Jordan? the professor2 contains his titles,
contact information and advising students.
Neither of these pages contain complete natural
language sentences that most IE and NLP tools
are designed to process. We propose to use
document categories (trained from a very large
scale and general purpose taxonomy, Open
Directory Project (ODP)) as document level
features for CDC. In this example, one can easily
differentiate these namesakes by categorizing the
former as ?Top/Sports/Basketball/Professional?
and the latter as ?Top/Computer/Artificial
Intelligence/Machine Learning?. We first
introduce the method to categorize Web
documents; then we show how to combine
these categories for coreferencing.
3.1 Very Large Scale Text Categorization
To handle the web CDC problem, the catago-
rization component needs to be able to catego-
rize documents of widely different topics. The
Open Directory Project (ODP), the largest and
most comprehensive human edited directory of
the Web3, contains hundreds of thousands of
categories labeled for 2 million Web pages. Lever-
aging this vast amount of web data and the large
Web taxonomy has called for the development of
very efficient text categorization methods. There
is significant research interest in scaling up to
categorize millions of pages to thousands of cat-
egories and beyond, called the many class classi-
fication setting (Madani and Huang, 2008). Flat
classification methods (e.g. (Crammer et al,
2006; Madani and Huang, 2008)), which treat
hierarchical categories as flat classes, have been
very successful due to their superior scalability
and simplicity compared to classical hierarchical
one-against-rest categorization. Flat methods also
achieve high accuracy that is on par with, or better
than, the traditional counterparts.
We adopt a flat multiclass online classification
algorithm Passive Aggressive (PA) (Crammer et
al., 2006) to predict ranked categories for web
1See www.nba.com/playerfile/michael jordan/index.html
2See www.eecs.berkeley.edu/?jordan/
3See http://www.dmoz.org/about.html for details.
documents. For a categorization problem with C
categories, PA associates each category k with a
weight vector wk, called its prototype. The degree
of confidence for predicting category k with re-
spect to an instance x4 (both in online training and
testing) is determined by the similarity between
the instance and the prototype ? the inner product
wk ? x. PA predicts a ranked list of categories
according to this confidence.
PA is a family of online and large-margin based
classifiers. Given an instance (xt, yt) during
online learning, the multiclass margin marg in
PA5 is the difference between the score of the true
category yt and that of the highest ranked false
positive category s, i.e.
marg = wyt ? xt ?ws ? xt (1)
where s = argmaxs 6=yt ws ? xt.A positive margin value indicates that the algo-
rithm makes a correct prediction. One is however
not only satisfied with a positive margin value, but
also seeks to achieve a margin value of at least
1. When this is not satisfied, the online algorithm
suffers a multiclass hinge loss:
Lmc(w; (xt, yt)) =
{
0 marg ? 1
1?marg otherwise
where w = (w1, ..,wC) denotes the concatena-
tion of the C prototypes (into a vector).
In an online learning step, the PA-II variant
updates the category prototype with the solution
of this constrained optimization problem,
wt+1 = argmin
w
1
2 ?w ?wt?
2 +A?2 (2)
s.t. Lmc(w; (xt, yt)) ? ?. (3)
Essentially, if the margin is met (also imply-
ing no misclassification), PA passively accepts
the current solution. Otherwise, PA aggressively
learns the new prototype which satisfies the loss
constraint and stays as close to the one previously
learned as possible. To cope with label noise, PA-
II introduces a slack variable ? in the optimization
4x is the vector representation of word frequencies of the
corresponding document, L2 normalized.
5For brevity of presentation, we consider the single label
multiclass categorization setting.
485
for a gentler update, a technique previously em-
ployed to derive soft-margin classifiers (Vapnik,
1998). A is a parameter that controls the aggres-
siveness of the update.
The solution to the above optimization problem
amounts to only changing the two prototypes
violating the margin in the update step:
wytt+1 = w
yt
t + ?xt wst+1 = wst ? ?xt
where ? = Lmc?xt?2+ 12A .To conclude, PA treats the hierarchy as flat cat-
egories for multiclass classification. It is similar
to Multiclass Perceptron (Crammer and Singer,
2003) but only updates two vectors per iteration
and thus is more efficient.
3.2 Categories as Coreference Evidence
Conceptually, the text categorization component
can be viewed as a function that maps a document
d to a ranked list of top K categories along with
their respective confidence scores, i.e.
?(d) = {< c1, s1 >, .., < cK , sK >}
We leverage these document categories to mea-
sure the pairwise similarity of any two docu-
ments, sim(?(du), ?(dv)), for entity disambigua-
tion. Given a taxonomy T , we first formally
define the affinity between a category c and one
of its ancestor category c? in T as:
affinity(c; c?) = 1? len(c, c
?)
depth(T )
where len is the length of the shortest path be-
tween the two categories and depth(T) denotes the
depth of the taxonomy. In other words, affinity is
the complementary of the normalized path length
between c and its ancestor c?.
Using graph theory terminology, LCA(c1, c2)
denote the lowest common ancestor of two cate-
gories c1 and c2 in T . Given two category lists,
?(du) = {< cu1 , su1 >, .., < cuK , suK >} and
?(dv) = {< cv1, sv1 >, .., < cvK , svK >}, we use
the LCA(cui , cvj ) of each category pair cui and cvj
as the basis to measure similarity. Formally, we
transform ?(du) to a K ?K dimensional vector:
~v(du) = [affinity(cui ;LCA(cui , cvj )) ? sui ]T (4)
where i, j = 1..K. In other words, we project
?(du) into a vector in the space spanned by the
LCAs of category pairs. Using the same bases,
we can derive ~v(dv) analogically.
With this transformation, ?(du) and ?(dv)
are expressed in the common bases, i.e. their
LCAs. Therefore, the similarity between the top
K categories of two documents can be measured
by the inner product of these two vectors:
sim(?(du), ?(dv)) = ~v(du) ? ~v(dv) (5)
3.3 Empirical Studies
To handle the diverse topics of Web documents,
we leverage the ODP data to train the many class
categorization algorithm. The public ODP data
contains 361,621 categories and links to over 2
million pages. We crawled the original web pages
from these links, which yielded 1.9 million pages
(50GB in size). The taxonomy was condensed to
depth three6 and then very rare categories (having
less than 5 instances) were discarded. The data
set is created with these categories and the vector
representation of the term weights of the extracted
raw text. This dataset has 1,889,683 instances and
4,891 categories in total. Finally, stratified 80-
20 split was performed on this dataset, i.e. 1.5M
pages for training and 377K pages for testing.
Figure 1: Categorization performance at different
positions in the ODP test set.
As we view the taxonomy as a set of flat
categories and we are interested in the top K
categories, we use the recall at K metric for eval-
uation. Recall at K is defined as the percentage
of instances having their true category ranked
6The original taxonomy has average depth 7, which is
too deep for the coreference purpose in this work and many
categories have too few instances for training.
486
among the top K slots in the category list. For
a single label dataset (most ODP pages have one
category) and K = 1, this is the accuracy metric
in multiclass classification. Note that in the many
class setting, recall at 1 is a very strict metric
as no credit is given for predicting the parent,
children or sibling categories; also, documents
may have valid secondary topics not labeled by
humans. Figure 1 shows recall at K in the test
set. We observe that the algorithm is able to
predict the category for 58.7% of the instances
in the first rank and more than 77% in top three.
There is only diminishing gains when we consider
the categories further down the list. Hence we
choose to use the similarity of the top 1 and top
3 categories (named TC1 and TC3, respectively)
and study their contributions for the CDC task.
3.4 Remarks
In this section, the entire document in the rep-
resentation of its categories is used as a unit
of analysis for CDC. Categorization based CDC
works best with namesakes appearing in docu-
ments of relatively heterogenous topics, which
is usually the case for web documents. Indeed,
experienced web searchers would add terms such
as ?baseball player? to the name search queries for
more relevant results; Wikipedia also (manually)
disambiguates namesakes by their professions.
Categorization can also be adopted as a robust
faceted search system for handling name search
queries: users select the interested category/facet
to efficiently disambiguate and filter out irrelevant
results. The majority of web persons can be
readily distinguished by the different underlying
categories of the documents where they appear.
For more homogeneous corpora or less benevolent
cases, the next sections introduce two comple-
mentary CDC strategies.
4 Information Extraction for CDC
Consider the following two snippets retrieved
with regard to the query ?George Bush?:
[Snippet 1]: ?George W. Bush and Bill Clinton
are trying to get Congress to allow Haiti to triple
the number of exports ...?
[Snippet 2]: ?George H. W. Bush succeeded
Reagan as the 41st U.S. President.?
Using categories alone in this case is insuffi-
cient as both will be assigned similar categories
such as ?Politics? or ?History/U.S.?. Also, it?s not
uncommon for these entities to co-occur in the
same document and thus making them even more
confounding. Properly disambiguating these two
mentions requires the usage of local informa-
tion: for instance, the extraction of full names,
the detection of co-occurring NEs and contextual
information. We introduce an IE system that
extracts precise disambiguation evidence in this
section and describe using the extraction context
as additional information in the next section.
Our CDC system leverages a state-of-the-art
commercial IE system AeroText (Taylor, 2004).
The IE system employs manually created knowl-
edge bases with statistically trained models to
extract named entities, detect, classify and link
relations between NEs. A summary of the most
important IE-based features that we use are listed
in Table 1. Based on the extracted attributes and
relations, we further define their pairwise simi-
larity used as coreference features. This ranges
from simple compatibility checking for ?gender?,
textual soft matching for ?names?, to sophisticated
semantic matching for ?mentions? and ?locations?
using WordNet. (Huang et al, 2009) provides
more detailed discussions on the development of
these IE based coreference features.
We note that several existing state-of-the-art
IE systems are also capable of extracting these
features. In particular, Named Entity Recognition
(NER) which focuses on a small set of predefined
categories of named entities (e.g. persons, orga-
nization, location) as well as the detection and
tracking of preselected relations have achieved
venerable empirical success in practice7. Also,
within document coreference is a mature and
well-studied technology in NLP (e.g. (Ng and
Cardie, 2002)). Therefore, our CDC system can
readily adopt alternative IE toolkits.
5 Context Matching
As mentioned earlier, achieving high extraction
accuracy and coverage for diverse web documents
7The Automatic Content Extraction (ACE) evaluation
and the Text Analysis Conference (TAC) also have IE-based
entity tracking tasks that are relevant to this component.
487
is still a challenging and open research problem
even for the state-of-the-art IE systems. We note
that one of the natural outcomes from extraction is
the context of the NE of interest, which covers the
NE with its surrounding text. For a specific NE,
our CDC system uses the context built from the
sentences which form the NE?s within document
coreference chain. The context is then represented
as a term vector whose terms are weighted by the
TF-IDF weighing scheme. For a pair of NEs, the
context matching component measures the cosine
similarity of their context term vectors.
Essentially, this component alone is similar to
the method presented in the seminal CDC work
in (Bagga and Baldwin, 1998b). We however note
that simply applying a predetermined threshold on
the context similarity for CDC as in this earlier
work is not sufficient. First, this method narrowly
focuses on the local word occurrence and may
miss the big picture, i.e. the correlation that exists
in the global scope of a document. Also, mere
word occurrence is incapable of accounting for the
variation of word choices or placing special em-
phases on evidence such as co-occurring named
entities, relations, etc. The categorization and IE
components presented earlier in this work over-
come these two pitfalls of the simple IR-based
approach. We will further showcase the advantage
of our comprehensive approach in section 7.2.
6 Composite Pairwise Coreference
In the previous sections, we describe the com-
ponents to obtain document, sub-document and
entity level disambiguation evidence in detail. In
this section, we propose to use Random Forest
(RF) to combine the experts components into one
single composite pairwise similarity score. RF is
an ensemble classifier, composed of a collection
of randomized decision trees (Breiman, 2001).
Each randomized tree is built on a different boot-
strap sample of the training data. Randomness is
also introduced into the tree construction process:
the variable selection for each split is conducted
not on the entire feature set, but from a small
random subset of features. Gini index is used as
the criteria in selecting the best split. Additionally,
each tree is unpruned, to keep the prediction
bias low. By aggregating many trees that are
lowly-correlated (through bootstrap sampling and
random variable selection), RF also reduces the
prediction variance.
An ensemble method such as Random Forests
is very suitable for the CDC task. First, the col-
lection of randomized decision trees is analogous
to a panel of different experts, where each makes
its decision using different criteria and different
features. Previously, RF has been used to aggre-
gate various features in the author disambiguation
task (Treeratpituk and Giles, 2009). One of the
significant challenges in combining these different
features in our CDC setting is that not all of them
are always active. For instance, the IE tool may
extract an employment relation for one entity and
a list relation for another. Also, when the IE
tool cannot infer the gender information or when
the categorization component does not confidently
predict the top K categories (e.g. all with low
scores), it?s desirable to not supply those features
for coreferencing. The traditional technique to
impute the missing values, e.g. by replacing them
with the mean value, is not suitable in this case.
In our work, we specify a special level ?NA? in
the decision tree base learner. In our development
set, this treatment improves pairwise coreference
accuracy by more than 6%.
Figure 2 shows the convergence plot of the
composite pairwise coreference function based on
Random Forest8. We observe that the Out-Of-Bag
8The R random forest (Liaw and Wiener, 2002) was used.
Figure 2: Convergence of OOB errors of the
composite pairwise coreference function using the
training portion of the WePS dataset.
488
(OOB) errors 9 drastically decrease with the first
50 trees and then level off (without signs of over-
fitting). Thus we choose to use the model built
with the first 100 trees for prediction. Overall, our
model can achieve more than 85% accuracy for
pairwise coreference prediction.
7 Experiments
We evaluate our CDC approach with the bench-
mark dataset from the ACL-2007 SemEval Web
Person Search (WePS) evaluation campaign (Ar-
tiles et al, 2007). The WePS task is: given a name
search query, cluster the search result documents
according to the underlying referents. Compared
to the CDC task which clusters mention level
entities, a simplifying assumption is made in this
task that each document refers to only one identity
with respect to the query. The WePS dataset
contains the training and test set. The training
set contains the top 100 web search results of
49 names from the Web03 corpus (Mann and
Yarowsky, 2003), Wikipedia and European Con-
ference on Digital Library (ECDL) participants;
the test data are comprised of the top 100 docu-
ments of 30 names from Wikipedia, US Census
and ACL participants.
Table 1: Expert component and their feature sets.
Feature Component Description
TC1 Categorization Sim. of the top 1 categoriesTC3 Sim. of the top 3 categories
CNTX Context Sim. of context
NAME
IE (attribute)
Sim. of full/first/last names
MENT Sim. of mentions
GEND Sim. of genders
EMP
IE (relation)
Sim. of full/first/last names
LIST Sim. of co-occurring persons
LOC Sim. of locations
FAM Sim. of family members
7.1 Evaluation of Pairwise Coreference
We conduct a thorough study of the importance
of the individual expert components and their
features with the WePS training set. Table 1 shows
the three components of the systems, their main
features and descriptions.
The importance of these expert components and
their features are illustrated in Figure 3. One of
9OOB error is an unbiased estimate of test error in RF
(Breiman, 2001), computed as the average misclassification
rates of each tree with samples not used for its construction.
Figure 3: Importance of the expert components
and their features found by Random Forest (note
the small spread in MeanDecreaseAccuracy).
the most important features is CNTX, this confirms
that the prior work on CDC (e.g. (Bagga and
Baldwin, 1998b)) can achieve good results with
the IE-driven context similarity feature (or its vari-
ation). The text categorization component also
contributes very important features. In particular,
TC3 is more significant than TC1 for reducing
the Gini index because it recalls more correct
categories. On the other hand, TC1 is slightly
more important than TC3 for its contribution to
accuracy, indicating TC1 is more precise (with
less noise categories). For the IE component,
attribute features NAME and MENT are the most
useful. As aforementioned, the IE component
may not always extract the relation features such
as EMP, LIST, LOC and FAM, and hence they
seemingly have limited effect on model learning
(with relatively low reduction in Gini index).
These relation features are however very accu-
rate when extracted and are present for predic-
tion. Therefore, they are strong disambiguation
evidence and their removal would significantly
hamper performance.
7.2 Evaluation for Web Person Search
Using the confidence of the pairwise corefer-
ence prediction as a distance metric, we adopt a
density-based clustering method DBSCAN (Ester
et al, 1996) as in (Huang et al, 2006)10 to induce
the person clusters. The final set of evaluation is
based on these person clusters generated for the
WePS test set.
Two sets of metrics are used to evaluate the
overall system. First, we use the B-CUBED
10DBSCAN is a robust and scalable algorithm suitable
for clustering relational data. In interest of space, we refer
readers to (Ester et al, 1996) for the original algorithm.
489
Table 2: Cross document coreference perfor-
mance (I. Pur. denotes inverse purity).
Method Purity I. Pur. F B-CUBED
CDC 0.812 0.796 0.793 0.775
CNTX 0.863 0.601 0.678 0.675
TC1+3 0.620 0.776 0.660 0.634
OIO 1.000 0.482 0.618 0.618
AIO 0.279 1.000 0.389 0.238
scores designed in (Bagga and Baldwin, 1998a)
for evaluating cross document coreference perfor-
mance. Second, we use the purity, inverse purity
and their F score as in WePS (Artiles et al, 2007).
Purity penalizes placing noise entities in a cluster,
while inverse purity penalizes splitting coreferent
entities into separate clusters.
Table 2 shows the performance of the
macro-averaged cross document coreference
performance on the WePS test sets. Note that
though our evaluation is based on the mention
level entities, the baselines One-In-One (OIO,
placing each entity in a separate cluster) and All-
In-One (AIO, putting all entities in one cluster)
have almost identical results as those in the
evaluation11. OIO can yield good performance,
indicating that the names in test data are highly
ambiguous. As alluded to in the title, context and
categories both are very useful disambiguation
features. CNTX is essentially very similar to the
system presented in (Bagga and Baldwin, 1998b)
and is a strong baseline12 (outperforming 3/4
of the systems in WePS). Note that CNTX has
high purity but inferior inverse purity, indicating
that using the context extracted by the IE system
alone is unable to link many coreferent entities.
Interestingly, we observe that using only the
top-K categories (TC1+3) can also achieve
competitive F score, though in a very different
manner. TC1+3 recalls much more coreferent
entities (significantly improving inverse purity),
but at the same time also introduces noise.
Finally, adding document categories and using
IE results (i.e. using all features in Table 1),
our CDC system achieves 22% and 18% relative
11Most person names in this set have only one underlying
identity per document; thus the results are comparable
despite the simplifying assumption of the WePS evaluation.
12We use context similarity 0.2 as the clustering threshold
(which has the best performance in training data).
improvement compared to CNTX in F (purity)
and B-CUBED scores, respectively. In particular,
inverse purity improves by 46% relatively, imply-
ing that the additional evidence significantly im-
proves the recall of coreferent entities (when there
is a lack of context similarity in the traditional
method). Overall, the comprehensive approach
in this work outperforms the top-tiered systems in
the WePS evaluation.
8 Conclusion and Future Work
This work proposes a synergy of three levels of
analysis for the web cross document coreference
task. On the document level, we use text cate-
gories, trained from thousands of ODP categories
and over a million pages, as a concise representa-
tion of the documents. Categorization is a robust
strategy for coreferencing web documents with
diverse topics, formats and when there is a lack of
extraction coverage or word matching. Two types
of sub-document level evidence are also used in
our approach. First, we apply an information ex-
traction system to extract attributes and relations
of named entities from the documents and per-
form within document coreference. Second, we
use the context of the entities, a natural outcome
of the IE system as a focused description of the
named entity that may miss the extraction process.
A CDC system has been implemented based on
the IE and the text categorization components
to provide a comprehensive solution to the web
CDC task. We demonstrate the importance of
each component in our system and benchmark
our system with the WePS dataset which shows
superior CDC performance.
There are a number of interesting directions for
future research. Recently, Open IE was proposed
in (Etzioni et al, 2008) for Web information
extraction. This can be a more powerful alter-
native to traditional IE toolkits for Web CDC,
though measuring the semantic similarity for a
vast variety of relations can be another research
issue. Employing external background knowledge
such as Wikipedia (Han and Zhao, 2009) while
maintaining scalability can also be an orthogonal
direction for further improvement.
490
References
Artiles, Javier, Julio Gonzalo, and Satoshi Sekine.
2007. The SemEval-2007 WePS evaluation:
Establishing a benchmark for the web people search
task. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval),
pages 64?69.
Bagga, Amit and Breck Baldwin. 1998a. Algorithms
for scoring coreference chains. In First Inter-
national Conference on Language Resources and
Evaluation Workshop on Linguistics Coreference.
Bagga, Amit and Breck Baldwin. 1998b. Entity-based
cross-document coreferencing using the vector
space model. In Proceedings of the 36th ACL and
17th COLING, pages 79?85.
Baron, Alex and Marjorie Freedman. 2008. Who
is who and what is what: experiments in cross-
document co-reference. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 274?283.
Breiman, Leo. 2001. Random forests. Machine
Learning, 45(1):5?32.
Chen, Ying and James Martin. 2007. Towards robust
unsupervised personal name disambiguation. In
Proc. of EMNLP and CoNLL, pages 190?198.
Crammer, Koby and Yoram Singer. 2003. A family of
additive online algorithms for category ranking. J.
Machine Learning Research, 3:1025?1058.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research (JMLR), 7:551?585.
Ester, M., H. Kriegel, J. Sander, and X. Xu. 1996. A
density-based algorithm for discovering clusters in
large spatial databases with noise. In Proceedings
of the 2nd KDD Conference, pages 226 ? 231.
Etzioni, Oren, Michele Banko, Stephen Soderland,
and Daniel S. Weld. 2008. Open information
extraction from the web. Communications of ACM,
51(12):68?74.
Gooi, Chung H. and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
Proceedings of HLT-NAACL 2004, pages 9?16.
Han, Xianpei and Jun Zhao. 2009. Named entity
disambiguation by leveraging Wikipedia semantic
knowledge. In Proceedings of the 18th Conf. on
Information and knowledge management (CIKM),
pages 215?224.
Huang, Jian, Seyda Ertekin, and C. Lee Giles.
2006. Efficient name disambiguation for large scale
databases. In Proc. of 10th European Conference
on Principles and Practice of Knowledge Discovery
in Databases (PKDD), pages 536 ? 544.
Huang, Jian, Sarah M. Taylor, Jonathan L. Smith,
Konstantinos A. Fotiadis, and C. Lee Giles. 2009.
Profile based cross-document coreference using
kernelized soft relational clustering. In Proceedings
of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 414?422.
Li, Xin, Paul Morie, and Dan Roth. 2004. Robust
reading: Identification and tracing of ambiguous
names. In Proceedings of the Human Language
Technology Conference and the North American
Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 17?24.
Liaw, Andy and Matthew Wiener. 2002. Classification
and regression by randomforest. R News, 2(3).
Madani, Omid and Jian Huang. 2008. On updates
that constrain the features? connections during
learning. In Proceedings of the 14th ACM SIGKDD
International Conference on Knowledge Discovery
& Data Mining (KDD), pages 515?523.
Mann, Gideon S. and David Yarowsky. 2003.
Unsupervised personal name disambiguation. In
Proceedings of the seventh conference on Natural
language learning (CoNLL), pages 33?40.
Ng, Vincent and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to im-
prove coreference resolution. In Proceedings of the
19th International Conference on Computational
Linguistics (COLING), pages 1?7.
Niu, Cheng, Wei Li, and Rohini K. Srihari.
2004. Weakly supervised learning for cross-
document person name disambiguation supported
by information extraction. In Proceedings of
the 42nd Annual Meeting on Association for
Computational Linguistics (ACL), pages 598?605.
Taylor, Sarah M. 2004. Information extraction tools:
Deciphering human language. IT Professional,
6(6):28 ? 34.
Treeratpituk, Pucktada and C. Lee Giles. 2009.
Disambiguating authors in academic publications
using random forests. In Proceedings of the
ACM/IEEE Joint Conference on Digital libraries
(JCDL), pages 39?48.
Vapnik, V. 1998. Statistical Learning Theory. John
Wiley and Sons, Inc., New York.
Wan, Xiaojun, Jianfeng Gao, Mu Li, and Binggong
Ding. 2005. Person resolution in person search
results: WebHawk. In Proceedings of the 14th
ACM International Conference on Information and
Knowledge management (CIKM), pages 163?170.
491
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1038?1046,
Beijing, August 2010
Modeling Socio-Cultural Phenomena in Discourse 
Tomek Strzalkowski1,2, George Aaron Broadwell1, Jennifer Stromer-Galley1, Samira Shaikh1, Sarah Taylor3 and Nick Webb1 1ILS Institute, University at Albany, SUNY 2IPI, Polish Academy of Sciences 3Lockheed Martin Corporation tomek@albany.edu 
 
Abstract In this paper, we describe a novel ap-proach to computational modeling and understanding of social and cul-tural phenomena in multi-party dia-logues. We developed a two-tier ap-proach in which we first detect and classify certain social language uses, including topic control, disagreement, and involvement, that serve as first order models from which presence the higher level social constructs such as leadership, may be inferred.  1. Introduction We investigate the language dynamics in small group interactions across various set-tings. Our focus in this paper is on English online chat conversations; however, the mod-els we are developing are more universal and applicable to other conversational situations: informal face-to-face interactions, formal meetings, moderated discussions, as well as interactions conducted in languages other than English, e.g., Urdu and Mandarin.  Multi-party online conversations are particu-larly interesting because they become a per-vasive form of communication within virtual communities, ubiquitous across all age groups. In particular, a great amount of communica-tion online occurs in virtual chat-rooms, typi-cally conducted using a highly informal text dialect. At the same time, the reduced-cue environment of online interaction necessitates more explicit linguistic devices to convey social and cultural nuances than is typical in face-to-face or even voice conversations.  Our objective is to develop computational models of how certain social phenomena such as leadership, power, and conflict are signaled and reflected in language through the choice of lexical, syntactic, semantic and conversa-tional forms by discourse participants. In this 
paper we report the results of an initial phase of our work during which we constructed a prototype system called DSARMD-1 (De-tecting Social Actions and Roles in Multi-party Dialogue). Given a representative seg-ment of multiparty task-oriented dialogue, DSARMD-1 automatically classifies all dis-course participants by the degree to which they deploy selected social language uses, such as topic control, task control, involve-ment, and disagreement. These are the mid-level social phenomena, which are de-ployed by discourse participants in order to achieve or assert higher-level social con-structs, including leadership. In this work we adopted a two-tier empirical approach where social language uses are modeled through observable linguistic features that can be automatically extracted from dialogue. The high-level social constructs are then inferred from a combination of language uses attrib-uted to each discourse participant; for exam-ple, a high degree of influence and a high de-gree of involvement by the same person may indicate a leadership role. In this paper we limit our discussion to the first tier only: how to effectively model and classify social lan-guage uses in multi-party dialogue.  2. Related Research Issues related to linguistic manifestation of social phenomena have not been systemati-cally researched before in computational lin-guistics; indeed, most of the effort thus far was directed towards the communicative di-mension of discourse. While the Speech Acts theory (Austin, 1962; Searle, 1969) provides a generalized framework for multiple levels of discourse analysis (locution, illocution and perlocution), most current approaches to dia-logue focus on information content and structural components (Blaylock, 2002; Car-berry & Lambert, 1999; Stolcke, et al, 2000) in dialogue; few take into account the effects that speech acts may have upon the social 
1038
roles of discourse participants. Also relevant is research on modeling sequences of dia-logue acts ? to predict the next one (Samuel et al 1998; Ji & Bilmes, 2006 inter alia) ? or to map them onto subsequences or ?dialogue games? (Carlson 1983; Levin et al, 1998), which are attempts to formalize participants? roles in conversation (e.g., Linell, 1990; Poe-sio &?Mikheev, 1998; Field et al, 2008). There is a body of literature in anthropology, linguistics, sociology, and communication on the relationship between language and power, as well as other social phenomena, e.g., con-flict, leadership; however, existing ap-proaches typically look at language use in situations where the social relationships are known, rather than using language predic-tively. For example, conversational analysis (Sacks et al, 1974) is concerned with the structure of interaction: turn-taking, when interruptions occur, how repairs are signaled, but not what they reveal about the speakers. Research in anthropology and communication has concentrated on how certain social norms and behaviors may be reflected in language (e.g., Scollon and Scollon, 2001; Agar, 1994) with few systematic studies attempting to ex-plore the reverse, i.e., what the linguistic phenomena tell us about social norms and behaviors.  3. Data & Annotation Our initial focus has been on on-line chat dialogues. While chat data is plentiful on-line, its adaptation for research purposes presents a number of challenges that include users? pri-vacy issues on the one hand, and their com-plete anonymity on the other. Furthermore, most data that may be obtained from public chat-rooms is of limited value for the type of modeling tasks we are interested in due to its high-level of noise, lack of focus, and rapidly shifting, chaotic nature, which makes any longitudinal studies virtually impossible. To derive complex models of conversational be-havior, we need the interaction to be reasona-bly focused on a task and/or social objectives within a group. Few data collections exist covering multiparty dialogue, and even fewer with on-line chat. Moreover, the few collections that exist were built primarily for the purpose of training dialogue act tagging and similar linguistic phenomena; few if any of these corpora are 
suitable for deriving pragmatic models of conversation, including socio-linguistic phe-nomena. Existing resources include a multi-person meeting corpus ICSI-MRDA and the AMI Meeting Corpus (Carletta, 2007), which contains 100 hours of meetings cap-tured using synchronized recording devices. Still, all of these resources look at spoken language rather than on-line chat. There is a parallel interest in the online chat environ-ment, although the development of useful re-sources has progressed less. Some corpora exist such as the NPS Internet chat corpus (Forsyth and Martell, 2007), which has been hand-anonymized and labeled with part-of-speech tags and dialogue act labels. The StrikeCom corpus (Twitchell et al, 2007) consists of 32 multi-person chat dialogues between players of a strategic game, where in 50% of the dialogues one participant has been asked to behave ?deceptively?. It is thus more typical that those interested in the study of Internet chat compile their own corpus on an as needed basis, e.g., Wu et al (2002), Khan et al (2002), Kim et al (2007).  Driven by the need to obtain a suitable dataset we designed a series of experiments in which recruited subjects were invited to participate in a series of on-line chat sessions in a spe-cially designed secure chat-room. The ex-periments were carefully designed around topics, tasks, and games for the participants to engage in so that appropriate types of behav-ior, e.g., disagreement, power play, persuasion, etc. may emerge spontaneously. These ex-periments and the resulting corpus have been described elsewhere (Shaikh et al, 2010b), and we refer the reader to this source. Ulti-mately a corpus of 50 hours of English chat dialogue was collected comprising more than 20,000 turns and 120,000 words. In addition we also assembled a corpus of 20 hours of Urdu chat.  A subset of English language dataset has been annotated at four levels: communication links, dialogue acts, local topics and meso-topics (which are essentially the most persistent lo-cal topics). Although full details of these an-notations are impossible to explain within the scope of this article, we briefly describe them below. Annotated datasets were used to de-velop and train automatic modules that detect and classify social uses of language in dis-course. It is important to note that the annota-
1039
tion has been developed to support the objec-tives of our project and does not necessarily conform to other similar annotation systems used in the past.  ? Communicative links. In a multi-party dia-logue an utterance may be directed towards a specific participant, a subgroup of par-ticipants or to everyone.  ? Dialogue Acts. We developed a hierarchy of 15 dialogue acts for annotating the func-tional aspect of the utterance in discussion.  The tagset we adopted is based on DAMSL (Allen & Core, 1997) and SWBD (Jurafsky et al, 1997), but compressed to 15 tags tuned significantly towards dialogue prag-matics and away from more surface char-acteristics of utterances (Shaikh et al, 2010a).  ? Local topics. Local topics are defined as nouns or noun phrases introduced into dis-course that are subsequently mentioned again via repetition, synonym, or pronoun.  ? Topic reference polarity. Some topics, which we call meso-topics, persist through a number of turns in conversation. A selec-tion of meso-topics is closely associated with the task in which the discourse par-ticipants are engaged. Meso-topics can be distinguished from the local topics because the speakers often make polarized state-ments about them.  4. Socio-linguistic Phenomena We are interested in modeling the social phe-nomena of Leadership and Power in discourse. These high-level phenomena (or Social Roles, SR) will be detected and attributed to dis-course participants based on their deployment of selected Language Uses (LU) in multi-party dialogue. Language Uses are mid-level socio-linguistic devices that link linguistic components deployed in discourse (from lexical to pragmatic) to social con-structs obtaining for and between the partici-pants. The language uses that we are currently studying are Agenda Control, Disagreement, and Involvement (Broadwell et al, 2010). Our research so far is focused on the analysis of English-language synchronous chat, and we are looking for correlations between vari-ous metrics that can be used to detect LU in multiparty dialogue. We expect that some of these correlations may be culturally specific or language-specific, as we move into the 
analysis of Urdu and Mandarin discourse in the next phase of this project. 4.1 Agenda Control in Dialogue Agenda Control is defined as efforts by a member or members of the group to advance the group?s task or goal. This is a complex LU that we will model along two dimensions: (1) Topic Control and (2) Task Control. Topic Control refers to attempts by any discourse participants to impose the topic of conversa-tion. Task Control, on the other hand, is an effort by some members of the group to de-fine the group?s project or goal and/or steer the group towards that goal. We believe that both behaviors can be detected using scalar measures per participant based on certain linguistic features of their utterances. For example, one hypothesis is that topic control is indicated by the rate of local topic introductions (LTI) per participant (Givon, 1983). Local topics may be defined quite simply as noun phrases introduced into dis-course, which are subsequently mentioned again via repetition, synonym, pronoun, or other form of co-reference. Thus, one meas-ure of topic control is the number of local topics introduced by each participant as per-centage of all local topics in a discourse.  Using an LTI index we can construct asser-tions about topic control in a discourse. For example, suppose the following information is discovered about the speaker LE in a multi-party discussion dialogue-11 where 90 local topics are identified: 1. LE introduces 23/90 (25.6%) of local top-ics in this dialogue. 2. The mean rate of local topic introductions is this dialogue is 14.29%, and standard deviation is 8.01. 3. LE is in the top quintile of participants for introducing new local topics We can now claim the following, with a de-gree of confidence (to be determined): TopicControlLTI (LE, 5, dialogue-1) We read this as follows: speaker LE exerts the highest degree of topic control in dialogue-1. Of course, LTI is just one source of evidence and we developed other metrics to comple-ment it. We mention three of them here:                                                 1 Dialogue-1 refers to an actual dataset of 90-minute chat among 7 participants, covering approximately 700 turns. The task is to select a candidate for a job given a set of resumes. 
1040
? SMT Index. This is a measure of topic con-trol suggested in (Givon, 1983) and it is based on subsequent mentions of already introduced local topics. Speakers who in-troduce topics that are discussed at length by the group tend to control the topic of the discussion. The subsequent mentions of lo-cal topics (SMT) index calculates the per-centage of second and subsequent refer-ences to the local topics, by repetition, synonym, or pronoun, relative to the speakers who introduced them.  ? Cite Score. This index measures the extent to which other participants discuss topics introduced by that speaker. The difference between SMT and CiteScore is that the lat-ter reflect to what degree a speaker?s efforts to control the topic are assented to by other participants in a conversation. ? TL Index (TL). This index stipulates that more influential speakers take longer turns than those who are less influential. The TL index is defined as the average number of words per turn for each speaker. Turn length also reflects the extent to which other participants are willing to ?yield the floor? in conversation. Like LTI, all the above indices are mapped into a degree of topic control, based on quin-tiles in normal distribution (Table 1).   
 
LTI SMT CS TL AVG LE 5 5 5 5 5.00 JR 4 4 4 3 3.75 KI 4 3 3 1 2.75 KN 3 5 4 4 4.00 KA 2 2 2 4 2.50 CS 2 2 2 2 2.00 JY 1 1 1 2 1.25 Table 1: Topic Control distribution in dialogue-1. Each row represents a speaker in the group (LE, JR, etc.). Columns show indices used, with degrees per speaker on 5-point scale based on quintiles in normal distribu-tion, and the average value. Ideally, all the above indices (and others yet to be defined) should predict the same out-come, i.e., for each dialogue participant they should assign the same degree of topic control, relative to other speakers. This is not always the case, and where the indices divert in their predictions, our level of confidence in the generated claims decreases. We are currently 
working on how these different metrics cor-relate to each other and how they should be weighted to maximize accuracy of making Topic Control claims. Nonetheless, we can already output a Topic Control map (shown in Table 1) that captures a sense of internal so-cial dynamics within the group.  The other aspect of Agenda Control phe-nomenon is Task Control. It is defined as an effort to determine the group's goal and/or steer the group towards that goal. Unlike Topic Control, which is imposed by influenc-ing the subject of conversation, Task Control is gained by directing other participants to perform certain tasks or accept certain opin-ions. Consequently, Task Control is detected by observing the usage of certain dialogue acts, including Action-Directive, Agree-Accept, Disagree-Reject, and related categories. Here again, we define several in-dices that allow us to compute a degree of Task Control in dialogue for each participant: ? Directive Index (DI). The participant who directs others is attempting to control the course of the task that the group is per-forming. We count the number of directives, i.e., utterances classified as Ac-tion-Directive, made by each participant as a percentage of all directives in discourse. ? Directed Topic Shift Index (DTSI). When a participant who controls the task offers a directive on the task, then the topic of con-versation shifts. In order to detect this con-dition, we calculate the ratio of coincidence of directive dialogue acts by each partici-pant with topic shifts following them.  ? Process Management index (PMI). Another measure of Task Control is the proportion of turns each participant has that explicitly address the problem solving process. This includes utterances that involve coordinat-ing the activities of the participants, plan-ning the order of activities, etc. These fall into the category of Task (or Process) Management in most DA tagging systems.  ? Process Management Success Index (PMSI). This index measures the degree of success by each speaker at controlling the task. A credit is given to the speaker whose suggested curse of action is supported by other speakers for each response that sup-ports the suggestion. Conversely, a credit is taken away for each response that rejects or 
1041
qualifies the suggestion. PMSI is computed as distribution of task management credits among the participants over all dialogue utterances classified as Task/Process Man-agement. 2 As an example, let?s consider the following information computed for the PMI index over dialogue-1:  1. Dialogue-1 contains 246 utterances classi-fied as Task/Process Management rather than doing the task. 2. Speaker KI makes 65 of these utterances for a PMI of 26.4%. 3. Mean PMI for participants is 14.3%; 80th percentile is >21.2%. PMI for KI is in the top quintile for all participants. Based on this evidence we may claim (with yet to be determined confidence) that: TaskControlPMI(KI, 5, dialogue-1) This may be read as follows: speaker KI ex-erts the highest degree of Task Control in dialogue-1. We note that Task Control and Topic Control do not coincide in this dis-course, at least based on the PMI index. Other index values for Task Control may be com-puted and tabulated in a way similar to LTI in Table 1. We omit these here due to space limitations. 4.2 Disagreement in Dialogue Disagreement is another language use that correlates with speaker?s power and leader-ship. There are two ways in which disagree-ment is realized: expressive disagreement and topical disagreement (Stromer-Galley, 2007; Price, 2002). Both can be detected using sca-lar measures applied to subsets of participants, typically any two participants. In addition, we can also measure for each participant the rate with which he or she generates disagreement (with any and all other speakers). Expressive Disagreement is normally understood at the level of dialogue acts, i.e., when discourse participants make explicit utterances of dis-agreement, disapproval, or rejection in re-sponse to a prior speaker?s utterance. Here is an example (KI and KA are two speakers in a multiparty dialogue in which participants                                                 2 The exact structure of the credit function is still being deter-mined experimentally. For example, more credit may be given to first supporting response and less for subsequent responses; more credit may be given for unprompted suggestions than for those that were responding to questions from others. 
discuss candidates for a youth counselor job): KA: CARLA... women are always better with kids KI: That?s not true! KI: Men can be good with kids too While such exchanges are vivid examples of expressive disagreement, we are interested in more sustained phenomenon where two speakers repeatedly disagree, thus revealing a social relationship between them. Therefore, one measure of Expressive Disagreement that we consider is the number of Disagree-Reject dialogue acts between any two speakers as a percentage of all utterances exchanged be-tween these two speakers. This becomes a basis for the Disagree-Reject Index (DRX). In dialogue-1 we have: 1. Speakers KI and KA have 47 turns between them. Among these there are 8 turns classi-fied as Disagree-Reject, for the DRX of 15.7%. 2. The mean DRX for speakers who make any Disagree-Reject utterances is 9.5%. The pair of speakers KI-KA is in the top quin-tile (>13.6%). Based on this evidence we can conclude the following:   ExpDisagreementDRX (KI,KA, 5, dialogue-1) which may be read as follows: speakers KI and KA have the highest level of expressive disagreement in dialogue-1. This measure is complemented by a Cumulative Disagreement Index (CDX), which is computed for each speaker as a percentage of all Disagree-Reject utterances in the discourse that are made by this speaker. Unlike DRX, which is computed for pairs of speakers, the CDX values are as-signed to each group participant and indicate the degree of disagreement that each person generates. While Expressive Disagreement is based on the use of more overt linguistic devices, Topical Disagreement is defined as a differ-ence in referential valence in utterances (statements, opinions, questions, etc.) made on a topic. Referential valence of an utterance is determined by the type of statement made about the topic in question, which can be positive (+), negative (?), or neutral (0). A positive statement is one in favor of (express advocacy) or in support of (supporting infor-mation) the topic being discussed. A negative statement is one that is against or negative on 
1042
the topic being discussed. A neutral statement is one that does not indicate the speaker?s po-sition on the topic. Here is an example of op-posing polarity statements about the same topic in discourse: Sp-1: I like that he mentions ?Volunteerism and Leadership? Sp-2: but if they?re looking for someone who is experienced then I?d cross him off Detecting topical disagreement in discourse is more complicated because its strength may vary from one topic in a conversation to the next. A reasonable approach is thus to meas-ure the degree of disagreement between two speakers on one topic first, and then extrapo-late over the entire discourse. Accordingly, our measure of topical disagreement is valua-tion differential between any two speakers as expressed in their utterances about a topic. Here, the topic (or an ?issue?) is understood more narrowly than the local topic defined in the previous section (as used in Topic Control, for example), and may be assumed to cover only the most persistent local topics, i.e., top-ics with the largest number of references in dialogue, or what we call the meso-topics. For example, in a discussion of job applicants, each of the applicants becomes a meso-topic, and there may be additional meso-topics pre-sent, such as qualifications required, etc.  The resulting Topical Disagreement Metric (TDM) captures the degree to which any two speakers advocate the opposite sides of a meso-topic. TDM is computed as an average of P-valuation differential for one speaker (advocating for a meso-topic) and (?P)-valuation differential for the other speaker (advocating against the meso-topic).  Using TDM we can construct claims related to disagreement in a given multiparty dia-logue of sufficient duration (exactly what constitutes a sufficient duration is still being researched). Below is an example based on a 90-minute chat dialogue-1 about several job candidates for a youth counselor. The discus-sion involved 7 participants, including KI and KA. Topical disagreement is measured on 5 points scale (corresponding to quintiles in normal distribution): TpDisAgreeTDM(KI,KA,?Carla?,4,dialogue-1) This may be read as follows: speakers KI and KA topically disagree to degree 4 on topic [job candidate] ?Carla? in dialogue-1. In or-
der to calculate this we compute the value of TDM index between these two speakers. We find that KA makes 30% of all positive utter-ances made by anyone about Carla (40), while KI makes 45% of all negative utterances against Carla. This places these two speakers in the top quintiles in the ?for Carla? polarity distribution and ?against Carla? distribution, respectively. Taking into account any oppos-ing polarity statements made by KA against Carla and any statements made by KI for Carla, we calculate the level of topical dis-agreement between KA and KI to be 4 on the 1-5 scale. TDM allows us to compute topical disagree-ment between any two speakers in a discourse, which may also be represented in a 2-dimensional table revealing another inter-esting aspect of internal group dynamics.  4.3 Involvement in Dialogue The third type of social language use that we discuss in this paper is Involvement. In-volvement is defined as a degree of engage-ment or participation in the discussion of a group. It is an important element of leader-ship, although its importance is expected to differ between cultures; in Western cultures, high involvement and influence (topic control) often correlates with group leadership. In order to measure Involvement we designed several indices based on turn characteristics for each speaker. Four of the indices are briefly explained below:  ? The NP index (NPI) is a measure of gross informational content contributed by each speaker in discourse. NPI counts the ratio of third-person nouns and pronouns used by a speaker to the total number of nouns and pronouns in the discourse.  ? The Turn index (TI) is a measure of inter-actional frequency; it counts the ratio of turns per participant to the total number of turns in the discourse.  ? The Topic Chain Index (TCI) counts the degree to which participants discuss of the most persistent topics. In order to calculate TCI values, we define a topic chains for all local topics. We compute frequency of mentions of these longest topics for each participant.  ? The Allotopicality Index (ATP) counts the number of mentions of local topics that were introduced by other participants. An 
1043
ATP value is the proportion of a speaker's allotopical mentions, i.e., excluding ?self-citations?, to all allotopical mentions in a discourse.  As an example, we may consider the follow-ing situation in dialogue-1: 1. Dialogue-1 contains 796 third person nouns and pronouns, excluding mentions of participants? names. 2. Speaker JR uses 180 nouns and pronouns for an NPI of 22.6%.  3. The median NPI is 14.3%; JR are in the upper quintile of participants (> 19.9%). From the above evidence we can draw the following claim: InvolvementNPI(JR, 5, dialogue-1) This may be read as: speaker JR is the most involved participant in dialogue-1. As with other language uses, multiple indices for Involvement can be combined into a 2-dimensional map capturing the group in-ternal dynamics.  5. Implementation & Evaluation We developed a prototype automated DSARMD system that comprises a series of modules that create automated annotation of the source dialogue for all the language ele-ments discussed above, including communi-cative links, dialogue acts, local/meso topics, and polarity. Automatically annotated dia-logue is then used to generate language use degree claims. In order to evaluate accuracy of the automated process we conducted a pre-liminary evaluation comparing the LU claims generated from automatically annotated data to the claims generated from manually coded dialogues. Below we briefly describe the methodology and metrics used. Each language use is asserted per a partici-pant in a discourse (or per each pair of par-ticipants, e.g., for Disagreement) on a 5-point ?strength? scale. This can be represented as an ordered sequence LUX(d1, d2, ? dn), where LU is the language use being asserted, X is the index used, di is the degree of LU attrib-uted to speaker i. This assignment is therefore a 5-way classification of all discourse par-ticipants and its correctness is measured by dividing the number of correct assignments by the total number of elements to be classi-fied, which gives the micro-averaged preci-sion. The accuracy metric is computed with 
several variants as follows: 1. Strict mapping: each complete match is counted as 1; all mismatches are counted as 0. For example, the outputs LUX (5,4,3,2,1) and LUX (4,5,3,1,1) produce two exact matches (for the third and the last speaker) for a precision of 0.4. 2. Weighted mapping: since each degree value di in LUX(d1, d2, ? dn) represents a quintile in normal distribution, we consider the po-sition of the value within the quintile. If two mismatched values are less than ? quintile apart we assign a partial credit (currently 0.5). 3. Highest ? Rest: we measure accuracy with which the highest LU degree (but not nec-essarily the same degree) is assigned to the right speaker vs. any other score. This re-sults in binary classification of scores. The sequences in (1) produce 0.6 match score. 4. High ? Low: An alternative binary classifi-cation where scores 5 and 4 are considered High, while the remaining scores are con-sidered Low. Under this metric, the se-quences in (1) match with 100% precision. The process of automatic assignment of lan-guage uses derived from automatically proc-essed dialogues was evaluated against the control set of assignments based on hu-man-annotated data. In order to obtain a reli-able ?ground truth?, each test dialogue was annotated by at least three human coders (linguistics and communication graduate stu-dents, trained). Since human annotation was done at the linguistic component level, a strict inter-annotator agreement was not required; instead, we were interested whether in each case a comparable statistical distribution of the corresponding LU index was obtained. Annotations that produced index distributions dissimilar from the majority were eliminated. Automated dialogue processing involved the following modules: ? Local topics detection identifies first men-tions by tracking occurrences of noun phrases. Subsequent mentions are identi-fied using fairly simple pronoun resolution (based mostly on lexical features), with Wordnet used to identify synonyms, etc. ? Meso-topics are identified as longest-chain local topics. Their polarity is assessed at the utterance level by noting presence of positive or negative cue words and phrases. ? Dialogue acts are tagged based on presence 
1044
of certain cue phrases derived from a train-ing corpus (Webb et al, 2008).  ? Communicative links are mapped by com-puting inter-utterance similarity based on n-gram overlap. Preliminary evaluation results are shown in Tables 3-5 with average performance over 3 chat sessions (approx 4.5 hours) involving three groups of speakers and different tasks (job candidates, political issues). Topic Con-trol and Involvement tables show average accuracy per index. For example, the LTI in-dex, computed over automatically extracted local topics, produces Topic Control assign-ments with the average precision of 80% when compared to assignments derived from human-annotated data using the strict accu-racy metric. However, automated prediction of Involvement based on NPI index is far less reliable, although we can still pick the most involved speaker with 67% accuracy. We omit the indices based on turn length (TL) and turn count (TI) because their values are trivially computed. At this time we do not combine indices into a single LU prediction. Addi-tional experiments are needed to determine how much each of these indices contributes to LU prediction. Topic  Control LTI? SMT? CS?Strict? 0.80? 0.40? 0.40?Weighted? 0.90? 0.53? 0.53?Highest?Rest? 0.90? 0.67? 0.67?High?Low? 1.00? 0.84? 0.90?Table 3: Topic Control LU assignment performance averages of selected indices over a subset of data cov-ering three dialogues with combined duration of 4.5 hours with total of 19 participants (7, 5, 7 per session). 
Involvement NPI? TCI? ATP?Strict? 0.31? 0.42? 0.39?Weighted? 0.46? 0.49? 0.42?Highest?Rest? 0.67? 0.77? 0.68?High?Low? 0.58? 0.74? 0.48?Table 4: Involvement LU assignment performance av-erages for selected indices over the same subset of data as in Table 3. Topical Disagreement performance is shown in Table 5. We calculated precision and recall of assigning a correct degree of disagreement 
to each pair of speakers who are members of a group. Precision and recall averages are then computed over all meso-topics identified in the test dataset, which consists of three separate 90-minute dialogues involving 7, 5 and 7 speakers, respectively. Our calculation includes the cases where different sets of meso-topics were identified by the system and by the human coder. A strict mapping of levels of disagreement between speakers is hard to compute accurately; however, finding the speakers who disagree the most, or the least, is significantly more robust. 
Topical Disagreement Prec.? Recall?Strict? 0.33? 0.32?Weighted? 0.54? 0.54?Highest?Rest? 0.89? 0.85?High?Low? 0.77? 0.73?Table 5: Topical Disagreement LU assignment per-formance averages over 13 meso-topics discussed in three dialogues with combined duration of 4.5 hours with total of 19 participants (7, 5, and 7 per session). 6. Conclusion In this paper we presented a preliminary design for modeling certain types of social phenomena in multi-party on-line dialogues. Initial, limited-scale evaluation indicates that the model can be effectively automated. Much work lies ahead, including large scale evaluation, testing index stability and resilience to NL component level error. Current performance of the system is based on only preliminary versions of linguistic modules (topic extraction, polarity assignments, etc.) which perform at only 70-80% accuracy, so these need to be improved as well. Research on Urdu and Chinese dialogues is just starting. Acknowledgements This research was funded by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), through the U.S. Army Research Lab. All statements of fact, opinion or conclusions contained herein are those of the authors and should not be construed as representing the official views or policies of IARPA, the ODNI or the U.S. Government. 
1045
References Agar, Michael. 1994. Language Shock, Under-standing the Culture of Conversation. Quill, William Morrow, New York. Allen, J. M. Core. 1997. Draft of DAMSL: Dialog Act Markup in Several Layers. www.cs. roch-ester.edu/research/cisd/resources/damsl/  Anderson, A., et al 1991. The HCRC Map Task Corpus. Language and Speech 34(4), 351--366. Austin, J. L. 1962. How to do Things with Words. Clarendon Press, Oxford. Bird, Steven, et al 2009. Natural Language Proc-essing with Python: Analyzing Text with the Natural Language Toolkit. O'Reilly Media.  Blaylock, Nate. 2002. Managing Communicative Intentions in Dialogue Using a Collaborative Problem-Solving Model. Technical Report 774, University of Rochester, CS Dept. Broadwell, G. A et al (2010). Social Phenomena and Language Use. ILS Technical report. Carberry, Sandra and Lynn Lambert. 1999. A Process Model for Recognizing Communicative Acts and Modeling Negotiation Dialogue. Computational Linguistics, 25(1), pp. 1-53. Carletta, J. (2007). Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus. Language Resources and Evaluation Journal 41(2): 181-190 Carlson, Lauri. 1983. Dialogue Games: An Ap-proach to Discourse Analysis. D. Reidel. Eric N. Forsyth and Craig H. Martell. 2007. Lexi-cal and Discourse Analysis of Online Chat Dia-log. First IEEE International Conference on Semantic Computing (ICSC 2007), pp. 19-26. Field, D., et al 2008. Automatic Induction of Dia-logue Structure from the Companions Dialogue Corpus, 4th Int. Workshop on Human-Computer Conversation, Bellagio. Givon, Talmy. 1983. Topic continuity in discourse: A quantitative cross-language study. Amster-dam: John Benjamins.  Ivanovic, Edward. 2005. Dialogue Act Tagging for Instant Messaging Chat Sessions. In Proceed-ings of the ACL Student Research Workshop. 79?84. Ann Arbor, Michigan. Ji, Gang Jeff Bilmes. 2006. Backoff Model Train-ing using Partially Observed Data: Application to Dialog Act Tagging. HLT-NAACL Jurafsky, Dan, Elizabeth Shriberg, and Debra Bi-asca. 1997. Switchboard SWBD-DAMSL Shal-low-Discourse-Function Annotation Coders Manual. http://stripe.colorado.edu/~jurafsky/ manual.august1.html Jurafsky, D., et al 1997. Automatic detection of discourse structure for speech recognition and understanding. IEEE Workshop on Speech Recognition and Understanding, Santa Barbara. Khan, Faisal M., et al 2002. Mining Chat-room Conversations for Social and Semantic Interac-
tions. Computer Science and Engineering, Le-high University. Kim, Jihie., et al 2007. An Intelligent Discus-sion-Bot for Guiding Student Interactions in Threaded Discussions. AAAI Spring Sympo-sium on Interaction Challenges for Intelligent Assistants Levin, L., et al (1998). A discourse coding scheme for conversational Spanish. Interna-tional Conference on Speech and Language Processing. Levin, L., et al (2003). Domain specific speech acts for spoken language translation. 4th SIG-dial Workshop on Discourse and Dialogue. Linell, Per. 1990. The power of dialogue dynamics. In Ivana Markov?a and Klaus Foppa, editors, The Dynamics of Dialogue. Harvester, 147?177. Poesio, Massimo and Andrei Mikheev. 1998. The predictive power of game structure in dialogue act recognition. International Conference on Speech and Language Processing (ICSLP-98). Price, V., Capella, J. N., & Nir, L. (2002). Does disagreement contribute to more deliberative opinion? Political Communication, 19, 95-112. Sacks, H. and Schegloff, E., Jefferson, G. 1974. A simplest systematic for the organization of turn-taking for conversation. In: Language 50(4), 696-735.  Samuel, K. et al 1998. Dialogue Act Tagging with Transformation-Based Learning. 36th Annual Meeting of the ACL. Scollon, Ron and Suzanne W. Scollon. 2001. Intercultural Communication, A Discourse Ap-proach. Blackwell Publishing, Second Edition. Searle, J. R. 1969. Speech Acts. Cambridge Uni-versity Press, London-New York. Shaikh, S. et al 2010. DSARMD Annotation Guidelines, V. 2.5. ILS Technical Report.  Shaikh S. et al 2010. MPC: A Multi-Party Chat Corpus for Modeling Social Phenomena in Discourse, Proc. LREC-2010, Malta. Stolcke, Andreas et al 2000. Dialogue Act Mod-eling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguis-tics, 26(3). Stromer-Galley, J. 2007. Measuring deliberation?s content: A coding scheme. Journal of Public Deliberation, 3(1).  Tianhao Wu, et al 2002. Posting Act Tagging Us-ing Transformation-Based Learning. Founda-tions of Data Mining and Discovery, IEEE In-ternational Conference on Data Mining Twitchell, Douglas P., Jay F. Nunamaker Jr., and Judee K. Burgoon. 2004. Using Speech Act Profiling for Deception Detection. Intelligence and Security Informatics, LNCS, Vol. 3073 Webb, N., T. Liu, M. Hepple and Y. Wilks. 2008. Cross-Domain Dialogue Act Tagging. 6th In-ternational Conference on Language Resources and Evaluation (LREC-2008), Marrakech. 
1046
Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 43?48,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
VCA: An Experiment With A Multiparty Virtual Chat Agent 
Samira Shaikh1, Tomek Strzalkowski1, 2, Sarah Taylor3, Nick Webb1 1ILS Institute, University at Albany, State University of New York 2Institute of Computer Science, Polish Academy of Sciences 3Advancded Technology Office, Lockheed Martin IS&GS E-mail: ss578726@albany.edu, tomek@albany.edu   Abstract 
The purpose of this research was to advance the understanding of the behavior of small groups in online chat rooms. The research was conducted using Internet chat data collected through planned exercises with recruited par-ticipants. Analysis of the collected data led to construction of preliminary models of social behavior in online discourse. Some of these models, e.g., how to effectively change the topic of conversation, were subsequently im-plemented into an automated Virtual Chat Agent (VCA) prototype. VCA has been dem-onstrated to perform effectively and convinc-ingly in Internet conversation in multiparty chat environments.  1 Introduction Internet chat rooms provide a ready means of communication for people of most age groups these days. More often than not, these virtual chat rooms have multiple participants conversing on a wide variety of topics, using a highly infor-mal and free-form text dialect. An increasing use of virtual chat rooms by a variety of demograph-ics such as small children and impressionable youth leads to the risk of exploitation by deceit-ful individuals or organizations. Such risks might be reduced by presence of virtual chat agents that could keep conversations from progressing into certain topics by changing the topic of conversa-tion.  Our aim was to study the behavior of small groups of online chat participants and derive models of social phenomena that occur fre-quently in a virtual chat environment. We used the MPC chat corpus (Shaikh et al, 2010), which is 20 hours of multi-party chat data collected through a series of carefully designed online chat sessions. Chat data collected from public chat rooms, while easily available, presents signifi-cant concerns regarding its adaptability for our research use. Publicly available chat data is com-
pletely anonymous, has a high level of noise and lack of focus, in addition to engendering user privacy issues for its use in modeling tasks. The MPC corpus was used in (1) understanding how certain social behaviors are reflected in language and (2) building an automated chat agent that could effectively achieve certain (initially lim-ited) social objectives in the chat-room. A brief description of the MPC corpus and its relevant characteristics is given in Section 3 of this paper. One specific phenomenon of social behavior we wanted to model was an effective change of conversation topic, when a participant or a group of participants deliberately (if perhaps only tem-porarily) shift the discussion to a different, pos-sibly related topic. Both success and failure of these actions was of interest because the outcome depended upon the choice of utterance, the per-sons to whom it was addressed, their reaction, and the time when it was produced. Our analysis of the corpus for such phenomena led to the use of an annotation scheme that allows us to anno-tate for topic and focus change in conversation. We describe the annotation scheme used in Sec-tion 4.  We constructed an autonomous virtual chat agent (VCA) that could achieve initially limited social goals in a chat room with human partici-pants. We used a novel approach of exploiting the topic of conversation underway to search the web and find related topics that could be inserted in the conversation to change its flow. We tested the first prototype with the capability to opportu-nistically change to topic of conversation using a combination of linguistic, dialogic, and topic reference devices, which we observed effectively deployed by the most influential chat participants in the MPC corpus.  The VCA design, architec-ture and mode of operation are described in de-tail in Section 5 of this paper. 2 Related Work Automated dialogue agents such as the early ELIZA (Weizenbaum, 1966) and PARRY 
43
(Colby, 1974) could conduct a one-on-one ?con-versation? with a human using rules and pattern-matching algorithms. More recently, the addition of heuristic pattern matching in A.L.I.C.E (Wallace, 2008) led to development of chat bots using AIML1 and its variations, such as Project CyN2. Most of the work on conversational agents was limited to one-on-one situations, where a single agent converses with a human user, whether to perform a transaction (such as book-ing a flight or banking transactions) (Hardy et al, 2006) or for companionship (e.g., browsing of family photographs) (Wilks, 2010). Many of these systems were inspired by the challenge of the Turing Test or its more limited variants such as Loebner Prize.   Research in the field of developing a multi-user chat-room agent has been limited. This is some-what surprising because a multi-user setting makes the agent?s task of maintaining conversa-tion far less onerous than in one-on-one situa-tions. In a chat-room, with many users engaged in conversations, it is much easier for an agent to pass as just another user. Indeed, a skillfully de-signed agent may be able to influence an ongoing conversation. 3 MPC Chat Corpus The MPC chat corpus is a collection of 20 hours of chat sessions with multiple participants (on average 4), conversing for about 90 minutes in a secure online chat room. The topics of conversa-tion vary from free-flowing chat in the initial collection phase to allow participants to build comfortable a rapport with each other, to specific task-oriented dialogues in the latter phase; such as choosing the right candidate for a job inter-view from a list of given resumes. This corpus is suitable for our research purposes since the chat sessions were designed around enabling the so-cial phenomena we were interested in modeling. 4 Annotation Scheme We wished to annotate the data we collected to derive models from language use for social phe-nomena. These represent complex pragmatic concepts that are difficult to annotate directly, let alne detect automatically. Our approach was to build a multi-level annotation scheme.  In this paper we briefly outline our annotation scheme that consists of three layers: communica-                                                1 http://www.alicebot.org/aiml.html 2 http://www.daxtron.com/123start.htm?Cyn 
tive links, dialogue acts, and topic/focus changes. A more detailed description of the annotation scheme will be presented in a future publication.  4.1 Communicative Links Annotators are asked to mark each utterance in one of three categories ? utterance is addressed to a participant or a set of participants, it is in response to a specific prior utterance by another participant or it is a continuation of the partici-pant?s own prior utterance. By an utterance, we mean the set of words in a single turn by a par-ticipant. In multi-party chat, participants do not generally add addressing information in their utterances and it is often ambiguous to whom they are speaking. Communicative link annota-tion allows us to accurately map who is speaking to whom in the conversation, which is required for tracking social phenomena across partici-pants.  4.2 Dialogue Acts At this annotation level, we developed a hierar-chy of 20 dialogue acts, based loosely on DAMSL (Allen & Core, 1997) and SWBD-DAMSL (Jurafsky et al, 1997), but greatly re-duced and more tuned to dialogue pragmatics. For example, the utterance ?It is cold here today? may function as a Response-Answer when given in response to a question about the weather, and would act as an Assertion-Opinion if it is evalu-ated alone. The dialogue acts, thus augmented, become an important feature in modeling partici-pant behavior for our research purpose. A de-tailed description of the tags is beyond the scope of this paper. 4.3 Topic and Focus boundaries The flow of discussion in chat shifts quite rapidly from one topic to another. Furthermore, within each topic (e.g., music bands) the focus of conver-sation (e.g., dc for cutie) moves just as rapidly. We distinguish between topic and focus to accom-modate both broader thematic shifts and more narrow aspect changes of the topic being dis-cussed. For example, participants might discuss the topic of healthcare reform, by focusing on President Obama, and then switch the focus to some particulars of the reform, such as the ?public op-tion?. Similarly, topics may shift while the focus remains the same (e.g., moving on to Obama?s economic policies), although such changes are less common. Annotators typically marked the first mention of a substantive noun phrase as a topic or focus introduction. 
44
The effect of topic change is apparent when a subsequent utterance by another participant is about the same topic. This is a successful attempt at changing the topic. Shown in Figure 1 is an example of topic shift annotated in our data col-lection.  
 Figure 1. A topic change in dialogue, with three participants (AA, KA and KN)  We found this model of topic change fairly con-sistently exhibited, where the participants would ask an open question, in order to get other par-ticipants to respond to them, thereby changing the course of conversation. We collected all ut-terances marked topic shifts and focus shifts and created a set of templates from them.  These templates served as a model for the VCA to util-ize when creating a response.  Another model of behavior that we found as a consequence of topic change is topic sustain. This is an instance where the utterance is marked to be on the same topic as the one currently being discussed, for example, utterance 5 in Figure 1. These may be in the form of offering support or agreement with a previous utterance or asking a question about a new in-topic aspect. We gave our annotators a fair amount of lev-erage on how to label the topics and how to rec-ognize the focus. Our primary interest was in an accurate detection of topic/focus boundaries and shifts. Of the 14 sessions we selected from the MPC corpus, we selected 10 for annotation, with at least 3 annotators for each session. In Table 1 some of the overall statistics computed from this set are shown. We computed inter-annotator agreement on all three levels of our annotation, i.e. Communication Links, Dialogue Acts and 
Topic/Focus Shifts. Topic and Focus shifts had the highest inter-annotator agreement scores on different measures such as Krippendorf?s Alpha (Krippendorff, 1980) and Fliess? Kappa (Fliess, 1971). In Figure 2, we show inter-annotator agreement measures on Topic/Focus shift anno-tation for four of the annotated sessions. Krip-pendorff?s Alpha and Fleiss? Kappa measures show inter-annotator agreement on topic shift alone, and Conflated Krippendorff?s Alpha measures show the agreement when topic and focus are conflated as one category. With such high degree of agreement, we can reliably derive models of topic shift behavior from our anno-tated data.  Total Number of Sessions Annotated 10 Number of annotators per file 3 Total Utterances Annotated 4640 Average number of utterances per ses-sion ~520 Total topics identified per session 174 Total topic shifts identified per ses-sion 344 Table 1. Selected statistics from annotated data set  
 Figure 2. Inter-annotator agreement measures for Topic/Focus shifts 5 VCA Design A virtual chat agent is an automated program with the ability to respond to utterances in chat. Our VCA is distinctive in its ability to participate in multi-party chat and manage to steer the flow of conversation to a new topic. We exploit the dialogue mechanism underlying HITIQA (Small et al 2009) to drive the dialogue in VCA.  The topic as defined by the information con-tained in the participant?s utterance is used to mine outside data sources (e.g., a corpus, the web) in order to locate and learn additional in-formation about that topic. The objective is to identify some of the salient concepts that appear 
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1	 ?
Krippendorff	 ?
's	 ?Alpha	 ?
Con?ated	 ?
Krippendorff	 ?
's	 ?Alpha	 ?
Fleiss	 ?'	 ?Kappa	 ?
AA 1: did anyone watch the morning talk shows today (MTP, for example)? KA 2: nope! AA 3: I missed them ? I was hoping someone else had. AA 4: My kids tell me the band you?re going to hear (dc for cutie) is great. (TOPIC: music bands, FOCUS: dc for cutie) KA 5: oh cool! Their lyrics are nice, I think. (TOPIC: music bands, FOCUS: dc for cutie) KA 6. what kind of music do you guys listen to? (TOPIC: music, FOCUS: none) KN 7: I don?t really have a favorite genre?.you on youtube right now? (TOPIC: music, FOCUS: youtube)  
45
associated with the topic, but are not directly mentioned in the utterance. Such associations may be postulated because additional concepts are repeatedly found near the concepts men-tioned in the utterance.  An illustrative example found in our annotated corpus is the utterance, ?Lars Ulrich might have a thing or two to say about technology.? Here, the topic of conversation prior to this utterance was ?tech-nology? and it was changed to ?music? after this utterance. Here, ?Lars Ulrich? is the bridge that connects the two concepts ?technology? and ?mu-sic? together. 5.1 VCA Architecture The VCA is composed of the following modules that interact as shown in Figure 3.   5.1.1 Chat Analyzer Every utterance in chat is first analyzed by the Chat Analyzer component. This process removes stop words, emoticons and punctuation, as well as any participant nicknames from the utterance. We postulate that the remaining content bearing words in the utterance represent the topic of that utterance. We call this analyzed utterance our chat ?query? which is sent in parallel to the Document Retrieval and NL Processing compo-nent.   5.1.2 Document Retrieval The document retrieval process retrieves docu-ments from either the web or a test document 
corpus. We use Google AJAX api for our web retrieval process and InQuery (Callan et al, 1992) retrieval engine for our offline mode of operation to retrieve documents from the test corpus. The test document corpus was collected by mining the web for all utterances in our data 
collection, creating a stable document set for ex-perimental purposes. Currently, the document corpus contains about 1Gb of text data.   5.1.3 Clustering We cluster the paragraphs in documents retrieved using clustering method in Hardy et al (Hardy et al, 2009) This process groups the paragraphs containing salient entities into sets of closely as-sociated concepts. From each cluster, we choose the most representative paragraph, usually called the ?seed? paragraph for further NL processing. Each seed paragraph and the chat query undergo the same further NL processing sequence.    5.1.4 Natural Language Processing We process each chat query by performing stemming, part-of-speech tagging and named-entity recognition on it. Each seed paragraph is also run through same three natural language processing tasks. We are using Stanford POS tagger for our part-of-speech tagging. For named entity recognition, we have the ability to choose between BBN?s IdentiFinder and AeroText? (Taylor, 2004).  5.1.5 Framing We build frames from the entities and attributes found in both the chat query and the paragraphs.. This work extends the concept of framing devel-oped for HITIQA (Small et al 2009) and COL-LANE (Strzalkowski, 2009). Framing provides an informative handle on text, which can be ex-
ploited to compare the underlying textual repre-sentations, as we explain in the next section.  5.1.6 Scoring and Frame Matching Using the information in the frames built in the previous step; we compare the chat query frame 
Figure 3. VCA Architecture 
46
built from the chat query, to the frames created from the paragraphs, called paragraph frames. We assign a score for each paragraph frame based on how many attributes and their corre-sponding values match; in the current version of VCA a very basic approach to counting how many attribute-value pairs match is taken. Of all the paragraph frames we select the highest scor-ing frames and select the attribute-value pairs that are not part of the chat query frame. For ex-ample, as shown in Figure 4a below, the chat utterance ?Aruba might be nice!? created the fol-lowing chat query frame.  
 a. Example chat query frame  
 b. Frame Matching, Scoring and Template  Selection  Figure 4. From frames to VCA responses  Correspondingly, we select all PLACE type en-tities from the highest-ranking paragraph frames. These are shown in Figure 4b as Aruba Entity list.  The entities ?NASCAR?, ?Women Seeking Men? and ?Mateo? are not of entity type ? PLACE, we assign them a score of 0. The score is the fre-quency of occurrence of that entity in the para-graph; in this example it is found to be 1. Assign-ing scores by frequency of occurrence ensures that the most commonly occurring concept around the one that is being discussed in the chat query utterance will be used to respond with.  5.1.7 Template Selection Once we have chosen the entity to respond with, we select a template from the set of templates for that entity. These are templates that are created based on the models created from topic change utterances annotated in our data set. For a select group of entities, which are quite frequently en-
countered in our data collection such as PLACE, PERSON, ORGANIZATION etc., we have a set of templates specific to that entity type. We also have several generic templates that may be used if the entity type does not match the ones that we have selected. For example, a PLACE specific template is ?Have you ever been to __?? and a PER-SON specific template is ?You heard about __??. Not all templates are formulated as questions. An-other example of a generic template is ?__rules!?.  6 Example of VCA Interaction Figure 5 represents an example of the VCA in action in a simulated environment; the VCA is the participant ?renee?. We can see how the con-versation changes from ?gun laws? to ?hunting? after renee?s utterance at 11:48 AM.  
  Figure 5. Topic change example 7 Evaluation  We ran two tests of this initial VCA prototype in a public chat-room. VCA was inserted into a public chat-room with multiple participants on two separate occasions. The general topic of dis-cussion during both instances was ?anime?. We have developed an evaluation protocol in order to test the effectiveness of the VCA prototype in a realistic setting. The initial metric of VCA ef-fectiveness is the rate of involvement measured in the number of utterances generated by the VCA during the test period. These utterances are subsequently judged for appropriateness using the metric developed for the Companions Project (Webb, 2010). The actual appropriateness anno-tation scheme can be quite involved, but for this simple test we reduced the coding to only binary assessment, so that the VCA utterances were an-notated as either appropriate or inappropriate, given the content of the utterance and the flow of dialogue thus far. Using this coarse grain evalua-tion on a live chat segment we noted that the VCA made 9 appropriate utterances and 7 inap-
[POS] NNP, Aruba JJ, nice [ENT] PLACE  
Aruba Entity List: VALUE = NASCAR and TYPE = ORGANIZATION and SCORE = 0 VALUE = Dallas and TYPE = PLACE and SCORE = 1 VALUE = Mateo and TYPE = PERSON and SCORE = 0  VCA: How about Dallas? 
47
propriate utterances, which gives the appropri-ateness score of 56%. While some of VCA utter-ances seem inappropriate (i.e., not related to the conversation topic), we noted also that other posters generally tolerated these inappropriate utterances that occurred early in the dialogue. Moreover, these early inappropriate utterances did generate appropriate responses from the hu-man users. This ?positive? dynamic changed gradually as the dialogue progressed, when the participants began to ignore VCA?s utterances.  While this coarse grained evaluation is useful, our plan is to conduct evaluation experiments by recruiting subjects for chat sessions and inserting the VCA in the discussion. We will measure the impact of the VCA in the chat session by having participants fill out post-session questionnaires, which can elicit their responses regarding (a) if they detect presence of a VCA at any time during the dialogue; (b) who was the VCA; (c) who changed the topic of conversation most often; and so on. Another metric of interest is the level of engagement of the VCA, which can be meas-ured by the number of direct responses to an ut-terance by the VCA. We are developing the evaluation process, and report on the results in a separate publication. References  Allen, J. M. Core. (1997). Draft of DAMSL: Dialog Act Markup in Several Layers. http://www.cs.rochester.edu/research/cisd/resources/damsl/  Callan, J. P., W. B. Croft, and S. M. Harding. 1992. The INQUERY Retrieval System, in Proceedings of the 3rd Inter- national Conference on Database and Expert Systems.  Colby, K.M, Hilf, F.D, and S. Weber. 1972. Turing-like indistinguishability tests for the validation of a computer simulation of paranoid processes. In: Ar-tificial Intelligence , Vol. 3, p. 199-221. Fleiss, Joseph L. 1971. Measuring nominal scale agreement among many raters. Psychological Bul-letin, 74(5):378{382. Hardy, Hilda, Nobuyuki Shimizu, Tomek Strzalk-owski, Ting Liu, Bowden Wise and Xinyang Zhang. 2002. Cross-document summarization by concept classification. In Proceedings of ACM SIGIR '02 Conference, pages 121-128, Tampere, Finland. Hardy, H., A Biermann, R. Bryce Inouye, A. McKenzie, T. Strzalkowski, C. Ursu, N. Webb and M. Wu. 2006. The AMITIES System: Data-Driven Techniques for Automated Dialogue. In 
Speech Communication 48 (3-4), pages 354-373.  Elsevier. Jurafsky, Dan, Elizabeth Shriberg, and Debra Biasca. (1997). Switchboard SWBD-DAMSL Shallow-Discourse-Function Annotation Coders Manual. http://stripe.colorado.edu/~jurafsky/manual.august1.html Krippendorff, Klaus. 1980. Content Analysis, an In-troduction to its Methodology. Sage Publications, Thousand Oaks, CA. Samira S., Tomek Strzalkowski, Sarah Taylor and Jonathan Smith (2009) Comparing an Integrated QA system performance - A Preliminary Model. Proceedings of PACLING Conference, Sapporo, Japan. Shaikh, S., Strzalkowski, T.,  Broadwell, A., Stromer-Galley, J., Taylor, Sarah and Webb, N. 2010. Pro-ceedings of LREC Conference, Malta. Sharon Small and Tomek Strzalkowski. 2009. HITIQA: High-Quality Intelligence through Inter-active Question Answering. Journal of Natural Language Engineering, Vol. 15 (1), pp. 31?54. Cambridge.  Tomek Strzalkowski, Sarah Taylor, Samira Shaikh, Ben-Ami Lipetz, Hilda Hardy, Nick Webb, Tony Cresswell, Min Wu, Yu Zhan, Ting Liu, and Song Chen. 2009. COLLANE: An experiment in com-puter-mediated tacit collaboration. In Aspects of Natural Language Processing (M. Marciniak and A. Mykowiecka, editors). Springer.  Taylor, Sarah M. 2004. "Information Extraction Tools: Deciphering Human Language." IT Profes-sional. Vol. 06, no. 6, pages: 28-34. Novem-ber/December, 2004. Online. http://ieeexplore.ieee.org/iel5/6294/30282/01390870.pdf?tp=&arnumber=1390870&isnumber=30282 Wallace, R. 2008. The Anatomy of A.L.I.C.E. In Parsing the Turing Test. (Robert Epstein, Gary Roberts and Grace Beber, editors). Springer. Webb, N., D. Benyon, P. Hansen and O. Mival. 2010. Evaluating Human-Machine Conversation for Ap-propriateness. In Proceedings of the 7th Interna-tional Conference on Language Resources and Evaluation (LREC2010), Valletta, Malta. Weizenbaum, Joseph. January 1966. "ELIZA ? A Computer Program For the Study of Natural Lan-guage Communication Between Man And Ma-chine", Communications of the ACM 9 (1): 36?45. Wilks, Y. 2010. Artificial Companions. In: Y.Wilks (ed.) Close Engagement with Companions: scien-tific, economic, psychological and philosophical perspectives. John Benjamins: Amsterdam. 
48
Proceedings of the First Workshop on Metaphor in NLP, pages 67?76,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Robust Extraction of Metaphors from Novel Data   Tomek Strzalkowski1, George Aaron Broadwell1, Sarah Taylor2, Laurie Feldman1, Boris Yamrom1, Samira Shaikh1, Ting Liu1, Kit Cho1, Umit Boz1, Ignacio Cases1 and Kyle El-liott3 1State University of New York 2Sarah M. Taylor Consulting LLC 3Plessas Experts University at Albany 121 South Oak St.  Network Inc. Albany NY USA 12222 Falls Church VA USA 22046 Herndon VA 20171 tomek@albany.edu talymail59@gmail.com  kelliot@plessas.net     Abstract 
This article describes our novel approach to the automated detection and analysis of meta-phors in text. We employ robust, quantitative language processing to implement a system prototype combined with sound social science methods for validation. We show results in 4 different languages and discuss how our methods are a significant step forward from previously established techniques of metaphor identification. We use Topical Structure and Tracking, an Imageability score, and innova-tive methods to build an effective metaphor identification system that is fully automated and performs well over baseline.  1 Introduction The goal of this research is to automatically identi-fy metaphors in textual data.  We have developed a prototype system that can identify metaphors in naturally occurring text and analyze their seman-tics, including the associated affect and force. Met-aphors are mapping systems that allow the semantics of a familiar Source domain to be ap-plied to a Target domain so that new frameworks of reasoning can emerge in the Target domain. Metaphors are pervasive in discourse, used to con-vey meanings indirectly. Thus, they provide criti-cal insights into the preconceptions, assumptions and motivations underlying discourse, especially valuable when studied across cultures. When met-aphors are thoroughly understood within the con-text of a culture, we gain substantial knowledge about cultural values. These insights can help bet-ter shape cross-cultural understanding and facili-
tate discussions and negotiations among different communities.  A longstanding challenge, however, is the large-scale, automated identification of metaphor in vol-umes of data, and especially the interpretation of their complex, underlying semantics.  We propose a data-driven computational ap-proach that can be summarized as follows: Given textual input, we first identify any sentence that contains references to Target concepts in a given Target Domain (Target concepts are elements that belong to a particular domain; for instance ?gov-ernment bureaucracy? is a Target concept in the ?Governance? domain). We then extract a passage of length 2N+1, where N is the number of sentenc-es preceding (or succeeding) the sentence with Target Concept. We employ dependency parsing to determine the syntactic structure of each input sen-tence. Topical structure and imageability analysis are then combined with dependency parsing output to locate the candidate metaphorical expressions within a sentence. For this step, we identify nouns and verbs in the passage (of length 2N+1) and link their occurrences ? including repetitions, pronomi-nal references, synonyms and hyponyms. This linking uncovers the topical structure that holds the narrative together.  We then locate content words that are outside the topical structure and compute their imageability scores. Any nouns or adjectives outside the main topical structure that also have high imageability scores and are dependency-linked in the parse structure to the Target Concept are identified as candidate source relations, i.e., expressions borrowed from a Source domain to describe the Target concept. In addition, any verbs that have a direct dependency on the Target Con-
67
cept are considered as candidate relations. These candidate relations are then used to compute and rank proto-sources. We search for their arguments in a balanced corpus, assumed to represent stand-ard use of the language, and cluster the results. Proto-source clusters and their ranks are exploited to determine whether the candidate relations are metaphorical or literal. Finally, we compute the affect and force associated with the metaphor.    Our approach is shown to work in four lan-guages ? American English, Mexican Spanish, Russian Russian and Iranian Farsi. We detail in this paper the application of our approach to detec-tion of metaphors using specific examples from the ?Governance? domain. However, our approach can be expanded to work on extracting metaphors in any domain, even unspecified ones. We shall brief-ly explain this in Section 5; we defer the details of the expanded version of the algorithm to a separate larger publication. In addition, we shall primarily present examples in English to illustrate details of our algorithms. However, modules for all four lan-guages have the same implementation in our sys-tem.  The rest of the paper is organized as follows: in Section 2, we discuss related research in this field. Section 3 presents our approach in detail; Section 4 describes our evaluation and results. In Section 5 we discuss our conclusions and future directions.  2 Related Work Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their fol-lowers) that generally look at metaphors as abstract language constructs with complex semantic prop-erties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O?Halloran, 2007) that at-tempt to correlate metaphor semantics with their usage in naturally occurring text but generally lack robust tools to do so; and (3) social science ap-proaches, particularly in psychology and anthro-pology that seek to explain how people deploy and understand metaphors in interaction, but which lack the necessary computational tools to work with anything other than relatively isolated exam-ples.     Metaphor study in yet other disciplines has in-cluded cognitive psychologists (e.g., Allbritton, McKoon & Gerrig, 1995) who have focused on the 
way metaphors may signify structures in human memory and human language processing. Cultural anthropologists, such as Malkki in her work on refugees (1992), see metaphor as a tool to help out-siders interpret the feelings and mindsets of the groups they study, an approach also reflective of available metaphor case studies, often with a Polit-ical Science underpinning (Musolff, 2008; Lakoff, 2001).      In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of meta-phors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al(2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example).     Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feld-man & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al(2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However they self-report their work to be an initial explora-tion and hence, inconclusive. Shutova et al(2010a) employ an unsupervised method of metaphor iden-tification using nouns and verb clustering to auto-matically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quanti-ties and may not be easily generated in different languages.  By contrast, we propose an approach that is fully automated and can be validated using empirical social science methods. Details of our algorithm follow next.  3 Our Approach In this section, we walk through the steps of meta-phor identification in detail. Our overall algorithm 
68
consists of five main steps from obtaining textual input to classification of input as metaphorical or literal.  3.1 Passage Identification The input to our prototype system is a piece of text. This text may be taken from any genre ? news articles, blogs, magazines, official announcements, broadcast transcripts etc.  Given the text, we first identify sentences that contain Target concepts in the domain we are in-terested in. Target concepts are certain keywords that occur within the given domain and represent concepts that may be targets of metaphor. For in-stance, in the ?Governance? domain, concepts such as ?federal bureaucracy? and ?state mandates? serve as Target concepts. We keep a list of Target concepts to search through when analyzing given input. This list can be automatically created by mining Target Concepts from resource such as Wikipedia, given the Target domain, or manually constructed. Space limits the discussion of how such lists may be automatically created; a separate larger publication addresses our approach to this task in greater detail.  In Figure 1, we show a piece of text drawn from a 2008 news article. The sentence in italics con-tains one of our Target concepts: ?federal bureau-cracy?. We extract the sentence containing Target concepts that match any of those in our list, includ-ing N sentences before and N sentences after the sentence if they exist, to yield a passage of at most 2N+1 sentences. For the example shown in Figure 1, the Target concept is ?federal bureaucracy?. In current system prototype, N=2. Hence, we extract two sentences prior to the sentence containing ?federal bureaucracy? (in Figure 1 example, these are omitted for ease of presentation) and two sen-tences following the given sentence.       Once this passage is extracted, we need to de-termine whether a metaphor is present in the mid-dle sentence. To accomplish that, we follow the steps as described in the next section.    
 Figure 1. Excerpt from news article. Passage containing target concept highlighted in italics. The callouts 1, 2 etc., indicate topic chains (see next section).      3.2 Topical Structure and Imageability Anal-ysis Our hypothesis is that metaphorically used terms are typically found outside the topical structure of the text. This is an entirely novel method of effec-tively selecting candidate relations. It draws on  Broadwell et al (2012), who proposed a method to establish the topic chains in discourse as a means of modeling associated socio-linguistic phenomena such as topic control and discourse cohesiveness. We adapted this method to identify and exclude any words that serve to structure the core discus-sion, since the metaphorical words, except in the cases of extended and highly elaborated meta-phors, are not the main subject, and thus unlikely to be repeated or referenced in the context sur-rounding the sentence.  We link the occurrences of each noun and verb in the passage (5 sentence length). Repetitions via synonyms, hyponyms, lexical variants and pronoun references are linked together. These words, as elements of the several topic chains in a text, are then excluded from further consideration. WordNet (Felbaum, 1998) is used to look up synonyms and hyponyms of the remaining content words. We 
These qualities1 have helped him4 navigate the labyrinthine federal bureaucracy in his demand-ing $191,300-a-year job as the top federal offi-cial3 responsible for bolstering airline, border2, port and rail security against a second cata-strophic terrorist attack.  But those same personal qualities1 also explain why the 55-year-old Cabinet officer3 has alienat-ed so many Texans along the U.S.-Mexico bor-der2 with his4 relentless implementation of the Bush administration's hard-nosed approach to immigration enforcement - led by his unyielding push to construct 670 miles of border2 fencing by the end of the year.  Some Texas officials are so exasperated that they say they'll just await the arrival of the next presi-dent before revisiting border enforcement with the federal government.  Copyright 2008. The Houston Chronicle Publishing Company. All Rights Reserved.  
69
illustrate this in Figure 1. We show the two sen-tences that form the latter context in the example passage. We show four of the topic chains discov-ered in this passage. These have been labeled via superscripts in Figure 1. 1 and 2 are the repetitions of word ?qualities? and ?border?. The 3 identifies repetition via lexical variants ?officer? and ?offi-cial? and 4 identifies the pronoun co-references  ?him? and ?his?. We shall exclude these words from consideration when searching for candidate metaphorical relations in the middle sentence of the passage.  To further narrow the pool of candidate relations in this sentence, we compute the imageability scores of the remaining words. The hypothesis is metaphors use highly imageable words to convey their meaning. The use of imageability scores for the primary purpose of metaphor detection distin-guishes our approach from other research on this problem. While Turney et al (2011) explored the use of word concreteness (a concept related but not identical to imageability) in an attempt to disam-biguate between abstract and concrete verb senses, their method was not specifically applied to detec-tion of metaphors; rather it was used to classify verb senses for the purpose of resolving textual entailment. Broadwell et al (2013) present a de-tailed description of our approach and how we use imageability scores to detect metaphors. Our assertion is that any highly imageable word is more likely to be a metaphorical relation. We use the MRCPD (Coltheart 1981, Wilson 1988) expanded lexicon to look up the imageability scores of words not excluded via the topic chains. Although the MRCPD contains data for over 150,000 words, a major limitation of the database for our purposes is that the MRCPD has imageabil-ity ratings (i.e., how easily and quickly the word evokes a mental image) for only ~9,240  (6%) of the total words in its database. To fill this gap, we expanded the MRCPD database by adding imagery ratings for an further 59,989 words. This was done by taking the words for which the MRCPD data-base has an imageability rating and using that word as an index to synsets determined using WordNet (Miller, 1995). The expansion and validation of the expanded MRCPD imageability rating is presented in a separate, future publication.  Words that have an imageability rating lower than an experimentally determined threshold are further excluded from consideration. In the exam-
ple shown in Figure 1, words that have sufficiently high imageability scores are ?labyrinthine?, ?port?, ?rail? and ?airline?. We shall consider them as candidate relations, to be further investigated, as explained in the dependency parsing step described next.   3.3 Relation Extraction Dependency parsing reveals the syntactic structure of the sentence with the Target concept. We use the Stanford parser (Klein and Manning, 2003) for English language data. We identify candidate met-aphorical relations to be any verbs that have the Target concept in direct dependency path (other than auxiliary and modal verbs). We exclude verbs of attitude (?think?, ?say?, ?consider?), since these have been found to be more indicative of metony-my than of metaphor. This list of attitude verbs is automatically derived from WordNet. From the example shown in Figure 1, one of the candidate relations extracted would be the verb ?navigate?.      In addition, we have a list of candidate relations from Step 3.2, which are the highly imageable nouns and adjectives that remain after topical structure analysis. Since ?port?, ?rail? and ?airline? do not have a direct dependency path to our Target concept of ?federal bureaucracy?, we drop these from further consideration. The highly imageable word remaining in this list is ?labyrinthine?.      Thus, two candidate relations are extracted from this passage ? ?navigate? and ?labyrinthine?. We shall now show how we use these to discover pro-to-sources for the potential metaphor.  3.4 Discovery of Proto-sources Once candidate relations are identified, we exam-ine whether the usage of these relations is meta-phorical or literal. To determine this, we search for all uses of these relations in a balanced corpus and examine in which contexts the candidate relations occur. To demonstrate this via our example, we shall consider one of the candidate relations identi-fied in Figure 1 ? ?navigate?; the search method is the same for all candidate relations identified. In the case of the verb ?navigate? we search a bal-anced corpus for the collocated words, that is, those that occur within a 4-word window following the verb, with high mutual information (>3) and occurring together in the corpus with a frequency 
70
at least 3. This search returns a list of words, most-ly nouns in this case, that are the objects of the verb ?navigate?, just as ?federal bureaucracy? is the object in the given example. However, since the search occurs in a balanced corpus, given the parameters we search for, we discover words where the objects are literally navigated. Given these search parameters, the top results we get are generally literal uses of the word ?navigate?. We cluster the resulting literal uses as semantically related words using WordNet and corpus statistics. Each such cluster is an emerging prototype source domain, or a proto-source, for the potential meta-phor. In Figure 2, we show three of the clusters ob-tained when searching for the literal usage of the verb ?navigate?. We use elements of the clusters to give names or label the proto-source domains. WordNet hypernyms or synonyms are used in most cases. The clusters shown in Figure 2 represent three potential source domains for the given exam-ple, the labels ?MAZE?, ?WAY? and ?COURSE? are derived from WordNet.  
 Figure 2. Three of several clusters obtained from bal-anced corpus search for objects of verb ?navigate?.       We rank the clusters according to the combined frequency of cluster elements in the balanced cor-pus. In a similar fashion, clusters are obtained for the candidate relation ?labyrinthine?; however here we search for the nouns modified by the adjective ?labyrinthine?.       
3.5 Estimation of Linguistic Metaphor A ranked list of proto-sources from the previous step serves as evidence for the presence of a meta-phor.   If any Target domain elements are found in the top two ranked clusters, we consider the phrase being investigated to be literal. This eliminates examples where one of the most frequently en-countered sources is within the target domain.  If neither of the top two most frequent clusters contains any elements from the target domain, we then compute the average imageability scores for each cluster from the mean imageability score of the cluster elements. If no cluster has a sufficiently high imageability score (experimentally deter-mined to be >.50 in the current prototype), we again consider the given input to be literal. This step reinforces the claim that metaphors use highly imageable language to convey their meaning. If a proto-source cluster is found to meet both criteria, we consider the given phrase to be metaphorical. For the example shown in Figure 1, our system finds ?navigate the ?federal bureaucracy? to be metaphorical. One of the top Source domains iden-tified for this metaphor is ?MAZE?. Hence the conceptual metaphor output for this example can be: ?FEDERAL BUREAUCRACY IS A MAZE?. Our system can thus classify input sentences as metaphorical or literal by the series of steps out-lined above. In addition, we have modules that can determine a more complex conceptual metaphor, based upon evidence of one or more metaphorical passages as identified above. We do not discuss those modules in this article. Once a metaphor is identified, we compute associated Mappings, Af-fect and Force. 3.6 Mappings In the current prototype system, we assign meta-phors to one of three types of mappings. Propertive mappings ? which state what the domain objects  
1. Proto-source Name: MAZE Proto-source Elements: [mazes, system, net-works] IMG Score: 0.74 2. Proto-source Name: WAY Proto-source Elements: [way, tools] IMG Score: 0.60 3. Proto-source Name: COURSE Proto-source Elements: [course, streams] IMG: 0.55 
Table 1. Algorithm assigns affect of metaphor based upon mappings. 
Rel  < Negative Rel  = Neutral 
Rel ? Positive 
71
are and descriptive features; Agentive mappings ? which describe what the domain elements do to other objects in the same or different domains; and Patientive mappings ? which describe what is done to the objects in these domains. These are broad categories to which relations can, with some ex-ceptions be assigned at the linguistic metaphor lev-el by the parse tag of the relation. Relations that take Target concepts as objects are usually Pa-tientive relations. Similarly, relations that are Agentive take Target concepts as subjects. Proper-tive relations are usually determined by adjectival relations.     Once mappings are assigned, we can use them to group linguistic metaphors. A set of linguistic met-aphors on the same or semantically equivalent Target concepts can be grouped together if the re-lations are all agentive, patientive or propertive. The mapping assigned to set of examples in Figure 3 is Patientive.      One immediate consequence of the proposed approach is the simplicity with which we can rep-resent domains, their elements, and the metaphoric mappings between domains. Regardless of what specific relations may operate within a domain (be it Source or Target), they can be classified into just 3 categories. We are further expanding this module to include semantically richer distinctions within the mappings. This includes the determination of the sub-dimensions of mappings i.e. assigning groups of relations to a semantic category.  3.7 Affect and Force  Affect of a metaphor may be positive, negative or neutral. Our affect estimation module computes an affect score taking into account the relation, Target concept and the subject or object of the relation based on the dependency between relation and Target concept. The algorithm is applied according to the categories shown in Table 1.      The expanded ANEW lexicon (Bradley and Lang, 2010) is used to look up affect scores of words. ANEW assigns scores from 0 (highly nega-tive) to 9 (highly positive); 5 being neutral. We compute the affect of a metaphorical phrase within a sentence by summing the affect scores of the re-lation and its object or subject.  If the relation is agentive, we then look at the object in source do-main that the Target concept is acting upon. If the object (denoted in above table as X) has an affect 
score that is greater than neutral, and the relation itself has an affect score that is greater than neutral, then a POSITIVE affect is assigned to the meta-phor. This is denoted by the cell at the intersection of the row labeled ?Rel > Positive? and the 3rd col-umn in Table 1. Similarly affect for the other map-ping categories can be assigned.   
 Figure 3. Four metaphors for the Target concept ?feder-al bureaucracy?.   We also seek to determine the impact of metaphor on the reader. This is explored using the concept of Force in our system. The force of a metaphor is estimated currently by the commonness of the ex-pression in the given Target domain. We compute the frequency of the relation co-occurring with Target concept in a corpus of documents in the given Target domain. This frequency represents the commonness of expression, which is the in-verse of Force. The more common a metaphorical expression is, the lesser its force.     For the example shown below in Figure 4, the affect is computed to be positive (?navigate? and ?veterans? are both found to have positive affect scores, the relation is patientive). The force of this expression is low, since its commonness is 742 (commonness score > 100 is high commonness, determined experimentally).   
 Figure 4. Example of metaphor with positive affect and low force. 
1. His attorney described him as a family man who was lied to by a friend and who got tangled in federal bureaucracy he knew nothing about. 2. The chart, composed of 207 boxes illustrates the maze of federal bureaucracy that would have been created by then-President Bill Clinton's rela-tion health reform plan in the early 1990s. 3. "Helping my constituents navigate the federal bureaucracy is one of the most important things I can do," said Owens. 4. A Virginia couple has donated $1 million to help start a center at Arkansas State University meant to help wounded veterans navigate the federal bureaucracy as they return to civilian life.  
A Virginia couple has donated $1 million to help start a center at Arkansas State University meant to help wounded veterans navigate the federal bureaucracy as they return to civilian life.  
72
   The focus of this article is the automatic identifi-cation of metaphorical sentences in naturally oc-curring text. Affect and force modules are utilized to understand metaphors in context and contrast them across cultures, if feasible. We defer more detailed discussion of affect and force and their implications to a future, larger article.  4 Evaluation and Results In order to determine the efficacy of our system in classifying metaphors as well as to validate various system modules such as affect and force, we per-formed a series of experiments to collect human validation of metaphors in a large set of examples.  4.1 Experimental Setup  We constructed validation tasks that aimed at per-forming evaluation of linguistic metaphor extrac-tion accuracy. The first task ? Task 1, consists of a series of examples, typically 50, split more or less equally between those proposed by the system to be metaphorical and those proposed to be literal. This task was designed to elicit subject and expert judgments on several aspects related to the pres-ence or absence of linguistic metaphors in text. Subjects are presented with brief passages where a Target concept and a relation are highlighted. They are asked to rank their responses on a 7-point scale for the following questions:  Q1: To what degree does the above passage use metaphor to describe the highlighted concept? Q2: To what degree does this passage convey an idea that is either positive or negative?  Q3: To what degree is it a common way to express this idea?      There are additional questions that ask subjects to judge the imageability and arousal of a given pas-sage, which we do not discuss in this article. Q1 deals with assessing the metaphoricity of the ex-ample, Q2 deals with affect and Q3 deals with force.   Each instance of Task 1 consists of a set of instruc-tions, training examples, and a series of passages to be judged. Instructions provide training examples whose ratings fall at each end the rating continu-um. Following the task, participants take a gram-
mar test to demonstrate native language proficien-cy in the target language. All task instances are then posted on Amazon?s Mechanical Turk. The goal is to collect at least 30 valid judgments per task instance. We typically collect ~50 judgments from Mechanical Turkers, so that after filtering for invalid data which includes turkers selecting items at random, taking too little time to complete the task, grammar test failures, and other inconsistent data, we would still retain 30 valid judgments per passage. In addition to grammar test and time fil-ter, we also inserted instance of known metaphors and known literal passages randomly within the Task. Any turker judgments that classify these known instance incorrectly more than 30% of the total known instance size are discarded.     The valid turker judgments are then converted to a binary judgment for the questions we presented. For example, for question Q1, the anchors to 7-point scale are 0 (none at all i.e. literal) to 7 (highly i.e metaphorical). We take [0, 2] as a literal judg-ment and [4, 6] as metaphorical and take a majority vote. If the majority vote is 3, we discard that pas-sage from our test set, since it is undetermined whether the passage is literal or metaphorical.         We have collected human judgments on hun-dreds of metaphors in all four languages of inter-est. In Section 4.3, we explain our performance and compare our results to baseline where appro-priate.  4.2 Test Reliability The judgments collected from subjects are tested for reliability and validity. Reliability among the raters is computed by measuring intra-class corre-lation (ICC) (McGraw & Wong, 1996; Shrout & Fleiss, 1979). A coefficient value above 0.7 indi-cates strong reliability.  Table 3 shows the current reliability coefficients established for the selected Task 1 questions in all 4 languages. In general, our analyses have shown that with approximately 30 or more subjects we obtain a reliability coefficient of at least 0.7. We note that Russian and Farsi reliability scores are low in some categories, primarily due to lack of sufficient subject rating data. However, reliability of subject ratings for metaphor question (Q1) is sufficiently high in three of the four languages we are interested in.  
73
Dimension English Spanish Russian  Farsi Metaphor .908 .882 .838 .606 Affect  .831 .776 .318 .798 Commonness .744 .753 .753 .618 Table 3. Intraclass correlations for linguistic metaphor assessment by Mechanical Turk subjects (Task 1) 4.3 Results In Table 4, we show our performance at classifying metaphors across four different languages. The baseline in this table assigns all given examples in the test set to be metaphorical. We note that per-formance of the system at the linguistic metaphor level when compared to human gold standard is significantly over baseline for all four languages. The system performances cited in Table 4 validate the system against test sets that contain the distri-bution of metaphorical vs. literal examples as out-lined in Table 5.   English Spanish Russian Farsi Baseline 45.8% 41.7% 56.4% 50% System 71.3% 80% 69.2% 78% Table 4. Performance accuracy of system when com-pared to baseline for linguistic metaphor classification.   English Spanish Russian Farsi Metaphor 50 50 22 25 Literal 59 70 17 25 Total 109 120 39 50 Table 5. Number of metaphorical and literal examples in test sets across all four languages.  Table 6 shows the accuracy in classification by the Affect and Force modules. We note that the low performance of affect and force for languages oth-er than English. Our focus has been on improving NLP tools for Spanish, Russian and Farsi, so that a similar robust performance for those language can be achieved as we can demonstrate in English.  Accuracy English Spanish Russian Farsi Affect  72% 54% 51% 40% Force 67% 50% 33% 66% Table 6. Affect and force performance of system on linguistic metaphor level.  5 Discussion and Future Work In this article, we described in detail our approach to detecting metaphors in text. We have developed 
an automated system that does not require the ex-istence of annotated training data or a knowledge base of predefined metaphors. We have described the various steps for detecting metaphors from re-ceiving an input, to selecting candidate relations, to the discovery of prototypical source domains, and leading to the identification of a metaphor as well as the discovery of the potential source domain being applied in the metaphor. We presented two novel concepts that have heretofore not been fully explored in computational metaphor identification systems. The first is the exclusion of words that form the thread of the discussion in the text, by the application of a Topic Tracking module. The se-cond is the application of Imageability scores in the selection of salient candidate relations.  Our evaluation consists first of validating the eval-uation task itself. Once we ensure that sufficient reliability has been established on the various di-mensions we seek to evaluate ? metaphoricity, af-fect and force ? we compare our system performance to the human gold standard. The per-formance of our system as compared to baseline is quite high, across all four languages of interest when measured against human assessed gold standard.  In this article, we discuss examples of metaphors belonging to a specific Target domain ? ?Govern-ance?. However, we can run our system through data in any domain perform the same kind of met-aphor identification. In cases where the Target do-main is unknown, we plan to use our Topic tracking module to recognize content words that may form part of a metaphorical phrase. This is essentially a process that is the reverse of that de-scribed in Section 3.3. We will find the salient Target concepts where there are directly dependent relations with the imageable verbs or adjectives.  In a separate larger publication, we plan to discuss in detail revisions to our Mapping module as well as the discovery and analyses of more complex conceptual metaphors. Such complex metaphors are based upon evidence from one or more instance of linguistic metaphors. Additional modules would recognize the manifold mappings, affect and force associated with the complex conceptual metaphors.   Acknowledgments This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of 
74
Defense US Army Research Laboratory contract num-ber W911NF-12-C-0024. The U.S. Government is au-thorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.  Disclaimer: The views and conclu-sions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Govern-ment. References  Allbritton, David W., Gail McKoon, and Richard J. Gerrig. 1995. Metaphor-Based Schemas and Text Representations: Making Connections Through Con-ceptual Metaphors, Journal of Experimental Psy-chology: Learning, Memory, and Cognition, Vol. 21, No. 3, pp. 612-625. Baumer, Erik. P.S., White, James., Tomlinson, Bill. 2010. Comparing Semantic Role Labeling with Typed Dependency Parsing in Computational Meta-phor Identification. Proceedings of the NAACL HLT 2010 Second Workshop on Computational Ap-proaches to Linguistic Creativity, pages 14?22, Los Angeles, California, June 2010.  Bradley, M.M. & Lang, P.J. 2010. Affective Norms for English Words (ANEW): Instruction manual and af-fective ratings. Technical Report C-2. University of Florida, Gainesville, FL. Broadwell George A., Jennifer Stromer-Galley, Tomek Strzalkowski, Samira Shaikh, Sarah Taylor, Umit Boz, Alana Elia, Laura Jiao, Ting Liu and Nick Webb. 2012. Modeling Socio-Cultural Phenomena in Discourse. Journal of Natural Language Engineer-ing, Cambridge Press. Broadwell, George A., Umit Boz, Ignacio Cases, Tomek Strzalkowski, Laurie Feldman, Sarah Taylor, Samira Shaikh, Ting Liu, Kit Cho, aand Nick Webb. 2013. Using imageability and topic chaining to locate met-aphors in linguistic corpora. in Ariel M. Greenberg, William G. Kennedy, Nathan D. Bos and Stephen Marcus, eds. Proceedings of the 6th International Conference on Social Computing, Behavioral-Cultural Modeling and Prediction SBP 2013. Carbonell, Jaime. 1980. Metaphor: a key to extensible semantic analysis. Proceedings of the 18th Annual Meeting on Association for Computational Linguis-tics. Charteris-Black, Jonathan 2002 Second Language Fig-urative Proficiency: A Comparative Study of Malay and English. Applied Linguistics 23/1: 104-133. 
Coltheart, M. 1981. The MRC Psycholinguistic Data-base. Quarterly Journal of Experimental Psychology, 33A, 497-505. Fass, Dan. 1991. met*: A Method for Discriminating Metonymy and Metaphor by Computer. Computa-tional Linguistics, Vol 17:49-90 Feldman, J. and S. Narayanan. 2004. Embodied mean-ing in a neural theory of language. Brain and Lan-guage, 89(2):385?392. Fellbaum, C. editor. 1998. WordNet: An Electronic Lexical Database (ISBN: 0-262-06197-X). MIT Press, first edition. Gedigian, M., Bryant, J., Narayanan, S., & Ciric, B. (2006). Catching Metaphors. Proceedings of the Third Workshop on Scalable Natural Language Un-derstanding ScaNaLU 06 (pp. 41-48). Association for Computational Linguistics. Klein, Dan and Manning, Christoper D. 2003. Accurate Unlexicalized Parsing. Proceedings of the 41st Meeting of the Association for Computational Linguistics, pp. 423-430. Krishnakumaran, S. and X. Zhu. 2007. Hunting elusive metaphors using lexical resources. In Proceedings of the Workshop on Computational Approaches to Fig-urative Language, pages 13?20, Rochester, NY. Lakoff, George and Johnson, Mark. 1980. Metaphors We Live By. University Of Chicago Press. Lakoff, George. 2001. Moral Politics: what Conserva-tives Know that Liberals Don?t. University of Chica-go Press. Malkki, Liisa. 1992. National Geographic: The Rooting of People and the Territorialization of National Iden-tity Among Scholars and Refugees. Society for Cul-tural Anthropology 7(1):24-44 Martin, James. 1988. A Computational Theory of Meta-phor. PH.D. Dissertation McGraw, K. O., & Wong, S. P. 1996. Forming infer-ences about some intraclass correlation coefficients. Psychological Methods, 1(1), 30-46. Musolff, Andreas. 2008. What can Critical Metaphor Analysis Add to the Understanding of Racist Ideolo-gy? Recent Studies of Hitler?s Anti-Semitic Meta-phors, Critical Approaches to Discourse Analysis across Disciplines, http://cadaad.org/ejournal, Vol. 2(2): 1-10. O?Halloran, Kieran. 2007. Critical Discourse Analysis and the Corpus-informed Interpretation of Metaphor at the Register Level. Oxford University Press 
75
Shrout, P. E., & Fleiss, J. L. 1979. Intraclass correla-tions: Uses in assessing rater reliability. Psychologi-cal Bulletin, 86 (2), 420-428. Shutova, E. 2010. Models of Metaphors in NLP. In Proceedings of ACL 2010, Uppsala, Sweden. Shutova, E. and S. Teufel. 2010a. Metaphor corpus an-notated for source - target domain mappings. In Pro-ceedings of LREC 2010, Malta. Shutova, E., T. Van de Cruys and A. Korhonen. 2012. Unsupervised Metaphor Paraphrasing Using a Vector Space Model, In Proceedings of COLING 2012, Mumbai, India Turney, Peter., Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identifi-cation through concrete and abstract context. In Pro-ceedings of EMNLP, pages 680?690, Edinburgh, UK Wilks, Yorick. 1975. Preference semantics. Formal Semantics of Natural Language, E. L. Keenan, Ed. Cambridge University Press, Cambridge, U.K., 329--348. Wilson, M.D. (1988) The MRC Psycholinguistic Data-base: Machine Readable Dictionary, Version 2. Be-havioural Research Methods, Instruments and Computers, 20(1), 6-11.     
76
Proceedings of the Second Workshop on Metaphor in NLP, pages 42?51,
Baltimore, MD, USA, 26 June 2014. c?2014 Association for Computational Linguistics
Computing Affect in Metaphors 
Tomek Strzalkowski1,2, Samira Shaikh1, Kit Cho1, George Aaron Broadwell1, Laurie Feldman1, Sarah Taylor3, Boris Yamrom1, Ting Liu1, Ignacio Cases1, Yuliya Peshkova1 and Kyle Elliot4 1State University of New York - Univer-sity at Albany  
2Polish Academy of Sciences 3Sarah M. Taylor Consulting LLC 4Plessas Experts Network tomek@albany.edu     Abstract 
This article describes a novel approach to automated determination of affect associ-ated with metaphorical language. Affect in language is understood to mean the at-titude toward a topic that a writer at-tempts to convey to the reader by using a particular metaphor. This affect, which we will classify as positive, negative or neutral with various degrees of intensity, may arise from the target of the meta-phor, from the choice of words used to describe it, or from other elements in its immediate linguistic context. We attempt to capture all these contributing elements in an Affect Calculus and demonstrate experimentally that the resulting method can accurately approximate human judgment. The work reported here is part of a larger effort to develop a highly ac-curate system for identifying, classifying, and comparing metaphors occurring in large volumes of text across four differ-ent languages: English, Spanish, Russian, and Farsi. 1 Introduction We present an approach to identification and val-idation of affect in linguistic metaphors, i.e., metaphorical expressions occurring in written language. Our method is specifically aimed at isolating the affect conveyed in metaphors as opposed to more broad approaches to sentiment classification in the surrounding text. We demonstrate experimentally that our basic Affect Calculus captures metaphor-related affect with a high degree of accuracy when applied to neutral metaphor targets. These are targets that them-selves do not carry any prior valuations. We sub-
sequently expanded and refined this method to properly account for the contribution of the prior affect associated with the target as well as its immediate linguistic context.  2 Metaphor in Language Metaphors are mapping systems that allow the semantics of a familiar Source domain to be ap-plied to a new Target domain so as to invite new frameworks for reasoning (usually by analogy) to emerge in the target domain. The purpose of a metaphor is (a) to simplify or enable reasoning and communication about the target domain that would otherwise be difficult (because of tech-nical complexity) or impossible (due to lack of agreed upon vocabulary) (e.g., Lakoff & John-son, 1980; 2004); or (b) to frame the target do-main in a particular way that enables one form of reasoning while inhibiting another (e.g., Thibodeau & Boroditsky, 2011). The two rea-sons for using metaphors are not necessarily mu-tually exclusive, in other words, (a) and (b) can operate at the same time. The distinction sug-gested above has to do with affect: a metaphor formed through (a) alone is likely to be neutral (e.g., client/server, messenger DNA), while a metaphor formed using (b) is likely to have a polarizing affect (e.g., tax?s burden).  The Source and Target domains that serve as endpoints of a metaphoric mapping can be repre-sented in a variety of ways; however, in a nut-shell they are composed of two kinds of things: concepts and relations. In a Target domain the concepts are typically abstract, disembodied, of-ten fuzzy concepts, such as crime, mercy, or vio-lence, but may also include more concrete, novel, or elaborate concepts such as democracy or eco-nomic inequality. In a Source domain, the con-cepts are typically concrete and physical; howev-er, mapping between two abstract domains is 
42
also possible. (E.g., crime may be both a target and a source domain.)  The relations of interest are those that operate between the concepts within a Source domain and can be ?borrowed? to link concepts within the Target domain, e.g., ?Crime(TARGET) spread to(RELATION) previously safe areas? may be bor-rowing from a DISEASE or a PARASITE source domain.  3 Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approach-es (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language constructs with complex semantic properties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O?Halloran, 2007) that attempt to correlate met-aphor semantics with their usage in naturally oc-curring text but generally lack robust tools to do so; and (3) social science approaches, particular-ly in psychology and anthropology that seek to explain how people produce and understand met-aphors in interaction, but which lack the neces-sary computational tools to work with anything other than relatively isolated examples. In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of meta-phors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the ex-istence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a sys-tem that can recognize metaphor; however their approach is only shown to work in a narrow do-main (The Wall Street Journal, for example).  Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an over-view). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an at-tempt towards computational metaphor identifi-cation. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met-
aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1  workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different ap-proach to metaphor understanding based on lexi-cal semantics and discourse analysis was intro-duced by Strzalkowski et al. (2013). Space con-straints limit our discussion about their work in this article, however in the foregoing, our discus-sion is largely consistent with their framework. 4 Affect in Metaphors Affect in language is understood to mean the atti-tude toward a topic that a speaker/writer attempts to convey to the reader or audience via text or speech (van der Sluis and Mellish 2008).  It is expressed through multiple means, many of which are unrelated to metaphor. While affect in text is often associated, at least in theory, with a variety of basic emotions (anger, fear, etc.), it is generally possible to classify the set of possible affective states by polarity: positive, negative, and sometimes neutral. Affect is also considered to have a graded strength, sometimes referred to as intensity.  Our approach to affect in metaphor has been vetted not only by our core linguistic team but also by an independent team of linguist-analysts with whom we work to understand metaphor across several language-culture groups. Our re-search continues to show no difficulties in com-prehension or disagreement across languages concerning the concept of linguistic affect, of its application to metaphor, and of its having both polarity and intensity.  5 Related Research: sentiment and af-fect There is a relatively large volume of research on sentiment analysis in language (Kim and Hovy, 2004; Strapparava and Mihalcea, 2007; Wiebe and Cardie, 2005; inter alia) that aim at detecting polarity of text, but is not specifically concerned with metaphors. A number of systems were de-veloped to automatically extract writer?s senti-                                                1 The First Workshop on Metaphor in NLP. http://aclweb.org/anthology//W/W13/W13-09.pdf 
43
ment towards specific products or services such as movies or hotels, from online reviews (e.g., Turney, 2002; Pang and Lee, 2008) or social me-dia messages (e.g., Thelwall et al., 2010). None of these techniques has been applied specifically to metaphorical language, and it is unclear if the-se alone would be sufficient due to the relatively complex semantics involved in metaphor inter-pretation. Socher et al. (2013 cite) have recently used recursive neural tensor networks to classify sentences into positive/negative categories. However, the presence of largely negative con-cepts such as ?poverty? in a given sentence overwhelms the sentiment for the sentence in their method. Other relevant efforts in sentence level sentiment analysis include Sem-Eval Task2.  While presence of affect in metaphorical lan-guage is well documented in linguistic and psy-cholinguistic literature (e.g., Osgood, 1980; Pavio and Walsh, 1993; Caffi and Janney, 1994; Steen, 1994), relatively little work was done to detect affect automatically. Some notable recent efforts include Zhang and Barnden (2010), Veale and Li (2012), and Kozareva (2013), who pro-posed various models of metaphor affect classifi-cation based primarily on lexical features of the surrounding text: specifically the word polarity information. In these and other similar approach-es, which are closely related to sentiment analy-sis, affect is attributed to the entire text fragment: a sentence or utterance containing a metaphor, or in some cases the immediate textual context around it.  In contrast, our objective is to isolate affect due to the metaphor itself, independently of its particular context, and also to determine how various elements of the metaphoric expression contribute to its polarity and strength. For exam-ple, we may want to know what is the affect conveyed about the Government as a target con-cept of the metaphor in ?Government regulations are crushing small businesses.? and how it dif-fers in  ?Government programs help to eradicate poverty in rural areas.? or in ?Feds plan to raise the tax on the rich.? In all these examples, there is a subtle interplay between the prior affect as-sociated with certain words (e.g., ?crush?, ?pov-erty?) and the semantic role they occupy in the sentence (e.g., agent vs. patient vs. location, etc.). Our objective is to develop an approach that can better explain such differences. Not sur-prisingly, in one of the target domains we are investigating, the Economic Inequality domain,                                                 2 https://www.cs.york.ac.uk/semeval-2013/task2/ 
there is considerable agreement on the basic atti-tudes across cultures towards the key target con-cepts: poverty is negative, wealth is positive, taxation is largely negative, and so on. This is in a marked contrast with another Target domain, the Governance domain where the target con-cepts tend to be neutral (e.g. bureaucracy, regula-tions etc.) Another important motivation in developing our approach (although not discussed in this pa-per) is to obtain a model of affect that would help to explain empirically why metaphorically rich language is considered highly influential. Persua-sion and influence literature (Soppory and Dillard, 2002) indicates messages containing metaphorical language produce somewhat great-er attitude change than messages that do not. However, some recent studies (e.g., Broadwell et al., 2012) found that lexical models of affect, sentiment, or emotion in language do not corre-late with established measures of influence, con-trary to expectations. Therefore, a different ap-proach to affect is needed based both on lexical and semantic features. We describe this new model below, and show some preliminary results in applications to metaphors interpretation. 6 Basic Affect Calculus The need for a new approach to affect arises from the inability of the current methods of sen-timent analysis to capture the affect that is con-veyed by the metaphor itself, which may be only a part of the overall affect expressed in a text. Affect conveyed in metaphors, while often more polarized than in literal language, is achieved using subtler, less explicit, and more modulated expressions. This presents a challenge for NLP approaches that base affect determination upon the presence of explicit sentiment markers in language that may mask affect arising from a metaphor. This problem becomes more challeng-ing when strong, explicit sentiment markers are present in a surrounding context or when the atti-tude of the speaker/writer towards the target con-cept is considered.  Our initial objective is thus to detect and clas-sify the portion of affect that the speaker/writer is trying to convey by choosing a specific meta-phor. The observables here are the linguistic metaphors that are actually uttered or written; therefore, our method must be able to determine affect present in the linguistic metaphors first and then extrapolate to the conceptual metaphor based on evidence across multiple uses of the 
44
same metaphor. Conceptual metaphors are posit-ed by instances of linguistic metaphors that point to the same source domain. We choose initially to model the speaker/writer perspective; howev-er, it may also be important to determine the ef-fect that a metaphor has on the reader/listener, which we do not address here. Affect in metaphor arises from the juxtaposi-tion of a Source and a Target domain through the relations explicated in linguistic metaphors. The-se relations typically involve one or more predi-cates from the source domain that are applied to a target concept. For example, in ?Government regulations are crushing small businesses.? the relation ?crushing? is borrowed from a concrete source domain (e.g., Physical Burden), and used with an abstract target concept of ?government regulation? which becomes the agentive argu-ment, i.e., crushed(GovReg, X), where X is an optional patientive argument, in this case ?small businesses?. Thus, government regulation is said to be doing something akin to ?crushing?, a harmful and negative activity according to the Affective Norms in English (ANEW) psycholin-guistic database (Bradley and Lang, 1999). Since ?government regulation? is doing something negative, the polarity of affect conveyed about it is also negative. The ANEW lexicon we are us-ing contains ratings of ~100K words. The origi-nal ANEW lexicon by Bradley and Lang was expanded following the work done by Liu et al. (2014) in expanding the MRC imageability lexi-con. While other sources of valence judgments exist such as NRC (Mohammad et al., 2013) and MPQA (Weibe and Cardie, 2005), there are limi-tations ? for instance ? NRC lexicon rates each words on a positive or negative scale, which does not allow for more fine-grained analysis of strength of valence.  Calculation from Table 1 is further general-ized by incorporating the optional second argu-ment of the relation and the role of the target concept (i.e., agentive or patientive). Thus, if X=?small business? as in the example above, the complete relation becomes crushed(GovReg, 
SmBus), which retains negative affect assuming that ?small business? is considered positive or at least neutral, an assessment that needs to be es-tablished independently. The above calculations are captured in the Af-fect Calculus (AC), which was derived from the sociolinguistic models of topical positioning and disagreement in discourse (Broadwell et al., 2013).     The Affect Calculus was conceived as a hypo-thetical model of metaphorical affect, involving the metaphor target, the source relation, as well as the arguments of this relation, one of which is the target itself. The basic version of the AC is shown in Table 1. We should note that the AC allows us to make affect inferences about any of the elements of the metaphoric relation given the values of the remaining elements. We should also note that this calculus does not yet incorpo-rate any discernable prior affect that the target concept itself may carry. When the target con-cept may be considered neutral (as is ?govern-ment regulation? when taken out of context) this table allows us to compute the affect value of any linguistic metaphor containing it. This is un-like the target concepts such as ?poverty? which bring their prior affect into the metaphor. We will return to this issue later. In the Affect Calculus table, Relation denotes a unary or binary predicate (typically a verb, an adjective, or a noun). In the extended version of the AC (Section 6) Relation may also denote a compound consisting of a predicate and one or more satellite arguments, i.e., arguments other than AGENT or PATIENT, such as ORIGIN or DES-TINATION for motion verbs, etc.  7 Extended Affect Calculus The basic Affect Calculus does not incorporate any prior affect that the target concept might bring into a metaphor. This is fine in some do-mains (e.g., Government), where most target concepts may be considered neutral. But in other target domains, such as the Economic Inequality domain, many of the target concepts have a 
Relation type Type 1 (proper-tive) Rel(Target) Type 2 (agentive) Rel (Target, X) Type 3 (patientive) Rel(X, Target) Relation/X  X ? neutral X < neutral X ? neutral X < neutral Positive POSITIVE POSITIVE ? UNSYMP POSITIVE ? SYMPAT Negative NEGATIVE ? UNSYMP ? SYMPAT ? SYMPAT ? SYMPAT Neutral NEUTRAL NEUTRAL ? NEUTRAL NEUTRAL ? NEUTRAL Table 1.  A simple affect calculus specifies affect polarity for linguistic metaphors using a 5-point polar-ity scale [negative < unsympathetic < neutral < sympathetic < positive]. X is the second argument. 
45
strong prior affect in most cultures (e.g., ?pov-erty? is universally considered negative). We thus need to incorporate this prior affect into our calculation whenever an affect-loaded target concept is invoked in a metaphor. Where the basic Affect Calculus simply imposes a context-borne affect upon a neutral target concept, the Advanced Affect Calculus must combine it with the prior affect carried by the target concept, de-pending upon the type of semantic context. As already discussed, we differentiate 3 basic se-mantic contexts (and additional contexts in the extended Affect Calculus discussed in the next section) where the target concept is positioned with respect to other arguments in a metaphorical expression:  ? Propertive context is when a property of a Target is specified (e.g. deep poverty, sea of wealth) ? Agentive context is when the Target appears as an agent of a relation that may involve an-other concept (Argument X) in the patient role (e.g. Government regulations are crush-ing?, Government programs help?) ? Patientive context is when the Target ap-pears in the patient role that involves another concept (possibly implicit, Argument X) in the agent role. (e.g. ?eradicate poverty., ?.navigate government bureaucracy)  Table 1 (in the previous section) specifies how to calculate the affect expressed towards the tar-get depending upon the affect associated with the Relation and the Argument X. In the Advanced Affect Calculus, this table specifies the context-borne affect that interacts with the affect associ-ated with the target. When the target prior affect is unknown or assumed neutral, the AC table is applied directly, as explained previously. When the target has a known polarized affect, either positive or negative, the values in the AC table are used to calculate the final affect by combin-ing the prior affect of the target with an appro-priate value from the table. This is necessary for affect-loaded target concepts such as ?poverty? or ?wealth? that have strong prior affect and can-not be considered neutral.  In order to calculate the combined affect we define two operators ? and ?. These operators form simple polarity algebra shown in Table 2. When the Target is in a Patientive relation, we use ?  to combine its affect with the context val-ue from the AC table; otherwise, we use ? .  In the table for ? operator, we note that combining opposing affects from the Target and the Rela-
tion causes the final affect to be undetermined (UND). In such cases we will take the affect of the stronger element (more polarized score) to prevail. ?  pos neg neu  ?  pos neg neu 
pos pos neg pos pos pos UND pos 
neg neg pos neg neg UND neg neg 
neu pos neg neu neu pos neg neu Table 2: Polarity algebra for extended affect calculus  More specifically, in order to determine the combined polarity score in these cases, we com-pute the distance between each element?s ANEW score and the closest boundary of the neutral range of scores. For example, ANEW scores are assigned on a 10-point continuum (derived from human judgments on 10-point Likert scale) from most negative (0) to most positive (9). Values in the range of 3.0 to 5.0 may be considered neutral (this range can be set differently for target con-cepts and relations): ? Poverty affect score = 1.67 (ANEW) ? 3 (neutral lower) = -1.33 ? Grasp affect score = 5.45 (ANEW) ? 5 (neutral upper)= +0.45 Consider the expression ?poverty?s grasp?. Since poverty is a polarized target concept in Propertive position, we use ? operator to com-bine its affect value with that of Relation (grasp). The result is negative: ? ?Poverty?s grasp? affect score (via AC?) = -1.33 + 0.45 = -0.82 (negative) When the combined score is close to 0 (-0.5 to +0.5) the final affect is neutral. 7.1 Exceptions The above calculus works in a majority of cases, but there are exceptions requiring specialized handling. An incomplete list of these is below (and cases will be added as we encounter them): Reflexive relations. In some cases the target is in the agentive position but semantically it is also a patient, as in ?poverty is spreading?. These cases need to be handled carefully ? although the current AC may be able to handle them in some contexts. When interpreted as an agentive rela-
46
tion, the affect of ?poverty is spreading? comes out as undetermined but would likely be output as negative on the basis of the strong negative affect associated with poverty (vs. weaker posi-tive affect of ?spreading?). When handled as a patientive relation (an unknown force is spread-ing poverty), it comes out clearly and strongly negative. Similarly, ?wealth is declining? is best handled through patientive relation. Therefore, for this AC we will treat intransitive relations as patientive.  Causative relations. Some relations denoted by causative verbs such as ?alleviate?, ?mitigate? or ?ease? appear to presuppose that their patient argument has negative affect, and their positive polarity already incorporates this assumption. Thus, ?alleviate? is best interpreted as ?reduce the negative of?, which inserts an extra negation into the calculation. Without considering this extra negation we would calculate ?alleviate(+) poverty(-)? as negative (doing something posi-tive to a negative concept), which is not the ex-pected reading. Therefore, the proposed special handling is to treat ?alleviate? and similar rela-tions as always producing positive affect when applied to negative targets.  8 Extensions to Basic Affect Calculus The basic model presented in the preceding sec-tion oversimplifies certain more complex cases where the metaphoric relation involves more than 2 arguments. Consequently, we are consid-ering several extensions to the basic Affect Cal-culus as suggested below. The foregoing should be treated as hypotheses subject to validation.  One possible extension involves relations rep-resented by verbs of motion (which is a common source domain) that involve satellite arguments such as ORIGIN and DESTINATION in addition to the main AGENT and PATIENT roles. Any polarity associated with these arguments may impact af-fect directed at the target concept appearing in one of the main role positions. Likewise, we need a mechanism to calculate affect for target concepts found in one of the satellite roles. In ?Federal cuts could push millions into poverty? the relation ?push into? involves three arguments: AGENT (Federal cuts), PATIENT (millions [peo-ple]) and DESTINATION (poverty). In calculating affect towards ?Federal cuts? it is not sufficient to consider the polarity of the predicate ?push? (or ?push into?), but instead one must consider the polarity of ?push into (poverty)? as the compo-site agentive relation involving ?federal cuts?. 
The polarity of this composite, in turn, depends upon the polarity of its destination argument. In other words: polarity(Rel(DEST)) = polarity (DEST) Thus, if ?poverty? is negative, then pushing someone or something into poverty is a harmful relation. Assuming that ?millions [people]? is considered at least neutral, we obtain negative affect for ?Federal cuts? from the basic Affect Calculus table. An analogous situation holds for the ?ORIGIN? argument, with the polarity reversed. Thus: polarity (Rel (ORIGIN)) = ~polarity (ORIGIN) In other words, the act of removing something from a bad place is helpful and positive. For ex-ample, in ?Higher retail wages would lift Ameri-cans out of poverty? the relation compound ?lift out of (poverty)? is considered helpful/positive. Again, once the polarity of the relation com-pound is established, the basic affect calculus applies as usual, thus we obtain positive affect towards ?higher retail wages?. In situations when both arguments are present at the same time and point towards potentially conflicting outcomes, we shall establish a precedence order based on the evidence from human validation data. Another class of multi-argument relations we are considering includes verbs that take an IN-STRUMENT argument, typically signaled by ?with? preposition. In this case, affect inference for the relation compound is postulated as fol-lows: polarity (Rel (INSTR))    = polarity (INSTR) if polarity(INSTR) < neutral   = polarity (Rel) otherwise In other words, using a negative (bad) instru-ment always makes the relation harmful, while using a positive or neutral instrument has no ef-fect on the base predicate polarity.  Other types of multi-argument relations may require similar treatment, and we are currently investigating further possible extensions. In all cases not explicitly covered in the extended Af-fect Calculus, we shall assume the default condi-tion that other satellite arguments (such as TIME, LOCATION, etc.) will have no impact on the po-larity of the source relation compound. In other words: polarity (Rel (s-role)) =default polarity (Rel) 9 Evaluation and Results For an evaluation, our objective is to construct a test that can evaluate the ability of an automated system to correctly identify and classify the af-
47
fect associated with linguistic and conceptual metaphors. A series of naturally occurring text samples containing a linguistic metaphor about a target concept are presented as input to the sys-tem. The system outputs the affect associated with the metaphor, as positive, negative, or neu-tral. The system output is then compared to hu-man generated answer key resulting in an accu-racy score. The evaluation thus consists of two components:  1. Determining the ground truth about affect in test samples;  2. Measuring the automated system?s ability to identify affect correctly.  Step 1 is done using human assessors who judge affect in a series of test samples. Assessors are presented with brief passages where a target concept and a relation are highlighted. They are asked to rank their responses on a 7-point scale for the following questions, among others: ? To what degree does the above passage use metaphor to describe the highlighted concept? ? To what degree does this passage convey an idea that is either positive or negative? It is strictly necessary that input to the system be metaphorical sentences, since affect may be associated with non-metaphoric expressions as well; in fact, some direct expressions may carry stronger affect than subtle and indirect meta-phors. This is why both questions on the survey are necessary: the first focuses the assessor?s at-tention on the highlighted metaphor before ask-ing about affect. If the purpose of the test is to measure the accuracy of assigning affect to a metaphor, then accuracy should be measured against the subset of expressions judged to be metaphorical.  The judgments collected from human asses-sors are tested for reliability and validity. Relia-bility among the raters is computed by measuring intra-class correlation (ICC) (McGraw & Wong, 1996; Shrout & Fleiss, 1979). Typically, a coef-ficient value above 0.7 indicates strong agree-ment. In general, our analyses have shown that we need approximately 30 or more subjects in order to obtain a reliability coefficient of at least 0.7. In addition, certain precautions were taken to ensure quality control in the data. We used the following criteria to discard a subject?s data: (1) completed the task too quickly (i.e., averaged fewer than 10 seconds for each passage); (2) gave the same answer to 85% or more of the test items; (3) did not pass a simple language profi-ciency test; or (4) did not provide correct an-swers to a set of randomly inserted control pas-
sages which have been previously judged by ex-perts to be unequivocally literal or metaphorical. Human judgments are collected using Amazon?s Mechanical Turk services. For each passage in surveys, we would collect at least 30 viable judgments. In addition, we have native language speakers who have been rigorously trained to provide expert judgments on metaphor and affect identification task. Table 3 shows the intra-class correlations for affect determination amongst Mechanical Turk subjects. Experiments were conducted in 4 languages: English, Spanish, Rus-sian, and Farsi.   English Spanish Russian Farsi Metaphor 0.864 0.853 0.916 0.720 Affect 0.924 0.791 0.713 0.797 Table 3: Intra-class correlations for metaphor and affect assessment by Mechanical Turk sub-jects In Figure 1, we present partial evidence that the human assessment collection method cap-tures the phenomenon of affect associated with metaphors. The chart clearly shows that affect tends to be more polarized in metaphors than in literal expressions. The chart is based on more than 11,000 affect judgments for English linguis-tic metaphors and literal expressions about Gov-ernance concepts. We see a highly pronounced tendency towards the polarization of affect (both positive and negative). Ratings of affect (y-axis) in metaphoric expressions (columns 5-7) are judged to be stronger, and in particular more negative than the literal expressions (columns 1-3). A similar trend occurs with other target con-cepts as well as other languages, although the data are less reliable due to smaller test samples. Once an answer key is established using the aforementioned procedures, system accuracy can be determined from a confusion matrix as shown in Table 4. In Table 4, we show system assign-ment of affect versus answer key for English Governance and Economic Inequality target metaphors. Overall accuracy across positive, negative and neutral affect for English test set of 220 samples is 74.5%. Analogous confusion ma-trices have been constructed for Spanish, Russian and Farsi. NLP resources such as parser and lex-icons for the languages other than English are not as robust or well rounded; therefore affect classi-fication accuracy in those languages is impacted.   
48
 Figure 1: Distribution of affect polarity in hu-man judgment of English literal and metaphori-cal expressions from the Governance domain. Metaphoricity of an expression (x-axis) is judged from highly literal (1) to highly metaphorical (7)   Table 5 shows the accuracy of affect detection for expressions that the system determined to be metaphors across all four languages under inves-tigation. Evaluation set for numbers reported in Table 5 contains a total of 526 linguistic meta-phors in these four languages.   English Affect Sample size = 220 System identified as Positive Negative Neutral 
Answ
er Key 
Positive 40 16 3 Nega-tive 12 109 1 Neutral 10 14 15  Table 4: Confusion matrix for affect classifi-cation in English linguistic metaphors in Gov-ernance and Economic Inequality Domain. Accu-racy is 74.5%   English Spanish Russian Farsi 
Accuracy 74.5% 71% 59% 64% Table 5: Performance on affect classification for linguistic metaphors in four languages 10 Conclusion In this paper we presented a new approach to automatic computing of affect in metaphors that exploits both lexical and semantic information in metaphorical expressions. Our method was eval-uated through a series of rigorous experiments 
where more than several dozen of qualified as-sessors judged hundreds of sentences (extracted from online sources) that contained metaphorical expressions. The objective was to capture affect associated with the metaphor itself. Our system can approximate human judgment with accuracy ranging from 59% for Russian to 74% for Eng-lish. These results are quite promising. The dif-ferences are primarily due to varied robustness of the language processing tools (such as parsers and morphological analyzers) that are available for each language. We note that a direct compar-ison to lexical approaches such as described by Kozareva (2013) is not possible at this time due to differences in assessment methodology, alt-hough it remains one of our objectives.  Our next step is to demonstrate that the new way of calculating affect can lead to a reliable model of affective language use that correlates with other established measures of influence.  Acknowledgements Supported by the Intelligence Advanced Re-search Projects Activity (IARPA) via Depart-ment of Defense US Army Research Laboratory contract number W911NF-12-C-0024. The U.S. Government is authorized to reproduce and dis-tribute reprints for Governmental purposes not-withstanding any copyright annotation thereon.  Disclaimer: The views and conclusions con-tained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either ex-pressed or implied, of IARPA, DoD/ARL, or the U.S. Government. References David W. Allbritton, Gail McKoon, and Richard J. Gerrig. 1995. Metaphor-based schemas and text Representations: making connections through conceptual metaphors, Journal of Experimental Psychology: Learning, Memory, and Cognition, 21(3):612-625. Eric P. S. Baumer, James P. White, and Bill Tomlin-son. 2010. Comparing semantic role labeling with typed dependency parsing in computational meta-phor identification. In Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 14?22, Los Angeles, California.  Margaret M. Bradley, and Peter Lang. 1999. Affective norms for English words (ANEW): Instruction manual and affective ratings. Technical Report C-2. University of Florida, Gainesville, FL. George Aaron Broadwell, Umit Boz, Ignacio Cases, Tomek Strzalkowski, Laurie Feldman, Sarah Tay-
49
lor, Samira Shaikh, Ting Liu, Kit Cho, and Nick Webb. 2013. Using imageability and topic chain-ing to locate metaphors in linguistic corpora. In Proceedings of International Conference on So-cial Computing, Behavioral-Cultural Modeling, & Prediction, pages 102?109. Washington D.C. George Aaron Broadwell, Jennifer Stromer-Galley, Tomek Strzalkowski, Samira Shaikh, Sarah Tay-lor, Umit Boz, Alana Elia, Laura Jiao, Ting Liu and Nick Webb. 2012. Modeling socio-cultural phenomena in discourse. Journal of Natural Lan-guage Engineering, pages 1?45. Cambridge Press. Claudia Caffi, and Richard W. Janney. 1994. Towards a pragmatics of emotive communication. Jour-nal of Pragmatics, 22:325?373. Jaime Carbonell. 1980. Metaphor: A key to extensible semantic analysis. In Proceedings of the 18th An-nual Meeting on Association for Computational Linguistics. Jonathan, Charteris-Black. 2002. Second language figurative proficiency: A comparative study of Malay and English. Applied Linguistics 23(1):104?133. Dan, Fass. 1991. met*: A Method for Discriminating Metonymy and Metaphor by Computer. Computa-tional Linguistics, 17:49-90 Jerome Feldman, and Srinivas Narayanan. 2004. Em-bodied meaning in a neural theory of language. Brain and Language, 89(2):385?392. Christiane D. Fellbaum. 1998. WordNet: An electron-ic lexical database (1st ed.). MIT Press. Matt Gedigian, John Bryant, Srini Narayanan and Branimir Ciric. 2006. Catching Metaphors. In Proceedings of the Third Workshop on Scalable Natural Language Understanding ScaNaLU 2006, pages 41?48. New York City: NY. Dirk Hovy, Shashank Shrivastava, Sujay Kumar Jau-har, Mrinmaya Sachan, Kartik Goyal, Huying Li, Whitney Sanders and Eduard Hovy. 2013. Identi-fying Metaphorical Word Use with Tree Kernels. In the Proceedings of the First Workshop on Met-aphor in NLP, (NAACL). Atlanta. Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th international conference on Computational Linguistics, COLING ?04. Zornitsa Kozareva. 2013. Multilingual Affect Polarity and   Valence Prediction in Metaphor-Rich Texts. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013) Saisuresh Krishnakumaran and Xiaojin Zhu. 2007. Hunting elusive metaphors using lexical resources. In Proceedings of the Workshop on Computation-al Approaches to Figurative Language, pages 13?20. Rochester, NY. George Lakoff, and Mark Johnson. 1980. Metaphors we live by. University Of Chicago Press, Chicago, Illinois. 
George, Lakoff. 2001. Moral politics: what conserva-tives know that liberals don?t. University of Chi-cago Press, Chicago, Illinois. Ting Liu, Kit Cho, George Aaron Broadwell, Samira Shaikh, Tomek Strzalkowski, John Lien, Sarah Taylor, Laurie Feldman, Boris Yamrom, Nick Webb, Umit Boz and Ignacio Cases. 2014. Auto-matic Expansion of the MRC Psycholinguistic Da-tabase Imageability Ratings. In Proceedings of 9th Language Resources and Evaluation Conference, (LREC 2014)Reykjavik, Iceland. Liisa, Malkki.  1992. National geographic: The root-ing of people and the territorialization of national identity among scholars and refugees. Society for Cultural Anthropology, 7(1):24?44. James Martin. 1988. A computational theory of meta-phor. Ph.D. Dissertation. Kenneth O. McGraw and S. P. Wong. 1996. Forming inferences about some intraclass correlation coef-ficients. Psychological Methods, 1(1): 30?46. Mohammad, S.M., S. Kiritchenko, and X. Zhu. 2013. NRC-Canada: Building the state-of-the-art insen-timent analysis of tweets. In Proceedings of the Seventh International Workshop on Semantic Evaluation Exercises (SemEval-2013), Atlanta, Georgia, USA, June 2013. Michael Mohler, David Bracewell, David Hinote, and Marc Tomlinson. 2013. Semantic signatures for example-based linguistic metaphor detection. In The Proceedings of the First Workshop on Meta-phor in NLP, (NAACL), pages 46?54. Musolff, Andreas. 2008. What can critical metaphor analysis add to the understanding of racist ideolo-gy? Recent studies of Hitler?s anti-semitic meta-phors, critical approaches to discourse analysis across disciplines. Critical Approaches to Dis-course Analysis Across Disciplines, 2(2):1?10. Kieran, O?Halloran. 2007. Critical discourse analysis and the corpus-informed interpretation of meta-phor at the register level. Oxford University Press Charles E. Osgood. 1981. The cognitive dynamics of synaesthesia and metaphor. In Proceedings of the National Symposium for Research in Art. Learn-ing in Art: Representation and Metaphor, pages 56-80. University of Illinois Press. Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135, January. Allan Pavio and Mary Walsh. 1993. Psychological processes in metaphor comprehension and memory. In Andrew Ortony, editor, Meta-phor and thought (2nd ed.). Cambridge: Cambridge University Press. Patrick E Shrout and Joseph L Fleiss. 1979. Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin, 86(2):420?428. Ekaterina Shutova. 2010. Models of metaphors in NLP. In Proceedings of ACL 2010. Uppsala, Swe-den. 
50
Ekaterina Shutova and Simone Teufel. 2010a. Meta-phor corpus annotated for source - target domain mappings. In Proceedings of Language Resources and Evaluation Conference 2010. Malta. Ekaterina Shutova. 2010b. Models of metaphor in nlp. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ?10, pages 688?697. Ekaterina Shutova, Tim Van de Cruys, and Anna Korhonen. 2012. Unsupervised metaphor para-phrasing using a vector space model In Proceed-ings of COLING 2012, Mumbai, India Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. 2013. In Proceedings Conference on Empir-ical Methods in Natural Language Processing (EMNLP 2013). Seattle, USA.  Sopory, P. and Dillard, J. P. (2002), The Persuasive Effects of Metaphor: A Meta-Analysis. Human Communication Research, 28: 382?419. doi: 10.1111/j.1468-2958.2002.tb00813.x Gerard Steen. 1994. Understanding metaphor in lit-erature: An empirical approach. London: Long-man. Carlo, Strapparava, and Rada Mihalcea. 2007. Semeval-2007 task 14: Affective text. In Proceed-ings of the Fourth International Workshop on Se-mantic Evaluations, pages 70?74. Association for Computational Linguistics. Tomek Strzalkowski, George Aaron Broadwell, Sarah Taylor, Laurie Feldman, Boris Yamrom, Samira Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases and Kyle Elliott. 2013. Robust extraction of metaphor from novel data. In Proceedings of Workshop on Metaphor in NLP, NAACL. Atlanta. Mike Thelwall, Kevan Buckley, and Georgios Pato-glou. Sentiment in Twitter events. 2011. Journal of the American Society for Information Science and Technology, 62(2):406?418. Paul H. Thibodeau and Lera Boroditsky. 2011. Meta-phors We Think With: The Role of Metaphor in Reasoning. PLoS ONE 6(2): e16782. Peter D, Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised clas-sification of reviews. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ?02, pages 417?424. Ielka van der Sluis,  and C. Mellish 2008. Toward affective natural language deneration: Empirical investigations. affective language in human and machine. AISB 2008 Proceedings Volume 2. Tony Veale and Guofu Li. 2012. Specifying view-point and information need with affective meta-phors: a system demonstration of the metaphor magnet web app/service. In Proceedings of the ACL 2012 System Demonstrations, ACL ?12, pag-es 7?12. Janyce, Wiebe and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. In Language Resources and Evaluation. 
Yorick, Wilks. 1975. Preference semantics. Formal Semantics of Natural Language, E. L. Keenan, Ed. Cambridge University Press, Cambridge, U.K., 329?348. Yorick Wilks, Lucian Galescu, James Allen, Adam Dalton. 2013. Automatic Metaphor Detection us-ing Large-Scale Lexical Resources and Conven-tional Metaphor Extraction. In the Proceedings of the First Workshop on Metaphor in NLP, (NAACL). Atlanta.  Wiebe, J., Wilson, T., and Cardie, C.: Annotating expressions of opinions and emotions in  lan-guage. Language Resources and Evaluation, 39(2-3), pp. 165-210 (2005). Li Zhang and John Barnden. 2010. Affect and meta-phor sensing in virtual drama. International Journal of Computer Games Technology. Vol. 2010.  
51
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 210?220,
Dublin, Ireland, August 23, 2014.
Discovering Conceptual Metaphors Using Source Domain Spaces 
  Samira Shaikh1, Tomek Strzalkowski1, Kit Cho1, Ting Liu1, George Aaron Broadwell1, Laurie Feldman1, Sarah Taylor2, Boris Yamrom1, Ching-Sheng Lin1, Ning Sa1, Ignacio Cases1, Yuli-ya Peshkova1 and Kyle Elliot3  1State University of New York  ? University at Albany 2Sarah M. Taylor Consulting LLC samirashaikh@gmail.com     
3Plessas Experts Network 
 Abstract This article makes two contributions towards the use of lexical resources and corpora; specifically making use of them for gaining access to and using word associations. The direct application of our approach is for detecting linguistic and conceptual metaphors automatically in text. We describe our method of building conceptual spaces, that is, defining the vocabulary that characterizes a Source Domain (e.g., Disease) of a conceptual metaphor (e.g., Poverty is a Disease). We also describe how these conceptual spaces are used to group linguistic metaphors into conceptual metaphors. Our method works in multiple languages, including English, Spanish, Russian and Farsi. We provide details of how our method can be evaluated and evaluation results that show satisfactory performance across all languages. 1 Introduction Metaphors are communicative devices that are pervasive in discourse. When understood in a cultural context, they provide insights into how a culture views certain salient concepts, typically broad, abstract concepts such as poverty or democracy. In our research, we are focusing on metaphors on targets of governance, economic inequality and democracy, although our approach works for metaphors on any target. Suppose it is found in a culture that its people use metaphors when speaking of poverty; for example, they may talk about ?symptom of poverty? or that ?poverty infects areas of the city?. These expressions are linguistic metaphors that are instances of a broader conceptual metaphor: Poverty is a Disease. Similarly, if it is found that common linguistic metaphors about poverty for peoples of a culture include ?deep hole of poverty? and ?fall into poverty?, it would lead to the conceptual metaphor: Poverty is an Abyss. A communicator wishing to speak of ways to deal with poverty would use metaphors such as ?treat poverty? and ?cure poverty? to make their framing consistent with the conceptual metaphor of Disease, whereas she would use metaphors such as ?lift out of poverty? when speaking to people who are attuned to the Abyss conceptual metaphor. Here Disease and Abyss are source domains, and poverty is the target domain. Relations, like ?symptom of?, ?infect? and ?fall into? from the respective source domains are mapped onto the target domain of poverty. In order to discover conceptual metaphors and group linguistic metaphors together, we make use of corpora to define the conceptual space that characterizes a source domain. We wish to discover the set of relations that are used literally for a given source domain, and would create metaphors if applied to some other target domain. That is, we wish to automatically discover that relations such as ?symptom?, ?infect?, ?treat? and ?cure? characterize the source domain of Disease, for example. To create the conceptual spaces, we employ a fully automated method in which we search a balanced corpus using specific search patterns. Search patterns are so created as to look for co-occurence of                                                 This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/  
210
relations with members of a given source domain. Relations could be nouns, verbs, verb phrases and adjectives that are frequently used literally within a source domain. In addition, we calculate the frequency with which relations occur in a given source domain, or Relation Frequency. We then calculate the Inverse Domain Frequency (IDF), a variant of the inverse document frequency measure quite commonly used in field of information retrieval; the IDF captures the degree of distribution of relations across all source domains under consideration. Using these two measures, the relation frequency and inverse domain frequency, we are able to rank relations within a source domain. This ranked list of relations are then used to group linguistic metaphors belonging to the same source domain together. A group of linguistic metaphors so formed is a conceptual metaphor.  2 Related Research Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as de-fined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language constructs with complex semantic properties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O?Halloran, 2007) that attempt to correlate metaphor semantics with their us-age in naturally occurring text but generally lack robust tools to do so; and (3) social science ap-proaches, particularly in psychology and anthropology that seek to explain how people deploy and understand metaphors in interaction, but which lack the necessary computational tools to work with anything other than relatively isolated examples.     Metaphor study in yet other disciplines has included cognitive psychologists (e.g., Allbritton, McKoon & Gerrig, 1995) who have focused on the way metaphors may signify structures in human memory and human language processing. Cultural anthropologists, such as Malkki in her work on ref-ugees (1992), see metaphor as a tool to help outsiders interpret the feelings and mindsets of the groups they study, an approach also reflective of available metaphor case studies, often with a Political Sci-ence underpinning (Musolff, 2008; Lakoff, 2001).      In computational investigations of metaphor, knowledge-based approaches include MetaBank (Mar-tin, 1994), a large knowledge base of metaphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfacto-rily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example).     Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is diffi-cult to produce in large quantities and may not be easily generated in different languages.  More recently, several important approaches to metaphor extraction have emerged from the IARPA Metaphor program, including Broadwell et al (2013), Strzalkowski et al. (2014), Wilks et al (2013), Hovy et al (2013) inter alia. These papers concentrate on the algorithms for detection and classification of individual linguistic metaphors in text rather than formation of conceptual metaphors in a broader cultural context. Taylor et al (2014) outlines the rationale why conceptual level metaphors may provide important insights into cross-cultural contrasts. Our work described here is a first attempt at automatic discovery of conceptual metaphors operating within a culture directly from the linguistic evidence in language. 3 Our Approach The process of discovering conceptual metaphors is necessarily divided into two phases: (1) collecting evidence about potential source domains that may be invoked when metaphorical expressions are used; and (2) building a conceptual space for each sufficiently evidenced source domain so that linguistic metaphors can be accurately classified as instances of appropriate conceptual metaphors. In 
211
this paper, we concentrate on the second phase only. Strzalkowski et al (2013) in their work have described a data-driven linguistic metaphor extraction method and our approach builds upon their work. During the source domain evidencing phase, we established a set of 50 source domains that operate frequently with the target concepts we are focusing on (government, bureaucracy, poverty, wealth, taxation, democracy and elections). These domains were a joint effort of several teams participating in the Metaphor program and we are taking this set as a starting point. These are shown in Table 1.   A_GOD	 ? CONFINEMENT	 ? GAME	 ? MONSTER	 ? PLANT	 ?A_RIGHT	 ? CRIME	 ? GAP	 ? MORAL_DUTY	 ? PORTAL	 ?ABYSS	 ? CROP	 ? GEOGRAPHIC_FEATURE	 ? MOVEMENT	 ? POSITION	 ?AND	 ?CHANGE	 ?OF	 ?	 ?POSITION	 ?ON	 ?A	 ?SCALE	 ?ADDICTION	 ? DARKNESS	 ? GREED	 ? NATURAL_PHYSICAL_FORCE	 ? RACE	 ?ANIMAL	 ? DESTROYER	 ?	 ? HUMAN_BODY	 ? OBESITY	 ? RESOURCE	 ?BATTLE	 ? DISEASE	 ? IMPURITY	 ? PARASITE	 ? STAGE	 ?BLOOD_STREAM	 ? ENERGY	 ? LIGHT	 ? PATHWAY	 ? STRUGGLE	 ?BODY_OF_WATER	 ? ENSLAVEMENT	 ? MACHINE	 ? PHYSICAL_BURDEN	 ? THEFT	 ?BUILDING	 ? FOOD	 ? MAZE	 ? PHYSICAL_HARM	 ? VISION	 ?COMPETITION	 ? FORCEFUL_EXTRACTION	 ? MEDICINE	 ? PHYSICAL_LOCATION	 ? WAR	 ?Table 1. Set of 50 source domains that operate frequently with target concepts being investigated. Only English names are shown for ease of presentation, equivalent sets in Spanish, Russian and Farsi have been created. Some of the domains are self explanatory, while others require a further specification since the labels are sometimes ambiguous. For example, PLANT represents things that grow in the soil, not factories; similarly, BUILDING represents artifacts such as houses or edifices, but not the act of constructing something; RACE refers to a running competition, not skin color, etc.  Consequently, each of these domains need to be seeded with the prototypical representative elements to make the meaning completely clear. This seeding occurs during the first phase of the process when a linguistic expression, such as ?cure poverty? is classified as a linguistic metaphor. This process of classifying ?cure poverty? as metaphorical is described in detail in Strzalkowski et al. (2013). Part of the seeding process is to establish that a source domain different than the target domain (here: poverty) is invoked by the relation (here: cure). To find the source domain where ?cure? is typically used literally, we form a linguistic pattern [cure [OBJ: X/nn]] (derived automatically from the parsed metaphoric expression) which is subsequently run through a balanced language corpus. Arguments matching the variable X are then clustered into semantic categories, using lexical resources such as Wordnet (Felbaum, 1998) and the most frequent and concrete category is selected as a possible source domain (proto-source domain). From the balanced language corpus, it is possible to compute the frequency with which the arguments resulting from search appear with relation (?cure?). We determine concreteness by looking up concreteness score in MRC psycholinguistic database (Coltheart 1981, Wilson 1988). As may be expected, the initial elements of the proto-source obtained from the above patterns will include: disease, cancer, plague, etc. These become the seeds of the source domain DISEASE in our list. The same process was performed for each of the 50 domains listed here, for each of the 4 languages under consideration. Additional Source Domains are continously generated bottom-up fashion by this phase 1 process elaborated above. In Table 2, we show seeds so obtained for a few source domains.    DISEASE	 ? disease,	 ?cancer,	 ?plague	 ?ABYSS	 ? abyss,	 ?chasm,	 ?crevasse	 ?BODY_OF_WATER	 ? ocean,	 ?lake	 ?river,	 ?pond,	 ?sea	 ?PLANT	 ? plant,	 ?tree,	 ?flower,	 ?weed,	 ?shrub,	 ?vegetable	 ?GEOGRAPHIC_FEATURE	 ? land,	 ?land	 ?form,	 ?earth,	 ?mountain,	 ?plateau,	 ?island,	 ?valley	 ?Table 2. Example of seeds corresponding to a few source domains 
212
Once such seeds are obtained, we perform another search through a balanced corpus in the corresponding language to discover relations that characterize the source domains. The purpose of source domain spaces in our research is two-fold: a) to provide a sufficiently complete characterization of a source domain via a list of relations ; and b) such a list of relations should sufficiently distinguish between different source domains. Creating these spaces is phase 2 of the conceptual metaphor discovery process. We search for nouns, verbs and verb phrases, and adjectives that co-occur with seeds of given source domain with sufficiently high frequency and sufficiently high mutual information. Our goal with this process is to approximate normal usage patterns of relations within source domains. The results of balanced corpora search form our conceptual spaces. The balanced corpora we use are English: Corpus of Contemporary American English (Davies, 2008), Spanish:  Corpus del Espa?ol Actual (Davies, 2002), Russian: Russian National Corpus2 and Farsi: Bijankhan Corpus (Oroumchian et al., 2006). In addition to retrieving the relations, we retrieve the frequency with which these relations can be found to co-occur with seeds of a source domain, Relation Frequency (RF). We calculate Inverse Domain Frequency (IDF) of all relations across all 50 source domains using a variant of the inverse document frequency measure. The formula for IDF is as given below:  IDF = log (total number of source domains / total number of source domains a relation appears in)  For example, if a relation such as ?dive into? is found to appear in two source domains, BODY_OF_WATER and GEOGRAPHIC_FEATURE, then the IDF for ?dive into? would be log (50/2). The rank of a relation is computed as the product of RF and IDF. However, computing rank using RF without normalization results in inflated ranks for relations that are quite common across domains even when they do not sufficiently disambiguate between the domains. We assume a normal distribution of frequencies of relations within a source domain and normalize RF by taking its logarithm. We also normalize with respect to seeds within a source domain. If a relation frequency is disproportionately high with a specific seed, we disregard that frequency. For example, one of the seeds for the source domain of BUILDING is ?house?. A search through balanced corpus for nouns adjacent to ?house? revealed a disproportionately large number for ?white?, which is meant to be the White House, and would be disregarded.  In Table 3, we show a few top ranked relations for the source domains DISEASE and BODY_OF_WATER. In columns 1 and 2, we show the source domain and the relation. Column 3 shows the relation frequency and column 4 shows the part of speech of relation (V=verb or verb phrase, N=noun, ADJ=adjective). An RF score of 800 for row 1 indicates that the relation ?diagnose with? appears 800 times with one or more of the seeds we search for source domain DISEASE (?diagnose with cancer?, ?diagnose with disease? and so on. In column 5, we show the position where the relation is commonly found to co-occur with the source domain. For example, ?afflict? in row 2 has a position ?after? which means it appears after DISEASE: ?DISEASE afflict(s)?; whereas row 3 would be read as ?affict with DISEASE? since it appears ?before?. In column 6, we show the normalized RF*IDF score. The highest RF*IDF score for a relation across our spaces is 2.165. From Table 3, we can see that even if  frequency for some relations may be relatively low, their rank would be high if they are strongly associated with a single source domain.    	 ? 1.	 ?Source	 ?Domain	 ? 2.	 ?Relation	 ? 3.	 ?RF	 ? 4.	 ?Type	 ? 5.	 ?Position	 ? 6.	 ?Norm	 ?RF*IDF	 ?1	 ? DISEASE	 ? diagnose	 ?with	 ? 800	 ? V	 ? before	 ? 1.94	 ?2	 ? DISEASE	 ? afflict	 ? 85	 ? V	 ? after	 ? 1.67	 ?3	 ? DISEASE	 ? afflict	 ?with	 ? 33	 ? V	 ? before	 ? 1.52	 ?4	 ? DISEASE	 ? cure	 ?of	 ? 29	 ? N	 ? before	 ? 1.46	 ?5	 ? BODY_OF_WATER	 ? dive	 ?into	 ? 49	 ? V	 ? before	 ? 2.01	 ?6	 ? BODY_OF_WATER	 ? wade	 ?through	 ? 44	 ? V	 ? before	 ? 1.88	 ?7	 ?	 ? BODY_OF_WATER	 ? wade	 ?into	 ? 42	 ? V	 ? before	 ? 1.84	 ?8	 ? BODY_OF_WATER	 ? rinse	 ?in	 ? 41	 ? V	 ? before	 ? 1.80	 ?Table 3. A few top ranking relations for the source domains DISEASE and BODY_OF_WATER. Relations are ranked by their normalized RF*IDF score.                                                 2 http://ruscorpora.ru/en/ 
213
With the conceptual spaces defined in this manner, we can now use them to group linguistic metaphors together. Shaikh et al (2014) have created a repository of thousands of automatically extracted lingusitic metaphors in all four languages, which we are using to create conceptual metaphors. To discover which conceptual metaphors exist within such large sets of linguistic metaphors would be quite challenging, if not impossible, for a human expert. We automatically assign each linguistic metaphor to ranked list of source domains.  Consider the linguistic metaphor ?plunge into poverty?, where the relation is ?plunge into?. We search through our conceptual spaces and retrieve a list of source domains where the relation ?plunge into? may appear. From this list, only the domains that have this relation RF*IDF score higher than a threshold are considered. This threshold is currently assigned to be 0.40, although it is subject to further experimentation. The source domain where the RF*IDF score of ?plunge into? is the highest is chosen as the source domain, along with the next source domains only if the difference in scores is 5% or lower. Tables 4 and 5 depicts this part of algorithm for two relations, ?plunge into? and ?explorar? (from Spanish ? ?explore?). The relation ?plunge into? is thus assigned to BODY_OF_WATER source domain. ?explorar? is assigned to GEOGRAPHIC_FEATURE and BODY_OF_WATER since difference in RF*IDF scores is less than 5%.  Relation	 ? Source	 ?Domains	 ? RF*IDF	 ?	 ? 	 ? Relation	 ? Source	 ?Domains	 ? RF*IDF	 ?
plunge	 ?into	 ?	 ?
BODY_OF_WATER	 ? 1.82	 ? 	 ?
explorar	 ?
GEOGRAPHIC_FEATURE	 ? 0.77	 ?DARKNESS	 ? 1.28	 ? 	 ? BODY_OF_WATER	 ? 0.76	 ?ABYSS	 ? 0.68	 ? 	 ? PHYSICAL_LOCATION	 ? 0.56	 ?WAR	 ? 0.57	 ? 	 ? PATHWAY	 ? 0.56	 ?GEOGRAPHIC_FEATURE	 ? 0.48	 ? 	 ? BUILDING	 ? 0.41	 ?Table 4 and Table 5. Assigning relations of linguistic metaphor to source domains. ?plunge into? is assigned to BODY_OF_WATER; ?explorar? is assigned to GEOGRAPHIC_FEATURE and BODY_OF_WATER Once this process of assigning linguistic metaphors to source domains is accomplished for all linguistic metaphors in our repository, we validate the resulting conceptual metaphors. A small percentage of metaphors cannot be assigned to any of the 50 Source Domains. We explain the validation process in Section 4. In Tables 6 and 7, we show sample conceptual metaphors in English and Spanish. Our validation process revealed an interesting insight regarding forming conceptual metaphor, wherein they should contain relations that are anchors for that given source domain that we shall describe next.  
 Table 6. A conceptual metaphor in English: POVERTY is a BODY_OF_WATER 
214
 Table 7. A conceptual metaphor in Spanish: POVERTY is a DISEASE 3.1 Anchor relations in Conceptual Metaphors When human assessors are presented with a set of linguistic metaphors and the task to assign them into a source domain, some relations will have stronger impact on their decision that others. For example, ?cure? would almost invariably be assigned to DISEASE domain, while ?dive in? would invoke BODY_OF_WATER domain. Other relations, such as ?spread? or ?fall into? are less specific, however, when paired with highly evocative relations above are likely to be classified the same way. Thus, there are two types of metaphorical relations in linguistic metaphors: (1) the highly evocative relations that unambigously point to a specific source domain ? we shall call them anchors; and (2) the relations that are compatible with the anchor but are not anchors themselves. We can add another class: (3) the relations that are not compatible with a given anchor. Thus, a set of linguistic metaphors that provides evidence for a conceptual metaphor should contain at least some anchor relations and the balance of the set may be composed of anchor-compatible relations. Our current hypothesis is that there should be at least one anchor for each 7 anchor compatible relations for a group of linguistic metaphors to provide a sufficient evidence for a conceptual metaphor.  As part of our validation process, we conducted a series of experiments with human assessors. One of the tasks was to assign a single linguistic metaphor to one of 50 source domains. As an illustrative example, we show in Table 8, one linguistic metaphor. When presented with this example, a majority of assessors chose ENEMY source domain, while DISEASE was selected second. Additionally, there was greater variance among their selections, only 31% chose the top source domain of ENEMY.  Subsequently, human assessors were presented a set of linguistic metaphors where at least one anchor relation was present. In this case, the majority of assessors chose the DISEASE source domain. Even though the ?fight against poverty? example was included in the set, the presence of anchors such as ?cure poverty? and ?treat poverty? lead assessors to choose DISEASE source domain. The variance in selection was also less, a 70% majority choosing DISEASE. We show the conceptual metaphor in Table 9.  The	 ?summit	 ?has	 ?proven	 ?that	 ?there	 ?is	 ?a	 ?renewed	 ?appetite	 ?for	 ?the	 ?fight	 ?against	 ?poverty.	 ?	 ?ENEMY:	 ?31%;	 ?DISEASE:	 ?17%;	 ?ANIMAL,	 ?MONSTER,?.<10%	 ?Table 8. A single linguistic metaphor was assigned a varied number of source domains by human assessors.   Of	 ?course,	 ?many	 ?government	 ?programs	 ?aim	 ?to	 ?alleviate	 ?poverty.	 ?We	 ?seek	 ?to	 ?stimulate	 ?true	 ?prosperity	 ?rather	 ?than	 ?simply	 ?treat	 ?poverty.	 ?Unless	 ?the	 ?fight	 ?against	 ?poverty	 ?is	 ?honestly	 ?addressed	 ?by	 ?the	 ?West,	 ?there	 ?will	 ?be	 ?many	 ?more	 ?Afghanistans.	 ?Above	 ?all,	 ?he	 ?knows	 ?that	 ?the	 ?only	 ?way	 ?to	 ?cure	 ?poverty	 ?is	 ?to	 ?grow	 ?the	 ?economy.	 ?	 ?DISEASE:	 ?70%;	 ?ENEMY:	 ?30%	 ?Table 9. A conceptual metaphor containing anchors. When sample metaphor from Table 8 is included in this set, human assessors still choose the source domain to be DISEASE. 
215
4 Evaluation and Results A group of human experts who are native speakers and have been substantively trained to achieve high levels of agreement (0.78 Krippendorf?s alpha (1970) or higher) form our validation team. In addition, we aim to run crowd-sourced experiments on Amazon Mechanical Turk. In Figure 1, we show a web interface we built to present our human assessors. The task shown here is the assignment of a single linguistic metaphor to one of 50 source domains. Then, we present our validation team with conceptual metaphors we created. Each conceptual metaphor is validated by at least two language experts. This interface is shown in Figure 2. These interfaces are carefully created by our team of social scientists and psychologists, designed to elicit proper responses from native speakers of the language.  
 Figure 1. Interface of task where human assessors select source domain for a single linguistic metaphor.  
216
 Figure 2. Interface of task where human assessors select source domains for a conceptual metaphor. Assessors provide their top two choices along with a description detailing how they made their decision.  In Table 10, we show the number of conceptual metaphors currently in the repository and the accuracy of our method across four languages, as computed by using validation data. We show the number of conceptual metaphors present in the Governance target domain (metaphors about government and bureaucracy), Economic Inequality (dealing with metaphors of poverty, wealth and taxation) and Democracy (democracy and elections metaphors). These conceptual metaphors on the three target domains of Governace, Economic Inequality and Democracy, when compared across cultures could provide deep insight about peoples? perceptions regarding salient concepts. We note that Russian and Farsi performance is lower than that in English and Spanish. The size of balanced corpus and accuracy of lexical tools such as stemmers and morphological analyzers affect performance of our algorithm.  The Farsi balanced corpus is relatively small when compared to English balanced corpus. The smaller size affects computation of statistics such as Relation Frequency and subsequently the thresholds of RF*IDF scores. One improvement we are currently investigating is that the thresholds may be set specifically for a language.   	 ? ENGLISH	 ? SPANISH	 ? RUSSIAN	 ? FARSI	 ?#	 ?of	 ?Governance	 ?Conceptual	 ?Metaphors	 ? 27	 ? 7	 ? 8	 ? 7	 ?#	 ?of	 ?Economic	 ?Inequality	 ?Conceptual	 ?Metaphors	 ? 32	 ? 26	 ? 57	 ? 7	 ?#	 ?of	 ?Democracy	 ?	 ?Conceptual	 ?Metaphors	 ? 51	 ? 16	 ? 18	 ? 8	 ?Total	 ?#	 ?of	 ?	 ?Conceptual	 ?Metaphors	 ? 110	 ? 49	 ? 83	 ? 22	 ?Accuracy	 ?(%)	 ?	 ? 85%	 ? 76%	 ? 67%	 ? 62%	 ?Table 10. Number of conceptual metaphors discovered thus far and performance of our approach across four languages. 
217
5 Conclusion and Future Work In this article, we presented our approach towards automatic discovery of conceptual metaphors directly from linguistic evidence in a given language. We make use of corpora in two unique ways: the first is to discover prototypical seeds that form the basis of source domains and second is to create conceptual spaces that allow us to characterize the relations that operate within source domains automatically. In addition, our approach also allows us to distinguish between source domains as necessary. The validation results show that this is indeed a promising first attempt of tackling a challenging research problem.  We note that the assignment of source domains is limited to the set of 50 in our current prototype. This assumes a closed set of 50 source domains, whereas in reality, there might be many others that operate in the realm of metaphors we are investigating. Although additional source domains are continually being discovered in a bottom-up fashion by the linguistic metaphor extraction process, we cannot account for every source domain that may be relevant. One way of overcoming this limitation would be to define a source domain ?OTHER? that would be the all-encompassing domain accounting for any yet undiscovered domains. The details of how it would be represented are still under investigation.  Another potential improvement to our method is to experimentally refine the threshold score of RF*IDF. Through large scale validation experiments, we could learn the optimal thresholds automatically by using machine learning. 6 Acknowledgements This paper is based on work supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Defense US Army Research Laboratory contract number W911NF-12-C-0024. The U.S. Government is authorized to reproduce and distribute reprints for Governmental pur-poses notwithstanding any copyright annotation thereon.  Disclaimer: The views and conclusions con-tained herein are those of the authors and should not be interpreted as necessarily representing the of-ficial policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Gov-ernment. References David W. Allbritton, Gail McKoon, and Richard J. Gerrig. 1995. Metaphor-based schemas and text Representa-tions: making connections through conceptual metaphors, Journal of Experimental Psychology: Learning, Memory, and Cognition, 21(3):612-625. Jonathan, Charteris-Black. 2002. Second language figurative proficiency: A comparative study of Malay and English. Applied Linguistics 23(1):104?133. George Aaron Broadwell, Umit Boz, Ignacio Cases, Tomek Strzalkowski, Laurie Feldman, Sarah Taylor, Samira Shaikh, Ting Liu and Kit Cho. 2013. Using Imageability and Topic Chaining to Locate Metaphors in Linguis-tic Corpora. In Proceedings of The 2013 International Conference on Social Computing, Behavioral-Cultural Modeling, & Prediction (SBP 2013), Washington D.C., USA. Jaime Carbonell. 1980. Metaphor: A key to extensible semantic analysis. In Proceedings of the 18th Annual Meeting on Association for Computational Linguistics. M. Coltheart. 1981. The MRC Psycholinguistic Database. Quarterly Journal of Experimental Psychology, 33A: 497-505. Davies, Mark. 2008-. The Corpus of Contemporary American English: 450 million words, 1990-present. Availa-ble online at http://corpus.byu.edu/coca/. Davies, Mark. 2002-. Corpus del Espa?ol: 100 million words, 1200s-1900s. Available online at http://www.corpusdelespanol.org. Dan, Fass. 1991. met*: A Method for Discriminating Metonymy and Metaphor by Computer. Computational Linguistics, 17:49-90 Jerome Feldman, and Srinivas Narayanan. 2004. Embodied meaning in a neural theory of language. Brain and Language, 89(2):385?392. 
218
Christiane D. Fellbaum. 1998. WordNet: An electronic lexical database (1st ed.). MIT Press. Matt Gedigian, John Bryant, Srini Narayanan and Branimir Ciric. 2006. Catching Metaphors. In Proceedings of the Third Workshop on Scalable Natural Language Understanding ScaNaLU 2006, pages 41?48. New York City: NY. Dirk Hovy, Shashank Shrivastava, Sujay Kumar Jauhar, Mrinmaya Sachan, Kartik Goyal, Huying Li, Whitney Sanders and Eduard Hovy. 2013. Identifying Metaphorical Word Use with Tree Kernels. In the Proceedings of the First Workshop on Metaphor in NLP, (NAACL). Atlanta. Krippendorff, Klaus. 1970. Estimating the reliability, systematic error, and random error of interval da-ta. Educational and Psychological Measurement, 30 (1),61-70. Saisuresh Krishnakumaran and Xiaojin Zhu. 2007. Hunting elusive metaphors using lexical resources. In Pro-ceedings of the Workshop on Computational Approaches to Figurative Language, pages 13?20. Rochester, NY. George Lakoff, and Mark Johnson. 1980. Metaphors we live by. University Of Chicago Press, Chicago, Illinois. George, Lakoff. 2001. Moral politics: what conservatives know that liberals don?t. University of Chicago Press, Chicago, Illinois. Liisa, Malkki.  1992. National geographic: The rooting of people and the territorialization of national identity among scholars and refugees. Society for Cultural Anthropology, 7(1):24?44. James Martin. 1988. A computational theory of metaphor. Ph.D. Dissertation. Musolff, Andreas. 2008. What can critical metaphor analysis add to the understanding of racist ideology? Recent studies of Hitler?s anti-semitic metaphors, critical approaches to discourse analysis across disciplines. Critical Approaches to Discourse Analysis Across Disciplines, 2(2):1?10. Kieran, O?Halloran. 2007. Critical discourse analysis and the corpus-informed interpretation of metaphor at the register level. Oxford University Press Farhad Oroumchian, Samira Tasharofi, Hadi Amiri, Hossein Hojjat, Fahime Raja. 2006. Creating a Feasible Corpus for Persian POS Tagging.Technical Report, no. TR3/06, University of Wollongong in Dubai. Samira Shaikh, Tomek Strzalkowski, Ting Liu, George Aaron Broadwell, Boris Yamrom, Sarah Taylor, Laurie Feldman, Kit Cho, Umit Boz, Ignacio Cases, Yuliya Peshkova and Ching-Sheng Lin. 2014. A Multi-Cultural Repository of Automatically Discovered Linguistic and Conceptual Metaphors. In Proceedings of the The 9th edition of the Language Resources and Evaluation Conference , Reykjavik, Iceland.  Ekaterina Shutova and Simone Teufel. 2010a. Metaphor corpus annotated for source - target domain mappings. In Proceedings of Language Resources and Evaluation Conference 2010. Malta. Ekaterina Shutova. 2010b. Models of metaphor in nlp. In Proceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics, ACL ?10, pages 688?697. Ekaterina Shutova, Tim Van de Cruys, and Anna Korhonen. 2012. Unsupervised metaphor paraphrasing using a vector space model In Proceedings of COLING 2012, Mumbai, India Tomek Strzalkowski, George Aaron Broadwell, Sarah Taylor, Laurie Feldman, Boris Yamrom, Samira Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases and Kyle Elliott. 2013. Robust extraction of metaphor from novel data. In Proceedings of Workshop on Metaphor in NLP, NAACL. Atlanta. Tomek Strzalkowski, Samira Shaikh, Kit Cho, George Aaron Broadwell, Laurie Feldman, Sarah Taylor, Boris Yamrom, Ting Liu, Ignacio Cases, Yuliya Peshkova and Kyle Elliot. 2014. Computing Affect in Metaphors. In Proceedings of the Second Workshop on Metaphor in NLP, Baltimore Maryland.  Sarah Taylor, Laurie Beth Feldman, Kit Cho, Samira Shaikh, Ignacio Cases,Yuliya  Peshkiva, George Aaron Broadwell Ting Liu, Umit Boz, Kyle Elliott. Boris Yamrom, and Tomek Strzalkowski. 2014. Extracting Un-derstanding from automated metaphor identification: Contrasting Concepts of Poverty across Cultures and Languages. AHFE Conference, Cracow, Poland. Yorick, Wilks. 1975. Preference semantics. Formal Semantics of Natural Language, E. L. Keenan, Ed. Cam-bridge University Press, Cambridge, U.K., 329?348. Yorick Wilks, Lucian Galescu, James Allen, Adam Dalton. 2013. Automatic Metaphor Detection using Large-Scale Lexical Resources and Conventional Metaphor Extraction. In the Proceedings of the First Workshop on Metaphor in NLP, (NAACL). Atlanta.  
219
Wilson, M. D. 1988. The MRC Psycholinguistic Database: Machine Readable Dictionary, Version 2. Behav-ioural Research Methods, Instruments and Computers, 20(1): 6-11. 
220
