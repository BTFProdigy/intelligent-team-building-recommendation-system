Speech Graffiti vs. Natural Language: Assessing the User Experience 
Stefanie Tomko and Roni Rosenfeld 
Language Technologies Institute, School of Computer Science 
Carnegie Mellon University 
5000 Forbes Ave., Pittsburgh PA 15213 
{stef, roni}@cs.cmu.edu 
 
Abstract 
Speech-based interfaces have great potential 
but are hampered by problems related to spo-
ken language such as variability, noise and 
ambiguity. Speech Graffiti was designed to 
address these issues via a structured, universal 
interface protocol for interacting with simple 
machines. Since Speech Graffiti requires that 
users speak to the system in a certain way, we 
were interested in how users might respond to 
such a system when compared with a natural 
language system. We conducted a user study 
and found that 74% of users preferred the 
Speech Graffiti system to a natural language 
interface in the same domain. User satisfac-
tion scores were higher for Speech Graffiti 
and task completion rates were roughly equal.  
1 Introduction 
Many problems still exist in the design of speech-based 
interfaces. Noisy environments and linguistic variability 
make interpretation of already uncertain input even 
more difficult, resulting in errors that must be handled 
effectively. What if many of these issues could be re-
duced by asking users to interact with speech-based 
systems in a structured way? Would they learn the in-
teraction protocol? Would they prefer a more efficient 
yet structured interaction to one that was more natural, 
but perhaps less efficient?  
One approach to structuring interaction is through 
directed-dialog systems. These generally work well for 
novice users, but they can be too meandering for expert 
users who know exactly what they want from the sys-
tem. Specialized command-and-control languages give 
more power to the user, but they can be difficult for 
novices and require learning a new language for each 
new application encountered.  
Speech Graffiti is a structured interaction protocol 
that is designed to be universal. Common input struc-
tures, output patterns and keywords are used for all 
Speech Graffiti applications, and once users are familiar 
with these for any single application, they can navigate 
their way through any other Speech Graffiti application. 
Fig. 1 shows a sample Speech Graffiti dialog 
User interactions with Speech Graffiti (independent 
of other speech interfaces) have previously been as-
sessed in Rosenfeld et al (2000). Here we consider a 
head-to-head comparison: given the chance to interact 
with both types of interfaces, which would people 
choose? What differences might be observed in per-
formance or user satisfaction?  
1.1 Related work 
Although much research has been conducted on the 
design of natural language spoken dialog systems, far 
less research has been done on more standardized 
speech interfaces. Several studies have previously found 
that users are able to interact successfully using 
constrained or subset languages (e.g. Guindon & 
Shuldberg, 1987; Ringle & Halstead-Nussloch, 1989; 
Sidner & Forlines, 2002). As far as we know, no studies 
have been done comparing constrained, ?universal? 
languages and natural language interfaces directly as we 
have done in this study. General information about the 
Speech Graffiti project and its motivation can be found 
in Rosenfeld et al (2001). 
User: Theater is the Manor. Title is Tuck Everlasting.  
System: THE CINEMAGIC MANOR THEATRE, TUCK EVERLASTING. 
Options.  
SHOW TIME, DAY, GENRE, {?}.  
What are the show times? 
2 MATCHES: 1:25, 5:30.  
Start over.   
STARTING OVER.  
Area is Monroeville. 
MONROEVILLE.   
What are the theaters?   
SHOWCASE CINEMAS PITTSBURGH EAST.   
Theater is Showcase Cinemas Pittsburgh East.  
SHOWCASE CINEMAS PITTSBURGH EAST. 
What are the titles?  
11 MATCHES, BLUE CRUSH, ABANDON, BROWN SUGAR, {?}.   
Genre is comedy.  
COMEDY. 
Where am I? 
AREA IS MONROEVILLE, THEATER IS SHOWCASE CINEMAS PITTSBURGH   
EAST, WHAT ARE THE TITLES?, GENRE IS COMEDY. 
What are the titles?   
5 MATCHES, MY BIG FAT GREEK WEDDING, BROWN SUGAR,  
JONAH - A VEGGIETALES MOVIE, {?}. 
Figure 1. Sample Speech Graffiti interaction.
2 Method 
We conducted a within-subjects user study in which 
participants attempted a series of queries to a movie 
information database with either a Speech Graffiti inter-
face (SG-ML) or a natural language interface (NL-ML). 
Participants repeated the process with the other system 
after completing their initial tasks and an evaluation 
questionnaire. System presentation order was balanced.  
2.1 Participants  
Twenty-three users (12 female, 11 male) accessed the 
systems via telephone in our lab. Most were under-
graduate students from Carnegie Mellon University, 
resulting in a limited range of ages represented. None 
had any prior experience with either of the two movie 
systems or interfaces, and all users were native speakers 
of American English. About half the users had computer 
science and/or engineering (CSE) backgrounds, and 
similarly about half reported that they did computer 
programming ?fairly often? or ?very frequently.? 
2.2 Training 
Users learned Speech Graffiti concepts prior to use dur-
ing a brief, self-paced, web-based tutorial session. 
Speech Graffiti training sessions were balanced between 
tutorials using examples from the MovieLine and tutori-
als using examples from a database that provided simu-
lated flight arrival, departure, and gate information. 
Regardless of training domain, most users spent ten to 
fifteen minutes on the Speech Graffiti tutorial. 
A side effect of the Speech Graffiti-specific training 
is that in addition to teaching users the concepts of the 
language, it also familiarizes users with the more gen-
eral task of speaking to a computer over the phone. To 
balance this effect for users of the natural language sys-
tem, which is otherwise intended to be a walk-up-and-
use interface, participants engaged in a brief NL ?fa-
miliarization session? in which they were simply in-
structed to call the system and try it out. To match the 
in-domain/out-of-domain variable used in the SG tutori-
als, half of the NL familiarization sessions used the NL 
MovieLine and half used MIT?s Jupiter natural lan-
guage weather information system (Zue et al, 2000). 
Users typically spent about five minutes exploring the 
NL systems during the familiarization session. 
2.3 Tasks 
After having completed the training session for a spe-
cific system, each user was asked to call that system and 
attempt a set of tasks (e.g. ?list what?s playing at the 
Squirrel Hill Theater,? ?find out & write down what the 
ratings are for the movies showing at the Oaks Thea-
ter?). Participant compensation included task comple-
tion bonuses to encourage users to attempt each task in 
earnest. Regardless of which system they were working 
with, all users were given the same eight tasks for their 
first interactions and a different set of eight tasks for 
their second system interactions. 
2.4 Evaluation 
After interacting with a system, each participant was 
asked to complete a user satisfaction questionnaire scor-
ing 34 subjective-response items on a 7-point Likert 
scale. This questionnaire was based on the Subjective 
Assessment of Speech System Interfaces (SASSI) pro-
ject (Hone & Graham, 2001), which sorts a number of 
subjective user satisfaction statements (such as ?I al-
ways knew what to say to the system? and ?the system 
makes few errors?) into six relevant factors: system 
response accuracy, habitability, cognitive demand, an-
noyance, likeability and speed. User satisfaction scores 
were calculated for each factor and overall by averaging 
the responses to the appropriate component statements.1 
In addition to the Likert scale items, users were also 
asked a few comparison questions, such as ?which of 
the two systems did you prefer?? 
For objective comparison of the two interfaces, we 
measured overall task completion, time- and turns-to-
completion, and word- and understanding-error rates. 
3 Results 
3.1 Subjective assessments 
Seventeen out of 23 participants preferred Speech Graf-
fiti to the natural language interface. User assessments 
were significantly higher for Speech Graffiti overall and 
for each of the six subjective factors, as shown in Fig. 2 
(REML analysis: system response accuracy F=13.8, 
p<0.01; likeability F=6.8, p<0.02; cognitive demand 
F=5.7, p<0.03; annoyance F=4.3, p<0.05; habitability 
F=7.7, p<0.02; speed F=34.7, p<0.01; overall F=11.2, 
p<0.01). All of the mean SG-ML scores except for an-
noyance and habitability are positive (i.e. > 4), while the 
NL-ML did not generate positive mean ratings in any 
category. For individual users, all those and only those 
who stated they preferred the NL-ML to the SG-ML 
gave the NL-ML higher overall subjective ratings.  
Although users with CSE/programming back-
grounds tended to give the SG-ML higher user satisfac-
tion ratings than non-CSE/programming participants, 
the differences were not significant. Training domain 
likewise had no significant effect on user satisfaction. 
                                                        
1  Some component statements are reversal items 
whose values were converted for analysis, so that high 
scores in all categories are considered good. 
 
3.2 Objective assessments 
Task completion. Task completion did not differ sig-
nificantly for the two interfaces. In total, just over two 
thirds of the tasks were successfully completed with 
each system: 67.4% for the NL-ML and 67.9% for the 
SG-ML. The average participant completed 5.2 tasks 
with the NL-ML and 5.4 tasks with the SG-ML. As with 
user satisfaction, users with CSE or programming back-
ground generally completed more tasks in the SG-ML 
system than non-CSE/programming users, but again the 
difference was not significant. Training domain had no 
significant effect on task completion for either system. 
To account for incomplete tasks when comparing 
the interfaces, we ordered the task completion measures 
(times or turn counts) for each system, leaving all in-
completes at the end of the list as if they had been com-
pleted in ?infinite time,? and compared the medians. 
Time-to-completion. For completed tasks, the aver-
age time users spent on each SG-ML task was lower 
than for the NL-ML system, though not significantly: 
67.9 versus 71.3 seconds. Considering incomplete tasks, 
the SG-ML performed better than the NL-ML, with a 
median time of 81.5 seconds, compared to 103 seconds. 
Turns-to-completion. For completed tasks, the av-
erage number of turns users took for each SG-ML task 
was significantly higher than for the NL-ML sys-tem: 
8.2 versus 3.8 (F=26.4, p<0.01). Considering incom-
plete tasks, the median SG-ML turns-to-completion rate 
was twice that of the NL-ML: 10 versus 5.  
Word-error rate. The SG-ML had an overall word-
error rate (WER) of 35.1%, compared to 51.2% for the 
NL-ML. When calculated for each user, WER ranged 
from 7.8% to 71.2% (mean 35.0%, median 30.0%) for 
the SG-ML and from 31.2% to 78.6% (mean 50.3%, 
median 48.9%) for the NL-ML. The six users with the 
highest SG-ML WER were the same ones who preferred 
the NL-ML system, and four of them were also the only 
users in the study whose NL-ML error rate was lower 
than their SG-ML error rate. This suggests, not surpris-
ingly, that WER is strongly related to user preference.  
To further explore this correlation, we plotted WER 
against users? overall subjective assessments of each 
system, with the results shown in Fig. 3. There is a sig-
nificant, moderate correlation between WER and user 
satisfaction for Speech Graffiti (r=-0.66, p<0.01), but no 
similar correlation for the NL-ML system (r=0.26).  
Understanding error. Word-error rate may not be 
the most useful measure of system performance for 
many spoken dialogue systems. Because of grammar 
redundancies, systems are often able to ?understand? an 
utterance correctly even when some individual words 
are misrecognized. Understanding error rate (UER) may 
therefore provide a more accurate picture of the error 
rate that a user experiences. For this analysis, we only 
made a preliminary attempt at assessing UER. These 
error rates were hand-scored, and as such represent an 
approximation of actual UER. For both systems, we 
calculated UER based on an entire user utterance rather 
than individual concepts in that utterance.  
SG-ML UER for each user ranged from 2.9% to 
65.5% (mean 26.6%, median 21.1%). The average 
change per user from WER to understanding-error for 
the SG-ML interface was ?29.2%.  
The NL-ML understanding-error rates differed little 
from the NL-ML WER rates. UER per user ranged from 
31.4% to 80.0% (mean 50.7%, median 48.5%). The 
average change per user from NL-ML WER was +0.8%.  
Figure 2. Mean user satisfaction for system
response accuracy, likeability, cognitive demand,
annoyance, habitability, speed and overall. 
1
2
3
4
5
6
7
syst.
resp.
acc.
like. cog.
dmd.
ann. hab. speed overall
NL SG
Figure 3. Word-error rate vs. overall user 
satisfaction for Speech Graffiti and natural 
language MovieLines. 
 
1
2
3
4
5
6
7
0 20 40 60 80
SG word-error rate 
 
1
2
3
4
5
6
7
0 20 40 60 80
NL word-error rate 
4 Discussion 
Overall, we found that Speech Graffiti performed 
favorably compared to the natural language interface. 
Speech Graffiti generated significantly higher user 
satisfaction scores, and task completion rates and times 
were similar.  
The higher turns-to-completion rate for Speech 
Graffiti is not necessarily problematic. The phrasal na-
ture of Speech Graffiti syntax seems to encourage users 
to input single phrases; we suspect that in a longitudinal 
study, we would find single-utterance command use in 
SG-ML increasing as users became more familiar with 
the system. Furthermore, because the SG-ML splits long 
output lists into smaller chunks, a user often has to ex-
plicitly issue a request to hear more items in a list, add-
ing at least one more turn to the interaction. Thus there 
exists a trade-off between turn-wise efficiency and re-
duced cognitive load. Because of the reasonable results 
shown for the SG-ML in user satisfaction and comple-
tion time, we view this as a reasonable trade-off.   
It is possible that if lower word-error rates can be 
achieved, Speech Graffiti would become unnecessary. 
This may be true for consistent, extremely low word-
error rates, but such rates do not appear to be attainable 
in the near term. Furthermore, the correlations in Fig. 3 
suggest that as WER decreases, users become more sat-
isfied with the SG interface but that this is not necessar-
ily true for the NL interface. Consider also the effect of 
understanding error. UER is the key to good system 
performance since even if the system has correctly de-
coded a word string, it must still match that string with 
the appropriate concepts in order to perform the desired 
action. Although WER may be reduced via improved 
language and acoustic models, matching input to under-
standing in NL systems is usually a labor-intensive and 
domain-specific task. In contrast, the structured nature 
of Speech Graffiti significantly reduces the need for 
such intensive concept mapping.  
Future work. For Speech Graffiti, scores for 
habitability (represented by statements like ?I always 
knew what to say to the system?) were typically the 
lowest of any of the six user satisfaction factors, 
suggesting that this is a prime area for further work.  
In this regard, it is instructive to consider the ex-
perience of the six users who preferred the NL interface. 
Overall, they accounted for the six highest SG-ML 
word- and understanding-error rates and the six lowest 
SG-ML task completion rates: clearly not a positive 
experience. An additional measure of habitability is 
grammaticality: how often do users speak within the 
Speech Graffiti grammar? The six NL-ML-preferring 
users also had low grammaticality rates (Tomko & 
Rosenfeld, 2004). These users have become a motivator 
of future work: what can be done to make the interface 
work for them and others like them? (Future studies will 
focus on a broader population of adults.) How can we 
help users who are having severe difficulties with an 
interface learn how to use it better and faster? To 
improve the habitability of Speech Graffiti, we plan to 
explore allowing more natural language-esque 
interaction while retaining an application-portable 
structure. We also plan to refine Speech Graffiti?s 
runtime help facilities in order to assist users more 
effectively in saying the right thing at the right time.  
In addition to these core interface goals, we plan to 
extend the functionality of Speech Graffiti beyond 
information access to support the creation, deletion and 
modification of information in a database. 
Acknowledgements 
This work was supported by an NDSEG Fellowship, 
Pittsburgh Digital Greenhouse and Grant N66001-99-1-
8905 from the Space & Naval Warfare Systems Center. 
The information in this publication does not necessarily 
reflect the position or the policy of the US Government 
and no official endorsement should be inferred.  
References 
Guindon, R. & Shuldberg, K. 1987. Grammatical and 
ungrammatical structures in user-adviser dialogues: 
evidence for sufficiency of restricted languages in 
natural language interfaces to advisory systems. In 
Proc. of the Annual Meeting of the ACL, pp. 41-44. 
Hone, K. & Graham, R. 2001. Subjective Assessment of 
Speech-System Interface Usability. In Proceedings of 
Eurospeech, Aalborg, Denmark. 
Ringle, M.D. & Halstead-Nussloch, R. 1989. Shaping 
user input: a strategy for natural language design. In-
teracting with Computers 1(3):227-244 
Rosenfeld, R., Zhu, X., Toth, A., Shriver, S., Lenzo, K. 
& Black, A. 2000. Towards a Universal Speech Inter-
face. In Proceedings of ISCLP, Beijing, China. 
Rosenfeld, R., Olsen, D. & Rudnicky, A. 2001. Uni-
versal Speech Interfaces. Interactions, 8(6):34-44.  
Sidner, C. & Forlines, C. 2002. Subset Languages for 
Conversing with Collaborative Interface Agents. In 
Proc. of ICSLP, Denver CO, pp. 281-284. 
Tomko, S. & Rosenfeld, R. 2004. Speech Graffiti habi-
tability: what do users really say? To appear in Pro-
ceedings of SIGDIAL. 
Zue, V., Seneff, S., Glass, J.R., Polifroni, J., Pao, C., 
Hazen, T.J. & Hetherington, L. 2000. JUPITER: A 
Telephone-Based Conversational Interface for 
Weather Information, IEEE Transactions on Speech 
and Audio Processing, 8(1): 85 ?96. 
Proceedings of NAACL HLT 2007, Companion Volume, pages 9?12,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Conquest ? an Open-Source Dialog System for Conferences 
 
Dan Bohus, Sergio Grau Puerto, David Huggins-Daines, Venkatesh Keri,  
Gopala Krishna,  Rohit Kumar, Antoine Raux, Stefanie Tomko 
School of Computer Science 
Carnegie Mellon University 
{ dbohus, sgrau, dhuggins, vkeri, gopalakr, rohitk, antoine, stef }@ cs.cmu.edu 
 
 
  
Abstract 
We describe ConQuest, an open-source, 
reusable spoken dialog system that pro-
vides technical program information dur-
ing conferences. The system uses a 
transparent, modular and open infrastruc-
ture, and aims to enable applied research 
in spoken language interfaces. The con-
ference domain is a good platform for ap-
plied research since it permits periodical 
redeployments and evaluations with a real 
user-base. In this paper, we describe the 
system?s functionality, overall architec-
ture, and we discuss two initial deploy-
ments.  
1 Introduction  
Conducting applied spoken language interface re-
search is generally a costly endeavor. Developing, 
deploying and maintaining real-world spoken lan-
guage interfaces requires an existing infrastructure, 
a significant amount of engineering effort, and can 
greatly benefit from the availability of certain re-
sources such as transcribed in-domain data.  
In an effort to enable applied research and to 
lower this high cost of entry, we have developed 
ConQuest (Conference Questions) an open-source 
spoken dialog system that provides access to 
schedule and technical program information during 
conferences. We believe the conference domain 
has a number of good properties for applied re-
search: it includes a number of tasks of different 
complexities, it provides regular access to a real-
world user population; it permits periodical rede-
ployments and evaluations and therefore can pro-
vide a natural common evaluation task for the 
spoken language interfaces community.  
The ConQuest system is constructed on top of 
the open, transparent and modular Olympus dialog 
system framework (2007), and can be easily reused 
across different conferences. To date, the system 
has been deployed in two conferences: InterSpeech 
2006 and IJCAI 2007. Together with corpora col-
lected from these deployments, the system is freely 
available for download (Conquest, 2007).  
We begin by describing the ConQuest function-
ality in the next section. Then, in section 3 we pro-
vide an overview of the system architecture and 
discuss the development process. In section 4 we 
briefly discuss the two deployment efforts. Finally, 
in section 5 we discuss related work and draw a 
number of conclusions.   
2 Functionality 
As Figure 1 illustrates, ConQuest is a mixed-
initiative spoken dialog system that provides ac-
cess to schedule and technical program information 
during conferences.  
Users can browse the schedule and find details 
about various papers or sessions of interest by pro-
viding identifying information, such as topics, ses-
sion names, special events, paper titles, author 
names, specific dates and times, specific locations, 
or a combination thereof (e.g. turns 2, 4, 14). Addi-
tionally, the system also allows uses to listen to 
current announcements and to cast their votes for a 
Best Paper Award (e.g. turns 10-17).  
The interaction is mixed-initiative; at any point, 
the user may switch the current focus of the con-
versation (e.g. turn 8). ConQuest employs an adap-
tive information presentation strategy that allows 
9
users to easily navigate the schedule (see turns 3, 5 
and 15). The system uses a rich repertoire of error 
recovery strategies to handle potential errors, in-
cluding several fall-back strategies (e.g. turn 13).  
3 System Architecture  
The ConQuest system was built using RavenClaw/ 
Olympus (2007), an open-source framework that 
facilitates research and development in task ori-
ented conversational spoken language interfaces. 
Olympus consists of a collection of components 
for recognition, language understanding, dialog 
management, language generation, speech synthe-
sis, etc., and the corresponding communication 
infrastructure. To date, Olympus has been used to 
develop and deploy a number of other systems 
spanning different domains and interaction types 
(Bohus and Rudnicky, 2003).  
A key characteristic of the Olympus framework 
is a clear separation between the domain independ-
ent programs (or components) and domain specific 
resources. This decoupling promotes reusability 
and significantly lessens the system development 
effort. In ConQuest, the authoring effort was fo-
cused on developing resources such as the lexicon, 
language model, grammar, dialog task specifica-
tion, etc. Some interesting, unanticipated engineer-
ing challenges we faced during development were 
dealing with foreign names and accented charac-
ters and performing text normalization on various 
fields (e.g. Alex Smith and Alexander Smith are 
the same author), while at the same time ensuring 
consistency between these various resources. Be-
low, we briefly comment of each component and 
the corresponding resources. Figure 2 provides a 
top-level architectural view. 
Speech Recognition. ConQuest uses a recogni-
tion server coupled to a set of parallel recognition 
engines: two SPHINX-II decoders (Huang et al, 
1992) that use gender-specific acoustic models, 
and a DTMF (touch-tone decoder). Each recogni-
tion engine uses class-based (e.g. paper titles, au-
thor names, etc.), state-specific trigram-language 
models. We started with an initial language model 
built using data collected with an early text-only 
prototype. We then internally deployed a speech 
based system, collected more data, transcribed it, 
and used it to retrain the language models. The 
Figure 1. A sample interaction with ConQuest 
1 S: Welcome to ConQuest [?] I can give you information about papers, sessions, special events and announcements at Inter-
Speech 2006. You can also cast a vote for the best paper award. How can I help you? 
2 U: are there any papers on spoken dialog systems today 
3 S:  There are 23 papers on spoken dialog systems today. These papers are being presented in 3 sessions: Dialog and Discourse, 
Language Modeling for Spoken Dialog Systems and Spoken Dialog Systems I. To browse this list of papers, please tell me the 
author name, the paper title, the time or the venue you?re looking for. If you?re not interested in any of these papers say ?quit?. 
4 U: which ones are in the afternoon 
5 S:  There will be 11 papers on spoken dialog systems this afternoon. Would you like to hear information about all these papers? 
6 U: yes 
7 S:  The list of papers on spoken dialog systems this afternoon is: Paper 1 - ?Dynamic Extension of a Grammar-based Dialogue 
System: Constructing an All-Recipes Knowing Robot?, Paper 2 ?  [?] Would you like more information about any of these? 
8 U: tell me more about the third one 
9 S:  Paper 3 is titled ?System- versus user-initiative dialog strategy for driver information systems?, and was presented yesterday at 2 
pm in the poster session in Allegheny Ballroom Foyer. This paper was written by Chantal Ackermann, and Marion Libossek. 
Would you like more information about any of these? 
8 U: no when is the banquet 
9 S:  The Banquet will be held Wednesday from 7pm to 9:30 pm in the Heintz History Center. The Heinz History Center is located on 
Smallman street between 12th and 13th streets. The History Center is a museum that relates and interprets 250 years of history 
in the Western Pennsylvania area. Is there anything else I can do for you? 
10 U: yes I?d like to cast a vote 
11 S:  Please tell me the title of the paper you?d like to vote for.. 
12 U: I don?t really know the title but I think it had Diane as one of the main authors ? [non-understanding] 
13 S:  Sorry, I didn?t catch that. Please dial the first six letters from the title of the paper you would like to vote for.  
14 U: I don?t know it it?s a paper by Diane Litman 
15 S:  There are 3 papers by Diane Litman. Paper 1 ? ?Discourse Structure and Speech Recognition Problems?, Paper 2 ? [?], Paper 
3 ? [?] Would you like to vote for any of these? 
16 U: Discourse Structure and Speech Recognition Problems 
17 S:  Okay, I?ve recorded your vote for the paper entitled ?Discourse Structure and Speech Recognition Problems? What else can I do 
for you?  
18 U: [?] 
10
final language models used during the InterSpeech 
deployment were trained from on a corpus of 6350 
utterances. The system operated with a lexicon of 
4795 words, which included 659 lexicalized (con-
catenated) paper titles, and 1492 lexicalized author 
names, and 78 lexicalized session names. The pro-
nunciations were generated using CMU Dictionary 
and later manually corrected.  
Language understanding. The system uses the 
Phoenix (Ward and Issar, 1994) robust parser to 
extract concepts from the recognition results. A 
domain-specific shallow semantic grammar was 
developed and concatenated with a domain-
independent grammar for generic expressions like 
[Yes], [No], [Date], [Time], etc.  
Dialog management. ConQuest uses a Raven-
Claw-based dialog manager (Bohus and Rudnicky, 
2003). We developed a dialog task specification 
for the conference schedule domain, expressed as a 
hierarchical plan for the interaction, which the 
RavenClaw engine uses to drive the dialog. In the 
process, the RavenClaw engine automatically pro-
vides additional generic conversational skills such 
as error recovery strategies and support for various 
universal dialog mechanisms (e.g. repeat, start-
over, what-can-I-say, etc.)  
Backend/Database. A backend agent looks up 
schedule information from the database (stored as 
a flat text file). The backend agent also performs 
domain specific pre-lookup normalization (e.g. 
mapping author names to their canonical forms), 
and post-lookup processing of the returned records 
(e.g. clustering papers by sessions). The database 
file serves as starting point for constructing a 
number of other system resources (e.g. language 
model classes, lexicon, etc.)  
Figure 2. The Olympus dialog system reference architecture (a typical system) 
Temporal reference resolution agent. Apart 
from the database agent, the dialog manager also 
communicates with an agent that resolves temporal 
expressions (e.g. tomorrow at four p.m.) into ca-
nonical forms.  
Language generation. ConQuest uses Rosetta, 
a template-based language generation component. 
The authoring effort at this level consisted of writ-
ing various templates for the different system ques-
tions and information presentation prompts.  
Speech synthesis. ConQuest uses the Cepstral 
(2005) speech synthesis engine, configured with an 
open-domain unit selection voice. We manually 
checked and corrected pronunciations for author 
names, various technical terms and abbreviations.  
4 Development and Deployment 
The first development of ConQuest system was 
done for the Interspeech 2006 conference held in 
Pittsburgh, PA. The iterative development process 
involved regular interaction with potential users 
i.e. researchers who regularly attend conferences. 
Seven developers working half time participated in 
this development for about three months. An esti-
mated one man-year of effort was spent. This esti-
mate does not include the effort involved in 
transcribing the data collected after the conference. 
Two systems were deployed at the Interspeech 
2006 conference: a desktop system using a close-
talking microphone placed by the registration desk, 
and a telephone-based system. Throughout the 
conference we collected a corpus of 174 sessions. 
We have orthographically transcribed the user ut-
11
terances and are currently analyzing the data; we 
plan to soon release it to the community, together 
with detailed statistics, the full system logs as well 
as the full system source code (Conquest, 2007). 
Following Interspeech 2006, ConQuest was re-
deployed at IJCAI 2007 conference held in Hy-
derabad, India. The second deployment took an 
estimated two man-months: three developers work-
ing half-time for over a month. The significant 
parts of the second deployment involved incorpo-
rating scheduling data for the IJCAI 2007 and im-
plementing two new requirements i.e. support for 
workshops and Indian English speech recognition. 
The IJCAI development had fewer iterations than 
the first effort. The two desktop systems set up at 
the conference venue collected 129 sessions of 
data. This data is currently being transcribed and 
will soon be released to the community through the 
Conquest website (Conquest, 2007). 
Through these two deployments of ConQuest 
the system specifications have been refined and we 
expect the development time to asymptote to less 
than a month after a few more deployments. 
5 Discussion and Conclusion  
Our primary goal in developing ConQuest was to 
enable research by constructing and releasing an 
open-source, full-fledged dialog system, as well as 
an initial corpus collected with this system. The 
system is built on top of an open, transparent and 
modular infrastructure that facilitates research in 
spoken language interfaces (Olympus, 2007).  
There have been a number of other efforts to 
collect and publish dialog corpora, for instance 
within the DARPA Communicator project. A more 
recent project, that operates in a domain similar to 
ConQuest is DiSCoH, a Dialog System for Confer-
ence Help developed by researchers at AT&T, 
ICSI and Edinburgh University, and deployed dur-
ing the SLT-2006 workshop (Adreani et al, 2006). 
While their goals are similar, i.e. to enable re-
search, DiSCoH and ConQuest differ in a number 
of dimensions. Functionality-wise, DiSCoH offers 
general conference information about the venue, 
accommodation options and costs, paper submis-
sion, etc., while ConQuest provides access to the 
technical schedule and allows participants to vote 
for a best paper award. DiSCoH is built using 
AT&T technology and a call-routing approach; 
ConQuest relies on a plan-based dialog manage-
ment framework (RavenClaw) and an open-source 
infrastructure (Olympus). Finally, the DiSCoH ef-
fort aims to develop a richly annotated dialog cor-
pus to be used for research; ConQuest?s aim is to 
provide both the full system and an initial tran-
scribed and annotated corpus to the community. 
The conference domain is interesting in that it 
allows for frequent redeployment and in theory 
provides regular access to a certain user-base. It 
should therefore facilitate research and periodical 
evaluations. Unfortunately, the dialog corpora col-
lected so far using DiSCoH and ConQuest have 
been somewhat smaller than our initial expecta-
tions. We believe this is largely due to the fact that 
the systems provide information that is already 
accessible to users by other means (paper confer-
ence program, web-sites, etc.). Perhaps combining 
the functionalities of these two systems, and ex-
panding into directions where the system provides 
otherwise hard-to-access information (e.g. local 
restaurants, transportation, etc.) would lead to in-
creased traffic.  
References  
Adreani, G., Di Fabbrizio, G., Gilbert, M., Gillick, D., 
Hakkani-Tur, D., and Lemon, O., 2006 Let?s DiS-
CoH: Collecting an Annotated Open Corpus with 
Dialogue Acts and Reward Signals for Natural Lan-
guage Helpdesk, in Proceedings of IEEE SLT-2006 
Workshop, Aruba Beach, Aruba. 
Bohus, D., and Rudnicky, A., 2003. RavenClaw: Dialog 
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda, in Proceedings of 
Eurospeech 2003, Geneva, Switzerland. 
Cepstral, LLC, 2005, SwiftTM: Small Footprint Text-to-
Speech Synthesizer, http://www.cepstral.com. 
Conquest, 2007, http://www.conquest-dialog.org. 
Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee, 
K.-F. and Rosenfeld, R., 1992. The SPHINX-II 
Speech Recognition System: an overview, in Com-
puter Speech and Language, 7(2), pp 137-148, 1992. 
Olympus/RavenClaw web page, as of January 2007: 
http://www.ravenclaw-olympus.org/. 
Ward, W., and Issar, S., 1994. Recent improvements in 
the CMU spoken language understanding system, in 
Proceedings of the ARPA Human Language Tech-
nology Workshop, pages 213?216, Plainsboro, NJ. 
12
Speech Graffiti habitability: What do users really say?                                 
Stefanie Tomko and Roni Rosenfeld 
Language Technologies Institute, School of Computer Science 
Carnegie Mellon University 
5000 Forbes Ave., Pittsburgh PA 15213 
{stef, roni}@cs.cmu.edu 
 
Abstract 
The Speech Graffiti interface is designed to be 
a portable, transparent interface for spoken 
language interaction with simple machines 
and information servers. Because it is a subset 
language, users must learn and adhere to the 
constraints of the language. We conducted a 
user study to determine habitability and found 
that more than 80% of utterances were Speech 
Graffiti-grammatical, suggesting that the lan-
guage is acceptably learnable and usable for 
most users. We also analyzed deviations from 
grammaticality and found that natural lan-
guage input accounted for the most deviations 
from Speech Graffiti. The results will suggest 
changes to the interface and can also inform 
design choices in other speech interfaces. 
1 Introduction 
Speech Graffiti (a.k.a. the Universal Speech Interface 
project) is an attempt to create a standardized, speech-
based interface for interacting with simple machines and 
information servers. Such standardization offers several 
benefits, including domain portability, lower speech 
recognition error rates and increased system transpar-
ency for users (Rosenfeld et al, 2001). This is realized 
via a subset language that must be learned and remem-
bered by users.  
This study was designed to assess habitability by 
measuring Speech Graffiti-grammaticality: how often 
do users actually speak within the subset grammar when 
interacting with a Speech Graffiti system? A high level 
of grammaticality would suggest that the language is 
reasonably habitable, while low grammaticality would 
indicate that the language design requires substantial 
changes.  
1.1 Speech Graffiti basics  
Speech Graffiti interfaces comprise a small set of stan-
dard rules and keywords that can be used in all Speech 
Graffiti applications. The rules are principles governing 
the regularities in the interaction, such as ?input is al-
ways provided in phrases with the syntax ?slot is 
value?? and ?the system will tersely paraphrase what-
ever part of the input it understood.? The keywords are 
designed to provide regular mechanisms for performing 
interaction universals such as help, orientation, naviga-
tion and error correction.  
By standardizing user input, Speech Graffiti aims to 
reduce the negative effects of variability on system 
complexity and recognition performance. At the same 
time, we hope that introducing a universal structure that 
is intended to be used with many different applications 
will mitigate any negative effects that might be other-
wise associated with learning an application-specific 
command language.  
1.2 Related work 
Although several studies have previously explored the 
usage of constrained or subset languages (for example, 
Hendler & Michaelis, 1983; Guindon & Shuldberg, 
1987; Ringle & Halstead-Nussloch, 1989; Sidner & 
Forlines, 2002), they have generally been concerned 
with performance effects such as task completion rates. 
Sidner & Forlines (2002) reported a ?correct utterance? 
rate of approximately 60-80% for their user studies, 
although this was not a main focus of their work. While 
we understand the focus on such performance measures, 
we believe that it is also important to understand how 
habitable the constrained language is for users, in what 
ways users deviate from it, and what impact habitability 
has on user satisfaction.  
2 Method 
Our data was generated from a user study in which par-
ticipants were asked to complete tasks using both a 
Speech Graffiti interface to a telephone-based movie 
information system (MovieLine) and a natural language 
interface to the same data. Tasks were designed to have 
the participants explore a variety of the functions of the 
systems (e.g. ?list what?s playing at the Squirrel Hill 
Theater? and ?find out & write down what the ratings 
are for the movies showing at the Oaks Theater?).  
After interacting with each system, each participant 
completed a user satisfaction questionnaire rating 34 
subjective-response items on a 7-point Likert scale. This 
questionnaire was based on the Subjective Assessment 
Figure 1. Grammaticality and user satis-
faction for Speech Graffiti MovieLine. 
of Speech System Interfaces (SASSI) project (Hone & 
Graham, 2001) and included statements such as ?I al-
ways knew what to say to the system? and ?the system 
makes few errors.? An overall user satisfaction rating 
was calculated for each user by averaging that user?s 
scores for each of the 34 response items. Users were 
also asked a few comparison questions, including sys-
tem preference. In this analysis we were only concerned 
with results from the Speech Graffiti MovieLine 
interactions and not the natural language MovieLine 
interactions (see Tomko & Rosenfeld, 2004). System 
presentation order was balanced and had no significant 
effect on grammaticality measures. 
2.1 Participants 
Twenty-three participants (12 female, 11 male) accessed 
the systems via telephone in our lab. Most were under-
graduate students from Carnegie Mellon University and 
all were native speakers of American English. We also 
asked users whether they considered themselves ?com-
puter science or engineering people? (CSE) and how 
often they did computer programming; the distributions 
of these categories were roughly equal. 
2.2 Training 
The Speech Graffiti approach requires users to learn the 
system prior to using it via a brief tutorial session. 15 
participants received unsupervised Speech Graffiti train-
ing consisting of a self-directed, web-based tutorial that 
presented sample dialog excerpts (in text) and proposed 
example tasks to the user. The other eight participants 
received supervised Speech Graffiti training. This train-
ing used the same web-based foundation as the unsu-
pervised version, but participants were encouraged to 
ask the experimenter questions if they were unsure of 
anything during the training session.  
Both supervised and unsupervised training sessions 
were balanced between web-based tutorials that used 
examples from the MovieLine and from a FlightLine 
system that provided simulated flight arrival, departure, 
and gate information. This enabled us to make an initial 
assessment of the effects of in-domain training.  
2.3 Analysis 
The user study generated 4062 Speech Graffiti Movie-
Line utterances, where an utterance is defined as one 
chunk of speech input sent to our Sphinx II speech rec-
ognizer (Huang et al, 1993). We removed all utterances 
containing non-task-related or unintelligible speech, or 
excessive noise or feed, resulting in a cleaned set of 
3626 utterances (89% of the total). We defined an utter-
ance to be grammatical if the Phoenix parser (Ward, 
1990) used by the system returns a complete parse with 
no extraneous words. 
3 Results 
82% (2987) of the utterances from the cleaned set were 
fully Speech Graffiti-grammatical. For individual users, 
grammaticality ranged from 41.1% to 98.6%, with a 
mean of 80.5% and a median of 87.4%. These averages 
are quite high, indicating that most users were able to 
learn and use Speech Graffiti reasonably well.  
The lowest individual grammaticality scores be-
longed to four of the six participants who preferred the 
natural language MovieLine interface to the Speech 
Graffiti one, which suggests that proficiency with the 
language is very important for its acceptance. Indeed, 
we found a moderate, significant correlation between 
grammaticality and user satisfaction, as shown in Fig. 1 
(r = 0.60, p < 0.01). We found no similar correlation for 
the natural language interface, using a strict definition 
of grammaticality. 
Users? grammaticality tended to increase over time. 
For each subject, we compared the grammaticality of 
utterances from the first half of their session with that of 
utterances in the second half. All but four participants 
increased their grammaticality in the second half of their 
Speech Graffiti session, with an average relative im-
provement of 12.4%. A REML analysis showed this 
difference to be significant, F = 7.54, p < 0.02. Only one 
of the users who exhibited a decrease in grammaticality 
over time was from the group that preferred the natural 
language interface. However, although members of that 
that group did tend to increase their grammaticality later 
in their interactions, none of their second-half gram-
maticality scores were above 80%. 
Summary by training and system preference. No 
significant effects on Speech Graffiti-grammaticality 
were found due to differences in CSE background, pro-
gramming experience, training supervision or training 
domain. This last point suggests that it may not be nec-
essary to design in-domain Speech Graffiti tutorials; 
0% 5% 10% 15% 20% 25%
endpo int
plural + options
subject-verb agreement
missing is/are
value + options
time syntax
more syntax
disfluency
slo t-value mismatch
value only 
keyword problem
out-o f-vocabulary word
slot only
general syntax
portion of all ungrammaticalities
Figure 2. Distribution of ungrammatical
utterances by type. 
instead, a single training application could be devel-
oped. The six users who preferred the natural language 
MovieLine generated 45.4% of the ungrammaticalities, 
further supporting the idea of language proficiency as a 
major factor for system acceptance.  
3.1 Deviations from grammar  
To help determine how users can be encouraged to 
speak within the grammar, we analyzed the ways in 
which they deviated from it in this experiment. We 
identified 14 general types of deviations from the gram-
mar; Fig. 2 shows the distribution of each type. Four 
trivial deviation types (lighter bars in Fig. 2) that 
resulted from unintentional holes in our grammar cover-
age comprised about 20% of the ungrammaticalities. 
When these trivial deviations are counted as grammati-
cal, mean grammaticality rises to 85.5% and the median 
to 91.3%. However, we have not removed the trivial 
ungrammaticalities from our overall analysis since they 
are likely to have resulted in errors that may have af-
fected user satisfaction. Each of the ten other deviation 
types is discussed in further detail in the sections below.  
General natural language syntax, 20.6%: Speech 
Graffiti requires input to have a slot is value phrase syn-
tax for specifying and querying information. The most 
common type of deviation in the Speech Graffiti utter-
ances involved a natural language (NL) deviation from 
this standard phrase syntax. For example, a correctly 
constructed Speech Graffiti query to find movie times at 
a theater might be theater is Galleria, title is Sweet Home 
Alabama, what are the show times? For errors in this cate-
gory, users would instead make more NL-style queries, 
like when is Austin Powers playing at Showcase West?  
Slot only, 14.6%: In these cases, users stated a slot 
name without an accompanying value or query words. 
For example, a user might attempt to ask about a slot 
without using what, as in title is Abandon, show times. In 
about a third of slot-only instances, the ungrammatical 
input appeared to be an artifact of the options function, 
which lists slots that users can talk about at any given 
point; users would just repeat back a slot name without 
adding a value, confirming Brennan?s (1996) findings of 
lexical entrainment. 
Out-of-vocabulary word, 14.0%: These were often 
movie titles that were not included in the database or 
synonyms for Speech Graffiti-grammatical concepts 
(e.g. category instead of genre).    
Keyword problem, 8.1%: Participants used a key-
word that was not part of the system (e.g. clear) or they 
used an existing keyword incorrectly.   
Value only, 6.7%: Users specified a value (e.g. 
comedy) without an accompanying slot name. 
Slot-value mismatch, 5.1%: Users paired slots and 
values that did not belong together. This often occurred 
when participants were working on tasks involving lo-
cating movies in a certain neighborhood. For instance, 
instead of stating area is Monroeville, users would say 
theater is Monroeville. Since the input is actually in the 
correct slot is value format, this type of ungrammaticality 
could perhaps be considered more of a disfluency than a 
true habitability problem.  
Disfluency, 4.3%: This category includes utterances 
where the parser failed because of disfluent speech, 
usually repeated words. 81% of the utterances in this 
category were indeed grammatical when stripped of 
their disfluencies, but we prefer to leave this category as 
a component of the non-trivial deviations in order to 
account for the unpredictable disfluencies that will al-
ways occur in interactions. 
More syntax, 4.0%: This is are a special case of a 
keyword problem in which participants misused the 
keyword more by pairing it with a slot name (e.g. theater, 
more) rather than using it to navigate through a list. 
Time syntax, 1.3%: In this special case of natural 
language syntax ungrammaticality, users created time 
queries that were initially well-formed but which had 
time modifiers appended to the end, as in what are show 
times after seven o?clock? 
Value + options, 1.1%: In grammatical usage, the 
keyword options can be used either independently (to get 
a list of all available slots) or paired with a slot (to get a 
list of all appropriate values for that slot). In a few 
cases, users instead used options with a value, as in Squir-
rel Hill options. 
4 Discussion 
We have shown a significant correlation between 
grammaticality and user satisfaction for the Speech 
Graffiti system. Grammaticality scores were generally 
high and tended to increase over time, demonstrating 
that the system is acceptably habitable.  
Based on the data shown in Fig.1, it appears that 
80% is a good target for Speech Graffiti grammaticality. 
Nearly all participants with grammaticality scores over 
80% gave positive (i.e. > 4) user satisfaction scores, and 
more than half of our users achieved this level. Fur-
thermore, users with grammaticality above 80% com-
pleted an average of 6.9 tasks, while users with 
grammaticality below 80% completed an average of 
only 3.5 tasks. A fundamental question for our future 
work is ?what can we do to help everyone speak within 
the bounds of the system at the 80% level??  
Several possible refinements are immediately appar-
ent beyond fixing our trivial grammar problems. System 
responses to options should be reworked to reduce incor-
rect lexical entrainment and alleviate slot-only devia-
tions. The out-of-vocabulary instances can be analyzed 
to decide whether certain synonyms should be added to 
the current system, although this will generate only do-
main-specific improvements. Many ungrammaticality 
types can also be addressed through refinements to 
Speech Graffiti?s help and tutorial functions. 
Addressing the general NL syntax category poses 
the biggest problem. Although it is responsible for the 
largest portion of ungrammaticalities, simply changing 
the grammar to accommodate these variations would 
likely lead to increased system complexity. A main con-
cern of our work is domain portability, and Speech 
Graffiti?s standard structure currently allows for fairly 
rapid creation of new interfaces (Toth et al, 2002). Any 
natural language expansion of Speech Graffiti grammar 
will have to be balanced with the ability to port such a 
grammar to all domains. We are currently analyzing the 
ungrammatical utterances in this and the time syntax 
categories to determine whether any Speech Graffiti-
consistent modifications could be made to the interface. 
However, most of the improvement in this area will 
likely have to be generated via better help and training. 
An important additional finding from this work is 
the scope of general NL syntax deviations. Considering 
items like movie and theater names as equivalence class 
members, the NL utterances used by participants in the 
Speech Graffiti system reduced to 94 patterns. In com-
parison, the NL utterances used by participants in the 
natural language MovieLine reduced to about 580 pat-
terns. One of the main differences between the NL pat-
terns in the two systems was the lack of conversational 
phrases like ?can you give me?? and ?I would like to 
hear about?? in the Speech Graffiti system. Thus the 
knowledge that they are interacting with a restricted 
language system seems to be enough to make users 
speak more simply, matching results from Ringle & 
Halstead-Nussloch (1989).  
Although many of our ungrammaticality types may 
appear to be specific to Speech Graffiti, they reinforce 
lessons applicable to most speech interfaces. The slot-
only issue demonstrates that lexical entrainment truly is 
a factor in spoken language interfaces and its effects 
should not be underestimated. Out-of-vocabulary words 
are a persistent problem, and keywords should be cho-
sen with care to ensure that they are task-appropriate 
and that their functions are as intuitive as possible.   
Overall, this study has provided us with a target 
level for Speech Graffiti-grammaticality, suggested 
changes to the language and provided insight about 
what aspects of the system might need greater support 
through help and tutorial functions. We plan to imple-
ment changes based on these results and re-evaluate the 
system through further user testing. 
References 
Brennan, S.E. 1996. Lexical entrainment in spontaneous 
dialog. In Proceedings of the International Sympo-
sium on Spoken Dialogue, pp. 41-44. 
Guindon, R. & Shuldberg, K. 1987. Grammatical and 
ungrammatical structures in user-adviser dialogues: 
evidence for sufficiency of restricted languages in 
natural language interfaces to advisory systems. In 
Proc. of the Annual Meeting of the ACL, pp. 41-44. 
Hendler, J. A. & Michaelis, P. R. 1983. The Effects of 
Limited Grammar On Interactive Natural Language. 
In Proceedings of CHI, pp. 190-192. 
Hone, K. & Graham, R. 2001. Subjective Assessment of 
Speech-System Interface Usability. In Proceedings of 
Eurospeech, Aalborg, Denmark. 
Huang, X., Alleva, F., Hon, H.W., Hwang, M.Y., Lee, 
K.F. & Rosenfeld, R. 1993. The Sphinx-II Speech 
Recognition System: An Overview. Computer, 
Speech and Language, 7(2):137-148. 
Ringle, M.D. & Halstead-Nussloch, R. 1989. Shaping 
user input: a strategy for natural language design. In-
teracting with Computers 1(3):227-244 
Rosenfeld, R., Olsen, D. & Rudnicky, A. 2001. Univer-
sal Speech Interfaces. Interactions, 8(6):34-44.  
Sidner, C. & Forlines, C. 2002. Subset Languages for 
Conversing with Collaborative Interface Agents. In 
Proc. of ICSLP, Denver CO, pp. 281-284. 
Tomko, S. & Rosenfeld, R. 2004. Speech Graffiti vs. 
Natural Language: Assessing the User Experience. 
To be published in Proc. of HLT/NAACL. 
Toth, A., Harris, T., Sanders, J., Shriver, S. & Rosen-
feld, R. 2002. Towards Every-Citizen's Speech Inter-
face: An Application Generator for Speech Interfaces 
to Databases. In Proc. of  ICSLP, Denver, CO. 
Ward, W. 1990. The CMU Air Travel Information Ser-
vice: Understanding Spontaneous Speech. In Proc. of 
the DARPA Speech and Language Workshop. 
