Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 447?457,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Effectiveness and Efficiency of Open Relation Extraction
Filipe Mesquita Jordan Schmidek Denilson Barbosa
Department of Computing Science, University of Alberta, Canada
{mesquita,schmidek,denilson}@ualberta.ca
Abstract
A large number of Open Relation Extrac-
tion approaches have been proposed recently,
covering a wide range of NLP machinery,
from ?shallow? (e.g., part-of-speech tagging)
to ?deep? (e.g., semantic role labeling?SRL).
A natural question then is what is the trade-
off between NLP depth (and associated com-
putational cost) versus effectiveness. This pa-
per presents a fair and objective experimental
comparison of 8 state-of-the-art approaches
over 5 different datasets, and sheds some light
on the issue. The paper also describes a novel
method, EXEMPLAR, which adapts ideas from
SRL to less costly NLP machinery, resulting
in substantial gains both in efficiency and ef-
fectiveness, over binary and n-ary relation ex-
traction tasks.
1 Introduction
Open Relation Extraction (ORE) (Banko and Et-
zioni, 2008) has become prevalent over traditional
relation extraction methods, especially on the Web,
because of the intrinsic difficulty in training indi-
vidual extractors for every single relation. Broadly
speaking, existing ORE approaches can be grouped
according to the level of sophistication of the NLP
techniques they rely upon: (1) shallow parsing, (2)
dependency parsing and (3) semantic role labelling
(SRL). Shallow methods annotate the sentences with
part-of-speech (POS) tags and the ORE approaches
in this category, such as ReVerb (Fader et al, 2011)
and SONEX (Merhav et al, 2012), identify rela-
tions by matching patterns over such tags. Depen-
dency parsing gives unambiguous relations among
each word in the sentence, and the ORE approaches
in this category such as PATTY (Nakashole et al,
2012), OLLIE (Mausam et al, 2012), and TreeK-
ernel (Xu et al, 2013) identify whole subtrees con-
necting the relation predicate and its arguments. Fi-
nally, semantic annotators, such as Lund (Johans-
son and Nugues, 2008) and SwiRL (Surdeanu et al,
2003), add roles to each node in a parse tree, en-
abling ORE approaches that identify the precise con-
nection between each argument and the predicate in
a relation, independently.
The first contribution of the paper is an objec-
tive and fair experimental comparison of the state-
of-the-art in ORE, on 5 datasets with varying de-
gree of ?difficulty?. Of these, 4 datasets were an-
notated manually, covering both well-formed sen-
tences, from the New York Times (NYT) and the
Penn Treebank, as well as mixed-quality sentences
from a popular Web corpus. A much larger corpus
with 12,000 sentences from NYT, automatically an-
notated is also used. Another experiment focuses
on n-ary relation extractions separately. The results
show, as expected, that the three broad classes above
are separated by orders of magnitude when it comes
to throughput. Shallow methods handle ten times
more sentences than dependency parsing methods,
which in turn handle ten times more sentences than
semantic parsing methods. Nevertheless, the cost-
benefit trade-off is not as simple; and the higher
computation cost of dependency or semantic parsing
does not always pays off with higher effectiveness.
The second contribution of the paper is a new
ORE method, called EXEMPLAR, which applies a
key idea in semantic approaches (namely, to iden-
447
tify the precise connection between the argument
and the predicate words in a relation) over a depen-
dency parse tree (i.e., without applying SRL). The
goal is to achieve the higher accuracy of the seman-
tic approaches at the lower computational cost of the
dependency parsing approaches. EXEMPLAR is a
rule-based system derived from a careful study of all
dependency types identified by the Stanford parser.
(Note, however, that other parsers can be used, as
shown later on.) EXEMPLAR works for both binary
and n-ary relations, and is evaluated separately in
each case. For binary relations, EXEMPLAR outper-
forms all previous methods in terms of accuracy, los-
ing to the shallow methods only in terms of through-
put. As for n-ary relations, EXEMPLAR outperforms
the methods that support this kind of extraction.
2 Related Work
Others have pointed out the importance of under-
standing the trade-off between ?shallow? versus
?deep? NLP in ORE. One side of the argument fa-
vors shallow methods, claiming deep NLP costs or-
ders of magnitude more and provide much less dra-
matic gains in terms of effectiveness (Christensen et
al., 2011). The counterpoint, illustrated with a re-
cent analysis on a industrial-scale Web crawl (Dalvi
et al, 2012), is that the diversity with which infor-
mation is encoded in text is too high. Framing the
debate as ?shallow? versus ?deep? is perhaps con-
venient, but nevertheless an oversimplification. This
paper sheds more light into the debate by compar-
ing the state-of-the-art from three broad classes of
approaches.
Shallow ORE. TextRunner (Banko and Etzioni,
2008) and its successor ReVerb (Fader et al, 2011)
are based on the idea that most relations are ex-
pressed using few syntactic patterns. ReVerb, for ex-
ample, detects only three types of relations (?verb?,
?verb+preposition? and ?verb+noun+preposition?).
Following a similar approach, SONEX (Merhav et
al., 2012) extends ReVerb by detecting patterns with
appositions and possessives.
ORE via dependency parsing. PATTY (Nakas-
hole et al, 2012) extracts textual patterns from sen-
tences based on paths in the dependency graph. For
all pairs of named entities, PATTY finds the shortest
Dataset Source # Sentences # Relations
WEB-500 Search Snippets 500 461
NYT-500 New York Times 500 150
PENN-100 Penn Treebank 100 51
Table 1: Binary relation datasets.
path in the dependency graph that connects the two
named entities. They limit the search to only paths
that start with one of these dependencies: nsubj, rc-
mod and partmod.
OLLIE (Mausam et al, 2012) also extracts rela-
tions between two entities. It applies pattern tem-
plates over the dependency subtree containing pairs
of entities. Pattern templates are learned automat-
ically from a large training set that is bootstrapped
from high confidence extractions from ReVerb. OL-
LIE merges binary relations that differ only in the
preposition and second argument to produce n-ary
extractions, as in: (A, ?met with?, B) and (A, ?met
in?, C) leading to (A, ?met?, [with B, in C]).
The TreeKernel (Xu et al, 2013) method uses a
dependency tree kernel to classify whether candi-
date tree paths are indeed instances of relations. The
shortest path between the two entities along with the
shortest path between relational words and an entity
are used as input to the tree kernel. An expanded set
of syntactic patterns based on those from ReVerb are
used to generate relation candidates.
ORE via semantic parsing. Recently, a method
based on SRL, called SRL-IE, has shown that the ef-
fectiveness of ORE methods can be improved with
semantic features (Christensen et al, 2011). We im-
plemented our version of SRL-IE by relying on the
output of two SRL systems: Lund (Johansson and
Nugues, 2008) and SwiRL (Surdeanu et al, 2003).
SwiRL is trained on PropBank and expands upon the
syntactic features used in previous work. One of its
major limitations is that it is only able to label ar-
guments with verb predicates. Lund, on the other
hand, is based on dependency parsing and is trained
on both PropBank and NomBank, making it able to
extract relations with both verb and noun predicates.
3 Experimental Study
This section compares the effectiveness and effi-
ciency of the following ORE methods: ReVerb,
448
Method
NYT-500 WEB-500 PENN-100
Time P R F Time P R F Time P R F
ReVerb 0.02 0.70 0.11 0.18 0.01 0.92 0.29 0.44 0.02 0.78 0.14 0.23
SONEX 0.04 0.77 0.22 0.34 0.02 0.98 0.30 0.45 0.04 0.92 0.43 0.59
OLLIE 0.05 0.62 0.27 0.38 0.04 0.81 0.29 0.43 0.14 0.81 0.43 0.56
EXEMPLAR[M] 0.08 0.70 0.39 0.50 0.06 0.95 0.44 0.61 0.16 0.83 0.49 0.62
EXEMPLAR[S] 1.03 0.73 0.39 0.51 0.47 0.96 0.46 0.62 0.62 0.79 0.51 0.62
PATTY 1.18 0.49 0.23 0.32 0.48 0.71 0.48 0.57 0.66 0.46 0.24 0.31
SwiRL 2.96 0.63 0.10 0.17 1.73 0.97 0.34 0.50 2.17 0.89 0.16 0.27
Lund 11.40 0.78 0.24 0.37 2.69 0.91 0.37 0.52 5.21 0.86 0.35 0.50
TreeKernel ? ? ? ? ? ? ? ? 0.85 0.85 0.33 0.48
Table 2: Results for the task of extracting binary relations. Methods are ordered by computing time per
sentence (in seconds). Best results for each column are underlined and marked in bold, for clarity.
SONEX, OLLIE, PATTY, TreeKernel, SwiRL,
Lund and our two variants of our method, EX-
EMPLAR, explained in detail in Appendix A: (1)
EXEMPLAR[S] uses the Stanford parser (Klein and
Manning, 2003) and (2) EXEMPLAR[M] uses the
Malt parser (Nivre and Nilsson, 2004).
3.1 Binary Relations ? Setup
We start by evaluating the extraction of binary re-
lations. Table 1 shows our experimental datasets.
WEB-500 is a commonly used dataset, developed
for the TextRunner experiments (Banko and Etzioni,
2008). These sentences are often incomplete and
grammatically unsound, representing the challenges
of dealing with web text. NYT-500 represents the
other end of the spectrum with formal, well written
new stories from the New York Times Corpus (Sand-
haus, 2008). PENN-100 contains sentences from the
Penn Treebank recently used in an evaluation of the
TreeKernel method (Xu et al, 2013). We manu-
ally annotated the relations for WEB-500 and NYT-
500 and use the PENN-100 annotations provided by
TreeKernel?s authors (Xu et al, 2013).
We annotate each sentence manually as follows.
We identify exactly two entities and a trigger (a sin-
gle token indicating a relation?see Section A.3) for
the relation between them, if one exists. In addi-
tion, we specify a window of tokens allowed to be
in a relation, including modifiers of the trigger and
prepositions connecting triggers to their arguments.
For each sentence annotated with two entities, a sys-
tem must extract a string representing the relation
between them. Our evaluation method deems an ex-
traction as correct if it contains the trigger and al-
lowed tokens only.
In our annotated sentences, entities are enclosed
in triple square brackets, triggers and enclosed in
triple curly brackets and the window of allowed to-
kens is defined by arrows (?--->? and ?<---?),
as in this example:
I?ve got a media call about
[[[ORG Google]]] --->?s
{{{acquisition}}} of<--- [[[ORG
YouTube]]] --->today<---.
where ?Google? and ?YouTube? are entities of the
type organization, ?acquisition? is the trigger and the
allowed tokens are ?acquisition?, ??s? and ?of?. We
include time and location modifiers (e.g., ?today?,
?here?) in the list of allowed tokens since OLLIE ex-
tracts them as part of the relation. OLLIE?s extrac-
tions may also include auxiliary verbs and preposi-
tions that are not present in the original sentence. To
be fair with OLLIE, we remove auxiliary verbs and
prepositions from OLLIE extractions.
Our benchmarks are available upon request.
Ensuring entities are recognized properly.
Since every method uses a different tool to recog-
nize entities, we try to ensure every method is able
to recognize the entities marked by our annotators.
We replace the original entities by a single word,
preventing any system from recognizing only part
of an entity. Entities are replaced by ?Europe? and
?Asia?, since we empirically found that, for 99.7%
of the sentences in our experiment, all methods
were able to recognize ?Europe? and ?Asia? as
entities (or nouns, for systems that do not use a NER
tool). In addition, we did not find any occurrence of
449
?Europe? and ?Asia? in the original sentences that
could conflict with our entity placeholders.
For methods that extract relations between noun
phrases (ReVerb, OLLIE, SwiRL and Lund), there
is the additional task of identifying whether a
noun phrase containing additional words surround-
ing ?Europe? and ?Asia? is still a reference to the an-
notated entity. For example, ?the beautiful Europe?
refers to the entity, while ?Europe?s enemy? does
not. In our evaluation, we ignore noun phrases that
do not reference the annotated entity. For SwiRL
and Lund, we ignore any noun phrase that do not
present ?Europe? or ?Asia? as its head word. For
ReVerb and OLLIE, we ignore noun phrases that do
not contain these words in the end of the phrase.
Metrics. Our evaluation focuses on sentence-level
extractions. Therefore, we only apply the steps of
each method that perform this task. Additional steps
to merge relations and remove infrequent relations
are not applied. In addition, we assume there is
only one relation between a pair of entities in a sen-
tence. The number of entity pairs with more than
one relation was insignificant in our datasets (less
than 0.5%).
The metrics used in this analysis are precision (P),
recall (R) and f-measure (F), defined as usual:
P =
# correct
# extractions
, R =
# correct
# relations
, F =
2PR
P + R
where ?# correct? is the number of extractions
deemed as correct.
We also measure the total computing time of each
method, excluding initialization or loading any li-
braries or models in memory. To ensure a fair com-
parison, we make sure each method runs in a single-
threaded mode, thus utilizing a single computing
core at all times.
3.2 Binary Relations ? Results
Table 2 presents the results for our experiment with
binary relations. WEB-500 turned out to contain the
easiest sentences as evidenced by the precision of
all methods in this dataset. This is because WEB-
500 sentences were collected by querying a search
engine with known relation instances. The other
two datasets, on the other hand, contain randomly
chosen sentences. Although WEB-500 is a popular
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.01  0.1  1  10
f-
m
ea
su
re
seconds per sentence
EXEMPLAR[S]EXEMPLAR[M]
REVERB
SONEX
OLLIE LUND
SWIRL
PATTY
Figure 1: Average f-measure vs average time for
NYT-500, WEB-500 and PEN-100.
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6
pr
ec
is
io
n
recall
EXEMPLAR[S]
EXEMPLAR[M]REVERB
SONEX
OLLIE
LUNDSWIRL
PATTY
Figure 2: Average precision vs average recall for
NYT-500, WEB-500 and PEN-100.
dataset, it perhaps does not represent the challenges
found in web text.
We were unable to run TreeKernel for NYT-500
and WEB-500 for lack of training data. We ran
TreeKernel, as trained by its authors, on the same
test set used in their paper (Xu et al, 2013).
Comparing methods based on effectiveness (f-
measure) or efficiency (computational cost) alone
can be misleading. To do so, we compare methods
in terms of dominance. We say method A dominates
method B if A is: (1) more effective and as efficient
as B; (2) more efficient and as effective as B; or (3)
both more effective and more efficient than B. The
methods that are not dominated by any other form
the state-of-the-art.
Figure 1 plots the effectiveness and efficiency of
450
all methods, averaged over all datasets (TreeKernel
was not included due to missing results). For effi-
ciency, there is a clear separation of approximately
one order of magnitude among methods based on
shallow parsing (ReVerb and SONEX), dependency
parsing (OLLIE, EXEMPLAR[M], EXEMPLAR[S],
and PATTY) and semantic parsing (SwiRL and
Lund). The lines in the plot identify the state-of-
the-art before (dashed) and after (solid) EXEMPLAR.
(Note: Although not clear in the figure, OLLIE and
Lund are dominated by SONEX as they tie in terms
of effectiveness.)
In terms of efficiency, EXEMPLAR[M] and EX-
EMPLAR[S] closely match OLLIE and PATTY, re-
spectively, since they use the same dependency
parsers. EXEMPLAR outperforms both systems by
covering a larger number of relational patterns. This
is possible because EXEMPLAR looks at each argu-
ment separately, as opposed to the whole subtree
connecting two arguments. As it turns out, this de-
sign choice greatly simplifies the task of designing
good patterns.
The poor performance of PATTY is due to its
rather permissive sentence-level extraction of rela-
tions, which looks at the shortest path between argu-
ments. PATTY relies on redundancy of extractions
to normalize the produced relations in order to re-
cover from mistakes done in the sentence-level.
Figure 2 illustrates the dominance relation dif-
ferently, using precision versus recall. Again, the
dashed line shows the previous state-of-the-art, and
the solid line shows the current situation. SONEX
dominates PATTY and Lund, since they tied in
recall. OLLIE, however, achieved greater recall
than SONEX. Somewhat surprisingly, EXEMPLAR
presents 44% more recall than the more sophisti-
cated Lund, at a close level of precision. This can be
explained by Lund?s dependency on training data,
which contains only a subset of all possible pred-
icates and roles. The importance of relations trig-
gered by nouns is illustrated by the higher recall
of SONEX and Lund when compared, respectively,
to ReVerb and SwiRL, similar methods that handle
verb triggers only.
3.3 Binary Relations ? Discussion
Differences in annotation. It is worth noting
some differences between our annotations (WEB-
500 and NYT-500) and the annotations from PEN-
100. The first difference concerns the definition
of an entity. Consider the following sentence from
PEN-100:
?. . . says Leslie Quick Jr., chairman of the
Quick & Reilly discount brokerage firm.?
Unlike our annotation style, the original annotation
defines that ?Leslie Quick Jr.? is the chairman of
?the Quick & Reilly discount brokerage firm?, as op-
posed to ?Quick & Reilly?. While we consider the
words surrounding ?Quick & Reilly? as apposition,
the original consider them as part of the entity.
Another difference concerns the definition of the
RE task. We assume that RE methods are respon-
sible for resolving co-references when necessary to
identify a relation. For example, consider the sen-
tence:
?It also marks P&G?s growing concern that its
Japanese rivals, such as Kao Corp., may bring
their superconcentrates to the U.S.?
According to our annotation style, there is a rela-
tion ?rivals? between ?P&G? and ?Kao Corp.? in
this sentence. On the other hand, the original an-
notations for PENN-100 consider only the relation
between ?Kap Corp.? and the pronoun ?it?, leav-
ing the task of resolving the coreference between
?P&G? and ?it? as a posterior step.
These differences in annotation illustrate the chal-
lenges of producing a benchmark for open relation
extraction.
Differences in evaluation methodology. A
sentence-level evaluation like ours focuses on each
sentence, separately. On the other hand, the evalua-
tions of SONEX, ReVerb, PATTY, TreeKernel and
OLLIE are performed at the corpus level. Corpus-
level evaluations consider an extracted relation as
correct regardless of whether a method was able
to identify one or all sentences that describe this
relation.
Creating a ground truth for corpus-level evalu-
ations is extremely hard, since one has to iden-
tify and curate (e.g., merge near-duplicate relations
and co-referential entities) all relations described
in a corpus. As a consequence, most corpus-level
evaluations perform only a manual inspection of a
451
Method
NYT n-ary
Time P R F
EXEMPLAR[M] 0.11 0.94 0.40 0.56
OLLIE 0.12 0.87 0.14 0.25
EXEMPLAR[S] 0.88 0.92 0.39 0.55
SwiRL 2.90 0.94 0.30 0.45
Lund 9.20 0.95 0.36 0.53
Table 3: Results for n-ary relations.
method?s extractions. This manual inspection mea-
sures a method?s precision, but is unable to measure
recall.
Other differences in methodology are as follows.
PATTY?s evaluation concerns relation patterns (e.g.,
?wrote hits for?) and their type signatures (e.g.
Musician?Musician), as opposed to the relation it-
self, which includes its two arguments. The eval-
uations of ReVerb and OLLIE consider any noun
phrase as a potential argument, while the evaluations
of TreeKernel and SONEX consider named entities
only.
Due to the lack of a ground truth and differences
in evaluation methodology, results from different pa-
pers are usually not comparable. This work tries to
alleviate this problem by providing reusable annota-
tions that are flexible and can be used to evaluate a
wide range of methods.
3.4 n-ary Relations
The goal of this experiment is to evaluate the accu-
racy and performance of our method when extract-
ing n-ary relations (n > 2). For this experiment, we
manually tagged 222 sentences with n-ary relations
from the New York Times. Every sentence is anno-
tated with a single relation trigger and its arguments.
This experiment measures precision and recall
over the extracted arguments. For each sentence, a
system can extract a number of relations of the form
r(a1, a2, . . . , an), where r is the relation name and
ai is an argument. We only use the extracted rela-
tion whose name contains the annotated trigger, if
it exists. An argument of such relation is deemed a
correct extraction if it is annotated in the sentence;
otherwise, it is deemed incorrect.
Precision and recall are now defined as follows:
P =
# correct
# extracted args
, R =
# correct
# annotated args
.
Method
NYT 12K
Time P R F
ReVerb 0.01 0.84 0.11 0.19
OLLIE 0.02 0.85 0.22 0.35
SONEX 0.03 0.87 0.20 0.32
EXEMPLAR[M] 0.05 0.87 0.26 0.40
EXEMPLAR[S] 1.20 0.86 0.29 0.43
PATTY 1.29 0.86 0.18 0.30
SwiRL 3.58 0.87 0.16 0.27
Lund 11.28 0.86 0.21 0.33
Table 4: Results for binary relations automatically
annotated using Freebase and WordNet.
where ?# correct? is the number of arguments
deemed as correct. There are 765 annotated argu-
ments in total. Table 3 reports the results for our
experiment with n-ary relations. EXEMPLAR[M]
shows a 6% increase in f-measure over Lund, the
second best system, while being almost two orders
of magnitude faster.
3.5 Automatically Annotated Sentences
The creation of datasets for open RE is an extremely
time-consuming task. In this section we investigate
whether external data sources such as Freebase1 and
WordNet2 can be used to automatically annotate a
dataset, leading to a useful benchmark.
Our automatic annotator identifies pairs of enti-
ties and a trigger of the relation between them. It
does so by first trying to link all entities to Wikipedia
(and consequently to Freebase, since Freebase is
linked to Wikipedia) by using the method proposed
by (Cucerzan, 2007). Given two entities appearing
within 10 tokens of each other in a sentence, our an-
notator checks whether there is a relation connect-
ing them in Freebase. If such a relation exists, the
annotator looks for a trigger in the sentence. A trig-
ger must be a synonym for the Freebase relation (ac-
cording to WordNet) and its distance to the nearest
entity cannot be more than 5 tokens.
We applied this method for the New York Times
and were able to annotate over 60,000 sentences
with over 13,000 distinct entity pairs. For our ex-
periments, we randomly selected one sentence for
each entity pair and separated a thousand for devel-
opment and over 12,000 for test.
1http://www.freebase.com
2http://wordnet.princeton.edu/
452
Comparing with human annotators. Although
we expect our automatic annotator to be less accu-
rate than a human annotator, we are interested to
measure the difference in accuracy between them.
To do so, two authors of this paper looked at our de-
velopment set and marked each sentence as correct
or incorrect. The agreement (that is, the percent-
age of matching answers) between the humans was
82%. On other hand, the agreement between our
automatic annotator and each human was 71% and
72%. This shows that our annotator?s accuracy is not
too far below human?s level of accuracy.
Table 4 shows the results for the test sentences.
Both EXEMPLAR[S] and EXEMPLAR[M] outper-
formed all systems in recall, while keeping the same
level of precision.
4 Conclusion
This work presents a fair and objective evaluation of
several ORE methods, shedding some light on the
trade-offs between f-measure and computational as
well as precision and recall. Our evaluation is able
to assess the effectiveness of different methods by
specifying a trigger and a window of allowed tokens
for each relation.
EXEMPLAR?s promising results indicate that rule-
based methods may still be very competitive, espe-
cially if rules are applied to each argument sepa-
rately. Looking at the recall levels of different meth-
ods, we conjecture that EXEMPLAR outperforms
machine learning methods like Ollie and TreeKer-
nel, because its rules apply in cases not trained by
these methods. From a pragmatic point of view, EX-
EMPLAR is also preferable because it doesn?t require
training data.
An interesting research question is whether ma-
chine learning can be used to learn more rules for
EXEMPLAR in order to improve recall without loss
in precision. Rules could be learned from both de-
pendency parsing and shallow parsing, or just shal-
low parsing if computing time is extremely limited.
The next step for our experimental study is to
evaluate corpus-level extractions, where an auto-
matic annotator is essential due to the massive num-
ber of annotations required for even one relation, let
alone thousands.
Acknowledgements
We thank the anonymous reviewers for useful sug-
gestions to improve the paper, and the Natural Sci-
ences and Engineering Council of Canada, through
the NSERC Business Intelligence Network, for fi-
nancial support.
A EXEMPLAR
ORE methods must recognize relations from the text
alone. To do this, each method tries to model how
relations are expressed in English. Banko and Et-
zioni claim that more than 90% of binary relations
are expressed through a few syntactic patterns, such
as ?verb? and ?noun+preposition? (Banko and Et-
zioni, 2008). It is unclear, however, whether n-ary
relations follow.
This section presents a study focusing on how n-
ary relations (n > 2) are expressed in English, based
on 100 distinct relations manually annotated from a
random sample of 514 sentences in the New York
Times Corpus (Sandhaus, 2008).
Table 5 shows six syntactic patterns that cover
86% of our n-ary relations. These patterns are
slightly different from those used for binary rela-
tions. In binary relations, the pattern implicitly de-
fines the roles of the two arguments. For instance,
a relation ?met with? indicates that the first argu-
ment is the subject and the second one is the object
of ?with?. In order to represent n-ary relations, our
patterns do not contain prepositions, possessives or
any other word connecting the relation to the argu-
ment. For instance, the sentence ?Obama met with
Putin in Russia? contains the relation ?meet? along
three arguments: ?Obama? (subject), ?Putin? (object
of preposition with) and ?Russia? (object of prepo-
sition in).
Relation types. A single relation can be repre-
sented in different ways using the patterns shown
in Table 5. For instance, the relation ?donate? can
be expressed as an active verb (?donates?), passive
voice (?was donated by?) and normalized verb (?do-
nation?). In addition, an apposition+noun relation
can be expressed as an copula+noun relation by re-
placing apposition for the copula verb ?be?. By
merging these patterns, we have that most relations
fall into one of the following types: verb, verb+noun
453
Pattern Frequency Example
Verb 30% Hingis beat Steffi Graf in the Italian Open two weeks ago.
Apposition+noun 19% Jaden and Willow, the famous children of Will Smith, ...
Passive verb 14% Jumbo the Elephant was exported from Sudan to Paris.
Verb+noun 14% D-League will move its offices from Greenville to New York.
Copula+noun 5% Kimball was a Fulbright scholar at the University of Heidelberg.
Nominalized verb 4% Thousands died in Saddam Hussein?s attack on Halabja in 1988.
Table 5: Patterns representing 86% of the relations with three or more arguments. Frequencies collected
from 100 relations from the New York Times Corpus. Relation triggers are highlighted in bold.
Relation Type Freq. Example Variations
Verb 48% beat Pass. verb, nom. verb
Copula+noun 24% is son Apposition+noun
Verb+noun 14% sign deal ?
Table 6: Relation types recognized by EXEMPLAR.
and copula+noun.
Table 6 present the relation types found through
our analysis. We developed EXEMPLAR to specifi-
cally recognize these relation types, including their
variations.
Argument roles. An argument role defines how
an argument participates in a relation. In ORE, the
roles for each relation are not provided and must also
be recognized from the text.
We use the following roles: subject,
direct object and prep object. An argu-
ment has a role prep object when its connected
to the relation by a preposition. The roles of
prepositional objects consist of their preposition and
the suffix ? object?, indicating that each preposition
corresponds to a different role. In the sentence
?Obama is the president of the U.S.?, ?U.S.? is
an object of the preposition ?of? and has the role
of object.
Multiple entities can play the same role in a
relation instance. For instance, in the sentence
?Obama and Putin discuss the Syria conflict?, both
?Obama? and ?Putin? have the subject role. Fur-
thermore, some relations accept less roles than oth-
ers. Verb relations accept all three roles, while cop-
ula+noun and verb+noun relations accept subject
and prep object only.
Our roles are different from those used in SRL.
SRL roles carry semantic information across differ-
ent relations. This information is unavailable for
ORE systems, and for this reason, we rely on syn-
tactic roles. An open problem is to determine which
subject: ?NFL?
relation: ?approve new stadium?
of object: ?Falcons?
in object: ?Atlanta?
Figure 3: A relation instance extracted by EXEM-
PLAR for the sentence ?NFL approves Falcons? new
stadium in Atlanta?.
NFL approves Falcons' new stadium in Atlanta.
nsubj
amod
poss
dobj prep_in
Figure 4: A input sentence after pre-processing. En-
tities are in bold, triggers are underline and arrows
represent dependencies.
syntactic roles correspond to the same semantic role
across different relations (Chambers and Jurafsky,
2011). However, this problem is out of the scope
of this work.
A.1 The method
EXEMPLAR takes a stream of textual documents and
extracts instances of n-ary relations as illustrated in
Figure 3.
A.2 Preprocessing
Given a document, EXEMPLAR extracts its syntactic
structure by applying a pipeline of NLP tools pro-
vided by the Stanford Parser (Klein and Manning,
2003). Our method converts the original text into
sentences, each containing a list of tokens. Each to-
ken is tagged with part of speech, lemma and de-
pendencies. EXEMPLAR also works with other de-
pendency parsers based on Stanford?s dependencies,
454
such as the Malt parser (Nivre and Nilsson, 2004).
Figure 4 illustrates our running example where
each word is a token and arrows represent dependen-
cies among tokens. In this example, ?stadium? de-
pends on ?approves? and the arrow connecting them
can be read as ?the direct object of approves is sta-
dium?.
Extracting Named Entities. EXEMPLAR em-
ploys the Stanford NER (Finkel et al, 2005) to rec-
ognize named entities. We consider these types of
entities: people, organization, location, miscella-
neous and date. Figure 4 shows entities highlighted
in bold.
A.3 Detecting triggers
After recognizing entities, EXEMPLAR detects re-
lation triggers. A trigger is a single token that in-
dicates the presence of a relation. A relation may
present one or two triggers. For instance, the rela-
tion in our running example has two triggers. EX-
EMPLAR also uses triggers to determine the relation
name, as discussed later.
A trigger can be any noun or verb that was not
tagged as being part of an entity mention. Other re-
quirements are enforced for each canonical pattern.
Verb relations. A verb relation is triggered by a
verb that does not include a noun as its direct object.
The name of the relation is the trigger?s lemma.
A noun must be a nominalized verb to be a trigger
for verb relations. To identify nominalized verbs,
EXEMPLAR checks if a noun is filed under the type
?event? in Wordnet?s Morphosemantic Database3.
Doing so may generate false positives; however,
EXEMPLAR has a filtering step to eliminate these
false positives, as discussed later.
The name of a relation triggered by a nominalized
verb is the trigger?s original verb (before nominal-
ization). For instance, ?donation? triggers the rela-
tion ?donate?.
Copula+noun relations. Only noun triggers are
accepted for copula+noun relations. The copula
used in the relation name can be a verb with cop-
ula dependency to the trigger, or the verb ?be? for
3http://wordnetcode.princeton.edu/
standoff-files/morphosemantic-links.xls
appositions. The relation name is the concatena-
tion of the copula?s lemma and the trigger?s lemma
along its modifiers. For instance, the relation in the
sentence ?Jaden and Willow, the famous children of
Will Smith? is ?be famous child?.
Verb+noun relations. EXEMPLAR recognizes
two triggers for each verb+noun relation: a verb
and a noun acting as its direct object. The relation
name is defined by concatenating the verb?s lemma
with the the noun and its modifiers. In our running
example, ?approves? and ?stadium? trigger the
relation ?approve new stadium?.
A.4 Detecting candidate arguments
After relation triggers are identified, EXEMPLAR
proceeds to detect their candidate arguments. For
this, we look at the dependency between each entity
and a trigger separately. EXEMPLAR relies on two
observations: (1) an argument is often adjacent to
a trigger in the dependency graph, and (2) the type
of the dependency can accurately predict whether an
entity is an argument for the relation or not.
Table 7 enumerates 12 types of dependencies
(from a total of 53) that often connect arguments and
triggers. EXEMPLAR identifies as a candidate argu-
ment every entity that is connected to trigger, as long
as their dependency type is listed in Table 7.
Our observations can be seen in our running ex-
ample. The entities ?NFL? and ?Atlanta? depends
on the trigger ?approves? and ?Falcons? depend on
the trigger ?stadium?. Since their dependency types
are listed in Table 7, these entities are marked as can-
didate arguments.
A.5 Role Detection
EXEMPLAR determines the role of an argument
based on the trigger type (noun or verb), the type of
dependency between the trigger and argument and
the direction of the dependency. To take into ac-
count the dependency direction, we prefix each de-
pendency type with ?>? when an entity depends on
the trigger and ?<? when the trigger depends on the
entity.
Table 8 shows EXEMPLAR?s rules that assign
roles to arguments for each relation type. Rules are
triples (trigger , dependency , role) whose meaning
is as follows:
455
Dependency Example
nsubj (Subject) Romeo loves Juliet
dobj (Direct Object) The Prince exiles Romeo
nsubjpass (Pass. Subj.) Romeo was seen in Verona
agent (Pass. Voice Obj.) Juliet is loved by Romeo
iobj (Indirect Object) Romeo gave Juliet a kiss
poss (Possessive) Romeo?s father Montague
appos (Apposition) Capulet, Juliet?s father,
amod (Adj. Modifier) The Italian city of Verona
nn (Noun Comp. Mod.) Romeo?s cousin Benvolio
prep * (Prep. Modifier) Romeo lived in Verona
partmod (Participal Mod.) Romeo, born in Italy
rcmod (Rel. Clause Mod.) Juliet, who loved Romeo
Table 7: Dependencies connecting arguments and
triggers. Arguments are in bold and triggers are un-
derlined.
If trigger type = trigger and dependency
type = dependency then assign role.
For example, the first rule in Table 8a specifies
that an argument must be assigned the role of a sub-
ject if this argument depends on a verb trigger and
the dependency type is >nsubj.
Rules are ordered by descending priority in Ta-
ble 8. In case several rules can be applied for an ar-
gument, we apply only the rule with higher priority.
If none of the rules applies to an argument, EXEM-
PLAR removes this argument from the relation.
Exceptions. There are three exceptions for the
rules above. The first exception concerns argu-
ments of verb relations whose dependency type is
<partmod or <rcmod. EXEMPLAR chooses the role
of direct object (as oppose to subject) for these ar-
guments when the verb trigger is in passive form.
For instance, in the sentence ?Barbie, (which was)
invented by Handler?, ?Barbie? has the role di-
rect object because ?invented? is in passive form.
The second exception is for nominalized verbs
followed by the preposition ?by?, such as in ?Geor-
gian invasion by Russia?. Arguments of this type
of trigger with dependency types >nn, >amod or
>poss are assigned the role direct object.
Finallly, there is an exception for copula+noun
relations expressed with close appositions of the
form: ?determiner entity noun?. An example is ?the
Greenwood Heights section of Brooklyn?. Here,
EXEMPLAR assigns the subject role to the entity be-
tween the determiner and the noun.
Trigger Type Dependency Type Role
Verb >nsubj subject
Verb >agent subject
Verb <partmod subject
Verb <rcmod subject
Verb >dobj direct object
Verb >subjpass direct object
Verb >iobj to object
Verb >prep * prep object
Noun >prep by subject
Noun >amod subject
Noun >nn subject
Noun >poss subject
Noun >prep of direct object
Noun >prep * prep object
(a) Rules for verb relations.
Trigger Type Dependency Type Role
Noun >nsubj subject
Noun >appos subject
Noun <appos subject
Noun <partmod subject
Noun <rcmod subject
Noun >prep of of object
Noun >amod of object
Noun >nn of object
Noun >poss of object
Noun >prep * prep object
(b) Rules for copula+noun relations.
Trigger Type Dependency Type Role
Verb >nsubj subject
Verb >agent subject
Verb <partmod subject
Verb <rcmod subject
Verb >iobj to object
Verb >prep * prep object
Noun >amod of object
Noun >nn of object
Noun >poss of object
Noun >prep * prep object
(c) Rules for verb+noun relations.
Table 8: Rules for assigning roles to arguments.
A.6 Filtering Relations
The final step in EXEMPLAR is to remove incom-
plete relations. EXEMPLAR removes relations with
less than two arguments and relations that do not
present subject and direct object.
For EXEMPLAR, Lund and SwiRL, which extract
n-ary relations, our evaluation needs to convert n-ary
relations into binary ones. This is done by selecting
all pairs of arguments from a n-ary relation and cre-
ating a new (binary) relation for each of them. Bi-
nary relations containing two prepositional objects
(or equivalent for SRL systems) are removed.
456
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics, pages 28?36, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages 976?
986, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2011. An analysis of open informa-
tion extraction based on semantic role labeling. In
Proceedings of the Sixth International Conference on
Knowledge Capture, pages 113?120. Association for
Computational Linguistics.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceed-
ings of Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL?12, pages 708?
716.
Nilesh Dalvi, Ashwin Machanavajjhala, and Bo Pang.
2012. An analysis of structured data on the web. Proc.
VLDB Endow., 5(7):680?691.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP?12.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of
propbank. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP?08, pages 69?78, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Mausam, Michael Schmitz, Robert Bart, Stephen Soder-
land, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, EMNLP-CoNLL?12.
Yuval Merhav, Filipe de Sa? Mesquita, Denilson Barbosa,
Wai Gen Yee, and Ophir Frieder. 2012. Extracting
information networks from the blogosphere. TWEB,
6(3):11.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: a taxonomy of relational
patterns with semantic types. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning, EMNLP-CoNLL?12, pages
1135?1145, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Joakim Nivre and Jens Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of Computational
Natural Language Learning, CoNLL?04.
Evan Sandhaus. 2008. The new york times anno-
tated corpus. http://www.ldc.upenn.edu/
Catalog/catalogEntry.jsp?catalogId=
LDC2008T19.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 8?15. Association
for Computational Linguistics.
Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,
and Denilson Barbosa. 2013. Open information ex-
traction with tree kernels. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 868?877, Atlanta, Georgia,
June. Association for Computational Linguistics.
457
Proceedings of NAACL-HLT 2013, pages 868?877,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Open Information Extraction with Tree Kernels
Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel and Denilson Barbosa
Department of Computing Science
University of Alberta
{yx2,miyoung2,kfjquinn,goebel,denilson}@cs.ualberta.ca
Abstract
Traditional relation extraction seeks to iden-
tify pre-specified semantic relations within
natural language text, while open Information
Extraction (Open IE) takes a more general ap-
proach, and looks for a variety of relations
without restriction to a fixed relation set. With
this generalization comes the question, what
is a relation? For example, should the more
general task be restricted to relations medi-
ated by verbs, nouns, or both? To help answer
this question, we propose two levels of sub-
tasks for Open IE. One task is to determine if
a sentence potentially contains a relation be-
tween two entities? The other task looks to
confirm explicit relation words for two enti-
ties. We propose multiple SVM models with
dependency tree kernels for both tasks. For
explicit relation extraction, our system can ex-
tract both noun and verb relations. Our results
on three datasets show that our system is su-
perior when compared to state-of-the-art sys-
tems like REVERB and OLLIE for both tasks.
For example, in some experiments our system
achieves 33% improvement on nominal rela-
tion extraction over OLLIE. In addition we
propose an unsupervised rule-based approach
which can serve as a strong baseline for Open
IE systems.
1 Introduction
Relation Extraction (RE) systems are designed to
discover various semantic relations (e.g. <Obama,
president, the United States>) from natural lan-
guage text. Traditional RE systems extract spe-
cific relations for prespecified name-entity types
(Bunescu and Mooney, 2005; Chan and Dan, 2011;
Zhou and Zhu, 2011). To train such systems, ev-
ery relation needs manually annotated training ex-
amples, which supports limited scope and is diffi-
cult to extend. For this reason, Banko et al (2007)
proposed Open Information Extraction (Open IE),
whose goal is to extract general relations for two en-
tities. The idea is to avoid the need for specific train-
ing examples, and to extract a diverse range of rela-
tions. This generalized form has received significant
attention, e.g., (Banko et al, 2007; Akbik, 2009; Wu
and Weld, 2010; Fader et al, 2011; Mausam et al,
2012).
Because Open IE is not guided by or not restricted
to a prespecified list of relations, the immediate chal-
lenge is determining about what counts as a relation?
Most recent Open IE systems have targeted verbal
relations (Banko et al, 2007; Mausam et al, 2012),
claiming that these are the majority. However, Chan
and Dan (2011) show that only 20% of relations in
the ACE programs Relation Detection and Charac-
terization (RDC) are verbal. Our manually extracted
relation triple set from the Penn Treebank shows that
there are more nominal relations than verbal ones,
3 to 2. This difference arises because of the ambi-
guity of what constitutes a relation in Open IE. It
is often difficult even for humans to agree on what
constitutes a relation, and which words in the sen-
tence establish a relation between a pair of entities.
For example, in the sentence ?Olivetti broke Cocom
rules? is there a relation between Olivetti and Co-
com? This ambiguity in the problem definition leads
to significant challenges and confusion when eval-
uating and comparing the performance of different
methods and systems. An example are the results
in Fader et al (2011) and Mausam et al (2012). In
Fader et al (2011), REVERB ?is reported? as su-
868
perior to WOEparse, a system proposed in Wu and
Weld (2010); while in Mausam et al (2012), it is
reported the opposite.
To better answer the question, what counts as a
relation? we propose two tasks for Open IE. The
first task seeks to determine whether there is a re-
lation between two entities (called ?Binary task?).
The other is to confirm whether the relation words
extracted for the two entities are appropriate (the
?Triple task?). The Binary task does not restrict re-
lation word forms, whether they are mediated by
nouns, verbs, prepositions, or even implicit rela-
tions. The Triple task requires an abstract repre-
sentation of relation word forms, which we develop
here. We assume that relation words are nouns or
verbs; in our data, these two types comprise 71% of
explicit relations.
We adapt an SVM dependency tree kernel model
(Moschitti, 2006) for both tasks. The input to our
tasks is a dependency parse, created by Stanford
Parser. Selecting relevant features from a parse tree
for semantic tasks is difficult. SVM tree kernels
avoid extracting explicit features from parse trees
by calculating the inner product of the two trees.
For the Binary task, our dependency path is the path
between two entities. For the Triple task, the path
is among entities and relation words (i.e. relation
triples). Tree kernels have been used in traditional
RE and have helped achieve state of the art perfor-
mance (Culotta and Sorensen, 2004; Bunescu and
Mooney, 2005; Wang, 2008; Nguyen et al, 2009;
Zhou and Zhu, 2011). But one challenge of using
tree kernels on Open IE is that the lexicon of re-
lations is much larger than those of traditional RE,
making it difficult to include the lexical information
as features. Here we proposed an unlexicalized tree
structure for Open IE. As far as we know, this is the
first time an SVM tree kernel has been applied in
Open IE. Experimental results on multiple datasets
show our system outperforms state-of-the-art sys-
tems REVERB and OLLIE. Typically an Open IE
system is tested on one dataset. However, because
the definition of relation is ambiguous, we believe
that is necessary to test with multiple datasets.
In addition to the supervised model, we also pro-
pose an unsupervised model which relies on several
heuristic rules. Results with this approach show that
this simple unsupervised model provides a robust
strong baseline for other approaches.
In summary, our main contributions are:
? Use SVM tree kernels for Open IE. Our sys-
tem is robust comparing with other Open IE
systems, achieving superior scores in two test
sets and comparative scores in another set.
? Extend beyond verbal relations, which are
prevalent in current systems. Analyze implicit
relation problem in Open IE, which is ignored
by other work.
? Propose an unsupervised model for Open IE,
which can be a strong baseline for other ap-
proaches.
The rest of this paper is organized as follows. Sec-
tion 2 provides the problem description and system
structure, before summarizing previous work in Sec-
tion 3. Section 4 defines our representation of rela-
tion word patterns crucial to our task two, and Sec-
tion 5 describes tree kernels for SVM. Section 6 de-
scribes the unsupervised model, and Section 7 ex-
plains our experiment design and results. Section 8
concludes with a summary, and anticipation of fu-
ture work.
2 Problem Definition and System
Structure
The common definition of the Open IE task is a
function from a sentence, s, to a set of triples,
{< E1, R,E2 >}, where E1 and E2 are entities
(noun phrases) and R is a textual fragment indicat-
ing a semantic relation between the two entities. Our
?Triple task? is within this definition. However it is
often difficult to determine which textual fragments
to extract. In addition, semantic relations can be im-
plicit, e.g., consider the located in relation in the sen-
tence fragment ?Washington, US.? To illustrate how
much information is lost when restricting the rela-
tion forms, we add another task (the ?Binary task?),
determining if there is a relation between the two en-
tities. It is a function from s, to a set of binary rela-
tions over entities, {< E1, E2 >}. This binary task
is designed to overcome the disadvantage of current
Open IE systems, which suffer because of restricting
the relation form, e.g., to only verbs, or only nouns.
The two tasks are independent to each other.
869
	

	

		

		
		Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1312?1320,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Identification of Speakers in Novels
Hua He? Denilson Barbosa ? Grzegorz Kondrak?
?Department of Computer Science ?Department of Computing Science
University of Maryland University of Alberta
huah@cs.umd.edu {denilson,gkondrak}@ualberta.ca
Abstract
Speaker identification is the task of at-
tributing utterances to characters in a lit-
erary narrative. It is challenging to auto-
mate because the speakers of the majority
of utterances are not explicitly identified in
novels. In this paper, we present a super-
vised machine learning approach for the
task that incorporates several novel fea-
tures. The experimental results show that
our method is more accurate and general
than previous approaches to the problem.
1 Introduction
Novels are important as social communication
documents, in which novelists develop the plot
by means of discourse between various charac-
ters. In spite of a frequently expressed opinion
that all novels are simply variations of a certain
number of basic plots (Tobias, 2012), every novel
has a unique plot (or several plots) and a different
set of characters. The interactions among charac-
ters, especially in the form of conversations, help
the readers construct a mental model of the plot
and the changing relationships between charac-
ters. Many of the complexities of interpersonal re-
lationships, such as romantic interests, family ties,
and rivalries, are conveyed by utterances.
A precondition for understanding the relation-
ship between characters and plot development in
a novel is the identification of speakers behind all
utterances. However, the majority of utterances
are not explicitly tagged with speaker names, as
is the case in stage plays and film scripts. In most
cases, authors rely instead on the readers? compre-
hension of the story and of the differences between
characters.
Since manual annotation of novels is costly, a
system for automatically determining speakers of
utterances would facilitate other tasks related to
the processing of literary texts. Speaker identifica-
tion could also be applied on its own, for instance
in generating high quality audio books without hu-
man lectors, where each character would be iden-
tifiable by a distinct way of speaking. In addi-
tion, research on spoken language processing for
broadcast and multi-party meetings (Salamin et
al., 2010; Favre et al, 2009) has demonstrated that
the analysis of dialogues is useful for the study of
social interactions.
In this paper, we investigate the task of speaker
identification in novels. Departing from previous
approaches, we develop a general system that can
be trained on relatively small annotated data sets,
and subsequently applied to other novels for which
no annotation is available. Since every novel has
its own set of characters, speaker identification
cannot be formulated as a straightforward tagging
problem with a universal set of fixed tags. Instead,
we adopt a ranking approach, which enables our
model to be applied to literary texts that are differ-
ent from the ones it has been trained on.
Our approach is grounded in a variety of fea-
tures that are easily generalizable across differ-
ent novels. Rather than attempt to construct com-
plete semantic models of the interactions, we ex-
ploit lexical and syntactic clues in the text itself.
We propose several novel features, including the
speaker alternation pattern, the presence of voca-
tives in utterances, and unsupervised actor-topic
features that associate speakers with utterances on
the basis of their content. Experimental evaluation
shows that our approach not only outperforms the
baseline, but also compares favorably to previous
approaches in terms of accuracy and generality,
even when tested on novels and authors that are
different from those used for training.
The paper is organized as follows. After dis-
cussing previous work, and defining the terminol-
ogy, we present our approach and the features that
it is based on. Next, we describe the data, the an-
1312
notation details, and the results of our experimen-
tal evaluation. At the end, we discuss an applica-
tion to extracting a set of family relationships from
a novel.
2 Related Work
Previous work on speaker identification includes
both rule-based and machine-learning approaches.
Glass and Bangay (2007) propose a rule gener-
alization method with a scoring scheme that fo-
cuses on the speech verbs. The verbs, such as
said and cried, are extracted from the communi-
cation category of WordNet (Miller, 1995). The
speech-verb-actor pattern is applied to the utter-
ance, and the speaker is chosen from the avail-
able candidates on the basis of a scoring scheme.
Sarmento and Nunes (2009) present a similar ap-
proach for extracting speech quotes from online
news texts. They manually define 19 variations of
frequent speaker patterns, and identify a total of
35 candidate speech verbs. The rule-based meth-
ods are typically characterized by low coverage,
and are too brittle to be reliably applied to differ-
ent domains and changing styles.
Elson and McKeown (2010) (henceforth re-
ferred to as EM2010) apply the supervised ma-
chine learning paradigm to a corpus of utterances
extracted from novels. They construct a single
feature vector for each pair of an utterance and
a speaker candidate, and experiment with various
WEKA classifiers and score-combination meth-
ods. To identify the speaker of a given utterance,
they assume that all previous utterances are al-
ready correctly assigned to their speakers. Our
approach differs in considering the utterances in
a sequence, rather than independently from each
other, and in removing the unrealistic assumption
that the previous utterances are correctly identi-
fied.
The speaker identification task has also been in-
vestigated in other domains. Bethard et al (2004)
identify opinion holders by using semantic pars-
ing techniques with additional linguistic features.
Pouliquen et al (2007) aim at detecting direct
speech quotations in multilingual news. Krestel
et al (2008) automatically tag speech sentences
in newspaper articles. Finally, Ruppenhofer et al
(2010) implement a rule-based system to enrich
German cabinet protocols with automatic speaker
attribution.
3 Definitions and Conventions
In this section, we introduce the terminology used
in the remainder of the paper. Our definitions are
different from those of EM2010 partly because we
developed our method independently, and partly
because we disagree with some of their choices.
The examples are from Jane Austen?s Pride and
Prejudice, which was the source of our develop-
ment set.
An utterance is a connected text that can be at-
tributed to a single speaker. Our task is to associate
each utterance with a single speaker. Utterances
that are attributable to more than one speaker are
rare; in such cases, we accept correctly identifying
one of the speakers as sufficient. In some cases, an
utterance may include more than one quotation-
delimited sequence of words, as in the following
example.
?Miss Bingley told me,? said Jane, ?that
he never speaks much.?
In this case, the words said Jane are simply a
speaker tag inserted into the middle of the quoted
sentence. Unlike EM2010, we consider this a sin-
gle utterance, rather than two separate ones.
We assume that all utterances within a para-
graph can be attributed to a single speaker. This
?one speaker per paragraph? property is rarely vi-
olated in novels ? we identified only five such
cases in Pride & Prejudice, usually involving one
character citing another, or characters reading let-
ters containing quotations. We consider this an
acceptable simplification, much like assigning a
single part of speech to each word in a corpus.
We further assume that each utterance is contained
within a single paragraph. Exceptions to this rule
can be easily identified and resolved by detecting
quotation marks and other typographical conven-
tions.
The paragraphs without any quotations are re-
ferred to as narratives. The term dialogue denotes
a series of utterances together with related narra-
tives, which provide the context of conversations.
We define a dialogue as a series of utterances and
intervening narratives, with no more than three
continuous narratives. The rationale here is that
more than three narratives without any utterances
are likely to signal the end of a particular dialogue.
We distinguish three types of utterances, which
are listed with examples in Table 1: explicit
speaker (identified by name within the paragraph),
1313
Category Example
Implicit
speaker
?Don?t keep coughing so, Kitty,
for heaven?s sake!?
Explicit
speaker
?I do not cough for my own
amusement,? replied Kitty.
Anaphoric
speaker
?Kitty has no discretion in her
coughs,? said her father.
Table 1: Three types of utterances.
anaphoric speaker (identified by an anaphoric ex-
pression), and implicit speaker (no speaker infor-
mation within the paragraph). Typically, the ma-
jority of utterances belong to the implicit-speaker
category. In Pride & Prejudice only roughly 25%
of the utterances have explicit speakers, and an
even smaller 15% belong to the anaphoric-speaker
category. In modern fiction, the percentage of ex-
plicit attributions is even lower.
4 Speaker Identification
In this section, we describe our method of extract-
ing explicit speakers, and our ranking approach,
which is designed to capture the speaker alterna-
tion pattern.
4.1 Extracting Speakers
We extract explicit speakers by focusing on the
speech verbs that appear before, after, or between
quotations. The following verbs cover most cases
in our development data: say, speak, talk, ask, re-
ply, answer, add, continue, go on, cry, sigh, and
think. If a verb from the above short list cannot be
found, any verb that is preceded by a name or a
personal pronoun in the vicinity of the utterance is
selected as the speech verb.
In order to locate the speaker?s name or
anaphoric expression, we apply a deterministic
method based on syntactic rules. First, all para-
graphs that include narrations are parsed with a
dependency parser. For example, consider the fol-
lowing paragraph:
As they went downstairs together, Char-
lotte said, ?I shall depend on hearing
from you very often, Eliza.?
The parser identifies a number of dependency rela-
tions in the text, such as dobj(went-3, downstairs-
4) and advmod(went-3, together-5). Our method
extracts the speaker?s name from the dependency
relation nsubj(said-8, Charlotte-7), which links a
speech verb with a noun phrase that is the syntac-
tic subject of a clause.
Once an explicit speaker?s name or an anaphoric
expression is located, we determine the corre-
sponding gender information by referring to the
character list or by following straightforward rules
to handle the anaphora. For example, if the utter-
ance is followed by the phrase she said, we infer
that the gender of the speaker is female.
4.2 Ranking Model
In spite of the highly sequential nature of the
chains of utterances, the speaker identification task
is difficult to model as sequential prediction. The
principal problem is that, unlike in many NLP
problems, a general fixed tag set cannot be de-
fined beyond the level of an individual novel.
Since we aim at a system that could be applied to
any novel with minimal pre-processing, sequential
prediction algorithms such as Conditional Ran-
dom Fields are not directly applicable.
We propose a more flexible approach that as-
signs scores to candidate speakers for each utter-
ance. Although the sequential information is not
directly modeled with tags, our system is able
to indirectly utilize the speaker alternation pat-
tern using the method described in the following
section. We implement our approach with SVM-
rank (Joachims, 2006).
4.3 Speaker Alternation Pattern
The speaker alternation pattern is often employed
by authors in dialogues between two charac-
ters. After the speakers are identified explicitly at
the beginning of a dialogue, the remaining odd-
numbered and even-numbered utterances are at-
tributable to the first and second speaker, respec-
tively. If one of the speakers ?misses their turn?, a
clue is provided in the text to reset the pattern.
Based on the speaker alternation pattern, we
make the following two observations:
1. The speakers of consecutive utterances are
usually different.
2. The speaker of the n-th utterance in a dia-
logue is likely to be the same as the speaker
of the (n? 2)-th utterance.
Our ranking model incorporates the speaker al-
ternation pattern by utilizing a feature expansion
scheme. For each utterance n, we first gener-
ate its own features (described in Section 5), and
1314
Features Novelty
Distance to Utterance No
Speaker Appearance Count No
Speaker Name in Utterance No
Unsupervised Actor-Topic Model Yes
Vocative Speaker Name Yes
Neighboring Utterances Yes
Gender Matching Yes
Presence Matching Yes
Table 2: Principal feature sets.
subsequently we add three more feature sets that
represent the following neighboring utterances:
n? 2, n? 1 and n+1. Informally, the features of
the utterances n? 1 and n+1 encode the first ob-
servation, while the features representing the utter-
ance n ? 2 encode the second observation. In ad-
dition, we include a set of four binary features that
are set for the utterances in the range [n?2, n+1]
if the corresponding explicit speaker matches the
candidate speaker of the current utterance.
5 Features
In this section, we describe the set of features used
in our ranking approach. The principal feature sets
are listed in Table 2, together with an indication
whether they are novel or have been used in previ-
ous work.
5.1 Basic Features
A subset of our features correspond to the features
that were proposed by EM2010. These are mostly
features related to speaker names. For example,
since names of speakers are often mentioned in
the vicinity of their utterances, we count the num-
ber of words separating the utterance and a name
mention. However, unlike EM2010, we consider
only the two nearest characters in each direction,
to reflect the observation that speakers tend to be
mentioned by name immediately before or after
their corresponding utterances. Another feature is
used to represent the number of appearances for
speaker candidates. This feature reflects the rela-
tive importance of a given character in the novel.
Finally, we use a feature to indicate the presence
or absence of a candidate speaker?s name within
the utterance. The intuition is that speakers are
unlikely to mention their own name.
Feature Example
start of utterance ?Kitty . . .
before period . . . Jane.
between commas . . . , Elizabeth, . . .
between comma & period . . . , Mrs. Hurst.
before exclamation mark . . . Mrs. Bennet!
before question mark . . . Lizzy?. . .
vocative phrase Dear . . .
after vocative phrase Oh! Lydia . . .
2nd person pronoun . . . you . . .
Table 3: Features for the vocative identification.
5.2 Vocatives
We propose a novel vocative feature, which en-
codes the character that is explicitly addressed in
an utterance. For example, consider the following
utterance:
?I hope Mr. Bingley will like it, Lizzy.?
Intuitively, the speaker of the utterance is neither
Mr. Bingley nor Lizzy; however, the speaker of the
next utterance is likely to be Lizzy. We aim at cap-
turing this intuition by identifying the addressee of
the utterance.
We manually annotated vocatives in about 900
utterances from the training set. About 25% of
the names within utterance were tagged as voca-
tives. A Logistic Regression classifier (Agresti,
2006) was trained to identify the vocatives. The
classifier features are shown in Table 3. The fea-
tures are designed to capture punctuation context,
as well as the presence of typical phrases that ac-
company vocatives. We also incorporate interjec-
tions like ?oh!? and fixed phrases like ?my dear?,
which are strong indicators of vocatives. Under
10-fold cross validation, the model achieved an F-
measure of 93.5% on the training set.
We incorporate vocatives in our speaker identi-
fication system by means of three binary features
that correspond to the utterances n? 1, n? 2, and
n ? 3. The features are set if the detected voca-
tive matches the candidate speaker of the current
utterance n.
5.3 Matching Features
We incorporate two binary features for indicating
the gender and the presence of a candidate speaker.
The gender matching feature encodes the gender
agreement between a speaker candidate and the
speaker of the current utterance. The gender in-
formation extraction is applied to two utterance
1315
groups: the anaphoric-speaker utterances, and the
explicit-speaker utterances. We use the technique
described in Section 4.1 to determine the gender
of a speaker of the current utterance. In contrast
with EM2010, this is not a hard constraint.
The presence matching feature indicates
whether a speaker candidate is a likely partic-
ipant in a dialogue. Each dialogue consists of
continuous utterance paragraphs together with
neighboring narration paragraphs as defined in
Section 3. The feature is set for a given character
if its name or alias appears within the dialogue.
5.4 Unsupervised Actor-Topic Features
The final set of features is generated by the unsu-
pervised actor-topic model (ACTM) (Celikyilmaz
et al, 2010), which requires no annotated train-
ing data. The ACTM, as shown in Figure 1, ex-
tends the work of author-topic model in (Rosen-
Zvi et al, 2010). It can model dialogues in a lit-
erary text, which take place between two or more
speakers conversing on different topics, as distri-
butions over topics, which are also mixtures of the
term distributions associated with multiple speak-
ers. This follows the linguistic intuition that rich
contextual information can be useful in under-
standing dialogues.
Figure 1: Graphical Representation of ACTM.
The ACTM predicts the most likely speakers of
a given utterance by considering the content of an
utterance and its surrounding contexts. The Actor-
Topic-Term probabilities are calculated by using
both the relationship of utterances and the sur-
rounding textual clues. In our system, we utilize
four binary features that correspond to the four top
ranking positions from the ACTM model.
Figure 2: Annotation Tool GUI.
6 Data
Our principal data set is derived from the text
of Pride and Prejudice, with chapters 19?26 as
the test set, chapters 27?33 as the development
set, and the remaining 46 chapters as the training
set. In order to ensure high-quality speaker anno-
tations, we developed a graphical interface (Fig-
ure 2), which displays the current utterance in con-
text, and a list of characters in the novel. After the
speaker is selected by clicking a button, the text
is scrolled automatically, with the next utterance
highlighted in yellow. The complete novel was
annotated by a student of English literature. The
annotations are publicly available1.
For the purpose of a generalization experiment,
we also utilize a corpus of utterances from the
19th and 20th century English novels compiled by
EM2010. The corpus differs from our data set in
three aspects. First, as discussed in Section 3, we
treat all quoted text within a single paragraph as
a single utterance, which reduces the total num-
ber of utterances, and results in a more realistic
reporting of accuracy. Second, our data set in-
cludes annotations for all utterances in the novel,
as opposed to only a subset of utterances from sev-
eral novels, which are not necessarily contiguous.
Lastly, our annotations come from a single expert,
while the annotations in the EM2010 corpus were
collected through Amazon?s Mechanical Turk, and
filtered by voting. For example, out of 308 utter-
ances from The Steppe, 244 are in fact annotated,
which raises the question whether the discarded
utterances tend to be more difficult to annotate.
Table 4 shows the number of utterances in all
1www.cs.ualberta.ca/?kondrak/austen
1316
IS AS ES Total
Pride & P. (all) 663 292 305 1260
Pride & P. (test) 65 29 32 126
Emma 236 55 106 397
The Steppe 93 39 112 244
Table 4: The number of utterances in various
data sets by the type (IS - Implicit Speaker; AS
- Anaphoric Speaker; ES - Explicit Speaker).
data sets. We selected Jane Austen?s Emma as
a different novel by the same author, and Anton
Chekhov?s The Steppe as a novel by a different au-
thor for our generalization experiments.
Since our goal is to match utterances to charac-
ters rather than to name mentions, a preprocess-
ing step is performed to produce a list of char-
acters in the novel and their aliases. For exam-
ple, Elizabeth Bennet may be referred to as Liz,
Lizzy, Miss Lizzy, Miss Bennet, Miss Eliza, and
Miss Elizabeth Bennet. We apply a name entity
tagger, and then group the names into sets of char-
acter aliases, together with their gender informa-
tion. The sets of aliases are typically small, except
for major characters, and can be compiled with
the help of web resources, such as Wikipedia, or
study guides, such as CliffsNotesTM . This pre-
processing step could also be performed automati-
cally using a canonicalization method (Andrews et
al., 2012); however, since our focus is on speaker
identification, we decided to avoid introducing an-
notation errors at this stage.
Other preprocessing steps that are required for
processing a new novel include standarizing the
typographical conventions, and performing POS
tagging, NER tagging, and dependency parsing.
We utilize the Stanford tools (Toutanova et al,
2003; Finkel et al, 2005; Marneffe et al, 2006).
7 Evaluation
In this section, we describe experiments conducted
to evaluate our speaker identification approach.
We refer to our main model as NEIGHBORS, be-
cause it incorporates features from the neighbor-
ing utterances, as described in Section 4.3. In
contrast, the INDIVIDUAL model relies only on
features from the current utterance. In an at-
tempt to reproduce the evaluation methodology of
EM2010, we also test the ORACLE model, which
has access to the gold-standard information about
the speakers of eight neighboring utterances in the
Pride & P. Emma Steppe
BASELINE 42.0 44.1 66.8
INDIVIDUAL 77.8 67.3 74.2
NEIGHBORS 82.5 74.8 80.3
ORACLE 86.5 80.1 83.6
Table 5: Speaker identification accuracy (in %) on
Pride & Prejudice, Emma, and The Steppe.
range [n ? 4, n + 4]. Lastly, the BASELINE ap-
proach selects the name that is the closest in the
narration, which is more accurate than the ?most
recent name? baseline.
7.1 Results
Table 5 shows the results of the models trained on
annotated utterances from Pride & Prejudice on
three test sets. As expected, the accuracy of all
learning models on the test set that comes from
the same novel is higher than on unseen novels.
However, in both cases, the drop in accuracy for
the NEIGHBORS model is less than 10%.
Surprisingly, the accuracy is higher on The
Steppe than on Emma, even though the differ-
ent writing style of Chekhov should make the
task more difficult for models trained on Austen?s
prose. The protagonists of The Steppe are mostly
male, and the few female characters rarely speak
in the novel. This renders our gender feature
virtually useless, and results in lower accuracy
on anaphoric speakers than on explicit speakers.
On the other hand, Chekhov prefers to mention
speaker names in the dialogues (46% of utterances
are in the explicit-speaker category), which makes
his prose slightly easier in terms of speaker identi-
fication.
The relative order of the models is the same
on all three test sets, with the NEIGHBORS
model consistently outperforming the INDIVID-
UAL model, which indicates the importance of
capturing the speaker alternation pattern. The per-
formance of the NEIGHBORS model is actually
closer to the ORACLE model than to the INDIVID-
UAL model.
Table 6 shows the results on Emma broken
down according to the type of the utterance. Un-
surprisingly, the explicit speaker is the easiest cat-
egory, with nearly perfect accuracy. Both the IN-
DIVIDUAL and the NEIGHBORS models do better
on anaphoric speakers than on implicit speakers,
which is also expected. However, it is not the
1317
IS AS ES Total
INDIVIDUAL 52.5 67.3 100.0 67.3
NEIGHBORS 63.1 76.4 100.0 74.8
ORACLE 74.2 69.1 99.1 80.1
Table 6: Speaker identification accuracy (in %) on
Austen?s Emma by the type of utterance.
case for the ORACLE model. We conjecture that
the ORACLE model relies heavily on the neighbor-
hood features (which are rarely wrong), and con-
sequently tends to downplay the gender informa-
tion, which is the only information extracted from
the anaphora. In addition, anaphoric speaker is the
least frequent of the three categories.
Table 7 shows the results of an ablation study
performed to investigate the relative importance of
features. The INDIVIDUAL model serves as the
base model from which we remove specific fea-
tures. All tested features appear to contribute to
the overall performance, with the distance features
and the unsupervised actor-topic features having
the most pronounced impact. We conclude that the
incorporation of the neighboring features, which
is responsible for the difference between the IN-
DIVIDUAL and NEIGHBORS models, is similar in
terms of importance to our strongest textual fea-
tures.
Feature Impact
Closest Mention -6.3
Unsupervised ACTM -5.6
Name within Utterance -4.8
Vocative -2.4
Table 7: Results of feature ablation (in % accu-
racy) on Pride & Prejudice.
7.2 Comparison to EM2010
In this section we analyze in more detail our re-
sults on Emma and The Steppe against the pub-
lished results of the state-of-the-art EM2010 sys-
tem. Recall that both novels form a part of the
corpus that was created by EM2010 for the devel-
opment of their system.
Direct comparison to EM2010 is difficult be-
cause they compute the accuracy separately for
seven different categories of utterances. For each
category, they experiment with all combinations
of three different classifiers and four score com-
bination methods, and report only the accuracy
Character
id name gender
. . .
9 Mr. Collins m
10 Charlotte f
11 Jane Bennet f
12 Elizabeth Bennet f
. . .
Relation
from to type mode
. . .
10 9 husband explicit
9 10 wife derived
10 12 friend explicit
12 10 friend derived
11 12 sister explicit
. . .
Figure 3: Relational database with extracted social
network.
achieved by the best performing combination on
that category. In addition, they utilize the ground
truth speaker information of the preceding utter-
ances. Therefore, their results are best compared
against our ORACLE approach.
Unfortunately, EM2010 do not break down their
results by novel. They report the overall ac-
curacy of 63% on both ?anaphora trigram? (our
anaphoric speaker), and ?quote alone? (similar to
our implicit speaker). If we combine the two cate-
gories, the numbers corresponding to our NEIGH-
BORS model are 65.6% on Emma and 64.4% on
The Steppe, while ORACLE achieves 73.2% and
70.5%, respectively. Even though a direct com-
parison is not feasible, the numbers are remarkable
considering the context of the experiment, which
strongly favors the EM2010 system.
8 Extracting Family Relationships
In this section, we describe an application of
the speaker identification system to the extraction
of family relationships. Elson et al (2010) ex-
tract unlabeled networks where the nodes repre-
sent characters and edges indicate their proxim-
ity, as indicated by their interactions. Our goal
is to construct networks in which edges are la-
beled by the mutual relationships between charac-
ters in a novel. We focus on family relationships,
but also include social relationships, such as friend
1318
INSERT INTO Relation (id1, id2, t, m)
SELECT r.to AS id1, r.from AS id2 , ?wife? AS t, ?derived? AS m
FROM Relation r
WHERE r.type=?husband? AND r.mode=?explicit? AND
NOT EXISTS(SELECT * FROM Relation r2
WHERE r2.from=r.to AND r2.to=r.from AND r2.type=t)
Figure 4: An example inference rule.
and attracted-to.
Our approach to building a social network from
the novel is to build an active database of relation-
ships explicitly mentioned in the text, which is ex-
panded by triggering the execution of queries that
deduce implicit relations. This inference process
is repeated for every discovered relationship until
no new knowledge can be inferred.
The following example illustrates how speaker
identification helps in the extraction of social re-
lations among characters. Consider, the following
conversation:
?How so? how can it affect them??
?My dear Mr. Bennet,? replied his wife,
?how can you be so tiresome!?
If the speakers are correctly identified, the utter-
ances are attributed to Mr. Bennet and Mrs. Ben-
net, respectively. Furthermore, the second utter-
ance implies that its speaker is the wife of the pre-
ceding speaker. This is an example of an explicit
relationship which is included in our database.
Several similar extraction rules are used to extract
explicit mentions indicating family and affective
relations, including mother, nephew, and fiancee.
We can also derive relationships that are not ex-
plicitly mentioned in the text; for example, that
Mr. Bennet is the husband of Mrs. Bennet.
Figure 3 shows a snippet of the relational
database of the network extracted from Pride &
Prejudice. Table Character contains all characters
in the book, each with a unique identifier and gen-
der information, while Table Relation contains all
relationships that are explicitly mentioned in the
text or derived through reasoning.
Figure 4 shows an example of an inference rule
used in our system. The rule derives a new re-
lationship indicating that character c1 is the wife
of character c2 if it is known (through an explicit
mention in the text) that c2 is the husband of c1.
One condition for the rule to be applied is that the
database must not already contain a record indi-
cating the wife relationship. This inference rule
would derive the tuple in Figure 3 indicating that
the wife or Mr. Collins is Charlotte.
In our experiment with Pride & Prejudice, a to-
tal of 55 explicitly indicated relationships were au-
tomatically identified once the utterances were at-
tributed to the characters. From those, another 57
implicit relationships were derived through infer-
ence. A preliminary manual inspection of the set
of relations extracted by this method (Makazhanov
et al, 2012) indicates that all of them are correct,
and include about 40% all personal relations that
can be inferred by a human reader from the text of
the novel.
9 Conclusion and Future Work
We have presented a novel approach to identifying
speakers of utterances in novels. Our system in-
corporates a variety of novel features which utilize
vocatives, unsupervised actor-topic models, and
the speaker alternation pattern. The results of our
evaluation experiments indicate a substantial im-
provement over the current state of the art.
There are several interesting directions for the
future work. Although the approach introduced
in this paper appears to be sufficiently general to
handle novels written in a different style and pe-
riod, more sophisticated statistical graphical mod-
els may achieve higher accuracy on this task. A re-
liable automatic generation of characters and their
aliases would remove the need for the preprocess-
ing step outlined in Section 6. The extraction of
social networks in novels that we discussed in Sec-
tion 8 would benefit from the introduction of ad-
ditional inference rules, and could be extended to
capture more subtle notions of sentiment or rela-
tionship among characters, as well as their devel-
opment over time.
We have demonstrated that speaker identifica-
tion can help extract family relationships, but the
converse is also true. Consider the following utter-
ance:
?Lizzy,? said her father, ?I have given
him my consent.?
1319
In order to deduce the speaker of the utterance,
we need to combine the three pieces of informa-
tion: (a) the utterance is addressed to Lizzy (voca-
tive prediction), (b) the utterance is produced by
Lizzy?s father (pronoun resolution), and (c) Mr.
Bennet is the father of Lizzy (relationship ex-
traction). Similarly, in the task of compiling a
list of characters, which involves resolving aliases
such as Caroline, Caroline Bingley, and Miss Bin-
gley, simultaneous extraction of family relation-
ships would help detect the ambiguity of Miss
Benett, which can refer to any of several sis-
ters. A joint approach to resolving speaker attri-
bution, relationship extraction, co-reference reso-
lution, and alias-to-character mapping would not
only improve the accuracy on all these tasks, but
also represent a step towards deeper understanding
of complex plots and stories.
Acknowledgments
We would like to thank Asli Celikyilmaz for col-
laboration in the early stages of this project, Su-
san Brown and Michelle Di Cintio for help with
data annotation, and David Elson for the attempt
to compute the accuracy of the EM2010 system
on Pride & Prejudice. This research was partially
supported by the Natural Sciences and Engineer-
ing Research Council of Canada.
References
Alan Agresti. 2006. Building and applying logistic re-
gression models. In An Introduction to Categorical
Data Analysis. John Wiley & Sons, Inc.
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In EMNLP-CoNLL.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Auto-
matic extraction of opinion propositions and their
holders. In AAAI Spring Symposium on Exploring
Attitude and Affect in Text.
Asli Celikyilmaz, Dilek Hakkani-Tur, Hua He, Grze-
gorz Kondrak, and Denilson Barbosa. 2010. The
actor-topic model for extracting social networks in
literary narrative. In Proceedings of the NIPS 2010
Workshop - Machine Learning for Social Comput-
ing.
David K. Elson and Kathleen McKeown. 2010. Auto-
matic attribution of quoted speech in literary narra-
tive. In AAAI.
David K. Elson, Nicholas Dames, and Kathleen McKe-
own. 2010. Extracting social networks from literary
fiction. In ACL.
Sarah Favre, Alfred Dielmann, and Alessandro Vincia-
relli. 2009. Automatic role recognition in multi-
party recordings using social networks and proba-
bilistic sequential models. In ACM Multimedia.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL.
Kevin Glass and Shaun Bangay. 2007. A naive
salience-based method for speaker identification in
fiction books. In Proceedings of the 18th Annual
Symposium of the Pattern Recognition.
Thorsten Joachims. 2006. Training linear SVMs in
linear time. In KDD.
Ralf Krestel, Sabine Bergler, and Rene? Witte. 2008.
Minding the source: Automatic tagging of reported
speech in newspaper articles. In LREC.
Aibek Makazhanov, Denilson Barbosa, and Grzegorz
Kondrak. 2012. Extracting family relations from
literary fiction. Unpublished manuscript.
Marie Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, 38:39?41.
Bruno Pouliquen, Ralf Steinberger, and Clive Best.
2007. Automatic detection of quotations in multi-
lingual news. In RANLP.
Michal Rosen-Zvi, Chaitanya Chemudugunta,
Thomas L. Griffiths, Padhraic Smyth, and Mark
Steyvers. 2010. Learning author-topic models from
text corpora. ACM Trans. Inf. Syst., 28(1).
Josef Ruppenhofer, Caroline Sporleder, and Fabian
Shirokov. 2010. Speaker attribution in cabinet pro-
tocols. In LREC.
Hugues Salamin, Alessandro Vinciarelli, Khiet Truong,
and Gelareh Mohammadi. 2010. Automatic role
recognition based on conversational and prosodic
behaviour. In ACM Multimedia.
Luis Sarmento and Sergio Nunes. 2009. Automatic ex-
traction of quotes and topics from news feeds. In 4th
Doctoral Symposium on Informatics Engineering.
Ronald B. Tobias. 2012. 20 Master Plots: And How to
Build Them. Writer?s Digest Books, 3rd edition.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL-HLT.
1320
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 28?36,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Topic Classification of Blog Posts Using Distant Supervision
Stephanie D. Husby
University of Alberta
shusby@ualberta.ca
Denilson Barbosa
University of Alberta
denilson@ualberta.ca
Abstract
Classifying blog posts by topics is useful
for applications such as search and market-
ing. However, topic classification is time
consuming and error prone, especially in an
open domain such as the blogosphere. The
state-of-the-art relies on supervised meth-
ods, requiring considerable training effort,
that use the whole corpus vocabulary as fea-
tures, demanding considerable memory to
process. We show an effective alternative
whereby distant supervision is used to ob-
tain training data: we use Wikipedia arti-
cles labelled with Freebase domains. We
address the memory requirements by using
only named entities as features. We test our
classifier on a sample of blog posts, and re-
port up to 0.69 accuracy for multi-class la-
belling and 0.9 for binary classification.
1 Introduction
With the ever increasing popularity of blogging
grows the need of finding ways for better or-
ganizing the blogosphere. Besides identifying
SPAM from legitimate blogs, one promising idea
is to classify blog posts into topics such as travel,
sports, religion, and so on, which could lead to
better ways of exploring the blogosphere. Be-
sides navigation, blog classification can be useful
as a data preprocessing step before other forms
of analysis can be done: for example companies
can view the perception and reception of prod-
ucts, movies, books and more based on opinions
in blogs of different segments.
We approach the problem by using machine
learning. In particular, in the development of a
learning-based classifier, two crucial tasks are the
choice of the features and the building of train-
ing data. We adopt a novel approach when se-
lecting features: we use an off-the-shelf Named
Entity Recognition (NER) tool to identify entities
in the text. Our hypothesis is that one can de-
tect the topic of a post by focusing on the entities
discussed in the post. Previous text classification
tools use the entire vocabulary as potential fea-
tures, which is a superset of our feature set. Our
results show that despite using a smaller feature
set, our method can achieve very high accuracy.
Obtaining training data is a challenge for most
learning tools, as it often involves manual inspec-
tion of hundreds or thousands of examples. We
address this by using distant supervision, where
a separate dataset is used to obtain training data
for the classifier. The distant dataset used here
is Freebase1, which is an open online database,
along with related Wikipedia articles. The classes
in our tests are domains in Freebase, which are
defined by their curators.
Summary of Results. For our evaluation, we
use a large sample of blog posts from a pub-
lic snapshot of the blogosphere, collected around
2008. These posts are manually labeled by volun-
teers (undergraduate students in Computing Sci-
ence), and used as the ground-truth test data.
Our results indicate that training a classifier
relying on named entities using Freebase and
Wikipedia, can achieve high accuracy levels on
manually annotated data. We also identify some
potential problems related to selecting the cate-
gories to be used in the classification. Overall,
our results indicate that robust classifiers are pos-
sible using off-the-shelf tools and freely available
1http://www.freebase.com/.
28
training data.
2 Related Work
Our work is related to topic identification tech-
niques such as Latent Dirichlet Analysis (LDA),
Latent Semantic Analysis (LSA) and Latent Se-
mantic Indexing (LSI) (Steyvers and Griffiths,
2007). These techniques infer possible topic
classes, and use unsupervised learning (cluster-
ing) approaches. In contrast, our technique allows
the specification of classes (topics) of interest and
attempts to classify text within those classes only.
Next we discuss two other lines of work more
closely related to ours.
Blog classification. There have been few at-
tempts at classifying blog posts by topic. Most
previous methods focus on classification of the
authors and the sentiment of the posts.
Ikeda et al (2008) discussed the classification
of blog authors by gender and age. They use a
semi-supervised technique and look at the blogs
in groups of two or more. These groupings are
based on which are relatively similar and rela-
tively different. They assume that multiple en-
tries from the same author are more similar to
each other than to posts from other blogs, and
use this to train the classifier. The classifier they
use is support vector machines, and the bag-of-
words feature representation. Thus, they consider
all unique words in their classification. They find
their methods to be 70-95% accurate on age clas-
sification, depending on the particular age class
(i.e. the 20s vs the 30s class is more difficult to
distinguish than the 20s vs the 50s) and up to 91%
accurate on gender identification. This is quite
different than the approach presented here, as we
are examining topic classification.
Yang et al (2007) consider the sentiment (posi-
tive or negative) analysis of blog posts. Their clas-
sifier is trained at the sentence level and applied
to the entire document. They use emoticons to
first create training data and then use support vec-
tor machines and conditional random fields in the
actual classification. They use individual words
as features and find that conditional random fields
outperform support vector machines. This paper
works both with blog posts and distance learning
based on the emoticons, however this type of dis-
tant supervision is slightly different than our ap-
proach. It may also be referred to as using weakly
labeled data.
Elgersma and de Rijke (2008) classify blogs
as personal vs non-personal. The authors define
personal blogs as diary or journal, presenting per-
sonal accounts of daily life and intimate thoughts
and feelings. They use the frequency of words
more often used in personal blogs versus those
more frequently used in general blogs, pronouns,
in-links, out-links and hosts as the features for the
blogs. They then perform supervised training on
the data using a set of 152 manually labeled blogs
to train their classifier. The results show that the
decision tree method produced the highest accu-
racy at about 90% (Elgersma and de Rijke, 2008).
A work which looks at true topic classifica-
tion of blogs, as is being done here, is that of
Hashimoto and Kurohashi (2008), who use a do-
main dictionary to classify blog posts without ma-
chine learning (i.e., using a rule-based system).
They use keywords for each domain, or category
as the basis for classification. They then create
a score of a blog post based on the number of
keywords from each domain; the domain with the
highest count becomes the category for that post.
They also expand the keywords in their domain by
adding new words on the fly. This is done by tak-
ing an unknown word (one that does not currently
exist in a domain) and attempting to categorize it
using its online search results and/or Wikipedia
article. They attempt to classify the results or ar-
ticle and then, in turn, classify the word. They
find their classification method to be up to 99%
accurate. This idea can be related to the use of
Freebase as the domain dictionary in the current
problem, but will be expanded to include machine
learning techniques, which these authors avoid.
Distant supervision. Distant supervision is a
relatively new idea in the field of machine learn-
ing. The term was first introduced by Mintz et
al. (2009) in 2009 in their paper on relation ex-
traction. The idea is to use facts in Freebase to
obtain training data (i.e., provide distant supervi-
sion), based on the premise that if a pair of enti-
ties that have a relation in Freebase, it will likely
be expressed in some way in a new context. They
found their approach to be about 66-69% accu-
rate on large amounts of data. Although the goal
of their work (namely, extracting relations from
the text) was different from ours, the use of Free-
base and entities is directly related to the work
29
presented here.
Go et al (Go et al, 2009) use distant supervi-
sion to label the sentiment associated with Twitter
posts. They use tweets containing emoticons to
label the training data, as follows. If a tweet con-
tains a :) or an : ( then it is considered to have
a positive or a negative sentiment. Those tweets
with multiple emoticons were discarded. Then
emoticons themselves are removed from all data
(to avoid them being used as features), and the
labeled data is used to train the classifier. They
found their approach to be around 78-83% ac-
curate using several different machine learning
techniques (Go et al, 2009). The authors do not
discuss their feature representations in detail, but
make use of both unigrams and bigrams.
Phan et al (Phan et al, 2008) consider using
a universal data set to train a classifier for web
data similar to blogs . This idea is very similar
to the concept of distant supervision. They con-
sider Wikipedia and MEDLINE, as universal data
sets, and they use the maximum entropy as their
classifier. They apply their methods to two prob-
lems, topic clustering of web search results and
disease classification for medical abstracts; they
report accuracy levels around 80%.
3 Method
Our hypothesis is that one can predict the topic of
a blog post based on ?what? that post is about.
More precisely, we focus on the recognizable
named entities that appear in the blog post. Our
intuition is that if a blog post mentions ?Barack
Obama? and the ?White House? prominently, it
is probably a post about politics. On the other
hand, a post mentioning ?Edmonton Oilers? and
?Boston Bruins? is most likely about hockey. Nat-
urally, there will be posts mentioning entities
from different topics, say for example, a comment
about the president attending a hockey game. In
such cases, our hypothesis is that the other enti-
ties in the same post would help break the tie as
to which class the post belongs to.
Our method consists of using a classifier
trained with all topics of interest. We obtain
training data using distant supervision, as fol-
lows. The topics come from Freebase, an open,
online database compiled by volunteers. At the
time of writing, it contains approximately 22 mil-
lion objects which belong to one or more of a to-
tal of 86 domains. Each object in Freebase is a
Category Articles Distinct Entities
government 2,000 265,974
celebrities 1,605 85,491
food & drink 2,000 70,000
religion 2,000 175,948
sports 2,000 189,748
travel 2,000 125,802
other 2,000 384,139
Table 1: Topic categories chosen from Freebase do-
mains
unique person, place, thing or concept that exists
in the world. An example of an entity would be
?Barack Obama? or ?republican?. A major data
source for Freebase is Wikipedia; indeed, there
is even a one-to-one mapping between articles in
Wikipedia and the corresponding objects in Free-
base.
Discussion. Our motivation to use Freebase and
Wikipedia comes from their large size and free
availability, besides the fact these are fairly high
quality resources?given the dedication of their
contributors. It should be noted that this is a per-
fect example where distant supervision comes as
an ideal approach, in the sense that the classifica-
tion of objects into domains (i.e., topics) is done
manually, and with great care, leading to high
quality training data. Moreover, the nature of both
datasets, which allow any web user to update and
contribute to them, leads us to believe they will re-
main up-to-date, and will likely contain mentions
to recent events which the bloggers would be dis-
cussing. Thus, one should expect a high overlap
between the named entities in these resources and
the blog posts.
3.1 Classifying Blog Posts
The classification of blog posts by topic is done by
using the named entity recognition tool to extract
all named entities (features) for the blog post, and
feeding those to the topic classifier. We consider
two classification tasks:
? Multi-class: In this case, we are given a blog
post and the task is to determine which of the
7 topics (as in Table 1) it belongs to.
? Binary classification: In this case, we are
given a blog post and a specific topic (i.e.,
30
Blog (Test) Data Wikipedia (Training) Data
words/post entities/post words/article entities/article
celebrities 420 49 2,411 311
food & drink 256 28 1,782 144
government 20,176 2,363 6,013 803
other 395 50 10,930 1,245
religion 516 52 3,496 402
sports 498 73 4,716 741
travel 359 41 2,101 239
Table 2: Average word count and entity count per blog post and per Wikipedia article.
class), and the task is to determine whether
or not the post belongs in that topic.
The multi-class task is more relevant in an ex-
ploratory scenario, where the user would browse
through a collection of posts and use the classi-
fier as a means to organize such exploration. The
binary classification, on the other hand, is more
relevant in a scenario where the user has a spe-
cific need. For example, a journalist interested in
politics would rather use a classifier that filtered
out posts which are not relevant. By their nature,
the binary classification task demands higher ac-
curacy.
Features The only features that make sense to
use in our classification are those named entities
that appear both in the training data (Wikipedia)
and the test data (the blog posts). That is, we
use only those entities which exist in at least one
blog post and in at least one Wikipedia article. It
is worth mentioning that this reduces drastically
the memory needed for classification, compared
to previous methods that use the entire vocabulary
as features.
Each data point (blog or Wikipedia article) is
represented by a vector, where each column of the
vector is an entity. Two feature representations
were created:
? In-out: in this representation we record the
presence (1) or absences (0) of the named en-
tity in the data point; and
? Count: in this representation we record the
number of times the named entity appears in
the data point.
In-Out Count
10-Fold Test 10-Fold Test
NB 0.59 0.37 0.51 0.29
SVM 0.26 0.18 0.49 0.22
NBM 0.71 0.57 0.68 0.60
Table 3: Summary of Accuracy on Multi-Class Data
4 Experimental Design
We collected the training data as follows. First,
we discarded generic Freebase domains such as
Common and Metaweb System Types, which do
not correspond to meaningful topics. We also
discarded other domains which were too narrow,
comprising only a few objects. We then concen-
trated on domains for which we could find many
objects and for which we could perform a reason-
able evaluation. For the purposes of this paper,
the 7 domains shown in Table 1 were used as top-
ics. For each topic, we find all Freebase objects
and their corresponding Wikipedia articles, and
we collect the 2,000 longest articles (as those are
most likely to contain the most named entities).
The exception was the celebrities topic, for which
only 1,605 articles were used. From these articles,
we extract the named entities (i.e., the features),
thus obtaining our training data. In the end, we
used 4,000 articles for each binary classification
experiment and 13,605 for the multi-class one.
As for test data, we used the ICWSM 2009
Spinn3r Blog Dataset (Burton et al, 2009), which
was collected during the summer of 2008, coin-
ciding with the build-up for the 2008 Presidential
Elections in the US. In total, the collections has
approximately 25M blog posts in English. For
31
a b c d e f g ? classified as
0 0 0 0 0 0 50 a celebrities
0 0 0 0 0 0 50 b food & drink
0 0 15 27 0 0 8 c government
0 0 0 0 0 0 50 d other
0 0 0 0 0 0 50 e religion
0 0 0 0 0 0 50 f sports
0 0 0 0 0 0 50 g travel
Table 4: Confusion Matrix of SVM on Test Set with
In-Out Rep.
our evaluations, we relied on volunteers2 who la-
beled hundreds of blogs, chosen among the most
popular ones (this information is provided in the
dataset), until we collected 50 blogs for each cat-
egory. For the binary classifications, we used 50
blogs as positive examples and 200 blogs ran-
domly chosen from the other topics as negative
examples. For the multi-class experiment, we use
the 350 blogs corresponding to the 7 categories.
Both the blogs and the Wikipedia articles were
tagged using the Stanford Named Entity Recog-
nizer (Finkel et al, 2005), which labels the en-
tities according to these types: Time, Location,
Organization, Person, Money, Percent, Date, and
Miscellaneous. After several tests, we found
that Location, Organization, Person and Miscel-
laneous were the most useful for topic classifi-
cation, and we thus ignored the rest for the re-
sults presented here. As mentioned above, we use
only the named entities in both the training and
test data, which, in our experiments, consisted of
14,995 unique entities.
Classifiers. We performed all our tests using
the Weka suite (Hall et al, 2009), and we tested
the following classifiers. The first was the Naive
Bayes (John and Langley, 1995) (NB for short),
which has been successfully applied to text clas-
sification problems (Manning et al, 2008). It
assumes attribute independence, which makes
learning simpler when the number of attributes
is large. A variation of the NB classifier, called
Naive Bayes Multinomial (NBM) (McCallum and
Nigam, 1998), was also tested, as it was shown
to perform better for text classification tasks in
which the vocabulary is large (as in our case). Fi-
nally, we also used the LibSVM classifier (Chang
2Undergraduate students in our lab.
In-Out Count
10-Fold Test 10-Fold Test
NB 0.66 0.59 0.58 0.32
SVM 0.33 0.22 0.53 0.22
NBM 0.76 0.64 0.72 0.64
Table 5: Summary of Accuracy on Multi-Class with-
out Travel
a b c d e ? classified as
46 0 0 3 1 a celebrities
3 25 21 0 1 b government
40 2 0 3 5 c other
5 1 1 43 0 d religion
13 0 0 0 37 e sports
Table 6: Confusion Matrix of NB on Test Set with In-
Out Rep
and Lin, 2001) (SVM), which is an implementa-
tion of support vector machines, a binary linear
classifier. The results reported in this paper were
obtained with LibSVM?s default tuning parame-
ters. SVMs are often used successfully in text
classification problems (Ikeda et al, 2008; Yang
et al, 2007; Go et al, 2009). These classifiers
were chosen specifically due to their success rate
with text classification as well as with other appli-
cations of distant supervision.
5 Experimental Results
We now present our experimental results, starting
with the multi-class task, in which the goal is to
classify each post into one of 7 possible classes
(as in Figure 1).
Accuracy in the Multi-class Task We report
accuracy numbers both for 10-fold cross valida-
tion (on the training data) as well as on the manu-
ally labelled blog posts (test data). The summary
of results is given in Table 3. Accuracy as high as
60% was obtained using the NBM classifier. The
standard NB technique performed quite poorly in
this case; as expected, NBM outperformed NB by
a factor of almost two, using the count represen-
tation. Overall, the count representation produced
better results than in-out on the test data, while
losing on the cross-validation tests. Surprisingly,
SVM performed very poorly in our tests.
These results were not as high as expected, so
32
In-Out Count
10-Fold Test 10-Fold Test
NB 0.70 0.60 0.62 0.40
SVM 0.47 0.38 0.67 0.40
NBM 0.79 0.67 0.76 0.69
Table 7: Summary of Accuracy on Multi-Class sans Travel, Food
(a) (b) (c)
Figure 1: Precision and Recall for Multi-Class Results Using Count Representation. Legend: CEL (Celebrities),
FOO (food & drink), GOV (government), OTH (other), REL (religion), SPO (sport), TRA (travel).
we inspected why that was the case. What we
found was that the classifiers were strongly bi-
ased towards the travel topic: NB, for instance,
classified 211/350=60% of the samples that way,
instead of the expected 14% (50/350). In the case
of SVM, this effect was more pronounced: 88%
of the posts were classified as travel. Table 4
shows the confusion matrix for the worst results
in our tests (SVM with in-out feature representa-
tion), and fully illustrates the point.
We then repeated the tests after removing the
travel topic, resulting in an increase in accuracy
of about 5%, as shown in Table 5. However, an-
other inspection at the confusion matrices in this
case revealed that the food & drink class received
a disproportionate number of classifications.
The highest accuracy numbers we obtained for
the multi-class setting were when we further re-
moved the food & drink class (Table 7). Consis-
tent with previous results, our highest accuracy
was achieved with NBM using the count feature
representation: 69%. Table 6. gives the confusion
matrix for this task, using NB. We can see that
the posts are much better distributed now than in
the previous cases, approximating the ideal confu-
sion matrix which would have only non-zero en-
tries in the diagonal, signifying all instances were
correctly classified.
Recall in Multi-Class experiment. Accuracy
(or precision, as used in information retrieval)
measures the fraction of correct answers among
those provided by the classifier. A complemen-
tary performance metric is recall, which indicates
the fraction of correctly classified instances out of
the total instances of the class. Figure 1 shows the
breakdown of precision and recall for each class
using the NBM classifier, using the Count feature
representation for the tests with all 7 classes (a),
as well as after removing travel (b) and both travel
and food&drink (c).
As one can see, the overall accuracy by class
does change (and improves) as we remove travel
and then food&drink. However, the most signif-
icant change is for the class other. On the other
hand, both the accuracy and recall for celebrities,
religion and sports remain virtually unchanged
with the removal of these classes.
Discussion of Multi-class results. One clear
conclusion from our tests is the superiority of
NBM using Count features for this task. The mar-
gin of this superiority comes somewhat as a sur-
prise in some cases, especially when one com-
pares against SVM, but does not leave much room
33
for argument.
As expected, some classes are much easier to
handle than others. Classes such as celebrities
are expected to be hard as documents in this topic
deal with everything about the celebrities, includ-
ing their preferences in politics, sports, the food
they like and the places they travel. Looking at
Figure 1, one possible factor for the relatively
lower performance for travel and food & drink
could be that the training data in these categories
have the lowest average word count and entity
count (recall Table 2). Another category with rel-
atively less counts is celebrities, which can also
be explained by the lower document count (1,605
available articles relating to this topic in Free-
base).
Another plausible explanation is that articles
in some classes can often be classified in either
topic. Articles in the travel topic can include in-
formation about many things that can be done and
seen around the world, such as the culinary traits
of the places being discussed and the celebrities
that visited them, or the religious figures that rep-
resent them. Thus, one would expect some over-
lap among the named entities relating to these less
well-defined classes. These concepts tie easily
into the various other topic categories we have
considered and help to explain why misclassifi-
cation was higher for these cases.
We also observed that with the NBM results, in
all three variations of the multi-class experiments,
there was a fairly consistent trade-off between re-
call and precision for the celebrities class. The
erroneous classification of posts into celebrities
could be explained in a similar way to those in
food&travel. The fact that celebrities can exist in
sports, politics, and religion means that many of
the posts may fit into two or more classes and ex-
plains the errors. The best way to explore this fur-
ther would be to do multiple class labels per post
rather than just choosing a single label.
One interesting point that Figure 1 supports is
the following. Recall that the need for the class
other is mostly to test whether the classifier can
handle ?noise? (blogs which are too general to be
classified). With this in mind, the trend in Figure 1
(increasing classification performance as classes
are removed) is encouraging, as it indicates that
more focused classes (e.g., religion and sports)
can actually be separated well by a classifier us-
ing distant supervision, even in the presence of
less well-defined classes. Indeed, taken to the ex-
treme, this argument would suggest that the per-
formance in the binary classification scenario for
such classes would be the highest (which is indeed
the case as we discuss next).
5.1 Binary Classification
We now consider a different scenario, in which
the task is to perform a binary classification. The
goal is to identify posts of a specific class amongst
posts of all other classes. The percentage of cor-
rectly classified posts (i.e. test data) in this task,
based on each feature representation can be seen
in Table 8.
Overall, all classifiers performed much better
in this setting, although NBM still produced con-
sistently better results, with accuracy in the mid-
90% level for the count feature representation. It
is worth noting that SVM performed much better
for binary classifications compared to the multi-
class experiments, in some cases tying or ever so
slightly surpassing other methods.
Also, note that the classifiers do a much bet-
ter job on the more focused classes (e.g., religion,
sports), just as was the case with the multi-class
scenario. In fact, the accuracy for such classes
is near-perfect (92% for religion and 93% for
sports).
6 Conclusion
This paper makes two observations. First, our
novel approach of using a standard named entity
tagger to extract features for classification does
not compromise classification accuracy. Reduc-
ing the feature contributes to increasing the scala-
bility of topic classification, compared to the state
of the art which is to process the entire vocabu-
lary. The second observation is that distant super-
vision is effective in obtaining training data: By
using Freebase and Wikipedia to obtain training
data for standard machine learning classifiers, ac-
curacy as high as mid-90% were achieved on our
binary classification task, and around 70% for the
multi-class task.
Our tests confirmed the superiority of NBM for
text classification tasks, which had been observed
before. Moreover, our test also showed that this
superior performance is very robust across a vari-
ety of settings. Our results also show that it is im-
portant to consider topics carefully, as there can
be considerable overlap in many general classes
34
In-Out Count
Class NB NBM SVM NB NBM SVM
religion 0.63 0.90 0.80 0.43 0.92 0.81
government 0.96 0.85 0.80 0.88 0.82 0.87
sports 0.62 0.79 0.79 0.90 0.93 0.79
celebrities 0.60 0.68 0.80 0.40 0.76 0.80
average 0.71 0.81 0.79 0.65 0.86 0.82
Table 8: Accuracy of Binary Classification.
and this can cause misclassification. Obviously,
such overlap is inevitable?and indeed expecting
that a single topic can be found for each post can
be viewed as a restriction. The most straight-
forward way to overcome this is by allowing mul-
tiple class labels per sample, rather than forcing a
single classification.
Given the difficulty of the task, we believe our
results are a clear indication that distant supervi-
sion is a very promising option for topic classifi-
cation of social media content.
Future Work. One immediate avenue for fu-
ture work is understanding whether there are tech-
niques that can separate the classes with high
overlap, such as celebrities, food&drinks and
travel. However, it is very hard even for humans
to separate these classes, so it is not clear what
level of accuracy can be achieved. Another option
is to examine additional features which could im-
prove the accuracy of the classifier without dras-
tically increasing the costs. Features of the blog
posts such as link structure and post length, which
we disregarded, may improve classification.
Moreover, one could use unsupervised meth-
ods to find relations between the named entities
and exploit those, e.g., for bootstrapping. A simi-
lar idea would be to exploit dependencies among
relational terms involving entities, which could
easily be done on blogs and the Wikipedia arti-
cles. Topic selection is another area for future
work. Our selection of topics was very general
and based on Freebase domains, but a more de-
tailed study of how to select more specific top-
ics would be worthwhile. For instance, one might
want to further classify government into political
parties, or issues (e.g., environment, energy, im-
migration, etc.).
Acknowledgements
This was supported in part by NSERC?Natural
Sciences and Engineering Research Council,
and the NSERC Business Intelligence Network
(project Discerning Intelligence from Text).
References
K. Burton, A. Java, and I. Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of
the Third Annual Conference on Weblogs and Social
Media (ICWSM 2009), San Jose, CA.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines.
E. Elgersma and M. de Rijke. 2008. Personal vs non-
personal blogs. SIGIR, July.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. Proceedings of the 43nd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2005), pages 363?370.
Evgeniy Gabrilovich and Shaul Markovitch. 2006.
Overcoming the brittleness bottleneck using
wikipedia: enhancing text categorization with
encyclopedic knowledge. In proceedings of the
21st national conference on Artificial intelligence -
Volume 2, pages 1301?1306. AAAI Press.
A. Go, R. Bhayani, and L. Huang. 2009. Twitter sen-
timent classification using distant supervision. Pro-
cessing, pages 1?6.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reuteman, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11.
C. Hashimoto and S. Kurohashi. 2008. Blog catego-
rization exploiting domain dictionary and dynam-
ically estimated domains of unknown words. Pro-
ceedings of ACL-08, HLT Short Papers (Companion
Volume), pages 69?72, June.
D. Ikeda, H. Takamura, and M. Okumura. 2008.
Semi-supervised learning for blog classification.
35
Association for the Advancement of Artificial Intel-
ligence.
George H. John and Pat Langley. 1995. Estimating
continuous distributions in bayesian classifiers. In
Proceedings of the Eleventh Conference on Uncer-
tainty in Artificial Intelligence, pages 338?345.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Informa-
tion Retrieval. Cambridge University Press.
A. McCallum and K. Nigam. 1998. A comparison
of event models for naive bayes text classification.
In AAAI-98 Workshop on Learning for Text Catego-
rization.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. ACL-IJCNLP ?09: Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
2:1003?1011.
X. Phan, L. Nguyen, and S. Horiguchi. 2008. Learn-
ing to classify short and sparse text & web with hid-
den topics from large-scale data collections. Inter-
national World Wide Web Conference Committee,
April.
M. Steyvers and T. Griffiths, 2007. Latent Semantic
Analysis: A Road to Meaning, chapter Probabilistic
topic models. Laurence Erlbaum.
C. Yang, K. Lin, and H. Chen. 2007. Emotion classi-
fication using web blog corpora.
36
