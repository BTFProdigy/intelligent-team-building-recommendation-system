Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1081?1091,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Harnessing WordNet Senses for Supervised Sentiment Classification
Balamurali A R1,2 Aditya Joshi2 Pushpak Bhattacharyya2
1 IITB-Monash Research Academy, IIT Bombay
2Dept. of Computer Science and Engineering, IIT Bombay
Mumbai, India - 400076
{balamurali,adityaj,pb}@cse.iitb.ac.in
Abstract
Traditional approaches to sentiment classifica-
tion rely on lexical features, syntax-based fea-
tures or a combination of the two. We pro-
pose semantic features using word senses for
a supervised document-level sentiment classi-
fier. To highlight the benefit of sense-based
features, we compare word-based representa-
tion of documents with a sense-based repre-
sentation where WordNet senses of the words
are used as features. In addition, we highlight
the benefit of senses by presenting a part-of-
speech-wise effect on sentiment classification.
Finally, we show that even if a WSD engine
disambiguates between a limited set of words
in a document, a sentiment classifier still per-
forms better than what it does in absence of
sense annotation. Since word senses used as
features show promise, we also examine the
possibility of using similarity metrics defined
on WordNet to address the problem of not
finding a sense in the training corpus. We per-
form experiments using three popular similar-
ity metrics to mitigate the effect of unknown
synsets in a test corpus by replacing them with
similar synsets from the training corpus. The
results show promising improvement with re-
spect to the baseline.
1 Introduction
Sentiment Analysis (SA) is the task of prediction of
opinion in text. Sentiment classification deals with
tagging text as positive, negative or neutral from the
perspective of the speaker/writer with respect to a
topic. In this work, we follow the definition of Pang
et al (2002) & Turney (2002) and consider a binary
classification task for output labels as positive and
negative.
Traditional supervised approaches for SA have
explored lexeme and syntax-level units as features.
Approaches using lexeme-based features use bag-
of-words (Pang and Lee, 2008) or identify the
roles of different parts-of-speech (POS) like adjec-
tives (Pang et al, 2002; Whitelaw et al, 2005).
Approaches using syntax-based features construct
parse trees (Matsumoto et al, 2005) or use text
parsers to model valence shifters (Kennedy and
Inkpen, 2006).
Our work explores incorporation of semantics
in a supervised sentiment classifier. We use the
synsets in Wordnet as the feature space to represent
word senses. Thus, a document consisting of
words gets mapped to a document consisting of
corresponding word senses. Harnessing WordNet
senses as features helps us address two issues:
1. Impact of WordNet sense-based features on the
performance of supervised SA
2. Use of WordNet similarity metrics to solve the
problem of features unseen in the training cor-
pus
The first points deals with evaluating sense-based
features against word-based features. The second is-
sue that we address is in fact an opportunity to im-
prove the performance of SA that opens up because
of the choice of sense space. Since sense-based
features prove to generate superior sentiment clas-
sifiers, we get an opportunity to mitigate unknown
1081
synsets in the test corpus by replacing them with
known synsets in the training corpus. Note that such
replacement is not possible if word-based represen-
tation were used as it is not feasible to make such a
large number of similarity comparisons.
We use the corpus by Ye et al (2009) that con-
sists of travel domain reviews marked as positive or
negative at the document level. Our experiments on
studying the impact of Wordnet sense-based features
deal with variants of this corpus manually or auto-
matically annotated with senses. Besides showing
the overall impact, we perform a POS-wise analysis
of the benefit to SA. In addition, we compare the ef-
fect of varying training samples on a sentiment clas-
sifier developed using word based features and sense
based features. Through empirical evidence, we also
show that disambiguating some words in a docu-
ment also provides a better accuracy as compared
to not disambiguating any words. These four sets of
experiments highlight our hypothesis that WordNet
senses are better features as compared to words.
Wordnet sense-based space allows us to mitigate
unknown features in the test corpus. Our synset re-
placement algorithm uses Wordnet similarity-based
metrics which replace an unknown synset in the test
corpus with the closest approximation in the training
corpus. Our results show that such a replacement
benefits the performance of SA.
The roadmap for the rest of the paper is as fol-
lows: Existing related work in SA and the differ-
entiating aspects of our work are explained in sec-
tion 2 Section 3 describes the sense-based features
that we use for this work. We explain the similarity-
based replacement technique using WordNet synsets
in section 4. Our experiments have been described
in section 5. In section 6, we present our results
and related discussions. Section 7 analyzes some of
the causes for erroneous classification. Finally, sec-
tion 8 concludes the paper and points to future work.
2 Related Work
This work studies the benefit of a word sense-based
feature space to supervised sentiment classification.
However, a word sense-based feature space is feasi-
ble subject to verification of the hypothesis that sen-
timent and word senses are related. Towards this,
Wiebe and Mihalcea (2006) conduct a study on hu-
man annotation of 354 words senses with polarity
and report a high inter-annotator agreement. The
work in sentiment analysis using sense-based fea-
tures, including ours, assumes this hypothesis that
sense decides the sentiment.
The novelty of our work lies in the following.
Firstly our approach is distinctly. Akkaya et al
(2009) and Martn-Wanton et al (2010) report per-
formance of rule-based sentiment classification us-
ing word senses. Instead of a rule-based implemen-
tation, We used supervised learning. The supervised
nature of our approach renders lexical resources un-
necessary as used in Martn-Wanton et al (2010).
Rentoumi et al (2009) suggest using word senses
to detect sentence level polarity of news headlines.
The authors use graph similarity to detect polarity of
senses. To predict sentence level polarity, a HMM
is trained on word sense and POS as the observa-
tion. The authors report that word senses partic-
ularly help understanding metaphors in these sen-
tences. Our work differs in terms of the corpus and
document sizes in addition to generating a general
purpose classifier.
Another supervised approach of creating an emo-
tional intensity classifier using concepts as features
has been reported by Carrillo de Albornoz et al
(2010). This work is different based on the feature
space used. The concepts used for the purpose are
limited to affective classes. This restricts the size of
the feature space to a limited set of labels. As op-
posed to this, we construct feature vectors that map
to a larger sense-based space. In order to do so, we
use synset offsets as representation of sense-based
features.
Akkaya et al (2009), Martn-Wanton et al (2010)
and Carrillo de Albornoz et al (2010) perform sen-
timent classification of individual sentences. How-
ever, we consider a document as a unit of sentiment
classification i.e. our goal is to predict a document
on the whole as positive or negative. This is different
from Pang and Lee (2004) which suggests that sen-
timent is associated only with subjective content. A
document in its entirety is represented using sense-
based features in our experiments. Carrillo de Al-
bornoz et al (2010) suggests expansion using Word-
Net relations which we also follow. This is a benefit
that can be achieved only in a sense-based space.
1082
3 Features based on WordNet Senses
In their original form, documents are said to be in
lexical space since they consist of words. When the
words are replaced by their corresponding senses,
the resultant document is said to be in semantic
space.
WordNet 2.1 (Fellbaum, 1998) has been used as
the sense repository. Each word/lexeme is mapped
to an appropriate synset in WordNet based on
its sense and represented using the corresponding
synset id of WordNet. Thus, the word love is dis-
ambiguated and replaced by the identifier 21758160
which consists of a POS category identifier 2 fol-
lowed by synset offset identifier 1758160. This pa-
per refers to synset offset as synset identifiers or sim-
ply, senses.
This section first gives the motivation for using
word senses and then, describes the approaches that
we use for our experiments.
3.1 Motivation
Consider the following sentences as the first sce-
nario.
1. ?Her face fell when she heard that she had
been fired.?
2. ?The fruit fell from the tree.?
The word ?fell? occurs in different senses in the
two sentences. In the first sentence, ?fell? has the
meaning of ?assume a disappointed or sad expres-
sion, whereas in the second sentence, it has the
meaning of ?descend in free fall under the influence
of gravity?. A user will infer the negative polarity of
the first sentence from the negative sense of ?fell? in
it while the user will state that the second sentence
does not carry any sentiment. This implies that there
is at least one sense of the word ?fell? that carries
sentiment and at least one that does not.
In the second scenario, consider the following ex-
amples.
1. ?The snake bite proved to be deadly for the
young boy.?
2. ?Shane Warne is a deadly spinner.?
The word deadly has senses which carry opposite
polarity in the two sentences and these senses as-
sign the polarity to the corresponding sentence. The
first sentence is negative while the second sentence
is positive.
Finally in the third scenario, consider the follow-
ing pair of sentences.
1. ?He speaks a vulgar language.?
2. ?Now that?s real crude behavior!?
The words vulgar and crude occur as synonyms
in the synset that corresponds to the sense ?conspic-
uously and tastelessly indecent?. The synonymous
nature of words can be identified only if they are
looked at as senses and not just words.
As one may observe, the first scenario shows that
a word may have some sentiment-bearing and some
non-sentiment-bearing senses. In the second sce-
nario, we show that there may be different senses
of a word that bear sentiments of opposite polarity.
Finally, in the third scenario, we show how a sense
can be manifested using different words, i.e., words
in a synset. The three scenarios motivate the use of
semantic space for sentiment prediction.
3.2 Sense versus Lexeme-based Feature
Representation
We annotate the words in the corpus with their
senses using two sense disambiguation approaches.
As the first approach, manual sense annotation
of documents is carried out by two annotators on two
subsets of the corpus, the details of which are given
in Section 5.1. This is done to determine the ideal
case scenario- the skyline performance.
As the second approach, a state-of-art algorithm
for domain-specific WSD proposed by Khapra et
al. (2010) is used to obtain an automatically sense-
tagged corpus. This algorithm called iterative WSD
or IWSD iteratively disambiguates words by rank-
ing the candidate senses based on a scoring function.
The two types of sense-annotated corpus lead us
to four feature representations for a document:
1. Word senses that have been manually annotated
(M)
2. Word senses that have been annotated by an au-
tomatic WSD (I)
1083
3. Manually annotated word senses and words
(both separately as features) (Words +
Sense(M))
4. Automatically annotated word senses and
words (both separately as features) (Words +
Sense(I))
Our first set of experiments compares the four
feature representations to find the feature represen-
tation with which sentiment classification gives the
best performance. W+S(M) and W+S(I) are used to
overcome non-coverage of WordNet for some noun
synsets. In addition to this, we also present a part-
of-speech-wise analysis of benefit to SA as well as
effect of varying the training samples on sentiment
classification accuracy.
3.3 Partial disambiguation as opposed to no
disambiguation
The state-of-the-art automatic WSD engine that we
use performs (approximately) with 70% accuracy on
tourism domain (Khapra et al, 2010). This means
that the performance of SA depends on the perfor-
mance of WSD which is not very high in case of the
engine we use.
A partially disambiguated document is a docu-
ment which does not contain senses of all words.
Our hypothesis is that disambiguation of even few
words in a document can give better results than
no disambiguation. To verify this, we create differ-
ent variants of the corpus by disambiguating words
which have candidate senses within a threshold. For
example, a partially disambiguated variant of the
corpus with threshold 3 for candidate senses is cre-
ated by disambiguating words which have a maxi-
mum of three candidate senses. These synsets are
then used as features for classification along with
lexeme based features. We conduct multiple experi-
ments using this approach by varying the number of
candidate senses.
4 Advantage of senses: Similarity Metrics
and Unknown Synsets
4.1 Synset Replacement Algorithm
Using WordNet senses provides an opportunity to
use similarity-based metrics for WordNet to reduce
the effect of unknown features. If a synset encoun-
tered in a test document is not found in the training
corpus, it is replaced by one of the synsets present
in the training corpus. The substitute synset is deter-
mined on the basis of its similarity with the synset
in the test document. The synset that is replaced is
referred to as an unseen synset as it is not known to
the trained model.
For example, consider excerpts of two reviews,
the first of which occurs in the training corpus while
the second occurs in the test corpus.
1. ? In the night, it is a lovely city and... ?
2. ? The city has many beautiful hot spots for hon-
eymooners. ?
The synset of ?beautiful? is not present in the train-
ing corpus. We evaluate a similarity metric for all
synsets in the training corpus with respect to the
sense of beautiful and find that the sense of lovely is
closest to it. Hence, the sense of beautiful in the test
document is replaced by the sense of lovely which is
present in the training corpus.
The replacement algorithm is described in Algo-
rithm 1. The algorithm follows from the fact that the
similarity value for a synset with itself is maximum.
4.2 Similarity metrics used
We conduct different runs of the replacement
algorithm using three similarity metrics, namely
LIN?s similarity metric, Lesk similarity metric and
Leacock and Chodorow (LCH) similarity metric.
These runs generate three variants of the corpus.
We compare the benefit of each of these metrics by
studying their sentiment classification performance.
The metrics can be described as follows:
LIN: The metric by Lin (1998) uses the infor-
mation content individually possessed by two con-
cepts in addition to that shared by them. The infor-
mation content shared by two concepts A and B is
given by their most specific subsumer (lowest super-
ordinate(lso). Thus, this metric defines the similarity
between two concepts as
simLIN (A,B) =
2? logPr(lso(A,B))
logPr(A) + logPr(B) (1)
1084
Input: Training Corpus, Test Corpus,
Similarity Metric
Output: New Test Corpus
T:= Training Corpus;
X:= Test Corpus;
S:= Similarity metric;
train concept list = get list concept(T) ;
test concept list = get list concept(X);
for each concept C in test concept list do
temp max similarity = 0 ;
temp concept = C ;
for each concept D in train concept list do
similarity value = get similarity value(C,D,S);
if (similarity value > temp max similarity) then
temp max similarity= similarity value;
temp concept = D ;
end
end
C = temp concept ;
replace synset corpus(C,X);
end
Return X ;
Algorithm 1: Synset replacement using similarity
metric
Lesk: Each concept in WordNet is defined
through gloss. To compute the Lesk similar-
ity (Banerjee and Pedersen, 2002) between A and
B, a scoring function based on the overlap of words
in their individual glosses is used.
Leacock and Chodorow (LCH): To measure
similarity between two concepts A and B, Leacock
and Chodorow (1998) compute the shortest path
through hypernymy relation between them under the
constraint that there exists such a path. The final
value is computed by scaling the path length by the
overall taxonomy depth (D).
simLCH(A,B) = ? log
( len(A,B)
2D
)
(2)
5 Experimentation
We describe the variants of the corpus generated and
the experiments in this section.
5.1 Data Preparation
We create different variants of the dataset by Ye et
al. (2009). This dataset contains 600 positive and
591 negative reviews about seven travel destinations.
Each review contains approximately 4-5 sentences
with an average number of words per review being
80-85.
To create the manually annotated corpus, two hu-
man annotators annotate words in the corpus with
senses for two disjoint subsets of the original cor-
pus by Ye et al (2009). The inter-annotation agree-
ment for a subset of the corpus showed 91% sense
overlap. The manually annotated corpus consists of
34508 words with 6004 synsets.
POS #Words P(%) R(%) F-Score(%)
Noun 12693 75.54 75.12 75.33
Adverb 4114 71.16 70.90 71.03
Adjective 6194 67.26 66.31 66.78
Verb 11507 68.28 67.97 68.12
Overall 34508 71.12 70.65 70.88
Table 1: Annotation Statistics for IWSD; P- Precision,R-
Recall
The second variant of the corpus contains word
senses obtained from automatic disambiguation us-
ing IWSD. The evaluation statistics of the IWSD is
shown in Table 1. Table 1 shows that the F-score for
noun synsets is high while that for adjective synsets
is the lowest among all. The low recall for adjective
POS based synsets can be detrimental to classifica-
tion since adjectives are known to express direct sen-
timent (Pang et al, 2002). Hence, in the context of
sentiment classification, disambiguation of adjective
synsets is more critical as compared to disambigua-
tion of noun synsets.
5.2 Experimental setup
The experiments are performed using C-SVM (lin-
ear kernel with default parameters1) available as a
part of LibSVM2 package. We choose to use SVM
since it performs the best for sentiment classification
(Pang et al, 2002). All results reported are average
of five-fold cross-validation accuracies.
To conduct experiments on words as features, we
first perform stop-word removal. The words are not
stemmed since stemming is known to be detrimen-
tal to sentiment classification (Leopold and Kinder-
mann, 2002). To conduct the experiments based on
1C=0.0,=0.0010
2http://www.csie.ntu.edu.tw/ cjlin/libsvm
1085
Feature Representations Accuracy(%) PF NF PP NP PR NR
Words (Baseline) 84.90 85.07 84.76 84.95 84.92 85.19 84.60
Sense (M) 89.10 88.22 89.11 91.50 87.07 85.18 91.24
Words + Sense (M) 90.20 89.81 90.43 92.02 88.55 87.71 92.39
Sense (I) 85.48 85.31 85.65 87.17 83.93 83.53 87.46
Words + Sense (I) 86.08 86.28 85.92 85.87 86.38 86.69 85.46
Table 2: Classification Results; PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP-
Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%)
the synset representation, words in the corpus are an-
notated with synset identifiers along with POS cat-
egory identifiers. For automatic sense disambigua-
tion, we used the trained IWSD engine from Khapra
et al (2010). These synset identifiers along with
POS category identifiers are then used as features.
For replacement using semantic similarity measures,
we used WordNet::Similarity 2.05 package by Ped-
ersen et al (2004).
To evaluate the result, we use accuracy, F-score,
recall and precision as the metrics. Classification
accuracy defines the ratio of the number of true in-
stances to the total number of instances. Recall is
calculated as a ratio of the true instances found to
the total number of false positives and true posi-
tives. Precision is defined as the number of true
instances divided by number of true positives and
false negatives. Positive Precision (PP) and Posi-
tive Recall (PR) are precision and recall for positive
documents while Negative Precision (NP) and Nega-
tive Recall (NR) are precision and recall for negative
documents. F-score is the weighted precision-recall
score.
6 Results and Discussions
6.1 Comparison of various feature
representations
Table 2 shows results of classification for different
feature representations. The baseline for our results
is the unigram bag-of-words model (Baseline).
An improvement of 4.2% is observed in the ac-
curacy of sentiment prediction when manually an-
notated sense-based features (M) are used in place
of word-based features (Words). The precision of
both the classes using features based on semantic
space is also better than one based on lexeme space.
While reported results suggest that it is more diffi-
cult to detect negative sentiment than positive senti-
ment (Gindl and Liegl, 2008), our results show that
negative recall increases by around 8% in case of
sense-based representation of documents.
The combined model of words and manually an-
notated senses (Words + Senses (M)) gives the best
performance with an accuracy of 90.2%. This leads
to an improvement of 5.3% over the baseline accu-
racy 3.
One of the reasons for improved performance is
the feature abstraction achieved due to the synset-
based features. The dimension of feature vector is
reduced by a factor of 82% when the document is
represented in synset space. The reduction in dimen-
sionality may also lead to reduction in noise (Cun-
ningham, 2008).
A comparison of accuracy of different sense rep-
resentations in Table 2 shows that manual disam-
biguation performs better than using automatic al-
gorithms like IWSD. Although overall classification
accuracy improvement of IWSD over baseline is
marginal, negative recall also improves. This bene-
fit is despite the fact that evaluation of IWSD engine
over manually annotated corpus gave an overall F-
score of 71% (refer Table 1). For a WSD engine
with a better accuracy, the performance of sense-
based SA can be boosted further.
Thus, in terms of feature representation of docu-
ments, sense-based features provide a better overall
performance as compared to word-based features.
1086
Sens
e
81.2
4
78.3
0
66.1
4
73 .
70.0
0
80.0
0
90.0
0
50.0
0
60.0
0
racy(%)
20.0
0
30.0
0
40.0
0
Accu
0.0010.0
0
20.0
0
Adve
rb?
Verb
? PO
Wor
ds 74.99
66.8
3
.78
71.8
1
80.0
3
Nou
n?
Adje
ctive
OS?c
ateg
ory
Figure 1: POS-wise statistics of manually annotated se-
mantic space
6.2 POS-wise analysis
For each POS, we compare the performance of two
models:
? Model trained on words of only that POS
? Model trained on word senses of only that POS
Figure 1 shows the parts-of-speech-wise classifica-
tion accuracy of sentiment classification for senses
(manual) and words. In the lexeme space, adjectives
directly impact the classification performance. But it
can be seen that disambiguation of adverb and verb
synsets impact the performance of SA higher than
disambiguation of nouns and adjectives.
While it is believed that adjectives carry direct
sentiments, our results suggest that using adjectives
alone as features may not improve the accuracy. The
results prove that sentiment may be subtle at times
and not expressed directly through adjectives.
As manual sense annotation is an effort and cost
intensive process, the parts-of-speech-wise results
suggest improvements expected from an automatic
WSD engine so that it can aid sentiment classifica-
tion. Table 1 suggests that the WSD engine works
better for noun synsets compared to adjective and
adverb synsets. While this is expected in a typical
WSD setup, it is the adverbs and verbs that are more
important for detecting sentiment in semantics space
3The improvement in results of semantic space is found to
be statistically significant over the baseline at 95% confidence
level when tested using a paired t-test.
than nouns. The future WSD systems will have to
show an improvement in their accuracy with respect
to adverb and verb synsets.
Sense Words
POS Category PF NF PF NF
Adverb 79.65 80.45 70.25 73.68
Verb 75.50 79.28 62.23 63.12
Noun 73.39 75.40 69.77 72.55
Adjective 63.11 65.03 78.29 79.20
Table 3: POS-wise F-score for sense (M) and Words;PF-
Positive F-score(%), NF- Negative F-score (%)
Table 3 shows the positive and negative F-score
statistics with respect to different POS. Detection
of negative reviews using lexeme space is difficult.
POS-wise statistics also suggest the same. It should
be noted that adverb and verb synsets play an im-
portant role in negative class detection. Thus, an au-
tomatic WSD engine should give importance to the
correct disambiguation of these POS categories.
6.3 Effect of size of training corpus
#Training
Documents
W M I W+S(M) W+S(I)
100 76.5 87 79.5 82.5 79.5
200 81.5 88.5 82 90 84
300 79.5 92 81 89.5 82
400 82 90.5 81 94 85.5
500 83.5 91 85 96 82.5
Table 4: Accuracy (%) with respect to number of training
documents; W: Words, M: Manual Annotation, I: IWSD-
based sense annotation, W+S(M): Word+Senses (Manual
annotation), W+S(I): Word+Senses(IWSD-based sense
annotation)
From table 2, the benefit of sense disambigua-
tion to sentiment prediction is evident. In addition,
Table 4 shows variation of classification accuracy
with respect to different number of training sam-
ples based on different approaches of annotation ex-
plained in previous sections. The results are based
on a blind set of 90 test samples from both the po-
larity labels 4.
4No cross validation is performed for this experiment
1087
Compared to lexeme-based features, manually an-
notated sense based features give better performance
with lower number of training samples. IWSD is
also better than lexeme-based features. A SA sys-
tem trained on 100 training samples using manually
annotated senses gives an accuracy of 87%. Word-
based features never achieve this accuracy. An
IWSD-based system requires lesser samples when
compared to lexeme space for an equivalent accu-
racy. Note that model based on words + senses(M)
features achieve an accuracy of 96% on this test set.
This implies that the synset space, in addition
to benefit to sentiment prediction in general, re-
quires lesser number of training samples in order to
achieve the accuracy that lexeme space can achieve
with a larger number of samples.
6.4 Effect of Partial disambiguation
Figure 2 shows the accuracy, positive F-score and
negative F-score with respect to different thresholds
of candidate senses for partially disambiguated doc-
uments as described in Section 3.3. We compare the
performance of these documents with word-based
features (B) and sense-based features based on man-
ually (M) or automatically obtained senses (I). Note
that Sense (I) and Sense (M) correspond to com-
pletely disambiguated documents.
In case of partial disambiguation using manual
annotation, disambiguating words with less than
three candidate senses performs better than others.
For partial disambiguation that relies on an auto-
matic WSD engine, a comparable performance to
full disambiguation can be obtained by disambiguat-
ing words which have a maximum of four candidate
senses.
As expected, completely disambiguated docu-
ments provide the best F-score and accuracy fig-
ures5. However, a performance comparable to com-
plete disambiguation can be attained by disam-
biguating selective words.
Our results show that even if highly ambiguous
(in terms of senses) words are not disambiguated by
a WSD engine, the performance of sentiment classi-
fication improves.
5All results are statistically significant with respect to base-
line
6se
nse
s(M
)
6?se
nse
s?(I
)Ne
gat
ive
?Fsc
ore
Pos
5?se
nse
s?(M
)
5?se
nse
s?(I
)
6?se
nse
s?(M
)
3?se
nse
s?(I
)
4?se
nse
s?(M
)
4?se
nse
s?(I
)
2?se
nse
s?(M
)
2?se
nse
s?(I
)
3?se
nse
s?(M
)
Wo
rds
?(B)
Sen
se(
M)
Sen
se(
I) 81
.00
82.
00
83.
00
84.
00
85.
00
Fsc
ore
/itiv
e?F
sco
re
Acc
ura
cy
86.
00
87.
00
88.
00
89.
00
90.
00
91.
00
/Ac
cur
acy
?(%
)
Figure 2: Partial disambiguation statistics: Accu-
racy,Positive F-score, Negative F-score variation with re-
spect to sense disambiguation difficult level is shown.
Words(B): baseline system
6.5 Synset replacement using similarity metrics
Table 5 shows the results of synset replacement ex-
periments performed using similarity metrics de-
fined in section 4. The similarity metric value NA
shown in the table indicates that synset replacement
is not performed for the specific run of experiment.
For this set of experiments, we use the combina-
tion of sense and words as features (indicated by
Senses+Words (M)).
Synset replacement using a similarity metric
shows an improvement over using words alone.
However, the improvement in classification accu-
racy is marginal compared to sense-based represen-
tation without synset replacement (Similarity Met-
ric=NA).
Replacement using LIN and LCH metrics gives
marginally better results compared to the vanilla set-
ting in a manually annotated corpus. The same phe-
nomenon is seen in the case of IWSD based ap-
proach6. The limited improvement can be due to
the fact that since LCH and LIN consider only IS-A
6Results based on LCH and LIN similarity metric for auto-
matic sense disambiguation is not statistically significant with
?=0.05
1088
Feature Representation Similarity
Metric
Accuracy PF NF PP NP PR NR
Words (Baseline) NA 84.90 85.07 84.76 84.95 84.92 85.19 84.60
Words + Sense(M) NA 90.20 89.81 90.43 92.02 88.55 87.71 92.39
Words + Sense(I) NA 86.08 86.28 85.92 85.87 86.38 86.69 85.46
Words + Sense (M) LCH 90.60 90.20 90.85 92.85 88.61 87.70 93.21
Words + Sense(M) LIN 90.70 90.26 90.97 93.17 88.50 87.53 93.57
Words + Sense (M) Lesk 91.12 90.70 91.38 93.55 88.97 88.03 93.92
Words + Sense (I) LCH 85.66 85.85 85.52 85.67 85.76 86.02 85.28
Words + Sense(I) LIN 86.16 86.37 86.00 86.06 86.40 86.69 85.61
Words + Sense (I) Lesk 86.25 86.41 86.10 86.31 86.26 86.52 85.95
Table 5: Similarity Metric Analysis using different similarity metrics with synsets and a combinations of synset and
words;PF-Positive F-score(%), NF-Negative F-score (%), PP-Positive Precision (%), NP-Negative Precision (%), PR-
Positive Recall (%), NR-Negative Recall (%)
Top information
content features
(in %)
IWSD
synset #
Manual
synsets #
Match
synset #
Match
Synsets (%)
Unmatched
Synset(%)
10 601 722 288 39.89 60.11
20 1199 1443 650 45.05 54.95
30 1795 2165 1005 46.42 53.58
40 2396 2889 1375 47.59 52.41
50 2997 3613 1730 47.88 52.12
Table 6: Comparison of top information gain-based features of manually annotated corpora and automatically anno-
tated corpora
relationship in WordNet, the replacement happens
only for verbs and nouns. This excludes adverb
synsets which we have shown to be the best features
for a sense-based SA system.
Among all similarity metrics, the best classifica-
tion accuracy is achieved using Lesk. The system
performs with an overall classification accuracy of
91.12%, which is a substantial improvement of 6.2%
over baseline. Again, it is only 1% over the vanilla
setting that uses combination of synset and words.
However, the similarity metric is not sophisticated
as LIN or LCH.
Thus, we observe a marginal improvement by us-
ing similarity-based metrics for WordNet. A good
metric which covers all POS categories can provide
substantial improvement in the classification accu-
racy.
7 Error Analysis
For sentiment classification based on semantic
space, we classify the errors into four categories.
The examples quoted are from manual evaluation of
the results.
1. Effect of low disambiguation accuracy of IWSD
engine: SA using automatic sense annotation
depends on the annotation system used. To as-
sess the impact of IWSD system on sentiment
classification, we compare the feature set based
on manually annotated senses with the feature
set based on automatically annotated senses.
We compare the most informative features of
the two classifiers. Table 6 shows the number
of top informative features (synset) selected as
the percentage of total synset features present
when the semantic representation of documen-
tation is used. The matched synset column rep-
resents the number of IWSD synsets that match
1089
with manually annotated synsets.
The number of top performing features is more
in case of manually annotated synsets. This
can be attributed to the total number of synsets
tagged in the two variant of the corpus. The re-
duction in the performance of SA for automati-
cally annotated senses is because of the number
of unmatched synsets.
Thus, although the accuracy of IWSD is cur-
rently 70%, the table indicates that IWSD can
match the performance of manually annotated
senses for SA if IWSD is able to tag correctly
those top information content synsets. This as-
pect needs to be investigated further.
2. Negation Handling: For the purpose of this
work, we concentrate on words as units for sen-
timent determination. Syntax and its contri-
bution in understanding sentiment is neglected
and hence, positive documents which con-
tain negations are wrongly classified as nega-
tive. Negation may be direct as in the excerpt
?....what is there not to like about Vegas.? or
may be double as in the excerpt?...that aren?t
insecure?.
3. Interjections and WordNet coverage: Recent
informal words are not covered in WordNet and
hence, do not get disambiguated. The same
is the case for interjections like ?wow?,?duh?
which sometimes carry direct sentiment. Lex-
ical resources which include them can be used
to incorporate information about these lexical
units.
4. Document Specificity: The assumption under-
lying our analysis is that a document contains
description of only one topic. However, re-
views are generic in nature and tend to express
contrasting sentiment about sub-topics . For
example, a travel review about Paris can talk
about restaurants in Paris, traffic in Paris, pub-
lic behaviour, etc. with opposing sentiments.
Assigning an overall sentiment to a document
is subjective in such cases.
8 Conclusion & Future Work
This work presents an empirical benefit of WSD to
sentiment analysis. The study shows that supervised
sentiment classifier modeled on wordNet senses per-
form better than word-based features. We show how
the performance impact differs for different auto-
matic and manual techniques, parts-of-speech, dif-
ferent training sample size and different levels of
disambiguation. In addition, we also show the bene-
fit of using WordNet based similarity metrics for re-
placing unknown features in the test set. Our results
support the fact that not only does sense space im-
prove the performance of a sentiment classification
system, but also opens opportunities for improve-
ment using better similarity metrics.
Incorporation of syntactical information along
with semantics can be an interesting area of work.
More sophisticated features which include the two
need to be explored. Another line of work is in the
context of cross-lingual sentiment analysis. Current
solutions are based on machine translation which is
very resource-intensive. Using a bi-lingual dictio-
nary which maps WordNet across languages, such a
machine translation sub-system can be avoided.
Acknowledgment
We thank Jaya Saraswati and Rajita Shukla from
CFILT Lab, IIT Bombay for annotating the dataset
used for this work. We also thank Mitesh Khapra
and Salil Joshi, IIT Bombay for providing us with
the IWSD engine for the required experiments.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Proc. of
EMNLP ?09, pages 190?199, Singapore.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
lesk algorithm for word sense disambiguation using
wordnet. In Proc. of CICLing?02, pages 136?145,
London, UK.
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gervs. 2010. Improving emotional intensity clas-
sification using word sense disambiguation. Special
issue: Natural Language Processing and its Appli-
cations. Journal on Research in Computing Science,
46:131?142.
1090
Pdraig Cunningham. 2008. Dimension reduction. In
Machine Learning Techniques for Multimedia, Cogni-
tive Technologies, pages 91?112.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Stefan Gindl and Johannes Liegl, 2008. Evaluation of
different sentiment detection methods for polarity clas-
sification on web-based reviews, pages 35?43.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational Intelligence, 22(2):110?
125.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense
disambiguation combining corpus basedand wordnet
based parameters. In Proc. of GWC?10, Mumbai, In-
dia.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context with wordnet similarity for word
sense identification. In WordNet: A Lexical Reference
System and its Application.
Edda Leopold and Jo?rg Kindermann. 2002. Text catego-
rization with support vector machines. how to repre-
sent texts in input space? Machine Learning, 46:423?
444.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In In Proc. of the 15th International Con-
ference on Machine Learning, pages 296?304.
Tamara Martn-Wanton, Alexandra Balahur-Dobrescu,
Andres Montoyo-Guijarro, and Aurora Pons-Porrata.
2010. Word sense disambiguation in opinion mining:
Pros and cons. In Proc. of CICLing?10, Madrid,Spain.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. In Proc.
of PAKDD?05,, Lecture Notes in Computer Science,
pages 301?311.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of ACL?04, pages
271?278, Barcelona, Spain.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2:1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. volume 10, pages 79?86.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the relat-
edness of concepts. In Demonstration Papers at HLT-
NAACL?04, pages 38?41.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Sen-
timent analysis of figurative language using a word
sense disambiguation approach. In Proc. of the In-
ternational Conference RANLP?09, pages 370?375,
Borovets, Bulgaria.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL?02, pages 417?424,
Philadelphia, US.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proc. of CIKM ?05, pages 625?631, New York, NY,
USA.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proc. of COLING-ACL?06, pages
1065?1072.
Qiang Ye, Ziqiong Zhang, and Rob Law. 2009. Senti-
ment classification of online reviews to travel destina-
tions by supervised machine learning approaches. Ex-
pert Systems with Applications, 36(3):6527 ? 6535.
1091
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 127?132,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
C-Feel-It: A Sentiment Analyzer for Micro-blogs
Aditya Joshi1 Balamurali A R2 Pushpak Bhattacharyya1 Rajat Mohanty 3
1Dept. of Computer Science and Engineering, IIT Bombay, Mumbai
2 IITB-Monash Research Academy, IIT Bombay, Mumbai
3 AOL India (R&D), Bangalore
India
{adityaj,balamurali,pb}@cse.iitb.ac.in r.mohanty@teamaol.com
Abstract
Social networking and micro-blogging sites
are stores of opinion-bearing content created
by human users. We describe C-Feel-It, a sys-
tem which can tap opinion content in posts
(called tweets) from the micro-blogging web-
site, Twitter. This web-based system catego-
rizes tweets pertaining to a search string as
positive, negative or objective and gives an ag-
gregate sentiment score that represents a senti-
ment snapshot for a search string. We present
a qualitative evaluation of this system based
on a human-annotated tweet corpus.
1 Introduction
A major contribution of Web 2.0 is the explosive rise
of user-generated content. The content has been a
by-product of a class of Internet-based applications
that allow users to interact with each other on the
web. These applications which are highly accessible
and scalable represent a class of media called social
media. Some of the currently popular social media
sites are Facebook (www.facebook.com), Myspace
(www.myspace.com), Twitter (www.Twitter.com)
etc. User-generated content on the social media rep-
resents the views of the users and hence, may be
opinion-bearing. Sales and marketing arms of busi-
ness organizations can leverage on this information
to know more about their customer base. In addi-
tion, prospective customers of a product/service can
get to know what other users have to say about the
product/service and make an informed decision.
C-Feel-It is a web-based system which
predicts sentiment in micro-blogs on
Twitter (called tweets). (Screencast at:
http://www.youtube.com/user/cfeelit/ ) C-Feel-
It uses a rule-based system to classify tweets as
positive, negative or objective using inputs from
four sentiment-based knowledge repositories. A
weighted-majority voting principle is used to predict
sentiment of a tweet. An overall sentiment score for
the search string is assigned based on the results of
predictions for the tweets fetched. This score which
is represented as a percentage value gives a live
snapshot of the sentiment of users about the topic.
The rest of the paper is organized as follows: Sec-
tion 2 gives background study of Twitter and related
work in the context of sentiment analysis for Twitter.
The system architecture is explained in section 3. A
qualitative evaluation of our system based on anno-
tated data is described in section 4. Section 5 sum-
marizes the paper and points to future work.
2 Background study
Twitter is a micro-blogging website and ranks sec-
ond among the present social media websites (Prelo-
vac, 2010). A micro-blog allows users to exchange
small elements of content such as short sentences,
individual pages, or video links (Kaplan and Haen-
lein, 2010). More about Twitter can be found here 1.
In Twitter, a micro-blogging post is called a
tweet which can be upto 140 characters in length.
Since the length is constrained, the language used in
tweets is highly unstructured. Misspellings, slangs,
contractions and abbreviations are commonly used
in tweets. The following example highlights these
problems in a typical tweet:
?Big brother doing sian massey no favours.
Let her ref. She?s good at it you know#lifesapitch?
We choose Twitter as the data source because
of the sheer quantity of data generated and its fast
reachability across masses. Additionally, Twitter al-
lows information to flow freely and instantaneously
unlike FaceBook or MySpace. These aspects of
1http://support.twitter.com/groups/31-twitter-basics
127
Twitter makes it a source for getting a live snapshot
of the things happenings on the web.
In the context of sentiment classification of tweets
Alec et al (2009a) describes a distant supervision-
based approach for sentiment classification. The
training data for this purpose is created following a
semi-supervised approach that exploits emoticons in
tweets. In their successive work, Alec et al (2009b)
additionally use hashtags in tweets to create train-
ing data. Topic-dependent clustering is performed
on this data and classifiers corresponding to each are
modeled. This approach is found to perform better
than a single classifier alone.
We believe that the models trained on data cre-
ated using semi-supervised approaches cannot clas-
sify all variants of tweets. Hence, we follow a rule-
based approach for predicting sentiment of a tweet.
An approach like ours provides a generic way of
solving sentiment classification problems in micro-
blogs.
3 Architecture
keywor
d (s)
Tweet fetcher
Tweet Sentime
nt 
Predicto
r
C-Feel-I
t
Sentime
nt score
Tweet Sentime
nt 
Collabo
rator
score
Figure 1: Overall Architecture
The overall architecture of C-Feel-It is shown in
Figure 1. C-Feel-It is divided into three parts: Tweet
Fetcher, Tweet Sentiment Predictor and Tweet
Sentiment Collaborator. All predictions are pos-
itive, negative or objective/neutral. C-Feel-It offers
two implementations of a rule-based sentiment pre-
diction system. We refer to them as version 1 and
2. The two versions differ in the Tweet Sentiment
Predictor module. This section describes different
modules of C-Feel-It and is organized as follows. In
subsections 3.1, 3.2 & 3.3, we describe the three
functional blocks of C-FeeL-It. In subsection 3.4,
we explain how four lexical resources are mapped
to the desired output labels. Finally, subsection 3.5
gives implementation details of C-Feel-It.
Input to C-Feel-It is a search string and a version
number. The versions are described in detail in sub-
section 3.2.
Output given by C-Feel-It is two-level: tweet-wise
prediction and overall prediction. For tweet-wise
prediction, sentiment prediction by each of the re-
sources is returned. On the other hand, overall pre-
diction combines sentiment from all tweets to return
the percentage of positive, negative and objective
content retrieved for the search string.
3.1 Tweet Fetcher
Tweet fetcher obtains tweets pertaining to a search
string entered by a user. To do so, we use live feeds
from Twitter using an API 2. The parameters passed
to the API ensure that system receives the latest 50
tweets about the keyword in English. This API re-
turns results in XML format which we parse using a
Java SAX parser.
3.2 Tweet Sentiment Predictor
Tweet sentiment predictor predicts sentiment for
a single tweet. The architecture of Tweet Senti-
ment Predictor is shown in Figure 2 and can be di-
vided into three fundamental blocks: Preprocessor,
Emoticon-based Sentiment Predictor, Lexicon-based
Sentiment Predictor (refer Figure 3 & 4). The first
two blocks are same for both the versions of C-Feel-
It. The two versions differ in the working of the
Lexicon-based Sentiment Predictor.
Preprocessor
The noisy nature of tweets is a classical challenge
that any system working on tweets needs to en-
counter. Preprocessor deals with obtaining clean
tweets. We do not deploy any spelling correction
module. However, the preprocessor handles exten-
sions and contractions found in tweets as follows.
Handling extensions: Extensions like ?besssssst?
are common in tweets. However, to look up re-
sources, it is essential that these words are normal-
ized to their dictionary equivalent. We replace con-
secutive occurrences of the same letter (if more than
2http://search.Twitter.com/search.atom
128
Lexicon
-based sentime
nt 
predicto
r
Word e
xtensio
n
handler
Tweet
if no em
oticonS
entimen
t 
predicti
on
Chat lin
go 
normali
zation
Emotico
n-based
 
sentime
nt 
predicto
r
Tweet P
reproce
ssing
Sentime
nt 
predicti
on
Figure 2: Tweet Sentiment Predictor: Version 1 and 2
three occurrences of the same letter) with a single
letter and replace the word.
An important issue here is that extensions are in fact
strong indicators of sentiment. Hence, we replace an
extended word by two occurences of the contracted
word. This gives a higher weight to the extended
word and retains its contribution to the sentiment of
the tweet.
Chat lingo normalization: Words used in
chat/Internet language that are common in tweets are
not present in the lexical resources. We use a dictio-
nary downloaded from http://chat.reichards.net/ . A
chat word is replaced by its dictionary equivalent.
Emoticon-based Sentiment Predictor
Emoticons are visual representations of emo-
tions frequently used in the user-generated con-
tent on the Internet. We observe that in most
cases, emoticons pinpoint the sentiment of a
tweet. We use an emoticon mapping from
http://chat.reichards.net/smiley.shtml. An emoticon
is mapped to an output label: positive or negative. A
tweet containing one of these emoticons that can be
mapped to the desired output labels directly. While
we understand that this heuristic does not work in
case of sarcastic tweets, it does provide a benefit in
most cases.
Lexicon-based Sentiment Predictor
For a tweet, the Lexicon-based Sentiment Predic-
tor gives a prediction each for four resources. In
addition, it returns one prediction which combines
the four predictions by weighting them on the ba-
Tweet
Lexical Resourc
e
Get 
sentime
nt pred
iction
For all w
ords Return 
output 
label 
corresp
onding 
to 
majorit
y of wo
rds
Sentime
nt 
Predicti
on
Figure 3: Lexicon-based Sentiment Predictor: C-Feel-It
Version 1
sis of their accuracies. We remove stop words 3
from the tweet and stem the words using Lovins
stemmer (Lovins, 1968). Negation in tweets is han-
dled by inverting sentiment of words after a negat-
ing word. The words ?no?, ?never?, ?not? are consid-
ered negating words and a context window of three
words after a negative words is considered for in-
version. The two versions of C-Feel-It vary in their
Lexicon-based Sentiment Predictor. Figure 3 shows
the Lexicon-based Sentiment Predictor for version
1. For each word in the tweet, it gets the predic-
tion from a lexical resource. We use the intuition
that a positive tweet has positive words outnumber-
ing other words, a negative tweet has negative words
outnumbering other words and an objective tweet
has objective words outnumbering other words.
Figure 4 shows the Lexicon-based Sentiment Predic-
tor for version 2. As opposed to the earlier version,
version 2 gets prediction from the lexical resource
for some words in the tweet. This is because certain
parts-of-speech have been found to be better indi-
cators of sentiment (Pang and Lee, 2004). A tweet
is annotated with parts-of-speech tags and the POS
bi-tags (i.e. a pattern of two consecutive POS) are
marked. The words corresponding to a set of opti-
mal POS bi-tags are retained and only these words
used for lookup. The prediction for a tweet uses
majority vote-based approach as for version 1. The
optimal POS bi-tags have been derived experimen-
tally by using top 10% features on information gain-
based-pruning classifier on polarity dataset by (Pang
and Lee, 2005). We used Stanford POS tagger(Tou,
3http://www.ranks.nl/resources/stopwords.html
129
2000) for tagging the tweets.
Note: The dataset we use to find optimal POS
bi-tags consists of movie reviews. We understand
that POS bi-tags hence derived may not be universal
across domains.
Tweet
Lexical Resourc
e
Get sentime
nt 
predicti
on
For all w
ords
POS tag
 the tweet
Retain words corresp
ond
Return 
output label corresp
ondin
g to ma
jority of word
s
Sentime
nt 
Predicti
on
corresp
ond
ing to select P
OS 
bi-tags
Figure 4: Lexicon-based Sentiment Predictor: C-Feel-It
Version 2
3.3 Tweet Sentiment Collaborator
Based on predictions of individual tweets, the Tweet
Sentiment Collaborator gives overall prediction
with respect to a keyword in form of percentage
of positive, negative and objective content. This
is on the basis of predictions by each resource by
weighting them according to their accuracies. These
weights have been assigned to each resource based
on experimental results. For each resource, the
following scores are determined.
posscore[r] =
m?
i=1
piwpi
negscore[r] =
m?
i=1
niwni
objscore[r] =
m?
i=1
oiwoi
where
posscore[r] = Positive score for search string r
negscore[r] = Negative score for search string r
objscore[r] = Objective score for search string r
m = Number of resources used for prediction
pi, ni, oi = Positive,negative & objective count of tweet
predicted respectively using resource i
wpi, wni, ooi = Weights for respective classes derived
for each resource i
We normalize these scores to get the final positive, neg-
ative and objective pertaining to search string r. These
scores are represented in form of percentage.
3.4 Resources
Sentiment-based lexical resources annotate
words/concepts with polarity. The completeness
of these resources individually remains a question.
To achieve greater coverage, we use four different
sentiment-based lexical resources for C-Feel-It. They are
described as follows.
1. SentiWordNet (Esuli and Sebastiani, 2006) assigns
three scores to synsets of WordNet: positive score,
negative score and objective score. When a word is
looked up, the label corresponding to maximum of
the three scores is returned. For multiple synsets of
a word, the output label returned by majority of the
synsets becomes the prediction of the resource.
2. Subjectivity lexicon (Wiebe et al, 2004) is a re-
source that annotates words with tags like parts-of-
speech, prior polarity, magnitude of prior polarity
(weak/strong), etc. The prior polarity can be posi-
tive, negative or neutral. For prediction using this
resource, we use this prior polarity.
3. Inquirer (Stone et al, 1966) is a list of words
marked as positive, negative and neutral. We use
these labels to use Inquirer resource for our predic-
tion.
4. Taboada (Taboada and Grieve, 2004) is a word-list
that gives a count of collocations with positive and
negative seed words. A word closer to a positive
seed word is predicted to be positive and vice versa.
3.5 Implementation Details
The system is implemented in JSP (JDK 1.6) using Net-
Beans IDE 6.9.1. For the purpose of tweet annotation,
an internal interface was written in PHP 5 with MySQL
5.0.51a-3ubuntu5.7 for storage.
4 System Analysis
4.1 Evaluation Data
For the purpose of evaluation, a total of 7000 tweets
were downloaded by using popular trending topics of
20 domains (like books, movies, electronic gadget, etc.)
as keywords for searching tweets. In order to download
the tweets, we used the API provided by Twitter 4 that
crawls latest tweets pertaining to keywords.
Human annotators assigned to a tweet one out of 4
classes: positive, negative, objective and objective-spam.
4http://search.twitter.com/search.atom?
130
A tweet is assigned to objective-spam category if it con-
tains promotional links or incoherent text which was pos-
sibly not created by a human user. Apart from these nom-
inal class labels, we also assigned the positive/negative
tweets scores ranging from +2 to -2 with +2 being the
most positive and -2 being the most negative score re-
spectively. If the tweet belongs to the objective category,
a value of zero is assigned as the score.
The spam category has been included in the annotation
as a future goal of modeling a spam detection layer prior
to the sentiment detection. However, the current version
of C-Feel-It does not have a spam detection module and
hence for evaluation purpose, we use only the data be-
longing to classes other than objective-spam.
4.2 Qualitative Analysis
In this section, we perform a qualitative evaluation of ac-
tual results returned by C-Feel-It. The errors described
in this section are in addition to the errors due to mis-
spellings and informal language. These erroneous results
have been obtained from both version 1 and 2. They
have been classified into eleven categories and explained
henceforth.
4.2.1 Sarcastic Tweets
Tweet: Hoge, Jaws, and Palantonio are brilliant to-
gether talking X?s and O?s on ESPN right now.
Label by C-Feel-It: Positive
Label by human annotator: Negative
The sarcasm in the above tweet lies in the use of a pos-
itive word ?brilliant? followed by a rather trivial action of
?talking Xs and Os?. The positive word leads to the pre-
diction by C-Feel-It where in fact, it is a negative tweet
for the human annotator.
4.2.2 Lack of Sense Understanding
Tweet: If your tooth hurts drink some pain killers and
place a warm/hot tea bag like chamomile on your tooth
and hold it. it will relieve the pain
Label by C-Feel-It: Negative
This tweet is objective in nature. The words ?pain?,
?killers?, etc. in the tweet give an indication to C-Feel-It
that the tweet is negative. This misguided implication is
because of multiple senses of these words (for example,
?pain? can also be used in the sentence ?symptoms of the
disease are body pain and irritation in the throat? where
it is non-sentiment-bearing). The lack of understanding
of word senses and being unable to distinguish between
them leads to this error.
4.2.3 Lack of Entity Specificity
Tweet: Casablanca and a lunch comprising of rice
and fish: a good sunday
Keyword: Casablanca
Label by C-Feel-It: Positive
Label by human annotator: Objective
In the above tweet, the human annotator understood
that though the tweet contains the keyword ?Casablanca?,
it is not Casablanca about which sentiment is expressed.
The system finds a positive word ?good? and marks the
tweet as positive. This error arises because the system
cannot find out which sentence/parts of sentence is ex-
pressing opinion about the target entity.
4.2.4 Coverage of Resources
Tweet: I?m done with this bullshit. You?re the psycho
not me.
Label by SentiWordNet: Negative
Label by Taboada/Inquirer: Objective
Label by human annotator: Negative
On manual verification, it was observed that an entry
for the emotion-bearing word ?bullshit? is present in Sen-
tiWordNet while Inquirer and Taboada resource do not
have them. This shows that the coverage of the lexical
resource affects the performance of a system and may in-
troduce errors.
4.2.5 Absence of Named Entity Recognition
Tweet: @user I don?t think I need to guess, but ok,
close encounters of the third kind? Lol
Entity: Close encounters of the third kind
Label by C-Feel-It: Positive
The words comprising the name of the film ?Close en-
counters of the third kind? are also looked up. Inability to
identify the named entity leads the system into this trap.
4.2.6 Requirement of World Knowledge
Tweet: The soccer world cup boasts an audience twice
that of the Summer Olympics.
Label by C-Feel-It: Negative
To judge the opinion of this tweet, one requires an un-
derstanding of the fact that larger the audience, more fa-
vorable it is for a sports tournament. This world knowl-
edge is important for a system that aims to handle tweets
like these.
4.2.7 Mixed Emotion Tweets
Tweet: oh but that last kiss tells me it?s goodbye, just
like nothing happened last night. but if i had one chance,
i?d do it all over again
Label by C-Feel-It: Positive
The tweet contains emotions of positive as well as neg-
ative variety and it would in fact be difficult for a human
as well to identify the polarity. The mixed nature of the
tweet leads to this error by the system.
4.2.8 Lack of Context
Tweet: I?ll have to say it?s a tie between Little Women
or To kill a Mockingbird
131
Label by C-Feel-It: Negative
Label by human user: Positive
The tweet has a sentiment which will possibly be clear
in the context of the conversation. Going by the tweet
alone, while one understands that an comparative opinion
is being expressed, it is not possible to tag it as positive
or negative.
4.2.9 Concatenated Words
Tweet: To Kill a Mockingbird is a #goodbook.
Label by C-Feel-It: Negative
The tweet has a hashtag containing concatenated
words ?goodbook? which get overlooked as out-of-
dictionary words and hence, are not used for sentiment
prediction. The sentiment of ?good? is not detected.
4.2.10 Interjections
Tweet: Oooh. Apocalypse Now is on bluray now.
Label by C-Feel-It: Objective
Label by human user: Positive
The extended interjection ?Oooh? is an indicator of
sentiment. Since it does not have a direct prior polar-
ity, it is not present in any of the resources. However, this
interjection is an important carrier of sentiment.
4.2.11 Comparatives
Tweet: The more years I spend at Colbert Heights..the
more disgusted I get by the people there. I?m soooo ready
to graduate.
Label by C-Feel-It: Positive
Label by human user: Negative
The comparatives in the sentence expressed by ?..more
disgusted I get..? have to be handled as a special case
because ?more? is an intensification of the negative senti-
ment expressed by the word ?disgusted?.
5 Summary & Future Work
In this paper, we described a system which categorizes
live tweets related to a keyword as positive, negative
and objective based on the predictions of four sentiment-
based resources. We also presented a qualitative evalua-
tion of our system pointing out the areas of improvement
for the current system.
A sentiment analyzer of this kind can be tuned to take in-
puts from different sources on the internet (for example,
wall posts on facebook). In order to improve the qual-
ity of sentiment prediction, we propose two additions.
Firstly, while we use simple heuristics to handle exten-
sions of words in tweets, a deeper study is required to
decipher the pragmatics involved. Secondly, a spam de-
tection module that eliminates promotional tweets before
performing sentiment detection may be added to the cur-
rent system. Our goal with respect to this system is to de-
ploy it for predicting share market values of firms based
on sentiment on social networks with respect to related
entitites.
Acknowledgement
We thank Akshat Malu and Subhabrata Mukherjee, IIT
Bombay for their assistance during generation of evalua-
tion data.
References
Go Alec, Huang Lei, and Bhayani Richa. 2009a. Twit-
ter sentiment classification using distant supervision.
Technical report, Standford University.
Go Alec, Bhayani Richa, Raghunathan Karthik, and
Huang Lei. 2009b. May.
Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWord-
Net: A publicly available lexical resource for opinion
mining. In Proceedings of LREC-06, Genova, Italy.
Andreas M. Kaplan and Michael Haenlein. 2010. The
early bird catches the news: Nine things you should
know about micro-blogging. Business Horizons,
54(2):05 ? 113.
Julie B. Lovins. 1968. Development of a Stemming Al-
gorithm. June.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of ACL-05.
Vladimir Prelovac. 2010. Top social media sites. Web,
May.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Maite Taboada and Jack Grieve. 2004. Analyzing Ap-
praisal Automatically. In Proceedings of the AAAI
Spring Symposium on Exploring Attitude and Affect in
Text: Theories and Applications, pages 158?161, Stan-
ford, US.
2000. Enriching the knowledge sources used in a maxi-
mum entropy part-of-speech tagger, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjec-
tive language. Computional Linguistics, 30:277?308,
September.
132
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 36?41,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Measuring Sentiment Annotation Complexity of Text
Aditya Joshi
1,2,3?
Abhijit Mishra
1
Nivvedan Senthamilselvan
1
Pushpak Bhattacharyya
1
1
IIT Bombay, India,
2
Monash University, Australia
3
IITB-Monash Research Academy, India
{adityaj, abhijitmishra, nivvedan, pb}@cse.iitb.ac.in
Abstract
The effort required for a human annota-
tor to detect sentiment is not uniform for
all texts, irrespective of his/her expertise.
We aim to predict a score that quantifies
this effort, using linguistic properties of
the text. Our proposed metric is called
Sentiment Annotation Complexity (SAC).
As for training data, since any direct judg-
ment of complexity by a human annota-
tor is fraught with subjectivity, we rely on
cognitive evidence from eye-tracking. The
sentences in our dataset are labeled with
SAC scores derived from eye-fixation du-
ration. Using linguistic features and anno-
tated SACs, we train a regressor that pre-
dicts the SAC with a best mean error rate of
22.02% for five-fold cross-validation. We
also study the correlation between a hu-
man annotator?s perception of complexity
and a machine?s confidence in polarity de-
termination. The merit of our work lies in
(a) deciding the sentiment annotation cost
in, for example, a crowdsourcing setting,
(b) choosing the right classifier for senti-
ment prediction.
1 Introduction
The effort required by a human annotator to de-
tect sentiment is not uniform for all texts. Com-
pare the hypothetical tweet ?Just what I wanted: a
good pizza.? with ?Just what I wanted: a cold
pizza.?. The two are lexically and structurally
similar. However, because of the sarcasm in the
second tweet (in ?cold? pizza, an undesirable sit-
uation followed by a positive sentiment phrase
?just what I wanted?, as discussed in Riloff et al
(2013)), it is more complex than the first for senti-
ment annotation. Thus, independent of how good
?
- Aditya is funded by the TCS Research Fellowship Pro-
gram.
the annotator is, there are sentences which will be
perceived to be more complex than others. With
regard to this, we introduce a metric called senti-
ment annotation complexity (SAC). The SAC of a
given piece of text (sentences, in our case) can be
predicted using the linguistic properties of the text
as features.
The primary question is whether such complex-
ity measurement is necessary at all. Fort et al
(2012) describe the necessity of annotation com-
plexity measurement in manual annotation tasks.
Measuring annotation complexity is beneficial in
annotation crowdsourcing. If the complexity of
the text can be estimated even before the annota-
tion begins, the pricing model can be fine-tuned
(pay less for sentences that are easy to annotate,
for example). Also, in terms of an automatic SA
engine which has multiple classifiers in its ensem-
ble, a classifier may be chosen based on the com-
plexity of sentiment annotation (for example, use
a rule-based classifier for simple sentences and a
more complex classifier for other sentences). Our
metric adds value to sentiment annotation and sen-
timent analysis, in these two ways. The fact that
sentiment expression may be complex is evident
from a study of comparative sentences by Gana-
pathibhotla and Liu (2008), sarcasm by Riloff et
al. (2013), thwarting by Ramteke et al (2013) or
implicit sentiment by Balahur et al (2011). To
the best of our knowledge, there is no general ap-
proach to ?measure? how complex a piece of text
is, in terms of sentiment annotation.
The central challenge here is to annotate a data
set with SAC. To measure the ?actual? time spent
by an annotator on a piece of text, we use an eye-
tracker to record eye-fixation duration: the time
for which the annotator has actually focused on
the sentence during annotation. Eye-tracking an-
notations have been used to study the cognitive as-
pects of language processing tasks like translation
by Dragsted (2010) and sense disambiguation by
36
Joshi et al (2011). Mishra et al (2013) present a
technique to determine translation difficulty index.
The work closest to ours is by Scott et al (2011)
who use eye-tracking to study the role of emotion
words in reading.
The novelty of our work is three-fold: (a) The
proposition of a metric to measure complexity of
sentiment annotation, (b) The adaptation of past
work that uses eye-tracking for NLP in the con-
text of sentiment annotation, (c) The learning of
regressors that automatically predict SAC using
linguistic features.
2 Understanding Sentiment Annotation
Complexity
The process of sentiment annotation consists of
two sub-processes: comprehension (where the an-
notator understands the content) and sentiment
judgment (where the annotator identifies the sen-
timent). The complexity in sentiment annotation
stems from an interplay of the two and we expect
SAC to capture the combined complexity of both
the sub-processes. In this section, we describe
how complexity may be introduced in sentiment
annotation in different classical layers of NLP.
The simplest form of sentiment annotation com-
plexity is at the lexical level. Consider the sen-
tence ?It is messy, uncouth, incomprehensible, vi-
cious and absurd?. The sentiment words used
in this sentence are uncommon, resulting in com-
plexity.
The next level of sentiment annotation com-
plexity arises due to syntactic complexity. Con-
sider the review: ?A somewhat crudely con-
structed but gripping, questing look at a person so
racked with self-loathing, he becomes an enemy to
his own race.?. An annotator will face difficulty
in comprehension as well as sentiment judgment
due to the complicated phrasal structure in this re-
view. Implicit expression of sentiment introduces
complexity at the semantic and pragmatic level.
Sarcasm expressed in ?It?s like an all-star salute to
disney?s cheesy commercialism? leads to difficulty
in sentiment annotation because of positive words
like ?an all-star salute?.
Manual annotation of complexity scores may
not be intuitive and reliable. Hence, we use a cog-
nitive technique to create our annotated dataset.
The underlying idea is: if we monitor annotation
of two textual units of equal length, the more com-
plex unit will take longer to annotate, and hence,
should have a higher SAC. Using the idea of ?an-
notation time? linked with complexity, we devise a
technique to create a dataset annotated with SAC.
It may be thought that inter-annotator agree-
ment (IAA) provides implicit annotation: the
higher the agreement, the easier the piece of text
is for sentiment annotation. However, in case of
multiple expert annotators, this agreement is ex-
pected to be high for most sentences, due to the
expertise. For example, all five annotators agree
with the label for 60% sentences in our data set.
However, the duration for these sentences has a
mean of 0.38 seconds and a standard deviation of
0.27 seconds. This indicates that although IAA is
easy to compute, it does not determine sentiment
annotation complexity of text in itself.
3 Creation of dataset annotated with
SAC
We wish to predict sentiment annotation complex-
ity of the text using a supervised technique. As
stated above, the time-to-annotate is one good can-
didate. However, ?simple time measurement? is
not reliable because the annotator may spend time
not doing any annotation due to fatigue or distrac-
tion. To accurately record the time, we use an
eye-tracking device that measures the ?duration of
eye-fixations
1
?. Another attribute recorded by the
eye-tracker that may have been used is ?saccade
duration
2
?. However, saccade duration is not sig-
nificant for annotation of short text, as in our case.
Hence, the SAC labels of our dataset are fixation
durations with appropriate normalization.
It may be noted that the eye-tracking device is
used only to annotate training data. The actual
prediction of SAC is done using linguistic features
alone.
3.1 Eye-tracking Experimental Setup
We use a sentiment-annotated data set consisting
of movie reviews by (Pang and Lee, 2005) and
tweets from http://help.sentiment140.
com/for-students. A total of 1059 sen-
tences (566 from a movie corpus, 493 from a twit-
ter corpus) are selected.
We then obtain two kinds of annotation from
five paid annotators: (a) sentiment (positive, nega-
tive and objective), (b) eye-movement as recorded
1
A long stay of the visual gaze on a single location.
2
A rapid movement of the eyes between positions of rest
on the sentence.
37
Figure 1: Gaze-data recording using Translog-II
by an eye-tracker. They are given a set of instruc-
tions beforehand and can seek clarifications. This
experiment is conducted as follows:
1. A sentence is displayed to the annotator on
the screen. The annotator verbally states the
sentiment of this sentence, before (s)he can
proceed to the next.
2. While the annotator reads the sentence, a
remote eye-tracker (Model: Tobii TX 300,
Sampling rate: 300Hz) records the eye-
movement data of the annotator. The eye-
tracker is linked to a Translog II soft-
ware (Carl, 2012) in order to record the data.
A snapshot of the software is shown in fig-
ure 1. The dots and circles represent position
of eyes and fixations of the annotator respec-
tively.
3. The experiment then continues in modules of
50 sentences at a time. This is to prevent fa-
tigue over a period of time. Thus, each an-
notator participates in this experiment over a
number of sittings.
We ensure the quality of our dataset in different
ways: (a) Our annotators are instructed to avoid
unnecessary head movements and eye-movements
outside the experiment environment. (b) To min-
imize noise due to head movements further, they
are also asked to state the annotation verbally,
which was then manually recorded, (c) Our an-
notators are students between the ages 20-24 with
English as the primary language of academic in-
struction and have secured a TOEFL iBT score of
110 or above.
We understand that sentiment is nuanced- to-
wards a target, through constructs like sarcasm and
presence of multiple entities. However, we want to
capture the most natural form of sentiment anno-
tation. So, the guidelines are kept to a bare mini-
mum of ?annotating a sentence as positive, nega-
tive and objective as per the speaker?. This exper-
iment results in a data set of 1059 sentences with
a fixation duration recorded for each sentence-
annotator pair
3
The multi-rater kappa IAA for sen-
timent annotation is 0.686.
3.2 Calculating SAC from eye-tracked data
We now need to annotate each sentence with a
SAC. We extract fixation durations of the five an-
notators for each of the annotated sentences. A
single SAC score for sentence s for N annotators
is computed as follows:
SAC(s) =
1
N
N
?
n=1
z(n,dur(s,n))
len(s)
where,
z(n, dur(s, n)) =
dur(s,n)??(dur(n))
?(dur(n))
(1)
In the above formula, N is the total number of an-
notators while n corresponds to a specific annota-
tor. dur(s, n) is the fixation duration of annotator
n on sentence s. len(s) is the number of words
in sentence s. This normalization over number
of words assumes that long sentences may have
high dur(s, n) but do not necessarily have high
SACs. ?(dur(n)), ?(dur(n)) is the mean and
standard deviation of fixation durations for anno-
tator n across all sentences. z(n, .) is a function
that z-normalizes the value for annotator n to stan-
dardize the deviation due to reading speeds. We
convert the SAC values to a scale of 1-10 using
min-max normalization. To understand how the
formula records sentiment annotation complexity,
consider the SACs of examples in section 2. The
sentence ?it is messy , uncouth , incomprehensi-
ble , vicious and absurd? has a SAC of 3.3. On the
other hand, the SAC for the sarcastic sentence ?it?s
like an all-star salute to disney?s cheesy commer-
cialism.? is 8.3.
4 Predictive Framework for SAC
The previous section shows how gold labels for
SAC can be obtained using eye-tracking experi-
ments. This section describes our predictive for
SAC that uses four categories of linguistic fea-
tures: lexical, syntactic, semantic and sentiment-
related in order to capture the subprocesses of an-
notation as described in section 2.
4.1 Experiment Setup
The linguistic features described in Table 3.2 are
extracted from the input sentences. Some of these
3
The complete eye-tracking data is available at:http://
www.cfilt.iitb.ac.in/
?
cognitive-nlp/.
38
Feature Description
Lexical
- Word Count
- Degree of polysemy Average number of Wordnet senses per word
- Mean Word Length Average number of characters per word (commonly used in readability studies
as in the case of Pascual et al (2005))
- %ge of nouns and adjs.
- %ge of Out-of-
vocabulary words
Syntactic
- Dependency Distance Average distance of all pairs of dependent words in the sentence (Lin, 1996)
- Non-terminal to Ter-
minal ratio
Ratio of the number of non-terminals to the number of terminals in the con-
stituency parse of a sentence
Semantic
- Discourse connectors Number of discourse connectors
- Co-reference distance Sum of token distance between co-referring entities of anaphora in a sentence
- Perplexity Trigram perplexity using language models trained on a mixture of sentences
from the Brown corpus, the Amazon Movie corpus and Stanford twitter corpus
(mentioned in Sections 3 and 5)
Sentiment-related (Computed using SentiWordNet (Esuli et al, 2006))
- Subjective Word
Count
- Subjective Score Sum of SentiWordNet scores of all words
- Sentiment Flip Count A positive word followed in sequence by a negative word, or vice versa counts
as one sentiment flip
Table 1: Linguistic Features for the Predictive Framework
features are extracted using Stanford Core NLP
4
tools and NLTK (Bird et al, 2009). Words that
do not appear in Academic Word List
5
and Gen-
eral Service List
6
are treated as out-of-vocabulary
words. The training data consists of 1059 tuples,
with 13 features and gold labels from eye-tracking
experiments.
To predict SAC, we use Support Vector Regres-
sion (SVR) (Joachims, 2006). Since we do not
have any information about the nature of the rela-
tionship between the features and SAC, choosing
SVR allows us to try multiple kernels. We carry
out a 5-fold cross validation for both in-domain
and cross-domain settings, to validate that the re-
gressor does not overfit. The model thus learned is
evaluated using: (a) Error metrics namely, Mean
Squared Error estimate, Mean Absolute Error esti-
mate and Mean Percentage Error. (b) the Pearson
correlation coefficient between the gold and pre-
4
http://nlp.stanford.edu/software/
corenlp.shtml
5
www.victoria.ac.nz/lals/resources/academicwordlist/
6
www.jbauman.com/gsl.html
dicted SAC.
4.2 Results
The results are tabulated in Table 2. Our obser-
vation is that a quadratic kernel performs slightly
better than linear. The correlation values are pos-
itive and indicate that even if the predicted scores
are not as accurate as desired, the system is capa-
ble of ranking sentences in the correct order based
on their sentiment complexity. The mean percent-
age error (MPE) of the regressors ranges between
22-38.21%. The cross-domain MPE is higher than
the rest, as expected.
To understand how each of the features per-
forms, we conducted ablation tests by con-
sidering one feature at a time. Based on
the MPE values, the best features are: Mean
word length (MPE=27.54%), Degree of Polysemy
(MPE=36.83%) and %ge of nouns and adjectives
(MPE=38.55%). To our surprise, word count per-
forms the worst (MPE=85.44%). This is unlike
tasks like translation where length has been shown
39
Kernel Linear Quadratic Cross Domain Linear
Domain Mixed Movie Twitter Mixed Movie Twitter Movie Twitter
MSE 1.79 1.55 1.99 1.68 1.53 1.88 3.17 2.24
MAE 0.93 0.89 0.95 0.91 0.88 0.93 1.39 1.19
MPE 22.49% 23.8% 25.45% 22.02% 23.8% 25% 35.01% 38.21%
Correlation 0.54 0.38 0.56 0.57 0.37 0.6 0.38 0.46
Table 2: Performance of Predictive Framework for 5-fold in-domain and cross-domain validation using
Mean Squared Error (MSE), Mean Absolute Error (MAE) and Mean Percentage Error (MPE) estimates
and correlation with the gold labels.
to be one of the best predictors in translation dif-
ficulty (Mishra et al, 2013). We believe that for
sentiment annotation, longer sentences may have
more lexical clues that help detect the sentiment
more easily. Note that some errors may be intro-
duced in feature extraction due to limitations of
the NLP tools.
5 Discussion
Our proposed metric measures complexity of sen-
timent annotation, as perceived by human annota-
tors. It would be worthwhile to study the human-
machine correlation to see if what is difficult for
a machine is also difficult for a human. In other
words, the goal is to show that the confidence
scores of a sentiment classifier are negatively cor-
related with SAC.
We use three sentiment classification tech-
niques: Na??ve Bayes, MaxEnt and SVM with un-
igrams, bigrams and trigrams as features. The
training datasets used are: a) 10000 movie reviews
from Amazon Corpus (McAuley et. al, 2013) and
b) 20000 tweets from the twitter corpus (same as
mentioned in section 3). Using NLTK and Scikit-
learn
7
with default settings, we generate six posi-
tive/negative classifiers, for all possible combina-
tions of the three models and two datasets.
The confidence score of a classifier
8
for given
text t is computed as follows:
P : Probability of predicted class
Confidence(t) =
?
?
?
P if predicted
polarity is correct
1? P otherwise
(2)
7
http://scikit-learn.org/stable/
8
In case of SVM, the probability of predicted class is com-
puted as given in Platt (1999).
Classifier (Corpus) Correlation
Na??ve Bayes (Movie) -0.06 (73.35)
Na??ve Bayes (Twitter) -0.13 (71.18)
MaxEnt (Movie) -0.29 (72.17)
MaxEnt (Twitter) -0.26 (71.68)
SVM (Movie) -0.24 (66.27)
SVM (Twitter) -0.19 (73.15)
Table 3: Correlation between confidence of the
classifiers with SAC; Numbers in parentheses in-
dicate classifier accuracy (%)
Table 3 presents the accuracy of the classifiers
along with the correlations between the confidence
score and observed SAC values. MaxEnt has the
highest negative correlation of -0.29 and -0.26.
For both domains, we observe a weak yet nega-
tive correlation which suggests that the perception
of difficulty by the classifiers are in line with that
of humans, as captured through SAC.
6 Conclusion & Future Work
We presented a metric called Sentiment Annota-
tion Complexity (SAC), a metric in SA research
that has been unexplored until now. First, the pro-
cess of data preparation through eye tracking, la-
beled with the SAC score was elaborated. Using
this data set and a set of linguistic features, we
trained a regression model to predict SAC. Our
predictive framework for SAC resulted in a mean
percentage error of 22.02%, and a moderate corre-
lation of 0.57 between the predicted and observed
SAC values. Finally, we observe a negative corre-
lation between the classifier confidence scores and
a SAC, as expected. As a future work, we would
like to investigate how SAC of a test sentence can
be used to choose a classifier from an ensemble,
and to determine the pre-processing steps (entity-
relationship extraction, for example).
40
References
Balahur, Alexandra and Hermida, Jes?us M and Mon-
toyo, Andr?es. 2011. Detecting implicit expressions
of sentiment in text based on commonsense knowl-
edge. Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis,53-60.
Batali, John and Searle, John R. 1995. The Rediscov-
ery of the Mind. Artif. Intell., Vol. 77, 177-193.
Steven Bird and Ewan Klein and Edward Loper. 2009.
Natural Language Processing with Python O?Reilly
Media.
Carl, M. 2012. Translog-II: A Program for Record-
ing User Activity Data for Empirical Reading and
Writing Research. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation, European Language Resources Associ-
ation.
Dragsted, B. 2010. 2010. Co-ordination of reading
and writing processes in translation. Contribution
to Translation and Cognition. Shreve, G. and An-
gelone, E.(eds.)Cognitive Science Society.
Esuli, Andrea and Sebastiani, Fabrizio. 2006. Sen-
tiwordnet: A publicly available lexical resource for
opinion mining. Proceedings of LREC, vol. 6, 417-
422.
Fellbaum, Christiane 1998. WordNet: An electronic
lexical database. 1998. Cambridge. MA: MIT Press.
Fort, Kar?en and Nazarenko, Adeline and Rosset, So-
phie et al2012. Modeling the complexity of manual
annotation tasks: A grid of analysis Proceedings of
the International Conference on Computational Lin-
guistics.
Ganapathibhotla, G and Liu, Bing. 2008. Identifying
preferred entities in comparative sentences. 22nd In-
ternational Conference on Computational Linguis-
tics (COLING).
Gonz?alez-Ib?a?nez, Roberto and Muresan, Smaranda and
Wacholder, Nina 2011. Identifying Sarcasm in
Twitter: A Closer Look. ACL (Short Papers) 581-
586.
Joachims, T. 2006 Training Linear SVMs in Lin-
ear Time Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Lin, D. 1996 On the structural complexity of natural
language sentences. Proceeding of the 16th Inter-
national Conference on Computational Linguistics
(COLING), pp. 729733.
Mart?nez-G?omez, Pascual and Aizawa, Akiko. 2013.
Diagnosing Causes of Reading Difficulty using
Bayesian Networks International Joint Conference
on Natural Language Processing, 13831391.
McAuley, Julian John and Leskovec, Jure 2013 From
amateurs to connoisseurs: modeling the evolution of
user expertise through online reviews. Proceedings
of the 22nd international conference on World Wide
Web.
Mishra, Abhijit and Bhattacharyya, Pushpak and Carl,
Michael. 2013. Automatically Predicting Sentence
Translation Difficulty Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), 346-351.
Narayanan, Ramanathan and Liu, Bing and Choudhary,
Alok 2009. Sentiment Analysis of Conditional Sen-
tences. Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
180-189.
Pang, Bo and Lee, Lillian. 2008. Opinion mining and
sentiment analysis Foundations and trends in infor-
mation retrieval, vol. 2, 1-135.
Pang, Bo and Lee, Lillian. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, 115-124.
Platt, John and others. 1999. Probabilistic outputs for
support vector machines and comparisons to regular-
ized likelihood methods Advances in large margin
classifiers, vol. 10, 61-74.
Ramteke, Ankit and Malu, Akshat and Bhattacharyya,
Pushpak and Nath, J. Saketha 2013. Detect-
ing Turnarounds in Sentiment Analysis: Thwarting
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), 860-865.
Riloff, Ellen and Qadir, Ashequl and Surve, Prafulla
and De Silva, Lalindra and Gilbert, Nathan and
Huang, Ruihong 2013. Sarcasm as Contrast be-
tween a Positive Sentiment and Negative Situation
Conference on Empirical Methods in Natural Lan-
guage Processing, Seattle, USA.
Salil Joshi, Diptesh Kanojia and Pushpak Bhat-
tacharyya. 2013. More than meets the eye: Study
of Human Cognition in Sense Annotation. NAACL
HLT 2013, Atlanta, USA.
Scott G. , O Donnell P and Sereno S. 2012. Emotion
Words Affect Eye Fixations During Reading. Jour-
nal of Experimental Psychology:Learning, Memory,
and Cognition 2012, Vol. 38, No. 3, 783-792
Siegel, Sidney and N. J. Castellan, Jr. 1988. Nonpara-
metric Statistics for the Behavioral Sciences. Second
edition. McGraw-Hill.
41
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 132?138,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Robust Sense-Based Sentiment Classification
Balamurali A R1 Aditya Joshi2 Pushpak Bhattacharyya2
1 IITB-Monash Research Academy, IIT Bombay
2Dept. of Computer Science and Engineering, IIT Bombay
Mumbai, India - 400076
{balamurali,adityaj,pb}@cse.iitb.ac.in
Abstract
The new trend in sentiment classification is
to use semantic features for representation
of documents. We propose a semantic space
based on WordNet senses for a supervised
document-level sentiment classifier. Not only
does this show a better performance for sen-
timent classification, it also opens opportuni-
ties for building a robust sentiment classifier.
We examine the possibility of using similar-
ity metrics defined on WordNet to address the
problem of not finding a sense in the training
corpus. Using three popular similarity met-
rics, we replace unknown synsets in the test
set with a similar synset from the training set.
An improvement of 6.2% is seen with respect
to baseline using this approach.
1 Introduction
Sentiment classification is a task under Sentiment
Analysis (SA) that deals with automatically tagging
text as positive, negative or neutral from the perspec-
tive of the speaker/writer with respect to a topic.
Thus, a sentiment classifier tags the sentence ?The
movie is entertaining and totally worth your money!?
in a movie review as positive with respect to the
movie. On the other hand, a sentence ?The movie is
so boring that I was dozing away through the second
half.? is labeled as negative. Finally, ?The movie is
directed by Nolan? is labeled as neutral. For the pur-
pose of this work, we follow the definition of Pang
et al (2002) & Turney (2002) and consider a binary
classification task for output labels as positive and
negative.
Lexeme-based (bag-of-words) features are com-
monly used for supervised sentiment classifica-
tion (Pang and Lee, 2008). In addition to this, there
also has been work that identifies the roles of dif-
ferent parts-of-speech (POS) like adjectives in sen-
timent classification (Pang et al, 2002; Whitelaw et
al., 2005). Complex features based on parse trees
have been explored for modeling high-accuracy po-
larity classifiers (Matsumoto et al, 2005). Text
parsers have also been found to be helpful in mod-
eling valence shifters as features for classifica-
tion (Kennedy and Inkpen, 2006). In general, the
work in the context of supervised SA has focused on
(but not limited to) different combinations of bag-
of-words-based and syntax-based models.
The focus of this work is to represent a document
as a set of sense-based features. We ask the follow-
ing questions in this context:
1. Are WordNet senses better features as com-
pared to words?
2. Can a sentiment classifier be made robust with
respect to features unseen in the training cor-
pus using similarity metrics defined for con-
cepts in WordNet?
We modify the corpus by Ye et al (2009) for the
purpose of our experiments related to sense-based
sentiment classification. To address the first ques-
tion, we show that the approach that uses senses (ei-
ther manually annotated or obtained through auto-
matic WSD techniques) as features performs better
than the one that uses words as features.
Using senses as features allows us to achieve ro-
bustness for sentiment classification by exploiting
the definition of concepts (sense) and hierarchical
structure of WordNet. Hence to address the second
question, we replace a synset not present in the test
set with a similar synset from the training set us-
ing similarity metrics defined on WordNet. Our re-
sults show that replacement of this nature provides a
boost to the classification performance.
The road map for the rest of the paper is as fol-
lows: Section 2 describes the sense-based features
that we use for this work. We explain the similarity-
based replacement technique using WordNet synsets
132
in section 3. Details about our experiments are de-
scribed in Section 4. In section 5, we present our
results and discussions. We contextualize our work
with respect to other related works in section 6. Fi-
nally, section 7 concludes the paper and points to
future work.
2 WordNet Senses as Features
In their original form, documents are said to be in
lexical space since they consist of words. When the
words are replaced by their corresponding senses,
the resultant document is said to be in semantic
space.
WordNet 2.1 (Fellbaum, 1998) has been used as
the sense repository. Each word/lexeme is mapped
to an appropriate synset in WordNet based on
its sense and represented using the corresponding
synset id of WordNet. Thus, the word love is dis-
ambiguated and replaced by the identifier 21758160
which consists of a POS category identifier 2 fol-
lowed by synset offset identifier 1758160. This
paper refers to POS category identifier along with
synset offset as synset identifiers or as senses.
2.1 Motivation
We describe three different scenarios to show the
need of sense-based analysis for SA. Consider the
following sentences as the first scenario.
1. ?Her face fell when she heard that she had
been fired.?
2. ?The fruit fell from the tree.?
The word ?fell? occurs in different senses in the
two sentences. In the first sentence, ?fell? has the
meaning of ?assume a disappointed or sad expres-
sion, whereas in the second sentence, it has the
meaning of ?descend in free fall under the influence
of gravity?. A user will infer the negative polarity
of the first sentence from the negative sense of ?fell?
in it. This implies that there is at least one sense of
the word ?fell? that carries sentiment and at least one
that does not.
In the second scenario, consider the following ex-
amples.
1. ?The snake bite proved to be deadly for the
young boy.?
2. ?Shane Warne is a deadly spinner.?
The word deadly has senses which carry opposite
polarity in the two sentences and these senses as-
sign the polarity to the corresponding sentence. The
first sentence is negative while the second sentence
is positive.
Finally in the third scenario, consider the follow-
ing pair of sentences.
1. ?He speaks a vulgar language.?
2. ?Now that?s real crude behavior!?
The words vulgar and crude occur as synonyms
in the synset that corresponds to the sense ?conspic-
uously and tastelessly indecent?. The synonymous
nature of words can be identified only if they are
looked at as senses and not just words.
As one may observe, the first scenario shows that
a word may have some sentiment-bearing and some
non-sentiment-bearing senses. In the second sce-
nario, we show that there may be different senses
of a word that bear sentiments of opposite polarity.
Finally, in the third scenario, we show how a sense
can be manifested using different words, i.e., words
in a synset. The three scenarios motivate the use of
semantic space for sentiment prediction.
2.2 Sense versus Lexeme-based Feature
Representations
We annotate the words in the corpus with their
senses using two sense disambiguation approaches.
As the first approach, manual sense annotation
of documents is carried out by two annotators on
two subsets of the corpus, the details of which are
given in Section 4.1. The experiments conducted on
this set determine the ideal case scenario- the skyline
performance.
As the second approach, a state-of-art algorithm
for domain-specific WSD proposed by Khapra et
al. (2010) is used to obtain an automatically sense-
tagged corpus. This algorithm called iterative WSD
or IWSD iteratively disambiguates words by rank-
ing the candidate senses based on a scoring function.
The two types of sense-annotated corpus lead us
to four feature representations for a document:
1. A group of word senses that have been manu-
ally annotated (M)
133
2. A group of word senses that have been anno-
tated by an automatic WSD (I)
3. A group of manually annotated word senses
and words (both separately as features) (Sense
+ Words(M))
4. A group of automatically annotated word
senses and words (both separately as features)
(Sense + Words(I))
Our first set of experiments compares the four fea-
ture representations to find the feature representa-
tion with which sentiment classification gives the
best performance. Sense + Words(M) and Sense
+ Words(I) are used to overcome non-coverage of
WordNet for some noun synsets.
3 Similarity Metrics and Unknown Synsets
3.1 Synset Replacement Algorithm
Using WordNet senses provides an opportunity to
use similarity-based metrics for WordNet to reduce
the effect of unknown features. If a synset encoun-
tered in a test document is not found in the training
corpus, it is replaced by one of the synsets present
in the training corpus. The substitute synset is deter-
mined on the basis of its similarity with the synset
in the test document. The synset that is replaced is
referred to as an unseen synset as it is not known to
the trained model.
For example, consider excerpts of two reviews,
the first of which occurs in the training corpus while
the second occurs in the test corpus.
1. ? In the night, it is a lovely city and... ?
2. ? The city has many beautiful hot spots for hon-
eymooners. ?
The synset of ?beautiful? is not present in the train-
ing corpus. We evaluate a similarity metric for all
synsets in the training corpus with respect to the
sense of beautiful and find that the sense of lovely is
closest to it. Hence, the sense of beautiful in the test
document is replaced by the sense of lovely which is
present in the training corpus.
The replacement algorithm is described in
Algorithm 1. The term concept is used in place
of synset though the two essentially mean the
same in this context. The algorithm aims to find a
concept temp concept for each concept in the test
corpus. The temp concept is the concept closest to
some concept in the training corpus based on the
similarity metrics. The algorithm follows from the
fact that the similarity value for a synset with itself
is maximum.
Input: Training Corpus, Test Corpus,
Similarity Metric
Output: New Test Corpus
T:= Training Corpus;
X:= Test Corpus;
S:= Similarity metric;
train concept list = get list concept(T) ;
test concept list = get list concept(X);
for each concept C in test concept list do
temp max similarity = 0 ;
temp concept = C ;
for each concept D in train concept list do
similarity value = get similarity value(C,D,S);
if (similarity value > temp max similarity) then
temp max similarity= similarity value;
temp concept = D ;
end
end
replace synset corpus(C,temp concept,X);
end
Return X ;
Algorithm 1: Synset replacement using similarity
metric
The for loop over C finds a concept temp concept
in the training corpus with the maximum
similarity value. The method replace synset corpus
replaces the concept C in the test corpus with
temp concept in the test corpus X.
3.2 Similarity Metrics Used
We evaluate the benefit of three similarity metrics,
namely LIN?s similarity metric, Lesk similarity
metric and Leacock and Chodorow (LCH) similarity
metric for the synset replacement algorithm stated.
These runs generate three variants of the corpus.
We compare the benefit of each of these metrics by
studying their sentiment classification performance.
The metrics can be described as follows:
LIN: The metric by Lin (1998) uses the infor-
mation content individually possessed by two con-
cepts in addition to that shared by them. The infor-
mation content shared by two concepts A and B is
given by their most specific subsumer (lowest super-
134
ordinate(lso). Thus, this metric defines the similarity
between two concepts as
simLIN (A,B) =
2? logPr(lso(A,B))
logPr(A) + logPr(B)
(1)
Lesk: Each concept in WordNet is defined
through gloss. To compute the Lesk similar-
ity (Banerjee and Pedersen, 2002) between A and
B, a scoring function based on the overlap of words
in their individual glosses is used.
Leacock and Chodorow (LCH): To measure
similarity between two concepts A and B, Leacock
and Chodorow (1998) compute the shortest path
through hypernymy relation between them under the
constraint that there exists such a path. The final
value is computed by scaling the path length by the
overall taxonomy depth (D).
simLCH(A,B) = ? log
(
len(A,B)
2D
)
(2)
4 Experimentation
We describe the variants of the corpus generated and
the experiments in this section.
4.1 Data Preparation
We create different variants of the dataset by Ye et
al. (2009). This dataset contains 600 positive and
591 negative reviews about seven travel destinations.
Each review contains approximately 4-5 sentences
with an average number of words per review being
80-85.
To create the manually annotated corpus, two hu-
man annotators annotate words in the corpus with
senses for two disjoint subsets of the original cor-
pus by Ye et al (2009). The inter-annotation agree-
ment for a subset(20 positive reviews) of the corpus
showed 91% sense overlap. The manually annotated
corpus consists of 34508 words with 6004 synsets.
The second variant of the corpus contains word
senses obtained from automatic disambiguation us-
ing IWSD. The evaluation statistics of the IWSD is
shown in Table 1. Table 1 shows that the F-score for
noun synsets is high while that for adjective synsets
is the lowest among all. The low recall for adjec-
tive POS based synsets can be detrimental to classi-
fication since adjectives are known to express direct
sentiment (Pang et al, 2002).
POS #Words P(%) R(%) F-Score(%)
Noun 12693 75.54 75.12 75.33
Adverb 4114 71.16 70.90 71.03
Adjective 6194 67.26 66.31 66.78
Verb 11507 68.28 67.97 68.12
Overall 34508 71.12 70.65 70.88
Table 1: Annotation Statistics for IWSD; P- Precision,R-
Recall
4.2 Experimental Setup
The experiments are performed using C-SVM (lin-
ear kernel with default parameters1) available as a
part of LibSVM2 package. We choose to use SVM
since it performs the best for sentiment classification
(Pang et al, 2002). All results reported are average
of five-fold cross-validation accuracies.
To conduct experiments on words as features, we
first perform stop-word removal. The words are
not stemmed as per observations by (Leopold and
Kindermann, 2002). To conduct the experiments
based on the synset representation, words in the
corpus are annotated with synset identifiers along
with POS category identifiers. For automatic sense
disambiguation, we used the trained IWSD engine
(trained on tourism domain) from Khapra et al
(2010). These synset identifiers along with POS cat-
egory identifiers are then used as features. For re-
placement using semantic similarity measures, we
used WordNet::Similarity 2.05 package by Pedersen
et al (2004).
To evaluate the result, we use accuracy, F-score,
recall and precision as the metrics. Classification
accuracy defines the ratio of the number of true in-
stances to the total number of instances. Recall is
calculated as a ratio of the true instances found to
the total number of false positives and true posi-
tives. Precision is defined as the number of true
instances divided by number of true positives and
false negatives. Positive Precision (PP) and Posi-
tive Recall (PR) are precision and recall for positive
documents while Negative Precision (NP) and Nega-
tive Recall (NR) are precision and recall for negative
documents. F-score is the weighted precision-recall
1C=0.0,=0.0010
2http://www.csie.ntu.edu.tw/ cjlin/libsvm
135
Feature Representation Accuracy PF NF PP NP PR NR
Words 84.90 85.07 84.76 84.95 84.92 85.19 84.60
Sense (M) 89.10 88.22 89.11 91.50 87.07 85.18 91.24
Sense + Words (M) 90.20 89.81 90.43 92.02 88.55 87.71 92.39
Sense (I) 85.48 85.31 85.65 87.17 83.93 83.53 87.46
Sense + Words(I) 86.08 86.28 85.92 85.87 86.38 86.69 85.46
Table 2: Classification Results; M-Manual, I-IWSD, W-Words, PF-Positive F-score(%), NF-Negative F-score (%),
PP-Positive Precision (%), NP-Negative Precision (%), PR-Positive Recall (%), NR-Negative Recall (%)
score.
5 Results and Discussions
5.1 Comparison of various feature
representations
Table 2 shows results of classification for different
feature representations. The baseline for our results
is the unigram bag-of-words model (Words).
An improvement of 4.2% is observed in the ac-
curacy of sentiment prediction when manually an-
notated sense-based features (M) are used in place
of word-based features (Words). The precision of
both the classes using features based on semantic
space is also better than one based on lexeme space.
Reported results suggest that it is more difficult to
detect negative sentiment than positive sentiment
(Gindl and Liegl, 2008). However, using sense-
based representation, it is important to note that neg-
ative recall increases by around 8%.
The combined model of words and manually an-
notated senses (Sense + Words (M)) gives the best
performance with an accuracy of 90.2%. This leads
to an improvement of 5.3% over the baseline accu-
racy 3.
One of the reasons for improved performance is
the feature abstraction achieved due to the synset-
based features. The dimension of feature vector is
reduced by a factor of 82% when the document is
represented in synset space. The reduction in dimen-
sionality may also lead to reduction in noise (Cun-
ningham, 2008).
A comparison of accuracy of different sense rep-
resentations in Table 2 shows that manual disam-
3The improvement in results of semantic space is found to
be statistically significant over the baseline at 95% confidence
level when tested using a paired t-test.
biguation performs better than using automatic al-
gorithms like IWSD. Although overall classification
accuracy improvement of IWSD over baseline is
marginal, negative recall also improves. This bene-
fit is despite the fact that evaluation of IWSD engine
over manually annotated corpus gave an overall F-
score of 71% (refer Table 1). For a WSD engine
with a better accuracy, the performance of sense-
based SA can be boosted further.
Thus, in terms of feature representation of docu-
ments, sense-based features provide a better overall
performance as compared to word-based features.
5.2 Synset replacement using similarity metrics
Table 3 shows the results of synset replacement ex-
periments performed using similarity metrics de-
fined in section 3. The similarity metric value NA
shown in the table indicates that synset replacement
is not performed for the specific run of experiment.
For this set of experiments, we use the combina-
tion of sense and words as features (indicated by
Senses+Words (M)).
Synset replacement using a similarity metric
shows an improvement over using words alone.
However, the improvement in classification accu-
racy is marginal compared to sense-based represen-
tation without synset replacement (Similarity Met-
ric=NA).
Replacement using LIN and LCH metrics gives
marginally better results compared to the vanilla set-
ting in a manually annotated corpus. The same phe-
nomenon is seen in the case of IWSD based ap-
proach4. The limited improvement can be due to
the fact that since LCH and LIN consider only IS-A
4Results based on LCH and LIN similarity metric for auto-
matic sense disambiguation is not statistically significant with
?=0.05
136
Features Representa-
tion
SM A PF NF
Words (Baseline) NA 84.90 85.07 84.76
Sense+Words (M) NA 90.20 89.81 90.43
Sense+Words (I) NA 86.08 86.28 85.92
Sense+Words (M) LCH 90.60 90.20 90.85
Sense+Words (M) LIN 90.70 90.26 90.97
Sense+Words (M) Lesk 91.12 90.70 91.38
Sense+Words (I) LCH 85.66 85.85 85.52
Sense+Words (I) LIN 86.16 86.37 86.00
Sense+Words (I) Lesk 86.25 86.41 86.10
Table 3: Similarity Metric Analysis using different
similarity metrics with synsets and a combinations of
synset and words; SM-Similarity Metric, A-Accuracy,
PF-Positive F-score(%), NF-Negative F-score (%)
relationship in WordNet, the replacement happens
only for verbs and nouns. This excludes adverb
synsets which we have shown to be the best features
for a sense-based SA system.
Among all similarity metrics, the best classifica-
tion accuracy is achieved using Lesk. The system
performs with an overall classification accuracy of
91.12%, which is a substantial improvement of 6.2%
over baseline. Again, it is only 1% over the vanilla
setting that uses combination of synset and words.
However, the similarity metric is not sophisticated as
LIN or LCH. A good metric which covers all POS
categories can provide substantial improvement in
the classification accuracy.
6 Related Work
This work deals with studying benefit of a word
sense-based feature space to supervised sentiment
classification. This work assumes the hypothesis
that word sense is associated with the sentiment as
shown by Wiebe and Mihalcea (2006) through hu-
man interannotator agreement.
Akkaya et al (2009) and Martn-Wanton et al
(2010) study rule-based sentiment classification us-
ing word senses where Martn-Wanton et al (2010)
uses a combination of sentiment lexical resources.
Instead of a rule-based implementation, our work
leverages on benefits of a statistical learning-based
methods by using a supervised approach. Rentoumi
et al (2009) suggest an approach to use word senses
to detect sentence level polarity using graph-based
similarity. While Rentoumi et al (2009) targets us-
ing senses to handle metaphors in sentences, we deal
with generating a general-purpose classifier.
Carrillo de Albornoz et al (2010) create an emo-
tional intensity classifier using affective class con-
cepts as features. By using WordNet synsets as fea-
tures, we construct feature vectors that map to a
larger sense-based space.
Akkaya et al (2009), Martn-Wanton et al (2010)
and Carrillo de Albornoz et al (2010) deal with
sentiment classification of sentences. On the other
hand, we associate sentiment polarity to a document
on the whole as opposed to Pang and Lee (2004)
which deals with sentiment prediction of subjectiv-
ity content only. Carrillo de Albornoz et al (2010)
suggests expansion using WordNet relations which
we perform in our experiments.
7 Conclusion & Future Work
We present an empirical study to show that sense-
based features work better as compared to word-
based features. We show how the performance im-
pact differs for different automatic and manual tech-
niques. We also show the benefit using WordNet
based similarity metrics for replacing unknown fea-
tures in the test set. Our results support the fact that
not only does sense space improve the performance
of a sentiment classification system but also opens
opportunities for building robust sentiment classi-
fiers that can handle unseen synsets.
Incorporation of syntactical information along
with semantics can be an interesting area of
work. Another line of work is in the context of
cross-lingual sentiment analysis. Current solutions
are based on machine translation which is very
resource-intensive. Using a bi-lingual dictionary
which maps WordNet across languages can prove to
be an alternative.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity word sense disambiguation. In Proc. of
EMNLP ?09, pages 190?199, Singapore.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
lesk algorithm for word sense disambiguation using
wordnet. In Proc. of CICLing?02, pages 136?145,
London, UK.
137
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gervs. 2010. Improving emotional intensity clas-
sification using word sense disambiguation. Special
issue: Natural Language Processing and its Appli-
cations. Journal on Research in Computing Science,
46:131?142.
Pdraig Cunningham. 2008. Dimension reduction. In
Machine Learning Techniques for Multimedia, Cogni-
tive Technologies, pages 91?112.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Stefan Gindl and Johannes Liegl, 2008. Evaluation of
different sentiment detection methods for polarity clas-
sification on web-based reviews, pages 35?43.
Alistair Kennedy and Diana Inkpen. 2006. Sentiment
classification of movie reviews using contextual va-
lence shifters. Computational Intelligence, 22(2):110?
125.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak
Bhattacharyya. 2010. Domain-specific word sense
disambiguation combining corpus basedand wordnet
based parameters. In Proc. of GWC?10, Mumbai, In-
dia.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context with wordnet similarity for word
sense identification. In WordNet: A Lexical Reference
System and its Application.
Edda Leopold and Jo?rg Kindermann. 2002. Text catego-
rization with support vector machines. how to repre-
sent texts in input space? Machine Learning, 46:423?
444.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In In Proc. of the 15th International Con-
ference on Machine Learning, pages 296?304.
Tamara Martn-Wanton, Alexandra Balahur-Dobrescu,
Andres Montoyo-Guijarro, and Aurora Pons-Porrata.
2010. Word sense disambiguation in opinion mining:
Pros and cons. In Proc. of CICLing?10, Madrid,Spain.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. In Proc.
of PAKDD?05,, Lecture Notes in Computer Science,
pages 301?311.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of ACL?04, pages
271?278, Barcelona, Spain.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2:1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. volume 10, pages 79?86.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the relat-
edness of concepts. In Demonstration Papers at HLT-
NAACL?04, pages 38?41.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Sen-
timent analysis of figurative language using a word
sense disambiguation approach. In Proc. of the In-
ternational Conference RANLP?09, pages 370?375,
Borovets, Bulgaria.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL?02, pages 417?424,
Philadelphia, US.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proc. of CIKM ?05, pages 625?631, New York, NY,
USA.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proc. of COLING-ACL?06, pages
1065?1072.
Qiang Ye, Ziqiong Zhang, and Rob Law. 2009. Senti-
ment classification of online reviews to travel destina-
tions by supervised machine learning approaches. Ex-
pert Systems with Applications, 36(3):6527 ? 6535.
138
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 142?146,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
A cognitive study of subjectivity extraction in sentiment annotation
Abhijit Mishra
1
Aditya Joshi
1,2,3
Pushpak Bhattacharyya
1
1
IIT Bombay, India
2
Monash University, Australia
3
IITB-Monash Research Academy, India
{abhijitmishra, adityaj, pb}@cse.iitb.ac.in
Abstract
Existing sentiment analysers are weak AI
systems: they try to capture the function-
ality of human sentiment detection faculty,
without worrying about how such faculty
is realized in the hardware of the human.
These analysers are agnostic of the actual
cognitive processes involved. This, how-
ever, does not deliver when applications
demand order of magnitude facelift in ac-
curacy, as well as insight into characteris-
tics of sentiment detection process.
In this paper, we present a cognitive study
of sentiment detection from the perspec-
tive of strong AI. We study the sentiment
detection process of a set of human ?sen-
timent readers?. Using eye-tracking, we
show that on the way to sentiment de-
tection, humans first extract subjectivity.
They focus attention on a subset of sen-
tences before arriving at the overall senti-
ment. This they do either through ?antici-
pation? where sentences are skipped dur-
ing the first pass of reading, or through
?homing? where a subset of the sentences
are read over multiple passes, or through
both. ?Homing? behaviour is also ob-
served at the sub-sentence level in com-
plex sentiment phenomena like sarcasm.
1 Introduction
Over the years, supervised approaches using
polarity-annotated datasets have shown promise
for SA (Pang and Lee, 2008). However, an al-
ternate line of thought has co-existed. Pang and
Lee (2004) showed that for SA, instead of a doc-
ument in its entirety, an extract of the subjec-
tive sentences alone can be used. This process
of generating a subjective extract is referred to
as subjectivity extraction. Mukherjee and Bhat-
tacharyya (2012) show that for sentiment predic-
tion of movie reviews, subjectivity extraction may
be used to discard the sentences describing movie
plots since they do not contribute towards the
speaker?s view of the movie.
While subjectivity extraction helps sentiment
classification, the reason has not been sufficiently
examined from the perspective of strong AI. The
classical definition of strong AI suggests that a
machine must be perform sentiment analysis in
a manner and accuracy similar to human beings.
Our paper takes a step in this direction. We study
the cognitive processes underlying sentiment an-
notation using eye-fixation data of the participants.
Our work is novel in two ways:
? We view documents as a set of sentences
through which sentiment changes. We show
that the nature of these polarity oscillations
leads to changes in the reading behavior.
? To the best of our knowledge, the idea of us-
ing eye-tracking to validate assumptions is
novel in case of sentiment analysis and many
NLP applications.
2 Sentiment oscillations & subjectivity
extraction
We categorize subjective documents as linear and
oscillating. A linear subjective document is the
one where all or most sentences have the same po-
larity. On the other hand, an oscillating subjective
document contains sentences of contrasting polar-
ity (viz. positive and negative). Our discussions
on two forms of subjectivity extraction use the
concepts of linear and oscillating subjective doc-
uments.
Consider a situation where a human reader
needs to annotate two documents with sentiment.
Assume that the first document is linear subjec-
tive - with ten sentences, all of them positive. In
142
case of this document, when he/she reads a cou-
ple of sentences with the same polarity, he/she be-
gins to assume that the next sentence will have the
same sentiment and hence, skips through it. We
refer to this behavior as anticipation. Now, let the
second document be an oscillating subjective doc-
ument with ten sentences, the first three positive,
the next four negative and the last three positive.
In this case, when a human annotator reads this
document and sees the sentiment flip early on, the
annotator begins to carefully read the document.
After completing a first pass of reading, the anno-
tator moves back to read certain crucial sentences.
We refer to this behavior as homing.
The following sections describe our observa-
tions in detail. Based on our experiments, we ob-
serve these two kinds of subjectivity extraction in
our participants: subjectivity extraction as a result
of anticipation and subjectivity extraction as a re-
sult of homing - for linear and oscillating docu-
ments respectively.
3 Experiment Setup
This section describes the framework used for our
eye-tracking experiment. A participant is given
the task of annotating documents with one out of
the following labels: positive, negative and ob-
jective. While she reads the document, her eye-
fixations are recorded.
To log eye-fixation data, we use Tobii T120
remote eye-tracker with Translog(Carl, 2012).
Translog is a freeware for recording eye move-
ments and keystrokes during translation. We con-
figure Translog for reading with the goal of senti-
ment.
3.1 Document description
We choose three movie reviews in English from
IMDB (http://www.imdb.com) and indicate them
as D0, D1 and D2. The lengths of D0, D1 and
D2 are 10, 9 and 13 sentences respectively. Using
the gold-standard rating given by the writer, we
derive the polarity of D0, D1 and D2 as positive,
negative and positive respectively. The three doc-
uments represent three different styles of reviews:
D0 is positive throughout (linear subjective), D1
contains sarcastic statements (linear subjective but
may be perceived as oscillating due to linguistic
difficulty) while D2 consists of many flips in sen-
timent (oscillating subjective).
It may seem that the data set is small and
may not lead to significant findings. However,
we wished to capture the most natural form of
sentiment-oriented reading. A larger data set
would have weakened the experiment because: (i)
Sentiment patterns (linear v/s subjective) begin to
become predictable to a participant if she reads
many documents one after the other. (ii) There
is a possibility that fatigue introduces unexpected
error. To ensure that our observations were signif-
icant despite the limited size of the data set, we
increased the number of our participants to 12.
3.2 Participant description
Our participants are 24-30 year-old graduate stu-
dents with English as the primary language of aca-
demic instruction. We represent them as P0, P1
and so on. The polarity for the documents as re-
ported by the participants are shown in Table 1.
All participants correctly identified the polarity of
document D0. Participant P9 reported that D1 is
confusing. 4 out of 12 participants were unable to
detect correct opinion in D2.
3.3 Experiment Description
We obtain two kinds of annotation from our an-
notators: (a) sentiment (positive, negative and ob-
jective), (b) eye-movement as recorded by an eye-
tracker. They are given a set of instructions before-
hand and can seek clarifications. This experiment
is conducted as follows:
1. A complete document is displayed on the
screen. The font size and line separation are
set to 17pt and 1.5 cm respectively to ensure
clear visibility and minimize recording error.
2. The annotator verbally states the sentiment of
this sentence, before (s)he can proceed to the
next.
3. While the annotator is reading the sentence,
a remote eye-tracker (Model: Tobii TX 300,
Sampling rate: 300Hz) records the eye-
movement data of the annotator. The eye-
tracker is linked to Translog II software (Carl,
2012) in order to record the data. A snap-
shot of the software is shown in figure 1. The
dots and circles represent position of eyes and
fixations of the annotator respectively. Each
eye-fixation that is recorded consists of: co-
ordinates, timestamp and duration. These
three parameters have been used to generate
sentence progression graphs.
143
Document Orig P0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11
D0 +ve +ve +ve +ve +ve +ve +ve +ve +ve +ve +ve +ve +ve
D1 -ve -ve +ve -ve -ve -ve -ve -ve -ve -ve Neu/-ve -ve -ve
D2 +ve +ve +ve -ve +ve +ve Neu +ve Neu Neu +ve +ve +ve
Table 1: Polarity of documents as perceived by the writer (original) and the participants +ve, -ve and
Neu represent positive, negative and neutral polarities respectively.
Figure 1: Gaze-data recording using Translog-II
Figure 2: Sentence progression graph for partici-
pant P7 document D0
4 Observations: Subjectivity extraction
through anticipation
In this section, we describe a case in which partic-
ipants skip sentences. We show that anticipation
of sentiment is linked with subjectivity extraction.
Table 2 shows the number of unique and non-
unique sentences that participants read for each
document. The numbers in the last column in-
dicate average values. The table can be read as:
participant P1 reads 8 unique sentences of docu-
ment D0 (thus skipping two sentences) and includ-
ing repetitions, reads 26 sentences. Participant P0
skips as many as six sentences in case of document
D1.
The number of unique sentences read is lower
than sentence count for four out of twelve partic-
ipants in case of document D0. This skipping is
negligible in case of document D1 and D2. Also,
the average non-unique sentence fixations are 21
in case of D0 and 33.83 for D1 although the total
number of sentences in D0 and D1 is almost the
same. This verifies that participants tend to skip
sentences while reading D0.
Figure 2 shows sentence progression graph for
participant P7. The participant reads a series of
sentences and then skips two sentences. This im-
plies that anticipation behaviour was triggered af-
ter reading sentences of the same polarity. Sim-
ilar traits are observed in other participants who
skipped sentences while reading document D0.
5 Observations: Subjectivity extraction
through homing
This section presents a contrasting case of sub-
jectivity extraction. We refer to a reading pattern
as homing
1
when a participant reads a document
completely and returns to read a selected subset of
sentences. We believe that during sentiment an-
notation, this subset is the subjective extract that
the user has created in her mind. We observe this
phenomenon in reading patterns of documents D1
and D2. The former contains sarcasm because of
which parts of sentences may appear to be of con-
trasting polarity while the latter is an oscillating
subjective document.
1
The word is derived from missile guidance systems. The
definition
2
of homing is ?the process of determining the lo-
cation of something, sometimes the source of a transmission,
and going to it.?
144
Document P0 P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 Avg.
D0
Non-unique 9 26 23 17 18 18 35 16 33 19 15 23 21
Unique 8 8 10 10 10 10 10 8 10 8 10 10
D1
Non-unique 5 23 46 13 15 44 35 26 56 57 40 46 33.83
Unique 3 9 9 9 9 9 8 9 9 9 9 9
D2
Non-unique 36 29 67 21 23 51 64 48 54 59 73 80 50.42
Unique 13 13 13 13 13 13 13 13 13 13 13 13
Table 2: Number of unique and non-unique sentences read by each participant
Figure 3: Sentence progression graph of partici-
pant P2 for document D1 (left) and document D2
(right)
Figure 3 shows sentence progression graphs of
participant P2 for documents D1 and D2. For doc-
ument D1, the participant performs one pass of
reading until sequence number 30. A certain sub-
set of sentences are re-visited in the second pass.
On analyzing sentences in the second pass of read-
ing, we observe a considerable overlap in case of
our participants. We also confirm that all of these
sentences are subjective. This means that the sen-
tences that are read after sequence number 30 form
the subjective extract of document D1.
Similar behaviour is observed in case of docu-
ment D2. The difference in this case is that there
is less overlap of sentences read in the second pass
among participants. This implies , for oscillat-
ing subjective documents, the subjective extract is
user/document-specific.
It may be argued that fixations corresponding
Participant TFD-SE PTFD TFC-SE
(secs) (%)
P5 7.3 8 21
P7 3.1 5 11
P9 51.94 10 26
P11 116.6 16 56
Table 3: Reading statistics for second pass reading
for document D1; TFD: Total fixation duration for
subjective extract; PTFD: Proportion of total fix-
ation duration = (TFD)/(Total duration); TFC-SE:
Total fixation count for subjective extract
to second pass reading are stray fixations and not
subjective extracts. Hence, for the second pass
reading of document D1, we tabulate fixation du-
ration, fixation count and proportion of total dura-
tion in Table 3. The fixation duration and fixation
count are both recorded by the eye-tracker. The
fixation counts are substantial and the participants
spend around 5-15% of the total reading time in
the second pass reading. We also confirm that all
of these sentences are subjective. This means that
these portions indeed correspond to subjective ex-
tracts as a result of homing.
6 A note on linguistic challenges
Our claim is that regression after reading an en-
tire document corresponds to the beginning of
a subjective extract. However, we observe that
some regressions may also happen due to senti-
ment changes at the sub-sentence level. Some of
these are as follows.
1. Sarcasm: Sarcasm involves an implicit flip
in the sentiment. Participant P9 does not cor-
rectly predict sentiment of Document D1. On
analyzing her data, we observe multiple re-
gressions on the sentence ?Add to this mess
some of the cheesiest lines and concepts, and
145
there you have it; I would call it a complete
waste of time, but in some sense it is so bad
it is almost worth seeing.? This sentence has
some positive words but is negative towards
the movie. Hence, the participant reads this
portion back and forth.
2. Thwarted expectations: Thwarted expecta-
tions are expressions with a sentiment rever-
sal within a sentence/snippet. Homing is ob-
served in this case as well. Document D2
has a case of thwarted expectations from sen-
tences 10-12 where there is an unexpected
flip of sentiment. In case of some partici-
pants, we observe regression on these sen-
tences multiple times.
7 Related Work
The work closest to ours is by Scott et al. (2011)
who study the role of emotion words in read-
ing using eye-tracking. They show that the eye-
fixation duration for emotion words is consistently
less than neutral words with the exception of high-
frequency negative words. Eye-tracking
3
technol-
ogy has also been used to study the cognitive as-
pects of language processing tasks like translation
and sense disambiguation. Dragsted (2010) ob-
serve co-ordination between reading and writing
during human translation. Similarly, Joshi et al.
(2011) use eye-tracking to correlate fixation dura-
tion with polysemy of words during word sense
disambiguation.
8 Conclusion & Future work
We studied sentiment annotation in the context of
subjectivity extraction using eye-tracking. Based
on how sentiment changes through a document,
humans may perform subjectivity extraction as a
result of either: (a) anticipation or (b) homing.
These observations are in tandem with the past
work that shows benefit of subjectivity extraction
for automatic sentiment classification.
Our study is beneficial in three perspectives: (i)
Sentiment classifiers may use interaction between
sentiment of sentences. Specifically, this can be
modeled using features like sentiment run length
(i.e. maximal span of sentences bearing same
3
Related Terms:
Eye-fixation: Long stay of visual gaze on a single location
Regression: Revisiting a previously read segment
Sentence Progression Graph: Graph showing reading se-
quence of sentences
sentiment) or sentiment flips (i.e. instances where
consecutive sentences bear opposite polarity),
(ii) Crowd-sourced sentiment annotation can
devise variable pricing models based on our study.
Based on anticipation and homing information
about documents, documents can be grouped into
difficulty categories and priced accordingly.
Acknowledgment
We thank Tobii Corporation for lending us their
eye-tracker for this study, and our annotators from
CFILT, IIT Bombay. Aditya is funded by the TCS
Research Fellowship Program.
References
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Sum-
marization Based on Minimum Cuts In Proceedings
of the ACL, 271-278.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis Foundations and Trends in In-
formation Retrieval, 2008, vol. 2, nos.12 1135.
B Dragsted. 2010. Co-ordination of reading and writ-
ing processes in translation. Contribution to Trans-
lation and Cognition. Shreve, G. and Angelone,
E.(eds.)Cognitive Science Society.
Michael Carl. 2012. Translog-II: A Program for
Recording User Activity Data for Empirical Reading
and Writing Research. In Proceedings of the Eight
International Conference on Language Resources
and Evaluation, European Language Resources As-
sociation.
Scott G. , ODonnell P and Sereno S. 2012. Emotion
Words Affect Eye Fixations During Reading. Jour-
nal of Experimental Psychology:Learning, Memory,
and Cognition 2012, Vol. 38, No. 3, 783792.
Salil Joshi, Diptesh Kanojia and Pushpak Bhat-
tacharyya. 2013. More than meets the eye: Study
of Human Cognition in Sense Annotation. NAACL
HLT 2013, Atlanta, USA.
Subhabrata Mukherjee and Pushpak Bhattacharyya.
2012. WikiSent : Weakly Supervised Senti-
ment Analysis Through Extractive Summarization
With Wikipedia European Conference on Machine
Learning (ECML PKDD 2012), Bristol, U.K.,
146
