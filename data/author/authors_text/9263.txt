Automatic Extraction of Subcategorization Frames for Czech* 
Anoop Sarkar 
CIS Dept, Univ of Pennsylvania 
200 South 33rd Street, 
Philadelphia, PA 19104 USA 
anoop@linc, cis. upenn, edu 
Daniel Zeman 
0stay formflnf a aplikovand lingvistiky 
Univerzita Karlova 
Praha, Czechia 
zeman@ufa l  .mf f .  cun i .  cz  
Abstract 
We present some novel nmchine learning techniques 
for the identilication of subcategorization infornm- 
tion for verbs in Czech. We compare three different 
statistical techniques applied to this problem. We 
show how the learning algorithm can be used to dis- 
cover previously unknown subcategorization frames 
from the Czech Prague 1)ependency Treebank. The 
algorithm can then be used to label dependents of 
a verb in the Czech treebank as either arguments 
or adjuncts. Using our techniques, we are able to 
achieve 88% precision on unseen parsed text. 
1 Introduction 
Tl-te subcategorization f verbs is an essential is- 
sue in parsing, because it helps disambiguate the 
attachment of arguments and recover the correct 
predicate-argument relations by a parser. (CmToll 
and Minnen, 1998; CmToll and Rooth, 1998) give 
several reasons why subcategorization information 
is important for a natural anguage parser. Machine- 
readable dictionaries are not comprehensive enough 
to provide this lexical infornaation (Manning, 1993; 
Briscoe and Carroll, 1997). Furthermore, such dic- 
tionaries are available only for very few languages. 
We need some general method for the automatic ex- 
traction of subcategorization information from text 
corpora.  
Several techniques and results have been reported 
on learning subcategorization frames (SFs) from 
text corpora (Webster and Marcus, 1989; Brent, 
1991; Brent, 1993; Brent, 1994; Ushioda et al, 
1993; Manning, 1993; Ersan and Charniak, 1996; 
Briscoe and Carroll, 1997; Carroll and Minnen, 
1998; Carroll and Rooth, 1998). All of this work 
" Tiffs work was done during the second author's visit to tl~e 
University of Pennsylvania. We would like to thank Prof. Ar- 
avind Joshi, l)avid Chiang, Mark l)ras and the anonymous re- 
viewers for their comments. The first at,thor's work is partially 
supported by NS F Grant S BR 8920230. Many tools used in this 
work are the resuhs of project No. VS96151 of the Ministry of 
Education of the Czech Republic. The data (PDT) is thanks 
to grant No. 405/96/K214 of the Grant Agency of the Czech 
Republic. Both grants were given to the Institute of Fornml 
and Applied linguistics, Faculty of Mathenmtics and Physics, 
Charles University, Prague. 
deals with English. In this paper we report on 
techniques that automatically extract SFs for Czech, 
which is a flee word-order language, where verb 
complements have visible case marking.I 
Apart from the choice of target language, this 
work also differs from previous work in other ways. 
Unlike all other previous work in this area, we do 
not assume that the set of SFs is known to us in ad- 
vance. Also in contrast, we work with syntactically 
annotated ata (the Prague Dependency Treebank, 
PDT (HajiC 1998)) where the subcategorization in-
formation is not  given; although this might be con? 
sidered a simpler problem as compared to using raw 
text, we have discovered interesting problems that a 
user of a raw or tagged corpus is unlikely to face. 
We first give a detailed description of the task 
of uncovering SFs and also point out those prop- 
erties of Czech that have to be taken into account 
when searching lbr SFs. Then we discuss some dif-. 
ferences fl'Oln the other research efforts. We then 
present he three techniques that we use to learn SFs 
from the input data. 
In the input data, many observed ependents of 
the verb are adjuncts. To treat this problem effec- 
tively, we describe a novel addition to the hypoth- 
esis testing technique that uses subset of observed 
fl'ames to permit he learning algorithm to better dis- 
tinguish arguments fl-om adjtmcts. 
Using our techniques, we arc able to achieve 88% 
precision in distinguishing argunaents from adjuncts  
on unseen parsed text. 
2 lhsk Description 
In this section we describe precisely the proposed 
task. We also describe the input training material 
and the output produced by our algorithms. 
2.1 Identifying subeategorization frames 
Ill general, the problem of identifying subcatego- 
rization fi-ames is to distinguish between arguments 
and adjuncts among the constituents modifying a 
IOI/c of the ammymous rcviewcrs pointed out that (Basili 
and Vindigni. 1908) presents a corpus-driven acquisition of 
subcategorization frames for Italian. 
691 
f N4 R2(od) {2} 
N4 R2(od) R2(do) _ R2(od) R2(do) {0~j~/  
{~}~. .~ N4 R2(do){0} q~"--_//...~ 
N4 R6(v) R6(na) { 11 ~ N4 R6(v) l I} ~/ / f f _  
N4 R6(po) {1 } /-----------~" 
R2(od) {0} 
R2(do) {0} 
R6(v) {0} 
R6(na) {0} 
R6(po) {0 } 
N4 {2+1+1} 
empty 10} 
Figure 2: Computing the subsets of observed frames for tile verb absoh,ovat. The counts for each frame are 
given within braces {}. In this example, the frames N4 R2(od), N4 R6(v) and N4 R6(po) have been observed 
with other verbs in the corpus. Note that the counts in this figure do not correspond to the real counts for the 
verb absoh,ovat in the training corpus. 
where c(.) are counts in the training data. Using 
the values computed above: 
Pl -- 
7tl 
k2 
P2 --= - -  
77, 2 
k l  +k2  p -- 
7z 1 .-\]- 'It 2 
Taking these probabilities to be binomially dis- 
tributed, the log likelihood statistic (Dunning, 1993) 
is given by: 
- 2 log A = 
2\[log L(pt, k:l, rtl) @ log L(p2, k2, rl,2) -- 
log L(p, kl, n2) - log L(p, k2, n2)\] 
where, 
log L(p, n, k) = k logp + (,z -- k)log(1 - p) 
According to this statistic, tile greater the value of 
-2  log A for a particular pair of observed frame and 
verb, the more likely that frame is to be valid SF of 
the verb. 
3.2 T-scores 
Another statistic that has been used for hypothesis 
testing is the t-score. Using tile definitions from 
Section 3.1 we can compute t-scores using the equa- 
tion below and use its value to measure the associa- 
tion between a verb and a frame observed with it. 
T = Pl - P2 
where, 
p)  = , p(1 - 
In particular, the hypothesis being tested using 
the t-score is whether the distributions Pi and P2 
are not independent. If the value of T is greater 
than some threshold then the verb v should take the 
frame f as a SF. 
3.3 B inomia l  Mode ls  o f  M iscue  Probabi l i t ies  
Once again assuming that the data is binomially dis- 
tributed, we can look for fiames that co-occur with a 
verb by exploiting the miscue probability: the prob- 
ability of a frame co-occuring with a verb when it 
is not a valid SF. This is the method used by several 
earlier papers on SF extraction starting with (Brent, 
1991; Brent, 1993; Brent, 1994). 
Let us consider probability PU which is the prob- 
ability that a given verb is observed with a fiame but 
this frame is not a valid SF for this verb. p!f is the 
error probability oil identifying a SF for a verb. Let 
us consider a verb v which does not have as one of 
its valid SFs the frame f .  How likely is it that v will 
be seen 'm, or more times in the training data with 
fi'ame f?  If v has been seen a total of n times ill the 
data, then H*(p!f; m, 7z) gives us this likelihood. 
/ x 
H'(p,f~,n,)'L) = ~__,pif(1 t" )n - i (  ~" ) 
- " f  i 
i=rn. X / 
If H*(p; rn, n) is less than or equal to some small 
threshold value then it is extremely unlikely that the 
hypothesis is tree, and hence the frame f must be 
a SF of tile verb v. Setting the threshold value to 
0.0,5 gives us a 95% or better contidence value that 
the verb v has been observed often enough with a 
flame f for it to be a valid SE 
Initially, we consider only the observed fnnnes 
(OFs) from the treebank. There is a chance that 
some are subsets of some others but now we count 
only tile cases when the OFs were seen themselves. 
Let's assume the test statistic reiected the flame. 
Then it is not a real SF but there probably is a sub- 
set of it that is a real SE So we select exactly one of 
694 
tile subsets whose length is one member less: this 
is the successor of the rejected flame and inherits 
its frequency. Of course one frame may be suc- 
cessor of several onger frames and it can have its 
own count as OF. This is how frequencies accumu- 
late and frames become more likely to survive. The 
exalnple shown in Figure 2 illustrates how the sub- 
sets and successors are selected. 
An important point is the selection of the succes- 
sor. We have to select only one of the ~t possible 
successors of a flame of length 7z, otherwise we 
would break tile total frequency of the verb. Sup- 
pose there is m rejected flames of length 7z. "Ellis 
yields m * n possible modifications to consider be- 
fore selection of the successor. We implemented 
two methods for choosing a single successor flame: 
1. Choose the one that results in the strongest 
preference for some frame (that is, the succes- 
sor flmne results in the lowest entropy across 
the corpus). This measure is sensitive to the 
frequency of this flame in the rest of corpus. 
2? Random selection of the successor frame from 
the alternatives. 
Random selection resulted in better precision 
(88% instead of 86%). It is not clear wily a method 
that is sensitive to the frequency of each proposed 
successor frame does not perform better than ran- 
dom selection. 
The technique described here may sometimes re- 
sult in subset of a correct SF, discarding one or more 
of its members. Such frame can still hel ) parsers be- 
cause they can at least look for the dependents that 
have survived. 
4 Evaluation 
For the evalnation of the methods described above 
we used the Prague l)ependency Treebank (PI)T). 
We used 19,126 sentences of training data from tile 
PDT (about 300K words). In this training set, there 
were 33,641 verb tokens with 2,993 verb types. 
There were a total of 28,765 observed fiames (see 
Section 2.1 for exphmation of these terms). There 
were 914 verb types seen 5 or more times. 
Since there is no electronic valence dictionary for 
Czech, we evaluated our tiltering technique on a set 
of 500 test sentences which were unseen and sep- 
arate flom the training data. These test sentences 
were used as a gold standard by distinguishing the 
arguments and adjuncts manually. We then com- 
pared the accuracy of our output set of items marked 
as either arguments or adjuncts against this gold 
standard. 
First we describe the baseline methods. Base- 
line method 1: consider each dependent of a verb 
an adjunct. Baseline method 2: use just the longest 
known observed frame matching the test pattern. If 
no matching OF is known, lind the longest partial 
match in the OFs seen in the training data. We ex- 
ploit the functional and morphological tags while 
matching. No statistical filtering is applied in either 
baseline method. 
A comparison between all three methods that 
were proposed in this paper is shown in Table 1. 
The experiments howed that the method im- 
proved precision of this distinction flom 57% to 
88%. We were able to classify as many as 914 verbs 
which is a number outperlormed only by Manning, 
with 10x more data (note that our results arc for a 
different language). 
Also, our method discovered 137 subcategoriza- 
tion frames from the data. The known upper bound 
of frames that the algorithm could have found (the 
total number of the obsem, edframe types) was 450. 
5 Comparison with related work 
Preliminary work on SF extraction from coq~ora 
was done by (Brent, 1991; Brunt, 1993; Brent, 
1994) and (Webster and Marcus, 1989; Ushioda et 
al., 1993). Brent (Brent, 1993; Brent, 1994) uses the 
standard method of testing miscue probabilities for 
filtering frames observed with a verb. (Brent, 1994) 
presents a method lbr estimating 1)7. Brent applied 
his method to a small number of verbs and asso- 
ciated SF types. (Manning, 1993) applies Brent's 
method to parsed data and obtains a subcategoriza- 
tion dictionary for a larger set of verbs. (Briscoe 
and Carroll, 1997; Carroll and Minnen, 1998) dif- 
fers from earlier work in that a substantially larger 
set of SF types are considered; (Canoll and Rooth, 
1998) use an EM algorithm to learn subcategoriza- 
tion as a result of learning rule probabilities, and, in 
tnrn, to improve parsing accuracy by applying the 
verb SFs obtained. (Basili and Vindigni, 1998) use 
a conceptual clustering algorithm for acquiring sub- 
categorization fl'ames for Italian. They establish a 
partial order on partially overlapping OFs (similar 
to our Ot: subsets) which is then used to suggest a 
potential SF. A complete comparison of all the pre- 
vious approaches with tile current work is given in 
Table 2. 
While these approaches differ in size and quality 
of training data, number of SF types (e.g. intran- 
sitive verbs, transitive verbs) and number of verbs 
processed, there are properties that all have in con> 
mon. They all assume that they know tile set of pos- 
sible SF types in advance. Their task can be viewed 
as assigning one or more of the (known) SF types 
to a given verb. In addition, except for (Briscoe and 
Carroll, 1997; Carroll and Minnen, 1998), only a 
small number of SF types is considered. 
695 
Baseline Lik. Ratio q-scores Hyp. Testing 
Precision 55% 82% 82% 88% 
Recall: 55% 77% 77% 74% 
_h'f~: l 55% 79% 79% 80% 
% unknown 0% 6% 6% 16% 
Total verb nodes 
Total complements 
Nodes with known verbs 
Complements of known verbs 
Correct Suggestions 
True Arguments 
Suggested Arguments 
Incorrect arg suggestions 
Incorrect adj suggestions 
1027 
2144 
1027 
2144 
1187.5 
956.5 
0 
0 
956.5 
1 Baseline 2 
78% 
73% 
75% 
6% 
1027 
2144 
981 
2010 
1573.5 
910.5 
1122 
324 
112.5 
1027 
2144 
981 
2010 
1642.5 
910.5 
974 
215.5 
152 
1027 
2144 
981 
2010 
1652.9 
910.5 
1026 
236.3 
120.8 
1027 
2144 
907 
1812 
1596.5 
834.5 
674 
27.5 
188 
Table 1: Comparison between the baseline methods and the three methods proposed in this paper. Some of 
the values are not integers ince for some difficult cases in the test data, the value for each argument/adjunct 
decision was set to a value between \[0, 1\]. Recall is computed as the number of known verb complements 
divided by the total number of complements. Precision is computed as the number of correct suggestions 
divided by the number of known verb complements. Ffl=l = (2 x p x r)/(p + r). % unknown represents 
the percent of test data not considered by a particular method. 
Using a dependency treebank as input to our 
learning algorithm has both advantages and draw- 
backs. There are two main advantages of using a 
treebank: 
? Access to more accurate data. Data is less 
noisy when compared with tagged or parsed in- 
put data. We can expect correct identification 
of verbs and their dependents. 
? We can explore techniques (as we have done in 
this paper) that try and learn the set of SFs from 
the data itself, unlike other approaches where 
the set of SFs have to be set in advance. 
Also, by using a treebank we can use verbs in dif- 
ferent contexts which are problematic for previous 
approaches, e.g. we can use verbs that appear in 
relative clauses. However, there are two main draw- 
backs: 
Treebanks are expensive to build and so the 
techniques presented here have to work with 
less data. 
All the dependents of each verb are visible to 
the learning algorithm. This is contrasted with 
previous techniques that rely on linite-state x= 
traction rules which ignore many dependents 
of the verb. Thus our technique has to deal 
with a different kind of data as compared to 
previous approaches. 
We tackle the second problem by using the 
method of observed frame subsets described in Sec- 
tion 3.3. 
6 Conclus ion 
We arc currently incorporating the SF information 
produced by the methods described in this paper 
into a parser for Czech. We hope to duplicate the 
increase in performance shown by treebank-based 
parsers for English when they use SF information. 
Our methods can also be applied to improve the 
annotations in the original treebank that we use as 
training data. The automatic addition of subcate- 
gorization to the treebank can be exploited to add 
predicate-argument i formation to the treebank. 
Also, techniques for extracting SF information 
fiom data can be used along with other research 
which aims to discover elationships between dif- 
ferent SFs of a verb (Stevenson and Merlo, t999; 
Lapata and Brew, 1999; Lapata, 1999; Stevenson et 
al., 1999). 
The statistical models in this paper were based on 
the assumption that given a verb, different SFs oc- 
cur independently. This assumption is used to jus- 
tify the use of the binomial. Future work perhaps 
should look towards removing this assumption by 
modeling the dependence between different SFs for 
the same verb using a multinomial distribution. 
To summarize: we have presented techniques that 
can be used to learn subcategorization information 
for verbs. We exploit a dependency treebank to 
learn this information, and moreover we discover 
the final set of valid subcategorization frames from 
the training data. We achieve upto 88% precision on 
unseen data. 
We have also tried our methods on data which 
was automatically morphologically tagged which 
696 
Previous 
work 
(Ushioda et al, 1993) 
(Brent, 1993) 
(Mmming, 1993) 
(Brent, 1994) 
(Ersan and Charniak, 1996) 
(Briscoe and Carroll, 1997) 
(CatToll and Rooth, 1998) 
Data 
POS + 
FS ntles 
raw + 
FS rules 
POS + 
FS rules 
raw + 
heurist ics 
Full  
parsing 
Full  
parsing 
Unlabeled 
#SFs  
Current Work Ful ly  Learned 
Parsed 137 
6 33 
6 193 
19 3104 
12 126 
16 30 
160 14 
9+ 3 
914 
Method 
heuristics 
Hypothesis 
testing 
Miscue 
rate 
NA 
iterative 
estimation 
Corpus 
WNJ (300K) 
Brown ( 1. IM)  
Hypothesis  hand NYT  (4.1 M) 
testing 
Hypothesis  non-iter CHIL I )ES  (32K) 
testing est imation 
Hypothesis  hand WSJ  (36M) 
testing 
Hypothesis  Dict ionary various (7OK) 
testing est imation 
Inside- NA BNC (5-30M) 
outside 
Subsets+ Est imate PDT (300K) 
Hyp. testing 
Table 2: Comparison with previous work on automatic SF extraction from corpora 
al lowed us to use more data (82K sentences instead 
of  19K). The performance went up to 89% (a 1% 
improvement) .  
Re ferences  
Roberto Basili and Michele Vmdigni. 1998. Adapting a sub- 
categorization lexicon to a domain. In I'roceedings of 
the ECML'98 Workshop TANLPS: Towards adaptive NLP- 
d,iven systems: lingui'stic information, learning methods 
and applications, Chemnitz, Germany, Apr 24. 
Peter Bickel and Kjell l)oksum. 1977. Mathematical Statis- 
tics. Holden-Day Inc. 
Michael Brent. 1991. Automatic acquisition of subcategoriza- 
tion flames from untagged text. In Proceedings of the 29th 
Meeting of the AUL. pages 209-214, Berkeley, CA. 
Michael Brent. 1993. From grammar to lexicon: unsuper- 
vised learning of lexical syntax. ('Omlmtational Linguistics, 
19(3):243-262. 
Michael Brent. 1994. Acquisition of subcategorization frames 
using aggregated evidence fiom local syntactic ues. Lin- 
gmt, 92:433-470. Reprinted in Acqttisition of the Lexicon, 
L. Gleinnan and B. Landau (Eds.). MIT Press, Cambridge, 
MA. 
Ted Briscoe and John Carroll. 1997. Automatic extraction of 
subcategorization from corpora. In Proceedings of the 5th 
ANI, P Conference, pages 356-363. Washington. D.C. ACI,. 
John Carroll and Guido Minnen. 1998. Can subcategorisa- 
tion probabilities help a statistical parser. In Proceedings 
of the 6th AClJSIGDAT Workshop on Very lztrge ('orpora 
(WVLC-6), Montreal, Canada. 
Glenn Carroll and Mats Rooth. 1998. Valence induction with 
a head-lcxicalized PCFG. In Proceedings of the 3rd Confer- 
ence on Empirical Methods in Natural Language Processing 
(EMNLI' 3), Granada, Spain. 
Ted Dunning. 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational Ling,istics. 
19( 1):61-74, March. 
Murat Ersan and Eugene Chamiak. 1996. A statistical syn- 
tactic disambiguation program and what it learns. In 
S. Wcrmter, E. Riloff, and G. Scheler. editors, Comwc- 
tionist, Statistical and Symbolic Approaches in Learning 
.fi~r Natural xmguage I'rocessing, volume 1040 of Lecture 
Notes in ArtiJical Intelligence, pages 146-159. Springer- 
Verlag, Berlin. 
Jan ttaji,.? and Barbora ttladkfi. 1998. "Fagging inllective lan- 
guages: Prediction of morphological categories for a rich, 
structured tagset. In Proceedings of COLING-ACI, 98, Uni- 
versitd e Montrdal, Montreal, pages 483-490. 
Jan Itaji,L 1998. Building a syntactically annotated corpus: 
The prague dependency treebank. In Issues off Valency and 
Meaning, pages 106-132. Karolinum, Praha. 
Maria Lapata and Chris Brew. 1999. Using subcategorization 
to resolve verb class ambiguity. In Pascale Furtg and Joe 
Zhou, editors, Proceedings o1' WVL(TEMNI,I ~, pages 266-- 
274, 21-22 June. 
Maria Lapata. 1999. Acquiring lexical generalizations from 
corpora: A case study for diathesis alternations. In Proceed- 
ings q/37th Meeting olA( :L, pages 397-404. 
Christopher I). Manning. 1993. Automatic acquisition of a 
large subcategorization dictionary from corpora. In Pro- 
ceedil~gs of the 31st Meeting q/' the ACI,, pagcs 235-242, 
Columbus, Ohio. 
Suzanne Stevenson and Paola Merlo. 1999. Automatic verb 
classilication using distributions of grammatical features. In 
Proceedings of I'JACL '99, pages 45-52, Bergen, Norway, 
8-12 J une. 
Suzanne Stevenson, Paoht Merlo, Natalia Kariaeva, and Kamin 
Whitehouse. 1999. Supervised learning of lexical semantic 
classes using frequency distributions. In SIGLEX-99. 
Akira Ushioda, David A. Evans, Ted Gibson, and Alex Waibel. 
1993. The autonaatic acquisition of frequencies ofverb st, b- 
categorization frames from tagged corpora. In B. Boguraev 
and J. Pustejovsky, editors, Proceedings of the Workshop on 
Acquisition of Lexical Knowledge fi'om 7kvt, pages 95-106, 
Columbus, Otl, 21 June. 
Mort Webster and Mitchell Marcus. 1989. Automatic acquisi- 
tion of the lexical frames of verbs from sentence frames. In 
Proceedings of the 27th Meeting of the ACL, pages 177-184. 
697 
&DQ 6XEFDWHJRUL]DWLRQ +HOS D 6WDWLVWLFDO 'HSHQGHQF\ 3DUVHU"
'DQLHO =(0$1
&HQWUXP NRPSXWD?Qt OLQJYLVWLN\
8QLYHU]LWD .DUORYD
0DORVWUDQVNp QiPVWt 
3UDKD &]HFKLD 
]HPDQ#XIDOPIIFXQLF]
$EVWUDFW
7RGD\ WKHUH LV D UHODWLYHO\ ODUJH ERG\ RI
ZRUN RQ DXWRPDWLF DFTXLVLWLRQ RI OH[LFR
V\QWDFWLFDO SUHIHUHQFHV VXEFDWHJRUL]DWLRQ
IURP FRUSRUD 9DULRXV WHFKQLTXHV KDYH EHHQ
GHYHORSHG WKDW QRW RQO\ SURGXFH PDFKLQH
UHDGDEOH VXEFDWHJRUL]DWLRQ GLFWLRQDULHV EXW
DOVR WKH\ DUH FDSDEOH RI ZHLJKLQJ WKH
YDULRXV VXEFDWHJRUL]DWLRQ IUDPHV
SUREDELOLVWLFDOO\ &OHDUO\ WKHUH VKRXOG EH D
SRWHQWLDO WR XVH VXFK ZHLJKWHG OH[LFDO
LQIRUPDWLRQ WR LPSURYH VWDWLVWLFDO SDUVLQJ
WKRXJK SXEOLVKHG H[SHULPHQWV SURYLQJ RU
GLVSURYLQJ VXFK K\SRWKHVLV DUH
FRPSDUDWLYHO\ UDUH 2QH H[SHULPHQW LV
GHVFULEHG LQ &DUUROO HW DO  ? WKH\
XVH VXEFDWHJRUL]DWLRQ SUREDELOLWLHV IRU
UDQNLQJ WUHHV JHQHUDWHG E\ XQLILFDWLRQEDVHG
SKUDVDO JUDPPDU 7KH SUHVHQW SDSHU RQ WKH
RWKHU KDQG LQYROYHV D VWDWLVWLFDO GHSHQGHQF\
SDUVHU $OWKRXJK GHSHQGHQF\ DQG
FRQVWLWXHQF\ SDUVLQJ DUH RI TXLWH D GLIIHUHQW
QDWXUH ZH VKRZ WKDW D VXEFDWHJRUL]DWLRQ
PRGHO LV RI PXFK XVH KHUH DV ZHOO
,QWURGXFWLRQ
/DUJHVFDOH PDFKLQHUHDGDEOH GLFWLRQDULHV DUH
EHFRPLQJ DYDLODEOH DV JURZV WKH VFDOH RI
FRUSRUD DEOH WR VHUYH DV VRXUFH RI VXFK
GLFWLRQDULHV DQG WKH YDULDELOLW\ RI WHFKQLTXHV IRU
H[WUDFWLQJ OH[LFDO LQIRUPDWLRQ IURP WKH FRUSRUD
DXWRPDWLFDOO\ 7KH UHVXOWLQJ GLFWLRQDULHV EHDU
DPRQJ RWKHUV GDWD RQ YHUE VXEFDWHJRUL]DWLRQ
LH W\SHV RI DUJXPHQWV WKH SDUWLFXODU YHUE
UHTXLUHV DV RSSRVHG WR DGMXQFWV WKDW FDQ PRGLI\
DQ\ YHUE 2IWHQ VXFK GDWD DUH DFFRPSDQLHG E\
WKH UHODWLYH IUHTXHQFLHV RI WKH DOWHUQDWLYHV
9DULRXV OH[LFDOLVHG JUDPPDUV KDYH EHHQ
SURSRVHG 5HVQLN  6FKDEHV  &DUUROO
DQG :HLU Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 35?42,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Cross-Language Parser Adaptation between Related Languages 
Daniel Zeman 
Univerzita Karlova 
?stav form?ln? a aplikovan? lingvistiky 
Malostransk? n?m?st? 25 
CZ-11800 Praha 
zeman@ufal.mff.cuni.cz 
Philip Resnik 
University of Maryland 
Department of Linguistics and  
Institute for Advanced Computer Studies 
College Park, MD 20742, USA 
resnik@umd.edu 
 
Abstract 
The present paper describes an approach to 
adapting a parser to a new language. 
Presumably the target language is much 
poorer in linguistic resources than the source 
language. The technique has been tested on 
two European languages due to test data 
availability; however, it is easily applicable 
to any pair of sufficiently related languages, 
including some of the Indic language group. 
Our adaptation technique using existing 
annotations in the source language achieves 
performance equivalent to that obtained by 
training on 1546 trees in the target language. 
1 Introduction 
Natural language parsing is one of the key areas of 
natural language processing, and its output is used 
in numerous end-user applications, e.g. machine 
translation or question answering. Unfortunately, it 
is not easy to build a parser for a resource-poor 
language. Either a reasonably-sized syntactically 
annotated corpus (treebank) or a human-designed 
formal grammar is typically needed. These types of 
resources are costly to build, both in terms of time 
and of the expenses on qualified manpower. Both 
also require, in addition to the actual annotation 
process, a substantial effort on treebank/grammar 
design, format specifications, tailoring of annota-
tion guidelines etc; the latter costs are rather con-
stant no matter how small the resulting corpus is. 
In this context, there is the intriguing question 
whether we can actually build a parser without a 
treebank (or a broad-coverage formal grammar) of 
the particular language. There is some related 
work that addresses the issue by a variety of means. 
Klein and Manning (2004) use a hybrid unsuper-
vised approach, which combines a constituency 
and a dependency model, and achieve an unlabeled 
F-score of 77.6% on Penn Treebank Wall Street 
Journal data (English), 63.9% on Negra Corpus 
(German), and 46.7% on the Penn Chinese Tree-
bank.1 Bod (2006) uses unsupervised data-oriented 
parsing; the input of his parser contains manually 
assigned gold-standard tags. He reports 64.2% 
unlabeled F-score on WSJ sentences up to 40 
words long.2 
Hwa et al (2004) explore a different approach to 
attacking a new language. They train Collins?s 
(1997) Model 2 parser on the Penn Treebank WSJ 
data and use it to parse the English side of a paral-
lel corpus. The resulting parses are converted to 
dependencies, the dependencies are projected to a 
second language using automatically obtained 
word alignments as a bridge, and the resulting de-
pendency trees cleaned up using a limited set of 
language-specific post-projection transformation 
rules. Finally a dependency parser for the target 
language is trained on this projected dependency 
treebank, and the accuracy of the parser is meas-
ured against a gold standard. Hwa et al report de-
pendency accuracy of 72.1 for Spanish, compara-
ble to a rule-based commercial parser; accuracy on 
Chinese is 53.9%, the equivalent of a parser trained 
on roughly 2000 sentences of the Penn Chinese 
Treebank (sentences ?40 words, average length 
20.6). 
                                                 
1 Note that in all these experiments they restrict themselves to 
sentences of 10 words or less. 
2 On sentences of ?10 words, Bod achieves 78.5% for English 
(WSJ), 65.4% for German (Negra) and 46.7% for Chinese 
(CTB). 
35
Our own approach is motivated by McClosky et 
al.?s (2006) reranking-and-self-training algorithm, 
used successfully in adapting a parser to a new 
domain. One can easily imagine viewing two dia-
lects of a language or even two related languages 
as two domains of one ?super-language? while the 
vocabulary will certainly differ (due to independ-
ently designed orthographies for the two lan-
guages), many morphological and syntactic proper-
ties may be shared. We trained Charniak and John-
son?s (2005) reranking parser on one language and 
applied it to another closely related language. In 
addition, we investigated the utility of large but 
unlabeled data in the target language, and of a 
large parallel corpus of the two languages.3 
2 Corpora and Other Resources 
The selection of our source and target languages 
was driven by the need for two closely related lan-
guages with associated treebanks. (In a real-world 
application we would not assume the existence of a 
target-language treebank, but one is needed here 
for evaluation.) Danish served as the source lan-
guage and Swedish as target, since these languages 
are closely related and there are freely available 
treebanks for both.4 
The Danish Dependency Treebank (Kromann et 
al. 2004) contains 5,190 sentences (94,386 tokens). 
The texts come from the Danish Parole Corpus 
(1998?2002, mixed domain). We split the data into 
4,900 training and 290 test sentences, keeping the 
276 not exceeding 40 words. 
The Swedish treebank Talbanken05 (Nivre et al 
2006) contains 11,042 sentences (191,467 tokens). 
It was converted at V?xj? from the much older 
Talbanken76 treebank, created at the Lund Univer-
sity. Again, the texts belong to mixed domains. We 
split the data to 10,700 training and 342 test sen-
tences, out of which 317 do not exceed 40 words. 
Both treebanks are dependency treebanks, while 
the Charniak-Johnson reranking parser works with 
phrase structures. For our experiments, we con-
                                                 
3 There are other approaches to domain adaptation as 
well. For instance, Steedman et al (2003) address do-
main adaptation using a weakly supervised method 
called co-training. Two parsers, each applying a differ-
ent strategy, mutually prepare new training examples for 
each other. We have not tested co-training for cross-
language adaptation. 
4 We used the CoNLL 2006 versions of these treebanks. 
verted the treebanks from dependencies to phrases, 
using the ?flattest-possible? algorithm (Collins et 
al. 1999; algorithm 2 of Xia and Palmer 2001). The 
morphological annotation of the treebanks helped 
us to label the non-terminals. Although the 
Charniak?s parser can be taught a new inventory of 
labels, we found it easier to map head morpho-tags 
directly to Penn-Treebank-style non-terminals. 
Hence the parser can think it?s processing Penn 
Treebank data. The morphological annotation of 
the treebanks is further discussed in Section 4. 
We also experimented with a large body of un-
annotated Swedish texts. Such data could theoreti-
cally be acquired by crawling the Web; here, how-
ever, we used the freely available JRC-Acquis cor-
pus of EU legislation (Steinberger et al 2006).5 
The Acquis corpus is segmented at the paragraph 
level. We ran a simple procedure to split the para-
graphs into sentences and pruned sentences with 
suspicious length, contents (sequence of dashes, 
for instance) or both. We ended up with 430,808 
Swedish sentences and 6,154,663 tokens. 
Since the Acquis texts are available in 21 lan-
guages, we can also exploit the Danish Acquis and 
its alignment with the Swedish one. We use it to 
study the similarity of the two languages, and for 
the ?gloss? experiment in Section 5.1. Paragraph-
level alignment is provided as part of Acquis and 
contains 283,509 aligned segments. Word-level 
alignment, needed for our experiment, was ob-
tained using GIZA++ (Och and Ney 2000). 
The treebanks are manually tagged with parts of 
speech and morphological information. For some 
of our experiments, we needed to automatically re-
tag the target (Swedish) treebank, and to tag the 
Swedish Acquis. For that purpose we used the 
Swedish tagger of Jan Haji?, a variant of Haji??s 
Czech tagger (Haji? 2004) retrained on Swedish 
data. 
3 Treebank Normalization 
The two treebanks were developed by different 
teams, using different annotation styles and guide-
lines. They would be systematically different even 
if their texts were in the same language, but it is 
                                                 
5 Legislative texts are a specialized domain that cannot 
be expected to match the domain of our treebanks, how-
ever vaguely defined it is. But presumably the domain 
matching would be even less trustworthy if we acquired 
the unlabeled data from the web. 
36
the impact of the language difference, not annota-
tion style differences, that we want to measure; 
therefore we normalize the treebanks so that they 
are as similar as possible. 
While this may sound suspicious at first glance 
(?wow, are they refining their test data?!?), it is 
important to understand why it does not 
unacceptably bias the results. If our method were 
applied to a new language, where no treebank 
exists, trees conforming to the annotation scenario 
of a treebank of related language would be 
perfectly satisfying. In addition, note that we apply 
only systematic changes, mostly reversible. 
Moreover, the transformations can be done on the 
training data side, instead of test data. 
Following are examples of the style differences 
that underwent normalization: 
DET-ADJ-NOUN. Da: de norske piger. Sv:6 en 
gammal institution (?an old institution?) In DDT, 
the determiner governs the adjective and the noun. 
The approach of Talbanken (and of a number of 
other dependency treebanks) is that both deter-
miner and adjective depend on the noun. 
NUM-NOUN. Da: 100 procent (?100 percent?) 
Sv: tv? eventuellt tre ?r (?two, possibly three 
years?) In DDT, the number governs the noun. In 
Talbanken, the number depends on the noun. 
GENITIVE-NOMINATIVE. Da: Ruslands vej 
(?Russia?s way?) Sv: ?rs inkomster (?year?s 
income?). In DDT, the nominative noun (the 
owned) governs the noun in genitive (the owner). 
Talbanken goes the opposite way. 
COORDINATION. Da: F?r?erne og 
Gr?nland (?Faroe Islands and Greenland?) Sv: 
socialgrupper, nationer och raser (?social groups, 
nations and races?) In DDT, the last coordination 
member depends on the conjunction, the 
conjunction and everything else (punctuation, inner 
members) depend on the first member, which is the 
head of the coordination. In Talbanken, every 
member depends on the previous member, commas 
and conjunctions depend on the member following 
them. 
4 Mapping Tag Sets 
The nodes (words) of the Danish Dependency 
Treebank are tagged with the Parole morphological 
                                                 
6 These are separate examples from the two treebanks. 
They are not translations of each other! 
tags. Talbanken is tagged using the much coarser 
Mamba tag set (part of speech, no morphology). 
The tag inventory of Haji??s tagger is quite similar 
to the Danish Parole tags, but not identical. We 
need to be able to map tags from one set to the 
other. In addition, we also convert pre-terminal 
tags to the Penn Treebank tag set when converting 
dependencies to constituents. 
Mapping tag sets to each other is obviously an 
information-lossy process, unless both tag sets 
cover identical feature-value spaces. Apart from 
that, there are numerous considerations that make 
any such conversion difficult, especially when the 
target tags have been designed for a different 
language. 
We take an Interlingua-like (or Inter-tag-set) 
approach. Every tag set has a driver that 
implements decoding of the tags into a nearly 
universal feature space that we have defined, and 
encoding of the feature values by the tags. The 
encoding is (or aims at being) independent of 
where the feature values come from, and the 
decoding does not make any assumptions about the 
subsequent encoding. Hence the effort put in 
implementing the drivers is reusable for other 
tagset pairs. 
The key function, responsible for the 
universality of the method, is encode(). 
Consider the following example. There are two 
features set, POS = ?noun? and GENDER = 
?masc?. The target set is not capable of encoding 
masculine nouns. However, it allows for ?noun? + 
?com? | ?neut?, or ?pronoun? + ?masc? | ?fem? | 
?com? | ?neut?. An internal rule of encode() 
indicates that the POS feature has higher priority 
than the GENDER feature. Therefore the algorithm 
will narrow the tag selection to noun tags. Then the 
gender will be forced to common (i.e. ?com?). 
Even the precise feature mapping does not 
guarantee that the distribution of the tags in two 
corpora will be reasonably close. All converted 
source tags will now fit in the target tag set. 
However, some tags of the target tag set may not 
be used, although they are quite frequent in the 
corpus where the target tags are native. Some 
examples:  
? Unlike in Talbanken, there are no deter-
miners in DDT. That does not mean there 
are no determiners in Danish ? but DDT 
tags them as pronouns. 
37
? Swedish tags encode a special feature of 
personal pronouns, ?subject? vs. ?object? 
form (the distinction between English he 
and him). DDT calls the same paradigm 
?nominative? vs. ?unmarked? case. 
? Most noun phrases in both languages 
distinguish just the common and neuter 
genders. However, some pronouns could be 
classified as masculine or feminine. 
Swedish tags use the masculine gender, 
Danish do not. 
? DDT does not use special part of speech for 
numbers ? they are tagged as adjectives. 
All of the above discrepancies are caused by 
differing designs, not by differences in language. 
The only linguistically grounded difference we 
were able to identify is the supine verb form in 
Swedish, missing from Danish. 
When not just the tag inventories, but also the 
tag distributions have to be made compatible 
(which is the case of our delexicalization 
experiments later in this paper), we can create a 
new hybrid tag set, omitting any information 
specific for one or the other side. Tags of both 
languages can then be converted to this new set, 
using the universal approach described above. 
5 Using Related Languages 
The Figure 1 gives an example of matching Danish 
and Swedish sentences. This is a real example 
from the Acquis corpus. Even a non-speaker of 
these languages can detect the evident correspon-
dence of at least 13 words, out of the total of 16 
(ignoring final punctuation). However, due to dif-
ferent spelling rules, only 5 word pairs are string-
wise identical. From a parser?s perspective, the rest 
is unknown words, as it cannot be matched against 
the vocabulary learned from training data. 
We explore two techniques of making unknown 
words known. We call them glosses and delexicali-
zation, respectively. 
5.1 Glosses 
This approach needs a Danish-Swedish (da-sv) 
bitext. As shown by Resnik and Smith (2003), 
parallel texts can be acquired from the Web, which 
makes this type of resource more easily available 
than a treebank. We benefited from the Acquis da-
sv alignments. 
Similarly to phrase-based translation systems, 
we used GIZA++ (Och and Ney 2000) to obtain 
one-to-many word alignments in both directions, 
then combined them into a single set of refined 
alignments using the ?final-and? method of Koehn 
et al (2003). The refined alignments provided us 
with two-way tables of a source word and all its 
possible translations, with weights. Using these 
tables, we glossed each Swedish word by its 
Danish, using the translation with the highest 
weight. 
The glosses are used to replace Swedish words 
in test data by Danish, making it more likely that 
the parser knows them. After a parse has been 
obtained, the trees are ?restuffed? with the original 
Swedish words, and evaluated. 
5.2 Delexicalization 
A second approach relies on the hypothesis that the 
interaction between morphology and syntax in the 
two languages will be very similar. The basic idea 
is as follows: Replace Danish words in training 
data with their morphological (POS) tags. Simi-
larly, replace the Swedish words in test data with 
tags. This replacement is called delexicalization. 
Note that there are now two levels of tags in the 
trees: the Danish/Swedish tags in terminal nodes, 
and the Penn-style tags as pre-terminals. The ter-
minal tags are more descriptive because both Nor-
Bestemmelserne i denne aftale kan ?ndres og revideres helt eller delvis efter f?lles 
Best?mmelserna i detta avtal f?r ?ndras eller revideras helt eller delvis efter gemensam 
overenskomst mellem parterne. 
?verenskommelse mellan parterna. 
Figure 1. Comparison of matching Danish (upper) and Swedish (lower) sentences from Acquis. De-
spite the one-to-one word mapping, only the 5 bold words have identical spelling. 
38
dic languages have a slightly richer morphology 
than English, and the conversion to the Penn tag 
set loses information. 
The crucial point is that both Danish and 
Swedish use the same tag set, which helps to deal 
with the discrepancy between the training and the 
test terminals. 
Otherwise, the algorithm is similar to that of 
glosses: train the parser on delexicalized Danish, 
run it over delexicalized Swedish, restuff the 
resulting trees with the original Swedish words 
(?re-lexicalize?) and evaluate them. 
6 Experiments: Part One 
We ran most experiments twice: once with 
Charniak?s parser alone (?C?) and once with the 
reranking parser of Charniak and Johnson, which 
we label simply Brown parser (?B?). 
We use the standard evalb program by Sekine 
and Collins to evaluate the parse trees. Keeping 
with tradition, we report the F-score of the labeled 
precision and recall on the sentences of up to 40 
words.7 
 
Language Parser P R F 
C 77.84 78.48 78.16 da 
B 78.28 78.20 78.24 
C 79.50 79.73 79.62 da-hybrid 
B 80.60 79.80 80.20 
C 77.61 78.00 77.81 sv 
B 79.16 78.33 78.74 
C 77.54 78.93 78.23 sv-mamba 
B 79.67 79.26 79.46 
C 76.10 76.04 76.07 sv-hybrid 
B 78.12 75.93 77.01 
Table 1. Monolingual parsing accuracy. 
 
To put the experiments in the right context, we 
first ran two monolingual tracks and evaluated 
Danish-trained parsers on Danish, and Swedish-
trained parsers on Swedish test data. Both 
treebanks have also been parsed after 
delexicalization into various tag sets: Danish gold 
standard converted to the hybrid sv/da tag set, 
Swedish Mamba gold standard, and Swedish 
automatically tagged with hybrid tags. 
The reranker did not prove useful for lexicalized 
Swedish, although it helped with Danish. (We cur-
                                                 
7 F = 2?P?R / (P+R) 
rently have no explanation of this.) On the other 
hand, delexicalized reranking parsers outperformed 
lexicalized parsers for both languages. This holds 
for delexicalization using the gold standard tags 
(even though the Mamba tag set encodes much less 
information than the hybrid tags). Automatically 
assigned tags perform significantly worse. 
Our baseline condition is simply to train the 
parsers on Danish treebank and run them over 
Swedish test data. Then we evaluate the two 
algorithms described in the previous section: 
glosses and delexicalization (hybrid tags). 
 
Approach Parser P R F 
C 44.59 42.04 43.28 baseline 
B 42.94 40.80 41.84 
C 61.85 65.03 63.40 glosses 
B 60.22 62.85 61.50 
C 63.47 67.67 65.50 delex 
B 64.74 68.15 66.40 
 
Table 2. Cross-language parsing accuracy. 
 
7 Self-Training 
Finally, we explored the self-training based 
domain-adaptation technique of McClosky et al 
(2006) in this setting. McClosky et al trained the 
Brown parser on one domain of English (WSJ), 
parsed a large corpus of a second domain 
(NANTC), trained a new Charniak (non-reranking) 
parser on WSJ plus the parsed NANTC, and tested 
the new parser on data from a third domain (Brown 
Corpus). They observed improvement over 
baseline in spite of the fact that the large corpus 
was not in the third domain. 
 
Our setting is similar. We train the Brown parser 
on Danish treebank and apply it to Swedish Acquis. 
Then we train new Charniak parser on Danish 
treebank and the parsed Swedish Acquis, and test 
the parser on the Swedish test data. The hope is 
that the parser will get lexical context for the 
structures from the parsed Swedish Acquis. 
 
We did not retrain the reranker on the parsed 
Acquis, as we found it prohibitively expensive in 
both time and space. Instead, we created a new 
Brown parser by combining the new Charniak 
parser, and the old reranker trained only on Danish. 
39
A different scenario is used with the gloss and 
delex techniques. In this case, we only use delexi-
calization/glosses to parse the Acquis corpus. The 
new Charniak model is always trained directly on 
lexicalized Swedish, i.e. the parsed Acquis is re-
stuffed before being handed over to the trainer. 
Table-3 shows the corresponding application chart. 
8 Experiments: Part Two 
The following table shows the results of the self-
training experiments. All F-scores outperform the 
corresponding results obtained without self-
training. 
 
Approach Parser P R F 
C 45.14 43.96 44.54 Plain 
B 43.12 42.23 42.67 
C 62.87 66.17 64.48 Glosses 
B 61.94 64.77 63.32 
C 55.87 63.86 59.60 Delex 
B 53.87 61.45 57.41 
Table 3. Self-training adaptation results. 
 
Not surprisingly, the Danish-trained reranker 
does not help here. However, even the first-stage 
parser failed to outperform the Part One results. 
Therefore the 66.40% labeled F-score of the del-
exicalized Brown parser is our best result. It im-
proves the baseline by 23% absolute, or 41% error 
reduction. 
9 Discussion 
As one way of assessing the usefulness of the 
result, we compared it to the learning curve on the 
Swedish treebank. This corresponds to the question 
?How big a treebank would we have to build, so 
that the parser trained on the treebank achieves the 
same F-score?? We measured the F-scores for 
Swedish-trained parsers on gradually increasing 
amounts of training data (50, 100, 250, 500, 1000, 
2500, 5000 and 10700 sentences). 
The learning curve is shown in Figure 3. Using 
interpolation, we see that more than 1500 Swedish 
parse trees would be required for training, in order 
to achieve the performance we obtained by adapt-
ing an existing Danish treebank. This result is 
similar in spirit to the results Hwa et al (2004) re-
port when training a Chinese parser using depend-
ency trees projected from English. As they observe, 
creating a treebank of even a few thousand trees is 
a daunting undertaking ? consistent annotation 
typically requires careful design of guidelines for 
the annotators, testing of the guidelines on data, 
refinement of those guidelines, ramp-up of annota-
tors, double-annotation for quality control, and so 
forth. As a case in point, the Prague Dependency 
Treebank (B?hmov? et al 2003) project began in 
Danish treebank
PARSER 0 RERANKER
 
Swedish 
Acquis 1 
PARSER 1
Swedish test
DELEX 
GLOSSES 
 
Swedish 
Acquis RESTUFF
 
Parsed Swedish
Acquis 
Figure 2. Scheme of the self-training system. 
40
1996, and required almost a year for its first 1000 
sentences to appear (although things sped up 
quickly, and over 20000 sentences were available 
by fall 1998). In contrast, if the source and target 
language are sufficiently related ? consider Danish 
and Swedish, as we have done, or Hindi and 
Urdu ? our approach should in principle permit a 
parser to be constructed in a matter of days.). 
9.1 Ways to Improve: Future Work 
The 77.01% F-score of a parser trained on 
delexicalized automatically assigned hybrid 
Swedish tags is an upper bound. Some obvious 
ways of getting closer to it include better treebank 
and tag-set mapping and better tagging. In addition, 
we are interested in seeing to what extent 
performance can be further improved by better 
iterative self-training. 
We also want to explore classifier combination 
techniques on glosses, delexicalization, and the N-
best outputs of the Charniak parser. One could also 
go further, and explore a combination of tech-
niques, e.g. taking advantage of the ideas proposed 
here in tandem with unsupervised parsing (as in 
Bod 2006) or projection of annotations across a 
parallel corpus (as in Hwa et al 2004). 
Acknowledgements 
The authors thank Eugene Charniak and Mark 
Johnson for making their reranking parser 
available, as well as the creators of the corpora 
used in this research. We also thank the 
anonymous reviewers for useful remarks on where 
to focus our workshop presentation. 
The research reported on in this paper has been 
supported by the Fulbright-Masaryk Fellowship 
(first author), and by Grant No. N00014-01-1-0685 
ONR. Ongoing research (first author) is supported 
by the Ministry of Education of the Czech 
Republic, project MSM0021620838, and Czech 
Academy of Sciences, project No. 1ET101470416. 
References 
Rens Bod. 2006a. Unsupervised Parsing with U-DOP. 
In: Proceedings of the Conference on Natural 
Language Learning (CoNLL-2006). New York, New 
York, USA. 
Rens Bod. 2006b. An All-Subtrees Approach to Unsu-
pervised Parsing. In: Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and 
the 44th Annual Meeting of the ACL (COLING-
ACL-2006). Sydney, Australia. 
0
10
20
30
40
50
60
70
80
50 100 250 500 1000 2500 5000 10700
Training sentences
F
66.40 
(delex) 
~ 1546 
sentences 
Figure 3. The learning curve on the Swedish training data. 
41
Alena B?hmov?, Jan Haji?, Eva Haji?ov?, Barbora 
Hladk?. 2003. The Prague Dependency Treebank: A 
Three-Level Annotation Scenario. In: Anne Abeill? 
(ed.): Treebanks: Building and Using Syntactically 
Annotated Corpora. Kluwer Academic Publishers, 
Dordrecht, The Netherlands. 
Eugene Charniak, Mark Johnson. 2005. Coarse-to-Fine 
N-Best Parsing and MaxEnt Discriminative 
Reranking. In: Proceedings of the 43rd Annual 
Meeting of the ACL (ACL-2005), pp. 173?180. Ann 
Arbor, Michigan, USA. 
Michael Collins. 1997. Three Generative, Lexicalized 
Models for Statistical Parsing. In: Proceedings of the 
35th Annual Meeting of the ACL, pp. 16?23. Madrid, 
Spain. 
Michael Collins, Jan Haji?, Lance Ramshaw, Christoph 
Tillmann. 1999. A Statistical Parser for Czech. In: 
Proceedings of the 37th Annual Meeting of the ACL 
(ACL-1999), pp. 505?512. College Park, Maryland, 
USA. 
Jan Haji?. 2004. Disambiguation of Rich Inflection 
(Computational Morphology of Czech). Karolinum, 
Charles University Press, Praha, Czechia. 
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara 
Cabezas, Okan Kolak. 2004. Bootstrapping Parsers 
via Syntactic Projection across Parallel Texts. In: 
Natural Language Engineering 1 (1): 1?15. 
Cambridge University Press, Cambridge, England. 
Dan Klein, Christopher D. Manning. 2004. Corpus-
Based Induction of Syntactic Structure: Models of 
Dependency and Constituency. In: Proceedings of the 
42nd Annual Meeting of the ACL (ACL-2004). 
Barcelona, Spain. 
Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. In: Proceedings 
of HLT-NAACL 2003, pp. 127?133. Edmonton, 
Canada. 
Matthias T. Kromann, Line Mikkelsen, Stine Kern 
Lynge. 2004. Danish Dependency Treebank. At: 
http://www.id.cbs.dk/~mtk/treebank/. K?benhavn, 
Denmark. 
Mitchell P. Marcus, Beatrice Santorini, Mary Ann Mar-
cinkiewicz. 1993. Building a Large Annotated Cor-
pus of English: the Penn Treebank. In: Computa-
tional Linguistics, vol. 19, pp. 313?330. 
David McClosky, Eugene Charniak, Mark Johnson. 
2006. Reranking and Self-Training for Parser Adap-
tation. In: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th 
Annual Meeting of the ACL (COLING-ACL-2006). 
Sydney, Australia. 
Joakim Nivre, Jens Nilsson, Johan Hall. 2006. 
Talbanken05: A Swedish Treebank with Phrase 
Structure and Dependency Annotation. In: 
Proceedings of the 5th International Conference on 
Language Resources and Evaluation (LREC-2006). 
May 24-26. Genova, Italy. 
Franz Josef Och, Hermann Ney. 2000. Improved 
Statistical Alignment Models. In: Proceedings of the 
38th Annual Meeting of the ACL (ACL-2000), pp. 
440?447. Hong Kong, China. 
Philip Resnik, Noah A. Smith. 2003. The Web as a 
Parallel Corpus. In: Computational Linguistics, 
29(3), pp. 349?380. 
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen 
Clark, Rebecca Hwa, Julia Hockenmaier, Paul 
Ruhlen, Steven Baker, Jeremiah Crim. 2003. 
Bootstrapping Statistical Parsers from Small 
Datasets. In: Proceedings of the 11th Conference of 
the European Chapter of the ACL (EACL-2003). 
Budapest, Hungary. 
Ralf Steinberger, Bruno Pouliquen, Anna Widiger, 
Camelia Ignat, Toma? Erjavec, Dan Tufi?, D?niel 
Varga. 2006. The JRC-Acquis: A Multilingual 
Aligned Parallel Corpus with 20+ Languages. In: 
Proceedings of the 5th International Conference on 
Language Resources and Evaluation (LREC-2006). 
May 24-26. Genova, Italy. 
Fei Xia, Martha Palmer. 2001. Converting Dependency 
Structures to Phrase Structures. In: Proceedings of 
the 1st Human Language Technology Conference 
(HLT-2001). San Diego, California, USA. 
42
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 171?178,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Improving Parsing Accuracy by Combining Diverse 
Dependency Parsers 
 
 
Daniel Zeman and Zden k ?abokrtsk? 
?stav form?ln? a aplikovan? lingvistiky, Univerzita Karlova 
Malostransk? n?m st? 25, CZ-11800  Praha 
{zeman|zabokrtsky}@ufal.mff.cuni.cz 
 
 
 
 
 
Abstract 
This paper explores the possibilities of 
improving parsing results by combining 
outputs of several parsers. To some ex-
tent, we are porting the ideas of Hender-
son and Brill (1999) to the world of 
dependency structures. We differ from 
them in exploring context features more 
deeply. All our experiments were con-
ducted on Czech but the method is lan-
guage-independent. We were able to 
significantly improve over the best pars-
ing result for the given setting, known so 
far. Moreover, our experiments show that 
even parsers far below the state of the art 
can contribute to the total improvement. 
1 Introduction 
Difficult and important NLP problems have the 
property of attracting whole range of researchers, 
which often leads to the development of several 
different approaches to the same problem. If these 
approaches are independent enough in terms of not 
producing the same kinds of errors, there is a hope 
that their combination can bring further improve-
ment to the field. While improving any single ap-
proach gets more and more difficult once some 
threshold has been touched, exploring the potential 
of approach combination should never be omitted, 
provided three or more approaches are available. 
Combination techniques have been successfully 
applied to part of speech tagging (van Halteren et 
al., 1998; Brill and Wu, 1998; van Halteren et al, 
2001). In both cases the investigators were able to 
achieve significant improvements over the previ-
ous best tagging results. Similar advances have 
been made in machine translation (Frederking and 
Nirenburg, 1994), speech recognition (Fiscus, 
1997), named entity recognition (Borthwick et al, 
1998), partial parsing (Inui and Inui, 2000), word 
sense disambiguation (Florian and Yarowsky, 
2002) and question answering (Chu-Carroll et al, 
2003). 
Brill and Hladk? (Haji  et al, 1998) have first 
explored committee-based dependency parsing. 
However, they generated multiple parsers from a 
single one using bagging (Breiman, 1994). There 
have not been more sufficiently good parsers 
available. A successful application of voting and of 
a stacked classifier to constituent parsing followed 
in (Henderson and Brill, 1999). The authors have 
investigated two combination techniques (constitu-
ent voting and na?ve Bayes), and two ways of their 
application to the (full) parsing: parser switching, 
and similarity switching. They were able to gain 
1.6 constituent F-score, using their most successful 
technique. 
In our research, we focused on dependency pars-
ing. One of the differences against Henderson and 
Brill?s situation is that a dependency parser has to 
assign exactly one governing node (parent word) to 
each word. Unlike the number of constituents in 
constituency-based frameworks, the number of 
dependencies is known in advance, the parser only 
has to assign a link (number 0 through N) to each 
word. In that sense, a dependency parser is similar 
to classifiers like POS taggers. Unless it deliber-
ately fails to assign a parent to a word (or assigns 
171
several alternate parents to a word), there is no 
need for precision & recall. Instead, a single metric 
called accuracy is used. 
On the other hand, a dependency parser is not a 
real classifier: the number of its ?classes? is theo-
retically unlimited (natural numbers), and no gen-
eralization can be drawn about objects belonging 
to the same ?class? (words that ? sometimes ? ap-
peared to find their parent at the position i). 
A combination of dependency parsers does not 
necessarily grant the resulting dependency struc-
ture being cycle-free. (This contrasts to not intro-
ducing crossing brackets in constituent parsing, 
which is granted according to Henderson and 
Brill.) We address the issue in 4.4. 
The rest of this paper is organized as follows: in 
Sections 2 and 3 we introduce the data and the 
component parsers, respectively. In Section 4 we 
discuss several combining techniques, and in Sec-
tion 5 we describe the results of the corresponding 
experiments. We finally compare our results to the 
previous work and conclude. 
2 The data 
To test our parser combination techniques, we use 
the Prague Dependency Treebank 1.0 (PDT; Haji  
et al 2001). All the individual parsers have been 
trained on its analytical-level training section 
(73,088 sentences; 1,255,590 tokens). 
The PDT analytical d-test section has been parti-
tioned into two data sets, Tune (last 77 files; 3646 
sentences; 63,353 tokens) and Test (first 76 files; 
3673 sentences; 62,677 tokens). We used the Tune 
set to train the combining classifiers if needed. The 
Test data were used to evaluate the approach. Nei-
ther the member parsers, nor the combining classi-
fier have seen this data set during their respective 
learning runs. 
3 Component parsers 
The parsers involved in our experiments are sum-
marized in Table 1. Most of them use unique 
strategies, the exception being thl and thr, which 
differ only in the direction in which they process 
the sentence. 
The table also shows individual parser accura-
cies on our Test data. There are two state-of-the art 
parsers, four not-so-good parsers, and one quite 
poor parser. We included the two best parsers 
(ec+mc) in all our experiments, and tested the con-
tributions of various selections from the rest. 
The necessary assumption for a meaningful 
combination is that the outputs of the individual 
parsers are sufficiently uncorrelated, i.e. that the 
parsers do not produce the same errors. If some 
Accuracy Par-
ser 
Author Brief description 
Tune Test 
ec 
Eugene 
Charniak 
A maximum-entropy inspired parser, home in constituency-based 
structures. English version described in Charniak (2000), Czech ad-
aptation 2002 ? 2003, unpublished. 
83.6 85.0 
mc 
Michael 
Collins 
Uses a probabilistic context-free grammar, home in constituency-
based structures. Described in (Haji  et al, 1998; Collins et al, 
1999). 
81.7 83.3 
z? Zden k ?abokrtsk? 
Purely rule-based parser, rules are designed manually, just a few lexi-
cal lists are collected from the training data. 2002, unpublished. 74.3 76.2 
dz Daniel Zeman 
A statistical parser directly modeling syntactic dependencies as word 
bigrams. Described in (Zeman, 2004). 73.8 75.5 
thr 71.0 72.3 
thl 69.5 70.3 
thp 
Tom?? 
Holan 
Three parsers. Two of them use a sort of push-down automata and 
differ from each other only in the way they process the sentence (left-
to-right or right-to-left). Described in (Holan, 2004). 62.0 63.5 
 
Table 1. A brief description of the tested parsers. Note that the Tune data is not the data used to train the 
individual parsers. Higher numbers in the right column reflect just the fact that the Test part is slightly 
easier to parse. 
172
parsers produced too similar results, there would 
be the danger that they push all their errors 
through, blocking any meaningful opinion of the 
other parsers. 
To check the assumption, we counted (on the 
Tune data set) for each parser in a given parser se-
lection the number of dependencies that only this 
parser finds correctly. We show the results in Ta-
ble 2. They demonstrate that all parsers are inde-
pendent on the others at least to some extent. 
4 Combining techniques 
Each dependency structure consists of a number of 
dependencies, one for each word in the sentence. 
Our goal is to tell for each word, which parser is 
the most likely to pick its dependency correctly. 
By combining the selected dependencies we aim at 
producing a better structure. We call the complex 
system (of component parsers plus the selector) the 
superparser. 
Although we have shown how different strate-
gies lead to diversity in the output of the parsers, 
there is little chance that any parser will be able to 
push through the things it specializes in. It is very 
difficult to realize that a parser is right if most of 
the others reject its proposal. Later in this section 
we assess this issue; however, the real power is in 
majority of votes. 
4.1 Voting 
The simplest approach is to let the member parsers 
vote. At least three parsers are needed. If there are 
exactly three, only the following situations really 
matter: 1) two parsers outvote the third one; 2) a 
tie: each parser has got a unique opinion. It would 
be democratic in the case of a tie to select ran-
domly. However, that hardly makes sense once we 
know the accuracy of the involved parsers on the 
Tune set. Especially if there is such a large gap 
between the parsers? performance, the best parser 
(here ec) should get higher priority whenever there 
Parsers compared All 7 4 best 3 best ec+mc+dz 2 best 3 worst 
Who is correct How many times correct 
ec 1.7 % 3.0 % 4.1 % 4.5 % 8.1 %  
z? 1.2 % 2.0 % 3.3 %    
mc 0.9 % 1.7 % 2.7 % 2.9 % 6.2 %  
thr 0.4 %     4.9 % 
thp 0.4 %     4.4 % 
dz 0.3 % 1.0 %  2.2 %   
a single parser 
(all other wrong) 
thl 0.3 %     4.3 % 
all seven parsers 42.5 %      
at least six 58.1 %      
at least five 68.4 %      
at least four 76.8 % 58.0 %     
at least three 84.0 % 75.1 % 63.6 % 64.7 %  50.6 % 
at least two 90.4 %  82.9 % 82.4 % 75.5 % 69.2 % 
at least one 95.8 % 94.0 % 93.0 % 92.0 % 89.8 % 82.7 % 
 
Table 2: Comparison of various groups of parsers. All percentages refer to the share of the total words in 
test data, attached correctly. The ?single parser? part shows shares of the data where a single parser is the 
only one to know how to parse them. The sizes of the shares should correlate with the uniqueness of the 
individual parsers? strategies and with their contributions to the overall success. The ?at least? rows give 
clues about what can be got by majority voting (if the number represents over 50 % of parsers compared) 
or by hypothetical oracle selection (if the number represents 50 % of the parsers or less, an oracle would 
generally be needed to point to the parsers that know the correct attachment). 
 
173
is no clear majority of votes. Van Halteren et al 
(1998) have generalized this approach for higher 
number of classifiers in their TotPrecision voting 
method. The vote of each classifier (parser) is 
weighted by their respective accuracy. For in-
stance, mc + z? would outvote ec + thr, as 81.7 + 
74.3 = 156 > 154.6 = 83.6 + 71.0. 
4.2 Stacking 
If the world were ideal, we would have an oracle, 
able to always select the right parser. In such situa-
tion our selection of parsers would grant the accu-
racy as high as 95.8 %. We attempt to imitate the 
oracle by a second-level classifier that learns from 
the Tune set, which parser is right in which situa-
tions. Such technique is usually called classifier 
stacking. Parallel to (van Halteren et al, 1998), we 
ran experiments with two stacked classifiers, 
Memory-Based, and Decision-Tree-Based. This 
approach roughly corresponds to (Henderson and 
Brill, 1999)?s Na?ve Bayes parse hybridization. 
4.3 Unbalanced combining 
For applications preferring precision to recall, un-
balanced combination ? introduced by Brill and 
Hladk? in (Haji  et al, 1998) ? may be of inter-
est. In this method, all dependencies proposed by 
at least half of the parsers are included. The term 
unbalanced reflects the fact that now precision is 
not equal to recall: some nodes lack the link to 
their parents. Moreover, if the number of member 
parsers is even, a node may get two parents. 
4.4 Switching 
Finally, we develop a technique that considers the 
whole dependency structure rather than each de-
pendency alone. The aim is to check that the result-
ing structure is a tree, i.e. that the dependency-
selecting procedure does not introduce cycles.1 
Henderson and Brill prove that under certain con-
ditions, their parse hybridization approach cannot 
                                                   
1
 One may argue that ?treeness? is not a necessary condition 
for the resulting structure, as the standard accuracy measure 
does not penalize non-trees in any way (other than that there is 
at least one bad dependency). Interestingly enough, even some 
of the component parsers do not produce correct trees at all 
times. However, non-trees are both linguistically and techni-
cally problematic, and it is good to know how far we can get 
with the condition in force. 
introduce crossing brackets. This might seem an 
analogy to our problem of introducing cycles ? 
but unfortunately, no analogical lemma holds. As a 
workaround, we have investigated a crossbreed 
approach between Henderson and Brill?s Parser 
Switching, and the voting methods described 
above. After each step, all dependencies that would 
introduce a cycle are banned. The algorithm is 
greedy ? we do not try to search the space of de-
pendency combinations for other paths. If there are 
no allowed dependencies for a word, the whole 
structure built so far is abandoned, and the struc-
ture suggested by the best component parser is 
used instead.2 
5 Experiments and results 
5.1 Voting 
We have run several experiments where various 
selections of parsers were granted the voting right. 
In all experiments, the TotPrecision voting scheme 
of (van Halteren et al, 1998) has been used. The 
voting procedure is only very moderately affected 
by the Tune set (just the accuracy figures on that 
set are used), so we present results on both the Test 
and the Tune sets. 
 
Accuracy Voters Tune Test 
ec (baseline) 83.6 85.0 
all seven 84.0 85.4 
ec+mc+dz 84.9 86.2 
all but thp 84.9 86.3 
ec+mc+z?+dz+thr 85.1 86.5 
ec+mc+z? 85.2 86.7 
ec+mc+z?+dz 85.6 87.0 
Table 3: Results of voting experiments. 
 
According to the results, the best voters pool 
consists of the two best parsers, accompanied by 
                                                   
2
 We have not encountered such situation in our test data. 
However, it indeed is possible, even if all the component pars-
ers deliver correct trees, as can be seen from the following 
example. Assume we have a sentence #ABCD and parsers P1 
(85 votes), P2 (83 votes), P3 (76 votes). P1 suggests the tree 
A?D?B?C?#, P2 suggests B?D?A?C?#, P3 suggests 
B?D?A?#, C?#. Then the superparser P gradually intro-
duces the following dependencies: 1. A?D; 2. B?D; 
3. C?#; 4. D?A or D?B possible but both lead to a cycle. 
174
the two average parsers. The table also suggests 
that number of diverse strategies is more important 
than keeping high quality standard with all the 
parsers. Apart from the worst parser, all the other 
together do better than just the first two and the 
fourth. (On the other hand, the first three parsers 
are much harder to beat, apparently due to the ex-
treme distance of the strategy of z? parser from all 
the others.) 
Even the worst performing parser combination 
(all seven parsers) is significantly3 better than the 
best component parser alone. 
We also investigated some hand-invented voting 
schemes but no one we found performed better 
than the ec+mc+z?+dz combination above. 
Some illustrative results are given in the Ta-
ble 4. Votes were not weighted by accuracy in 
these experiments, but accuracy is reflected in the 
priority given to ec and mc by the human scheme 
inventor. 
 
Accuracy Voters Selection 
scheme Tune Test 
all seven most votes 
or ec 
82.8 84.3 
all seven 
at least 
half, or ec 
if there is 
no absolute 
majority 
84.4 85.8 
all seven 
absolute 
majority, 
or ec+2, or 
mc+2, or 
ec 
84.6 85.9 
Table 4: Voting under hand-invented schemes. 
 
5.2 Stacking ? using context 
We explored several ways of using context in 
pools of three parsers.4 If we had only three parsers 
we could use context to detect two kinds of situa-
tions: 
                                                   
3
 All significance claims refer to the Wilcoxon Signed Ranks 
Test at the level of p = 0.001. 
4
 Similar experiments could be (and have been) run for sets of 
more parsers as well. However, the number of possible fea-
tures is much higher and the data sparser. We were not able to 
gain more accuracy on context-sensitive combination of more 
parsers. 
1. Each parser has its own proposal and a 
parser other than ec shall win. 
2. Two parsers agree on a common pro-
posal but even so the third one should 
win. Most likely the only reasonable in-
stance is that ec wins over mc + the 
third one. 
?Context? can be represented by a number of 
features, starting at morphological tags and ending 
up at complex queries on structural descriptions. 
We tried a simple memory-based approach, and a 
more complex approach based on decision trees. 
Within the memory-based approach, we use just 
the core features the individual parsers themselves 
train on: the POS tags (morphological tags or m-
tags in PDT terminology). We consider the m-tag 
of the dependent node, and the m-tags of the gov-
ernors proposed by the individual parsers. 
We learn the context-based strengths and weak-
nesses of the individual parsers on their perform-
ance on the Tune data set. In the following table, 
there are some examples of contexts in which ec is 
better than the common opinion of mc + dz. 
 
Dep. 
tag 
Gov. 
tag 
(ec) 
Context 
occurrences 
No. of 
times 
ec was 
right 
Percent 
cases ec 
was 
right 
J^ # 67 44 65.7 
Vp J^ 53 28 52.8 
VB J^ 46 26 56.5 
N1 Z, 38 21 55.3 
Rv Vp 25 13 52.0 
Z, Z, 15 8 53.3 
A1 N1 15 8 53.3 
Vje J^ 14 9 64.3 
N4 Vf 12 9 75.0 
Table 5: Contexts where ec is better than mc+dz. 
J^ are coordination conjunctions, # is the root, V* 
are verbs, Nn are nouns in case n, R* are preposi-
tions, Z* are punctuation marks, An are adjectives. 
 
For the experiment with decision trees, we used 
the C5 software package, a commercial version of 
the well-known C4.5 tool (Quinlan, 1993). We 
considered the following features: 
For each of the four nodes involved (the de-
pendent and the three governors suggested by the 
three component parsers): 
175
? 12 attributes derived from the morpho-
logical tag (part of speech, subcategory, 
gender, number, case, inner gender, in-
ner number, person, degree of compari-
son, negativeness, tense and voice) 
? 4 semantic attributes (such as Proper-
Name, Geography etc.) 
For each of the three governor-dependent pairs 
involved: 
? mutual position of the two nodes (Left-
Neighbor, RightNeighbor, LeftFar, 
RightFar) 
? mutual position expressed numerically 
? for each parser pair a binary flag 
whether they do or do not share opin-
ions 
The decision tree was trained only on situations 
where at least one of the three parsers was right 
and at least one was wrong. 
 
Voters Scheme Accuracy 
ec+mc+dz context free 86.2 
ec+mc+dz memory-based 86.3 
ec+mc+z? context free 86.7 
ec+mc+z? decision tree 86.9 
Table 6: Context-sensitive voting. Contexts trained 
on the Tune data set, accuracy figures apply to the 
Test data set. Context-free results are given for the 
sake of comparison. 
 
It turns out that there is very low potential in the 
context to improve the accuracy (the improvement 
is significant, though). The behavior of the parsers 
is too noisy as to the possibility of formulating 
some rules for prediction, when a particular parser 
is right. C5 alone provided a supporting evidence 
for that hypothesis, as it selected a very simple tree 
from all the features, just 5 levels deep (see Fig-
ure 1). 
Henderson and Brill (1999) also reported that 
context did not help them to outperform simple 
voting. Although it is risky to generalize these ob-
servations for other treebanks and parsers, our en-
vironment is quite different from that of Henderson 
and Brill, so the similarity of the two observations 
is at least suspicious. 
5.3 Unbalanced combining 
Finally we compare the balanced and unbalanced 
methods. Expectedly, precision of the unbalanced 
combination of odd number of parsers rose while 
recall dropped slightly. A different situation is ob-
served if even number of parsers vote and more 
than one parent can be selected for a node. In such 
case, precision drops in favor of recall. 
 
Method Precision Recall F-measure 
ec only 
(baseline) 85.0 
balanced 
(all seven) 85.4 
unbalanced 
(all seven) 90.7 78.6 84.2 
balanced 
(best four) 87.0 
unbalanced 
(best four) 85.4 87.7 86.5 
balanced 
(ec+mc+dz) 86.2 
unbalanced 89.5 84.0 86.7 
 agreezzmc = yes: zz (3041/1058) 
 agreezzmc = no: 
 :...agreemcec = yes: ec (7785/1026) 
     agreemcec = no: 
     :...agreezzec = yes: ec (2840/601) 
         agreezzec = no: 
         :...zz_case = 6: zz (150/54) 
             zz_case = 3: zz (34/10) 
             zz_case = X: zz (37/20) 
             zz_case = undef: ec (2006/1102) 
             zz_case = 7: zz (83/48) 
             zz_case = 2: zz (182/110) 
             zz_case = 4: zz (108/57) 
             zz_case = 1: ec (234/109) 
             zz_case = 5: mc (1) 
             zz_case = root: 
             :...ec_negat = A: mc (117/65) 
                 ec_negat = undef: ec (139/65) 
                 ec_negat = N: ec (1) 
                 ec_negat = root: ec (2) 
 
Figure 1. The decision tree for ec+mc+z?, 
learned by C5. Besides pairwise agreement be-
tween the parsers, only morphological case and 
negativeness matter. 
176
Method Precision Recall F-measure 
(ec+mc+dz) 
balanced 
(ec+mc+z?) 86.7 
unbalanced 
(ec+mc+z?) 90.2 84.7 87.3 
Table 7: Unbalanced vs. balanced combining. All 
runs ignored the context. Evaluated on the Test 
data set. 
 
5.4 Switching 
Out of the 3,673 sentences in our Test set, 91.6 % 
have been rendered as correct trees in the balanced 
decision-tree based stacking of ec+mc+z?+dz (our 
best method). 
After we banned cycles, the accuracy dropped 
from 97.0 to 96.9 %. 
6 Comparison to related work 
Brill and Hladk? in (Haji  et al, 1998) were able to 
improve the original accuracy of the mc parser on 
PDT 0.5 e-test data from 79.1 to 79.9 (a nearly 4% 
reduction of the error rate). Their unbalanced5 vot-
ing pushed the F-measure from 79.1 to 80.4 (6% 
error reduction). We pushed the balanced accuracy 
of the ec parser from 85.0 to 87.0 (13% error re-
duction), and the unbalanced F-measure from 85.0 
to 87.7 (18% reduction). Note however that there 
were different data and component parsers (Haji  
et al found bagging the best parser better than 
combining it with other that-time-available pars-
ers). This is the first time that several strategically 
different dependency parsers have been combined. 
(Henderson and Brill, 1999) improved their best 
parser?s F-measure of 89.7 to 91.3, using their na-
?ve Bayes voting on the Penn TreeBank constituent 
structures (16% error reduction). Here, even the 
framework is different, as has been explained 
above. 
7 Conclusion 
We have tested several approaches to combining of 
dependency parsers. Accuracy-aware voting of the 
four best parsers turned out to be the best method, 
as it significantly improved the accuracy of the 
best component from 85.0 to 87.0 % (13 % error 
                                                   
5
 Also alternatively called unrestricted. 
rate reduction). The unbalanced voting lead to the 
precision as high as 90.2 %, while the F-measure 
of 87.3 % outperforms the best result of balanced 
voting (87.0). 
At the same time, we found that employing con-
text to this task is very difficult even with a well-
known and widely used machine-learning ap-
proach. 
The methods are language independent, though 
the amount of accuracy improvement may vary 
according to the performance of the available pars-
ers. 
Although voting methods are themselves not 
new, as far as we know we are the first to propose 
and evaluate their usage in full dependency pars-
ing. 
8 Acknowledgements 
Our thanks go to the creators of the parsers used 
here for making their systems available. 
The research has been supported by the Czech 
Academy of Sciences, the ?Information Society? 
program, project No. 1ET101470416. 
References  
Andrew Borthwick, John Sterling, Eugene Agichtein, 
Ralph Grishman. 1998. Exploiting Diverse Knowl-
edge Sources via Maximum Entropy in Named Entity 
Recognition. In: Eugene Charniak (ed.): Proceedings 
of the 6th Workshop on Very Large Corpora, pp. 
152?160. Universit? de Montr?al, Montr?al, Qu?bec. 
Leo Breiman. 1994. Bagging Predictors. Technical Re-
port 421, Department of Statistics, University of 
California at Berkeley, Berkeley, California. 
Eric Brill, Jun Wu. 1998. Classifier Combination for 
Improved Lexical Combination. In: Proceedings of 
the 17th International Conference on Computational 
Linguistics (COLING-98), pp. 191?195. Universit? 
de Montr?al, Montr?al, Qu?bec. 
Eugene Charniak. 2000. A Maximum-Entropy-Inspired 
Parser. In: Proceedings of NAACL. Seattle, Wash-
ington. 
Jennifer Chu-Carroll, Krzysztof Czuba, John Prager, 
Abraham Ittycheriah. 2003. In Question Answering, 
Two Heads Are Better Than One. In: Proceedings of 
the HLT-NAACL. Edmonton, Alberta. 
Michael Collins, Jan Haji   , Eric Brill, Lance Ramshaw, 
Christoph Tillmann. 1999. A Statistical Parser of 
Czech. In: Proceedings of the 37th Meeting of the 
177
ACL, pp. 505?512. University of Maryland, College 
Park, Maryland. 
Jonathan G. Fiscus. 1997. A Post-Processing System to 
Yield Reduced Word Error Rates: Recognizer Output 
Voting Error Reduction (ROVER). In: EuroSpeech 
1997 Proceedings, vol. 4, pp. 1895?1898. Rodos, 
Greece. 
Radu Florian, David Yarowsky. 2002. Modeling Con-
sensus: Classifier Combination for Word Sense Dis-
ambiguation. In: Proceedings of the Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP), pp. 25?32. Philadelphia, Pennsylvania. 
Robert Frederking, Sergei Nirenburg. 1994. Three 
Heads Are Better Than One. In: Proceedings of the 
4th Conference on Applied Natural Language Proc-
essing, pp. 95?100. Stuttgart, Germany. 
Jan Haji   , Eric Brill, Michael Collins, Barbora Hladk?, 
Douglas Jones, Cynthia Kuo, Lance Ramshaw, Oren 
Schwartz, Christoph Tillmann, Daniel Zeman. 1998. 
Core Natural Language Processing Technology Ap-
plicable to Multiple Languages. The Workshop 98 
Final Report. http://www.clsp.jhu.edu/ws98/projects/ 
nlp/report/. Johns Hopkins University, Baltimore, 
Maryland. 
Jan Haji   , Barbora Vidov? Hladk?, Jarmila Panevov?, 
Eva Haji   ov?, Petr Sgall, Petr Pajas. 2001. Prague 
Dependency Treebank 1.0 CD-ROM. Catalog # 
LDC2001T10, ISBN 1-58563-212-0. Linguistic Data 
Consortium, Philadelphia, Pennsylvania. 
Hans van Halteren, Jakub Zav  el, Walter Daelemans. 
1998. Improving Data-Driven Wordclass Tagging by 
System Combination. In: Proceedings of the 17th In-
ternational Conference on Computational Linguistics 
(COLING-98), pp. 491?497. Universit? de Montr?al, 
Montr?al, Qu?bec. 
Hans van Halteren, Jakub Zav  el, Walter Daelemans. 
2001. Improving Accuracy in Word Class Tagging 
through the Combination of Machine Learning Sys-
tems. In: Computational Linguistics, vol. 27, no. 2, 
pp. 199?229. MIT Press, Cambridge, Massachusetts. 
John C. Henderson, Eric Brill. 1999. Exploiting Diver-
sity in Natural Language Processing: Combining 
Parsers. In: Proceedings of the Fourth Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP-99), pp. 187?194. College Park, Maryland. 
Tom?? Holan. 2004. Tvorba z?vislostn?ho syntaktick?ho 
analyz?toru. In: David Obdr??lek, Jana Teskov? 
(eds.): MIS 2004 Josef v D  l, Sborn?k semin?  e. 
Matfyzpress, Praha, Czechia. 
Inui Takashi, Inui Kentaro. 2000. Committee-Based 
Decision Making in Probabilistic Partial Parsing. In: 
Proceedings of the 18th International Conference on 
Computational Linguistics (COLING 2000), pp. 
348?354. Universit?t des Saarlandes, Saarbr?cken, 
Germany. 
J. Ross Quinlan. 1993. C4.5: Programs for Machine 
Learning. Morgan Kaufmann, San Mateo, California. 
Daniel Zeman. 2004. Parsing with a Statistical Depend-
ency Model (PhD thesis). Univerzita Karlova, Praha, 
Czechia. 
178
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 120?125,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Simple Generative Pipeline Approach to Dependency Parsing and Se-
mantic Role Labeling 
 
 
Daniel Zeman 
?stav form?ln? a aplikovan? lingvistiky 
Univerzita Karlova v Praze 
Malostransk? n?mst? 25, Praha, CZ-11800, Czechia 
zeman@ufal.mff.cuni.cz 
 
 
 
 
 
Abstract 
We describe our CoNLL 2009 Shared Task 
system in the present paper. The system in-
cludes three cascaded components: a genera-
tive dependency parser, a classifier for 
syntactic dependency labels and a semantic 
classifier. The experimental results show that 
the labeled macro F1 scores of our system on 
the joint task range from 43.50% (Chinese) to 
57.95% (Czech), with an average of 51.07%. 
1 Introduction 
The CoNLL 2009 shared task is an extension of 
the tasks addressed in previous years: unlike the 
English-only 2008 task, the present year deals with 
seven languages; and unlike 2006 and 2007, se-
mantic role labeling is performed atop the surface 
dependency parsing. 
We took part in the closed challenge of the joint 
task.1 The input of our system contained gold stan-
dard lemma, part of speech and morphological fea-
tures for each token. Tokens which were 
considered predicates were marked in the input 
data. The system was required to find the follow-
ing information: 
? parent (syntactic dependency) for each to-
ken 
                                                          
1
 For more details on the two tasks and challenges, see Haji et 
al. (2009). 
? label for each syntactic dependency (to-
ken) 
? label for every predicate 
? for every token (predicate or non-
predicate) A and every predicate P in the 
sentence, say whether there is a semantic 
relation between P and A (A is an argu-
ment of P) and if so, provide a label for the 
relation (role of the argument) 
The organizers of the shared task provided train-
ing and evaluation data (Haji et al, 2006; Sur-
deanu et al, 2008; Burchardt et al, 2006; Taul? et 
al., 2008; Kawahara et al, 2002; Xue and Palmer, 
2009) converted to a uniform CoNLL Shared Task 
format. 
2 System Description 
The system is a sequence of three components: a 
surface syntactic parser, a syntactic tagger that as-
signs labels to the syntactic dependencies and a 
semantic classifier (labels both the predicates and 
the roles of their arguments). We did not attempt to 
gain advantage from training a joint classifier for 
all the subtasks. We did not have time to do much 
beyond putting together the basic infrastructure. 
The components 2 and 3 are thus fairly primitive. 
2.1 Surface Dependency Parser 
We use the parser described by Zeman (2004). The 
parser takes a generative approach. It has a model 
of dependency statistics in which a dependency is 
120
specified by the lemma and tag of the parent and 
the child nodes, by direction (left or right) and ad-
jacency. The core of the algorithm can be de-
scribed as repeated greedy selecting of best-
weighted allowed dependencies and adding them 
to the dependency tree. 
There are other components which affect the de-
pendency selection, too. They range from support-
ing statistical models to a few hard-coded rules. 
However, some features of the parser are designed 
to work with Czech, or even with the Prague De-
pendency Treebank. For instance, there is a spe-
cialized model for coordinative constructions. The 
model itself is statistical but it depends on the PDT 
annotation guidelines in various ways. Most nota-
bly, the training component recognizes coordina-
tion by the Coord dependency label, which is not 
present in other treebanks. Other rules (e.g. the 
constraints on the set of allowed dependencies) 
rely on correct interpretation of the part-of-speech 
tags. 
In order to make the parser less language-
dependent in the multilingual environment of the 
shared task, we disabled most of the abovemen-
tioned treebank-bound features. Of course, it led to 
decreased performance on the Czech data.2 
2.2 Assignment of Dependency Labels 
The system learns surface dependency labels as a 
function of the part-of-speech tags and features of 
the parent and the child node. Almost no back-off 
is applied. The most frequent label for the given 
pair of tags (and feature structures) is always se-
lected. If the pair of tags is unknown, the label is 
based on the features of the child node, and if it is 
unknown, too, the most frequent label of the train-
ing data is selected. 
Obviously, both the training and the labeling 
procedures have to know the dependencies. Gold 
standard dependencies are examined during train-
ing while parser-generated dependencies are used 
for real labeling. 
2.3 Semantic Classifier 
The semantic component solves several tasks. 
First, all predicates have to be labeled. Tokens that 
                                                          
2
 However, the parser ? without adaptation ? would not do 
well on Czech anyway because the PDT tags are presented in 
a different format in the shared task data. 
are considered predicates in the particular treebank 
are marked on input, so this is a simple classifica-
tion problem. Again, we took the path of least re-
sistance and trained the PRED labels as a function 
of gold-standard lemmas. 
Second, we have to find semantic dependencies. 
Any token (predicate or not) can be the argument 
of one or more predicates. These relations may or 
may not be parallel to a syntactic dependency. For 
each token, we need to find out 1. which predicates 
it depends on, and 2. what is the label of its seman-
tic role in this relation? 
The task is complex and there are apparently no 
simple solutions to it. We learn the semantic role 
labels as a function of the gold-standard part of 
speech of the argument, the gold-standard lemma 
of the predicate and the flag whether there is a syn-
tactic dependency between the two nodes or not. 
This approach makes it theoretically possible to 
make one token semantically dependent on more 
than one predicate. However, we have no means to 
control the number of the dependencies. 
3 Results 
The official results of our system are given in 
Table 1. The system made the least syntactic errors 
(attachment and labels) for Japanese. The Japanese 
treebank seems to be relatively easy to parse, as 
many other systems achieved very high scores on 
this data. At the other end of the rating scale, Chi-
nese seems to be the syntactically hardest lan-
guage. Our second-worst syntactic score was for 
Czech, most likely owing to the turning off all lan-
guage-dependent (and Czech-biased) features of 
the parser. 
An obvious feature of the table is the extremely 
poor semantic scores (in contrast to the accuracy of 
surface dependency attachment and labels). While 
the simplicity of the additional models does not 
seem to hurt too much the dependency labeling, it 
apparently is too primitive for semantic role label-
ing. We analyze the errors in more detail in Sec-
tion 4. 
The system is platform-independent;3 we have 
been running all the experiments under Linux on 
an AMD Opteron 848 processor, 2 GHz, with 
32 GB RAM. The running times and memory re-
quirements are shown in Table 2. 
                                                          
3
 It is written entirely in Perl. 
121
To assess the need for data, Table 3 presents se-
lected points on the learning curve of our system. 
The system has been retrained on 25, 50 and 75% 
of the training data for each language (the selection 
process was simple: the first N% of sentences of 
the training data set were used). 
Generally, our method does not seem very data-
hungry. Even for Japanese, with the smallest train-
ing data set, reducing training data to 25% of the 
original size makes the scores drop less than 1% 
point. The drop for other languages lies mostly 
between 1 and 2 points. The exceptions are (unla-
beled) syntactic attachment accuracies of Czech 
and Spanish, and labeled semantic F1 of Spanish 
and Chinese. The Chinese learning curve also con-
tains a nonmonotonic anomaly of syntactic de-
pendency labeling between data sizes of 50 and 
75% (shown in boldface). This can be probably 
explained by uneven distribution of the labels in 
training data. 
As to the comparison of the various languages 
and corpora, Japanese seems to be the most spe-
cific (relatively high scores even with such small 
data). Spanish and Catalan are related languages, 
their treebanks are of similar size, conform to simi-
lar guidelines and were prepared by the same team. 
Their scores are very similar. 
4 Discussion 
In order to estimate sources of errors, we are now 
going to provide some analysis of the data and the 
errors our system does. 
4.1 DEPREL Coverage 
The syntactic tagger (assigns DEPREL syntactic 
labels) and the semantic tagger (assigns PRED and 
APRED labels) are based on simple statistical 
models without sophisticated back-off techniques. 
Language Cs En De Es Ca Ja Zh 
Training sentences 43955 40613 38020 15984 14924 4643 24039 
Training tokens 740532 991535 680710 477810 443317 119144 658680 
Average sentence length 17 24 18 30 30 26 27 
Training minutes 9:21 10:41 8:28 6:17 5:42 1:24 7:01 
Training sentences per secnd 78 63 75 42 44 55 57 
Training tokens per second 1320 1547 1340 1267 1296 1418 1565 
Training rsize memory 3.9 GB 2.2 GB 2.7 GB 2.7 GB 2.4 GB 416 MB 1.5 GB 
Test sentences 4213 2399 2000 1725 1862 500 2556 
Test tokens 70348 57676 31622 50630 53355 13615 73153 
Parsing minutes 6:36 3:11 2:24 5:47 6:05 0:46 5:45 
Parsing sentences per second 10.6 12.6 13.9 5.0 5.1 10.9 7.4 
Parsing tokens per second 178 302 220 146 146 296 212 
Parsing rsize memory 980 MB 566 MB 779 MB 585 MB 487 MB 121 MB 444 MB 
 
Table 2. Time and space requirements of the syntactic parser. 
Language Average Cs En De Es Ca Ja Zh 
Labeled macro F1 51.07 57.95 50.27 49.57 48.90 49.61 57.69 43.50 
OOD lab mac F1 43.67 54.49 48.56 27.97     
Labeled syn accur 64.92 57.06 61.82 69.79 65.98 67.68 82.66 49.48 
Unlab syn accur 70.84 66.04 70.68 72.91 71.22 73.81 83.36 57.87 
Syn labeling accur 79.20 69.10 74.24 84.63 81.83 82.46 95.98 66.13 
OOD lab syn acc 50.20 51.45 62.83 36.31     
OOD unl syn acc 58.08 60.56 71.78 41.90     
OOD syn labeling 69.65 65.64 75.22 68.08     
Semantic lab F1 32.14 58.13 36.05 16.44 25.36 24.19 30.13 34.71 
OOD sem lab F1 32.86 56.83 31.77 9.98     
 
Table 1. The official results of the system. ISO 639-1 language codes are used (cs = Czech, en = English, de = Ger-
man, es = Spanish, ca = Catalan, ja = Japanese, zh = Chinese). ?OOD? means ?out-of-domain test data?. 
122
Sparse data could pose a serious problem. So how 
sparse are the data? Some cue could be drawn from 
Table 3. However, we should also know how often 
the labels had to be assigned to an unknown set of 
input features. 
DEPREL (syntactic dependency label) is esti-
mated based on morphological tag (i.e. POS + 
FEAT) of both the child and parent. If the pair of 
tags is unknown, then it is based on the tag of the 
child, and if it is unknown, too, the most frequent 
label is chosen. Coverage is high: 93 (Czech) to 
97 % (Chinese) of the pairs of tags in test data 
were known from training data. Moreover, the er-
ror rate on the unknown pairs is actually much 
lower than on the whole data!4 
4.2 PRED Coverage 
PRED (predicate sense label) is estimated based on 
lemma. For most languages, this seems to be a 
good selection. Japanese predicate labels are al-
ways identical to lemmas; elsewhere, there are by 
average 1.05 (Chinese) to 1.48 (Spanish) labels per 
lemma; the exception is German with a label-
lemma ratio of 2.33. 
Our accuracy of PRED label assignment ranges 
from 71% (German) to 100% (Japanese). We al-
ways assign the most probable label for the given 
                                                          
4
 This might suggest that the input features are chosen inap-
propriately and that the DEPREL label should be based just on 
the morphology of the child. 
lemma; if the lemma is unknown, we copy the 
lemma to the PRED column. Coverage is not an 
issue here. It goes from 94% (Czech) to almost 
100% (German).5 The accuracy on unknown lem-
mas could probably be improved using the sub-
categorization dictionaries accompanying the 
training data. 
 
Language Lemma PREDs 
1. m?t 77 Cs 2. pijmout 8 
1. take 20 En 2. go 18 
1. kommen 28 De 2. nehmen 25 
1. pasar 10 
1. dar 10 
3. llevar 9 Es 
3. hacer 9 
1. fer 11 Ca 2. pasar 9 
Ja Always 1 PRED per lemma 
1.  (y?o) 8 
1.  (yu) 8 Zh 
1.  (d) 8 
Table 4. Most homonymous predicates. 
                                                          
5
 The coverage of Japanese is 88% but since Japanese PRED 
labels are exact copies of lemmas, even unknown lemmas 
yield 100%-correct labels. 
Score TrSize Average Cs En De Es Ca Ja Zh 
25% 69.38 63.72 69.70 71.36 68.99 72.41 82.58 56.90 
50% 70.14 64.96 70.13 72.11 70.37 72.83 82.99 57.58 
75% 70.51 65.50 70.37 72.50 70.83 73.47 83.17 57.73 
UnLab 
Syn 
Attach 100% 70.84 66.04 70.68 72.91 71.22 73.81 83.36 57.87 
25% 78.47 68.28 73.79 84.21 80.67 81.92 95.70 64.71 
50% 78.94 68.68 74.08 84.44 81.59 81.99 95.86 65.94 
75% 79.03 68.87 74.14 84.51 81.67 82.19 95.97 65.83 
Syn 
Label 
100% 79.20 69.10 74.24 84.63 81.83 82.46 95.98 66.13 
25% 30.10 56.29 34.47 15.51 22.78 22.14 28.91 30.58 
50% 33.85 57.24 35.34 16.03 24.46 23.13 29.60 33.31 
75% 31.76 57.76 35.85 16.29 24.96 23.77 29.96 33.71 
Labeled 
Sem F1 
100% 32.14 58.13 36.05 16.44 25.36 24.19 30.13 34.71 
25% 49.19 55.87 49.06 48.10 46.22 47.76 56.66 40.64 
50% 50.28 56.99 49.66 48.90 47.97 48.53 57.23 42.66 
75% 50.68 57.53 50.01 49.26 48.47 49.21 57.52 42.73 
Labeled 
Macro 
F1 100% 51.07 57.95 50.27 49.57 48.90 49.61 57.69 43.50 
 
Table 3. The learning curve of the principal scores. 
123
4.3 APRED Assignment Analysis 
The most complicated part of the task is the as-
signment of the APRED labels. In a sense, APRED 
labeling is dependency parsing on a deeper level. It 
consists of several sub-problems: 
? Is the node an argument of any predicate at 
all? 
? If so, how many predicates is the node ar-
gument of? Should the predicate be, say, 
coordination, then the node would seman-
tically depend on all members of the coor-
dination. 
? In what way is the semantic dependency 
related to the syntactic dependency be-
tween the node and its syntactic parent? In 
majority of cases, syntactic and semantic 
dependencies go parallel; however, there 
are still a significant number of semantic 
relations for which this assumption does 
not hold.6 
? Once we know that there is a semantic re-
lation (an APRED field should not be 
empty), we still have to figure out the cor-
rect APRED label. This is the semantic 
role labeling (or tagging) proper. 
                                                          
6
 Nearly all Spanish and Catalan semantic dependencies are 
parallel to syntactic ones (but not all syntactic dependencies 
are also semantic); in most other languages, about two thirds 
of semantic relations match syntax. Japanese is the only lan-
guage in which this behavior does not prevail. 
Our system always makes semantic roles paral-
lel to surface syntax. It even does not allow for 
empty APRED if there is a syntactic dependency?
this turned out to be one of the major sources of 
errors.7 
The role labels are estimated based on the 
lemma of the predicate and the part of speech of 
the argument. Low coverage of this pair of features 
in the training data turns to be another major 
source of errors. If the pair is not known from 
training data, the system selects the most frequent 
APRED in the given treebank. Table 5 gives an 
overview of the principal statistics relevant to the 
analysis of APRED errors. 
5 Post-evaluation Experiments 
Finally, we performed some preliminary experi-
ments focused on the syntactic parser. As men-
tioned in Section 2.1, many features of the parser 
have to be turned off unless the parser understands 
the part-of-speech and morphological features. We 
used DZ Interset (Zeman, 2008) to convert Czech 
and English CoNLL POS+FEAT strings to PDT-
like positional tags. Then we switched back on the 
parser options that use up the tags and re-ran pars-
ing. The results (Table 6) confirm that the tag ma-
nipulation significantly improves Czech parsing 
while it does not help with English. 
 
                                                          
7
 This is a design flaw that we overlooked. Most likely, mak-
ing empty APRED one of the predictable values would im-
prove accuracy. 
Language Cs En De Es Ca Ja Zh 
Potential APRED slots 1287545 195029 12066 192103 197976 57394 329757 
Filled in APREDs 87934 32968 10480 49904 52786 6547 49047 
Feature pair coverage (%) 46.05 40.04 14.99 29.34 29.89 18.31 38.08 
Non-empty APRED accuracy 73.19 64.65 67.37 56.90 57.89 59.20 68.77 
Unlabeled precision 34.94 26.86 10.88 21.71 20.25 9.13 25.66 
Unlabeled recall 62.61 63.86 97.52 93.40 92.72 22.10 67.82 
Unlabeled F 44.86 37.81 19.57 35.23 33.24 12.93 37.23 
Labeled precision 25.58 17.36 7.33 12.35 11.72 5.41 17.64 
Labeled recall 45.83 41.28 65.70 53.15 53.67 13.08 46.64 
Labeled F 32.83 24.44 13.19 20.05 19.24 7.65 25.60 
 
Table 5. APRED detailed analysis. Non-empty APRED accuracy includes only APRED cells that were non-empty 
both in gold standard and system output. Feature-pair coverage includes all cells filled by the system. Unlabeled preci-
sion and recall count non-empty vs. empty APREDs without respect to their actual labels. Counted on development 
data with gold-standard surface syntax. 
124
 Cs En 
Before 65.81 69.48 
After 71.76 68.92 
Table 6. Unlabeled attachment accuracy on de-
velopment data before and after tagset conversion. 
6 Conclusion 
We described one of the systems that participated 
in the CoNLL 2009 Shared Task. We analyzed the 
weaknesses of the system and identified possible 
room for improvement. The most important point 
to focus on in future work is specifying where 
APRED should be filled in. The links between syn-
tactic and semantic structures have to be studied 
further. Subcategorization frames could probably 
help improve these decisions, too?our present 
system ignores the subcategorization dictionaries 
that accompany the participating treebanks. 
Acknowledgments 
This research has been supported by the Ministry 
of Education of the Czech Republic, project No. 
MSM0021620838. 
References  
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea 
Kowalski, Sebastian Pad? and Manfred Pinkal. 2006. 
The SALSA Corpus: a German Corpus Resource for 
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and 
Evaluation (LREC-2006). Genova, Italy. 
Jan Haji, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?s 
M?rquez, Adam Meyers, Joakim Nivre, Sebastian 
Pad?, Jan ?tp?nek, Pavel Stra?k, Mihai Surdeanu, 
Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009 
Shared Task: Syntactic and Semantic Dependencies 
in Multiple Languages. Proceedings of the 13th Con-
ference on Computational Natural Language Lear-
ning (CoNLL-2009). June 4-5. pp. 3-22. Boulder, 
Colorado, USA. 
Jan Haji, Jarmila Panevov?, Eva Hajiov?, Petr Sgall, 
Petr Pajas, Jan ?tp?nek, Ji? Havelka, Marie Miku-
lov? and Zdenk ?abokrtsk?. 2006. The Prague De-
pendency Treebank 2.0. CD-ROM. Linguistic Data 
Consortium, Philadelphia, Pennsylvania, USA. ISBN 
1-58563-370-4. LDC Cat. No. LDC2006T01. URL: 
http://ldc.upenn.edu/. 
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 
2002. Construction of a Japanese Relevance-tagged 
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). pp. 2008-2013. Las Palmas, Spain. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Llu?s M?rquez and Joakim Nivre. 2008. The CoNLL-
2008 Shared Task on Joint Parsing of Syntactic and 
Semantic Dependencies. In Proceedings of the 12th 
Conference on Computational Natural Language 
Learning (CoNLL-2008). August 16 ? 17. Manches-
ter, UK. 
Mariona Taul?, Maria Ant?nia Mart? and Marta Reca-
sens. 2008. AnCora: Multilevel Annotated Corpora 
for Catalan and Spanish. Proceedings of the 6th In-
ternational Conference on Language Resources and 
Evaluation (LREC-2008). Marrakech, Morocco. 
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank.  Natural Language 
Engineering, 15(1):143-172. 
Daniel Zeman. 2004. Parsing with a Statistical Depend-
ency Model (PhD thesis). Univerzita Karlova, Praha, 
Czechia. URL: http://ufal.mff.cuni.cz/~zeman/pro-
jekty/parser/index.html 
Daniel Zeman. 2008. Reusable Tagset Conversion Us-
ing Tagset Drivers. In Proceedings of the 6th Interna-
tional Conference on Language Resources and 
Evaluation (LREC-2008). ISBN 2-9517408-4-0. 
Marrakech, Morocco. 
125
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 517?527,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Coordination Structures in Dependency Treebanks
Martin Popel, David Marec?ek, Jan S?te?pa?nek, Daniel Zeman, Zdene?k Z?abokrtsky?
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics (U?FAL)
Malostranske? na?me?st?? 25, CZ-11800 Praha, Czechia
{popel|marecek|stepanek|zeman|zabokrtsky}@ufal.mff.cuni.cz
Abstract
Paratactic syntactic structures are noto-
riously difficult to represent in depen-
dency formalisms. This has painful con-
sequences such as high frequency of pars-
ing errors related to coordination. In other
words, coordination is a pending prob-
lem in dependency analysis of natural lan-
guages. This paper tries to shed some
light on this area by bringing a system-
atizing view of various formal means de-
veloped for encoding coordination struc-
tures. We introduce a novel taxonomy of
such approaches and apply it to treebanks
across a typologically diverse range of 26
languages. In addition, empirical obser-
vations on convertibility between selected
styles of representations are shown too.
1 Introduction
In the last decade, dependency parsing has grad-
ually been receiving visible attention. One of
the reasons is the increased availability of depen-
dency treebanks, be they results of genuine depen-
dency annotation projects or converted automat-
ically from previously existing phrase-structure
treebanks.
In both cases, a number of decisions have to be
made during the construction or conversion of a
dependency treebank. The traditional notion of
dependency does not always provide unambiguous
solutions, e.g. when it comes to attaching func-
tional words. Worse, dependency representation is
at a loss when it comes to representing paratactic
linguistic phenomena such as coordination, whose
nature is symmetric (two or more conjuncts play
the same role), as opposed to the head-modifier
asymmetry of dependencies.1
1We use the term modifier (or child) for all types of de-
pendent nodes including arguments.
The dominating solution in treebank design is to
introduce artificial rules for the encoding of coor-
dination structures within dependency trees using
the same means that express dependencies, i.e., by
using edges and by labeling of nodes or edges. Ob-
viously, any tree-shaped representation of a coor-
dination structure (CS) must be perceived only as
a ?shortcut? since relations present in coordination
structures form an undirected cycle, as illustrated
already by Tesnie`re (1959). For example, if a noun
is modified by two coordinated adjectives, there
is a (symmetric) coordination relation between the
two conjuncts and two (asymmetric) dependency
relations between the conjuncts and the noun.
However, as there is no obvious linguistic in-
tuition telling us which tree-shaped CS encoding
is better and since the degree of freedom has sev-
eral dimensions, one can find a number of distinct
conventions introduced in particular dependency
treebanks. Variations exist both in topology (tree
shape) and labeling. The main goal of this pa-
per is to give a systematic survey of the solutions
adopted in these treebanks.
Naturally, the interplay of dependency and co-
ordination links in a single tree leads to serious
parsing issues.2 The present study does not try to
decide which coordination style is the best from
the parsing point of view.3 However, we believe
that our survey will substantially facilitate experi-
ments in this direction in the future, at least by ex-
ploring and describing the space of possible can-
didates.
2CSs have been reported to be one of the most frequent
sources of parsing errors (Green and Z?abokrtsky?, 2012; Mc-
Donald and Nivre, 2007; Ku?bler et al, 2009; Collins, 2003).
Their impact on quality of dependency-based machine trans-
lation can also be substantial; as documented on an English-
to-Czech dependency-based translation system (Popel and
Z?abokrtsky?, 2009), 39% of serious translation errors which
are caused by wrong parsing have to do with coordination.
3There might be no such answer, as different CS conven-
tions might serve best for different applications or for differ-
ent parser architectures.
517
The rest of the paper is structured as follows.
Section 2 describes some known problems related
to CS. Section 3 shows possible ?styles? for rep-
resenting CS. Section 4 lists treebanks whose CS
conventions we studied. Section 5 presents empir-
ical observations on CS convertibility. Section 6
concludes the paper.
2 Related work
Let us first recall the basic well-known character-
istics of CSs.
In the simplest case of a CS, a coordinating
conjunction joins two (usually syntactically and
semantically compatible) words or phrases called
conjuncts. Even this simplest case is difficult to
represent within a dependency tree because, in the
words of Lombardo and Lesmo (1998): Depen-
dency paradigms exhibit obvious difficulties with
coordination because, differently from most lin-
guistic structures, it is not possible to characterize
the coordination construct with a general schema
involving a head and some modifiers of it.
Proper formal representation of CSs is further
complicated by the following facts:
? CSs with more than two conjuncts (multi-
conjunct CSs) exist and are frequent.
? Besides ?private? modifiers of individual
conjuncts, there are modifiers shared by
all conjuncts, such as in ?Mary came and
cried?. Shared modifiers may appear along-
side with private modifiers of particular con-
juncts.
? Shared modifiers can be coordinated, too:
?big and cheap apples and oranges?.
? Nested (embedded) coordinations are possi-
ble: ?John and Mary or Sam and Lisa?.
? Punctuation (commas, semicolons, three
dots) is frequently used in CSs, mostly with
multi-conjunct coordinations or juxtaposi-
tions which can be interpreted as CSs with-
out conjunctions (e.g. ?Don?t worry, be
happy!?).
? In many languages, comma or other punctu-
ation mark may play the role of the main co-
ordinating conjunction.
? The coordinating conjunction may be a mul-
tiword expression (?as well as?).
? Deficient CSs with a single conjunct exist.
? Abbreviations like ?etc.? comprise both the
conjunction and the last conjunct.
? Coordination may form very intricate struc-
tures when combined with ellipsis. For ex-
ample, a conjunct can be elided while its ar-
guments remain in the sentence, such as in
the following traditional example: ?I gave
the books to Mary and the records to Sue.?
? The border between paratactic and hypotactic
surface means of expressing coordination re-
lations is fuzzy. Some languages can use en-
clitics instead of conjunctions/prepositions,
e.g. Latin ?Senatus Populusque Romanus?.
Purely hypotactic surface means such as the
preposition in ?John with Mary? occur too.4
? Careful semantic analysis of CSs discloses
additional complications: if a node is mod-
ified by a CS, it might happen that it is
the node itself (and not its modifiers) what
should be semantically considered as a con-
junct. Note the difference between ?red and
white wine? (which is synonymous to ?red
wine and white wine?) and ?red and white
flag of Poland?. Similarly, ?five dogs and
cats? has a different meaning than ?five dogs
and five cats?.
Some of these issues were recognized already
by Tesnie`re (1959). In his solution, conjuncts are
connected by vertical edges directly to the head
and by horizontal edges to the conjunction (which
constitutes a cycle in every CS). Many different
models have been proposed since, out of which the
following are the most frequently used ones:
? MS = Mel?c?uk style used in the Meaning-
Text Theory (MTT): the first conjunct is the
head of the CS, with the second conjunct at-
tached as a dependent of the first one, third
conjunct under the second one, etc. Coor-
dinating conjunction is attached under the
penultimate conjunct, and the last conjunct
is attached under the conjunction (Mel?c?uk,
1988),
? PS = Prague Dependency Treebank (PDT)
style: all conjuncts are attached under the
coordinating conjunction (along with shared
modifiers, which are distinguished by a spe-
cial attribute) (Hajic? et al, 2006),
4As discussed by Stassen (2000), all languages seem to
have some strategy for expressing coordination. Some of
them lack the paratactic surface means (the so called WITH-
languages), but the hypotactic surface means are present al-
most always.
518
? SS = Stanford parser style:5 the first conjunct
is the head and the remaining conjuncts (as
well as conjunctions) are attached under it.
One can find various arguments supporting the
particular choices. MTT possesses a complex
set of linguistic criteria for identifying the gov-
ernor of a relation (see Mazziotta (2011) for an
overview), which lead to MS. MS is preferred in
a rule-based dependency parsing system of Lom-
bardo and Lesmo (1998). PS is advocated by
S?te?pa?nek (2006) who claims that it can represent
shared modifiers using a single additional binary
attribute, while MS would require a more complex
co-indexing attribute. An argumentation of Tratz
and Hovy (2011) follows a similar direction: We
would like to change our [MS] handling of coordi-
nating conjunctions to treat the coordinating con-
junction as the head [PS] because this has fewer
ambiguities than [MS]. . .
We conclude that the influence of the choice of
coordination style is a well-known problem in de-
pendency syntax. Nevertheless, published works
usually focus only on a narrow ad-hoc selection of
few coordination styles, without giving any sys-
tematic perspective.
Choosing a file format presents a different prob-
lem. Despite various efforts to standardize lin-
guistic annotation,6 no commonly accepted stan-
dard exists. The primitive format used for CoNLL
shared tasks is widely used in dependency parsing,
but its weaknesses have already been pointed out
(cf. Stran?a?k and S?te?pa?nek (2010)). Moreover, par-
ticular treebanks vary in their contents even more
than in their format, i.e. each treebank has its own
way of representing prepositions or different gran-
ularity of syntactic labels.
3 Variations in representing
coordination structures
Our analysis of variations in representing coordi-
nation structures is based on observations from a
set of dependency treebanks for 26 languages.7
5We use the already established MS-PS-SS distinction to
facilitate literature overview; as shown in Section 3, the space
of possible coordination styles is much richer.
6For example, TEI (TEI Consortium, 2013), PML (Hana
and S?te?pa?nek, 2012), SynAF (ISO 24615, 2010).
7The primary data sources are the following: Ancient
Greek: Ancient Greek Dependency Treebank (Bamman and
Crane, 2011), Arabic: Prague Arabic Dependency Tree-
bank 1.0 (Smrz? et al, 2008), Basque: Basque Dependency
Treebank (larger version than CoNLL 2007 generously pro-
In accordance with the usual conventions, we as-
sume that each sentence is represented by one de-
pendency tree, in which each node corresponds
to one token (word or punctuation mark). Apart
from that, we deliberately limit ourselves to CS
representations that have shapes of connected sub-
graphs of dependency trees.
We limit our inventory of means of expressing
CSs within dependency trees to (i) tree topology
(presence or absence of a directed edge between
two nodes, Section 3.1), and (ii) node labeling
(additional attributes stored insided nodes, Sec-
tion 3.2).8 Further, we expect that the set of pos-
sible variations can be structured along several di-
mensions, each of which corresponds to a certain
simple characteristic (such as choosing the left-
most conjunct as the CS head, or attaching shared
modifiers below the nearest conjunct). Even if it
does not make sense to create the full Cartesian
product of all dimensions because some values
cannot be combined, it allows to explore the space
of possible CS styles systematically.9
3.1 Topological variations
We distinguish the following dimensions of topo-
logical variations of CS styles (see Figure 1):
Family ? configuration of conjuncts. We di-
vide the topological variations into three main
groups, labeled as Prague (fP), Moscow (fM), and
vided by IXA Group) (Aduriz and others, 2003), Bulgarian:
BulTreeBank (Simov and Osenova, 2005), Czech: Prague
Dependency Treebank 2.0 (Hajic? et al, 2006), Danish: Dan-
ish Dependency Treebank (Kromann et al, 2004), Dutch:
Alpino Treebank (van der Beek and others, 2002), English:
Penn TreeBank 3 (Marcus et al, 1993), Finnish: Turku De-
pendency Treebank (Haverinen et al, 2010), German: Tiger
Treebank (Brants et al, 2002), Greek (modern): Greek De-
pendency Treebank (Prokopidis et al, 2005), Hindi, Ben-
gali and Telugu: Hyderabad Dependency Treebank (Husain
et al, 2010), Hungarian: Szeged Treebank (Csendes et al,
2005), Italian: Italian Syntactic-Semantic Treebank (Mon-
temagni and others, 2003), Latin: Latin Dependency Tree-
bank (Bamman and Crane, 2011), Persian: Persian Depen-
dency Treebank (Rasooli et al, 2011), Portuguese: Floresta
sinta?(c)tica (Afonso et al, 2002), Romanian: Romanian De-
pendency Treebank (Ca?la?cean, 2008), Russian: Syntagrus
(Boguslavsky et al, 2000), Slovene: Slovene Dependency
Treebank (Dz?eroski et al, 2006), Spanish: AnCora (Taule?
et al, 2008), Swedish: Talbanken05 (Nilsson et al, 2005),
Tamil: TamilTB (Ramasamy and Z?abokrtsky?, 2012), Turk-
ish: METU-Sabanci Turkish Treebank (Atalay et al, 2003).
8Edge labeling can be trivially converted to node labeling
in tree structures.
9The full Cartesian product of variants in Figure 1 would
result in topological 216 variants, but only 126 are applicable
(the inapplicable combinations are marked with ??? in Fig-
ure 1). Those 126 topological variants can be further com-
bined with labeling variants defined in Section 3.2.
519
Main family Prague family (code fP)[14 treebanks]
Moscow family (code fM)
[5 treebanks]
Stanford family (code fS)
[6 treebanks]
Choice of head
Head on left (code hL)
[10 treebanks]
dogs
and,  cats rats dooooogsanoos, oocsan
 trn
Head on right (code hR)
[14 treebanks]
Mixed head (code hM) [1 treebank] A mixture of hL and hR
Attachment of shared modifiers
Shared modifier
below the nearest conjunct
(code sN)
[15 treebanks]
Shared modifier below head
(code sH)
[11 treebanks]
dogs
dogsaaan, caaaaataaaaaarocaaaaaoc
on
dogs an, cct do
dogs
an, cct
orao 
o 
dogsaaan, caaaataaaarocaaaon
oc
Attachment of coordinating conjunction
Coordinating conjunction
below previous conjunct (code cP)
[2 treebanks]
?
dogs
and,, acst,,racs dooooogsanoooooooooo,san
 ctn
sr 
Coordinating conjunction
below following conjunct (code cF)
[1 treebank]
?
dogssadn,
g c,
adn,tssrdn,dog dooooogsanoooooooooo,san
 ctn
sr 
Coordinating conjunction
between two conjuncts (code cB)
[8 treebanks]
?
dogs
and,  cats rats dooooogsanoos, oocsan
 trn
Coordinating conjunction as the head (code cH)
is the only applicable style for the Prague family [14 treebanks] ? ?
Placement of punctuation
values pP [7 treebanks], pF [1 treebank] and pB [15 treebanks] are analogous to cP, cF and cB
(but applicable also to the Prague family)
Figure 1: Different coordination styles, variations in tree topology. Example phrase: ?(lazy) dogs, cats
and rats?. Style codes are described in Section 3.1.
Stanford (fS) families.10 This first dimension dis-
tinguishes the configuration of conjuncts: in the
Prague family, all the conjuncts are siblings gov-
erned by one of the conjunctions (or a punctuation
fulfilling its role); in the Moscow family, the con-
juncts form a chain where each node in the chain
depends on the previous (or following) node; in
the Stanford family, the conjuncts are siblings ex-
cept for the first (or last) conjunct, which is the
10Names are chosen purely as a mnemonic device, so that
Prague Dependency Treebank belongs to the Prague family,
Mel?c?uk style belongs to the Moscow family, and Stanford
parser style belongs to the Stanford family.
head.11
Choice of head ? leftmost or rightmost. In
the Prague family, the head can be either the left-
most12 (hL) or the rightmost (hR) conjunction or
punctuation. Similarly, in the Moscow and Stan-
ford families, the head can be either the leftmost
(hL) or the rightmost (hR) conjunct. A third op-
11Note that for CSs with just two conjuncts, fM and fS
may look exactly the same (depending on the attachment of
conjunctions and punctuation as described below).
12For simplicity, we use the terms left and right even if
their meaning is reversed for languages with right-to-left
writing systems such as Arabic or Persian.
520
tion (hM) is to mix hL and hR based on some cri-
terion, e.g. the Persian treebank uses hR for coor-
dination of verbs and hL otherwise. For the exper-
iments in Section 5, we choose the head which is
closer to the parent of the whole CS, with the mo-
tivation to make the edge between CS head and its
parent shorter, which may improve parser training.
Attachment of shared modifiers. Shared mod-
ifiers may appear before the first conjunct or after
the last one. Therefore, it seems reasonable to at-
tach shared modifiers either to the CS head (sH),
or to the nearest (i.e. first or last) conjunct (sN).
Attachment of coordinating conjunctions. In
the Moscow family, conjunctions may be either
part of the chain of conjuncts (cB), or they may be
put outside of the chain and attached to the previ-
ous (cP) or following (cF) conjunct. In the Stan-
ford family, conjunctions may be either attached
to the CS head (and therefore between conjuncts)
(cB), or they may be attached to the previous (cP)
or the following (cF) conjunct. The cB option in
both Moscow and Stanford families, treats con-
junctions in the same way as conjuncts (with re-
spect to topology only). In the Prague family, there
is just one option available (cH) ? one of the con-
junctions is the CS head while the others are at-
tached to it.
Attachment of punctuation. Punctuation to-
kens separating conjuncts (commas, semicolons
etc.) could be treated the same way as conjunc-
tions. However, in most treebanks it is treated
differently, so we consider it as well. The val-
ues pP, pF and pB are analogous to cP, cF and
cB except that punctuation may be also attached
to the conjunction in case of pP and pF (other-
wise, a comma before the conjunction would be
non-projectively attached to the member follow-
ing the conjunction).
The three established styles mentioned in Sec-
tion 2 can be defined in terms of the newly intro-
duced abbreviations: PS = fPhRsHcHpB, MS =
fMhLsNcBp?, and SS = fShLsNcBp?.13
3.2 Labeling variations
Most state-of-the-art dependency parsers can pro-
duce labeled edges. However, the parsers produce
only one label per edge. To fully capture CSs,
we need more than one label, because there are
several aspects involved (see the initial assump-
13The question marks indicate that the original Mel?c?uk
and Stanford parser styles ignore punctuation.
tions in Section 3): We need to identify the co-
ordinating conjunction (its POS tag might not be
enough), conjuncts, shared modifiers, and punctu-
ation that separates conjuncts. Besides that, there
should be a label classifying the dependency rela-
tion between the CS and its parent.
Some of the information can be retrieved from
the topology of the tree and the ?main label? of
each node, but not everything. The additional in-
formation can be attached to the main label, but
such approach obscures the logical structure.
In the Prague family, there are two possible
ways to label a conjunction and conjuncts:
Code dU (?dependency labeled at the upper
level of the CS?). The dependency relation of the
whole CS to its parent is represented by the label
of the conjunction, while the conjuncts are marked
with a special label for conjuncts (e.g. ccof in the
Hyderabad Dependency Treebank).
Code dL (?lower level?). The CS is represented
by a coordinating conjunction (or punctuation if
there is no conjunction) with a special label (e.g.
Coord in PDT). Subsequently, each conjunct has
its own label that reflects the dependency relation
towards the parent of the whole CS, therefore, con-
juncts of the same CS can have different labels,
e.g. ?Who[SUBJ] and why[ADV] did it??
Most Prague family treebanks use sH, i.e.
shared modifiers are attached to the head (coor-
dinating conjunction). Each child of the head has
to belong to one of three sets: conjuncts, shared
modifiers, and punctuation or additional conjunc-
tions. In PDT, conjuncts, punctuation and addi-
tional conjunctions are recognized by specific la-
bels. Any other children of the head are shared
modifiers.
In the Stanford and Moscow families, one of
the conjuncts is the head. In practice, it is never la-
beled as a conjunct explicitly, because the fact that
it is a conjunct can be deduced from the presence
of conjuncts among its children. Usually, the other
conjuncts are labeled as conjuncts; conjunctions
and punctuation also have a special label. This
type of labeling corresponds to the dU type.
Alternatively (as found in the Turkish treebank,
dL), all conjuncts in the Moscow chain have their
own dependency labels and the fact that they are
conjuncts follows from the COORDINATION la-
bels of the conjunction and punctuation nodes be-
tween them.
To represent shared modifiers in the Stan-
521
ford and Moscow families, an additional label
is needed again to distinguish between private
and shared modifiers since they cannot be distin-
guished topologically. Moreover, if nested CSs
are allowed, a binary label is not sufficient (i.e.
?shared? versus ?private?) because it also has to
indicate which conjuncts the shared modifier be-
longs to.14
We use the following binary flag codes for cap-
turing which CS participants are distinguished in
the annotation: m01 = shared modifiers anno-
tated; m10 = conjuncts annotated; m11 = both
annotated; m00 = neither annotated.
4 Coordination Structures in Treebanks
In this section, we identify the CS styles defined
in the previous section as used in the primary tree-
bank data sources; statistical observations (such
as the amount of annotated shared modifiers) pre-
sented here, as well as experiments on CS-style
convertibility presented in Section 5.2, are based
on the normalized shapes of the treebanks as con-
tained in the HamleDT 1.0 treebank collection
(Zeman et al, 2012).15
Some of the treebanks were downloaded indi-
vidually from the web, but most of them came
from previously published collections for depen-
dency parsing campaigns: six languages from
CoNLL-2006 (Buchholz and Marsi, 2006), seven
languages from CoNLL-2007 (Nivre et al, 2007),
two languages from CoNLL-2009 (Hajic? and oth-
ers, 2009), three languages from ICON-2010 (Hu-
sain et al, 2010). Obviously, there is a certain
risk that the CS-related information contained in
the source treebanks was slightly biased by the
properties of the CoNLL format upon conversion.
In addition, many of the treebanks were natively
dependency-based (cf. the 2nd column of Table 1),
but some were originally based on constituents
and thus specific converters to the CoNLL for-
mat had to be created (for instance, the Span-
ish phrase-structure trees were converted to de-
pendencies using a procedure described by Civit
et al (2006); similarly, treebank-specific convert-
ers have been used for other languages). Again,
14This is not needed in Prague family where shared modi-
fiers are attached to the conjunction provided that each shared
modifier is shared by conjuncts that form a full subtree to-
gether with their coordinating conjunctions; no exceptions
were found during the annotation process of the PDT.
15A subset of the treebanks whose license
terms permit redistribution is available directly at
http://ufal.mff.cuni.cz/hamledt/.
Danish Romanian
dogsa
n,  anctttr  attt,

ddog
san,n           cntnrnsn     c,n
Hungarian
dogsadnnnn,nnnn ctrdadnnnnrncgdasd
Figure 2: Annotation styles of a few treebanks do
not fit well into the multidimensional space de-
fined in Section 3.1.
there is some risk that the CS-related information
contained in treebanks resulting from such conver-
sions is slightly different from what was intended
in the very primary annotation.
There are several other languages (e.g. Esto-
nian or Chinese) which are not included in our
study, despite of the fact that constituency tree-
banks do exist for them. The reason is that the
choice of their CS style would be biased, because
no independent converters exist ? we would have
to convert them to dependencies ourselves. We
also know about several more dependency tree-
banks that we have not processed yet.
Table 1 shows 26 languages whose treebanks
we have studied from the viewpoint of their CS
styles. It gives the basic quantitative properties of
the treebanks, their CS style in terms of the tax-
onomy introduced in Section 3, as well as statis-
tics related to CSs: the average number of CSs per
100 tokens, the average number of conjuncts per
one CS, the average number of shared modifiers
per one CS,16 and the percentage of nested CSs
among all CSs. The reader can return to Figure
1 to see the basic statistics on the ?popularity? of
individual design decisions among the developers
of dependency treebanks or constituency treebank
converters.
CS styles of most treebanks are easily classifi-
able using the codes introduced in Section 3, plus
a few additional codes:
? p0 = punctuation was removed from the tree-
bank.
16All non-Prague family treebanks are marked sN and
m00 or m10, (i.e. shared modifiers not marked in the origi-
nal annotation, but attached to the head conjunct) because we
found no counterexamples (modifiers attached to a conjunct,
but not the nearest one). The HamleDT normalization proce-
dure contains a few heuristics to detect shared modifiers, but
it cannot recover the missing distinction reliably, so the num-
bers in the ?SMs/CJ? column are mostly underestimated.
522
Language Orig. Data Sents. Tokens Original CS CSs / CJs / SMs / Nested RT
type set style code 100 tok. CS CS CS[%] UAS
Ancient
Greek dep prim. 31 316 461 782 fP hR sH cH pB dL m11 6.54 2.17 0.16 10.3 97.86
Arabic dep C07 3 043 116 793 fP hL sH cH pB dL m00 3.76 2.42 0.13 10.6 96.69
Basque dep prim. 11 225 151 593 fP hR sN cH pP dU m00 3.37 2.09 0.03 5.1 99.32
Bengali dep I10 1 129 7 252 fP hR sH cH pP dU m11 4.87 1.71 0.05 24.1 99.97
Bulgarian phr C06 13 221 196 151 fS hL sN cB pB dU m10 2.99 2.19 0.00 0.0 99.74
Czech dep C07 25 650 437 020 fP hR sH cH pB dL m11 4.09 2.16 0.20 14.6 99.42
Danish dep C06 5 512 100 238 fS* hL sN cP pB dU m10 3.68 1.93 0.13 7.5 99.76
Dutch phr C06 13 735 200 654 fP hR sN cH pP dU m10 2.06 2.17 0.05 3.3 99.47
English phr C07 40 613 991 535 fP hR sH cH pB dU m10 2.07 2.33 0.05 6.3 99.84
Finnish dep prim. 4 307 58 576 fS hL sN cB pB dU m10 4.06 2.41 0.00 6.4 99.70
German phr C09 38 020 680 710 fM hL sN cP pP dU m10 2.79 2.09 0.01 0.0 99.73
Greek dep C07 2 902 70 223 fP hR sH cH pB dL m11 3.25 2.48 0.18 7.2 99.43
Hindi dep I10 3 515 77 068 fP hR sH cH pP dU m11 2.45 1.97 0.04 10.3 98.35
Hungarian phr C07 6 424 139 143 fT hX sN cX pX dL m00 2.37 1.90 0.01 2.2 99.84
Italian dep C07 3 359 76 295 fS hL sN cB pB dU m10 3.32 2.02 0.03 3.8 99.51
Latin dep prim. 3 473 53 143 fP hR sH cH pB dL m11 6.74 2.24 0.41 12.3 97.45
Persian dep prim. 12 455 189 572 fM*hM sN cB pP dU m00 4.18 2.10 0.18 3.7 99.82
Portuguese phr C06 9 359 212 545 fS hL sN cB pB dU m10 2.51 1.95 0.26 11.1 99.16
Romanian dep prim. 4 042 36 150 fP* hR sN cH p0 dU m10 1.80 2.00 0.00 0.0 100.00
Russian dep prim. 34 895 497 465 fM hL sN cB p0 dU m10 4.02 2.02 0.07 3.9 99.86
Slovene dep C06 1 936 35 140 fP hR sH cH pB dL m00 4.31 2.49 0.00 10.8 98.87
Spanish phr C09 15 984 477 810 fS hL sN cB pB dU m10 2.79 1.98 0.14 12.7 99.24
Swedish phr C06 11 431 197 123 fM hL sN cF pF dU m10 3.94 2.19 0.13 0.7 99.66
Tamil dep prim. 600 9 581 fP hR sH cH pB dL m11 1.66 2.46 0.22 3.8 99.67
Telugu dep I10 1 450 5 722 fP hR sH cH pP dU m11 3.48 1.59 0.06 5.0 100.00
Turkish dep C07 5 935 69 695 fM hR sN cB pB dL m10 3.81 2.04 0.00 34.3 99.23
Table 1: Overview of analyzed treebanks. prim. = primary source; C06?C09 = CoNLL 2006?2009;
I10 = ICON 2010; SM = shared modifier; CJ = conjunct; Nested CS = portion of CSs participating in
nested CSs (both as the inner and outer CS); RT UAS = unlabeled attachment score of the roundtrip
experiment described in Section 5. Style codes are defined in Sections 3 and 4.
? fM* = Persian treebank uses a mix of fM and
fS: fS for coordination of verbs and fM oth-
erwise.
Figure 2 shows three other anomalies:
? fS* = Danish treebank employs a mixture of
fS and fM, where the last conjunct is attached
indirectly via the conjunction.
? fP* = Romanian treebank omits punctuation
tokens and multi-conjunct coordinations get
split.
? fT = Hungarian Szeged treebank uses
?Tesnie`re family? ? disconnected graphs for
CSs where conjuncts (and conjunction and
punctuation) are attached directly to the par-
ent of CS, and so the other style dimensions
are not applicable (hX, cX, pX).
5 Empirical Observations on
Convertibility of Coordination Styles
The various styles cannot represent the CS-related
information to the same extent. For example,
it is not possible to represent nested CSs in the
Moscow and Stanford families without signifi-
cantly changing the number of possible labels.17
The dL style (which is most easily applicable to
the Prague family) can represent coordination of
different dependency relations. This is again not
possible in the other styles without adding e.g. a
special ?prefix? denoting the relations.
We can see that the Prague family has a greater
expressive power than the other two families: it
can represent complex CSs using just one addi-
tional binary label, distinguishing between shared
modifiers and conjuncts. A similar additional label
is needed in the other styles to distinguish between
shared and private modifiers.
Because of the different expressive power, con-
verting a CS from one style to another may
lead to a loss of information. For example, as
17Mel?c?uk uses ?grouping? to nest CSs ? cf. related so-
lutions involving coindexing or bubble trees (Kahane, 1997).
However, these approaches were not used in any of the re-
searched treebanks. To combine grouping with shared modi-
fiers, each group in a tree should have a different identifier.
523
there is no way of representing shared modifiers
in the Moscow family without an additional at-
tribute, converting a CS with shared modifiers
from Prague to Moscow family makes the modi-
fiers private. When converting back, one can use
certain heuristics to handle the most obvious cases,
but sometimes the modifiers will stay private (very
often, the nature of a modifier depends on context
or is debatable even for humans, e.g. ?Young boys
and girls?).
5.1 Transformation algorithm
We developed an algorithm to transform one CS
style to another. Two subtasks must be solved by
the algorithm: identification of individual CSs and
their participants, and transforming of the individ-
ual CSs.
Obviously, the individual CSs cannot be trans-
formed independently because of coordination
nesting. For instance, when transforming a nested
coordination from the Prague style to the Moscow
style (e.g. to fMhL), the leftmost conjunct in the
inner (lower) coordination must climb up to be-
come the head of the inner CS, but then it must
climb up once again to become the head of the
outer (upper) CS too. This shows that inner CSs
must be transformed first.
We tackle this problem by a depth-first recur-
sion. When going down the tree, we only recog-
nize all the participants of the CSs, classify them
and gather them in a separate data structure (one
for each visited CS). The following four types
of CS participants are distinguished: coordinat-
ing conjunctions, conjuncts, shared modifiers, and
punctuations that separate conjuncts.18 No change
of the tree is performed during these descent steps.
When returning back from the recursion (i.e.,
when climbing from a node back up to its par-
ent), we test whether the abandoned node is the
topmost node of some CS. If so, then this CS is
transformed, which means that its participants are
rehanged and relabelled according the the target
CS style.
This procedure naturally guarantees that the in-
18Conjuncts are explicitly marked in most styles. Coordi-
nating conjunctions can be usually identified with the help of
dependency labels and POS tags. Punctuation separating con-
juncts can be detected with high accuracy using simple rules.
If shared modifiers are not annotated (code m00 or m10),
one can imagine rule-based heuristics or special classifiers
trained to distinguish shared modifiers. For the experiments
in this section, we use the HamleDT gold annotation attribute
is shared modifier.
ner CSs are transformed first and that all CSs are
transformed when the recursions returns to the
root.
5.2 Roundtrip experiment
The number of possible conversion directions ob-
viously grows quadratically with the number of
styles. So far, we limited ourselves only to con-
versions from/to the style of the HamleDT tree-
bank collection, which contains all the treebanks
under our study already converted into a com-
mon scheme. The common scheme is based
on the conventions of PDT, whose CS style is
fPhRsHcHpB.19
We selected nine styles (3 families times 3 head
choices) and transformed all the HamleDT scheme
treebanks to these nine styles and back, which we
call a roundtrip. Resulting averaged unlabeled at-
tachment scores (UAS, evaluated against the Ham-
leDT scheme) in the last column of Table 1 indi-
cate that the percentage of transformation errors
(i.e. tokens attached to a different parent after the
roundtrip) is lower than 1% for 20 out of the 26
languages.20 A manual inspection revealed two
main error sources. First, as noted above, the Stan-
ford and Moscow families have lower expressive
power than the Prague family, so naturally, the in-
verse transformation was ambiguous and the trans-
formation heuristics were not capable of identify-
ing the correct variant every time. Second, we also
encountered inconsistencies in the original tree-
banks (which we were not trying to fix in Ham-
leDT for now).
6 Conclusions and Future Work
We described a (theoretically very large) space of
possible representations of CSs within the depen-
dency framework. We pointed out a range of de-
tails that make CSs a really complex phenomenon;
anyone dealing with CSs in treebanking should
take these observations into account.
We proposed a taxonomy of those approaches
19As documented in Zeman et al (2012), the normalization
procedures used in HamleDT embrace many other phenom-
ena as well (not only those related to coordination), and in-
volve both structural transformation and dependency relation
relabeling.
20Table 1 shows that Latin and Ancient Greek treebanks
have on average more than 6 CSs per 100 tokens, more than
2 conjuncts per CS, and Latin has also the highest number of
shared modifiers per CS. Therefore the percentage of nodes
affected by the roundtrip is the highest for these languages
and the lower roundtrip UAS is not surprising.
524
that have been argued for in literature or employed
in real treebanks.
We studied 26 existing treebanks of different
languages. For each value of each dimension in
Figure 1, we found at least one treebank where the
value is used; even so, several treebanks take their
own unique path that cannot be clearly classified
under the taxonomy (the taxonomy could indeed
be extended, for the price of being less clearly ar-
ranged).
We discussed the convertibility between the var-
ious styles and implemented a universal tool that
transforms between any two styles of the taxon-
omy. The tool achieves a roundtrip accuracy close
to 100%. This is important because it opens the
door to easily switching coordination styles for
parsing experiments, phrase-to-dependency con-
version etc.
While the focus of this paper is to explore and
describe the expressive power of various annota-
tion styles, we did not address the learnability of
the styles by parsers. That will be a complemen-
tary point of view, and thus a natural direction of
future work for us.
Acknowledgments
We thank the providers of the primary data re-
sources. The work on this project was sup-
ported by the Czech Science Foundation grants
no. P406/11/1499 and P406/2010/0875, and by
research resources of the Charles University in
Prague (PRVOUK). This work has been using lan-
guage resources developed and/or stored and/or
distributed by the LINDAT-Clarin project of the
Ministry of Education of the Czech Republic
(project LM2010013). Further, we would like to
thank Jan Hajic?, Ondr?ej Dus?ek and four anony-
mous reviewers for many useful comments on the
manuscript of this paper.
References
Itzair Aduriz et al 2003. Construction of a Basque de-
pendency treebank. In Proceedings of the 2nd Work-
shop on Treebanks and Linguistic Theories.
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. ?Floresta sinta?(c)tica?: a tree-
bank for Portuguese. In LREC, pages 1968?1703.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003.
The annotation process in the Turkish treebank. In
Proceedings of the 4th Intern. Workshop on Linguis-
tically Interpreteted Corpora (LINC).
David Bamman and Gregory Crane. 2011. The An-
cient Greek and Latin dependency treebanks. In
Language Technology for Cultural Heritage, Theory
and Applications of Natural Language Processing,
pages 79?98. Springer Berlin Heidelberg.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grig-
oriev, Leonid Kreidlin, and Nadezhda Frid. 2000.
Dependency treebank for Russian: Concept, tools,
types of information. In Proceedings of the 18th
conference on Computational linguistics-Volume 2,
pages 987?991. Association for Computational Lin-
guistics Morristown, NJ, USA.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
Montserrat Civit, Maria Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: From constituents to
dependencies. In FinTAL, volume 4139 of Lec-
ture Notes in Computer Science, pages 141?152.
Springer.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Do?ra Csendes, Ja?nos Csirik, Tibor Gyimo?thy, and
Andra?s Kocsor. 2005. The Szeged treebank. In
TSD, volume 3658 of Lecture Notes in Computer
Science, pages 123?131. Springer.
Mihaela Ca?la?cean. 2008. Data-driven dependency
parsing for Romanian. Master?s thesis, Uppsala
University, August.
Sas?o Dz?eroski, Tomaz? Erjavec, Nina Ledinek, Petr Pa-
jas, Zdene?k Z?abokrtsky?, and Andreja Z?ele. 2006.
Towards a Slovene dependency treebank. In LREC
2006, pages 1388?1391, Genova, Italy. European
Language Resources Association (ELRA).
Nathan Green and Zdene?k Z?abokrtsky?. 2012. Hy-
brid combination of constituency and dependency
trees into an ensemble dependency parser. In Pro-
ceedings of the Workshop on Innovative Hybrid Ap-
proaches to the Processing of Textual Data, pages
19?26, Avignon, France. Association for Computa-
tional Linguistics.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka,
Marie Mikulova?, Zdene?k Z?abokrtsky?, and Magda
S?evc???kova?-Raz??mova?. 2006. Prague Dependency
Treebank 2.0. CD-ROM, Linguistic Data Consor-
tium, LDC Catalog No.: LDC2006T01, Philadel-
phia.
525
Jan Hajic? et al 2009. The CoNLL-2009 shared
task: Syntactic and semantic dependencies in mul-
tiple languages. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
Jirka Hana and Jan S?te?pa?nek. 2012. Prague
markup language framework. In Proceedings of the
Sixth Linguistic Annotation Workshop, pages 12?
21, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics, Association for Computational
Linguistics.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking Finnish. In Proceedings of
the Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9), pages 79?90.
Samar Husain, Prashanth Mannem, Bharat Ambati,
and Phani Gadde. 2010. The ICON-2010 tools
contest on Indian language dependency parsing. In
Proceedings of ICON-2010 Tools Contest on Indian
Language Dependency Parsing, Kharagpur, India.
ISO 24615. 2010. Language resource management ?
Syntactic annotation framework (SynAF).
Sylvain Kahane. 1997. Bubble trees and syntactic
representations. In Proceedings of the 5th Meeting
of the Mathematics of the Language, DFKI, Saar-
brucken.
Matthias T. Kromann, Line Mikkelsen, and Stine Kern
Lynge. 2004. Danish dependency treebank.
Sandra Ku?bler, Erhard Hinrichs, Wolfgang Maier, and
Eva Klett. 2009. Parsing coordinations. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 406?414,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Vincenzo Lombardo and Leonardo Lesmo. 1998. Unit
coordination and gapping in dependency theory. In
Processing of Dependency-Based Grammars; pro-
ceedings of the workshop. COLING-ACL, Montreal.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Nicolar Mazziotta. 2011. Coordination of verbal de-
pendents in Old French: Coordination as a specified
juxtaposition or apposition. In Proceedings of In-
ternational Conference on Dependency Linguistics
(DepLing 2011).
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State University of New York Press.
Simonetta Montemagni et al 2003. Building the Ital-
ian syntactic-semantic treebank. In Building and us-
ing Parsed Corpora, Language and Speech series,
pages 189?210, Dordrecht. Kluwer.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proceedings of the
NODALIDA Special Session on Treebanks.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
2007 Shared Task. EMNLP-CoNLL, June.
Martin Popel and Zdene?k Z?abokrtsky?. 2009.
Improving English-Czech Tectogrammatical MT.
The Prague Bulletin of Mathematical Linguistics,
(92):1?20.
Prokopis Prokopidis, Elina Desipri, Maria Koutsom-
bogera, Harris Papageorgiou, and Stelios Piperidis.
2005. Theoretical and practical issues in the con-
struction of a Greek dependency treebank. In Pro-
ceedings of the 4th Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 149?160.
Loganathan Ramasamy and Zdene?k Z?abokrtsky?. 2012.
Prague dependency style treebank for Tamil. In
Proceedings of LREC 2012, pages 23?25, I?stanbul,
Turkey. European Language Resources Association.
Mohammad Sadegh Rasooli, Amirsaeid Moloodi,
Manouchehr Kouhestani, and Behrouz Minaei-
Bidgoli. 2011. A syntactic valency lexicon for
Persian verbs: The first steps towards Persian de-
pendency treebank. In 5th Language & Technology
Conference (LTC): Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, pages 227?231, Poznan?, Poland.
Kiril Simov and Petya Osenova. 2005. Extending
the annotation of BulTreeBank: Phase 2. In The
Fourth Workshop on Treebanks and Linguistic Theo-
ries (TLT 2005), pages 173?184, Barcelona, Decem-
ber.
Otakar Smrz?, Viktor Bielicky?, Iveta Kour?ilova?, Jakub
Kra?c?mar, Jan Hajic?, and Petr Zema?nek. 2008.
Prague Arabic dependency treebank: A word on the
million words. In Proceedings of the Workshop on
Arabic and Local Languages (LREC) 2008, pages
16?23, Marrakech, Morocco. European Language
Resources Association.
Leon Stassen. 2000. And-languages and with-
languages. Linguistic Typology, 4(1):1?54.
Jan S?te?pa?nek. 2006. Capturing a Sentence Struc-
ture by a Dependency Relation in an Annotated Syn-
tactical Corpus (Tools Guaranteeing Data Consis-
tence) (in Czech). Ph.D. thesis, Charles Univer-
526
sity in Prague, Faculty of Mathematics and Physics,
Prague, Czech Republic.
Pavel Stran?a?k and Jan S?te?pa?nek. 2010. Represent-
ing layered and structured data in the CoNLL-ST
format. In Alex Fang, Nancy Ide, and Jonathan
Webster, editors, Proceedings of the Second Inter-
national Conference on Global Interoperability for
Language Resources, pages 143?152, Hong Kong,
China. City University of Hong Kong, City Univer-
sity of Hong Kong.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel annotated cor-
pora for Catalan and Spanish. In LREC. European
Language Resources Association.
TEI Consortium. 2013. TEI P5: Guidelines for Elec-
tronic Text Encoding and Interchange.
Lucien Tesnie`re. 1959. Ele?ments de syntaxe struc-
turale. Paris.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMNLP, pages 1257?1268, Edin-
burgh, Scotland, UK, July. Association for Compu-
tational Linguistics.
Leonoor van der Beek et al 2002. Chapter 5. The
Alpino dependency treebank. In Algorithms for Lin-
guistic Processing NWO PIONIER Progress Report,
Groningen, The Netherlands.
Daniel Zeman, David Marec?ek, Martin Popel,
Loganathan Ramasamy, Jan S?te?pa?nek, Zdene?k
Z?abokrtsky?, and Jan Hajic?. 2012. HamleDT: To
parse or not to parse? In Proceedings of LREC 2012,
pages 2735?2741, I?stanbul, Turkey. European Lan-
guage Resources Association.
527
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63?72,
Dublin, Ireland, August 23-24, 2014.
SemEval 2014 Task 8:
Broad-Coverage Semantic Dependency Parsing
Stephan Oepen
??
, Marco Kuhlmann
?
, Yusuke Miyao
?
, Daniel Zeman
?
,
Dan Flickinger
?
, Jan Haji
?
c
?
, Angelina Ivanova
?
, and Yi Zhang
?
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
?
Link?ping University, Department of Computer and Information Science
?
National Institute of Informatics, Tokyo
?
Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
?
Stanford University, Center for the Study of Language and Information
?
Nuance Communications Aachen GmbH
sdp-organizers@emmtee.net
Abstract
Task 8 at SemEval 2014 defines Broad-
Coverage Semantic Dependency Pars-
ing (SDP) as the problem of recovering
sentence-internal predicate?argument rela-
tionships for all content words, i.e. the se-
mantic structure constituting the relational
core of sentence meaning. In this task
description, we position the problem in
comparison to other sub-tasks in compu-
tational language analysis, introduce the se-
mantic dependency target representations
used, reflect on high-level commonalities
and differences between these representa-
tions, and summarize the task setup, partic-
ipating systems, and main results.
1 Background and Motivation
Syntactic dependency parsing has seen great ad-
vances in the past decade, in part owing to rela-
tively broad consensus on target representations,
and in part reflecting the successful execution of a
series of shared tasks at the annual Conference for
Natural Language Learning (CoNLL; Buchholz &
Marsi, 2006; Nivre et al., 2007; inter alios). From
this very active research area accurate and efficient
syntactic parsers have developed for a wide range
of natural languages. However, the predominant
data structure in dependency parsing to date are
trees, in the formal sense that every node in the de-
pendency graph is reachable from a distinguished
root node by exactly one directed path.
This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and the
proceedings footer are added by the organizers: http://
creativecommons.org/licenses/by/4.0/.
Unfortunately, tree-oriented parsers are ill-suited
for producing meaning representations, i.e. mov-
ing from the analysis of grammatical structure to
sentence semantics. Even if syntactic parsing ar-
guably can be limited to tree structures, this is not
the case in semantic analysis, where a node will
often be the argument of multiple predicates (i.e.
have more than one incoming arc), and it will often
be desirable to leave nodes corresponding to se-
mantically vacuous word classes unattached (with
no incoming arcs).
Thus, Task 8 at SemEval 2014, Broad-Coverage
Semantic Dependency Parsing (SDP 2014),
1
seeks
to stimulate the dependency parsing community
to move towards more general graph processing,
to thus enable a more direct analysis of Who did
What to Whom? For English, there exist several
independent annotations of sentence meaning over
the venerable Wall Street Journal (WSJ) text of the
Penn Treebank (PTB; Marcus et al., 1993). These
resources constitute parallel semantic annotations
over the same common text, but to date they have
not been related to each other and, in fact, have
hardly been applied for training and testing of data-
driven parsers. In this task, we have used three
different such target representations for bi-lexical
semantic dependencies, as demonstrated in Figure 1
below for the WSJ sentence:
(1) A similar technique is almost impossible to apply to
other crops, such as cotton, soybeans, and rice.
Semantically, technique arguably is dependent on
the determiner (the quantificational locus), the mod-
ifier similar, and the predicate apply. Conversely,
the predicative copula, infinitival to, and the vac-
1
See http://alt.qcri.org/semeval2014/
task8/ for further technical details, information on how to
obtain the data, and official results.
63
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
A1 A2
(a) Partial semantic dependencies in PropBank and NomBank.
A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.
top
ARG2 ARG3 ARG1
ARG2mwe _and_cARG1ARG1
BV
ARG1 implicit_conjARG1
(b) DELPH-IN Minimal Recursion Semantics?derived bi-lexical dependencies (DM).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice
top
ARG1
ARG2
ARG1
ARG2
ARG2
ARG1
ARG1 ARG1 ARG1ARG1
ARG1
ARG2
ARG1
ARG2
ARG1
ARG2
ARG1 ARG1 ARG1 ARG2
(c) Enju Predicate?Argument Structures (PAS).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
RSTR
PAT
EXT
PAT
ACT
RSTR
ADDR
ADDR
ADDR
ADDR
APPS.m
APPS.m
CONJ.m
CONJ.m CONJ.m
top
(d) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PCEDT).
Figure 1: Sample semantic dependency graphs for Example (1).
uous preposition marking the deep object of ap-
ply can be argued to not have a semantic contri-
bution of their own. Besides calling for node re-
entrancies and partial connectivity, semantic depen-
dency graphs may also exhibit higher degrees of
non-projectivity than is typical of syntactic depen-
dency trees.
In addition to its relation to syntactic dependency
parsing, the task also has some overlap with Se-
mantic Role Labeling (SRL; Gildea & Jurafsky,
2002). In much previous work, however, target
representations typically draw on resources like
PropBank and NomBank (Palmer et al., 2005; Mey-
ers et al., 2004), which are limited to argument
identification and labeling for verbal and nominal
predicates. A plethora of semantic phenomena?
for example negation and other scopal embedding,
comparatives, possessives, various types of modi-
fication, and even conjunction?typically remain
unanalyzed in SRL. Thus, its target representations
are partial to a degree that can prohibit seman-
tic downstream processing, for example inference-
based techniques. In contrast, we require parsers
to identify all semantic dependencies, i.e. compute
a representation that integrates all content words in
one structure. Another difference to common inter-
pretations of SRL is that the SDP 2014 task defini-
tion does not encompass predicate disambiguation,
a design decision in part owed to our goal to focus
on parsing-oriented, i.e. structural, analysis, and in
part to lacking consensus on sense inventories for
all content words.
Finally, a third closely related area of much cur-
rent interest is often dubbed ?semantic parsing?,
which Kate and Wong (2010) define as ?the task of
mapping natural language sentences into complete
formal meaning representations which a computer
can execute for some domain-specific application.?
In contrast to most work in this tradition, our SDP
target representations aim to be task- and domain-
independent, though at least part of this general-
ity comes at the expense of ?completeness? in the
above sense; i.e. there are aspects of sentence mean-
ing that arguably remain implicit.
2 Target Representations
We use three distinct target representations for se-
mantic dependencies. As is evident in our run-
ning example (Figure 1), showing what are called
the DM, PAS, and PCEDT semantic dependencies,
there are contentful differences among these anno-
tations, and there is of course not one obvious (or
even objective) truth. In the following paragraphs,
64
we provide some background on the ?pedigree? and
linguistic characterization of these representations.
DM: DELPH-IN MRS-Derived Bi-Lexical De-
pendencies These semantic dependency graphs
originate in a manual re-annotation of Sections 00?
21 of the WSJ Corpus with syntactico-semantic
analyses derived from the LinGO English Re-
source Grammar (ERG; Flickinger, 2000). Among
other layers of linguistic annotation, this resource?
dubbed DeepBank by Flickinger et al. (2012)?
includes underspecified logical-form meaning rep-
resentations in the framework of Minimal Recur-
sion Semantics (MRS; Copestake et al., 2005).
Our DM target representations are derived through
a two-step ?lossy? conversion of MRSs, first to
variable-free Elementary Dependency Structures
(EDS; Oepen & L?nning, 2006), then to ?pure?
bi-lexical form?projecting some construction se-
mantics onto word-to-word dependencies (Ivanova
et al., 2012). In preparing our gold-standard
DM graphs from DeepBank, the same conversion
pipeline was used as in the system submission of
Miyao et al. (2014). For this target representa-
tion, top nodes designate the highest-scoping (non-
quantifier) predicate in the graph, e.g. the (scopal)
degree adverb almost in Figure 1.
2
PAS: Enju Predicate-Argument Structures
The Enju parsing system is an HPSG-based parser
for English.
3
The grammar and the disambigua-
tion model of this parser are derived from the Enju
HPSG treebank, which is automatically converted
from the phrase structure and predicate?argument
structure annotations of the PTB. The PAS data
set is extracted from the WSJ portion of the Enju
HPSG treebank. While the Enju treebank is an-
notated with full HPSG-style structures, only its
predicate?argument structures are converted into
the SDP data format for use in this task. Top
nodes in this representation denote semantic heads.
Again, the system description of Miyao et al. (2014)
provides more technical detail on the conversion.
PCEDT: Prague Tectogrammatical Bi-Lexical
Dependencies The Prague Czech-English De-
pendency Treebank (PCEDT; Haji
?
c et al., 2012)
4
is a set of parallel dependency trees over the WSJ
2
Note, however, that non-scopal adverbs act as mere in-
tersective modifiers, e.g. loudly is a predicate in DM, but the
main verb provides the top node in structures like Abrams
sang loudly.
3
See http://kmcs.nii.ac.jp/enju/.
4
See http://ufal.mff.cuni.cz/pcedt2.0/.
id form lemma pos top pred arg1 arg2
#20200002
1 Ms. Ms. NNP ? + _ _
2 Haag Haag NNP ? ? compound ARG1
3 plays play VBZ + + _ _
4 Elianti Elianti NNP ? ? _ ARG2
5 . . . ? ? _ _
Table 1: Tabular SDP data format (showing DM).
texts from the PTB, and their Czech translations.
Similarly to other treebanks in the Prague family,
there are two layers of syntactic annotation: an-
alytical (a-trees) and tectogrammatical (t-trees).
PCEDT bi-lexical dependencies in this task have
been extracted from the t-trees. The specifics of
the PCEDT representations are best observed in the
procedure that converts the original PCEDT data to
the SDP data format; see Miyao et al. (2014). Top
nodes are derived from t-tree roots; i.e. they mostly
correspond to main verbs. In case of coordinate
clauses, there are multiple top nodes per sentence.
3 Graph Representation
The SDP target representations can be character-
ized as labeled, directed graphs. Formally, a se-
mantic dependency graph for a sentence x =
x
1
, . . . , x
n
is a structure G = (V,E, `
V
, `
E
) where
V = {1, . . . , n} is a set of nodes (which are in
one-to-one correspondence with the tokens of the
sentence); E ? V ? V is a set of edges; and `
V
and `
E
are mappings that assign labels (from some
finite alphabet) to nodes and edges, respectively.
More specifically for this task, the label `
V
(i) of a
node i is a tuple consisting of four components: its
word form, lemma, part of speech, and a Boolean
flag indicating whether the corresponding token
represents a top predicate for the specific sentence.
The label `
E
(i? j) of an edge i? j is a seman-
tic relation that holds between i and j. The exact
definition of what constitutes a top node and what
semantic relations are available differs among our
three target representations, but note that top nodes
can have incoming edges.
All data provided for the task uses a column-
based file format (dubbed the SDP data format)
similar to the one of the 2009 CoNLL Shared Task
(Haji
?
c et al., 2009). As in that task, we assume gold-
standard sentence and token segmentation. For
ease of reference, each sentence is prefixed by a
line with just a unique identifier, using the scheme
2SSDDIII, with a constant leading 2, two-digit sec-
tion code, two-digit document code (within each
65
section), and three-digit item number (within each
document). For example, identifier 20200002 de-
notes the second sentence in the first file of PTB
Section 02, the classic Ms. Haag plays Elianti. The
annotation of this sentence is shown in Table 1.
With one exception, our fields (i.e. columns in
the tab-separated matrix) are a subset of the CoNLL
2009 inventory: (1) id, (2) form, (3) lemma, and
(4) pos characterize the current token, with token
identifiers starting from 1 within each sentence. Be-
sides the lemma and part-of-speech information, in
the closed track of our task, there is no explicit
analysis of syntax. Across the three target represen-
tations in the task, fields (1) and (2) are aligned and
uniform, i.e. all representations annotate exactly
the same text. On the other hand, fields (3) and (4)
are representation-specific, i.e. there are different
conventions for lemmatization, and part-of-speech
assignments can vary (but all representations use
the same PTB inventory of PoS tags).
The bi-lexical semantic dependency graph over
tokens is represented by two or more columns start-
ing with the obligatory, binary-valued fields (5)
top and (6) pred. A positive value in the top
column indicates that the node corresponding to
this token is a top node (see Section 2 below). The
pred column is a simplification of the correspond-
ing field in earlier tasks, indicating whether or not
this token represents a predicate, i.e. a node with
outgoing dependency edges. With these minor dif-
ferences to the CoNLL tradition, our file format can
represent general, directed graphs, with designated
top nodes. For example, there can be singleton
nodes not connected to other parts of the graph,
and in principle there can be multiple tops, or a
non-predicate top node.
To designate predicate?argument relations, there
are as many additional columns as there are pred-
icates in the graph (i.e. tokens marked + in the
pred column); these additional columns are called
(7) arg1, (8) arg2, etc. These colums contain
argument roles relative to the i-th predicate, i.e. a
non-empty value in column arg1 indicates that
the current token is an argument of the (linearly)
first predicate in the sentence. In this format, graph
reentrancies will lead to a token receiving argument
roles for multiple predicates (i.e. non-empty arg
i
values in the same row). All tokens of the same sen-
tence must always have all argument columns filled
in, even on non-predicate words; in other words,
all lines making up one block of tokens will have
the same number n of fields, but n can differ across
DM PAS PCEDT
(1) # labels 51 42 68
(2) % singletons 22.62 4.49 35.79
(3) # edge density 0.96 1.02 0.99
(4) %
g
trees 2.35 1.30 56.58
(5) %
g
projective 3.05 1.71 53.29
(6) %
g
fragmented 6.71 0.23 0.56
(7) %
n
reentrancies 27.35 29.40 9.27
(8) %
g
topless 0.28 0.02 0.00
(9) # top nodes 0.9972 0.9998 1.1237
(10) %
n
non-top roots 44.71 55.92 4.36
Table 2: Contrastive high-level graph statistics.
sentences, depending on the count of graph nodes.
4 Data Sets
All three target representations are annotations of
the same text, Sections 00?21 of the WSJ Cor-
pus. For this task, we have synchronized these
resources at the sentence and tokenization levels
and excluded from the SDP 2014 training and test-
ing data any sentences for which (a) one or more of
the treebanks lacked a gold-standard analysis; (b) a
one-to-one alignment of tokens could not be estab-
lished across all three representations; or (c) at least
one of the graphs was cyclic. Of the 43,746 sen-
tences in these 22 first sections of WSJ text, Deep-
Bank lacks analyses for close to 15%, and the Enju
Treebank has gaps for a little more than four per-
cent. Some 500 sentences show tokenization mis-
matches, most owing to DeepBank correcting PTB
idiosyncrasies like ?G.m.b, H.?, ?S.p, A.?, and
?U.S., .?, and introducing a few new ones (Fares
et al., 2013). Finally, 232 of the graphs obtained
through the above conversions were cyclic. In total,
we were left with 34,004 sentences (or 745,543
tokens) as training data (Sections 00?20), and 1348
testing sentences (29,808 tokens), from Section 21.
Quantitative Comparison As a first attempt at
contrasting our three target representations, Table 2
shows some high-level statistics of the graphs com-
prising the training data.
5
In terms of distinctions
5
These statistics are obtained using the ?official? SDP
toolkit. We refer to nodes that have neither incoming nor
outgoing edges and are not marked as top nodes as singletons;
these nodes are ignored in subsequent statistics, e.g. when
determining the proportion of edges per node (3) or the per-
centages of rooted trees (4) and fragmented graphs (6). The
notation ?%
n
? denotes (non-singleton) node percentages, and
?%
g
? percentages over all graphs. We consider a root node any
(non-singleton) node that has no incoming edges; reentrant
nodes have at least two incoming edges. Following Sagae and
Tsujii (2008), we consider a graph projective when there are
no crossing edges (in a left-to-right rendering of nodes) and no
roots are ?covered?, i.e. for any root j there is no edge i? k
66
Directed Undirected
DM PAS PCEDT DM PAS PCEDT
DM ? .6425 .2612 ? .6719 .5675
PAS .6688 ? .2963 .6993 ? .5490
PCEDT .2636 .2963 ? .5743 .5630 ?
Table 3: Pairwise F
1
similarities, including punctu-
ation (upper right diagonals) or not (lower left).
drawn in dependency labels (1), there are clear dif-
ferences between the representations, with PCEDT
appearing linguistically most fine-grained, and PAS
showing the smallest label inventory. Unattached
singleton nodes (2) in our setup correspond to
tokens analyzed as semantically vacuous, which
(as seen in Figure 1) include most punctuation
marks in PCEDT and DM, but not PAS. Further-
more, PCEDT (unlike the other two) analyzes some
high-frequency determiners as semantically vacu-
ous. Conversely, PAS on average has more edges
per (non-singleton) nodes than the other two (3),
which likely reflects its approach to the analysis of
functional words (see below).
Judging from both the percentage of actual trees
(4), the proportions of projective graphs (5), and the
proportions of reentrant nodes (7), PCEDT is much
more ?tree-oriented? than the other two, which at
least in part reflects its approach to the analysis
of modifiers and determiners (again, see below).
We view the small percentages of graphs without
at least one top node (8) and of graphs with at
least two non-singleton components that are not
interconnected (6) as tentative indicators of general
well-formedness. Intuitively, there should always
be a ?top? predicate, and the whole graph should
?hang together?. Only DM exhibits non-trivial (if
small) degrees of topless and fragmented graphs,
and these may indicate imperfections in the Deep-
Bank annotations or room for improvement in the
conversion from full MRSs to bi-lexical dependen-
cies, but possibly also exceptions to our intuitions
about semantic dependency graphs.
Finally, in Table 3 we seek to quantify pairwise
structural similarity between the three representa-
tions in terms of unlabeled dependency F
1
(dubbed
UF in Section 5 below). We provide four variants
of this metric, (a) taking into account the direc-
tionality of edges or not and (b) including edges
involving punctuation marks or not. On this view,
DM and PAS are structurally much closer to each
other than either of the two is to PCEDT, even more
such that i < j < k.
so when discarding punctuation. While relaxing
the comparison to ignore edge directionality also
increases similarity scores for this pair, the effect
is much more pronounced when comparing either
to PCEDT. This suggests that directionality of se-
mantic dependencies is a major source of diversion
between DM and PAS on the one hand, and PCEDT
on the other hand.
Linguistic Comparison Among other aspects,
Ivanova et al. (2012) categorize a range of syntac-
tic and semantic dependency annotation schemes
according to the role that functional elements take.
In Figure 1 and the discussion of Table 2 above, we
already observed that PAS differs from the other
representations in integrating into the graph aux-
iliaries, the infinitival marker, the case-marking
preposition introducing the argument of apply (to),
and most punctuation marks;
6
while these (and
other functional elements, e.g. complementizers)
are analyzed as semantically vacuous in DM and
PCEDT, they function as predicates in PAS, though
do not always serve as ?local? top nodes (i.e. the se-
mantic head of the corresponding sub-graph): For
example, the infinitival marker in Figure 1 takes the
verb as its argument, but the ?upstairs? predicate
impossible links directly to the verb, rather than to
the infinitival marker as an intermediate.
At the same time, DM and PAS pattern alike
in their approach to modifiers, e.g. attributive ad-
jectives, adverbs, and prepositional phrases. Un-
like in PCEDT (or common syntactic dependency
schemes), these are analyzed as semantic predi-
cates and, thus, contribute to higher degrees of
node reentrancy and non-top (structural) roots.
Roughly the same holds for determiners, but here
our PCEDT projection of Prague tectogrammatical
trees onto bi-lexical dependencies leaves ?vanilla?
articles (like a and the) as singleton nodes.
The analysis of coordination is distinct in the
three representations, as also evident in Figure 1.
By design, DM opts for what is often called
the Mel?
?
cukian analysis of coordinate structures
(Mel?
?
cuk, 1988), with a chain of dependencies
rooted at the first conjunct (which is thus consid-
ered the head, ?standing in? for the structure at
large); in the DM approach, coordinating conjunc-
tions are not integrated with the graph but rather
contribute different types of dependencies. In PAS,
the final coordinating conjunction is the head of the
6
In all formats, punctuation marks like dashes, colons, and
sometimes commas can be contentful, i.e. at times occur as
both predicates, arguments, and top nodes.
67
employee stock investment plans
compound compound compound
employee stock investment plans
ARG1
ARG1
ARG1
employee stock investment plans
ACT
PAT REG
Figure 2: Analysis of nominal compounding in DM, PAS, and PCEDT, respectively .
structure and each coordinating conjunction (or in-
tervening punctuation mark that acts like one) is a
two-place predicate, taking left and right conjuncts
as its arguments. Conversely, in PCEDT the last
coordinating conjunction takes all conjuncts as its
arguments (in case there is no overt conjunction, a
punctuation mark is used instead); additional con-
junctions or punctuation marks are not connected
to the graph.
7
A linguistic difference between our representa-
tions that highlights variable granularities of anal-
ysis and, relatedly, diverging views on the scope
of the problem can be observed in Figure 2. Much
noun phrase?internal structure is not made explicit
in the PTB, and the Enju Treebank from which
our PAS representation derives predates the brack-
eting work of Vadas and Curran (2007). In the
four-way nominal compounding example of Fig-
ure 2, thus, PAS arrives at a strictly left-branching
tree, and there is no attempt at interpreting seman-
tic roles among the members of the compound ei-
ther; PCEDT, on the other hand, annotates both the
actual compound-internal bracketing and the as-
signment of roles, e.g. making stock the PAT(ient)
of investment. In this spirit, the PCEDT annota-
tions could be directly paraphrased along the lines
of plans by employees for investment in stocks. In
a middle position between the other two, DM dis-
ambiguates the bracketing but, by design, merely
assigns an underspecified, construction-specific de-
pendency type; its compound dependency, then,
is to be interpreted as the most general type of de-
pendency that can hold between the elements of
this construction (i.e. to a first approximation either
an argument role or a relation parallel to a prepo-
sition, as in the above paraphrase). The DM and
PCEDT annotations of this specific example hap-
pen to diverge in their bracketing decisions, where
the DM analysis corresponds to [...] investments
in stock for employees, i.e. grouping the concept
7
As detailed by Miyao et al. (2014), individual con-
juncts can be (and usually are) arguments of other predicates,
whereas the topmost conjunction only has incoming edges in
nested coordinate structures. Similarly, a ?shared? modifier of
the coordinate structure as a whole would take as its argument
the local top node of the coordination in DM or PAS (i.e. the
first conjunct or final conjunction, respectively), whereas it
would depend as an argument on all conjuncts in PCEDT.
employee stock (in contrast to ?common stock?).
Without context and expert knowledge, these de-
cisions are hard to call, and indeed there has been
much previous work seeking to identify and anno-
tate the relations that hold between members of a
nominal compound (see Nakov, 2013, for a recent
overview). To what degree the bracketing and role
disambiguation in this example are determined by
the linguistic signal (rather than by context and
world knowledge, say) can be debated, and thus the
observed differences among our representations in
this example relate to the classic contrast between
?sentence? (or ?conventional?) meaning, on the one
hand, and ?speaker? (or ?occasion?) meaning, on
the other hand (Quine, 1960; Grice, 1968). In
turn, we acknowledge different plausible points of
view about which level of semantic representation
should be the target representation for data-driven
parsing (i.e. structural analysis guided by the gram-
matical system), and which refinements like the
above could be construed as part of a subsequent
task of interpretation.
5 Task Setup
Training data for the task, providing all columns in
the file format sketched in Section 3 above, together
with a first version of the SDP toolkit?including
graph input, basic statistics, and scoring?were
released to candidate participants in early Decem-
ber 2013. In mid-January, a minor update to the
training data and optional syntactic ?companion?
analyses (see below) were provided, and in early
February the description and evaluation of a sim-
ple baseline system (using tree approximations and
the parser of Bohnet, 2010). Towards the end of
March, an input-only version of the test data was
released, with just columns (1) to (4) pre-filled; par-
ticipants then had one week to run their systems on
these inputs, fill in columns (5), (6), and upwards,
and submit their results (from up to two different
runs) for scoring. Upon completion of the testing
phase, we have shared the gold-standard test data,
official scores, and system results for all submis-
sions with participants and are currently preparing
all data for general release through the Linguistic
Data Consortium.
68
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Peking 85.91 90.27 88.54 89.40 26.71 93.44 90.69 92.04 38.13 78.75 73.96 76.28 11.05
Priberam 85.24 88.82 87.35 88.08 22.40 91.95 89.92 90.93 32.64 78.80 74.70 76.70 09.42
Copenhagen-
80.77 84.78 84.04 84.41 20.33 87.69 88.37 88.03 10.16 71.15 68.65 69.88 08.01
Malm?
Potsdam 77.34 79.36 79.34 79.35 07.57 88.15 81.60 84.75 06.53 69.68 66.25 67.92 05.19
Alpage 76.76 79.42 77.24 78.32 09.72 85.65 82.71 84.16 17.95 70.53 65.28 67.81 06.82
Link?ping 72.20 78.54 78.05 78.29 06.08 76.16 75.55 75.85 01.19 60.66 64.35 62.45 04.01
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Priberam 86.27 90.23 88.11 89.16 26.85 92.56 90.97 91.76 37.83 80.14 75.79 77.90 10.68
CMU 82.42 84.46 83.48 83.97 08.75 90.78 88.51 89.63 26.04 76.81 70.72 73.64 07.12
Turku 80.49 80.94 82.14 81.53 08.23 87.33 87.76 87.54 17.21 72.42 72.37 72.40 06.82
Potsdam 78.60 81.32 80.91 81.11 09.05 89.41 82.61 85.88 07.49 70.35 67.33 68.80 05.42
Alpage 78.54 83.46 79.55 81.46 10.76 87.23 82.82 84.97 15.43 70.98 67.51 69.20 06.60
In-House 75.89 92.58 92.34 92.46 48.07 92.09 92.02 92.06 43.84 40.89 45.67 43.15 00.30
Table 4: Results of the closed (top) and open tracks (bottom). For each system, the second column (LF)
indicates the averaged LF score across all target representations), which was used to rank the systems.
Evaluation Systems participating in the task
were evaluated based on the accuracy with which
they can produce semantic dependency graphs for
previously unseen text, measured relative to the
gold-standard testing data. The key measures for
this evaluation were labeled and unlabeled preci-
sion and recall with respect to predicted dependen-
cies (predicate?role?argument triples) and labeled
and unlabeled exact match with respect to complete
graphs. In both contexts, identification of the top
node(s) of a graph was considered as the identifi-
cation of additional, ?virtual? dependencies from
an artificial root node (at position 0). Below we
abbreviate these metrics as (a) labeled precision,
recall, and F
1
: LP, LR, LF; (b) unlabeled precision,
recall, and F
1
: UP, UR, UF; and (c) labeled and
unlabeled exact match: LM, UM.
The ?official? ranking of participating systems, in
both the closed and the open tracks, is determined
based on the arithmetic mean of the labeled depen-
dency F
1
scores (i.e. the geometric mean of labeled
precision and labeled recall) on the three target rep-
resentations (DM, PAS, and PCEDT). Thus, to be
considered for the final ranking, a system had to
submit semantic dependencies for all three target
representations.
Closed vs. Open Tracks The task was sub-
divided into a closed track and an open track, where
systems in the closed track could only be trained
on the gold-standard semantic dependencies dis-
tributed for the task. Systems in the open track, on
the other hand, could use additional resources, such
as a syntactic parser, for example?provided that
they make sure to not use any tools or resources
that encompass knowledge of the gold-standard
syntactic or semantic analyses of the SDP 2014
test data, i.e. were directly or indirectly trained or
otherwise derived from WSJ Section 21.
This restriction implies that typical off-the-shelf
syntactic parsers had to be re-trained, as many data-
driven parsers for English include this section of
the PTB in their default training data. To simplify
participation in the open track, the organizers pre-
pared ready-to-use ?companion? syntactic analyses,
sentence- and token-aligned to the SDP data, in
two formats, viz. PTB-style phrase structure trees
obtained from the parser of Petrov et al. (2006) and
Stanford Basic syntactic dependencies (de Marn-
effe et al., 2006) produced by the parser of Bohnet
and Nivre (2012).
6 Submissions and Results
From 36 teams who had registered for the task,
test runs were submitted for nine systems. Each
team submitted one or two test runs per track. In
total, there were ten runs submitted to the closed
track and nine runs to the open track. Three teams
submitted to both the closed and the open track.
The main results are summarized and ranked in
Table 4. The ranking is based on the average LF
score across all three target representations, which
is given in the LF column. In cases where a team
submitted two runs to a track, only the highest-
ranked score is included in the table.
69
Team Track Approach Resources
Link?ping C extension of Eisner?s algorithm for DAGs, edge-factored
structured perceptron
?
Potsdam C & O graph-to-tree transformation, Mate companion
Priberam C & O model with second-order features, decoding with dual decom-
position, MIRA
companion
Turku O cascade of SVM classifiers (dependency recognition, label
classification, top recognition)
companion,
syntactic n-grams,
word2vec
Alpage C & O transition-based parsing for DAGs, logistic regression, struc-
tured perceptron
companion,
Brown clusters
Peking C transition-based parsing for DAGs, graph-to-tree transforma-
tion, parser ensemble
?
CMU O edge classification by logistic regression, edge-factored struc-
tured SVM
companion
Copenhagen-Malm? C graph-to-tree transformation, Mate ?
In-House O existing parsers developed by the organizers grammars
Table 5: Overview of submitted systems, high-level approaches, and additional resources used (if any).
In the closed track, the average LF scores across
target representations range from 85.91 to 72.20.
Comparing the results for different target represen-
tations, the average LF scores across systems are
85.96 for PAS, 82.97 for DM, and 70.17 for PCEDT.
The scores for labeled exact match show a much
larger variation across both target representations
and systems.
8
In the open track, we see very similar trends.
The average LF scores across target representations
range from 86.27 to 75.89 and the corresponding
scores across systems are 88.64 for PAS, 84.95
for DM, and 67.52 for PCEDT. While these scores
are consistently higher than in the closed track,
the differences are small. In fact, for each of the
three teams that submitted to both tracks (Alpage,
Potsdam, and Priberam) improvements due to the
use of additional resources in the open track do not
exceed two points LF.
7 Overview of Approaches
Table 5 shows a summary of the systems that sub-
mitted final results. Most of the systems took
a strategy to use some algorithm to process (re-
stricted types of) graph structures, and apply ma-
chine learning like structured perceptrons. The
methods for processing graph structures are clas-
sified into three types. One is to transform graphs
into trees in the preprocessing stage, and apply con-
ventional dependency parsing systems (e.g. Mate;
Bohnet, 2010) to the converted trees. Some sys-
tems simply output the result of dependency pars-
ing (which means they inherently lose some depen-
8
Please see the task web page at the address indicated
above for full labeled and unlabeled scores.
dencies), while the others apply post-processing
to recover non-tree structures. The second strat-
egy is to use a parsing algorithm that can directly
generate graph structures (in the spirit of Sagae &
Tsujii, 2008; Titov et al., 2009). In many cases
such algorithms generate restricted types of graph
structures, but these restrictions appear feasible for
our target representations. The last approach is
more machine learning?oriented; they apply classi-
fiers or scoring methods (e.g. edge-factored scores),
and find the highest-scoring structures by some de-
coding method.
It is difficult to tell which approach is the best;
actually, the top three systems in the closed and
open tracks selected very different approaches. A
possible conclusion is that exploiting existing sys-
tems or techniques for dependency parsing was
successful; for example, Peking built an ensemble
of existing transition-based and graph-based depen-
dency parsers, and Priberam extended an existing
dependency parser. As we indicated in the task de-
scription, a novel feature of this task is that we have
to compute graph structures, and cannot assume
well-known properties like projectivity and lack of
reentrancies. However, many of the participants
found that our representations are mostly tree-like,
and this fact motivated them to apply methods that
have been well studied in the field of syntactic de-
pendency parsing.
Finally, we observe that three teams participated
in both the closed and open tracks, and all of them
reported that adding external resources improved
accuracy by a little more than one point. Systems
with (only) open submissions extensively use syn-
tactic features (e.g. dependency paths) from exter-
nal resources, and they are shown effective even
70
with simple machine learning models. Pre-existing,
tree-oriented dependency parsers are relatively ef-
fective, especially when combined with graph-to-
tree transformation. Comparing across our three
target representations, system scores show a ten-
dency PAS> DM> PCEDT, which can be taken as
a tentative indicator of relative levels of ?parsabil-
ity?. As suggested in Section 4, this variation most
likely correlates at least in part with diverging de-
sign decisions, e.g. the inclusion of relatively local
and deterministic dependencies involving function
words in PAS, or the decision to annotate contex-
tually determined speaker meaning (rather than
?mere? sentence meaning) in at least some construc-
tions in PCEDT.
8 Conclusions and Outlook
We have described the motivation, design, and out-
comes of the SDP 2014 task on semantic depen-
dency parsing, i.e. retrieving bi-lexical predicate?
argument relations between all content words
within an English sentence. We have converted to
a common format three existing annotations (DM,
PAS, and PCEDT) over the same text and have put
this to use for the first time in training and testing
data-driven semantic dependency parsers. Building
on strong community interest already to date and
our belief that graph-oriented dependency parsing
will further gain importance in the years to come,
we are preparing a similar (slightly modified) task
for SemEval 2015. Candidate modifications and
extensions will include cross-domain testing and
evaluation at the level of ?complete? predications
(in contrast to more lenient per-dependency F
1
used
this year). As optional new sub-tasks, we plan on
offering cross-linguistic variation and predicate (i.e.
semantic frame) disambiguation for at least some of
the target representations. To further probe the role
of syntax in the recovery of semantic dependency
relations, we will make available to participants
a wider selection of syntactic analyses, as well as
add a third (idealized) ?gold? track, where syntactic
dependencies are provided directly from available
syntactic annotations of the underlying treebanks.
Acknowledgements
We are grateful to ?eljko Agi
?
c and Bernd Bohnet
for consultation and assistance in preparing our
baseline and companion parses, to the Linguistic
Data Consortium (LDC) for support in distributing
the SDP data to participants, as well as to Emily M.
Bender and two anonymous reviewers for feedback
on this manuscript. Data preparation was supported
through access to the ABEL high-performance com-
puting facilities at the University of Oslo, and we
acknowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. Part of this
work has been supported by the infrastructural fund-
ing by the Ministry of Education, Youth and Sports
of the Czech Republic (CEP ID LM2010013).
References
Bohnet, B. (2010). Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (p. 89 ? 97). Beijing, China.
Bohnet, B., & Nivre, J. (2012). A transition-based
system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 1455 ? 1465). Jeju
Island, Korea.
Buchholz, S., & Marsi, E. (2006). CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of the 10th Conference on Natural Lan-
guage Learning (p. 149 ? 164). New York, NY,
USA.
Copestake, A., Flickinger, D., Pollard, C., & Sag, I. A.
(2005). Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4),
281 ? 332.
de Marneffe, M.-C., MacCartney, B., & Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 449 ? 454). Genoa, Italy.
Fares, M., Oepen, S., & Zhang, Y. (2013). Machine
learning for high-quality tokenization. Replicating
variable tokenization schemes. In Computational lin-
guistics and intelligent text processing (p. 231 ? 244).
Springer.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1), 15 ? 28.
Flickinger, D., Zhang, Y., & Kordoni, V. (2012). Deep-
Bank. A dynamically annotated treebank of the Wall
Street Journal. In Proceedings of the 11th Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries (p. 85 ? 96). Lisbon, Portugal: Edi??es Colibri.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling
of semantic roles. Computational Linguistics, 28,
71
245 ? 288.
Grice, H. P. (1968). Utterer?s meaning, sentence-
meaning, and word-meaning. Foundations of Lan-
guage, 4(3), 225 ? 242.
Haji?c, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Mart?, M. A., M?rquez, L., . . . Zhang, Y. (2009).
The CoNLL-2009 Shared Task. syntactic and seman-
tic dependencies in multiple languages. In Proceed-
ings of the 13th Conference on Natural Language
Learning (p. 1 ? 18). Boulder, CO, USA.
Haji?c, J., Haji?cov?, E., Panevov?, J., Sgall, P., Bojar,
O., Cinkov?, S., . . . ?abokrtsk?, Z. (2012). An-
nouncing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(p. 3153 ? 3160). Istanbul, Turkey.
Ivanova, A., Oepen, S., ?vrelid, L., & Flickinger, D.
(2012). Who did what to whom? A contrastive study
of syntacto-semantic dependencies. In Proceedings
of the Sixth Linguistic Annotation Workshop (p. 2 ?
11). Jeju, Republic of Korea.
Kate, R. J., & Wong, Y. W. (2010). Semantic pars-
ing. The task, the state of the art and the future. In
Tutorial abstracts of the 20th Meeting of the Associ-
ation for Computational Linguistics (p. 6). Uppsala,
Sweden.
Marcus, M., Santorini, B., & Marcinkiewicz, M. A.
(1993). Building a large annotated corpora of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19, 313 ? 330.
Mel?
?
cuk, I. (1988). Dependency syntax. Theory and
practice. Albany, NY, USA: SUNY Press.
Meyers, A., Reeves, R., Macleod, C., Szekely, R.,
Zielinska, V., Young, B., & Grishman, R. (2004).
Annotating noun argument structure for NomBank.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (p. 803 ?
806). Lisbon, Portugal.
Miyao, Y., Oepen, S., & Zeman, D. (2014). In-house:
An ensemble of pre-existing off-the-shelf parsers. In
Proceedings of the 8th International Workshop on
Semantic Evaluation. Dublin, Ireland.
Nakov, P. (2013). On the interpretation of noun com-
pounds: Syntax, semantics, and entailment. Natural
Language Engineering, 19(3), 291 ? 330.
Nivre, J., Hall, J., K?bler, S., McDonald, R., Nilsson,
J., Riedel, S., & Yuret, D. (2007). The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 915 ? 932). Prague,
Czech Republic.
Oepen, S., & L?nning, J. T. (2006). Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The
Proposition Bank. A corpus annotated with semantic
roles. Computational Linguistics, 31(1), 71 ? 106.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Meeting of the Association for Computational
Linguistics (p. 433 ? 440). Sydney, Australia.
Quine, W. V. O. (1960). Word and object. Cambridge,
MA, USA: MIT press.
Sagae, K., & Tsujii, J. (2008). Shift-reduce depen-
dency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (p. 753 ? 760). Manchester, UK.
Titov, I., Henderson, J., Merlo, P., & Musillo, G.
(2009). Online graph planarisation for synchronous
parsing of semantic and syntactic dependencies. In
Proceedings of the 21st International Joint Confer-
ence on Artifical Intelligence (p. 1562 ? 1567).
Vadas, D., & Curran, J. (2007). Adding Noun Phrase
Structure to the Penn Treebank. In Proceedings of
the 45th Meeting of the Association for Computa-
tional Linguistics (p. 240 ? 247). Prague, Czech Re-
public.
72
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335?340,
Dublin, Ireland, August 23-24, 2014.
In-House: An Ensemble of Pre-Existing Off-the-Shelf Parsers
Yusuke Miyao
?
, Stephan Oepen
??
, and Daniel Zeman
?
?
National Institute of Informatics, Tokyo
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
?
Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
yusuke@nii.ac.jp, oe@ifi.uio.no, zeman@ufal.mff.cuni.cz
Abstract
This submission to the open track of
Task 8 at SemEval 2014 seeks to connect
the Task to pre-existing, ?in-house? pars-
ing systems for the same types of target
semantic dependency graphs.
1 Background and Motivation
The three target representations for Task 8 at
SemEval 2014, Broad-Coverage Semantic Depen-
dency Parsing (SDP; Oepen et al., 2014), are
rooted in language engineering efforts that have
been under continuous development for at least
the past decade. The gold-standard semantic de-
pendency graphs used for training and testing in
the Task result from largely manual annotation, in
part re-purposing and adapting resources like the
Penn Treebank (PTB; Marcus et al., 1993), Prop-
Bank (Palmer et al., 2005), and others. But the
groups who prepared the SDP target data have also
worked in parallel on automated parsing systems
for these representations.
Thus, for each of the target representations,
there is a pre-existing parser, often developed in
parallel to the creation of the target dependency
graphs, viz. (a) for the DM representation, the
parser of the hand-engineered LinGO English Re-
source Grammar (ERG; Flickinger, 2000); (b) for
PAS, the Enju parsing system (Miyao, 2006), with
its probabilistic HPSG acquired through linguis-
tic projection of the PTB; and (c) for PCEDT,
the scenario for English analysis within the Treex
framework (Popel and ?abokrtsk?, 2010), com-
bining data-driven dependency parsing with hand-
engineered tectogrammatical conversion. At least
This work is licenced under a Creative Commons At-
tribution 4.0 International License; page numbers and the
proceedings footer are added by the organizers. http://
creativecommons.org/licenses/by/4.0/
for DM and PAS, these parsers have been exten-
sively engineered and applied successfully in a
variety of applications, hence represent relevant
points of comparison. Through this ?in-house?
submission (of our ?own? parsers to our ?own?
task), we hope to facilitate the comparison of dif-
ferent approaches submitted to the Task with this
pre-existing line of parser engineering.
2 DM: The English Resource Grammar
Semantic dependency graphs in the DM target rep-
resentation, DELPH-IN MRS-Derived Bi-Lexical
Dependencies, stem from a two-step ?reduc-
tion? (simplification) of the underspecified logical-
form meaning representations output natively by
the ERG parser, which implements the linguis-
tic framework of Head-Driven Phrase Structure
Grammar (HPSG; Pollard and Sag, 1994). Gold-
standard DM training and test data for the Task
were derived from the manually annotated Deep-
Bank Treebank (Flickinger et al., 2012), which
pairs Sections 00?21 of the venerable PTB Wall
Street Journal (WSJ) Corpus with complete ERG-
compatible HPSG syntactico-semantic analyses.
DeepBank as well as the ERG rely on Minimal Re-
cursion Semantics (MRS; Copestake et al., 2005)
for meaning representation, such that the exact
same post-processing steps could be applied to the
parser outputs as were used in originally reducing
the gold-standard MRSs from DeepBank into the
SDP bi-lexical semantic dependency graphs.
Parsing Setup The ERG parsing system is a hy-
brid, combining (a) the hand-built, broad-coverage
ERG with (b) an efficient chart parser for uni-
fication grammars and (c) a conditional proba-
bility distribution over candidate analyses. The
parser most commonly used with the ERG, called
PET (Callmeier, 2002),
1
constructs a complete,
1
The SDP test data was parsed using the 1212 release
of the ERG, using PET and converter versions from what
335
subsumption-based parse forest of partial HPSG
derivations (Oepen and Carroll, 2000), and then
extracts from the forest n-best lists (in globally
correct rank order) of complete analyses according
to a discriminative parse ranking model (Zhang et
al., 2007). For our experiments, we trained the
parse ranker on Sections 00?20 of DeepBank and
otherwise used the default, non-pruning develop-
ment configuration, which is optimized for accu-
racy. In this setup, ERG parsing on average takes
close to ten seconds per sentence.
Post-Parsing Conversion After parsing, MRSs
are reduced to DM bi-lexical semantic dependen-
cies in two steps. First, Oepen and L?nning
(2006) define a conversion to variable-free Ele-
mentary Dependency Structures (EDS), which (a)
maps each predication in the MRS logical-form
meaning representation to a node in a dependency
graph and (b) transforms argument relations rep-
resented by shared logical variables into directed
dependency links between graph nodes. This first
step of the conversion is ?mildly? lossy, in that
some scope-related information is discarded; the
EDS graph, however, will contain the same num-
ber of nodes and the same set of argument de-
pendencies as there are predications and semantic
role assignments in the original MRS. In particu-
lar, the EDS may still reflect non-lexical semantic
predications introduced by grammatical construc-
tions like covert quantifiers, nominalization, com-
pounding, or implicit conjunction.
2
Second, in another conversion step that is not
information-preserving, the EDS graphs are fur-
ther reduced into strictly bi-lexical form, i.e. a set
of directed, binary dependency relations holding
exclusively between lexical units. This conversion
is defined by Ivanova et al. (2012) and seeks to
(a) project some aspects of construction seman-
tics onto word-to-word dependencies (for example
introducing specific dependency types for com-
pounding or implicit conjunction) and (b) relate
the linguistically informed ERG-internal tokeniza-
tion to the conventions of the PTB.
3
Seeing as both
is called the LOGON SVN trunk as of January 2014; see
http://moin.delph-in.net/LogonTop for detail.
2
Conversely, semantically vacuous parts of the original
input (e.g. infinitival particles, complementizers, relative pro-
nouns, argument-marking prepositions, auxiliaries, and most
punctuation marks) were not represented in the MRS in the
first place, hence have no bearing on the conversion.
3
Adaptations of tokenization encompass splitting ?multi-
word? ERG tokens (like such as or ad hoc), as well as ?hiding?
ERG token boundaries at hyphens or slashes (e.g. 77-year-
conversion steps are by design lossy, DM seman-
tic dependency graphs present a true subset of the
information encoded in the full, original MRS.
3 PAS: The Enju Parsing System
Enju Predicate?Argument Structures (PAS) are
derived from the automatic HPSG-style annota-
tion of the PTB, which was primarily used for the
development of the Enju parsing system
4
(Miyao,
2006). A notable feature of this parser is that the
grammar is not developed by hand; instead, the
Enju HPSG-style treebank is first developed, and
the grammar (or, more precisely, the vast major-
ity of lexical entries) is automatically extracted
from the treebank (Miyao et al., 2004). In this
?projection? step, PTB annotations such as empty
categories and coindexation are used for deriv-
ing the semantic representations that correspond
to HPSG derivations. Its probabilistic model for
disambiguation is also trained using this treebank
(Miyao and Tsujii, 2008).
5
The PAS data set is an extraction of predicate?
argument structures from the Enju HPSG tree-
bank. The Enju parser outputs results in ?ready-
to-use? formats like phrase structure trees and
predicate?argument structures, as full HPSG anal-
yses are not friendly to users who are not famil-
iar with the HPSG theory. The gold-standard PAS
target data in the Task was developed using this
function; the conversion program from full HPSG
analyses to predicate?argument structures was ap-
plied to the Enju Treebank.
Predicate?argument structures (PAS) represent
word-to-word semantic dependencies, such as se-
mantic subject and object. Each dependency type
is represented with two elements: the type of the
predicate, such as verb and adjective, and the ar-
gument label, such as ARG1 and ARG2.
6
old), which the PTB does not split.
4
See http://kmcs.nii.ac.jp/enju/.
5
Abstractly similar to the ERG, the annotations of the
Enju treebank instantiate the linguistic theory of HPSG.
However, the two resources have been developed indepen-
dently and implementation details are quite different. The
most significant difference is that the Enju HPSG treebank is
developed by linguistic projection of PTB annotations, and
the Enju parser derived from the treebank; conversely, the
ERG was predominantly manually crafted, and it was later
applied in the DeepBank re-annotation of the WSJ Corpus.
6
Full details of the predicate?argument structures in the
Enju HPSG Treebank, are available in two documents linked
from the Enju web site (see above), viz. the Enju Output
Specification Manual and the XML Format Documentation.
336
Parsing Setup Basically we used the publicly
available package of the Enju parser ?as is? (see the
above web site). We did not change default pars-
ing parameters (beam width, etc.) and features.
However, the release version of the Enju parser is
trained with the HPSG treebank corresponding to
the Penn Treebank WSJ Sections 2?21, which in-
cludes the test set of the Task (Section 21). There-
fore, we re-trained the Enju parser using Sections
0?20, and used this re-trained parser in preparing
the PAS semantic dependency graphs in this en-
semble submission.
Post-Parsing Conversion The dependency for-
mat of the Enju parser is almost equivalent to what
is provided as the PAS data set in this shared task.
Therefore, the post-parsing conversion for the PAS
data involves only formatting, viz. (a) format con-
version into the tabular file format of the Task; and
(b) insertion of dummy relations for punctuation
tokens ignored in the output of Enju.
7
4 PCEDT: The Treex Parsing Scenario
The Prague Czech-English Dependency Treebank
(PCEDT; Haji
?
c et al., 2012)
8
is a set of parallel de-
pendency trees over the same WSJ texts from the
Penn Treebank, and their Czech translations. Sim-
ilarly to other treebanks in the Prague family, there
are two layers of syntactic annotation: analytical
(a-trees) and tectogrammatical (t-trees). Unlike
for the other two representations used in the Task,
for PCEDT there is no pre-existing parsing system
designed to deliver the full scale of annotations
of the SDP gold-standard data. The closest avail-
able match is a parsing scenario implemented in
the Treex natural language processing framework.
Parsing Setup Treex
9
(Popel and ?abokrtsk?,
2010) is a modular, open-source framework origi-
nally developed for transfer-based machine trans-
lation. It can accomplish any NLP-related task
by sequentially applying to the same piece of data
various blocks of code. Blocks operate on a com-
mon data structure and are chained in scenarios.
Some early experiments with scenarios for tec-
togrammatical analysis of English were described
by Klime? (2007). It is of interest that they report
7
The Enju parser ignores tokens tagged as ?.?, while
the PAS representation includes them with dummy relations;
thus, missing periods are inserted in post-processing by com-
parison to the original PTB token sequence.
8
See http://ufal.mff.cuni.cz/pcedt2.0/.
9
See http://ufal.mff.cuni.cz/treex/.
U.S. should regulate X more stringently than  Y
CPR
PAT
PRED
ACT
PAT
MANN
CPR
PAT
PRED
ACT
PAT
MANN CPR
Figure 1: PCEDT asserts two copies of the token
regulate (shown here as ?regulate? and ??, under-
lined). Projecting t-nodes onto the original tokens,
required by the SDP data format, means that the
 node will be merged with regulate. The edges
going to and from  will now lead to and from reg-
ulate (see the dotted arcs), which results in a cycle.
To get rid of the cycle, we skip  and connect di-
rectly its children, as shown in the final SDP graph
below the sentence.
an F
1
score of assigning functors (dependency la-
bels in PCEDT terminology) of 70.3%; however,
their results are not directly comparable to ours.
Due to the modular nature of Treex, there are
various conceivable scenarios to get the t-tree of
a sentence. We use the default scenario that con-
sists of 48 blocks: two initial blocks (reading the
input), one final block (writing the output), two
A2N blocks (named entity recognition), twelve
W2A blocks (dependency parsing at the analytical
layer) and 31 A2T and T2T blocks (creating the
t-tree based on the a-tree).
Most blocks are highly specialized in one par-
ticular subtask (e.g. there is a block just to make
sure that quotation marks are attached to the root
of the quoted subtree). A few blocks are respon-
sible for the bulk of the work. The a-tree is con-
structed by a block that contains the MST Parser
(McDonald et al., 2005), trained on the CoNLL
2007 English data (Nivre et al., 2007), i.e. Sec-
tions 2?11 of the PTB, converted to dependencies.
The annotation style of CoNLL 2007 differs from
PCEDT 2.0, and thus the unlabeled attachment
score of the analytical parser is only 66%.
Obviously one could expect better results if we
retrained the MST Parser directly on the PCEDT
a-trees, and on the whole training data. The only
reason why we did not do so was lack of time.
Our results thus really demonstrate what is avail-
able ?off-the-shelf?; on the other hand, the PCEDT
component of our ensemble fails to set any ?upper
bound? of output quality, as it definitely is not bet-
337
John brought and ate ripe apples and pears
ACT
CONJ
CONJ
PRED.m PRED.m
RSTR
PAT.m PAT.m
PAT
TOP TOP
PAT
PAT
ACT
ACT
CONJ.m CONJ.m
RSTR
RSTR
PAT
CONJ.m CONJ.m
Figure 2: Coordination in PCEDT t-tree (above)
and in the corresponding SDP graph (below).
ter informed than the other systems participating
in the Task.
Functor assignment is done heuristically, based
on POS tags and function words. The primary
focus of the scenario was on functors that could
help machine translation, thus it only generated
25 different labels (of the total set of 65 labels in
the SDP gold-standard data)
10
and left about 12%
of all nodes without functors. Precision peaks at
78% for ACT(or) relations, while the most fre-
quent error type (besides labelless dependencies)
is a falsely proposed RSTR(iction) relation. Both
ACT and RSTR are among the most frequent de-
pendency types in PCEDT.
Post-Parsing Conversion Once the t-tree has
been constructed, it is converted to the PCEDT
target representation of the Task, using the same
conversion code that was used to prepare the gold-
standard SDP data.
11
SDP graphs are defined over surface tokens but
the set of nodes of a t-tree need not correspond
one-to-one to the set of tokens. For example, there
are no t-nodes for punctuation and function words
(except in coordination); these tokens are rendered
as semantically vacuous in SDP, i.e. they do not
participate in edges. On the other hand, t-trees can
contain generated nodes, which represent elided
words and do not correspond to any surface to-
10
The system was able to output the following functors (or-
dered in the descending order of their frequency in the sys-
tem output): RSTR, PAT, ACT, CONJ.member, APP, MANN,
LOC, TWHEN, DISJ.member, BEN, RHEM, PREC, ACMP,
MEANS, ADVS.member, CPR, EXT, DIR3, CAUS, COND,
TSIN, REG, DIR2, CNCS, and TTILL.
11
In the SDP context, the target representation derived
from the PCEDT is called by the same name as the origi-
nal treebank; but note that the PCEDT semantic dependency
graphs only encode a subset of the information annotated at
the tectogrammatical layer of the full treebank.
DM PAS PCEDT
LF LM LF LM LF LM
Priberam .8916 .2685 .9176 .3783 .7790 .1068
In-House .9246 .4807 .9206 .4384 .4315 .0030
UF UM UF UM UF UM
Priberam .9032 .2990 .9281 .3924 .8903 .3071
In-House .9349 .5230 .9317 .4429 .6919 .0148
Table 1: End-to-end ?in-house? parsing results.
ken. Most generated nodes are leaves and, thus,
can simply be omitted from the SDP graphs. Other
generated nodes are copies of normal nodes and
they are linked to the same token to which the
source node is mapped. As a result, one token can
appear at several different positions in the tree; if
we project these occurrences into one node, the
graph will contain cycles. We decided to remove
all generated nodes causing cycles. Their chil-
dren are attached to their parents and inherit the
functor of the generated node (Figure 1). The con-
version procedure also removes cycles caused by
more fine-grained tokenization of the t-layer.
Furthermore, t-trees use technical edges to cap-
ture paratactic constructions where the relations
are not ?true? dependencies. The conversion pro-
cedure extracts true dependency relations: Each
conjunct is linked to the parent or to a shared child
of the coordination. In addition, there are also
links from the conjunction to the conjuncts and
they are labeled CONJ.m(ember). These links pre-
serve the paratactic structure (which can even be
nested) and the type of coordination. See Figure 2
for an example.
5 Results and Reflections
Seeing as our ?in-house? parsers are not directly
trained on the semantic dependency graphs pro-
vided for the Task, but rather are built from ad-
ditional linguistic resources, we submitted results
from the parsing pipelines sketched in Sections 2
to 4 above to the open SDP track. Table 1
summarizes parser performance in terms of la-
beled and unlabeled F
1
(LF and UF)
12
and full-
sentence exact match (LM and UM), comparing
to the best-performing submission (dubbed Prib-
eram; Martins and Almeida, 2014) to this track.
Judging by the official SDP evaluation metric, av-
erage labeled F
1
over the three representations,
our ensemble ranked last among six participating
12
Our ensemble members exhibit comparatively small dif-
ferences in recall vs. precision.
338
teams; in terms of unlabeled average F
1
, the ?in-
house? submission achieved the fourth rank.
As explained in the task description (Oepen et
al., 2014), parts of the WSJ Corpus were excluded
from the SDP training and testing data because
of gaps in the DeepBank and Enju treebanks, and
to exclude cyclic dependency graphs, which can
sometimes arise in the DM and PCEDT conver-
sions. For these reasons, one has to allow for the
possibility that the testing data is positively bi-
ased towards our ensemble members.
13
But even
with this caveat, it seems fair to observe that the
ERG and Enju parsers both are very competitive
for the DM and PAS target representations, respec-
tively, specifically so when judged in exact match
scores. A possible explanation for these results
lies in the depth of grammatical information avail-
able to these parsers, where DM or PAS seman-
tic dependency graphs are merely a simpliefied
view on the complete underlying HPSG analyses.
These parsers have performed well in earlier con-
trastive evaluation too (Miyao et al., 2007; Bender
et al., 2011; Ivanova et al., 2013; inter alios).
Results for the Treex English parsing scenario,
on the other hand, show that this ensemble mem-
ber is not fine-tuned for the PCEDT target rep-
resentation; due to the reasons mentioned above,
its performance even falls behind the shared task
baseline. As is evident from the comparison of
labeled vs. unlabeled F
1
scores, (a) the PCEDT
parser is comparatively stronger at recovering se-
mantic dependency structure than at assigning la-
bels, and (b) about the same appears to be the case
for the best-performing Priberam system (on this
target representation).
Acknowledgements
Data preparation and large-scale parsing in the
DM target representation was supported through
access to the ABEL high-performance computing
facilities at the University of Oslo, and we ac-
knowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. This project
has been supported by the infrastructural funding
13
There is no specific evidence that the WSJ sentences ex-
cluded in the Task for technical issues in either of the under-
lying treebanks or conversion procedures would be compara-
tively much easier to parse for other submissions than for the
members of our ?in-house? ensemble, but unlike other sys-
tems these parsers ?had a vote? in the selection of the data,
particularly so for the DM and PAS target representations.
by the Ministry of Education, Youth and Sports of
the Czech Republic (CEP ID LM2010013).
References
Bender, E. M., Flickinger, D., Oepen, S., and
Zhang, Y. (2011). Parser evaluation over local
and non-local deep dependencies in a large cor-
pus. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Pro-
cessing (p. 397 ? 408). Edinburgh, Scotland,
UK.
Callmeier, U. (2002). Preprocessing and encoding
techniques in PET. In S. Oepen, D. Flickinger,
J. Tsujii, and H. Uszkoreit (Eds.), Collabora-
tive language engineering. A case study in effi-
cient grammar-based processing (p. 127 ? 140).
Stanford, CA: CSLI Publications.
Copestake, A., Flickinger, D., Pollard, C., and
Sag, I. A. (2005). Minimal Recursion Seman-
tics. An introduction. Research on Language
and Computation, 3(4), 281 ? 332.
Flickinger, D. (2000). On building a more ef-
ficient grammar by exploiting types. Natural
Language Engineering, 6 (1), 15 ? 28.
Flickinger, D., Zhang, Y., and Kordoni, V. (2012).
DeepBank. A dynamically annotated treebank
of the Wall Street Journal. In Proceedings of the
11th International Workshop on Treebanks and
Linguistic Theories (p. 85 ? 96). Lisbon, Portu-
gal: Edi??es Colibri.
Haji
?
c, J., Haji
?
cov?, E., Panevov?, J., Sgall, P.,
Bojar, O., Cinkov?, S., . . . ?abokrtsk?, Z.
(2012). Announcing Prague Czech-English De-
pendency Treebank 2.0. In Proceedings of the
8th International Conference on Language Re-
sources and Evaluation (p. 3153 ? 3160). Istan-
bul, Turkey.
Ivanova, A., Oepen, S., Dridan, R., Flickinger, D.,
and ?vrelid, L. (2013). On different approaches
to syntactic analysis into bi-lexical dependen-
cies. An empirical comparison of direct, PCFG-
based, and HPSG-based parsers. In Proceedings
of the 13th International Conference on Parsing
Technologies (p. 63 ? 72). Nara, Japan.
Ivanova, A., Oepen, S., ?vrelid, L., and
Flickinger, D. (2012). Who did what to whom?
339
A contrastive study of syntacto-semantic depen-
dencies. In Proceedings of the Sixth Linguistic
Annotation Workshop (p. 2 ? 11). Jeju, Republic
of Korea.
Klime?, V. (2007). Transformation-based tec-
togrammatical dependency analysis of English.
In V. Matou?ek and P. Mautner (Eds.), Text,
speech and dialogue 2007, LNAI 4629 (p. 15 ?
22). Berlin / Heidelberg, Germany: Springer.
Marcus, M., Santorini, B., and Marcinkiewicz,
M. A. (1993). Building a large annotated cor-
pora of English: The Penn Treebank. Computa-
tional Linguistics, 19, 313 ? 330.
Martins, A. F. T., and Almeida, M. S. C. (2014).
Priberam. A turbo semantic parser with second
order features. In Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation.
Dublin, Ireland.
McDonald, R., Pereira, F., Ribarov, K., and Haji
?
c,
J. (2005). Non-projective dependency parsing
using spanning tree algorithms. In Proceedings
of the Human Language Technology Conference
and Conference on Empirical Methods in Nat-
ural Language Processing (p. 523 ? 530). Van-
couver, British Columbia, Canada.
Miyao, Y. (2006). From linguistic theory to
syntactic analysis. Corpus-oriented grammar
development and feature forest model. Doc-
toral Dissertation, University of Tokyo, Tokyo,
Japan.
Miyao, Y., Ninomiya, T., and Tsujii, J. (2004).
Corpus-oriented grammar development for ac-
quiring a Head-Driven Phrase Structure Gram-
mar from the Penn Treebank. In Proceedings of
the 1st International Joint Conference on Natu-
ral Language Processing (p. 684 ? 693).
Miyao, Y., Sagae, K., and Tsujii, J. (2007).
Towards framework-independent evaluation of
deep linguistic parsers. In Proceedings of
the 2007 Workshop on Grammar Engineering
across Frameworks (p. 238 ? 258). Palo Alto,
California.
Miyao, Y., and Tsujii, J. (2008). Feature forest
models for probabilistic HPSG parsing. Com-
putational Linguistics, 34(1), 35 ? 80.
Nivre, J., Hall, J., K?bler, S., McDonald, R., Nils-
son, J., Riedel, S., and Yuret, D. (2007). The
CoNLL 2007 shared task on dependency pars-
ing. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Conference on Natural
Language Learning (p. 915 ? 932). Prague,
Czech Republic.
Oepen, S., and Carroll, J. (2000). Ambiguity
packing in constraint-based parsing. Practical
results. In Proceedings of the 1st Meeting of the
North American Chapter of the Association for
Computational Linguistics (p. 162 ? 169). Seat-
tle, WA, USA.
Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D.,
Flickinger, D., Haji
?
c, J., . . . Zhang, Y. (2014).
SemEval 2014 Task 8. Broad-coverage seman-
tic dependency parsing. In Proceedings of the
8th International Workshop on Semantic Evalu-
ation. Dublin, Ireland.
Oepen, S., and L?nning, J. T. (2006).
Discriminant-based MRS banking. In Proceed-
ings of the 5th International Conference on
Language Resources and Evaluation (p. 1250 ?
1255). Genoa, Italy.
Palmer, M., Gildea, D., and Kingsbury, P. (2005).
The Proposition Bank. A corpus annotated with
semantic roles. Computational Linguistics,
31(1), 71 ? 106.
Pollard, C., and Sag, I. A. (1994). Head-Driven
Phrase Structure Grammar. Chicago, USA:
The University of Chicago Press.
Popel, M., and ?abokrtsk?, Z. (2010). TectoMT.
Modular NLP framework. Advances in Natural
Language Processing, 293 ? 304.
Zhang, Y., Oepen, S., and Carroll, J. (2007).
Efficiency in unification-based n-best parsing.
In Proceedings of the 10th International Con-
ference on Parsing Technologies (p. 48 ? 59).
Prague, Czech Republic.
340
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 212?215,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Hierarchical Phrase-Based MT at the Charles University
for the WMT 2010 Shared Task
Daniel Zeman
Charles University in Prague, Institute of Formal and Applied Linguistics (?FAL)
Univerzita Karlova v Praze, ?stav form?ln? a aplikovan? lingvistiky (?FAL)
Malostransk? n?m?st? 25, Praha, CZ-11800, Czechia
zeman@ufal.mff.cuni.cz
Abstract
We describe our experiments with hier-
archical phrase-based machine translation
for WMT 2010 Shared Task. We provide
a detailed description of our configuration
and data so the results are replicable. For
English-to-Czech translation, we experi-
ment with several datasets of various sizes
and with various preprocessing sequences.
For the other 7 translation directions, we
just present the baseline results.
1 Introduction
Czech is a language with rich morphology (both
inflectional and derivational) and relatively free
word order. In fact, the predicate-argument struc-
ture, often encoded by fixed word order in English,
is usually captured by inflection (especially the
system of 7 grammatical cases) in Czech. While
the free word order of Czech is a problem when
translating to English (the text should be parsed
first in order to determine the syntactic functions
and the English word order), generating correct in-
flectional affixes is indeed a challenge for English-
to-Czech systems. Furthermore, the multitude
of possible Czech word forms (at least order of
magnitude higher than in English) makes the data
sparseness problem really severe, hindering both
directions.
There are numerous ways how these issues
could be addressed. For instance, parsing and
syntax-aware reordering of the source-language
sentences can help with the word order differ-
ences (same goal could be achieved by a reorder-
ing model or a synchronous context-free grammar
in a hierarchical system). Factored translation, a
secondary language model of morphological tags
or even a morphological generator are some of the
possible solutions to the poor-to-rich translation is-
sues.
Our submission to the shared task should reveal
where a pure hierarchical system stands in this jun-
gle and what of the above mentioned ideas match
the phenomena the system suffers from. Although
our primary focus lies on English-to-Czech trans-
lation, we also report the accuracy of the same
system on moderately-sized corpora for the other
three languages and seven translation directions.
2 The Translation System
Our translation system belongs to the hierarchi-
cal phrase-based class (Chiang, 2007), i.e. phrase
pairs with nonterminals (rules of a synchronous
context-free grammar) are extracted from sym-
metrized word alignments and subsequently used
by the decoder. We use Joshua, a Java-based open-
source implementation of the hierarchical decoder
(Li et al, 2009), release 1.1.1
Word alignment was computed using the first
three steps of the train-factored-phrase-
model.perl script packed with Moses2 (Koehn et
al., 2007). This includes the usual combination of
word clustering using mkcls3 (Och, 1999), two-
way word alignment using GIZA++4 (Och and
Ney, 2003), and alignment symmetrization using
the grow-diag-final-and heuristic (Koehn et al,
2003).
For language modeling we use the SRILM
toolkit5 (Stolcke, 2002) with modified Kneser-
Ney smoothing (Kneser and Ney, 1995; Chen and
Goodman, 1998).
We use the Z-MERT implementation of mini-
mum error rate training (Zaidan, 2009). The fol-
lowing settings have been used for Joshua and Z-
MERT:
1http://sourceforge.net/projects/joshua/
2http://www.statmt.org/moses/
3http://fjoch.com/mkcls.html
4http://fjoch.com/GIZA++.html
5http://www-speech.sri.com/projects/srilm/
212
? Grammar extraction:
--maxPhraseLength=5
? Decoding: span_limit=10 fuzz1=0.1
fuzz2=0.1 max_n_items=30 rela-
tive_threshold=10.0 max_n_rules=50
rule_relative_threshold=10.0
? N-best decoding: use_unique_nbest=true
use_tree_nbest=false
add_combined_cost=true top_n=300
? Z-MERT: -m BLEU 4 closest -maxIt 5
-ipi 20
3 Data and Pre-processing Pipeline
3.1 Baseline Experiments
We applied our system to all eight language pairs.
However, for all but one we ran only a baseline ex-
periment. From the data point of view the baseline
experiments were even more constrained than the
organizers of the shared task suggested. We did not
use the Europarl corpus, we only used the News
Commentary corpus6 for training. The target side
of the News Commentary corpus was also the only
source to train the language model. Table 1 shows
the size of the corpus.
Corpus SentPairs Tokens xx Tokens en
cs-en 94,742 2,077,947 2,327,656
de-en 100,269 2,524,909 2,484,445
es-en 98,598 2,742,935 2,472,860
fr-en 84,624 2,595,165 2,137,407
Table 1: Number of sentence pairs and tokens for
every language pair in the News Commentary cor-
pus. Unlike the organizers of the shared task, we
stick with the standard ISO 639 language codes: cs
= Czech, de = German, en = English, es = Spanish,
fr = French.
Note that in some cases the grammar extraction
algorithm in Joshua fails if the training corpus con-
tains sentences that are too long. Removing sen-
tences of 100 or more tokens (per advice by Joshua
developers) effectively healed all failures. Unfor-
tunately, for the baseline corpora the loss of train-
ing material was still considerable and resulted in
drop of BLEU score, though usually insignificant.7
6Available for download at http://www.statmt.org/
wmt10/translation-task.html using the link ?Parallel
corpus training data?.
7Table 1 and Table 2 present statistics before removing the
long sentences.
The News Test 2008 data set (2051 sentences
in each language) was used as development data
for MERT. BLEU scores reported in this paper
were computed on the News Test 2009 set (2525
sentences each language). The official scores on
News Test 2010 are given only in the main WMT
2010 paper.
Only lowercased data were used for the baseline
experiments.
3.2 English-to-Czech
A separate set of experiments has been conducted
for the English-to-Czech direction and larger data
were used. We used CzEng 0.9 (Bojar and
?abokrtsk?, 2009)8 as our main parallel corpus.
Following CzEng authors? request, we did not use
sections 8* and 9* reserved for evaluation pur-
poses.
As the baseline training dataset (?Small? in the
following) only the news section of CzEng was
used. For large-scale experiments (?Large? in the
following), we used all CzEng together with the
EMEA corpus9 (Tiedemann, 2009).10
As our monolingual data we use the mono-
lingual data provided by WMT10 organizers for
Czech. Table 2 shows the sizes of these corpora.
Corpus SentPairs Tokens cs Tokens en
Small 126,144 2,645,665 2,883,893
Large 7,543,152 79,057,403 89,018,033
Mono 13,042,040 210,507,305
Table 2: Number of sentences and tokens in the
Czech-English corpora.
Again, the official WMT 201011 development
set (News Test 2008, 2051 sentences each lan-
guage) and test set (News Test 2009, 2525 sen-
tences each language) are used forMERT and eval-
uation, respectively. The official scores on News
Test 2010 are given only in the main WMT 2010
paper.
We use a slightly modified tokenization rules
compared to CzEng export format. Most notably,
we normalize English abbreviated negation and
auxiliary verbs (?couldn?t? ? ?could not?) and
8http://ufal.mff.cuni.cz/czeng/
9http://urd.let.rug.nl/tiedeman/OPUS/EMEA.php
10Unfortunately, the EMEA corpus is badly tokenized on
the Czech side with fractional numbers split into several to-
kens (e.g. ?3, 14?). We attempted to reconstruct the original
detokenized form using a small set of regular expressions.
11http://www.statmt.org/wmt10
213
attempt at normalizing quotation marks to distin-
guish between opening and closing one following
proper typesetting rules.
The rest of our pre-processing pipeline matches
the processing employed in CzEng (Bojar and
?abokrtsk?, 2009).12 We use ?supervised truecas-
ing?, meaning that we cast the case of the lemma
to the form, relying on our morphological analyz-
ers and taggers to identify proper names, all other
words are lowercased.
4 Experiments
All BLEU scores were computed directly by
Joshua on the News Test 2009 set. Note that
they differ from what the official evaluation script
would report, due to different tokenization.
4.1 Baseline Experiments
The set of baseline experiments with all translation
directions involved running the system on lower-
cased News Commentary corpora. Word align-
ments were computed on 4-character stems (in-
cluding the en-cs and cs-en directions). A trigram
language model was trained on the target side of
the parallel corpus.
Direction BLEU
en-cs 0.0905
en-de 0.1114
cs-en 0.1471
de-en 0.1617
en-es 0.1966
en-fr 0.2001
fr-en 0.2020
es-en 0.2025
Table 3: Lowercased BLEU scores of the baseline
experiments on News Test 2009 data.
4.2 English-to-Czech
The extended (non-baseline) English-to-Czech ex-
periments were trained on larger parallel and
monolingual data, described in Section 3.2. Note
that the dataset denoted as ?Small? still falls into
the constrained task because it only uses CzEng
0.9 and the WMT 2010 monolingual data.
12Due to the subsequent processing, incl. parsing, the tok-
enization of English follows PennTreebenk style. The rather
unfortunate convention of treating hyphenated words as sin-
gle tokens increases our out-of-vocabulary rate.
Word alignments were computed on lemmatized
version of the parallel corpus. Hexagram language
model was trained on the monolingual data. True-
cased data were used for training, as described
above; the BLEU scores of these experiments in
Table 4 are computed on truecased system output.
Setup BLEU
Baseline 0.0905
Small 0.1012
Large 0.1300
Table 4: BLEU scores (lowercased baseline, true-
cased rest) of the English-to-Czech experiments,
including the baseline experiment with News
Commentary, mentioned earlier.
As for the official evaluation on News Test
2010, we used the Small setup as our primary sub-
mission, and the Large setup as secondary despite
its better results. The reason was that it was not
clear whether the experiment would be finished in
time for the official evaluation.13
An interesting perspective on the three en-cs
models is provided by the feature weights opti-
mized duringMERT.We can see in Table 5 that the
small and relatively weak baseline LM is trusted
less than the most influential translation feature
while for large parallel data and even much larger
LM the weights are distributed more evenly.
Setup LM Pt0 Pt1 Pt2 WP
Baseline 1.0 1.55 0.51 0.63 ?2.63
Small 1.0 1.03 0.72 ?0.09 ?0.34
Large 1.0 0.98 0.97 ?0.02 ?0.82
Table 5: Feature weights are relative to the weight
of LM , the score by the language model. Then
there are the three translation features: Pt0 =
P (e|f), Pt1 = Plex(f |e) and Pt2 = Plex(e|f).
WP is the word penalty.
4.3 Efficiency
The machines on which the experiments were con-
ducted are 64bit Intel Xeon dual core 2.8 GHz
CPUs with 32 GB RAM.
Word alignment of each baseline corpus took
about 1 hour, time needed for data preprocessing
13In fact, it was not finished in time. Due to a failure of
a MERT run, we used feature weights from the primary sub-
mission for the secondary one, too.
214
and training of the language model was negligible.
Grammar extraction took about four hours but it
could be parallelized. For decoding the test data
were split into 20 chunks that were processed in
parallel. OneMERT iteration, including decoding,
took from 30 minutes to 1 hour.
Training the large en-cs models requires more
careful engineering. The grammar extraction eas-
ily consumes over 20 GB memory so it is impor-
tant to make sure Java really has access to it. We
parallelized the extraction in the same way as we
had done with the decoding; even so, about 5 hours
were needed to complete the extraction. The de-
coder now must use the SWIG-linked SRILM li-
brary because Java-based languagemodeling is too
slow and memory-consuming. Otherwise, the de-
coding times are comparable to the baseline exper-
iments.
5 Conclusion
We have described the hierarchical phrase-based
SMT system we used for the WMT 2010 shared
task. For English-to-Czech translation, we dis-
cussed experiments with large data from the point
of view of both the translation accuracy and effi-
ciency.
This has been our first attempt to switch to hier-
archical SMT and we have not gone too far beyond
just putting together the infrastructure and apply-
ing it to the available data. Nevertheless, our en-cs
experiments not only confirm that more data helps;
in the Small and Large setup, the data was not only
larger than in Baseline, it also underwent a more
refined preprocessing. In particular, we took ad-
vantage of the Czeng corpus being lemmatized to
produce better word alignment; also, the truecas-
ing technique helped to better target named enti-
ties.
Acknowledgements
The work on this project was supported by the
grant MSM0021620838 by the Czech Ministry of
Education.
References
Ond?ej Bojar and Zden?k ?abokrtsk?. 2009. Czeng
0.9: Large parallel treebank with rich annotation.
The Prague Bulletin of Mathematical Linguistics,
92:63?83.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. In Technical report TR-10-98, Computer
Science Group, Harvard, MA, USA, August. Har-
vard University.
David Chiang. 2007. Hierarchical Phrase-
Based Translation. Computational Linguistics,
33(2):201?228.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
pages 181?184, Los Alamitos, California, USA.
IEEE Computer Society Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Compan-
ion Volume Proceedings of the Demo and Poster Ses-
sions, pages 177?180, Praha, Czechia, June. Associ-
ation for Computational Linguistics.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur,
and Wren Thornton. 2009. Decoding in Joshua:
Open Source, Parsing-Based Machine Translation.
The Prague Bulletin of Mathematical Linguistics,
91:47?56, 1.
Franz Josef Och andHermannNey. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
Ninth Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL?99),
pages 71?76, Bergen, Norway, June. Association for
Computational Linguistics.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, Den-
ver, Colorado, USA.
J?rg Tiedemann. 2009. News from opus ? a collection
of multilingual parallel corpora with tools and inter-
faces. InRecent Advances in Natural Language Pro-
cessing (vol. V), pages 237?248. John Benjamins.
Omar F. Zaidan. 2009. Z-mert: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
215
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 496?500,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Hierarchical Phrase-Based MT at the Charles University
for the WMT 2011 Shared Task
Daniel Zeman
Charles University in Prague, Institute of Formal and Applied Linguistics (?FAL)
Univerzita Karlova v Praze, ?stav form?ln? a aplikovan? lingvistiky (?FAL)
Malostransk? n?m?st? 25, Praha, CZ-11800, Czechia
zeman@ufal.mff.cuni.cz
Abstract
We describe our experiments with hier-
archical phrase-based machine translation
for the WMT 2011 Shared Task. We
trained a system for all 8 translation di-
rections between English on one side and
Czech, German, Spanish or French on
the other side, though we focused slightly
more on the English-to-Czech direction.
We provide a detailed description of our
configuration and data so the results are
replicable.
1 Introduction
With so many official languages, Europe is a par-
adise for machine translation research. One of the
largest bodies of electronically available parallel
texts is being nowadays generated by the European
Union and its institutions. At the same time, the
EU also provides motivation and boosts potential
market for machine translation outcomes.
Most of the major European languages belong
to one of the following three branches of the
Indo-European language family: Germanic, Ro-
mance or Slavic. Such relatedness is responsible
for many structural similarities in European lan-
guages, although significant differences still ex-
ist. Within the language portfolio selected for the
WMT shared task, English, French and Spanish
seem to be closer to each other than to the rest.
German, despite being genetically related to En-
glish, differs in many properties. Its word or-
der rules, shifting verbs from one end of the sen-
tence to the other, easily create long-distance de-
pendencies. Long German compound words are
notorious for increasing out-of-vocabulary rate,
which has led many researchers to devising un-
supervised compound-splitting techniques. Also,
uppercase/lowercase distinction is more important
because all German nouns start with an uppercase
letter by the rule.
Czech is a language with rich morphology (both
inflectional and derivational) and relatively free
word order. In fact, the predicate-argument struc-
ture, often encoded by fixed word order in English,
is usually captured by inflection (especially the
system of 7 grammatical cases) in Czech. While
the free word order of Czech is a problem when
translating to English (the text should be parsed
first in order to determine the syntactic functions
and the English word order), generating correct in-
flectional affixes is indeed a challenge for English-
to-Czech systems. Furthermore, the multitude
of possible Czech word forms (at least order of
magnitude higher than in English) makes the data
sparseness problem really severe, hindering both
directions.
There are numerous ways how these issues
could be addressed. For instance, parsing and
syntax-aware reordering of the source-language
sentences can help with the word order differ-
ences (same goal could be achieved by a reorder-
ing model or a synchronous context-free grammar
in a hierarchical system). Factored translation, a
secondary language model of morphological tags
or even a morphological generator are some of the
possible solutions to the poor-to-rich translation is-
sues.
Our goal is to run one system under as simi-
lar conditions as possible to all eight translation
directions, to compare their translation accuracies
and see why some directions are easier than others.
Future work will benefit from knowing what are
the special processing needs for a given language
pair. The current version of the system does not in-
clude really language-specific techniques: we nei-
ther split German compounds, nor do we address
the peculiarities of Czech mentioned above. Still,
comparability of the results is limited, as the qual-
ity and quantity of English-Czech data differs from
that of the other pairs.
496
2 The Translation System
Our translation system belongs to the hierarchi-
cal phrase-based class (Chiang, 2007), i.e. phrase
pairs with nonterminals (rules of a synchronous
context-free grammar) are extracted from sym-
metrized word alignments and subsequently used
by the decoder. We use Joshua, a Java-based open-
source implementation of the hierarchical decoder
(Li et al, 2009), release 1.3.1
Word alignment was computed using the first
three steps of the train-factored-phrase-
model.perl script packed with Moses2 (Koehn et
al., 2007). This includes the usual combination of
word clustering using mkcls3 (Och, 1999), two-
way word alignment using GIZA++4 (Och and
Ney, 2003), and alignment symmetrization using
the grow-diag-final-and heuristic (Koehn et al,
2003).
For language modeling we use the SRILM
toolkit5 (Stolcke, 2002) with modified Kneser-
Ney smoothing (Kneser and Ney, 1995; Chen and
Goodman, 1998).
We use the Z-MERT implementation of mini-
mum error rate training (Zaidan, 2009). The fol-
lowing settings have been used for Joshua and Z-
MERT (for the sake of reproducibility, we keep the
original names of the options; for their detailed ex-
planation please refer to the documentation avail-
able on-line at the Joshua project site). -ipi is the
number of intermediate initial points per Z-MERT
iteration.
? Grammar extraction:
maxPhraseSpan=10 maxPhraseLength=5
maxNonterminals=2 maxNontermi-
nalSpan=2 requireTightSpans=true
edgeXViolates=true sentenceIni-
tialX=true sentenceFinalX=true
ruleSampleSize=300
? Language model order: 6 (hexagram)
? Decoding: span_limit=10 fuzz1=0.1
fuzz2=0.1 max_n_items=30 rela-
tive_threshold=10.0 max_n_rules=50
rule_relative_threshold=10.0
1http://sourceforge.net/projects/joshua/
2http://www.statmt.org/moses/
3http://fjoch.com/mkcls.html
4http://fjoch.com/GIZA++.html
5http://www-speech.sri.com/projects/srilm/
? N-best decoding: use_unique_nbest=true
use_tree_nbest=false
add_combined_cost=true top_n=300
? Z-MERT: -m BLEU 4 closest -maxIt 5
-ipi 20
3 Data and Pre-processing Pipeline
We applied our system to all eight language pairs.
From the data point of view the experiments
were even more constrained than the organizers
of the shared task suggested. We used neither
the French/Spanish-English UN corpora nor the
109 French-English corpus. For 7 translation di-
rections we used the Europarl ver6 and News-
Commentary ver6 corpora6 for training. The target
side of the corporawas our only source ofmonolin-
gual data for training the language model. Table 1
shows the size of the training data.
For the English-Czech direction, we used
CzEng 0.9 (Bojar and ?abokrtsk?, 2009)7 as our
main parallel corpus. Following CzEng authors?
request, we did not use sections 8* and 9* reserved
for evaluation purposes.
In addition, we also used the EMEA corpus8
(Tiedemann, 2009).9
Czech was also the only language where we
used extra monolingual data for the language
model. It was the set provided by the organizers of
WMT 2010 (13,042,040 sentences, 210,507,305
tokens).
We use a slightly modified tokenization rules
compared to CzEng export format. Most notably,
we normalize English abbreviated negation and
auxiliary verbs (?couldn?t? ? ?could not?) and
attempt at normalizing quotation marks to distin-
guish between opening and closing one following
proper typesetting rules.
The rest of our pre-processing pipeline matches
the processing employed in CzEng (Bojar and
?abokrtsk?, 2009).10 We use ?supervised truecas-
ing?, meaning that we cast the case of the lemma
to the form, relying on our morphological analyz-
ers and taggers to identify proper names, all other
6Available for download at http://www.statmt.org/
wmt11/translation-task.html using the link ?Parallel
corpus training data?.
7http://ufal.mff.cuni.cz/czeng/
8http://urd.let.rug.nl/tiedeman/OPUS/EMEA.php
9Unfortunately, the EMEA corpus is badly tokenized on
the Czech side with fractional numbers split into several to-
kens (e.g. ?3, 14?). We attempted to reconstruct the original
detokenized form using a small set of regular expressions.
497
Corpus SentPairs Tokens xx Tokens en
cs-en 583,124 13,224,596 15,397,742
de-en 1,857,087 48,834,569 51,243,594
es-en 1,903,562 54,488,621 52,369,658
fr-en 1,920,363 61,030,918 52,686,784
en-cs 7,543,152 79,057,403 89,018,033
Table 1: Number of sentence pairs and tokens for
every language pair in the parallel training cor-
pus. Languages are identified by their ISO 639
codes: cs = Czech, de =German, en = English, es =
Spanish, fr = French. The en-cs line describes the
CzEng + EMEA combined corpus, all other lines
correspond to the respective versions of EuroParl
+ News Commentary.
words are lowercased.
Note that in some cases the grammar extraction
algorithm in Joshua fails if the training corpus con-
tains sentences that are too long. Removing sen-
tences of 100 or more tokens (per advice by Joshua
developers) effectively healed all failures.11
The News Test 2008 data set12 (2051 sentences
in each language) was used as development data
for MERT. BLEU scores reported in this paper
were computed on the News Test 2011 set (3003
sentences each language). We do not use the News
Test 2009 and 2010.
4 Experiments
All BLEU scores were computed directly by
Joshua on the News Test 2011 set. Note that
they differ from what the official evaluation script
would report, due to different tokenization.
4.1 Baseline Experiments
The set of baseline experiments with all translation
directions involved running the system on lower-
cased News Commentary corpora. Word align-
ments were computed on lowercased 4-character
stems. A hexagram language model was trained
on the target side of the parallel corpus.
In the en-cs case, word alignments were com-
puted on lemmatized version of the parallel cor-
10Due to the subsequent processing, incl. parsing, the tok-
enization of English follows PennTreebenk style. The rather
unfortunate convention of treating hyphenated words as sin-
gle tokens increases our out-of-vocabulary rate.
11Table 1 presents statistics before removing the long sen-
tences.
12http://www.statmt.org/wmt11/translation-
task.html
pus. Hexagram language model was trained on
the monolingual data. Truecased data were used
for training, as described above; the BLEU score
of this experiment in Table 2 is computed on true-
cased system output.
Direction BLEUJ BLEUl BLEUt
en-cs 0.1274 0.141 0.123
en-de 0.1324 0.128 0.052
en-es 0.2756 0.274 0.221
en-fr 0.2727 0.212 0.174
cs-en 0.1782 0.178 0.137
de-en 0.1957 0.187 0.137
es-en 0.2630 0.255 0.197
fr-en 0.2471 0.248 0.193
Table 2: Lowercased BLEU scores of the baseline
experiments on News Test 2011 data: BLEUJ is
computed by the system, BLEUl is the official
evaluation by matrix.statmt.org (it differs be-
cause of different tokenization). BLEUt is offi-
cial truecased evaluation.
An interesting perspective on the models is pro-
vided by the feature weights optimized during
MERT. We can see in Table 3 that translation
models are trusted significantly more than lan-
guage models for the en-de, de-en and es-en di-
rections. In fact, the language model has a low rel-
ative weight in all language pairs but en-cs, which
was the only pair where we used a significant
amount of extra monolingual data. In the future,
we should probably use the Gigaword corpus for
the to-English directions.
Setup LM Pt0 Pt1 Pt2 WP
en-cs 1.0 1.04 0.84 ?0.06 ?1.19
en-de 1.0 2.60 0.57 0.47 ?3.17
en-es 1.0 1.67 0.81 0.60 ?2.96
en-fr 1.0 1.41 0.92 0.53 ?2.80
cs-en 1.0 1.48 0.94 1.08 ?4.55
de-en 1.0 2.28 1.11 0.34 ?2.88
es-en 1.0 2.26 1.67 0.23 ?0.84
fr-en 1.0 1.89 1.32 0.13 ?0.04
Table 3: Feature weights are relative to the weight
of LM , the score by the language model. Then
there are the three translation features: Pt0 =
P (e|f), Pt1 = Plex(f |e) and Pt2 = Plex(e|f).
WP is the word penalty.
498
4.2 Efficiency
The machines on which the experiments were con-
ducted are 64bit Intel Xeon dual core 2.8 GHz
CPUs with 32 GB RAM.
Word alignment of each parallel corpus was the
most resource-consuming subtask. It took between
12 and 48 hours, though it could be cut to one half
by running both GIZA++ directions in parallel.
The time needed for data preprocessing and train-
ing of the language model was negligible. Paral-
lelized grammar extraction took 19 processors for
about an hour. For decoding the test data were split
into 20 chunks that were processed in parallel. One
MERT iteration, including decoding, took from 30
minutes to 1 hour.
Training of large models requires some careful
engineering. The grammar extraction easily con-
sumes over 20 GB memory so it is important to
make sure Java really has access to it. The de-
coder must use the SWIG-linked SRILM library
because Java-based language modeling is too slow
and memory-consuming.
4.3 Supervised Truecasing
Our baseline experiments operated on lowercased
data, except for en-cs, where truecased word forms
were obtained using lemmas from morphological
annotation (note that guessing of the true case is
only needed for the sentence-initial token, other
words can just be left in their original form).
As contrastive runs we applied the supervised
truecasing to other directions as well. We used
the Mor?e tagger for English lemmatization, Tree-
Tagger for German and two simple rule-based ap-
proaches to Spanish and French lemmatization.
All these tools are embedded in the TectoMT anal-
ysis framework (?abokrtsk? et al, 2008).
The results are in Table 4. BLEUt has increased
in all cases w.r.t. the baseline results.
4.4 Alignment on Lemmas
Once we are able to lemmatize all five languages
we can also experiment with word alignments
based on lemmas. Table 5 shows that the differ-
ences in BLEU are insignificant.
5 Conclusion
We have described the hierarchical phrase-based
SMT system we used for the WMT 2011 shared
task. We discussed experiments with large data
Direction BLEUJ BLEUl BLEUt
en-cs 0.1191 0.126 0.119
en-de 0.1337 0.131 0.127
en-es 0.2573 0.276 0.265
en-fr 0.2591 0.211 0.189
cs-en 0.1692 0.180 0.168
de-en 0.1885 0.191 0.178
es-en 0.2446 0.260 0.236
fr-en 0.2243 0.245 0.221
Table 4: Results of experiments with supervised
truecasing. Note that training on truecased corpus
slightly influenced even the lowercased BLEU (cf.
with Table 2). This is because probabilities of to-
kens that may appear both uppercased and lower-
cased (with different meanings) have changed, and
thus different translation may have been chosen.
Direction BLEUJ l4 BLEUJ lm
en-cs 0.1191 0.1193
en-de 0.1337 0.1318
en-es 0.2573 0.2590
en-fr 0.2591 0.2592
cs-en 0.1692 0.1690
de-en 0.1885 0.1892
es-en 0.2446 0.2452
fr-en 0.2243 0.2244
Table 5: Results of experiments with word align-
ment computed on different factors. BLEUJ l4 is
the score computed by Joshua on lowercased test
data for the original experiments (alignment based
on lowercased 4-character prefixes). BLEUJ lm
is the corresponding score for alignment based on
lemmas.
from the point of view of both the translation ac-
curacy and efficiency. We used moderately-sized
training data and took advantage from their ba-
sic linguistic annotation (lemmas). The truecasing
technique helped us to better target named entities.
Acknowledgements
The work on this project was supported by the
grant P406/11/1499 of the Czech Science Founda-
tion (GA?R).
References
Ond?ej Bojar and Zden?k ?abokrtsk?. 2009. Czeng
0.9: Large parallel treebank with rich annotation.
499
The Prague Bulletin of Mathematical Linguistics,
92:63?83.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. In Technical report TR-10-98, Computer
Science Group, Harvard, MA, USA, August. Har-
vard University.
David Chiang. 2007. Hierarchical Phrase-
Based Translation. Computational Linguistics,
33(2):201?228.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
pages 181?184, Los Alamitos, California, USA.
IEEE Computer Society Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Compan-
ion Volume Proceedings of the Demo and Poster Ses-
sions, pages 177?180, Praha, Czechia, June. Associ-
ation for Computational Linguistics.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur,
and Wren Thornton. 2009. Decoding in Joshua:
Open Source, Parsing-Based Machine Translation.
The Prague Bulletin of Mathematical Linguistics,
91:47?56, 1.
Franz Josef Och andHermannNey. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
Ninth Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL?99),
pages 71?76, Bergen, Norway, June. Association for
Computational Linguistics.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, Den-
ver, Colorado, USA.
J?rg Tiedemann. 2009. News from opus ? a collection
of multilingual parallel corpora with tools and inter-
faces. InRecent Advances in Natural Language Pro-
cessing (vol. V), pages 237?248. John Benjamins.
Zden?k ?abokrtsk?, Jan Pt??ek, and Petr Pajas. 2008.
TectoMT: Highly modular MT system with tec-
togrammatics used as transfer layer. In ACL 2008
WMT: Proceedings of the Third Workshop on Statis-
tical Machine Translation, pages 167?170, Colum-
bus, OH, USA. Association for Computational Lin-
guistics.
Omar F. Zaidan. 2009. Z-mert: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
500
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 395?400,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Data Issues of the Multilingual Translation Matrix
Daniel Zeman
Charles University in Prague, Faculty of Mathematics and Physics,
Institute of Formal and Applied Linguistics
Malostransk? n?m?st? 25, CZ-11800 Praha, Czechia
zeman@ufal.mff.cuni.cz
Abstract
We describe our experiments with phrase-
based machine translation for the WMT
2012 Shared Task. We trained one sys-
tem for 14 translation directions between
English or Czech on one side and English,
Czech, German, Spanish or French on the
other side. We describe a set of results
with different training data sizes and sub-
sets.
1 Introduction
With so many official languages, Europe is a
paradise for machine translation research. One
of the largest bodies of electronically available
parallel texts is being nowadays generated by
the European Union and its institutions. At the
same time, the EU also provides motivation and
boosts potential market for machine translation
outcomes.
Most of the major European languages belong
to one of three branches of the Indo-European
language family: Germanic, Romance or Slavic.
Such relatedness is responsible for many struc-
tural similarities in European languages, al-
though significant differences still exist. Within
the language portfolio selected for the WMT
shared task, English, French and Spanish seem
to be closer to each other than to the rest.
German, despite being genetically related to
English, differs in many properties. Its word or-
der rules, shifting verbs from one end of the sen-
tence to the other, easily create long-distance de-
pendencies. Long German compound words are
notorious for increasing out-of-vocabulary rate,
which has led many researchers to devising unsu-
pervised compound-splitting techniques. Also,
uppercase/lowercase distinction is more impor-
tant because all German nouns start with an
uppercase letter by the rule.
Czech is a language with rich morphology
(both inflectional and derivational) and rela-
tively free word order. In fact, the predicate-
argument structure, often encoded by fixed word
order in English, is usually captured by inflec-
tion (especially the system of 7 grammatical
cases) in Czech. While the free word order of
Czech is a problem when translating to English
(the text should be parsed first in order to de-
termine the syntactic functions and the English
word order), generating correct inflectional af-
fixes is indeed a challenge for English-to-Czech
systems. Furthermore, the multitude of possible
Czech word forms (at least order of magnitude
higher than in English) makes the data sparse-
ness problem really severe, hindering both direc-
tions.
Our goal is to run one system under as similar
conditions as possible to all fourteen translation
directions, to compare their translation accura-
cies and see why some directions are easier than
others. Future work will benefit from knowing
what are the special processing needs for a given
language pair. The current version of the system
does not include really language-specific tech-
niques: we neither split German compounds,
nor do we address the peculiarities of Czech
mentioned above.
395
2 The Translation System
Our translation system is built around Moses1
(Koehn et al, 2007). Two-way word align-
ment was computed using GIZA++2 (Och and
Ney, 2003), and alignment symmetrization us-
ing the grow-diag-final-and heuristic (Koehn et
al., 2003). Weights of the system were optimized
using MERT (Och, 2003). No lexical reordering
model was trained.
For language modeling we use the SRILM
toolkit3 (Stolcke, 2002) with modified Kneser-
Ney smoothing (Kneser and Ney, 1995; Chen
and Goodman, 1998).
3 Data and Pre-processing Pipeline
We applied our system to all the eight offi-
cial language pairs. In addition, we also ex-
perimented with translation between Czech on
one side and German, Spanish or French on
the other side. Training data for these addi-
tional language pairs were obtained by combin-
ing parallel corpora of the officially supported
pairs. For instance, to create the Czech-German
parallel corpus, we identified the intersection of
the English sides of Czech-English and English-
German corpora, respectively; then we com-
bined the corresponding Czech and German sen-
tences.
We took part in the constrained task. Un-
less explicitly stated otherwise, the translation
model in our experiments was trained on the
combined News-Commentary v7 and Europarl
v7 corpora.4 Table 1 shows the sizes of the train-
ing data.
The News Test 2010 data set5 (2489 sentences
in each language) was used as development data
for MERT. BLEU scores reported in this paper
were computed on the News Test 2012 set (3003
sentences each language). We do not use the
News Tests 2008, 2009 and 2011.
1http://www.statmt.org/moses/
2http://code.google.com/p/giza-pp/
3http://www-speech.sri.com/projects/srilm/
4http://www.statmt.org/wmt12/
translation-task.html\#download
5http://www.statmt.org/wmt12/
translation-task.html
Corpus SentPairs Tokens lng1 Tokens lng2
cs-en 782,756 17,997,673 20,964,639
de-en 2,079,049 55,143,719 57,741,141
es-en 2,123,036 61,784,972 59,217,471
fr-en 2,144,820 69,568,241 59,939,548
de-cs 652,193 17,422,620 15,383,601
es-cs 692,118 20,189,811 16,324,910
fr-cs 686,300 22,220,780 16,190,365
Table 1: Number of sentence pairs and tokens for
every language pair in the parallel training corpus.
Languages are identified by their ISO 639 codes: cs
= Czech, de = German, en = English, es = Spanish,
fr = French. Every line corresponds to the respective
version of EuroParl + News Commentary.
All parallel and monolingual corpora un-
derwent the same preprocessing. They were
tokenized and some characters normalized or
cleaned. A set of language-dependent heuris-
tics was applied in an attempt to restore and
normalize the directed (opening/closing) quota-
tion marks (i.e. "quoted" ? ?quoted?). The
motivation is twofold here: First, we hope that
paired quotation marks could occasionally work
as brackets and better denote parallel phrases
for Moses; second, if Moses learns to output di-
rected quotation marks, subsequent detokeniza-
tion will be easier.
The data are then tagged and lemmatized.
We used the Mor?e tagger for Czech and En-
glish lemmatization and TreeTagger for Ger-
man, Spanish and French lemmatization. All
these tools are embedded in the Treex analysis
framework (?abokrtsk? et al, 2008).
The lemmas are used later to compute word
alignment. Besides, they are needed to apply
?supervised truecasing? to the data: we cast
the case of the lemma to the form, relying on
our morphological analyzers and taggers to iden-
tify proper names, all other words are lower-
cased. Note that guessing of the true case is
only needed for the sentence-initial token. Other
words can typically be left in their original form,
unless they are uppercased as a form of HIGH-
LIGHTING.
396
3.1 Quotation Marks
A broad range of characters is used to represent
quotation marks in the training data: straight
ASCII quotation mark; Unicode directed quo-
tation marks (U+2018 to U+201F); acute and
grave accents; math symbols such as prime and
double prime (U+2032 to U+2037) etc. Spaces
around quotes in the original untokenized text
ought to provide hints as to the direction of the
quotes (no space between the opening quote and
the next word, and no space between the clos-
ing quote and the previous word) but unfortu-
nately there are numerous cases where superflu-
ous spaces are inserted or required spaces are
missing.
Nested quoting is also possible, such as in
As the Wise Men ? s Report also says , and
I quote : ? It is elementary ? common sense ?
that the Commission should have supported the
Parliament ? s decision - making process . ?
We want all possible quotation marks con-
verted to one pair of characters. We do not mind
the distinction between single and double quotes
but we want to keep (or restore) the distinction
between opening and closing quotes. In addi-
tion, we want to identify the apostrophe acting
as grapheme in some languages, and keep it (or
normalize it, as it could also be mis-typed as
acute accent or something else):
As the Wise Men ? s Report also says , and
I quote : ? It is elementary ? common sense ?
that the Commission should have supported the
Parliament ? s decision - making process . ?
We attempt at solving the problem by a set
of rules that consider mutual positions of quota-
tion marks, spaces and other punctuation, and
also some language-dependent rules (especially
on the lexical apostrophe, e.g. in French d?, l?).
Our rules applied to 1.84 % of Spanish sen-
tences, 2.47 % Czech, 2.77 % German, 4.33 %
English and 16.9 % French (measured on Eu-
roparl data).
Our approach is different from the normaliza-
tion script provided and applied by the organiz-
ers of the shared task, which merely converts all
quotes to the undirected ASCII characters. We
believe that such MT output is incorrect, so we
submitted two versions of each system run: the
primary version is intended for human evalua-
tion and does not apply the ?official? normaliza-
tion of punctuation. In contrast, the secondary
version is normalized, which naturally leads to
higher scores in the automatic evaluation.
4 Experiments
In the following section we describe several dif-
ferent settings and corpora combinations we ex-
perimented with. BLEU scores have been com-
puted by our system, comparing truecased tok-
enized hypothesis with truecased tokenized ref-
erence translation.
Such scores must differ from the official evalu-
ation?see Section 4.4 for discussion of the final
results.
The confidence interval for most of the scores
lies between ?0.5 and ?0.6 BLEU % points.
4.1 Baseline Experiments
The set of baseline experiments were trained on
the supervised truecased combination of News
Commentary and Europarl. As we had lem-
matizers for the languages, word alignment was
computed on lemmas. (But our previous ex-
periments showed that there was little differ-
ence between using lemmas and lowercased 4-
character ?stems?.) A hexagram language model
was trained on the monolingual version of the
News Commentary + Europarl corpus (typically
a slightly larger superset of the target side of the
parallel corpus).
4.2 Larger Monolingual Data
Besides the monolingual halves of the parallel
corpora, additional monolingual data were pro-
vided / permitted:
? The Crawled News corpus from the years
2007 to 2011, various sizes for each language
and year.
? The Gigaword corpora published by the
Linguistic Data Consortium, available only
for English (4th edition), Spanish (3rd) and
French (3rd).
397
Due to bugs in the lemmatizers, we were not
able to process certain parts of the large corpora
in time. Table 2 gives the sizes of the subsets
available for our experiments and Table 3 com-
pares BLEU scores with large language models
against the baseline.
Corpus Segments Tokens
newsc+euro.cs 819,434 18,491,692
newsc+euro.de 2,360,811 58,683,607
newsc+euro.en 2,430,718 65,934,441
newsc+euro.es 2,307,429 66,072,443
newsc+euro.fr 2,361,764 74,083,166
news.all.cs 14,552,899 244,728,011
news.all.de 24,446,319 462,924,303
news.all.en 42,161,804 1,039,806,242
news.all.es 8,627,438 249,022,213
news.all.fr 16,708,622 438,489,352
gigaword.en 70,592,779 2,546,581,646
gigaword.es 31,304,148 1,064,660,498
gigaword.fr 21,674,453 963,571,174
Table 2: Number of segments (paragraphs in Giga-
word, sentences elsewhere) and tokens of additional
monolingual training corpora. ?newsc+euro? are the
monolingual versions of the News Commentary and
Europarl parallel corpora. ?news.all? denotes all
years of the Crawled News corpus for the given lan-
guage.
The Crawled News corpora, in-domain and
larger than the parallel corpora by an order of
magnitude, turned out to help significantly im-
prove the scores of all language pairs. On the
other hand, and to our surprise, we were not
able to achieve any further improvement by us-
ing the Gigaword corpora. Taking into account
the extra requirements on memory when build-
ing such big language models, this makes the
usefulness of Gigaword questionable. We have
no plausible explanation at the moment.
4.3 Larger Parallel Data
Even stranger behavior was observed when
adding the large UN parallel corpus (over 10
million sentence pairs). When used separately
(even for language model) it decreased BLEU
significantly, which could be explained by dif-
ferent domain. When used together with News
Direction Baseline news.all gigaword
en-cs 0.1196 0.1434
en-de 0.1426 0.1629
en-es 0.2778 0.3136 0.3136
en-fr 0.2599 0.2897 0.2874
cs-en 0.1796 0.2031 0.2013
de-en 0.1877 0.2136 0.2144
es-en 0.2219 0.2428 0.2390
fr-en 0.2459 0.2764 0.2756
cs-de 0.1365 0.1550
cs-es 0.1952 0.2211 0.2184
cs-fr 0.1953 0.2167 0.2147
de-cs 0.1212 0.1400
es-cs 0.1281 0.1489
fr-cs 0.1253 0.1442
Table 3: BLEU scores of the baseline experiments
(left column) on News Test 2012 data, computed by
the system on tokenized data, versus similar setup
with large monolingual corpus (news.all, middle col-
umn). Gigaword never brought significant improve-
ment.
Commentary and Europarl, and with a language
model trained on the Crawled News corpus, it
barely outperformed the same setting without
the UN corpus.6 However, the es-en direction is
a notable exception where the UN corpus alone
gave by far the best score. See Table 4 for de-
tails.
We failed to lemmatize the giga French-
English corpus in time, so we do not present
any results with that corpus.
4.4 Final Results
Table 5 compares our BLEU scores with those
computed at matrix.statmt.org.
BLEU (without flag) denotes BLEU score
computed by our system, comparing truecased
tokenized hypothesis with truecased tokenized
reference translation.
The official evaluation by matrix.statmt.
org gives typically lower numbers, reflecting the
loss caused by detokenization and new (differ-
ent) tokenization.
6One of the anonymous reviewers mentioned that the
quality of the UN corpus is relatively low. That could
explain our observations.
398
Direction Parallel Mono BLEU
en-es news-euro-un news.all 0.3194
en-es news-euro news.all 0.3136
en-es un un 0.2694
en-fr news-euro news.all 0.2897
en-fr un un 0.2541
es-en un un 0.2688
es-en news-euro news.all 0.2428
fr-en news-euro news.all 0.2764
fr-en un un 0.2392
Table 4: BLEU scores with different parallel corpora.
4.5 Efficiency
The baseline experiments were conducted
mostly on 64bit AMD Opteron quad-core
2.8 GHz CPUs with 32 GB RAM (decoding
run on 15 machines in parallel) and the whole
pipeline typically required between a half and a
whole day.
However, we used machines with up to 500 GB
RAM to train the large language models and
translation models. Aligning the UN corpora
with Giza++ took around 5 days.
5 Conclusion
We have described the Moses-based SMT system
we used for the WMT 2012 shared task. We
discussed experiments with large data for many
language pairs from the point of view of both
the translation accuracy and efficiency. We were
unable to process all data that was available;
even the experiments where we did use larger
data did not outperform the smaller experiments
significantly. Nevertheless, using the Crawled
News monolingual corpus proved essential.
Acknowledgements
The work on this project was supported by the
grant P406/11/1499 of the Czech Science Foun-
dation (GA?R).
References
Stanley F. Chen and Joshua Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. In Technical report TR-10-98,
Direction BLEU BLEUl BLEUt
en-cs 0.1434 0.144 0.136
en-de 0.1629 0.159 0.154
en-es 0.3136 0.316 0.297
en-fr 0.2897 0.263 0.251
cs-en 0.2031 0.207 0.192
de-en 0.2136 0.214 0.200
es-en 0.2428 0.253 0.240
fr-en 0.2764 0.280 0.266
cs-de 0.1550 0.153 0.147
cs-es 0.2211 0.224 0.207
cs-fr 0.2167 0.197 0.186
de-cs 0.1400 0.141 0.134
es-cs 0.1489 0.150 0.143
fr-cs 0.1442 0.145 0.138
Table 5: BLEU scores with the large language mod-
els. BLEU is computed by the system, BLEUl is the
official lowercased evaluation by matrix.statmt.
org. BLEUt is official truecased evaluation. Al-
though lower official scores are expected, notice the
larger gap in en-fr and cs-fr translation. There seems
to be a problem in our French detokenization proce-
dure.
Computer Science Group, Harvard, MA, USA, Au-
gust. Harvard University.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
pages 181?184, Los Alamitos, California, USA.
IEEE Computer Society Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Lan-
guage Technology, pages 48?54, Morristown, NJ,
USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ond?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180, Praha,
399
Czechia, June. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL
?03: Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics, pages
160?167, Morristown, NJ, USA. Association for
Computational Linguistics.
Andreas Stolcke. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
Denver, Colorado, USA.
Zden?k ?abokrtsk?, Jan Pt??ek, and Petr Pajas.
2008. TectoMT: Highly modular MT system with
tectogrammatics used as transfer layer. In ACL
2008 WMT: Proceedings of the Third Workshop
on Statistical Machine Translation, pages 167?170,
Columbus, OH, USA. Association for Computa-
tional Linguistics.
400
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 85?91,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
CUni Multilingual Matrix in the WMT 2013 Shared Task
Karel B?lek Daniel Zeman
Charles University in Prague, Faculty of Mathematics and Physics,
Institute of Formal and Applied Linguistics
Malostransk? n?m?st? 25, CZ-11800 Praha, Czechia
kb@karelbilek.com, zeman@ufal.mff.cuni.cz
Abstract
We describe our experiments with
phrase-based machine translation for
the WMT 2013 Shared Task. We
trained one system for 18 translation
directions between English or Czech
on one side and English, Czech, Ger-
man, Spanish, French or Russian on
the other side. We describe a set of re-
sults with different training data sizes
and subsets. For the pairs containing
Russian, we describe a set of indepen-
dent experiments with slightly different
translation models.
1 Introduction
With so many official languages, Europe is
a paradise for machine translation research.
One of the largest bodies of electronically
available parallel texts is being nowadays gen-
erated by the European Union and its insti-
tutions. At the same time, the EU also pro-
vides motivation and boosts potential market
for machine translation outcomes.
Most of the major European languages be-
long to one of three branches of the Indo-
European language family: Germanic, Ro-
mance or Slavic. Such relatedness is respon-
sible for many structural similarities in Eu-
ropean languages, although significant differ-
ences still exist. Within the language portfo-
lio selected for the WMT shared task, English,
French and Spanish seem to be closer to each
other than to the rest.
German, despite being genetically related
to English, differs in many properties. Its
word order rules, shifting verbs from one
end of the sentence to the other, easily cre-
ate long-distance dependencies. Long Ger-
man compound words are notorious for in-
creasing out-of-vocabulary rate, which has
led many researchers to devising unsupervised
compound-splitting techniques. Also, upper-
case/lowercase distinction is more important
because all German nouns start with an up-
percase letter by the rule.
Czech is a language with rich morphology
(both inflectional and derivational) and rela-
tively free word order. In fact, the predicate-
argument structure, often encoded by fixed
word order in English, is usually captured by
inflection (especially the system of 7 grammat-
ical cases) in Czech. While the free word order
of Czech is a problem when translating to En-
glish (the text should be parsed first in order
to determine the syntactic functions and the
English word order), generating correct inflec-
tional affixes is indeed a challenge for English-
to-Czech systems. Furthermore, the multitude
of possible Czech word forms (at least order of
magnitude higher than in English) makes the
data sparseness problem really severe, hinder-
ing both directions.
Most of the above characteristics of Czech
also apply to Russian, another Slavic language.
Similar issues have to be expected when trans-
lating between Russian and English. Still,
there are also interesting divergences between
Russian and Czech, especially on the syntactic
level. Russian sentences typically omit cop-
ula in the present tense and there is also no
direct equivalent of the verb ?to have?. Pe-
riphrastic constructions such as ?there is XXX
by him? are used instead. These differences
make the Czech-Russian translation interest-
85
ing as well. Interestingly enough, results of
machine translation between Czech and Rus-
sian has so far been worse than between En-
glish and any of the two languages, language
relatedness notwithstanding.
Our goal is to run one system under as
similar conditions as possible to all eighteen
translation directions, to compare their trans-
lation accuracies and see why some directions
are easier than others. The current version of
the system does not include really language-
specific techniques: we neither split German
compounds, nor do we address the peculiari-
ties of Czech and Russian mentioned above.
In an independent set of experiments, we
tried to deal with the data sparseness of Rus-
sian language with the addition of a backoff
model with a simple stemming and some ad-
ditional data; those experiments were done for
Russian and Czech|English combinations.
2 The Translation System
Both sets of experiments use the same ba-
sic framework. The translation system is
built around Moses1 (Koehn et al, 2007).
Two-way word alignment was computed us-
ing GIZA++2 (Och and Ney, 2003), and
alignment symmetrization using the grow-
diag-final-and heuristic (Koehn et al, 2003).
Weights of the system were optimized using
MERT (Och, 2003). No lexical reordering
model was trained.
For language modeling we use the SRILM
toolkit3 (Stolcke, 2002) with modified Kneser-
Ney smoothing (Kneser and Ney, 1995; Chen
and Goodman, 1998).
3 General experiments
In the first set of experiments we wanted to
use the same setting for all language pairs.
3.1 Data and Pre-processing Pipeline
We applied our system to all the ten official
language pairs. In addition, we also exper-
imented with translation between Czech on
one side and German, Spanish, French or Rus-
sian on the other side. Training data for
these additional language pairs were obtained
1http://www.statmt.org/moses/
2http://code.google.com/p/giza-pp/
3http://www-speech.sri.com/projects/srilm/
by combining parallel corpora of the officially
supported pairs. For instance, to create the
Czech-German parallel corpus, we identified
the intersection of the English sides of Czech-
English and English-German corpora, respec-
tively; then we combined the corresponding
Czech and German sentences.
We took part in the constrained task. Un-
less explicitly stated otherwise, the transla-
tion model in our experiments was trained on
the combined News-Commentary v8 and Eu-
roparl v7 corpora.4 Note that there is only
News Commentary and no Europarl for Rus-
sian. We were also able to evaluate several
combinations with large parallel corpora: the
UN corpus (English, French and Spanish),
the Giga French-English corpus and CzEng
(Czech-English). We did not use any large
corpus for Russian-English. Table 1 shows the
sizes of the training data.
Corpus SentPairs Tkns lng1 Tkns lng2
cs-en 786,929 18,196,080 21,184,881
de-en 2,098,430 55,791,641 58,403,756
es-en 2,140,175 62,444,507 59,811,355
fr-en 2,164,891 70,363,304 60,583,967
ru-en 150,217 3,889,215 4,100,148
de-cs 657,539 18,160,857 17,788,600
es-cs 697,898 19,577,329 18,926,839
fr-cs 693,093 19,717,885 18,849,244
ru-cs 103,931 2,642,772 2,319,611
Czeng
cs-en 14,833,358 204,837,216 235,177,231
UN
es-en 11,196,913 368,154,702 328,840,003
fr-en 12,886,831 449,279,647 372,627,886
Giga
fr-en 22,520,400 854,353,231 694,394,577
Table 1: Number of sentence pairs and tokens
for every language pair in the parallel training
corpus. Languages are identified by their ISO
639 codes: cs = Czech, de = German, en =
English, es = Spanish, fr = French, ru = Rus-
sian. Every line corresponds to the respective
version of EuroParl + News Commentary; the
second part presents the extra corpora.
The News Test 2010 (2489 sentences in
each language) and 2012 (3003 sentences)
data sets5 were used as development data for
MERT. BLEU scores reported in this paper
were computed on the News Test 2013 set
4http://www.statmt.org/wmt13/
translation-task.html\#download
5http://www.statmt.org/wmt13/
translation-task.html
86
(3000 sentences each language). We do not
use the News Tests 2008, 2009 and 2011.
All parallel and monolingual corpora un-
derwent the same preprocessing. They were
tokenized and some characters normalized
or cleaned. A set of language-dependent
heuristics was applied in an attempt to re-
store the opening/closing quotation marks (i.e.
"quoted" ? ?quoted?) (Zeman, 2012).
The data are then tagged and lemmatized.
We used the Featurama tagger for Czech
and English lemmatization and TreeTagger for
German, Spanish, French and Russian lemma-
tization. All these tools are embedded in the
Treex analysis framework (?abokrtsk? et al,
2008).
The lemmas are used later to compute word
alignment. Besides, they are needed to ap-
ply ?supervised truecasing? to the data: we
cast the case of the lemma to the form, rely-
ing on our morphological analyzers and tag-
gers to identify proper names, all other words
are lowercased. Note that guessing of the true
case is only needed for the sentence-initial to-
ken. Other words can typically be left in their
original form, unless they are uppercased as a
form of HIGHLIGHTING.
3.2 Experiments
BLEU scores were computed by our sys-
tem, comparing truecased tokenized hypoth-
esis with truecased tokenized reference trans-
lation. Such scores must differ from the official
evaluation?see Section 3.2.4 for discussion of
the final results.
The confidence interval for most of the
scores lies between ?0.5 and ?0.6 BLEU %
points.
3.2.1 Baseline Experiments
The set of baseline experiments were trained
on the supervised truecased combination of
News Commentary and Europarl. As we had
lemmatizers for the languages, word alignment
was computed on lemmas. (But our previous
experiments showed that there was little dif-
ference between using lemmas and lowercased
4-character ?stems?.) A hexagram language
model was trained on the monolingual version
of the News Commentary + Europarl corpus
(typically a slightly larger superset of the tar-
get side of the parallel corpus).
3.2.2 Larger Monolingual Data
Besides the monolingual halves of the par-
allel corpora, additional monolingual data
were provided / permitted. Our experiments
in previous years clearly showed that the
Crawled News corpus (2007?2012), in-domain
and large, contributed significantly to better
BLEU scores. This year we included it in
our baseline experiments for all language pairs:
translation model on News Commentary +
Europarl, language model on monolingual part
of the two, plus Crawled News.
In addition there are the Gigaword corpora
published by the Linguistic Data Consortium,
available only for English (5th edition), Span-
ish (3rd) and French (3rd). Table 2 gives
the sizes and Table 3 compares BLEU scores
with Gigaword against the baseline. Gigaword
mainly contains texts from news agencies and
as such it should be also in-domain. Neverthe-
less, the crawled news are already so large that
the improvement contributed by Gigaword is
rarely significant.
Corpus Segments Tokens
newsc+euro.cs 830,904 18,862,626
newsc+euro.de 2,380,813 59,350,113
newsc+euro.en 2,466,167 67,033,745
newsc+euro.es 2,330,369 66,928,157
newsc+euro.fr 2,384,293 74,962,162
newsc.ru 183,083 4,340,275
news.all.cs 27,540,827 460,356,173
news.all.de 54,619,789 1,020,852,354
news.all.en 68,341,615 1,673,187,787
news.all.es 13,384,314 388,614,890
news.all.fr 21,195,476 557,431,929
news.all.ru 19,912,911 361,026,791
gigaword.en 117,905,755 4,418,360,239
gigaword.es 31,304,148 1,064,660,498
gigaword.fr 21,674,453 963,571,174
Table 2: Number of segments (paragraphs
in Gigaword, sentences elsewhere) and tokens
of additional monolingual training corpora.
?newsc+euro? are the monolingual versions of
the News Commentary and Europarl parallel
corpora. ?news.all? denotes all years of the
Crawled News corpus for the given language.
87
Direction Baseline Gigaword
en-cs 0.1632
en-de 0.1833
en-es 0.2808 0.2856
en-fr 0.2987 0.2988
en-ru 0.1582
cs-en 0.2328 0.2367
de-en 0.2389 0.2436
es-en 0.2916 0.2975
fr-en 0.2887
ru-en 0.1975 0.2003
cs-de 0.1595
cs-es 0.2170 0.2220
cs-fr 0.2220 0.2196
cs-ru 0.1660
de-cs 0.1488
es-cs 0.1580
fr-cs 0.1420
ru-cs 0.1506
Table 3: BLEU scores of the baseline experi-
ments (left column) on News Test 2013 data,
computed by the system on tokenized data,
versus similar setup with Gigaword. The dif-
ference was typically not significant.
3.2.3 Larger Parallel Data
Various combinations with larger parallel cor-
pora were also tested. We do not have results
for all combinations because these experiments
needed a lot of time and resources and not all
of them finished in time successfully.
In general the UN corpus seems to be of low
quality or too much off-domain. It may help
a little if used in combination with news-euro.
If used separately, it always hurts the results.
The Giga French-English corpus gave the
best results for English-French as expected,
even without the core news-euro data. How-
ever, training the model on data of this size is
extremely demanding on memory and time.
Finally, Czeng undoubtedly improves
Czech-English translation in both directions.
The news-euro dataset is smaller for this
language pair, which makes Czeng stand out
even more. See Table 4 for details.
3.2.4 Final Results
Table 5 compares our BLEU scores with those
computed at matrix.statmt.org.
BLEU (without flag) denotes BLEU score
Dir Parallel Mono BLEU
en-es news-euro +gigaword 0.2856
en-es news-euro-un +gigaword 0.2844
en-es un un+gigaw. 0.2016
en-fr giga +gigaword 0.3106
en-fr giga +newsall 0.3037
en-fr news-euro-un +gigaword 0.3010
en-fr news-euro +gigaword 0.2988
en-fr un un 0.2933
es-en news-euro +gigaword 0.2975
es-en news-euro-un baseline 0.2845
es-en un un+news 0.2067
fr-en news-euro-un +gigaword 0.2914
fr-en news-euro baseline 0.2887
fr-en un un+news 0.2737
Table 4: BLEU scores with different parallel
corpora.
computed by our system, comparing truecased
tokenized hypothesis with truecased tokenized
reference translation.
The official evaluation by matrix.statmt.
org gives typically lower numbers, reflecting
the loss caused by detokenization and new
(different) tokenization.
3.2.5 Efficiency
The baseline experiments were conducted
mostly on 64bit AMD Opteron quad-core
2.8 GHz CPUs with 32 GB RAM (decoding
run on 15 machines in parallel) and the whole
pipeline typically required between a half and
a whole day.
However, we used machines with up to
500 GB RAM to train the large language mod-
els and translation models. Aligning the UN
corpora with Giza++ took around 5 days.
Giga French-English corpus was even worse
and required several weeks to complete. Us-
ing such a large corpus without pruning is not
practical.
4 Extra Experiments with Russian
In a separate set of experiments, we tried to
take a basic Moses framework and change the
setup a little for better results on morpholog-
ically rich languages.
Tried combinations were Russian-Czech and
Russian-English.
88
Direction BLEU BLEUl BLEUt
en-cs 0.1786 0.180 0.170
en-de 0.1833 0.179 0.173
en-es 0.2856 0.288 0.271
en-fr 0.3010 0.270 0.259
en-ru 0.1582 0.142 0.142
cs-en 0.2527 0.259 0.244
de-en 0.2389 0.244 0.230
es-en 0.2856 0.288 0.271
fr-en 0.2887 0.294 0.280
ru-en 0.1975 0.203 0.191
cs-de 0.1595 0.159 0.151
cs-es 0.2220 0.225 0.210
cs-fr 0.2220 0.191 0.181
cs-ru 0.1660 0.150 0.149
de-cs 0.1488 0.151 0.142
es-cs 0.1580 0.160 0.152
fr-cs 0.1420 0.145 0.137
ru-cs 0.1506 0.151 0.144
Table 5: Final BLEU scores. BLEU is true-
cased computed by the system, BLEUl is
the official lowercased evaluation by matrix.
statmt.org. BLEUt is official truecased eval-
uation. Although lower official scores are ex-
pected, notice the larger gap in en-fr and cs-fr
translation. There seems to be a problem in
our French detokenization procedure.
4.1 Data
For the additional Russian-to-Czech systems,
we used following parallel data:
? UMC 0.1 (Klyueva and Bojar, 2008) ? tri-
parallel set, consisting of news articles ?
93,432 sentences
? data mined from movie subtitles (de-
scribed in further detail below) ?
2,324,373 sentences
? Czech-Russian part of InterCorp ? a cor-
pus from translation of fiction books (?er-
m?k and Rosen, 2012) ? 148,847 sentences
For Russian-to-English translation, we used
combination of
? UMC 0.1 ? 95,540 sentences
? subtitles ? 1,790,209 sentences
? Yandex English-Russian parallel corpus 6
? 1,000,000 sentences
? wiki headlines from WMT website 7 ?
514,859 sentences
? common crawl from WMT website ?
878,386 sentences
Added together, Russian-Czech parallel
data consisted of 2,566,615 sentences and
English-Czech parallel data consisted of
4,275,961 sentences 8.
We also used 765 sentences from UMC003
as a devset for MERT training.
We used the following monolingual corpora
to train language models. Russian:
? Russian sides of all the parallel data ?
4,275,961 sentences
? News commentary from WMT website ?
150,217 sentences
? News crawl 2012 ? 9,789,861 sentences
For Czech:
? Czech sides of all the parallel data ?
2,566,615 sentences
? Data downloaded from Czech news arti-
cles9 ? 1,531,403 sentences
? WebColl (Spoustov? et al, 2010) ?
4,053,223 sentences
? PDT 10 ? 115,844 sentences
? Complete Czech Wikipedia ? 3,695,172
sentences
? Sentences scraped from Czech social
server okoun.cz ? 580,249 sentences
For English:
? English sides of all the paralel data ?
4,275,961 sentences
? News commentary from WMT website ?
150,217 sentences
Table 6 and Table 7 shows the sizes of the
training data.
6https://translate.yandex.ru/corpus?lang=en
7http://www.statmt.org/wmt13/
translation-task.html
8some sentences had to be removed for technical
reasons
9http://thepiratebay.sx/torrent/7121533/
10http://ufal.mff.cuni.cz/pdt2.0/
89
Corpus SentPairs Tok lng1 Tok lng2
cs-ru 2,566,615 19,680,239 20,031,688
en-ru 4,275,961 64,619,964 58,671,725
Table 6: Number of sentence pairs and tokens
for every language pair.
Corpus Sentences Tokens
en mono 13,426,211 278,199,832
ru mono 13,701,213 231,076,387
cs mono 12,542,506 202,510,993
Table 7: Number of sentences and tokens for
every language.
4.1.1 Tokenization, tagging
Czech and English data was tokenized and
tagged using Mor?e tagger; Russian was to-
kenized and tagged using TreeTagger. Tree-
Tagger also does lemmatization; however, we
didn?t use lemmas for alignment or translation
models, since our experiments showed that
primitive stemming got better results.
However, what is important to mention is
that TreeTagger had problems with some cor-
pora, mostly Common Crawl. For some rea-
son, Russian TreeTagger has problems with
?dirty? data?sentences in English, French or
random non-unicode noise. It either slows
down significantly or stops working at all. For
this reason, we wrapped TreeTagger in a script
that detected those hangs and replaced the
erroneous Russian sentences with bogus, one-
letter Russian sentences (we can?t delete those,
since the lines already exist in the opposite lan-
guages; but since the pair doesn?t really make
sense in the first place, it doesn?t matter as
much).
All the data are lowercased for all the mod-
els and we recase the letters only at the very
end.
4.1.2 Subtitle data
For an unrelated project dealing with movie
subtitles translation, we obtained data from
OpenSubtitles.org for Czech and English sub-
titles. However, those data were not aligned
on sentence level and were less structured?we
had thousands of .srt files with some sort of
metadata.
When exploiting the data from the subtitles,
we made several observations:
? language used in subtitles is very different
from the language used in news articles
? one of the easiest and most accurate sen-
tence alignments in movie subtitles is the
one based purely on the time stamps
? allowing bigger differences in the time
stamps in the alignment produced more
data, but less accurate
? the subtitles are terribly out of domain (as
experiments with using only the subtitle
data showed us), but adding the corpus
mined from the subtitles still increases
the accuracy of the translation
? allowing bigger differences in the time
stamps and, therefore, more (albeit less
accurate) data always led to better results
in our tests.
In the end, we decided to pair as much sub-
titles as possible, even with the risk of some
being misaligned, because we found out that
this helped the most.
4.2 Translation model, language model
For alignment, we used primitive stemming
that takes just first 6 letters from a word.
We found out that using this ?brute force?
stemming?for reasons that will have to be
explored in a further research?return better
results than regular lemmatization, for both
alignment and translation model, as described
further.
For each language pair, we used a transla-
tion model with two translation tables, one of
them as backoff model. More exactly, the pri-
mary translation is from a form to a combina-
tion of (lower case) form and tag, and the sec-
ondary backoff translation is from a ?stem? de-
scribed above to a combination of (lower case)
form and tag.
We built two language models?one for tags
and one for lower case forms.
The models were actually a mixed model us-
ing interpolate option in SRILM?we trained a
different language model for each corpus, and
then we mixed the language models using a
small development set from UMC003.
90
4.3 Final Results
The final results from matrix.statmt.org are
in the table Table 8. You might notice a sharp
difference between lowercased and truecased
BLEU?that is due to a technical error that
we didn?t notice before the deadline.
Direction BLEUl BLEUt
ru-cs 0.158 0.135
cs-ru 0.165 0.162
ru-en 0.224 0.174
en-ru 0.163 0.160
Table 8: Lowercased and cased BLEU scores
5 Conclusion
We have described two independent Moses-
based SMT systems we used for the WMT
2013 shared task. We discussed experiments
with large data for many language pairs from
the point of view of both the translation accu-
racy and efficiency.
Acknowledgements
The work on this project was supported by
the grant P406/11/1499 of the Czech Science
Foundation (GA?R), and by the grant 639012
of the Grant Agency of Charles University
(GAUK).
References
Franti?ek ?erm?k and Alexandr Rosen. 2012. The
case of InterCorp, a multilingual parallel cor-
pus. International Journal of Corpus Linguis-
tics, 13(3):411?427.
Stanley F. Chen and Joshua Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. In Technical report TR-10-98,
Computer Science Group, Harvard, MA, USA,
August. Harvard University.
Natalia Klyueva and Ond?ej Bojar. 2008. UMC
0.1: Czech-Russian-english multilingual corpus.
In International Conference Corpus Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language mod-
eling. In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, pages 181?184, Los Alamitos, Califor-
nia, USA. IEEE Computer Society Press.
Philipp Koehn, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based transla-
tion. In NAACL ?03: Proceedings of the 2003
Conference of the North American Chapter of
the Association for Computational Linguistics
on Human Language Technology, pages 48?54,
Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ond?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages
177?180, Praha, Czechia, June. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003.
A systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL
?03: Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics,
pages 160?167, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Johanka Spoustov?, Miroslav Spousta, and Pavel
Pecina. 2010. Building a web corpus of czech.
In Proceedings of the Seventh International Con-
ference on Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Andreas Stolcke. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of Inter-
national Conference on Spoken Language Pro-
cessing, Denver, Colorado, USA.
Zden?k ?abokrtsk?, Jan Pt??ek, and Petr Pa-
jas. 2008. TectoMT: Highly modular MT sys-
tem with tectogrammatics used as transfer layer.
In ACL 2008 WMT: Proceedings of the Third
Workshop on Statistical Machine Translation,
pages 167?170, Columbus, OH, USA. Associa-
tion for Computational Linguistics.
Daniel Zeman. 2012. Data issues of the multi-
lingual translation matrix. In Proceedings of the
Seventh Workshop on Statistical Machine Trans-
lation, pages 395?400, Montr?al, Canada. Asso-
ciation for Computational Linguistics.
91
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 221?228,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Machine Translation of Medical Texts in the Khresmoi Project
Ond
?
rej Du
?
sek, Jan Haji
?
c, Jaroslava Hlav
?
a
?
cov
?
a, Michal Nov
?
ak,
Pavel Pecina, Rudolf Rosa, Ale
?
s Tamchyna, Zde
?
nka Ure
?
sov
?
a, Daniel Zeman
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk?e n?am?est?? 25, 11800 Prague, Czech Republic
{odusek,hajic,hlavacova,mnovak,pecina,rosa,tamchyna,uresova,zeman}@ufal.mff.cuni.cz
Abstract
This paper presents the participation of
the Charles University team in the WMT
2014 Medical Translation Task. Our sys-
tems are developed within the Khresmoi
project, a large integrated project aim-
ing to deliver a multi-lingual multi-modal
search and access system for biomedical
information and documents. Being in-
volved in the organization of the Medi-
cal Translation Task, our primary goal is
to set up a baseline for both its subtasks
(summary translation and query transla-
tion) and for all translation directions.
Our systems are based on the phrase-
based Moses system and standard meth-
ods for domain adaptation. The con-
strained/unconstrained systems differ in
the training data only.
1 Introduction
The WMT 2014 Medical Translation Task poses
an interesting challenge for Machine Translation
(MT). In the ?standard? translation task, the end
application is the translation itself. In the Medi-
cal Translation Task, the MT system is considered
a part of a larger system for Cross-Lingual Infor-
mation Retrieval (CLIR) and is used to solve two
different problems: (i) translation of user search
queries, and (ii) translation of summaries of re-
trieved documents.
In query translation, the end user does not even
necessarily see the MT output as their queries are
translated and search is performed on documents
in the target language. In summary translation, the
sentences to be translated come from document
summaries (snippets) displayed to provide infor-
mation on each of the documents retrieved by the
search. Therefore, translation quality may not be
the most important measure in this task ? the per-
formance of the CLIR system as a whole is the
final criterion. Another fundamental difference
from the standard task is the nature of the trans-
lated texts. While we can consider document sum-
maries to be ordinary texts (despite their higher in-
formation density in terms of terminology from a
narrow domain), search queries in the medical do-
main are an extremely specific type of data, and
traditional techniques for system development and
domain adaptation are truly put to a test here.
This work is a part of the of the large integrated
EU-funded Khresmoi project.
1
Among other
goals, such as joint text and image retrieval of ra-
diodiagnostic records, Khresmoi aims to develop
technology for transparent cross-lingual search of
medical sources for both professionals and laypeo-
ple, with the emphasis primarily on publicly avail-
able web sources.
In this paper, we describe the Khresmoi sys-
tems submitted to the WMT 2014 Medical Trans-
lation Task. We participate in both subtasks (sum-
mary translation and query translation) for all
language pairs (Czech?English, German?English,
and French?English) in both directions (to English
and from English). Our systems are based on the
Moses phrase-based translation toolkit and stan-
dard methods for domain adaptation. We submit
one constrained and one unconstrained system for
each subtask and translation direction. The con-
strained and unconstrained systems differ in train-
ing data only: The former use all allowed training
data, the latter take advantage of additional web-
crawled data.
We first summarize previous works in MT do-
main adaptation in Section 2, then describe the
data we used for our systems in Section 3. Sec-
1
http://www.khresmoi.eu/
221
tion 4 contains an account of the submitted sys-
tems and their performance in translation of search
queries and document summaries. Section 5 con-
cludes the paper.
2 Related work
To put our work in the context of other approaches,
we first describe previous work on domain adap-
tation in Statistical Machine Translation (SMT),
then focus specifically on SMT in the medical do-
main.
2.1 Domain adaptation of Statistical machine
translation
Many works on domain adaptation examine the
usage of available in-domain data to directly im-
prove in-domain performance of SMT. Some au-
thors attempt to combine the predictions of two
separate (in-domain and general-domain) transla-
tion models (Langlais, 2002; Sanchis-Trilles and
Casacuberta, 2010; Bisazza et al., 2011; Nakov,
2008) or language models (Koehn and Schroeder,
2007). Wu and Wang (2004) use in-domain data
to improve word alignment in the training phase.
Carpuat et al. (2012) explore the possibility of us-
ing word sense disambiguation to discriminate be-
tween domains.
Other approaches concentrate on the acquisition
of larger in-domain corpora. Some of them ex-
ploit existing general-domain corpora by select-
ing data that resemble the properties of in-domain
data (e.g., using cross-entropy), thus building a
larger pseudo-in-domain training corpus. This
technique is used to adapt language models (Eck
et al., 2004b; Moore and Lewis, 2010) as well as
translation models (Hildebrand et al., 2005; Axel-
rod et al., 2011) or their combination (Mansour et
al., 2011). Similar approaches to domain adapta-
tion are also applied in other tasks, e.g., automatic
speech recognition (Byrne et al., 2004).
2.2 Statistical machine translation in the
medical domain
Eck et al. (2004a) employ an SMT system for the
translation of dialogues between doctors and pa-
tients and show that according to automatic met-
rics, a dictionary extracted from the Unified Medi-
cal Language System (UMLS) Metathesaurus and
its semantic type classification (U.S. National Li-
brary of Medicine, 2009) significantly improves
translation quality from Spanish to English when
applied to generalize the training data.
Wu et al. (2011) analyze the quality of MT on
PubMed
2
titles and whether it is sufficient for pa-
tients. The conclusions are very positive espe-
cially for languages with large training resources
(English, Spanish, German) ? the average fluency
and content scores (based on human evaluation)
are above four on a five-point scale. In automatic
evaluation, their systems substantially outperform
Google Translate. However, the SMT systems are
specifically trained, tuned, and tested on the do-
main of PubMed titles, and it is not evident how
they would perform on other medical texts.
Costa-juss`a et al. (2012) are less optimistic re-
garding SMT quality in the medical domain. They
analyze and evaluate the quality of public web-
based MT systems (such as Google Translate) and
conclude that in both automatic and manual eval-
uation (on 7 language pairs), the performance of
these systems is still not good enough to be used
in daily routines of medical doctors in hospitals.
Jimeno Yepes et al. (2013) propose a method
for obtaining in-domain parallel corpora from ti-
tles and abstracts of publications in the MED-
LINE
3
database. The acquired corpora contain
from 30,000 to 130,000 sentence pairs (depending
on the language pair) and are reported to improve
translation quality when used for SMT training,
compared to a baseline trained on out-of-domain
data. However, the authors use only one source
of in-domain parallel data to adapt the translation
model, and do not use any in-domain monolingual
data to adapt the language model.
In this work, we investigate methods combining
the different kinds of data ? general-domain, in-
domain, and pseudo-in-domain ? to find the opti-
mal approach to this problem.
3 Data description
This section includes an overview of the parallel
and monolingual data sources used to train our
systems. Following the task specification, they
are split into constrained and unconstrained sec-
tions. The constrained section includes medical-
domain data provided for this task (extracted by
the provided scripts), and general-domain texts
provided as constrained data for the standard task
(?general domain? here is used to denote data
2
http://www.ncbi.nlm.nih.gov/pubmed/
3
http://www.nlm.nih.gov/pubs/
factsheets/medline.html
222
Czech?English German?English French?English
dom set pairs source target pairs source target pairs source target
med con 2,498 18,126 19,964 4,998 123,686 130,598 6,139 202,245 171,928
gen con 15,788 226,711 260,505 4,520 112,818 119,404 40,842 1,470,016 1,211,516
gen unc ? ? ? 9,320 525,782 574,373 13,809 961,991 808,222
Table 1: Number of sentence pairs and tokens (source/target) in parallel training data (in thousands).
dom set English Czech German French
med con 172,991 1,848 63,499 63,022
gen con 6,132,107 627,493 1,728,065 1,837,457
med unc 3,275,272 36,348 361,881 908,911
gen unc 618,084 ? 339,595 204,025
Table 2: Number of tokens in monolingual training data (in thousands).
which comes from a mixture of various different
domains, mostly news, parliament proceedings,
web-crawls, etc.). The unconstrained section con-
tains automatically crawled data from medical and
health websites and non-medical data from patent
collections.
3.1 Parallel data
The parallel data summary is presented in Table 1.
The main sources of the medical-domain data
for all the language pairs include the EMEA cor-
pus (Tiedemann, 2009), the UMLS metathesaurus
of health and biomedical vocabularies and stan-
dards (U.S. National Library of Medicine, 2009),
and bilingual titles of Wikipedia articles belonging
to the categories identified to be medical domain.
Additional medical-domain data comes from the
MAREC patent collection: PatTR (W?aschle and
Riezler, 2012) available for DE?EN and FR?EN,
and COPPA (Pouliquen and Mazenc, 2011) for
FR?EN (only patents from the medical categories
A61, C12N, and C12P are allowed in the con-
strained systems).
The constrained general-domain data include
three parallel corpora for all the language pairs:
CommonCrawl (Smith et al., 2013), Europarl ver-
sion 6 (Koehn, 2005), the News Commentary cor-
pus (Callison-Burch et al., 2012). Further, the con-
strained data include CzEng (Bojar et al., 2012)
for CS?EN and the UN corpus for FR?EN.
For our unconstrained experiments, we also em-
ploy parallel data from the non-medical patents
from the PatTR and COPPA collections (other cat-
egories than A61, C12N, and C12P).
3.2 Monolingual data
The monolingual data is summarized in Table 2.
The main sources of the medical-domain mono-
lingual data for all languages involve Wikipedia
pages, UMLS concept descriptions, and non-
parallel texts extracted from the medical patents
of the PatTR collections. For English, the main
source is the AACT collection of texts from Clin-
icalTrials.gov. Smaller resources include: Drug-
Bank (Knox et al., 2011), GENIA (Kim et al.,
2003), FMA (Rosse and Mejino Jr., 2008), GREC
(Thompson et al., 2009), and PIL (Bouayad-Agha
et al., 2000).
In the unconstrained systems, we use additional
monolingual data from web pages crawled within
the Khresmoi project: a collection of about one
million HON-certified
4
webpages in English re-
leased as the test collection for the CLEF 2013
eHealth Task 3 evaluation campaign,
5
additional
web-crawled HON-certified pages (not publicly
available), and other webcrawled medical-domain
related webpages.
The constrained general-domain resources in-
clude: the News corpus for CS, DE, EN, and FR
collected for the purpose of the WMT 2014 Stan-
dard Task, monolingual parts of the Europarl and
News-Commentary corpora, and the Gigaword for
EN and FR.
For the FR?EN and DE?EN unconstrained sys-
tems, the additional general domain monolingual
data is taken from monolingual texts of non-
medical patents in the PatTR collection.
4
https://www.hon.ch/
5
https://sites.google.com/site/
shareclefehealth/
223
medical general
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
u
n
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
Figure 1: Distribution of the domain-specificity
scores in the English?French parallel data sets.
3.3 Data preprocessing
The data consisting of crawled web pages, namely
CLEF, HON, and non-HON, needed to be cleaned
and transformed into a set of sentences. The
Boilerpipe (Kohlsch?utter et al., 2010) and Justext
(Pomik?alek, 2011) tools were used to remove boil-
erplate texts and extract just the main content from
the web pages. The YALI language detection tool
(Majli?s, 2012) trained on both in-domain and gen-
eral domain data then filtered out those cleaned
pages which were not identified as written in one
of the concerned languages.
The rest of the preprocessing procedure was ap-
plied to all the datasets mentioned above, both
parallel and monolingual. The data were tok-
enized and normalized by converting or omit-
ting some (mostly punctuation) characters. A
set of language-dependent heuristics was applied
in an attempt to restore and normalize the open-
ing/closing quotation marks, i.e. convert "quoted"
to ?quoted? (Zeman, 2012). The motivation here
is twofold: First, we hope that paired quota-
tion marks could occasionally work as brackets
and better denote parallel phrases for Moses; sec-
ond, if Moses learns to output directed quotation
marks, the subsequent detokenization will be eas-
ier. For all systems which translate from German,
decompounding is employed to reduce source-side
data sparsity. We used BananaSplit for this task
(M?uller and Gurevych, 2006).
We perform all training and internal evaluation
on lowercased data; we trained recasers to post-
process the final submissions.
medical general
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
u
n
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
Figure 2: Distribution of the domain-specificity
scores in the French monolingual data sets.
4 Submitted systems
We first describe our technique of psedo-in-
domain data selection in Section 4.1, then com-
pare two methods of combining the selected data
in Section 4.2. This, along with using constrained
and unconstrained data sets to train the systems
(see Section 3), amounts to a total of four system
variants submitted for each task. A description of
the system settings used is given in Section 4.3.
4.1 Data selection
We follow an approach originally proposed for
selection of monolingual sentences for language
modeling (Moore and Lewis, 2010) and its modi-
fication applied to selection of parallel sentences
(Axelrod et al., 2011). This technique assumes
two language models for sentence scoring, one
trained on (true) in-domain text and one trained
on (any) general-domain text in the same lan-
guage (e.g., English). For both data domains
(general and medical), we score each sentence
by the difference of its cross-perplexity given the
in-domain language model and cross-perplexity
given the general-domain language model (in this
order). We only keep sentences with a negative
score in our data, assuming that these are the
most ?medical-like?. Visualisation of the domain-
specificity scores (cross-perplexity difference) in
the FR?EN parallel data and FR monolingual data
is illustrated in Figures 1 and 2, respectively.
6
The
scores (Y axis) are presented for each sentence in
increasing order from left to right (X axis).
6
For the medical domain, constrained and unconstrained
parallel data are identical.
224
cs?en de?en en?cs en?de en?fr fr?en
con concat 33.64?1.14 32.84?1.24 18.10?0.94 18.29?0.92 33.39?1.11 36.71?1.17
con interpol 32.94?1.11 32.31?1.20 18.96?0.93 18.41?0.93 34.06?1.11 37.42?1.21
unc concat 34.10?1.11 34.52?1.20 21.12?1.03 19.76?0.92 36.23?1.03 38.15?1.16
unc interpol 34.48?1.16 34.92?1.17 22.15?1.06 20.81?0.95 36.26?1.13 37.91?1.13
Table 3: BLEU scores of summary translations.
cs?en de?en en?cs en?de en?fr fr?en
con concat 30.87?4.70 33.21?5.03 23.25?4.85 17.72?4.75 28.64?3.77 35.56?4.94
con interpol 32.46?5.05 33.74?4.97 21.56?4.80 16.90?4.39 29.34?3.73 35.28?5.26
unc concat 34.88?5.04 31.24?5.59 22.61?4.91 19.13?5.66 33.08?3.80 36.73?4.88
unc interpol 33.82?5.16 34.19?5.27 23.93?5.16 15.87?11.31 31.19?3.73 40.25?5.14
Table 4: BLEU scores of query translations.
The two language models for sentence scoring
are trained with a restricted vocabulary extracted
from the in-domain training data as words occur-
ring at least twice (singletons and other words are
treated as out-of-vocabulary). In our experiments,
we apply this technique to select both monolin-
gual data for language models and parallel data
for translation models. Selection of parallel data
is based on the English side only. The in-domain
models are trained on the monolingual data in the
target language (constrained or unconstrained, de-
pending on the setting). The general-domain mod-
els are trained on the WMT News data.
Compared to the approach of Moore and Lewis
(2010) and Axelrod et al. (2011), we prune the
model vocabulary more aggressively ? we discard
not only the singletons, but also all words with
non-Latin characters, which helps clean the mod-
els from noise introduced by the automatic process
of data acquisition by web crawling.
4.2 Data combination
For both parallel and monolingual data, we obtain
two data sets after applying the data selection:
? ?medical-like? data from the medical domain
? ?medical-like? data from the general domain.
For each language pair and for each system
type (constrained/unconstrained), we submitted
two system variants which differ in how the se-
lected data are combined. The first variant uses
a simple concatenation of the two datasets both
for parallel data and for language model data. In
the second variant, we train separate models for
each section and use linear interpolation to com-
bine them into a single model. For language mod-
els, we use the SRILM linear interpolation feature
(Stolcke, 2002). We interpolate phrase tables us-
ing Tmcombine (Sennrich, 2012). In both cases,
the held-out set for minimizing the perplexity is
the system development set.
4.3 System details
We compute word alignment on lowercase 4-cha-
racter stems using fast align (Dyer et al., 2013).
We create phrase tables using the Moses toolkit
(Koehn et al., 2007) with standard settings. We
train 5-gram language models on the target-side
lowercase forms using SRILM. We use MERT
(Och, 2003) to tune model weights in our systems
on the development data provided for the task.
The only difference between the system variants
for query and summary translation is the tuning
set. In both cases, we use the respective sets pro-
vided offcially for the shared task.
4.4 Results
Tables 3 and 4 show case-insensitive BLEU scores
of our systems.
7
As expected, the unconstrained
systems outperform the constrained ones. Linear
interpolation outperforms data concatenation quite
reliably across language pairs for summary trans-
lation. While the picture for query translation is
similar, there is more variance in the results, so
we cannot state that interpolation definitely works
7
As we use the same recasers for both summary and query
translation, our systems are heavily penalized for wrong let-
ter case in query translation. However, letter case is not taken
into account in most CLIR systems. All BLEU scores re-
ported in this paper will be case-insensitive for this reason.
225
better in this case. This is due to the sizes of the
development and test sets and most importantly
due to sentence lengths ? queries are very short,
making BLEU unreliable, MERT unstable, and
bootstrap resampling intervals wide.
If we compare our score to the other competi-
tors, we are clearly worse than the best systems for
summary translation. From this perspective, our
data filtering seems overly eager (i.e., discarding
all sentence pairs with a positive perplexity differ-
ence). An experiment which we leave for future
work is doing one more round of interpolation to
combine a model trained on the data with negative
perplexity with models trained on the remainder.
5 Conclusions
We described the Charles University MT system
used in the Shared Medical Translation Task of
WMT 2014. Our primary goal was to set up a
baseline for both the subtasks and all translation
directions. The systems are based on the Moses
toolkit, pseudo-in-domain data selection based on
perplexity difference and two different methods of
in-domain and out-of-domain data combination:
simple data concatenation and linear model inter-
polation.
We report results of constrained and uncon-
strained systems which differ in the training data
only. In most experiments, using additional data
improved the results compared to the constrained
systems and using linear model interpolation out-
performed data concatenation. While our systems
are on par with best results for case-insensitive
BLEU score in query translation, our overly ea-
ger data selection techniques caused lower scores
in summary translation. In future work, we plan
to include a special out-of-domain model in our
setup to compensate for this problem.
Acknowledgments
This work was supported by the EU FP7 project
Khresmoi (contract no. 257528), the Czech Sci-
ence Foundation (grant no. P103/12/G084), and
SVV project number 260 104. This work has
been using language resources developed, stored,
and distributed by the LINDAT/CLARIN project
of the Ministry of Education, Youth and Sports of
the Czech Republic (project LM2010013).
References
A. Axelrod, X. He, and J. Gao. 2011. Domain adap-
tation via pseudo in-domain data selection. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 355?
362, Edinburgh, United Kingdom. ACL.
A. Bisazza, N. Ruiz, and M. Federico. 2011. Fill-
up versus interpolation methods for phrase-based
SMT adaptation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 136?143, San Francisco, CA, USA. Interna-
tional Speech Communication Association.
O. Bojar, Z.
?
Zabokrtsk?y, O. Du?sek, P. Galu?s?c?akov?a,
M. Majli?s, D. Mare?cek, J. Mar?s??k, M. Nov?ak,
M. Popel, and A. Tamchyna. 2012. The joy of
parallelism with CzEng 1.0. In Proceedings of the
Eighth International Conference on Language Re-
sources and Evaluation, pages 3921?3928, Istanbul,
Turkey. European Language Resources Association.
N. Bouayad-Agha, D. R. Scott, and R. Power. 2000.
Integrating content and style in documents: A case
study of patient information leaflets. Information
Design Journal, 9(2?3):161?176.
W. Byrne, D. S. Doermann, M. Franz, S. Gustman,
J. Haji?c, D. W. Oard, et al. 2004. Automatic recog-
nition of spontaneous speech for access to multilin-
gual oral history archives. Speech and Audio Pro-
cessing, IEEE Transactions on, 12(4):420?435.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montr?eal,
Canada. ACL.
M. Carpuat, H. Daum?e III, A. Fraser, C. Quirk,
F. Braune, A. Clifton, et al. 2012. Domain adap-
tation in machine translation: Final report. In
2012 Johns Hopkins Summer Workshop Final Re-
port, pages 61?72. Johns Hopkins University.
M. R. Costa-juss`a, M. Farr?us, and J. Serrano Pons.
2012. Machine translation in medicine. A qual-
ity analysis of statistical machine translation in the
medical domain. In Proceedings of the 1st Virtual
International Conference on Advanced Research in
Scientific Areas, pages 1995?1998,
?
Zilina, Slovakia.
?
Zilinsk?a univerzita.
C. Dyer, V. Chahuneau, and N. A. Smith. 2013. A sim-
ple, fast, and effective reparameterization of IBM
model 2. In Proceedings of NAACL-HLT, pages
644?648.
M. Eck, S. Vogel, and A. Waibel. 2004a. Improv-
ing statistical machine translation in the medical do-
main using the Unified Medical Language System.
In COLING 2004: Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 792?798, Geneva, Switzerland. ACL.
226
M. Eck, S. Vogel, and A. Waibel. 2004b. Language
model adaptation for statistical machine translation
based on information retrieval. In Maria Teresa
Lino, Maria Francisca Xavier, F?atima Ferreira, Rute
Costa, and Raquel Silva, editors, Proceedings of the
International Conference on Language Resources
and Evaluation, pages 327?330, Lisbon, Portugal.
European Language Resources Association.
A. S. Hildebrand, M. Eck, S. Vogel, and A. Waibel.
2005. Adaptation of the translation model for statis-
tical machine translation based on information re-
trieval. In Proceedings of the 10th Annual Con-
ference of the European Association for Machine
Translation, pages 133?142, Budapest, Hungary.
European Association for Machine Translation.
A. Jimeno Yepes,
?
E. Prieur-Gaston, and A. N?ev?eol.
2013. Combining MEDLINE and publisher data to
create parallel corpora for the automatic translation
of biomedical text. BMC Bioinformatics, 14(1):1?
10.
J.-D Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus ? a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(suppl 1):i180?
i182.
C. Knox, V. Law, T. Jewison, P. Liu, Son Ly, A. Frolkis,
A. Pon, K. Banco, C. Mak, V. Neveu, Y. Djoum-
bou, R. Eisner, A. C. Guo, and D. S. Wishart.
2011. DrugBank 3.0: a comprehensive resource for
?Omics? research on drugs. Nucleic acids research,
39(suppl 1):D1035?D1041.
P. Koehn and J. Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 224?227, Prague,
Czech Republic. ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages
177?180, Praha, Czechia, June. ACL.
P. Koehn. 2005. Europarl: a parallel corpus for sta-
tistical machine translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages
79?86, Phuket, Thailand. Asia-Pacific Association
for Machine Translation.
C. Kohlsch?utter, P. Fankhauser, and W. Nejdl. 2010.
Boilerplate detection using shallow text features. In
Proceedings of the Third ACM International Confer-
ence on Web Search and Data Mining, WSDM ?10,
pages 441?450, New York, NY, USA. ACM.
P. Langlais. 2002. Improving a general-purpose statis-
tical translation engine by terminological lexicons.
In COLING-02 on COMPUTERM 2002: second
international workshop on computational terminol-
ogy, volume 14, pages 1?7, Taipei, Taiwan. ACL.
M. Majli?s. 2012. Yet another language identifier. In
Proceedings of the Student Research Workshop at
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, pages
46?54, Avignon, France. ACL.
S. Mansour, J. Wuebker, and H. Ney. 2011. Com-
bining translation and language model scoring for
domain-specific data filtering. In International
Workshop on Spoken Language Translation, pages
222?229, San Francisco, CA, USA. ISCA.
R. C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the ACL 2010 Conference Short Papers, pages 220?
224, Uppsala, Sweden. ACL.
C. M?uller and I. Gurevych. 2006. Exploring the po-
tential of semantic relatedness in information re-
trieval. In LWA 2006 Lernen ? Wissensentdeck-
ung ? Adaptivit?at, 9.-11.10.2006, Hildesheimer In-
formatikberichte, pages 126?131, Hildesheim, Ger-
many. Universit?at Hildesheim.
P. Nakov. 2008. Improving English?Spanish statistical
machine translation: Experiments in domain adapta-
tion, sentence paraphrasing, tokenization, and recas-
ing. In Proceedings of the Third Workshop on Statis-
tical Machine Translation, pages 147?150, Colum-
bus, OH, USA. ACL.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL ?03: Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160?167, Morristown,
NJ, USA. ACL.
J. Pomik?alek. 2011. Removing Boilerplate and Du-
plicate Content from Web Corpora. PhD thesis,
Masaryk University, Faculty of Informatics, Brno.
B. Pouliquen and C. Mazenc. 2011. COPPA, CLIR
and TAPTA: three tools to assist in overcoming the
patent barrier at WIPO. In Proceedings of the Thir-
teenth Machine Translation Summit, pages 24?30,
Xiamen, China. Asia-Pacific Association for Ma-
chine Translation.
C. Rosse and Jos?e L. V. Mejino Jr. 2008. The foun-
dational model of anatomy ontology. In A. Burger,
D. Davidson, and R. Baldock, editors, Anatomy On-
tologies for Bioinformatics, volume 6 of Computa-
tional Biology, pages 59?117. Springer London.
G. Sanchis-Trilles and F. Casacuberta. 2010. Log-
linear weight optimisation via Bayesian adaptation
in statistical machine translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1077?1085, Bei-
jing, China. ACL.
227
R. Sennrich. 2012. Perplexity minimization for trans-
lation model domain adaptation in statistical ma-
chine translation. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 539?549. ACL.
J. R. Smith, H. Saint-Amand, M. Plamada, P. Koehn,
C. Callison-Burch, and A. Lopez. 2013. Dirt cheap
web-scale parallel text from the common crawl. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1374?1383, Sofia, Bulgaria.
ACL.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, Den-
ver, Colorado, USA.
P. Thompson, S. Iqbal, J. McNaught, and Sophia Ana-
niadou. 2009. Construction of an annotated corpus
to support biomedical information extraction. BMC
bioinformatics, 10(1):349.
J. Tiedemann. 2009. News from OPUS ? a collection
of multilingual parallel corpora with tools and in-
terfaces. In Recent Advances in Natural Language
Processing, volume 5, pages 237?248, Borovets,
Bulgaria. John Benjamins.
U.S. National Library of Medicine. 2009. UMLS
reference manual. Metathesaurus. Bethesda, MD,
USA.
K. W?aschle and S. Riezler. 2012. Analyzing paral-
lelism and domain similarities in the MAREC patent
corpus. In M. Salampasis and B. Larsen, edi-
tors, Multidisciplinary Information Retrieval, vol-
ume 7356 of Lecture Notes in Computer Science,
pages 12?27. Springer Berlin Heidelberg.
H. Wu and H. Wang. 2004. Improving domain-specific
word alignment with a general bilingual corpus. In
Robert E. Frederking and Kathryn B. Taylor, editors,
Machine Translation: From Real Users to Research,
volume 3265 of Lecture Notes in Computer Science,
pages 262?271. Springer Berlin Heidelberg.
C. Wu, F. Xia, L. Deleger, and I. Solti. 2011. Statistical
machine translation for biomedical text: are we there
yet? AMIA Annual Symposium proceedings, pages
1290?1299.
D. Zeman. 2012. Data issues of the multilingual trans-
lation matrix. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 395?
400, Montr?eal, Canada. ACL.
228
