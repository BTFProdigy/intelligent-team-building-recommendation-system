Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1379?1388, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Part-of-Speech Tagging for Chinese-English Mixed Texts
with Dynamic Features
Jiayi Zhao? Xipeng Qiu? Shu Zhang? Feng Ji? Xuanjing Huang?
School of Computer Science, Fudan University, Shanghai, China ? ?
Fujitsu Research and Development Center, Beijing, China?
zjy.fudan@gmail.com?
{xpqiu,fengji,xjhuang}@fudan.edu.cn?
zhangshu@cn.fujitsu.com ?
Abstract
In modern Chinese articles or conversations,
it is very popular to involve a few English
words, especially in emails and Internet liter-
ature. Therefore, it becomes an important and
challenging topic to analyze Chinese-English
mixed texts. The underlying problem is how
to tag part-of-speech (POS) for the English
words involved. Due to the lack of specially
annotated corpus, most of the English words
are tagged as the oversimplified type, ?foreign
words?. In this paper, we present a method
using dynamic features to tag POS of mixed
texts. Experiments show that our method
achieves higher performance than traditional
sequence labeling methods. Meanwhile, our
method also boosts the performance of POS
tagging for pure Chinese texts.
1 Introduction
Nowadays, Chinese-English mixed texts are
prevalent in modern articles or emails. More and
more English words are used in Chinese texts as
names of organizations, products, terms and abbre-
viations, such as ?eBay?, ?iPhone?, ?GDP?, ?An-
droid? etc. On the other hand, it is also a common
phenomenon to use Chinese-English mixed texts
in daily conversation, especially in communication
among employers in large international corporations.
There are some challenges for analyzing Chinese-
English mixed texts:
1. How to define the POS tags for English words
in these mixed texts. Since the standard of
POS tags for English and Chinese are different,
we cannot use English POS to tag the English
words in mixed texts.
2. Due to lack of annotated corpus for mixed texts,
most of the English words are tagged as ?for-
eign words?, which is oversimplified. So we
cannot use them in further processing for the
syntactic and semantic analysis.
3. Most English words used in mixed texts are of-
ten out-of-vocabulary (OOV), which thus in-
creases the difficulties to tag them.
Currently, the mainstream method of Chinese
POS tagging is joint segmentation & tagging with
cross-labels, which can avoid the problem of error
propagation and achieve higher performance on both
subtasks(Ng and Low, 2004). Each label is the cross-
product of a segmentation label and a tagging la-
bel, e.g. {B-NN, I-NN, E-NN, S-NN, ...}. The fea-
tures are generated by position-based templates on
character-level.
Since the main part of mixed texts is in Chinese
and the role of English word is more like Chinese,
we use Chinese POS tags (Xia, 2000) to tag English
words. Since the categories of the most commonly
used English words are nouns, verbs and adjectives,
we can use ?NN?, ?NR?, ?VV?, ?VA?, ?JJ? to label
their POS tags.
For the English proper nouns and verbs, there
are no significant differences in Chinese and En-
glish POS tags except that English features plural
and tense forms.
For the English nouns, these are some English
nouns used as verbs, such as ??? [fan/VV]??(I
adore him very much.)? where ?fan? means ?adore?
and is used as a verb.
For the English adjectives, there are two corre-
sponding Chinese POS tags ?VA? and ?JJ?. For ex-
ample, the roles of some English words in Table 1,
1379
Table 1: The POS tags of English Adjectives in Mixed
Texts
Chinese English
? ? ? [profes-
sional/VA]?
I am very profes-
sional.
??? [high/VA]? Feel very high.
?? [super/JJ] [star/NN] He is a super star.
such as ?professional? and ?high?, are different with
their original ones.
Therefore, the POS tagging for mixed texts cannot
be settled with simple methods, such as looking up
in a dictionary.
One of the main differences between Chinese and
English in POS tagging is that the two languages
have character-based features and word-based fea-
tures respectively. To ensure the consistency of tag-
ging models, we prefer to use word-level informa-
tion in Chinese, which is both useful for Chinese-
English mixed texts and Chinese-only texts. For in-
stance, in a sentence ?X ?? Y ... (X or Y ...)?,
the word Y ought to have the same POS tag as the
word X . Another example is that the word follow-
ing a pronoun is usually a verb, and adjectives of-
ten describe nouns. Some related works show that
word-level features can improve the performance of
Chinese POS tagging (Jiang et al 2008; Sun, 2011).
In this paper, we propose a method to tag mixed
texts with dynamic features. Our method combines
these dynamic features, which are dynamically gen-
erated at the decoding stage, with traditional static
features. For Chinese-English mixed texts, the tra-
ditional features cannot yield a satisfied result due to
lack of training data. The proposed dynamic features
can improve the performance by using the informa-
tion of a word, such as POS tag or length of the whole
word, which is proven effective by experiments.
The rest of the paper is organized as follows: In
section 2, we introduce the sequence labeling mod-
els, thenwe describe our method of dynamic features
in section 3 and analyze its complexity in section 4.
Section 5 describes the training method. The exper-
imental results are manifested in section 6. Finally,
We review the relevant research works in section 7
and conclude our work in section 8.
2 Sequence Labeling Models
Sequence labeling is the task of assigning labels
y = y1, . . . , yn to an input sequence x = x1, . . . , xn.
Given a sample x, we define the feature ?(x, y).
Thus, we can label x with a score function,
y? = argmax
y
F (w,?(x, y)), (1)
where w is the parameter of function F (?).
For sequence labeling, the feature can be denoted
as?k(yi, yi?1, x, i), where i stands for the position in
the sequence and k stands for the number of feature
templates.
we use online Passive-Aggressive (PA) algorithm
(Crammer and Singer, 2003; Crammer et al 2006)
to train the model parameters. Following (Collins,
2002), the average strategy is used to avoid the over-
fitting problem.
3 Dynamic Features
The form of traditional features is shown in Table
2, where C represents a Chinese character, and T
represents the character-based tag. The subscript i
indicates its position related to the current character.
Table 2: Traditional Feature Templates
Ci, T0(i = ?2,?1, 0, 1, 2)
Ci, Cj , T0(i, j = ?2,?1, 0, 1, 2 and i ?= j)
T?1, T0
Traditional features are generated by position-
fixed templates. Since the length of Chinese word
is unfixed, their meanings are incomplete. We cat-
egorize them as ?static? features since they can be
calculated before tagging (except ?T?1, T0?).
The form of dynamic features is shown in Table
3, where WORD represents a Chinese word, and
POS (LEN ) is the POS tag (length) of the word.
The subscript of dynamic feature template indicates
its position related to the current word.
Table 4 shows an example. If the current posi-
tion is ? Apple?, then {POS?1=CC, POS?2=NR,
WORD?1=???, LEN?2=2}. Since these features
are unavailable before tagging, we call them ?dy-
namic? features.
1380
Table 3: Examples of Dynamic Feature Templates
POSi, POSj , T0(i, j = ?2,?1, 0 and i ?= j)
POSi,WORDj , T0(i, j = ?2,?1, 0)
WORDi, LENj , POSk, T0(i, j, k = ?2,?1, 0)
?
Dynamic features are more flexible because the
number of involved characters is dependent on the
length of previous words. Unlike static features, dy-
namic features do not merely rely on the input se-
quence C1:n, so the weights of dynamic features, in
which POS/LEN are involved, can be trained by
Chinese-only texts and used by mixed texts, which
resolve the problem of the lack of training data.
4 Tagging with Dynamic Features
In the tagging stage, we use the current best result
to approximately calculate the unknown tag infor-
mation. For an input sequence C1:n, the current best
tags from index 0 to i?1 can be calculated by Viterbi
algorithm and they can be used to generate dynamic
features for index i. The specific algorithm is shown
in Algorithm 1.
Here is an example to explain the time com-
plexity of the dynamic features. Normal template
xi?2xi?1yi requires to look for the positions of
i ? 2 and i ? 1 related to the current character
xi, but dynamic template posi?2posi?1yi needs to
know the pos tags of two words. If the length of
wordi?1/wordi?2 is 2, then the positions of i?4, i?
3, i?2, i?1 are needed to generate the dynamic fea-
tures.
For all dynamic features, it is unnecessary to
repetitively calculate the POS/WORD/LEN ar-
ray. Apart from that one time calculation of the ar-
ray, no distinction can be found between the time
complexity of the dynamic features and the tradi-
tional features. For input C1:n, the time complexity
isO(n?[O(op.2)+(Ts.num+Td.num)?O(op.1)+
O(op.4)]), n.b. O(op.1) = O(op.3). Universally
the dynamic features only require the information of
position i ? 2 and i ? 1, so the time complexity of
calculating the POS/WORD/LEN array can be
ignored as compared with the complexity of Viterbi
algorithm and feature extraction. The approximate
algorithm is thus faster than the Brute-Force way by
input : character sequence C1:n
static templates Ts
dynamic templates Td
number of labelsm
trans matrixM
output: resultsMax & Vp
Initialize: weight matrixW (n?m)
viterbi score matrix Vs (n?m)
viterbi path matrix Vp (n?m)
the index of current best labelMax
for i = 1 ? ? ?n do
for ts in Ts do
// create feature string Fs (Op.1)
Fs = createFeature(C1:n, ts);
W [i] += getWeightVector(Fs);
end
// create a list of <posk,wordk,lenk>
// (k = 0,?1,?2 . . .) (Op.2)
dList = getCurrentBestPath(Max, Vp);
for td in Td do
// create dynamic features string Fd
// (Op.3)
Fd = createFeature(C1:n, td, dList);
W [i] += getWeightVector(Fd);
end
// Update Vs[i], Vp[i] (Op.4)
viterbi_OneStep(Vs[i? 1],W [i],M );
Max = argmaxi(Vs[i]) ;
end
Algorithm 1: Tagging Algorithm with Dynamic
Features
using word-level information.
5 Training
Given an example (x, y), y? are denoted as the in-
correct labels with the highest score
y? = argmax
z ?=y
wT?(x, z). (2)
The margin ?(w; (x, y)) is defined as
?(w; (x, y)) = wT?(x, y)? wT?(x, y?). (3)
Thus, we calculate the hinge loss ?(w; (x, y), (ab-
breviated as ?w) by
1381
Table 4: Example for Chinese-English Mixed POS Tagging
? ? ? Apple ? OS ? ? ? ? ?
B-NR E-NR S-CC S-NR S-DEG S-NN B-NN E-NN B-VA E-VA S-PU
?w =
{
0, ?(w; (x, y)) > 1
1? ?(w; (x, y)), otherwise (4)
In round k, the new weight vector wk+1 is calcu-
lated by
wk+1 = argminw
1
2
||w? wk||2 + C ? ?,
s.t. ?(w; (xk, yk)) <= ? and ? >= 0 (5)
where ? is a non-negative slack variable, and C is
a positive parameter which controls the influence of
the slack term on the objective function.
Following the derivation in PA (Crammer et al
2006), we can get the update rule,
wk+1 = wk + ?k(?(xk, yk)? ?(xk, y?k)), (6)
where
?k = min(C,
?wk
??(xk, yk)? ?(xk, y?k)?2
) (7)
Our algorithm based on PA algorithm is shown in
Algorithm 2.
6 Experiments
We implement our system based on FudanNLP1.
We employ the commonly used label set {B, I, E,
S} for the segmentation part of cross-labels. {B,
I, E} represent Begin, Inside, End of a multi-node
segmentation respectively, and S represents a Single
node segmentation.
The F1 score is used for evaluation, which is the
harmonic mean of precision P (percentage of pre-
dict phrases that exactlymatch the reference phrases)
and recallR (percentage of reference phrases that re-
turned by system).
The feature templates, which are used to extract
features, are listed in Table 5. We set traditional
method (static features) as the baseline. The detailed
experimental settings and results are reported in the
following subsections.
1Available at http://code.google.com/p/fudannlp/
input : training data sets:
(xi, yi), i = 1, ? ? ? , N , and parameters:
C,K
output: wK
Initialize: wTemp? 0,w? 0;
for k = 0 ? ? ?K ? 1 do
for i = 1 ? ? ?N do
receive an example (xi, yi);
predict: y?i = argmax
y
?wk,?(xi, y)?;
if y?i ?= yi then
update wk+1 with Eq. 6;
end
end
wTemp = wTemp+ wk+1 ;
end
wK = wTemp/K ;
Algorithm 2: Training Algorithm
Table 5: Feature Templates
Static
xi?2yi, xi?1yi, xiyi, xi+1yi, xi+2yi
xi?1xiyi, xi+1xiyi, xi?1xi+1yi,
yi?1yi
Dynamic
posi?2posi?1yi, posi?1posiyi
posi?2wordi?1yi, posi?1wordiyi
posi?1wordi?1yi, posiwordiyi
wordi?2wordi?1yi, wordi?1wordiyi
wordileniyi
6.1 POS Tagging for Chinese-only Texts
Before the experiments onChinese-Englishmixed
texts, we evaluate the performance of our method on
Chinese-only texts. We use the CTB dataset from
the POS tagging task of the Fourth International Chi-
nese Language Processing Bakeoff (SIGHAN Bake-
off 2008)(Jin and Chen, 2008). The details are
shown in Table 6.
The performance comparison on joint segmenta-
tion & POS tagging is shown in Table 7. Our method
obtains an error reduction of 6.7% over the baseline.
The reason is that our dynamic features can utilize
1382
Table 6: POS Tagging Dataset in SIGHAN Bakeoff 2008
Train Set Test Set
(number) (number)
Sentence 23444 2079
Word
Total 642246 59955
NN 168896 16793
NR 42906 3970
VV 92887 8641
VA 9106 649
JJ 15640 1581
word-level information effectively and the feature
templates are more flexible.
Table 7: Performances of POS Tagging on Chinese-only
Texts with Static and Dynamic Features
Method P R F1
Baseline 89.68 89.60 89.64
Our 90.35 90.31 90.33
6.2 POS Tagging for Chinese-English Mixed
Texts
Without annotated corpus for Chinese-English
mixed texts, we use synthetic data as the alternative.
In Chinese-English mixed texts, English words of
noun(NN/NR), verb(VV/VA) and adjective(JJ) cat-
egories are the most commonly used, so we ran-
domly transform a certain percentage of Chinese
words with these POS tags in the SIGHAN Bakeoff
2008 dataset(Jin and Chen, 2008) into their English
counterparts.
6.2.1 Synthetic Data
Before trying out an experiment, we first study
how to generate the data of mixed texts.
We use two ways to produce the synthetic data:
?Respective Replacement? and ?Unified Replace-
ment?.
Respective Replacement We replace the selected
Chinese words into their corresponding English
counterparts.
Unified Replacement We replace the selected Chi-
nese words with a unified labelENG. The rea-
son we use the labelENG instead of real words
is that we want to consider the context of these
words but not the words themselves and over-
come the problem of out-of-vocabulary (OOV)
English words.
For our experiments, we just select 5% of the Chi-
nese nouns and verbs from SIGHAN dataset, and re-
place them in the above two ways. After replace-
ment, the training and test data have 12780 and 1254
English words, respectively. 5189 words are gener-
ated by way of ?Respective Replacement?. In the
test data, 326words are OOV, which comprises 25%
of the whole vocabulary. The information of gener-
ated data is shown in Table 8.
Table 8: The Synthetic Chinese-English Mixed Dataset
H
Dataset Numbers of ENGNN VV
H Train Set 8191 4589Test Set 842 412
We use H1 to represent the dataset generated by
way of ?Respective Replacement?, and H2 for the
dataset by way of with ?Unified Replacement?. The
experimental results on these two datasets are shown
in Table 9.
Table 9: Performances of POS Tagging on Dataset H1
and H2
Method Dataset ENG OOV TotalF1 F1oov F1
Baseline H1 73.60 54.91 88.93H2 77.59 73.93 89.11
Our H1 75.60 54.60 89.79H2 79.82 77.61 89.81
From Table 9, we can see that the ?Unified
Replacement? way is better than the ?Respective
Replacement? way for both the baseline and our
method. The main reason is that the ?Unified Re-
placement? way can greatly improve the tagging per-
formance of OOV words.
6.2.2 Detail Comparisons
For detail comparisons of all situations of
mixed texts, we design six synthetic datasets,
A/B/C/D1/D2/E by randomly selecting 10% or
15% of Chinese words (?NN/NR/VV/VA/JJ?) in the
1383
above SIGHANBakeoff 2008 dataset, and replacing
them with English label ENG.
The differences of these datasets are as following:
? Dataset A only contains English words with
tags ?NN/VV?.
? Dataset B contains English words with tags
?NN/VV/VA?.
? Dataset C contains one more tag ?NR? than
Dataset B.
? Datasets D1 and D2 contain one more tag ?JJ?
than Dataset B. The difference between D1
andD2 is thatD2 has about 50%more English
words than D1 in training set.
? Dataset E contains English words with all the
tags ?NN/NR/VV/VA/JJ?.
The detailed information of datasets
A/B/C/D1/D2/E is shown in Table 10.
Table 10: The Synthetic Chinese-English Mixed Dataset
Dataset Numbers of ENGNN NR VV VA JJ
A Train 16302 0 9007 0 0Test 1675 0 841 0 0
B Train 16116 0 8882 906 0Test 1573 0 830 58 0
C Train 16312 4057 9067 899 0Test 1549 400 795 61 0
D1
Train 16042 0 8957 855 1539
Test 1588 0 845 58 150
D2
Train 23705 0 13154 1300 2211
Test 1588 0 845 58 150
E Train 16066 4162 9156 886 1547Test 1647 415 809 57 141
The results are shown in Table 11. On dataset E,
our method achieves 6.78% higher performance on
tagging ENG labels than traditional static features.
This result is reasonable because our model can use
more flexible feature templates to extract features
and reduce the problem of being dependent on spe-
cific English words.
Tables 12/13/14/15/16/17 show the detailed re-
sults on datasets A/B/C/D1/D2/E.
Table 11: Performances of POS Tagging on Datasets
A/B/C/D1/D2/E
Dataset Method ENG labels TotalF1 F1
A Baseline 80.25 88.74Our 83.03 89.72
B Baseline 76.72 88.51Our 80.54 89.55
C Baseline 68.16 88.13Our 70.34 88.99
D1
Baseline 71.30 88.33
Our 74.02 89.15
D2
Baseline 69.59 88.09
Our 74.10 89.15
E Baseline 61.58 87.71Our 68.36 88.83
Experiment on dataset A gets the best result be-
cause ?NN? and ?VV? can be easily distinguished by
its context. Sometimes, ?VA? has the similar context
with ?VV?, experiment on datasetB shows its influ-
ence. The performances on datasetsB/C/E descend
in turn. The reason is that words with tag ?NN? or
?NR/JJ? have the similar usage/contexts in Chinese.
Since we use the same form ENG instead of real
words, there are no differences between these words,
which leads to some errors. Though the datasets is
generated randomly, we can see our method perform
better on every dataset than the baseline.
Table 12: Performances on Dataset A
POS tag Method P R F1
NN Baseline 84.36 86.33 85.33Our 85.37 89.91 87.58
VV Baseline 71.45 68.13 69.75Our 77.53 69.32 73.20
Table 13: Performances on Dataset B
POS tag Method P R F1
NN Baseline 84.89 80.36 82.56Our 83.51 88.87 86.11
VV Baseline 65.90 72.65 69.11Our 75.75 67.35 71.30
VA Baseline 36.84 36.21 36.52Our 51.02 43.10 46.73
1384
Table 14: Performances on Dataset C
POS tag Method P R F1
NN Baseline 73.77 78.24 75.94Our 76.84 77.99 77.41
VV Baseline 61.67 66.79 64.13Our 64.94 67.80 66.34
NR Baseline 55.22 37.00 44.31Our 55.65 50.50 52.95
VA Baseline 63.64 34.43 44.68Our 60.00 39.34 47.52
Table 15: Performances on DatasetD1
POS tag Method P R F1
NN Baseline 77.15 81.42 79.23Our 76.70 88.54 82.20
VV Baseline 67.53 64.50 65.98Our 79.65 59.76 68.29
JJ Baseline 25.00 18.00 20.93Our 22.92 14.67 17.89
VA Baseline 36.00 31.03 33.33Our 28.57 37.93 32.59
Table 16: Performances on DatasetD2
POS tag Method P R F1
NN Baseline 79.11 74.87 76.93Our 79.29 82.68 80.95
VV Baseline 55.77 72.78 65.64Our 69.17 70.89 70.02
JJ Baseline 27.27 12.00 16.67Our 34.38 22.00 26.83
VA Baseline 37.21 27.59 31.68Our 52.17 20.69 29.63
6.3 POS Tagging for Mixed Texts with a Real
Dataset
To investigate the actual performance, we collect
a real dataset from Web, which consists of 142 rep-
resentative Chinese-English mixed sentences. This
dataset contains 4, 238 Chinese characters and 275
English words. Since we focus on the performance
for English words, we only label the POS tags of the
English words. Table 18 shows some examples in
the real dataset of mixed texts.
Table 17: Performances on Dataset E
POS tag Method P R F1
NN Baseline 72.41 68.85 70.59Our 71.18 84.88 77.43
VV Baseline 63.65 59.09 61.28Our 76.19 55.38 64.14
JJ Baseline 28.57 25.53 26.97Our 30.21 20.57 24.47
VA Baseline 44.83 45.61 45.22Our 60.42 50.88 55.24
NR Baseline 38.03 52.05 43.95Our 52.01 46.75 49.24
Table 18: Examples in Real Dataset of Mixed Texts
?? [Ninja Cloud/NR] ????? [Ninja
Blocks/NR] ? ? [Facebook/NR]? [Twit-
ter/NR]?[Dropbox/NR]??????
By using [Ninja Cloud/NR], [Ninja
Blocks/NR] can connect to [Facebook/NR],
[Twitter/NR], [Dropbox/NR].
?? [follow/VV]?????????
You should [follow/VV] this man?s work.
?????????? [COOL/VA]?
... very [COOL/VA]!
The information of the real dataset is shown in Ta-
ble 19. If all involved English words are tagging as
?NN?, the precision is just 56%.
Table 19: The Numbers of English Words with Different
Tags in Dataset R
Dataset NN VV VA NR
R 154 58 28 35
Since there is no noun-modifier ?JJ? in our col-
lected data. We use the models trained on dataset
B and C to tag the real data. The results are shown
in Table 20. The difference between model B and
C is that model B regards all words with tag ?NR?
as ?NN?. Since it is difficult to distinguish between
?NR? and ?NN? merely according to the context,
model B performs better than model C.
The detail results of model B and C are shown in
Table 21 and 22.
1385
Table 20: Performances of POS Tagging on R
Model Method ENGF1
B Baseline 74.91Our 82.55
C Baseline 70.91Our 74.91
Table 21: Performances of Model B on Dataset R
POS tag Method P R F1
NN Baseline 88.62 78.31 83.15Our 91.67 87.30 89.43
VV Baseline 48.31 74.14 58.50Our 60.53 79.31 68.66
VA Baseline 78.95 53.57 63.83Our 84.21 57.14 68.09
Table 22: Performances of Model C on Dataset R
POS tag Method P R F1
NN Baseline 80.25 81.82 81.03Our 84.56 81.82 83.17
VV Baseline 54.88 77.59 64.29Our 61.25 84.48 71.01
VA Baseline 84.62 39.29 53.66Our 88.24 53.57 66.67
NR Baseline 56.52 37.14 44.83Our 55.17 45.71 50.00
7 Related Works
In recent years, POS tagging has undergone great
development. The mainstream method is to regard
POS tagging as sequence labeling problems (Ra-
biner, 1990; Xue, 2003; Peng et al 2004; Ng and
Low, 2004).
However, the analysis of Chinese-English mixed
texts is rarely involved in previous literature. In
the aspect of the general multilingual POS tagging,
most works focus on modeling cross-lingual corre-
lations and tagging multilingual POS on respective
monolingual texts, not on mixed texts (Cucerzan and
Yarowsky, 2002; Yarowsky et al 2001; Naseem et
al., 2009).
Since we choose to use dynamic word-level fea-
tures to improve the performance of POS tagging,
we also review some works on word-level features.
Semi-Markov Conditional Random Fields (semi-
CRF) (Sarawagi and Cohen, 2004) is a model in
which segmentation task is implicitly included into
the decoding algorithm. In this model, feature rep-
resentation would be more flexible than traditional
CRFs, since features can be extracted from the previ-
ous/the next segmentation within a window of vari-
able size. The problem of this approach lies in that
the decoding algorithm depends on the predefined
window size to exploit the boundaries of segmenta-
tions but not the real length of words.
Bunescu (2008) presents an improved pipeline
model in which the output of the previous subtasks
are considered as hidden variables, and the hidden
variables together with their probabilities denoting
the confidence are used as probabilistic features in
the next subtasks. One shortcoming of this method
is inefficiency caused by the calculation of marginal
probabilities of features. The other disadvantages
of the pipeline method are error propagation and the
need of separate training of different subtasks in the
pipeline. Another disadvantage of pipeline method
is error propagation.
Jiang et al(2008) proposes a cascaded linear
model for joint Chinese word segmentation and POS
tagging. With a character-based perceptron as the
core, combinedwith real-valued features such as lan-
guage models, the cascaded model can efficiently
utilize knowledge sources that are inconvenient to
incorporate into the perceptron directly. However,
they use POS tags or word information in a Brute-
Force way, which may suffer from the problem of
time complexity.
Sun (2011) presents a stacked sub-word model for
joint Chinese word segmentation and POS tagging.
By merging the outputs of the three predictors (in-
cluding one word-based segmenter) into sub-word
sequences, rich contextual features can be approx-
imately derived. The experiments are conducted to
show the effectiveness of using word-based informa-
tion.
The difference between the above methods and
ours is that our word-level features are dynamically
generated in the decoding stage without exhaustive
or preprocessed word segmentation.
1386
8 Conclusion
In this paper, we focus on Chinese-English mixed
texts and use dynamic features for POS tagging.
To overcome the problem of the lack of annotated
corpus on mixed texts, our features use both lo-
cal and non-local information and take advantage of
the characteristics of Chinese-English mixed texts.
The experiments demonstrate the effectiveness of
our method. It should be noted that our method is
also effective for the mixed texts of Chinese and any
foreign languages since we use ?Unified Replace-
ment?.
For future works, we plan to improve our approx-
imate tagging algorithm to reduce error propagation.
In addition, we will refer to an English dictionary
to generate some useful features to distinguish be-
tween ?NR? and ?NN? in Chinese-English mixed
texts and add some statistical features derived from
English resources, such as the most common tag of
each English word. We would also like to investi-
gate these features in more applications of natural
language processing, such as name entity recogni-
tion, information extraction, etc.
Acknowledgements
We would like to thank the anonymous reviewers
for their valuable comments. We also thanks Amy
Zhou for her help in spell and grammar checking.
This work was funded by NSFC (No.61003091 and
No.61073069), 863 Program (No.2011AA010604)
and 973 Program (No.2010CB327900).
References
Razvan C. Bunescu. 2008. Learning with probabilistic
features for improved pipeline models. In EMNLP,
pages 670?679. ACL.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing - Volume 10, EMNLP ?02, pages
1?8, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. J. Mach. Learn. Res.,
7:551?585, December.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In proceedings of the 6th conference on
Natural language learning - Volume 20, COLING-
02, pages 1?7, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L?.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Kath-
leen McKeown, Johanna D. Moore, Simone Teufel,
James Allan, and Sadaoki Furui, editors, ACL, pages
897?904. The Association for Computer Linguistics.
C. Jin and X. Chen. 2008. The fourth international chi-
nese language processing bakeoff: Chinese word seg-
mentation, named entity recognition and chinese pos
tagging. In Sixth SIGHAN Workshop on Chinese Lan-
guage Processing, page 69.
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two unsu-
pervised approaches. Journal of Artificial Intelligence
Research, 36(1):341?385.
H.T. Ng and J.K. Low. 2004. Chinese part-of-speech
tagging: One-at-a-time or all-at-once? word-based or
character-based. In Proceedings of EMNLP, volume
2004, page 277.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of the
20th international conference on Computational Lin-
guistics, COLING ?04, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Lawrence R. Rabiner. 1990. Readings in speech recog-
nition. chapter A tutorial on hidden Markov mod-
els and selected applications in speech recognition,
pages 267?296. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In NIPS.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Dekang Lin, Yuji Matsumoto, and Rada
Mihalcea, editors, ACL, pages 1385?1394. The Asso-
ciation for Computer Linguistics.
F. Xia. 2000. The part-of-speech tagging guidelines for
the Penn Chinese Treebank (3.0).
N. Xue. 2003. Chinese word segmentation as character
tagging. Computational Linguistics and Chinese Lan-
guage Processing, 8(1):29?48.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. In-
ducing multilingual text analysis tools via robust pro-
jection across aligned corpora. In Proceedings of
1387
the first international conference on Human language
technology research, pages 1?8. Association for Com-
putational Linguistics.
1388
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 32?39,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Detecting Hedge Cues and their Scopes with Average Perceptron
Feng Ji, Xipeng Qiu, Xuanjing Huang
Fudan University
{fengji,xpqiu,xjhuang}@fudan.edu.cn
Abstract
In this paper, we proposed a hedge de-
tection method with average perceptron,
which was used in the closed challenge
in CoNLL-2010 Shared Task. There are
two subtasks: (1) detecting uncertain sen-
tences and (2) identifying the in-sentence
scopes of hedge cues. We use the unified
learning algorithm for both subtasks since
that the hedge score of sentence can be de-
composed into scores of the words, espe-
cially the hedge words. On the biomedical
corpus, our methods achieved F-measure
with 77.86% in detecting in-domain un-
certain sentences, 77.44% in recognizing
hedge cues, and 19.27% in identifying the
scopes.
1 Introduction
Detecting hedged information in biomedical lit-
eratures has received considerable interest in the
biomedical natural language processing (NLP)
community recently. Hedge information indicates
that authors do not or cannot back up their opin-
ions or statements with facts (Szarvas et al, 2008),
which exists in many natural language texts, such
as webpages or blogs, as well as biomedical liter-
atures.
For many NLP applications, such as question
answering and information extraction, the infor-
mation extracted from hedge sentences would be
harmful to their final performances. Therefore,
the hedge or speculative information should be
detected in advance, and dealt with different ap-
proaches or discarded directly.
In CoNLL-2010 Shared Task (Farkas et al,
2010), there are two different level subtasks: de-
tecting sentences containing uncertainty and iden-
tifying the in-sentence scopes of hedge cues.
For example, in the following sentence:
These results suggest that the IRE motif
in the ALAS mRNA is functional and
imply that translation of the mRNA is
controlled by cellular iron availability
during erythropoiesis.
The words suggest and imply indicate that the
statements are not supported with facts.
In the first subtask, the sentence is considered
as uncertainty.
In the second subtask, suggest and imply are
identified as hedge cues, while the consecutive
blocks suggest that the IRE motif in the ALAS
mRNA is functional and imply that translation of
the mRNA is controlled by cellular iron availabil-
ity during erythropoiesis are recognized as their
corresponding scopes.
In this paper, we proposed a hedge detec-
tion method with average perceptron (Collins,
2002), which was used in the closed challenges in
CoNLL-2010 Shared Task (Farkas et al, 2010).
Our motivation is to use a unified model to de-
tect two level hedge information (word-level and
sentence-level) and the model is easily expanded
to joint learning of two subtasks. Since that the
hedge score of sentence can be decomposed into
scores of the words, especially the hedge words,
we chosen linear classifier in our method and used
average perceptron as the training algorithm.
The rest of the paper is organized as follows. In
Section 2, a brief review of related works is pre-
sented. Then, we describe our method in Section
3. Experiments and results are presented in the
section 4. Finally, the conclusion will be presented
in Section 5.
2 Related works
Although the concept of hedge information has
been introduced in linguistic community for a
long time, researches on automatic hedge detec-
tion emerged from machine learning or compu-
32
tational linguistic perspective in recent years. In
this section, we give a brief review on the related
works.
For speculative sentences detection, Medlock
and Briscoe (2007) report their approach based
on weakly supervised learning. In their method,
a statistical model is initially derived from a seed
corpus, and then iteratively modified by augment-
ing the training dataset with unlabeled samples
according the posterior probability. They only
employ bag-of-words features. On the public
biomedical dataset1, their experiments achieve the
performance of 0.76 in BEP (break even point).
Although they also introduced more linguistic fea-
tures, such as part-of-speech (POS), lemma and
bigram (Medlock, 2008), there are no significant
improvements.
In Ganter and Strube (2009), the same task on
Wikipedia is presented. In their system, score of a
sentence is defined as a normalized tangent value
of the sum of scores over all words in the sentence.
Shallow linguistic features are introduced in their
experiments.
Morante and Daelemans (2009) present their re-
search on identifying hedge cues and their scopes.
Their system consists of several classifiers and
works in two phases, first identifying the hedge
cues in a sentence and secondly finding the full
scope for each hedge cue. In the first phase, they
use IGTREE algorithm to train a classifier with
3 categories. In the second phase, three different
classifiers are trained to find the first token and last
token of in-sentence scope and finally combined
into a meta classifier. The experiments shown
that their system achieves an F1 of nearly 0.85
of identifying hedge cues in the abstracts sub cor-
pus, while nearly 0.79 of finding the scopes with
predicted hedge cues. More experiments could
be found in their paper (Morante and Daelemans,
2009). They also provide a detail statistics on
hedge cues in BioScope corpus2.
3 Hedge detection with average
perceptron
3.1 Detecting uncertain sentences
The first subtask is to identify sentences con-
taining uncertainty information. In particular,
1http://www.benmedlock.co.uk/
hedgeclassif.html
2http://www.inf.u-szeged.hu/rgai/
bioscope
this subtask is a binary classification problem at
sentence-level.
We define the score of sentence as the confi-
dence that the sentence contains uncertainty infor-
mation.
The score can be decomposed as the sum of the
scores of all words in the sentence,
S(x, y) = ?
xi?x
s(xi, y) =
?
xi?x
wT?(xi, y)
where, x denotes a sentence and xi is the i-
th word in the sentence x, ?(xi, y) is a sparse
high-dimensional binary feature vector of word xi.
y ? {uncertain, certain} is the category of the
sentence. For instance, in the example sentence,
if current word is suggest while the category of
this sentence is uncertain, the following feature is
hired,
?n(xi, y) =
{
1, if xi=??suggest??y=??uncertain?? ,
0, otherwise
where n is feature index.
This representation is commonly used in struc-
tured learning algorithms. We can combine the
features into a sparse feature vector ?(x, y) =?
i ?(xi, y).
S(x, y) = wT?(x, y) = ?
xi?x
wT?(xi, y)
In the predicting phase, we assign x to the cate-
gory with the highest score,
y? = argmax
y
wT?(x, y)
We learn the parameters w with online learning
framework. The most common online learner is
the perceptron (Duda et al, 2001). It adjusts pa-
rameters w when a misclassification occurs. Al-
though this framework is very simple, it has been
shown that the algorithm converges in a finite
number of iterations if the data is linearly separa-
ble. Moreover, much less training time is required
in practice than the batch learning methods, such
as support vector machine (SVM) or conditional
maximum entropy (CME).
Here we employ a variant perceptron algorithm
to train the model, which is commonly named
average perceptron since it averages parameters
w across iterations. This algorithm is first pro-
posed in Collins (2002). Many experiments of
33
NLP problems demonstrate better generalization
performance than non averaged parameters. More
theoretical proofs can be found in Collins (2002).
Different from the standard average perceptron al-
gorithm, we slightly modify the average strategy.
The reason to this modification is that the origi-
nal algorithm is slow since parameters accumulate
across all iterations. In order to keep fast training
speed and avoid overfitting at the same time, we
make a slight change of the parameters accumu-
lation strategy, which occurs only after each iter-
ation over the training data finished. Our training
algorithm is shown in Algorithm 1.
input : training data set:
(xn, yn), n = 1, ? ? ? , N ,
parameters: average number: K,
maximum iteration number: T .
output: average weight: cw
Initialize: cw? 0,;
for k = 0 ? ? ?K ? 1 do
w0 ? 0 ;
for t = 0 ? ? ?T ? 1 do
receive an example (xt, yt);
predict: y?t = argmaxy wTt ?(xt, y) ;
if y?t 6= yt then
wt+1 = wt+?(xt, yt)??(xt, y?t)
end
end
cw = cw +wT ;
end
cw = cw/K ;
Algorithm 1: Average Perceptron algorithm
Binary context features are extracted from 6
predefined patterns, which are shown in Figure 1.
By using these patterns, we can easily obtain the
complicate features. As in the previous example,
if the current word is suggest, then a new com-
pound feature could be extracted in the form of
w?1 =results//w0 =suggest by employing the pat-
tern w?1w0. // is the separate symbol.
3.2 Identifying hedge cues and their scopes
Our approach for the second subtask consists of
two phases: (1) identifying hedge cues in a sen-
tence, then (2) recognizing their corresponding
scopes.
3.2.1 Identifying hedge cues
Hedge cues are the most important clues for de-
termining whether a sentence contains uncertain
? unigram: w0,p0
? bigram: w0w1, w0p0, p0p1
? trigram: w?1w0w1
Figure 1: Patterns employed in the sentence-level
hedge detection. Here w denotes single word, p is
part of speech, and the subscript denotes the rela-
tive offset compared with current position.
? unigram: w?2, w?1, w0, w1, w2, p0
? bigram: w?1w0, w0w1, w0p0, p?1p0, p0p1
? trigram: w?1w0w1
Figure 2: Patterns employed in the word-level
hedge detection.
information. Therefore in this phase, we treat the
problem of identifying hedge cues as a classifica-
tion problem. Each word in a sentence would be
predicted a category indicating whether this word
is a hedge cue word or not. In the previous ex-
ample, there are two different hedge cues in the
sentence (show in bold manner). Words suggest
and imply are assigned with the category CUE de-
noting hedge cue word, while other words are as-
signed with label O denoting non hedge cue word.
In our system, this module is much similar to
the module of detecting uncertain sentences. The
only difference is that this phase is word level. So
that each training sample in this phase is a word,
while in detecting speculative sentences training
sample is a sentence. The training algorithm is the
same as the algorithm shown in Algorithm 1. 12
predefined patterns of context features are shown
in Figure 2.
3.2.2 Recognizing in-sentence scopes
After identifying the hedge cues in the first phase,
we need to recognize their corresponding in-
sentence scopes, which means the boundary of
scope should be found within the same sentence.
We consider this problem as a word-cue pair
classification problem, where word is any word
in a sentence and cue is the identified hedge cue
word. Similar to the previous phase, a word-level
linear classifier is trained to predict whether each
34
word-cue pair in a sentence is in the scope of the
hedge cue.
Besides base context features used in the pre-
vious phase, we introduce additional syntactic de-
pendency features. These features are generated
by a first-order projective dependency parser (Mc-
Donald et al, 2005), and listed in Figure 3.
The scopes of hedge cues are always covering
a consecutive block of words including the hedge
cue itself. The ideal method should recognize only
one consecutive block for each hedge cue. How-
ever, our classifier cannot work so well. Therefore,
we apply a simple strategy to process the output
of the classifier. The simple strategy is to find a
maximum consecutive sequence which covers the
hedge cue. If a sentence is considered to contain
several hedge cues, we simply combine the con-
secutive sequences, which have at least one com-
mon word, to a large block and assign it to the
relative hedge cues.
4 Experiments
In this section, we report our experiments on
datasets of CoNLL-2010 shared tasks, including
the official results and our experimental results
when developing the system.
Our system architecture is shown in Figure 4,
which consists of the following modules.
1. corpus preprocess module, which employs a
tokenizer to normalize the corpus;
2. sentence detection module, which uses a bi-
nary sentence-level classifier to determine
whether a sentence contains uncertainty in-
formation;
3. hedge cues detection module, which identi-
fies which words in a sentence are the hedge
cues, we train a binary word-level classifier;
4. cue scope recognition module, which recog-
nizes the corresponding scope for each hedge
cue by another word-level classifier.
Our experimental results are obtained on the
training datasets by 10-fold cross validation. The
maximum iteration number for training the aver-
age perceptron is set to 20. Our system is imple-
mented with Java3.
3http://code.google.com/p/fudannlp/
biomedical Wikipedia
#sentences 14541 11111
#words 382274 247328
#hedge sentences 2620 2484
%hedge sentences 0.18 0.22
#hedge cues 3378 3133
average number 1.29 1.26
average cue length 1.14 2.45
av. scope length 15.42 -
Table 1: Statistical information on annotated cor-
pus.
4.1 Datasets
In CoNLL-2010 Shared Task, two different
datasets are provided to develop the system: (1)
biological abstracts and full articles from the Bio-
Scope corpus, (2) paragraphs from Wikipedia. Be-
sides manually annotated datasets, three corre-
sponding unlabeled datasets are also allowed for
the closed challenges. But we have not employed
any unlabeled datasets in our system.
A preliminary statistics can be found in Ta-
ble 1. We make no distinction between sen-
tences from abstracts or full articles in biomedi-
cal dataset. From Table 1, most sentences are cer-
tainty while about 18% sentences in biomedical
dataset and 22% in Wikipedia dataset are spec-
ulative. On the average, there exists nearly 1.29
hedge cues per sentence in biomedical dataset and
1.26 in Wikipedia. The average length of hedge
cues varies in these two corpus. In biomedical
dataset, hedge cues are nearly one word, but more
than two words in Wikipedia. On average, the
scope of hedge cue covers 15.42 words.
4.2 Corpus preprocess
The sentence are processed with a maximum-
entropy part-of-speech tagger4 (Toutanova et al,
2003), in which a rule-based tokenzier is used to
separate punctuations or other symbols from reg-
ular words. Moreover, we train a first-order pro-
jective dependency parser with MSTParser5 (Mc-
Donald et al, 2005) on the standard WSJ training
corpus, which is converted from constituent trees
to dependency trees by several heuristic rules6.
4http://nlp.stanford.edu/software/
tagger.shtml
5http://www.seas.upenn.edu/?strctlrn/
MSTParser/MSTParser.html
6http://w3.msi.vxu.se/?nivre/research/
Penn2Malt.html
35
? word-cue pair: current word and the hedge cue word pair,
? word-cue POS pair: POS pair of current word and the hedge cue word,
? path of POS: path of POS from current word to the hedge cue word along dependency
tree,
? path of dependency: relation path of dependency from current word to the hedge cue
word along dependency tree,
? POS of hedge cue word+direction: POS of hedge cue word with the direction to the
current word. Here direction can be ?LEFT? if the hedge cue is on the left to the current
word, or ?RIGHT? on the right,
? tree depth: depth of current in the corresponding dependency tree,
? surface distance: surface distance between current word and the hedge cue word. The
value of this feature is always 10 in the case of surface distance greater than 10,
? surface distance+tree depth: combination of surface distance and tree depth
? number of punctuations: number of punctuations between current word and the hedge
cue word,
? number of punctuations + tree depth: combination of number of punctuations and tree
depth
Figure 3: Additional features used in recognizing in-sentence scope
4.3 Uncertain sentences detection
In the first subtask, we carry out the experiments
within domain and cross domains. As previously
mentioned, we do not use the unlabeled datasets
and make no distinction between abstracts and full
articles in biomedical dataset. This means we
train the models only with the official annotated
datasets. The model for cross-domain is trained
on the combination of annotated biomedical and
Wikipedia datasets.
In this subtask, evaluation is carried out on the
sentence level and F-measure of uncertainty sen-
tences is employed as the chief metric.
Table 2 shows the results within domain. Af-
ter 10-fold cross validation over training dataset,
we achieve 84.39% of F1-measure on biomedical
while 56.06% on Wikipedia.
We analyzed the low performance of our sub-
mission result on Wikipedia. The possible rea-
son is our careless work when dealing with the
trained model file. Therefore we retrain a model
for Wikipedia and the performance is listed on the
bottom line (Wikipedia?) in Table 2.
Dataset Precision Recall F1
10-fold cross validation
biomedical 91.03 78.66 84.39
Wikipedia 66.54 48.43 56.06
official evaluation
biomedical 79.45 76.33 77.86
Wikipedia 94.23 6.58 1.23
Wikipedia? 82.19 32.86 46.95
Table 2: Results for in-domain uncertain sentences
detection
Table 3 shows the results across domains. We
split each annotated dataset into 10 folds. Then
training dataset is combined by individually draw-
ing 9 folds out from the split datasets and the
rests are used as the test data. On biomedical
dataset, F1-measure gets to 79.24% while 56.16%
on Wikipedia dataset. Compared with the results
within domain, over 5% performance decreases
from 84.39% to 79.24% on biomedical, but a
slightly increase on Wikipedia.
36
Figure 4: System architecture of our system
Dataset Precision Recall F1
10-fold cross validation
biomedical 87.86 72.16 79.24
Wikipedia 67.78 47.95 56.16
official evaluation
biomedical 62.81 79.11 70.03
Wikipedia 62.66 55.28 58.74
Table 3: Results for across-domain uncertain sen-
tences detection
4.3.1 Results analysis
We investigate the weights of internal features and
found that the words, which have no uncertainty
information, also play the significant roles to pre-
dict the uncertainty of the sentence.
Intuitively, the words without uncertainty infor-
mation should just have negligible effect and the
corresponding features should have low weights.
However, this ideal case is difficult to reached by
learning algorithm due to the sparsity of data.
In Table 4, we list the top 10 words involved
in features with the largest weights for each cate-
gory. These words are ranked by the accumulative
scores of their related features.
In Table 5, we list the top 10 POS involved in
features with the largest weight for each category.
4.4 Hedge cue identification
Hedge cues identification is one module for the
second subtask, we also analyze the performance
on this module.
Since we treat this problem as a binary classi-
fication problem, we evaluate F-measure of hedge
cue words. The results are listed in Table 6.
We have to point out that our evaluation is
Dataset Precision Recall F1
10-fold cross validation(word-level)
biomedical 90.15 84.43 87.19
Wikipedia 57.74 39.81 47.13
official evaluation(phrase-level)
biomedical 78.7 76.22 77.44
Table 6: Results for in-domain hedge cue identifi-
cation
based on word while official evaluation is based
on phrase. That means our results would seem
to be higher than the official results, especially on
Wikipedia dataset because average length of hedge
cues in Wikipedia dataset is more than 2 words.
4.4.1 Result Analysis
We classify the results into four categories: false
negative, false positive, true positive and true neg-
ative. We found that most mistakes are made be-
cause of polysemy and collocation.
In Table 7, we list top 10 words for each cate-
gory. For the false results, the words are difficult to
distinguish without its context in the correspond-
ing sentence.
4.5 Scopes recognition
For recognizing the in-sentence scopes, F-measure
is also used to evaluate the performance of the
word-cue pair classifier. The results using gold
hedge cues are shown in Table 8. From the re-
sults, F-measure achieves respectively 70.44% and
75.94% when individually using the base context
features extracted by 12 predefined patterns (see
Figure 1) and syntactic dependency features (see
Figure 3), while 79.55% when using all features.
The results imply that syntactic dependency
37
biomedical Wikipedia cross domain
uncertain certain uncertain certain uncertain certain
whether show probably the other suggest show
may demonstrate some often whether used to
suggest will many patients probably was
likely can one of another indicate CFS
indicate role believed days appear demonstrate
possible found possibly CFS putative the other
putative human considered are some of all
appear known to such as any other thought ?:?
thought report several western possibly people
potential evidence said to pop likely could not
Table 4: Top 10 significant words in detecting uncertain sentences
biomedical Wikipedia cross domain
uncertain certain uncertain certain uncertain certain
MD SYM RB VBZ JJS SYM
VBG PRP JJS CD RBS ?:?
VB NN RBS ?:? RB JJR
VBZ CD FW WRB EX WDT
IN WDT VBP PRP CC CD
Table 5: Top 5 significant POS in detecting uncertain sentences
Dataset Precision Recall F1
base context features
biomedical 66.04 75.48 70.44
syntactic dependency features
biomedical 93.77 63.05 75.94
all features
biomedical 78.72 80.41 79.55
Table 8: Results for scopes recognizing with gold
hedge cues (word-level)
features contribute more benefits to recognize
scopes than surface context features.
Official results evaluated at block level are also
listed in Table 9.
dataset Precision Recall F1
biomedical 21.87 17.23 19.27
Table 9: Official results for scopes recognizing
(block level)
From Table 9 and the official result on hedge
cue identification in Table 6, we believe that our
post-processing strategy would be responsible for
the low performance on recognizing scopes. Our
strategy is to find a maximum consecutive block
covering the corresponding hedge cue. This strat-
egy cannot do well with the complex scope struc-
ture. For example, a scope is covered by another
scope. Therefore, our system would generate a
block covering all hedge cues if there exists more
than one hedge cues in a sentence.
5 Conclusion
We present our implemented system for CoNLL-
2010 Shared Task in this paper. We introduce
syntactic dependency features when recognizing
hedge scopes and employ average perceptron al-
gorithm to train the models. On the biomedi-
cal corpus, our system achieves F-measure with
77.86% in detecting uncertain sentences, 77.44%
in recognizing hedge cues, and 19.27% in identi-
fying the scopes.
Although some results are low and beyond our
expectations, we believe that our system can be at
least improved within the following fields. Firstly,
we would experiment on other kinds of features,
such as chunk or named entities in biomedical.
Secondly, the unlabeled datasets would be ex-
plored in the future.
38
False Negative False Positive True Positive True Negative
support considered suggesting chemiluminescence
of potential may rhinitis
demonstrate or proposal leukemogenic
a hope might ribosomal
postulate indicates indicating bp
supports expected likely nc82
good can appear intronic/exonic
advocates should possible large
implicated either speculate allele
putative idea whether end
Table 7: Top 10 words with the largest scores for each category in hedge cue identification
Acknowledgments
This work was funded by Chinese NSF (Grant
No. 60673038), 973 Program (Grant No.
2010CB327906, and Shanghai Committee of Sci-
ence and Technology(Grant No. 09511501404).
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing, pages 1?8. Associa-
tion for Computational Linguistics.
Richard O. Duda, Peter E. Hart, and David G. Stork.
2001. Pattern classification. Wiley, New York.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176. Association for Com-
putational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the ACL, pages
91?98. Association for Computational Linguistics.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992?999. Association for Computational Lin-
guistics.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41(4):636?654.
Roser Morante andWalter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the Workshop on BioNLP, pages 28?36.
Association for Computational Linguistics.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, and
Ja?nos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In Proceedings of the Workshop
on Current Trends in Biomedical Natural Language
Processing, pages 38?45. Association for Computa-
tional Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
39
