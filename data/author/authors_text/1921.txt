Adaptive Transformation-based Learning for
Improving Dictionary Tagging
Burcu Karagol-Ayan, David Doermann, and Amy Weinberg
Institute for Advanced Computer Studies (UMIACS)
University of Maryland
College Park, MD 20742
{burcu,doermann,weinberg}@umiacs.umd.edu
Abstract
We present an adaptive technique that en-
ables users to produce a high quality dic-
tionary parsed into its lexicographic com-
ponents (headwords, pronunciations, parts
of speech, translations, etc.) using an
extremely small amount of user provided
training data. We use transformation-
based learning (TBL) as a postprocessor at
two points in our system to improve per-
formance. The results using two dictio-
naries show that the tagging accuracy in-
creases from 83% and 91% to 93% and
94% for individual words or ?tokens?, and
from 64% and 83% to 90% and 93% for
contiguous ?phrases? such as definitions
or examples of usage.
1 Introduction
The availability and use of electronic resources
such as electronic dictionaries has increased tre-
mendously in recent years and their use in
Natural Language Processing (NLP) systems is
widespread. For languages with limited electronic
resources, i.e. low-density languages, however,
we cannot use automated techniques based on par-
allel corpora (Gale and Church, 1991; Melamed,
2000; Resnik, 1999; Utsuro et al, 2002), compa-
rable corpora (Fung and Yee, 1998), or multilin-
gual thesauri (Vossen, 1998). Yet for these low-
density languages, printed bilingual dictionaries
often offer effective mapping from the low-density
language to a high-density language, such as En-
glish.
Dictionaries can have different formats and can
provide a variety of information. However, they
typically have a consistent layout of entries and a
1 Headword 5 Translation
2 POS 6 Example of usage
3 Sense number 7 Example of usage translation
4 Synonym 8 Subcategorization
Figure 1: Sample tagged dictionary entries. Eight
tags are identified and tagged in the given entries.
consistent structure within entries. Publishers of
dictionaries often use a combination of features to
impose this structure including (1) changes in font
style, font-size, etc. that make implicit the lexico-
graphic information1, such as headwords, pronun-
ciations, parts of speech (POS), and translations,
(2) keywords that provide an explicit interpreta-
tion of the lexicographic information, and (3) var-
ious separators that impose an overall structure on
the entry. For example, a boldface font may in-
dicate a headword, italics may indicate an exam-
ple of usage, keywords may designate the POS,
commas may separate different translations, and a
numbering system may identify different senses of
a word.
We developed an entry tagging system that rec-
ognizes, parses, and tags the entries of a printed
dictionary to reproduce the representation elec-
tronically (Karagol-Ayan et al, 2003). The sys-
tem aims to use features as described above and
the consistent layout and structure of the dictio-
1For the purposes of this paper, we will refer to the lexi-
cographic information as tag when necessary.
257
naries to capture and recover the lexicographic in-
formation in the entries. Each token2 or group of
tokens (phrase)3 in an entry associates with a tag
indicating its lexicographic information in the en-
try. Figure 1 shows sample tagged entries in which
eight different types of lexicographic information
are identified and marked. The system gets for-
mat and style information from a document image
analyzer module (Ma and Doermann, 2003) and
is retargeted at many levels with minimal human
assistance.
A major requirement for a human aided dic-
tionary tagging application is the need to mini-
mize human generated training data.4 This re-
quirement limits the effectiveness of data driven
methods for initial training. We chose rule-based
tagging that uses the structure to analyze and tag
tokens as our baseline, because it outperformed
the baseline results of an HMM tagger. The ap-
proach has demonstrated promising results, but we
will show its shortcomings can be improved by ap-
plying a transformation-based learning (TBL) post
processing technique.
TBL (Brill, 1995) is a rule-based machine learn-
ing method with some attractive qualities that
make it suitable for language related tasks. First,
the resulting rules are easily reviewed and under-
stood. Second, it is error-driven, thus directly min-
imizes the error rate (Florian and Ngai, 2001).
Furthermore, TBL can be applied to other annota-
tion systems? output to improve performance. Fi-
nally, it makes use of the features of the token and
those in the neighborhood surrounding it.
In this paper, we describe an adaptive TBL
based technique to improve the performance of the
rule-based entry tagger, especially targeting cer-
tain shortcomings. We first investigate how using
TBL to improve the accurate rendering of tokens?
font style affects the rule-based tagging accuracy.
We then apply TBL on tags of the tokens. In our
experiments with two dictionaries, the range of
font style accuracies is increased from 84%-94%
to 97%-98%, and the range of tagging accuracies
is increased from 83%-90% to 93%-94% for to-
kens, and from 64%-83% to 90%-93% for phrases.
Section 2 discusses the rule-based entry tagging
2Token is a set of glyphs (i.e., a visual representation of a
set of characters) in the OCRed output. Each punctuation is
counted as a token as well.
3In Figure 1, not on time is a phrase consisting of 3 tokens.
4For our experiments we required hand tagging of no
more than eight pages that took around three hours of human
effort.
method. In Section 3, we briefly describe TBL,
and Section 4 recounts how we apply TBL to im-
prove the performance of the rule-based method.
Section 5 explains the experiments and results, and
we conclude with future work.
2 A Rule-based Dictionary Entry Tagger
The rule-based entry tagger (Karagol-Ayan et al,
2003) utilizes the repeating structure of the dic-
tionaries to identify and tag the linguistic role
of tokens or sets of tokens. Rule-based tagging
uses three different types of clues?font style, key-
words and separators?to tag the entries in a sys-
tematic way. The method accommodates noise in-
troduced by the document analyzer by allowing
for a relaxed matching of OCRed output to tags.
For each dictionary, a human operator must spec-
ify the lexicographic information used in that par-
ticular dictionary, along with the clues for each
tag. This process can be performed in a few hours.
The rule-based method alone achieved token accu-
racy between 73%-87% and phrase accuracy be-
tween 75%-89% in experiments conducted using
three different dictionaries5.
The rule-based method has demonstrated prom-
ising results, but has two shortcomings. First, the
method does not consider the relations between
different tags in the entries. While not a prob-
lem for some dictionaries, for others ordering the
relations between tags may be the only informa-
tion that will tag a token correctly. Consider the
dictionary entries in Figure 1. In this dictionary,
the word ?a? represents POS when in italic font,
and part of a translation if in normal font. How-
ever if the font is incorrect (font errors are more
likely to happen with short tokens), the only way
to mark correctly the tag involves checking the
neighboring tokens and tags to determine its rel-
ative position within the entry. When the token
has an incorrect font or OCR errors exist, and
the other clues are ambiguous or inconclusive, the
rule-based method may yield incorrect results.
Second, the rule-based method can produce in-
correct splitting and/or merging of phrases. An er-
roneous merge of two tokens as a phrase may take
place either because of a font error in one of the
tokens or the lack of a separator, such as a punctu-
ation mark. A phrase may split erroneously either
5Using HMMs for entry tagging on the same set of dic-
tionaries produced slightly lower performance, resulting in
token accuracy between 73%-88% and phrase accuracy be-
tween 57%-85%.
258
as a result of a font error or an ambiguous separa-
tor. For instance, a comma may be used after an
example of usage to separate it from its translation
or within it as a normal punctuation mark.
3 TBL
TBL (Brill, 1995), a rule-based machine learning
algorithm, has been applied to various NLP tasks.
TBL starts with an initial state, and it requires a
correctly annotated training corpus, or truth, for
the learning (or training) process. The iterative
learning process acquires an ordered list of rules
or transformations that correct the errors in this
initial state. At each iteration, the transformation
which achieved the largest benefit during appli-
cation is selected. During the learning process,
the templates of allowable transformations limit
the search space for possible transformation rules.
The proposed transformations are formed by in-
stantiation of the transformation templates in the
context of erroneous tags. The learning algorithm
stops when no improvement can be made to the
current state of the training data or when a pre-
specified threshold is reached.
A transformation modifies a tag when its con-
text (such as neighboring tags or tokens) matches
the context described by the transformation. Two
parts comprise a transformation: a rewrite rule?
what to replace? and a triggering environment?
when to replace. A typical rewrite rule is: Change
the annotation from aa to ab, and a typical trig-
gering environment is: The preceding word is wa.
The system?s output is the final state of this data
after applying all transformations in the order they
are produced.
To overcome the lengthy training time associ-
ated with this approach, we used fnTBL, a fast ver-
sion of TBL that preserves the performance of the
algorithm (Ngai and Florian, 2001). Our research
contribution shows this method is effective when
applied to a miniscule set of training data.
4 Application of TBL to Entry Tagging
In this section, we describe how we used TBL in
the context of tagging dictionary entries.
We apply TBL at two points: to render correctly
the font style of the tokens and to label correctly
the tags of the tokens6. Although our ultimate goal
6In reality, TBL improves the accuracy of tags and phrase
boundary flags. In this paper, whenever we say ?application
of TBL to tagging?, we mean tags and phrase boundary flags
 
	
 	Desparately Seeking Cebuano
Douglas W. Oard, David Doermann, Bonnie Dorr, Daqing He, Philip Resnik, and Amy Weinberg
UMIACS, University of Maryland, College Park, MD, 20742
(oard,doermann,bonnie,resnik,weinberg)@umiacs.umd.edu
William Byrne, Sanjeev Khudanpur and David Yarowsky
CLSP, Johns Hopkins University, 3400 North Charles Street, Barton Hall, Baltimore, MD 21218
(byrne,khudanpur,yarowsky)@jhu.edu
Anton Leuski, Philipp Koehn and Kevin Knight
USC Information Sciences Institute, 4676 Admiralty Way, Marina Del Rey, CA 90292
(leuski,koehn,knight)@isi.edu
Abstract
This paper describes an effort to rapidly de-
velop language resources and component tech-
nology to support searching Cebuano news sto-
ries using English queries. Results from the
first 60 hours of the exercise are presented.
1 Introduction
The Los Angeles Times reported that at about 5:20 P.M.
on Tuesday March 4, 2003, a bomb concealed in a back-
pack exploded at the airport in Davao City, the second
largest city in the Philippines. At least 23 people were
reported dead, with more than 140 injured, and Pres-
ident Arroyo of the Philippines characterized the blast
as a terrorist act. With the 13 hour time difference, it
was then 4:20 A.M on the same date in Washington, DC.
Twenty-four hours later, at 4:13 A.M. on March 5, partic-
ipants in the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program were notified
that Cebuano had been chosen as the language of interest
for a ?surprise language? practice exercise that had been
planned quite independently to begin on that date. The
notification observed that Cebuano is spoken by 24% of
the population of the Philippines, and that it is the lingua
franca in the south Philippines, where the event occurred.
One goal of the TIDES program is to develop the abil-
ity to rapidly deploy a broad array of language technolo-
gies for previously unforeseen languages in response to
unexpected events. That capability will be formally ex-
ercised for the first time during June 2003, in a month-
long ?Surprise Language Experiment.? To prepare for
that event, the Linguistic Data Consortium (LDC) orga-
nized a ?dry run? for March 5-14 in order to refine pro-
cedures for rapidly developing language resources of the
type that the TIDES community will need during the July
evaluation.
Development of interactive Cross-Language Informa-
tion Retrieval (CLIR) systems that can be rapidly adapted
to accommodate new languages has been the focus of
extensive collaboration between the University of Mary-
land and The Johns Hopkins University, and more re-
cently with the University of Southern California. The
capability for rapid development of necessary language
resources is an essential part of that process, so we had
been planning to participate in the surprise language dry
run to refine our procedures for sharing those resources
with other members of the TIDES community. Naturally,
we chose CLIR as a driving application to focus our ef-
fort. Our goal, therefore, was to build an interactive sys-
tem that would allow a searcher posing English queries
to find relevant Cebuano news articles from the period
immediately following the bombing.
2 Obtaining Language Resources
Our basic approach to development of an agile system for
interactive CLIR relies on three strategies: (1) create an
infrastructure in advance for English as a query language
that makes only minimal assumptions about the docu-
ment language; (2) leverage the asymmetry inherent in
the problem by assembling strong resources for English
in advance; and (3) develop a robust suite of capabilities
to exploit any language resources that can be found for
the ?surprise language.? We defer the first two topics to
the next section, and focus here on the third. We know of
five possible sources of translation expertise:
People. People who know the language are an excellent
source of insight, and universities are an excellent
place to find such people. We were able to locate
a speaker of Cebuano within 50 feet of one of our
offices, and to schedule an interview with a second
Cebuano speaker within 36 hours of the announce-
ment of the language.
Scholarly literature. Major research universities are
also an excellent place to find written materials de-
scribing a broad array of languages. Within 12 hours
of the announcement, reference librarians at the Uni-
versity of Maryland had identified a textbook on
?Beginning Cebuano,? and we had located a copy
at the University of Southern California. Together
with the excellent electronic resources located by the
LDC, this allowed us to develop a rudimentary stem-
mer within 36 hours.
Translation lexicons. Simple bilingual term lists are
available for many language pairs. Using links pro-
vided by the LDC and our own Web searches, we
were able to construct an English-Cebuano term list
with over 14,000 translation pairs within 12 hours of
the announcement. This largely duplicated a simul-
taneous effort at the LDC, and we later merged our
term list with theirs.
Parallel text. Translation-equivalent documents, when
aligned at the word level, provide an excellent
source of information about not just possible trans-
lations, but their relative predominance. Within 24
hours of the announcement, we had aligned Ce-
buano and English versions of the Holy Bible at
the word level using Giza++. An evaluation by a
native Cebuano speaker of a stratified random sam-
ple of 88 translation pairs showed remarkably high
precision. On a 4-point scale with 1=correct and
4=incorrect the most frequent 100 words averaged
1.3, the next 400 most frequent terms averaged 1.6,
and the 500 next most frequent terms after that aver-
aged 1.7. The Bible?s vocabulary covers only about
half of the words found in typical English news text
(counted by-token), so it is useful to have additional
sources of parallel text. For this reason, we have ex-
tended our previously developed STRAND system
to locate likely translations in the Internet Archive.
Those runs were not yet complete when this paper
was submitted.
Printed Dictionaries. People learning a new language
make extensive use of bilingual dictionaries, so we
have developed a system that mimics that process
to some extent. Within 12 hours of the announce-
ment we had zoned page images from a Cebuano-
English dictionary that was available commercially
in Adobe Page Description Format (PDF) to iden-
tify each dictionary entry, performed optical charac-
ter recognition, and parsed the entries to construct a
bilingual term list. We were aided in this process by
the fact that Cebuano is written in a Roman script.
Again, we achieved good precision, with a sampled
word error rate for OCR of 6.9% and a precision for
a random sample of translation pairs of 87%. Part of
speech tags were also extracted, although they are
not used in our process.
As this description illustrates, these five sources pro-
vide complementary information. Since there is some
uncertainty at the outset about how long it will be before
each delivers useful results, we chose a strategy based
on concurrency, balancing our investment over each the
five sources. This allowed us to use whatever resources
became available first to get an initial system running,
with refinements subsequently being made as additional
resources became available. Because Cebuano and En-
glish are written in the same script, we did not need char-
acter set conversion or phonetic cognate matching in this
case. The CLIR system described in the next section
was therefore constructed using only English resources
that were (or could have been) pre-assembled, plus a
Cebuano-English bilingual term list, a rule-based stem-
mer, and the Cebuano Bible.
3 Building a Cross-Language Retrieval
System
Ideally, we would like to build a system that would find
whatever documents the searcher would wish to read in a
fully automatic mode. In practice, fully automatic search
systems are imperfect even in monolingual applications.
We therefore have developed an interactive approach that
functions something like a typical Web search engine: (1)
the searcher poses their query in English, (2) the sys-
tem ranks the Cebuano documents in decreasing order
of likely relevance to the query, (3) the searcher exam-
ines a list of document titles in something approximat-
ing English, and (4) the searcher may optionally exam-
ine the full text of any document in something approx-
imating English. The intent is to support an iterative
process in which searchers learn to better express their
query through experience. We are only able to provide
very rough translations, so we expect that such a sys-
tem would be used in an environment where searchers
could send documents that appear promising off for pro-
fessional translation when necessary.
At the core of our system is the capability to au-
tomatically rank Cebuano documents based on an En-
glish query. We chose a query translation architecture
using backoff translation and Pirkola?s structured query
method, implemented using Inquery version 3.1p1. The
key idea in backoff translation is to first try to find con-
secutive sequences of query words on the English side
of the bilingual term list, where that fails to try to find
the surface form of each remaining English term, to fall
back to stem matching when necessary, and ultimately to
fall back to retaining the English term unchanged in the
hope that it might be a proper name or some other form
of cognate with Cebuano. Accents are stripped from the
documents and all language resources to facilitate match-
ing at that final step.
Although we have chosen techniques that are relatively
robust and therefore require relatively little domain-
specific tuning, stemmer design is an area of uncertainty
that could adversely affect retrieval effectiveness. We
therefore needed a test collection on which we could try
out variants of the Cebuano stemmer. We built this test
collection using 34,000 Cebuano Bible verses and 50 En-
glish questions that we found on the Web for which ap-
propriate Bible verses were known. Each question was
posed as a query using the batch mode of Inquery, and
the rank of the known relevant verse was taken as a mea-
sure of effectiveness. We took the mean reciprocal rank
(the inverse of the harmonic mean) as a figure of merit
for each configuration, and used a paired two-tailed   -
test (with p  0.05) to assess the statistical significance of
observed differences. Our initial configuration, without
stemming, obtained a mean inverse rank of 0.14, which
is a statistically significant improvement over no transla-
tion at all (mean inverse rank 0.02 from felicitous cognate
and loan word matches). The addition of Cebuano stem-
ming resulted in a reduction in mean inverse rank to 0.09.
Although the reduction is not statistically significant in
that case, the result suggests that our initial stemmer is
not yet useful for information retrieval tasks.
The other key capability that is needed is title and doc-
ument translation. We can accomplish this in one of two
ways. The simplest approach is to reverse the bilingual
term list, and to reverse the role of Cebuano and En-
glish in the process described above for query transla-
tion. Our user interface is capable of displaying multi-
ple translations for a single term (arranged horizontally
for compact depiction or vertically for clearer depiction),
but searchers can choose to display only the single most
likely translation. When reliable translation probability
statistics (from parallel text) are not available, we use the
relative word unigram frequency of each translation of a
Cebuano term in a representative English collection as a
substitute for that probability. A more sophisticated way
is to build a statistical machine translation system using
parallel text. We built our first statistical machine trans-
lation system within 40 hours of the announcement, and
one sentence of the resulting translation using each tech-
nique is shown below:
Cebuano: ?ang rebeldeng milf, kinsa
lakip sa nangamatay, nagdala og
backpack nga dunay explosives nga
niguba sa waiting lounge sa airport,
matod sa mga defense official.?
Term-by-term translation:
?(carelessness, circumference,
conveyence) rebeldeng milf, who lakip
(at in of) nangamatay, nagdala og
backpack nga valid explosives nga
niguba (at, in of) waiting lounge
(at, in, of) airport, matod (at, in,
of) mga defense official?
Statistical translation: ?who was
accused of rank, ehud og niguba
waiting lounge defense of those dumah
milf rebeldeng explosives backpack
airport matod official.?
At this point, term-by-term translation is clearly the bet-
ter choice. But as more parallel text becomes available,
we expect the situation to reverse. The LDC is prepar-
ing a set of human reference translations that will allow
us to detect that changeover point automatically using the
NIST variant of the BLEU measure for machine transla-
tion effectiveness.
4 Conclusion
The results reported in this paper were accomplished by
a team of 20 people with expertise in various facets of te
task that invested about 250 person-hours over two and
a half days. As additional Cebuano-specific evaluation
resources are developed, we expect to gain additional in-
sight into the quality of these early resources. Moreover,
once we see what works best for Cebuano by the end of
the process, we plan to revisit our process design with
an eye towards better optimizing our initial time invest-
ments. We expect to be able to address both of those
points in detail by the time of the conference.
This exercise was originally envisioned as a dry run to
work out the kinks in our process, and indeed we have
already learned a lot on that score. First, we learned that
our basic approach seems sound; we built the key com-
ponents of an interactive CLIR system in about 40 hours,
and by the 60-hour point we had some basis for believing
that each of those components could at least minimally
fulfill their role in a fully integrated system. Some of our
time was, however, spent on things that could have been
done in advance. Perhaps the most important of these
was the development of an information retrieval test col-
lection using the Bible. That job, and numerous smaller
ones, are now done, so we expect that we will be able to
obtain similar results with about half the effort next time
around.
Acknowledgments
Thanks to Clara Cabezas, Tim Hackman, Margie Hi-
nonangan, Burcu Karagol-Ayan, Okan Kolak, Huanfeng
Ma, Grazia Russo-Lassner, Michael Subotin, Jianqiang
Wang and the LDC! This work has been supported in part
by DARPA contract N660010028910.
Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 60?68,
New York City, USA, June 2006. c?2006 Association for Computational Linguistics
Morphology Induction from Limited Noisy Data
Using Approximate String Matching
Burcu Karagol-Ayan, David Doermann, and Amy Weinberg
Institute for Advanced Computer Studies (UMIACS)
University of Maryland
College Park, MD 20742
{burcu,doermann,weinberg}@umiacs.umd.edu
Abstract
For a language with limited resources, a
dictionary may be one of the few available
electronic resources. To make effective
use of the dictionary for translation, how-
ever, users must be able to access it us-
ing the root form of morphologically de-
formed variant found in the text. Stem-
ming and data driven methods, however,
are not suitable when data is sparse. We
present algorithms for discovering mor-
phemes from limited, noisy data obtained
by scanning a hard copy dictionary. Our
approach is based on the novel applica-
tion of the longest common substring and
string edit distance metrics. Results show
that these algorithms can in fact segment
words into roots and affixes from the lim-
ited data contained in a dictionary, and ex-
tract affixes. This in turn allows non na-
tive speakers to perform multilingual tasks
for applications where response must be
rapid, and their knowledge is limited. In
addition, this analysis can feed other NLP
tools requiring lexicons.
1 Introduction
In order to develop morphological analyzers for lan-
guages that have limited resources (either in terms of
experienced linguists, or electronic data), we must
move beyond data intensive methods developed for
rich resource languages that rely on large amounts
of data for statistical methods. New approaches that
can deal with limited, and perhaps noisy, data are
necessary for these languages.
Printed dictionaries often exist for languages be-
fore large amounts of electronic text, and provide
a variety of information in a structured format. In
this paper, we propose Morphology Induction from
Noisy Data (MIND), a natural language morphology
induction framework that operates on from informa-
tion in dictionaries, specifically headwords and ex-
amples of usage. We use string searching algorithms
to morphologically segment words and identify pre-
fixes, suffixes, circumfixes, and infixes in noisy and
limited data. We present our preliminary results on
two data sources (Cebuano and Turkish), give a de-
tailed analysis of results, and compare them to a
state-of-the-art morphology learner. We employ the
automatically induced affixes in a simple word seg-
mentation process, decreasing the error rate of in-
correctly segmented words by 35.41%.
The next section discusses prior work on mor-
phology learning. In Section 3 and 4, we describe
our approach and MIND framework in detail. Sec-
tion 6 explains the experiments and presents results.
We conclude with future work.
2 Related Work
Much of the previous work on morphology learning
has been reported on automatically acquiring affix
lists. Inspired by works of Harris (1955), Dejean
(1998) attempted to find a list of frequent affixes
for several languages. He used successor and pre-
decessor frequencies of letters in a given sequence
of letters in identifying possible morpheme bound-
60
aries. The morpheme boundaries are where the pre-
dictability of the next letter in the letter sequence is
the lowest.
Several researchers (Brent, 1993; Brent et al,
1995; Goldsmith, 2001) used Minimum Description
Length (MDL) for morphology learning. Snover
and Brent (2001) proposed a generative probabil-
ity model to identify stems and suffixes. Schone
and Jurafsky (2001) used latent semantic analysis
to find affixes. Baroni et al (2002) produced a
ranked list of morphologically related pairs from
a corpus using orthographic or semantic similarity
with minimum edit distance and mutual informa-
tion metrics. Creutz and Lagus (2002) proposed
two unsupervised methods for word segmentation,
one based on maximum description length, and one
based on maximum likelihood. In their model,
words consisted of lengthy sequences of segments
and there is no distinction between stems and af-
fixes. The Whole Word Morphologizer (Neuvel and
Fulop, 2002) uses a POS-tagged lexicon as input, in-
duces morphological relationships without attempt-
ing to discover or identify morphemes. It is also ca-
pable of generating new words beyond the learning
sample.
Mystem (Segalovich, 2003) uses a dictionary for
unknown word guessing in a morphological analysis
algorithm for web search engines. Using a very sim-
ple idea of morphological similarity, unknown word
morphology is taken from all the closest words in
the dictionary, where the closeness is the number of
letters on its end.
The WordFrame model (Wicentowski, 2004) uses
inflection-root pairs, where unseen inflections are
transformed into their corresponding root forms.
The model works with imperfect data, and can han-
dle prefixes, suffixes, stem-internal vowel shifts, and
point-of-affixation stem changes. The WordFrame
model can be used for co-training with low-accuracy
unsupervised algorithms.
Monson (2004) concentrated on languages with
limited resources. The proposed language-
independent framework used a corpus of full word
forms. Candidate suffixes are grouped into candi-
date inflection classes, which are then arranged in a
lattice structure.
A recent work (Goldsmith et al, 2005) proposed
to use string edit distance algorithm as a bootstrap-
ping heuristic to analyze languages with rich mor-
phologies. String edit distance is used for rank-
ing and quantifying the robustness of morphological
generalizations in a set of clean data.
All these methods require clean and most of the
time large amounts of data, which may not exist
for languages with limited electronic resources. For
such languages, the morphology induction is still a
problem. The work in this paper is applicable to
noisy and limited data. String searching algorithms
are used with information found in dictionaries to
extract the affixes.
3 Approach
Dictionary entries contain headwords, and the exam-
ples of how these words are used in context (i.e. ex-
amples of usage). Our algorithm assumes that each
example of usage will contain at least one instance
of the headword, either in its root form, or as one
of its morphological variants. For each headword?
example of usage pair, we find the headword occur-
rence in the example of usage, and extract the affix
if the headword is in one of its morphological vari-
ants. We should note that we do not require the data
to be perfect. It may have noise such as OCR errors,
and our approach successfully identifies the affixes
in such noisy data.
4 Framework
Our framework has two stages, exact match and ap-
proximate match, and uses three string distance met-
rics, the longest common substring (LCS), approx-
imate string matching with k differences (k-DIFF),
and string edit distance (SED). We differentiate be-
tween exact and approximate matches and assign
two counts for each identified affix, exact count
and approximate count. We require that each affix
should have a positive exact count in order to be in
the final affix list. Although approximate match can
be used to find exact matches to identify prefixes,
suffixes, and circumfixes, it is not possible to differ-
entiate between infixes and OCR errors. For these
reasons, we process the two cases separately.
First we briefly describe the three metrics we use
and the adaptations we made to find the edit opera-
tions in SED, and then we explain how we use these
metrics in our framework.
61
4.1 String Searching Algorithms
Longest Common Substring (LCS) Given two
strings p = p1...pn and q = q1...qm, LCS finds the
longest contiguous sequence appearing in p and q.
The longest common substring is not same as the
longest common subsequence because the longest
common subsequence need not be contiguous.
There is a dynamic programming solution for
LCS1 that finds the longest common substring for
two strings with length n and m in O(nm).
String Edit Distance (SED) Given two strings p
and q, SED is the minimum number of edit opera-
tions which transforms p to q. The edit operations al-
lowed are insertions, deletions, and substitutions. In
our algorithm, we set the cost of each edit operation
to 1. A solution based on dynamic programming
computes the distance between strings in O(mn),
where m and n are the lengths of the strings (Wag-
ner and Fischer, 1974).
Approximate string matching with k differ-
ences (k-DIFF) Given two strings p and q, the prob-
lem of approximate string matching with k differ-
ences is finding all the substrings of q which are
at a distance less than or equal to a given value k
from p. Insertions, deletions and substitutions are
all allowed. A dynamic programming solution to
this problem is the same as the classical string edit
distance solution with one difference: the values of
the first row of the table are initialized to 0 (Sellers,
1980). This initialization means that the cost of in-
sertions of letters of q at the beginning of p is zero.
The solutions are all the values of the last row of ta-
ble which are less or equal to k. Consequently, the
minimum value on the last row gives us the distance
of the closest occurrence of the pattern.
String Edit Distance with Edit Operations
(SED-path) In our framework, we are also inter-
ested in tracing back the editing operations per-
formed in achieving the minimum cost alignment.
In order to obtain the sequence of edit operations,
we can work backwards from the complete distance
matrix. For two strings p and q with lengths n and
m respectively, the cell L[n,m] of the distance ma-
trix L gives us the SED between p and q. To get
to the cell L[n,m], we had to come from one of 1)
L[n ? 1,m] (insertion), 2) L[n,m ? 1] (deletion),
1http://www.ics.uci.edu/ dan/class/161/notes/6/Dynamic.html
or 3) L[n ? 1,m ? 1] (substitution). Which of the
three options was chosen can be reconstructed given
these costs, edit operation costs, and the characters
p[n], q[m] of the strings. By working backwards,
we can trace the entire path and thus reconstruct the
alignment. However, there are ambiguous cases; the
same minimum cost may be obtained by a number
of edit operation sequences. We adapted the trace of
the path for our purposes as explained below.
Let path be the list of editing operations to obtain
minimum distance, and SED-path be the SED algo-
rithm that also returns a path. The length of the path
is max(n,m), and path[j] contains the edit oper-
ation to change q[j] (or p[j] if n > m). Path can
contain four different types of operations: Match
(M), substitution (S), insertion (I), and deletion (D).
Our goal is finding affixes and in case of ambiguity,
we employed the following heuristics for finding the
SED operations leading the minimum distance:
Case 1: If one string is longer than the other, choose
I for extra characters
Case 2: Until an M is found, choose I in case of
ambiguity
Case 3: If an M is found previously, choose M/S in
case of ambiguity
Case 4: If there is an M between two I?s, switch this
with the last I
Case 1 ensures that if one word has more charac-
ters than the other, an insertion operation is selected
for those characters.
If there is an ambiguity, and an M/S or I oper-
ation have the same minimum cost, Case 2 gives
priority to the insertion operation until a match
case is encountered, while Case 3 gives priority to
match/substitution operations if a match case was
seen previously.
Below example shows how Case 4 helps us
to localize all the insertion operations. For the
headword?candidate example word pair abirids ?
makaabir??ds, the path changes from (1) to (2) using
Case 4, and correct prefix is identified as we explain
in the next section.
(1) I M I I I M M M S M M? Prefix m-
(2) I I I I M M M M S M M? Prefix maka-
62
5 Morphology Induction from Noisy Data
(MIND)
The MIND framework consists of two stages. In the
exact match stage, MIND framework checks if the
headword occurs without any changes or errors (i.e.
if headword occurs exactly in the example of us-
age). If no such occurrence is found an approximate
match search is performed in second stage. Below
we describe these two stages in detail.
5.1 Exact Match
Given a list of (noisy) headword?example of usage
pairs (w,E), the exact match first checks if the head-
word occurs in E in its root form.2 If the headword
cannot be found in E in its root form, for each ei
in E, the longest common substring, LCS(w, ei),
is computed.3 Let el be the ei that has the longest
common substring (l) with w.4 If w = l, and for
some suffix s and some prefix p one of the following
conditions is true, the affix is extracted.
1. el = ws (suffix) or
2. el = pw (prefix) or
3. el = pws (circumfix)
The extracted affixes are added to the induced af-
fix list, and their exact counts are incremented. In
the third case p?s is treated together as a circumfix.
For the infixes, there is one further step. If w =
w?l and el = e?ll, we compute LCS(w?, e?l). If e?l =
w?s, for some suffix s, s is added as an infix to the
induced affix list. (This means el = w?sl wherew =
w?l.)
The following sample run illustrates how the ex-
act match part identifies affixes. Given the Ce-
buano headword?example of usage pair (abtik) ?
(naabtikan sad ku sa ba?ta?), the word naabtikan is
marked as the candidate that has the longest com-
mon substring with headword abtik. These two
words have the following alignment, and we ex-
tract the circumfix na?an. In the illustration below,
2Headwords consisting of one character are not checked.
3In order to reduce the search space, we do not check the
example words that are shorter than the headword. Although
there are some languages, such as Russian, in which headwords
may be longer than the inflected forms, such cases are not in the
scope of this paper.
4Note that the length of the longest common substring can
be at most the length of the headword, in which case the longest
common substring is the headword itself.
straight lines represent matches, and short lines end-
ing in square boxes represent insertions.
5.2 Approximate Match
When we cannot find an exact match, there may be
an approximate match resulting from an error with
OCR or morphophonemic rules5, and we deal with
such cases separately in the second part of the al-
gorithm. For each ei in E, we compute the dif-
ference between headword, and example word, k-
DIFF(w, ei). The example word that has the min-
imum difference from the headword is selected as
the most likely candidate (ecand). We then find the
sequence of the edit operations performed in achiev-
ing the minimum distance alignment to transform
ecand to w using SED-path algorithm we described
above.6
Let cnt(X) be the count of X operation in the
computed path. If cnt(I) = 0, this case is consid-
ered as an approximate root form (with OCR errors).
The following conditions are considered as possible
errors and no further analysis is done for such cases:
cnt(M) = 0 ||
cnt(M) < max(cnt(S), cnt(D), cnt(I)) ||
cnt(M) < cnt(S) + cnt(D) + cnt(I)
Otherwise, we use the insertion operations at the
beginning and at the end of the path to identify the
type of the affix (prefix, suffix, or circumfix) and the
length of the suffix (number of insertion operations).
The identified affix is added to the affix list, and
its approximate count is incremented. All the other
cases are dismissed as errors. In its current state, the
infix affixes are not handled in approximate match
case.
The following sample shows how approximate
match works with noisy data. In the Cebuano input
5At this initial version, MIND does not make any distinc-
tions between noise in the data such as OCR errors, and mor-
phophonemic rules. Making this distinction will be one of our
future focuses
6Computing k-difference, and the edit path can be done in
parallel to reduce the computing time.
63
pair (ambihas) ? (ambsha?sa pagbutang ang duha
ka silya arun makakita? ang maglingkud sa luyu), the
first word in the example of usage has an OCR er-
ror, i is misrecognized as s. Moreover, there is a
vowel change in the word caused by the affix. An
exact match of the headword cannot be found in the
example of usage. The k-DIFF algorithm returns
ambsha?sa as the candidate example of usage word,
with a distance 2. Then, the SED-path algorithm
returns the path M M M S M S M I, and algorithm
successfully concludes that a is the suffix as shown
below in illustration (dotted lines represent substitu-
tions).
6 Experiments
6.1 Dictionaries
The BRIDGE system (Ma et al, 2003) processes
scanned and OCRed dictionaries to reproduce elec-
tronic versions and extract information from dictio-
nary entries. We used the BRIDGE system to pro-
cess two bilingual dictionaries, a Cebuano-English
(CebEng) dictionary (Wolff, 1972) and a Turkish-
English (TurEng) dictionary (Avery et al, 1974),
and extract a list of headword-example of usage
pairs for our experiments. The extracted data is not
perfect: it has mistagged information, i.e. it may in-
clude some information that is not the headword or
example of usage, or some useful information may
be missing, and OCR errors may occur. OCR errors
can be in different forms: Two words can be merged
into one, one word can be split into two, or charac-
ters can be misrecognized.
Dictionary # of # of # of
Dictionary pages hw-ex pairs words
Cebuano-all 1163 27129 206149
Turkish-all 1000 27487 111334
Cebuano-20 20 562 4134
Turkish-20 20 503 1849
Table 1: Details of Data from Two Dictionaries Used
in Experiments
Along with the headword?example of usage pairs
from more than 1000 pages, we randomly selected
20 pages for detailed analysis. Table 1 provides de-
tails of the data from two dictionaries we use in our
experiments.
Both Cebuano and Turkish are morphologically
rich. Cebuano allows prefixes, suffixes, circumfixes,
infixes, while Turkish is an agglunative language.
The two dictionaries have different characteristics.
The example of usages in CebEng are complete sen-
tences given in italic font while TurEng has phrases,
idioms, or complete sentences as examples of usages
indicated in bold font.
6.2 Protocol
We ran our algorithm first on all of the data and then
on a randomly selected 20 pages from each dictio-
nary. We manually extracted the affixes from each
of the 20 pages. We then evaluated the MIND re-
sults with this ground truth. During the evaluation,
even if the number of an affix in the ground truth and
result are same, if they were extracted from different
words, this is counted as an error. We also examined
the cause of each error in this data.
We then compare our results from the whole
TurEng data with the state-of-the-art Linguistica
(Goldsmith, 2001) algorithm. Finally, we used the
suffixes extracted by MIND and Linguistica to seg-
ment words in a Turkish treebank.
6.3 Analysis
Dict. Affix Sample words
mu- galing/mugaling hiku?h??ku?/muhiku`h??ku`
C nag- kisdum/nagkisdum kugkugl/nagkugkug
E mi- iktin/miiktin k??rus/mika?rus
B i- kunsuylu/ikunsuylu paz??ha/ipar??ha
U na- p??l/nap??l ulatl/nau?lat
A gi- buga/gibuga da?lit/gida?dit
N gi-an labuk/gilabukan ??kug/giiku?gan
O -un gihay/gihayun ga?yung/gayu?ngun
-a pisar/pisara sirnpul/simpu?la
-? ad/ad? ilac?/ilae?
T -i heves/hevesi ilim/ilmi
U -a saz/saza sonsuz/sonsuza
R -e deniz/denize zmim/mime
K -?na etraf/etraf?na kolay/kolay?na
I -ya hasta/hastaya orta/ortaya
S -u? u?st/u?stu? zyu?z/yu?zu?
H -ini bel/belini zevk/zevkini
-ine derin/derinine ic?/ic?ine
Table 3: Sample Affixes Extracted from Two Dictio-
naries
Table 2 shows result of MIND runs. The total
number of affixes and number of different types of
64
Cebuano Turkish
Whole dict. 20 pages Whole dict. 20 pages
Total 26106 542 27314 502
Root form 5727 180 18416 345
Prefix (diff. type) 10300 (180) 197 (26) 6 (6) 0 (0)
Suffix (diff. type) 1315 (253) 16 (8) 6983 (447) 128 (59)
Infix (diff. type) 25 (11) 0 (0) 1 (1) 0 (0)
Circumfix (diff. type) 717 (221) 18 (11) 9 (9) 0 (0)
App. Root form 1023 14 103 1
App. Prefix (diff. type) 1697 (116) 23 (9) 8 (8) 1 (1)
App. Suffix (diff. type) 2930 (199) 63 (19) 168 (100) 5 (5)
App. Circumfix (diff. type) 1060 (207) 14 (5) 20 (20) 0 (0)
Couldn?t decide 1159 13 765 15
Table 2: Total Number and Different Types of Affixes Extracted from Two Dictionaries Using MIND
affixes (in parenthesis) are presented for two dictio-
naries, CebEng and TurEng, and two data sets, the
whole dictionary and 20 randomly selected pages.
The top part of the table gives the exact match results
and the bottom part shows the approximate match
results. For Cebuano, approximate match part of the
framework finds many more affixes than it does for
Turkish. This is due to the different structures in
the two dictionaries. We should note that although
MIND incorrectly finds a few prefixes, circumfixes,
and infixes for Turkish, these all have count one.
Table 3 contains some of the most frequent ex-
tracted affixes along with their exact and approxi-
mate counts, and samples of headword?example of
usage word pairs they were extracted from. Each
word is segmented into one root and one suffix,
therefore when a word takes multiple affixes, they
are all treated as a compound affix.
Dictionary GT cnt. Res.cnt. Misses Additions
Cebuano 311 314 17 14
Turkish 155 142 8 10
Table 4: Detailed Analysis of Affixes from 20 Pages
Table 4 shows the number of affixes in ground
truth and MIND results along with number of
missed and incorrectly added affixes on 20 of these
pages of data. MIND only missed 5% of the affixes
in the ground truth in both data sets.
We also examined the causes of each miss and ad-
dition. Table 5 presents the causes of errors in the
output of MIND with an example for each cause. We
should emphasize that a valid affix such as Turkish
suffix -m? is counted as an error since the suffix -
?n? should be extracted for that particular headword?
example of usage pair. An OCR error such as the
misrecognition of a as d, causes both the miss of the
prefix mag- and incorrect addition of mdg- for Ce-
buano. There are some cases that cannot be correctly
identified by the framework. These usually involve
dropping the last vowel because of morphophone-
mic rules. For the Cebuano dictionary, merge and
split caused several errors, while Turkish data does
not have any such errors. Main reason is the differ-
ent structure and format of the original dictionaries.
In the Cebuano dictionary, an italic font which may
result in merge and split is used to indicate example
of usages.
For the Cebuano data, five invalid suffixes, three
invalid prefixes, and two invalid circumfixes are
found, while one valid suffix and one valid circumfix
are missed. For the Turkish data, three invalid suf-
fixes, one invalid prefix, and two valid suffixes are
found while two valid suffix are missed. When we
look at the invalid affixes in the data, most of them
(six of the Cebuano, and all of the Turkish ones)
have count one, and maximum count in an invalid
affix is five. Therefore, if we use a low threshold,
we can eliminate many of the invalid affixes.
6.4 Comparison to Linguistica
We compared our system with Linguistica, a pub-
licly available unsupervised corpus-based morphol-
ogy learner (Goldsmith, 2001). Linguistica induces
paradigms in a noise-free corpus, while MIND
makes use of string searching algorithms and allows
one to deal with noise at the cost of correctness.
MIND emphasize segmenting a word into its root
and affixes. We trained Linguistica using two dif-
ferent data sets from TurEng7: 1) Whole headword-
7We would like to do the same comparison in Cebuano. For
the time being, we could not find a treebank and native speakers
65
Reason Cebuano Turkish
OCR 8 M?lbi 11 ?n??m? or ?m
Algorithm 8 (uluy, giuylan)? 7 (al?n, aln?nda)?
not gi-an, -lan is found not -?nda, -da is found
Merge 9 ??mung gila?ug???munggila?ug 0 -
Split 1 nag-ku?gus?nag- ku?gus 0 -
Other 5 apr.?april 0 -
Headword is an abbreviation
Table 5: The Distribution of the Causes of Errors in 20 Pages with Samples
example of usage sentence pairs, and 2) Headword-
candidate example words that our algorithm returns.
In the first case (Ling-all), Linguistica uses more
data than our algorithm, so to avoid any biases re-
sulting from this, we also trained Linguistica using
the headword and candidate example word (Ling-
cand). We only used the suffixes, since Turkish is a
suffix-based language. The evaluation is done by a
native speaker.
Figure 1 presents the analysis of the suffix lists
produced by Linguistica using two sets of training
data, and MIND. The suffix lists are composed of
suffixes the systems return that have counts more
than a threshold. The results are presented for six
threshold values for all of the data. We use thresh-
olding to decrease the number of invalid affixes
caused primarily by the noise in the data. For the
MIND results, the suffixes over threshold are the
ones that have positive exact counts and total counts
(sum of exact and approximate counts) more than
the threshold. Although Linguistica is not designed
for thresholding, the data we use is noisy, and we
explored if suffixes with a corpus count more than
a threshold will eliminate invalid suffixes. The ta-
ble on the left gives the total number of suffixes,
the percentage of suffixes that have a count more
than a threshold value, the percentage of invalid suf-
fixes, and percentage of missed suffixes that are dis-
carded by thresholding for the whole TurEng dictio-
nary. The number of affixes MIND finds are much
more than that of Linguistica. Furthermore, number
of invalid affixes are lower. On the other hand, the
number of missed affixes is also higher for MIND
since, for this particular data, there are many affixes
with counts less than 5. 41% of the affixes have an
exact count of 1. The main reason for this is the
agglunative nature of Turkish language. The effect
of thresholding can also be examined in the graph
for Cebuano.
on the right in Figure1 which gives the percentage
of valid suffixes as a function of threshold values.
MIND takes advantage of thresholding, and percent-
age of valid suffixes rapidly decrease for threshold
value 1.
System Th. Total Over Th. Invalid Missed
Ling-cand 0 6 100.00 0.00 0.00
Ling-all 0 4 100.00 0.00 0.00
MIND 0 60 96.67 1.72 0.00
Ling-cand 1 6 66.67 0.00 33.33
Ling-all 1 4 100.00 0.00 0.00
MIND 1 60 41.67 0.00 53.33
Ling-cand 2 6 50.00 0.00 50.00
Ling-all 2 4 75.00 0.00 25.00
MIND 2 60 18.33 0.00 76.67
Table 6: Total Number and Percentage of Over the
Threshold, Invalid, and Missed Suffixes Found by
Linguistica and MIND for Different Threshold Val-
ues for 20 pages of Turkish Data
Table 6 presents the same results for 20 pages
from TurEng for three threshold values. MIND per-
forms well even with very small data and finds many
valid affixes. Linguistica on the other hand finds
very few.
6.5 Stemming
To test the utility of the results, we perform a sim-
ple word segmentation, with the aim of stripping the
inflectional suffixes, and find the bare form of the
word. A word segmenter takes a list of suffixes, and
their counts from the morphology induction system
(Linguistica or MIND), a headword list as a dictio-
nary, a threshold value, and the words from a tree-
bank. For each word in the treebank, there is a root
form (rf ), and a usage form (uf ). The suffixes with
a count more than the threshold are indexed accord-
ing to their last letters. For each word in the tree-
bank, we first check if uf is already in the dictio-
nary, i.e. in the headword list. If we cannot find it
66
System Th. Total % Over Th. % Invalid % Missed
Ling-cand 0 116 100.00 18.10 0.00
Ling-all 0 274 100.00 34.67 0.00
MIND 0 499 89.58 13.20 3.61
Ling-cand 1 116 98.28 17.54 0.86
Ling-all 1 274 94.89 32.69 1.46
MIND 1 499 50.50 4.37 33.07
Ling-cand 2 116 92.24 16.82 5.17
Ling-all 2 274 87.96 31.12 4.74
MIND 2 499 38.48 4.17 44.49
Ling-cand 3 116 91.38 16.98 6.03
Ling-all 3 274 85.40 31.20 6.57
MIND 3 499 28.86 2.78 53.31
Ling-cand 4 116 81.03 12.77 11.21
Ling-all 4 274 81.39 30.94 9.12
MIND 4 499 25.65 3.13 56.51
Ling-cand 5 116 80.17 12.90 12.07
Ling-all 5 274 79.56 31.19 10.58
MIND 5 499 23.25 2.59 58.72
Figure 1: Total Number and Percentage of Over the Threshold, Invalid, Missed and Valid Suffixes Found by
Linguistica and MIND for Different Threshold Values
in the dictionary, we repeatedly attempt to find the
longest suffix that matches the end of uf , and check
the dictionary again. The process stops when a dic-
tionary word is found or when no matching suffixes
can be found at the end of the word. If the word the
segmenter returns is same as rf in the treebank, we
increase the correct count. Otherwise, this case is
counted as an error.
In our stemming experiments we used METU-
Sabanci Turkish Treebank8, a morphologically and
syntactically annotated treebank corpus of 7262
grammatical sentences (Atalay et al, 2003; Oflazer
et al, 2003). We skipped the punctuation and mul-
tiple parses,9 and ran our word segmentation on
14950 unique words. We also used the headword
list extracted from TurEng as the dictionary. Note
that, the headword list is not error-free, it has OCR
errors. Therefore even if the word segmenter returns
the correct root form, it may not be in the dictionary
and the word may be stripped further.
The percentage of correctly segmented words are
presented in Figure 2. We show results for six
threshold values. Suffixes with counts more than the
threshold are used in each case. Again for MIND
results, we require that the exact match counts are
more than zero, and the total of exact match and ap-
8http://www.ii.metu.edu.tr/ corpus/treebank.html
9Multiple parses are the cases where a suffix is attached not
to a single word, but to a group of words. The suffix -ti in takip
etti is attached to takip et.
Figure 2: Percentage of Correctly Segmented Words
by Different Systems for Different Threshold Values
proximate match counts are more than the thresh-
old. For Linguistica, suffixes with a corpus count
more than the threshold are used. For each thresh-
old value, MIND did much better than Ling-cand.
MIND outperformed Ling-all for thresholds 0 and
1. For the other values, the difference is small. We
should note that Ling-all uses much more training
data than MIND (503 vs. 1849 example of words),
and even with this difference the performance of
MIND is close to Ling-all. We believe the reason
for the close performance of MIND and Ling-all in
segmentation despite the huge difference in the num-
ber of correct affixes they found due to the fact that
affixes Ling-all finds are shorter, and more frequent.
In its current state, MIND does not segment com-
pound affixes, and find several long and less fre-
quent affixes. These long affixes can be composed
67
by shorter affixes Linguistica finds.
7 Conclusion and Future Work
We presented a framework for morphology induc-
tion from noisy data, that is especially useful for lan-
guages which have limited electronic data. We use
the information in dictionaries, specifically head-
word and the corresponding example of usage sen-
tences, to acquire affix lists of the language. We pre-
sented results on two data sets and demonstrated that
our framework successfully finds the prefixes, suf-
fixes, circumfixes, and infixes. We also used the ac-
quired suffix list from one data set in a simple word
segmentation process, and outperformed a state-of-
the-art morphology learner using the same amount
of training data.
At this point we are only using headword and
corresponding example of usage pairs. Dictionaries
provide much more information. We plan to make
use of other information, such as POS, to categorize
the acquired affixes. We will also investigate how
using all the words in example of usages and split-
ting the compound affixes in agglunative languages
can help us to increase the confidence of correct af-
fixes, and decrease the number of invalid affixes.
Finally we will work on identifying morphophone-
mic rules (especially stem-interval vowel shifts and
point-of-affixation stem changes).
Acknowledgments
The partial support of this research under contract
MDA-9040-2C-0406 is gratefully acknowledged.
References
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003. The an-
notation process in the Turkish Treebank. In Proceedings of
the EACL Workshop on Linguistically Interpreted Corpora?
LINC, Budapest, Hungary, April.
Robert Avery, Serap Bezmez, Anna G. Edmonds, and Mehlika
Yaylal?. 1974. Redhouse ?Ingilizce-Tu?rkc?e So?zlu?k. Red-
house Yay?nevi.
Marco Baroni, Johannes Matiasek, and Harald Trost. 2002.
Unsupervised discovery of morphologically related words
based on orthographic and semantic similarity. In Proceed-
ings of the ACL-02 Workshop on Morphological and Phono-
logical Learning, pages 48?57.
Michael R. Brent, Sreerama K. Murthy, and Andrew Lundberg.
1995. Discovering morphemic suffixes: A case study in
minimum description length induction. In Proceedings of
the 15th Annual Conference of the Cognitive Science Soci-
ety, pages 28?36, Hillsdale, NJ.
Michael R. Brent. 1993. Minimal generative models: A mid-
dle ground between neurons and triggers. In Proceedings of
the 5th International Workshop on Artificial Intelligence and
Statistics, Ft. Laudersdale, FL.
Mathias Creutz and Krista Lagus. 2002. Unsupervised discov-
ery of morphemes. In Proceedings of the ACL-02 Workshop
on Morphological and Phonological Learning.
H. Dejean. 1998. Morphemes as necessary concepts for struc-
tures: Discovery from untagged corpora. In Workshop on
Paradigms and Grounding in Natural Language Learning,
pages 295?299.
John Goldsmith, Yu Hu, Irina Matveeva, and Colin Sprague.
2005. A heuristic for morpheme discovery based on string
edit distance. Technical Report TR-2205-04, Department of
Computer Science, University of Chicago.
John Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Linguistics,
27(2):153?198.
Zellig Harris. 1955. From phoneme to morpheme. Language,
31:190?222.
Huanfeng Ma, Burcu Karagol-Ayan, David Doermann, Dou-
glas Oard, and Jianqiang Wang. 2003. Parsing and tag-
ging of bilingual dictionaries. Traitement Automatique Des
Langues, pages 125?150.
Christian Monson. 2004. A framework for unsupervised nat-
ural language morphology induction. In Proceedings of the
Student Research Workshop: ACL 2004, pages 67?72.
Sylvain Neuvel and Sean A. Fulop. 2002. Unsupervised learn-
ing of morphology without morphemes. In Proceedings of
the ACL-02 Workshop on Morphological and Phonological
Learning, pages 31?40.
Kemal Oflazer, Bilge Say, Dilek Hakkani-Tu?r, and Go?khan Tu?r.
2003. Building a Turkish Treebank. In Anne Abeille?, edi-
tor, Building and Using Parsed Corpora. Kluwer Academic
Publishers.
Patrick Schone and Daniel Jurafsky. 2001. Knowledge-free
induction of inflectional morphologies. In Second Meeting
of the NAACL, pages 183?191.
Ilya Segalovich. 2003. A fast morphological algorithm with
unknown word guessing induced by a dictionary for a web
search engine. In Proceedings of MLMTA, Las Vegas, NV.
P.H. Sellers. 1980. The theory and computation of evolution-
ary distances: pattern recognition. Journal of Algorithms,
1:359?373.
Matthew G. Snover and Michael R. Brent. 2001. A bayesian
model for morpheme and paradigm identification. In Pro-
ceedings of the 39th Annual Meeting of the ACL, pages 482?
490.
Robert A. Wagner and Michael J. Fischer. 1974. The string-
to-string correction problem. Journal of the Association for
Computing Machinery, 21(1):168?173.
Richard Wicentowski. 2004. Multilingual noise-robust super-
vised morphological analysis using the wordframe model.
In Proceedings of the 7th Meeting of the ACL Special In-
terest Group in Computational Phonology, pages 70?77,
Barcelona, Spain.
John U. Wolff. 1972. A Dictionary of Cebuano Visaya. South-
east Asia Program, Cornell University, Ithaca, New York.
68
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 78?86,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
A random forest system combination approach for error detection in
digital dictionaries
Michael Bloodgood and Peng Ye and Paul Rodrigues
and David Zajic and David Doermann
University of Maryland
College Park, MD
meb@umd.edu, pengye@umiacs.umd.edu, prr@umd.edu,
dzajic@casl.umd.edu, doermann@umiacs.umd.edu
Abstract
When digitizing a print bilingual dictionary,
whether via optical character recognition or
manual entry, it is inevitable that errors are
introduced into the electronic version that is
created. We investigate automating the pro-
cess of detecting errors in an XML repre-
sentation of a digitized print dictionary us-
ing a hybrid approach that combines rule-
based, feature-based, and language model-
based methods. We investigate combin-
ing methods and show that using random
forests is a promising approach. We find
that in isolation, unsupervised methods ri-
val the performance of supervised methods.
Random forests typically require training
data so we investigate how we can apply
random forests to combine individual base
methods that are themselves unsupervised
without requiring large amounts of training
data. Experiments reveal empirically that
a relatively small amount of data is suffi-
cient and can potentially be further reduced
through specific selection criteria.
1 Introduction
Digital versions of bilingual dictionaries often
have errors that need to be fixed. For example,
Figures 1 through 5 show an example of an er-
ror that occurred in one of our development dic-
tionaries and how the error should be corrected.
Figure 1 shows the entry for the word ?turfah? as
it appeared in the original print copy of (Qureshi
and Haq, 1991). We see this word has three senses
with slightly different meanings. The third sense
is ?rare?. In the original digitized XML version
of (Qureshi and Haq, 1991) depicted in Figure 2,
this was misrepresented as not being the meaning
Figure 1: Example dictionary entry
Figure 2: Example of error in XML
of ?turfah? but instead being a usage note that fre-
quency of use of the third sense was rare. Figure 3
shows the tree corresponding to this XML repre-
sentation. The corrected digital XML representa-
tion is depicted in Figure 4 and the corresponding
corrected tree is shown in Figure 5.
Zajic et al (2011) presented a method for re-
pairing a digital dictionary in an XML format us-
ing a dictionary markup language called DML. It
remains time-consuming and error-prone however
to have a human read through and manually cor-
rect a digital version of a dictionary, even with
languages such as DML available. We therefore
investigate automating the detection of errors.
We investigate the use of three individual meth-
ods. The first is a supervised feature-based
method trained using SVMs (Support Vector Ma-
chines). The second is a language-modeling
78
.ENTRY
. .? ? ?
.
.SENSE
.USG
.rare
.? ? ?FORM
. .PRON
.t?r?fah
ORTH
.????
Figure 3: Tree structure of error
Figure 4: Example of error in XML, fixed
.ENTRY
. .? ? ?
.
.SENSE
.TRANS
.TR
.rare
.
.? ? ?FORM
. .PRON
.t?r?fah
ORTH
.????
Figure 5: Tree structure of error, fixed
method that replicates the method presented in
(Rodrigues et al, 2011). The third is a simple
rule inference method. The three individual meth-
ods have different performances. So we investi-
gate how we can combine the methods most effec-
tively. We experiment with majority vote, score
combination, and random forest methods and find
that random forest combinations work the best.
For many dictionaries, training data will not be
available in large quantities a priori and therefore
methods that require only small amounts of train-
ing data are desirable. Interestingly, for automati-
cally detecting errors in dictionaries, we find that
the unsupervised methods have performance that
rivals that of the supervised feature-based method
trained using SVMs. Moreover, when we com-
bine methods using the random forest method, the
combination of unsupervised methods works bet-
ter than the supervised method in isolation and al-
most as well as the combination of all available
methods. A potential drawback of using the ran-
dom forest combination method however is that it
requires training data. We investigated how much
training data is needed and find that the amount
of training data required is modest. Furthermore,
by selecting the training data to be labeled with
the use of specific selection methods reminiscent
of active learning, it may be possible to train the
random forest system combination method with
even less data without sacrificing performance.
In section 2 we discuss previous related work
and in section 3 we explain the three individual
methods we use for our application. In section 4
we explain the three methods we explored for
combining methods; in section 5 we present and
discuss experimental results and in section 6 we
conclude and discuss future work.
2 Related Work
Classifier combination techniques can be broadly
classified into two categories: mathematical and
behavioral (Tulyakov et al, 2008). In the first
category, functions or rules combine normalized
classifier scores from individual classifiers. Ex-
amples of techniques in this category include Ma-
jority Voting (Lam and Suen, 1997), as well as
simple score combination rules such as: sum rule,
min rule, max rule and product rule (Kittler et al,
1998; Ross and Jain, 2003; Jain et al, 2005). In
the second category, the output of individual clas-
sifiers are combined to form a feature vector as
79
the input to a generic classifier such as classifi-
cation trees (P. and Chollet, 1999; Ross and Jain,
2003) or the k-nearest neighbors classifier (P. and
Chollet, 1999). Our method falls into the second
category, where we use a random forest for sys-
tem combination.
The random forest method is described in
(Breiman, 2001). It is an ensemble classifier con-
sisting of a collection of decision trees (called a
random forest) and the output of the random for-
est is the mode of the classes output by the indi-
vidual trees. Each single tree is trained as follows:
1) a random set of samples from the initial train-
ing set is selected as a training set and 2) at each
node of the tree, a random subset of the features is
selected, and the locally optimal split is based on
only this feature subset. The tree is fully grown
without pruning. Ma et al (2005) used random
forests for combining scores of several biometric
devices for identity verification and have shown
encouraging results. They use all fully supervised
methods. In contrast, we explore minimizing the
amount of training data needed to train a random
forest of unsupervised methods.
The use of active learning in order to re-
duce training data requirements without sacri-
ficing model performance has been reported on
extensively in the literature (e.g., (Seung et al,
1992; Cohn et al, 1994; Lewis and Gale, 1994;
Cohn et al, 1996; Freund et al, 1997)). When
training our random forest combination of indi-
vidual methods that are themselves unsupervised,
we explore how to select the data so that only
small amounts of training data are needed because
for many dictionaries, gathering training data may
be expensive and labor-intensive.
3 Three Single Method Approaches for
Error Detection
Before we discuss our approaches for combining
systems, we briefly explain the three individual
systems that form the foundation of our combined
system.
First, we use a supervised approach where we
train a model using SVMlight (Joachims, 1999)
with a linear kernel and default regularization pa-
rameters. We use a depth first traversal of the
XML tree and use unigrams and bigrams of the
tags that occur as features for each subtree to
make a classification decision.
We also explore two unsupervised approaches.
The first unsupervised approach learns rules for
when to classify nodes as errors or not. The rule-
based method computes an anomaly score based
on the probability of subtree structures. Given
a structure A and its probability P(A), the event
that A occurs has anomaly score 1-P(A) and the
event that A does not occur has anomaly score
P(A). The basic idea is if a certain structure hap-
pens rarely, i.e. P(A) is very small, then the oc-
currence of A should have a high anomaly score.
On the other hand, if A occurs frequently, then
the absence of A indicates anomaly. To obtain
the anomaly score of a tree, we simply take the
maximal scores of all events induced by subtrees
within this tree.
The second unsupervised approach uses a reim-
plementation of the language modeling method
described in (Rodrigues et al, 2011). Briefly,
this methods works by calculating the probabil-
ity a flattened XML branch can occur, given a
probability model trained on the XML branches
from the original dictionary. We used (Stolcke,
2002) to generate bigram models using Good Tur-
ing smoothing and Katz back off, and evaluated
the log probability of the XML branches, ranking
the likelihood. The first 1000 branches were sub-
mitted to the hybrid system marked as an error,
and the remaining were submitted as a non-error.
Results for the individual classifiers are presented
in section 5.
4 Three Methods for Combining
Systems
We investigate three methods for combining the
three individual methods. As a baseline, we in-
vestigate simple majority vote. This method takes
the classification decisions of the three methods
and assigns the final classification as the classifi-
cation that the majority of the methods predicted.
A drawback of majority vote is that it does not
weight the votes at all. However, it might make
sense to weight the votes according to factors such
as the strength of the classification score. For ex-
ample, all of our classifiers make binary decisions
but output scores that are indicative of the confi-
dence of their classifications. Therefore we also
explore a score combination method that consid-
ers these scores. Since measures from the differ-
ent systems are in different ranges, we normal-
ize these measurements before combining them
(Jain et al, 2005). We use z-score which com-
80
putes the arithmetic mean and standard deviation
of the given data for score normalization. We then
take the summation of normalized measures as
the final measure. Classification is performed by
thresholding this final measure.1
Another approach would be to weight them by
the performance level of the various constituent
classifiers in the ensemble. Weighting based on
performance level of the individual classifiers is
difficult because it would require extra labeled
data to estimate the various performance lev-
els. It is not clear how to translate the differ-
ent performance estimates into weights, or how
to have those weights interact with weights based
on strengths of classification. Therefore, we did
not weigh based on performance level explicitly.
We believe that our third combination method,
the use of random forests, implicitly cap-
tures weighting based on performance level and
strengths of classifications. Our random forest ap-
proach uses three features, one for each of the in-
dividual systems we use. With random forests,
strengths of classification are taken into account
because they form the values of the three fea-
tures we use. In addition, the performance level
is taken into account because the training data
used to train the decision trees that form the for-
est help to guide binning of the feature values into
appropriate ranges where classification decisions
are made correctly. This will be discussed further
in section 5.
5 Experiments
This section explains the details of the experi-
ments we conducted testing the performance of
the various individual and combined systems.
Subsection 5.1 explains the details of the data we
experiment on; subsection 5.2 provides a sum-
mary of the main results of our experiments; and
subsection 5.3 discusses the results.
5.1 Experimental Setup
We obtained the data for our experiments using
a digitized version of (Qureshi and Haq, 1991),
the same Urdu-English dictionary that Zajic et
al. (2011) had used. Zajic et al (2011) pre-
sented DML, a programming language used to
fix errors in XML documents that contain lexico-
graphic data. A team of language experts used
1In our experiments we used 0 as the threshold.
Recall Precision F1-Measure Accuracy
LM 11.97 89.90 21.13 57.53
RULE 99.79 70.83 82.85 80.37
FV 35.34 93.68 51.32 68.14
Table 1: Performance of individual systems at
ENTRY tier.
DML to correct errors in a digital, XML repre-
sentation of the Kitabistan Urdu dictionary. The
current research compared the source XML doc-
ument and the DML commands to identify the el-
ements that the language experts decided to mod-
ify. We consider those elements to be errors. This
is the ground truth used for training and evalua-
tion. We evaluate at two tiers, corresponding to
two node types in the XML representation of the
dictionary: ENTRY and SENSE. The example de-
picted in Figures 1 through 5 shows an example of
SENSE. The intuition of the tier is that errors are
detectable (or learnable) from observing the ele-
ments within a tier, and do not cross tier bound-
aries. These tiers are specific to the Kitabistan
Urdu dictionary, and we selected them by observ-
ing the data. A limitation of our work is that we do
not know at this time whether they are generally
useful across dictionaries. Future work will be
to automatically discover the meaningful evalua-
tion tiers for a new dictionary. After this process,
we have a dataset with 15,808 Entries, of which
47.53% are marked as errors and 78,919 Senses,
of which 10.79% are marked as errors. We per-
form tenfold cross-validation in all experiments.
In our random forest experiments, we use 12 de-
cision trees, each with only 1 feature.
5.2 Results
This section presents experimental results, first
for individual systems and then for combined sys-
tems.
5.2.1 Performance of individual systems
Tables 1 and 2 show the performance of lan-
guage modeling-based method (LM), rule-based
method (RULE) and the supervised feature-based
method (FV) at different tiers. As can be seen,
at the ENTRY tier, RULE obtains the highest F1-
Measure and accuracy, while at the SENSE tier,
FV performs the best.
81
Recall Precision F1-Measure Accuracy
LM 9.85 94.00 17.83 90.20
RULE 84.59 58.86 69.42 91.96
FV 72.44 98.66 83.54 96.92
Table 2: Performance of individual systems at
SENSE tier.
5.2.2 Improving individual systems using
random forests
In this section, we show that by applying ran-
dom forests on top of the output of individual sys-
tems, we can have gains (absolute gains, not rel-
ative) in accuracy of 4.34% to 6.39% and gains
(again absolute, not relative) in F1-measure of
3.64% to 11.39%. Tables 3 and 4 show our ex-
perimental results at ENTRY and SENSE tiers
when applying random forests with the rule-based
method.2 These results are all obtained from 100
iterations of the experiments with different parti-
tions of the training data chosen at each iteration.
Mean values of different evaluation measures and
their standard deviations are shown in these ta-
bles. We change the percentage of training data
and repeat the experiments to see how the amount
of training data affects performance.
It might be surprising to see the gains in per-
formance that can be achieved by using a ran-
dom forest of decision trees created using only
the rule-based scores as features. To shed light
on why this is so, we show the distribution of
RULE-based output scores for anomaly nodes and
clean nodes in Figure 6. They are well separated
and this explains why RULE alone can have good
performance. Recall RULE classifies nodes with
anomaly scores larger than 0.9 as errors. How-
ever, in Figure 6, we can see that there are many
clean nodes with anomaly scores larger than 0.9.
Thus, the simple thresholding strategy will bring
in errors. Applying random forest will help us
identify these errorful regions to improve the per-
formance. Another method for helping to identify
these errorful regions and classify them correctly
is to apply random forest of RULE combined with
the other methods, which we will see will even
further boost the performance.
2We also applied random forests to our language mod-
eling and feature-based methods, and saw similar gains in
performance.
0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
0
500
1000
1500
output score of rule-based system
o
c
c
u
r
r
e
n
c
e
s
 
 
anomaly
clean
Figure 6: Output anomalies score from RULE
(ENTRY tier).
5.2.3 System combination
In this section, we explore different methods
for combining measures from the three systems.
Table 5 shows the results of majority voting and
score combination at the ENTRY tier. As can
be seen, majority voting performs poorly. This
may be due to the fact that the performances of
the three systems are very different. RULE sig-
nificantly outperforms the other two systems, and
as discussed in Section 4 neither majority voting
nor score combination weights this higher perfor-
mance appropriately.
Tables 6 and 7 show the results of combining
RULE and LM. This is of particular interest since
these two systems are unsupervised. Combin-
ing these two unsupervised systems works better
than the individual methods, including supervised
methods. Tables 8 and 9 show the results for com-
binations of all available systems. This yields the
highest performance, but only slightly higher than
the combination of only unsupervised base meth-
ods.
The random forest combination technique does
require labeled data even if the underlying base
methods are unsupervised. Based on the ob-
servation in Figure 6, we further study whether
choosing more training data from the most error-
ful regions will help to improve the performance.
Experimental results in Table 10 show how the
choice of training data affects performance. It
appears that there may be a weak trend toward
higher performance when we force the selection
of the majority of the training data to be from
ENTRY nodes whose RULE anomaly scores are
82
Training % Recall Precision F1-Measure Accuracy
0.1 78.17( 14.83) 75.87( 3.96) 76.18( 7.99) 77.68( 5.11)
1 82.46( 4.81) 81.34( 2.14) 81.79( 2.20) 82.61( 1.69)
10 87.30( 1.96) 84.11( 1.29) 85.64( 0.46) 86.10( 0.35)
50 89.19( 1.75) 83.99( 1.20) 86.49( 0.34) 86.76( 0.28)
Table 3: Mean and std of evaluation measures from 100 iterations of experiments using RULE+RF.
(ENTRY tier)
Training % Recall Precision F1-Measure Accuracy
0.1 60.22( 12.95) 69.66( 9.54) 63.29( 7.92) 92.61( 1.57)
1 70.28( 3.48) 86.26( 3.69) 77.31( 1.39) 95.55( 0.25)
10 71.52( 1.23) 91.26( 1.39) 80.18( 0.41) 96.18( 0.07)
50 72.11( 0.75) 91.90( 0.64) 80.81( 0.39) 96.30( 0.06)
Table 4: Mean and std of evaluation measures from 100 iterations of experiments using RULE+RF.
(SENSE tier)
larger than 0.9. However, the magnitudes of the
observed differences in performance are within a
single standard deviation so it remains for future
work to determine if there are ways to select the
training data for our random forest combination
in ways that substantially improve upon random
selection.
5.3 Discussion
Majority voting (at the entry level) performs
poorly, since the performance of the three individ-
ual systems are very different and majority voting
does not weight votes at all. Score combination
is a type of weighted voting. It takes into account
the confidence level of output from different sys-
tems, which enables it to perform better than ma-
jority voting. However, score combination does
not take into account the performance levels of
the different systems, and we believe this limits its
performance compared with random forest com-
binations.
Random forest combinations perform the best,
but the cost is that it is a supervised combination
method. We investigated how the amount of train-
ing data affects the performance, and found that a
small amount of labeled data is all that the random
forest needs in order to be successful. Moreover,
although this requires further exploration, there is
weak evidence that the size of the labeled data can
potentially be reduced by choosing it carefully
from the region that is expected to be most error-
ful. For our application with a rule-based system,
this is the high-anomaly scoring region because
although it is true that anomalies are often errors,
it is also the case that some structures occur rarely
but are not errorful.
RULE+LM with random forest is a little bet-
ter than RULE with random forest, with gain of
about 0.7% on F1-measure when evaluated at the
ENTRY level using 10% data for training.
An examination of examples that are marked as
being errors in our ground truth but that were not
detected to be errors by any of our systems sug-
gests that some examples are decided on the ba-
sis of features not yet considered by any system.
For example, in Figure 7 the second FORM is
well-formed structurally, but the Urdu text in the
first FORM is the beginning of the phrase translit-
erated in the second FORM. Automatic systems
detected that the first FORM was an error, how-
ever did not mark the second FORM as an error
whereas our ground truth marked both as errors.
Examination of false negatives also revealed
cases where the systems were correct that there
was no error but our ground truth wrongly indi-
cated that there was an error. These were due to
our semi-automated method for producing ground
truth that considers elements mentioned in DML
commands to be errors. We discovered instances
in which merely mentioning an element in a DML
command does not imply that the element is an er-
ror. These cases are useful for making refinements
to how ground truth is generated from DML com-
mands.
Examination of false positives revealed two
categories. One was where the element is indeed
an error but was not marked as an element in our
ground truth because it was part of a larger error
83
Method Recall Precision F1-Measure Accuracy
Majority voting 36.71 90.90 52.30 68.18
Score combination 76.48 75.82 76.15 77.23
Table 5: LM+RULE+FV (ENTRY tier)
Training % Recall Precision F1-Measure Accuracy
0.1 77.43( 15.14) 72.77( 6.03) 74.26( 8.68) 75.32( 6.71)
1 86.50( 3.59) 80.41( 1.95) 83.27( 1.33) 83.51( 1.11)
10 88.12( 1.12) 84.65( 0.57) 86.34( 0.46) 86.76( 0.39)
50 89.12( 0.62) 87.39( 0.56) 88.25( 0.30) 88.72( 0.29)
Table 6: System combination based on random forest (LM+RULE). (ENTRY tier, mean (std))
Training % Recall Precision F1-Measure Accuracy
0.1 65.85( 12.70) 71.96( 7.63) 67.68( 7.06) 93.38( 1.03)
1 80.29( 3.58) 84.97( 3.13) 82.45( 1.36) 96.31( 0.28)
10 82.68( 2.49) 90.91( 2.37) 86.53( 0.41) 97.22( 0.07)
50 83.22( 2.43) 92.21( 2.29) 87.42( 0.35) 97.42( 0.04)
Table 7: System combination based on random forest (LM+RULE). (SENSE tier, mean (std))
Training % Recall Precision F1-Measure Accuracy
20 91.57( 0.55) 87.77( 0.43) 89.63( 0.23) 89.93( 0.22)
50 92.04( 0.54) 88.85( 0.48) 90.41( 0.29) 90.72( 0.28)
Table 8: System combination based on random forest (LM+RULE+FV). (ENTRY tier, mean (std))
Training % Recall Precision F1-Measure Accuracy
20 86.47( 1.01) 90.67( 1.02) 88.51( 0.26) 97.58( 0.06)
50 86.50( 0.81) 92.04( 0.85) 89.18( 0.30) 97.73( 0.06)
Table 9: System combination based on random forest (LM+RULE+FV). (SENSE tier, mean (std))
Recall Precision F1-Measure Accuracy
50% 85.40( 4.65) 80.71( 3.49) 82.82( 1.57) 82.63( 1.54)
70% 86.13( 3.94) 80.97( 2.64) 83.36( 1.33) 83.30( 1.21)
90% 85.77( 3.61) 81.82( 2.72) 83.65( 1.45) 83.69( 1.35)
95% 85.93( 3.46) 82.14( 2.98) 83.89( 1.32) 83.94( 1.18)
random 86.50( 3.59) 80.41( 1.95) 83.27( 1.33) 83.51( 1.11)
Table 10: Effect of choice of training data based on rule based method (Mean evaluation measures
from 100 iterations of experiments using RULE+LM at ENTRY tier). We choose 1% of the data for
training and the first column in the table specifies the percentage of training data chosen from Entries
with anomalous score larger than 0.9.
84
Figure 7: Example of error in XML
that got deleted and therefore no DML command
ever mentioned the smaller element but lexicog-
raphers upon inspection agree that the smaller el-
ement is indeed errorful. The other category was
where there were actual errors that the dictionary
editors didn?t repair with DML but that should
have been repaired.
A major limitation of our work is testing how
well it generalizes to detecting errors in other dic-
tionaries besides the Urdu-English one (Qureshi
and Haq, 1991) that we conducted our experi-
ments on.
6 Conclusions
We explored hybrid approaches for the applica-
tion of automatically detecting errors in digitized
copies of dictionaries. The base methods we
explored consisted of a variety of unsupervised
and supervised methods. The combination meth-
ods we explored also consisted of some methods
which required labeled data and some which did
not.
We found that our base methods had differ-
ent levels of performance and with this scenario
majority voting and score combination methods,
though appealing since they require no labeled
data, did not perform well since they do not
weight votes well.
We found that random forests of decision trees
was the best combination method. We hypothe-
size that this is due to the nature of our task and
base systems. Random forests were able to help
tease apart the high-error region (where anoma-
lies take place). A drawback of random forests
as a combination method is that they require la-
beled data. However, experiments reveal empiri-
cally that a relatively small amount of data is suf-
ficient and the amount might be able to be further
reduced through specific selection criteria.
Acknowledgments
This material is based upon work supported, in
whole or in part, with funding from the United
States Government. Any opinions, findings and
conclusions, or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the views of the University of
Maryland, College Park and/or any agency or en-
tity of the United States Government. Nothing
in this report is intended to be and shall not be
treated or construed as an endorsement or recom-
mendation by the University of Maryland, United
States Government, or the authors of the product,
process, or service that is the subject of this re-
port. No one may use any information contained
or based on this report in advertisements or pro-
motional materials related to any company prod-
uct, process, or service or in support of other com-
mercial purposes.
References
Leo Breiman. 2001. Random forests. Machine
Learning, 45:5?32. 10.1023/A:1010933404324.
David A. Cohn, Les Atlas, and Richard Ladner. 1994.
Improving generalization with active learning. Ma-
chine Learning, 15:201?221.
David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research,
4:129?145.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and
Naftali Tishby. 1997. Selective sampling using the
query by committee algorithm. Machine Learning,
28:133?168.
Anil K. Jain, Karthik Nandakumar, and Arun Ross.
2005. Score normalization in multimodal biometric
systems. Pattern Recognition, pages 2270?2285.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf, Christo-
pher J. Burges, and Alexander J. Smola, editors, Ad-
vances in Kernel Methods ? Support Vector Learn-
ing, chapter 11, pages 169?184. The MIT Press,
Cambridge, US.
J. Kittler, M. Hatef, R.P.W. Duin, and J. Matas.
1998. On combining classifiers. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
20(3):226 ?239, mar.
L. Lam and S.Y. Suen. 1997. Application of majority
voting to pattern recognition: an analysis of its be-
havior and performance. Systems, Man and Cyber-
netics, Part A: Systems and Humans, IEEE Trans-
actions on, 27(5):553 ?568, sep.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In
SIGIR ?94: Proceedings of the 17th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 3?12,
85
New York, NY, USA. Springer-Verlag New York,
Inc.
Yan Ma, Bojan Cukic, and Harshinder Singh. 2005.
A classification approach to multi-biometric score
fusion. In AVBPA?05, pages 484?493.
Verlinde P. and G. Chollet. 1999. Comparing deci-
sion fusion paradigms using k-nn based classifiers,
decision trees and logistic regression in a multi-
modal identity verification application. In Proceed-
ings of the 2nd International Conference on Audio
and Video-Based Biometric Person Authentication
(AVBPA), pages 189?193.
Bashir Ahmad Qureshi and Abdul Haq. 1991. Stan-
dard Twenty First Century Urdu-English Dictio-
nary. Educational Publishing House, Delhi.
Paul Rodrigues, David Zajic, David Doermann,
Michael Bloodgood, and Peng Ye. 2011. Detect-
ing structural irregularity in electronic dictionaries
using language modeling. In Proceedings of the
Conference on Electronic Lexicography in the 21st
Century, pages 227?232, Bled, Slovenia, Novem-
ber. Trojina, Institute for Applied Slovene Studies.
Arun Ross and Anil Jain. 2003. Information fusion in
biometrics. Pattern Recognition Letters, 24:2115?
2125.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In COLT ?92: Proceedings of
the fifth annual workshop on Computational learn-
ing theory, pages 287?294, New York, NY, USA.
ACM.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing.
Sergey Tulyakov, Stefan Jaeger, Venu Govindaraju,
and David Doermann. 2008. Review of classi-
fier combination methods. In Machine Learning in
Document Analysis and Recognition, volume 90 of
Studies in Computational Intelligence, pages 361?
386. Springer Berlin / Heidelberg.
David Zajic, Michael Maxwell, David Doermann, Paul
Rodrigues, and Michael Bloodgood. 2011. Cor-
recting errors in digital lexicographic resources us-
ing a dictionary manipulation language. In Pro-
ceedings of the Conference on Electronic Lexicog-
raphy in the 21st Century, pages 297?301, Bled,
Slovenia, November. Trojina, Institute for Applied
Slovene Studies.
86
