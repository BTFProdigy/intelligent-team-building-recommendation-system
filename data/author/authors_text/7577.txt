Automatic Optimization of Dialogue Management 
Diane J. Litman, Michael S. Kearns, Satinder Singh, Mari lyn A. Walker  
AT&T Labs -  Research  
180 Park  Avenue 
F lo rham Park ,  N J  07932 USA 
{d iane ,mkearns ,bave j  a,walker} @research.ar t .corn  
Abstract 
Designing the dialogue strategy of a spoken dialogue 
system involves many nontrivial choices. This pa- 
per I)resents a reinforcement learning approach for 
automatically optimizing a dialogue strategy that 
addresses the technical challenges in applying re- 
inforcement learning to a working dialogue system 
with hulnan users. \?e then show that our approach 
measurably improves performance in an experimen- 
tal system. 
1 I n t roduct ion  
Recent advances in spoken language understanding 
have made it 1)ossible to develop dialogue systems 
tbr many applications. The role of the dialogue man- 
ager in such systems is to interact in a naturM w~y 
to hel 1 ) the user complete the tasks that the system 
is designed to support. Tyl)ically, an expert designs 
a dialogue manager by hand, and has to make m~ny 
nontrivial design choices that can seriously impact 
system performance. This paper applies reintbrce- 
ment learning (RL) to automatically learn design 
choices that optimize system pertbrnmnee for a cho- 
seLL pertbrmance measure (Levin et al, 2000; Walker 
et al, 1998). 
Consider the spoken dialogue system named N J- 
Fun, wlfich we built to help users find fun places 
to go in New Jersey. A sample dialogue with N J- 
Fun is shown in Figure 1, with system utterances 
labeled Si and user utterances labeled Ui. When 
NJFun greets the user, it can provide an open greet- 
ing such as utterance S1, expecting that the user 
will take the initiative. Alternatively, it can pro- 
vide a directive greeting in which the system takes 
the initiative, as in "Welcome to NJFun. Please say 
an activity name or say 'list activities' for a list of 
activities I know about." After tile user's response 
U1, NJFun must decide whether it should explicitly 
confirm its understanding, as in utterances $2 and 
$3. NJFun can also simply continue on with the di- 
alogue, as when it does not explicitly confirm that 
the user wants to find out M)out wineries. In NJFun, 
as shown in more detail below, decisions about ini- 
tiative and confirmation strategies alone result in a 
SI: Welcome to N.llSm. How may I help you? 
UI: I'd like to find mn winetasting in Lambertville 
in the morning. (ASR output: I'd like to find 
out wirterics the in the Lambertvillc in the 
mornin.q. ) 
$2: Did you say you are interested in Lambertville? 
U2: Yes. 
$3: Did you say you want to go in the morning? 
U3: Yes. 
$4: I found a winery near l~ambertville that is 
open in the morning, it is \[...\] Please give 
lne feedback by saying 'good', ~so-so', or 'bad'. 
U4: Good. 
Figure 1: An example dialogue with NJFun. 
search space of 242 potential globnl dialogue strate- 
gies. Furthermore, the performance of a dialogue 
strategy depends on many other factors, such as 
the user population, the robustness of the automatic 
speech recognizer (ASR), and task difficulty (Kamm 
et al, 1998; DanMi and Gerbino, 1995). 
In the main, previous research has treated the 
specification of the dialogue management s rategy 
as an iterative design problem: several versions of a 
system are created, dialogue corpora are collected 
with human users interacting with different ver- 
sions of tile system, a number of evaluation met- 
rics are collected ibr each dialogue, and the differ- 
ent versions are statistically compared (Danieli and 
Gerbino, 1995; Sanderman et al, 1998). Due to the 
costs of experimentation, only a few global strategies 
are typically explored in any one experiment. 
However, recent work has suggested that dialogue 
strategy can be designed using tile formalism of 
Markov decision processes (MDPs) and the algo- 
rithms of RL (Biermann and Long, 1996; Levin et 
al., 2000; Walker et nl., 1998; Singh et al, 1999). 
More specifically, the MDP formalism suggests a 
method for optimizing dialogue strategies from sam- 
ple dialogue data. The main advantage of this ap- 
proach is the 1)otential tbr computing an optilnal di- 
alogue strategy within a much larger search space, 
using a relatively small nmnber of training dialogues. 
This paper presents an application of RL to the 
502 
problem of oi)timizing dialogue strategy selection in 
the NJFnn system, and exl)erimentally demonstrates 
the utility of the ~l)proach. Section 2 exl)lahls how 
we apply RL to dialogue systems, then Se('tion 3 
describes t.he NJFun system in detail. Section 4 dee 
scribes how NJFun optimizes its dialogue strategy 
from experimentally obtained dialogue data. Sec- 
tion 5 reports results from testing the learned strat- 
egy demonstrating that our al)l)roach improves task 
coml)letion rates (our chosen measure for 1)erfor- 
mance optimization). A conll)alliOll paper provides 
only an al)brevi~tted system and dialogue manager 
description, but includes additional results not pre- 
sented here (Singh et al, 2000), such as analysis es- 
tablishing the veracity of the MDP we learn, and 
comparisons of our learned strategy to strategies 
hand-picked by dialogue xperts. 
2 Reinforcement Learning for 
Dialogue 
Due to space limitations, we 1)resent only a 1)rief 
overview of how di~dogue strategy optimization can 
be viewed as an llL 1)roblem; for more details, 
see Singh ctal .  (\]999), Walker el; a.1. (\]998), Levin 
et al (2000). A dialogue strategy is a mapl)ing h'om 
a set ot! states (which summarize the entire dialogue 
so far) to a set of actions (such as the system's utter- 
mines and database queries). There are nmltil)l(~ rea- 
sonable action choices in each state; tyl)ically these 
choices are made by the system designer. Our RL- 
I)ased at)l)roach is to build a system that explores 
these choices in a systematic way through experi- 
ments with rel)resentative human us(!rs. A scalar 
i)erf()rmanee l l leasllre, called a rewal'd, is t h(m (;al- 
eulated for each Cxl)erimental diMogue. (We dis- 
cuss various choices for this reward measure later, 
but in our experiments only terminal dialogue states 
ha,re nonzero  rewi-l,rds, s l id  the reward lneasul'(}s arc  
quantities directly obtMnable from the experimental 
set-up, such as user satisfaction or task coml)letion. ) 
This experimental data is used to construct an MDP 
which models the users' intera(:tion with the system. 
The l)roblem of learning the best dialogue strategy 
from data is thus reduced to computing the optimal 
policy tbr choosing actions in an MDP - that is, the 
system's goal is to take actions so as to maximize 
expected reward. The comput~ttion of the ol)timal 
policy given the MDP can be done etficiently using 
stan&trd RL algorithms. 
How do we build the desired MDP from sample 
dialogues? Following Singh et al (1999), we can 
view a dialogue as a trajectory in the chosen state 
space determined by the system actions and user 
resl) onses: 
S1 -~a l , r l  '5'2 --}a~,rs 83 "-~aa,ra " '"  
Here si -%,,,.~ si+l indicates that at the ith ex- 
change, the system was in state si, executed action 
ai, received reward ri, and then the state changed 
to si+~. Dialogue sequences obtained froln training 
data can be used to empirically estimate the transi- 
tion probabilities P(.s"la', a) (denoting the probabil- 
ity of a mmsition to state s', given that the system 
was in state .s and took ;ration a), and the reward 
function R(.s, (t). The estilnated transition 1)tel)abil- 
ities and rewi~rd flmction constitute an MDP model 
of the nser population's interaction with the system. 
Given this MDP, the exl)ected cumnlative reward 
(or Q-value) Q(s, a) of taking action a from state s 
can be calculated in terms of the Q-wdues of succes- 
sor states via the following recursive quation: 
Q(.% = ,) + r(,,'l,% ,)n,a:,: Q(,,", , ' ) .  
t ;  ! 
These Q-values can be estimated to within a desired 
threshold using the standard RL value iteration al- 
gorithm (Sutton, 1.991.), which iteratively updates 
the estimate of Q(s, a) based on the current Q-vahms 
of neighboring states. Once value iteration is con> 
pleted, the optima\] diah)gue strategy (according to 
our estimated model) is obtained by selecting the 
action with the maximum Q-value at each dia.logue 
state. 
While this apl)roach is theoretically appealing, the 
cost of obtaining sample human dialogues makes it 
crucial to limit the size of the state space, to mini- 
mize data sparsity problems, while retaining enough 
information in the state to learn an accurate model. 
Our approad~ is to work directly in a minimal but 
carefully designed stat;e space (Singh et al, 1999). 
The contribution of this paper is to eml)irically 
vMi(tate a practical methodology for using IlL to 
build a dialogue system that ol)timizes its behav- 
ior from dialogue data. Our methodology involves 
1) representing a dialogue strategy as a mapl)il~g 
fronl each state in the chosen state space S to a 
set of dialogue actions, 2) deploying an initial trah> 
ing system that generates exploratory training data 
with respect o S, 3) eonstrncting an MDP model 
from the obtained training data, 4) using value iter- 
ation to learn the optimal dialogue strategy in the 
learned MDP, and 4) redeploying the system using 
the learned state/~mtion real)ping. The next section 
details the use of this methodology to design the 
NJFun system. 
3 The N JFun  System 
NJFnn is a real-time spoken dialogue system that 
provides users with intbrmation about things to do 
in New Jersey. NJFun is built using a general pur- 
pose 1)latt'ornl tbr spoken dialogue systems (Levin 
et al, 1.999), with support tbr modules tbr attto- 
rustic speech recognition (ASI/.), spoken language 
503 
Action Prompt 
Greets Welcome to NJIqm. Please say an activity name or say 'list activities' for a list of activities I know 
about. 
GreetU \Velcome to NdPun. How may I help you? 
ReAsklS Iknow about amusement parks, aquariums, cruises, historic sites, museums, parks, theaters, 
wineries, and zoos. Please say an activity name from this list. 
ReAsklM Please tell me the activity type.You can also tell me the location and time. 
Ask2S Please say the name of the town or city that you are interested in. 
Ask2U Please give me more information. 
ReAsk2S Please teli me the name of the town or city that you are interested in. 
ReAsk2M Please tell me the location that you are interested in. You call also tell me the time. 
Figure 2: Sample initiative strategy choices. 
understanding, text-to-speech (TTS), database ac- 
cess, and dialogue management. NJFnn uses a 
speech recognizer with stochastic language and un- 
derstanding models trained from example user ut- 
terances, and a TTS system based on concatena- 
tive diphone synthesis. Its database is populated 
from the nj .  on l ine  webpage to contain information 
about activities. NJFun indexes this database using 
three attributes: activity type, location, and time of 
day (which can assume values morning, afternoon, 
or evening). 
hffornmlly, the NJFun dialogue manager sequen- 
tially queries the user regarding the activity, loca- 
tion and time attributes, respectively. NJFun first 
asks the user for the current attribute (and 1)ossibly 
the other attributes, depending on the initiative). 
If the current attribute's value is not obtained, N.J- 
Fun asks for the attrilmte (and possibly the later 
attributes) again. If NJFun still does not obtain 
a value, NJFun moves on to the next attribute(s). 
Whenever NJFun successihlly obtains a value, it 
can confirm the vMue, or move on to the next at- 
tribute(s). When NJFun has finished acquiring at- 
tributes, it queries the database (using a wildcard 
for each unobtained attribute value). The length of 
NJFun dialogues ranges from 1 to 12 user utterances 
before the database query. Although the NJFun di- 
alogues are fairly short (since NJFun only asks for 
an attribute twice), the information access part of 
the dialogue is similar to more complex tasks. 
As discussed above, our methodology for using RL 
to optimize dialogue strategy requires that all poten- 
tim actions tbr each state be specified. Note that at 
some states it is easy for a human to make the cor- 
rect action choice. We made obvious dialogue strat- 
egy choices in advance, and used learIfing only to 
optimize the difficult choices (Walker et al, 1998). 
Ill NJFun, we restricted the action choices to 1) the 
type of initiative to use when asking or reasking for 
an attribute, and 2) whether to confirm an attribute 
value once obtained. The optimal actions may vary 
with dialogue state, and are subject o active debate 
in the literature. 
Tile examples in Figure 2 show that NJFun can 
ask the user about the first 2 attributes I using three 
types of initiative, based on the combination of tile 
wording of the system prompt (open versus direc- 
tive), and the type of grammar NJFun uses during 
ASR (restrictivc versus non-restrictive). If NJFun 
uses an open question with m~ unrestricted gram- 
mar, it is using v.scr initiative (e.g., Greet\[l). If N J- 
Fun instead uses a directive prompt with a restricted 
grammar, the system is using systcm initiative (e.g., 
GreetS). If NJFun uses a directive question with a 
non-restrictive granmlar, it is using mizcd initiative, 
because it allows the user to take the initiative by 
supplying extra intbrnlation (e.g., ReAsklM). 
NJFun can also vary the strategy used to confirm 
each attribute. If NJFun asks the user to explicitly 
verify an attribute, it is using czplicit confirmation 
(e.g., ExpConf2 for the location, exemplified by $2 
in Figure 1). If NJFun does not generate any COlt- 
firnmtion prompt, it is using no confirmation (the 
NoConf action). 
Solely tbr the purposes of controlling its operation 
(as opposed to the le~trning, which we discuss in a 
moment), NJNm internally maintains an opcratio'ns 
vector of 14 variables. 2 variables track whether the 
system has greeted the user, and which attribute 
the system is currently attempting to obtain. For 
each of the 3 attributes, 4 variables track whether 
the system has obtained the attribute's value, the 
systent's confidence in the value (if obtained), the 
number of times the system has asked the user about 
the attribute, and the type of ASR grammar most 
recently used to ask for the attribute. 
The formal state space S maintained by NJFun 
for tile purposes of learning is nmch silnt)ler than 
the operations vector, due to the data sparsity con- 
cerns already discussed. The dialogue state space 
$ contains only 7 variables, as summarized in Fig- 
sire 3. S is computed from the operations vector us- 
ing a hand-designed algorithm. The "greet" variable 
1 "Greet" is equivalent toasking for the first attribute. N J- 
Fun always uses system initiative for the third attribute, be- 
cause at that point the user can only provide the time of (lay. 
504 
greet attr conf val times gram hist \] 
0,1 1,2,3,4 0,1,2,3,4 0,1 0,1,2 0,1 0,1 \] 
Figure 3: State features and vahles. 
tracks whether tile system has greeted tile user or  
not (no=0, yes=l). "Attr ~: specifies which attrihute 
NJFun is ('urrently ~tttelnpting to obtain or ver- 
ify (activity=l, location=2, time=a, done with at- 
tributes=4). "Conf" tel)resents the confidence that 
NaFun has after obtaining a wdue for an attribute. 
The values 0, 1, and 2 represent the lowest, middle 
and highest ASR confidence vahms? The wdues 3 
and 4 are set when ASR hears "yes" or "no" after a 
confirmation question. "Val" tracks whether NaFun 
has obtained a value, for tile attribute (no=0, yes=l). 
"Times" tracks the number of times that N,lFun has 
aske(1 the user ~d)out he attribute. "(4ram" tracks 
the type of grammar most ree(mtly used to obtain 
the attribute (0=non-restrictive, 1=restrictive). Fi- 
nally, "hist" (history) represents whether Nalflm had 
troullle understanding the user ill the earlier p~trt of 
the conversation (bad=0, good=l).  We omit the full 
detinition, but as a,n ex~unl>le , when N.lFun is work- 
ing on the secon(1 attribute (location), the history 
variable is set to 0 if NJFun does not have an ac- 
tivity, has an activity but has no confidence in the 
value, or needed two queries to obtain the activity. 
As mentioned above, the goal is to design a small 
state space that makes enough critical distin('tions to 
suPi)ort learning. The use of 6" redu(:es the mmfl)er 
of states to only 62, and SUl)l)orts the constru('tion of
mt MI)P model that is not sparse with respect o ,g, 
even using limite(1 trMning (btta. :~ Tit(.' state sp~t(;e 
that we utilize here, although minimal, allows us 
to make initiative decisions based on the success of 
earlier ex(:ha.nges, and confirmation de(:isions based 
on ASR. confidence scores and gralnmars. 
'.Phe state/~t('tiol: real)ping r(-`l)resenting NaFun's 
initial dialogue strategy EIC (Explor:ttory for Ini- 
tiative and Confirmation) is in Figure 4. Only the 
exploratory portion of the strategy is shown, namely 
those states for which NaFun has an action choice. 
~klr each such state, we list tile two choices of actions 
available. (The action choices in boldfime are the 
ones eventually identified as el)ritual 1)y the learning 
process, an(1 are discussed in detail later.) The EIC 
strategy chooses random, ly between these two ac- 
21"or each uttermme, the ASH. outfmt includes 11o|, only the 
recognized string, but also aIl asso(:ia.ted acoustic (:onJld(mce 
score, iBased on data obtaintM dm'ing system deveJolmmnt , 
we defined a mapl)ing from raw confidence, values into 3 ap- 
proximately equally potmlated p~rtitions. 
362 refers to those states that can actually occur in a di- 
alogue. \[<)r example, greet=0 is only possible in the initial 
dialogue state "0 1 0 0 0 0 0". Thus, all other states beginning 
with 0 (e.g. "0 I 0 0 I 0 0") will never occur. 
g 
1 0 0 (} 0 0 
1 1 0 (} 1 0 0 
1 1 0 1 0 0 0 
1 1 0 1 0 1 0 
1 1 1 1 0 0 0 
1 1 1 1 0 \] 0 
1 1 2 1 0 0 0 
1 1 2 1 0 1 0 
1 1 4 0 0 0 0 
\] 1 4 (} 1 0 0 
St~tte Action Choices 
a c v t g it 
1 2 0 0 0 0 0 
1 2 0 0 0 0 1 
1 2 (} 0 1 (} 0 
1 2 0 (} 1 0 1 
1 2 0 1 0 (} 0 
1 2 0 \] 0 0 1 
1 2 0 1 0 1 0 
1 2 0 1 0 1 1 
1 2 1 1 0 (1 0 
1 2 1 \] 0 0 1 
1 2 1 1 0 1 0 
\] 2 1 1 0 \] 1 
1 2 2 1 0 0 0 
1 2 2 \] 0 0 1 
1 2 2 \] 0 1 0 
\] 2 2 1 0 1 1 
1 2 4 0 0 (1 0 
1 2 4 0 0 0 1 
1 2 4 0 1 0 0 
1 2 4 0 1 0 1 
1 3 () 1 () () 0 
\] 3 0 1 0 0 1 
1 3 (1 1 0 1 0 
1 3 \[) 1 0 \] \] 
1. 3 1 1 0 0 0 
1 3 1 1 0 0 1 
1 3 1 1. 0 1 0 
1 3 1 1 0 \] 1 
1 3 2 1 0 0 0 
1 3 2 1 0 0 1 
1 3 2 1 0 \] 0 
\] 3 2 1 0 1 1 
GreetS,GreetU 
ReAsklS,ReAsklM 
NoConf, ExpConf l  
NoConf, ExpColffl 
NoCont, Exp Confl  
NoConf, ExpConf l  
NoConf~ExpConfl 
NoConf, ExpConfl 
ReAsklS,ReAsklM 
ReAsklS,RcAsklM 
Ask2S,Ask2U 
Ask2S,Ask2U 
I{eAsk2S,ReAsk2M 
ReAsk2S,ReAsk2M 
NoConf, ExpCont'2 
NoConf, ExpConP2 
NoConf~ExpCont)2 
NoConf,ExpConf2 
NoCoiff, Exp C oaF2 
NoConf, ExpConf2 
NoConf, ExpCont2 
NoConf,ExpCong2 
NoConf~ExpConf2 
NoConf, ExpConf2 
NoConf, ExpConf2 
NoConf, ExpCon\[2 
ReAsk2S,RcAsk2M 
I/.eAsk2S,ReAsk2M 
RcAsk2S,ReAsk2M 
ReAsk2S,ReAsk2M 
NoConf, ExpCon\[3 
NoConf, Exl) Conf3 
NoConf, ExpConf3 
NoConf,Ext)ConF3 
NoConf, ExpCont~ 
NoConf, ExpConf3 
NoConf, ExpConf3 
NoConf,ExpConF3 
NoConf, ExpConi~J 
NoConf, ExpConf3 
NoColff, ExpConf3 
NoConf, ExpConf3 
Figure 4: Exploratory portion of EIC strategy. 
tions in the indicated state, to maximize xploration 
and minimize data sparseness when constructing our 
model. Since there are 42 states with 2 choices each, 
there is n search space of 242 potential global di- 
alogue strategies; the goal of RL is to identify an 
apparently optimal strategy fl'om this large search 
space. Note that due to the randomization of the 
EIC strategy, the prompts are designed to ensure 
the coherence of all possible action sequences. 
Figure 5 illustrates how the dialogue strategy in 
Figure 4 generates the diMogue in Figure 1. Each 
row indicates the state that NJFun is in, the ac- 
505 
State Action %Irn Reward 
gacvtgh  
0100000 GreetU $1 0 
1121000 NoConf 0 
1221001 ExpConf2 $2 0 
1 3 2 1 0 0 1 ExpConf3 $3 0 
1400000 Tell $4 1 
Figure 5: Generating the dialogue in Figure 1. 
tion executed in this state, the corresponding turn 
in Figure 1, and the reward received. The initial 
state represents that NaFun will first attempt o ob- 
tain attribute 1. NJFun executes GreetU (although 
as shown in Figure 4, GreetS is also possible), gen- 
erating the first utterance in Figure 1. Alter the 
user's response, the next state represents that N J- 
Fun has now greeted the user and obtained the ac- 
tivity value with high confidence, by using a non- 
restrictive grmnmar. NJFnn then chooses the No- 
Conf strategy, so it does not attempt to confirm 
the activity, which causes the state to change but 
no prompt to be generated. The third state repre- 
sents that NJFun is now working on the second at- 
tribute (location), that it already has this vahle with 
high confidence (location was obtained with activity 
after the user's first utterance), and that the dia- 
logue history is good. 4 This time NaFun chooses the 
ExpConf2 strategy, and confirms the attribute with 
the second NJFun utterance, and the state changes 
again. The processing of time is similar to that of lo- 
cation, which leads NJFun to the final state, where it 
performs the action "Tell" (corresponding to query- 
ing the database, presenting the results to the user, 
and asking the user to provide a reward). Note that 
in NJFun, the reward is always 0 except at the ter- 
minal state, as shown in the last column of Figure 5. 
4 Experimental ly Optimizing a 
Strategy 
We collected experimental dialogues for both train- 
ing and testing our system. To obtain training di- 
alogues, we implemented NJFun using the EIC dia- 
logue strategy described in Section 3. We used these 
dialogues to build an empirical MDP, and then com- 
puted the optimal dialogue strategy in this MDP (as 
described in Section 2). In this section we describe 
our experimental design and the learned dialogue 
strategy. In the next section we present results from 
testing our learned strategy and show that it im- 
proves task completion rates, the performance mea- 
sure we chose to optimize. 
Experimental subjects were employees not associ- 
a, ted with the NJFun project. There were 54 sub- 
4Recall that only the current attribute's features are ill the 
state, lIowever, the operations vector contains information 
regarding previous attributes. 
jects for training and 21 for testing. Subjects were 
distributed so tile training and testing pools were 
balanced for gender, English as a first language, and 
expertise with spoken dialogue systems. 
During both training and testing, subjects carried 
out free-form conversations with NJFun to complete 
six application tasks. For examl)le , the task exe- 
cuted by the user in Figure 1 was: "You feel thirsty 
and want to do some winetasting in the morning. 
Are there any wineries (;lose by your house in Lam- 
bertville?" Subjects read task descriptions on a web 
page, then called NJFun from their office phone. 
At the end of the task, NJFun asked for feedback 
on their experience (e.g., utterance $4 in Figure 1). 
Users then hung up the phone and filled out a user 
survey (Singh et al, 2000) on the web. 
The training phase of the experiment resulted in 
311 complete dialogues (not all subjects completed 
all tasks), for which NJFun logged the sequence 
of states and the corresponding executed actions. 
The number of samples per st~tte for the initi~fl ask 
choices are: 
0 1 0 0 0 0 0 GreetS=IS5 GreetU=156 
1 2 0 0 0 0 0 Ask2S=93 Ask2U=72 
1 2 0 0 0 0 1 Ask2S=36 Ask2U=48 
Such data illustrates that the random action choice 
strategy led to a fairly balanced action distribution 
per state. Similarly, the small state space, and the 
fact that we only allowed 2 action choices per state, 
prevented a data sparseness problem. The first state 
in Figure 4, the initial state for every dialogue, was 
the most frequently visited state (with 311 visits). 
Only 8 states that occur near the end of a dialogue 
were visited less tlmn 10 times. 
The logged data was then used to construct he 
empirical MDP. As we have mentioned, the measure 
we chose to optinfize is a binary reward flmction 
based on the strongest possible measure of task com- 
pletion, called S t rongComp,  that takes on value 
1 if NJFun queries the database using exactly the 
attributes pecified in the task description, and 0 
otherwise. Then we eoml)uted the optimal dialogue 
strategy in this MDP using RL (cf. Section 2). The 
action choices constituting the learned strategy are 
in boldface in Figure 4. Note that no choice was 
fixed for several states, inealfing that the Q-values 
were identical after value iteration. Thus, even when 
using the learned strategy, NJFun still sometimes 
chooses randomly between certain action pairs. 
Intuitively, the learned strategy says that the op- 
timal use of initiative is to begin with user initia- 
tive, then back off to either mixed or system ini- 
tiative when reasking for an attribute. Note, how- 
ever, that the specific baekoff method differs with 
attribute (e.g., system initiative for attribute 1, but 
gcnerMly mixed initiative for attribute 2). With 
respect to confirmation, the optimal strategy is to 
506 
mainly contirm at lower contidenee -values. Again, 
however, the point where contirlnation becomes un- 
necessary difl'ers across attributes (e.g., confidence 
level 2 for attribute 1, but sometimes lower levels 
for attributes 2 and 3), and  also dt!txmds on other 
features of the state besides confidence (e.g., gram- 
mar and history). This use (if ASP, (:ontidence. by the 
dialogue strategy is more Sol)hisli('ated than previ- 
ous al)proaches, e.g. (Niimi and Kot)ayashi, 1996; 
Lit\]nan and Pan, 2000). N.lI,'un ('an learn such line- 
grained distinctions l}ecause the el)ritual strategy is 
based on a eonll)arisoi) of 24~ l}ossible exl}h)ratory 
strategies. Both the initiative and confirmation re- 
suits sugge.sl that the begimfing of the dialogue was 
the most problenmtie for N.lli'un. Figure I ix an ex- 
ample dialogue using the Ol)tilnal strategy. 
5 Experimentally Evaluating the 
Strategy 
For the testing i)\]tase, NJFun was reilnplemented to
use the learned strategy. 2:t test sul)je(;Is then per- 
formed the same 6 tasks used during training, re- 
sulling in 124 complete test dialogues. ()he of our 
main resull;s is that task completion its measured by 
StrongCom 11 increased front 52cX} in training 1o 64% 
in testing (p < .06)) 
There is also a signilicant in~twaction (!II'(~c.t 
between strategy nnd task (p<.01) for Strong- 
Colnl).  \]'revious work has suggested l;hat novic(~ 
users l)erform (:Oml)arably to eXl)erts after only 2 
tasks (Kamm et ill., \] 9!18). Sill('e Ollr \]oarllt}d sl.rat- 
egy was based on 6 tasks with each user, one (?xpla- 
nation of the interaction eft'cot is that the learnc.d 
strategy is slightly optimized for expert users. ~lb 
explore this hyi)othesis, we divided our corpus into 
diah)gues with "novice" (tasks \] and 2) and "ex- 
pert" (tasks 3-6) users. We fOltltd that the learned 
strategy did in fact lc'a(l to a large an(1 significant 
improvement in StrongComp tbr (;Xl)erts (EIC=.d6, 
learned==.69, 11<.001), and a non-signilieant degra- 
dation for novices (1,31C=.66, learned=.55, 11<.3). 
An apparent limitation of these results is that EIC 
may not 1)e the best baseline strategy tbr coral)arisen 
to our learned strategy. A more standard alternative 
would be comparison to the very best hand-designed 
fixed strategy. However, there is no itgreement in the 
literature, nor amongst he authors, its to what the 
1)est hand-designed strategy might have been. There 
is agreement, however, that the best strategy ix sen- 
sitive to lnally unknown and unmodeled factors: the 
aThe ('.xlmrimental design (lescribed above Colmists of 2 
factors: the within-in groul) fa(:tor sl~ntefly aim the l)etween- 
groui)s facl;or task. \,Ve 11812, ~1, l,WO-~,g~l,y D.llO.ly,qiS of variance 
(ANOVA) to comtmte wlmtlmr main and int(!raction (!flk!cts 
of strategy are statistically signitica nt (t)<.05) or indicative 
of a statistical trend (p < .101. Main effe.cts of strategy are 
task-in(lel)endent , while interaction eIt'(!cts involving strat(%y 
are task-dependent. 
~4(~aSIlIX~ 
StrongComp 
\VcakComp 
ASR 
Fecdlmck 
UserSat 
EIC 
(n=:3111 
0.52 
1.75 
2.50 
0.18 
1.3.38 
v _ _  
l~eatned p 
(n=124) 
0.64 
2.19 .02 
2.67 .04 
0.11 .d2 
13.29 .86 
Table 1: Main ett'ects of dialogue strategy. 
user 1)olmlation, the specitics of the, task, the 1)ar- 
ticular ASR used, etc. Furthernlore, \]P, IC was (:are- 
fully designed so that the random choices it makes 
never results in tm unnatural dialogue. Finally, a 
companion paper (Singh et al, 2000) shows that the 
1)erforntanee of the learned strategy is better tha l l  
several "stmtdard" fixed strategies (such as always 
use system-initiative and no-confirmation). 
Although many types of measures have been used 
to evaluate dialogue systems (e.g., task success, 
dialogue quality, ettit:ieney, usability (l)anieli and 
Gerbino, 1995; Kamm et al, 11998)), we optimized 
only tbr one task success measure, StrongConll). 
Ilowever, we also examined the 1)erl 'ornmnee of the 
learned strategy using other ewduation measures 
(which t)ossibly could have llo011 used its our  reward 
function). WeakComp is a relaxed version of task 
comt)letion that gives partial credit: if all attribute 
values are either correct or wihh:ards, the value is the 
sum of the correct munl)er of attrilmtes. ()tlmrwise, 
at least one attribute is wrong (e.g., the user says 
"Lanfl)ertvilhf' but the system hears "Morristown"), 
and the wdue is -1. ASR is a dialogue quality lllea- 
sure that itl)l)roxinmtes Sl)eech recognition act:uracy 
for tl,e datM)ase query, a.nd is computed 1:) 3, adding 
1 for each correct attribute value altd .5 for every 
wihtca.rd. Thus, if the task ix to go winetasting 
near Lambertville in the morning, and the systenl 
queries the database for an activity in New Jersey 
in the morning, StrongComp=0, \VeakComp=l, and 
ASR=2. In addition to the objective measures dis- 
cussed a,bove, we also COmlmted two subjective us- 
ability measures. Feedback  is obtained front the 
dialogue (e.g. $4 in Figure 5), by mapping good, 
so-so, bad to 1, 0, m~d -1, respectively. User satis- 
faction (UserSat, ranging front 0-20) is obtained by 
summing the answers of the web-based user survey. 
Table I summarizes the diflhrence in performance 
of NJFun tbr our original reward flmction and the 
above alternative valuation measures, from trail> 
ing (EIC) to test (learned strategy for StrongComp). 
For WeakComp, the average reward increased from 
1.75 to 2.19 (p < 0.02), while tbr ASll the average 
reward increased from 2.5 to 2.67 (p < 0.04). Again, 
these iml)rovements occur even though the learned 
strategy was not optilnized for these measures. 
The last two rows of the table show that for the 
507 
subjective measures, i)erformmme does not signifi- 
cantly differ for the EIC and learned strategies. In- 
terestingly, the distributions of the subjective mea- 
sures move to the middle from training to testing, 
i.e., test users reply to the survey using less extreme 
answers than training users. Explaining the subjec- 
tire results is an area for future work. 
6 Discussion 
This paper presents a practical methodology for ap- 
plying RL to optimizing dialogue strategies in spo- 
ken dialogue systems, and shows empirically that the 
method improves performance over the EIC strategy 
in NJFun. A companion paper (Singh et al, 2000) 
shows that the learned strategy is not only better 
than EIC, but also better than other fixed choices 
proposed in the literature. Our results demonstrate 
that the application of RL allows one to empirically 
optimize a system's dialogue strategy by searching 
through a much larger search space than can be ex- 
plored with more traditional lnethods (i.e. empiri- 
cally testing several versions of a systent). 
RL has been appled to dialogue systems in pre- 
vious work, but our approach ditlhrs from previous 
work in several respects. Biermann and Long (1996) 
did not test RL in an implemented system, and the 
experiments of Levin et 31. (2000) utilized a simu- 
lated user model. Walker et al (1998)'s methodol- 
ogy is similar to that used here, in testing RL with 
an imt)lelnented system with human users. However 
that work only explored strategy choices at 13 states 
in the dialogue, which conceivably could have been 
explored with more traditional methods (~ts com- 
pared to the 42 choice states explored here). 
We also note that our learned strategy made di- 
alogue decisions based on ASR confidence in con- 
junction with other features, mid alto varied initia- 
tive and confirmation decisions at a finer grain than 
previous work; as such, our learned strategy is not; 
a standard strategy investigated in the dialogue sys- 
teln literature. For example, we would not have pre- 
dicted the complex and interesting back-off strategy 
with respect o initiative when reasking for an at- 
tribute. 
To see how our method scales, we are al)plying RL 
to dialogue systems for eustolner care and tbr travel 
planning, which are more complex task-oriented do- 
mains. As fllture work, we wish to understand 
the aforementioned results on the subjective reward 
measures, explore the potential difference between 
optimizing tbr expert users and novices, automate 
the choice of state space for dialogue systems, ilwes- 
tigate the use of a learned reward function (Walker 
et al, 1998), and explore the use of more informative 
non-terminal rewards. 
Acknowledgements 
The authors thank Fan Jiang for his substantial effort 
in implenmnting NJFun, Wieland Eckert, Esther Levin, 
Roberto Pieraccini, and Mazin R.ahinl for their technical 
help, Julia Hirsehberg for her comments on a draft of this 
paper, and David McAllester, I~ichard Sutton, Esther 
Levin and Roberto Pieraccini for hell)tiff conversations. 
References 
A. W. Biermann and P. M. Long. 1996. The composition 
of messages in sl)eeeh-graphies interactive systems. In 
Proe. of the International Symposium on Spoken Dia- 
logue, pages 97 100. 
M. Danieli and E. Gerbino. 1995. Metrics for evaluating 
dialogue strategies in a spoken language system. In 
P~vc. of the AAAI  Spring Symposium on Empirical 
Methods in Discourse Interpretation and Generation, 
pages 34 39. 
C. Kamm, D. Litman, and M. A. Walker. 1998. From 
novice to expert: The effect of tutorials on user exl)er- 
tise with spoken dialogue systems. In P~vc. of the In- 
ternational Conference on Spolccn Language P~vccss- 
in.q, ICSLP98. 
E. Levin, R. Pieraccini, W. Eekere, G. Di Fabbrizio, and 
S. Narayanan. 1999. Spoken language dialogue: lh'om 
theory to practice. In Pwc. IEEE Workshop on Au- 
tomatic Speech R.ecognition and Understanding, AS- 
R U U99. 
E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochas- 
tic model of human machine interaction for learning 
dialog strategies. IEEE TTnnsactions on Speech and 
Audio Processing, 8(1):11-23. 
D. J. Litman and S. Pan. 2000. Predicting and adapting 
to poor Sl)eech recognition in a spoken dialogue sys- 
tern. In Proc. of the Scv('ntccnth National Confl:rcncc 
on Artificial Intclligcncc, AAAI-2000. 
Y. Niimi and Y. Kobayashi. 1996. A dialog control strat- 
egy based on the reliability of speech recognition. In 
Proc. of the International Symposium on Spoken Dia- 
loguc, pages 157--160. 
A. Sanderman, J. Sturm, E. den Os, L. Boves, and 
A. Cremers. 1998. Evaluation of the dutchtrain 
timetable inibrmation system developed in the arise 
project. In Interactive Voice Technology for Tclccom- 
munications Applications, IVT2'A, pages 91-96. 
S. Singh, M. S. Kearns, D. J. Litman, and M. A. \Valker. 
1999. Reinforcement learning for spoken dialogue sys- 
tems. In Proc. NIPS99. 
S. B. Singh, M. S. Kearns, D. J. Litman, and 
M. A. Walker. 2000. Empirical evaluation of a rein- 
forccment learning spoken dialogue system. In Proc. 
of thc Scvcntccnth National Conference on Artificial 
Intelligence, AAAI-2000. 
R. S. Sutton. 1991. Plamfing by incremental dynamic 
programming. In Proc. Ninth Confcwztcc on Machine 
Learning, pages 353-357. 
M. A. Walker, J. C. Promer, and S. Narayanan. 1998. 
Learning optimal dialogue strategies: A ease study of 
a Sl)oken dialogue agent br email. In P~vc. of the 36th 
Annual Meeting of the Association of Computational 
Linguistics, COLING//ACL 98, pages 1345 1352. 
508 
NJFun: A Reinforcement Learning Spoken Dialogue System 
Diane  L i tman,  Sat inder  S ingh ,  M ichae l  Kearns  and  Mar i l yn  Walker  
AT&T Labs - -  Research 
180 Park  Avenue 
F lo rham Park,  NJ  07932 USA 
{diane,bavej  a,mkearns,walker} @research.att .com 
Abst rac t  
This paper describes NJFun, a real-time spoken dia- 
logue systemthat-provides users with information 
about things to d~ in New Jersey. NJFun auto- 
matically optimizes its dialogue strategy over time, 
by using a methodology for applying reinforcement 
learning to a working dialogue system with human 
users .  
1 In t roduct ion  
Using the formalism of Markov decision processes 
(MDPs) and the algorithms of reinforcement learn- 
ing (RL) has become a standard approach to many 
AI problems that involve an agent learning to 
optimize reward by interaction with its environ- 
ment (Sutton and Barto, 1998). We have adapted 
the methods of RL to the problem of automatically 
learning a good dialogue strategy in a fielded spoken 
dialogue system. Here is a summary of our proposed 
methodology for developing and evaluating spoken 
dialogue systems using R.L: 
? Choose an appropriate reward measure for di- 
alogues, and an appropriate representation for 
dialogue states. 
? Build an initial state-based training system that 
creates an exploratory data set. Despite being 
exploratory, this system should provide the de- 
sired basic functionality. 
? Use these training dialogues to build an empir- 
ical MDP model on the state space. 
? Compute the optimal dialogue policy according 
to this MDF, using RL. 
? Reimplement the system using the learned dia- 
logue policy. 
In this demonstration session paper, we briefly de- 
scribe our system, present some sample dialogues, 
and summarize our main contributions and limita- 
tions. Full details of our work (e.g. our reinforce- 
ment learning methodology, analysis establishing the 
veracity of the MDP we learn, a description of an 
experimental evaluation of NJFun, analysis of our 
learned ialogue strategy) can be found in two forth- 
coming technical papers (Singh et al, 2000; Litman 
et al, 2000). 
2 The  N JFun  System 
NJFun is a reM-time spoken dialogue system that 
provides users with information about things to do 
in New Jersey. 1 An example dialogue with NJFun is 
shown in Figure 1. NJFun is built using an internal 
platform for spoken dialogue systems. NJFun uses 
a speech recognizer with stochastic language models 
trained from example user utterances, and a TTS 
system based on concatenative diphone synthesis. 
Its database is populated from the nj.  onl ine web- 
page to contain information about activities. NJFun 
indexes this database using three attributes: activity 
type, location, and time of day. 
Informally, the NJFun dialogue manager sequen- 
tially queries the user regarding the activity, location 
and time attributes, respectively. NJFun first asks 
the user for the current attribute (and possibly the 
other attributes, depending on the initiative). If the 
current attribute's value is not obtained, NJFun asks 
for the attribute (and possibly the later attributes) 
again. If NJFun still does not obtain a value, N J- 
Fun moves on to the next attribute(s). Whenever 
NJFun successfully obtains a value, it can confirm 
the value, or move on and attempt o obtain the 
next attribute(s)? When NJFun has finished asking 
about the attributes, it queries the database (using 
a wildcard for each unobtained attribute value). 
We use reinforcement learning (RL) to optimize 
dialogue strategy, lq.L requires that all potential ac- 
tions for each state be specified. Note that at some 
states it is easy for a human to make the correct 
action choice. We made obvious dialogue strategy 
choices in advance, and used learning only to opti- 
mize the difficult choices. In NJFun, we restricted 
the action choices to 1) the type of initiative to use 
1This system description section is taken from (Litman et 
al., 2000) .
2Note that it is possible for users to specify multiple at- 
tributes, in any order, in a single utterance. However, NJFun 
will always process multiple attributes using its predefined 
sequential ordering. 
17 
SI: Welcome to N JFun .  How may i help you? ._ 
UI :  I 'd like to find um winetas t ing  in Lambertv i l le  in the morn ing .  
(ASR output: I 'd like to f ind out wineries the in the Lambertville in the morning.) 
$2: Did you say you are interested in Lambertv i l le?  
U2: Yes. 
$3: Did you say you want  to go in the morn ing?  
U3: Yes. 
$4: I found a winery near  Lambertv i l le  that  is open in the morn ing .  It is \ [ . . .  \] 
P lease give me feedback by say ing 'good ' ,  'so-so',  or  'bad ' .  
U4: Good.  
Figure 1: An example dialogue with NJFun (after optimization via RL). 
Act ion  
GreetS 
GreetU  
ReAsk 1 S 
ReAsk  1 M 
Ask2S 
Ask2U 
ReAsk2S 
ReAsk2~ 
m 
Welcome to N JFun.  P lease say an act iv i ty  name or say 'l ist act iv i t ies '  for a list of act iv i t ies I know about .  
Welcome to N JFun.  How may I help you?  
I know about  amusement  parks ,  aquar iums,  cruises, histor ic sites, museums,  parks ,  theaters ,  wineries,  
and  zoos. P lease say an act iv i ty  name f rom this  list. 
P lease tell me the act iv i ty  type.You can  also tell me the locat ion  and time. 
P lease say the name of the town or city that  you are interested in. 
P lease give me more in format ion.  
P lease tell me the name of the town or c i ty  that  you are interested in. 
"P lease  tell me the locat ion that  you are interested in. You can  also tell me the t ime. 
Figure 2: Sample initiative strategy choices. 
when asking or reasking for an attribute, and 2) 
whether to confirm an attribute value once obtained. 
The optimal actions may vary with dialogue state, 
and are subject o active debate in the literature. 
The examples in Figure 2 shows that NJFun can 
ask the user about the first 2 attributes 3 using three 
types of initiative, based on the combination of the 
wording of the system prompt (open versus direc- 
tive), and the type of grammar NJFun uses during 
ASR (restrictive versus non-restrictive). If NJFun 
uses an open question with an unrestricted gram- 
mar, it is using user initiative (e.g., GreetU). If N J- 
Fun instead uses a directive prompt with a restricted 
grammar, the system is using system initiative (e.g., 
GreetS). If NJFun uses a directive question with a 
non-restrictive grammar, it is using mixed initiative, 
because it is giving the user an opportunity to take 
the initiative by supplying extra information (e.g., 
ReAsklM). 
NJFun can also vary the strategy used to confirm 
each attribute. If NJFun asks the user to explicitly 
verify an attribute, it is using explicit confirmation 
(e.g., ExpConf2 for the location, exemplified by $2 
in Figure 1). If NJFun does not generate any con- 
firmation prompt, it is using no confirmation (an 
action we call NoConf). 
Solely for the purposes of controlling its operation 
(as opposed to the learning, which we discuss in a 
moment), NJFun internally maintains an operations 
vector of 14 variables. 2variables track whether the 
system has greeted the user, and which attribute 
the system is currently attempting to obtain. For 
each of the 3 attributes, 4 variables track whether 
'~ "Greet"  is equ iva lent  to  ask ing  for the f i rs t  a t t r ibute .  N J -  
Fun  a lways  uses  sys tem in i t ia t ive  fo r  the  th i rd  a t t r ibute ,  be -  
cause  a t  that  po in t  the  user  can  on ly  prov ide  the time of day .  
the system has obtained the attribute's value, the 
system's confidence in the value (if obtained), the 
number of times the system has asked the user about 
the attribute, and the type of ASR grammar most 
recently used to ask for the attribute. 
The formal state space S maintained by NJFun 
for the purposes of learning is much simpler than 
the operations vector, due to data sparsity concerns. 
The dialogue state space $ contains only 7 variables, 
which are summarized in Figure 3, and is easily com- 
puted from the operations vector. The "greet" vari- 
able tracks whether the system has greeted the user 
or not (no=0, yes=l). "Attr" specifies which at- 
tribute NJFun is currently attempting to obtain or 
verify (activity=l, location=2, time=3, done with 
attributes=4). "Conf" represents the confidence 
that NJFun has after obtaining a value for an at- 
tribute. The values 0, 1, and 2 represent low, 
medium and high ASR confidence. The values 3 
and 4 are set when ASR hears "yes" or "no" after a 
confirmation question. "Val" tracks whether NJFun 
has obtained avalue for the attribute (no=0, yes=l). 
"Times" tracks the number of times that NJFun has 
asked the user about the attribute. "Gram" tracks 
the type of grammar most recently used to obtain 
the attribute (0=non-restrictive, l=restrictive). Fi- 
nally, "history" represents whether NJFun had trou- 
ble understanding the user in the earlier part of the 
conversation (bad=0, good=l). We omit the full 
definition, but as an example, when NJFun is work- 
ing on the second attribute (location), the history 
variable is set to 0 if NJFun does not have an ac- 
tivity, has an activity but has no confidence in the 
value, or needed two queries to obtain the activity. 
In order to apply RL with a limited amount of 
training data, we need to design a small state space 
18 
I greet  a t t r  conf  val t imes gram history \[ 
0,1 1,2,3,4 0,1,2,3,4 0,1 0,1,2 0,1 0,1 I 
Figure 3: State features and values. 
that makes enough critical distinctions to support 
learning. The use of S yields a state space of size 
62. The state space that we utilize here, although 
minimal, allows us to make initiative decisions based 
on the success of earlier exchanges, and confirmation 
decisions based on ASR confidence scores and gram- 
mars. 
In order to learn a good dialogue strategy via RL 
we have to explore the state action space. The 
state/action mapping representing NJFun's initial 
exploratory dialog@ strategy EIC (Exploratory for 
Initiative and Confirmation) is given in Figure 4. 
Only the exploratory portion of the strategy is 
shown, namely all those states for which NJFun has 
an action choice. For each such state, we list the 
two choices of actions available. (The action choices 
in boldface are the ones eventually identified as op- 
timal by the learning process.) The EIC strategy 
chooses randomly between these two actions when in 
the indicated state, in order to maximize xploration 
and minimize data sparseness when constructing our 
model. Since there are 42 states with 2 choices 
each, there is a search space of 242 potential dia- 
logue strategies; the goal of the RL is to identify an 
apparently optimal strategy from this large search 
space. Note that due to the randomization of the 
EIC strategy, the prompts are designed to ensure 
the coherence of all possible action sequences. 
Figure 5 illustrates how the dialogue strategy in 
Figure 4 generates the dialogue in Figure 1. Each 
row indicates the state that NJFun is in, the ac- 
tion executed in this state, the corresponding turn 
in Figure 1, and the reward received. The initial 
state represents hat NJFun will first attempt to ob- 
tain attribute 1. NJFun executes GreetU (although 
as shown in Figure 4, Greets is also possible), gen- 
erating the first utterance in Figure 1. After the 
user's response, the next state represents that N J- 
Fun has now greeted the user and obtained the ac- 
tivity value with high confidence, by using a non- 
restrictive grammar. NJFun chooses not to confirm 
the activity, which causes the state to change but no 
prompt o be generated. The third state represents 
that NJFun is now working on the second attribute 
(location), that it already has this value with high 
confidence (location was obtained with activity af- 
ter the user's first utterance), and that the dialogue 
history is good. This time NJFun chooses to confirm 
the attribute with the second NJFun utterance, and 
the state changes again. The processing of time is 
similar to that of location, which leads NJFun to the 
final state, where it performs the action "Tell" (cor- 
State  
C V t g 
0 1 0 0 0 0 0 
1 1 0 0 1 0 0 
1 1 0 1 0 0 0 
1 1 0 1 0 1 0 
1 1 1 1 0 0 0 
1 1 1 1 0 1 0 
1 1 2 I 0 0 0 
1 1 2 1 0 1 0 
1 1 4 0 0 0 0 
1 1 4 0 1 0 
1 2 0 0 0 0 
1 2 0 0 0 0 
1 2 0 0 1 0 
1 2 0 0 1 0 
1 2 0 1 0 0 
1 2 0 1 0 0 
1 2 0 1 0 1 
I 2 0 I 0 I 
1 2 i 1 0 0 
1 2 1 1 0 0 
1 2 1 1 0 1 
1 2 1 1 0 1 
1 2 2 1 0 0 
1 2 2 1 0 0 
I 2 2 1 0-  1 
1 2 2 1 0 1 
I 2 4 0 0 0 
1 2 4 0 0 0 
1 2 4 0 I 0 
1 2 4 0 1 0 
1 3 0 1 0 0 
1 3 0 1 0 0 
1 3 0 1 0 1 
1 3 0 1 0 1 
1 3 1 1 0 0 
1 3 1 1 0 0 
1 3 1 1 0 1 
i 3 1 I 0 i 
1 3 2 1 0 0 
1 3 2 1 0 0 
1 3 2 1 0 1 
i 3 2 1 0 I 
Act ion  Choices  
GreetS ,GreetU  
ReAsk  1 S ,ReAsk  1 M 
NoConf ,  ExpConf l  
NoConf ,  ExpConf l  
NoConf ,ExpConf l  
NoConf ,  ExpConf l  
NoConf ,  ExpConf l  
NoConf ,  ExpConf l  
ReAsk lS ,ReAsk lM 
0 ReAsk lS ,ReAsk lM 
0 - Ask2S,Ask2U 
1 Ask2S,Ask2U 
0 ReAsk2S,ReAsk2M 
1 ReAsk2S,ReAsk2  M 
0 NoConf ,  ExpConf2  
1 NoConf, ExpConf2  
0 NoConf, ExpConf2  
1 NoConf ,  ExpConf2  
0 NoConf ,  ExpConf2  
1 NoConf ,ExpConf2  
0 NoConf ,  ExpConf2  
1 NoConf ,  ExpConf2  
0 NoConf ,  ExpConf2  
1 NoConf ,  ExpConf2  
0 NoConf ,  ExpConf2  
1 NoConf, ExpConf2 
0 ReAsk2S,ReAsk2M 
1 ReAsk2S,ReAsk2M 
0 ReAsk2S,R .eAsk2M 
1 ReAsk2S,ReAskSM 
0 NoConf ,  ExpConf3  
1 NoConf ,  ExpConf3  
0 NoConf, ExpConf3 
I NoConf, ExpConf3 
0 NoConf, ExpConf3  
1 NoConf ,  ExpConf3  
0 NoConf ,  ExpConf3  
1 NoConf ,  ExpConf3  
0 NoConf ,ExpCon: f3  
1 NoConf ,ExpConf3  
0 NoConf ,  ExpConf3  
I NoConf, ExpConf3  
Figure 4: Exploratory portion of EIC strategy. 
State  Act ion Turn Reward  
gaevtgh  
0100000 GreetU S1 0 
I 121000 NoConf  0 
1 2 2 1 0 0 1 ExpConf2  $2 0 
1 3 2 1 0 0 1 ExpConf3  $3 0 
1 4 0 0 0 0 0 Tell S4 1 
Figure 5: Generating the dialogue in Figure 1. 
responding to querying the database, presenting the 
results to the user, and asking the user to provide a 
reward). Note that in NJFun, the reward is always 
0 except at the terminal state, as shown in the last 
column of Figure 5. 
3 Sample Dialogues 
Figures 1 and 6 present two dialogues that were gen- 
erated using different dialogue strategies in the EIC 
search space. For example, although NJFun begins 
both dialogues in the first state in Figure 4, NJFun 
executes GreetU in one dialogue and GreetS in the 
other. Similarly, although the next state of NJFun 
19 
SI: Welcome to N JFun.  P lease say an act iv i ty  name or say 'list act iv i t ies '  for  a list of act iv i t ies I know about .  
U I :  I would like to visit a h istor ic  site in S tanhope this  morn ing.  
(ASR output: I would like to the zoo historic sites in stanhope historic) 
$2: Did you say you are interested in going to a zoo?  
U2: No. 
$3: I know about  amusement  parks ,  aquar iums,  cruises, histor ic sites, museums,  parks,  theaters ,  wineries, and  zoos. 
P lease say an act iv i ty  name f rom this list. 
U3: I would like to visit a h istor ic  site. (ASR output: I would like to visit historic sites) 
$4: Did you say you are interested in going to a h istor ic  site? 
U4: Yes. 
$5: P lease give me more in format ion .  
U5: (user says nothing) 
S6: P lease tell me the locat ion that  you are interested in. You can also tell me the time. 
U6: Visit a histor ic .  (ASR output: What  historic) 
$7: What  t ime of  the day  do you want  to go? 
U7: S tanhope.  (no ASR output) 
$8: Do you want  to go in the morn ing ,  in the a f ternoon,  or  in the evening? 
US: Morn ing.  
S9: Did you say you want  to go in the morn ing?  
U9: Yes. 
S l0 :  I found 11 histor ic  sites in New Jersey that  are open in the morn ing .  The  first 3 \ [ . . .  \] Would you like to  hear  more?  
UiO: No. 
$11: Thank~ou for using the system.  P lease give me feedback by say ing 'good ' ,  'so-so',  or  'bad ' .  
U I I :  Bad'. _. 
Figure 6: An example training dialogue with NJFun. 
is the same in both dialogues ("1 1 2 1 0 0 0"), the 
activity is not confirmed in the first dialogue. 
4 Contributions 
The main contribution of this work is that we 
have developed and empirically validated a practi- 
cal methodology for using RL to build a real dia- 
logue system that optimizes its behavior from dia- 
logue data. Unlike traditional approaches to learn- 
ing dialogue strategy from data, which are limited 
to searching a handful of policies, our RL approach 
is able to search many tens of thousands of dialogue 
strategies. In particular, the traditional approach 
is to pick a handful of strategies that experts in- 
tuitively feel are good, implement each policy as a 
separate system, collect data from representative hu- 
man users for each system, and then use standard 
statistical tests on that data to pick the best sys- 
tem, e.g. (Danieli and Gerbino, 1995). In contrast, 
our use of RL allowed us to explore 242 strategies 
that were left in our search space after we excluded 
strategies that were clearly suboptimal. 
An empirical validation of our approach is de- 
tailed in two forthcoming technical papers (Singh 
et al, 2000; Litman et al, 2000). We obtained 311 
dialogues with the exploratory (i.e., training) ver- 
sion of NJFun, constructed an MDP from this train- 
ing data, used RL to compute the optimal dialogue 
strategy in this MDP, reimplemented NJFun such 
that it used this learned dialogue strategy, and ob- 
tained 124 more dialogues. Our main result was 
that task completion improved from 52% to 64% 
from training to test data. Furthermore, analysis 
of our MDP showed that the learned strategy was 
not only better than EIC, but also better than other 
fixed choices proposed in the literature (Singh et al, 
2000). 
5 Limitations 
The main limitation of this effort to automate the 
design of a good dialogue strategy is that our current 
framework has nothing to say about how to choose 
the reward measure, or how to best represent dia- 
logue state. In NJFun we carefully but manually de- 
signed the state space of the dialogue. In the future, 
we hope to develop a learning methodology to auto- 
mate the choice of state space for dialogue systems. 
With respect o the reward function, our empirical 
evaluation investigated the impact of using a number 
of reward measures (e.g., user feedback such as U4 in 
Figure 1, task completion rate, ASR accuracy), and 
found that some rewards worked better than others. 
We would like to better understand these differences 
among the reward measures, investigate the use of 
a learned reward function, and explore the use of 
non-terminal rewards. 
Re ferences  
M. Danieli and E. Gerbino. 1995. Metrics for eval- 
uating dialogue strategies in a spoken language 
system. In Proceedings of the 1995 AAA1 Spring 
Symposium on Empirical Methods in Discourse 
Interpretation and Generation, pages 34-39. 
D. Litman, M. Kearns, S. Singh, and M. Walker. 
2000. Automatic optimization of dialogue man- 
agement. Manuscript submitted for publication. 
S. Singh, M. Kearns, D. Litman, and M. Walker. 
2000. Empirical evaluation of a reinforcement 
learning spoken dialogue system. In Proceedings 
of AAAI 2000. 
R. S. Sutton and A. G. Barto. 1998. Reinforcement 
Learning: An Introduction. MIT Press. 
20 
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 1?9,
Baltimore, Maryland USA, June 26 2014.
c
?2014 Association for Computational Linguistics
Computationally Rational Saccadic Control: An Explanation of Spillover
Effects Based on Sampling from Noisy Perception and Memory
Michael Shvartsman
Department of Psychology
University of Michigan
mshvarts@umich.edu
Richard L. Lewis
Department of Psychology
University of Michigan
rickl@umich.edu
Satinder Singh
Computer Science & Eng.
University of Michigan
baveja@umich.edu
Abstract
Eye-movements in reading exhibit frequency
spillover effects: fixation durations on a word are
affected by the frequency of the previous word. We
explore the idea that this effect may be an emer-
gent property of a computationally rational eye-
movement strategy that is navigating a tradeoff be-
tween processing immediate perceptual input, and
continued processing of past input based on mem-
ory. We present an adaptive eye-movement con-
trol model with a minimal capacity for such pro-
cessing, based on a composition of thresholded se-
quential samplers that integrate information from
noisy perception and noisy memory. The model
is applied to the List Lexical Decision Task and
shown to yield frequency spillover?a robust prop-
erty of human eye-movements in this task, even
with parafoveal masking. We show that spillover in
the model emerges in approximately optimal con-
trol policies that sometimes process memory rather
than perception. We compare this model with one
that is able to give priority to perception over mem-
ory, and show that the perception-priority policies
in such a model do not perform as well in a range
of plausible noise settings. We explain how the
frequency spillover arises from a counter-intuitive
but fundamental property of sequenced thresholded
samplers.
1 Introduction and overview
Our interest is in understanding how eye-
movements are controlled in service of linguis-
tic tasks involving reading?more specifically,
how saccadic decisions are conditioned on the
moment-by-moment state of incremental percep-
tual and cognitive processing. The phenomena
we are concerned with here are spillover effects,
where fixation durations on a word are affected by
linguistic properties of the prior word or words.
The specific idea we explore is that spillover ef-
fects may be emergent properties of a computa-
tionally rational control strategy that is navigating
a tradeoff between processing immediate percep-
tual input, and continued processing of past input
based on a memory of recent stimuli.
The paper is organized as follows. We first
review evidence that eye-movement control in
reading is strategically adaptive, and describe our
theoretical approach. We then review evidence
from gaze-contingent eye-tracking paradigms?
some existing and some new?that suggests that
frequency spillover is not driven exclusively by
parafoveal preview of upcoming words. We take
this as evidence that frequency spillover may be
driven in part by processing of words that con-
tinues after the eyes have moved away. We then
extend an existing adaptive control model of eye-
movements with a minimal capacity for such con-
tinued processing, by allowing it to process a
memory of past input. The model is based on
a simple composition of thresholded sequential
samplers that integrate information from noisy
perception and noisy memory. Threshold parame-
ters define the control policy and their values de-
termine how processing resources are allocated
to perception and memory. We provide a com-
putational rationality analysis of the model?s pol-
icy space: First, we show that frequency spillover
emerges in top-performing policies, where perfor-
mance is evaluated on the same task and payoff
given to human participants. Second, we show
that a model capable of spillover does no worse
than an otherwise identical model that can elim-
inate spillover by always attending to perception
when it can, and that the spillover-capable poli-
cies in such a model do no worse than spillover-
incapable ones across the speed-accuracy tradeoff
curve, and in fact do better in some portions of
the noise parameter space. Finally, we trace the
origin of the effect to a counter-intuitive but fun-
damental property of the dynamics of sequenced
thresholded samplers.
2 Adaptive control of eye-movements:
Evidence and theoretical approach
A growing body of evidence suggests that eye-
movements in reading are strategic adaptations
that manifest at the level of individual fixations.
For example, Rayner and Fischer (1996) showed
1
that when participants are searching for a partic-
ular word in a text rather than reading for full
comprehension, saccade durations are shortened
and the magnitude of frequency effects is reduced.
Wotschack (2009) showed that readers assigned
the task of proofreading read more slowly and per-
formed more second-pass reading with fewer skips
than in a control reading-for-comprehension task.
People also adapt reading behavior to within-
task manipulations of difficulty and payoff.
Wotschack (2009) showed that people change
their reading behavior in response to manipula-
tions of the difficulty of comprehension questions.
Lewis et al. (2013) showed that people adapt their
eye movements in response to changes in quanti-
tative task payoffs. Payoffs emphasizing speed at
the expense of accuracy result in shorter fixation
durations and lower accuracies.
We seek to develop a model that can explain
such variation in eye-movement behavior as a ra-
tional adaptation to the task (including utility) and
the internal oculomotor and cognitive architecture
(Lewis et al., 2013). Such a model would permit a
computational rationality analysis (Lewis et al., to
appear) because the problem of rational behavior
is defined in part by the bounded mechanisms of
the posited computational architecture.
We constrain our architectural assumptions by
building on existing theories of oculomotor archi-
tecture, such as E-Z Reader (Reichle et al., 2009).
But we enrich these architectures with explicit as-
sumptions about the policy space of saccadic con-
trol, and with assumptions about the processing of
noisy perception and memory. This enriched ar-
chitecture is then embedded in a minimal cogni-
tive system that is capable of performing a com-
plete experimental task. The complete model af-
fords computational rationality analyses because it
can be used to derive the implications of saccadic
control policies for task performance.
3 The nature of spillover effects
Our aim in this section is to establish a link be-
tween spillover and the continued processing of
past input based on memory. Consider a pair of
words in sequence: word
n?1
and word
n
. There
are three natural explanations for how the fre-
quency of word
n?1
could affect the duration of
fixations on word
n
. (1) During fixation of word
n
,
perceptual information from word
n?1
is available
in the parafovea and continues to be processed.
masked unmasked
?
??
?
?
??
?
230
250
270
290
310
high low high low
Fixated Word Frequency
Si
ng
le
 F
ixa
tio
n 
Du
ra
tio
n Prev. Word Frequency
?
?
high
low
Figure 1: Frequency spillover in the List Lexical
Decision Task. Single fixation durations (fixations
when the word was fixated only once) on words
as a function of the fixated and previous word?s
frequency. Frequencies are binned by a median
split; error bars are bootstrapped standard errors.
We call this the parafoveal review explanation.
(2) During fixation on word
n?1
, perceptual infor-
mation from word
n
is available in the parafovea;
the frequency of word
n?1
affects the degree to
which this information is processed, and this in
turns affects the subsequent fixation duration on
word
n
. We call this the parafoveal preview expla-
nation. (3) During fixation of word
n
, processing
of word
n?1
continues based on some memory of
the perception of word
n?1
, and this processing is
affected by the frequency of word
n?1
. We call this
the memory explanation.
It is unlikely that spillover is driven by
parafoveal review because the effective visual field
in reading does not extend to the left of the current
word (Rayner et al., 1980).
The standard paradigm for investigating the re-
lationship between spillover effects and parafoveal
preview is some form of parafoveal masking
(Rayner, 1975): a nonveridical preview of word
n
is shown until the eye crosses an invisible bound-
ary just before word
n
, at which point word
n
is
shown. When participants are not informed of
the manipulation or do not notice it, they do not
exhibit frequency spillover (Henderson and Fer-
reira, 1990; Kennison and Clifton, 1995; White et
al., 2005). However, when participants are aware
of preview being unavailable or not veridical, the
spillover frequency effect remains (White et al.,
2005; Schroyens et al., 1999). These results sug-
gest that parafoveal preview (or review) cannot be
the only explanation of spillover and therefore the
2
Figure 2: Example dynamics of a decision to saccade from word
n?1
to word
n
. The memory-driven
attention shift decision can delay the start of perceptual sampling on the next word, potentially creating
spillover. A detailed description of the dynamics depicted in this figure is in ?4.
memory explanation warrants consideration. We
now summarize unpublished data consistent with
these findings in a simple linguistic task that we
also use to test the new model reported below.
Spillover in the List Lexical Decision Task
(LLDT). We use the List Lexical Decision Task
(LLDT) (Lewis et al., 2013), an extension of a task
introduced by Meyer and Schvaneveldt (1971). In
the LLDT participants must determine whether
a list of six strings contains all words, or con-
tains a single nonword. All strings are four char-
acters in length and separated by six character
spaces. The task was designed to require sequen-
tial eye-movements and contact with the mental
lexicon (but not higher-level linguistic process-
ing), to minimize parafoveal processing (via the
wide spacing), and to yield a high proportion of
single-fixation durations (via short strings).
Two versions of the task were performed by
separate participant groups. In the masked con-
dition, we used a gaze-contingent moving window
paradigm wherein all strings but the fixated string
were replaced with hashmarks (####). In the un-
masked condition, all six strings remained visible.
Figure 1 shows the effects of word frequency on
single fixation durations. The main result of cur-
rent interest is that frequency spillover is evident
in both conditions, despite the wide spacing in the
unmasked condition, and the complete denial of
parafoveal preview in the masked condition.
The work reviewed above and our new data
are consistent with an account of spillover in
which both parafoveal preview (if available) and
memory-based processing are operative. Our con-
cern here is with the latter: understanding how a
noisy memory of recently seen stimuli might be
incorporated into an adaptive oculomotor architec-
ture, and exploring whether rational exploitation
of that memory might lead to spillover.
4 A model of saccadic control with noisy
memory for recent perception
Our new model extends the one presented in Lewis
et al. (2013) to include a noisy memory that
buffers perceptual input. We develop it in the con-
text of the LLDT, but its essential elements are not
tied to this task. It is most easily understood by
first considering the dynamics of a single decision
to saccade from one word to the next, as presented
in Figure 2. After describing these dynamics we
summarize the model?s key assumptions and asso-
ciated mathematical specification.
The dynamics of a decision to saccade from
word
n?1
to word
n
. The eye first fixates
word
n?1
. Some time passes before information
from the retina becomes available for perceptual
processing (the eye-brain lag, EBL in Figure 2). A
sequence of noisy perceptual samples then arrive
and are integrated via an incremental and noisy
Bayesian update of a probability distribution over
lexical hypotheses in a manner described below.
The perceptual samples are also buffered by stor-
ing them in a memory that contains samples from
only one word. When the probability of one of the
hypotheses reaches the saccade threshold, saccade
planning is initiated. Perceptual sampling (marked
as free sampling in Figure 2 because its length is
not under adaptive control) continues in parallel
with saccade planning until the fixation ends, and
then for another EBL amount longer (these are
samples received at the retina during the fixation
and only now arriving at the lexical processor).
The model then switches to sampling from its
memory, continuing to update the distribution over
lexical hypotheses until one of the hypotheses
reaches an attention shift threshold. If this thresh-
old had already been reached during the earlier
perceptual sampling stages, attention shifts in-
stantly. Otherwise attention remains on word
n?1
even if the eye has saccaded to word
n
, and the eye-
3
brain lag on word
n
is completed. Perceptual sam-
ples from word
n
will not be processed until atten-
tion is shifted away from the memory-based pro-
cessing of word
n?1
. Thus the memory processing
on word
n?1
may delay processing of perceptual
samples from word
n
; perceptual samples arriving
during this time are buffered in the memory. In
this way the posterior update is a limited compu-
tational resource and its relative allocation to per-
ception or memory is determined by the saccade
and attention shift thresholds. To the extent that
the time to reach the attention shift threshold is
sensitive to the frequency of word
n?1
, the model
may exhibit a spillover frequency effect.
Lexical processing as rise-to-threshold deci-
sionmaking. The decisions to plan a saccade,
shift attention, and make a motor response are re-
alized as Multi-hypothesis Sequential Probability
Ratio Tests (Baum and Veeravalli, 1994; Dragalin
et al., 2000). At each timestep, the model per-
forms a Bayes update based on a noisy sample
drawn from perception or memory, with the pos-
terior at each timestep becoming the prior for the
next timestep. Our choice of word representation
follows Norris (2006) in representing a letter as a
unit-basis vector encoding and a word as a con-
catenation of such vectors.
To generate a perceptual sample, mean-zero
Gaussian perception noise with standard devia-
tion (SD) ?
p
is added to each component of the
word representation vector. Each perceptual sam-
ple is also stored in a memory buffer, and mem-
ory samples are generated by uniformly draw-
ing a stored sample from memory (with replace-
ment), and adding an additional mean-zero Gaus-
sian memory noise with SD ?
m
to each posi-
tion. Before each Bayesian update, whether us-
ing a sample from perception or memory, mean-
zero Gaussian update noise with SD ?
u
is added to
each component of the word representation vector.
Thus a Bayes update from a perceptual sample in-
cludes two noise terms, while a Bayes update from
a memory sample includes three noise terms. All
noises are drawn independently. The three SD?s,
?
p
, ?
m
and ?
u
, are free parameters in the model,
and we explore their implications below.
The model uses the update specified in the ap-
pendix in Lewis et al. (2013) except for the noise
generation specified above and the consequent
change in the likelihood computation. The lexical
hypotheses are updated as follows:
Pr
new
(S
k
|s
k
, T ) =
Pr(s
k
|S
k
, T )Pr
old
(S
k
, T )
?
S
Pr(s
k
|S
k
, T )Pr
old
(S
k
, T )
(1)
where s
k
is a sample generated as above from the
letterstring (word or nonword) in the current posi-
tion k, S
k
is the hypothesis that the string at posi-
tion k is S, and T is a multinomial distribution re-
flecting the current belief of (a) whether this is an
all-words trial and (b) otherwise, where the non-
word is located. The eye movement planning and
attention shift decisions are conditioned on the dis-
tribution of probabilities Pr(S
k
) for all strings in
the current position. When the maximum of these
probabilities crosses a saccade planning threshold
?
s
, saccade planning begins. When the maximum
crosses the attention shift threshold ?
a
, attention
shifts to the next word
1
. Each sample takes 10ms,
a fixed discretization parameter.
The likelihood of drawing perceptual or mem-
ory sample s for a string S is computed from the
unit-basis word representation as follows:
Pr(s|S) =
?
i
f(s
i
;?
i
, ?) (2)
where i indexes the unit-basis vector representa-
tion of sample s and some true letterstring S (and
so ?
i
is either 0 or 1), ? is the sampling noise
(dependent on whether the samples are memory
or perceptual samples as specified below), and
f(x;?, ?) is the probability density function of the
normal distribution with mean ? and standard de-
viation ?.
We simplify the likelihood computation for
memory samples by treating the perception and
memory samples as independent. For present
purposes this assumption may be treated as a
bound on the architecture. The ? in Equa-
tion 2 is
?
(?
2
p
+ ?
2
u
) for perceptual samples and
?
(?
2
p
+ ?
2
m
+ ?
2
u
) for memory samples. At each
sample the string-level probabilities in each posi-
tion are aggregated to the multinomial trial-level
decision variable T as described above. Given T
the model computes the probability of a word trial
Pr(W) or nonword trial Pr(N ) = 1 ? Pr(W).
When either of these probabilities exceeds the mo-
tor response threshold ?
r
, motor response plan-
ning commences.
1
Because there is a fixed set of memory samples available,
the attention shift decision is not guaranteed to converge, un-
like the saccade threshold. It nearly always converges, but we
use a 30-sample deadline to prevent infinite sequences.
4
0.5 1.5 2.5
?
??
?
?
?
??
?
???
?
?
?
?
??
?
??
???
?
??
?
?
?
?
?
?
?
?
?
???
??
?
?
?
??
?
0
20
40
60
0
20
40
60
0
20
40
60
0.5
1.5
2.5
0.5 1.5 2.5 0.5 1.5 2.5 0.5 1.5 2.5
Update Noise
Sp
illo
ve
r E
ffe
ct
Perceptual Noise
Memory Noise
Figure 3: Spillover effects generated by the top 5% of policies across different settings of memory, per-
ception, and update noise. On each distinct machine defined by a combination of noise settings, policies
(settings of ?
s
, ?
m
, ?
r
) were evaluated by the same task payoff given to human participants in the exper-
iment described in ?3. Boxplots show spillover effects of the top-performing 5% of policies. Spillover
effects are the difference in mean single fixation durations on word
n
when word
n?1
is low frequency
and when word
n?1
is high frequency (low/high determined by median split). The highest noise settings
in the bottom row are not shown because performance was near-chance even for the best policies.
The prior probability of an all-words trial is 0.5,
so the prior probability of a word in each position
k is 1?
0.5
6
. Therefore, we set the prior probabili-
ties of words in each position to corpus frequency
counts (Ku?cera and Francis, 1967), normalized to
sum to this value, 1 ?
0.5
6
. Nonword probabilities
are uniformly distributed over the remainder,
0.5
6
.
Oculomotor and Manual Architecture. The
remainder of the architectural parameters are stage
durations that are simulated as gamma deviates
with means based on previous work or indepen-
dently estimated from data. The key parameters
for present purposes are the 50ms mean eye-brain
lag and 125ms saccade planning time, following
Reichle et al. (2009), and the 40ms mean sac-
cade execution time, based on estimates from our
own human participants. The standard deviation
of each distribution is 0.3 times the mean. We
transform the means and standard deviations into
scale and shape parameters for a Gamma distri-
bution and then draw duration values from these
Gammas independently for every word and trial.
5 A computational rationality analysis
We explore whether spillover effects might be a
signature of computationally rational behavior in
two ways. First, we evaluate a space of policies
(parameterized by ?
s
, ?
m
, ?
r
) against the task pay-
off given to our human participants, and show that
top-performing policies yield frequency spillover
consistent with human data, and poor-performing
policies do not. Second, we extend the model?s
policy space to allow it to prioritize perception
over memory samples when both are available
(eliminating spillover in those policies), and show
that the spillover portions of the policy space per-
form better than non-spillover ones under any im-
posed speed-accuracy tradeoff in plausible noise
settings, and never perform worse.
In computational rationality analyses, we dis-
tinguish between policy parameters, fixed archi-
tecture parameters, and free architecture parame-
ters. Policy parameters are determined by select-
ing those policies that maximize a given task pay-
off, given the hypothesized architectural bounds.
Fixed architecture parameters are based on pre-
vious empirical or theoretical work. Free archi-
tecture parameters can be fit to data or explored
to show the range of predictions with which the
model is compatible. We focus here on the lat-
ter, showing not only that the model is compatible
with human data, but that it is incompatible with
results significantly different from the human data.
Our first evaluation of the model asks the ques-
tion of whether we see spillover effects emerging
in approximately optimal policies under our as-
sumptions about mechanism and task. We eval-
uated our model in the LLDT, under the balanced
payoff presented in Lewis et al. (2013), the same
5
??
?
?
?
?
?
?
?
?2
0
2
4
6
0.5 1.0 1.5 2.0 2.5
Memory Noise
M
ea
n 
Ra
tio
 o
f N
?1
 to
 
 N
 F
re
qu
en
cy
 E
ffe
ct Best 5% of policies
Best 5% of policies with 
 memory threshold = 0
Bottom 50% of policies
Model
?
?
?
masked unmasked
Humans
Figure 4: Normalized spillover effect in model (vs. memory noise) and human participants. We define
normalized spillover as the ratio of the spillover (word
n?1
) frequency effect size to the foveal (word
n
)
frequency effect size; this normalizes against scale differences between high and low noise architectures.
Left: Mean normalized spillover effect at different memory noises for best performing 5% of policies
with and without memory sampling, and worst 50% performing policies. Right: Mean human spillover
effect sizes in masked and unmasked versions of LLDT.
payoff given to our participants in the unpublished
masking experiment described above. We ex-
plored a discretized policy space as follows: we let
?
s
range between 0.199 and 0.999 in steps of 0.05;
?
m
between 0.19999 and 0.99999 in steps of 0.05,
and also include ?
m
= 0 which prevents memory
sampling; and ?
r
between 0.599 and 0.999 in steps
of 0.1. We explored all 1530 permutations.
Figure 3 shows the distribution of spillover ef-
fect sizes in the top 5% of policies (evaluated by
task payoff, not fit to human data), for a range
of noise parameter settings (at higher noise set-
tings, even the best policies are close to chance
performance). The top 5% of policies average 7.78
points per trial across the noise and policy range,
and the bottom 50% average 1.32 points. The fig-
ure shows that top-performing policies show lit-
tle to no spillover when update noise is low, posi-
tive but small spillover effects when update noise
is moderate, and sizable positive spillover effects
when update noise is relatively high. These results
are consistent with spillover as a rational adapta-
tion to belief update noise.
Figure 4 (left panel) shows normalized spillover
effects (the ratio of the word
n?1
frequency effect
to the word
n
frequency effect) for the best poli-
cies, the bottom 50% of policies, and the best
policies constrained with a memory threshold of
zero (?
m
= 0). When ?
m
= 0, the spillover ef-
fect is zero as expected. The top performing poli-
cies in the unconstrained space generate nonzero
spillover effects that are consistent with the human
data, but the poor performing policies do not (Fig-
ure 4, right panel). We know that the top perform-
ing policies exploit memory because they do yield
nonzero spillover effects, and the values of ?
m
are
nonzero for these policies.
Our second evaluation asks whether a model
that is constrained to always give priority to pro-
cessing perceptual samples over memory samples
will perform better than the present model, which
has the flexibility to give priority to memory over
perception. To explore this, we added a single bi-
nary policy parameter, the perceptual priority bit.
If this bit is set, then the model has the choice be-
tween memory sampling from word
n?1
and per-
ceptual sampling from word
n
, it always chooses
the latter. Such an option is not available in the
previous model?there is no setting of the saccade
and memory thresholds that will always use mem-
ory samples when only they are available, but also
never choose to use memory samples when per-
ceptual samples can be used. With the perceptual
priority bit set, the model is capable of exploiting
the least noisy samples available to it, but is inca-
pable of exhibiting spillover effects.
Figure 5 shows speed-accuracy tradeoffs for
the model, with the perceptual-priority bit not set
(spillover-capable) and set (spillover-incapable),
in three representative noise settings. Individual
points are policies and the lines mark the best ac-
curacy available at a particular reaction time for
the two classes of policies; i.e. these lines repre-
sent the best speed-accuracy tradeoff possible for
6
0.5, 0.5, 0.5 0.5, 0.5, 2.5 1.5, 1.5, 1.5
0.5
0.6
0.7
0.8
0.9
1.0
1000 2000 3000 1000 2000 3000 1000 2000 3000
RT
Ac
cu
ra
cy
spillover <= 5ms
spillover > 5ms
Spillover?capable
spillover?incapable
Figure 5: Speed-accuracy tradeoff curves for some representative noise settings. Each individual point
corresponds to one policy (i.e. setting of the three decision thresholds). Plotted are mean trial RT and
accuracy (computed from 5000 simulated trials), color-coded by whether the policies yielded spillover
frequency effects. Lines mark the best speed-accuracy tradeoff available to spillover-capable and inca-
pable policies. Each plot is labeled at the top with the noise setting (perceptual, memory, update).
both spillover-capable and -incapable policies. In
the left plot of the figure, noise is low enough over-
all such that responses are very fast and spillover-
capable policies do no worse and no better than
spillover-incapable policies. In the middle plot,
update noise is higher, and the optimal speed-
accuracy tradeoff is better for the model that can
yield spillover, consistent with the exploitation of
memory sampling to mitigate update noise. In the
right plot, perception and memory noise are high
enough that it is not useful to sample from mem-
ory at the expense of perception. All the noise
settings we explored (see Figure 3 for the range)
yield one of these three patterns, or the uninter-
esting case of near-chance performance. In no
setting does the spillover-capable model perform
worse than the spillover-incapable one. The noise
settings cover a range from implausibly-high ac-
curacy to chance performance, and so we con-
clude that spillover-capable policies dominate, in
that they do no worse, and occasionally do better,
than those constrained to give priority to percep-
tion over memory.
6 Why spillover arises from sequenced
thresholded samplers
We have demonstrated through simulations that
the model yields frequency spillover through a
composed sequence of perception and memory
sampling. We have not yet addressed the ques-
tion of how or why this happens. Indeed, it is ini-
tially somewhat puzzling that an effect of priors
(set by lexical frequency) would persist after the
initial perceptual sampling threshold ?
p
is passed,
because this fixed threshold must be exceeded no
matter the starting prior.
The crucial insight is that it is not always the
case that the true word hypothesis reaches the
threshold first; i.e., the decision to initiate saccade
planning may be based on (partial) recognition of
a different word than the true word. In such cases,
at the start of memory sampling, the hypothesis for
the true word is farther from the memory threshold
?
m
than if the true word had been (partially) recog-
nized. Incorrect decisions are more likely for low
frequency words, so in expectation the memory-
driven attention shift mechanism will start farther
from its threshold for low-frequency words, and
therefore take longer to reach threshold, delaying
the following word more.
We constructed a minimal two-sampler exam-
ple to clearly illustrate this phenomenon. The left-
most panel of Figure 6 illustrates the dynamics of
such a trial. In this panel, the threshold is crossed
for the incorrect hypothesis (green line) in the first
sampler, triggering the start of the second sampler.
The second sampler recovers from the mistake, al-
lowing the correct (red) hypothesis to cross the
threshold, but at the cost of additional time. The
middle panel shows that incorrect (and thus eligi-
ble for recovery) trials are more frequent for low
priors. The rightmost panel shows that the finish-
ing time of the second sampler is proportional to
the prior probability of the correct hypothesis for
the first sampler. It is also inversely proportional
to accuracy (middle plot), consistent with inaccu-
rate trials driving the relationship between the first
sampler prior and second sampler finishing times.
7
0 50 100 150 2000
.0
0.2
0.4
0.6
0.8
1.0
sample
pro
bab
ility
Sampler 1 Sampler 2 ll
l
l
l
l
lllllll
l
l
l
l
l
l
l
l
l
l
llllll
lllllllllllll
l
l
l
l
l
l
l
l
l
llllllllllll
l
l
l
l
0.00
0.25
0.50
0.75
1.00
1e?05 1e?04 1e?03 1e?02 1e?01Starting prior of correct
 hypothesis (Sampler 1)
Acc
ura
cy
(Sam
pler 
1)
Number of Hypotheses
l
l
l
l
2
5
100
1000
ll
l
l
l
l
lllll
ll
l
l
l
l
l
l
l
l
l
llllll
l
lllll
llllll
ll
ll
l
l
l
l
l
l
l
l
ll
ll
lllll
llllllll
ll
l
l
l
l
0
25
50
75
1e?05 1e?04 1e?03 1e?02 1e?01Starting prior of correct
 hypothesis (Sampler 1)
Num
. sa
mp
les 
to t
hre
sho
ld
(Sam
pler 
2, co
rrec
t cas
es)
Number of Hypotheses
l
l
l
l
2
5
100
1000
Figure 6: A simple example illustrating how the prior for a thresholded sampler affects its final posterior,
and therefore the prior for a subsequent coupled sampler, despite the fixed threshold. Left: An example
?recovery? trial for 500 hypotheses (words). Middle: Accuracy for the first sampler as a function of the
prior of the true hypothesis. Right: Second sampler finishing times as a function of to the true-hypothesis
prior in the first sampler.
7 Discussion and Conclusion
We briefly highlight the key properties of the
model that yield our result and how they may gen-
eralize beyond our particular implementation.
Post-perceptual processing. Although we
adopted a second MSPRT sampler, spillover may
arise from other processes with access to the pos-
terior of the perceptual sampling, such that it can
recover from perceptually misidentified words. In
the present model we investigated the possibil-
ity that post-perceptual memory-based processing
could be partially motivated by mitigating noise
in the update process itself. But it is almost cer-
tainly the case that post-perceptual processing is
required in the course of reading for indepen-
dent reasons, and such processing could also yield
spillover frequency effects in a way that the mem-
ory sampling process does. (A challenge for such
an alternate process is that spillover effects per-
sist in the LLDT in the absence of required higher
level syntactic or semantic processing).
A tradeoff between processing perception and
memory. The serial queuing model is a simple re-
alization (inspired by EZ-Reader (Reichle et al.,
1998)) of a limited resource that can be allocated
to perceptual and memory processing, but an alter-
native parallel attention machine might recover the
results, as long as it suffers from the same tradeoff
that processing the previous word from memory
will slow down processing of the fixated word.
Direct oculomotor control. In the present model
saccade planning is triggered directly by the per-
ceptual evidence accumulation process, and as
such is not obviously compatible with autonomous
saccade generation models like SWIFT (Engbert
et al., 2005). It may be possible to layer SWIFT?s
time-delayed foveal inhibition over a sequential
sampling process, but we note that spillover ef-
fects were part of the empirical motivation for
such delayed control.
The present model and results open several av-
enues for future work. These include the interac-
tions of memory-based or post-perceptual process-
ing with models of saccade planning that include
saccade targeting, re-targeting, and cancellation,
as well as buttonpress behavior (e.g. in the self-
paced moving window paradigm). The role that
parafoveal preview plays in spillover effects can
also be explored, including how the model (and
thus human participants) might navigate the trade-
off between using parafoveal preview information
(noisy due to eccentricity) and using memory of
past input in the service of a reading task. Fi-
nally, it is possible to explore the spillover expla-
nation in an architecture capable of higher-level
sentence processing in service of different reading
task goals.
Acknowledgments
This material is based upon work supported by the
National Science Foundation under Grant BCS-
1152819 to RL and SB. We thank MindMod-
eling@Home for invaluable computational re-
sources.
8
References
C.W. Baum and V.V. Veeravalli. 1994. A sequential
procedure for multihypothesis testing. IEEE Trans-
actions on Information Theory, 40(6):1994?2007.
Vladimir P Dragalin, Alexander G Tartakovsky, Venu-
gopal V Veeravalli, and Senior Member. 2000. Mul-
tihypothesis Sequential Probability Ratio Tests Part
II : Accurate Asymptotic Expansions for the Ex-
pected Sample Size. IEEE Transactions on Infor-
mation Theory, 46(4):1366?1383.
Ralf Engbert, Antje Nuthmann, Eike M. Richter, and
Reinhold Kliegl. 2005. SWIFT: a dynamical model
of saccade generation during reading. Psychological
review, 112(4):777?813, October.
John M. Henderson and Fernanda Ferreira. 1990.
Effects of foveal processing difficulty on the per-
ceptual span in reading: Implications for attention
and eye movement control. Journal of Experimen-
tal Psychology: Learning, Memory, and Cognition,
16(3):417?429.
Sheila M. Kennison and Charles Clifton. 1995. De-
terminants of parafoveal preview benefit in high and
low working memory capacity readers: implications
for eye movement control. Journal of experimen-
tal psychology. Learning, memory, and cognition,
21(1):68?81, January.
Henry Ku?cera and W. Nelson Francis. 1967. Compu-
tational analysis of present-day American English.
Brown University Press, Providence, RI.
Richard L. Lewis, Michael Shvartsman, and Satinder
Singh. 2013. The Adaptive Nature of Eye Move-
ments in Linguistic Tasks: How Payoff and Archi-
tecture Shape Speed-Accuracy Trade-Offs. Topics
in cognitive science, pages 1?30, June.
Richard L. Lewis, Andrew Howes, and Satinder Singh.
to appear. Computational rationality: Linking
mechanism and behavior through utility maximiza-
tion. Topics in Cognitive Science.
David E. Meyer and Roger Schvaneveldt. 1971. Fa-
cilitation in recognizing pairs of words: Evidence of
a dependence between retrieval operations. Journal
of Experimental Psychology, 90:22?34.
Dennis Norris. 2006. The Bayesian reader: explain-
ing word recognition as an optimal Bayesian de-
cision process. Psychological review, 113(2):327?
357, April.
Keith Rayner and Martin H. Fischer. 1996. Mind-
less reading revisited: eye movements during read-
ing and scanning are different. Perception & psy-
chophysics, 58(5):734?47, July.
Keith Rayner, Arnold D. Well, and Alexander Pollat-
sek. 1980. Asymmetry of the effective visual field
in reading. Perception & psychophysics, 27(6):537?
44, June.
Keith Rayner. 1975. The perceptual span and periph-
eral cues in reading. Cognitive Psychology, 7(1):65?
81, January.
E D Reichle, a Pollatsek, D L Fisher, and K Rayner.
1998. Toward a model of eye movement control in
reading. Psychological review, 105(1):125?57, Jan-
uary.
Erik D. Reichle, Tessa Warren, and Kerry McConnell.
2009. Using E-Z Reader to model the effects of
higher level language processing on eye movements
during reading. Psychonomic bulletin & review,
16(1):1?21, February.
Walter Schroyens, Franc?oise Vitu, Marc Brysbaert, and
G?ery D?Ydewalle. 1999. Eye movement control
during reading: foveal load and parafoveal process-
ing. The Quarterly journal of experimental psychol-
ogy, 52(4):1021?46, November.
Sarah J. White, Keith Rayner, and Simon P. Liv-
ersedge. 2005. Eye movements and the modulation
of parafoveal processing by foveal processing dif-
ficulty: A reexamination. Psychonomic bulletin &
review, 12(5):891?6, October.
Christiane Wotschack. 2009. Eye Movements in Read-
ing Strategies. Ph.D. thesis.
9
